Proceedings of the 12th Conference of the European Chapter of the ACL, pages 719?727,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsWord Lattices for Multi-Source TranslationJosh Schroeder, Trevor Cohn, and Philipp KoehnSchool of InformaticsUniversity of Edinburgh10 Crichton Street, Edinburgh EH8 9ABScotland, United Kingdom{jschroe1, tcohn, pkoehn}@inf.ed.ac.ukAbstractMulti-source statistical machine transla-tion is the process of generating a singletranslation from multiple inputs.
Previouswork has focused primarily on selectingfrom potential outputs of separate transla-tion systems, and solely on multi-parallelcorpora and test sets.
We demonstrate howmulti-source translation can be adapted formultiple monolingual inputs.
We also ex-amine different approaches to dealing withmultiple sources, including consensus de-coding, and we present a novel methodof input combination to generate latticesfor multi-source translation within a singletranslation model.1 IntroductionMulti-source statistical machine translation wasfirst formally defined by Och and Ney (2001)as the process of translating multiple meaning-equivalent source language texts into a single tar-get language.
Multi-source translation is of par-ticular use when translating a document that hasalready been translated into several languages, ei-ther by humans or machines, and needs to be fur-ther translated into other target languages.
Thissituation occurs often in large multi-lingual organ-isations such as the United Nations and the Euro-pean Parliament, which must translate their pro-ceedings into the languages of the member in-stitutions.
It is also common in multi-nationalcompanies, which need to translate product andmarketing documentation for their different mar-kets.
Clearly, any existing translations for a docu-ment can help automatic translation into other lan-guages.
These different versions of the input canresolve deficiencies and ambiguities (e.g., syntac-tic and semantic ambiguity) present in a single in-put, resulting in higher quality translation output.In this paper, we present three models of multi-source translation, with increasing degrees of so-phistication, which we compare empirically on anumber of different corpora.
We generalize thedefinition of multi-source translation to includeany translation case with multiple inputs and a sin-gle output, allowing for, e.g., multiple paraphrasedinputs in a single language.
Our methods includesimple output selection, which treats the multi-source translation task as many independent trans-lation steps followed by selection of one of theiroutputs (Och and Ney, 2001), and output combina-tion, which uses consensus decoding to constructa string from n-gram fragments of the translationoutputs (Bangalore et al, 2001).
We also presenta novel method, input combination, in which wecompile the input texts into a compact lattice, overwhich we perform a single decoding pass.
Weshow that as we add additional inputs, the simplestoutput selection method performs quite poorly rel-ative to a single input translation system, while thelatter two methods are able to make better use ofthe additional inputs.The paper is structured as follows.
?2 presentsthe three methods for multi-source translation indetail: output selection, output combination, andour novel lattice-based method for input combina-tion.
We report experiments applying these tech-niques to three different corpora, with both mono-lingual inputs (?3) and multilingual inputs (?4).We finish in ?5 by analyzing the benefits and draw-backs of these approaches.2 Approaches to Multi-SourceTranslationWe now present three ways to combine multipleinputs into a single output translation, in the con-text of related work for each technique.7192.1 Output SelectionThe most straightforward approach to multi-source translation, proposed by Och and Ney(2001), is to independently translate each of theN source languages and then select a singletranslation from the outputs.
Given N sourcessN1 = s1, .
.
.
, sN , first translate each with a sep-arate translation system, p1, .
.
.
, pN , to obtain Ntarget translations, tN1 = t1, .
.
.
, tN .
Och and Neypresent two approaches for selecting a single tar-get from these outputs.The first, PROD, finds the maximiser of theproduct, argmaxt?tN1 p(t)?Nn=1 pn(sn|t), wherep(t) is the language model probability.
For rea-sons of tractability, the maximisation is performedonly over targets generated by the translation sys-tems, tN1 , not the full space of all translations.The PROD method requires each model to pro-vide a model score for each tn generated by theother models.
However, this is often impossibledue to the models?
highly divergent output spaces(Schwartz, 2008), and therefore the technique can-not be easily applied.The second approach, MAX, solvesargmaxt?tN1 maxNn=1 p(t)pn(sn|t), which ismuch easier to calculate.
As with PROD, thetranslation models?
outputs are used for thecandidate translations.
While different modelsmay have different score ranges, Och and Ney(2001) state that there is little benefit in weightingthese scores to normalise the output range.
In theirexperiments, they show that MAX used on pairs ortriples of language inputs can outperform a modelwith single language input, but that performancedegrades as more languages are added.These methods limit the explored space to a fulltranslation output of one of the inputs, and there-fore cannot make good use of the full diversity ofthe translations.
In this paper we present MAXscores as a baseline for output selection, and ap-proximate an oracle using the BLEU metric as anupper bound for the output selection technique.2.2 Output CombinationConsensus decoding as a form of system combi-nation is typically used to integrate the outputs ofmultiple translation systems into a single syntheticoutput that seeks to combine the best fragmentsfrom each component system.
Multi-source trans-lation can be treated as a special case of consen-sus decoding.
Indeed, several authors have seenthe  dog barked very loudlya big dog barked  loudlysub insert ?
shift delete ?Table 1: Example minimum TER edit script.0 1thea 2?big 3dog 4barked 5very?
6loudlyFigure 1: Conversion of TER script from Table 1to a confusion network.improvements in translation quality by perform-ing multi-source translation using generic systemcombination techniques (Matusov et al, 2006;Paulik et al, 2007).One class of approaches to consensus decodingfocuses on construction of a confusion networkor lattice1 from translation outputs, from whichnew sentences can be created using different re-orderings or combinations of translation fragments(e.g., Bangalore et al (2001); Rosti et al (2007b)).These methods differ in the types of lattices used,their means of creation, and scoring method usedto extract the best consensus output from the lat-tice.
The system used in this paper is a variant ofthe one proposed in Rosti et al (2007a), which wenow describe in detail.The first step in forming a lattice is to align theinputs.
Consensus decoding systems often use thescript of edit operations that minimises the transla-tion edit rate (TER; Snover et al (2006)).
TER isa word-based measure of edit distance which alsoallows n-gram shifts when calculating the bestmatch between a hypothesis and reference.
Be-cause TER describes the correspondence betweenthe hypothesis and reference as a sequence of in-sertions, substitutions, deletions, and shifts, theedit script it produces can be used to create a con-fusion network.Consider a reference of ?The dog barked veryloudly?
and a hypothesis ?A big dog loudlybarked.?
The TER alignment is shown in Ta-ble 1, along with the edit operations.
Note that thematching ?barked?
tokens are labelled shift, as oneneeds to be shifted for this match to occur.
Usingthe shifted hypothesis, we can form a confusion1Different authors refer to ?lattices,?
?confusion net-works,?
?word sausages,?
etc.
to describe these data struc-tures, and specific terminology varies from author to author.We define a lattice as a weighted directed acyclic graph, anda confusion network as a special case where each node n inthe ordered graph has word arcs only to node n + 1.720Confusion Network 1Confusion Network 2Confusion Network 3Figure 2: Structure of a lattice of confusion net-works for consensus decoding.network as in Figure 1.
Additional sentences canbe added by aligning them to the reference as well.Each link is weighted by the number of componentsentences sharing that particular word at the givenlocation.Similar to Rosti et al (2007a), we let each hy-pothesis take a turn as the ?reference?
for TER,using it as a skeleton for a confusion network.
Wethen form a lattice of confusion networks (Fig-ure 2), assigning a prior weight to each confusionnetwork based on the average TER of the selectedskeleton with the other hypotheses.
This allowseach system to set the word order for a componentconfusion network, but at the cost of a more com-plex lattice structure.We can score pathsP through these lattices withthe assistance of a language model.
Formally, thepath score is given by:w(P) = ?
log pLM (t(P))+?d?P[N?n=1?n log pn(d|sn)+ ??
(d, ) + ?(1?
?
(d, ))]where pLM is the language model probability ofthe target string specified by the lattice path, t(P),pn(d|sn) is the proportion of system n?s k-bestoutputs that use arc d in path P , and the last twoterms count the number of epsilon and non-epsilontransitions in the path.
The model parameters are?1, .
.
.
, ?n, ?, ?, ?, which are trained using Pow-ell?s search to maximise the BLEU score for thehighest scoring path, argmaxP w(P).2.3 Input CombinationLoosely defined, input combination refers to find-ing a compact single representation of N transla-tion inputs.
The hope is that the new input pre-serves as many of the salient differences betweenthe inputs as possible, while eliminating redundantinformation.
Lattices are well suited to this task.0 1?watchit 2?out's 3?for 4thepursepicka?5robberthiefsnatcherburglarcrookpocket6.Figure 3: A monolingual confusion network.Thicker lines indicate higher probability wordarcs.When translating speech recognition output,previous work has shown that representing theambiguity in the recognized text via confusionnetworks leads to better translations than simplytranslating the single best hypothesis of the speechrecognition system (Bertoldi et al, 2007).
The ap-plication of input lattices to other forms of inputambiguity has been limited to encoding input re-orderings, word segmentation, or morphologicalsegmentation, all showing improvements in trans-lation quality (Costa-jussa` et al, 2007; Xu et al,2005; Dyer et al, 2008).
However, these appli-cations encode the ambiguity arising from a sin-gle input, while in this work we combine distinctinputs into a more compact and expressive singleinput format.When given many monolingual inputs, we canapply TER and construct a confusion network asin Section 2.2.2 In this application of confusionnetworks, arc weights are calculated by summingvotes from each input for a given word, and nor-malizing all arcs leaving a node to sum to 1.Figure 3 shows an example of a TER-derivedinput from IWSLT data.
Because the decoder willhandle reordering, we select the input with thelowest average TER against the other inputs toserve as the skeleton system, and do not create alattice with multiple skeletons.The problem becomes more complex when weconsider cases of multi-lingual multi-source trans-lation.
We cannot easily apply TER across lan-guages because there is no clear notion of an exactmatch between words.
Matusov et al (2006) pro-pose using a statistical word alignment algorithmas a more robust way of aligning (monolingual)outputs into a confusion network for system com-2Barzilay and Lee (2003) construct lattices over para-phrases using an iterative pairwise multiple sequence align-ment (MSA) algorithm.
Unlike our approach, MSA does notallow reordering of inputs.721bination.
We take a similar approach for multi-lingual lattice generation.Our process consists of four steps: (i) Alignwords for each of the N(N ?
1) pairs of inputs;(ii) choose an input (or many inputs) to be thelattice skeleton; (iii) extract all minimal consis-tent alignments between the skeleton and the otherinputs; and (iv) add links to the lattice for eachaligned phrase pair.A multi-parallel corpus such as Europarl(Koehn, 2005) is ideally suited for training thissetup, as training data is available for each pair ofinput languages needed by the word aligner.
Weused the GIZA++ word alignment tool (Och andNey, 2003) for aligning inputs, trained on a por-tion of the Europarl training data for each pair.We select a skeleton input based on whichsingle-language translation system performs thebest when translating a development set.
For ourEuroparl test condition, this was French.We define a minimal consistent alignment(MCA) as a member of the set of multi-wordalignment pairs that can be extracted from a many-to-many word alignment between skeleton sen-tence x and non-skeleton sentence y with the fol-lowing restrictions: (i) no word in x or y is usedmore than once in the set of MCAs; (ii) wordsand phrases selected from y cannot be aligned tonull; and (iii) no smaller MCA can be decomposedfrom a given pair.
This definition is similar tothat of minimal translation units as described inQuirk and Menezes (2006), although they allownull words on either side.Different word alignment approaches will resultin different sets of MCAs.
For input lattices, wewant sets of MCAs with as many aligned wordsas possible, while minimising the average num-ber of words in each pair in the set.
Experimentswith GIZA++ on the Europarl data showed thatthe ?grow-diag-final-and?
word alignment sym-metrization heuristic had the best balance betweencoverage and pair length: over 85% of skeletonwords were part of a non-null minimal pair, andthe average length of each pair was roughly 1.5words.
This indicates that our lattices will pre-serve most of the input space while collapsing eas-ily alignable sub-segments.Once a set of phrase alignments has been found,we construct a lattice over the skeleton sentencex.
For each additional input yn we add a set oflinks and nodes for each word in x to any relevant?
podr?a darnos las cifras correspondientes a espa?a y grecia ?pouvez-vous nous donner leschiffres pour  l' espagne et la gr?ce ?siffrorna f?rkan ni ge oss spanien och grekland ?Figure 4: A multi-lingual alignment betweenFrench, Spanish and Swedish, showing the min-imal consistent alignments.
The lattice generatedby this alignment is shown in Figure 5.words in yn, rejoining at the last word in x thatis covered by the pair.
Figures 4 and 5 show anexample of the alignments and lattice generated byusing a French skeleton with Spanish and Swedishsentences.Once a lattice is created, we can submit it to aphrase-based decoder in place of text input.
Thedecoder traverses lattice nodes in a manner simi-lar to how words are traversed in text translation.Instead of one input word represented by each lo-cation in the coverage vector as in text input, withlattices there are a set of possible input word arcs,each with its own translation possibilities.
Theconcept of compatible coverage vectors for the lo-cations of translated words becomes the notion ofreachability between frontier nodes in the lattice(Dyer et al, 2008).It is possible to construct multi-skeleton lat-tices by connecting up a set of N lattices, eachbuilt around a different skeleton xn, in much thesame manner as multiple confusion networks canbe connected to form a lattice in output combina-tion.
With sufficient diversity in the input order-ing of each skeleton, the decoder need not performreordering.
Because of the size and complexityof these multi-skeleton lattices, we attempt onlymonotonic decoding.
In this scenario, as in con-sensus decoding, we hope to exploit the additionalword order information provided by the alternativeskeletons.3 Experiments: Monolingual InputWe start our experimental evaluation by translat-ing multiple monolingual inputs into a foreign lan-guage.
This is a best-case scenario for testingand analytic purposes because we have a singletranslation model from one source language to onetarget language.
While translating from multiplemonolingual inputs is not a common use for ma-chine translation, it could be useful in situationswhere we have a number of paraphrases of the in-put text, e.g., cross-language information retrievaland summarization.7220 5pouvez-vous1?
2kan 7darnos6nouspodr?a 3ni 4ge oss 9lasles 8siffrornadonner 11chiffres10cifras 12f?r 13l' 14espa?aspanienapourcorrespondientes espagne 15yetoch 17la 18grecia 16grekland gr?ce 19??
?Figure 5: A multi-lingual lattice input for French, Spanish, and Swedish from Europarl dev2006.Data sets for this condition are readily availablein the form of test sets created for machine trans-lation evaluation, which contains multiple targetreferences for each source sentence.
By flippingthese test sets around, we create multiple mono-lingual inputs (the original references) and a sin-gle reference output (the original source text).
Weexamine two datasets: the BTEC Italian-Englishcorpus (Takezawa et al, 2002), and the MultipleTranslation Chinese to English (MTC) corpora,3as used in past years?
NIST MT evaluations.All of our translation experiments use theMoses decoder (Koehn et al, 2007), and are eval-uated using BLEU-4.
Moses is a phrase-baseddecoder with features for lexicalized reordering,distance-based reordering, phrase and word trans-lation probabilities, phrase and word counts, andan n-gram language model.3.1 English to ItalianWe use the portion of the BTEC data made avail-able for the Italian-English translation task atIWSLT 2007, consisting of approximately 24,000sentences.
We also use the Europarl English-Italian parallel corpus to supplement our train-ing data with approximately 1.2 million out-of-domain sentences.
We train a 5-gram languagemodel over both training corpora using SRILM(Stolcke, 2002) with Kneser-Ney smoothing andlinear interpolation, the interpolation weight cho-sen to minimise perplexity on the Italian side ofthe development tuning set.For multiple translation data, we use IWSLTtest sets devset1-3 which have sixteen Englishtranslations for each Italian sentence.
The Ital-ian version of the BTEC corpus was created af-ter the original Japanese-English version, and onlythe first English translation was used to generatethe Italian data.
The other fifteen versions of eachEnglish sentence were generated as paraphrasesof the primary English translation.
We exploretranslation conditions using only the fifteen para-phrased inputs (?Para.?
in Table 2), as well as us-ing all sixteen English inputs (?All?
).3LDC2002T01, LDC2003T17, LDC2004T07 andLDC2006T04.All Para.BEST 40.06 24.02ORACLE 51.64 47.27MAX 29.32 23.94SYSCOMB 32.89 30.39CN INPUT 31.86 27.62Table 2: BLEU scores on the BTEC test set fortranslating English inputs into Italian.We tune our translation models on devset1, sys-tem combination on devset2 and report results ondevset3 for each condition.When tuning the single input ?Para.?
and ?All?baselines, we include all relevant copies of the 506lines of devset1 English data, and repeat the Ital-ian reference fifteen or sixteen times on the targetside, resulting in a total of 7,590 and 8,096 sen-tence pairs respectively.The results for devset3 are shown in Table 2.For comparison, we show the BEST score any in-put produced, as well as an approximated ORA-CLE output selection generated by choosing thebest BLEU-scoring output for each sentence usinga greedy search.
Our output combination method,SYSCOMB, uses no system-specific weights todistinguish the inputs.
For SYSCOMB and MAX,we translated all versions of the English input sep-arately, and we use the top ten distinct hypothe-ses from each input sentence for n-best input toSYSCOMB.For input combination, CN INPUT, we used theTER-based monolingual input lattice approach de-scribed in Section 2.3, choosing as a skeleton theinput with the lowest average TER score whencompared with the other inputs (assessed sepa-rately for each sentence).
Each input was givenequal probability in the confusion network links.Note that the quality of output from translat-ing the primary English input is much higher thanfrom translating any of the paraphrases.
The pri-mary input sentence scores a BLEU of 40.06, whilethe highest scoring paraphrased input managesonly a 24.02.
When we look at ?Para.?
the dif-ference in the scores when using a single input723(BEST) versus all the inputs (SYSCOMB and CNINPUT) is striking ?
clearly there is considerableinformation in the other inputs which can radicallyimprove the translation output.
Removing the pri-mary input from ORACLE reinforces this observa-tion: the score drops by only 4.37 BLEU despitethe nearly 16 BLEU drop for the single best input.Interestingly, the output selection technique,MAX, performs at a similar level to the combina-tion techniques when we include the primary in-put, but degrades when given only the lower qual-ity translations of paraphrased input under condi-tion ?Para.?
In previous work on multi-lingual out-put selection, the MAX score degraded after twoor three outputs were combined, but even with-out the primary reference it maintains a score nearthe best single paraphrased input when combiningfifteen outputs.
One possible explanation for thisis that the inputs are all being translated with thesame translation model, so comparing their scorescan give a more accurate ranking of their relativetranslation quality according to the model.
Theinput combination method, CN INPUT, performsbetter than MAX and only slightly worse than theoutput combination approach.3.2 English to ChineseWe can add an extra dimension to monolingualmulti-source translation by considering inputs ofdiffering quality.
A multi-source translation sys-tem can exploit features indicating the origin of theinput to improve output quality.
For these exper-iments, we use the MTC English-Chinese corpus,parts 1?4.
This data was translated from Chineseinto English by four teams of annotators, denotedE01?E04.
This allows us to examine the resultsfor translating the same team?s work over multipleyears.We train on the news domain portion of the of-ficial NIST data4 (excluding the UN and HongKong data) for both the translation model and the5-gram Chinese language model.While we still have a single translation model,all of our inputs are now of a traceable origin andare known to have quality differences when judgedby human evaluators.
With this information wecan tune one of two ways: We can create a set ofall input systems and replicate the reference as wedid for English to Italian translation (?All tuned?
),4http://www.nist.gov/speech/tests/mt/2008Team Tuning Part 3 Part 4E01 All 16.18 15.52E01 Self 16.02 15.63E02 All 14.29 14.00E02 Self 13.88 14.05E03 All 14.99 15.06E03 Self 15.10 14.94E04 All 14.03 12.65E04 Self 14.03 12.59Table 3: BLEU scores using single inputs fromeach different team on the MTC.
Bold indicatesthe better score between All and Self tuning.Approach Tuning Part 3 Part 4MAX All 15.06 15.08MAX Self 14.97 13.75SYSCOMB All 16.82 16.24SYSCOMB Self 16.87 16.45Table 4: BLEU scores for multi-source translationsof MTC test sets.
Better score for each output-based multi-source method is shown in bold.or we can tune each input using only the version ofthe tuning data generated by the same translationteam (?Self tuned?
).5 For example, we can tunea system with the MTC Part 2 data provided bytranslation team E01, and then decode E01?s trans-lations of parts 3 and 4 with the weights obtainedin tuning.
The results for each system are shownin Table 3.
Despite the different tuning conditions,there is no clear advantage to tuning to all inputsversus tuning to each input separately ?
on aver-age we see a 0.06 BLEU score advantage by using?All?
weights.With four different inputs to our multi-sourcetranslation system, and two ways of weighting thefeatures for each input, how can we best utilizethese systems in output selection and combina-tion?
We perform system combination and MAXselection and obtain the scores shown in Table 4.The consensus decoding approach uses system-specific features as described in Section 2.2 to dis-tinguish between E01-E04.As with English to Italian, output combinationperforms the best of the multi-source techniques.MAX performs better with translations generatedby ?All?
weights than with ?Self?, and the con-5Note that in the ?Self tuned?
setting we have only a quar-ter as much tuning data as for ?All tuned?.724Input Language test2006 test2007French (FR) 29.72 30.21Spanish (ES) 29.55 29.62Swedish (SV) 29.33 29.44Portuguese (PT) 28.75 28.79Danish (DA) 27.20 27.48Greek (EL) 26.93 26.78Italian (IT) 26.82 26.51German (DE) 24.04 24.41Dutch (NL) 23.79 24.28Finnish (FI) 18.96 18.85Table 5: BLEU scores for individual translationsystems into English trained on Europarl, frombest to worst.verse is true for SYSCOMB.
Given the robust per-formance of MAX when translation scores origi-nated from the same translation model in Englishto Italian, it is not surprising that it favors thecase where all the outputs are scored by the samemodel (?All tuned?).
On the other hand, diversityamongst the system outputs has been shown to beimportant to the performance of system combina-tion techniques (Macherey and Och, 2007).
Thismay give an indication as to why the ?Self tuned?data produced higher scores in consensus decod-ing ?
the outputs will be more highly divergent dueto their different tuning conditions.4 Experiments: Multilingual InputMultilingual cases are the traditional realm ofmulti-source translation.
We no longer have di-rectly comparable translation models; instead eachinput language has a separate set of rules for trans-lating to the output language.
However, the avail-ability of (and demand for) multi-parallel corporamakes this form of multi-source translation ofgreat practical use.4.1 Lattice InputsAs described in Section 2.3, lattices can be usedto provide a compact format for translating multi-lingual inputs to a multi-source translation system.We trim all non-skeleton node paths to a maximumlength of four to reduce complexity when decod-ing.
Such long paths are mostly a result of errors inthe original word alignments, and therefore prun-ing these links is largely innocuous.We train on the Europarl corpus and use theFR SV ES DA PT IT EL NL DE FIBLEU0.150.200.250.300.350.400.45 OracleMAXSoloFigure 6: Performance for multilingual multi-source translation (test2005) as each language in-put is added, showing Oracle target selection,MAX score, or just a single language input (Solo).in-domain test sets provided for previous years?Workshops on Statistical Machine Translation.Because of the computational complexity of deal-ing with so many models, we train on only the first100,000 sentences of each parallel corpus.
Sin-gle system baseline scores for each language areshown in Table 5.Besides comparing the different multi-sourcetranslation methods discussed above, in this taskwe also want to examine what happens when weuse different numbers of input languages.
To de-termine the best order to add languages, we per-formed a greedy search over oracle BLEU scoresfor test set test2005.
We started with the best scor-ing single system, French to English, and in eachiteration picked one additional system that wouldmaximise BLEU if we always selected the trans-lation system output closest to the reference.
Theresults are shown in Figure 6.The oracle selection order differs from the or-der of the best performing systems, which couldbe due to the high scoring systems having verysimilar output while lower scoring systems exhibitgreater diversity.
Interestingly, the order of thelanguages chosen iterates between the Roman andGermanic language families and includes Greekearly on.
This supports our claim that diversityis important.
Note though that Finnish, which isalso in a separate language family, is selected last,most likely due to difficulties in word alignmentand translation stemming from its morphologicalcomplexity (Birch et al, 2008).
This finding mightalso carry over to phrase-table triangulation (Cohnand Lapata, 2007), where multi-parallel data isused in training to augment a standard translation725Approach test2006 test2007French Only 29.72 30.21French + SwedishMAX 29.86 30.13LATTICE 29.33 29.97MULTILATTICE 29.55 29.88SYSCOMB 31.32 31.77French + Swedish + SpanishMAX 30.18 30.33LATTICE 29.98 30.45MULTILATTICE 30.50 30.50SYSCOMB 33.77 33.876 LanguagesMAX 28.37 28.33LATTICE 30.22 30.91MULTILATTICE 30.59 30.59SYSCOMB 35.47 36.03Table 6: BLEU scores for multi-source translationsystems into English trained on Europarl.
Singlesource French decoding is shown as a baseline.system.We choose to evaluate translation perfor-mance at three combination levels: two lan-guages (French and Swedish), three languages(+Spanish), and six languages (+Danish, Por-tuguese, Italian).
For each combination we ap-ply MAX, SYSCOMB, French skeleton lattice in-put translation LATTICE, and monotone decodingover multiple skeleton lattices, MULTILATTICE.Results are shown in Table 6.To enable the decoder used in LATTICE andMULTILATTICE to learn weights for differentsources, we add a feature to the phrase table foreach of the languages being translated.
This fea-ture takes as its value the number of words on thesource side of the phrase.
By weighting this fea-ture up or down for each language, the decoder canprefer word links from specific languages.As seen in previous work in multi-source trans-lation, MAX output selection performs well withtwo or three languages but degrades as more lan-guages are added to the input.
Conversely, ourlattice input method shows upward trends: LAT-TICE is comparable with MAX on three inputs andscores increase in the six language case.Given the higher scores for output combinationover input combination, what differences can weobserve between the systems?
Both systems havefeatures that indicate the contributions of each in-put language to the final output.
With input com-bination, we are forced by the decoder to take themaximum scoring path through the lattice, but inoutput combination we have the aggregate vote ofword confidences generated by each system.
If wecould combine word arc scores across inputs, as inoutput combination, we might get a more robustsolution for taking advantage of the available sim-ilarities on the target side of the translation.
Thispoints to a direction for future research.Other differences between the systems may ex-plain the score gap between our input and outputcombination approaches.
Consensus decoding al-lows you to mix and match fragments that aren?tnecessarily stored as fragments in the phrase table.Another difference is the richer space of reorder-ings in TER-based lattices, due to the ability of themetric to handle long-distance alignments.5 ConclusionWe analyzed three approaches for dealing withmulti-source translation.
While MAX is mostly apoor performer, the upper bound of output selec-tion is stunning.
The very positive results for out-put system combination across all data conditionsare quite promising.
Output combination achievesthese results while the using the limited expres-sive power of n-best inputs.
The potential of usinga more expressive format ?
such as lattices thatrepresent the joint search space of multiple mod-els ?
is high.
Our first attempts at adapting latticesto multi-source translation input show promise forfuture development.
We have only scratched thesurface of methods for constructing input lattices,and plan to actively continue research into improv-ing these methods.AcknowledgmentsThanks to Chris Callison-Burch for many insight-ful discussions, and to Chris Dyer for his imple-mentation of lattice decoding in Moses.This work was supported by the EuroMatrixproject funded by the European Commission (6thFramework Programme), and has made use ofthe resources provided by the Edinburgh Com-pute and Data Facility (http://www.ecdf.ed.ac.uk/), which is partially supported by theeDIKT initiative (http://www.edikt.org).We also acknowledge the support of the EPSRC(grant GR/T04557/01).726ReferencesSrinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In Proceed-ings of ASRU, pages 351?354, Trento, Italy, Decem-ber.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In Proceedingsof NAACL: HLT, pages 16?23, Edmonton, Canada,May.Nicola Bertoldi, Richard Zens, and Marcello Federico.2007.
Speech translation by confusion network de-coding.
In Proceedings of IEEE ICASSP, pages1297?1300, Honolulu, Hawaii, USA, April.Alexandra Birch, Miles Osborne, and Philipp Koehn.2008.
Predicting success in machine translation.
InProceedings of EMNLP, pages 745?754, Honolulu,Hawaii, USA, October.Trevor Cohn and Mirella Lapata.
2007.
Machinetranslation by triangulation: Making effective useof multi-parallel corpora.
In Proceedings of ACL,pages 728?735, Prague, Czech Republic, June.Marta Ruiz Costa-jussa`, Josep M. Crego, Patrik Lam-bert, Maxim Khalilov, Jose?
A. R. Fonollosa, Jose?
B.Mario, and Rafael E. Banchs.
2007.
Ngram-basedstatistical machine translation enhanced with mul-tiple weighted reordering hypotheses.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, pages 167?170, Prague, Czech Repub-lic, June.Christopher J. Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL: HLT, pages 1012?1020, Columbus, Ohio, USA, June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Christopher J. Dyer, Ondr?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of ACL: Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public, June.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit X, pages 79?86, Phuket, Thailand,September.Wolfgang Macherey and Franz J. Och.
2007.
Anempirical study on computing consensus transla-tions from multiple machine translation systems.In Proceedings of EMNLP-CoNLL, pages 986?995,Prague, Czech Republic, June.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation for multi-ple machine translation systems using enhanced hy-pothesis alignment.
In Proceedings of EACL, pages33?40, Trento, Italy, April.Franz Josef Och and Hermann Ney.
2001.
Statis-tical multi-source translation.
In Proceedings ofMT Summit VIII, pages 253?258, Santiago de Com-postela, Spain, September.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?52.Matthias Paulik, Kay Rottmann, Jan Niehues, Al-mut Silja Hildebrand, and Stephan Vogel.
2007.The ISL phrase-based MT system for the 2007 ACLworkshop on statistical machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 197?202, Prague, CzechRepublic, June.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
Challenging the conventional wisdom instatistical machine translation.
In Proceedings ofACL: HLT, Main Conference, pages 9?16, NewYork, New York, USA, June.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007a.
Improved word-level systemcombination for machine translation.
In Proceed-ings of ACL, pages 312?319, Prague, Czech Repub-lic, June.Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,Richard Schwartz, Necip Fazil Ayan, and Bon-nie J. Dorr.
2007b.
Combining output from mul-tiple machine translation systems.
In Proceedingsof NAACL: HLT, pages 228?235, Rochester, NewYork, USA, April.Lane Schwartz.
2008.
Multi-source translation meth-ods.
In Proceedings of AMTA, pages 279?288,Waikiki, Hawaii, USA, October.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted human an-notation.
In Proceedings of AMTA, pages 223?231,Boston, Massachusetts, USA, August.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,pages 901?904, Denver, Colorado, USA, October.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-aya, Hirofumi Yamamoto, and Seiichi Yamamoto.2002.
Toward a broad-coverage bilingual corpus forspeech translation of travel conversations in the realworld.
In Proceedings of LREC, pages 147?152,Las Palmas, Canary Islands, Spain, May.Jia Xu, Evgeny Matusov, Richard Zens, and HermannNey.
2005.
Integrated Chinese word segmentationin statistical machine translation.
In Proceedings ofIWSLT, Pittsburgh, Pennsylvania, USA, October.727
