Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
610?619, Prague, June 2007. c?2007 Association for Computational LinguisticsA Discriminative Learning Model for Coordinate ConjunctionsMasashi Shimbo?Graduate School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara 630-0192, Japanshimbo@is.naist.jpKazuo Hara?Graduate School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara 630-0192, Japankazuo-h@is.naist.jpAbstractWe propose a sequence-alignment basedmethod for detecting and disambiguating co-ordinate conjunctions.
In this method, av-eraged perceptron learning is used to adaptthe substitution matrix to the training datadrawn from the target language and domain.To reduce the cost of training data con-struction, our method accepts training exam-ples in which complete word-by-word align-ment labels are missing, but instead onlythe boundaries of coordinated conjuncts aremarked.
We report promising empirical re-sults in detecting and disambiguating coor-dinated noun phrases in the GENIA corpus,despite a relatively small number of train-ing examples and minimal features are em-ployed.1 IntroductionCoordination, along with prepositional phrase at-tachment, is a major source of syntactic ambiguityin natural language.
Although only a small numberof previous studies in natural language processinghave dealt with coordinations, this does not meandisambiguating coordinations is easy and negligible;it still remains one of the difficulties for state-of-the-art parsers.
in Charniak and Johnson?s recent work(Charniak and Johnson, 2005), for instance, two ofthe features incorporated in their parse reranker areaimed specifically at resolving coordination ambi-guities.Previous work on coordinations includes (Agar-wal and Boggess, 1992; Chantree et al, 2005; Kuro-?Equal contribution.hashi and Nagao, 1994; Nakov and Hearst, 2005;Okumura and Muraki, 1994; Resnik, 1999).
Ear-lier studies (Agarwal and Boggess, 1992; Okumuraand Muraki, 1994) attempted to find heuristic rulesto disambiguate coordinations.
More recent re-search are concerned with capturing structural sim-ilarity between conjuncts using thesauri and cor-pora (Chantree et al, 2005), or web-based statistics(Nakov and Hearst, 2005).We identify three problems associated with theprevious work.1.
Most of these studies evaluate the proposedheuristics against restricted forms of conjunc-tions.
In some cases, they only deal with co-ordinations with exactly two conjuncts, leavingthe generality of these heuristics unclear.2.
Most of these studies assume that the bound-aries of coordinations are known in advance,which, in our opinion, is impractical.3.
The proposed heuristics and statistics capturemany different aspects of coordination.
How-ever, it is not clear how they interact and howthey can be combined.To address these problems, we propose a newframework for detecting and disambiguating coor-dinate conjunctions.
Being a discriminative learningmodel, it can incorporate a large number of overlap-ping features encoding various heuristics for coordi-nation disambiguation.
It thus provides a test bed forexamining combined use of the proposed heuristicsas well as new ones.
As the weight on each featureis automatically tuned on the training data, assessingthese weights allows us to evaluate the relative meritof individual features.610writern nt e rivFigure 1: An alignment between ?writer?
and ?vint-ner,?
represented as a path in an edit graphOur learning model is also designed to admit ex-amples in which only the boundaries of coordinatedconjuncts are marked, to reduce the cost of trainingdata annotation.The state space of our model resembles that ofKurohashi and Nagao?s Japanese coordination de-tection method (Kurohashi and Nagao, 1994).
How-ever, they considered only the decoding of coordi-nated phrases and did not address automatic param-eter tuning.2 Coordination disambiguation assequence alignmentIt is widely acknowledged that coordinate conjunc-tions often consist of two or more conjuncts havingsimilar syntactic constructs.
Our coordination detec-tion model also follows this observation.
To detectsuch similar constructs, we use the sequence align-ment technique (Gusfield, 1997).2.1 Sequence alignmentSequence alignment is defined in terms of transfor-mation of one sequence (string) into another throughan alignment, or a series of edit operations.
Each ofthe edit operations has an associated cost, and thecost of an alignment is defined as the total cost ofedit operations involved in the alignment.
The min-imum cost alignment can be computed by dynamicprogramming in a state space called an edit graph,such as illustrated in Figure 1.
In this graph, a com-plete path starting from the upper-left initial vertexand arriving at the lower-right terminal vertex con-stitutes a global alignment.
Likewise, a partial pathcorresponds to a local alignment.Sequence alignment can also be formulated withthe scores of edit operations instead of their costs.
Inthis case, the sequence alignment problem is that offinding a series of edit operations with the maximumscore.2.2 Edit graph for coordinate conjunctionsA fundamental difference between biological localsequence alignment and coordination detection isthat the former deals with finding local homologiesbetween two (or more) distinct sequences, whereascoordination detection is concerned with local simi-larities within a single sentence.The maximal local alignment between two iden-tical sequences is a trivial (global) alignment ofidentity transformation (the diagonal path in an editgraph).
Coordination detection thus reduces to find-ing off-diagonal partial paths with the highest sim-ilarity score.
Such paths never cross the diagonal,and we can limit our search space to the upper trian-gular part of the edit graph, as illustrated in Figure 2.3 Automatic parameter tuningGiven a suitable substitution matrix, i.e., functionfrom edit operations to scores, it is straightforwardto find optimal alignments, or coordinate conjunc-tions in our task, by running the Viterbi algorithm inan edit graph.In computational biology, there exist establishedsubstitution matrices (e.g., PAM and BLOSUM)built on a generative model of mutations and theirassociated probabilities.Such convenient substitution matrices do not ex-ist for coordination detection.
Moreover, optimalscore functions are likely to vary from one domain(or language) to another.
Instead of designing aspecific function for a single domain, we propose ageneral discriminative learning model in which thescore function is a linear function of the features as-signed to vertices and edges in the state space, andthe weight of the features are automatically tuned forgiven gold standard data (training examples) drawnfrom the application domain.
Designing heuristicrules for coordination detection, such as those pro-posed in previous studies, translates to the design ofsuitable features in our model.Our learning method is an extension of Collins?sperceptron-based method for sequence labeling(Collins, 2002).
However, a few incompatibilitiesexists between Collins?
sequence labeling methodand edit graphs used for sequence alignment.611mediandoseintensitywas99%forstandardarm182%thedosearmandforthedense.mediandoseintensitywas99%forstandardarm182%thedosearmandforthedense.Figure 2: An edit graph for coordinate detection1.
Collins?s method, like the linear-chain condi-tional random fields (CRFs) (Lafferty et al,2001; Sha and Pereira, 2003), seeks for a com-plete path from the initial vertex to the terminalusing the Viterbi algorithm.
In an edit graph, onthe other hand, coordinations are representedby partial paths.
And we somehow need tocomplement the partial path to make a com-plete path.2.
A substitution matrix, which defines the scoreof edit operations, can be represented as a func-tion of features defined on edges.
But to dealwith complex coordinations, a more expressivescore function is sometimes desirable, so thatscores can be computed not only on the basis ofa single edit operation, but also on consecutiveedit operations.
Edit graphs are not designed toaccommodate features for such a higher-orderinteraction of edit operations.To reconcile these incompatibilities, we derivea more finer-grained model from the original editgraph.
In presenting the description of our model be-low, we reserve the terminology ?vertex?
and ?edge?for the original edit graph, and use ?node?
and ?arc?for our new model, to avoid confusion.3.1 State space for learning coordinateconjunctionsThe new model is also based on the edit graph.
Inthis model, we create a node for each triple (v, p,e),(a) (b) (c) (d) (e)Figure 3: Five node types created for a vertex in anedit graph: (a) Inside Delete, (b) Inside Insert, (c) In-side Substitute, (d) Outside Delete, and (e) OutsideInsert.
(a) (b)Figure 4: Series of edit operations with an equiv-alent net effect.
(a) (Insert,Delete), and (b)(Delete, Insert).
(b) is prohibited in our model.where v is a vertex in the original edit graph, e ?
{Delete, Insert,Substitute} is an admissible1 edit op-eration at v, and p ?
{Inside,Outside} is a polaritydenoting whether or not the edit operation e is in-volved in an alignment.For a node (v, p,e), we call the pair (p,e) its type.All five possible node types for a single vertex of anedit graph are shown in Figure 3.
We disallow type(Outside,Substitute), as it is difficult to attribute anintuitive meaning to substitution when two wordsare not aligned (i.e., Outside).Arcs between nodes are built according to thetransitions allowed in the original edit graph.
To beprecise, an arc between node (v1, p1,e1) and node(v2, p2,e2) is created if and only if the followingthree conditions are met.
(i) Edit operations e1 ande2 are admissible at v1 and v2, respectively; (ii) thesink of the edge for e1 at v1 is v2; and (iii) it is notthe case with p1 = p2 and (e1,e2) = (Delete, Insert).Condition (iii) is introduced so as to disallow tran-sition (Delete, Insert) depicted in Figure 4(b).
Incontrast, the sequence (Insert,Delete) (Figure 4(a))is allowed.
The net effects of these edit operationsequences are identical, in that they both skip oneword each from the two sequences to be aligned.
Asa result, there is no use in discriminating betweenthese two, and one of them, namely (Delete, Insert),is prohibited.1For a vertex v at the border of an edit graph, some edit op-erations are not applicable (e.g., Insert and Substitute at verticeson the right border in Figure 2); we say such operations are in-admissible at v. Otherwise, an edit operation is admissible.612A,B,CandDandA B ,, C DA,B,CandDandA B ,, C D(a) chainable (b) non-chainableFigure 5: A coordination with four conjuncts repre-sented as (a) chainable, and (b) non-chainable partialpaths.
We take (a) as the canonical representation.3.2 Learning taskBy the restriction of condition (iii) introduced aboveand the omission of (Outside, Substitute) from thenode types, we can uniquely determine the com-plete path (from the initial node to the terminal node)that conjoins all the local alignments by Outsidenodes (which corresponds to edges in the originaledit graph).
In Figure 2, the augmented Outsideedges in this unique path are plotted as dotted linesfor illustration.Thus we obtain a complete path which is compat-ible with Collins?s perceptron-based sequence learn-ing method.
The objective of the learning algo-rithms, which we will describe in Section 4, is tooptimize the weight of features so that running theViterbi algorithm will yield the same path as the goldstandard.Because a node in our state space corresponds toan edge in the original edit graph (see Figure 3), anarc in our state space is actually a pair of consec-utive edges (or equivalently, edit operations) in theoriginal graph.
Hence our model is more expressivethan the original edit graph in that the score functioncan have a term (feature) defined on a pair of editoperations instead of one.3.3 More complex coordinationsEven if a coordination comprises three or more con-juncts, our model can handle them, as it can be rep-resented as a set of pairwise local alignments thatare chainable (Gusfield, 1997, Section 13.3).
If pair-wise local alignments are chainable, a unique com-plete path that conjoins all these alignments can bedetermined, allowing the same treatment as the casewith two conjuncts.For instance, a coordination with four conjuncts(A, B, C and D) can be decomposed into a set of pair-wise alignments {(A,B),(B,C),(C,D)} as depictedin Figure 5(a).
This set of alignments are chain-able and thus constitute the canonical encoding forthis coordination; any other pairwise decompositionfor these four conjuncts, like {(A,B),(B,C),(A,D)}(Figure 5(b)), is not chainable.Our model can handle multiple non-nested coor-dinations in a single sentence as well, as they canalso be decomposed into chainable pairwise align-ments.
It cannot encode nested coordinations like(A, B, and (C and D)), however.4 Algorithms4.1 Reducing the cost of training dataconstructionOur learning method is supervised, meaning that itrequires training data annotated with correct labels.Since a label in our problem is local alignments(or paths in an edit graph) representing coordina-tions, the training sentences have to be annotatedwith word-by-word alignments.There are two reasons relaxing this requirementis desirable.
First, it is expensive to construct suchdata.
Second, there are coordinate conjunctionsin which word-by-word correspondence is uncleareven for humans.
In Figure 2, for example, a word-by-word alignment of ?standard?
with ?dense?
is de-picted, but it might be more natural to regard a word?standard?
as being aligned with two words ?dosedense?
combined together.Even if word-by-word alignment is uncertain, theboundaries of conjuncts are often obvious, and it isalso much easier for human annotators to mark onlythe beginning and end of each conjunct.
Thus wewould like to allow for training examples in whichonly alignment boundaries are specified, instead ofa full word-by-word alignment.For these examples, conjunct boundaries corre-sponds to a rectangular region rather than a sin-gle path in an edit graph.
The shaded box in Fig-ure 2 illustrates the rectangular region determined bythe boundaries of an alignment between the phrases?182% for the dose dense arm?
and ?99% for thestandard arm.?
There are many possible alignmentpaths in this box, among which we do not knowwhich one is correct (or even likely).
To deal with613input: Set of examples S = {(xi,Yi)}Iteration cutoff Toutput: Averaged weight vector w?1: w?
?
0; w ?
02: for t ?
1 .
.
.T do3: ?w ?
04: for each (xi,Yi) ?
S do5: y ?
argmaxy?Yi w ?
f (xi,y)6: y?
?
argmaxy?A(xi) w ?
f (xi,y)7: ?
f ?
f (xi,y)?
f (xi,y?
)8: ?w ?
?w+?
f9: end for10: if ?w = 0 then11: return w?12: end if13: w ?
w+?w14: w?
?
[(t ?1)w?+w]/t15: end for16: return w?Figure 6: Path-based algorithmthis difficulty, we propose two simple heuristics wecall the (i) path-based and (ii) box-based methods.As mentioned earlier, both of these methods arebased on Collins?s averaged-perceptron algorithmfor sequence labeling (Collins, 2002).4.2 Path-based methodOur first method, which we call the ?path-based?algorithm, is shown in Figure 6.
We denote by A(x)all possible alignments (paths) over x.
The algorithmreceives T , the maximum number of iterations, anda set of examples S = {(xi,Yi)} as input, where xi is asentence (a sequence of words with their attributes,e.g., part-of-speech, lemma, prefixes, and suffixes)and Yi ?
A(xi) is the set of admissible alignments(paths) for xi.
When a sentence is fully annotatedwith a word-by-word alignment y, Yi = {y} is a sin-gleton set.
In general boundary-only examples wedescribed in Section 4.1, Yi holds all possible align-ments compatible with the marked range, or equiv-alently, paths that pass through the upper-left andlower-right corners of a rectangular region.
Notethat it is not necessary to explicitly enumerate all themember paths of Yi; the set notation here is only forthe sake of presentation.The external function f (x,y) returns a vector(called the global feature vector in (Sha and Pereira,2003)) of the number of feature occurrences alongthe alignment path y.
In the beginning (line 5 in thefigure) of the inner loop, the target path (alignment)input: Set of examples S = {(xi,Yi)}Iteration cutoff Toutput: Averaged weight vector w?1: w?
?
0; w ?
02: for each (xi,Yi) ?
S do3: gi ?
(1/|Yi|)?y?Yi f (xi,y)4: end for5: for t ?
1 .
.
.T do6: ?w ?
07: for each (xi,Yi) ?
S do8: y?
?
argmaxy?A(xi) w ?
f (xi,y)9: Convert y?
into its box representation Y ?10: g?
?
(1/|Y ?i |)?y?Y ?i f (xi,y)11: ?
f ?
gi ?g?12: ?w ?
?w+?
f13: end for14: if ?w = 0 then15: return w?16: end if17: w ?
w+?w18: w?
?
[(t ?1)w?+w]/t19: end for20: return w?Figure 7: Box-based algorithmis recomputed with the current weight vector w. Theargmax in lines 5 and 6 can be computed efficiently(O(n2), where n is the number of words in x) by run-ning a pass of the Viterbi algorithm in the edit graphfor x.
The weight vector w varies between iterations,and so does the most likely alignment with respectto w. Hence the recomputation in line 5 is needed.4.3 Box-based methodOur next method, called ?box-based,?
is designedon the following heuristic.
Given a rectangle regionrepresenting a local alignment (hence all nodes inthe region are of polarity Inside) in an edit graph,we distribute feature weights in proportion to theprobability of a node (or an arc) being passed by apath from the initial (upper left) node to the termi-nal (lower right) node of the rectangle.
We assumepaths are uniformly distributed.Figure 8 displays an 8?
8 sub-grid of an editgraph.
The figure under each vertex shows the num-ber of paths passing through the vertex.
Verticesnear the upper-left and the lower-right corner havea large frequency, and the frequency drops exponen-tially towards the top right corner and the bottomleft corner, hence placing a strong bias on the pathsnear diagonals.
This distribution fits our preference614Figure 8: Number of paths passing through the ver-tices of an 8?8 grid.towards alignments with a larger number of substi-tutions.The pseudo-code for the box-based algorithm isshown in Figure 7.
For each example xi and its pos-sible target labels (alignments)Yi, this algorithm first(line 3) computes and stores in the vector gi the aver-age number of feature occurrences in all possible tar-get paths in Yi.
This quantity can be computed sim-ply by summing over all nodes and edges feature oc-currences multiplied by the pre-computed frequencyof each nodes and arcs at which these features occur.analogously to the forward-backward algorithm.
Ineach iteration, the algorithm scans every example(lines 7?13), computing the Viterbi path y?
(line 8)according to the current weight vector w. Line 9then converts y?
to its box representation Y ?, by se-quentially collapsing consecutive Inside nodes in y?as a box.
For instance, let y?
be the local alignmentdepicted as the bold line in Figure 2.
The box Y ?computed in line 9 for this y?
is the shaded area in thefigure.
In parallel to the initialization step in line 3,we store in g?
the average feature occurrences in Y ?and update the current weight vector w by the differ-ence between the target gi and g?.
These steps canbe interpreted as a Viterbi approximation for com-puting the optimal set Y ?
of alignments directly.5 Related work5.1 Discriminative learning of edit distanceIn our model, the state space of sequence alignment,or edit graph, is two-dimensional (which is actu-ally three-dimensional if the dimension for labels istaken into account).
This is contrastive to the onedimensional models used by Collins?s perceptron-based sequence method (Collins, 2002) which ouralgorithms are based upon, and by the linear-chainCRFs.McCallum et al (McCallum et al, 2005) pro-posed a CRF tailored to learning string edit distancefor the identity uncertainty problem.
The state spacein their work is two dimensional just like our model,but it is composed of two decoupled subspaces, eachcorresponding to ?match?
and ?mismatch,?
thus shar-ing only the initial state.
It is not possible to makea transition from a state in the ?match?
state space tothe ?mismatch?
space (and vice versa).
As we cansee from the decoupled state space, this method isbased on global alignment rather than local align-ment; it is not clear whether their method can iden-tify local homologies in sequences.
Our method usesa single state space in which both ?match (inside)?and ?mismatch (outside)?
nodes co-exist and transi-tion between them is permitted.5.2 Inverse sequence alignment incomputational biologyIn computational biology, the estimation of a sub-stitution matrix from data is called the inverse se-quence alignment problem.
Until recently, therehave been a relatively small number of papers inthis field despite a large body of literature in se-quence alignment.
Theoretical studies in the inversesequence alignment include (Pachter and Sturmfels,2004; Sun et al, 2004).
Recently, CRFs have beenapplied for optimizing the substitution matrix in thecontext of global protein sequence alignment (Do etal., 2006).6 Empirical evaluation6.1 Dataset and TaskWe used the GENIA Treebank beta corpus (Kim etal., 2003)2 for evaluation of our methods.
The cor-2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA615pus consists of 500 parsed abstracts in Medline witha total of 4529 sentences.Although the Penn Treebank Wall Street Journal(WSJ) is the de facto standard corpus for evaluatingchunking and parsing performance, it lacks adequatestructural information on coordinate conjunctions,and therefore does not serve our purpose.
Manycoordinations in the Penn Treebank are given a flatbracketing like (A, B, and C D), and thus we cannottell which of ((A, B, and C) D) and ((A), (B), and(C D)) gives a correct alignment.
The GENIA cor-pus, in contrast, distinguishes ((A, B, and C) D) and((A), (B), and (C D)) explicitly, by providing moredetailed bracketing.
In addition, the corpus containsan explicit tag ?COOD?
for marking coordinations.To avoid nested coordinations, which admittedlyrequire techniques other than the one proposed inthis paper, we selected from the GENIA corpus sen-tences in which the conjunction ?and?
occurs justonce.
After this operation, the number of sentencesreduced to 1668, from which we further removed 32that are not associated with the ?COOD?
tag, and3 more whose annotated tree structures containedobvious errors.
Of the remaining 1633 sentences,1061 were coordinated noun phrases annotated withNP-COOD tags, 226 coordinated verb phrases (VP-COOD), 142 coordinated adjective phrases (ADJP-COOD), and so on.
Because the number of VP-COOD, ADJP-COOD, and other types of coordi-nated phrases are too small to make a meaningfulbenchmark, we focus on coordinated noun phrasesin this experiment.The task hence amounts to identifying coordi-nated NPs and their constituent conjuncts in the1633 sentences, all of which contain a coordinationmarker ?and?
but only 1061 of which are actuallycoordinated NPs.6.2 BaselinesWe used several publicly available full parsersas baselines: (i) the Bikel parser (Bikel,2005) version 0.9.9c with configuration filebikel.properties (denoted as Bikel/Bikel),(ii) the Bikel parser in the Collins parser emula-tion mode (using collins.properties file)(Bikel/Collins), and (iii) Charniak and Johnson?sreranking parser (Charniak-Johnson) (Charniak andJohnson, 2005).
We trained Bikel?s parser and itsCollins emulator with the GENIA corpus, WSJ, andthe combination of the two.
Charniak and Johnson?sparser was used as distributed at Charniak?s homepage (and is WSJ trained).Another baseline we used is chunkers basedon linear-chain CRFs and the standard BIO la-bels.
We trained two types of CRF-based chun-kers by using different BIO sequences, one forthe conjunct bracketing and the other for coor-dination bracketing.
The chunkers were imple-mented with T. Kudo?s CRF++ package version0.45.
We varied its regularization parameters Camong C ?
{0.01,0.1,1,10,100,1000}, and the bestresults among these are reported below.6.3 FeaturesLet x = (x1, .
.
.
,xn) be a sentence, with its memberxk a vector of attributes for the kth word.
The at-tributes include word surface, part-of-speech (POS),and suffixes, among others.Table 1 summarizes (i) the features assigned to anode whose corresponding edge in the original editgraph for x is emanating from row i and column j,and (ii) the features assigned to the arcs (consistingof two edges in the original edit graph) whose joint(the vertex between the two edges) is a vertex at rowi and column j.We also tested the path-based and box-basedmethods, and the CRF chunkers both with and with-out the word and suffix features.Although this is not a requirement of our model oralgorithms, every feature we use in this experimentis binary; if the condition associated with a featureis satisfied, the feature takes a value of 1; otherwise,it is 0.
A condition typically asks whether or notspecific attributes match those at a current node, arc,or their neighbors.We used the POS tags from the GENIA corpusas the POS attribute.
The morphological featuresinclude 3- and 4-gram suffixes and indicators ofwhether a word includes capital letters, hyphens, anddigits.For the baseline CRF-based chunkers, we assignthe word, POS (from GENIA), and the morphologi-cal features to nodes, and the POS features to edges.The feature set is identical to those used for our pro-posed methods, except for features defined on row-column combination (i.e., those defined over both i616Table 1: Features for the proposed methodsSubstitute (diagonal) nodes(?,Substitute,?
)Indicators of the word, POS, and morphological attributes of xi, x j , (xi?1,xi),(xi,xi+1), (x j?1,x j), (x j,x j+1), and (xi, x j), respectively combined with thetype of the node.For each of the word, POS, and morphological attributes, an indicator ofwhether the respective attribute is identical in xi and x j , combined with thetype of the node.Delete (vertical) nodes(?,Delete,?
)Indicators of the word, POS, and morphological attributes of xi, x j , x j?1,(xi?1,xi), (xi,xi+1), and (x j?1,x j), combined with the type of the node.Insert (horizontal) nodes(?, Insert,?
)Indicators of the word, POS, and morphological attributes of xi, xi?1, x j,(xi?1,xi), (x j?1,x j), and (x j, x j+1), combined with the type of the node.Any arcs(?,?,?)?
(?,?,?
)Indicators of the POS attribute of xi, xi?1, x j, x j?1, (xi?2,xi?1), (xi?1,xi),(xi,xi+1), (x j?2,x j?1), (x j?1,x j), (x j,x j+1), (xi?1,x j?1), (xi?1,x j), (xi, x j?1)and (xi,x j), combined with the type pair of the arc.Arcs between nodes of different polarity(?, Inside,?)?
(?,Outside,?)
and(?,Outside,?)?
(?, Inside,?
)Indicator of the distance j?
i between two words xi and x j, combined with thetype pair of the arc.and j in Table 1.
The latter cannot be incorporatedas a local features in chunkers based on linear chain.For the Bikel (and its Collins emulation) parserswhich accepts POS tags output by external taggersupon testing, we gave them the POS tags from theGENIA corpus, for fair comparison with the pro-posed methods and CRF-based chunkers.6.4 Evaluation criteriaWe employed two evaluation criteria: (i) correctnessof the conjuncts output by the algorithm, and (ii) cor-rectness of the range of coordinations as a whole.For the correctness of conjuncts, we further usetwo evaluation criteria.
The first evaluation method(?pairwise evaluation?)
is based on the decomposi-tion of coordinations into the canonical set of pair-wise alignments, as described in Section 3.3.
Afterthe set of pairwise alignments is obtained, each pair-wise alignment is transformed into a box surroundedby their boundaries.
Using these boxes, we evaluateprecision, recall and F rates through the followingdefinition.
The precision measures how many of theboxes output by the algorithm exactly match thosein the gold standard, and the recall rate is the per-centage of boxes found by the algorithm.
The F rateis the harmonic mean of the precision and the recall.The second evaluation method (?chunk-basedevaluation?)
for conjuncts is based on whether thealgorithm correctly outputs the beginning and end ofeach conjunct, in the same manner as the chunkingtasks.
Here, we adopt the evaluation criteria for theCoNLL 99 NP bracketing task3; the precision equalshow many of the NP conjuncts output by the algo-rithm are correct, and the recall is the percentage ofNP conjuncts found by the algorithm.Of these two evaluation methods for conjuncts, itis harder to obtain a higher pairwise evaluation scorethan the chunk-based evaluation.
To be counted as atrue positive in the pairwise evaluation, two consec-utive chunks must be output correctly by the algo-rithm.For the correctness of the coordination range, wecheck if both the start of the first coordinated con-junct and the end of the last conjunct in the goldmatch those output by the algorithm The reason weevaluate coordination range is to compare our pro-posed method with the full parsers trained on WSJ(but applied to GENIA).
Although WSJ and GE-NIA differ in the way conjuncts are annotated, theyare mostly identical on how the range of coordina-tions are annotated, and hence comparison is feasi-ble in terms of coordination range.
For the baselineparsers, we regard the bracketing directly surround-ing the coordination marker ?and?
as their output.In (Clegg and Shepherd, 2007), an F score of 75.5is reported for the Bikel parser on coordination de-tection.
Their evaluation is based on dependencies,which is different from our evaluation criteria whichare all based on boundaries.
Generally speaking, ourevaluation criterion seems stricter, as exemplified inFigures 7 and 8 of Clegg and Shepherd?s paper; inthese figures, our evaluation criterion would result3http://www.cnts.ua.ac.be/conll99/npb/617Table 2: Performance on conjunct bracketing.
P: precision (%), R: recall (%), F: F rate.Pairwise evaluation Chunk-based evaluationMethod P R F P R FPath-based method 61.4 56.2 58.7 70.9 66.9 68.9Path-based method without word and suffix features 61.7 58.8 60.2 71.2 69.7 70.5Box-based method 60.6 58.3 59.4 70.5 69.1 69.8Box-based method without word and suffix features 59.5 58.3 58.9 69.7 69.5 69.6Linear-chain CRF chunker (conjunct bracketing) 62.6 51.4 56.4 71.0 66.1 68.5Bikel/Collins, trained with GENIA 50.0 48.6 49.3 65.0 64.2 64.6Bikel/Bikel, trained with GENIA 50.1 47.8 49.0 63.9 61.3 62.6Table 3: Performance on coordination bracketing.
P: precision (%), R: recall (%), F: F rate.Method P R FPath-based method 58.2 55.3 56.7Path-based method without words and suffix features 57.7 56.6 57.2Box-based method 55.6 54.4 55.0Box-based method without words and suffix features 54.8 54.6 54.7Linear-chain CRF chunker, trained with conjunct bracketing 43.9 46.7 45.3Linear-chain CRF chunker, trained with coordination bracketing 58.4 51.0 54.5Bikel/Collins, trained with GENIA 44.0 45.4 44.7Bikel/Collins, trained with WSJ 42.3 43.2 42.7Bikel/Collins, trained with GENIA+WSJ 43.3 45.1 44.1Bikel/Bikel, trained with GENIA 44.8 45.4 45.1Bikel/Bikel, trained with WSJ 40.7 41.5 41.1Bikel/Bikel, trained with GENIA+WSJ 43.9 45.8 44.9Charniak-Johnson reranking parser 48.3 45.2 46.7in zero true positive, whereas their evaluation countsthe dependency arc from ?genes?
to ?human?
as onetrue positive.6.5 ResultsThe results of conjunct and coordination bracketingare shown in Tables 2 and 3, respectively.
Theseare the results of a five-fold cross validation.
Weran the proposed methods until convergence or thecutoff iteration of T = 10000, whichever comes first.The path-based method (without words and suf-fixes) and box-based method (with full features)each achieved 2.0 and 1.3 point improvements overthe CRF chunker in terms of the F score in conjunctidentification (chunk-based evaluation), 3.8 and 3.0point improvement in terms of pairwise evaluation,and 2.7 and 0.5 points in coordinate identification,respectively.
Our methods also showed a perfor-mance considerably higher than the baseline parsers.The performance of the path-based method wasbetter when the word and suffix features were re-moved, while the box-based method and CRF chun-kers performed better with these features.7 ConclusionsWe have proposed a new coordination learning anddisambiguation method that can incorporate manydifferent features, and automatically optimize theirweights on training data.In the experiment of Section 6, the proposedmethod obtained a performance superior to a linear-chain chunker and to the state-of-art full parsers.We used only syntactic and morphological fea-tures, and did not use external similarity measureslike thesauri and corpora, although they are reportedto be effective for disambiguating coordinations.
Wenote that it is easy to incorporate such external sim-ilarity measures as a feature in our model, thanks toits two-dimensional state space.
The similarity oftwo words derived from an external knowledge basecan be assigned to a Substitute node at a correspond-ing location in the state space in a straightforwardmanner.
This is a topic we are currently working on.We are also planning to reimplement our algo-rithms using CRFs instead of the averaged percep-tron algorithm.618ReferencesRajeev Agarwal and Lois Boggess.
1992.
A simple butuseful approach to conjunct identification.
In Proceed-ings of the 30th Annual Meeting of the Association forComputing Linguistics (ACL?92), pages 15?21.Daniel M. Bikel.
2005.
Multilingual statistical pars-ing engine version 0.9.9c.
http://www.cis.upenn.edu/?dbikel/software.html.Francis Chantree, Adam Kilgarriff, Anne de Roeck, andAlistair Willis.
2005.
Disambiguating coordina-tions using word distribution information.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP2005), Borovets, Bulgaria.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics (ACL-2005).Andrew B Clegg and Adrian J Shepherd.
2007.
Bench-marking natural-language parsers for biological appli-cations using dependency graphs.
BMC Bioinformat-ics, 8(24).Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2002).C.
B.
Do, S. S. Gross, and S. Batzoglou.
2006.
CON-TRAlign: discriminative training for protein sequencealignment.
In Proceedings of the Tenth Annual Inter-national Conference on Computational Molecular Bi-ology (RECOMB 2006).Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
GE-NIA corpus: a semantically annotated corpus for bio-textmining.
Bioinformatics, 19(Suppl.
1):i180?i182.Sadao Kurohashi and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based onthe detection of conjunctive structures.
ComputationalLinguistics, 20:507?534.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the 18th International Conference on Ma-chine Learning (ICML-2001), pages 282?289.
MorganKaufmann.Andrew McCallum, Kedar Bellare, and Fernando Pereira.2005.
A conditional random field for discriminatively-trained finite-state string edit distance.
In Proceedingsof the 21st Conference on Uncertainty in Artificial In-telligence (UAI-2005).Preslav Nakov and Marti Hearst.
2005.
Using the web asan implicit training set: application to structural ambi-guity resolution.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language (HLT/EMNLP), pages835?842, Vancouver.Akitoshi Okumura and Kazunori Muraki.
1994.
Sym-metric pattern matching analysis for English coordi-nate structures.
In Proceedings of the Fourth Confer-ence on Applied Natural Language Processing, pages41?46.Lior Pachter and Bernd Sturmfels.
2004.
Parametricinference for biological sequence analysis.
Proceed-ings of the National Academy of Sciences of the USA,101(46):16138?16143.Philip Resnik.
1999.
Semantic similarity in a taxonomy:an information-based measure and its application toproblems of ambiguity in natural language.
Journalof Artificial Intelligence Research, 11:95?130.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof the Human Language Technology Conference NorthAmerican Chapter of Association for ComputationalLinguistics (HLT-NAACL 2003), pages 213?220, Ed-monton, Alberta, Canada.
Association for Computa-tional Linguistics.Fangting Sun, David Ferna?ndez-Baca, and Wei Yu.
2004.Inverse parametric sequence alignment.
Journal of Al-gorithms, 53:36?54.619
