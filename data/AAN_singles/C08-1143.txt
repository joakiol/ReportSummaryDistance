Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1137?1144Manchester, August 2008Active Learning with Sampling by Uncertainty and Density for WordSense Disambiguation and Text ClassificationJingbo Zhu  Huizhen Wang  Tianshun YaoNatural Language Processing LaboratoryNortheastern UniversityShenyang, Liaoning, P.R.China 110004zhujingbo@mail.neu.edu.cnwanghuizhen@mail.neu.edu.cnBenjamin K TsouLanguage Information SciencesResearch CentreCity University of Hong KongHK, P.R.Chinarlbtsou@cityu.edu.hkAbstractThis paper addresses two issues of activelearning.
Firstly, to solve a problem ofuncertainty sampling that it often fails byselecting outliers, this paper presents anew selective sampling technique, sam-pling by uncertainty and density (SUD),in which a k-Nearest-Neighbor-baseddensity measure is adopted to determinewhether an unlabeled example is an out-lier.
Secondly, a technique of samplingby clustering (SBC) is applied to build arepresentative initial training data set foractive learning.
Finally, we implement anew algorithm of active learning withSUD and SBC techniques.
The experi-mental results from three real-world datasets show that our method outperformscompeting methods, particularly at theearly stages of active learning.1 IntroductionCreating a large labeled training corpus is expen-sive and time-consuming in some real-world ap-plications (e.g.
word sense annotation), and isoften a bottleneck to build a supervised classifierfor a new application or domain.
Our study aimsto minimize the amount of human labeling ef-forts required for a supervised classifier (e.g.
forautomated word sense disambiguation) toachieve a satisfactory performance by using ac-tive learning.Among the techniques to solve the knowledgebottleneck problem, active learning is a widelyused framework in which the learner has the abil-ity to automatically select the most informative?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.unlabeled examples for human annotation.
Theability of the active learner can be referred to asselective sampling.
Uncertainty sampling (Lewisand Gale, 1994) is a popular selective samplingtechnique, and has been widely studied in naturallanguage processing (NLP) applications such asword sense disambiguation (WSD) (Chen et al,2006; Chan and Ng, 2007), text classification(TC) (Lewis and Gale, 1994; Zhu et al, 2008),statistical syntactic parsing (Tang et al, 2002),and named entity recognition (Shen et al, 2004).Actually the motivation behind uncertaintysampling is to find some unlabeled examplesnear decision boundaries, and use them to clarifythe position of decision boundaries.
However,uncertainly sampling often fails by selecting out-liers (Roy and McCallum, 2001; Tang et al,2002).
These selected outliers (i.e.
unlabeled ex-amples) have high uncertainty, but can not pro-vide much help to the learner.
To solve the out-lier problem, we proposed in this paper a newmethod, sampling by uncertainty and density(SUD), in which a K-Nearest-Neighbor-baseddensity (KNN-density) measure is used to deter-mine whether an unlabeled example is an outlier,and a combination strategy based on KNN-density measure and uncertainty measure is de-signed to select the most informative unlabeledexamples for human annotation at each learningiteration.The second effort we made is to study how tobuild a representative initial training data set foractive learning.
We think building a more repre-sentative initial training data set is very helpfulfor active learning.
In previous studies on activelearning, the initial training data set is generallygenerated at random, based on an assumptionthat random sampling will be likely to build theinitial training set with same prior data distribu-tion as that of whole corpus.
However, this situa-tion seldom occurs in real-world applications dueto the small size of initial training set used.
In1137this paper, we utilize an approach, sampling byclustering (SBC), to selecting the most represen-tative examples to form initial training data setfor active learning.
To do it, the whole unlabeledcorpus should be first clustered into predefinednumber of clusters (i.e.
the predefined size of theinitial training data set).
The example closest tothe centroid of each cluster will be selected toaugment initial training data set, which is consid-ered as the most representative case.Finally, we describe an implementation of ac-tive learning with SUD and SBC techniques.
Ex-perimental results of active learning for WSDand TC tasks show that our proposed methodoutperforms competing methods, particularly atthe early stages of active learning process.
It isnoteworthy that these proposed techniques areeasy to implement, and can be easily applied toseveral learners, such as Maximum Entropy(ME), na?ve Bayes (NB) and Support VectorMachines (SVMs).2 Active Learning ProcessIn this work, we are interested in uncertaintysampling (Lewis and Gale, 1994) for pool-basedactive learning, in which an unlabeled example xwith maximum uncertainty is selected for humanannotation at each learning cycle.
The maximumuncertainty implies that the current classifier (i.e.the learner) has the least confidence on its classi-fication of this unlabeled example.Actually active learning is a two-stage processin which a small number of labeled samples anda large number of unlabeled examples are firstcollected in the initialization stage, and a closed-loop stage of query and retraining is adopted.Procedure: Active Learning ProcessInput: initial small training set L, and pool of unla-beled data set UUse L to train the initial classifier CRepeat1.
Use the current classifier C to label all unla-beled examples in U2.
Use uncertainty sampling technique to selectm2  most informative unlabeled examples, andask oracle H for labeling3.
Augment L with these m new examples, andremove them from U4.
Use L to retrain the current classifier CUntil the predefined stopping criterion SC is met.Figure 1.
Active learning with uncertainty sam-pling technique2 A batch-based sample selection labels the top-m mostinformative unlabeled examples at each learning cycle todecrease the number times the learner is retrained.3 Uncertainty MeasuresIn real-world applications, only limited size oftraining sample set can be provided to train asupervised classifier.
Due to manual efforts in-volved, such brings up a considerable issue: whatis the best subset of examples to annotate.
In theuncertainty sampling scheme, the unlabeled ex-ample with maximum uncertainty is viewed asthe most informative case.
The key point of un-certainty sampling is how to measure the uncer-tainty of an unlabeled example x.3.1 Entropy MeasureThe well-known entropy is a popular uncertaintymeasurement widely used in previous studies onactive learning (Tang et al, 2002; Chen et al2006; Zhu and Hovy, 2007):??
?=YyxyPxyPxH )|(log)|()(             (1)where P(y|x) is the a posteriori probability.
Wedenote the output class y?Y={y1, y2, ?, yk}.
H isthe uncertainty measurement function based onthe entropy estimation of the classifier?sposterior distribution.In the following comparison experiments, theuncertainty sampling based on entropy criterionis considered as the baseline method, also calledtraditional uncertainty sampling.3.2 Density*Entropy MeasureTo analyze the outlier problem of traditional un-certainty sampling, we first give an example toexplain our motivation.Figure 2.
An example of two points A and B withmaximum uncertainty at the ith learning iterationAs mentioned in Section 1, the motivation be-hind uncertainty sampling is to find some unla-beled examples near decision boundaries, andassume that these examples have the maximumuncertainty.
Fig.
2 shows two unlabeled exam-ples A and B with maximum uncertainty at the ith1138learning cycle.
Roughly speaking, there are threeunlabeled examples near or similar to B, but,none for A.
We think example B has higher rep-resentativeness than example A, and A is likelyto be an outlier.
We think adding B to the train-ing set will help the learner more than A.The motivation of our study is that we prefernot only the most informative example in termsof uncertainty measure, but also the most repre-sentative example in terms of density measure.The density measure can be evaluated based onhow many examples there are similar or near to it.An example with high density degree is lesslikely to be an outlier.In most real-world applications, because thescale of unlabeled corpus would be very large,Tang et al (2002) and Shen et al (2004) evalu-ated the density of an example within a cluster.Unlike their work 3 , we adopt a new approach,called K-Nearest-Neighbor-based density (KNN-density) measure, to evaluating the density of anunlabeled example x.
Given a set of K (i.e.
=20used in our experiments) most similar examplesS(x)={s1, s2, ?, sK} of the example x,  the KNN-density DS(.)
of example x is defined as:KsxxDS xSsii?
?= )(),cos()(                     (2)As discussed above, we prefer to select exam-ples with maximum uncertainty and highest den-sity for human annotation.
We think getting theirlabels can help the learner greatly.
To do it, weproposed a new method, sampling by uncertaintyand density (SUD), in which entropy-based un-certainty measure and KNN-density measure areconsidered simultaneously.In SUD scheme, a new uncertainty measure,called density*entropy measure4 , is defined as:)()()( xHxDSxDSH ?=                 (3)4 Initial Training Set GenerationAs shown in Fig.
1, only a small number of train-ing samples are provided at the beginning of ac-tive learning process.
In previous studies on ac-tive learning, the initial training set is generallygenerated by random sampling from the wholeunlabeled corpus.
However, random samplingtechnique can not guarantee selecting a most rep-3 We also tried their cluster-based density measure, but per-formance was essentially degraded.4 We also tried other ways like ?*DS(x)+(1-?)
H(x)measure used in previous studies, but it seems to be random.Actually it is very difficult to determine an appropriate?value for a specific task.resentative subset, because the size of initialtraining set is generally too small (e.g.
10).
Wethink selecting some representative examples toform initial training set can help the activelearner.In this section we utilize an approach, sam-pling by clustering (SBC), to selecting the mostrepresentative examples to form initial trainingdata set.
In the SBC scheme, the whole unlabeledcorpus has been first clustered into a predefinednumber of clusters (i.e.
the predefined size of theinitial training set).
The example closest to thecentroid of each cluster will be selected to aug-ment initial training set, which is viewed as themost representative case.We use the K-means clustering algorithm(Duda and Hart, 1973) to cluster examples in thewhole unlabeled corpus.
In the following K-means clustering algorithm, the traditional cosinemeasure is adopted to estimate the similarity be-tween two examples, that isjijiji wwwwww ?
?=),cos(                     (4)where wi and wj are the feature vectors of the ex-amples i and j.To summarize the SBC-based initial trainingset generation algorithm, let U={U1, U2, ?, UN}be the set of unlabeled examples to be clustered,and k be the predefined size of initial trainingdata set.
In other words, SBC technique selects kmost representative unlabeled examples from Uto generate the initial training data set.
The SBC-based initial training set generation procedure issummarized as follows:SBC-based Initial Training Set GenerationInput: U, kPhrase 1: Cluster the corpus U into k clusters?
j(j=1,?,k) by using K-means clustering algo-rithm as follows:1.
Initialization.
Randomly choosing k exam-ples as the centroid ?j(j=1,?,k) for initialclusters ?
j(j=1,?,k), respectively.2.
Re-partition {U1, U2, ?, UN} into k clus-ters ?
j(j=1,?,k), where}.
),,cos(),cos(:{ jtUUU tijiij ??=?
??3.
Re-estimate the centroid ?j for each clus-ters ?
j, that is:mUjiUij???=?
, where m is the size of ?
j.4.
Repeat Step 2 and Step 3 until the algo-rithm converges.1139Phrase 2: Select the example uj closest to thecentroid?j for each cluster j to augment ini-tial training data set ?, where?
]},1[,),,cos(),cos(:{ kjUuUuu ijjijjj ???=?
??Return?
;The computation complexity of the K-meansclustering algorithm is O(NdkT), where d is thenumber of features and T is the number of itera-tions.
In practice, we can define the stopping cri-terion (i.e.
shown in Step 4) of K-means cluster-ing algorithm that relative change of the totaldistortion is smaller than a threshold.5 Active Learning with SUD and SBCProcedure: Active Learning with SUD and SBCInput: Pool of unlabeled data set U; k is the prede-fined size of initial training data setInitialization.z Evaluate the density of each unlabeled examplein terms of KNN-density measure;z Use SBC technique to generate the small initialtraining data set of size k.Use L to train the initial classifier CRepeat1.
Use the current classifier C to label all unla-beled examples in U2.
Use uncertainty sampling technique in termsof density*entropy measure to select m mostinformative unlabeled examples, and ask ora-cle H for labeling, namely SUD scheme.3.
Augment L with these m new examples, andremove them from U4.
Use L to retrain the current classifier CUntil the predefined stopping criterion SC is met.Figure 3.
Active learning with SUD and SBCFig.
3 shows the algorithm of active learningwith SUD and SBC techniques.
Actually thereare some variations.
For example, if the initialtraining data set is generated by SBC, and en-tropy-based uncertainty measure is used, it isactive learning with SBC.
Similarly, if the initialtraining data set is generated at random, and thedensity*entropy uncertainty measure is used, it isactive learning with SUD.
If both SBC and SUDtechniques are not used, we call it (traditional)uncertainty sampling as baseline method.6 EvaluationIn the following comparison experiments, weevaluate the effectiveness of various active learn-ing methods for WSD and TC tasks on three pub-licly available real-world data sets.6.1 Deficiency MeasureTo compare various active learning methods,deficiency is a statistic developed to compareperformance of active learning methods globallyacross the learning curve, which has been used inprevious studies (Schein and Unga, 2007).
Thedeficiency measure can be defined as:??==?
?= nt tnnt tnnREFaccREFaccALaccREFaccREFALDef11))()(())()((),( (5)where acct is the average accuracy at tth learningiteration.
REF is the baseline active learningmethod, and AL is the active learning variant ofthe learning algorithm of REF, e.g.
active learn-ing with SUD and SBC.
n refers to the evaluationstopping points (i.e.
the number of learned ex-amples).
Smaller deficiency value (i.e.
<1.0) in-dicates AL method is better than REF method.Conversely, a larger value (i.e.
>1.0) indicates anegative result.In the following comparison experiments, weevaluate the effectiveness of six active learningmethods, including random sampling (random),uncertainty sampling (uncertainty), SUD, ran-dom sampling with SBC (random+SBC), uncer-tainty sampling with SBC (uncertainty+SBC),and SUD with SBC (SUD+SBC).
?+SBC?
indi-cates initial training data set generated by SBCtechnique.
Otherwise, initial training set is gen-erated at random.
To evaluate deficiency of eachmethod, the REF method (i.e.
the baselinemethod) defined in Equation (5) refers to (tradi-tional) uncertainty sampling.6.2 Experimental SettingsWe utilize a maximum entropy (ME) model(Berger et al, 1996) to design the basic classifierfor WSD and TC tasks.
The advantage of the MEmodel is the ability to freely incorporate featuresfrom diverse sources into a single, well-groundedstatistical model.
A publicly available ME tool-kit 5  was used in our experiments.
To build theME-based classifier for WSD, three knowledgesources are used to capture contextual informa-tion: unordered single words in topical context,POS of neighboring words with position infor-mation, and local collocations, which are thesame as the knowledge sources used in (Lee andNg, 2002).
In the design of text classifier, themaximum entropy model is also utilized, and nofeature selection technique is used.5See  http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html1140In the following comparison experiments, thealgorithm starts with a initial training set of 10labeled examples, and make 10 queries after eachlearning iteration.
A 10 by 10-fold cross-validation was performed.
All results reportedare the average of 10 trials in each activelearning process.6.3 Data SetsThree publicly available natural data sets havebeen used in the following active learning com-parison experiments.
Interest data set is used forWSD tasks.
Comp2 and WebKB data sets areused for TC tasks.The Interest data set developed by Bruce andWiebe (1994) has been previously used for WSD(Ng and Lee, 1996).
This data set consists of2369 sentences of the noun ?interest?
with itscorrect sense manually labeled.
The noun?interest?
has six different senses in this data set.The Comp2 data set consists of comp.graphicsand comp.windows.x categories from News-Groups,  which has been previously used in ac-tive learning for TC (Roy and McCallum, 2001;Schein and Ungar, 2007).The WebKB dataset was widely used in TCresearch.
Following previous studies (McCallumand Nigam, 1998), we use the four most popu-lous categories: student, faculty, course and pro-ject, altogether containing 4199 web pages.
Inthe preprocessing step, we remove those wordsthat occur merely once without using stemming.The resulting vocabulary has 23803 words.Data sets Interest Comp2 WebKBAccuracy 0.908 0.90 0.91Table 1.
Average accuracy of supervised learningon each data set when all examples have beenlearned.6.4 Active Learning for WSD Task0.550.60.650.70.750.80.850.90  50  100  150  200  250  300AccuracyNumber of Learned ExamplesActive Learning for WSD on Interestrandomrandom + SBCuncertaintyuncertainty + SBCSUDSUD + SBCFigure 4.
Active learning curve for WSD on In-terest data setRandom Random+SBC Uncertainty1.926 1.886 NAUncertainty+SBC SUD SUD+SBC0.947 0.811 0.758Table 2.
Average deficiency achieved by variousactive learning methods on Interest data set.
Thestopping point is 300.Fig.
4 depicts performance curves of various ac-tive learning methods for WSD task on Interestdata set.
Among these six methods, random sam-pling method shows the worst performance.
SUDmethod constantly outperforms uncertainty sam-pling.
As discussed above, SUD method prefersnot only the most uncertainty examples, but alsothe most representative examples.
In the SUDscheme, the factor of KNN-density can effec-tively avoid selecting the outliers that often causeuncertainty sampling to fail.It is noteworthy that using SBC to generateinitial training data set can improve random (-0.04 deficiency), uncertainty (-0.053 deficiency)and SUD (-0.053 deficiency) methods, respec-tively.
If the initial training data set is generatedat random, the initial accuracy is only 55.6%.Interestingly, SBC achieves 62.2% initial accu-racy, and makes 6.6% accuracy performance im-provement.
However, SBC only makes perform-ance improvement for each method at the earlystages of active learning.
After 50 unlabeled ex-amples have been learned, it seems that SBC hasvery little contribution to random, uncertaintyand SUD methods.
Table 2 shows that the bestmethod is SUD with SBC (0.758 deficiency),followed by SUD method.6.5 Active Learning for TC Tasks0.550.60.650.70.750.80.850  50  100  150AccuracyNumber of Learned ExamplesActive Learning for Text Classification on Comp2uncertaintyuncertainty + SBCSUDSUD + SBCFigure 5.
Active learning curve for text classifi-cation on Comp2 data setUncertainty Uncertainty+SBC SUD SUD+SBCNA 0.409 0.588 0.257Table 3.
Average deficiency achieved by variousactive learning methods on Comp2 data set.
Thestopping point is 150.11410.30.350.40.450.50.550.60.650.70.750.80  50  100  150AccuracyNumber of Learned ExamplesActive Learning for Text Classification on WebKBuncertaintyuncertainty + SBCSUDSUD + SBCFigure 6.
Active learning curve for text classifi-cation on WebKB data setUncertainty Uncertainty+SBC SUD SUD+SBCNA 0.669 0.748 0.595Table 4.
Average deficiency achieved by variousactive learning methods on WebKB data set.
Thestopping point is 150.Fig.
5 and 6 show the effectiveness of variousactive learning methods for text classificationtasks.
Since random sampling performs poorly asshown in Fig.
4, it is not further shown in Fig.
5and 6.
We only compare uncertainty samplingand our proposed methods for both text classifi-cation tasks.Similarly, SUD method constantly outper-forms uncertainty sampling on two data sets.SBC greatly improves uncertainty sampling (i.e.0.591 and 0.331 deficiencies degraded) and SUDmethod (i.e.
0.331 and 0.153 deficiencies de-graded), respectively.
Interestingly, unlike WSDtask shown in Fig.
4, Table 3 and 4 show thatuncertainty sampling with SBC outperforms ourSUD method for text classification on both datasets.
The reason is that SBC makes about 15%initial accuracy improvement on Comp2 data set,and about 23% initial accuracy improvement onWebKB data set.
Such improvements indicatethat selecting high representative initial trainingset is very necessary and helpful for active learn-ing.
Table 3 and 4 show that the best activelearning method for TC task is SUD with SBC,following by uncertainty sampling with SBCmethod.
It is noteworthy that on WebKB uncer-tainty sampling with SBC (0.669 deficiency)achieves only slight better performance thanSUD method (0.748 deficiency) as shown in Ta-ble 4, simply because SBC only introduce goodperformance improvement at the early stages.Actually on WebKB SUD method achievesslight better performance than uncertainty sam-pling with SBC after about 50 unlabeled exam-ples have been learned.7 Related WorkIn recent years active learning has been widelystudied in various natural language processing(NLP) tasks, such as word sense disambiguation(Chen et al, 2006; Zhu and Hovy, 2007), textclassification (TC) (Lewis and Gale, 1994;McCallum and Nigam, 1998), named entityrecognition (NER) (Shen et al, 2004), chunking(Ngai and Yarowsky, 2000), informationextraction (IE) (Thompson et al, 1999), andstatistical parsing (Tang et al, 2002).In addition to uncertainty sampling, there isanother popular selective sampling scheme,Query-by-committee (Engelson and Dagan,1999), which generates a committee of classifiers(always more than two classifiers) and selects thenext unlabeled example by the principle ofmaximal disagreement among these classifiers.
Amethod similar to committee-based sampling isco-testing proposed by Muslea et al (2000),which trains two learners individually on twocompatible and uncorrelated views that should beable to reach the same classification accuracy.
Inpractice, however, these conditions of view se-lection are difficult to meet in real-world applica-tions.
Cohn et al (1996) and Roy and McCallum(2001) proposed a method that directly optimizesexpected future error on future test examples.However, the computational complexity of theirmethods is very high.There are some similar previous studies (Tanget al, 2002; Shen et al, 2004) in which the rep-resentativeness criterion in active learning isconsidered.
Unlike our sampling by uncertaintyand density technique, Tang et al (2002) adopteda sampling scheme of most uncertain per clusterfor NLP parsing, in which the learner selects thesentence with the highest uncertain score fromeach cluster, and use the density to weight theselected examples while we use density informa-tion to select the most informative examples.
Ac-tually the scheme of most uncertain per clusterstill can not solve the outlier problem faced byuncertainty sampling technique.
Shen et al(2004) proposed an approach to selecting exam-ples based on informativeness, representativenessand diversity criteria.
In their work, the densityof an example is evaluated within a cluster, andmultiple criteria have been linearly combinedwith some coefficients.
However, it is difficult toautomatically determine sufficient coefficients inreal-world applications.
Perhaps there are differ-ent appropriate coefficients for various applica-tions.11428 DiscussionFor batch mode active learning, we found some-times there is a redundancy problem that someselected examples are identical or similar.
Suchsituation would reduce the representativeness ofselected examples.
To solve this problem, wetried the sampling scheme of ?most uncertain percluster?
(Tang et al, 2002) to select the mostinformative examples.
We think selecting exam-ples from each cluster can alleviate the redun-dancy problem.
However, this sampling schemeworks poorly for WSD and TC on the three datasets, compared to traditional uncertainty sam-pling.
From the clustering results, we found theseresulting clusters are very imbalanced.
It makessense that more informative examples are con-tained in a bigger cluster.
In this work, we onlyuse SUD technique to select the most informativeexamples for active learning.
We plan to studyhow combining SBC and SUD techniques canenhance the selection of the most informativeexamples in the future work.Furthermore, we think that a misclassifiedunlabeled example may convey moreinformation than a correctly classified unlabeledexample which is closer to the decision boundary.But there is a difficulty that the true label of eachunlabeled example is unknown.
To use misclassi-fication information to select the most informa-tive examples, we should study how to automati-cally determine whether an unlabeled examplehas been misclassified.
For example, we canmake an assumption that an unlabeled examplemay be misclassified if this example was previ-ously ?outside?
and is now ?inside?.
We willstudy this issue in the future work.Actually these proposed techniques can beeasily applied for committee-based sampling foractive learning.
However, to do so, we shouldadopt a new uncertainty measurement such asvote entropy to measure the uncertaity of eachunlabled example in committee-based samplingscheme.9 Conclusion and Future WorkIn this paper, we have addressed two issues ofactive learning, involving the outlier problem oftraditional uncertainty sampling, and initial train-ing data set generation.
To solve the outlier prob-lem of traditional uncertainly sampling, we pro-posed a new method of sampling by uncertaintyand density (SUD) in which KNN-density meas-ure and uncertainty measure are combined to-gether to select the most informative unlabeledexample for human annotation at each learningcycle.
We employ a method of sampling by clus-tering (SBC) to generate a representative initialtraining data set.
Experimental results on threeevaluation data sets show that our combinedSUD with SBC method achieved the best per-formance compared to other competing methods,particularly at the early stages of active learningprocess.
In future work, we will focus on the re-dundancy problem faced by batch mode activelearning, and how to make use of misclassifiedinformation to select the most useful examplesfor human annotation.AcknowledgmentsThis work was supported in part by the National863 High-tech Project (2006AA01Z154) and theProgram for New Century Excellent Talents inUniversity (NCET-05-0287).ReferencesBerger Adam L., Vincent J. Della Pietra, Stephen A.Della Pietra.
1996.
A maximum entropy approachto natural language processing.
ComputationalLinguistics 22(1):39?71.Bruce Rebecca and Janyce Wiebe.
1994.
Word sensedisambiguation using decomposable models.
Pro-ceedings of the 32nd annual meeting on Associa-tion for Computational Linguistics, pp.
139-146.Chan Yee Seng and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
Proceedings of the 45th annual meet-ing on Association for Computational Linguistics,pp.
49-56Chen Jinying, Andrew Schein, Lyle Ungar andMartha Palmer.
2006.
An empirical study of thebehavior of active learning for word sense disam-biguation.
Proceedings of the main conference onHuman Language Technology Conference of theNorth American Chapter of the Association ofComputational Linguistics, pp.
120-127Cohn David A., Zoubin Ghahramani and Michael I.Jordan.
1996.
Active learning with statistical mod-els.
Journal of Artificial Intelligence Research, 4,129?145.Duda Richard O. and Peter E. Hart.
1973.
Patternclassification and scene analysis.
New York:Wiley.Engelson S. Argamon and I. Dagan.
1999.
Commit-tee-based sample selection for probabilistic classi-fiers.
Journal of Artificial Intelligence Research(11):335-360.1143Lee Yoong Keok and Hwee Tou Ng.
2002.
An em-pirical evaluation of knowledge sources and learn-ing algorithm for word sense disambiguation.
InProceedings of the ACL-02 conference on Empiri-cal methods in natural language processing, pp.
41-48Lewis David D. and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
InProceedings of the 17th annual international ACMSIGIR conference on Research and development ininformation retrieval, pp.
3-12McCallum Andrew and Kamal Nigam.
1998.
A com-parison of event models for na?ve bayes text classi-fication.
In AAAI-98 workshop on learning for textcategorization.Muslea Ion, Steven Minton and Craig A. Knoblock.2000.
Selective sampling with redundant views.
InProceedings of the Seventeenth National Confer-ence on Artificial Intelligence and Twelfth Confer-ence on Innovative Applications of Artificial Intel-ligence, pp.
621-626.Ng Hwee Tou and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: an exemplar-based approach.
In Proceed-ings of the Thirty-Fourth Annual Meeting of theAssociation for Computational Linguistics, pp.
40-47Ngai Grace and David Yarowsky.
2000.
Rule writingor annotation: cost-efficient resource usage forbased noun phrase chunking.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics, pp.
117-125Roy Nicholas and Andrew McCallum.
2001.
Towardoptimal active learning through sampling estima-tion of error reduction.
In Proceedings of theEighteenth International Conference on MachineLearning, pp.
441-448Schein Andrew I. and Lyle H. Ungar.
2007.
Activelearning for logistic regression: an evaluation.Machine Learning 68(3): 235-265Schohn Greg and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProceedings of the Seventeenth International Con-ference on Machine Learning, pp.
839-846Shen Dan, Jie Zhang, Jian Su, Guodong Zhou andChew-Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In Proceed-ings of the 42nd Annual Meeting on Associationfor Computational Linguistics.Tang Min, Xiaoqiang Luo and Salim Roukos.
2002.Active learning for statistical natural languageparsing.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pp.
120-127Thompson Cynthia A., Mary Elaine Califf and Ray-mond J. Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
InProceedings of the Sixteenth International Confer-ence on Machine Learning, pp.
406-414Zhu Jingbo and Eduard Hovy.
2007.
Active learningfor word sense disambiguation with methods foraddressing the class imbalance problem.
In Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pp.783-790Zhu Jingbo, Huizhen Wang and Eduard Hovy.
2008.Learning a stopping criterion for active learningfor word sense disambiguation and text classifica-tion.
In Proceedings of the Third International JointConference on Natural Language Processing, pp.366-3721144
