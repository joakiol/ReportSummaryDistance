Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomatic Scoring of Children's Read-Aloud Text Passages andWord ListsKlaus Zechner and John Sabatini and Lei ChenEducational Testing ServiceRosedale RoadPrinceton, NJ 08541, USA{kzechner,jsabatini,lchen}@ets.orgAbstractAssessment of reading proficiency is typicallydone by asking subjects to read a text passagesilently and then answer questions related tothe text.
An alternate approach, measuringreading-aloud proficiency, has been shown tocorrelate well with the aforementioned com-mon method and is used as a paradigm in thispaper.We describe a system that is able to automati-cally score two types of children?s read speechsamples (text passages and word lists), usingautomatic speech recognition and the targetcriterion ?correctly read words per minute?.Its performance is dependent on the data type(passages vs. word lists) as well as on the rela-tive difficulty of passages or words for indi-vidual readers.
Pearson correlations withhuman assigned scores are around 0.86 forpassages and around 0.80 for word lists.1 IntroductionIt has long been noted that a substantial number ofU.S.
students in the 10-14 years age group havedeficiencies in their reading competence (NationalCenter of Educational Statistics, 2006).
With theenactment of the No Child Left Behind Act (2002),interest and focus on objectively assessing and im-proving this unsatisfactory situation has come tothe forefront.While assessment of reading is usually done post-hoc with measures of reading comprehension, di-rect reading assessment is also often performedusing a different method, oral (read-aloud) reading.In this paradigm, students read texts aloud andtheir proficiency in terms of speed, fluency, pro-nunciation, intonation etc.
can be monitored di-rectly while reading is in progress.
In the readingresearch literature, oral reading has been one of thebest diagnostic and predictive measures of founda-tional reading weaknesses and of overall readingability (e.g., Deno et al, 2001; Wayman et al,2007).
An association between low reading com-prehension and slow, inaccurate reading rate hasbeen confirmed repeatedly in middle school popu-lations (e.g., Deno & Marsten, 2006).
Correlationsconsistently fall in the 0.65-0.7 range for predict-ing untimed passage reading comprehension testoutcomes (Wayman et al, 2007).In this paper, we investigate the feasibility oflarge-scale, automatic assessment of read-aloudspeech of middle school students with a reasonabledegree of accuracy (these students typically attendgrades 6-8 and their age is in the 10-14 yearsrange).
If possible, this would improve the utilityof oral reading as a large-scale, school-based as-sessment technique, making it more efficient bysaving costs and time of human annotations andgrading of reading errors.The most widely used measure of oral reading pro-ficiency is ?correctly read words per minute?
(cwpm) (Wayman et al, 2007).
To obtain thismeasure, students?
read speech samples are first10recorded, then the reading time is determined, andfinally a human rater has to listen to the recordingand note all reading errors and sum them up.
Read-ing errors are categorized into word substitutions,deletions etc.We have several sets of digitally recorded read-aloud samples from middle school students avail-able which were not collected for use with auto-matic speech recognition (ASR) but which werescored by hand.Our approach here is to pass the children?s speechsamples through an automatic speech recognizerand then to align its output word hypotheses withthe original text that was read by the student.
Fromthis alignment and from the reading time, an esti-mate for the above mentioned measure of cwpmcan then be computed.
If the automatically com-puted cwpm measures are close enough to thoseobtained by human hand-scoring, this process maybe employed in real world settings eventually tosave much time and money.Recognizing children?s speech, however, has beenshown to be substantially harder than adult speech(Lee et al, 1999; Li and Russell, 2002), which ispartly due to children?s higher degree of variabilityin different dimensions of language such as pro-nunciation or grammar.
In our data, there was alsoa substantial number of non-native speakers ofEnglish, presenting additional challenges.
We usedtargeted training and adaptation of our ASR sys-tems to achieve reasonable word accuracies.
Whilefor text passages, the word accuracy on unseenspeakers was about 72%, it was only about 50%for word lists, which was due in part to a higherpercentage of non-native speakers in this data set,to the fact that various sources of noise often pre-vented the recognizer from correctly locating thespoken words in the signal, and also due to ourchoice of a uniform language model since conven-tional n-gram models did not work on this datawith many silences and noises between words.The remainder of this paper is organized as fol-lows: in Section 2 we review related work, fol-lowed by a description of our data in Section 3.Section 4 provides a brief description of our speechrecognizer as well as the experimental setup.
Sec-tion 5 provides the results of our experiments, fol-lowed by a discussion in Section 6 and conclusionsand future work in Section 7.2 Related workFollowing the seminal paper about the LISTENproject (Mostow et al 1994), a number of studieshave been conducted on using automatic speechrecognition technology to score children?s readspeech.Similar to automated assessment of adults?
speech(Neumeyer, Franco et al 2000; Witt, 1999), thelikelihood computed in the Hidden Markov Model(HMM) decoding and some measurements of  flu-ency, e.g., speaking rate, are widely used as fea-tures for predicting children?s speakingproficiency.
Children?s speech is different thanadults?.
For example, children?s speech exhibitshigher fundamental frequencies (F0) than adults onaverage.
Also, children?s more limited knowledgeof vocabulary and grammar results in more errorswhen reading printed text.
Therefore, to achievehigh-quality recognition on children?s speech,modifications have to be made on recognizers thatotherwise work well for adults.In the LISTEN project (Mostow et al, 1994), thebasic technology is to use speech recognition toclassify each word of text as correctly read or not.Such a classification task is hard in that the chil-dren?s speaking deviations from the text may in-clude arbitrary words and non-words.
In a study,they modeled variations by the modification of thelexicon and the language model of the Sphinx1speech recognizer.Recently, the Technology Based Assessment  ofLanguage and Literacy project (TBALL,  (Alwan,2007)) has been attempting to assess and evaluatethe language and literacy skills of young childrenautomatically.
In the TBALL project, a variety oftests including word verification, syllable blending,letter naming, and reading comprehension, arejointly used.
Word verification is an assessmentthat measures the child?s pronunciation of read-aloud target words.
A traditional pronunciationverification method based on log-likelihoods fromHMM models is used initially (Tepperman et al,2006).
Then an improvement based on a Bayesiannetwork classifier (Tepperman et al, 2007) is em-1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php11ployed to handle complicated errors such as pro-nunciation variations and other reading mistakes.Many other approaches have been developed tofurther improve recognition performance on chil-dren?s speech.
For example, one highly accuraterecognizer of children?s speech has been developedby Hagen et al (2007).
Vocal tract length normali-zation (VTLN) has been utilized to cope with thechildren?s different acoustic properties.
Some spe-cial processing techniques, e.g., using a generalgarbage model to model all miscues in speaking,have been devised to improve the language modelused in the recognition of children?s speech (Li etal., 2007).3 DataFor both system training and evaluation, we use adata set containing 3 passages read by the same265 speakers (Set1) and a fourth passage (a longerversion of Passage 1), read by a different set of 55speakers (Set2).
Further, we have word lists readby about 500 different speakers (Set3).
All speak-ers from Set12 and most (84%) from the third setwere U. S. middle school students in grades 6-8(age 10-14).
A smaller number of older students ingrades 10-12 (age 15-18) was also included in thethird set (16%).3 4In terms of native language, about 15% of Set1 andabout 76% of Set35 are non-native speakers ofEnglish or list a language different from English astheir preferred language.Table 1 provides the details of these data sets.
Inthe word lists data set, there are 178 different wordlists containing 212 different word types in total(some word lists were read by several differentstudents).All data was manually transcribed using a spread-sheet where each word is presented in one line andthe annotator, who listens to the audio file, has to2 For Set1, we have demographics for 254 of 265 speakers(both for grade level and native language).3 Grade demographics are available for 477 speakers of Set3.4 We do not have demographic data for the small Set2 (55speakers).5 This set (Set 3) has information on native language for 165speakers.mark-up any insertions, substitutions or deletionsby the student.Name Recordings Length inwordsPassage 1(?Bed?, Set1-A)265 158Passage 2(?Girls?, Set1-B)265 74Passage 3(?Keen?, Set1-C)265 100Passage 4(?Bed*?)
(Set2)55 197Word lists (Set3) 590 62 (average)Table 1.
Text passages and word lists data sets.For ASR system training only, we additionallyused parts of the OGI (Oregon Graduate Institute)and CMU (Carnegie Mellon University) Kids datasets as well (CSLU, 2008; LDC, 1997).4 ASR system and experimentsThe ASR system?s acoustic model (AM) wastrained using portions of the OGI and CMU Kids?corpora as well as a randomly selected sub-set ofour own passage and word list data sets describedin the previous section.
About 90% of each data set(Set1, Set2, Set3) was used for that purpose.
Sincethe size of our own data set was too small for AMtraining, we had to augment it with the two men-tioned corpora (OGI, CMU Kids), although theywere not a perfect match in age range and accent.All recordings were first converted and down-sampled to 11 kHz, mono, 16 bit resolution, PCMformat.
There was no speaker overlap betweentraining and test sets.For the language model (LM), two different mod-els were created: for passages, we built an interpo-lated trigram LM where 90% of the weight isassigned to a LM trained only on the 4 passagesfrom the training set (Set1, Set2) and 10% to a ge-neric LM using the Linguistic Data Consortium(LDC) Broadcast News corpus (LDC, 1997).
Thedictionary contains all words from the transcribedpassages in the training set, augmented with the1,000 most frequent words from the BroadcastNews corpus.
That way, the LM is not too restric-tive and allows the recognizer to hypothesize some12reading mistakes not already encountered in thehuman transcriptions of the training set.For the word lists, a trigram LM was found to benot working well since the words were spoken inisolation with sometimes significant pauses in be-tween and automatic removal of these silencesproved too hard given other confounding factorssuch as microphone, speaker, or background noise.Therefore it was decided to implement a grammarLM for the word list decoder where all possiblewords are present in a network that allows them tooccur at any time and in any sequence, allowingfor silence and/or noises in between words.
Thismodel with uniform priors, however, has the dis-advantage of not including any words not presentin the word list training set, such as common mis-pronunciations and is therefore more restrictivethan the LM for text passages.One could make the argument of using forcedalignment instead of a statistical LM to determinereading errors.
In fact, this approach is typicallyused when assessing the pronunciation of readspeech.
However, in our case, the interest is morein determining how many words were read cor-rectly in the sequence of the text (and how fastthey were read) as opposed to details in pronuncia-tion.
Further, even if we had confidence scoresattached to words in forced alignment, deciding onwhich of the words obtained low confidence due topoor pronunciation or due to substitution wouldnot be an easy decision.
Finally, word deletionsand insertions, if too frequent, might prevent theforced alignment algorithm from terminating.After training was complete, we tested the recog-nizer on the held-out passage and word list data.After recognizing, we computed our target meas-ure of ?correct words per minute?
(cwpm) accord-ing to the following formula (W= all words in atext, S= substitutions, D= deletions, T= readingtime in minutes), performing a string alignmentbetween the recognizer hypothesis and the passageor word list to be read:(1)W S DcwpmT?
?=The reason that insertions are not considered hereis that they contribute to an increase in readingtime and therefore can be considered to be ac-counted for already in the formula.Next, we performed an experiment that looks atwhether automatic scoring of read-aloud speechallows for accurate predictions of student place-ments in broad cohorts of reading proficiency.We then also look more closely at typical errorsmade by human readers and the speech recognizer.All these experiments are described and discussedin the following section.Table 2 describes the set-up of the experiments.Note that Passage4 (Set2) was included only in thetraining but not in the evaluation set since this setwas very small.
As mentioned in the previous sec-tion, most speakers from the passage sets readmore than one passage and a few speakers from theword lists set read more than one word list.Data set Recordings Speakers LanguagemodeltypePassages1-3101 37 TrigramWord lists 42 38 GrammarTable 2.
Experiment set-up (evaluation sets).5 Results5.1 Overall resultsTable 3 depicts the results of our evaluation runwith the ASR system described above.
Word accu-racy is measured against the transcribed speakerreference (not against the true text that was read).Word accuracy is computed according to Equation(2), giving equal weight to reference and ASR hy-pothesis (c=correct, s=substitutions, d=deletions,i=insertions).
This way, the formula is unbiasedwith respect to insertions or deletions:(2)0.5 100.0c cwaccc s d c s i?
?= ?
?
+?
?+ + + +?
?13Data set Recordings Speakers Average wordAccuracy over allspeech sampleMinimum wordaccuracy on aspeech sampleMaximum wordaccuracy on a speechsampleAll Passages(1-3)101 37 72.2 20.4 93.8Passage1(?Bed?
)28 28 70.8 20.4 83.6Passage2(?Girls?
)36 36 64.1 25.4 85.7Passage3(?Keen?
)37 37 77.7 27.4 93.8Word lists 42 38 49.6 10.8 78.9Table 3.
ASR experiment results (word accuracies in percent)The typical run-time on a 3.2GHz Pentium proces-sor was less than 30 seconds for a recording (fasterthan real time).We next compute cwpm measures for both humanannotations (transcripts, ?gold standard?)
and ma-chine (ASR) hypothesesHuman annotators went over each read passageand word list and marked all reading errors of thespeakers (here, only deletions and substitutions arerelevant).
The reading time is computed directlyfrom the speech sample, so machine and humancwpm scores only differ in error counts of dele-tions and substitutions.
Currently we only have onehuman annotation available per speech sample, butwe aim to obtain a second annotation for the pur-pose of determining inter-annotator agreement.Table 4 presents the overall results of comparingmachine and human cwpm scoring.
We performedboth Pearson correlation as well as Spearman rankcorrelation.
While the former provides a more ge-neric measure of cwpm correlation, the latter fo-cuses more on the question of the relativeperformance of different speakers compared totheir peers which is usually the more interestingquestion in practical applications of reading as-sessment.
Note that unlike for Table 3, the ASRhypotheses are now aligned with the text to be readsince in a real-world application, no human tran-scriptions would be available.We can see that despite the less than perfect recog-nition rate of the ASR system which causes a muchlower average estimate for cwpm or cw (for word-lists), both Pearson and Spearman correlation coef-ficients are quite high, all above 0.7 for Spearmanrank correlation and equal to 0.8 or higher for thePearson product moment correlation.
This is en-couraging as it indicates that while current ASRtechnology is not yet able to exactly transcribechildren?s read speech, it isData set GoldcwpmASR-basedcwpmPearsonr corre-lationSpearmanrank cor-relationAll Pas-sages(1-3)152.0 109.8 0.86 NAPassage1(Bed)174.3 123.5 0.87 0.72Passage2(Girls)133.1 86.5 0.86 0.73Passage3(Keen)153.4 122.2 0.86 0.77Wordlists*48.0 29.4 0.80 0.81Table 4.
CWPM results for passages and wordlists.
All correlations are significant at p<0.01.
*For word lists, we use ?cw?
(correct words, nu-merator of Equation (1)) as the measure, since stu-dents were not told to be rewarded for fasterreading time here.possible to use its output to compute reasonableread-aloud performance measures such as cwpm14which can help to quickly and automatically assessreading proficiencies of students.5.2 Cohort assignment experimentTo follow up on the encouraging results with basicand rank correlation, we conducted an experimentto explore the question of practical importancewhether the automatic system can assign studentsto reading proficiency cohorts automatically.For better comparison, we selected those 27 stu-dents from 37 total who read all 3 passages (Set 1)and grouped them into three cohorts of 9 studentseach, based on their human generated cwpm scorefor all passages combined: (a) proficient(cwpm>190), (b) intermediate (135<cwpm<190),and (c) low proficient (cwpm<135).We then had the automatic system predict eachstudent?s cohort based on the cwpm computedfrom ASR.
Since ASR-based cwpm values are co-nsistently lower than human annotator based cwpmvalues, the automatic cohort assignment is notbased on the cwpm values but rather on their rank-ing.The outcome of this experiment is very encourag-ing in that there were no cohort prediction errorsby the automatic system.
While the precise rankingdiffers, the system is very well able to predictoverall cohort placement of students based oncwpm.5.3 Overall comparison of students?
reading er-rors and ASR recognition errorsTo look into more detail of what types of readingerrors children make and to what extent they arereflected by the ASR system output, we used thesclite-tool by the National Institute for Standardsand Technology (NIST, 2008) and performed twoalignments on the evaluation set:1.
TRANS-TRUE: Alignment between humantranscription and true passage or word list text tobe read: this alignment informs us about the kindsof reading errors made by the students.2.
HYPO-TRANS: Alignment between the ASRhypotheses and the human transcriptions; thisalignment informs us of ASR errors.
(Note that thisis different from the experiments reported in Table4 above where we aligned the ASR hypotheseswith the true reference texts to compute cwpm.
)Table 5 provides general statistics on these twoalignments.Data set Alignment SUB DEL INSPassages1-3TRANS-TRUE2.0% 6.1% 1.8%Pas-sages1-3HYPO-TRANS18.7% 9.6% 8.1%WordlistsTRANS-TRUE5.6% 6.2% 0.6%WordlistsHYPO-TRANS42.0%  8.9% 6.4%Table 5.
Word error statistics on TRANS-TRUEand HYPO-TRANS alignments for both evaluationdata sets.From Table 5 we can see that while for students,deletions occur more frequently than substitutionsand, in particular, insertions, the ASR system, dueto its imperfect recognition, generates mostly sub-stitutions, in particular for the word lists where theword accuracy is only around 50%.Further, we observe that the students?
averagereading word error rate (only taking into accountsubstitutions and deletions as we did above for thecwpm and cw measures) lies around 8% for pas-sages and 12% for wordlists (all measured on theheld-out evaluation data).5.4 Specific examplesNext, we look at some examples of frequent confu-sion pairs for those 4 combinations of data sets andalignments.
Table 6 lists the top 5 most frequentconfusion pairs (i.e., substitutions).For passages, all of the most frequent reading er-rors by students are morphological variants of thetarget words, whereas this is only true for some ofthe ASR errors, while other ASR errors can be faroff the target words.
For word lists, student errorsare sometimes just orthographically related to thetarget word (e.g., ?liner?
instead of ?linear?
), andsometimes of different part-of-speech (e.g.,?equally?
instead of ?equality?).
ASR errors aretypically related to the target word by some pho-netic similarity (e.g., ?example?
instead of ?sim-ple?
).15Finally, we look at a comparison between errorsmade by the students and the fraction of those cor-rectly identified by the ASR system in the recogni-tion hypotheses.
Table 7 provides the statistics onthese matched errors for text passages and wordlists.DatasetAlign-mentRefer-enceSpoken/recog-nizedCountPas-sages1-3TRANS-TRUEaskssavingsprojectsteacher?stimeasksavingprojectteachertimes65444Pas-sages1-3HYPO-TRANSstormlee?slee?sobserverthousandstormsbeweandthe116666WordlistsTRANS-TRUEnatureover-sleepequalitylinearware-housedNaturaloversleptequallylinerware-house65443WordlistsHYPO-TRANSplanseesimpleunoffi-cialloudplannedseasonexamplecompeti-tionthrough-out86654Table 6.
Top 5 most frequent confusion pairs forpassages and word list evaluation sets in two dif-ferent alignments.
For passages, substitutionsamong closed class words such as determiners orprepositions are omitted.Table 7 shows that while for text passages, almosthalf of the relevant errors (substitutions and dele-tions) were correctly identified by the recognizer,for word lists, this percentage is substantiallysmaller.6 DiscussionThe goal of this paper is to evaluate the possibilityof creating a system for automatic oral reading as-sessment for middle school children, based on textpassages and word lists.We decided to use the common reading profi-ciency measure of ?correct words per minute?which enables us to align ASR word hypotheseswith the correct texts, estimate cwpm based on thisalignment and the reading time, and then comparethe automatically estimated cwpm with human an-notations of the same texts.Data set / error type Percentage of correctlyidentified errorsPassages 1-3 ?
SUB 20.6Passages 1-3 ?
DEL 56.4Passages 1-3 ?SUB+DEL47.7Word lists ?
SUB 2.7Word lists ?
DEL 29.4Word lists ?SUB+DEL16.8Table 7.
Statistics on matched errors: percentage ofstudents?
reading errors (substitutions and dele-tions) that were also correctly identified by theASR system.We built a recognizer with an acoustic modelbased on CMU and OGI kids?
corpora as well asabout 90% of our own text passages and word listdata (Sets 1-3).
For the in-context reading (textpassages) we trained a trigram model focusedmostly on transcriptions of the passages.
For theout-of-context isolated word reading, we used agrammar language model where every possibleword of the word lists in the training set can followany other word at any time, with silence and/ornoise between words.
(While this was not our pre-ferred choice, standard n-gram language modelsperformed very poorly given the difficulty of re-moving inter-word silences or noise automati-cally.
)Given how hard ASR for children?s speech is andgiven our small matched data sets, the word accu-racy of 72% for text passages was not unreason-able and was acceptable, particularly in a firstdevelopment cycle.
The word accuracy of onlyabout 50% for word lists, however, is more prob-16lematic and we conjecture that the two main rea-sons for the worse performance were (a) the ab-sence of time stamps for the location of wordswhich made it sometimes hard for the recognizer tolocate the correct segment in the signal for worddecoding (given noises in between), and (b) thesometimes poor recording conditions where vol-umes were set too high or too low, too much back-ground or speaker noise was present etc.
Further,the high relative number of non-native speakers inthat data set may also have contributed to the lowerword accuracy of the word lists.While the current data collection had not beendone with speech recognition in mind, in futuredata collection efforts, we will make sure that thesound quality of recordings is better monitored,with some initial calibration, and that we store timestamps when words are presented on the screen tofacilitate the recognition task and to allow the rec-ognizer to expect one particular word at one par-ticular point in time.Despite imperfect word accuracies, however, forboth passages and word lists we found encourag-ingly high correlations between human and auto-matic cwpm measures (cw measures for wordlists).
Obviously, the absolute values of cwpm dif-fer greatly as the ASR system generates manymore errors on average than the readers, but bothPearson correlation as well as Spearman rank cor-relation measures are all above 0.7.
This meansthat if we would use our automatic scoring resultsto rank students?
reading proficiency, the rankingorder would be overall quite similar to an orderproduced by human annotators.
This observationabout the rank, rather than the absolute value ofcwpm, is important in so far as it is often the casethat educators are interested in separating ?co-horts?
of readers with similar proficiency and inparticular to identify the lowest performing cohortfor additional reading practice and tutoring.An experiment testing the ability of the system toplace students into three reading proficiency co-horts based on cwpm was very encouraging in thatall 27 students of the test set were placed in thecorrect cohort by the system.When we compare frequent student errors withthose made by the machine (Table 6), we see thatoften times, students just substitute slight morpho-logical variants (e.g., ?ask?
for ?asks?
), whereas inthe ASR system, errors are typically more complexthan just simple substitutions of morphologicalvariants.
However, in the case of word lists, we dofind substitutions with related phonological contentin the ASR output (e.g., ?example?
for ?simple?
).Finally, we observed that, only for the text pas-sages, the ASR system could correctly identify asubstantial percentage of readers?
substitutions anddeletions (about 48%, see Table 7).
This is alsoencouraging as it is a first step towards meaningfulfeedback in a potential interactive setting.
How-ever, we here only look at recall ?
because of themuch larger number of ASR substitutions, preci-sion is much lower and therefore the risk of over-correction (false alarms) is still quite high.Despite all of the current shortcomings, we feelthat we were able to demonstrate a ?proof-of-concept?
with our initial system in that we can useour trained ASR system to make reliable estimateson students?
reading proficiency as measured with?correct words per minute?, where correlationsbetween human and machine scores are in the0.80-0.86 range for text passages and word lists.7 Conclusions and future workThis paper demonstrates the feasibility of buildingan automatic scoring system for middle school stu-dents?
reading proficiency, using a targeted trainedspeech recognition system and the widely usedmeasure of ?correctly read words per minute?
(cwpm).The speech recognizer was trained both on externaldata (OGI and CMU kids?
corpora) and internaldata (text passages and word lists), yielding twodifferent modes for text passages (trigram languagemodel) and word lists (grammar language model).Automatically estimated cwpm measures agreedclosely with human cwpm measures, achieving 0.8and higher correlation with Pearson and 0.7 andhigher correlation with Spearman rank correlationmeasures.Future work includes an improved set-up for re-cordings such as initial calibration and on-linesound quality monitoring, adding time stamps torecordings of word lists, adding more data fortraining/adaptation of the ASR system, and explor-ing other features (such as fluency features) andtheir potential role in cwpm prediction.17AcknowledgementsThe authors would like to acknowledge the contri-butions of Kathy Sheehan, Tenaha O?Reilly andKelly Bruce to this work.
We further are gratefulfor the useful feedback and suggestions from ourcolleagues at ETS and the anonymous reviewersthat greatly helped improve our paper.ReferencesAlwan, A.
(2007).
A System for Technology BasedAssessment of Language and Literacy in YoungChildren: the Role of Multiple InformationSources.
Proceedings of MMSP, Greece.Center for Spoken Language Understanding(CSLU), 2008.
Kids?
Speech Corpus,http://www.cslu.ogi.edu/corpora/kids/.LDC, BN.Deno, S. L., Fuchs, L. S., Marston, D., & Shin, J.(2001).
Using curriculum-based measurementsto establish growth standards for students withlearning disabilities.
School Psychology Re-view, 30(4), 507-524.Deno, S. L. and D. Marsten (2006).
Curriculum-based measurement of oral reading: An indicatorof growth in fluency.
What Research Has to Sayabout Fluency Instruction.
S. J. Samuels and A.E.
Farstrup.
Newark, DE, International ReadingAssociation: 179-203.Hagen, A., B. Pellom, & R. Cole.
(2007).
"Highlyaccurate children?s speech recognition for inter-active reading tutors using subword units.
"Speech Communication 49(6): 861-873.Lee, S., A. Potamianos, & S. Narayanan.
(1999).
"Acoustics of children's speech: developmentalchanges of temporal and spectral parameters.
"Journal of Acoustics Society of American(JASA) 105: 1455-1468.Li, X., Y. C. Ju, L. Deng & A. Acero.
(2007).
Effi-cient and Robust Language Modeling in anAutomatic Children's Reading Tutor System.Proc.
IEEE International Conference on Acous-tics, Speech and Signal Processing ICASSP2007.Li, Q. and M. Russell (2002).
An analysis of thecauses of increased error rates in children'sspeech recognition.
ICSLP.
Denver, CO.Linguistic Data Consortium (LDC), 1997.
1996English Broadcast News Speech (HUB4),LDC97S44.Linguistic Data Consortium (LDC), 1997.
TheCMU Kids Corpus, LDC97S63.Mostow, J., S. F. Roth, G. Hauptmann & M.
Kane.(1994).
A prototype reading coach that listens.AAAI '94: Proceedings of the twelfth nationalconference on Artificial intelligence, MenloPark, CA, USA, American Association for Arti-ficial Intelligence.National Center of Educational Statistics.
(2006).National Assessment of Educational Progress.Washington DC: U.S. Government Printing Of-fice.National Institute for Standards and Technology(NIST), 2008.
Sclite software package.http://www.nist.gov/speech/tools/Neumeyer, L., H. Franco, V. Digalakis & M.Weintraub.
(2000).
"Automatic Scoring of Pro-nunciation Quality."
Speech Communication 6.No Child Left Behind Act of 2001, Pub.
L. No.107-110, 115 Stat.
1425 (2002).Tepperman, J., J. Silva, A. Kazemzadeh, H. You,S.
Lee, A. Alwan & S. Narayanan.
(2006).
Pro-nunciation verification of children's speech forautomatic literacy assessment.
INTERSPEECH-2006.
Pittsburg, PA.Tepperman, J., M. Black, P. Price, S. Lee, A. Ka-zemzadeh, M. Gerosa, M. Heritage, A. Alwan &S.
Narayanan.(2007).
A bayesian network clas-sifier for word-level reading assessment.
Pro-ceedings of ICSLP, Antwerp, Belgium.Wayman, M. M., Wallace, T., Wiley, H. I., Ticha,R., & Espin, C. A.
(2007).
Literature synthesison curriculum-based measurement in reading.The Journal of Special Education, 41(2), 85-120.Witt, S. M. (1999).
Use of Speech Recognition inComputer-assisted Language Learning, Univer-sity of Cambridge.18
