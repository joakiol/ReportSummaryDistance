Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2097?2106, Dublin, Ireland, August 23-29 2014.An Analysis of Causality between Events andits Relation to Temporal InformationParamita MirzaFondazione Bruno Kessler,University of TrentoTrento, Italyparamita@fbk.euSara TonelliFondazione Bruno KesslerTrento, Italysatonelli@fbk.euAbstractIn this work we present an annotation framework to capture causality between events, inspiredby TimeML, and a language resource covering both temporal and causal relations.
This data setis then used to build an automatic extraction system for causal signals and causal links betweengiven event pairs.
The evaluation and analysis of the system?s performance provides an insightinto explicit causality in text and the connection between temporal and causal relations.1 IntroductionCausality is a concept that has been widely investigated from a philosophical, psychological and logicalpoint of view, but how to model its recognition and representation in NLP-centered applications isstill an open issue.
However, information on causality could be beneficial to a number of naturallanguage processing tasks such as question answering, text summarization, decision support, etc.
Thelack of information extraction systems focused on causality may depend also on the lack of unifiedannotation guidelines and standard benchmarks, which usually foster the comparison of different systemsperformances.
Specific phenomena related to causality, such as causal arguments (Bonial et al., 2010),causal discourse relations (The PDTB Research Group, 2008) or causal relations between nominals (Girjuet al., 2007), have been investigated, but no unified framework has been proposed to capture causalrelations between events, as opposed to the existing TimeML standard for temporal relations (Pustejovskyet al., 2010).The work presented in this paper copes with this issue by i) proposing an annotation framework tomodel causal relations between events and ii) detailing the development and the evaluation of a supervisedsystem based on such framework.We take advantage of the formalization work carried out for the TimeML standard, in which events,temporal relations and temporal signals have been carefully defined and annotated.
We propose to modelcausal relations in a similar way to temporal relations, inheriting from TimeML the notion of event,relation and signal, even though our approach to causality is well rooted in the force dynamic model byTalmy (1985).Besides, we focus our preliminary annotation on TimeBank (Pustejovsky et al., 2006), a corpus widelyused by the research community working on temporal processing.
This should possibly enable theadaptation of existing temporal processing systems to the analysis of causal information, given that werely on well-known standards and data.
On the other hand, this makes it easier for us to straightforwardlyinvestigate the relation between temporal and causal information, given that a causing event should alwaystake place before a resulting event.2 Related WorkResearch on the extraction of event relations has concerned both the analysis of the temporal orderingof events and the recognition of causality relations.
However, the two research lines have progressedquite independently from each other.
Recent works on temporal relations mostly revolve around the lastThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/2097TempEval-31shared task on temporal and event processing.
The task organizers released some data setsannotated with events, time expressions and temporal relations in TimeML format (Pustejovsky et al.,2003), mainly used for training and evaluation purposes.
The results of TempEval-3 reported by UzZamanet al.
(2013) show that, even though the performance of systems for extracting TimeML events andtime expressions is quite good (>80% F-score), the overall performance of end-to-end event extractionpipelines is negatively affected by the poor performance of modules for temporal relation extraction.
Infact, the state-of-the-art performance on the temporal relation extraction task yields only around 36%F-score (Bethard, 2013).The problem of detecting causality between events is as challenging as recognizing their temporalorder, but less analyzed from an NLP perspective.
Besides, it has mostly focused on specific types ofevent pairs and causal expressions in text, and has failed to provide a global account of causal phenomenathat can be captured with NLP techniques.
SemEval-2007 Task 4 ?Classification of Semantic Relationsbetween Nominals?
(Girju et al., 2007) gives access to a corpus containing nominal causal relations amongothers, as causality is one of the considered semantic relations in the task.
Bethard et al.
(2008) collected1,000 conjoined event pairs connected by and from the Wall Street Journal corpus.
The event pairswere annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE,NO-REL).
They use 697 event pairs to train a classification model for causal relations, and use the restfor evaluating the system, which results in 37.4% F-score.
Rink et al.
(2010) perform textual graphclassification using the same corpus, and make use of manually annotated temporal relation types as afeature to build a classification model for causal relations between events.
This results in 57.9% F-score,15% improvement in performance compared with the system without the additional feature of temporalrelations.The interaction between temporal and causal information, and the contribution of temporal informationto the identification of causal links, are also one of the issues investigated in this paper.
However, we aimat providing a more comprehensive account of how causal relations can be explicitly expressed in a text,and we do not limit our analysis to specific connectives.Do et al.
(2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowingthe detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs.
Causalitybetween event pairs is measured by taking into account Point-wise Mutual Information (PMI) betweenthe cause and the effect.
They also incorporate discourse information, specifically the connective typesextracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score.Unfortunately, the data set is not freely available, hence, comparing our work with theirs is not possible.The most recent work of Riaz and Girju (2013) focuses on the identification of causal relations betweenverbal events.
They rely on the unambiguous discourse markers because and but to automatically collecttraining instances of cause and non-cause event pairs, respectively.
The result is a knowledge base ofcausal associations of verbs, which contains three classes of verb pairs: strongly causal, ambiguous andstrongly non-causal.The lack of a standard benchmark to evaluate systems for the extraction of causal relations betweenevents makes it difficult to compare the performance of different systems, and to identify the state-of-the-art approach to this particular task.
For this reason, we annotated TimeBank, a freely available corpus,with the aim of making it available to the research community for further evaluations.3 Data annotationIn order to develop a classifier for the detection of causal relations between events, we first defineannotation guidelines for explicit causality and then manually annotate a data set for training and testing.3.1 Annotation schemeSince one of the goals of this work is to investigate the interaction between temporal and causal information,we define an annotation scheme strongly inspired by the TimeML standard for events, time expressionsand temporal relations.
First, we inherit from TimeML the definition of events, which includes all types1http://www.cs.york.ac.uk/semeval-2013/task1/2098of actions (punctual and durative) and states.
Hence, we do not limit our annotation only to specific PoSsuch as verbal or nominal events.Similar to the <TLINK> tag in TimeML for temporal relations, we introduce the <CLINK> tag tomark a causal relation between two events.
Both TLINKs and CLINKs mark directional relations, i.e.they involve a source and a target event.
However, while a list of relation types is part of the attributes forTLINKs (e.g.
BEFORE, AFTER, INCLUDES, etc.
), for CLINKs only one relation type is foreseen, goingfrom a source (the cause, indicated withSin the examples) to a target (the effect, indicated withT).We also introduce the notion of causal signals through the <C-SIGNAL> tag.
<SIGNAL>s havebeen introduced in TimeML to annotate temporal prepositions and other temporal connectives andsubordinators.
If a SIGNAL marks the presence of a temporal relation in a text, its ID is added to theattributes of such TLINK.
In a similar way, C-SIGNALs are used to mark-up textual elements signallingthe presence of causal relations, which include all causal uses of prepositions (e.g.
because of, as a resultof, due to), conjunctions (e.g.
because, since, so that), adverbial connectors (e.g.
so, therefore, thus)and clause-integrated expressions (e.g.
the reason why, the result is, that is why).
Also for CLINKs itis possible to assign a c-signalID attribute, in case a C-SIGNAL marks the causal relation between twoevents in text.Concerning the notion of causality, it is particularly challenging to provide guidelines that clearlydefine how to identify it in text, since causality exists as a psychological tool for understanding the worldindependently of language and it is not necessarily grounded in text (van de Koot and Neeleman, 2012).There have been several attempts in the psychology field to model causality, including the counterfactualmodel (Lewis, 1973), the probabilistic contrast model (Cheng and Novick, 1991; Cheng and Novick,1992) and the dynamics model (Wolff and Song, 2003; Wolff et al., 2005; Wolff, 2007), which is based onTalmy?s force dynamic account of causality (Talmy, 1985; Talmy, 1988).
We choose to lean our guidelineson the latter model, since it accounts also for different ways in which causal concepts are lexicalized.Specifically, Wolff (2007) claims that causation covers three main types of causal concepts, i.e.
CAUSE,ENABLE and PREVENT.
These causal concepts are lexicalized through three types of verbs listed inWolff and Song (2003): i) CAUSE-type verbs, e.g.
cause, prompt, force; ii) ENABLE-type verbs, e.g.allow, enable, help; and iii) PREVENT-type verbs, e.g.
block, prevent, restrain.
These categories ofcausation and the corresponding verbs are taken into account in our guidelines (Tonelli et al., 2014).We assign a CLINK if, given two annotated events, there is an explicit causal construction linking them.Such construction can be expressed in one of the following ways:1.
Expressions containing affect verbs (affect, influence, determine, change, etc.
), e.g.
Ogun ACNcrisisSinfluences the launchTof the All Progressive Congress.2.
Expressions containing link verbs (link, lead, depend on, etc.
), e.g.
An earthquakeTin NorthAmerica was linked to a tsunamiSin Japan.3.
Basic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type, e.g.
ThepurchaseScaused the creationTof the current building.4.
Periphrastic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type,e.g.
The blastScaused the boat to heelTviolently.
With ?periphrastic?
we mean constructionswhere a causative verb (caused) takes an embedded clause or predicate as a complement expressinga particular result (heel).5.
Expressions containing CSIGNALs, e.g.
Its shipments declinedTas a result of a reductionSininventories by service centers.We annotate both intra- and inter-sentential causal relations between events, provided that one ofthe above constructions is present.
We do not annotate causal relations that are implicit and must beinferred by annotators, because they may be highly ambiguous and would probably affect inter-annotatoragreement.20993.2 Corpus statisticsBased on the guidelines above, we manually annotated causality in the TimeBank corpus taken fromTempEval-3, containing 183 documents with 6,811 annotated events in total.2We chose this corpusbecause gold events were already present, between which we could add causal links.
Besides, one of ourresearch goals is the analysis of the interaction between temporal and causal information, and TimeBankalready presents full manual annotation of temporal information according to TimeML standard.However, during annotation, we noticed that some events involved in causal relations were not annotated,probably because the corpus was originally built focusing on events involved in temporal relations.Therefore, we annotated also 137 new events, which led to around 56% increase in the number ofannotated CLINKs.The total number of annotated CSIGNALs is 171 and there are 318 CLINKs, much less than the numberof TLINKs found in the corpus, which is 5,118.
Besides, not all documents contain causality relationsbetween events.
From the total number of documents in TimeBank, only 109 (around 60%) of themcontain explicit causal links and only 87 (around 47%) of them contain CSIGNALs.
We also found thatthere is no temporal signal (marked by <SIGNAL> tag) annotated in TimeBank, which is unfortunatesince it could help in disambiguating causal signals from temporal signals.Annotation was performed using the CAT tool (Bartalesi Lenzi et al., 2012), a web-based applicationwith a plugin to import annotated data in TimeML and add information on top of it.
The agreement reachedby two annotators on a subset of 5 documents is 0.844 Dice?s coefficient on C-SIGNALs (micro-averageover markables) and of 0.73 on CLINKs.
The built corpus is then used as training and test data in theexperiments for the classification of CSIGNALs and CLINKs, as described in Section 4.
This preliminaryanalysis on the corpus, however, shows that explicit causal relations between events are less frequentlyfound in texts than temporal ones.
This may lead to data sparseness problems.4 ExperimentsUsing the 183 documents from TimeBank manually enriched with causal information for training andtesting, we implement two different classifiers: the first one is a CSIGNAL labeler, that takes in inputinformation on events and temporal expressions as annotated in the original TimeBank, and classifieswhether a token is part of a causal signal or not (Section 4.1).
The second one is a CLINK classifier,which given an event pair detects whether they are connected by an explicit causal link (Section 4.2).
Bothexperiments are carried out based on five-fold cross-validation.
The overall approach is largely inspiredby our existing framework for the classification of temporal relations (Mirza and Tonelli, 2014).4.1 Automatic Extraction of CSIGNALsThe task of recognizing CSIGNALs can be seen as a text chunking task, i.e.
using a classifier to determinewhether a token is part of a causal signal or not.
Since the extent of causal signals can be expressed bymulti-word expressions, we employ the IOB tagging convention to annotate the data, where each tokencan either be classified into B-CSIGNAL, I-CSIGNAL or O (for other).
We build our classification modelusing the Support Vector Machine (SVM) implementation provided by YamCha3, a generic, customizable,and open source text chunker.
In order to provide the classifier a feature vector to learn from, we performthe two following steps:1.
Run the TextPro tool (Pianta et al., 2008) to get information on base NP chunking and whether atoken is part of named entity or not.2.
Run Stanford CoreNLP tool4to get information on lemma, part-of-speech (PoS) tags and dependencyrelations between tokens.In the end, the feature vector includes token, lemma, PoS tag, NP chunking, dependency path, andseveral binary features, indicating whether a token is: i) an event or part of a temporal expression,2The annotated data set is available at http://hlt.fbk.eu/technologies/causal-timebank3http://chasen.org/?taku/software/yamcha/4http://nlp.stanford.edu/software/corenlp.shtml2100according to gold TimeML annotation; ii) part of a named entity or not; and iii) part of a specific discourseconnective type.Dependency information is encoded as the dependency path between the current token and its governor.For example, in ?He fell because the chair was broken?, there is a dependency relation mark (broken,because), where mark indicates the presence of a finite clause subordinate to another clause (de Marneffeand Manning, 2008).
Thus, we encode the dependency feature for the token because as mark (broken).
Ifthe governor is an event, e.g.
broken is annotated as an event, the dependency feature is represented asmark (EVENT) instead.The mentioned binary features are introduced to exclude the corresponding token as a candidate tokenfor a causal signal.
In other words, if a token is part of a named entity or an event, it is very unlikely that itwill be part of a causal signal.
The same holds for all connective types that do not express causal relations,e.g.
temporal or concessive ones.
In order to obtain this information, we include in the feature vector theinformation about discourse connectives acquired using the addDiscourse tool (Pitler and Nenkova, 2009),which identifies connectives and assigns them to one of four semantic classes in the framework of thePenn Discourse Treebank (The PDTB Research Group, 2008): TEMPORAL, EXPANSION, CONTINGENCYand COMPARISON.
Note that causality is part of the CONTINGENCY class.System Precision Recall F-scoreRule-based (baseline) 54.33% 40.35% 46.31%Supervised chunking 91.03% 41.76% 57.26%Table 1: Evaluation of CSIGNAL extraction systemTable 1 shows the performance of our classification model in a five-fold cross-validation setting, whichyields a good precision but a poorer recall, summing up into 57.26% F-score.
We also compare oursupervised model with a baseline rule-based system, which labels as CSIGNALs all causal connectorslisted in our annotation guidelines and those appearing in specific syntactic constructions.
For instance,from and by are always labeled as CSIGNAL when they are governed by a passive verb annotated asevent and govern another event, as in the sentence ?The building was damagedTby the earthquakeS.
?Note that this is quite a strong baseline, since the rule-based algorithm embeds some of the intuitions onsyntactic dependencies expressed also as features in the supervised approach.4.2 Automatic Extraction of CLINKsSimilar to causal signal extraction, we approach the problem of detecting causal links between events as asupervised classification task.
Given an ordered pair or events (e1,e2), the classifier has to decide whetherthere is a causal relation between them or not.
However, since we also consider the directionality of thecausal link, an event pair (e1,e2) is classified into 3 classes: CLINK (where e1is the source and e2is thetarget), CLINK-R (with the reverse order or source and target) or NO-REL.
Again, we use YamCha to buildthe classifier.
This time, a feature vector is built for each pair of events and not for each token as in theprevious classification task.As candidate event pairs, we take into account every possible combination of events in a sentencein a forward manner.
For example, if we have e1, e2and e3in a sentence (in this order), the candidateevent pairs are (e1,e2), (e1,e3) and (e2,e3).
We also include as candidate event pairs the combinationof each event in a sentence with events in the following one.
This is necessary to account for inter-sentential causality, under the simplifying assumption that causality may occur only between events intwo consecutive sentences.We implement a number of features, some of which are computed independently based on either e1ore2, e.g.
lemma, PoS, while some others are pairwise features, which are computed based on both elements,e.g.
dependency path, signals in between, etc.
The implemented features are as follows:String and grammatical features.
The tokens and lemmas of e1and e2, along with their PoS and abinary feature indicating whether e1and e2have the same PoS tags.Textual context.
The sentence distance and event distance of e1and e2.
Sentence distance measures2101how far e1and e2are from each other in terms of sentences, i.e.
0 if they are in the same sentence.
Theevent distance corresponds to the number of events occurring between e1and e2(i.e.
if they are adjacent,the distance is 0).Event attributes.
Event attributes as specified in TimeML annotation, which consist of class, tense,aspect and polarity.
Events being a noun, adjective and preposition do not have tense and aspect attributesin TimeML.
Therefore, we retrieve this information by extracting the tense and aspect of the verbs thatgovern them, based on their dependency relation.
We also include four binary features representingwhether e1and e2have the same event attributes or not.
These features, especially the tense and aspectone, are very relevant for detecting causality.
For instance, if e1is in the future tense and e2in the pasttense, there cannot be a causal relation connecting e1(as source) and e2(as target or result).Dependency information.
We include as features i) the dependency path that exists between e1ande2, ii) the type of causative verb connecting them (if any) and iii) binary features indicating whethere1/e2is the root of the sentence.
This information is based on the collapsed representation of dependencyrelations provided by the parsing module of Stanford CoreNLP.
Consider the sentence ?Profit from coalfellTto $41 million from $58 million, partly because of a miners?
strikeS.?
Based on the collapsedtyped dependencies, we would obtain a direct relation between fell and strike, which is prep because of(fell, strike).
This information combined with the classification of because of as a causal signal wouldstraightforwardly identify the relation connecting the two events as causal.Causal signals.
We take into account the annotated CSIGNALs connecting two candidate events.
Welook for causal signals occurring between e1and e2, or before e1.
We also include the position of thesignals (between or before) as feature, since it is crucial to determine the direction of the causality ofa given ordered event pair.
This is particularly evident if you consider the position of causal signalsin the following examples: i) ?The building collapsedTbecause of the earthquakeS?
vs. ii) ?Becauseof the earthquakeSthe building collapsedT.?
This feature is also very relevant in connection with theTextual context, since two events being in two different sentences are linked by an explicit causal relationonly in specific cases, for instance if there is a CSIGNAL in between, typically at the beginning of thesecond sentence.
Note that in case of several CSIGNALs occurring between e1and e2, we take theclosest CSIGNAL to e2, as in the sentence ?The building was damagedSby the earthquake,thus, peoplemovedTaway?.
The dependency path between the causal signal and e1/e2is also important to determinethe correct involved events in the causal relations.
For instance, in the sentence ?They decidedTto movebecause of the earthquakeS?, the involved event is decided instead of move.Temporal relations (TLINKs).
Rink et al.
(2010) showed that including temporal relation informationin detecting causal links results in improving classification performance.
Nevertheless, they only analyzethis phenomenon when causality is expressed by the conjunction and.
We decided to include thisinformation in the feature set by specifying the temporal relation type connecting e1and e2, if any, to seewhether TLINKs help in improving causality detection also in a more comprehensive setting.We evaluate our approach in a five-fold cross-validation setting, and we compare the performance ofour classifier with a baseline rule-based system.
This relies on an algorithm that, given a term t belongingto affect, link, causative verbs (basic and periphrastic constructions) or causal signals (as listed in theannotation guidelines), looks for specific dependency constructions where t is connected to two events.
Ifsuch dependencies are found, a CLINK is automatically set between the two events identifying the sourceand the target of the relation.
Further details on the baseline system and its evaluation can be found inMirza et al.
(2014).In our experimental setting, we evaluate two versions of the CLINK classifier: the first includes asfeatures the gold annotated CSIGNALs in the classification model, while the second takes in inputthe CSIGNALs automatically annotated by the classifier described in Section 4.1.
We also evaluatethe contribution of dependency, CSIGNAL and TLINK features by excluding each of them from theclassification model.Evaluation results are reported in Table 2.
We observe that the baseline is always outperformed by theother classifiers.
CSIGNAL is the most important feature, with a particularly high impact on recall.
The2102intuition behind this result is that, if a CSIGNAL is present, it is a strong indicator of a causal relation beingpresent in the surrounding context.
This is similar to what Derczynski and Gaizauskas (2012) report fortemporal information, showing that temporal signals provide useful information in TLINK classification.Dependency information contributes to the performance of the classifier, but is less relevant than TLINKinformation.
A more detailed analysis of the relation between temporal and causal information is reportedin the following section.
The significantly decreasing recall of the classifier using the automatic extractedCSIGNALs as features is most probably caused by the low recall of the CSIGNAL extraction system.System Precision Recall F-scoreRule-based (baseline) 36.79% 12.26% 18.40%Supervised classification (with gold CSIGNALs) 74.67% 35.22% 47.86%- without dependency feature 65.77% 30.82% 41.97%- without CSIGNAL feature 57.53% 13.21% 21.48%- without TLINK feature 61.59% 29.25% 39.66%Supervised classification (with automatic CSIGNALs) 67.29% 22.64% 33.88%Table 2: Performance of CLINK extraction system5 DiscussionWe further analyse the output of the automatic extraction systems, in order to understand some phenomenatriggering the results.5.1 Recognizing CSIGNALsWhen we manually inspect the output of the CSIGNAL extraction system, we find that the false positivesare actually the causal signals that annotators missed in the corpus, and not ambiguous connectives.
Thesystem surprisingly yields better precision than human annotation, finding new correct signals.The recall, however, suffers most probably from data sparseness.
It is possible that during the cross-validation experiments some splits do not have enough data to learn from, recalling that only around 47%of the documents contain annotated CSIGNALs.
Furthermore, 20% of the false negative cases are due toclassifier?s mistakes in detecting the causal signal by, which is highly ambiguous.
Our assumption withthe rule-based system that ?by is likely to be a causal signal when it is used to modify a passive verb?
istoo restrictive, since by can convey a causal meaning even if the target event is not in the passive voice, asin the example ?The embargo is meant to crippleTIraq by cuttingSoff its exports of oil and imports offood and military supplies.
?Another ambiguous causal signal that the classifier fails to detect is the conjunction and.
We believethat more training data, and perhaps more lexical information on the tokens connected by the conjunctionand, are needed for the classifier to be able to disambiguate them.5.2 Detecting CLINKsWe found that most of the mistakes done by the classifier, as well as by the rule-based system, are causedby the dependency parser output that tends to establish a dependency relation between a causative verb orcausal signal and the closest verb.
For example, in the sentence ?StatesWest Airlines withdrewTits offerto acquire Mesa Airlines because the Farmington carrier did not respondSto its offer?, the dependencyparser identify because as the mark of acquire instead of withdrew.Moreover, also for this task data sparseness is definitely an issue.
One possible solution would be toannotate more data, for instance the AQUAINT data set used for TempEval-3 competition (UzZaman etal., 2013).
Another possibility would be to automatically generate additional data from the Penn DiscourseTreeBank corpus, where causality is one of the discourse relations annotated between argument pairs.However, a further processing step would be needed to identify inside the argument spans the eventsbetween which a relation holds, which may introduce some errors.2103Regarding the directionality of causal relations, the classifier is generally quite precise.
112 out of 150CLINKs detected by the classifier actually match a causal relation present in the gold annotated data.Only 8 of them have been classified with the wrong direction.
We believe that using the TLINK types asfeatures contributes to this good performance in disambiguating causality direction (CLINK vs. CLINK-R).5.3 Interaction between temporal and causal informationWe provide in Table 3 some statistics on the overlaps between causal links and temporal relation typesfrom the gold data.
The Others class in the table includes SIMULTANEOUS, IS INCLUDED, BEGUN BYand DURING INV relations.
These counts were obtained by overlapping the temporal information inTimeBank with the causal information manually added for our experiments.
In total, only 32% of the goldcausal links have the underlying temporal relations.
Note that the annotators could not see the temporallinks already present in the data, therefore they were not biased by TLINKs when assessing causal links.BEFORE AFTER IBEFORE IAFTER Others TotalCLINK 15 5 0 0 4 24CLINK-R 1 67 0 3 8 79Table 3: Statistics of CLINKs overlapping with TLINKsThe data confirm our intuition that temporal information is a strong constraint when detecting causalrelations, with the BEFORE class having the most overlaps with CLINK and AFTER with CLINK-R. Thisis in line with the outcome of our feature analysis reported in Table 2, suggesting that feeding temporalinformation into a causal relation classifier yields an improvement in performance.
However, the conversewould be less effective, since the occurrences of explicit causal relations are by far less frequent thantemporal ones.
Besides, we found that the few cases where CLINKs overlap with AFTER relation are notdue to annotation mistakes, as in the example ?But some analysts questionedThow much of an impactthe retirement package will have, because few jobs will endSup being eliminated.
?Finally, the performance achieved by our system in causal relation extraction (with gold C-SIGNALs)is 47.86% F-score, which is better than the performance of the state-of-the-art temporal relation extractionsystem with 36.26% (Bethard, 2013).
This probably depends on the fact that extracting CLINKs is asimpler task compared with TLINK extraction: in the first case 3 classes are considered, while temporalrelation types are classified into 14 classes.6 ConclusionsIn this paper, we presented a framework for annotating causal signals and causal relations between events.Besides, we implemented and evaluated two supervised systems, one classifying C-SIGNALs and theother CLINKs.With the first task, we showed that while recognizing unambiguous causal signals is very trivial,ambiguous signals such as by and and are very difficult to identify because they occur in diverse syntacticconstructions.
We definitely need more data to learn from, and perhaps use more lexical informationon the words connected by such causal signals as features.
The knowledge base of causal associationsbetween verbs developed by Riaz and Girju (2013) may be a useful resource to provide such information,and we will explore this possibility in the future.We found that the low recall achieved by the CLINK classifier is probably affected by wrong dependen-cies identified by the Stanford parser.
In the future, we would like to test also the C&C tool (Curran et al.,2007) to extract dependency relations, since it has a better coverage of long-range dependencies.
We havealso shown that causal signals are very important in detecting explicit causal links holding between twoevents.
Finally, we showed that temporal relation types help in disambiguating the direction of causality,i.e.
to determine the source and target event.
However, the converse may not hold, since the causal linksin the data set are very sparse, and only 2% of the total TLINKs overlap with CLINKs.2104AcknowledgementsThe research leading to this paper was partially supported by the European Union?s 7th FrameworkProgramme via the NewsReader Project (ICT-316404).
We also thank Rachele Sprugnoli and ManuelaSperanza for their contribution in defining the annotation guidelines.ReferencesValentina Bartalesi Lenzi, Giovanni Moretti, and Rachele Sprugnoli.
2012.
CAT: the CELCT Annotation Tool.
InProceedings of LREC 2012.Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin.
2008.
Building a Corpus of Temporal-Causal Structure.
In European Language Resources Association (ELRA), editor, Proceedings of the Sixth Inter-national Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may.Steven Bethard.
2013.
ClearTK-TimeML: A minimalist approach to TempEval 2013.
In Proceedings of the Sev-enth International Workshop on Semantic Evaluation, SemEval ?13, Atlanta, Georgia, USA, June.
Associationfor Computational Linguistics.Claire Bonial, Olga Babko-Malaya, Jinho D. Choi, Jena Hwang, and Martha Palmer.
2010.
PropBank An-notation Guidelines, Version 3.0.
Technical report, Center for Computational Language and Education Re-search, Institute of Cognitive Science, University of Colorado at Boulder.
http://clear.colorado.edu/compsem/documents/propbank_guidelines.pdf.Patricia W. Cheng and Laura R. Novick.
1991.
Causes versus enabling conditions.
Cognition, 40(1-2):83 ?
120.Patricia W. Cheng and Laura R. Novick.
1992.
Covariation in natural causal induction.
Psychological Review,99(2):365?382.James Curran, Stephen Clark, and Johan Bos.
2007.
Linguistically Motivated Large-Scale NLP with C&C andBoxer.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Sessions, pages 33?36, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Marie-Catherine de Marneffe and Christopher D. Manning.
2008.
The Stanford typed dependencies representation.In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages1?8.
Association for Computational Linguistics.Leon Derczynski and Robert J. Gaizauskas.
2012.
Using Signals to Improve Automatic Classification of TemporalRelations.
CoRR, abs/1203.5055.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.
Minimally Supervised Event Causality Identification.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294?303,Stroudsburg, PA, USA.
Association for Computational Linguistics.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret.
2007.
SemEval-2007 Task 04: Classification of Semantic Relations between Nominals.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), pages 13?18, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.David Lewis.
1973.
Causation.
The Journal of Philosophy, 70(17):pp.
556?567.Paramita Mirza and Sara Tonelli.
2014.
Classifying Temporal Relations with Simple Features.
In Proceedings ofthe 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308?317,Gothenburg, Sweden, April.
Association for Computational Linguistics.Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza.
2014.
Annotating Causality in theTempEval-3 Corpus.
In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causalityin Language (CAtoCL), pages 10?19, Gothenburg, Sweden, April.
Association for Computational Linguistics.Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008.
The TextPro Tool Suite.
In Proceedings ofthe 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco.European Language Resources Association.2105Emily Pitler and Ani Nenkova.
2009.
Using syntax to disambiguate explicit discourse connectives in text.
InProceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ?09, pages 13?16, Stroudsburg, PA,USA.
Association for Computational Linguistics.James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser Saur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
TimeML: Robust specification of event and temporal expressions in text.
In Proceedings of theFifth International Workshop on Computational Semantics (IWCS-5).James Pustejovsky, Jessica Littman, Roser Saur?
?, and Marc Verhagen.
2006.
Timebank 1.2 documentation.
Tech-nical report, Brandeis University, April.James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent Romary.
2010.
ISO-TimeML: An international stan-dard for semantic annotation.
In Proceedings o the Fifth International Workshop on Interoperable SemanticAnnotation.Mehwish Riaz and Roxana Girju.
2013.
Toward a better understanding of causality between verbal events: Ex-traction and analysis of the causal power of verb-verb associations.
In Proceedings of the SIGDIAL 2013Conference, pages 21?30, Metz, France, August.
Association for Computational Linguistics.Bryan Rink, Cosmin Adrian Bejan, and Sanda M. Harabagiu.
2010.
Learning Textual Graph Patterns to DetectCausal Event Relations.
In Proceedings of the Twenty-Third International FLAIRS Conference.Leonard Talmy.
1985.
Force dynamics in language and thought.
Chicago Linguistic Society, 21:293?337.Leonard Talmy.
1988.
Force dynamics in language and cognition.
Cognitive science, 12(1):49?100.The PDTB Research Group.
2008.
The PDTB 2.0.
Annotation Manual.
Technical Report IRCS-08-01, Institutefor Research in Cognitive Science, University of Pennsylvania.Sara Tonelli, Rachele Sprugnoli, and Manuela Speranza.
2014.
Newsreader guidelines for annotationat document level.
Technical Report NWR-2014-2, Fondazione Bruno Kessler.
http://www.newsreader-project.eu/files/2013/01/NWR-2014-2.pdf.Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky.
2013.Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.
In Proceedingsof the Seventh International Workshop on Semantic Evaluation, SemEval ?13, pages 1?9, Atlanta, Georgia,USA, June.
Association for Computational Linguistics.H.
van de Koot and A. Neeleman, 2012.
The Theta System: Argument Structure at the Interface, chapter TheLinguistic Expression of Causation, pages 20 ?
51.
Oxford University Press: Oxford.Phillip Wolff and Grace Song.
2003.
Models of causation and the semantics of causal verbs.
Cognitive Psychology,47(3):276?332.Phillip Wolff, Bianca Klettke, Tatyana Ventura, and Grace Song.
2005.
Expressing causation in english and otherlanguages.
Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin, pages29?48.Phillip Wolff.
2007.
Representing causation.
Journal of experimental psychology: General, 136(1):82.2106
