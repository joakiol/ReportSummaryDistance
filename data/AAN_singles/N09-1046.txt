Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 406?414,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing a maximum entropy model to build segmentation lattices for MTChris DyerLaboratory for Computational Linguistics and Information ProcessingDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742, USAredpony AT umd.eduAbstractRecent work has shown that translating seg-mentation lattices (lattices that encode alterna-tive ways of breaking the input to an MT sys-tem into words), rather than text in any partic-ular segmentation, improves translation qual-ity of languages whose orthography does notmark morpheme boundaries.
However, muchof this work has relied on multiple segmentersthat perform differently on the same input togenerate sufficiently diverse source segmen-tation lattices.
In this work, we describe amaximum entropy model of compound wordsplitting that relies on a few general featuresthat can be used to generate segmentation lat-tices for most languages with productive com-pounding.
Using a model optimized for Ger-man translation, we present results showingsignificant improvements in translation qual-ity in German-English, Hungarian-English,and Turkish-English translation over state-of-the-art baselines.1 IntroductionCompound words pose significant challenges to thelexicalized models that are currently common in sta-tistical machine translation.
This problem has beenwidely acknowledged, and the conventional solu-tion, which has been shown to work well for manylanguage pairs, is to segment compounds into theirconstituent morphemes using either morphologicalanalyzers or empirical methods and then to trans-late from or to this segmented variant (Koehn et al,2008; Dyer et al, 2008; Yang and Kirchhoff, 2006).But into what units should a compound word besegmented?
Taken as a stand-alone task, the goal ofa compound splitter is to produce a segmentation forsome input that matches the linguistic intuitions of anative speaker of the language.
However, there areoften advantages to using elements larger than sin-gle morphemes as the minimal lexical unit for MT,since they may correspond more closely to the unitsof translation.
Unfortunately, determining the op-timal segmentation is challenging, typically requir-ing extensive experimentation (Koehn and Knight,2003; Habash and Sadat, 2006; Chang et al, 2008).Recent work has shown that by combining a vari-ety of segmentations of the input into a segmentationlattice and effectively marginalizing over many dif-ferent segmentations, translations superior to thoseresulting from any single single segmentation of theinput can be obtained (Xu et al, 2005; Dyer et al,2008; DeNeefe et al, 2008).
Unfortunately, this ap-proach is difficult to utilize because it requires mul-tiple segmenters that behave differently on the sameinput.In this paper, we describe a maximum entropyword segmentation model that is trained to assignhigh probability to possibly several segmentations ofan input word.
This model enables generation of di-verse, accurate segmentation lattices from a singlemodel that are appropriate for use in decoders thataccept word lattices as input, such as Moses (Koehnet al, 2007).
Since our model relies a small num-ber of dense features, its parameters can be tunedusing very small amounts of manually created ref-erence lattices.
Furthermore, since these parame-ters were chosen to have valid interpretation acrossa variety of languages, we find that the weights esti-mated for one apply quite well to another.
We showthat these lattices significantly improve translationquality when translating into English from three lan-guages exhibiting productive compounding: Ger-man, Turkish, and Hungarian.The paper is structured as follows.
In the next sec-406tion, we describe translation from segmentation lat-tices and give a motivating example, Section 3 de-scribes our segmentation model and its tuning andhow it is used to generate segmentation lattices, Sec-tion 5 presents experimental results, Section 6 re-views relevant related work, and in Section 7 weconclude and discuss future work.2 Segmentation lattice translationIn this section we give a brief overview of latticetranslation and then describe the characteristics ofsegmentation lattices that are appropriate for trans-lation.2.1 Lattice translationWord lattices have been used to represent ambiguousinput to machine translation systems for a variety oftasks, including translating automatic speech recog-nition transcriptions and translating from morpho-logically complex languages (Bertoldi et al, 2007;Dyer et al, 2008).
The intuition behind using lat-tices in both approaches is to avoid the error propa-gation effects that are found when a one-best guessis used.
By carrying a certain amount of uncertaintyforward in the processing pipeline, information con-tained in the translation models can be leveraged tohelp resolve the upstream ambiguity.
In our case, wewant to propagate uncertainty about the proper seg-mentation of a compound forward to the decoder,which can use its full translation model to selectproper segmentation for translation.
Mathemati-cally, this can be understood as follows: whereas thegoal in conventional machine translation is to findthe sentence e?I1 that maximizes Pr(eI1|fJ1 ), the lat-tice adds a latent variable, the path f?
from a des-ignated start start to a designated goal state in thelattice G:e?I1 = arg maxeI1Pr(eI1|G) (1)= arg maxeI1?f??GPr(eI1|f?)Pr(f?
|G) (2)?
arg maxeI1maxf??GPr(eI1|f?)Pr(f?
|G) (3)If the transduction formalism used is a synchronousprobabilistic context free grammar or weighted finitetonband aufnahmetonbandaufnahmetonbandaufnahmewieder aufnahmewiederaufnahmewiederaufnahmeFigure 1: Segmentation lattice examples.
The dottedstructure indicates linguistically implausible segmenta-tion that might be generated using dictionary-driven ap-proaches.state transducer, the search represented by equation(3) can be carried out efficiently using dynamic pro-gramming (Dyer et al, 2008).2.2 Segmentation latticesFigure 1 shows two lattices that encode themost linguistically plausible ways of segment-ing two prototypical German compounds withcompositional meanings.
However, while thesewords are structurally quite similar, translatingthem into English would seem to require differ-ent amounts of segmentation.
For example, thedictionary fragment shown in Table 1 illustratesthat tonbandaufnahme can be rendered into En-glish by following 3 different paths in the lat-tice, ton/audio band/tape aufnahme/recording, ton-band/tape aufnahme/recording, and tonbandauf-nahme/tape recording.
In contrast, wiederaufnahmecan only be translated correctly using the unseg-mented form, even though in German the meaningof the full form is a composition of the meaning ofthe individual morphemes.1It should be noted that phrase-based models cantranslate multiple words as a unit, and therefore cap-ture non-compositional meaning.
Thus, by default ifthe training data is processed such that, for example,aufnahme, in its sense of recording, is segmentedinto two words, then more paths in the lattices be-1The English word resumption is likewise composed of twomorphemes, the prefix re- and a kind of bound morphemethat never appears in other contexts (sometimes called a ?cran-berry?
morpheme), but the meaning of the whole is idiosyncraticenough that it cannot be called compositional.407German Englishauf on, up, in, at, ...aufnahme recording, entryband reel, tape, bandder the, of thenahme took (3P-SG-PST)ton sound, audio, claytonband tape, audio tapetonbandaufnahme tape recordingwie how, like, aswieder againwiederaufnahme resumptionTable 1: German-English dictionary fragment for wordspresent in Figure 1.come plausible translations.
However, using a strat-egy of ?over segmentation?
and relying on phrasemodels to learn the non-compositional translationshas been shown to degrade translation quality sig-nificantly on several tasks (Xu et al, 2004; Habashand Sadat, 2006).
We thus desire lattices containingas little oversegmentation as possible.We have now have a concept of a ?gold standard?segmentation lattice for translation: it should con-tain all linguistically motivated segmentations thatalso correspond to plausible word-for-word transla-tions into English.
Figure 2 shows an example of thereference lattice for the two words we just discussed.For the experiments in this paper, we generated adevelopment and test set by randomly choosing 19German newspaper articles, identifying all wordsgreater than 6 characters is length, and segmentingeach word so that the resulting units could be trans-lated compositionally into English.
This resulted in489 training sentences corresponding to 564 pathsfor the dev set (which was drawn from 15 articles),and 279 words (302 paths) for the test set (drawnfrom the remaining 4 articles).3 A maximum entropy segmentationmodelWe now turn to the problem of modeling word seg-mentation in a way that facilitates lattice construc-tion.
As a starting point, we consider the workof Koehn and Knight (2003) who observe that inmost languages that exhibit compounding, the mor-tonband aufnahmetonbandwiederaufnahmeFigure 2: Manually created reference lattices for the twowords from Figure 1.
Although only a subset of alllinguistically plausible segmentations, each path corre-sponds to a plausible segmentation for word-for-wordGerman-English translation.phemes used to construct compounds frequentlyalso appear as individual tokens.
Based on this ob-servation, they propose a model of word segmenta-tion that splits compound words into pieces foundin the dictionary based on a variety heuristic scoringcriteria.
While these models have been reasonablysuccessful (Koehn et al, 2008), they are problem-atic for two reasons.
First, there is no principled wayto incorporate additional features (such as phonotac-tics) which might be useful to determining whethera word break should occur.
Second, the heuristicscoring offers little insight into which segmentationsshould be included in a lattice.We would like our model to consider a wide vari-ety of segmentations of any word (including perhapshypothesized morphemes that are not in the dictio-nary), to make use of a rich set of features, and tohave a probabilistic interpretation of each hypothe-sized split (to incorporate into the downstream de-coder).
We decided to use the class of maximumentropy models, which are probabilistically sound,can make use of possibly many overlapping features,and can be trained efficiently (Berger et al, 1996).We thus define a model of the conditional proba-bility distribution Pr(sN1 |w), where w is a surfaceform and sN1 is the segmented form consisting of Nsegments as:Pr(sN1 |w) = exp?i ?ihi(sN1 , w)?s?
exp?i ?ihi(s?, w)(4)To simplify inference and to make the lattice repre-sentation more natural, we only make use of localfeature functions that depend on properties of eachsegment:408Pr(sN1 |w) ?
exp?i?iN?jhi(sj , w) (5)3.1 From model to segmentation latticeThe segmentation model just introduced is equiva-lent to a lattice where each vertex corresponds toa particular coverage (in terms of letters consumedfrom left to right) of the input word.
Since we onlymake use of local features, the number of verticesin a lattice for word w is |w| ?
m, where m is theminimum segment length permitted.
In all experi-ments reported in this paper, we use m = 3.
Eachedge is labeled with a morpheme s (correspondingto the morpheme associated with characters delim-ited by the start and end nodes of the edge) as wellas a weight, ?i ?ihi(s, w).
The cost of any pathfrom the start to the goal vertex will be equal to thenumerator in equation (4).
The value of the denomi-nator can be computed using the forward algorithm.In most of our experiments, s will be identicalto the substring of w that the edge is designated tocover.
However, this is not a requirement.
For exam-ple, German compounds frequently have so-calledFugenelemente, one or two characters that ?gluetogether?
the primary morphemes in a compound.Since we permit these characters to be deleted, thenan edge where they are deleted will have fewer char-acters than the coverage indicated by the edge?sstarting and ending vertices.3.2 Lattice pruningExcept for the minimum segment length restriction,our model defines probabilities for all segmentationsof an input word, making the resulting segmenta-tion lattices are quite large.
Since large latticesare costly to deal with during translation (and maylead to worse translations because poor segmenta-tions are passed to the decoder), we prune them us-ing forward-backward pruning so as to contain justthe highest probability paths (Sixtus and Ortmanns,1999).
This works by computing the score of thebest path passing through every edge in the latticeusing the forward-backward algorithm.
By findingthe best score overall, we can then prune edges us-ing a threshold criterion; i.e., edges whose score issome factor ?
away from the global best edge score.3.3 Maximum likelihood trainingOur model defines a conditional probability distribu-tion over virtually all segmentations of a word w. Totrain our model, we wish to maximize the likelihoodof the segmentations contained in the reference lat-tices by moving probability mass away from the seg-mentations that are not in the reference lattice.
Thus,we wish to minimize the following objective (whichcan be computed using the forward algorithm overthe unpruned hypothesis lattices):L = ?
log?i?s?Rip(s|wi) (6)The gradient with respect to the feature weights fora log linear model is simply:?L?
?k= ?iEp(s|wi)[hk]?
Ep(s|wi,Ri)[hk] (7)To compute these values, the first expectation iscomputed using forward-backward inference overthe full lattice.
To compute the second expecta-tion, the full lattice is intersected with the referencelattice Ri, and then forward-backward inferenceis redone.2 We use the standard quasi-Newtonianmethod L-BFGS to optimize the model (Liu et al,1989).
Training generally converged in only a fewhundred iterations.3.3.1 Training to minimize 1-best errorIn some cases, such as when performing wordalignment for translation model construction, lat-tices cannot be used easily.
In these cases, a 1-best segmentation (which can be determined fromthe lattice using the Viterbi algorithm) may be de-sired.
To train the parameters of the model for thiscondition (which is arguably slightly different fromthe lattice generation case we just considered), weused the minimum error training (MERT) algorithmon the segmentation lattices to find the parametersthat minimized the error on our dev set (Macherey2The second expectation corresponds to the empirical fea-ture observations in a standard maximum entropy model.
Be-cause this is an expectation and not an invariant observation,the log likelihood function is not guaranteed to be concave andthe objective surface may have local minima.
However, exper-imentation revealed the optimization performance was largelyinvariant with respect to its starting point.409et al, 2008).
The error function we used was WER(the minimum number of insertions, substitutions,and deletions along any path in the reference lattice,normalized by the length of this path).
The WER onthe held-out test set for a system tuned using MERTis 9.9%, compared to 11.1% for maximum likeli-hood training.3.4 FeaturesWe remark that since we did not have the resourcesto generate training data in all the languages wewished to generate segmentation lattices for, wehave confined ourselves to features that we expect tobe reasonably informative for a broad class of lan-guages.
A secondary advantage of this is that weused denser features than are often used in maxi-mum entropy modeling, meaning that we could trainour model with relatively less training data thanmight otherwise be required.The features we used in our compound segmen-tation model for the experiments reported below areshown in Table 2.
Building on the prior work thatrelied heavily on the frequency of the hypothesizedconstituent morphemes in a monolingual corpus, weincluded features that depend on this value, f(si).|si| refers to the number of letters in the ith hypothe-sized segment.
Binary predicates evaluate to 1 whentrue and 0 otherwise.
f(si) is the frequency of thetoken si as an independent word in a monolingualcorpus.
p(#|si1 ?
?
?
si4) is the probability of a wordstart preceding the letters si1 ?
?
?
si4.
We found itbeneficial to include a feature that was the probabil-ity of a certain string of characters beginning a word,for which we used a reverse 5-gram character modeland predicted the word boundary given the first fiveletters of the hypothesized word split.3 Since we didhave expertise in German morphology, we did builda special German model.
For this, we permitted thestrings s, n, and es to be deleted between words.Each deletion fired a count feature (listed as fugenin the table).
Analysis of errors indicated that thesegmenter would periodically propose an incorrectsegmentation where a single word could be dividedinto a word and a nonword consisting of common in-3In general, this helped avoid situations where a word maybe segemented into a frequent word and then a non-word stringof characters since the non-word typically violated the phono-tactics of the language in some way.Feature de-only neutral?si ?
N -3.55 ?f(si) > 0.005 -3.13 -3.31f(si) > 0 3.06 3.64log p(#|si1si2si3si4) -1.58 -2.11segment penalty 1.18 2.04|si| ?
12 -0.9 -0.79oov -0.88 -1.09?fugen -0.76 ?|si| ?
4 -0.66 -1.18|si| ?
10, f(si) > 2?10 -0.51 -0.82log f(si) -0.32 -0.362?10 < f(si) < 0.005 -0.26 -0.45Table 2: Features and weights learned by maximum like-lihood training, sorted by weight magnitude.flectional suffixes.
To address this, an additional fea-ture was added that fired when a proposed segmentwas one of a setN of 30 nonwords that we saw quitefrequently.
The weights shown in Table 2 are thoselearned by maximum likelihood training on modelsboth with and without the special German features,which are indicated with ?.4 Model evalatuionTo give some sense of the performance of the modelin terms of its ability to generate lattices indepen-dently of a translation task, we present precision andrecall of segmentations for pruning parameters (cf.Section 3.2) ranging from ?
= 0 to ?
= 5.
Pre-cision measures the number of paths in the hypoth-esized lattice that correspond to paths in the refer-ence lattice; recall measures the number of paths inthe reference lattices that are found in the hypothesislattice.
Figure 3 shows the effect of manipulating thedensity parameter on the precision and recall of theGerman lattices.
Note that very high recall is possi-ble; however, the German-only features have a sig-nificant impact, especially on recall, because the ref-erence lattices include paths where Fugenelementehave been deleted.5 Translation experimentsWe now review experiments using segmentation lat-tices produced by the segmentation model we justintroduced in German-English, Hungarian-English,4100.920.930.940.950.960.970.980.9910.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1RecallPrecisionMLMERTML, no special GermanFigure 3: The effect of the lattice density parameter onprecision and recall.and Turkish-English translation tasks and then showresults elucidating the effect of the lattice density pa-rameter.
We begin with a description of our MT sys-tem.5.1 Data preparation and system descriptionFor all experiments, we used a 5-gram English lan-guage model trained on the AFP and Xinua por-tions of the Gigaword v3 corpus (Graff et al, 2007)with modified Kneser-Ney smoothing (Kneser andNey, 1995).
The training, development, and testdata for German-English and Hungarian-Englishsystems used were distributed as part of the 2009EACL Workshop on Machine Translation,4 and theTurkish-English data corresponds to the training andtest sets used in the work of Oflazer and Durgar El-Kahlout (2007).
Corpus statistics for all languagepairs are summarized in Table 3.
We note that in alllanguage pairs, the 1BEST segmentation variant ofthe training data results in a significant reduction intypes.Word alignment was carried out by runningGiza++ implementation of IBM Model 4 initializedwith 5 iterations of Model 1, 5 of the HMM aligner,and 3 iterations of Model 4 (Och and Ney, 2003)in both directions and then symmetrizing using thegrow-diag-final-and heuristic (Koehn et al,2003).
For each language pair, the corpus wasaligned twice, once in its non-segmented variant andonce using the single-best segmentation variant.For translation, we used a bottom-up parsing de-coder that uses cube pruning to intersect the lan-4http://www.statmt.org/wmt09guage model with the target side of the synchronousgrammar.
The grammar rules were extracted fromthe word aligned parallel corpus and scored as de-scribed in Chiang (2007).
The features used by thedecoder were the English language model log prob-ability, log f(e?|f?
), the ?lexical translation?
log prob-abilities in both directions (Koehn et al, 2003), anda word count feature.
For the lattice systems, wealso included the unnormalized log p(f?
|G), as it isdefined in Section 3, as well as an input word countfeature.
The feature weights were tuned on a held-out development set so as to maximize an equallyweighted linear combination of BLEU and 1-TER(Papineni et al, 2002; Snover et al, 2006) using theminimum error training algorithm on a packed for-est representation of the decoder?s hypothesis space(Macherey et al, 2008).
The weights were indepen-dently optimized for each language pair and each ex-perimental condition.5.2 Segmentation lattice resultsIn this section, we report the results of an experimentto see if the compound lattices constructed using ourmaximum entropy model yield better translationsthan either an unsegmented baseline or a baselineconsisting of a single-best segmentation.For each language pair, we define three condi-tions: BASELINE, 1BEST, and LATTICE.
In theBASELINE condition, a lowercased and tokenized(but not segmented) version of the test data istranslated using the grammar derived from a non-segmented training data.
In the 1BEST condition,the single best segmentation s?N1 that maximizesPr(sN1 |w) is chosen for each word using the MERT-trained model (the German model for German, andthe language-neutral model for Hungarian and Turk-ish).
This variant is translated using a grammarinduced from a parallel corpus that has also beensegmented according to the same decision rule.
Inthe LATTICE condition, we constructed segmenta-tion lattices using the technique described in Sec-tion 3.1.
For all languages pairs, we used d = 2 asthe pruning density parameter (which corresponds tothe highest F-score on the held out test set).
Addi-tionally, if the unsegmented form of the word wasremoved from the lattice during pruning, it was re-stored to the lattice with zero weight.Table 4 summarizes the results of the translation411f -tokens f -types e-tokens.
e-typesDE-BASELINE 38M 307k 40M 96kDE-1BEST 40M 136k ?
?HU-BASELINE 25M 646k 29M 158kHU-1BEST 27M 334k ?
?TR-BASELINE 1.0M 56k 1.3M 23kTR-1BEST 1.1M 41k ?
?Table 3: Training corpus statistics.BLEU TERDE-BASELINE 21.0 60.6DE-1BEST 20.7 60.1DE-LATTICE 21.6 59.8HU-BASELINE 11.0 71.1HU-1BEST 10.7 70.4HU-LATTICE 12.3 69.1TR-BASELINE 26.9 61.0TR-1BEST 27.8 61.2TR-LATTICE 28.7 59.6Table 4: Translation results for German (DE)-English,Hungarian (HU)-English, and Turkish (TR)-English.Scores were computed using a single reference and arecase insensitive.experiments comparing the three input variants.
Forall language pairs, we see significant improvementsin both BLEU and TER when segmentation latticesare used.5 Additionally, we also confirmed previousfindings that showed that when a large amount oftraining data is available, moving to a one-best seg-mentation does not yield substantial improvements(Yang and Kirchhoff, 2006).
Perhaps most surpris-ingly, the improvements observed when using lat-tices with the Hungarian and Turkish systems werelarger than the corresponding improvement in theGerman system, but German was the only languagefor which we had segmentation training data.
Thesmaller effect in German is probably due to there be-ing more in-domain training data in the German sys-tem than in the (otherwise comparably sized) Hun-garian system.5Using bootstrap resampling (Koehn, 2004), the improve-ments in BLEU, TER, as well as the linear combination used intuning are statistically significant at at least p < .05.Targeted analysis of the translation output showsthat while both the 1BEST and LATTICE systemsgenerally produce adequate translations of com-pound words that are out of vocabulary in the BASE-LINE system, the LATTICE system performs bet-ter since it recovers from infelicitous splits that theone-best segmenter makes.
For example, one classof error we frequently observe is that the one-bestsegmenter splits an OOV proper name into twopieces when a portion of the name corresponds to aknown word in the source language (e.g.
tom tan-credo?tom tan credo which is then translated astom tan belief ).65.3 The effect of the density parameterFigure 4 shows the effect of manipulating the den-sity parameter (cf.
Section 3.2) on the performanceand decoding time of the Turkish-English transla-tion system.
It further confirms the hypothesis thatincreased diversity of segmentations encoded in asegmentation lattice can improve translation perfor-mance; however, it also shows that once the den-sity becomes too great, and too many implausiblesegmentations are included in the lattice, translationquality will be harmed.6 Related workAside from improving the vocabulary coverage ofmachine translation systems (Koehn et al, 2008;Yang and Kirchhoff, 2006; Habash and Sadat,2006), compound word segmentation (also referredto as decompounding) has been shown to be help-ful in a variety of NLP tasks including mono- and6We note that our maximum entropy segmentation modelcould easily address this problem by incorporating informationabout whether a word is likely to be a named entity as a feature.4128484.284.484.684.8851  1.5  2  2.5  3  3.52468101214161-(TER-BLEU)/2secs/sentenceSegmentation lattice densityTranslation qualityDecoding timeFigure 4: The effect of the lattice density parameter ontranslation quality and decoding time.crosslingual IR (Airio, 2006) and speech recognition(Hessen and Jong, 2003).
A number of researchershave demonstrated the value of using lattices to en-code segmentation alternatives as input to a machinetranslation system (Dyer et al, 2008; DeNeefe et al,2008; Xu et al, 2004), but this is the first work todo so using a single segmentation model.
Anotherstrand of inquiry that is closely related is the work onadjusting the source language segmentation to matchthe granularity of the target language as a way of im-proving translation.
The approaches suggested thusfar have been mostly of a heuristic nature tailored toChinese-English translation (Bai et al, 2008; Ma etal., 2007).7 Conclusions and future workIn this paper, we have presented a maximum entropymodel for compound word segmentation and used itto generate segmentation lattices for input into a sta-tistical machine translation system.
These segmen-tation lattices improve translation quality (over analready strong baseline) in three typologically dis-tinct languages (German, Hungarian, Turkish) whentranslating into English.
Previous approaches togenerating segmentation lattices have been quite la-borious, relying either on the existence of multiplesegmenters (Dyer et al, 2008; Xu et al, 2005) orhand-crafted rules (DeNeefe et al, 2008).
Althoughthe segmentation model we propose is discrimina-tive, we have shown that it can be trained using aminimal amount of annotated training data.
Further-more, when even this minimal data cannot be ac-quired for a particular language (as was the situa-tion we faced with Hungarian and Turkish), we havedemonstrated that the parameters obtained in onelanguage work surprisingly well for others.
Thus,with virtually no cost, this model can be used with avariety of diverse languages.While these results are already quite satisfying,there are a number of compelling extensions to thiswork that we intend to explore in the future.
First,unsupervised segmentation approaches offer a verycompelling alternative to the manually crafted seg-mentation lattices that we created.
Recent worksuggests that unsupervised segmentation of inflec-tional affixal morphology works quite well (Poon etal., 2009), and extending this work to compoundingmorphology should be feasible, obviating the needfor expensive hand-crafted reference lattices.
Sec-ond, incorporating target language information intoa segmentation model holds considerable promisefor inducing more effective translation models thatperform especially well for segmentation lattice in-puts.AcknowledgmentsSpecial thanks to Kemal Oflazar and Reyyan Yen-iterzi of Sabanc?
University for providing theTurkish-English corpus and to Philip Resnik, AdamLopez, Trevor Cohn, and especially Phil Blunsomfor their helpful suggestions.
This research was sup-ported by the Army Research Laboratory.
Any opin-ions, findings, conclusions or recommendations ex-pressed in this paper are those of the authors and donot necessarily reflect the view of the sponsors.ReferencesEija Airio.
2006.
Word normalization and decompound-ing in mono- and bilingual IR.
Information Retrieval,9:249?271.Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.2008.
Improving word alignment by adjusting Chi-nese word segmentation.
In Proceedings of the ThirdInternational Joint Conference on Natural LanguageProcessing.A.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.N.
Bertoldi, R. Zens, and M. Federico.
2007.
Speech413translation by confusion network decoding.
In Pro-ceeding of ICASSP 2007, Honolulu, Hawaii, April.Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-ning.
2008.
Optimizing Chinese word segmentationfor machine translation performance.
In Proceedingsof the Third Workshop on Statistical Machine Transla-tion, Prague, Czech Republic, June.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.S.
DeNeefe, U. Hermjakob, and K. Knight.
2008.
Over-coming vocabulary sparsity in mt using lattices.
InProceedings of AMTA, Honolulu, HI.C.
Dyer, S. Muresan, and P. Resnik.
2008.
Generalizingword lattice translation.
In Proceedings of HLT-ACL.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2007.
Englishgigaword third edition.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Proc.
ofNAACL, New York.Arjan Van Hessen and Franciska De Jong.
2003.
Com-pound decomposition in dutch large vocabulary speechrecognition.
In Proceedings of Eurospeech 2003, Gen-eve, pages 225?228.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proceedings of IEEEInternation Conference on Acoustics, Speech, and Sig-nal Processing, pages 181?184.P.
Koehn and K. Knight.
2003.
Empirical methods forcompound splitting.
In Proc.
of the EACL 2003.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of NAACL2003, pages 48?54, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.P.
Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Annual Meetingof the Association for Computation Linguistics (ACL),Demonstration Session, pages 177?180, June.Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008.Towards better machine translation quality for theGerman-English language pairs.
In ACL Workshop onStatistical Machine Translation.P.
Koehn.
2004.
Statistical signficiance tests for machinetranslation evluation.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 388?395.Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-cedal.
1989.
On the limited memory BFGS methodfor large scale optimization.
Mathematical Program-ming B, 45(3):503?528.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 304?311,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In Proceedings of EMNLP, Honolulu, HI.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.Kemal Oflazer and Ilknur Durgar El-Kahlout.
2007.
Ex-ploring different representational units in English-to-Turkish statistical machine translation.
In Proceedingsof the Second Workshop on Statistical Machine Trans-lation, pages 25?32, Prague, Czech Republic, June.Association for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the ACL, pages 311?318.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proc.
of NAACL 2009.S.
Sixtus and S. Ortmanns.
1999.
High quality wordgraphs using forward-backward pruning.
In Proceed-ings of ICASSP, Phoenix, AZ.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas.J.
Xu, R. Zens, and H. Ney.
2004.
Do we need Chi-nese word segmentation for statistical machine trans-lation?
In Proceedings of the Third SIGHAN Work-shop on Chinese Language Learning, pages 122?128,Barcelona, Spain.J.
Xu, E. Matusov, R. Zens, and H. Ney.
2005.
Inte-grated Chinese word segmentation in statistical ma-chine translation.
In Proc.
of IWSLT 2005, Pittsburgh.M.
Yang and K. Kirchhoff.
2006.
Phrase-based back-off models for machine translation of highly inflectedlanguages.
In Proceedings of the EACL 2006, pages41?48.414
