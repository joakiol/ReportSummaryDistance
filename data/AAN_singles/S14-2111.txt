Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628?632,Dublin, Ireland, August 23-24, 2014.TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping andWeighting Scheme for Unbalanced DataYasuhide MiuraFuji Xerox Co., Ltd. / Japanyasuhide.miura@fujixerox.co.jpShigeyuki SakakiFuji Xerox Co., Ltd. / Japansakaki.shigeyuki@fujixerox.co.jpKeigo HattoriFuji Xerox Co., Ltd. / Japankeigo.hattori@fujixerox.co.jpTomoko OhkumaFuji Xerox Co., Ltd. / Japanohkuma.tomoko@fujixerox.co.jpAbstractThis paper describes the system that hasbeen used by TeamX in SemEval-2014Task 9 Subtask B.
The system is a senti-ment analyzer based on a supervised textcategorization approach designed with fol-lowing two concepts.
Firstly, since lex-icon features were shown to be effectivein SemEval-2013 Task 2, various lexiconsand pre-processors for them are introducedto enhance lexical information.
Secondly,since a distribution of sentiment on tweetsis known to be unbalanced, an weightingscheme is introduced to bias an output of amachine learner.
For the test run, the sys-tem was tuned towards Twitter texts andsuccessfully achieved high scoring resultson Twitter data, average F170.96 on Twit-ter2014 and average F156.50 on Twit-ter2014Sarcasm.1 IntroductionThe growth of social media has brought a ris-ing interest to make natural language technologiesthat work with informal texts.
Sentiment anal-ysis is one such technology, and several work-shops such as SemEval-2013 Task 2 (Nakov etal., 2013), CLEF 2013 RepLab 2013 (Amig?oet al., 2013), and TASS 2013 (Villena-Rom?anand Garc?
?a-Morera, 2013) have recently targetedtweets or cell phone messages as analysis text.This paper describes a system that has submit-ted a sentiment analysis result to Subtask B ofSemEval-2014 Task9 (Rosenthal et al., 2014).SemEval-2014 Task9 is a rerun of SemEval-2013Task 2 with different test data, and Subtask B is atask of message polarity classification.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/The system we prepared is a sentiment ana-lyzer based on a supervised text categorizationapproach.
Various features and their extractionmethods are integrated in the system following theworks presented in SemEval-2013 Task 2.
Addi-tionally to these features, we assembled followingnotable functionalities to the system:1.
Processes to enhance word-to-lemma map-ping.
(a) A spelling corrector to normalize out-of-vocabulary words.
(b) Two Part-of-Speech (POS) taggers torealize word-to-lemma mapping in twoperspectives.
(c) A word sense disambiguator to obtainword senses and their confidence scores.2.
An weighting scheme to bias an output of amachine learner.Functionalities 1a to 1c are introduced to enhanceinformation based on lexical knowledge, sincefeatures based on lexicons are shown to be ef-fective in SemEval-2013 Task 2 (Mohammad etal., 2013).
Functionality 2 is introduced to makethe system adjustable to polarity unbalancednessknown to exists in Twitter data (Nakov et al.,2013).The accompanying sections of this papers areorganized as follows.
Section 2 describes re-sources such as labeled texts and lexicons used inour system.
Section 3 explains the details of thesystem.
Section 4 discusses the submission testrun and some extra test runs that we performedafter the test data release.
Finally, section 5 con-cludes the paper.2 Resources2.1 Sentiment Labeled DataThe system is a constrained system, therefore onlythe sentiment labeled data distributed by the task628Type #Used #Full %Twitter(train) 6949 9684 71.8Twitter(dev) 1066 1654 64.4Twitter(dev-test) 3269 3813 85.7SMS(dev-test) 2094 2094 100Table 1: The numbers of messages for each type.
?train?, ?dev?, and ?dev-test?
denote training, devel-opment, and development-test respectively.
#Usedis the number of messages that we were able toobtain, and #Full is the maximum number of mes-sages that were provided.Criterion LexiconGeneral InquirerFORMAL MPQA Subjectivity LexiconSentiWordNetAFINN-111INFORMALBing Liu?s Opinion LexiconNRC Hashtag Sentiment LexiconSentiment140 LexiconTable 2: The seven sentiment lexicons and theircriteria.organizers were used.
However, due to accessibil-ity changes in tweets, a subset of the training, thedevelopment, and the development-test data wereused.
Table 1 shows the numbers of messages foreach type.2.2 Sentiment LexiconsThe system includes seven sentiment lexiconsnamely: AFINN-111 (Nielsen, 2011), Bing Liu?sOpinion Lexicon1, General Inquirer (Stone et al.,1966), MPQA Subjectivity Lexicon (Wilson et al.,2005), NRCHashtag Sentiment Lexicon (Moham-mad et al., 2013), Sentiment140 Lexicon (Moham-mad et al., 2013), and SentiWordNet (Baccianellaet al., 2010).
We categorized these seven lexi-cons to two criteria: ?FORMAL?
and ?INFOR-MAL?.
Lexicons that include lemmas of erroneouswords (e.g.
misspelled words) were categorizedto ?INFORMAL?.
Table 2 illustrates the criteria ofthe seven lexicons.
These criteria are used in theprocess of word-to-lemma mapping processes andwill be explained in Section 3.1.3.3 System DetailsThe system is a modularized system consistingof a variety of pre-processors, feature extractors,1http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.htmlText?NormalizerStanford?POS?TaggerWord?Sense?Disambiguator Negation?Detectorword?senses FORMAL?lexiconsPre?processorsFeature?ExtractorsInput Spelling?CorrectorCMU?ARK?POS?TaggerNegation?Detectorword?ngramscharacter?ngrams clustersMachine?Learner Prediction?AdjusterOutputINFORMAL?lexiconsFigure 1: An overview of the system?and a machine learner.
Figure 1 illustrates theoverview of the system.3.1 Pre-processors3.1.1 Text NormalizerThe text normalizer performs following three rule-based normalization of an input text:?
Unicode normalization in form NFKC2.?
All upper case letters are converted to lowercase ones (ex.
?GooD?
to ?good?).?
URLs are exchanged with string ?URL?s (ex.?http://example.org?
to ?URL?
).3.1.2 Spelling CorrectorA spelling corrector is included in the system tonormalize misspellings.
We used Jazzy3, an opensource spell checker with US English dictionariesprovided along with Jazzy.
Jazzy combines Dou-bleMetaphone phonetic matching algorithm and anear-miss match algorithm based on Levenshteindistance to correct a misspelled word.3.1.3 POS TaggersThe system includes two POS taggers to realizeword-to-lemma mapping in two perspectives.Stanford POS Tagger Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003) isone POS tagger which is used to map words2http://www.unicode.org/reports/tr15/3http://jazzy.sourceforge.net/629to lemmas of ?FORMAL?
criterion lexicons,and to extract word sense features.
A finite-state transducer based lemmatizer (Minnen etal., 2001) included in the POS tagger is usedto obtain lemmas of tokenized words.CMU ARK POS Tagger A POS tagger fortweets by CMU ARK group (Owoputi et al.,2013) is another POS tagger used to mapwords to lemmas of ?INFORMAL?
criterionlexicons, and to extract ngram features and acluster feature.3.1.4 Word Sense DisambiguatorAword sense disambiguator is included in the sys-tem to determine a sense of a word.
We usedUKB4which implements graph-based word sensedisambiguation based on Personalized PageRankalgorithm (Agirre and Soroa, 2009) on a lexicalknowledge base.
As a lexical knowledge base,WordNet 3.0 (Fellbaum, 1998) included in theUKB package is used.3.1.5 Negation DetectorThe system includes a simple rule-based negationdetector.
The detector is an implementation of thealgorithm on Christopher Potts?
Sentiment Sym-posium Tutorial5.
The algorithm is a simple algo-rithm that appends a negation suffix to words thatappear within a negation scope surrounded by anegation key (ex.
?no?)
and a certain punctuation(ex.
?:?
).3.2 FeaturesThe followings are the features used in the system.word ngrams Contiguous 1, 2, 3, and 4 gramsof words, and non-contiguous 3 and 4 gramsof words are extracted from a given words.Non-contiguous ngram are ngrams where oneof words are replaced with a wild card word?*?.
Example of contiguous 3 grams is?by the way?, and the corresponding noncon-tiguous variation is ?by * way?.character ngrams Contiguous 3, 4, and 5 gramsof characters with in a word are extractedfrom given words.lexicons Words are mapped to seven lexicons ofsection 2.2.
For two sentiment labels (pos-itive and negative) in each lexicon, follow-ing four values are extracted: total matched4http://ixa2.si.ehu.es/ukb/5http://sentiment.christopherpotts.net/lingstruc.html#negationI?liked an?example.org?video http://example.orgSense ID Score01824736?v 0.44231301777210?v 0.35567901776952?v 0.148101?Sense?ID Score06277280?n 0.68865506277803?n 0.16334304534127?n 0.103199?textWSD?resultFeature Weight01824736?v 0.442313Features 01777210?v 0.35567901776952?v 0.14810106277280?n 0.688655?Figure 2: An example of word senses feature?word count, total score, maximal score, andlast word score6.
For lexicons without senti-ment scores, score 1.0 is used for all entries.Note that different POS taggers are used inword-to-lemma mapping as described in Sec-tion 3.1.3.clusters Words are mapped to Twitter Word Clus-ters of CMU ARK group7.
The largest clus-tering result consisting of 1000 clusters fromapproximately 56 million tweets is used asclusters.word senses A result of the word sense disam-biguator is extracted as weighted features ac-cording to their scores.
Figure 2 shows anexample of this feature.The ngram features are introduced as basic bag-of-words features in a supervised text categoriza-tion approach.
Lexicon features are designed tostrengthen the lexical features of Mohammad etal.
(2013) which have been shown to be effectivein the last year?s task.
Cluster features are im-plemented as an improvement for an supervisedNLP system following the work of Turian et al.(2010).
Word sense features are utilized to helpsubjectivity analysis and contextual polarity anal-ysis (Akkaya et al., 2009).3.3 Machine LearnerLogistic Regression is utilized as an algorithm ofa supervised machine learning method.
As animplementation of Logistic Regression, LIBLIN-EAR (Fan et al., 2008) is used.
A Logistic Regres-sion is trained using the features of Section 3.2with the three polarities (positive, negative, andneutral) as labels.6The total number of lexical features is 7?
2?
4 = 56.7http://www.ark.cs.cmu.edu/TweetNLP/630Parameters SourcesParameter Selection SourceC wposwnegLiveJournal SMS Twitter Twitter Twitter20142014 2013 2013 2014 SarcasmTwitter(train)+Twitter(dev) 0.07 1.7 2.6 71.23 62.33 71.28 70.40 53.32Twitter(dev-test)* 0.03 2.4 3.3 69.44 57.36 72.12 70.96 56.50SMS(dev-test) 0.80 1.1 1.2 72.99 68.92 65.65 66.66 48.24SMS(dev-test)+Twitter(dev-test) 0.07 1.9 2.0 72.54 65.44 70.41 69.80 51.09Table 3: The scores for each source in the test runs.
The run with asterisk (*) denotes the submissionrun.
The values in the ?Sources?
columns represent scores in SemEval-2014 Task 9 metric (the averageof positive F1and negative F1).3.4 Prediction AdjusterSince the labels in the tweets data are unbalanced(Nakov et al., 2013), we prepared a prediction ad-juster for Logistic Regression output.
For each po-larity l, an weighting factorwlthat adjusts a proba-bility output Pr(l) is introduced.
An updated pre-diction label is decided by selecting an l that max-imizes score(l) which can be expressed as equa-tion 1.arg maxl?
{pos,neg,neu}score(l) = wlPr(l) (1)The approach we took in this prediction adjusteris a simple approach to bias an output of LogisticRegression, but may not be a typical approach tohandle unbalanced data.
For instance, LIBLIN-EAR includes the weighting option ?-wi?
whichenables a use of different cost parameter C for dif-ferent classes.
One advantage of our approach isthat the change in wldoes not require a training ofLogistic Regression.
Various values of wlcan betested with very low computational cost, which ishelpful in a situation like SemEval tasks where thetime for development is limited.4 Test Runs4.1 Submission Test RunThe system was trained using the 8,015 tweets in-cluded in Twitter(train) and Twitter(dev) describedin Section 2.1.
Three parameters: cost parameterC of Logistic Regression, weight wposof the pre-diction adjuster, and weight wnegof the predic-tion adjuster, were considered in the submissiontest run.
For the wneuof the prediction adjuster, afixed value of 1.0 was used.Prior to the submission test run, the followingtwo steps were performed to select a parametercombination for the submission run.Step 1 The system with all combinations of C inrange of {0.01 to 0.09 by step 0.01, 0.1 to 0.9by step 0.1, 1 to 10 by step 1}, wposin rangeof {1.0 to 5.0 by step 0.1}, and wnegin rangeof {1.0 to 5.0 by step 0.1} were prepared8.Step 2 The performances of the system for allthese parameter combinations were calcu-lated using Twitter(dev-test) described inSection 2.1.As a result, the parameter combination C = 0.03,wpos= 2.4, and wneg= 3.3 which performedbest in Twitter(dev-test) was selected as a parame-ter combination for the submission run.Finally, the system with the selected parameterswas applied to the test set of SemEval-2014 Task9.
?Twitter(dev-test)?
in Table 3 shows the val-ues of this submission run.
The system achievedhigh performances on Twitter data: 72.12, 70.96,and 56.50 on Twitter2013, Twitter2014, and Twit-ter2014Sarcasm respectively.4.2 Post-Submission Test RunsThe system performed quite well on Twitterdata but not so well on other data on the sub-mission run.
After the release of the golddata of the 2014 test tun, we conducted sev-eral test runs using different parameter combina-tions.
?Twitter(train)+Twitter(dev)?, ?SMS(dev-test)?, and ?SMS(dev-test)+Twitter(dev-test)?
arethe results of test runs with different data sourcesused for the parameter selection process.
In ?Twit-ter(train)+Twitter(dev)?, the parameter combina-tion that maximizes a micro-average score of 5-fold cross validation was chosen since the trainingdata and the parameter selection are equivalent.The parameter combination selected with ?Twit-ter(train)+Twitter(dev)?
showed similar result asthe submission run, which is high performanceson Twitter data.
In the case of ?SMS(dev-test)?, thesystem performed well on ?LiveJournal2014?
and?SMS(dev-test)?
namely 72.99 and 68.92.
How-8The total number of parameter combination is 29?51?51 = 75429.631ever, in this parameter combination the scores onTwitter data were clearly lower than the submis-sion run.
Finally, ?SMS(dev-test)+Twitter(dev-test)?
resulted to a mid performing result, wherescores for each source marked in-between valuesof ?Twitter(dev-test)?
and ?SMS(dev-test)?.5 ConclusionWe proposed a system that is designed to enhanceinformation based on lexical knowledge and tobe adjustable to unbalanced training data.
Withparameters tuned towards Twitter data, the sys-tem successfully achieved high scoring results onTwitter data, average F170.96 on Twitter2014 andaverage F156.50 on Twitter2014Sarcasm.Additional test runs with different parametercombination showed that the system can be tunedto perform well on non-Twitter data such as blogsor short messages.
However, the limitation of ourapproach to directly weight a machine learner?soutput was shown, since we could not find ageneral purpose parameter combination that canachieve high scores on any types of data.AcknowledgementsWe would like to thank the anonymous reviewersfor their valuable comments to improve this paper.ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.
In Pro-ceedings of EACL 2009, pages 33?41.Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InProceedings of EMNLP 2009, pages 190?199.Enrique Amig?o, Jorge Carrillo de Albornoz, IrinaChugur, Adolfo Corujo, Julio Gonzalo, TamaraMart?
?n, Edgar Meij, Maarten de Rijke, and DamianoSpina.
2013.
Overview of RepLab 2013: Evaluat-ing online reputation monitoring systems.
In CLEF2013 Evaluation Labs and Workshop, Online Work-ing Notes.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of LREC 2010, pages 2200?2204.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
In Journal ofMachine Learning Research, volume 9, pages 1871?1874.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the seventh international workshop on SemanticEvaluation Exercises (SemEval-2013), pages 321?327.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: Sentiment analysisin Twitter.
In Proceedings of the seventh interna-tional workshop on Semantic Evaluation Exercises(SemEval-2013), pages 312?320.Finn?Arup Nielsen.
2011.
A new ANEW: Evalu-ation of a word list for sentiment analysis in mi-croblogs.
In Proceedings of the ESWC2011 Work-shop on ?Making Sense of Microposts?
: Big thingscome in small packages, pages 93?98.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL 2013, pages 380?390.Sara Rosenthal, Alan Ritter, Preslav Nakov, andVeselin Stoyanov.
2014.
SemEval-2014 task 9:Sentiment analysis in Twitter.
In Proceedings of theeighth international workshop on Semantic Evalua-tion Exercises (SemEval-2014).Philip Stone, Dexter Dunphy, Marshall Smith, andDaniel Ogilvie.
1966.
General Inquirer: A Com-puter Approach to Content Analysis.
MIT Press.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL 2003, pages 252?259.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings ofACL 2010, pages 384?394.Julio Villena-Rom?an and Janine Garc??a-Morera.
2013.TASS 2013 - Workshop on sentiment analysis at SE-PLN 2013: An overview.
In Proceedings of theTASS workshop at SEPLN 2013.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP 2005, pages 347?354.632
