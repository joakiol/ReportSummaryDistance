Proceedings of EACL '99An annotation scheme for discourse-level argumentationin research articlesS imone Teufe l  t and Jean  Car le t ta  f and  Marc  Moens  ~tHCRC Language Techno logy  Group andtHuman Communicat ion  Research Cent reDiv is ion of  In fo rmat icsUnivers i ty  of Ed inburghS.
Teufel@ed.
ac.
uk, J. Carletta@ed.
ac.
uk, M. Moens@ed.
ac.
ukAbst ractIn order to build robust automatic ab-stracting systems, there is a need for bet-ter training resources than are currentlyavailable.
In this paper, we introducean annotation scheme for scientific ar-ticles which can be used to build sucha resource in a consistent way.
Theseven categories of the scheme are basedon rhetorical moves of argumentation.Our experimental results show that thescheme is stable, reproducible and intu-itive to use.1 I n t roduct ionCurrent approaches to automatic summariza-tion cannot create coherent, flexible automaticsummaries.
Sentence selection techniques (e.g.Brandow et al, 1995; Kupiec et al 1995) pro-duce extracts which can be incoherent and which,because of the generality of the methodology,can give under-informative r sults; fact extrac-tion techniques (e.g.
Rau et al, 1989, Young andHayes, 1985) are tailored to particular domains,but have not really scaled up from restricted textsand restricted omains to larger domains and un-restricted text.
Sp~irck Jones (1998) argues thattaking into account the structure of a text willhelp when summarizing the text.The problem with sentence selection is that itrelies on extracting sentences out of context, butthe meaning of extracted material tends to dependon where in the text the extracted sentence wasfound.
However, sentence selection still has thedistinct advantage of robustness.We think sentence selection could be improvedsubstantially if the global rhetorical context of theextracted material was taken into account more.Marcu (1997) makes a similar point based onrhetorical relations as defined by Rhetorical Struc-ture Theory (RST, (Mann and Thompson, 1987)).In contrast o this approach, we stress the impor-tance of rhetorical moves which are global to theargumentation of the paper, as opposed to localRST-type moves.
For example, sentences whichdescribe weaknesses of previous approaches canprovide a good characterization f the scientificarticles in which they occur, since they are likelyto also be a description of the problem that pa-per is intending to solve.
Take a sentence like"Un\]ortunately, this work does not solve problemX": if X is a shortcoming in someone lse's work,this usually means that the current paper will tryto solve X.
Sentence xtraction methods can lo-cate sentences like these, e.g.
using a cue phrasemethod (Paice, 1990).But a very similar-looking sentence can play acompletely different argumentative role in a sci-entific text: when it occurs in the section "FutureWork", it might refer to a minor weakness in thework presented in the source paper (i.e.
of the au-thor's own solution).
In that case, the sentence isnot a good characterization f the paper.Our approach to automatic text summarizationis to find important sentences in a source text bydetermining their most likely argumentative role.In order to create an automatic process to do so,either by symbolic or machine learning techniques,we need training material: a collection of texts (inthis case, scientific articles) where each sentenceis annotated with information about the argumen-tative role that sentence plays in the paper.
Cur-rently, no such resource is available.
We developedan annotation scheme as a starting point for build-ing up such a resource, which we will describe insection 2.
In section 3, we use content analysistechniques to test the annotation scheme's relia-bility.2 The  annotat ion  schemeWe wanted the scheme to cover one text type,namely research articles, but from different pre-sentational traditions and subject matters, so that110Proceedings of EACL '99we can use it for text summarization i  a range offields.
This means we cannot rely on similaritiesin external presentation, e.g.
section structure andtypical inguistic formulaic expressions.Previous discourse-level annotation schemes(e.g.
Liddy, 1991; Kircz, 1991) show that infor-mation retrieval can profit from added rhetoricalinformation in scientific texts.
However, the def-initions of the categories in these schemes relieson domain dependent knowledge like typical re-search methodology, and are thus too specific forour purposes.General frameworks of text structure and argu-mentation, like Cohen's (1984) theoretical frame-work for general argumentation and RhetoricalStructure Theory (Mann and Thompson, 1987),are theoretically applicable to many differentkinds of text types.
However, we believe that re-stricting ourselves to the text type of research ar-ticles will give us an advantage over such generalschemes, because it will allow us to rely on com-municative goals typically occurring within thattext type.STales' (1990) CARS (Creating a ResearchSpace) model provides a description at the rightlevel for our purposes.
STales claims that theregularities in the argumentative structure of re-search article introductions follow from the au-thors' primary communicative goal: namely toconvince their audience that they have provideda contribution to science.
From this goal followhighly predictable subgoals which he calls argu-mentative moves ("recurring and regularized com-municative vents").
An example for such a moveis "Indication of a gap", where the author arguesthat there is a weakness in an earlier approachwhich needs to be solved.STales' model has been used extensively by dis-course analysts and researchers in the field of En-glish for Specific Purposes, for tasks as varied asteaching English as a foreign language, humantranslation and citation analysis (Myers, 1992;Thompson and Ye, 1991; Duszak, 1994), but al-ways for manual analysis by a single person.
Ourannotation scheme is based on STales' model butwe needed to modify it.
Firstly, the CARS modelonly applies to introductions of research articles,so we needed new moves to cover the other papersections; secondly, we needed more precise guide-lines to make the scheme applicable to reliable an-notation for several non-discourse analysts (andfor potential automatic annotation).For the development of our scheme, we usedcomputational linguistics articles.
The papers inour collection cover a challenging range of sub-ject matters due to the interdisciplinarity of thefield, such as logic programming, statistical lan-guage modelling, theoretical semantics and com-putational psycholinguistics.
Because the researchmethodology and tradition of presentation is sodifferent in these fields, we would expect thescheme to be equally applicable in a range of dis-ciplines other than those named.Our annotation scheme consists of the sevencategories shown in Figure 1.
There are two ver-sions of the annotation scheme.
The basic schemeprovides a distinction between three textual seg-ments which we think is a necessary precondi-tion for argumentatively-justified summarization.This distinction is concerned with the attributionof authorship to scientific ideas and solutions de-scribed in the text.
Authors need to make clear,and readers need to understand:?
which sections describe generally acceptedstatements (BACKGROUND);?
which ideas are attributed to some other, spe-cific piece of research outside the given paper,including own previous work (OTHER);?
and which statements are the authors' ownnew contributions (OWN).The/u l l  annotation scheme consists of the ba-sic scheme plus four other categories, which arebased on STales' moves.
The most important ofthese is AIM (STales' move "Explicit statementsof research goal"), as these moves are good char-acterizations of the entire paper.
We are inter-ested in how far humans can be trained to con-sistently annotate these sentences; imilar experi-ments where subjects elected one or several 'mostrelevant' sentences from a paper have traditionallyreported low agreement (Rath et al, 1961).
Thereis also the category TEXTUAL ( STales' move "In-dicate structure"), which provides helpful infor-mation about section structure, and two moveshaving to do with attitude towards previous re-search, namely BASIS and CONTRAST.The relative simplicity of the scheme was a com-promise between two demands: we wanted thescheme to contain enough information for auto-matic summarization, but still be practicable forhand coding.Annotation proceeds entence by sentence ac-cording to the decision tree given in Figure 2.
Noinstructions about the use of cue phrases weregiven, although some of the example sentencesgiven in the guidelines contained cue phrases.
Thecategorisation task resembles the judgements per-formed e.g.
in dialogue act coding (Carletta et al,111Proceedings of EACL '99BASICSCHEMEBACKGROUNDOTHERSentences describing some (generally accepted) backgroundknowledgeSentences describing aspects of some specific other research in aneutral way (excluding contrastive or BASIS statements)OWN Sentences describing any aspect of the own work presented inthis paper - except what is covered by AIM or TEXTUAL, e.g.details of solution (methodology), limitations, and further work.AIM Sentences best portraying the particular (main) research goal ofthe articleTEXTUAL Explicit statements about the textual section structure of thepaperCONTRAST Sentences contrasting own work to other work; sentences point-ing out weaknesses in other research; sentences stating that theresearch task of the current paper has never been done before;direct comparisonsBASIS Statements that the own work uses some other work as its basisor starting point, or gets support from this other workFigure 1: Overview of the annotation schemeFULLSCHEME1997; Alexandersson et al, 1995; Jurafsky et al,1997), but our task is more difficult since it re-quires more subjective interpretation.3 Annotat ion  exper imentOur annotation scheme is based on the intuitionthat its categories provide an adequate and in-tuitive description of scientific texts.
But thisintuition alone is not enough of a justification:we believe that our claims, like claims about anyother descriptive account of textual interpreta-tion, should be substantiated by demonstratingthat other humans can apply this interpretationconsistently to actual texts.We did three studies.
Study I and II were de-signed to find out if the two versions of the an-notation scheme (basic vs. full) can be learned byhuman coders with a significant amount of train-ing.
We are interested in two formal properties ofthe annotation scheme: stability and reproducibil-ity (Krippendorff, 1980).
Stability, the extent towhich one annotator will produce the same classi-fications at different imes, is important becausean instable annotation scheme can never be re-producible.
Reproducibility, the extent to whichdifferent annotators will produce the same clas-sifications, is important because it measures theconsistency of shared understandings (or mean-ing) held between annotators.We use the Kappa coefficient K (Siegel andCastellan, 1988) to measure stability and repro-ducibility among k annotators on N items: Inour experiment, he items are sentences.
Kappais a better measurement of agreement than rawpercentage agreement (Carletta, 1996) because itfactors out the level of agreement which wouldbe reached by random annotators using the samedistribution of categories as the real coders.
Nomatter how many items or annotators, or how thecategories are distributed, K--0 when there is noagreement other than what would be expected bychance, and K=I  when agreement is perfect.
Weexpect high random agreement for our annotationscheme because so many sentences fall into theOWN category.Studies I and II will determine how far we cantrust in the human-annotated training materialfor both learning and evaluation of the automaticmethod.
The outcome of Study II (full annota-tion scheme) is crucial to the task, as some of thecategories pecific to the full annotation scheme(particularly AIM) add considerable value to theinformation contained in the training material.Study III tries to answer the question whetherthe considerable training effort used in Studies Iand II can be reduced.
If it were the case thatcoders with hardly any task-specific training canproduce similar results to highly trained coders,the training material could be acquired in a moreefficient way.
A positive outcome of Study IIIwould also strengthen claims about the intuitivityof the category definitions.112Proceedings of EACL '99Does this sentence r fer to ownwork (excluding previous workof the same author)?Does this sentence contain materialthat describes the specific aimdescribed in the paper?Does this sentence makereference to the structureof the paper?I TEXTUAL \]Does the sentence describe generalbackground, including phenomenato be explained or linguistic example sentences?t\[ BACKGROUND 1 Does it describe anegative aspectJ of the other work, or a contrastor comparison of the own work to it?Y ~ N O\[ CONTRAST I Does this sentence mentionthe other work as basis ofor support for own work?Figure 2: Decision tree for annotationOur materials consist of 48 computational lin-guistics papers (22 for Study I, 26 for Study II),taken from the Computation and Language E-Print Archive (h t tp : / /xxx .
lan l .
gov/cmp-lg/).We chose papers that had been presented at COL-ING, ANLP or ACL conferences (including stu-dent sessions), or ACL-sponsored workshops, andbeen put onto the archive between April 1994 andApril 1995.3.1 Studies I and IIFor Studies I and II, we used three highly trainedannotators.
The annotators (two graduate stu-dents and the first author) can be consideredskilled at extracting information from scientificpapers but they were not experts in all of the sub-domains of the papers they annotated.
The anno-tators went through asubstantial mount of train-ing, including the reading of coding instructionsfor the two versions of the scheme (6 pages for thebasic scheme and 17 pages for the full scheme),four training papers and weekly discussions, inwhich previous annotations were discussed.
How-ever, annotators were not allowed to change anyprevious decisions.
For the stability figures (intra-annotator agreement), annotators re-coded 6 ran-domly chosen papers 6 weeks after the end of theannotation experiment.
Skim-reading and anno-tation of an average length paper (3800 words)typically took the annotators 20-30 minutes.During the annotation phase, one of the pa-pers turned out to be a review paper.
This papercaused the annotators difficulty as the scheme wasnot intended to cover reviews.
Thus, we discardedthis paper from the analysis.The results show that the basic annotationscheme is stable (K=.83, .79, .81; N=1248; k=2for all three annotators) and reproducible (K=.78,N=4031, k=3).
This reconfirms that trained an-notators are capable of making the basic dis-tinction between own work, specific other work,and general background.
The full annotationscheme is stable (K=.82, .81, .76; N--1220; k=2for all three annotators) and reproducible (K=.71,N=4261, k=3).
Because of the increased cogni-tive difficulty of the task, the decrease in stabilityand reproducibility in comparison to Study I isacceptable.
Leaving the coding developer out ofthe coder pool for Study II did not change the re-sults (K=.71, N=4261, k=2), suggesting that thetraining conveyed her intentions fairly well.We collected informal comments from our an-notators about how natural the task felt, but didnot conduct a formal evaluation of subjective per-ception of the difficulty of the task.
As a generalapproach in our analysis, we wanted to look at thetrends in the data as our main information source.Figure 3 reports how well the four non-basic at-egories could be distinguished from all other cat-egories, measured by Krippendorff's diagnosticsfor category distinctions (i.e.
collapsing all otherdistinctions).
When compared to the overall re-producibility of .71, we notice that the annota-tors were good at distinguishing AIM and TEx-113Proceedings of EACL '990.80.70.60,5K 0.40.30.20.10,; i::!i,i!ii: .
: : : .
:I ~:~:i;it i i::~i:!::}iIz!
!~;is!l :::.
;.i:~:!CONTRAST AIM BASIS TEXTUALFigure 3: Reproducibility diagnostics: non-basiccategories (Study II).,o4~-3r~~2~?
10 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1KFigure 4: Distribution by reproducibility (StudyII)TUAL.
This is an important result: as AIM sen-tences constitute the best characterization f theresearch paper for the summarization task we areparticularly interested in having them annotatedconsistently in our training material.
The anno-tators were less good at determining BASIS andCONTRAST.
This might have to do with the loca-tion of those types of sentences in the paper: AIMand TEXTUAL are usually found at the beginningor end of the introduction section, whereas CON-TRAST, and even more so BASIS, are usually in-terspersed within longer stretches of OWN.
As aresult, these categories are more exposed to lapsesof attention during annotation.If we blur the less important distinctions be-tween CONTRAST, OTHER, and BACKGROUND,the reproducibility of the scheme increases toK=.75.
Structuring our training set in this wayseems to be a good compromise for our task, be-cause with high reliability, it would still give usthe crucial distinctions contained in the basic an-notation scheme, plus the highly important AIMsentences, plus the useful TEXTUAL and BASISsentences.The variation in reproducibility across papers islarge, both in Study I and Study II (cf.
the quasi-bimodal distribution shown in Figure 4).
Somehypotheses for why this might be so are the fol-0.90.8K 0.70.60.5none low highFigure 5: Effect of self-citation ratio on repro-ducibility (Study I)lowing:?
One problem our annotators reported was adifficulty in distinguishing OTHEa work fromOWN work, due to the fact that some authorsdid not express a clear distinction betweenprevious own work (which, according to ourinstructions, had to be coded as OTHEa) andcurrent, new work.
This was particularly thecase where authors had published several pa-pers about different aspects of one piece ofresearch.
We found a correlation with self ci-tation ratio (ratio of self citations to all cita-tions in running text): papers with many selfcitations are more difficult to annotate thanpapers that have few or no self citations (cf.Figure 5).?
Another persistent problematic distinctionfor our annotators was that between OWNand BACKGROUND.
This could be a sign thatsome authors aimed their papers at an expertaudience, and thus thought it unnecessary tosignal clearly which statements are commonlyagreed in the field, as opposed to their ownnew claims.
If a paper is written in such away, it can indeed only be understood witha considerable amount of domain knowledge,which our annotators did not have.?
There is also a difference in reproducibil-ity between papers from different conferencetypes, as Figure 6 suggests.
Out of our 25 pa-pers, 4 were presented in student sessions, 4came from workshops, the remaining 16 oneswere main conference papers.
Student sessionpapers are easiest o annotate, which mightbe due to the fact that they are shorter andhave a simpler structure, with less mentionsof previous research.
Main conference pa-pers dedicate more space to describing and114Proceedings of EACL '990.80.70,5:!!i~?:?
i : ; .
:L:Mai~ conf.
Student  Wad(shopFigure 6: Effect of conference type on repro-ducibility (Study II)criticising other people's work than studentor workshop papers (on average about onefourth of the paper).
They seem to be care-fully prepared (and thus easy to annotate);conference authors must express themselvesmore clearly than workshop authors becausethey are reporting finished work to a wideraudience.3.2  S tudy  I I IFor Study III, we used a different subject pool:18 subjects with no prior annotation training.
Allof them had a graduate degree in Cognitive Sci-ence, with two exceptions: one was a graduatestudent in Sociology of Science; and one was a sec-retary.
Subjects were given only minimal instruc-tions (1 page A4), and the decision tree in Fig-ure 2.
Each annotator was randomly assigned to agroup of six, all of whom independently annotatedthe same single paper.
These three papers wererandomly chosen from the set of papers for whichour trained annotators had previously achievedgood reproducibility in Study II (K=.65,N=205,k=3; K=.85,N=192,k=3; K=.87,N=144,k=3, re-spectively).Reproducibility varied considerably betweengroups (K=.35, N=205, k=6; K=.49, N=192,k=6; K=.72, N=144, k=6).
Kappa is designedto abstract over the number of coders.
Lower reli-ablity for Study III as compared to Studies I andII is not an artefact of how K was calculated.Some subjects in Group 1 and 2 did not un-derstand the instructions as intended - we mustconclude that our very short instructions did notprovide enough information for consistent anno-tation.
This is not surprising, given that humanindexers (whose task is very similar to the taskintroduced here) are highly skilled professionals.However, part of this result can be attributed tothe papers: Group 3, which annotated the pa-per found to be most reproducible in Study II,performed almost as well as trained annotators;Group 1, which performed worst, also happenedto have the paper with the lowest reproducibil-ity.
In Groups 1 and 2, the most similar threeannotators reached a respectable reproducibility(K=.5, N=205, k=3; K=.63, N=192, k=3).
That,together with the good performance of Group 3,seems to show that the instructions did at leastconvey some of the meaning of the categories.It is remarkable that the two subjects who hadno training in computational linguistics performedreasonably well: they were not part of the circleof the three most similar subjects in their groups,but they were also not performing worse than theother two annotators.4 D iscuss ionIt is an interesting question how far shallow (hu-man and automatic) information extraction meth-ods, i.e.
those using no domain knowledge, can besuccessful in a task such as ours.
We believe thatargumentative structure has so many reliable lin-guistic or non-linguistic orrelates on the surface- physical layout being one of these correlates,others are linguistic indicators like "to our knowl-edge" and the relative order of the individual ar-gumentative moves - that it should be possible todetect he line of argumentation f a text withoutmuch world knowledge.
The two non-experts inthe subject pool of Study III, who must have usedsome other information besides computational lin-guistics knowledge, performed satisfactorily - afact that seems to confirm the promise of shallowmethods.Overall, reproducibility and stability for trainedannotators does not quite reach the levels foundfor, for instance, the best dialogue act codingschemes (around K=.80).
Our annotation re-quires more subjective judgments and is possi-bly more cognitively complex.
Our reproducibilityand stability results are in the range which Krip-pendorff (1980) describes as giving marginally sig-nificant results for reasonable size data sets whencorrelating two coded variables which would showa clear correlation if there were prefectly agree-ment.
That is, the coding contains enough signalto be found among the noise of disagreement.Of course, our requirements are rather lessstringent han Krippendorff's because only onecoded variable is involved, although coding is ex-pensive enough that simply building larger datasets is not an attractive option.
Overall, we findthe level of agreement which we achieved accept-able.
However, as with all coding schemes, itsusefulness will only be clarified by the final appli-115Proceedings of EACL '99cation.The single most surprising result of the experi-ments is the large variation in reproducibility be-tween papers.
Intuitively, the reason for this arequalitative differences in individual writing style- annotators reported that some papers are bet-ter structured and better written than others, andthat some authors tend to write more clearly thanothers.
It would be interesting to compare our re-producibility results to independent quality judge-ments of the papers, in order to determine if ourexperiments can indeed measure the clarity of sci-entific argumentation.Most of the problems we identified in our stud-ies have to do with a lack of distinction betweenown and other people's work (or own previouswork).
Because our scheme discriminates basedon these properties, as well as being useful forsummarizing research papers, it might be used forautomatically detecting whether a paper is a re-view, a position paper, an evaluation paper or a'pure' research article by looking at the relativefrequencies of automatically annotated categories.5 ConclusionsWe have introduced an annotation scheme for re-search articles which marks the aims of the pa-per in relation to past literature.
We have ar-gued that this scheme is useful for building betterabstracts, and have conducted some experimentswhich show that the annotation scheme can belearned by trained annotators and subsequentlyapplied in a consistent way.
Because the schemeis reliable, hand-annotated data can be used totrain a system which applies the scheme automat-ically to unseen text.The novel aspects of our scheme are that it ap-plies to different kinds of scientific research arti-cles, because it relies on the form and meaningof argumentative aspects found in the text typerather than on contents or physical format.
Assuch, it should be independent of article lengthand article discipline.
In the future, we planto show this by applying our scheme to journaland conference articles from a range of disciplines.Practical reasons have kept us from using journalarticles as data so far (namely the difficulty of cor-pus collection and the increased length and subse-quent time effort of human experiments), but weare particularly interested in them as they can beexpected to be of higher quality.
As the basic ar-gumentation is the same as in conference articles,our scheme should be applicable to journal arti-cles at least as consistently as to the papers in ourcurrent collection.6 AcknowledgementsWe wish to thank our annotators, VasilisKaraiskos and Ann Wilson, for their patience anddiligence in this work, and for their insightful, crit-ical, and very useful observations.The first author is supported by an EPSRC stu-dentship.ReferencesJan Alexandersson, Elisabeth Mater, and Norbert Re-ithinger.
1995.
A robust and efficient hree-layereddialogue component for a speech-to-speech transla-tion system.
In Proceedings of the Seventh Euro-pean Meeting of the ACL, pages 188-193.Ronald Brandow, Karl Mitze, and Lisa F. Rau.
1995.Automatic ondensation of electronic publicationsby sentence selection.
Information Processing andManagement, 31(5):675-685.Jean Carletta, Amy Isard, Stephen Isard, Jacque-line C. Kowtko, Gwyneth Doherty-Sneddon, andAnne H. Anderson.
1997.
The reliability of a dia-logue structure coding scheme.
Computational Lin-guistics, 23(1):13-31.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: the kappa statistic.
ComputationalLinguistics, 22(2):249-254.Robin Cohen.
1984.
A computational theory of thefunction of clue words in argument understanding.In Proceedings of COLING-8~, pages 251-255.Anna Duszak.
1994.
Academic discourse and intellec-tual styles.
Journal of Pragmatics, 21:291-313.Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca, 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.University of Colorado, Institute of Cognitive Sci-ence.
TR-97-02.Joost G. Kircz.
1991.
The rhetorical structure of sci-entific articles: the case for argumentational analy-sis in information retrieval.
Journal of Documenta-tion, 47(4):354-372.Klaus Krippendorff.
1980.
Content analysis: an in-troduction to its methodology.
Sage Commtext se-ries; 5.
Sage, Beverly Hills London.Julian Kupiec, Jan O. Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of the 18th ACM-SIGIR Conference, Asso-ciation for Computing Machinery, Special InterestGroup Information Retrieval, pages 68-73.Elizabeth DuRoss Liddy.
1991.
The discourse-levelstructure of empirical abstracts: an exploratorystudy.
Information Processing and Management,27(1):55-81.116Proceedings of EACL '99William C. Mann and Sandra A. Thompson.
1987.Rhetorical structure theory: description and con-struction of text structures.
In G. Kempen, edi-tor, Natural Language Generation: New Results inArtificial Intelligence, Psychology and Linguistics,pages 85-95, Dordrecht.
Nijhoff.Daniel Marcu.
1997.
From discourse structures totext summaries.
In Inderjeet Mani and Mark T.Maybury, editors, Proceedings of the workshop onIntelligent Scalable Text Summarization, in associ-ation with A CL//BA CL- 97.Greg Myers.
1992.
In this paper we report... - speechacts and scientific facts.
Journal of Pragmatics,17(4):295-313.Chris D. Paice.
1990.
Constructing literature ab-stracts by computer: techniques and prospects.Information Processing and Management, 26:171-186.G.J Rath, A. Resnick, and T. R. Savage.
1961.
Theformation of abstracts by the selection of sentences.American Documentation, 12(2):139-143.Lisa F. Rau, Paul S. Jacobs, and Uri Zernik.
1989.Information extraction and text processing usinglinguistic knowledge acquisition.
Information Pro-cessing and Management, 25(4):419-428.Sidney Siegel and N.J. Jr. Castellan.
1988.
Non-parametric statistics for the Behavioral Sciences.McGraw-Hill, second edition edition.Karen Sp~ixck Jones.
1998.
Automatic summarising:factors and directions.
In ACL/EACL-97 Work-shop 'Intelligent Scalable Text Summarization'.John Swales.
1990.
Genre analysis: English in aca-demic and research settings.
Cambridge UniversityPress.Geoff Thompson and Yiyun Ye.
1991.
Evaluation inthe reporting verbs used in academic papers.
Ap-plied Linguistics, 12(4):365-382.Sheryl R. Young and Phillip J. Hayes.
1985.
Auto-matic classification and summarization of bankingtelexes.
In Proceedings of the Second Conference onArtificial Intelligence Applications.117
