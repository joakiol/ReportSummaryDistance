First Joint Conference on Lexical and Computational Semantics (*SEM), pages 49?53,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics?Could you make me a favour and do coffee, please??
:Implications for Automatic Error Correction in English and DutchSophia KatrenkoUiL-OTSUtrecht Universitys.katrenko@uu.nlAbstractThe correct choice of words has proven chal-lenging for learners of a second language anderrors of this kind form a separate categoryin error typology.
This paper focuses on oneknown example of two verbs that are oftenconfused by non-native speakers of Germaniclanguages, to make and to do.
We conduct ex-periments using syntactic information and im-mediate context for Dutch and English.
Ourresults show that the methods exploiting syn-tactic information and distributional similarityyield the best results.1 IntroductionWhen learning a second language, non-native speak-ers make errors at all levels of linguistic analy-sis, from pronunciation and intonation to languageuse.
Word choice errors form a substantial partof all errors made by learners and may also beobserved in writing or speech of native speak-ers.
This category of errors includes homophones.Some commonly known confusions in English areaccept-except, advice-advise, buy-by-bye, ate-eight,to name but a few.
Other errors can be explainedby a non-native speaker?s inability to distinguish be-tween words because there exists only one corre-sponding word in their native language.
For ex-ample, Portuguese and Spanish speakers have diffi-culties to differentiate between te doen (to do) andte maken (to make), and Turkish between kunnen(can), weten (to know) and kennen (to know) inDutch (Coenen et al, 1979).
Adopting terminol-ogy from Golding and Roth (1999) and Rozovskayaand Roth (2010), do/make and kunnen/kennen/wetenform two confusion sets.
However, unlike the caseof kunnen/kennen/weten, where the correct choice isoften determined by syntactic context 1, the choicebetween to make and to do can be motivated bysemantic factors.
It has been argued in the litera-ture that the correct use of these verbs depends onwhat is being expressed: to do is used to refer todaily routines and activities, while to make is used todescribe constructing or creating something.
Sinceword choice errors have different nature, we hypoth-esize that there may exist no uniform approach tocorrect them.State-of-the-art spell-checkers are able to detectspelling and agreement errors but fail to find wordsused incorrectly, e.g.
to distinguish to make from todo.
Motivated by the implications that the correctprediction of two verbs of interest may have for au-tomatic error correction, we model the problem ofchoosing the correct verb in a similar vein to selec-tional preferences.
The latter has been consideredfor a variety of applications, e. g. semantic role la-beling (Zapirain et al, 2009).
Words such as be ordo have been often excluded from consideration be-cause they are highly polysemous and ?do not selectstrongly for their arguments?
(McCarthy and Car-roll, 2003).
In this paper, we study whether semanticclasses of arguments may be used to determine thecorrect predicate (e.g., to make or to do) and con-sider the following research questions:1.
Can information on semantic classes of direct1Kunnen is a modal verb followed by the main verb, kennentakes a direct object as in, e.g., to know somebody, and weten isoften followed by a clause (as in I know that).49objects potentially help to correct verb choiceerrors?2.
How do approaches using contextual and syn-tactic information compare when predicting tomake vs. to do?The paper is organised as follows.
Section 2.1discusses the methods, followed by Section 2.2 ondata.
The experimental findings are presented inSection 2.3.
We conclude in Section 3.2 ExperimentsWe re-examine several approaches to selectionalpreferences in the context of error correction.
Ex-isting methods fall into one of two categories, eitherthose relying on information from WordNet (Mc-Carthy and Carroll, 2003), or data-driven (Erk,2007; Schulte im Walde, 2010; Pado et al, 2007).For the purpose of our study, we focus on the latter.2.1 MethodsFor each verb in question, we have a frequency-based ranking list of nouns co-occurring with it(verb-object pairs) which we use for the first twomethods.Latent semantic clustering (LSC) Rooth etal.
(1999) have proposed a soft-clustering method todetermine selectional preferences, which models thejoint distribution of nouns n and verbs v by condi-tioning them on a hidden class c. The probability ofa pair (v, n) then equalsP (v, n) =?c?CP (c)P (v|c)P (n|c) (1)Similarity-based method The next classifier weuse combines similarity between nouns with rank-ing information and is a modification of the methoddescribed in (Pado et al, 2007).
First, for all wordsni on the ranking list their frequency scores are nor-malised between 0 and 1, fi.
Then, they are weighedby the similarity score between a new noun nj and acorresponding word on the ranking list, ni, and thenoun with the highest score (1-nearest neighbour) isselected:argmaxnifi ?
sim(nj , ni) (2)Finally, two highest scores for each verb?s rankinglist are compared and the verb with higher score isselected as a preferred one.In addition, if we sum over all seen words insteadof choosing the nearest neighbour, this will lead tothe original approach by Pado et al (2007).
In theexperimental part we consider both approaches (theoriginal method is referred to as SMP while thenearest neighbour approach is marked by SMknn)and study whether there is any difference betweenthe two when a verb that allows many different ar-guments is considered (e.g., it may be better to usethe nearest neighbour approach for to do rather thanaggregating over all similarity scores).Bag-of-words (BoW) approach This widely usedapproach to document classification considers con-textual words and their frequencies to represent doc-uments (Zellig, 1954).
We restrict the length of thecontext around two verbs (within a window of ?2and ?3 around the focus word, make or do) andbuild a Naive Bayes classifier.2.2 DataBoth verbs, to make and to do, license complementsof various kinds, e. g. they can be mono-transitive,ditransitive, and complex transitive (sentences 1, 2,and 3, respectively).
Furthermore, make can be partof idiomatic ditransitives (e.g., make use of, makefun of, make room for) and phrasal mono-transitives(e.g., make up) .1.
Andrew made [a cake]dobj .2.
Andrew made [his mum]iobj [a cake]dobj .3.
Andrew made [his mum]dobj happy.For English, we use one of the largest cor-pora available, the PukWAC (over 2 billion words,30GB) (Baroni et al, 2009), which has been parsedby MaltParser (Nivre and Scholz, 2004).
We extractall sentences with to do or to make (based on lem-mata).
The verb to make occurs in 2,13% of sen-tences, and the verb to do in 3,27% of sentences inthe PukWAC corpus.
Next, we exclude from con-sideration phrasal mono-transitives and select sen-tences where verb complements are nouns (Table 1).For experiments in Dutch, we use the ?WikipediaDump Of 2010?
corpus, which is a part of LassyLarge corpus (159 million tokens), and is parsed by50LANG # sent # dobj (to make) # dobj (to do)EN 181,813,571 1,897,747 881,314NL 8,639,837 15,510 6,197Table 1: The number of sentences in English (EN) and Dutch (NL) corpora (the last two columns correspond to thenumber of sentences where direct objects are nouns).the Alpino parser (Bouma et al, 2001).
Unlike inEnglish data, to make occurs here more often thanto do (3,3% vs. 1%).
This difference can be ex-plained by the fact that to do is also an auxiliary verbin English which leads to more occurrences in to-tal.
Similarly to the English data set, phrasal mono-transitives are filtered out.
Finally, the sentencesthat contain either to make or to do from wiki01 upto wiki07 (19,847 sentences in total) have been se-lected for training and wiki08 (1,769 sentences intotal) for testing.
To be able to compare our resultsagainst the performance on English data, we samplea subset from PukWAC which is of the same size asDutch data set and is referred to as EN (sm).To measure distributional similarity for the near-est neighbour method, we use first-order andsecond-order similarity based on Lin?s informationtheoretic measure (Lin, 1998).
For both languages,similarity scores have been derived given a subsetof Wikipedia (276 million tokens for English and114 million tokens for Dutch) using the DISCOAPI (Kolb, 2009).2.3 ResultsTable 2 and Table 3 summarize our results.
When re-ferring to similarity-based methods, the symbols (f)and (s) indicate first-order and second-order similar-ity.
For the BoW models, ?2 and ?3 correspondsto the context length.
The performance is measuredby true positive rate (TP) per class, overall accuracy(Acc) and coverage (Cov).
The former indicates inhow many cases the correct class label (make or do)has been predicted, while the latter shows how manyexamples a system was able to classify.
Coverage isespecially indicative for LCS and semantic similar-ity approaches because they may fail to yield pre-dictions.
For these methods, we provide two evalua-tions.
First, in order to be able to compare resultsagainst the BoW approach, we measure accuracyand coverage on all test examples.
In such a case,if some direct objects occur very often in the test setand are classified correctly, accuracy scores will beboosted.
Therefore, we also provide the second eval-uation where we measure accuracy and coverage on(unique) test examples regardless of how frequentthey are.
This evaluation will give us a better in-sight into how well LCS and similarity-based meth-ods work.
Finally, we tested several settings for theLSC method and the results presented here are ob-tained for 20 clusters and 50 iterations.
We removestop words 2 but do not take any other preprocessingsteps.For both languages, it is more difficult to predictto do than to make, although the differences in per-formance on Dutch data (NL) are much smaller thanon English data (EN (sm)).
An interesting obser-vation is that using second-order similarity slightlyboosts performance for to make but is highly unde-sirable for predicting to do (decrease in accuracy foraround 15%) in Dutch.
This may be explained by thefact that the objects of to do are already very generic.Our findings on English data are that the similarity-based approach is more sensitive to the choice ofaggregating over all words in the training set or se-lecting the nearest neighbour.
In particular, we ob-tained better performance when choosing the nearestneighbour for to do but aggregating over all scoresfor to make.
The results on Dutch and English dataare in general not always comparable.
In additionto the differences in performance of similarity-basedmethods, the BoW models work better for predictingto do in English but to make in Dutch.As expected, similarity-based approaches yieldhigher coverage than LSC, although the latter is su-perior in terms of accuracy (in all cases but to doin English).
Since LSC turned out to be the mostcomputationally efficient method, we have also runit on larger subsets of the PukWAC data set, up tothe entire corpus.
We have not noticed any signifi-2We use stop word lists for English and Dutch from http://snowball.tartarus.org/algorithms/.51LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)EN (all) LSC 91.70 98.75 73.40 97.16 85.90 98.24EN (sm) LSC 89.81 90.00 75.81 86.70 86.91 89.30SMP (f) 84.89 98.82 69.89 95.14 81.78 98.03SMP (s) 82.92 98.82 55.65 95.14 77.27 98.03SMknn (f) 62.61 98.82 91.13 95.14 68.52 98.03SMknn (s) 4.36 98.82 99.46 95.14 24.07 98.03BoW ?2 36.41 100 82.21 100 46.01 100BoW ?3 32.26 100 84.10 100 43.13 100NL LSC 98.75 91.79 95.74 93.37 98.09 92.13SMP (f) 95.64 95.82 92.97 98.14 95.06 96.32SMP (s) 97.52 95.82 76.75 98.14 93.00 96.32SMknn (f) 94.14 95.82 92.97 98.14 93.89 96.32SMknn (s) 96.09 95.82 78.64 98.14 92.30 96.32BoW ?2 89.34 100 61.19 100 83.44 100BoW ?3 91.06 100 54.18 100 83.32 100Table 2: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)and Dutch (NL) data.LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)EN (sm) LSC 80.88 77.12 52.60 74.76 73.73 76.51SMP (f) 73.17 97.29 45.99 90.78 66.49 95.60SMP (s) 77.00 97.29 33.69 90.78 66.36 95.60SMknn (f) 31.18 97.29 82.35 90.78 43.76 95.60SMknn (s) 4.36 98.82 98.93 90.78 25.76 95.60NL LSC 94.85 63.40 86.59 76.64 92.39 66.83SMP (f) 87.55 81.37 77.00 93.45 84.24 84.50SMP (s) 91.16 81.37 54.00 93.45 80.52 84.50SMknn (f) 80.72 81.37 76.00 93.45 79.66 84.50SMknn (s) 85.54 81.37 55.00 93.45 76.79 84.50Table 3: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)and Dutch (NL) unique direct objects.cant changes in performance; the results for the en-tire data set, EN (all), are given in the first row ofTable 2.
Table 3 shows the results for the methodsusing direct object information on unique objects,which gives a more realistic assessment of their per-formance.
At closer inspection, we noticed thatmany non-classified cases in Dutch refer to com-pounds.
For instance, bluegrassmuziek (bluegrassmusic) cannot be compared against known words inthe training set.
In order to cover such cases, existingmethods may benefit from morphological analysis.3 ConclusionsIn order to predict the use of two often confusedverbs, to make and to do, we have compared twomethods to modeling selectional preferences againstthe bag-of-words approach.
The BoW method is al-ways outperformed by LCS and similarity-based ap-proaches, although the differences in performanceare much larger for to do in Dutch and for to makein English.
In this study, we do not use any corpus ofnon-native speakers?
errors and explore how well itis possible to predict one of two verbs provided thatthe context words have been chosen correctly.
In thefuture work, we plan to label all incorrect uses of tomake and to do and to correct them.AcknowledgmentsThe author thanks anonymous reviewers for their valu-able comments.
This work is supported by a VICI grantnumber 277-80-002 by the Netherlands Organisation forScientific Research (NWO).52ReferencesMarco Baroni and Silvia Bernardini and Adriano Fer-raresi and Eros Zanchetta.
2009.
The WaCky WideWeb: A Collection of Very Large Linguistically Pro-cessed Web-Crawled Corpora.
Language Resourcesand Evaluation 43(3), pp.
209-226.Gosse Bouma, Gertjan van Noord, and Robert Malouf.2001.
Alpino: Wide-coverage Computational Analysisof Dutch.
In Computational Linguistics in the Nether-lands 2000.
Enschede.Jose?e A. Coenen, W. van Wiggen, and R. Bok-Bennema.1979.
Leren van fouten: een analyse van de meestvoorkomende Nederlandse taalfouten, die gemaaktworden door Marokkaanse, Turkse, Spaanse en Por-tugese kinderen.
Amsterdam: Stichting ABC, Contac-torgaan voor de Innovatie van het Onderwijs.Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of ACL 2007.Prague, Czech Republic, 2007.Andrew R. Golding and Dan Roth.
1999.
A Winnow-Based Approach to Context-Sensitive Spelling Correc-tion.
Machine Learning 34(1-3), pp.
107-130.Peter Kolb.
2009.
Experiments on the difference be-tween semantic similarity and relatedness.
In Pro-ceedings of the 17th Nordic Conference on Compu-tational Linguistics - NODALIDA ?09, Odense, Den-mark, May 2009.Dekang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING-ACL1998, Montreal.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automaticallyacquired selectional preferences.
Computational Lin-guistics, 29(4), pp.
639-654.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedings ofCOLING 04.Sebastian Pado?, Ulrike Pado?
and Katrin Erk.
2007.
Flex-ible, Corpus-Based Modelling of Human PlausibilityJudgements.
In Proceedings of EMNLP/CoNLL 2007.Prague, Czech Republic, pp.
400-409.Mats Rooth, Stefan Riezler and Detlef Prescher.
1999.Inducing a Semantically Annotated Lexicon via EM-Based Clustering.
In Proceedings of ACL 99.Anna Rozovskaya and Dan Roth.
2010.
GeneratingConfusion Sets for Context-Sensitive Error Correction.In Proceedings of EMNLP, pp.
961-970.Sabine Schulte im Walde.
2010.
Comparing Com-putational Approaches to Selectional Preferences ?Second-Order Co-Occurrence vs.
Latent SemanticClusters.
In Proceedings of the 7th International Con-ference on Language Resources and Evaluation, Val-letta, Malta, pp.
1381?1388.Ben?at Zapirain, Eneko Agirre and Llu?
?s Ma`rquez.
2009.Generalizing over Lexical Features: Selectional Pref-erences for Semantic Role Classification.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers.
Suntec, Singapore, pp.
73-76.Harris Zellig.
1954.
Distributional Structure.
Word 10(2/3), p. 146-62.53
