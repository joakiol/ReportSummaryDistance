STRUCTURAL DISAMBIGUATION WITHCONSTRAINT PROPAGATIONHi rosh i  MaruyamaIBM Research ,  Tokyo  Research  Laboratory5-19 Sanbancho ,  Ch iyoda-ku ,Tokyo  102 Japanmaruyama@jpntscvm.b i tnetAbst ractWe present a new grammatical formalism called Con-straint Dependency Grammar (CDG) in which everygrammatical rule is given as a constraint on word-to-word modifications.
CDG parsing is formalizedas a constraint satisfaction problem over a finite do-main so that efficient constraint-propagation algo-rithms can be employed to reduce structural am-biguity without generating individual parse trees.The weak generative capacity and the computationalcomplexity of CDG parsing are also discussed.1 INTRODUCTIONWe are interested in an efficient reatment of struc-tural ambiguity in natural language analysis.
It isknown that "every-way" ambiguous constructs, uchas prepositional ttachment in English, have a Cata-lan number of ambiguous parses (Church and Patil1982), which grows at a faster than exponential rate(Knuth 1975).
A parser should be provided witha disambiguation mechanism that does not involvegenerating such a combinatorial number of parsetrees explicitly.We have developed a parsing method in which anintermediate parsing result is represented as a datastructure called a constraint network.
Every solutionthat satisfies all the constraints simultaneously corre-sponds to an individual parse tree.
No explicit parsetrees are generated until ultimately necessary.
Pars-ing and successive disambiguation are performed byadding new constraints to the constraint network.Newly added constraints are efficiently propagatedover the network by Constraint Propagation (Waltz1975, Montanari 1976) to remove inconsistent values.In this paper, we present the basic ideas of aformal grammatical theory called Constraint Depen-dency Grammar (CDG for short) that makes thisparsing technique possible.
CDG has a reasonabletime bound in its parsing, while its weak generativecapacity is strictly greater than that of Context FreeGrammar (CFG).We give the definition of CDG in the next section.Then, in Section 3, we describe the parsing methodbased on constraint propagation, using a step-by-step example.
Formal properties of CDG are dis-cussed in Section 4.312 CDG:  DEF IN IT IONLet a sentence s = wlw2 .
.
.
w,, be a finite string ona finite alphabet E. Let R -- { r l , r2 , .
.
.
, rk}  be afinite set of role-iris.
Suppose that each word i in asentence s has k-different roles rl( i) ,  r2(i) .
.
.
.
, rk(i).Roles are like variables, and each role can have a pair<a, d> as its value, where the label a is a member ofa finite set L = {a l ,a2 , .
.
.
,a t}  and the modifiee dis either 1 < d < n or a special symbol n i l .
Ananalysis of the sentence s is obtained by assigningappropriate values to the n x k roles (we can regardthis situation as one in which each word has a framewith k slots, as shown in Figure 1).An assignment A of a sentence s is a function thatassigns values to the roles.
Given an assignment A,the label and the modifiee of a role x are determined.We define the following four functions to representthe various aspect of the role x, assuming that x isan rj-role of the word i:rt-roler=-roleW~ W= W.I I \ [ - - - \ ]I I l -  Ir.-ro,e I I 1"  t I IFigure 1: Words and their roles.?
pos(x)~ f the position i?
r id(x)~ r the role id rj?
lab(x)d-~ f the label of x?
mod(x)d-~ f the modifiee of xWe also define word(i) as the terminal symboloccurring at the position i.
1An individual grammar G =< ~, R, L, C > in theCDG theory determines a set of possible assignmentsof a given sentence, where?
~ is a finite set of terminal symbols.?
R is a finite set of role-ids.?
L is a finite set of labels.?
C is a constraint hat an assignment A shouldsatisfy.A constraint C is a logical formula in a formVxlx2...xp : role; PI&P2&...&P,~where the wHables Xl, x2, ..., xp range over the setof roles in an assignment A and each subformula P~consists only of the following vocabulary:?
Variables: xl,  x2, ..., xp?
Constants: elements and subsets ofE U L U RU {n i l ,  l ,2 , .
.
.}?
Function symbols: word(), posO, rid(), lab(),and modOl In  this paper, when referring to a word, we purposely usethe position (1,2,...,n) of the word rather than the word itself(Wl,W2, ,--,Wn), because the same word can occur in manydifferent positions in a sentence.
For readability, however, wesometimes use the notation word~os~tion.?
Predicate symbols: =, <, >, and E?
Logical connectors: &, l, "~, andSpecifically, we call a subformula Pi  a unary con-straint when P.i contains only one variable, and abinary constraint when Pi contains exactly two vari-ables.The semantics of the functions have been definedabove.
The semantics of the predicates and the logi-cal connectors are defined as usual, except that com-paring an expression containing n i l  with anothervalue by the inequality predicates always yields thetruth value false.These conditions guarantee that, given an assign-ment A, it is possible to compute whether the valuesof xl,  x2 .
.
.
.
, xp satisfy C in a constant ime, regard-less of the sentence length n.Definit ion?
The degree of a grammar G is the size k of therole-id set R.?
The arity of a grammar G is the number of vari-ables p in the constraint C.Unless otherwise stated, we deal with only ar-i ty -2  cases.?
A nonnull string s over the alphabet ~ is gener-ated iff there exits an assignment A that satisfiesthe constraint C.?
L(G) is a language generated by the grammar Giff L(G) is the set of all sentences generated bya grammar G.ExampleLet us consider G1 =< E1,R1,L1,C1 > where?
=?
R1 = {governor}?
n l  = {DET,SUBJ,ROOT}?
C1 = Vxy : role; P1.The formula P1 of the constraint C1 is the con-junction of the following four subformulas (an infor-mal description is attached to each constraint):GI-1) word(pos(x))=D ~ ( lab(x)=DgT,word(mod(x))=N, pos(x) < rood(x) )"A determiner (D) modifies a noun (N) on theright with the label DET.
"32Role Valuegovernor( "al" )governor("dog2")governor( "runs3" )<DET,2><SUBJ,3><R00T,nil>Figure 2: Assignment Satisfying (GI-1) to (G1-4)~SUB3(G1-2) word(pos(x))=N ~ ( lab(x)=SUBJ,word(mod(x))=V, pos(x) < mod(x) )"A noun modifies a verb (V) on the right withthe label SUBJ.
"(G1-3) word(pos(x))=V ~ ( lab(x)=ROOT,mod(x)=nil )"A verb modifies nothing and its label shouldbe ROOT.
"(G1-4) (mod(x)=mod(y), lab(x)=lab(y)  ~ x=y"No two words can modify the same word withthe same label.
"Analyzing a sentence with G1 means assigninga label-modifiee pair to the only role "governor" ofeach word so that the assignment satisfies (GI-1) to(G1-4) simultaneously.
For example, sentence (1)is analyzed as shown in Figure 2 provided that thewords "a," "dog," and "runs" are given parts-of-speech D, N, and V, respectively (the subscript at-tached to the words indicates the position of the wordin the sentence).
(1) A1 dog2 runs3.Thus, sentence (1) is generated by the grammarG1.
On the other hand, sentences (2) and (3) arenot generated since there are no proper assignmentsfor such sentences.
(2) A runs.
(3) Dog dog runs.We can graphically represent the parsing result ofsentence (1) as shown in Figure 3 if we interpret thegovernor ole of a word as a pointer to the syntacticgovernor of the word.
Thus, the syntactic structureproduced by a CDG is usually a dependency structure(Hays 1964) rather than a phrase structure.Figure 3: Dependency tree3 PARSING WITHCONSTRAINT  PROPAGATIONCDG parsing is done by assigning values to n ?
kroles, whose values are selected from a finite setL x {1,2, .
.
.
,n,  ni l}.
Therefore, CDG parsing canbe viewed as a constraint satisfaction problem overa finite domain.
Many interesting artificial intelli-gence problems, including raph coloring and scenelabeling, are classified in this group of problems, andmuch effort has been spent on the development ofefficient techniques to solve these problems.
Con-straint propagation (Waltz 1975, Montanari 1976),sometimes called filtering, is one such technique.
Oneadvantage of the filtering algorithm is that it allowsnew constraints to be added easily so that a bettersolution can be obtained when many candidates re-main.
Usually, CDG parsing is done in the followingthree steps:1.
Form an initial constraint network using a"core" grammar.2.
Remove local inconsistencies byfiltering.3.
If any ambiguity remains, add new constraintsand go to Step 2.In this section, we will show, through a step-by-stepexample, that the filtering algorithms can be effec-tively used to narrow down the structural ambigui-ties of CDG parsing.The ExampleWe use a PP-attachment example.
Consider sen-tence (4).
Because of the three consecutive preposi-tional phrases (PPs), this sentence has many struc-tural ambiguities.
(4) Put the block on the floor on the table inthe room.33Pu._t the block on the floor on the table in the roomV, NI~ PP3 PP4 PPs~'rMO0Figure 4: Possible dependency structureOne of the possible syntactic structures i shownin Figure 42 .To simplify tile following discussion, we treat thegrammatical symbols V, NP, and PP as terminal sym-bols (words), since the analysis of the internal struc-tures of such phrases is irrelevant o the point be-ing made.
The correspondence b tween such simpli-fied dependency structures and the equivalent phrasestructures hould be clear.
Formally, the input sen-tence that we will parse with CDG is (5).
(5) V1 NP2 PP3 PP4 PP5First, we consider a "core" grammar that con-tains purely syntactic rules only.
We define a CDGG2a =< E2, R2, L2, C2 > as follows:?
E2  = {V,NP ,PP}?
R2 = {governor}?
L2  = {ROOT, 0B J, LOC,POSTMOD}?
C2 = Vxy : role; P2,L1 P2P3 L1 P2P3P41 11  Rnil 1 111  Rnil{Rn,I}/-A--~ 1 / ( '~  {L1P2 p3 p4}' ' ' , .
, .2 .3 .
./,L1 01  1P21111Figure 5: Initial constraint network (the values Rnil,L1, P2, ... should be read as <ROOT,nil>, <LOC,I>,<POSTMOD,2>, ..., and so on.
)(G2a-4) word(pos(x))=NP =~ ( word(mod(x))=V,lab(x)=OBJ, mod(x) < pos(x) )"An NP modifies a V on the left with the labelOBJ.
"(G2a-5) word(pos(x))=V ~ ( mod(x)=nil,lab(x)=KOOT )"A Y modifies nothing with the label ROOT.
"(G2a-6) mod(x) < pos(y) < pos(x) =~mod(x) < mod(y) < pos(x)"Modification links do not cross each other.
"where the formula P2 is the conjunction of thefollowing unary and binary constraints :(G2a-1) word(pos(x))=PP ~ (word(mod(x)) 6{PP,NP,V}, rood(x) < pos(x) )"A PP modifies a PP, an NP, or a V on the left.
"(G2a-2) word(pos(x))=PP, word(rood(x)) 6 {PP,NP}lab(x)=POSTMOD"If a PP modifies a PP or an NP, its label shouldbe POSTMOD.
"(G2a-3) word(pos(x) )=PP, word(mod(x) )=Vlab(x) =LOC"If a PP modifies a V, its label should be L0?.
"2In linguistics, arrows are usually drawn in the oppositedirection in a dependency diagram: from a governor (modifiee)to its dependent (modifier).
In this paper, however, we drawan arrow from a modifier to its modifiee in order to emphasizethat this information is contained in a modifier's role.According to the grammar G2a , sentence (5) has14 (= Catalan(4)) different syntactic structures.
Wedo not generate these syntactic structures one byone, since the number of the structures may growmore rapidly than exponentially when the sentencebecomes long.
Instead, we build a packed data struc-ture, called a constraint network, that contains allthe syntactic structures implicitly.
Explicit parsetrees can be generated whenever necessary, but itmay take a more than exponential computation time.Format ion  of init ial  networkFigure 5 shows the initial constraint network for sen-tence (5).
A node in a constraint network corre-sponds to a role.
Since each word has only one rolegovernor in the grammar G2, the constraint networkhas five nodes corresponding to the five words in the34sentence.
In the figure, the node labeled Vl repre-sents the governor ole of the word Vl, and so on.
Anode is associated with a set of possible values thatthe role can take as its value, called a domain.
Thedomains of the initial constraint network are com-puted by examining unary constraints ((G2a-1) to(G2a-5) in our example).
For example, the modifieeof the role of the word Vl must be ROOT and its labelmust be n i l  according to the unary constraint (G2a-5), and therefore the domain of the correspondingnode is a singleton set {<R00T,nil>).
In the figure,values are abbreviated by concatenating the initialletter of the label and the modifiee, such as Rni l  for<R00T,nil>, 01 for <0BJ,I>, and so on.An arc in a constraint network represents a bi-nary constraint imposed on two roles.
Each arcis associated with a two-dimensional matrix calleda constraint matlqx, whose xy-elements are either1 or 0.
The rows and the columns correspond tothe possible values of each of the two roles.
Thevalue 0 indicates that this particular combinationof role values violates the binary constraints.
Aconstraint matrix is calculated by generating everypossible pair of values and by checking its validityaccording to the binary constraints.
For example,the case in which governor(PP3) = <LOC,I> andgovernor(PP4) -- <POSTMOD,2> violates the binaryconstraint (G2a-6), so the L1-P2 element of the con-straint matrix between PPs and PPa is set to zero.The reader should not confuse the undirected arcsin a constraint network with the directed modifica-tion links in a dependency diagram.
An arc in aconstraint network represents the existence of a bi-nary constraint between two nodes, and has nothingto do with the modifier-modifiee r lationships.
Thepossible modification relationships are represented asthe modifiee part of the domain values in a constraintnetwork.A constraint network contains all the informationneeded to produce the parsing results.
No grammati-cal knowledge is necessary to recover parse trees froma constraint network.
A simple backtrack searchcan generate the 14 parse trees of sentence (5) fromthe constraint network shown in Figure 5 at anytime.
Therefore, we regard a constraint network asa packed representation f parsing results.F i l te r ingA constraint network is said to be arc consistent if,for any constraint matrix, there are no rows and nocolumns that contain only zeros.
A node value cor-responding to such a row or a column cannot partici-pate in any solution, so it can be abandoned withoutfurther checking.
The filtering algorithm identifiessuch inconsistent values and removes them from thedomains.
Removing a value from one domain maymake another value in another domain inconsistent,so the process is propagated over the network untilthe network becomes arc consistent.Filtering does not generate solutions, but may sig-nificantly reduce the search space.
In our example,the constraint network shown in Figure 5 is alreadyarc consistent, so nothing can be done by filtering atthis point.Add ing  New Const ra in tsTo illustrate how we can add new constraints to nar-row down the ambiguity, let us introduce additionalconstraints (G2b-1) and (G2b-2), assuming that ap-propriate syntactic and/or semantic features are at-tached to each word and that the function /e(i) isprovided to access these features.
(G2b-1) word(pos(x))=PP, on_table E \]e(pos(x))~(:floor e /e(mM(x)) )"A floor is not on a table.
"(G2b-2) lab(x)=LOC, lab(y)=LOC, mod(x)=mod(y),ward(mod(x) )--V ~ x=y"No verb can take two locatives.
"Note that these constraints are not purely syntac-tic.
Any kind of knowledge, syntactic, semantic, oreven pragmatic, can be applied in CDG parsing aslong as it is expressed as a unary or binary constrainton word-to-word modifications.Each value or pair of values is tested against henewly added constraints.
In the network in Figure 5,the value P3 (i.e.
<POSTMOD,3>) of the node PP4 (i.e.
;"on the table (PP4)" modifies "on the floor (PP3)") vi-olates the constraint (G2b-1), so we remove P3 fromthe domain of PP4.
Accordingly, corresponding rowsand columns in the four constraint matrices adjacentto the node PP4 are removed.
The binary constraint(G2b-2) affects the elements of the constraint ma-trices.
For the matrix between the nodes PP3 and35I L1 P2 P3 P4~i l i l  1 1 1{FInIIi{"UT'D_ 1 / ~ {L1 p2 p3 p4} / " T t  ' 'I \  , ,o .oo,/ \ /,.=")':~- / W P2tl 1 0 1I L1P2P3P_4 .~/  ,,.~.
!
L1 i"Z ~/~,.
\011'i, 1 '  J_.
~ \/Ftni, l ,  1 /  ~.__~_.
.S/ {L1,P2} xILl P2 P3 P4~ 2  L1 0"0 1 1P2 1 1 11Figure 6: Modified network!
L1 I P4Rnill 1 Rnill 1{O l}( .~/  ".~ S ~ {L1}!p2 ~011 1Figure 8: Unambiguous parsing resultFlnil L1P2P4 Rnill 1 1 1 1 1/o,,,, / \1 P2 1 IFigure 7: Filtered networkSince the sentence is still ambiguous, let us con-sider another constraint.
(G2c-1) Iab(x)=POSTMOD, lab(y)=POSTMOD,mod(x)=mod(y), on e fe(po~(x)), one fe(pos(y)) ~ x=y"No object can be on two distinct objects.
"This sets the P2-P2 element of the matrix PP3-PP4to zero.
Filtering on this network again results in thenetwork shown in Figure 8, which is unambiguous,since every node has a singleton domain.
Recoveringthe dependency structure (the one in Figure 4) fromthis network is straightforward.Re la ted  WorkPP4, the element in row L1 (<LOC,I>) and columnL1 (<LOC, 1>) is set to zero, since both are modifica-tions to Vl with the label LOC.
Similarly, the L1-L1elements of the matrices PP3-PP5 and PP4-PP5 areset to zero.
The modified network is shown in Fig-ure 6, where the updated elements are indicated byasterisks.Note that the network in Figure 6 is not arcconsistent.
For example, the L1 row of the matrixPP3-PP4 consists of all zero elements.
The filteringalgorithm identifies such locally inconsistent valuesand eliminates them until there are no more incon-sistent values left.
The resultant network is shownin Figure 7.
This network implicitly represents theremaining four parses of sentence (5).Several researchers have proposed variant data struc-tures for representing a set of syntactic structures.Chart (Kaplan 1973) and shared, packed for-est (Tomita 1987) are packed data structures forcontext-free parsing.
In these data structures, asubstring that is recognized as a certain phrase isrepresented as a single edge or node regardless ofhow many different readings are possible for thisphrase.
Since the production rules are context free,it is unnecessary to check the internal structure of anedge when combining it with another edge to forma higher edge.
However, this property is true onlywhen the grammar is purely context-free.
If one in-troduces context sensitivity by attaching augmenta-tions and controlling the applicability of the produc-tion rules, different readings of the same string with36the same nonterminal symbol have to be representedby separate dges, and this may cause a combinato-rial explosion.Seo and Simmons (1988) propose adata structurecalled a syntactic graph as a packed representation fcontext-free parsing.
A syntactic graph is similar to aconstraint network in the sense that it is dependency-oriented (nodes are words) and that an exclusion ma-trix is used to represent the co-occurrence onditionsbetween modification links.
A syntactic graph is,however, built after context-free parsing and is there-fore used to represent only context-free parse trees.The formal descriptive power of syntactic graphs isnot known.
As will be discussed in Section 4, theformal descriptive power of CDG is strictly greaterthan that of CFG and hence, a constraint networkcan represent non-context-free parse trees as well.Sugimura et al (1988) propose the use of a con-straint logic program for analyzing modifier-modifieerelationships of Japanese.
An arbitrary logical for-mula can be a constraint, and a constraint solvercalled CIL (Mukai 1985) is responsible for solving theconstraints.
The generative capacity and the compu-tational complexity of this formalism are not clear.The above-mentioned works seem to have concen-trated on the efficient representation f the output ofa parsing process, and lacked the formalization of astructural disambiguation process, that is, they didnot specify what kind of knowledge can be used inwhat way for structural disambiguation.
In CDGparsing, any knowledge is applicable to a constraintnetwork as long as it can be expressed as a constraintbetween two modifications, and an efficient filteringalgorithm effectively uses it to reduce structural am-biguities.4 FORMAL PROPERT IESWeak Generative Capacity of CDGConsider the language Lww = {wwlw E (a+b)*},the language of strings that are obtained by con-catenating the same arbitrary string over an alpha-bet {a,b}.
Lww is known to be non-context-free(Hopcroft and Ullman 1979), and is frequently men-tioned when discussing the non-context-freeness ofthe "respectively" construct (e.g.
"A, B, and C doD, E, and F, respectively") of various natural an-guages (e.g., Savitch et al 1987).
Although there37= (a, b}L = ( l}R = (partner}C = conjunction of the following subformulas:?
(word(pos(x))=a ~ word(mod(x))=a)& (word(pos(x))=b ~ word(mod(x))=b)?
mod(x) = pos(y) ~ rood(y) = pos(x)?
rood(x) ?
pos(x) & rood(x)  n i l?
pos(x) < pos(y) < mod(y)pos(x) < mod(x) < mod(y)?
rood(y) < pos(y) < pos(x)mod(y) < mod(x) < pos(x)Figure 9: Definition of Gww~ a a  a bFigure 10: Assignment for a sentence of Lwwis no context-free grammar that generates Lww, thegrammar Gww =< E,L ,R ,C  > shown in Figure 9generates it (Maruyama 1990).
An assignment givento a sentence "aabaab" is shown in Figure 10.On the other hand, any context-free languagecan be generated by a degree=2 CDG.
This canbe proved by constructing a constraint dependencygrammar GCDG from an arbitrary context-free gram-mar GCFG in Greibach Normal Form, and by show-ing that the two grammars generate exactly the samelanguage.
Since GcFc is in Greibach Normal Form,it is easy to make one-to-one correspondence betweena word in a sentence and a rule application in aphrase-structure t e. The details of the proof aregiven in Maruyama (1990).
This, combined with thefact that Gww generates Lww, means that the weakgenerative capacity of CDG with degree=2 is strictlygreater than that of CFG.Computat ional  complexity of CDG parsingLet us consider a constraint dependency grammarG =< E, R, L, C > with arity=2 and degree=k.
Letn be the length of the input sentence.
Consider thespace complexity of the constraint network first.
InCDG parsing, every word has k roles, so there are n ?k nodes in total.
A role can have n x l possible values,where l is the size of L, so the maximum domainsize is n x l. Binary constraints may be imposed onarbitrary pairs of roles, and therefore the number ofconstraint matrices i  at most proportional to (nk) 2.Since the size of a constraint matrix is (nl) 2, thetotal space complexity of the constraint network isO(12k~n4).
Since k and l are grammatical constants,it is O(n 4) for the sentence l ngth n.As the initial formation of a constraint networktakes a computation time proportional to the size ofthe constraint network, the time complexity of theinitial formation of a constraint network is O(n4).The complexity of adding new constraints o a con-straint network never exceeds the complexity of theinitial formation of a constraint network, so it is alsobounded by O(n4).The most efficient filtering algorithm developedso far runs in O(ea 2,) time, where e is the numberof arcs and a is the size of the domains in a con-straint network (Mohr and Henderson 1986).
Sincethe number of arcs is at most O((nk)2), filtering canbe performed in O((nk)2(nl)2), which is O(n 4) with-out grammatical constants.Thus, in CDG parsing with arity 2, both the ini-tial formation of a constraint network and filteringare bounded in O(n 4) time.5 CONCLUSIONWe have proposed a formal grammar that allows effi-cient structural disambiguation.
Grammar ules areconstraints on word-to-word modifications, and pars-ing is done by adding the constraints o a data struc-ture called a constraint network.
The initial forma-tion of a constraint network and the filtering have apolynomial time bound whereas the weak generativecapacity of CDG is strictly greater than that of CFG.CDG is actually being used for an interac-tive Japanese parser of a Japanese-to-English ma-chine translation system for a newspaper domain(Maruyama et.
al.
1990).
A parser for such a widedomain should make use of any kind of informationavailable to the system, including user-supplied in-formation.
The parser treats this information as an-other set of unary constraints and applies it to theconstraint network.38References1.
Church, K. and Patil, R. 1982, "Coping withsyntactic ambiguity, or how to put the blockin the box on the table," American Journal ofComputational Linguistics, Vol.
8, No.
3-4.2.
Hays, D.E.
1964, "Dependency theory: a for-malism and some observations," Language, Vol.40.3.
Hopcroft, J.E.
and Ullman, J.D., 1979, Intro-duction to Automata Theory, Languages, andComputation, Addison-Wesley.4.
Kaplan, R.M.
1973, "A general syntactic pro-cessor," in: Rustin, R.
(ed.)
Natural LanguageProcessing, Algorithmics Press.5.
Maruyama, H. 1990, "Constraint DependencyGrammar," TRL Research Report RT0044, IBMResearch, Tokyo Research Laboratory.6.
Maruyama, H., Watanabe, H., and Ogino, S,1990, "An interactive Japanese parser for ma-chine translation," COLING 'gO, to appear.7.
Mohr, R. and Henderson, T. 1986, "Arc andpath consistency revisited," Artificial Intelli-gence, Vol.
28.8.
Montanari, U.
1976, "Networks of constraints:Fundamental properties and applications to pic-ture processing," Information Science, Vol.
7.9.
Mukai, K. 1985, "Unification over complex inde-terminates in Prolog," ICOT Technical ReportTR-113.10.
Savitch, W.J.
et al (eds.)
1987, The FormalComplexity of Natural Language, Reidel.11.
Seo, J. and Simmons, R. 1988, "Syntacticgraphs: a representation for the union of allambiguous parse trees," Computational Linguis.tics, Vol.
15, No.
7.12.
Sugimura, R., Miyoshi, H., and Mukai, K. 1988,"Constraint analysis on Japanese modification,"in: Dahl, V. and Saint-Dizier, P.
(eds.)
Natu-ral Language Understanding and Logic Program-ming, II, Elsevier.13.
Tomita, M. 1987, "An efficient augmented-context-free parsing algorithm," ComputationalLinguistics, Vol.
13.14.
Waltz, D.L.
1975, "Understanding line draw-ings of scenes with shadows," in: Winston,P.H.
(ed.
): The Psychology of Computer Vision,McGraw-Hill.
