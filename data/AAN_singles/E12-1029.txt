Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286?295,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsBootstrapped Training of Event Extraction ClassifiersRuihong Huang and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{huangrh,riloff}@cs.utah.eduAbstractMost event extraction systems are trainedwith supervised learning and rely on a col-lection of annotated documents.
Due tothe domain-specificity of this task, eventextraction systems must be retrained withnew annotated data for each domain.
Inthis paper, we propose a bootstrapping so-lution for event role filler extraction that re-quires minimal human supervision.
We aimto rapidly train a state-of-the-art event ex-traction system using a small set of ?seednouns?
for each event role, a collectionof relevant (in-domain) and irrelevant (out-of-domain) texts, and a semantic dictio-nary.
The experimental results show thatthe bootstrapped system outperforms previ-ous weakly supervised event extraction sys-tems on the MUC-4 data set, and achievesperformance levels comparable to super-vised training with 700 manually annotateddocuments.1 IntroductionEvent extraction systems process stories aboutdomain-relevant events and identify the role fillersof each event.
A key challenge for event extrac-tion is that recognizing role fillers is inherentlycontextual.
For example, a PERSON can be aperpetrator or a victim in different contexts (e.g.,?John Smith assassinated the mayor?
vs. ?JohnSmith was assassinated?).
Similarly, any COM-PANY can be an acquirer or an acquiree dependingon the context.Many supervised learning techniques havebeen used to create event extraction systems us-ing gold standard ?answer key?
event templatesfor training (e.g., (Freitag, 1998a; Chieu and Ng,2002; Maslennikov and Chua, 2007)).
How-ever, manually generating answer keys for eventextraction is time-consuming and tedious.
Andmore importantly, event extraction annotationsare highly domain-specific, so new annotationsmust be obtained for each domain.The goal of our research is to use bootstrap-ping techniques to automatically train a state-of-the-art event extraction system without human-generated answer key templates.
The focus of ourwork is the TIER event extraction model, whichis a multi-layered architecture for event extrac-tion (Huang and Riloff, 2011).
TIER?s innova-tion over previous techniques is the use of fourdifferent classifiers that analyze a document at in-creasing levels of granularity.
TIER progressivelyzooms in on event information using a pipelineof classifiers that perform document-level classi-fication, sentence classification, and noun phraseclassification.
TIER outperformed previous eventextraction systems on the MUC-4 data set, but re-lied heavily on a large collection of 1,300 docu-ments coupled with answer key templates to trainits four classifiers.In this paper, we present a bootstrapping solu-tion that exploits a large unannotated corpus fortraining by using role-identifying nouns (Phillipsand Riloff, 2007) as seed terms.
Phillips andRiloff observed that some nouns, by definition,refer to entities or objects that play a specific rolein an event.
For example, ?assassin?, ?sniper?,and ?hitman?
refer to people who play the roleof PERPETRATOR in a criminal event.
Similarly,?victim?, ?casualty?, and ?fatality?
refer to peo-ple who play the role of VICTIM, by virtue oftheir lexical semantics.
Phillips and Riloff calledthese words role-identifying nouns and used them286to learn extraction patterns.
Our research alsouses role-identifying nouns to learn extraction pat-terns, but the role-identifying nouns and patternsare then used to create training data for event ex-traction classifiers.
Each classifier is then self-trained in a bootstrapping loop.Our weakly supervised training procedure re-quires a small set of ?seed nouns?
for each eventrole, and a collection of relevant (in-domain) andirrelevant (out-of-domain) texts.
No answer keytemplates or annotated texts are needed.
The seednouns are used to automatically generate a setof role-identifying patterns, and then the nouns,patterns, and a semantic dictionary are used tolabel training instances.
We also propagate theevent role labels across coreferent noun phraseswithin a document to produce additional train-ing instances.
The automatically labeled texts areused to train three components of TIER: its twotypes of sentence classifiers and its noun phraseclassifiers.
To create TIER?s fourth component,its document genre classifier, we apply heuristicsto the output of the sentence classifiers.We present experimental results on the MUC-4 data set, which is a standard benchmark forevent extraction research.
Our results show thatthe bootstrapped system, TIERlite, outperformsprevious weakly supervised event extraction sys-tems and achieves performance levels comparableto supervised training with 700 manually anno-tated documents.2 Related WorkEvent extraction techniques have largely focusedon detecting event ?triggers?
with their argumentsfor extracting role fillers.
Classical methods areeither pattern-based (Kim and Moldovan, 1993;Riloff, 1993; Soderland et al 1995; Huffman,1996; Freitag, 1998b; Ciravegna, 2001; Califf andMooney, 2003; Riloff, 1996; Riloff and Jones,1999; Yangarber et al 2000; Sudo et al 2003;Stevenson and Greenwood, 2005) or classifier-based (e.g., (Freitag, 1998a; Chieu and Ng, 2002;Finn and Kushmerick, 2004; Li et al 2005; Yu etal., 2005)).Recently, several approaches have been pro-posed to address the insufficiency of using onlylocal context to identify role fillers.
Some ap-proaches look at the broader sentential contextaround a potential role filler when making a de-cision (e.g., (Gu and Cercone, 2006; Patwardhanand Riloff, 2009)).
Other systems take a moreglobal view and consider discourse properties ofthe document as a whole to improve performance(e.g., (Maslennikov and Chua, 2007; Ji and Gr-ishman, 2008; Liao and Grishman, 2010; Huangand Riloff, 2011)).
Currently, the learning-basedevent extraction systems that perform best all usesupervised learning techniques that require a largenumber of texts coupled with manually-generatedannotations or answer key templates.A variety of techniques have been exploredfor weakly supervised training of event extrac-tion systems, primarily in the realm of pattern orrule-based approaches (e.g., (Riloff, 1996; Riloffand Jones, 1999; Yangarber et al 2000; Sudo etal., 2003; Stevenson and Greenwood, 2005)).
Insome of these approaches, a human must man-ually review and ?clean?
the learned patterns toobtain good performance.
Research has also beendone to learn extraction patterns in an unsuper-vised way (e.g., (Shinyama and Sekine, 2006;Sekine, 2006)).
But these efforts target open do-main information extraction.
To extract domain-specific event information, domain experts areneeded to select the pattern subsets to use.There have also been weakly supervised ap-proaches that use more than just local context.
(Patwardhan and Riloff, 2007) uses a semanticaffinity measure to learn primary and secondarypatterns, and the secondary patterns are appliedonly to event sentences.
The event sentence clas-sifier is self-trained using seed patterns.
Mostrecently, (Chambers and Jurafsky, 2011) acquireevent words from an external resource, group theevent words to form event scenarios, and groupextraction patterns for different event roles.
How-ever, these weakly supervised systems producesubstantially lower performance than the best su-pervised systems.3 Overview of TIERThe goal of our research is to develop a weaklysupervised training process that can successfullytrain a state-of-the-art event extraction system fora new domain with minimal human input.
We de-cided to focus our efforts on the TIER event ex-traction model because it recently produced bet-ter performance on the MUC-4 data set than priorlearning-based event extraction systems (Huangand Riloff, 2011).
In this section, we briefly givean overview of TIER?s architecture and its com-287Figure 1: TIER Overviewponents.TIER is a multi-layered architecture for eventextraction, as shown in Figure 1.
Documents passthrough a pipeline where they are analyzed at dif-ferent levels of granularity, which enables the sys-tem to gradually ?zoom in?
on relevant facts.
Thepipeline consists of a document genre classifier,two types of sentence classifiers, and a set of nounphrase (role filler) classifiers.The lower pathway in Figure 1 shows that alldocuments pass through an event sentence clas-sifier.
Sentences labeled as event descriptionsthen proceed to the noun phrase classifiers, whichare responsible for identifying the role fillers ineach sentence.
The upper pathway in Figure 1 in-volves a document genre classifier to determinewhether a document is an ?event narrative?
story(i.e., an article that primarily discusses the detailsof a domain-relevant event).
Documents that areclassified as event narratives warrant additionalscrutiny because they most likely contain a lot ofevent information.
Event narrative stories are pro-cessed by an additional set of role-specific sen-tence classifiers that look for role-specific con-texts that will not necessarily mention the event.For example, a victim may be mentioned in a sen-tence that describes the aftermath of a crime, suchas transportation to a hospital or the identifica-tion of a body.
Sentences that are determined tohave ?role-specific?
contexts are passed along tothe noun phrase classifiers for role filler extrac-tion.
Consequently, event narrative documentspass through both the lower pathway and the up-per pathway.
This approach creates an event ex-traction system that can discover role fillers in avariety of different contexts by considering thetype of document being processed.TIER was originally trained with supervisedlearning using 1,300 texts and their correspondinganswer key templates from the MUC-4 data set(MUC-4 Proceedings, 1992).
Human-generatedanswer key templates are expensive to producebecause the annotation process is both difficultand time-consuming.
Furthermore, answer keytemplates for one domain are virtually neverreusable for different domains, so a new set ofanswer keys must be produced from scratch foreach domain.
In the next section, we present ourweakly supervised approach for training TIER?sevent extraction classifiers.4 Bootstrapped Training of EventExtraction ClassifiersWe adopt a two-phase approach to train TIER?sevent extraction modules using minimal human-generated resources.
The goal of the first phaseis to automatically generate positive training ex-amples using role-identifying seed nouns as input.The seed nouns are used to automatically gener-ate a set of role-identifying patterns for each eventrole.
Each set of patterns is then assigned a setof semantic constraints (selectional restrictions)that are appropriate for that event role.
The se-mantic constraints consist of the role-identifyingseed nouns as well as general semantic classesthat constrain the event role (e.g., a victim mustbe a HUMAN).
A noun phrase will satisfy the se-mantic constraints if its head noun is in the seednoun list or if it has the appropriate semantic type(based on dictionary lookup).
Each pattern is thenmatched against the unannotated texts, and if theextracted noun phrase satisfies its semantic con-straints, then the noun phrase is automatically la-beled as a role filler.The second phase involves bootstrapped train-ing of TIER?s classifiers.
Using the labeled in-stances generated in the first phase, we iterativelytrain three of TIER?s components: the two typesof sentential classifiers and the noun phrase clas-sifiers.
For the fourth component, the documentclassifier, we apply heuristics to the output of thesentence classifiers to assess the density of rel-evant sentences in a document and label high-density stories as event narratives.
In the fol-lowing sections, we present the details of each ofthese steps.4.1 Automatically Labeling Training DataFinding seeding instances of high precision andreasonable coverage is important in bootstrap-ping.
However, this is especially challengingfor event extraction task because identifying rolefillers is inherently contextual.
Furthermore, role288Figure 2: Using Basilisk to Induce Role-IdentifyingPatternsfillers occur sparsely in text and in diverse con-texts.In this section, we explain how we gener-ate role-identifying patterns automatically usingseed nouns, and we discuss why we add seman-tic constraints to the patterns when producing la-beled instances for training.
Then, we discuss thecoreference-based label propagation that we usedto obtain additional training instances.
Finally, wegive examples to illustrate how we create traininginstances.4.1.1 Inducing Role-Identifying PatternsThe input to our system is a small set ofmanually-defined seed nouns for each event role.Specifically, the user is required to provide10 role-identifying nouns for each event role.
(Phillips and Riloff, 2007) defined a noun as be-ing ?role-identifying?
if its lexical semantics re-veal the role of the entity/object in an event.
Forexample, the words ?assassin?
and ?sniper?
arepeople who participate in a violent event as a PER-PETRATOR.
Therefore, the entities referred to byrole-identifying nouns are probable role fillers.However, treating every context surrounding arole-identifying noun as a role-identifying patternis risky.
The reason is that many instances of role-identifying nouns appear in contexts that do notdescribe the event.
But, if one pattern has beenseen to extract many role-identifying nouns andseldomly seen to extract other nouns, then the pat-tern likely represents an event context.As (Phillips and Riloff, 2007) did, we useBasilisk to learn patterns for each event role.Basilisk was originally designed for semanticclass learning (e.g., to learn nouns belonging tosemantic categories, such as building or human).As shown in Figure 2, beginning with a small setof seed nouns for each semantic class, Basilisklearns additional nouns belonging to the same se-mantic class.
Internally, Basilisk uses extractionpatterns automatically generated from unanno-tated texts to assess the similarity of nouns.
First,Basilisk assigns a score to each pattern based onthe number of seed words that co-occur with it.Basilisk then collects the noun phrases extractedby the highest-scoring patterns.
Next, the headnoun of each noun phrase is assigned a scorebased on the set of patterns that it co-occurredwith.
Finally, Basilisk selects the highest-scoringnouns, automatically labels them with the seman-tic class of the seeds, adds these nouns to the lex-icon, and restarts the learning process in a boot-strapping fashion.For our work, we give Basilisk role-identifyingseed nouns for each event role.
We run the boot-strapping process for 20 iterations and then har-vest the 40 best patterns that Basilisk identifiesfor each event role.
We also tried using the addi-tional role-identifying nouns learned by Basilisk,but found that these nouns were too noisy.4.1.2 Using the Patterns to Label NPsThe induced role-identifying patterns can bematched against the unannotated texts to producelabeled instances.
However, relying solely on thepattern contexts can be misleading.
For example,the pattern context <subject> caused damagewill extract some noun phrases that are weapons(e.g., the bomb) but some noun phrases that arenot (e.g., the tsunami).Based on this observation, we add selectionalrestrictions to each pattern that requires a nounphrase to satisfy certain semantic constraints inorder to be extracted and labeled as a positiveinstances for an event role.
The selectional re-strictions are satisfied if the head noun is amongthe role-identifying seed nouns or if the semanticclass of the head noun is compatible with the cor-responding event role.
In the previous example,tsunami will not be extracted as a weapon becauseit has an incompatible semantic class (EVENT),but bomb will be extracted because it has a com-patible semantic class (WEAPON).We use the semantic class labels assigned bythe Sundance parser (Riloff and Phillips, 2004) inour experiments.
Sundance looks up each nounin a semantic dictionary to assign the semanticclass labels.
As an alternative, general resources(e.g., WordNet (Miller, 1990)) or a semantic tag-ger (e.g., (Huang and Riloff, 2010)) could beused.289John Smith was killed by.
.
.
.
.
.was killed by <np>Role?IdentifyingPatternstwo armed men1an hour later.Police arrested the unidentified men3in broad daylight this morning.left his house to go to work about 8:00 am.The assassins2attacked the mayor as he<subject> fired shotsmen = HumanRole?Identifying  SemanticDictionaryterroristssnipersassassins.
.
.building = Object <subject> attackedNounConstraints ConstraintsFigure 3: Automatic Training Data Creation4.1.3 Propagating Labels with CoreferenceTo enrich the automatically labeled training in-stances, we also propagate the event role labelsacross coreferent noun phrases within a docu-ment.
The observation is that once a noun phrasehas been identified as a role filler, its corefer-ent mentions in the same document likely fill thesame event role since they are referring to thesame real world entity.To leverage these coreferential contexts, weemploy a simple head noun matching heuristic toidentify coreferent noun phrases.
This heuristicassumes that two noun phrases that have the samehead noun are coreferential.
We considered us-ing an off-the-shelf coreference resolver, but de-cided that the head noun matching heuristic wouldlikely produce higher precision results, which isimportant to produce high-quality labeled data.4.1.4 Examples of Training InstanceCreationFigure 3 illustrates how we label training in-stances automatically.
The text example showsthree noun phrases that are automatically labeledas perpetrators.
Noun phrases #1 and #2 oc-cur in role-identifying pattern contexts (was killedby <np> and <subject> attacked) and satisfythe semantic constraints for perpetrators because?men?
has a compatible semantic type and ?assas-sins?
is a role-identifying noun for perpetrators.Noun phrase #3 (?the unidentified men?)
doesnot occur in a pattern context, but it is deemedto be coreferent with ?two armed men?
becausethey have the same head noun.
Consequently, wepropagate the perpetrator label from noun phrase#1 to noun phrase #3.4.2 Creating TIERlite with BootstrappingIn this section, we explain how the labeled in-stances are used to train TIER?s classifiers withbootstrapping.
In addition to the automaticallylabeled instances, the training process dependson a text corpus that consists of both relevant(in-domain) and irrelevant (out-of-domain) doc-uments.
Positive instances are generated fromthe relevant documents and negative instances aregenerated by randomly sampling from the irrele-vant documents.The classifiers are all support vector machines(SVMs), implemented using the SVMlin software(Keerthi and DeCoste, 2005).
When applying theclassifiers during bootstrapping, we use a slidingconfidence threshold to determine which labelsare reliable based on the values produced by theSVM.
Initially, we set the threshold to be 2.0 toidentify highly confident predictions.
But if fewerthan k instances pass the threshold, then we slidethe threshold down in decrements of 0.1 until weobtain at least k labeled instances or the thresh-old drops below 0, in which case bootstrappingends.
We used k=10 for both sentence classifiersand k=30 for the noun phrase classifiers.The following sections present the details of thebootstrapped training process for each of TIER?scomponents.Figure 4: The Bootstrapping Process4.2.1 Noun Phrase ClassifiersThe mission of the noun phrase classifiers is todetermine whether a noun phrase is a plausibleevent role filler based on the local features sur-rounding the noun phrase (NP).
A set of classifiersis needed, one for each event role.As shown in Figure 4, to seed the classifiertraining, the positive noun phrase instances are290generated from the relevant documents follow-ing Section 4.1.
The negative noun phrase in-stances are drawn randomly from the irrelevantdocuments.
Considering the sparsity of role fillersin texts, we set the negative:positive ratio to be10:1.
Once the classifier is trained, it is applied tothe unlabeled noun phrases in the relevant docu-ments.
Noun phrases that are assigned role fillerlabels by the classifier with high confidence (us-ing the sliding threshold) are added to the set ofpositive instances.
New negative instances aredrawn randomly from the irrelevant documents tomaintain the 10:1 (negative:positive) ratio.We extract features from each noun phrase(NP) and its surrounding context.
The featuresinclude the NP head noun and its premodifiers.We also use the Stanford NER tagger (Finkel etal., 2005) to identify Named Entities within theNP.
The context features include four words to theleft of the NP, four words to the right of the NP,and the lexico-syntactic patterns generated by Au-toSlog to capture expressions around the NP (see(Riloff, 1993) for details).4.2.2 Event Sentence ClassifierThe event sentence classifier is responsiblefor identifying sentences that describe a relevantevent.
Similar to the noun phrase classifier train-ing, positive training instances are selected fromthe relevant documents and negative instances aredrawn from the irrelevant documents.
All sen-tences in the relevant documents that contain oneor more labeled noun phrases (belonging to anyevent role) are labeled as positive training in-stances.
We randomly sample sentences from theirrelevant documents to obtain a negative:positivetraining instance ratio of 10:1.
The bootstrappingprocess is then identical to that of the noun phraseclassifiers.
The feature set for this classifier con-sists of unigrams, bigrams and AutoSlog?s lexico-syntactic patterns surrounding all noun phrases inthe sentence.4.2.3 Role-Specific Sentence ClassifiersThe role-specific sentence classifiers aretrained to identify the contexts specific to eachevent role.
All sentences in the relevant doc-uments that contain at least one labeled nounphrase for the appropriate event role are usedas positive instances.
Negative instances arerandomly sampled from the irrelevant documentsto maintain the negative:positive ratio of 10:1.The bootstrapping process and feature set are thesame as for the event sentence classifier.The difference between the two types of sen-tence classifiers is that the event sentence classi-fier uses positive instances from all event roles,while each role-specific sentence classifiers onlyuses the positive instances for one particular eventrole.
The rationale is similar as in the super-vised setting (Huang and Riloff, 2011); the eventsentence classifier is expected to generalize overall event roles to identify event mention contexts,while the role-specific sentence classifiers are ex-pected to learn to identify contexts specific to in-dividual roles.4.2.4 Event Narrative Document ClassifierTIER also uses an event narrative documentclassifier and only extracts information from role-specific sentences within event narrative docu-ments.
In the supervised setting, TIER usesheuristic rules derived from answer key templatesto identify the event narrative documents in thetraining set, which are used to train an event nar-rative document classifier.
The heuristic rules re-quire that an event narrative should have a highdensity of relevant information and tend to men-tion the relevant information within the first sev-eral sentences.In our weakly supervised setting, we use theinformation density heuristic directly instead oftraining an event narrative classifier.
We approxi-mate the relevant information density heuristic bycomputing the ratio of relevant sentences (bothevent sentences and role-specific sentences) out ofall the sentences in a document.
Thus, the eventnarrative labeller only relies on the output of thetwo sentence classifiers.
Specifically, we label adocument as an event narrative if ?
50% of thesentences in the document are relevant (i.e., la-beled positively by either sentence classifier).5 EvaluationIn this section, we evaluate our bootstrapped sys-tem, TIERlite, on the MUC-4 event extractiondata set.
First, we describe the IE task, the dataset, and the weakly supervised baseline systemsthat we use for comparison.
Then we present theresults of our fully bootstrapped system TIERlite,the weakly supervised baseline systems, and twofully supervised event extraction systems, TIER291and GLACIER.
In addition, we analyze the per-formance of TIERlite using different configura-tions to assess the impact of its components.5.1 IE Task and DataWe evaluated the performance of our systems onthe MUC-4 terrorism IE task (MUC-4 Proceed-ings, 1992) about Latin American terrorist events.We used 1,300 texts (DEV) as our training set and200 texts (TST3+TST4) as the test set.
All thedocuments have answer key templates.
For thetraining set, we used the answer keys to separatethe documents into relevant and irrelevant sub-sets.
Any document containing at least one rel-evant event was considered to be relevant.PerpInd PerpOrg Target Victim Weapon129 74 126 201 58Table 1: # of Role Fillers in the MUC-4 Test SetFollowing previous studies, we evaluate oursystem on five MUC-4 string event roles: perpe-trator individuals (PerpInd), perpetrator organi-zations (PerpOrg), physical targets, victims, andweapons.
Table 1 shows the distribution of rolefillers in the MUC-4 test set.
The complete IE taskinvolves the creation of answer key templates, onetemplate per event1.
Our work focuses on extract-ing individual role fillers and not template genera-tion, so we evaluate the accuracy of the role fillersirrespective of which template they occur in.We used the same head noun scoring schemeas previous systems, where an extraction is cor-rect if its head noun matches the head noun in theanswer key2.
Pronouns were discarded from boththe system responses and the answer keys sinceno coreference resolution is done.
Duplicate ex-tractions were conflated before being scored, sothey count as just one hit or one miss.5.2 Weakly Supervised BaselinesWe compared the performance of our system withthree previous weakly supervised event extractionsystems.AutoSlog-TS (Riloff, 1996) generates lexico-syntactic patterns exhaustively from unannotatedtexts and ranks them based on their frequency andprobability of occurring in relevant documents.A human expert then examines the patterns and1Documents may contain multiple events per article.2For example, ?armed men?
will match ?5 armed men?.manually selects the best patterns for each eventrole.
During testing, the patterns are matchedagainst unseen texts to extract event role fillers.PIPER (Patwardhan and Riloff, 2007; Patward-han, 2010) learns extraction patterns using a se-mantic affinity measure, and it distinguishes be-tween primary and secondary patterns and ap-plies them selectively.
(Chambers and Jurafsky,2011) (C+J) created an event extraction systemby acquiring event words from WordNet (Miller,1990), clustering the event words into differentevent scenarios, and grouping extraction patternsfor different event roles.5.3 Performance of TIERliteTable 2 shows the seed nouns that we used in ourexperiments, which were generated by sorting thenouns in the corpus by frequency and manuallyidentifying the first 10 role-identifying nouns foreach event role.3 Table 3 shows the number oftraining instances (noun phrases) that were auto-matically labeled for each event role using ourtraining data creation approach (Section 4.1).Event Role Seed NounsPerpetrator terrorists assassins criminals rebelsIndividual murderers death squads guerrillasmember members individualsPerpetrator FMLN ELN FARC MRTA M-19 FrontOrganization Shining Path Medellin CartelThe ExtraditablesArmy of National LiberationTarget houses residence building home homesoffices pipeline hotel car vehiclesVictim victims civilians children jesuits Galanpriests students women peasants RomeroWeapon weapons bomb bombs explosives riflesdynamite grenades device car bombTable 2: Role-Identifying Seed NounsPerpInd PerpOrg Target Victim Weapon296 157 522 798 248Table 3: # of Automatically Labeled NPsTable 4 shows how our bootstrapped systemTIERlite compares with previous weakly super-vised systems and two supervised systems, its su-pervised counterpart TIER (Huang and Riloff,2011) and a model that jointly considers localand sentential contexts, GLACIER (Patwardhan3We only found 9 weapon terms among the high-frequency terms.292Weakly Supervised BaselinesPerpInd PerpOrg Target Victim Weapon AverageAUTOSLOG-TS (1996) 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46PIPERBest (2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/46/45C+J (2011) - - - - - 44/36/40Supervised ModelsGLACIER (2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52TIER (2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56Weakly Supervised ModelsTIERlite 47/51/49 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50Table 4: Performance of the Bootstrapped Event Extraction System (Precision/Recall/F-score)0 200 400 600 800 1000 1200 140030354045505560# of training documentsIEperformance(F1)Figure 5: The Learning Curve of Supervised TIERand Riloff, 2009).
We see that TIERlite outper-forms all three weakly supervised systems, withslightly higher precision and substantially morerecall.
When compared to the supervised sys-tems, the performance of TIERlite is similar toGLACIER, with comparable precision but slightlylower recall.
But the supervised TIER system,which was trained with 1,300 annotated docu-ments, is still superior, especially in recall.Figure 5 shows the learning curve for TIERwhen it is trained with fewer documents, rang-ing from 100 to 1,300 in increments of 100.
Eachdata point represents five experiments where werandomly selected k documents from the train-ing set and averaged the results.
The bars showthe range of results across the five runs.
Figure 5shows that TIER?s performance increases from anF score of 34 when trained on just 100 documentsup to an F score of 56 when training on 1,300 doc-uments.
The circle shows the performance of ourbootstrapped system, TIERlite, which achieves anF score comparable to supervised training withabout 700 manually annotated documents.5.4 AnalysisTable 6 shows the effect of the coreference prop-agation step described in Section 4.1.3 as part oftraining data creation.
Without this step, the per-formance of the bootstrapped system yields an Fscore of 41.
With the benefit of the additionaltraining instances produced by coreference prop-agation, the system yields an F score of 53.
Thenew instances produced by coreference propaga-tion seem to substantially enrich the diversity ofthe set of labeled instances.Seeding P/R/Fwo/Coref 45/38/41w/Coref 47/53/50Table 6: Effects of Coreference PropagationIn the evaluation section, we saw that the su-pervised event extraction systems achieve higherrecall than the weakly supervised systems.
Al-though our bootstrapped event extraction sys-tem TIERlite produces higher recall than previ-ous weakly supervised systems, a substantial re-call gap still exists.Considering the pipeline structure of the eventextraction system, as shown in Figure 1, the nounphrase extractors are responsible for identifyingall candidate role fillers.
The sentential classifiersand the document classifier effectively serve asfilters to rule out candidates from irrelevant con-texts.
Consequently, there is no way to recovermissing recall (role fillers) if the noun phrase ex-tractors fail to identify them.Since the noun phrase classifiers are so centralto the performance of the system, we comparedthe performance of the bootstrapped noun phraseclassifiers directly with their supervised conter-parts.
The results are shown in Table 5.
Both setsof classifiers produce low precision when used inisolation, but their precision levels are compara-293PerpInd PerpOrg Target Victim Weapon AverageSupervised Classifier 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42Bootstrapped Classifier 30/54/39 37/53/44 30/71/42 28/63/39 36/57/44 32/60/42Table 5: Evaluation of Bootstrapped Noun Phrase Classifiers (Precision/Recall/F-score)ble.
The TIER pipeline architecture is successfulat eliminating many of the false hits.
However,the recall of the bootstrapped classifiers is consis-tently lower than the recall of the supervised clas-sifiers.
Specifically, the recall is about 10 pointslower for three event roles (PerpInd, Target andVictim) and 20 points lower for the other two eventroles (PerpOrg and Weapon).
These results sug-gest that our bootstrapping approach to traininginstance creation does not fully capture the diver-sity of role filler contexts that are available in thesupervised training set of 1,300 documents.
Thisissue is an interesting direction for future work.6 ConclusionsWe have presented a bootstrapping approach fortraining a multi-layered event extraction modelusing a small set of ?seed nouns?
for each eventrole, a collection of relevant (in-domain) and ir-relevant (out-of-domain) texts and a semantic dic-tionary.
The experimental results show that thebootstrapped system, TIERlite, outperforms pre-vious weakly supervised event extraction sys-tems on a standard event extraction data set, andachieves performance levels comparable to super-vised training with 700 manually annotated docu-ments.
The minimal supervision required to trainsuch a model increases the portability of event ex-traction systems.7 AcknowledgmentsWe gratefully acknowledge the support of theNational Science Foundation under grant IIS-1018314 and the Defense Advanced ResearchProjects Agency (DARPA) Machine ReadingProgram under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0172.Any opinions, findings, and conclusions or rec-ommendations expressed in this material are thoseof the authors and do not necessarily reflect theview of the DARPA, AFRL, or the U.S. govern-ment.ReferencesM.E.
Califf and R. Mooney.
2003.
Bottom-up Re-lational Learning of Pattern Matching rules for In-formation Extraction.
Journal of Machine LearningResearch, 4:177?210.Nathanael Chambers and Dan Jurafsky.
2011.Template-Based Information Extraction without theTemplates.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-11).H.L.
Chieu and H.T.
Ng.
2002.
A Maximum EntropyApproach to Information Extraction from Semi-Structured and Free Text.
In Proceedings of the18th National Conference on Artificial Intelligence.F.
Ciravegna.
2001.
Adaptive Information Extractionfrom Text by Rule Induction and Generalisation.
InProceedings of the 17th International Joint Confer-ence on Artificial Intelligence.J.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating Non-local Information into InformationExtraction Systems by Gibbs Sampling.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 363?370,Ann Arbor, MI, June.A.
Finn and N. Kushmerick.
2004.
Multi-levelBoundary Classification for Information Extraction.In In Proceedings of the 15th European Conferenceon Machine Learning, pages 111?122, Pisa, Italy,September.Dayne Freitag.
1998a.
Multistrategy Learning forInformation Extraction.
In Proceedings of the Fif-teenth International Conference on Machine Learn-ing.
Morgan Kaufmann Publishers.Dayne Freitag.
1998b.
Toward General-PurposeLearning for Information Extraction.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics.Z.
Gu and N. Cercone.
2006.
Segment-Based HiddenMarkov Models for Information Extraction.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 481?488, Sydney, Australia, July.Ruihong Huang and Ellen Riloff.
2010.
InducingDomain-specific Semantic Class Taggers from (Al-most) Nothing.
In Proceedings of The 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL 2010).Ruihong Huang and Ellen Riloff.
2011.
Peeling Backthe Layers: Detecting Event Role Fillers in Sec-ondary Contexts.
In Proceedings of the 49th Annual294Meeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-11).S.
Huffman.
1996.
Learning Information ExtractionPatterns from Examples.
In Stefan Wermter, EllenRiloff, and Gabriele Scheler, editors, Connectionist,Statistical, and Symbolic Approaches to Learningfor Natural Language Processing, pages 246?260.Springer-Verlag, Berlin.H.
Ji and R. Grishman.
2008.
Refining Event Extrac-tion through Cross-Document Inference.
In Pro-ceedings of ACL-08: HLT, pages 254?262, Colum-bus, OH, June.S.
Keerthi and D. DeCoste.
2005.
A Modified FiniteNewton Method for Fast Solution of Large ScaleLinear SVMs.
Journal of Machine Learning Re-search.J.
Kim and D. Moldovan.
1993.
Acquisition ofSemantic Patterns for Information Extraction fromCorpora.
In Proceedings of the Ninth IEEE Con-ference on Artificial Intelligence for Applications,pages 171?176, Los Alamitos, CA.
IEEE ComputerSociety Press.Y.
Li, K. Bontcheva, and H. Cunningham.
2005.
Us-ing Uneven Margins SVM and Perceptron for Infor-mation Extraction.
In Proceedings of Ninth Confer-ence on Computational Natural Language Learn-ing, pages 72?79, Ann Arbor, MI, June.Shasha Liao and Ralph Grishman.
2010.
Using Docu-ment Level Cross-Event Inference to Improve EventExtraction.
In Proceedings of the 48st AnnualMeeting on Association for Computational Linguis-tics (ACL-10).M.
Maslennikov and T. Chua.
2007.
A Multi-Resolution Framework for Information Extractionfrom Free Text.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics.G.
Miller.
1990.
Wordnet: An On-line LexicalDatabase.
International Journal of Lexicography,3(4).MUC-4 Proceedings.
1992.
Proceedings of theFourth Message Understanding Conference (MUC-4).
Morgan Kaufmann.S.
Patwardhan and E. Riloff.
2007.
Effective Informa-tion Extraction with Semantic Affinity Patterns andRelevant Regions.
In Proceedings of 2007 the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2007).S.
Patwardhan and E. Riloff.
2009.
A Unified Modelof Phrasal and Sentential Evidence for InformationExtraction.
In Proceedings of 2009 the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-2009).S.
Patwardhan.
2010.
Widening the Field of Viewof Information Extraction through Sentential EventRecognition.
Ph.D. thesis, University of Utah.W.
Phillips and E. Riloff.
2007.
Exploiting Role-Identifying Nouns and Expressions for InformationExtraction.
In Proceedings of the 2007 Interna-tional Conference on Recent Advances in NaturalLanguage Processing (RANLP-07), pages 468?473.E.
Riloff and R. Jones.
1999.
Learning Dictionar-ies for Information Extraction by Multi-Level Boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence.E.
Riloff and W. Phillips.
2004.
An Introduction to theSundance and AutoSlog Systems.
Technical ReportUUCS-04-015, School of Computing, University ofUtah.E.
Riloff.
1993.
Automatically Constructing a Dictio-nary for Information Extraction Tasks.
In Proceed-ings of the 11th National Conference on ArtificialIntelligence.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intel-ligence, pages 1044?1049.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proceedings of Joint Conference of the In-ternational Committee on Computational Linguis-tics and the Association for Computational Linguis-tics (COLING/ACL-06.Y.
Shinyama and S. Sekine.
2006.
Preemptive In-formation Extraction using Unrestricted RelationDiscovery.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 304?311, New York City, NY,June.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a conceptual dictio-nary.
In Proc.
of the Fourteenth International JointConference on Artificial Intelligence, pages 1314?1319.M.
Stevenson and M. Greenwood.
2005.
A Seman-tic Approach to IE Pattern Induction.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, pages 379?386, AnnArbor, MI, June.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL-03).R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the Eighteenth International Conference onComputational Linguistics (COLING 2000).K.
Yu, G. Guan, and M. Zhou.
2005.
Resume?
In-formation Extraction with Cascaded Hybrid Model.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics, pages499?506, Ann Arbor, MI, June.295
