Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 96?102,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsThree models for discriminative machine translation usingGlobal Lexical Selection and Sentence ReconstructionSriram VenkatapathyLanguage Technologies ResearchCentre, IIIT-HyderabadHyderabad - 500019, India.sriram@research.iiit.ac.inSrinivas BangaloreAT&T Labs - ResearchFlorham Park, NJ 07932USAsrini@research.att.comAbstractMachine translation of a source languagesentence involves selecting appropriatetarget language words and ordering the se-lected words to form a well-formed tar-get language sentence.
Most of the pre-vious work on statistical machine transla-tion relies on (local) associations of targetwords/phrases with source words/phrasesfor lexical selection.
In contrast, in thispaper, we present a novel approach to lex-ical selection where the target words areassociated with the entire source sentence(global) without the need for local asso-ciations.
This technique is used by threemodels (Bag?of?words model, sequentialmodel and hierarchical model) which pre-dict the target language words given asource sentence and then order the wordsappropriately.
We show that a hierarchi-cal model performs best when comparedto the other two models.1 IntroductionThe problem of machine translation can be viewedas consisting of two subproblems: (a) lexical se-lection, where appropriate target language lexi-cal items are chosen for each source languagelexical item and (b) lexical reordering, wherethe chosen target language lexical items are rear-ranged to produce a meaningful target languagestring.
Most of the previous work on statisti-cal machine translation, as exemplified in (Brownet al, 1993), employs word?alignment algorithm(such as GIZA++ (Och et al, 1999)) that provideslocal associations between source words and targetwords.
The source?to?target word?alignments aresometimes augmented with target?to?source wordalignments in order to improve the precision ofthese local associations.
Further, the word?levelalignments are extended to phrase?level align-ments in order to increase the extent of local asso-ciations.
The phrasal associations compile someamount of (local) lexical reordering of the targetwords?those permitted by the size of the phrase.Most of the state?of?the?art machine translationsystems use these phrase?level associations inconjunction with a target language model to pro-duce the target sentence.
There is relatively littleemphasis on (global) lexical reordering other thanthe local re-orderings permitted within the phrasalalignments.
A few exceptions are the hierarchical(possibly syntax?based) transduction models (Wu,1997; Alshawi et al, 1998; Yamada and Knight,2001; Chiang, 2005) and the string transductionmodels (Kanthak et al, 2005).In this paper, we present three models for doingdiscriminative machine translation using globallexical selection and lexical reordering.1.
Bag?of?Words model : Given a source sen-tence, each of the target words are chosen bylooking at the entire source sentence.
Thetarget language words are then permuted invarious ways and then, the best permutationis chosen using the language model on thetarget side.
The size of the search space ofthese permutations can be set by a parametercalled the permutation window.
This modeldoes not allow long distance re-orderings oftarget words unless a very large permutationwindow chosen which is very expensive.2.
Sequential Lexical Choice model : Givena source sentence, the target words are pre-dicted in an order which is faithful to the or-96der of words in the source sentence.
Now,the number of permutations that need to beexamined to obtain the best target languagestrings are much less when compared to theBag?of?Words model.
This model is ex-pected to give good results for language pairssuch as English?French for which only lo-cal word order variations exist between sen-tences.3.
Hierarchical lexical association and re-ordering model : For language pairs suchas English?Hindi or English?Japanese wherethere is a high degree of global reordering(Figure 1), it is necessary to be able to handlelong distance movement of words/phrases.In this approach, the target words predictedthrough global lexical selection are associ-ated with various nodes of the source depen-dency tree and then, hierarchical reordering isdone to obtain the order of words in the tar-get sentence.
Hierarchical reordering allowsphrases to distort to longer distances than theprevious two models.Figure 1: Sample distortion between En-glish?HindiThe outline of the paper is as follows.
In Section2, we talk about the global lexical selection.
Sec-tion 3 describes three models for global lexical se-lection and reordering.
In Section 4, we report theresults of the translation models on English?Hindilanguage pair and contrast the strengths and limi-tations of the models.2 Global lexical selectionFor global lexical selection, in contrast to thelocal approaches of associating target words tothe source words, the target words are associatedto the entire source sentence.
The intuition isthat there may be lexico?syntactic features of thesource sentence (not necessarily a single sourceword) that might trigger the presence of a targetword in the target sentence.
Furthermore, it mightbe difficult to exactly associate a target word toa source sentence in many situations - (a) whentranslations are not exact but paraphrases (b) thetarget language does not have one lexical itemto express the same concept that is expressed inthe source word.
The extensions of word align-ments to phrasal alignments attempt to addresssome of these situations in additional to alleviat-ing the noise in word?level alignments.As a consequence of the global lexical selectionapproach, we no longer have a tight associationbetween source language words/phrases and tar-get language words/phrases.
The result of lexicalselection is simply a bag of words(phrases) in thetarget language and the target sentence has to bereconstructed using this bag of words.The target words in the bag, however, mightbe enhanced with rich syntactic information thatcould aid in the reconstruction of the target sen-tence.
This approach to lexical selection andsentence reconstruction has the potential to cir-cumvent the limitations of word?alignment basedmethods for translation between significantly dif-ferent word order languages.
However, in this pa-per, to handle large word order variations, we asso-ciate the target words with source language depen-dency structures to enable long distance reorder-ing.3 Training the discriminative models forlexical selection and reorderingIn this section, we present our approach for aglobal lexical selection model which is based ondiscriminatively trained classification techniques.Discriminant modeling techniques have becomethe dominant method for resolving ambiguity inspeech and natural language processing tasks, out-performing generative models for the same task.We expect the discriminatively trained global lex-ical selection models to outperform generativelytrained local lexical selection models as well asprovide a framework for incorporating rich mor-pho?syntactic information.Statistical machine translation can be formu-lated as a search for the best target sequence thatmaximizes P (T | S), where S is the source sen-tence and T is the target sentence.
Ideally, P (T |S) should be estimated directly to maximize theconditional likelihood on the training data (dis-criminant model).
However, T corresponds toa sequence with a exponentially large combina-tion of possible labels, and traditional classifica-tion approaches cannot be used directly.
Although97Conditional Random Fields (CRF) (Lafferty et al,2001) train an exponential model at the sequencelevel, in translation tasks such as ours the compu-tational requirements of training such models areprohibitively expensive.3.1 Bag-of-Words Lexical Choice ModelThis model doesn?t require the sentences to beword aligned in order to learn the local associa-tions.
Instead, we take the sentence aligned cor-pus as before but we treat the target sentence as abag?of?words or BOW assigned to the source sen-tence.
The goal is, given a source sentence S, toestimate the probability that we find a given word(tj) in its translation ie.., we need to estimate theprobabilities P (true|tj , S) and P (false|tj, S).To train such a model, we need to build binaryclassifiers for all the words in the target lan-guage vocabulary.
The probability distributionsof these binary classifiers are learnt using maxi-mum entropy model (Berger et al, 1996; Haffner,2006).
For the word tj , the training sentencepairs are considered as positive examples wherethe word appears in the target, and negative other-wise.
Thus, the number of training examples foreach binary classifier equals the number of train-ing examples.
In this model, classifiers are train-ing using n?gram features (BOgrams(S)).During decoding, instead of producing the tar-get sentence directly, what we initially obtain isthe target bag of words.
Each word in the targetvocabulary is detected independently, so we havehere a very simple use of binary static classifiers.Given a sentence S, the bag of words (BOW (T )contains those words whose distributions have thepositive probability greater than a threshold (?
).BOW (T ) = {t | P (true | t, BOgrams(S)) > ?
}(1)In order to reconstruct the proper order of wordsin the target sentence, we consider various permu-tations of words in BOW (T ) and weight them bya target language model.
Considering all possiblepermutations of the words in the target sentenceis computationally not feasible.
But, the numberof permutations examined can be reduced by us-ing heuristic forward pruning or by constrainingthe permutations to be within a local window ofadjustable size (also see (Kanthak et al, 2005)).We have chosen to constrain permutations here.Constraining the permutation using a local win-dow can provide us some very useful local re-orderings.The bag?of?words approach can also be modi-fied to allow for length adjustments of target sen-tences, if we add optional deletions in the finalstep of permutation decoding.
The parameter ?and an additional word deletion penalty ?
can thenbe used to adjust the length of translated outputs.3.2 Sequential Lexical Choice ModelThe previous approach gives us a predeterminedorder of words initially which are then permuted toobtain the best target string.
Given that we wouldnot be able to search the entire space, it would be ahelpful if we could start searching various permu-tations using a more definite string.
One such def-inite order in which the target words can be placedis the order of source words itself.
In this model,during the lexical selection, we try to place thetarget words in an order which is faithful to thesource sentence.This model associates sets of target words withevery position in the source sentence and yet re-tains the power of global lexical selection.
Forevery position (i) of the source sentence, a prefixstring is formed which consists of the sequence ofwords from positions 1 to i.
Each of these prefixstrings are used to predict bags of target words us-ing the global lexical selection.
Now, these bagsgenerated using the prefix strings are processed inthe order of source positions.
Let Ti be the bag oftarget words generated by prefix string i (Figure2).T (i+1)T (i)i i+1Figure 2: The generation of target bags associatedwith source sentence positionThe goal is to associate a set of target wordswith every source position.
A target word tis attached to the ith source position if it ispresent in Ti but not in Ti?1 and the probabilityP (true|t, Ti) > ?
.
The intuition behind this ap-proach is that a word t is associated with a positioni if there was some information present at the ithsource position that triggered the probability of thet to exceed the threshold ?
.98Hence, the initial target string is the sequenceof target language words associated with the se-quence of source language positions.
This stringis now permuted in all possible ways (section 3.1)and the best target string is chosen using the lan-guage model.3.3 Hierarchical lexical association andreordering modelThe Sequential Lexical Choice Model presented inthe last section is expected to work best for lan-guage pairs for which there are mostly local wordorder variations.
For language pairs with signifi-cant word order variation, the search for the targetstring may still fail examine the best target lan-guage string given the source sentence.
The modelproposed in this section should be able to handlesuch long distance movement of words/phrases.In this model, the goal is to search for the besttarget string T which maximizes the probabilityP (T |S,D(S)), where S is the source sentenceand D(S) is the dependency structure associatedwith the source sentence S. The probabilities ofthe target words given the source sentence areestimated in the same way as the bag?of?wordsmodel.
The only main difference during the esti-mation stage is that we consider the dependencytree based features apart from the n-gram features.The decoding of the source sentence S takesplace in three steps,1.
Predict the bag?of?words : Given a sourcesentence S, predict the bag of words BOW(T)whose distributions have a positive probabil-ities greater than a threshold (?
).2.
Attachment to Source nodes : These targetwords are now attached to the nodes of sourcedependency trees.
For making the attach-ments, the probability distributions of targetwords conditioned on features local to thesource nodes are used.3.
Ordering the target language words : Tra-verse the source dependency tree in a bottom-up fashion to obtain the best target string.3.3.1 Predict the bag?of?wordsGiven a source sentence S, all the target wordswhose positive probability distributions are above?
are included in the bag.BOW (T ) = {t | P (true|t, f(S))} (2)In addition to the n?gram features, this model usescues provided by the dependency structure to pre-dict the target bag?of?words.S1S2S3 S4S5Figure 3: Dependency tree of a source sentencewith words s1, s2, s3, s4 and s5Hence, the features that we have considered inthe model are (Figure 3),1.
N-grams.
For example, in Figure 2, ?s1?, ?s2s3 s4?, ?s4 s5?
etc.2.
Dependency pair (The pair of nodes and itsparents).
Example in Figure 2., ?s2 s1?, ?s4s2?
etc.3.
Dependency treelet (The triplet of a node, it?sparent and sibling).
For example, ?s3 s2 s4?,?s2 s1 s5?
etc.3.3.2 Attachment to Source nodesFor every target word tj in the bag, the mostlikely source nodes are determined by measuringthe positive distribution of the word tj given thefeatures of the particular node (Figure 4).
LetS(tj) denote the set of source nodes to which theword tj can be attached to, then S(tj) is deter-mined as,S1S2S3 S4S5T1          T2        T3          T4Figure 4: Dependency tree of a source sentencewith words S1, S2, S3, S4 and S5S(tj) = argmaxs(P (true|tj , f(s)) (3)where f(s) denotes the features of S in whichonly those features are active which contain the99lexical item representing the node s. The targetwords are in the global bag are processed in theorder of their global probabilities p(t|S).
Whileattaching the target words, it is ensured that nosource node had more than ?
target words attachedto it.
Also, a target word should not be attachedto more to more than ?
number of times.
Thereis another constraint that can be applied to ensurethat the ratio of the total target words (which areattached to source nodes) to the total number ofwords in the source sentence does exceed a value(?
).3.4 Ordering the target language wordsIn this step, the source sentence dependency tree istraversed in a bottom?up fashion.
At every node,the best possible order of target words associatedwith the sub-tree rooted at the node is determined.This string is then used as a cohesive unit by thesuperior nodes.S1S2S3 S4S5t1 t2 t3 t4 t5t6 t7t1 t2 t3 t7 t4 t5 t6Figure 5: The target string associated with nodeS1 is determined by permuting strings attached tothe children (in rectangular boxes, to signify thatthey are frozen) and the lexical items attached toS1For example, in Figure 5, let ?t1 t2 t3?, ?t4 t5?be the best strings associated with the children ofnodes s2 and s3 respectively.
Let t6 and t7 be thewords that are attached to node s1.
The best stringfor the node s1 is determined by permuting thestrings ?t1 t2 t3?, ?t4 t5?, ?t6?
?t7?
in all possibleways and then choosing the best string using thelanguage model.4 DatasetThe language pair that we considered for our ex-periments are English?Hindi.
The training setconsists of 37967 sentence pairs, the developmentset contains 819 sentence pairs and the test sethas 699 sentence pairs.
The dataset is from thenewspaper domain with topics ranging from pol-itics to tourism.
The sentence pairs have a maxi-mum source sentence length of 30 words.
The av-erage length of English sentences is 18 while thatof Hindi sentences is 20.The source language vocabulary is 41017 andtarget sentence vocabulary is 48576.
The to-ken/type ratio of English in the dataset is 16.70and that of Hindi is 15.64.
This dataset is rela-tively sparse.
So, the translation accuracies on thisdataset would be relatively less when compared tothose on much larger datasets.
In the target sideof the development corpus, the percentage of un-seen tokens is 13.48%(3.87% types) while in thesource side, the percentage of unseen tokens is10.77%(3.20% types).
On furthur inspection ofa small portion of the dataset, we found that themaximum percentage of the unseen words on thetarget side are the named entities.5 Results5.1 Bag-of-Words modelThe quality of the bag?of?words obtained is gov-erned by the parameter ?
(probability threshold).To determine the best ?
value, we experiment withvarious values of ?
and measure the lexical accu-racies (F-score) of the bags generated on the de-velopment set (See Figure 6).
The total numberof features used for training this model are 53166(with count-cutoff of 2).Figure 6: Lexical Accuracies of the Bags-of-wordsNow, we order the bags of words obtainedthrough global selection to get the target lan-guage strings.
While reordering using the lan-guage model, some of the noisy words from thebag can be deleted by setting a deletion cost (?
).We experimented with various deletion costs, andtuned it according to the best BLEU score that we100obtained on the development set.
Figure 7 showsthe best BLEU scores obtained by reordering thebags associated with various threshold values.Figure 7: Lexical Accuracies of the Bags-of-wordsWe can see that we obtained the best BLEUwhen we choose a threshold of 0.17 to obtain thebag?of?words, when the deletion cost is set to 19.The reference target strings of the developmentset has 15986 tokens.
So, while tuning the param-eters, we should ensure that the bags (obtained us-ing the global lexical selection) that we considerhave more tokens than 15986 to allow some dele-tions during reordering, and in effect obtain thetarget strings whose total token count is approx-imately equal to 15986.
Figure 8 shows the varia-tion in BLEU scores for various deletion costs byfixing the threshold at 0.17.Figure 8: BLEU scores for various deletion costswhen the threshold for global lexical selection isset to 0.17On the test set, we now fix the threshold at 0.17(? )
and the deletion cost (?)
at 19 to obtain thetarget language strings.
The BLEU score that weobtained for this set is 0.0428.5.2 Sequential Lexical Choice ModelThe lexical accuracy values of the sequence ofwords obtained by the sequential lexical choicemodel are comparable to those obtained using thebag?of?words model.
The real difference comesfor the BLEU score.
The best BLEU score ob-tained on the development set was 0.0586 when ?was set to 0.14 and deletion cost was 15.
On thetest set, the BLEU score obtained was 0.0473.5.3 Tree based modelThe lexical accuracy values of the words obtainedin this model are comparable to the lexical accu-racy values of the bag of words model.
The totalnumber of features used for training this model are118839 (with count-cutoff of 2).
On the develop-ment set, we obtained a BLEU score of 0.0650 for?
set at 0.17 and the deletion cost set at 20.
Onthe test set, we obtained a BLEU score of 0.0498.We can see that the BLEU scores are now bet-ter than the ones obtained using any of the othermodels discussed before.
This is because the Treebased model has both the strengths of the globallexical selection that ensures high quality lexicalitems in the target sentences and that of an efficientreconstruction model which takes care of long dis-tance reordering.
The table summarizes the BLEUscores obtained by the three models on the devel-opment and test sets.Devel.
Set Test.
SetBag-of-Words 0.0545 0.0428Sequential 0.0586 0.0473Hierarchical 0.0650 0.0498Table 1: Summary of the results6 ConclusionIn this paper, we present a novel approach to lex-ical selection where the target words are associ-ated with the entire source sentence (global) with-out the need for local associations.
This techniqueis used by three models (Bag?of?words model, se-quential model and hierarchical model) which pre-dict the target language words given a source sen-tence and then order the words appropriately.
Weshow that a hierarchical model performs best whencompared to the other two models.
The hierar-chical model presented in this paper has both thestrengths of the global lexical selection and effi-cient reconstruction model.101In the future, we are planning to improve the hi-erarchical model by making two primary additions?
Handling cases of structural non-isomorphism between source and targetsentences.?
Obtaining K-best target string per node of thesource dependency tree instead of just oneper node.
This would allow us to exploremore possibilities without having to compro-mise much on computational complexity.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Dou-glas.
1998.
Automatic acquisition of hierarchicaltransduction models for machine translation.
In Pro-ceedings of the 36th Annual Meeting Association forComputational Linguistics, Montreal, Canada.A.L.
Berger, Stephen A. D. Pietra, D. Pietra, and J. Vin-cent.
1996.
A Maximum Entropy Approach to Nat-ural Language Processing.
Computational Linguis-tics, 22(1):39?71.P.
Brown, S.D.
Pietra, V.D.
Pietra, and R. Mercer.1993.
The Mathematics of Machine Translation:Parameter Estimation.
Computational Linguistics,16(2):263?312.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages263?270, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.P.
Haffner.
2006.
Scaling large margin classifiers forspoken language understanding.
Speech Communi-cation, 48(iv):239?261.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In Proceedings ofthe ACL Workshop on Building and Using ParallelTexts, pages 167?174, Ann Arbor, Michigan.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML, San Francisco, CA.Franz Och, Christoph Tillmann, and Herman Ney.1999.
Improved alignment models for statisticalmachine translation.
In In Proc.
of the Joint Conf.
ofEmpirical Methods in Natural Language Processingand Very Large Corpora, pages 20?28.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proceedings of 39thACL.102
