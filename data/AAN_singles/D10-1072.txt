Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 736?744,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsWhat a Parser can Learn from a Semantic Role Labeler and Vice VersaStephen A. Boxwell, Dennis N. Mehay, Chris BrewThe Ohio State University{boxwell, mehay, cbrew}@ling.ohio-state.eduAbstractIn many NLP systems, there is a unidirectional flowof information in which a parser supplies input to asemantic role labeler.
In this paper, we build a sys-tem that allows information to flow in both direc-tions.
We make use of semantic role predictions inchoosing a single-best parse.
This process relies onan averaged perceptron model to distinguish likelysemantic roles from erroneous ones.
Our system pe-nalizes parses that give rise to low-scoring semanticroles.
To explore the consequences of this we per-form two experiments.
First, we use a baseline gen-erative model to produce n-best parses, which arethen re-ordered by our semantic model.
Second, weuse a modified version of our semantic role labelerto predict semantic roles at parse time.
The perfor-mance of this modified labeler is weaker than thatof our best full SRL, because it is restricted to fea-tures that can be computed directly from the parser?spacked chart.
For both experiments, the resulting se-mantic predictions are then used to select parses.
Fi-nally, we feed the selected parses produced by eachexperiment to the full version of our semantic rolelabeler.
We find that SRL performance can be im-proved over this baseline by selecting parses withlikely semantic roles.1 IntroductionIn the semantic role labeling task, words or groups ofwords are described in terms of their relations to a pred-icate.
For example, the sentence Robin admires Lesliehas two semantic role-bearing words: Robin is the agentor experiencer of the admire predicate, and Leslie isthe patient.
These semantic relations are distinct fromsyntactic relations like subject and object ?
the propernouns in the sentence Leslie is admired by Robin havethe same semantic relationships as Robin admires Leslie,even though the syntax differs.Although syntax and semantics do not always align witheach other, they are correlated.
Almost all automatic se-mantic role labeling systems take a syntactic representa-tion of a sentence (taken from an automatic parser or ahuman annotator), and use the syntactic information topredict semantic roles.
When a semantic role labeler pre-dicts an incorrect role, it is often due to an error in theparse tree.
Consider the erroneously annotated sentencefrom the Penn Treebank corpus shown in Figure 1.
If asemantic role labeling system relies heavily upon syntac-tic attachment decisions, then it will likely predict thatin 1956 describes the time that asbestos was used, ratherthan when it ceased to be used.Errors of this kind are common in treebanks and in au-tomatic parses.
It is telling, though, that while the hand-annotated Penn Treebank (Marcus et al, 1993), the Char-niak parser (Charniak, 2001), and the C&C parser (Clarkand Curran, 2004) all produce the erroneous parse fromFigure 1, the hand-annotated Propbank corpus of verbalsemantic roles (Palmer et al, 2005) correctly identifies in1956 as a temporal modifier of stopped, rather than using.This demonstrates that while syntactic attachment deci-sions like these are difficult for humans and for automaticparsers, a human reader has little difficulty identifying thecorrect semantic relationship between the temporal mod-ifier and the verbs.
This is likely due to the fact that themeaning suggested by the parse in Figure 1 is unlikely ?the reader instinctively feels that a temporal modifier fitsbetter with the verb stop than with the verb use.In this paper, we will use the idea that semantic rolespredicted by correct parses are more natural than seman-tic roles predicted by erroneous parses.
By modifying astate-of-the-art CCG semantic role labeler to predict se-mantic roles at parse time, or by using it to select froman n-best list, we can prefer analyses that yield likely se-mantic roles.
Syntactic analysis is treated not as an au-tonomous task, but rather as a contributor to the final goalof semantic role labeling.2 Related WorkThere has been a great deal of work in joint parsing andsemantic role labeling in recent years.
Two notable ef-forts have been the CoNLL 2008 and 2009 shared tasks736SHHHHHNPthe companyVPHHHHVBstoppedVPHHHHHVBusingNPasbestosPPin 1956Figure 1: A parse tree based on the treebank parse of wsj 0003.3.
Notice that the temporal adjunct is erroneously attached low.
Ina syntax-based SRL system, this will likely lead to a role prediction error.
(Surdeanu et al, 2008; Hajic?
et al, 2009).
Many of thesesystems perform joint syntactic and semantic analysis bygenerating an n-best list of syntactic parses, labeling se-mantic roles on all of them, then re-ranking these parsesby some means.
Our approach differs from this strategyby abandoning the preliminary ranking and predicting se-mantic roles at parse time.
By doing this, we effectivelyopen semantic roles in the entire parse forest to exami-nation by the ranking model, rather than restricting themodel to an n-best list generated by a baseline parser.
Thespirit of this work more closely resembles that of Finkeland Manning (2009) , which improves both parsing andnamed entity recognition by combining the two tasks.3 Why Predicting Semantic Roles in aPacked Chart is DifficultPredicting semantic roles in the environment of a packedchart is difficult when using an atomic CFG.
In order toachieve the polynomial efficiency appropriate for wide-coverage parsing, it is necessary to ?pack?
the chart ?that is, to combine distinct analyses of a given span ofwords that produce the same category.
The only otherwidely used option for wide-coverage parsing is to usebeam search with a narrow beam, which runs the riskof search errors.
On methodological grounds we pre-fer an exhaustive search, since systems that rely heav-ily on heuristics for their efficiency are difficult to un-derstand, debug or improve.
It is straightforward to readoff the highest scoring parse from a packed chart, andsimilarly routine to generate an n-best list containing ahighly-ranked subset of the parses.
However, a packedchart built on an atomic CFG does not make availableall of the features that are important to many CFG-basedSRL systems.
In particular, the very useful treepath fea-ture, which lists the categories touched by walking thetree from the predicate to the target word, only makessense when you have a complete tree, so cannot easilybe computed from the chart (Figure 2).
Chart edges canSHHHHHHNPpeoplePPPPPMore intelligent peopleVPsawPPPPPsaw kids with telescopesFigure 2: In the context of a packed chart, it is meaningless tospeak of a treepath between saw and people because multipleanalyses are ?packed?
under a single category.be lexicalized with their headwords, and this informationwould be useful in role labeling ?
but even this missesvital subcategorization information that would be avail-able in the complete parse.
An ideal formalism for ourpurpose would condense into the category label a widerange of information about combinatory potential, heads,and syntactic dependencies.
At the same time it shouldallow the creation of a packed chart, come with labeledtraining data, and have a high-quality parser and semanticrole labeler already available.
Fortunately, CombinatoryCategorial Grammar offers these desiderata, so this is ourformalism of choice.4 Combinatory Categorial GrammarCombinatory Categorial Grammar (Steedman, 2000) isa grammar formalism that describes words in terms oftheir combinatory potential.
For example, determinersbelong to the category np/n, or ?the category of wordsthat become noun phrases when combined with a nounto the right?.
The rightmost category indicates the argu-ment that the category is seeking, the leftmost categoryindicates the result of combining this category with itsargument, and the slash (/ or \) indicates the direction ofcombination.
Categories can be nested within each other:a transitive verb like devoured belongs to the category737The man devoured the steaknp/n n (s\np)/npx npx/nx nx> >np npx>s\np<sFigure 3: A simple CCG derivation.The steak that the man devourednp (npx\npx)/(s/npx) np (s\np)/npx>Ts/(s\np)>Bs/npx>npx\npx<npxFigure 4: An example of CCG?s treatment of relative clauses.The syntactic dependency between devoured and steak is thesame as it was in figure 3.
Co-indexations (the ?xs?)
have beenadded here and above to aid the eye in following the relevant[devoured-steak] dependency.
(s\np)/np, or ?the category that would become a sentenceif it could combine with a noun phrase to the right andanother noun phrase to the left?.
An example of how cat-egories combine to make sentences is shown in Figure 3.CCG has many capabilities that go beyond that of a typ-ical context-free grammar.
First, it has a sophisticatedinternal system of managing syntactic heads and depen-dencies1.
These dependencies are used to great effect inCCG-based semantic role labeling systems (Gildea andHockenmaier, 2003; Boxwell et al, 2009), as they donot suffer the same data-sparsity effects encounted withtreepath features in CFG-based SRL systems.
Secondly,CCG permits these dependencies to be passed through in-termediary categories in grammatical structures like rel-ative clauses.
In Figure 4, the steak is still in the objectrelation to devoured, even though the verb is inside a rel-ative clause.
Finally and most importantly, these depen-dencies are represented directly on the CCG categoriesthemselves.
This is what makes CCG resistant to theproblem described in Section 3 ?
because the dependencyis formed when the two heads combine, it is available tobe used as a local feature by the semantic role labeler.1A complete explanation of CCG predicate-argument dependenciescan be found in the CCGbank user manual (Hockenmaier and Steed-man, 2005)5 Semantic Role LabelingWe use a modified version of the Brutus semantic rolelabeling system (Boxwell et al, 2009)2.
The original ver-sion of this system takes complete CCG derivations as in-put, and predicts semantic roles over them.
For our pur-poses, however, it is necessary to modify the system tomake semantic predictions at parse time, inside a packedchart, before the complete derivation is available.
Forthis reason, it is necessary to remove the global featuresfrom the system (that is, features that rely on the com-plete parse), leaving only local features (features that areknown at the moment that the predicate is attached to theargument).
Crucially, dependency features count as ?lo-cal?
features, even though they have the potential to con-nect words that are very far apart in the sentence.Brutus is arranged in a two-stage pipeline.
First, a max-imum entropy classifier3 predicts, for each predicate inturn, which words in the sentence are likely headwords ofsemantic roles.
Then, a second maximum entropy classi-fier assigns role labels to each of these words.
The fea-tures used in the identification model of the local-onlyversion of Brutus are as follows:?
Words.
A three-word window surrounding the can-didate word.
For example, if we were consideringthe word steak in Figure 3, the three features wouldbe represented as word -1=the, word 0=steak, andword 1=#, with the last feature representing an out-of-bounds index.?
Predicate.
The predicate whose semantic roles thesystem is looking for.
For example, the sentence infigure 3 contains one predicate: devour.?
Syntactic Dependency.
As with a previous ap-proach in CCG semantic role labeling (Gildea andHockenmaier, 2003), this feature shows the ex-act nature of the syntactic dependency between thepredicate and the word we are considering, if anysuch dependency exists.
This feature is representedby the category of the predicate, the argument slotthat this word fits into, and whether or not the predi-cate is the head of the resultant category, representedwith a left or right arrow.
In the example from fig-ure 3, the relationship between devoured and steakwould be represented as (s\np)/np.2.
?.The second maximum entropy classifier uses all of thefeatures from the identifier, plus several more:2Found at http://www.ling.ohio-state.edu/?boxwell/software/brutus.html3Brutus uses Zhang Le?s maxent toolkit, available athttp://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.738Model P R FLocal 89.8% 80.8% 85.1%Global 89.8% 84.3% 87.0%Table 1: SRL results for treebank parses, using the local modeldescribed in Section 5 and the full global model.?
Before / After.
A binary indicator feature indicat-ing whether the candidate word is before or after thepredicate.?
Result Category Detail.
This indicates the featureon the result category of the predicate.
Possiblevalues include dcl (for declarative sentences), pss(for passive sentences), ng (for present-progressivephrases like ?running the race?
), etc.
These are readtrivially off of the verbal category.?
Argument Mapping.
An argument mapping is aprediction of a likely set of semantic roles for agiven CCG predicate category.
For example, a likelyargument mapping for devoured:(s[dcl]\np)/np is[Arg0,Arg1].
These are predicted from string-levelfeatures, and are useful for bringing together oth-erwise independent classification decisions for in-dividual roles.
Boxwell et al (2009) describe thisfeature in detail.The Maximum-Entropy models were trained to 500 it-erations.
To prevent overfitting, we used Gaussian pri-ors with global variances of 1 and 5 for the identifierand the labeler, respectively.
Table 1 shows SRL perfor-mance for the local model described above, and the fullglobal CCG-system described by Boxwell et al (2009).We use the method for calculating the accuracy of Prop-bank verbal semantic roles described in the CoNLL-2008shared task on semantic role labeling (Surdeanu et al,2008).
Because the Brutus SRL system is not designedto accommodate Nombank roles (Meyers et al, 2004),we restrict ourselves to predicting Propbank roles in thepresent work.The local system has the same precision as the globalone, but trails it on recall and F-measure.
Note that thisperformance is achieved with gold standard parses.6 Performing Semantic Role Predictions atParse TimeRecall that the reasoning for using a substantially pareddown version of the Brutus SRL system is to allow it topredict semantic roles in the context of a packed chart.Because we predict semantic roles for each constituentimmediately after the constituent is formed and before itis added to the chart, we can use semantic roles to informparsing.
We use a CKY parsing algorithm, though thisapproach could be easily adapted to other parse strate-gies.Whenever two constituents are combined, the SRL sys-tem checks to see if either of the constituents containspredicates.
The system then attempts to identify seman-tic roles in the other constituent related to this predicate.This process repeats at every step, creating a combinedsyntax-semantics parse forest.
Crucially, this allows usto use features derived from the semantic roles to rankparses inside the packed chart.
This could result in animprovement over ranking completed parses, because re-ranking completed parses requires first generating an n-best list of parse candidates, potentially preventing there-ranker from examining high value parses falling out-side the n-best list.In order to train our parse model, it is necessary to firstemploy a baseline parse model over the training set.
Thebaseline model is a PCFG model, where the products ofthe probabilities of individual rule applications are usedto rank candidate parses.
We use a cross-fold validationtechnique to parse the training set (train on sections 02-20 to parse section 21, train on sections 02-19 and 21 toparse section 20, and so on).
As we parse these sentences,we use the local SRL model described in Section 5 topredict semantic roles inside the packed chart.
We theniterate over the packed chart and extract features basedon the semantic roles in it, effectively learning from ev-ery possible semantic role in the parse forest.
Notice thatthis does not require enumerating every parse in the for-est (which would be prohibitively expensive) ?
the rolesare labeled at parse time and can therefore be read di-rectly from the packed chart.
For each role in the packedchart, we label it as a ?good?
semantic role if it appears inthe human-judged Propbank annotation for that sentence,and a ?bad?
semantic role if it does not.The features extracted from the packed chart are as fol-lows:?
Role.
The semantic role itself, concatenated withthe predicate.
For example, play.Arg1.
This willrepresent the intuition described in Section 1 thatcertain roles are more semantically appealing thanothers.?
Role and Headword.
The semantic role concate-nated with the predicate and the headword of the se-mantic role.
This reflects the idea that certain wordsfit with particular roles better than others.These features are used to train an averaged percep-tron model to distinguish between likely and unlikely se-mantic roles.
We incorporate the perceptron directly withthe parser using a packed feature forest implementation,following an approach used by the current state-of-the-art CCG parser (Clark and Curran, 2004).
By prefer-739ring sentences with good semantic roles, we hope to pro-duce parses that give better overall semantic role predic-tions.
The parser prefers spans with better semantic roles,and breaks ties that would have arisen using the base-line model alone.
Similarly the baseline model can breakties between equivalent semantic roles; this has the addedbenefit of encouraging normal-form derivations in casesof spurious ambiguity.
The result is a single-best com-plete parse with semantic roles already predicted.
Oncethe single-best parse is selected, we allow the global SRLmodel to predict any additional roles over the parse, tocatch those roles that are difficult to predict from localfeatures alone.7 Experiment 1: Choosing a Single-BestDerivation from an N-best ListOur first experiment demonstrates our model?s perfor-mance in a ranking task.
In this task, a list of candidateparses are generated by our baseline model.
This base-line model treats rule applications as a PCFG ?
each ruleapplication (say, np + s\np = s) is given a probability inthe standard way.
The rule probabilities are unsmoothedmaximum likelihood estimates derived from rule countsin the training portion of CCGbank.
After n-best deriva-tions are produced by the baseline model, we use the Bru-tus semantic role labeler to assign roles to each candi-date derivation.
We vary the size of the n-best list from1 to 10 (note that an n-best list of size 1 is equivalent tothe single-best baseline parse).
We then use the seman-tic model to re-rank the candidate parses and produce asingle-best parse.
The outcomes are shown in Table 2.n P R F1 85.1 71.7 77.82 85.9 74.8 79.95 84.5 76.8 80.510 83.7 76.8 80.1C&C 83.6 76.8 80.0Table 2: SRL performance on the development set (section 00)for various values of n. The final row indicates SRL perfor-mance on section 00 parses from the Clark and Curran CCGparser.The availability of even two candidate parses yieldsa 2.1% boost to the balanced F-measure.
This is be-cause the semantic role labeler is very sensitive to syn-tactic attachment decisions, and in many cases the set ofrule applications used in the derivation are very similar oreven the same.
Consider the simplified version of a phe-nomenon found in wsj 0001.1 shown in Figures 5 and 6.The only difference in rule applications in these deriva-tions is whether the temporal adjunct attaches to s[b]\npor s[dcl]\np.
Because the s[dcl]\np case is slightly moreHe will join Nov. 27thnp (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)>s[dcl]\np>s[dcl]\np<s[dcl]Figure 5: The single-best analysis for He will join Nov 27thaccording to the baseline model.
Notice that the temporal ad-junct is attached high, leading the semantic role labeler to failto identify ArgM-TMP.He will join Nov. 27thnp (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)<s[b]\np>s[dcl]\np<s[dcl]Figure 6: The second-best analysis of He will join Nov 27th.This analysis correctly predicts Nov 27th as the ArgM-TMP ofjoin, and the semantic model correctly re-ranks this analysis tothe single-best position.common in the treebank, the baseline model identifies itas the single-best parse, and identifies the derivation infigure 6 as the second-best parse.
The semantic model,however, correctly recognizes that the semantic roles pre-dicted by the derivation in Figure 6 are superior to thosepredicted by the derivation in figure 5.
This demonstrateshow a second or third-best parse according to the baselinemodel can be greatly superior to the single-best in termsof semantics.8 Experiment 2: Choosing a Single-BestDerivation Directly from the PackedChartOne potential weakness with the n-best list approach de-scribed in Section 7 is choosing the size of the n-best list.As the length of the sentence grows, the number of can-didate analyses grows.
Because sentences in the treebankand in real-world applications are of varying length andcomplexity, restricting ourselves to an n-best list of a par-ticular size opens us to considering some badly mangledderivations on short, simple sentences, and not enoughderivations on long, complicated ones.
One possible so-lution to this is to simply choose a single best derivationdirectly from the packed chart using the semantic model,eschewing the baseline model entirely except for break-ing ties.
In this approach, we use the local SRL modeldescribed in section 6 to predict semantic roles at parsetime, inside the packed chart.
This frees us from the740need to have a complete derivation (as in the n-best listapproach in Section 7).
We use the semantic model tochoose a single-best parse from the packed chart, then wepass this complete parse through the global SRL model togive it all the benefits afforded to the parses in the n-bestapproach.
The results for the semantic model comparedto the baseline model are shown in table 3.
Interestingly,Model P R FBaseline 85.1 71.7 77.8Semantic 82.7 70.5 76.1Table 3: A comparison of the performance of the baseline modeland the semantic model on semantic role labeling.
The seman-tic model, when unrestrained by the baseline model, performssubstantially worse.the semantic model performs considerably worse than thebaseline model.
To understand why, it is necessary to re-member that the semantic model uses only semantic fea-tures ?
probabilities of rule applications are not consid-ered.
Therefore, the semantic model is perfectly happy topredict derivations with sequences of highly unlikely ruleapplications so long as they predict a role that the modelhas been trained to prefer.Apparently, the reckless pursuit of appealing semanticroles can ultimately harm semantic role labeling accuracyas well as parse accuracy.
Consider the analysis shownin Figure 7.
Because the averaged perceptron semanticmodel is not sensitive to the relationships between differ-ent semantic roles, and because Arg1 of name is a ?good?semantic role, the semantic model predicts as many ofthem as it can.
The very common np-appositive construc-tion is particularly vulnerable to this kind of error, as itcan be easily mistaken for a three-way coordination (likecarrots, peas and watermelon).
Many of the precisionerrors generated by the local model are of this nature,and the global model is unlikely to remove them, giventhe presence of strong dependencies between each of the?subjects?
and the predicate.Coordination errors are also common when dealing withrelative clause attachment.
Consider the analysis in Fig-ure 8.
To a PCFG model, there is little difference be-tween attaching the relative clause to the researchers orLorillard nor the researchers.
The semantic model, how-ever, would rather predict two semantic roles than justone (because study:Arg0 is a highly appealing semanticrole).
Once again, the pursuit of appealing semantic roleshas led the system astray.We have shown in Section 7 that the semantic modelcan improve SRL performance when it is constrained tothe most likely PCFG derivations, but enumerating n-bestlists is costly and cumbersome.
We can, however, com-bine the semantic model with the baseline PCFG.
Ourmethod for doing this is designed to avoid the kinds of er-ror described above.
We first identify the highest-scoringparse according to the PCFG model.
This parse will beused in later processing unless we are able to identify an-other parse that satisfies the following criteria:1.
It must be closely related to the parse that has thebest score according to the semantic model.
To iden-tify such parses, we ask the chart unpacking algo-rithm to generate all the parses that can be reachedby making up to five attachment changes to this se-mantically preferred parse ?
no more.2.
It must have a PCFG score that is not much less thanthat of the single-best PCFG parse.
We do this byrequiring that it has a score that is within a factor of?
of the best available.
That is, the single-best parsefrom the semantic model must satisfylogP (sem) > logP (baseline) + log(?
)where the ?
value is tuned on the development set.If no semantically preferred parse meets the above cri-teria, the single-best PCFG parse is used.
We find thatthe PCFG-preferred parse is used about 35% of the timeand an alternative used instead about 65% of the time.The SRL performance for this regime, using a range ofcut-off factors, is shown in table 4.
On this basis we se-lect a cut-off of 0.5 as suitable for use for final testing.On the development set this method gives the best pre-cision in extracting dependencies, but is slightly inferiorto the method using a 2-best list on recall and balancedF-measure.Factor (?)
P R F0.5 86.3 71.9 78.50.1 85.4 72.0 78.10.05 85.2 72.0 78.00.005 84.3 71.3 77.3Table 4: SRL accuracy when the semantic model is constrainedby the baseline model9 Results and DiscussionWe use the method for calculating SRL performance de-scribed in the CoNNL 2008 and 2009 shared tasks.
How-ever, because the semantic role labeler we use was not de-signed to work with Nombank (and it is difficult to sepa-rate Nombank and Propbank predicates from the publiclyreleased shared task output), it is not feasible to compareresults with the candidate systems described there.
Wecan, however, compare our two experimental models withour baseline parser and the current state-of-the-art CCG741Arg1 Arg1 Arg1 mod rel Arg2Rudolph Agnew, 61 and the former chairman, was named a nonexecutive directornp np conj np/n n/n n (s\np)/(s\np) (s\np)/np np/n n/n n> >n/n n/n> >np np<?> >np s\np<?> >np s\np<sFigure 7: A parse produced by the unrestricted semantic model.
Notice that Rudolph Agnew, 61 and the former chairman iserroneously treated as a three-way conjunction, assigning semantic roles to all three heads.Arg0 Arg0 rel Arg1Neither Lorillard nor the researchers who studied the workers were awarenp/np np conj np (np\np)/(s\np) (s\np)/np np (s\np)/(s\np) s\np<?> > >np s\np s\np>np\np<np>np>sFigure 8: Relative clause attachment poses problems when preceded by a conjunction ?
the system generally prefers attachingrelative clauses high.
In this case, the relative clause should be attached low.parser (Clark and Curran, 2004).
The results on the testset (WSJ Section 23, <40 words) are shown in Table 5.There are many areas for potential improvement for thesystem.
The test set scores of both of our experimentalmodels are lower than their development set scores,wherethe n-best model outperforms even the Clark and Curranparser in the SRL task.
This may be due to vocabularyissues (we are of course unable to evaluate if the vocab-ulary of the training set more closely resembles the de-velopment set or the test set).
If there are vocabulary is-sues, they could be alleviated by experimenting with POSbased lexical features, or perhaps even generalizing a la-tent semantics over heads of semantic roles (essentiallyidentifying broad categories of words that appear withparticular semantic roles, rather than counting on havingencountered that particular word in training).
Alternately,this drop in performance could be caused by a mismatchin the average length of sentences, which would cause our?
factor and the size of our n-best lists (which were tunedon the development set) to be suboptimal.
We anticipatethe opportunity to further explore better ways of deter-mining n-best list size.
We also anticipate the possibilityof integrating the semantic model with a state-of-the-artCCG parser, potentially freeing the ranker from the limi-tations of a simple PCFG baseline.It is also worth noting that the chart-based model seemsheavily skewed towards precision.
Because the parser candig deeply into the chart, it is capable of choosing a parsethat predicts only semantic roles that it is highly confi-dent about.
By choosing these parses (and not parses withless attractive semantic roles), the model can maximizethe average score of the semantic roles it predicts.
Thistendency towards identifying only the most certain rolesis consistent with high-precision low-recall results.
Then-best parser has a much more restricted set of semanticroles from parses more closely resembling the single-bestparse, and therefore is less likely to be presented with theopportunity to choose parses that do away with less likely(but still reasonable) roles.10 Conclusions and Future WorkIn this paper, we discuss the procedure for identifying se-mantic roles at parse time, and using these roles to guidethe parse.
We demonstrate that using semantic roles toguide parsing can improve overall SRL performance, butthat these same benefits can be realized by re-ranking ann-best list with the same model.
Regardless, there areseveral reasons why it is useful to have the ability to pre-dict semantic roles inside the chart.Predicting semantic roles inside the chart could be usedto perform SRL on very long or unstructured passages.742SRL Labeled DepsModel P R F P R FBaseline 84.7 70.7 77.0 80.0 79.8 79.9Rank n=5 82.0 73.7 77.7 80.1 80.0 80.0Chart 90.0 68.4 77.7 82.3 80.2 81.2C&C 83.3 77.6 80.4 84.9 84.6 84.7Char 77.1 75.5 76.5 - - -Table 5: The full system results on the test set of the WSJcorpus (Section 23).
Included are the baseline parser, the n-best reranking model from Section 7, the single-best chart-unpacking model from Section 8, and the state-of-the-art C&Cparser.
The final row shows the SRL performance obtained byPunyakanok et al (2008) using the Charniak parser.
Unfor-tunately, their results are evaluated based on spans of words(rather than headword labels), which interferes with direct com-parison.
The Charniak parser is a CFG-style parser, making la-beled dependency non-applicable.Most parsing research on the Penn Treebank (the presentwork included) focuses on sentences of 40 words or less,because parsing longer sentences requires an unaccept-ably large amount of computing resources.
In practice,however, semantic roles are rarely very distant from theirpredicates ?
generally they are only a few words away;often they are adjacent.
In long sentences, the predictionof an entire parse may be unnecessary for the purposes ofSRL.The CKY parsing algorithm works by first predicting allconstituents spanning two words, then all constituentsspanning three words, then four, and so on until it pre-dicts constituents covering the whole sentence.
By settinga maximum constituent size (say, ten or fifteen), we couldabandon the goal of completing a spanning analysis in fa-vor of identifying semantic roles in the neighborhood oftheir predicates, eliminating the need to unpack the chartat all.
This could be used to efficiently perform SRL onpoorly structured text or even spoken language transcrip-tions that are not organized into discrete sentences.
Doingso would also eliminate the potentially noisy step of au-tomatically separating out individual sentences in a largertext.
Alternately, roles predicted in the chart could evenbe incorporated into a low-precision-high-recall informa-tion retrieval system seeking a particular semantic rela-tionship by scanning the chart for a particular semanticrole.Another use for the packed forest of semantic roles couldbe to predict complete sets of roles for a given sentenceusing a constraint based method like integer linear pro-gramming.
Integer linear programming takes a largenumber of candidate results (like semantic roles), and ap-plies a set of constraints over them (like ?roles may notoverlap?
or ?no more than one of each role is allowed ineach sentence?)
to find the optimal set.
Doing so couldeliminate the need to unpack the chart at all, effectivelyproducing semantic roles without committing to a singlesyntactic analysis.11 AcknowledgementsWe would like to thank Mike White, William Schuler,Eric Fosler-Lussier, and Matthew Honnibal for their help-ful feedback.ReferencesStephen A. Boxwell, Dennis N. Mehay, and Chris Brew.
2009.Brutus: A semantic role labeling system incorporating CCG,CFG, and Dependency features.
In Proc.
ACL-09.E.
Charniak.
2001.
Immediate-head parsing for language mod-els.
In Proc.
ACL-01, volume 39, pages 116?123.Stephen Clark and James R. Curran.
2004.
Parsing the WSJusing CCG and Log-Linear Models.
In Proc.
ACL-04.J.R.
Finkel and C.D.
Manning.
2009.
Joint parsing and namedentity recognition.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computational Lin-guistics, pages 326?334.
Association for Computational Lin-guistics.Daniel Gildea and Julia Hockenmaier.
2003.
Identifying se-mantic roles using Combinatory Categorial Grammar.
InProc.
EMNLP-03.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Mart??,L.
Ma`rquez, A. Meyers, J. Nivre, S.
Pado?, J.
S?te?pa?nek, et al2009.
The CoNLL-2009 shared task: Syntactic and seman-tic dependencies in multiple languages.
In Proceedings ofthe Thirteenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.
Association forComputational Linguistics.J.
Hockenmaier and M. Steedman.
2005.
CCGbank manual.Technical report, MS-CIS-05-09, University of Pennsylva-nia.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,B.
Young, and R. Grishman.
2004.
The nombank project:An interim report.
In A. Meyers, editor, HLT-NAACL 2004Workshop: Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May 2 - May 7.
Associationfor Computational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of Semantic Roles.Computational Linguistics, 31(1):71?106.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2008.
TheImportance of Syntactic Parsing and Inference in SemanticRole Labeling.
Computational Linguistics, 34(2):257?287.Mark Steedman.
2000.
The Syntactic Process.
MIT Press.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task on joint pars-ing of syntactic and semantic dependencies.
In Proceedings743of the Twelfth Conference on Computational Natural Lan-guage Learning, pages 159?177.
Association for Computa-tional Linguistics.744
