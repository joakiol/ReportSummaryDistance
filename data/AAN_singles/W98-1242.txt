!1IIIIilIISyntaetico-Semantic Learning of Categorial Grammarslsabelle TellierLIFL and Universitd Charles de Gaulle-lille3 CUFR IDIST)59 653 Villeneuve d'Ascq Cedex, FRANCETel : 03-20-41-61-78 ; fax : 03-20-41-61-71te l l i e r@un iv - l i l l e3 .
f r1.
IntroductionNatural language learning seems, from a formalpoint o f  view, an enigma.
As a matter of fact, everyhuman being, given nearly exclusively positiveexamples (as psycholinguists have noticed) is able atthe age of  about five to master his/her mothertongue.
Though, no linguistically interesting class o fformal languages is learnable with positive data inusual models (Gold's (67) and Valiant's (84)).To solve this paradox, various solutions havebeen proposed.
Following the chomskian intuitions(Chomsky 65, 68), it can be admitted that naturallanguages belong to a restricted family and that thehuman mind includes an innate knowing of  thestructure of  this class (Shinohara 90).
Anotherapproach consists in putting structural, statistical orcomplexity constraints on the examples proposed tothe learner, making his/her inferences easier(Sakakibara 92).A particular family of  research, more concernedwith the cognitive relevance of its models, considersthat in a natura l ,  situations, examples are alwaysprovided with semantic and pragmatic informationand tries to make profit of  it (Anderson 77;Hamburger & Wexler 75 ; Hill 83;  Langley 82).This is the family our research belongs to.But the property of  meaningfulness of  naturallanguages is computationally tractable only i f  wehave at our disposal a theory that preciselyarticulates syntax and semantics.
The strongestpossible articulation is known as the Fredge'sprinciple of compositionality.
This principle hasacquired an explicit formulation with the works ofRichard Montague (Dowry, Wall & Peters 81;Montague 74) and his inheritors.We will first briefly recall an adapted version o fthis syntaetico-semantie framework, based on a typeof grammars called << classical categorialg rammars ,  (or CCGs), and we will then show howit can been used in a formal theory of  naturallanguage learning.2.
Syntactic analysis with CCGsA categorial grammar G is a 4-tuple G=<V, C, f, S>with :- V is the finite alphabet (or vocabulary) of  G ;- C is the finite set of basic categories ofG ;From C, we define the set of  all possiblecategories of  G, noted C', as the closure of  C forthe operators / and \.
C' is the smallest set ofcategories verifying :* Cc_C' ;* i f  XeC '  and YeC'  then: X/Y~C' andY~XeC' ;- f is a function : V-->Pf(C') where Pf(C') is theset of  finite subsets of C', which associates eachelement v in V with the finite set f(v)_cC' of itscategories ;- SeC is the axiomatic ategory of  G.In this framework, the set of syntactically correctsentences is the set of finite concatenations o felements of  the vocabulary for which there exists anaffectation of  categories that can be <~ reduced ~ to theaxiomatic category S. In CCGs, the admittedreduction rules for any categories X and Y in C' are :-R1 : X/Y.
Yw> X-R ' I  : Y .
YkXw> XThe language L(G) defined by G is then :L(G)= {weV*; 3neN Vie { 1 ..... n}wieV ,W=Wl...w n and 3Cie f(wi) ,C l..-Cn - - * ' - ->s  }.The class of languages defined by CCGs is theclass of  context-free languages (Bar Hillel, Gaifman& Shamir 60).
CCGs are lexieally oriented becausegrammatical information is entirely supported by thecategories associated with each word.
They are alsowell adapted to natural languages (Oehrle, Bach &Wheeler 88).Example  :Let us define a CCG for the analysis of a smallsubset of  natural anguage, including the vocabularyV={a, every, man, John, Paul, runs, is .... }.
The set o fbasic categories i C={S, T, CN} where T stands fora te rms,  and is affected to proper names, CN meansa common nouns >~, intransitive verbs receive thecategory "l'kS, transitive ones : ('IAS)/T anddeterminers: (S/(T~S))/CN.
Figures 1 and 2 displayanalysis trees.a man FUrlS(S/(T~S))/CN CN TkSS/(TX.S) /figure 1 : analysis tree n ?
1Tellier 311 Syntactico-Semantic LearningIsabelle Tellier (1998) Syntacfico-Semaati?
Learning of Categorical Grammars.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98 Workshop on Paradigms and Grounding inLanguage Learning, ACL, pp 311-314.John isT (T~SYTPaulTfigure 2 : analysis tree n?23.
F rom syntax to semanticsThe key idea of  Montague's work (74) was to definean isomorphism between syntactic trees andsemantic ones.
This definition is the formalexpression of  the principle of compositionality.
Itallows to automatically translate sentences in naturallanguage into formulas of an adapted semanticlanguage that Montague called << intentional logic ,.3.1 The  semant ic  representat ionIntentional Logic (or IL) generalizes the first orderpredicate logic by including typed lambda-calculusand by making a general use of  the notion ofmodality through the concept of  intension (Dowty81).
Only a simplified version of  this framework(not taking into account intensions) is recalled here.- IL is a typed language : the set I of  all possibletypes o f  IL includes* elementary types : ee l  (type of  <~ entities >>)and tel (type of<< truth values >>) ;* for any types uel and vel, <u,v>el (<u,v> isthe type of functions taking an argument oftype u and giving a result of type v).- semantics of IL : a denotation set Dw isassociated with every type we I  as follows :* De=E where E is the denumerable set of allentities of the world ;* D,={O,1}~* D,~,~--D, : the denotation set of  acomposed type is a function.3.2 Translation as an isomorphismEach analysis tree produced by a CCG can be<< translated >> into IL :- translation of the categories into logical types(function k : C' ~> I) :* basic categories : in our example,k(S)--t, k(T)=e, k(CN)=<e,t> ;* derived categories :for any XeC '  and YeC '  :k(X/Y)=k(Y~)=<k(Y),k(X)>.- translation of the words (q : V ?
C' ~> IL) :each couple (v,U) where v is a word in V andUef(v)~_C' is (one of) its eategory(ies) isassociated with a logical formula q(v,U) of  ILwhose type is k(U)eI.
The most usual anduseful translations are :* q(a,(S/(T~S))/CN)=;~,P;~.Q3x\[P(x)AQ(x)\]q(every,(S/(T~S ))/CN)=LP~.QVx\[P(x)-->Q(x)\]where x and y are variables of type e, P and Qvariables of  type <e,t>.
* the verb << to be >>, as a transitive verb, istranslated by :q(is,(T~S)/T)=~xZ.y\[y=x\]with x and y variables of type e.* Every other word w is translated into a logicalconstant noted w'.- translation of the rules of combination :Rules RI and R'I are translated into orientedfunctional applications (Moortgat 88) :W l : f .
x - ->  f(x)W'I : x .
f - ->  f(x)These definitions preserve the correspondencebetween categories of the grammar and types o f  logic.This property assures for example that syntacticallycorrect sentences (of category S) will be translatedinto logical propositions (of type k(S)----t, i.e.
with atruth value).Example :The example sentences analyzed in figures 1 and 2can now be translated into IL, as shown in figures 3and 4 respectively.~.P~.Q3x\[P(x)AQ(x)\] man' run'/ , /   Q3xtp!x?
.
/=~.QBx\[man (x)^Q(~,~\] fZ.Q3x\[man'(x)^Q(x)\](run')=3x\[man'(x)Arun'(x)\]figure 3 : semantic translation of tree n?lJohn' kxky\[y=x\] Paul'\ Wl \  / ,LxLy\[y=x~(Paul )W'l  ~ ~y\[y=Paul'\]~.y\[y=Paur\](John')=\[John'-Paul'\]figure 4 : semantic translation of tree n?24.
The  learn ing  mode l4.1 Innate knowledge and concepts to learnWhen a human being learns a natural language, wesuppose that he has at his disposal sentencesSyntactically correct and semantically relevant.
Thecorresponding situation in our model is an algorithmwhich takes as inputs a sentence that can be analyzedby a CCG together with its logical translation into IL.The innate knowing supposed is reduced to theinference rules R1 and R ' I  and the correspondingtranslation rules WI  and W' I .
As opposed to usualsemantic-based methods of  learning, no wordmeaning is supposed to be initially known.
!1I!IIIIII|IIIIIImIITellier 312 Syntactico-Semantic Learning ?lIIIIII/I/mI IFinally, what does the learner has to learn ?
Inour linguistic framework, syntactic and semanticinformation are attached to the members of thevocabulary by functions f and q.
These functions arethe target outputs of  the algorithm.
More precisely,the syntactic and semantic knowledge to be learnedcan be represented as a finite list of  triplets of theform: (v,U,w) where v~V, Uaf(v)c_C' andw=q(v,U) EIL.Example  :Learning the example grammar previously usedmeans learning the following set :H={(John, T, John'), (Paul, T, Paul'),(is, (T~S)/T, Lx~.y\[y=x\]), (runs, ~S,  run'),(a, (S/(TkS))/CN, ZP3.Q3x\[P(x)^Q(x)\])...}.4.2 The  learn ing  algorithmThe proposed leaning strategy, given in figure 5,consists in building a hypothesis et, updated aftereach new input, to approach the target set.For every couple <s,x(s)> where s is a sentence andx(s) its logical translation in IL, do :- if there is one, affect to the words in s theircategory in the current hypothesis set ;else, make hypotheses on the categoryassociated by fwith the unknown words ofs  ;- For every possible analysis tree :* translate the tree into IL ;* compare the final translation with x(s) andinfer possible values for the unknown semantictranslation of  words to update the currenthypothesis set.Figure 5 : the learning strategy4.3 A deta i led exampleAt the beginning, the current hypothesis et is theempty set.
Let us suppose that the first givenexample is <John runs, nm'(John')>.- the syntactic hypotheses : the only categoriesallowing to build an analysis tree are* fwst possibility : f(John)=A and f(nans)=A\S ;* second one : f(John)=S/B and f(runs)=B.where A and B can be any category in C', basicor not.- the semantic translation :* first possibility : see fig.
6 (the input data areput into rectangles).\[ John runs 1 q(John,A) q(runs,A\S)A \  A)S W' l~  /R' 1 ~ --=> q(runs,A~S)(q(John,A))s I --nan'(J?hn') Ifigure 6 : hypothesis HII f  we compare q(nans,A\S)(q(John,A)) withx(s)=run'(John'), it leads to :Tellier 313q(nan,A\S)=nan' and q(John,A)=John'.So a possible hypothesis set is :H 1 = {(Jolm,A,John'), (runs,A\S,nan') }.Similarly, the  second possibility leads toanother possible hypothesis set :H2={(John,S/B,run'), (mns,B,John')}.At this stage, we have no reason to prefer onehypothesis to the other (the learner does not know thatJohn is linked with John', neither about runs andrun').
The current hypothesis i then : H1 OR H2.
Butsuppose now that a second given example is <Paulruns, nan'(Panl')>.
The same process applies to thisexample, except that a runs >> now belongs twice tothe current hypothesis set.- the syntactic hypotheses : the new sentencetreated with H1 forces to affect he category A to<~ Paul >>, while H2 forces to affect the categoryS/B.- the semantic translation :* in the first possibility, H 1 becomesH 1 '= {(John,A,John'),(nans,A\S,run'),, (Paul,A,Paul') }* it is impossible to provide a value toq(Paul,S/B) following the tree built withhypothesis H2.So H2 is abandoned and only H 1' remains.
It canbe noticed that a similar conclusion would havefollowed if the second example had been :<John sleeps, sleeps'(John')>.Any other example sentence including one of  thewords concerned by the current hypothesis enoughto discredit hypothesis H2.5.
Eva luat ion  and  conc lus ionThe choices made in this model have theoreticalbackgrounds and consequences.First, CCG seem to be particularly adapted to thelearning process.
Recent researches have foundconditions under which the syntax of  these grammarsis learnable (Buszkowski & Penn 90, Kanazawa 96).But, in these frameworks, tree structures are providedas inputs to the learning algorithm : in our model, thesemantic translation plays a close role but in a weakerand more cognitively relevant fashion.
Adriaans (92)also proposed a learning algorithm for categorialgrammars, using both syntactic and semantic inputs,but he treated them separately : the semantic learningcould only start when the syntactic learning wasachieved, instead of  helping it as we propose.Previous models built in the syntactieo-semanticspirit (Anderson 77, Hamburger & Wexler 75, Hill83, Langley 82,) used more traditional syntax andsemantic representations very close to syntacticstructures (Pinker 79) : they failed to representcomplex logical relations like quantification orBoolean operators.
Logical languages like IL aremore powerful and a priori independent fromlinguistic structures.
In fact, our approach assumesthat logic is the natural << language of the mind ,  inthat situations perceived by our learner are supposedSyntactico-Semantic Learningto be automatically translated into logical formulasbefore being compared with linguistic expressions.Fundamentally, what makes natural languageslearnable in our model is the presupposition thatthere exists an isomorphism between the syntax ofsentences and their semantics.
This strong principleof compositionality is contested by some linguistsbut remains an interesting approximation.
The~< graph deformation condition, used in (Anderson77) was a weaker version of it.
Under this condition,the inputs provided to the learner are the leaves androot respectively of two isomorphic trees and what isto be reconstituted is the body of these trees, asdisplayed in figure 6.
But, as opposed to (Anderson77), there is an asymmetry : the formalism chosen isadapted to language analysis but not to languagegeneration.The efficiency of the algorithm seems tocrucially rely on the complexity of the inputrelatively to the current hypothesis.
This complexitycan be measured by the number of new wordsappearing in a sentence xample.
If  few new wordsare introduced in each new example, the number ofhypotheses to explore will remain reasonable.
Else,the learning may be too complicated.
Of course, thisvaluable intuition still needs to be formulated andproved in a more formal way.It is not possible to develop here how to treat hecases when a word needs more than one category,but it remains possible to learn in this context.
Thelearning is incremental.The framework is still incomplete because wehaven't chosen any learning model and we haven'tproved the learnability of any language in it with ourstrategy.
An extended and more general version ofthe algorithm in figure 5, using Lambek grammars(Lambek 58), is being implemented and tested.
Butthe approach seems original and interesting enoughto be developed further.6.
BibliographyAdriaans, P. W. (1992).
Language Learning from aCategorial Perspective, Doctoral dissertation,University of Amsterdam.Anderson, J., R. (1977).
Induction of AugmentedTransition Networks.
Cognitive Science, 1, 125-157.Bar Hillel, Y.
(1953).
A quasi-arithmetical notationfor syntactic description.
Language 29, 47-58.Bar Hillel, Y., Gaifman, C. & Shamir, E. (1960).
OnCategorial and Phrase Structure Grammars,Bulletin of the Research Council of Israel.
9F, 1-16.Buszkowski, W., Penn, G. (1990).
CategorialGrammars Determined from Linguistic Data byUnification, Studia Logica.
49, 431-454.Chomsky, N. (1965).
Aspects of the Theory ofSyntax, Cambridge, MIT Press.Chomsky, N. (1968).
Language and Mind.
Brace &World.Dowty, D. R., Wall, R. E., Peters, S. (1981).Introduction to Montague Semantics.
Reidel,Dordrecht.Gold, E. M. (1967).
Language Identification in theLimit.
Information and Control, 10, 447-474.Hamburger, H., Wexler, K. (1975).
A MathematicalTheory of Learning Transformational Grammar.Journal of Mathematical Psychology, 12, 137-177.Hill, J.
A. C. (1983).
A computational model oflanguage acquisition in the two-year-old.Cognition and Brain Theory, 6(3), 287-317.Kanazawa, M. (1996).
Identification i  the Limit ofCategorial Grammars.
Journal of Logic, Language& Information, 5(2), 115-155.Lambek, J.
(1958).
The Mathematics of SentenceStructures., American Mathematical Monthly, 65,154-170.Langley, P. (1982).
Language acquisition througherror discovery.
Cognition and Brain Theory, 5,211-255.Montague, R. (1974).
Formal Philosophy; Selectedpapers of Richard Montague.
Yale UniversityPress, New Haven.Moortgat, M. (1988).
Categorial investigations,logical and linguistic aspects of the LambekCalculus.
Foris, Dordrecht.Oehrle, R. T., Bach, E., & Wheeler, D.
(Eds.)
(1988).Categorial Grammars and Natural LanguageStructure.
Reidel, Dordrecht.Pinker, S. (1979).
Formal models of languagelearning.
Cognition, 7, 217-283.Shinohara, , T. (1990).
Inductive inference ofmonotonic formal systems from positive data,in Axikam, S., Goto, S., Oshuga, S. & Yokomori,T.
(Eds) Algorithmic Learning Theory, 339-351,Ohmsha nd New York and Berlin, Springer.Sakakibara, Y.
(1992).
Efficient learning of context-free grammars from positive structural examples.Information & Computation, 97, 23-60.Valiant, L. G. (1984).
A theory of the learnable.Communication f the ACM, 1134-1142.Tellier 314 Syntactico-Semantic LearningI|IIIIIIlI
