Proceedings of the 43rd Annual Meeting of the ACL, pages 107?114,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Distributional Inclusion Hypotheses and Lexical EntailmentMaayan GeffetSchool of Computer Science and EngineeringHebrew University, Jerusalem, Israel, 91904mary@cs.huji.ac.ilIdo DaganDepartment of Computer ScienceBar-Ilan University, Ramat-Gan, Israel, 52900dagan@cs.biu.ac.ilAbstractThis paper suggests refinements for theDistributional Similarity Hypothesis.
Ourproposed hypotheses relate the distribu-tional behavior of pairs of words to lexicalentailment ?
a tighter notion of semanticsimilarity that is required by many NLPapplications.
To automatically explore thevalidity of the defined hypotheses we de-veloped an inclusion testing algorithm forcharacteristic features of two words, whichincorporates corpus and web-based featuresampling to overcome data sparseness.
Thedegree of hypotheses validity was then em-pirically tested and manually analyzed withrespect to the word sense level.
In addition,the above testing algorithm was exploitedto improve lexical entailment acquisition.1 IntroductionDistributional Similarity between words has beenan active research area for more than a decade.
It isbased on the general idea of Harris' DistributionalHypothesis, suggesting that words that occurwithin similar contexts are semantically similar(Harris, 1968).
Concrete similarity measures com-pare a pair of weighted context feature vectors thatcharacterize two words (Church and Hanks, 1990;Ruge, 1992; Pereira et al, 1993; Grefenstette,1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002;Weeds and Weir, 2003).As it turns out, distributional similarity capturesa somewhat loose notion of semantic similarity(see Table 1).
It does not ensure that the meaningof one word is preserved when replacing it withthe other one in some context.However, many semantic information-orientedapplications like Question Answering, InformationExtraction and Paraphrase Acquisition require atighter similarity criterion, as was also demon-strated by papers at the recent PASCAL Challengeon Recognizing Textual Entailment (Dagan et al,2005).
In particular, all these applications need toknow when the meaning of one word can be in-ferred (entailed) from another word, so that oneword could substitute the other in some contexts.This relation corresponds to several lexical seman-tic relations, such as synonymy, hyponymy andsome cases of meronymy.
For example, in Ques-tion Answering, the word company in a questioncan be substituted in the text by firm (synonym),automaker (hyponym) or division (meronym).
Un-fortunately, existing manually constructed re-sources of lexical semantic relations, such asWordNet, are not exhaustive and comprehensiveenough for a variety of domains and thus are notsufficient as a sole resource for application needs1.Most works that attempt to learn such concretelexical semantic relations employ a co-occurrencepattern-based approach (Hearst, 1992; Ravi-chandran and Hovy, 2002; Moldovan et al, 2004).Typically, they use a set of predefined lexico-syntactic patterns that characterize specific seman-tic relations.
If a candidate word pair (like com-pany-automaker) co-occurs within the samesentence satisfying a concrete pattern (like "?companies, such as automakers"), then it is ex-pected that the corresponding semantic relationholds between these words (hypernym-hyponym inthis example).In recent work (Geffet and Dagan, 2004) weexplored the correspondence between the distribu-tional characterization of two words (which mayhardly co-occur, as is usually the case for syno-1We found that less than 20% of the lexical entailment relations extracted by ourmethod appeared as direct or indirect WordNet relations (synonyms, hyponymsor meronyms).107nyms) and the kind of tight semantic relationshipthat might hold between them.
We formulated alexical entailment relation that corresponds to theabove mentioned substitutability criterion, and istermed meaning entailing substitutability (whichwe term here for brevity as lexical entailment).Given a pair of words, this relation holds if thereare some contexts in which one of the words canbe substituted by the other, such that the meaningof the original word can be inferred from the newone.
We then proposed a new feature weightingfunction (RFF) that yields more accurate distribu-tional similarity lists, which better approximate thelexical entailment relation.
Yet, this method stillapplies a standard measure for distributional vectorsimilarity (over vectors with the improved featureweights), and thus produces many loose similari-ties that do not correspond to entailment.This paper explores more deeply the relationshipbetween distributional characterization of wordsand lexical entailment, proposing two new hy-potheses as a refinement of the distributional simi-larity hypothesis.
The main idea is that if one wordentails the other then we would expect that virtu-ally all the characteristic context features of theentailing word will actually occur also with theentailed word.To test this idea we developed an automaticmethod for testing feature inclusion between a pairof words.
This algorithm combines corpus statis-tics with a web-based feature sampling technique.The web is utilized to overcome the data sparse-ness problem, so that features which are not foundwith one of the two words can be considered astruly distinguishing evidence.Using the above algorithm we first tested theempirical validity of the hypotheses.
Then, wedemonstrated how the hypotheses can be leveragedin practice to improve the precision of automaticacquisition of the entailment relation.2 Background2.1 Implementations of Distribu-tional  SimilarityThis subsection reviews the relevant details of ear-lier methods that were utilized within this paper.In the computational setting contexts of wordsare represented by feature vectors.
Each word w isrepresented by a feature vector, where an entry inthe vector corresponds to a feature f. Each featurerepresents another word (or term) with which w co-occurs, and possibly specifies also the syntacticrelation between the two words as in (Grefenstette,1994; Lin, 1998; Weeds and Weir, 2003).
Padoand Lapata (2003) demonstrated that using syntac-tic dependency-based vector space models can helpdistinguish among classes of different lexical rela-tions, which seems to be more difficult for tradi-tional ?bag of words?
co-occurrence-based models.A syntactic feature is defined as a triple <term,syntactic_relation, relation_direction> (the direc-tion is set to 1, if the feature is the word?s modifierand to 0 otherwise).
For example, given the word?company?
the feature <earnings_report, gen, 0>(genitive) corresponds to the phrase ?company?searnings report?, and <profit, pcomp, 0> (preposi-tional complement) corresponds to ?the profit ofthe company?.
Throughout this paper we used syn-tactic features generated by the Minipar depend-ency parser (Lin, 1993).The value of each entry in the feature vector isdetermined by some weight function weight(w,f),which quantifies the degree of statistical associa-tion between the feature and the correspondingword.
The most widely used association weightfunction is (point-wise) Mutual Information (MI)(Church and Hanks, 1990; Lin, 1998; Dagan, 2000;Weeds et al, 2004).<=> element, component <=> gap, spread *      town, airport <=   loan, mortgage=>   government, body *      warplane, bomb <=> program, plan *      tank, warplane*      match, winner =>   bill, program <=   conflict, war =>   town, locationTable 1: Sample of the data set of top-40 distributionally similar word pairs produced by the RFF-based method of (Geffet and Dagan, 2004).
Entailment judgments are marked by the arrow direction,with '*' denoting no entailment.108Once feature vectors have been constructed, thesimilarity between two words is defined by somevector similarity metric.
Different metrics havebeen used, such as weighted Jaccard (Grefenstette,1994; Dagan, 2000), cosine (Ruge, 1992), variousinformation theoretic measures (Lee, 1997), andthe widely cited and competitive (see (Weeds andWeir, 2003)) measure of Lin (1998) for similaritybetween two words, w and v, defined as follows:,),(),(),(),(),()()()()( ???
?++=fvweightfwweightfvweightfwweightvwsimvFfwFfvFwFfLinwhere F(w) and F(v) are the active features of thetwo words (positive feature weight) and the weightfunction is defined as MI.
As typical for vectorsimilarity measures, it assigns high similarityscores if many of the two word?s features overlap,even though some prominent features might bedisjoint.
This is a major reason for getting suchsemantically loose similarities, like company -government and country - economy.Investigating the output of Lin?s (1998) similar-ity measure with respect to the above criterion in(Geffet and Dagan, 2004), we discovered that thequality of similarity scores is often hurt by inaccu-rate feature weights, which yield rather noisy fea-ture vectors.
Hence, we tried to improve thefeature weighting function to promote those fea-tures that are most indicative of the word meaning.A new weighting scheme was defined for boot-strapping feature weights, termed RFF (RelativeFeature Focus).
First, basic similarities are gener-ated by Lin?s measure.
Then, feature weights arerecalculated, boosting the weights of features thatcharacterize many of the words that are most simi-lar to the given one2.
As a result the most promi-nent features of a word are concentrated within thetop-100 entries of the vector.
Finally, word simi-larities are recalculated by Lin's metric over thevectors with the new RFF weights.The lexical entailment prediction task of(Geffet and Dagan, 2004) measures how many ofthe top ranking similarity pairs produced by the2In concrete terms RFF is defined by: ?
?= ),()()(),( vwsimwNfWSvfwRFF ,where sim(w,v) is an initial approximation of the similarity space by Lin?smeasure, WS(f) is a set of words co-occurring with feature f, and N(w) is the setof the most similar words of w by Lin?s measure.RFF-based metric hold the entailment relation, inat least one direction.
To this end a data set of1,200 pairs was created, consisting of top-N(N=40) similar words of 30 randomly selectednouns, which were manually judged by the lexicalentailment criterion.
Quite high Kappa agreementvalues of 0.75 and 0.83 were reported, indicatingthat the entailment judgment task was reasonablywell defined.
A subset of the data set is demon-strated in Table 1.The RFF weighting produced 10% precisionimprovement over Lin?s original use of MI, sug-gesting the RFF capability to promote semanticallymeaningful features.
However, over 47% of theword pairs in the top-40 similarities are not relatedby entailment, which calls for further improve-ment.
In this paper we use the same data set 3 andthe RFF metric as a basis for our experiments.2.2 Predicting  Semantic InclusionWeeds et al (2004) attempted to refine the distri-butional similarity goal to predict whether oneterm is a generalization/specification of the other.They present a distributional generality conceptand expect it to correlate with semantic generality.Their conjecture is that the majority of the featuresof the more specific word are included in the fea-tures of the more general one.
They define the fea-ture recall of w with respect to v as the weightedproportion of features of v that also appear in thevector of w. Then, they suggest that a hypernymwould have a higher feature recall for its hypo-nyms (specifications), than vice versa.However, their results in predicting the hy-ponymy-hyperonymy direction (71% precision) arecomparable to the na?ve baseline (70% precision)that simply assumes that general words are morefrequent than specific ones.
Possible sources ofnoise in their experiment could be ignoring wordpolysemy and data sparseness of word-feature co-occurrence in the corpus.3 The Distributional Inclusion Hy-pothesesIn this paper we suggest refined versions of thedistributional similarity hypothesis which relatedistributional behavior with lexical entailment.3 Since the original data set did not include the direction of entailment, we haveenriched it by adding the judgments of entailment direction.109Extending the rationale of Weeds et al, wesuggest that if the meaning of a word v entails an-other word w then it is expected that all the typicalcontexts (features) of v will occur also with w. Thatis, the characteristic contexts of v are expected tobe included within all w's contexts (but not neces-sarily amongst the most characteristic ones for w).Conversely, we might expect that if v's characteris-tic contexts are included within all w's contextsthen it is likely that the meaning of  v does entailw.
Taking both directions together, lexical entail-ment is expected to highly correlate with character-istic feature inclusion.Two additional observations are needed beforeconcretely formulating these hypotheses.
As ex-plained in Section 2, word contexts should be rep-resented by syntactic features, which are morerestrictive and thus better reflect the restrained se-mantic meaning of the word (it is difficult to tieentailment to looser context representations, suchas co-occurrence in a text window).
We also noticethat distributional similarity principles are intendedto hold at the sense level rather than the wordlevel, since different senses have different charac-teristic contexts (even though computational com-mon practice is to work at the word level, due tothe lack of robust sense annotation).We can now define the two distributional inclu-sion hypotheses, which correspond to the two di-rections of inference relating distributional featureinclusion and lexical entailment.
Let vi and wj betwo word senses of the words w and v, correspond-ingly, and let vi => wj denote the (directional) en-tailment relation between these senses.
Assumefurther that we have a measure that determines theset of characteristic features for the meaning ofeach word sense.
Then we would hypothesize:Hypothesis I:If vi => wj then all the characteristic (syntactic-based) features of vi are expected to appear with wj.Hypothesis II:If all the characteristic (syntactic-based) features ofvi appear with wj then we expect that vi => wj.4 Word Level Testing of Feature In-clusionTo check the validity of the hypotheses we need totest feature inclusion.
In this section we present anautomated word-level feature inclusion testingmethod, termed ITA (Inclusion Testing Algorithm).To overcome the data sparseness problem we in-corporated web-based feature sampling.
Given atest pair of words, three main steps are performed,as detailed in the following subsections:Step 1: Computing the set of characteristic featuresfor each word.Step 2: Testing feature inclusion for each pair, inboth directions, within the given corpus data.Step 3: Complementary testing of feature inclusionfor each pair in the web.4.1 Step 1: Corpus-based generationof characteristic featuresTo implement the first step of the algorithm, theRFF weighting function is exploited and its top-100 weighted features are taken as most character-istic for each word.
As mentioned in Section 2,(Geffet and Dagan, 2004) shows that RFF yieldshigh concentration of good features at the top ofthe vector.4.2 Step 2: Corpus-based featureinclusion testWe first check feature inclusion in the corpus thatwas used to generate the characteristic feature sets.For each word pair (w, v) we first determine whichfeatures of w do co-occur with v in the corpus.
Thesame is done to identify features of v that co-occurwith w in the corpus.4.3 Step 3: Complementary Web-based Inclusion TestThis step is most important to avoid inclusionmisses due to the data sparseness of the corpus.
Afew recent works (Ravichandran and Hovy, 2002;Keller et al, 2002; Chklovski and Pantel, 2004)used the web to collect statistics on word co-occurrences.
In a similar spirit, our inclusion test iscompleted by searching the web for the missing(non-included) features on both sides.
We call thisweb-based technique mutual web-sampling.
Theweb results are further parsed to verify matching ofthe feature's syntactic relationship.110We denote the subset of w's features that aremissing for v as M(w, v) (and equivalently M(v,w)).
Since web sampling is time consuming werandomly sample a subset of k features (k=20 inour experiments), denoted as M(v,w,k).Mutual Web-sampling Procedure:For each pair (w, v) and their k-subsetsM(w, v, k) and M(v, w, k) execute:1.
Syntactic Filtering of ?Bag-of-Words?
Search:Search the web for sentences including v and a fea-ture f from M(w, v, k) as ?bag of words?, i. e. sen-tences where w and f appear in any distance and ineither order.
Then filter out the sentences that donot match the defined syntactic relation between fand v (based on parsing).
Features that co-occurwith w in the correct syntactic relation are removedfrom M(w, v, k).
Do the same search and filteringfor w and features from M(v, w, k).2.
Syntactic Filtering of ?Exact String?
Matching:On the missing features on both sides (which areleft in M(w, v, k) and M(v, w, k) after stage 1), ap-ply ?exact string?
search of the web.
For this, con-vert the tuple (v, f) to a string by addingprepositions and articles where needed.
For exam-ple, for (element, <project, pcomp_of, 1>) gener-ate the corresponding string ?element of theproject?
and search the web for exact matches ofthe string.
Then validate the syntactic relationshipof f and v in the extracted sentences.
Remove thefound features from M(w, v, k) and M(v, w, k), re-spectively.3.
Missing Features Validation:Since some of the features may be too infrequentor corpus-biased, check whether the remainingmissing features do co-occur on the web with theiroriginal target words (with which they did occur inthe corpus data).
Otherwise, they should not beconsidered as valid misses and are also removedfrom M(w, v, k) and M(v, w, k).Output: Inclusion in either direction holds if thecorresponding set of missing features is nowempty.We also experimented with features consisting ofwords without syntactic relations.
For example,exact string, or bag-of-words match.
However, al-most all the words (also non-entailing) were foundwith all the features of each other, even for seman-tically implausible combinations (e.g.
a word and afeature appear next to each other but belong to dif-ferent clauses of the sentence).
Therefore we con-clude that syntactic relation validation is veryimportant, especially on the web, in order to avoidcoincidental co-occurrences.5 Empirical ResultsTo test the validity of the distributional inclusionhypotheses we performed an empirical analysis ona selected test sample using our automated testingprocedure.5.1 Data and settingWe experimented with a randomly picked testsample of about 200 noun pairs of 1,200 pairs pro-duced by RFF (for details see Geffet and Dagan,2004) under Lin?s similarity scheme (Lin, 1998).The words were judged by the lexical entailmentcriterion (as described in Section 2).
The originalpercentage of correct (52%) and incorrect (48%)entailments was preserved.To estimate the degree of validity of the distri-butional inclusion hypotheses we decomposedeach word pair of the sample (w, v) to two direc-tional pairs ordered by potential entailment direc-tion: (w, v) and (v, w).
The 400 resulting orderedpairs are used as a test set in Sections 5.2 and 5.3.Features were computed from co-occurrences ina subset of the Reuters corpus of about 18 millionwords.
For the web feature sampling the maximalnumber of web samples for each query (word -feature) was set to 3,000 sentences.5.2 Automatic Testing the Validityof the Hypotheses at the  WordLevelThe test set of 400 ordered pairs was examined interms of entailment (according to the manualjudgment) and feature inclusion (according to theITA algorithm), as shown in Table 2.According to Hypothesis I we expect that a pair(w, v) that satisfies entailment will also preservefeature inclusion.
On the other hand, by Hypothe-sis II if all the features of w are included by v thenwe expect that w entails v.111We observed that Hypothesis I is better attestedby our data than the second hypothesis.
Thus 86%(97 out of 113) of the entailing pairs fulfilled theinclusion condition.
Hypothesis II holds for ap-proximately 70% (97 of 139) of the pairs for whichfeature inclusion holds.
In the next section we ana-lyze the cases of violation of both hypotheses andfind that the first hypothesis held to an almost per-fect extent with respect to word senses.It is also interesting to note that thanks to theweb-sampling procedure over 90% of the non-included features in the corpus were found on theweb, while most of the missing features (in theweb) are indeed semantically implausible.5.3 Manual Sense Level Testing ofHypotheses ValiditySince our data was not sense tagged, the automaticvalidation procedure could only test the hypothesesat the word level.
In this section our goal is to ana-lyze the findings of our empirical test at the wordsense level as our hypotheses were defined forsenses.
Basically, two cases of hypotheses invalid-ity were detected:Case 1: Entailments with non-included features(violation of Hypothesis I);Case 2: Feature Inclusion for non-entailments(violation of Hypothesis II).At the word level we observed 14% invalid pairsof the first case and 30% of the second case.
How-ever, our manual analysis shows, that over 90% ofthe first case pairs were due to a different sense ofone of the entailing word, e.g.
capital - town (capi-tal as money) and spread - gap (spread as distribu-tion) (Table 3).
Note that ambiguity of the entailedword does not cause errors (like town ?
area, areaas domain) (Table 3).
Thus the first hypothesisholds at the sense level for over 98% of the cases(Table 4).Two remaining invalid instances of the first casewere due to the web sampling method limitationsand syntactic parsing filtering mistakes, especiallyfor some less characteristic and infrequent featurescaptured by RFF.
Thus, in virtually all the exam-ples tested in our experiment Hypothesis I wasvalid.We also explored the second case of invalidpairs: non-entailing words that pass the feature in-clusion test.
After sense based analysis their per-centage was reduced slightly to 27.4%.
Threepossible reasons were discovered.
First, there arewords with features typical to the general meaningof the domain, which tend to be included by manyother words of this domain, like valley ?
town.
Thefeatures of valley (?eastern valley?, ?central val-ley?, ?attack in valley?, ?industry of the valley?
)are not discriminative enough to be distinguishedfrom town, as they are all characteristic to any geo-graphic location.InclusionEntailment+     -+      97       16-      42           245Table 2: Distribution of 400 entailing/non-entailing ordered pairs that hold/do not holdfeature inclusion at the word level.InclusionEntailment+     -+        111       2-        42       245Table 4: Distribution of the entailing/non-entailing ordered pairs that hold/do not holdfeature inclusion at the sense level.spread ?
gap (mutually entail each other)<weapon, pcomp_of>The Committee was discussing the Pro-gramme of the ?Big Eight,?
aimed againstspread of weapon of mass destruction.town ?
area (?town?
entails ?area?
)<cooperation, pcomp_for>This is a promising area for cooperation andexchange of experiences.capital ?
town (?capital?
entails ?town?
)<flow, nn>Offshore financial centers affect cross-bordercapital flow in China.Table 3: Examples of ambiguity of entailment-related words, where the disjoint features be-long to a different sense of the word.112The second group consists of words that can beentailing, but only in a context-dependent (ana-phoric) manner rather than ontologically.
For ex-ample, government and neighbour, whileneighbour is used in the meaning of ?neighbouring(country) government?.
Finally, sometimes one orboth of the words are abstract and general enoughand also highly ambiguous to appear with a widerange of features on the web, like element (vio-lence ?
element, with all the tested features of vio-lence included by element).To prevent occurrences of the second case morecharacteristic and discriminative features should beprovided.
For this purpose features extracted fromthe web, which are not domain-biased (like fea-tures from the corpus) and multi-word featuresmay be helpful.
Overall, though, there might beinherent cases that invalidate Hypothesis II.6 Improving Lexical Entailment Pre-diction by ITA (Inclusion TestingAlgorithm)In this section we show that ITA can be practicallyused to improve the (non-directional) lexical en-tailment prediction task described in Section 2.Given the output of the distributional similaritymethod, we employ ITA at the word level to filterout non-entailing pairs.
Word pairs that satisfy fea-ture inclusion of all k features (at least in one direc-tion) are claimed as entailing.The same test sample of 200 word pairs men-tioned in Section 5.1 was used in this experiment.The results were compared to RFF under Lin?ssimilarity scheme (RFF-top-40 in Table 5).Precision was significantly improved, filteringout 60% of the incorrect pairs.
On the other hand,the relative recall (considering RFF recall as100%) was only reduced by 13%, consequentlyleading to a better relative F1, when consideringthe RFF-top-40 output as 100% recall (Table 5).Since our method removes about 35% of theoriginal top-40 RFF output, it was interesting tocompare our results to simply cutting off the 35%of the lowest ranked RFF words (top-26).
Thecomparison to the baseline (RFF-top-26 in Table5) showed that ITA filters the output much betterthan just cutting off the lowest ranking similarities.We also tried a couple of variations on featuresampling for the web-based procedure.
In one ofour preliminary experiments we used the top-kRFF features instead of random selection.
But weobserved that top ranked RFF features are less dis-criminative than the random ones due to the natureof the RFF weighting strategy, which promotesfeatures shared by many similar words.
Then, weattempted doubling the sampling to 40 random fea-tures.
As expected the recall was slightly de-creased, while precision was increased by over 5%.In summary, the behavior of ITA sampling ofk=20 and k=40 features is closely comparable(ITA-20 and ITA-40 in Table 5, respectively)4.7 Conclusions and Future WorkThe main contributions of this paper were:1.
We defined two Distributional Inclusion Hy-potheses that associate feature inclusion with lexi-cal entailment at the word sense level.
TheHypotheses were proposed as a refinement forHarris?
Distributional hypothesis and as an exten-sion to the classic distributional similarity scheme.2.
To estimate the empirical validity of the de-fined hypotheses we developed an automatic inclu-sion testing algorithm (ITA).
The core of thealgorithm is a web-based feature inclusion testingprocedure, which helped significantly to compen-sate for data sparseness.3.
Then a thorough analysis of the data behaviorwith respect to the proposed hypotheses was con-ducted.
The first hypothesis was almost fully at-tested by the data, particularly at the sense level,while the second hypothesis did not fully hold.4.
Motivated by the empirical analysis we pro-posed to employ ITA for the practical task of im-proving lexical entailment acquisition.
Thealgorithm was applied as a filtering technique onthe distributional similarity (RFF) output.
We ob-4The ITA-40 sampling fits the analysis from section 5.2 and 5.3 as well.Method Precision Recall F1ITA-20 0.700 0.875 0.777ITA-40 0.740 0.846 0.789RFF-top-40 0.520 1.000 0.684RFF-top-26 0.561 0.701 0.624Table 5: Comparative results of using thefilter, with 20 and 40 feature sampling, com-pared to RFF top-40 and RFF top-26 simi-larities.
ITA-20 and ITA-40 denote the web-sampling method with 20 and random 40features, respectively.113tained 17% increase of precision and succeeded toimprove relative F1 by 15% over the baseline.Although the results were encouraging our man-ual data analysis shows that we still have to handleword ambiguity.
In particular, this is important inorder to be able to learn the direction of entailment.To achieve better precision we need to increasefeature discriminativeness.
To this end syntacticfeatures may be extended to contain more than oneword, and ways for automatic extraction of fea-tures from the web (rather than from a corpus) maybe developed.
Finally, further investigation ofcombining the distributional and the co-occurrencepattern-based approaches over the web is desired.AcknowledgementWe are grateful to Shachar Mirkin for his help inimplementing the web-based sampling procedureheavily employed in our experiments.
We thankIdan Szpektor for providing the infrastructure sys-tem for web-based data extraction.ReferencesChklovski, Timothy and Patrick Pantel.
2004.VERBOCEAN: Mining the Web for Fine-Grained Se-mantic Verb Relations.
In Proc.
of EMNLP-04.
Bar-celona, Spain.Church, Kenneth W. and Hanks Patrick.
1990.
Wordassociation norms, mutual information, and Lexicog-raphy.
Computational Linguistics, 16(1), pp.
22?29.Dagan, Ido.
2000.
Contextual Word Similarity, in RobDale, Hermann Moisl and Harold Somers (Eds.
),Handbook of Natural Language Processing, MarcelDekker Inc, 2000, Chapter 19, pp.
459-476.Dagan, Ido, Oren Glickman and Bernardo Magnini.2005.
The PASCAL Recognizing Textual EntailmentChallenge.
In Proc.
of the PASCAL ChallengesWorkshop for Recognizing Textual Entailment.Southampton, U.K.Geffet, Maayan and Ido Dagan, 2004.
Feature VectorQuality and Distributional Similarity.
In Proc.
of Col-ing-04.
Geneva.
Switzerland.Grefenstette, Gregory.
1994.
Exploration in AutomaticThesaurus Discovery.
Kluwer Academic Publishers.Harris, Zelig S. Mathematical structures of language.Wiley, 1968.Hearst, Marti.
1992.
Automatic acquisition of hypo-nyms from large text corpora.
In Proc.
of COLING-92.
Nantes, France.Keller, Frank, Maria Lapata, and Olga Ourioupina.2002.
Using the Web to Overcome Data Sparseness.In Jan Hajic and Yuji Matsumoto, eds., In Proc.
ofEMNLP-02.
Philadelphia, PA.Lee, Lillian.
1997.
Similarity-Based Approaches toNatural Language Processing.
Ph.D. thesis, HarvardUniversity, Cambridge, MA.Lin, Dekang.
1993.
Principle-Based Parsing withoutOvergeneration.
In Proc.
of ACL-93.
Columbus,Ohio..Lin, Dekang.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proc.
of COLING?ACL98,Montreal, Canada.Moldovan, Dan, Badulescu, A., Tatu, M., Antohe, D.,and Girju, R. 2004.
Models for the semantic classifi-cation of noun phrases.
In Proc.
of HLT/NAACL-2004 Workshop on Computational Lexical Seman-tics.
Boston.Pado, Sebastian and Mirella Lapata.
2003.
Constructingsemantic space models from parsed corpora.
In Proc.of ACL-03, Sapporo, Japan.Pantel, Patrick and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
In Proc.
of ACM SIGKDDConference on Knowledge Discovery and Data Min-ing (KDD-02).
Edmonton, Canada.Pereira, Fernando, Tishby Naftali, and Lee Lillian.1993.
Distributional clustering of English words.
InProc.
of ACL-93.
Columbus, Ohio.Ravichandran, Deepak and Eduard Hovy.
2002.
Learn-ing Surface Text Patterns for a Question AnsweringSystem.
In Proc.
of ACL-02.
Philadelphia, PA.Ruge,  Gerda.
1992.
Experiments on linguistically-based term associations.
Information Processing &Management, 28(3), pp.
317?332.Weeds, Julie and David Weir.
2003.
A General Frame-work for Distributional Similarity.
In Proc.
ofEMNLP-03.
Sapporo, Japan.Weeds, Julie, D. Weir, D. McCarthy.
2004.
Characteriz-ing Measures of Lexical Distributional Similarity.
InProc.
of Coling-04.
Geneva, Switzerland.114
