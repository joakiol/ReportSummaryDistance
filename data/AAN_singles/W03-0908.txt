Towards Light Semantic Processing for Question AnsweringBenjamin Van Durme?, Yifen Huang?, Anna Kups?c?
?+, Eric Nyberg?
?Language Technologies Institute, Carnegie Mellon University+Polish Academy of Sciences{vandurme,hyifen,aniak,ehn}@cs.cmu.eduAbstractThe paper1 presents a lightweight knowledge-based reasoning framework for the JAVELINopen-domain Question Answering (QA) sys-tem.
We propose a constrained representationof text meaning, along with a flexible unifica-tion strategy that matches questions with re-trieved passages based on semantic similaritiesand weighted relations between words.1 IntroductionModern Question Answering (QA) systems aim at pro-viding answers to natural language questions in an open-domain context.
This task is usually achieved by com-bining information retrieval (IR) with information extrac-tion (IE) techniques, modified to be applicable to unre-stricted texts.
Although semantics-poor techniques, suchas surface pattern matching (Soubbotin, 2002; Ravichan-dran and Hovy, 2002) or statistical methods (Ittycheriahet al, 2002), have been successful in answering fac-toid questions, more complex tasks require a consider-ation of text meaning.
This requirement has motivatedwork on QA systems to incorporate knowledge process-ing components such as semantic representation, ontolo-gies, reasoning and inference engines, e.g., (Moldovan etal., 2003), (Hovy et al, 2002), (Chu-Carroll et al, 2003).Since world knowledge databases for open-domain tasksare unavailable, alternative approaches for meaning rep-resentation must be adopted.
In this paper, we presentour preliminary approach to semantics-based answer de-tection in the JAVELIN QA system (Nyberg et al, 2003).In contrast to other QA systems, we are trying to realizea formal model for a lightweight semantics-based open-domain question answering.
We propose a constrainedsemantic representation as well as an explicit unification1The authors appear in alphabetical order.framework based on semantic similarities and weightedrelations between words.
We obtain a lightweight roboustmechanism to match questions with answer candidates.The organization of the paper is as follows: Section 2briefly presents system components; Section 3 discussessyntactic processing strategies; Sections 4 and 5 describeour preliminary semantic representation and the unifica-tion framework which assigns confidence values to an-swer candidates.
The final section contains a summaryand future plans.2 System ComponentsThe JAVELIN system consists of four basic components:a question analysis module, a retrieval engine, a passageanalysis module (supporting both statistical and NLPtechniques), and an answer selection module.
JAVELINalso includes a planner module, which supports feedbackloops and finer control over specific components (Nyberget al, 2003).
In this paper we are concerned with the twocomponents which support linguistic analysis: the ques-tion analysis and passage understanding modules (Ques-tion Analyzer and Information Extractor, respectively).The relevant aspects of syntactic processing in both mod-ules are presented in Section 3, whereas the semantic rep-resentation is introduced in Section 4.3 ParsingThe system employs two different parsing techniques:a chart parser with hand-written grammars for ques-tion analysis, and a lexicalized, broad coverage skippingparser for passage analysis.
For question analysis, pars-ing serves two goals: to identify the finest answer focus(Moldovan et al, 2000; Hermjakob, 2001), and to pro-duce a grammatical analysis (f-structure) for questions.Due to the lack of publicly available parsers which havesuitable coverage of question forms, we have manuallydeveloped a set of grammars to achieve these goals.
Onthe other hand, the limited coverage and ambiguity inthese grammars made adopting the same approach forpassage analysis inefficient.
In effect, we use two dis-tinct parsers which provide two syntactic representations,including grammatical functions.
These syntactic struc-tures are then transformed into a common semantic rep-resentation discussed in Section 4.
( (Brill-pos VBN)(adjunct ((object ((Brill-pos WRB)(atype temporal)(cat n)(ortho When)(q-focus +)(q-token +)(root when)(tokens 1)))(time +)))(cat v)(finite +)(form finite)(modified +)(ortho founded)(passive +)(punctuation ((Brill-pos ".
")(cat punct)(ortho ?
)(root ?
)(tokens 6)))(qa ((gap ((atype temporal)(path (*MULT* adjunctobject))))(qtype entity)))(root found)(subject ((BBN-name person)(Brill-pos NNP)(cat n)(definite +)(gen-pn +)(human +)(number sg)(ortho "Wendy?s")(person third)(proper-noun +)(root wendy)(tokens 3)))(tense past)(tokens 5))Figure 1: When was Wendy?s founded: KANTOO f-structure3.1 QuestionsThe question analysis consists of two steps: lexical pro-cessing and syntactic parsing.
For the lexical process-ing step, we have integrated several external resources:the Brill part-of-speech tagger (Brill, 1995), BBN Identi-Finder (BBN, 2000) (to tag named entities such as propernames, time expressions, numbers, etc.
), WordNet (Fell-baum, 1998) (for semantic categorization), and the KAN-TOO Lexifier (Nyberg and Mitamura, 2000) (to access asyntactic lexicon for verb valence information).The hand-written grammars employed in the projectare based on the Lexical Functional Grammar (LFG) for-malism (Bresnan, 1982), and are used with the KANTOOparser (Nyberg and Mitamura, 2000).
The parser out-puts a functional structure (f-structure) which specifiesthe grammatical functions of question components, e.g.,subject, object, adjunct, etc.
As illustrated in Fig.
1, theresulting f-structure provides a deep, detailed syntacticanalysis of the question.3.2 PassagesPassages selected by the retrieval engine are processedby the Link Grammar parser (Grinberg et al, 1995).
Theparser uses a lexicalized grammar which specifies links,i.e., grammatical functions, and provides a constituentstructure as output.
The parser covers a wide range ofsyntactic constructions and is robust: it can skip over un-recognized fragments of text, and is able to handle un-known words.An example of the passage analysis produced by theLink Parser is presented in Fig.
2.
Links are treated aspredicates which relate various arguments.
For exam-ple, O in Fig.
2 indicates that Wendy?s is an object of theverb founded.
In parallel to the Link parser, passages aretagged with the BBN IdentiFinder (BBN, 2000), in or-der to group together multi-word proper names such asR.
David Thomas.4 Semantic RepresentationAt the core of our linguistic analysis is the semantic rep-resentation, which bridges the distinct representations ofthe functional structure obtained for questions and pas-sages.
Although our semantic representation is quite sim-ple, it aims at providing the means of understanding andprocessing broad-coverage linguistic data.
The represen-tation uses the following main constructs:2?
formula is a conjunction of literals and representsthe meaning of the entire sentence (or question);?
literal is a predicate relation over two terms; in par-ticular, we distinguish two types of literals: extrin-sic literal, a literal which relates a label to a label,and intrinsic literal, a literal which relates a label toa word;2The use of terminology common in the field of formal logicis aimed at providing an intuitive understanding to the reader,but is not meant to give the impression that our work is built ona firm logic-theoretic framework.+------------------------Xp------------------------+| +-------MVp-------+ |+--------Wd-------+ +------O------+ | || +-G-+---G--+---S---+ +--YS-+ +-IN+ || | | | | | | | | |LEFT-WALL R. David Thomas founded.v Wendy ?s.p in 1969 .Constituent tree:(S (NP R. David Thomas)(VP founded(NP (NP Wendy ?s))(PP in(NP 1969))).
)Figure 2: R. David Thomas founded Wendy?s in 1969.: Link Grammar parser output?
predicate is used to capture relations betweenterms;?
term is either a label, a variable which refers to aspecific entity or an event, or a word, which is eithera single word (e.g., John) or a sequence of wordsseparated by whitespace (e.g., for proper names suchas John Smith).The BNF syntax corresponding to this representationis given in (1).
(1) <formula> := <literal>+<literal> := <pred>(<term>,<term>)<term> := <label>|<word><word> := |[a-nA-Z0-9\s]+|<label> := [a-z]+[0-9]+<pred> := [A-Z_-]+With the exception of the unary ANS predicate whichindicates the sought answer, all predicates are binary re-lations (see examples in Fig.
3).
Currently, most pred-icate names are based on grammatical functions (e.g.,SUBJECT, OBJECT, DET) which link events and entitieswith their arguments.
Unlike in (Moldovan et al, 2003),names of predicates belong to a fixed vocabulary, whichprovides a more sound basis for a formal interpretation.Names of labels and terms are restricted only by the syn-tax in (1).
Examples of semantic representations for thequestion When was Wendy?s founded?
and the passage R.David Thomas founded Wendy?s in 1969. are shown inFig.
4.Note that our semantic representation reflects the?canonical?
structure of an active sentence.
This designdecision was made in order to eliminate structural differ-ences between semantically equivalent structures.
Hence,at the semantic level, all passive sentences correspond totheir equivalents in the active form.
Semantic representa-tion of questions is not always derived directly from thef-structure.
For some types of questions, e.g., definitionWhen was Wendy?s R. David Thomas foundedfounded?
Wendy?s in 1969.ROOT(x6,|Wendy?s|) ROOT(x6,|Wendy?s|)ROOT(x2,|found|) ROOT(x2,|ound|)ADJUNCT(x2,x1) ADJUNCT(x2,x1)OBJECT(x2,x6) OBJECT(x2,x6)SUBJECT(x2,x7) SUBJECT(x2,x7)ROOT(x7,|R.
David Thomas|)TYPE(x2,|event|) TYPE(x2,|event|)TENSE(x2,|past|)ROOT(x1,x9) ROOT(x1,|1969|)TYPE(x1,|time|) TYPE(x1,|time|)ANS(x9)Figure 4: An example of question and passage semanticrepresentationquestions such as What is the definition of hazmat?, spe-cialized (dedicated) grammars are used, which allows usto more easily arrive at an appropriate representation ofmeaning.
Also, in the preliminary implementation of theunification algorithm (see Section 5), we have adoptedsome simplifying assumptions, and we do not incorpo-rate sets in the current representation.The present formalism can quite successfully handlequestions (or sentences) which refer to specific events orrelations.
However, it is more difficult to represent ques-tions like What is the relationship between Jesse Venturaand Target Stores?, which seek a relation between enti-ties or a common event they participated in.
In the nextsection, we discuss the unification scheme which allowsus to select answer candidates based on the proposed rep-resentation.5 Fuzzy UnificationA unification algorithm is required to match question rep-resentations with the representations of extracted pas-sages which might contain answers.
Using a precursorpredicate example commentsROOT ROOT(x13,|John|) the root form of entity/event x13OBJECT OBJECT(x2,x3) x3 is the object of verbor preposition x2SUBJECT SUBJECT(x2,x3) x3 is the subject of verb x2DET DET(x2,x1) x1 is a determiner/quantifier of x2TYPE TYPE(x3,|event|) x3 is of the type eventTENSE TENSE(x1,|present|) x1 is a verb in present tenseEQUIV EQUIV(x1,x3) semantic equivalence:apposition: ?John, a student of CMU?equality operator in copular sentences:?John is a student of CMU?ATTRIBUTE ATTRIBUTE(x1,x3) x3 is an adjective modifier of x1:adjective-noun: ?stupid John?copular constructions: ?John is stupid?PREDICATE PREDICATE(x2,x3) copular constructions: ?Y is x3?ROOT(x2,|be|) SUBJECT(x2,Y)PREDICATE(x2,x3)POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2?x4?s x2?
or ?x2 of x4?AND AND(x3,x1) ?John and Mary laughed.
?AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|)ROOT(x4,|laugh|) TYPE(x4,|event|)AND(x3,x1)AND(x3,x2)SUBJECT(x4,x3)ANS ANS(x1) only for questions: x1 indicates the answerFigure 3: Examples of predicatesto the representation presented above, we constructedan initial prototype using a traditional theorem prover(Kalman, 2001).
Answer extraction was performed by at-tempting a unification between logical forms of the ques-tion and retrieved passages.
Early tests showed that a uni-fication strategy based on a strict boolean logic was notas flexible as we desired, given the lack of traditional do-main constraints that one normally possesses when con-sidering this type of approach.
Unless a retrieved pas-sage exactly matched the question, as in Fig.
4, the sys-tem would fail due to lack of information.
For instance,knowing that Benjamin killed Jefferson.
would not an-swer the question Who murdered Jefferson?, using a strictunification strategy.This has led to more recent experimentation with prob-abilistic models that perform what we informally refer toas fuzzy unification.3 The basic idea of our unificationstrategy is to treat relationships between question termsas a set of weighted constraints.
The confidence scoreassigned to each extracted answer candidate is related tothe number of constraints the retrieved passage satisfies,along with a measure of similarity between the relevantterms.5.1 DefinitionsIn this section, we present definitions which are necessaryfor discussion of the similarity measure employed by ourfuzzy unification framework.Given a user query Q, where Q is a formula, we re-trieve a set of passages P. Our task to is find the bestpassage Pbest ?
P from which an answer candidate canbe extracted.
An answer candidate exists within a pas-sage P if the result of a fuzzy unification between Q andP results in the single term of ANS(x0) being ground in aterm from P .
(2) Pbest = argmaxP?
Psim(Q,P )The restriction that an answer candidate must be foundwithin a passage P must be made explicit, as our no-tion of fuzzy unification is such that a passage can unifyagainst a query with a non-zero level of confidence evenif one or more constraints from the query are left unsat-isfied.
Since the final goal is to find and return the bestpossible answer, we are not concerned with those pas-sages which seem highly related yet do not offer answercandidates.In Section 4, we introduced extrinsic literals wherepredicates serve as relations over two labels.
Extrinsic lit-erals can be thought of as relations defined over distinct3Fuzzy unification in a formal setting generally refers to aunification framework that is employed in the realm of fuzzylogics.
Our current representation is of an ad-hoc nature, butour usage of this term does foreshadow future progression to-wards a representation scheme dependent on such a formal,non-boolean model.entities in our formula.
For example, SUBJECT(x1, x2)is an extrinsic literal, while ROOT(x1, |Benjamin|) is not.The latter has been defined as an intrinsic literal in Sec-tion 4 and it relates a label and a word.This terminology is motivated by the intuitive distinc-tion between intrinsic and extrinsic properties of an entityin the world.
We use this distinction as a simplifying as-sumption in our measurements of similarity, which wewill now explain in more detail.5.2 Similarity MeasureGiven a set of extrinsic literals PE and QE from a pas-sage and the question, respectively, we measure the sim-ilarity between QE and a given ordering of PE as thegeometric mean of the similarity between each pair ofextrinsic literals from the sets QE and PE .Let O be the set of all possible orderings of PE , Oan element of O, QEj literal j of QE , and Oj literal j ofordering O.
Then:(3) sim(Q,P )= sim(QE , PE)= maxO?
O(?nj=0 sim(QEj , Oj))1nThe similarity of two extrinsic literals, lE and lE?
, iscomputed by the square root of the similarity scores ofeach pair of labels, multiplied by the weight of the givenliteral, dependent on the equivilance of the predicatesp, p?
of the respective literals lE , lE?
.
If the predicates arenot equivilant, we rely on the engineers tactic of assign-ing an epsilon value of similarity, where  is lower thanany possible similarity score4.
Note that the similarityscore is still dependent on the weight of the literal, mean-ing that failing to satisfy a heavier constraint imposes agreater penalty than if we fail to satisfy a constraint oflesser importance.Let tj and t?j be the respective j-th term of lE , lE?.Then:(4) sim(lE , lE?)
= weight(lE)?
{(sim(t0,t?0)?sim(t1,t?1))12 ,p=p?,otherwiseThe weight of a literal is meant to capture the relativeimportance of a particular constraint in a query.
In stan-dard boolean unification the importance of a literal is uni-form, as any local failure dooms the entire attempt.5 In anon-boolean framework the importance of one literal vs.another becomes an issue.
As an example, given a ques-tion concerning a murder we might be more interested inthe suspect?s name than in the fact that he was tall.
This4The use of a constant value of  is ad hoc, and we are in-vestigating more principled methods for assigning this penalty.5That is to say, classic unification is usually an all or nothingaffair.idea is similar to that commonly seen in information re-trieval systems which place higher relative importance onterms in a query that are judged a priori to posses higherinformation value.
While our prototype currently sets allliterals with a weight of 1.0, we are investigating methodsto train these weights to be specific to question type.Per our definition, all terms within an extrinsic literalwill be labels.
Thus, in equation (10), t0 is a label, as ist1, and so on.
Given a pair of labels, b and b?, we let I, I ?be the respective sets of intrinsic literals from the formulacontaining b, b?
such that for all intrinsic literals lI ?
I ,the first term of lI is b, and likewise for b?, I ?.Much like similarity between two formulae, the sim-ilarity between two labels relies on finding the maximalscore over all possible orderings of a set of literals.Now let O be the set of all possible orderings of I ?, Oan element of O, Ij the j-th literal of I , and Oj the j-thliteral of O.
Then:(5) sim(b, b?)
= maxO?
O(?nj=0 sim(Ij , Oj))1nWe measure the similarity between a pair of intrinsicliterals as the similarity between the two words multi-plied by the weight of the first literal, dependent on thepredicates p, p?
of the respective literals being equivilant.
(6)sim(lI , lI?)
= weight(lI) ?
{sim(t1,t?1),p=p?,otherwiseThe similarity between two words is currently measuredusing a WordNet distance metric, applying weights intro-duced in (Moldovan et al, 2003).
We will soon be inte-grating metrics which rely on other dimensions of simi-larity.5.3 ExampleWe now walk through a simple example in order topresent the current framework used to measure the levelof constraint satisfaction (confidence score) achieved bya given passage.
While a complete traversal of even asmall passage would exceed the space available here, wewill present a single instance of each type of usage of thesim() function.If we limit our focus to only a few key relationships,we get the following analysis of a given question and pas-sage.
(7) Who killed Jefferson?ANS(x0), ROOT(x1,x0), ROOT(x2,|kill|),ROOT(x3,|Jefferson|), TYPE(x2,|event|),TYPE(x1,|person|), TYPE(x3,|person|), SUB-JECT(x2,x1), OBJECT(x2,x3)(8) Benjamin murdered Jefferson.ROOT(y1,|Benjamin|), ROOT(y2,|murder|),ROOT(y3,|Jefferson|), TYPE(y2,|event|),TYPE(y1,|person|), TYPE(y3,|person|), SUB-JECT(y2,y1), OBJECT(y2,y3)Computing the similarity between two formulae,(loosely referred to here by their original text), gives thefollowing:(9) sim[|Who killed Jefferson?|,|Benjamin murdered Jefferson.|] =(sim[ SUBJECT(x2,x1), SUBJECT(y2,y1)]?sim[ OBJECT(x2,x3), OBJECT(y2,y3)]) 12The similarity between the given extrinsic literals shar-ing the predicate SUBJECT:(10) sim[SUBJECT(x2,x1), SUBJECT(y2,y1)] =(sim[x2, y2] ?
sim[x1, y1]) 12 ?weight[SUBJECT(x2,x1)]In order to find the result of this extrinsic similarityevaluation, we need to determine the similarity betweenthe paired terms, (x1,y1) and (x2,y2).
The similarity be-tween x1 and y1 is measured as:(11) sim[x2, y2] =(sim[ROOT(x2,|kill|), ROOT(y2,|murder|)]?sim[TYPE(x2,|event|), TYPE(y2,|event|)]) 12The result of this function depends on the combinedsimilarity of the intrinsic literals that relate the giventerms to values.
The similarity between one of these in-trinsic literal pairs is measured by:(12) sim[ROOT(x2,|kill|), ROOT(y2,|murder|)] =sim[|kill|, |murder|]?weight[ROOT(x2,|kill|)]Finally, the similarity between a pair of words is com-puted as:(13) sim[|kill|, |murder|] = 0.8As stated earlier, our similarity metrics at the wordlevel are currently based on recent work on WordNet dis-tance functions.
We are actively developing methods tocomplement this approach.6 Summary and Future WorkThe paper presents a lightweight semantic processingtechnique for open-domain question answering.
We pro-pose a uniform semantic representation for questions andpassages, derived from their functional structure.
We alsodescribe the unification framework which allows for flex-ible matching of query terms with retrieved passages.One characteristics of the current representation is thatit is built from grammatical functions and does not uti-lize a canonical set of semantic roles and concepts.
Ouroverall approach in JAVELIN was to start with the sim-plest form of meaning-based matching that could extendsimple keyword-based approaches.
Since it was possi-ble to extract grammatical functions from unrestrictedtext fairly quickly (using KANTOO for questions and theLink Grammar parser for answer passages), this frame-work provides a reasonable first step.
We intend to extendour representation and unification algorithm by incorpo-rating the Lexical Conceptual Structure Database (Dorr,2001), which will allow us to use semantic roles insteadof grammatical relations as predicates in the represen-tation.
We also plan to enrich the representation withtemporal expressions, incorporating the ideas presentedin (Han, 2003).Another limitation of the current implementation isthe limited scope of the similarity function.
At present,the similarity function is based on relationships foundin WordNet, and only relates words which belong to thesame syntactic category.
We plan to extend our similar-ity measure by using name lists, gazetteers and statisticalcooccurrence in text corpora.
A complete approach toword similarity will also require a suitable algorithm forreference resolution.
Unrestricted text makes heavy useof various forms of co-reference, such as anaphora, def-inite description, etc.
We intend to adapt the anaphoraresolution algorithms used in KANTOO for this purpose,but a general solution to resolving definite reference (e.g.,the use of ?the organization?
to refer to ?Microsoft?)
is atopic for ongoing research.AcknowledgmentsResearch presented in this paper has been supported inpart by an ARDA grant under Phase I of the AQUAINTprogram.
The authors wish to thank all members of theJAVELIN project for their support in preparing the workpresented in this paper.
We are also grateful to two anony-mous reviewers, Laurie Hiyakumoto and Kathryn Bakerfor their comments and suggestions for improving thispaper.
Needless to say, all remaining errors and omis-sions are entirely our responsibility.ReferencesBBN Technologies, 2000.
IdentiFinder User Manual.Joan Bresnan, editor.
1982.
The Mental Representationof Grammatical Relations.
MIT Press Series on Cog-nitive Theory and Mental Representation.
The MITPress, Cambridge, MA.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case studyin part-of-speech tagging.
Computational Linguistics,21:543?565.Jenifer Chu-Carroll, John Prager, Christopher Welty,Krzysztof Czuba, and David Ferrucci.
2003.
A multi-strategy and multi-source approach to question an-swering.
In TREC 2002 Proceedings.Bonnie J. Dorr.
2001.
LCS Database Docu-mentation.
HTML Manual.
available fromhttp://www.umiacs.umd.edu/?bonnie/LCS Data-base Documentation.html.Christiane Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press.Dennis Grinberg, John Lafferty, and Daniel Sleator.1995.
A robust parsing algorithm for link grammars.In Proceedings of the Fourth International Workshopon Parsing Technologies, Prague, September.Benjamin Han.
2003.
Text temporal analysis.
Researchstatus summary.
Draft of January 2003.Ulf Hermjakob.
2001.
Parsing and question classifica-tion for question answering.
In Proceedings of theWorkshop on Open-Domain Question Answering atACL-2001.Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.
2002.The use of external knowledge in factoid qa.
In Pro-ceedings of the TREC-10 Conference.Abraham Ittycheriah and Salim Roukos.
2003.
IBM?sstatistical question answering system ?
TREC-11.
InTREC 2002 Proceedings.Abraham Ittycheriah, Martin Franz, and Salim Roukos.2002.
IBM?s statistical question answering system ?TREC-10.
In TREC 2001 Proceedings.John A. Kalman.
2001.
Automated Reasoning with OT-TER.
Rinton Press.Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Roxana Girju, Richard Goodrum, and VasileRus.
2000.
The structure and performance of an open-domain question answering system.
In Proceedings ofthe Conference of the Association for ComputationalLinguistics (ACL-2000).Dan Moldovan, Sanda Harabagiu, Roxana Girju, PaulMorarescu, Finley Lacatusu, Adrian Novischi, AdrianaBadulescu, and Orest Bolohan.
2003.
LCC tools forquestion answering.
In TREC 2002 Proceedings.Eric Nyberg and Teruko Mitamura.
2000.
The KAN-TOO machine translation environment.
In Proceed-ings of AMTA 2000.Eric Nyberg, Teruko Mitamura, Jaime Carbonell, JaimeCallan, Kevyn Collins-Thompson, Krzysztof Czuba,Michael Duggan, Laurie Hiyakumoto, Ng Hu, YifenHuang, Jeongwoo Ko, Lucian V. Lita, StephenMurtagh, Vasco Pedro, and David Svoboda.
2003.
TheJAVELIN question answering system at TREC 2002.In TREC 2002 Proceedings.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a Question Answering system.In Proceedings of the ACL Conference.Martin M. Soubbotin.
2002.
Patterns of potential answerexpressions as clues to the right answer.
In TREC 2001Proceedings.
