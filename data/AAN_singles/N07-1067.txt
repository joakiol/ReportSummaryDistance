Proceedings of NAACL HLT 2007, pages 532?539,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsQuestion Answering using Integrated Information Retrieval andInformation ExtractionBarry Schiffman and Kathleen R. McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027bschiff,kathy@cs.columbia.eduRalph GrishmanDepartment of Computer ScienceNew York UniversityNew York, NY 10003grishman@cs.nyu.eduJames AllanUniversity of MassachusettsDepartment of Computer ScienceAmherst, MA 01003allan@cs.umass.eduAbstractThis paper addresses the task of provid-ing extended responses to questions re-garding specialized topics.
This task is anamalgam of information retrieval, topicalsummarization, and Information Extrac-tion (IE).
We present an approach whichdraws on methods from each of these ar-eas, and compare the effectiveness of thisapproach with a query-focused summa-rization approach.
The two systems areevaluated in the context of the prosecutionqueries like those in the DARPA GALEdistillation evaluation.1 IntroductionAs question-answering systems advance from han-dling factoid questions to more complex requests,they must be able to determine how much informa-tion to include while making sure that the informa-tion selected is indeed relevant.
Unlike factoid ques-tions, there is no clear criterion that defines the kindof phrase that answers the question; instead, theremay be many phrases that could make up an answerand it is often unclear in advance, how many.
Assystem developers, our goal is to yield high recallwithout sacrificing precision.In response to questions about particular events ofinterest that can be enumerated in advance, it is pos-sible to perform a deeper semantic analysis focusingon the entities, relations, and sub-events of interest.On the other hand, the deeper analysis may be error-ful and will also not always provide complete cov-erage of the information relevant to the query.
Thechallenge, therefore, is to blend a shallower, robustapproach with the deeper approach in an effectiveway.In this paper, we show how this can be achievedthrough a synergistic combination of information re-trieval and information extraction.
We interleave in-formation retrieval (IR) and response generation, us-ing IR in high precision mode in the first stage toreturn a small number of documents that are highlylikely to be relevant.
Information extraction of enti-ties and events within these documents is then usedto pinpoint highly relevant sentences and associatedwords are selected to revise the query for a sec-ond pass of retrieval, improving recall.
As part ofthis process, we approximate the relevant context bymeasuring the proximity of the target name in thequery and extracted events.Our approach has been evaluated in the frame-work of the DARPA GALE1 program.
One of theGALE evaluations involves responding to questionsbased on a set of question templates, ranging frombroad questions like ?Provide information on X?,where X is an organization, to questions focused onparticular classes of events.
For the experiments pre-sented here, we used the GALE program?s prosecu-tion class of questions.
These are given in the fol-lowing form: ?Describe the prosecution of X for Y,?where X is a person and Y is a crime or charge.
Ourresults show that we are able to achieve higher accu-1Global Autonomous Language Exploitation532racy with a system that exploits the justice eventsidentified by IE than with an approach based onquery-focused summarization alone.In the following sections, we first describe thetask and then review related work in question-answering.
Section 3 details our procedure for find-ing answers as well as performing the informationretrieval and information extraction tasks.
Section 4compares the results of the two approaches.
Finally,we present our conclusion and plans for future work.1.1 The TaskThe language of the question immediately raises thequestion of what is meant by prosecution.
Unlike aquestion such as ?When was X born?
?, which is ex-pected to be answered by a clear, concrete phrase,the prosecution question asks for a much greaterrange of material.
The answer is in no way limitedto the statements and activities of the prosecuting at-torney, although these would certainly be part of acomprehensive answer.In the GALE relevance guidelines2 , the answercan include many facets of the case:?
Descriptions of the accused?s involvement inthe crime.?
Descriptions of the activities, motivations, andinvolvement in the crime.?
Descriptions of the person as long as they arerelated to the trial.?
Information about the defense of the suspect.?
Information about the sentencing of the person.?
Information about similar cases involving theperson.?
Information about the arrest of the person andstatements made by him or her.?
Reactions of people involved in the trial, aswell as statements by officials or reactions bythe general public.2BAE Systems Advanced Information Technologies, ?Rele-vance Guidelines for Distillation Evaluation for GALE: GlobalAutonomous Language Exploitation?, Version 2.2, January 25,2007The guidelines also provide a catchall instructionto ?include reported information believed to be rele-vant to the case, but deemed inadmissible in a courtof law.
?It is easy to see that the use of a few search termsalone will be insufficient to locate a comprehensiveanswer.We took a broad view of the question type andconsider that any information about the investiga-tion, accusation, pursuit, capture, trial and punish-ment of the individual, whether a person or organi-zation, would be desireable in the answer.1.2 OverviewThe first step in our procedure sends a query tai-lored to this question type to the IR system to ob-tain a small number of high-quality documents withwhich we can determine what name variations areused in the corpus and estimate how many docu-ments contain references to the individual.
In thefuture we will expand the type of information wewant to glean from this small set of documents.
Asecondary search is issued to find additional docu-ments that refer to the individual, or individuals.Once we have the complete document retrieval,the foundation for finding these types of eventslies in the Proteus information extraction compo-nent (Grishman et al, 2005).
We employ an IE sys-tem trained for the tasks of the 2005 Automatic Con-tent Extraction evaluation, which include entity andevent extraction.
ACE defines a number of generalevent types, including justice events, which cover in-dictments, accusations, arrests, trials, and sentenc-ings.
The union of all these specific categories givesus many of the salient events in a criminal justicecase from beginning to end.
The program uses theevents, as well as the entities, to help identify thepassages that respond to the question.The selection of sentences is based on the as-sumption that the co-occurrence of the target indi-vidual and a judicial event indicates that the targetis indeed involved in the event, but these two do notnecesssarily occur in the same sentence.2 Related WorkA large body of work in question-answering has fol-lowed from the opening of the Text Retrieval Con-533ference?s Q&A track in 1999.
The task started as agroup of factoid questions and expanded from thereinto more sophisticated problems.
TREC providesa unique testbed of question-answer pairs for re-searchers and this data has been influential in fur-thering progress.In TREC 2006, there was a new secondary taskcalled ?complex, interactive Question Answering,?
(Dang et al, 2006) which is quite close to the GALEproblem, though it incorporated interaction to im-prove results.
Questions are posed in a canonicalform plus a narrative elaborating on the kind of in-formation requested.
An example question (from theTREC guidelines) asks, ?What evidence is there fortransport of [drugs] from [Bonaire] to the [UnitedStates]??
Our task is most similar to the fully-automatic baseline runs of the track, which typicallytook the form of passage retrieval with query ex-pansion (Oard et al, 2006) or synonym processing(Katz et al, 2006), and not the deeper processingemployed in this work.Within the broader QA task, the other questiontype is closest to the requirements in GALE, but itis too open ended.
In TREC, the input for otherquestions is the name or description of the target,and the response is supposed to be all informationthat did not fit in the answers to the previous ques-tions.
While a few GALE questions have similar in-put, most, including the prosecution questions, pro-vide more detail about the topic in question.A number of systems have used techniques in-spired by information extraction.
One of the top sys-tems in the other questions category at the 2004 and2005 evaluations generated lexical-syntactic pat-terns and semantic patterns (Schone et al, 2004).But they build these patterns from the question.
Inour task, we took advantage of the structured ques-tion format to make use of extensive work on thesemantics of selected domains.
In this way wehope to determine whether we can obtain better per-formance by adding more sophisticated knowledgeabout these domains.
The Language Computer Cor-poration (LCC) has long experimented with incorpo-rating information extraction techniques.
Recently,in its system for the other type questions at TREC2005, LCC developed search patterns for 33 targetclasses (Harabagiu et al, 2005).
These patterns werelearned with features from WordNet, stemming andnamed entity recognition.More and more systems are exploiting the sizeand redundancy of the Web to help find answers.Some obtain answers from the Web and thenproject the answer back to the test corpus to finda supporting document (Voorhees and Dang, 2005).LCC used ?web boosting features?
to add to keywords (Harabagiu et al, 2005).
Rather than go tothe Web and enhance the question terms, we madea beginning at examining the corpus for specific bitsof information, in this prototype, to determine alter-native realizations of names.3 ImplementationAs stated above, the system takes a query in theXML format required by the GALE program.
Thequery templates allow users to amplify their requestsby specifying a timeframe for the information and/ora locale.
In addition, there are provisions for en-tering synonyms or alternate terms for either of themain arguments, i.e.
the accused and the crime, andfor related but less important terms.Since this system is a prototype written especiallyfor the GALE evaluation in July 2006, we paid closeattention to the way example questions were given,as well as to the evaluation corpus, which consistedof more than 600,000 short news articles.
The goalin GALE was to offer comprehensive results to theuser, providing all snippets, or segments of texts,that responded to the information request.
This re-quired us to develop a strategy that balanced pre-cision against recall.
A system that reported onlyhigh-confidence answers was in danger of having noanswers or far fewer answers than other systems,while a system that allowed lower confidence an-swers risked producing answers with a great deal ofirrelevant material.
Another way to look at this bal-ancing act was that it was necessary for a system toknow when to quit.
For this reason, we sought toobtain a good estimate of the number of documentswe wanted to scan for answers.Answer selection focused first on the name of thesuspect, which was always given in the query tem-plate.
In many of the training cases, the suspect wasin the news only because of a criminal charge againsthim; and in most, the charge specified was the onlyaccusation reported in the news.
Both location and534date constraints seemed to be largely superfluous,and so we ignored these.
But we did have a mecha-nism for obtaining supplementary answers keyed tothe brief description of the crime and other relatedwordsThe first step in the process is to request a seedcollection of 10 documents from the IR system.This number was established experimentally.
TheIR query combines terms tailored to the prosecutiontemplate and the specific template parameters for aparticular question.
The 10 documents returned arethen examined to produce a list of name variationsthat substantially match the name as rendered in thequery template.
The IR system is then asked for thenumber of times that the name appears in the cor-pus.
This figure is adjusted by the frequency perdocument in the seed collection and a new query issubmitted, set to obtain the N documents in whichwe expect to find the target?s name.3.1 Information RetrievalThe goal of the information retrieval component ofthe system was to locate relevant documents that thesummarization system could then use to construct ananswer.
All search, whether high-precision or high-recall, was performed using the Indri retrieval sys-tem 3 (Strohman et al, 2005).Indri provides a powerful query language thatis used here to combine numerous aspects of thequery.
The Indri query regarding Saddam Hus-sein?s prosecution for crimes against humanity in-cludes the following components: source restric-tions, prosecution-related words, mentions of Sad-dam Hussein, justice events, dependence modelphrases (Metzler and Croft, 2005) regarding thecrime, and a location constraint.The first part of the query located references toprosecutions by looking for the keywords prosecu-tion, defense, trial, sentence, crime, guilty, or ac-cuse, all of which were determined on training datato occur in descriptions of prosecutions.
Thesewords were important to have in documents for themto be considered relevant, but the individual?s nameand the description of the crime were far more im-portant (by a factor of almost 19 to 1).The more heavily weighted part of the query,3http://lemurproject.org/indrithen, was a ?justice event?
marker found using in-formation extraction (Section 3.2) and the more de-tailed description of that event based on phrases ex-tracted from the crime (here crimes against human-ity).
Those phrases give more probability of rele-vance to documents that use more terms from thecrime.
It also included a location constraint (here,Iraq) that boosted documents referring to that lo-cation.
And it captured user-provided equivalentwords such as Saddam Hussein being a synonym forformer President of Iraq.The most complex part of the query handled ref-erences to the individual.
The extraction system hadannotated all person names throughout the corpus.We used the IR system to index all names acrossall documents and used Indri to retrieve any nameforms that matched the individual.
As a result, wewere able to find references to Saddam, Hussein,and so on.
This task could have also been accom-plished with cross-document coreference technol-ogy but our approach appeared to compensate forincorrectly translated names slightly better than thecoreference system we had available at the time.
Forexample, Present rust Hussein was one odd formthat was matched by our simple approach.The final query looked like the following:#filreq( #syn( #1(AFA).source ... #1(XIE).source )#weight(0.05 #combine( prosecution defense trial sentencecrime guilty accuse )0.95 #combine(#any:justice#weight(1.0 #combine(humanity against crimes)1.0 #combine(#1(against humanity)#1(crimes against)#1(crimes against humanity))1.0 #combine#uw8(against humanity)#uw8(crimes humanity)#uw8(crimes against)#uw12(crimes against humanity)))Iraq#syn( #1(saddam hussein)#1(former president iraq))#syn( #equals( entity 126180 ) ...))))The actual query is much longer because it con-tains 100 possible entities and numerous sources.The processing is described in more detail else-where (Kumaran and Allan, 2007).3.2 Information ExtractionThe Proteus system produces the full range of anno-tations as specified for the ACE 2005 evaluation, in-cluding entities, values, time expressions, relations,535and events.
We focus here on the two annotations,entities and events, most relevant to our question-answering task.
The general performance on entityand event detection in news articles is within a fewpercentage points of the top-ranking systems fromthe evaluation.The extraction engine identifies seven semanticclasses of entities mentioned in a document, ofwhich the most frequent are persons, organizations,and GPE?s (geo-political entities ?
roughly, regionswith a government).
Each entity will have one ormore mentions in the document; these mentions in-clude names, nouns and noun phrases, and pro-nouns.
Text processing begins with an HMM-basednamed entity tagger, which identifies and classifiesthe names in the document.
Nominal and pronomi-nal mentions are identified either with a chunker ora full Penn-Treebank parser.
A rule-based coref-erence component identifies coreference relations,forming entities from the mentions.
Finally, a se-mantic classifier assigns a class to each entity basedon the type of the first named mention (if the entityincludes a named mention) or the head of the firstnominal mention (using statistics gathered from theACE training corpus).The ACE annotation guidelines specify 33 differ-ent event subtypes, organized into 8 major types.One of the major types is justice events, which in-clude arrest, charge, trial, appeal, acquit, convict,sentence, fine, execute, release, pardon, sue, and ex-tradite subtypes.
In parallel to entities, the eventtagger first identifies individual event mentions andthen uses event coreference to form events.
For theACE evaluation, an annotated corpus of approxi-mately 300,000 words is used to train the event tag-ger.For each event mention in the corpus, we collectthe trigger word (the main word indicating the event)and a pattern recording the path from the triggerto each event argument.
These paths are recordedin two forms: as the sequence of heads of maxi-mal constituents between the trigger and the argu-ment, and as the sequence of predicate-argument re-lations connecting the trigger to the argument4 .
In4These predicate argument relations are based on a repre-sentation called GLARF (Grammatical-Logical Argument Rep-resentation Framework), which incorporates deep syntactic re-lations and the argument roles from PropBank and NomBank.addition, a set of maximum-entropy classifiers aretrained: to distinguish events from non-events, toclassify events by type and subtype, to distinguisharguments from non-arguments, and to classify ar-guments by argument role.
In tagging new data, wefirst match the context of each instance of a trig-ger word against the collected patterns, thus iden-tifying some arguments.
The argument classifier isthen used to collect additional arguments within thesentence.
Finally, the event classifier (which usesthe proposed arguments as features) is used to re-ject unlikely events.
The patterns provide somewhatmore precise matching, while the argument classi-fiers improve recall, yielding a tagger with betterperformance than either strategy separately.3.3 Answer GenerationOnce the final batch of documents is received,the answer generator module selects candidate pas-sages.
The names, with alternate renderings, are lo-cated through the entity mentions by the IE system.All sentences that contain a justice event and thatfall within a mention of a target by no more thann sentences, where n is a settable parameter, whichwas put at 5 for this evaluation, form the core of thesystem?s answer.The tactic takes the place of topic segmentation,which we used for other question types in GALEthat did not have the benefit of the sophisticatedevent recognition offered by the IE system.
Segmen-tation is used to give users sufficient context in theanswer without needing a means of identifying dif-ficult definite nominal resolution cases that are nothandled by extraction.In order to increase recall, in keeping with theneed for a comprehensive answer in the GALE eval-uation, we added sentences that contain the name ofthe target in documents that have justice events andsentences that contain words describing the crime.However, we imposed a limitation on the growth ofthe answer size.
When the target individual is well-known, he or she will be mentioned in numerouscontexts, reducing the likelihood that this additionalmention will be relevant.
Thus, when the size of theanswer grew too rapidly, we stopped including theseadditional sentences, and produced sentences onlyfrom the justice events.
The threshold for triggeringthis shift was 200 sentences.5363.4 SummarizationAs a state-of-the-art baseline, we used a genericmultidocument summarization system that has beentested in numerous contexts.
It is, indeed, thebackup answer generator for several question types,including the prosecution questions, in our GALEsystem, and has been been tested in the topic-basedtasks of the 2005 and 2006 Document Understand-ing Conferences.A topic statement is formed by collapsing thetemplate arguments into one list, e.g., ?saddam hus-sein crimes against humanity prosecution?, and theanswer generation module proceeds by using a hy-brid approach that combines top-down strategiesbased on syntactic patterns, alongside a suite ofsummarization methods which guide content in abottom-up manner that clusters and combines thecandidate sentences (Blair-Goldensohn and McKe-own, 2006).4 EvaluationThe results of our evaluation are shown in Table 1.We increased the number of test questions over thenumber used in the official GALE evaluation and weused only previously unseen questions.
Documentsfor the baseline system were selected without use ofthe event annotations from Proteus.We paired the 25 questions for judges, so that boththe system?s answer and the baseline answer wereassigned to the same person.
We provided explicitinstructions on the handling on implicit references,allowing the judges to use the context of the ques-tion and other answer sentences to determine if asentence was relevant ?
following the practice of theGALE evaluation.Our judges were randomly assigned questionsand asked whether the snippets, which in our casewere individual sentences, were relevant or not;they could respond Relevant, Not Relevant or Don?tKnow.
In cases where references were unclear, thejudges were asked to choose Don?t Know and thesewere removed from the scoring.55In the GALE evaluation, the snippets are broken down byhand into nuggets ?
discrete pieces of information ?
and theanswers are scored on that basis.
However, we scored our re-sponses on the basis of snippets (sentences) only, as it is muchmore efficient, and therefore more feasible to repeat in the fu-ture.Our system using IE event detection and en-tity tracking outperformed the summarization-basedbaseline, with average precision of 68% comparedwith 57%.
Moreover, the specialized system sus-tained that level of precision although it returned amuch larger number of snippets, totaling 2,086 overthe 25 questions, compared with 363 for the base-line system.
We computed a relative recall score, us-ing the union of the sentences found by the systemsand judged relevant as the ground truth.
For recall,the specialized system scored an average 89% ver-sus 17% for the baseline system.
Computing an F-measure weighting precision and recall equally, thespecialized system outperformed the baseline sys-tem 75% to 23%.
The difference in relative recalland F-measure are both statisticaly significant undera two-tailed, paired t-test, with p < 0.001.5 Conclusion and Future WorkOur results show that the specialized system statis-tically outperforms the baseline, a well-tested queryfocused summarization approach, on precision.
Thespecialized system produced a much larger answeron average (Table 1).
Moreover, our answer gener-ator seemed to adapt well to information in the cor-pus.
Of the six cases where it returned fewer than10 sentences, the baseline found no additional sen-tences four times (Questions B006, B011, B015 andB022).
We regard this as an important property inthe question-answering task.A major challenge is to ascertain whether themention of the target is indeed involved in the rec-ognized justice event.
Our event recognition systemwas developed within the ACE program and onlyseeks to assigns roles within the local context of asingle sentence.
We currently use a threshold to con-sider whether an entity mention is reliable, but wewill experiment with ways to measure the likelihoodthat a particular sentence is about the prosecution orsome other issue.
We are planning to obtain vari-ous pieces of information from additional secondaryqueries to the search engine.
Within the GALE pro-gram, we are limited to the defined corpus, but in thegeneral case, we could add more varied resources.In addition, we are working to produce answersusing text generation, to bring more sophisticatedsummarization techniques to make a better presen-537QID System with IE Baseline SystemPrecision Recall F-meas Count Precision Recall F-meas CountB001 0.728 0.905 0.807 92 0.818 0.122 0.212 11B002 0.713 0.906 0.798 108 0.889 0.188 0.311 18B003 0.770 0.942 0.848 148 0.875 0.058 0.109 8B004 0.930 0.879 0.904 86 1.000 0.154 0.267 14B005 0.706 0.923 0.800 34 0.400 0.231 0.293 15B006 1.000 1.000 1.000 3 0.000 0.000 0.000 17B007 0.507 1.000 0.673 73 0.421 0.216 0.286 19B008 0.791 0.909 0.846 201 0.889 0.091 0.166 18B009 0.759 0.960 0.848 158 0.941 0.128 0.225 17B010 1.000 0.828 0.906 24 0.500 0.276 0.356 16B011 0.500 1.000 0.667 6 0.000 0.000 0.000 18B012 0.338 0.714 0.459 74 0.765 0.371 0.500 17B013 0.375 0.900 0.529 120 0.700 0.280 0.400 20B014 0.571 0.800 0.667 7 0.062 0.200 0.095 16B015 0.500 1.000 0.667 2 0.000 0.000 0.000 10B016 1.000 0.500 0.667 5 0.375 0.600 0.462 16B017 1.000 1.000 1.000 13 0.125 0.077 0.095 7B018 0.724 0.993 0.837 199 0.875 0.048 0.092 8B019 0.617 0.954 0.749 201 0.684 0.100 0.174 19B020 0.923 0.727 0.814 26 0.800 0.364 0.500 15B021 0.562 0.968 0.711 162 0.818 0.096 0.171 11B022 0.667 1.000 0.800 6 0.000 0.000 0.000 18B023 0.684 0.950 0.795 196 0.778 0.050 0.093 9B024 0.117 0.636 0.197 60 0.714 0.455 0.556 7B025 0.610 0.943 0.741 82 0.722 0.245 0.366 18Aver 0.684 0.893 0.749 83 0.566 0.174 0.229 14Table 1: The table compares results of our answer generator combining the Indri and the Proteus ACE sys-tem, against the focused-summarization baseline.
This experiment is over 25 previously unseen questions.The differences between the two systems are statistically significant (p < 0.001) for recall and f-measure bya two-tailed, paired t-test.
A big difference between the two systems is that the answer generator producesa total of 2,086 answer sentences while sustaining an average precision of 0.684.
In only three cases, doesthe precision fall below 0.5.
In contrast, the baseline system produced only 362, one-sixth the number ofanswer sentences.
While its average precision was not significantly worse than the answer-generator?s, itsprecision varied widely, failing to find any correct sentences four times.538tation than an unordered list of sentences.Finally, we will look into applying the techniquesused here on other topics.
The first test would rea-sonably be Conflict events, for which the ACE pro-gram has training data.
But ultimately, we wouldlike to adapt our system to arbitrary topic areas.AcknowledgementsThis material is based in part upon work supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the Defense Advanced Research Projects Agency(DARPA).ReferencesSasha Blair-Goldensohn and Kathleen McKeown.
2006.Integrating rhetorical-semantic relation models forquery-focused summarization.
In Proceedings of 6thDocument Understanding Conference (DUC2006).Hoa Trang Dang, Jimmy Lin, and Diane Kelly.
2006.Overview of the TREC 2006 question answering track.In Proceedings TREC.
Forthcoming.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
NYU?s english ACE 2005 system descrip-tion.
In ACE 05 Evaluation Workshop.
On-line athttp://nlp.cs.nyu.edu/publication.Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005.
Employing two question answering systems inTREC 2005.
In Proceedings of the Fourteenth TextRetrieval Conference.B.
Katz, G. Marton, G. Borchardt, A. Brownell,S.
Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,F.
Mora, S. Stiller, O. Uzuner, and A. Wilcox.2006.
External knowledge sources for question an-swering.
In Proceedings of TREC.
On-line athttp://www.trec.nist.gov.Giridhar Kumaran and James Allan.
2007.
Informationretrieval techniques for templated queries.
In Proceed-ings of RIAO.
Forthcoming.D.
Metzler and W.B.
Croft.
2005.
A Markov randomfield model for term dependencies.
In Proceedings ofACM SIGIR, pages 472?479.D.
Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels,J.
Lin, and D. Soergel.
2006.
Trec 2006 at maryland:Blog, enterprise, legal and QA tracks.
In Proceedingsof TREC.
On-line at http://www.trec.nist.gov.Patrick Schone, Gary Ciany, Paul McNamee, JamesMayeld, Tina Bassi, and Anita Kulman.
2004.
Ques-tion answering with QACTIS at TREC-2004.
In Pro-ceedings of the Thirteenth Text Retrieval Conference.T.
Strohman, D. Metzler, H. Turtle, and W.B.
Croft.2005.
Indri: A language-model based search enginefor complex queries (extended version).
Technical Re-port IR-407, CIIR, UMass Amherst.Ellen M. Voorhees and Hoa Trang Dang.
2005.Overview of the TREC 2005 question answering track.In Proceedings of the Fourteenth Text Retrieval Con-ference.539
