Automatically Assessing Machine SummaryContent Without a Gold StandardAnnie Louis?University of PennsylvaniaAni Nenkova?
?University of PennsylvaniaThe most widely adopted approaches for evaluation of summary content follow some protocolfor comparing a summary with gold-standard human summaries, which are traditionally calledmodel summaries.
This evaluation paradigm falls short when human summaries are not availableand becomes less accurate when only a single model is available.
We propose three novelevaluation techniques.
Two of them are model-free and do not rely on a gold standard for theassessment.
The third technique improves standard automatic evaluations by expanding the setof available model summaries with chosen system summaries.We show that quantifying the similarity between the source text and its summary withappropriately chosen measures produces summary scores which replicate human assessmentsaccurately.
We also explore ways of increasing evaluation quality when only one human modelsummary is available as a gold standard.
We introduce pseudomodels, which are system sum-maries deemed to contain good content according to automatic evaluation.
Combining thepseudomodels with the single human model to form the gold-standard leads to higher correlationswith human judgments compared to using only the one available model.
Finally, we explorethe feasibility of another measure?similarity between a system summary and the pool of allother system summaries for the same input.
This method of comparison with the consensus ofsystems produces impressively accurate rankings of system summaries, achieving correlationwith human rankings above 0.9.1.
IntroductionIn this work, we present evaluation metrics for summary content which make use oflittle or no human involvement.
Evaluation methods such as manual pyramid scores(Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin andHovy 2003) rely on multiple human summaries as a gold standard (model) againstwhich they compare a summary to assess how informative the candidate summaryis.
It is desirable that evaluation of similar quality be done quickly and cheaply?
E-mail: lannie@seas.upenn.edu.??
University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St.,Philadelphia, PA 19104.
E-mail: nenkova@seas.upenn.edu.Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication:18 April 2012.doi:10.1162/COLI a 00123?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 2on non-standard test sets that have few or no human summaries, or on large testsets for which creating human model summaries is infeasible.
In our work, we aimto identify indicators of summary content quality that do not make use of humansummaries but can replicate scores based on comparison with a gold standard veryaccurately.Such indicators would need to be easily computable from existing resources andto provide rankings of systems that agree with rankings obtained through humanjudgments.
There have been some early proposals for alternative methods.
Donaway,Drummey, and Mather (2000) propose that a comparison of the source text with asummary can tell us how good the summary is.
A summary that has higher similaritywith the source text can be considered better than one with lower similarity.
Radev andTam (2003) perform a large scale evaluation with thousands of test documents.
Theirwork is set up in a search engine scenario.
They first rank the test documents using thesearch engine.
Then they perform the same experiment now substituting the summariesfrom one system in place of the original documents.
The systemwhose summaries havethe most similar ranking as that generated for the full documents is considered thebest system because not much information loss is introduced by the summarizationprocess.But these methods did not gain much popularity and their performance was nevercompared to human evaluations.
Part of the reason is that only in the last decadehave several large data sets with system summaries and their ratings from humanjudges become available for performing such studies.
Our work is the first to providea comprehensive report of the strengths of such approaches and we show that humanratings can be reproduced by these fully automatic metrics with high accuracy.
Ourresults are based on data for multi-document news summarization.The key insights of our approach can be summarized as follows:Input?summary similarity:Good summaries are representative of the input and so onewould expect that the more similar a summary is to the input, the better its content.Identifying a suitable input?summary similarity metric will provide a means for fullyautomatic evaluation of summaries.
We present a quantitative analysis of this hypothe-sis and show that input?summary similarity is highly predictive of scores assigned byhumans for the summaries.
The choice of an appropriate metric to measure similarity iscritical, however, and we show that information-theoretic measures turn out to be themost powerful for this task (Section 4).Addition of pseudomodels: Having a larger number of model summaries has beenshown to give more stable evaluation results, but for some data sets only a single modelsummary is available.
We test the utility of pseudomodels, which are system summariesthat are chosen to be added to the human summary pool and that are used as additionalmodels.
We find that augmenting the gold standard with pseudomodels helps obtainbetter correlations with human judgments than if a single model is used (Section 5).System summaries as models: Most current summarization systems perform contentselection reasonably well.
We examine an approach to evaluation that exploits systemoutput and considers all system summaries for a given input as a gold standard (Sec-tion 6).
We find that similarity between a summary and such a gold standard constitutesa powerful automatic evaluation measure.
The correlation between this measure andhuman evaluations is over 0.9.We analyze a number of similarity metrics to identify the ones that perform bestfor automatic evaluation.
The tool we developed, SIMetrix (Summary Input similarity268Louis and Nenkova Automatic Content EvaluationMetrics), is freely available.1 We test these resource-poor approaches to predict sum-mary content scores assigned by human assessors.
We evaluate the results on data fromthe Text Analysis Conferences.2We find that our automatic methods to estimate summary quality are highly predic-tive of human judgments.
Our best result is 0.93 correlation with human rankings usingno model summaries and this is on par with automatic evaluation methods that do usehuman summaries.
Our study provides some direction towards alternative methodsof evaluation on non-standard test sets.
The goal of our methods is to aid systemdevelopment and tuning on new, especially large, data sets using little resources.
Ourmetrics complement but are not intended to replace existing manual and automaticapproaches to evaluation wherein the latter?s strength and reliability are importantfor high confidence evaluations.
Some of our findings are also relevant for systemdevelopment as we identify desirable properties of automatic summaries that can becomputed from the input (see Section 4).
Our results are also strongly suggestive thatsystem combination has the potential for improving current summarization systems(Section 6).We start out with an outline of existing evaluation methods and the potentialshortcomings of these approaches which we wish to address.2.
Current Content Evaluation MethodsSummary quality is defined by two key aspects?content and linguistic quality.
A goodsummary should contain the most important content in the input and also structurethe content and present it as well-written text.
Several methods have been proposed forevaluating system-produced summaries; some only assess content, others only linguis-tic quality, and some combine assessment of both.
Some of these approaches are manualand others can be performed automatically.In our work, we consider the problem of automatic evaluation of content quality.To establish the context for our work, we provide an overview of current contentevaluation methods used at the annual evaluations run by NIST.The Text Analysis Conference (TAC, previously called the Document Understand-ing Conference [DUC]3) conducts large scale evaluation of automatic systems on dif-ferent summarization tasks.
These conferences have been held every year since 2001and the test sets and evaluation methods adopted by TAC/DUC have become thestandard for reporting results in publications.
TAC has employed a range of manualand automatic metrics over the years.Manual evaluations of the systems are performed at NIST by trained assessors.The assessors score the summaries eithera) by comparing with a gold-standard summary written by humans, orb) by providing a direct rating on a scale (1 to 5 or 1 to 10).The human summaries against which other summaries are compared are inter-changeably called models, gold standards, and references.
Within TAC, they are typ-ically calledmodels.1 SIMetrix can be downloaded at http://www.seas.upenn.edu/?lannie/IEval2.html.2 http://www.nist.gov/tac/.3 http://duc.nist.gov/.269Computational Linguistics Volume 39, Number 22.1 Content Coverage ScoresThe methods relying on a gold standard have evolved over the years.
In the firstyears of DUC, a single model summary was used.
System summaries were evaluatedby manually assessing how much of the model?s content is expressed in the systemsummary.
Each clause in the model represents one unit for the evaluation.
For each ofthese clauses, assessors specify the extent to which its content is expressed in a givensystem summary.
The average degree to which the model summary?s clauses overlapwith the system summary?s content is called coverage.
These coverage scores weretaken as indicators of content quality for the system summaries.Different people include very different content in their summaries, however, andso the coverage scores can vary depending on which model is used (Rath, Resnick, andSavage 1961).
This problem of bias in evaluation was later addressed by the pyramidtechnique, which combines information from multiple model summaries to composethe reference for evaluation.
Since 2005, the pyramid evaluation method has becomestandard.2.2 Pyramid EvaluationThe pyramid evaluation method (Nenkova and Passonneau 2004) has been developedfor reliable and diagnostic assessment of content selection quality in summarization andhas been used in several large scale evaluations (Nenkova, Passonneau, and McKeown2007).
It uses multiple human models from which annotators identify semanticallydefined Summary Content Units (SCUs).
Each SCU is assigned a weight equal tothe number of human model summaries that express that SCU.
An ideal maximallyinformative summary would express a subset of the most highly weighted SCUs, withmultiple maximally informative summaries being possible.
The pyramid score for asystem summary S is equal to the following ratio:py(S) =sum of weights of SCUs expressed in Ssum of weights of an ideal summary with the same number of SCUs as S(1)In this way, a more reliable score for a summary is obtained usingmultiple referencesummaries.
Four human summaries are normally used for pyramid evaluation at TAC.2.3 Responsiveness EvaluationResponsiveness of a summary is a measure of overall quality combining both contentselection and linguistic quality.
It measures to what extent summaries convey appropri-ate content in a structured fashion.
Responsiveness is assessed by direct ratings givenby the judges.
For example, a scale of 1 (poor summary) to 5 (very good summary) isused and these assessments are done without reference to any model summaries.Pyramid and responsiveness are the standardly used manual approaches forcontent evaluation.
They produce rather similar rankings of systems at TAC.
The(Spearman) correlation between the two for ranking systems that participated inthe TAC 2009 conference is 0.85 (p-value 6.8e-16, 53 systems).
The responsivenessmeasure involves some aspects of linguistic quality whereas the pyramid metric wasdesigned for content only.
Such high correlation indicates that the content factor has270Louis and Nenkova Automatic Content Evaluationsubstantial influence on the responsiveness judgments, however.
The high correlationalso indicates that two types of human judgments made on very different basis?gold-standard summaries and direct judgments?can agree and provide fairly similarrankings of summaries.2.4 ROUGEManual evaluation methods require significant human effort.
Moreover, the pyramidevaluation involves detailed annotation for identifying SCUs in human and systemsummaries and requires training of assessors to perform the evaluation.
Outside ofTAC, therefore, system developments and results are regularly reported using ROUGE,a suite of automatic evaluation metrics (Lin and Hovy 2003; Lin 2004b).ROUGE automates the comparison betweenmodel and system summaries based onn-gram overlaps.
These overlap scores have been shown to correlate well with humanassessment (Lin 2004b) and so ROUGE removes the need for manual judgments in thispart of the evaluation.ROUGE scores are computed typically using unigram (R1) or bigram (R2) overlaps.In TAC, four human summaries are used as models and their contents are combinedfor computing the overlap scores.
For fixed length summaries, the recall from thecomparison is used as the quality metric.
Other metrics such as longest subsequencematch are also available.
Another ROUGE variant is RSU4, which computes the overlapin terms of skip bigrams, where two unigrams with a gap of up to four interveningwords are considered as bigrams.
This latter metric provides some additional flexibilitycompared to the stricter R2 scores.The correlations between ROUGE and manual evaluations for systems in TAC2009 are shown in Table 1 and vary between 0.76 and 0.94 for the different variants.4Here, and in all subsequent experiments, Spearman correlations are computed usingthe R toolkit (R Development Core Team 2011).
In this implementation, significancevalues for the correlations are produced using the AS 89 algorithm (Best and Roberts1975).These correlations are highly significant and show that ROUGE is a high perfor-mance automatic evaluation metric.We can consider the ROUGE results as the upper bound of performance for themodel-free evaluations that we propose because ROUGE involves direct comparisonwith the gold-standard summaries.
Our metrics are designed to be used when modelsummaries are not available.2.5 Automatic Evaluation Without Gold-Standard SummariesAll of thesemethods require significant human involvement.
In evaluations where gold-standard summaries are needed, assessors first read the input documents (10 or moreper input) and write a summary.
Then manual comparison of system and gold standardis done, which takes additional time.
Gillick and Liu (2010) hypothesize that at least17.5 hours are needed to evaluate two systems under this set up on a standard testset.
Moreover, multiple gold-standard summaries are needed for the same input, sodifferent assessors have to read and create summaries.
The more reliable evaluation4 The scores were computed after stemming but stop words were retained in the summaries.271Computational Linguistics Volume 39, Number 2Table 1Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data(53 systems).
All correlations are highly significant with p-value < 10?10.ROUGE variant Pyramid ResponsivenessROUGE-1 0.88 0.76ROUGE-2 0.94 0.82ROUGE-SU4 0.92 0.79methods such as pyramid involve even more annotations at the clause level.
Althoughresponsiveness does not require gold-standard summaries, in a system developmentsetting, responsiveness judgments are resource-intensive.
It requires judges to directlyassign scores to summaries, so humans are in the loop each time the evaluation needsto be done, making it rather costly.
For ROUGE, however, once the human summariesare created, the scores can be computed automatically for repeated system developmentruns.
This benefit has made ROUGE immensely popular.
But the initial investment oftime for gold-standard creation is still necessary.Another important point is that for TAC, the gold standards are created by trainedassessors at NIST.
Non-expert evaluation options such asMechanical Turk have recentlybeen explored by Gillick and Liu (2010).
They provided annotators with gold-standardreferences and system summaries and asked them to score the system summaries on ascale from 1 to 10 with respect to how well they convey the same information as themodels.
They analyzed how these scores are related to responsiveness judgments givenby the expert TAC assessors.
The study assessed only eight automatic systems fromTAC 2009 and the correlation between the ratings from experts and Mechanical Turkannotations was 0.62 (Spearman).
The analysis concludes that evaluations produced inthis way tend to be noisy.One reason was that non-expert annotators were quite influenced by the readabilityof the summaries.
For example, they tended to assign high scores to the baselinesummary that picks the lead paragraph.
The baseline summary, however, is rankedby expert annotators as low in responsiveness compared to other systems?
summaries.Further, the non-expert evaluation led to few significant differences in the system rank-ings (score of system A is significantly greater/lesser than that of B) compared with theTAC evaluations of the same systems.Another problemwith non-expert evaluation is the quality of themodel summaries.Evaluations based on model summaries assume that the gold standards are of highquality.
Through the years at TAC, considerable effort has been invested to ensure thatthe evaluation scores do not vary depending on the particular gold standard.
In theearly years of TAC only one gold-standard summary was used.
During this time, papersreported ANOVA tests examining the factors that most influenced summary scoresfrom the evaluations and found that the identity of the judge turned out to be the mostsignificant factor (McKeown et al2001; Harman andOver 2004).
But it is desirable that amodel summary or a human judgment be representative of important content in generaland does not depict the individual biases of the person who created the summary ormade the judgment.
So the evaluationmethodologywas refined to remove the influenceof the assessor identity on the evaluation.
The pyramid evaluation was also developedwith this goal of smoothing out the variation between judges.
Gillick and Liu (2010)point out that Mechanical Turk evaluations have this undesirable outcome: The identity272Louis and Nenkova Automatic Content Evaluationof the judges turns out to be the most significant factor influencing summary scores.Gillick and Liu do not elicit model summaries, only direct judgments on quality.
Wesuspect that the task would only be harder if model summaries were to be created bynon-experts.The problem that has been little addressed by any of these discussed metrics is eval-uation when there are no gold-standard summaries available.
Systems are developedby fine-tuning on the TAC data sets, but in non-TAC data sets in novel or very largedomains model summaries may not be available.
Even though ROUGE provides goodperformance in automatic evaluation, it is not usable under these conditions.
Further,pyramid and ROUGE use multiple gold-standard summaries for evaluation (ROUGEcorrelates with human judgments better when computed using multiple models; wediscuss this aspect further in Section 5) so even a single gold-standard summary maynot be sufficient for reliable evaluation.In our work, we propose fully automatic methods for content evaluation whichcan be used in the absence of human summaries.
We also explore methods to furtherimprove the evaluation performance when only one model summary is available.3.
Data and Evaluation PlanIn this section, we describe the data we use throughout our article.
We carry outour analysis on the test sets and system scores from TAC 2009.
TAC 2009 is also theyear when NIST introduced a special track called AESOP (Automatically EvaluatingSummaries of Peers).
The goal of AESOP is to identify automatic metrics that correlatewell with human judgments of summary quality.We use the data from the TAC 2009 query focused-summarization task.5 Each inputconsists of ten news documents.
In addition, the user?s information needs associatedwith each input is given by a query statement consisting of a title and narrative.
Anexample query statement is shown here:Title: Airbus A380Narrative: Describe developments in the production and launch of the Airbus A380.A system must produce a summary that addresses the information required by thequery.
The maximum length for summaries is 100 words.The test set contains 44 inputs, and 53 automatic systems (including baselines)participated that year.
These systems were manually evaluated for content using bothpyramid and responsiveness methods.
In TAC 2009, two oracle systems were intro-duced during evaluation whose outputs are in fact summaries created by people.
Weignore these two systems and use only the automatic participant submissions and theautomatic baseline systems.As a development set, we use the inputs, summaries, and evaluations from theprevious year, TAC 2008.
There were 48 inputs in the query-focused task in 2008 and58 automatic systems participated.TAC 2009 also involved an update summarization task and we obtained similarresults on the summaries from this task.
In this article, for clarity we only present results5 http://www.nist.gov/tac/2009/Summarization/update.summ.09.guidelines.html.273Computational Linguistics Volume 39, Number 2on evaluating the query-focused summaries, but the update task results are describedin detail in Louis and Nenkova (2008, 2009a, 2009c).3.1 Evaluating Automatic MetricsFor each of our proposed metrics, we need to assess their performance in replicatingmanually produced rankings given by the pyramid and responsiveness evaluations.We use two measures to compare these human scores for a system with the automaticscores from one of our metrics:a) SPEARMAN CORRELATION: Reporting correlations with human evaluation metricsis the norm for validating automatic metrics.
We report Spearman correlation, whichcompares the rankings of systems produced by the two methods instead of the actualscores assigned to systems.b) PAIRWISE ACCURACY: To complement correlation results with numbers that haveeasier intuitive interpretation, we also report the pairwise accuracy of our metrics inpredicting the human scores.
For every pair of systems (A, B), we examine whethertheir pairwise ranking (eitherA > B,A < B, orA = B) according to the automatic metricagrees with the ranking of the same pair according to human evaluation.
If it does, thepair is concordant with human judgments.
The pairwise accuracy is the percentage ofconcordant pairs out of the total system pairs.
This accuracy measure is more inter-pretable than correlations in terms of the errors made by a metric.
A metric with 90%accuracy incorrectly flips 10% of the pairs, on average, in a ranking it produces.
Thismeasure is inspired by the Kendall tau coefficient.We test themetrics for success in replicating human scores overall across the full testset as well as identifying good and bad summaries for individual inputs.
We thereforereport the correlation and accuracy of our metrics at the following two levels.a) SYSTEM LEVEL (MACRO): The average score for a system is computed over the entireset of test inputs using both manual and our automatic methods.
The correlationsbetween ranks assigned to systems by these average scores will be indicative of thestrength of our features to predict overall system rankings on the test set.
Similarly, thepairwise accuracies are computed using the average scores for the systems in the pair.b) INPUT LEVEL (MICRO): For each individual input, we compare the rankings for thesystem summaries using manual and automatic evaluations.
Here the correlation oraccuracy is computed for each input.
For correlations, we report the percentage of inputsfor which significant correlations (p-value < 0.05) were obtained.
For accuracy, thesystems are paired within each input.
Then these pairs for all the inputs are put togetherand the fraction of concordant pairs is computed.
Micro-level analysis highlights theability of an evaluation metric to identify good and poor quality system summariesproduced for a specific input and this task is bound to be harder than system levelpredictions.
For example, even with wrong prediction of rankings on a few inputs, theaverage scores (macro-level) for a system might not be affected.In the following sections, we describe three experiments in which we analyze thepossibility of performing automatic evaluation involving only minimal or no humanjudgments: Using input?summary similarity (Section 4), using system summaries aspseudomodels alongside gold-standard summaries created by people (Section 5), andusing the collection of system summaries as a gold standard (Section 6).
All the auto-matic systems, including baselines, were evaluated.274Louis and Nenkova Automatic Content Evaluation4.
Input?Summary Similarity: Evaluation Using Only the Source TextHere we present and evaluate a suite of metrics which do not require gold-standardhuman summaries for evaluation.
The underlying intuition is that good summaries willtend to be similar to the input in terms of content.
Accordingly, we use the similarity ofthe distribution of terms in the input and summaries as a measure of summary content.Although the motivation for this metric is highly intuitive, it is not clear how simi-larity should be defined for this particular problem.
Here we provide a comprehensivestudy of input?summary similarity metrics and show that some of these measurescan indeed be very accurate predictors of summary quality even while using no gold-standard human summaries at all.Prior to our work, the proposal for using the input for evaluation has been broughtup in a few studies.
These studies did not involve a direct evaluation of the capacityof input?summary similarity to replicate human ratings, however, and they did notcompare similarity metrics for the task.
Because large scale manual evaluation resultsare available now, our work is the first to evaluate this possibility in a direct mannerand involving study of correlations with different types of human evaluations.
In thefollowing section we detail some of the prior studies on input?summary similarity forsummary evaluation.4.1 Related WorkOne of the motivations for using the input text rather than gold-standard summariescomes from the need to perform large scale evaluations with test sets comprised ofthousands of inputs.
Creating human summaries for all of themwould be an impossibletask indeed.In Radev and Tam (2003), therefore, a large scale fully automatic evaluation of eightsummarization systems on 18,000 documents was performed without any human effortby using the idea of input?summary similarity.
A search engine was used to rank docu-ments according to their relevance to a given query.
The summaries for each documentwere also ranked for relevance with respect to the same query.
For good summarizationsystems, the relevance ranking of summaries is expected to be similar to that of thefull documents.
Based on this intuition, the correlation between relevance rankingsof summaries and original documents was used to compare the different systems.
Asystem whose summaries obtained highly similar rankings to the original documentscan be considered better than a system whose rankings have little agreement.Another situation where input?summary similarity was hypothesized as a possibleevaluation was in work concerned with reducing human bias in evaluation.
Becausehumans vary considerably in the content they include for the same input (Rath,Resnick, and Savage 1961; van Halteren and Teufel 2003), rankings of systems arerather different depending on the identity of the model summary used (also notedby McKeown et al[2001] and Jing et al[1998]).
Donaway, Drummey, and Mather(2000) therefore suggested that there are considerable benefits to be had in adopting amethod of evaluation that does not require human gold standards but instead directlycompares the original document and its summary.
In their experiments, Donaway,Drummey, and Mather demonstrated that the correlations between manual evaluationusing a gold-standard summary anda) manual evaluation using a different gold-standard summary275Computational Linguistics Volume 39, Number 2b) automatic evaluation by directly comparing input and summary6are the same.
Their conclusion was that such automatic methods should be seriouslyconsidered as an alternative to evaluation protocols built around the need to comparewith a gold standard.These studies, however, do not directly assess the performance of input?summarysimilarity for ranking systems.
In Louis and Nenkova (2009a), we provided the firststudy of several metrics for measuring similarity for this task and presented correla-tions of these metrics with human produced rankings of systems.
We have released atool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similaritymetrics that we explored.74.2 Metrics for Computing SimilarityIn this section, we describe a suite of similarity metrics for comparing the input andsummary content.
We use cosine similarity, which is standard for many applications.The other metrics fall under three main classes: distribution similarity, summary likeli-hood, and use of topic signature words.
The distribution similarity metrics compare thedistribution of words in the input with those in the summary.
The summary likelihoodmetrics are based on a generative model of word probabilities in the input and usethe model to compute the likelihood of the summary.
Topic signature metrics focus on asmall set of descriptive and topical words from the input and compare them to summarycontent rather than using the full vocabulary of the input.Both input and summary words were stopword-filtered and stemmed before com-puting the features.4.2.1 Distribution Similarity.
Measures of similarity between two probability distribu-tions are a natural choice for our task.
One would expect good summaries to be charac-terized by low divergence between probability distributions of words in the input andsummary, and by high similarity with the input.We experimented with three common measures: Kullback Leibler (KL) divergence,Jensen Shannon (JS) divergence, and cosine similarity.These three metrics have already been applied for summary evaluation, albeit ina different context.
In their study of model-based evaluation, Lin et al(2006) used KLand JS divergences to measure the similarity between human and machine summaries.They found that JS divergence always outperformed KL divergence.
Moreover, the per-formance of JS divergence was better than standard ROUGE scores for multi-documentsummarization when multiple human models were used for the comparison.The use of input?summary similarity in Donaway, Drummey, and Mather (2000),which we described in the previous section, is more directly related to our work.
Buthere, inputs and summaries were compared using only one metric: cosine similarity.Kullback Leibler (KL) divergence: The KL divergence between two probability distri-butions P and Q is given byD(P||Q) =?wpP(w) log2pP(w)pQ(w)(2)6 They used cosine similarity to perform the input?summary comparison.7 http://www.seas.upenn.edu/?lannie/IEval2.html.276Louis and Nenkova Automatic Content EvaluationIt is defined as the average number of bits wasted by coding samples belonging to Pusing another distribution Q, an approximate of P. In our case, the two distributionsof word probabilities are estimated from the input and summary, respectively.
BecauseKL divergence is not symmetric, both input?summary and summary?input divergencesare introduced as metrics.
In addition, the divergence is undefined when pP(w) > 0 butpQ(w) = 0.
We perform simple smoothing to overcome the problem.p(w) =C+ ?N + ?
?
B (3)Here C is the count of word w and N is the number of tokens; B = 1.5|V|, where Vis the input vocabulary and ?
was set to a small value of 0.0005 to avoid shifting toomuch probability mass to unseen events.Jensen Shannon (JS) divergence: The JS divergence incorporates the idea that the dis-tance between two distributions cannot be very different from the average of distancesfrom their mean distribution.
It is formally defined asJ(P||Q) = 12[D(P||A)+D(Q||A)], (4)where A = P+ Q2 is the mean distribution of P and Q.
In contrast to KL divergence,the JS distance is symmetric and always defined.
We compute both smoothed andunsmoothed versions of the divergence as summary scores.Vector space similarity: The third metric is cosine overlap between the tf ?
idf vectorrepresentations of input and summary contents.cos?
=vinp.vsumm||vinp|| ||vsumm||(5)We compute two variants:1.
Vectors contain all words from input and summary.2.
Vectors contain only topic signature words from the input and all words ofthe summary.Topic signatures are words highly descriptive of the input, as determined by theapplication of the log-likelihood test (Lin and Hovy 2000).
Using only topic signaturesfrom the input to represent text is expected to be more accurate because the reducedvector has fewer dimensions compared with using all the words from the input.4.2.2 Summary Likelihood.
For this approach, we view summaries as being generatedaccording to word distributions in the input.
Then the probability of a word in the inputwould be indicative of how likely it is to be emitted into a summary.
Under this gen-erative model, the likelihood of a summary?s content can be computed using differentmethods and we expect the likelihood to be higher for better quality summaries.We compute both a summary?s unigram probability as well as its probability undera multinomial model.277Computational Linguistics Volume 39, Number 2Unigram summary probability:(pinpw1)n1 (pinpw2)n2 ...(pinpwr)nr (6)where pinpwi is the probability in the input of word wi, ni is the number of times wiappears in the summary, and w1.
.
.wr are all words in the summary vocabulary.Multinomial summary probability:N!n1!n2!
.
.
.
nr!
(pinpw1)n1 (pinpw2)n2 .
.
.
(pinpwr)nr (7)where N = n1 + n2 + .
.
.+ nr is the total number of words in the summary.4.2.3 Use of Topic Words in the Summary.
Summarization systems that directly optimizethe number of topic signature words during content selection have fared very wellin evaluations (Conroy, Schlesinger, and O?Leary 2006).
Hence the number of topicsignatures from the input present in a summary might be a good indicator of summarycontent quality.
In contrast to the previous methods, by limiting to topic words, we useonly a representative subset of the input?s words for comparing with summary content.We experiment with two features that quantify the presence of topic signatures in asummary:1.
The fraction of the summary composed of input?s topic signatures.2.
The percentage of topic signatures from the input that also appear in thesummary.Although both features will obtain higher values for summaries containing manytopic words, the first is guided simply by the presence of any topic word and the secondmeasures the diversity of topic words used in the summary.4.2.4 Feature Combination Using Linear Regression.
We also evaluated the performanceof a linear regression metric combining all of these features.
During development, thevalue of the regression-based score for each summary was obtained using a leave-one-out approach.
For a particular input and system-summary combination, the training setconsisted only of examples which included neither the same input nor the same system.Hence during training, no examples of either the test input or system were seen.4.3 ResultsWe first present an analysis of all the similarity metrics on our development data,TAC?08.
In the next section, we analyze the performance of our two best features onthe TAC?09 data set.4.3.1 Feature Analysis: Which Similarity Metric is Best?.
Table 2 shows the macro-levelSpearman correlations between manual and automatic scores averaged across the48 inputs in TAC?08.Overall, we find that both distribution similarity and topic signature features pro-duce system rankings very similar to those produced by humans.
Summary likelihood,on the other hand, turns out to not be predictive of content selection performance.
The278Louis and Nenkova Automatic Content EvaluationTable 2Spearman correlation on the macro level for TAC?08 data (58 systems).
All results are highlysignificant with p-values < 0.000001 except unigram and multinomial summary probability,which are not significant even at the 0.05 level.Features Pyramid ResponsivenessJS div ?0.880 ?0.736JS div smoothed ?0.874 ?0.737% of input topic words 0.795 0.627KL div summary?input ?0.763 ?0.694cosine overlap, all words 0.712 0.647% of summary = topic words 0.712 0.602cosine overlap, topic words 0.699 0.629KL div input?summary ?0.688 ?0.585multinomial summary probability 0.222 0.235unigram summary probability ?0.188 ?0.101regression 0.867 0.705ROUGE-1 recall 0.859 0.806ROUGE-2 recall 0.905 0.873linear regression combination of features obtains high correlations with manual scoresbut does not lead to better results than the single best feature: JS divergence.JS divergence obtains the best correlations with both types of manual scores?0.88 with pyramid score and 0.74 with responsiveness.
The regression metric performscomparably, with correlations of 0.86 and 0.70.
The correlations obtained by both JSdivergence and the regression metric with pyramid evaluations are in fact better thanthat obtained by ROUGE-1 recall (0.85).The best topic signature-based feature?the percentage of input?s topic signaturesthat are present in the summary?ranks next only to JS divergence and regression.
Thecorrelations between this feature and pyramid and responsiveness evaluations are 0.79and 0.62, respectively.
The proportion of summary content composed of topic wordsperforms worse as an evaluation metric with correlations 0.71 and 0.60.
This resultindicates that summaries that cover more topics from the input are judged to have bettercontent than those in which fewer topics are mentioned.Cosine overlaps and KL divergences obtain good correlations but still lower thanJS divergence and the percentage of input topic words.
Further, rankings based onunigram and multinomial summary likelihood do not correlate significantly withmanual scores.On a per input basis, the proposed metrics are not that effective in distinguishingwhich summaries have good and poor content.
The minimum and maximum correla-tions with manual evaluations across the 48 inputs are given in Table 3.
The numberand percentage of inputs for which correlations were significant are also reported.JS divergence obtains significant correlations with pyramid scores for 73%.
The bestcorrelation was 0.71 on a particular input and the worst performance was 0.27 correla-tion for another input.
The results are worse for other features and for comparison withresponsiveness scores.At the micro level, combining features with regression gives the best result overall,in contrast to the findings for the macro-level setting.
This result has implications forsystem development; no single feature can reliably predict good content for a partic-ular input.
Even a regression combination of all features is a significant predictor of279Computational Linguistics Volume 39, Number 2Table 3Spearman correlations at micro level for TAC?08 data (58 systems).
Only the minimum andmaximum values of the significant correlations are reported, together with the number andpercentage of inputs that obtained significant correlation.Pyramid Responsivenessnumber numberFeatures max min significant (%) max min significant (%)JS div ?0.714 ?0.271 35 (72.9) ?0.654 ?0.262 35 (72.9)JS div smoothed ?0.712 ?0.269 35 (72.9) ?0.649 ?0.279 33 (68.8)KL div summary-input ?0.736 ?0.276 35 (72.9) ?0.628 ?0.261 35 (72.9)% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)cosine overlap - all words 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)KL div input-summary ?0.628 ?0.262 28 (58.3) ?0.577 ?0.267 22 (45.8)cosine overlap - topic words 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)% summary = topic words 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)multinomial summary prob.
0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)unigram summary prob.
0.292 0.261 2 (4.2) 0.466 0.287 2 (4.2)regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)content selection quality in only 77% of the cases.
For example, a set of documents,each describing a different opinion on an issue, is likely to have less repetition onboth the lexical and content unit levels.
Because the input?summary similarity metricsrely on the word distribution of the input for clues about important content, theirpredictiveness will be limited for such inputs.8 Follow-up work to our first results onfully automatic evaluation by Saggion et al(2010) has assessed the usefulness of the JSdivergence measure for evaluating summaries from other tasks and for languages otherthan English.
Whereas JS divergence was significantly predictive of summary qualityfor other languages as well, it did not work well for tasks where opinion and biograph-ical type inputs were summarized.
We provide further analysis and some examples inSection 7.Overall, the micro level results suggest that the fully automatic measures we ex-amined will not be useful for providing information about summary quality for anindividual input.
For averages over many test sets, the fully automatic evaluationsgive more reliable results, and are highly correlated with rankings produced by manualevaluations.
On the other hand, model summaries written for the specific input wouldgive a better indication of what information in the input was important and interesting.This is indeed the case as we shall see from the ROUGE scores in the next section.4.3.2 Comparison with ROUGE.
The aim of our study is to assess metrics for evaluationin the absence of human gold standards, scenarios where ROUGE cannot be used.We do not intend to directly compare the performance of ROUGE with our metrics,8 In fact, it would be surprising to find an automatically computable feature or feature combination whichwould be able to consistently predict good content for all individual inputs.
If such features existed,an ideal summarization system would already exist.280Louis and Nenkova Automatic Content Evaluationtherefore.
We discuss the correlations obtained by ROUGE in the following, however, toprovide an idea of the reliability of our metrics compared with evaluation quality thatis provided by ROUGE and multiple human summaries.At the macro level, the correlation between ROUGE-1 and pyramid scores is 0.85(Table 2).
For ROUGE-2 the correlation with pyramid scores is 0.90, practically identicalwith JS divergence.Because the performance of these two measures seem close, we further analyzedtheir errors.
The focus of this analysis is to understand if JS divergence and ROUGE-2are making errors in ordering the same systems or whether their errors are different.This result would also help us to understand if ROUGE and JS divergence have com-plementary strengths that can be combined.
For this, we considered pairs of systemsand computed the better system in each pair according to the pyramid scores.
Then,for ROUGE-2 and JS divergence, we recorded how often they provided the correctjudgment for the pairs as indicated by the pyramid evaluation.
There were 1,653 pairsof systems at the macro level and the results are in Table 4.This table shows that a large majority (80%) of the same pairs are correctly predictedby both ROUGE and JS divergence.
Another 6% of the pairs are such that both metricsdo not provide the correct judgment.
Therefore, ROUGE and JS divergence appear toagree on a large majority of the system pairs.
There is a small percentage (14%) that iscorrectly predicted by only one of the metrics.
The chances of combining ROUGE andJS divergence to get a better metric appears small, therefore.
To test this hypothesis, wetrained a simple linear regression model combining JS divergence and ROUGE-2 scoresas predictors for the pyramid scores and tested the predictions of this model on datafrom TAC 2009.
The combination did not give improved correlations compared withusing ROUGE-2 alone.In the case of manual responsiveness, which combines aspects of linguistic qualityalong with content selection evaluation, the correlation with JS divergence is 0.73.For ROUGE, it is 0.80 for R1 and 0.87 for R2.
Here, ROUGE-1 outperforms all thefully automatic evaluations.
This is evidence that the human gold-standard summariesprovide information that is unlikely to ever be approximated by information from theinput alone, regardless of feature sophistication.At the micro level, ROUGE clearly does better than all the fully automatic measuresfor replicating both pyramid and responsiveness scores.
The results are shown in thelast two rows of Table 3.
ROUGE-1 recall obtains significant correlations for over 95%of inputs for responsiveness and 98% of inputs for pyramid evaluation compared to73% (JS divergence) and 77% (regression).
Undoubtedly, at the input level, comparisonwith model summaries is substantially more informative.When gold-standard summaries are not available, however, our features can pro-vide reliable estimates of system quality when averaged over a set of test inputs.Table 4Overlap between ROUGE-2 and JS divergence predictions for the best system in a pair(TAC 2008, 1,653 pairs).
The gold-standard judgment for a better system is computedusing the pyramid scores.JSD correct JSD incorrectROUGE-2 correct 1,319 (79.8%) 133 (8.1%)ROUGE-2 incorrect 96 (5.8%) 105 (6.3%)281Computational Linguistics Volume 39, Number 2Table 5Input?summary similarity evaluation: Results on TAC?09 (53 systems).Correlations Pairwise accuracyMacro level Micro level Macro level Micro levelMetric py resp py resp py resp py respJS div 0.74 0.70 84.1 75.0 78.0 75.7 65.1 50.1Regr 0.77 0.67 81.8 65.9 80.1 74.8 64.7 49.4RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.04.3.3 Results on TAC?09 Data.
To evaluate our metrics for fully automatic evaluation, wemake use of the TAC?09 data.
The regression metric was trained on all of the 2008 datawith pyramid scores as the target.
Table 5 shows the results on the TAC?09 data.
Wealso report the correlations obtained by ROUGE-SU4 because it was the official baselinemeasure adopted at TAC?09 for comparison of automatic evaluation metrics.The correlations are lower than on our development set.
The highest correlation atmacro level is 0.77 (regression) in contrast to 0.88 (JS divergence) and 0.86 (regression)obtained on the TAC?08.
The regression metric turns out better than JS divergence onthe TAC?09 data for predicting pyramid scores.
JS divergence continues to be the bestmetric on the basis of correlations with responsiveness, however.In terms of the pairwise scores, the automatic metrics have 80% accuracy in pre-dicting the pyramid scores at the system level, about 8% lower than that obtained byROUGE.
For responsiveness, the best accuracy is obtained by regression (75%).
Thisresult shows that the ranking according to responsiveness is likely to have a largenumber of flips.
ROUGE is 5 percentage points better than regression for predictingresponsiveness but this value is still low compared to accuracies in replicating thepyramid scores.The pairwise accuracy at the micro level is 65% for the automatic metrics and herethe gap between ROUGE and our metrics is 5 percentage points but it is a significantpercentage as the total pairs at micro level are about 60,000 (all pairings of 53 systemsin 44 inputs).Overall, the performance of the fully automatic evaluation is still high for useduring system development.
A further advantage is that these metrics are consistentlypredictive across two years as shown by these results.
In Section 7, we analyze somereasons for the difference in performance in the two years.
In terms of best metrics, bothJS divergence and regression turn out to be useful with little difference in performancebetween them.5.
Pseudomodels: Use of System Summaries in Addition to Human SummariesMethods such as pyramid use multiple human summaries to avoid bias in evaluationwhen using a single gold standard.
ROUGE metrics are also currently used with mul-tiple models, when available.
But often, even if gold-standard summaries are availableon non-standard test sets, they are few in number.
Data sets with one gold-standardsummary (such as abstracts of scientific papers and editor-produced summaries of newsarticles) are common.
The question now is whether we can provide the same quality282Louis and Nenkova Automatic Content Evaluationevaluation using a single gold-standard summary as compared to using several goldstandards.To tackle this problem, we propose the use of pseudomodel system summaries.Our approach is as follows: We first predict the scores of systems on the basis of the fewavailable models.
The top ranking systems from this evaluation are then consideredas ?pseudo-models;?
their summaries are added to the gold-standard set alg withthe existing human models.
The final evaluation scores are produced by comparisonwith this expanded model set?original model summaries plus the pseudomodels.
Ourhypothesis is that the scores produced after the addition of pseudomodels would bemore reliable and correlate better with human scores compared with evaluation usinga single model summary.Before we describe our method, we provide a glimpse of the variation in evaluationquality depending on the number of models used.
Previous studies have shown thatat the system level, system rankings even with a single model will be stable whencomputed over a large enough number of test inputs.
Harman and Over (2004) showthat the relative ranks of systems computed using one model do not change whencomputed using another model when the number of inputs is large.
Again under thesame conditions of having a large number of inputs, Lin (2004a) and Owkzarzak andDang (2009) show that ROUGE correlations with human scores are stable when usingfew human models.
In machine translation evaluation, similar results are noted byZhang and Vogel (2010), who found that the lack of additional reference translationscan be handled by evaluating the systems on more test examples.Multiple models are particularly important for evaluation at the level of individualinputs, however.
Table 6 shows the difference in correlations and pairwise accuracy ofROUGE with human scores when one and four model summaries are used.
We pickedthe first model in alphabetical order of their names for the computation of correlationbetween metrics and a single model.At the system level, the correlations from both set-ups are similar.
But at the microlevel, there is considerable difference in performance.
Using all four models, significantcorrelations with pyramid scores are obtained for 95% of the inputs.
The evaluationsthat rely on a single model produce significant correlations for only 84% of the inputs,however.
For responsiveness scores, which are model-independent, we see that themicro-level evaluations have a smaller increase as more models are added (79% to 81%).Again in terms of pairwise accuracy, the accuracy in predicting micro-level pyramidscores improves by 4% when additional models are used and the improvement is 3%for predicting responsiveness scores.
Given this difference in performance when oneand many models are used, we investigate how to improve evaluation when only onemodel is available.Table 6ROUGE evaluation with different number of models: macro level (Spearman correlations), microlevel (percentage of inputs with significant correlations on TAC?09 data).
No.
of systems = 53.Correlations Pairwise accuracyMacro level Micro level Macro level Micro levelTask py resp py resp py resp py respRSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0283Computational Linguistics Volume 39, Number 2We explore the possibility of augmenting the model set with good system sum-maries.
These system summaries or ?pseudomodels?
are chosen to be the ones whichreceive high scores based on the one available model summary.
We expect that thebenefit of pseudomodels will be noticeable in micro-level correlations with pyramidscores.
At the macro level, even with multiple human models there is no improvementin correlations compared with a single model, and the addition of less-ideal systemsummaries is not likely to be better than adding human summaries.5.1 Related WorkThe idea of using system output for evaluationwas introduced in the context of machinetranslation by Albrecht andHwa (2007, 2008).
In their method, Albrecht andHwa (2007)designate some systems to act as pseudoreferences.
Then, every candidate translationto be evaluated is compared to the translations produced by the pseudoreferences usinga variety of similarity metrics.
Each similarity value is then used as a feature andtrained to predict the human assigned score for that candidate translation.
They showthat the scores produced by their regression metric using only system-based referencescorrelates with human judgments to the same extent as scores produced using multiplehuman reference translations.
Also, when the regression method was used with humanreferences and some pseudoreferences put together, the correlations obtained by thefinal metric was better than using the human references alone.In Albrecht and Hwa (2007), pseudoreferences of different quality?best, moderateand worst?are chosen using the gold-standard judgments and evaluated for use aspseudoreferences.
They found that having the best systems as pseudoreferences workedbest, although even adding the worst system as pseudoreference gave reasonable per-formance as their regression approach is trained to predict quality by comparison to thestandard of the reference.
In their work, however, pseudoreferences of different qualityare chosen in an oracle manner (using the human-assigned scores).
This setting is notpractical because it depends on the actual system scores.
In later work, Albrecht andHwa (2008) use off-the-self machine translation systems as pseudoreferences and showthat they can contribute to good results.
This later work is a more realistic set-up andhere regression is important because we have no guarantees as to the quality of theoff-the-shelf systems on the test data.A similar idea of augmenting machine output to human gold standard was ex-plored in Madnani et al(2007) in the context of machine translation (MT).
For tuningMT systems, often multiple reference translations are required.
Madnani et alaug-mented reference translations of a sentence with automatically generated paraphrasesof the reference.
They found in the experiments that such augmentation helped inMT tuning?the number of reference translations needed could be cut in half andcompensated with automatic paraphrases.5.2 Choice of Pseudoreference SystemsFor this evaluation, the choice of the pseudoreference system is an important step.
Inthis section, we detail some development experiments that we performed to understandhow to best choose such pseudoreferences for the summary evaluation task.We examined a similar regression approach as followed by Albrecht and Hwa(2007).
We chose systems of different quality (best, mediocre, worst) based on the284Louis and Nenkova Automatic Content Evaluationoracle human-assigned scores.
The remaining systems were taken as the evaluationset.
For each summary in the evaluation data, we computed features to indicate theirsimilarity with the summaries of the chosen pseudoreference systems.
Our similarityfeatures were the recall scores from ROUGE overlaps.
We computed one feature eachfor unigram, bigram, trigram, and four-gram ROUGE scores.
Each of these four featuresis computed for each pseudoreference summary.The scores are used in a linear regression model to predict the summary score.We used a cross-validation approach where the summaries from one of the evaluationsystems were used as the test set and the summaries from the remaining systemsare used for training the regression model.
Then the average predicted score of eachsystem in the evaluation data was computed and compared with their average scoresas assigned during manual evaluations.The experiment was performed using data from four years of DUC conferences,2001 to 2004.
The manual scores in these earlier DUC years were the content coveragescores (described in Section 2.1), which use a single model summary for comparison.Table 7 shows the Spearman correlations between the scores from evaluations onlyagainst the pseudoreferences and those from the manual evaluation with the singlemodel.
The different settings for choice of pseudoreference systems are also indicated.These results showed that using the best systems as pseudoreferences provided thebest performance across different years.
When only worst or only mediocre systemswere used, the performance was much worse for predicting system scores.
Even whenthe best systems were augmented with the worst systems as pseudoreferences, theevaluation quality decreased compared with using the best systems only.Whereas Albrecht and Hwa (2007, 2008) obtained a slight improvement by alsousing a worst quality pseudoreference in the mix, for summarization it is better to haveonly the best systems.
One reason for this difference could be that for summary eval-uation, examples of worse summaries are not very informative.
Two good summariesmay have considerable variation in the content.
When a summary is similar to a bestsystem, therefore, we can say that the candidate summary is also of good quality.
Onthe other hand, when a candidate summary is similar to a worst system summary, itmay either be a worse summary or it may be a good summary with different contentthan the best system?s summary.
Indeed, when ROUGE was first introduced, it washeavily emphasized that it is a recall measure and that precision-oriented measuresdo worse.
Hence the weights learned for the similarity with the worst system maynot be very informative.
In summarization, the space of both good summaries andworse summaries for the same input is large.
Having more examples of good sum-maries appears to benefit evaluation more compared with having samples of worstquality.Table 7Spearman correlations between pseudoreference-based regression scores and manual contentscores.
The first column lists the type of pseudoreference chosen.Pseudoreference 2001 2002 2003 task 2 2004 task 2 2004 task 52 best systems 0.58 0.77 0.51 0.93* 0.83*2 worst systems 0.45 ?0.94* ?0.09 0.13 ?0.232 mediocre systems 0.72 0.08 0.53 0.64 0.182 best, 2 worst systems 0.38 0.20 0.25 0.92 0.73*The correlation was significant with p-value < 0.05.285Computational Linguistics Volume 39, Number 2Because the best systems turned out to have the maximum potential for actingas pseudoreferences, we wanted a way to identify some best systems without havingto rely on the oracle scores, as before.
This idea is feasible for our set-up.
In ourevaluation, we aimed to augment an existing model, so we used the available modelto automatically obtain an idea of some of the good systems from the pool.
Then wechose some of these top systems as pseudoreferences and combined them with the oneavailable model to form the reference set for final evaluation.
Because the reference sethas mostly best summaries, we did not use a regression approach based on similarity tothe different references.
Rather, we considered all of them as models and computeda single ROUGE score comparing a system summary with the pool of model pluspseudomodel summaries.5.3 Experimental Set-upWe now detail our experiments on the TAC 2009 data.TAC provides four model summaries for each input.
We assume that only one isavailable and choose a model for each input: the first in alphabetical order of identifiernames.
Based on this model, we compute the RSU4 scores for all systems.
We use twomethods to choose the pseudomodel systems.In the first approach, we rank all the systems based on their average scores over theentire test set.
The summaries of the top three overall best systems (global selection) areadded to the set of models for all inputs.
Alternatively, we also investigate a differentselection method.
For each input, the top scoring three summaries are added as modelsfor that input (local selection).
In both cases RSU4 was used to identify the best systemsaccording to the single available gold standard.The final rankings for all systems are produced using the RSU4 comparison basedon the expanded set of models (1 human model + 3 pseudomodel summaries).
Weimplemented a jackknifing procedure so that the systems selected to be pseudomodels(and therefore reference systems) could also be compared to other systems.
For eachinput, one of the reference systems (pseudomodels or human model) was removed at atime from the set of models and added to the set of systems.
The scores for the systemswere then computed by comparison with the three remaining models.
The final scorefor a system summary (not a pseudomodel) is the mean value of the scores with thefour different sets of reference summaries created by the jackknifing procedure.
Forpseudomodel systems, a single score value will be obtained per input resulting fromthe comparison with the other three models.5.4 ResultsThe system and input level performance before and after the addition of pseudomodelsis shown in Table 8.
The performance using four human models is shown in the last linefor comparison.At the macro level, the pseudomodel summaries provide little improvements.
Onlyfor the global model is there an increase in correlation, from 0.80 to 0.82.As expected, however, for the micro level, pseudomodels prove beneficial.
Bothglobal and local selection methods improve the number of inputs that receive signif-icant micro-level correlations with pyramid scores.
The improvement is close to 10%compared with using only one model summary.
Also note that, after the addition ofpseudomodels, the percentage of significant correlations is 93%, which is only 2% lesscompared with the results using four human models (95%).286Louis and Nenkova Automatic Content EvaluationTable 8Performance before and after the addition of pseudomodel summaries: TAC?09 data(53 systems).Correlations Pairwise accuracyMacro level Micro level Macro level Micro levelEvaluation type py resp py resp py resp py respRSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7Global 0.91 0.82 93.2 79.5 88.6 83.5 66.8 51.3Local 0.92 0.79 93.2 75.0 89.6 80.8 67.4 51.3RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0For responsiveness scores that are model-independent, however, little improve-ments are seen at both macro and micro levels.
The pairwise accuracy at micro levelfor responsiveness is 1% better after the addition of the pseudomodels.Comparing the two methods for selecting the best system that can serve as apseudomodel, the global selection of the system that performed best over the entireavailable data set appears to be more desirable.
It improves the correlations with pyra-mid scores while keeping the same correlations with responsiveness as with one model.Local selection provides the same performance as global selection for pyramid scores,although it decreases the micro-level evaluation quality for responsiveness.6.
Consensus-Based: Evaluation Using Only Collection of System SummariesFrom our experiments with pseudomodels, we see that the addition of system sum-maries to availablemodels proved beneficial and improved themicro-level performanceof ROUGE.
One question that arises is whether the collection of system summariestogether will be useful for evaluation without any human models at all.
Again, this ideais related to model-free evaluation.
When several systems are available, we investigateif their collective knowledge can help assess summary quality.Systems use varied methods to select content, and agreement among systems couldbe indicative of important information.
This intuition is similar to that behind the man-ual pyramid method: Facts mentioned only in one human summary are less importantcompared to content that is mentioned in multiple human models.
For the experimentsreported in this section, we rely entirely on the combined knowledge from systemsummaries as a gold standard.6.1 Related WorkThe closest work to this idea of combining system output can be found in the area ofinformation retrieval (IR).
Soboroff, Nicholas, and Cahan (2001) proposed a methodfor evaluating IR systems without relevance judgments.
In addition to requiring lesshuman input, the need for automatic evaluation in IR is also motivated by the fact thatfor systems such as those on the Internet, the documents keep changing and so it isdifficult to collect relevance judgments that are stable and meaningful for a long time.287Computational Linguistics Volume 39, Number 2Soboroff, Nicholas, and Cahan (2001) combine the top n results from all the systemsand then sample a certain number of documents from this pool.
Those documentsselected by many systems are more likely to be in the chosen sample and assumedto be most relevant.
The systems are then evaluated by considering this chosen set ofdocuments as the gold-standard relevant set.In our work, we do not attempt to pick out common content explicitly from thesummary pool.
If we were to follow the same approach as IR, we would be samplingsentences from the summary pool.
But in multi-document summarization, sentencesfrom different documents could contain similar content and we do not want to sampleone sentence and use it in the gold standard because then systems would be penalizedfor choosing other similar sentences.
In our work, therefore, we break down the sen-tences and represent the content as a probability distribution over words.
A summaryis evaluated by comparing its word distribution to that of the pool.
We expect that thedistribution would implicitly capture the common content.6.2 Evaluation Set-upFor each input, we collect all the summaries produced by automatic systems andcalculate the probabilities of words in the combined set.
In this way, we obtain a globalprobability distribution of words selected in system summaries.
In this distribution,the content selected by multiple systems will be more prominent, representing themore important information.
The word probabilities from each individual summaryare then calculated and compared to the overall distribution using JS divergence.
If weassume that system summaries are collectively indicative of important content, thengood summaries will tend to have properties that are similar to this global distribution,resulting in low divergence values.
We compute the correlations of these divergencevalues with human-assigned summary scores and Table 9 shows the results from thisevaluation.6.3 ResultsThe correlations are on par with those based on multiple human gold standards.
Atboth macro and micro levels, the correlations and pairwise accuracy are similar to thoseobtained by ROUGE comparison with four human models.
The macro-level correlationis 0.93 with pyramid scores, which is very high for a metric that uses no human inputat all.
Further, the micro-level correlations are also significant for 90% of the inputs.
Inour pseudomodel experiments, the gains after the addition of system summaries wereTable 9Performance of consensus evaluation approach on TAC?09 data (53 systems).
For input level(micro), the percentage of inputs with significant correlations is reported.Correlations Pairwise accuracyMacro level Micro level Macro level Micro levelEvaluation type py resp py resp py resp py respSysSumm 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0288Louis and Nenkova Automatic Content Evaluationmodest (only at micro level).
Here we see that a large collection of system summariesby themselves have the information required for evaluation.From this experiment, we find that consensus among system summaries is indica-tive of important content.
This result suggests that by combining the content selectedby multiple systems, one might be able to build a summary that is better than eachof them individually.
In fact, this idea of system consensus has been utilized in thedevelopment of MT systems for quite some time.
One approach in MT is rescoring then-best list from an individual system?s decoder, and picking the (consensus) translationthat is close on average to all translations.
Such rescoring is implemented using aminimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al2008).
The otherapproach is system combinationwhere the output frommultiple systems is combined toproduce a new translation.
Several techniques includingminimumBayes risk have beenapplied to perform system combination in machine translation.
Shared tasks on systemcombination have also been organized in recent years to encourage the developmentof such methods (Callison-Burch et al2010, 2011).
Such strategies could be a usefuldirection to explore for summarization as well.7.
DiscussionIn this article, we have discussed metrics for summary evaluation when human sum-maries are not present.
Our results have shown that these metrics in fact correlate highlywith human judgments.
But we also need to understand how robust these metrics areand be aware of their limitations.
In this section, therefore, we provide a brief discussionof the use of these metrics in different settings.7.1 Including Input?Summary Similarity or Consensus-Based Measures in aSummarization SystemFirstly, because input?summary similarity features are computed using the input, theycan be useful features to incorporate in a summarization system.
The combinationof systems to perform evaluation also provides a way to build a better system.
Theconcern would be how the usefulness of these metrics will change if systems werealso optimizing for them.
To optimize a metric such as JS divergence exactly would bedifficult because the JS divergence score cannot be factored or divided among individualsentences, a necessary condition if the problem should be solved using an Integer LinearProgram as in McDonald (2007) and Gillick and Favre (2009).
Therefore only greedymethods are possible.
In fact, KL divergence was greedily optimized in Haghighi andVanderwende (2009) to obtain a high performance summarizer.
Gaming the evaluationshould carry little concern, however, as thesemetrics are proposedwith a view to tuningsystems.The metrics we presented are developed for evaluation in a new setting wheremodel summaries are not available and to aid system development and tuning.
Further,notice from the micro-level evaluation that a single metric such as JS divergence doesnot predict content selection performance well for all inputs.
System developers shouldtherefore involve other specialized features as well.
Regression of similarity metrics is abetter predictor at the micro level but optimizing that would involve computation of allmetrics.
Another point to note here is that these similarity measures and the consensuspool are only indicative of summary content quality.
Other key aspects of summaryquality, however, involve sentence ordering, proper generation of referring expressions289Computational Linguistics Volume 39, Number 2and grammatical sentences, and maintaining non-redundancy.
Systems should there-fore be optimizing for a wide variety of factors and thus input?summary similarity andconsensus evaluation can be used in the final output to measure the content quality ofthe summary.
Any content evaluation should obviously be accompanied by linguisticquality evaluation in contrast to the current trend to only report content scores.The high performance of the JS divergence metric also has another implicationfor system development.
On average, the JS divergence measure is highly predictiveof summary quality.
It indicates that for a large number of inputs in the TAC datasets, good content can be predicted with high accuracy just based on the input?s termdistributions.
Such inputs should therefore be easy to summarize for systems.
Althoughdiscourse-based and other semantic approaches to summarization have been proposed,most of the systems in TAC rely on surface features such as word distributions.
Inthis situation, we may not be focusing on robust systems that can handle a variety ofinputs.
In the early years of DUC, the test set comprised a variety of inputs such asbiographies, collections of multiple events, opinions, and descriptions of single events.Later years switched to more single-event-type test sets.
The results from our analysispoint out that current inputs might be too simple for systems and that the range ofinputs in the TAC conference should be expanded to include some input types wheremore sophisticated methods become necessary.
Perhaps the input?summary similaritymetrics will be helpful in picking out those inputs that need deeper analysis.
In thefollowing section, we provide some further analysis into the cases where the input?summary similarity turns out less predictive.7.2 Input?Summary Similarity and Dependence on Input CharacteristicsJS divergence is useful for the average rating of systems on the test set and, in ourcase, we have 44 examples over which the scores are averaged.
At the micro level,certain inputs received poor evaluations from JS divergence.
Here we provide someinsights into the types of inputs where JS divergence worked and the cases whichproved difficult.Table 10 shows the titles of articles in input D0913, the input that received the bestevaluation from JSD (correlation of 0.86).
These articles were all published on the sameday and deal with the same event, a Supreme Court hearing of a case.
This input canbe said to be highly cohesive and to be discussing the same topic.
For such inputs,the term distribution in the input would reflect content importance since some wordshave higher probability than others because they are discussed repeatedly in the inputdocuments.
Such a term distribution when compared with summaries will give goodevaluation performance.
We can also see that the human summaries for this input (alsoshown in Table 10) seem to report the common issues observed in the input.
In thiscase, therefore, input?summary similarity scores can predict the pyramid scores thatwere assigned based on the model summaries.
We also show in the table the summarythat is chosen to be best according to JS divergence and the summary that had theworst score.
We find that the best summary indeed conveys some of the main issuesalso reported in the human summaries.
On the other hand, the low-scoring summarypresents a story line about one of the lawyers involved in the case, which is a peripheraltopic described in only one of the input documents.
In fact, the summary scored asworst by JS divergence has a pyramid score of 0, whereas the chosen best summary hasa pyramid score of 0.39.On the other hand, summaries for input D0940 obtained only 0.3 correlation us-ing JSD evaluation.
Both ROUGE and consensus evaluation (SysSumm) methods can290Louis and Nenkova Automatic Content EvaluationTable 10Titles of articles and two human summaries for input D0913-A.
The summaries chosen as bestand worst according to JS divergence are also listed.Articles in input D0913-APublication date TitleMar 02 US Supreme Court examines issue of displaying Ten CommandmentsMar 02 US Supreme Court examines Ten Commandments displaysMar 02 Supreme Court wrestles with Ten Commandments issueMar 02 Justices examine 10 Commandments caseMar 02 High Course argues in 10 Commandments caseMar 02 Supreme Course wrestles with Ten CommandmentsMar 02 High Court argues Ten Commandments casesMar 02 Texas seeks to keep ten commandments display on capitol groundsMar 02 An unlikely journey up the legal ladderModel summary HThe Supreme Court heard arguments in two cases on March 2nd about the conditions under whichthe government could display the Ten Commandments and whether such displays violated theFirst Amendment.The Texas case concerns a 40-year-old granite monument of the Ten Commandments, one of 17 monumentson the grounds of the state capital.The Kansas case concerns framed copies of the Ten Commandments which hang with non-religiousdocuments in two county courthouses.The displays have been modified twice in response to lower court rulings.Justice O?Connor is expected to cast the swing votes.Model summary COn March 2, 2005, the Supreme Court heard two cases concerning the display of the Ten Commandmentson government property.In Texas, a homeless former lawyer challenged the constitutionality of an inscribed monument displayedon state capitol grounds.In Kentucky, the ACLU claimed copies displayed in two courthouses, modified twice before in responseto court rulings, still violated the First Amendment because the original purpose was religious and themodifications were a sham.Supporters argued that the Ten Commandments are a recognized symbol of law, with both secular andreligious functions.The justices will issue an opinion by late June.Lowest JSD summary: System 26The Supreme Court Wednesday wrestled with whether the Constitution allows displays of theTen Commandments on government property.Lower courts have issued conflicting rulings in dozens of cases involving Ten Commandments displaysin recent years.The justices also heard a challenge to Ten Commandments exhibits in Kentucky courthouses.Abbott, making his first argument before the high court, contended that the Ten Commandmentsmonument, while containing a sacred religious text.Abbott, 47, a former state Supreme Court justice.It is hardly little ?
a constitutional challenge to displaying the Ten Commandments on the grounds of theTexas Capitol, and so on.Highest JSD summary: System 39He said his happiest moment was not when he heard the Supreme Court was taking his case but whenhis teenage daughter read the story and tracked him down by e-mail, breaking a long estrangement.If religious, it was unacceptable, Van Orden said.He ate on food stamps at the upscale Central Market, pitched his tent nightly and took it down eachmorning in a wooded location he did not specify, traveled on a free bus pass granted for a veteran?sdisability and read newspapers and magazines free at newsstands.evaluate the same summaries, however, with correlation of 0.84 (ROUGE) and 0.74(SysSumm).
The titles of the articles in that input and in the human summaries areprovided in Table 11.
This input?s topic is the opening of Disneyland in Hong Kongbut its articles cover varied aspects around the topic such as ticket sales, environmental291Computational Linguistics Volume 39, Number 2Table 11Titles of articles and two human summaries for input D0940-A.
The summaries chosen as bestand worst by JS divergence are also listed.Articles in input D0940-APublication date TitleJune 22 Opening of HK Disneyland to be divided into 3 phasesJune 26 Disney officials consulted feng shui experts for Hong Kong DisneylandJuly 01 Hong Kong Disneyland starts on-line tickets sellingJuly 01 Disneyland rehearsal days to start in AugustJuly 04 HK Disneyland ticket sale proceeds wellSept 04 High hopes for Hong Kong Disneyland?s economic impact, but critics say Disneymagic overratedSept 08 Hong Kong Park: Classic Disney with an Asian accentSept 08 Hong Kong Disneyland won?t cut its maximum capacity despite overcrowding fearsSept 08 All tickets for opening day of HK Disneyland sold outSept 10 Shark fins, stray dogs and smog - Hong Kong Disneyland has had a bumpy rideModel summary BHong Kong Disneyland (HKD) was scheduled to open in three phases in the summer of 2005.Early to mid-August transportation would be available to some areas of the park.August-Sept 11 all public services would become available.Sept 12 the grand opening of the park and hotels would take place.On Aug 16 HKD will begin its rehearsals to which special guests will be invited.In September, HKD announced it would not cut its daily capacity of 30,000 visitors.On Sept 9 it was revealed that all tickets for the grand opening had been sold.Model summary HHong Kong Disneyland, a joint venture between Disney and the Hong Kong government, was scheduledto open on 12 September.Rehearsal days were staged for a month before opening, giving ?cast members?
a chance to practice theirperformances.Disneyland refused to reduce its daily maximum capacity of 30,000 despite complaints from early visitorsabout large crowds and long lines.All 16,000 opening day tickets were sold out.The park is vintage Disney, with aspects of local culture including feng shui, Asian foods, and signsin Chinese.Protests forced them to remove shark fin soup from their menus.Lowest JSD summary: System 54But critics say Hong Kong Disneyland is overrated.The opening of Hong Kong Disneyland is expected to turn on a new page of Hong Kong tourism, withfocus on family tourists, she said.Hong Kong Disneyland will stage its rehearsal from Aug. 16 to the theme park?s opening on Sept. 12,the park said in a press release on Friday.Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.Hong Kong Disneyland Hotel starts at 1,600 (euro 170) a night and Disney?s Hollywood Hotel?s cheapestroom costs 1,000 (euro 106).Highest JSD summary: System 1Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.But not dog-lovers, shark-defenders and fireworks foes.The opposition may seem odd, in a Chinese city where fireworks are a fixture, shark fin soup is hugelypopular, and stray dogs are summarily dealt with as health hazards.But eight years after the British colony was returned to China, the capitalist city is much freer than theCommunist mainland, and advocacy groups are vocal.concerns, and use of feng shui.
The human summaries for this input focus on differentaspects.
The term distributions in such an input by themselves do not provide anindication of what was important in contrast to the more cohesive input we discussedpreviously.
The semantics of the content should be better understood to be able topredict the content that humans would choose in their summaries.
Subsequently, wecan also observe that the summary that is top ranked by JS divergence does not have292Louis and Nenkova Automatic Content Evaluationmuch of the same information as the model summaries and talks about yet anotherset of aspects such as hotel rates and family tourism.
The worst summary presents adifferent set of facts.
The pyramid scores for both these summaries are low (0.13 forthe best JS summary and 0.0 for the worst JS summary).
Input?summary similarity istherefore less helpful here and the information provided by model summaries wouldbe the best gold standard.Saggion et al(2010) report that trends can be observed in the JSD metric perfor-mance although it does not provide good evaluations for opinion and biographical typeinputs.
Automatic evaluations in different genres therefore have different requirementsand exploring these is an avenue for future work.
Input?summary similarity based onlyon word distribution works well for evaluating summaries of cohesive-type inputs.We can also envision a situation where we will be able to predict whether the JSdivergence evaluation will be accurate or not on a particular test set.
In prior work inNenkova and Louis (2008) and Louis and Nenkova (2009b), we have explored proper-ties of summarization inputs and provided a characterization of inputs into cohesiveand less cohesive based on automatic features.
The less cohesive inputs were found tobe the ones where automatic systems in general performed poorly.
In that work, weproposed features to predict if an input is cohesive or not.
We now apply these featuresto the TAC?09 data with the intention of automatically identifying inputs suitable forJS divergence evaluation (the cohesive ones).
The features were trained on data fromprevious years of TAC evaluations.
Among the top ten inputs for which JS divergencegave the best correlations, six of them were predicted as cohesive, and, similarly forthe bottom ten, six inputs were predicted as ?not cohesive.?
This result provides morevalidation of the relationship between input and evaluation quality but the automaticprediction of evaluation quality does not appear to be very accurate based on ourcurrent features.
We plan to explore this direction further in future work.7.3 Requirements for Consensus-Based EvaluationIn a similar vein, one would like to understand the performance guarantees fromthe consensus-based evaluation method (SysSumm).
Here, the metric depends on theavailability of a number of diverse system summaries.
In the TAC workshops, over50 systems compete and thus we have a large pool of system summaries with whichto compute consensus.
For other data sets, when we have to evaluate a few differentsystems, it is unclear if the same performance can be obtained.
To understand thedependence on the number of systems, we study how well the consensus evaluationmethod works when a small set of standard summarization methods is taken as theavailable system pool.
We expected that when the standard algorithms are chosen tobe diverse, their strengths can be combined usefully in a similar manner as the TACsystems.We choose a set of nine different summarization approaches.
They are briefly de-scribed here.Baseline: One of the commonly used baseline approaches for multi-document sum-marization.
The first sentence from each document in the input is first included in thesummary.
After including the first sentence from each document, the second sentenceis included and so on up to the length limit.Mead: Radev et al(2004a, 2004b) rank sentences using a combination of three aspects(sentence length, position in the article, and a centroid score which indicates how centralthe content of the sentence is) computed by comparison with all other sentences.293Computational Linguistics Volume 39, Number 2Average probability: This is a competitive summarizer (Nenkova, Vanderwende, andMcKeown 2006) using only the frequency of words as the indicator of content impor-tance.
We implement this method by first computing the unigram probability of allcontent words in the documents of the input combined together.
Then we score eachsentence by the average value of the probability for the content words in that sentence.Topic word: This is a strong, yet simple, method for generic summarization (i.e., theset of documents given as input must be summarized to reflect the sources as best aspossible; in contrast, TAC 2009 tasks can be considered as focused summarizationwhereeither a query is provided or an update is required).
This method first computes a set oftopic words from the input using a loglikelihood ratio.
The sentences are ranked usingthe score introduced by Conroy, Schlesinger, and O?Leary (2006): the ratio of the numberof unique topic words in the sentence to the unique content words in the sentence.Graph centrality: This approach (Erkan and Radev 2004; Mihalcea and Tarau 2005)performs selection over a graph representation of the input sentences.
Each sentenceis represented in vector space using unigram word counts.
Two sentences are linkedwhen their vectors have a cosine similarity of at least 0.1.
When this graph is convertedinto aMarkov chain, we can compute the stationary distribution of the transition matrixdefined by the graph?s edges.
This stationary distribution gives the probability of visit-ing each node during repeated random walks through the graph.
The high probabilitynodes are the ones that are most visited and these correspond to central sentences forsummarization.
The probability from the stationary distribution is the ranking score forthis method.Latent Semantic Analysis (LSA): The LSA technique (Deerwester et al1990) is basedon the idea of dimensionality reduction.
For summarization, first, the input is repre-sented as a matrix indexed by words (rows) and sentences (columns) and each cell indi-cates the count of the word in that sentence.
This matrix is converted by singular valuedecomposition and dimensionality reduction to obtain a matrix of sentences versusconcepts where concepts implicitly capture sets of co-occurring terms.
The number ofconcepts is much smaller than the size of the vocabulary.
The process also produces thesingular values that indicate the importance of concepts.
Sentences are selected for eachconcept in order of concept importance up to the summary length limit.
We obtainedsummaries using the approach detailed in Gong and Liu (2001).Greedy-KL: This method selects sentences by minimizing the KL divergence of thesummary?s word distribution to that of the input.
The idea is similar to findingsfrom our input?summary similarity evaluations.
Because the selection of sentences thatminimize divergence can only be done by examining all combinations of sentences,Haghighi and Vanderwende (2009) introduce a greedy approach that, at each step, addsthe sentence si to the existing summary E such that the combination E ?
si has the lowestKL among all options for si.CLASSY 04: This system (Conroy and O?Leary 2001) combines the occurrence of topicwords and position of the sentence to predict the score for a sentence.
In addition, itemploys a hidden Markov model?based approach, so that the probability of a sentencebeing a summary sentence is dependent on the importance of its adjacent sentences inthe document.
This system was introduced in the DUC evaluations in 2004 (Conroyet al2004).
We obtained these summaries (and the subsequent CLASSY 11) from theauthors.294Louis and Nenkova Automatic Content EvaluationCLASSY 11: This is a query-focused summarization system used by Conroy et al(2011)in TAC 2011.
It uses features related to topic words and other important keywordsidentified using a graph-based approach.
Rather than greedy selection of top sentences,CLASSY 11 solves an approximate knapsack problem to obtain a more globally optimalsummary.
Further, the scoring in this method uses bigrams as the basic unit/keyword incontrast to the other methods we have described previously that assume that a sentenceis composed of a bag of unigrams.We generated 100 word summaries from each described system.
Except for theCLASSY system, which performsmore sophisticated redundancy removal, for the othermethods we used the greedy Maximum Marginal Relevance technique (Carbonell andGoldstein 1998) for reducing redundancy.
After the sentence rankings were obtained,we added each sentence in order if it was not highly similar (a threshold value oncosine overlap is specified to indicate high similarity) to any of the already addedsentences.Each of the original TAC systems summaries were evaluated as follows.
We addedthe candidate summary to the pool of summaries from these other standard methods.Then we computed the JS divergence between the candidate summary and the com-bined pool to obtain the score for the candidate.
The procedure is the same as the onewe followed in Section 6 except that here we assumed that for each TAC system, weonly had these standard systems as peers rather than the full set of all TAC systems.The results from this evaluation are shown in Table 12 as SysSumm-std9.
The previousevaluation results, using all TAC systems as consensus, is reproduced in the table asSysSumm-full.We found that even with these few systems, the consensus evaluation is ratherstrong and produces correlations of 0.91 with pyramid and 0.77 with responsivenessscores.
These results provide additional support for the argument that high qualityevaluation is feasible even with standard systems as peers and that a small set of suchsystems appears to be sufficient for forming the consensus.Because the CLASSY systems are currently some of the top performing systemsat TAC, we also evaluated how useful the consensus is if the CLASSY summariesare left out.
So we evaluated the TAC systems using only the seven other standardsystems (i.e., all except CLASSY04 and CLASSY11) as the peers and the results fromthis evaluation are reported in Table 12 as SysSumm-std7.
We find that the correlationsTable 12Performance of consensus evaluation approach on TAC?09 data (53 systems).
For input level(micro), the percentage of inputs with significant correlations is reported.
The results usingTAC?09 systems as pseudomodels are indicated as SysSumm-full and those with off-the-shelfsystems as SysSumm-std.Correlations Pairwise accuracyMacro level Micro level Macro level Micro levelEvaluation type py resp py resp py resp py respSysSumm - full 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7SysSumm - std9 0.91 0.77 86.3 75.0 87.4 78.7 66.4 50.9SysSumm - std7 0.91 0.78 84.0 75.0 87.7 78.8 65.9 50.6RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0295Computational Linguistics Volume 39, Number 2remain the same even when the strongest systems are removed.
The usefulness of theconsensus therefore is not heavily dependent on the presence of best-quality systems inthe pool.7.4 Cross-Year Variation in Metric PerformanceThe performance of a metric should also be discussed under the effect of different testdata sets.
In our feature analysis for input?summary similarity, we found that JS diver-gence produces very high correlations on the TAC 2008 data, about 0.88 with pyramidscores.
The performance on the TAC 2009 data, however, although still high (0.74), islower than the previous year?s data.
Such a difference could be attributed to differentfactors.
One possible factor is the cohesiveness of the input, as we have discussedearlier.
In the TAC evaluations, there is no control over the input types, so inputs fromdifferent years may not have the same characteristics.
It could be, therefore, that TAC2009 had more inputs that were less cohesive as our second example above comparedto inputs that have more homogenity in the topic discussed.
A further evidence for thishypothesis can be seen from the cross-year performance of different metrics presentedin Table 13.Here, improved correlations from 2008 to 2009 are bolded and those that decreasedare italicized.
The correlations with pyramid scores increased for SysSumm and ROUGEevaluations but dropped for JS divergence.
This trend indicates that the model-basedevaluations (SysSumm has characteristics similar to model-based evaluation) havemore strength on the 2009 data.
For inputs where model information cannot be obtainedfrom the general term distribution in the inputs (as could have been the case in 2009),therefore, input?summary similarity that is model-free obtains worse performance.
Thismodel dependence can also explain why ROUGE and SysSummhave lower correlationswith responsiveness in 2009 despite being able to predict pyramid scores better.
BecauseROUGE scores are computed based on these specific models, its correlations with themodel-free responsiveness judgments drops in 2009.8.
Conclusion and Future WorkWe have presented successful metrics for summary evaluation that require very littleor no human input.
We have explored two scenarios: fewer model summaries and nomodel summaries at all.
For both these cases, our newly proposed evaluation metricshave provided good performance.We analyzed two methods for evaluation in the absence of gold-standardsummaries.
One was based on input?summary similarity.
We examined differentTable 13Cross-year system level correlations for different metrics.
The ROUGE-SU4 scores use all fourhuman summaries for reference.
Improved correlations in 2009 are bolded and decreases incorrelations are italicized.JSD SysSumm ROUGE-SU4year py resp py resp py resp2008 0.89 0.74 0.85 0.82 0.88 0.832009 0.74 0.70 0.93 0.81 0.92 0.79296Louis and Nenkova Automatic Content Evaluationpossibilities for measuring similarity and quantified their accuracy in predictinghuman-assigned scores.
Our results showed that the strength of features varies con-siderably.
The best metric is JS divergence, which compares the distribution of termsin the input and summary.
Combination of JS divergence with other metrics such ascosine similarity and topic word features also gave high correlations with human scores,around 0.77.Another method we have introduced is the addition of pseudomodel system sum-maries to themodel set when the number of models is low.
Our aim here was to improvethemicro-level evaluations and our results show that improvements along this linewereprovided by the pseudomodels.We also proposed a model-free metric that measures the similarity of a system sum-mary with the collection of all system summaries for that input.
This method actuallyprovided even better performance (0.93 correlations with pyramid scores), which iscompetitive with ROUGE scores computed using four human models.Furthermore, our evaluations provide consistent performance.
In Louis andNenkova (2009c), we report the correlations for adjacent years showing that our metricsproduce reliable performance for two consecutive years of TAC evaluation and for twotasks, query and update summarization.These evaluation methods highlight considerations that have received little atten-tion so far and give indications of how to perform evaluations on non-standard testsets with little human input.
The situation of having only one model summary is notuncommon and so are test sets where there are no model summaries at all.
Here, onecould use our proposed approaches in system development and then at a later stageuse manual evaluations on a small test set to confirm the results.
Further, our metricsalso provide valuable insights for system development.
From our results, it is evidentthat optimizing for input?summary similarity using an information-theoretic measuresuch as JS divergence and optimizing for topic signatures are indeed good approachesfor building a generic summarization system.
In addition, the results from consensusevaluation show that combining summaries from different systems has the potentialof creating a system better than the pool.
Currently, more than 50 systems compete inthe TAC summarization tasks and we want to explore system combination techniquesover their summaries in future work.We also plan to focus on the incorporation of both content and linguistic quality forevaluation.
As we already saw, the correlation between system rankings based on pyra-mid and responsiveness scores is only 0.85.
Furthermore, the correlations of ROUGE aswell as our metrics are lower with responsiveness compared with the pyramid.
Contentscores should therefore always be used together with assessments of linguistic quality,and combining both scores would be necessary for obtaining better correlations withresponsiveness.AcknowledgmentsWe would like to thank John Conroyfor providing us with the summariesfrom the CLASSY system and Xi Lin forthe implementation of the LSA-basedsummarizer.
We would also like tothank the reviewers for their comments;in particular, the expanded evaluationof the consenus-based metric was addedbased on their feedback.
The workhas been partly supported by an NSFCAREER award (09-53445).ReferencesAlbrecht, Joshua and Rebecca Hwa.2007.
Regression for sentence-level MTevaluation with pseudo references.In Proceedings of ACL, pages 296?303,Prague.297Computational Linguistics Volume 39, Number 2Albrecht, Joshua and Rebecca Hwa.2008.
The role of pseudo referencesin MT evaluation.
In Proceedings of theThird Workshop on Statistical MachineTranslation, ACL, pages 187?190,Columbus, OH.Best, D. J. and D. E. Roberts.
1975.
Algorithmas 89: The upper tail probabilities ofSpearman?s rho.
Journal of the RoyalStatistical Society.
Series C (AppliedStatistics), 24(3):377?379.Callison-Burch, Chris, Philipp Koehn,Christof Monz, Kay Peterson, MarkPrzybocki, and Omar Zaidan.
2010.Findings of the 2010 joint workshop onstatistical machine translation and metricsfor machine translation.
In Proceedingsof the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR,pages 17?53, Uppsala.Callison-Burch, Chris, Philipp Koehn,Christof Monz, and Omar Zaidan.2011.
Findings of the 2011 workshopon statistical machine translation.In Proceedings of the Sixth Workshopon Statistical Machine Translation,pages 22?64, Edinburgh.Carbonell, Jaime and Jade Goldstein.1998.
The use of MMR, diversity-basedreranking for reordering documents andproducing summaries.
In Proceedings ofSIGIR, pages 335?336, Melbourne.Conroy, John M., Jade Goldstein, Judith D.Schlesinger, and Dianne P. O?Leary.
2004.Left-brain/right-brain multi-documentsummarization.
In Proceedings of the4th Document Understanding Conference(DUC?04), Boston, MA.
Available at:http://duc.nist.gov/pubs/2004papers/ida.conroy.ps.Conroy, John M. and Dianne P. O?Leary.
2001.Text summarization via hidden Markovmodels.
In Proceedings of SIGIR,pages 406?407, New Orleans, LA.Conroy, John M., Judith D. Schlesinger,Jeff Kubina, Peter A. Rankel, and Dianne P.O?Leary.
2011.
Classy 2011 at TAC:Guided and multi-lingual summaries andevaluation metrics.
In Proceedings of TAC,Gaithersburg, MD.
Available at:http://www.nist.gov/tac/publications/2011/participant.papers/CLASSY.proceedings.pdf.Conroy, John M., Judith D. Schlesinger,and Dianne P. O?Leary.
2006.
Topic-focusedmulti-document summarization using anapproximate oracle score.
In Proceedingsof the COLING-ACL, pages 152?159,Sydney.Deerwester, Scott, Susan T. Dumais,George W. Furnas, Thomas K. Landauer,and Richard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of theAmerical Society for Information Science,41(6):391?407.Donaway, Robert L., Kevin W. Drummey,and Laura A. Mather.
2000.
A comparisonof rankings produced by summarizationevaluation measures.
In Proceedings of theNAACL-ANLP Workshop on AutomaticSummarization, pages 69?78, Seattle, WA.Erkan, Gu?nes?
and Dragomir R. Radev.
2004.Lexpagerank: Prestige in multi-documenttext summarization.
In Proceedings ofEMNLP, pages 365?371, Barcelona.Gillick, Dan and Benoit Favre.
2009.
Ascalable global model for summarization.In Proceedings of the Workshop on IntegerLinear Programming for Natural LanguageProcessing, pages 10?18, Boulder, CO.Gillick, Dan and Yang Liu.
2010.
Non-expertevaluation of summarization systems isrisky.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech andLanguage Data with Amazon?s MechanicalTurk, pages 148?151, Los Angeles, CA.Gong, Yihong and Xin Liu.
2001.
Generictext summarization using relevancemeasure and latent semantic analysis.In Proceedings of SIGIR, pages 19?25,New Orleans, LA.Haghighi, Aria and Lucy Vanderwende.2009.
Exploring content models formulti-document summarization.In Proceedings of HLT-NAACL,pages 362?370, Boulder, CO.Harman, Donna and Paul Over.
2004.The effects of human variation inDUC summarization evaluation.In Proceedings of the ACL-04 Workshop:Text Summarization Branches Out,pages 10?17, Barcelona.Jing, Hongyan, Regina Barzilay, KathleenMckeown, and Michael Elhadad.
1998.Summarization evaluation methods:Experiments and analysis.
In AAAISymposium on Intelligent Summarization,pages 60?68, Palo Alto, CA.Kumar, Shankar and William Byrne.2004.
Minimum Bayes-risk decodingfor statistical machine translation.In Proceedings of HLT-NAACL,pages 169?176, Boston, MA.Lin, Chin-Yew.
2004a.
Looking for a fewgood metrics: Automatic summarizationevaluation-how many samples areenough.
In Proceedings of the NTCIRWorkshop, volume 4, pages 1?10, Tokyo.298Louis and Nenkova Automatic Content EvaluationLin, Chin-Yew.
2004b.
ROUGE: A packagefor automatic evaluation of summaries.
InProceedings of the ACL Text SummarizationWorkshop, pages 74?81, Barcelona.Lin, Chin-Yew, Guihong Cao, JianfengGao, and Jian-Yun Nie.
2006.
Aninformation-theoretic approach toautomatic evaluation of summaries.In Proceedings of HLT-NAACL,pages 463?470, New York, NY.Lin, Chin-Yew and Eduard Hovy.
2000.
Theautomated acquisition of topic signaturesfor text summarization.
In Proceedings ofCOLING, pages 495?501, Saarbru?cken.Lin, Chin-Yew and Eduard Hovy.
2003.Automatic evaluation of summariesusing n-gram co-occurrence statistics.
InProceedings of HLT-NAACL, pages 71?78,Edmonton.Louis, Annie and Ani Nenkova.
2008.Automatic summary evaluation withouthuman models.
In Proceedings of TAC,Gaithersburg, MD.
Available at:http://www.nist.gov/tac/publications/2008/additional.papers/Penn.proceedings.pdf.Louis, Annie and Ani Nenkova.
2009a.Automatically evaluating content selectionin summarization without human models.In Proceedings of EMNLP, pages 306?314,Singapore.Louis, Annie and Ani Nenkova.
2009b.Performance confidence estimation forautomatic summarization.
In Proceedingsof EACL, pages 541?548, Athens.Louis, Annie and Ani Nenkova.
2009c.Predicting summary quality usinglimited human input.
In Proceedingsof TAC, Gaithersburg, MD.
Available at:http://www.nist.gov/tac/publications/2009/participant.papers/UPenn.proceedings.pdf.Madnani, Nitin, Necip Fazil Ayan,Philip Resnik, and Bonnie J. Dorr.
2007.Using paraphrases for parametertuning in statistical machine translation.In Proceedings of the Second Workshop onStatistical Machine Translation,pages 120?127, Prague.McDonald, Ryan.
2007.
A study of globalinference algorithms in multi-documentsummarization.
In Proceedings of ECIR,pages 557?564, Rome.McKeown, Kathy, Regina Barzilay,David Evans, Vasileios Hatzivassiloglou,Barry Schiffman, and Simone Teufel.2001.
Columbia multi-documentsummarization: Approach and evaluation.In Proceedings of DUC, New Orleans, LA.Available at: http://www-nlpir.nist.gov/projects/duc/pubs/2001papers/columbia redo.pdf.Mihalcea, Rada and Paul Tarau.
2005.Multi-document summarization withiterative graph-based algorithms.In Proceedings of the First InternationalConference on Intelligent Analysis Methodsand Tools (IA 2005), McLean, VA.Nenkova, Ani and Annie Louis.
2008.
Canyou summarize this?
Identifying correlatesof input difficulty for multi-documentsummarization.
In Proceedings of ACL-HLT,pages 825?833, Columbus, OH.Nenkova, Ani and Rebecca Passonneau.2004.
Evaluating content selection insummarization: The pyramid method.
InProceedings of HLT-NAACL, pages 145?152,Boston, MA.Nenkova, Ani, Rebecca Passonneau, andKathleen McKeown.
2007.
The pyramidmethod: Incorporating human contentselection variation in summarizationevaluation.
ACM Transactions on Speechand Language Processing, 4(2):4.Nenkova, Ani, Lucy Vanderwende, andKathleen McKeown.
2006.
A compositionalcontext sensitive multi-documentsummarizer: Exploring the factors thatinfluence summarization.
In Proceedingsof SIGIR, pages 573?580, Seattle, WA.Owkzarzak, Karolina and Hoa Trang Dang.2009.
Evaluation of automatic summaries:Metrics under varying data conditions.In Proceedings of the Workshop on LanguageGeneration and Summarisation,pages 23?30, Singapore.R Development Core Team.
2011.R: A Language and Environment forStatistical Computing.
R Foundationfor Statistical Computing, Vienna.Radev, Dragomir, Timothy Allison, SashaBlair-Goldensohn, John Blitzer, ArdaC?elebi, Stanko Dimitrov, Elliott Drabek,Ali Hakim, Wai Lam, Danyu Liu, JahnaOtterbacher, Hong Qi, Horacio Saggion,Simone Teufel, Michael Topper,AdamWinkel, and Zhu Zhang.
2004a.MEAD?A platform for multidocumentmultilingual text summarization.In Proceedings of LREC 2004, pages 1?4,Lisbon.Radev, Dragomir, Hongyan Jing,Malgorzata Sty, and Daniel Tam.
2004b.Centroid-based summarization ofmultiple documents.
InformationProcessing and Management, 40:919?938.Radev, Dragomir and Daniel Tam.
2003.Single-document and multi-document299Computational Linguistics Volume 39, Number 2summary evaluation via relative utility.In Proceedings of CIKM, pages 508?511,New Orleans, LA.Rath, G. J., A. Resnick, and R. Savage.1961.
The formation of abstracts bythe selection of sentences: Part 1:Sentence selection by man andmachines.
American Documentation,2(12):139?208.Saggion, Horacio, Juan-ManuelTorres Moreno, Iria da Cunha,Eric SanJuan, and Patricia Velazquez-Morales.
2010.
Multilingualsummarization evaluation withouthuman models.
In Proceedings ofCOLING, pages 1,059?1,067, Beijing.Soboroff, Ian, Charles Nicholas, andPatrick Cahan.
2001.
Ranking retrievalsystems without relevance judgments.In Proceedings of SIGIR, pages 66?73,New Orleans, LA.Tromble, Roy W., Shankar Kumar,Franz Och, and Wolfgang Macherey.2008.
Lattice minimum Bayes-riskdecoding for statistical machinetranslation.
In Proceedings of EMNLP,pages 620?629, Honolulu, HI.van Halteren, Hans and Simone Teufel.2003.
Examining the consensusbetween human summaries: Initialexperiments with factoid analysis.In Proceedings of the HLT-NAACL DUCon Text Summarization Workshop,pages 57?64, Edmonton.Zhang, Ying and Stephan Vogel.
2010.Significance tests of automatic machinetranslation evaluation metrics.MachineTranslation, 24(1):51?65.300
