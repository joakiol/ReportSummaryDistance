Proceedings of the TextGraphs-8 Workshop, pages 6?10,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsJoBimText Visualizer:A Graph-based Approach to Contextualizing Distributional SimilarityAlfio Gliozzo1 Chris Biemann2 Martin Riedl2Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com{biem,riedl}@cs.tu-darmstadt.deAbstractWe introduce an interactive visualization com-ponent for the JoBimText project.
JoBim-Text is an open source platform for large-scaledistributional semantics based on graph rep-resentations.
First we describe the underly-ing technology for computing a distributionalthesaurus on words using bipartite graphs ofwords and context features, and contextualiz-ing the list of semantically similar words to-wards a given sentential context using graph-based ranking.
Then we demonstrate the ca-pabilities of this contextualized text expan-sion technology in an interactive visualization.The visualization can be used as a semanticparser providing contextualized expansions ofwords in text as well as disambiguation toword senses induced by graph clustering, andis provided as an open source tool.1 IntroductionThe aim of the JoBimText1 project is to build agraph-based unsupervised framework for computa-tional semantics, addressing problems like lexicalambiguity and variability, word sense disambigua-tion and lexical substitutability, paraphrasing, frameinduction and parsing, and textual entailment.
Weconstruct a semantic analyzer able to self-adapt tonew domains and languages by unsupervised learn-ing of semantics from large corpora of raw text.
Atthe moment, this analyzer encompasses contextual-ized similarity, sense clustering, and a mapping ofsenses to existing knowledge bases.
While its pri-mary target application is functional domain adap-tation of Question Answering (QA) systems (Fer-1http://sf.net/projects/jobimtext/rucci et al 2013), output of the semantic analyzerhas been successfully utilized for word sense disam-biguation (Miller et al 2012) and lexical substitu-tion (Szarvas et al 2013).
Rather than presentingthe different algorithms and technical solutions cur-rently implemented by the JoBimText community indetail, in this paper we will focus on available func-tionalities and illustrate them using an interactive vi-sualization.2 Underlying TechnologiesWhile distributional semantics (de Saussure, 1959;Harris, 1951; Miller and Charles, 1991) and thecomputation of distributional thesauri (Lin, 1998)has been around for decades, its full potential has yetto be utilized in Natural Language Processing (NLP)tasks and applications.
Structural semantics claimsthat meaning can be fully defined by semantic oppo-sitions and relations between words.
In order to per-form a reliable knowledge acquisition process in thisframework, we gather statistical information aboutword co-occurrences with syntactic contexts fromvery large corpora.
To avoid the intrinsic quadraticcomplexity of the similarity computation, we havedeveloped an optimized process based on MapRe-duce (Dean and Ghemawat, 2004) that takes advan-tage of the sparsity of contexts, which allows scal-ing the process through parallelization.
The result ofthis computation is a graph connecting the most dis-criminative contexts to terms and explicitly linkingthe most similar terms.
This graph represents localmodels of semantic relations per term rather than amodel with fixed dimensions.
This representationdeparts from the vector space metaphor (Schu?tze,1993; Erk and Pado?, 2008; Baroni and Zamparelli,62010), commonly employed in other frameworks fordistributional semantics such as LSA (Deerwester etal., 1990) or LDA (Blei et al 2003).The main contribution of this paper is to de-scribe how we operationalize semantic similarity ina graph-based framework and explore this seman-tic graph using an interactive visualization.
We de-scribe a scalable and flexible computation of a dis-tributional thesaurus (DT), and the contextualizationof distributional similarity for specific occurrencesof language elements (i.e.
terms).
For related workson the computation of distributional similarity, seee.g.
(Lin, 1998; Lin and Dyer, 2010).2.1 Holing SystemTo keep the framework flexible and abstract with re-spect to the pre-processing that identifies structurein language material, we introduce the holing op-eration, cf.
(Biemann and Riedl, 2013).
It is ap-plied to observations over the structure of text, andsplits these observations into a pair of two parts,which we call the ?Jo?
and the ?Bim?2.
All JoBimpairs are maintained in the bipartite First-Order Jo-Bim graph TC(T,C,E) with T set of terms (Jos),C set of contexts (Bims), and e(t, c, f) ?
E edgesbetween t ?
T , c ?
C with frequency f .
Whilethese parts can be thought of as language elementsreferred to as terms, and their respective context fea-tures, splits over arbitrary structures are possible (in-cluding pairs of terms for Jos), which makes thisformulation more general than similar formulationsfound e.g.
in (Lin, 1998; Baroni and Lenci, 2010).These splits form the basis for the computation ofglobal similarities and for their contextualization.
AHoling System based on dependency parses is illus-trated in Figure 1: for each dependency relation, twoJoBim pairs are generated.2.2 Distributed Distributional ThesaurusComputationWe employ the Apache Hadoop MapReduce Fram-work3, and Apache Pig4, for parallelizing and dis-tributing the computation of the DT.
We describethis computation in terms of graph transformations.2arbitrary names to emphasize the generality, should bethought of as ?term?
and ?context?3http://hadoop.apache.org4http://pig.apache.org/Figure 1: Jos and Bims generated applying a dependencyparser (de Marneffe et al 2006) to the sentence I sufferedfrom a cold and took aspirin.
The @@ symbolizes thehole.Staring from the JoBim graph TC with counts asweights, we first apply a statistical test5 to com-pute the significance of each pair (t, c), then we onlykeep the p most significant pairs per t. This consti-tutes our first-order graph for Jos FOJO.
In analogy,when keeping the p most significant pairs per c, wecan produce the first-order graph for Bims FOBIM .The second order similarity graph for Jos is definedas SOJO(T,E) with Jos t1, t2 ?
T and undirectededges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?FOJO, e(t2, c) ?
FOJO}|, which defines similar-ity between Jos as the number of salient featurestwo Jos share.
SOJO defines a distributional the-saurus.
In analogy, SOBIM is defined over theshared Jos for pairs of Bims and defines similar-ity of contexts.
This method, which can be com-puted very efficiently in a few MapReduce steps, hasbeen found superior to other measures for very largedatasets in semantic relatedness evaluations in (Bie-mann and Riedl, 2013), but could be replaced by anyother measure without interfering with the remain-der of the system.2.3 Contextualization with CRFWhile the distributional thesaurus provides the sim-ilarity between pairs of terms, the fidelity of a par-ticular expansion depends on the context.
From theterm-context associations gathered in the construc-tion of the distributional thesaurus we effectivelyhave a language model, factorized according to theholing operation.
As with any language model,smoothing is critical to performance.
There may be5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,2004)7many JoBim (term-context) pairs that are valid andyet under represented in the corpus.
Yet, there maybe some similar term-context pair that is attested inthe corpus.
We can find similar contexts by expand-ing the term arguments with similar terms.
However,again we are confronted with the fact that the simi-larity of these terms depends on the context.This suggests some technique of joint inferenceto expand terms in context.
We use marginal in-ference in a conditional random field (CRF) (Laf-ferty et al 2001).
A particular world, x is definedas single, definite sequence of either original or ex-panded words.
The weight of the world, w(x) de-pends on the degree to which the term-context as-sociations present in this sentence are present in thecorpus and the general out-of-context similarity ofeach expanded term to the corresponding term in theoriginal sentence.
Therefore the probability associ-ated with any expansion t for any position xi is givenby Equation 1.
Where Z is the partition function, anormalization constant.P (xi = t) =1Z?
{x | xi=t}ew(x) (1)The balance between the plausibility of an ex-panded sentence according to the language model,and its per-term similarity to the original sentence isan application specific tuning parameter.2.4 Word Sense Induction, Disambiguationand Cluster LabelingThe contextualization described in the previous sub-section performs implicit word sense disambigua-tion (WSD) by ranking contextually better fittingsimilar terms higher.
To model this more explicitly,and to give rise to linking senses to taxonomies anddomain ontologies, we apply a word sense induction(WSI) technique and use information extracted byIS-A-patterns (Hearst, 1992) to label the clusters.Using the aggregated context features of the clus-ters, the word cluster senses are assigned in con-text.
The DT entry for each term j as given inSOJO(J,E) induces an open neighborhood graphNj(Vj , Ej) with Vj = {j?|e(j, j?, s) ?
E) and Ejthe projection of E regarding Vj , consisting of sim-ilar terms to j and their similarities, cf.
(Widdowsand Dorow, 2002).We cluster this graph using the Chinese Whispersgraph clustering algorithm (Biemann, 2010), whichfinds the number of clusters automatically, to ob-tain induced word senses.
Running shallow, part-of-speech-based IS-A patterns (Hearst, 1992) overthe text collection, we obtain a list of extracted IS-A relationships between terms, and their frequency.For each of the word clusters, consisting of similarterms for the same target term sense, we aggregatethe IS-A information by summing the frequency ofhypernyms, and multiplying this sum by the numberof words in the cluster that elicited this hypernym.This results in taxonomic information for labelingthe clusters, which provides an abstraction layer forterms in context6.
Table 1 shows an example of thislabeling from the model described below.
The mostsimilar 200 terms for ?jaguar?
have been clusteredinto the car sense and the cat sense and the high-est scoring 6 hypernyms provide a concise descrip-tion of these senses.
This information can be usedto automatically map these cluster senses to sensesin an taxonomy or ontology.
Occurrences of am-biguous words in context can be disambiguated tothese cluster senses comparing the actual contextwith salient contexts per sense, obtained by aggre-gating the Bims from the FOJO graph per cluster.sense IS-A labels similar termsjaguarN.0car, brand,company,automaker,manufacturer,vehiclegeely, lincoln-mercury,tesla, peugeot, ..., mit-subishi, cadillac, jag, benz,mclaren, skoda, infiniti,sable, thunderbirdjaguarN.1animal, species,wildlife, team,wild animal, catpanther, cougar, alligator,tiger, elephant, bull, hippo,dragon, leopard, shark,bear, otter, lynx, lionTable 1: Word sense induction and cluster labeling exam-ple for ?jaguar?.
The shortened cluster for the car sensehas 186 members.3 Interactive Visualization3.1 Open Domain ModelThe open domain model used in the current vi-sualization has been trained from newspaper cor-6Note that this mechanism also elicits hypernyms for unam-biguous terms receiving a single cluster by the WSI technique.8Figure 2: Visualization GUI with prior expansions for?cold?.
Jobims are visualized on the left, expansions onthe right side.pora using 120 million sentences (about 2 Giga-words), compiled from LCC (Richter et al 2006)and the Gigaword (Parker et al 2011) corpus.
Weconstructed a UIMA (Ferrucci and Lally, 2004)pipeline, which tokenizes, lemmatizes and parsesthe data using the Stanford dependency parser (deMarneffe et al 2006).
The last annotator in thepipeline annotates Jos and Bims using the collapseddependency relations, cf.
Fig.
1.
We define the lem-matized forms of the terminals including the part-of-speech as Jo and the lemmatized dependent wordand the dependency relation name as Bim.3.2 Interactive Visualization FeaturesEvaluating the impact of this technology in applica-tions is an ongoing effort.
However, in the contextof this paper, we will show a visualization of the ca-pabilities allowed by this flavor of distributional se-mantics.
The visualization is a GUI as depicted inFigure 2, and exemplifies a set of capabilities thatcan be accessed through an API.
It is straightfor-ward to include all shown data as features for seman-tic preprocessing.
The input is a sentence in naturallanguage, which is processed into JoBim pairs as de-scribed above.
All the Jos can be expanded, showingtheir paradigmatic relations with other words.We can perform this operation with and withouttaking the context into account (cf.
Sect.
2.3).
Thelatter performs an implicit disambiguation by rank-ing similar words higher if they fit the context.
Inthe example, the ?common cold?
sense clearly dom-inates in the prior expansions.
However, ?weather?and ?chill?
appear amongst the top-similar prior ex-pansions.We also have implemented a sense view, whichdisplays sense clusters for the selected word, seeFigure 3.
Per sense, a list of expansions is pro-vided together with a list of possible IS-A types.
Inthis example, the algorithm identified two senses of?cold?
as a temperature and a disease (not all clus-ter members shown).
Given the JoBim graph of thecontext (as displayed left in Fig.
2), the particularoccurrence of ?cold?
can be disambiguated to Clus-ter 0 in Fig.
3, since its Bims ?amod(@@,nasty)?and ?-dobj(catch, @@)?
are found in FOJO for farmore members of cluster 0 than for members of clus-ter 1.
Applications of this type of information in-clude knowledge-based word sense disambiguation(Miller et al 2012), type coercion (Kalyanpur et al2011) and answer justification in question answering(Chu-Carroll et al 2012).4 ConclusionIn this paper we discussed applications of the Jo-BimText platform and introduced a new interactivevisualization which showcases a graph-based unsu-pervised technology for semantic processing.
Theimplementation is operationalized in a way that itcan be efficiently trained ?off line?
using MapRe-duce, generating domain and language specific mod-els for distributional semantics.
In its ?on line?
use,those models are used to enhance parsing with con-textualized text expansions of terms.
This expansionstep is very efficient and runs on a standard laptop,so it can be used as a semantic text preprocessor.
Theentire project, including pre-computed data models,is available in open source under the ASL 2.0, andallows computing contextualized lexical expansionon arbitrary domains.ReferencesM.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Comp.
Ling., 36(4):673?721.M.
Baroni and R. Zamparelli.
2010.
Nouns are vectors,adjectives are matrices: representing adjective-nounconstructions in semantic space.
In Proc.
EMNLP-2010, pages 1183?1193, Cambridge, Massachusetts.C.
Biemann and M. Riedl.
2013.
Text: Now in 2D!
aframework for lexical expansion with contextual simi-larity.
Journal of Language Modelling, 1(1):55?95.C.
Biemann.
2010.
Co-occurrence cluster features forlexical substitutions in context.
In Proceedings ofTextGraphs-5, pages 55?59, Uppsala, Sweden.9Figure 3: Senses induced for the term ?cold?.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alcation.
J. Mach.
Learn.
Res., 3:993?1022,March.J.
Chu-Carroll, J.
Fan, B. K. Boguraev, D. Carmel,D.
Sheinwald, and C. Welty.
2012.
Finding needlesin the haystack: search and candidate generation.
IBMJ.
Res.
Dev., 56(3):300?311.M.-C. de Marneffe, B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
LREC-2006, Genova,Italy.Ferdinand de Saussure.
1916.
Cours de linguistiquege?ne?rale.
Payot, Paris, France.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied Data Processing on Large Clusters.
In Proc.
OSDI?04, San Francisco, CA.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Society forInformation Science, 41(6):391?407.T.
Dunning.
1993.
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics,19(1):61?74.K.
Erk and S. Pado?.
2008.
A structured vector spacemodel for word meaning in context.
In Proc.
EMNLP-2008, pages 897?906, Honolulu, Hawaii.S.
Evert.
2004.
The statistics of word cooccurrences:word pairs and collocations.
Ph.D. thesis, IMS, Uni-versita?t Stuttgart.D.
Ferrucci and A. Lally.
2004.
UIMA: An ArchitecturalApproach to Unstructured Information Processing inthe Corporate Research Environment.
In Nat.
Lang.Eng.
2004, pages 327?348.D.
Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.Mueller.
2013.
Watson: Beyond Jeopardy!
ArtificialIntelligence, 199-200:93?105.Z.
S. Harris.
1951.
Methods in Structural Linguistics.University of Chicago Press, Chicago.M.
A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
COLING-1992, pages 539?545, Nantes, France.A.
Kalyanpur, J.W.
Murdock, J.
Fan, and C. Welty.
2011.Leveraging community-built knowledge for type co-ercion in question answering.
In Proc.
ISWC 2011,pages 144?156.
Springer.J.
D. Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.ICML 2001, pages 282?289, San Francisco, CA, USA.J.
Lin and C. Dyer.
2010.
Data-Intensive Text Processingwith MapReduce.
Morgan & Claypool Publishers, SanRafael, CA.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
COLING-98, pages 768?774,Montre?al, Quebec, Canada.G.
A. Miller and W. G. Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.T.
Miller, C. Biemann, T. Zesch, and I. Gurevych.
2012.Using distributional similarity for lexical expansionin knowledge-based word sense disambiguation.
InProc.
COLING-2012, pages 1781?1796, Mumbai, In-dia.R.
Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.2011.
English Gigaword Fifth Edition.
LinguisticData Consortium, Philadelphia.M.
Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-mann.
2006.
Exploiting the leipzig corpora collection.In Proc.
IS-LTC 2006, Ljubljana, Slovenia.H.
Schu?tze.
1993.
Word space.
In Advances in Neu-ral Information Processing Systems 5, pages 895?902.Morgan Kaufmann.G.
Szarvas, C. Biemann, and I. Gurevych.
2013.
Super-vised all-words lexical substitution using delexicalizedfeatures.
In Proc.
NAACL-2013, Atlanta, GA, USA.D.
Widdows and B. Dorow.
2002.
A graph model forunsupervised lexical acquisition.
In Proc.
COLING-2002, pages 1?7, Taipei, Taiwan.10
