Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115?124,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsULISSE:an Unsupervised Algorithm for Detecting Reliable Dependency ParsesFelice Dell?Orletta, Giulia Venturi and Simonetta MontemagniIstituto di Linguistica Computazionale ?Antonio Zampolli?
(ILC?CNR)via G. Moruzzi, 1 ?
Pisa (Italy){felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.itAbstractIn this paper we present ULISSE, an unsu-pervised linguistically?driven algorithm to se-lect reliable parses from the output of a de-pendency parser.
Different experiments weredevised to show that the algorithm is robustenough to deal with the output of differentparsers and with different languages, as wellas to be used across different domains.
Inall cases, ULISSE appears to outperform thebaseline algorithms.1 IntroductionWhile the accuracy of state?of?the?art parsers is in-creasing more and more, this is still not enough fortheir output to be used in practical NLP?based ap-plications.
In fact, when applied to real?world texts(e.g.
the web or domain?specific corpora such asbio?medical literature, legal texts, etc.)
their accu-racy decreases significantly.
This is a real problemsince it is broadly acknowledged that applicationssuch as Information Extraction, Question Answer-ing, Machine Translation, and so on can benefit sig-nificantly from exploiting the output of a syntacticparser.
To overcome this problem, over the last fewyears a growing interest has been shown in assessingthe reliability of automatically produced parses: theselection of high quality parses represents nowadaysa key and challenging issue.
The number of stud-ies devoted to detecting reliable parses from the out-put of a syntactic parser is spreading.
They mainlydiffer with respect to the kind of selection algo-rithm they exploit.
Depending on whether trainingdata, machine learning classifiers or external parsersare exploited, existing algorithms can be classifiedinto i) supervised?based, ii) ensemble?based and iii)unsupervised?based methods.The first is the case of the construction of a ma-chine learning classifier to predict the reliability ofparses on the basis of different feature types.
Yateset al (2006) exploited semantic features derivedfrom the web to create a statistical model to de-tect unreliable parses produced by a constituencyparser.
Kawahara and Uchimoto (2008) relied onfeatures derived from the output of a supervised de-pendency parser (e.g.
dependency lengths, num-ber of unknown words, number of coordinated con-junctions, etc.
), whereas Ravi et al (2008) exploitedan external constituency parser to extract text?basedfeatures (e.g.
sentence length, unknown words, etc.
)as well as syntactic features to develop a super-vised predictor of the target parser accuracy.
Theapproaches proposed by Reichart and Rappoport(2007a) and Sagae and Tsujii (2007) can be classi-fied as ensemble?based methods.
Both select highquality parses by computing the level of agreementamong different parser outputs: wheras the formeruses several versions of a constituency parser, eachtrained on a different sample from the training data,the latter uses the parses produced by different de-pendency parsing algorithms trained on the samedata.
However, a widely acknowledged problem ofboth supervised?based and ensemble?based meth-ods is that they are dramatically influenced by a) theselection of the training data and b) the accuracy andthe typology of errors of the used parser.To our knowledge, Reichart and Rappoport(2009a) are the first to address the task of high qual-115ity parse selection by resorting to an unsupervised?based method.
The underlying idea is that syntacticstructures that are frequently created by a parser aremore likely to be correct than structures producedless frequently.
For this purpose, their PUPA (POS?based Unsupervised Parse Assessment Algorithm)uses statistics about POS tag sequences of parsedsentences produced by an unsupervised constituencyparser.In this paper, we address this unsupervised sce-nario with two main novelties: unlike Reichart andRappoport (2009a), a) we address the reliable parsesselection task using an unsupervised method in asupervised parsing scenario, and b) we operate ondependency?based representations.
Similarly to Re-ichart and Rappoport (2009a) we exploit text inter-nal statistics: but whereas they rely on features thatare closely related to constituency representations,we use linguistic features which are dependency?motivated.
The proposed algorithm has been eval-uated for selecting reliable parses from English andItalian corpora; to our knowledge, this is the firsttime that such a task has been applied to a less re-sourced language such as Italian.
The paper is or-ganised as follows: in Section 2 we illustrate theULISSE algorithm; sections 3 and 4 are devoted tothe used parsers and baselines.
Section 5 describesthe experiments and discusses achieved results.2 The ULISSE AlgorithmThe ULISSE (Unsupervised LInguiStically?drivenSelection of dEpendency parses) algorithm takes asinput a set of parsed sentences and it assigns to eachdependency tree a score quantifying its reliability.
Itoperates in two different steps: 1) it collects statis-tics about a set of linguistically?motivated featuresextracted from a corpus of parsed sentences; 2) itcalculates a quality (or reliability) score for each an-alyzed sentence using the feature statistics extractedfrom the whole corpus.2.1 Selection of featuresThe features exploited by ULISSE are all linguis-tically motivated and rely on the dependency treestructure.
Different criteria guided their selection.First, as pointed out in Roark et al (2007), weneeded features which could be reliably identifiedwithin the automatic output of a parser.
Second,we focused on dependency structures that are widelyagreed in the literature a) to reflect sentences?
syn-tactic and thus parsing complexity and b) to imposea high cognitive load on the parsing of a completesentence.Here follows the list of features used in the exper-iments reported in this paper, which turned out to bethe most effective ones for the task at hand.Parse tree depth: this feature is a reliable indicatorof sentence complexity due to the fact that, with sen-tences of approximately the same length, parse treedepth can be indicative of increased sentence com-plexity (Yngve, 1960; Frazier, 1985; Gibson, 1998;Nenkova, 2010).Depth of embedded complement ?chains?
: thisfeature is a subtype of the previous one, focusing onthe depth of chains of embedded complements, ei-ther prepositional complements or nominal and ad-jectival modifiers.
Long chains of embedded com-plements make the syntactic structure more complexand their analysis much more difficult.Arity of verbal predicates: this feature refers to thenumber of dependency links sharing the same ver-bal head.
Here, there is no obvious relation betweenthe number of dependents and sentence complexity:both a small number and a high number of depen-dents can make the sentence processing quite com-plex, although for different reasons (elliptical con-structions in the former case, a high number of mod-ifiers in the latter).Verbal roots: this feature counts the number of ver-bal roots with respect to number of all sentence rootsin the target corpus.Subordinate vs main clauses: subordination is gen-erally considered to be an index of structural com-plexity in language.
Two distinct features are con-sidered for monitoring this aspect: one measuringthe ratio between main and subordinate clauses andthe other one focusing on the relative ordering ofsubordinate clauses with respect to the main clause.It is a widely acknowledged fact that highly com-plex sentences contain deeply embedded subordi-nate clauses; however, subordinate clauses are easierto process if they occur in post?verbal rather than inpre?verbal position (Miller, 1998).Length of dependency links: McDonald and Nivre(2007) report that statistical parsers have a drop in116accuracy when analysing long distance dependen-cies.
This is in line with Lin (1996) and Gibson(1998) who claim that the syntactic complexity ofsentences can be predicted with measures based onthe length of dependency links, given the memoryoverhead of very long distance dependencies.
Here,the dependency length is measured in terms of thewords occurring between the syntactic head and thedependent.Dependency link plausibility (henceforth, Arc-POSFeat): this feature is used to calculate the plausi-bility of a dependency link given the part?of?speechof the dependent and the head, by also consideringthe PoS of the head father and the dependency link-ing the two.2.2 Computation ScoreThe quality score (henceforth, QS) of parsed sen-tences results from a combination of the weights as-sociated with the monitored features.
ULISSE ismodular and can use several weights combinationstrategies, which may be customised with respect tothe specific task exploiting the output of ULISSE.For this study, QS is computed as a simple prod-uct of the individual feature weights.
This followsfrom the necessity to recognize high quality parseswithin the input set of parsed sentences: the prod-uct combination strategy is able to discard low qual-ity parse trees even in presence of just one lowweight feature.
Therefore, QS for each sentence iin the set of input parsed sentences I is QS(Si) =?ny=1 Weight(Si, fy), where Si is the i?th sentenceof I , n is the total number of selected features andWeight(Si, fy) is the computed weight for the y?thfeature.Selected features can be divided into two classes,depending on whether they are computed with re-spect to each sentence and averaged over all sen-tences in the target corpus (global features), or theyare computed with respect to individual dependencylinks and averaged over all of them (local features).The latter is the case of the ArcPOSFeat feature,whereas the all other ones represent global features.For the global features, the Weight(Si, fy) is de-fined as:Weight(Si, fy) = F (V (fy), range(L(Si), r))|range(L(Si), r)| ,(1)where V (fy) is the value of the y?th feature (ex-tracted from Si), L(Si) is the length of the sen-tence Si, range(L(Si), r) defines a range cov-ering values from L(Si) ?
r and L(Si) + r,F (V (fy), range(L(Si), r)) is the frequency ofV (fy) in all sentences in I that has a value oflength in range(L(Si), r1) and |range(L(Si), r)|is the total number of sentences in I with lengthin range(L(Si), r).
For what concerns the lo-cal feature ArcPOSFeat, ULISSE assigns a weightfor each arc in Si: in principle different strate-gies can be used to compute a unique weight forthis feature for Si.
Here, the sentence weightfor the feature ArcPOSFeat is computed as theminimum weight among the weights of all arcsof Si.
Therefore, Weight(Si, ArcPOSFeat) =min{weight((Pd, Ph, t)), ?
(Pd, Ph, t) ?
Si},where the triple (Pd, Ph, t) is an arc in Si in whichPd is the POS of the dependent, Ph is the POSof the syntactic head and t is the type of the de-pendency relation and weight((Pd, Ph, t)) is theweight of the specific arc (Pd, Ph, t).
The individ-ual arc weight is computed as follows:weight((Pd, Ph, t)) = F ((Pd, Ph, t))F ((Pd,X, t)) ??
F ((Pd, Ph, t))F ((X,Ph, t)) ??
F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((Pd, Ph, t)) ??
F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((Ph, Ph2, t2)) ??
F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((((Pd,X, t))(X,Ph2, t2))) ,where F (x) is the frequency of x in I , X is a vari-able and (arc1 arc2) represent two consecutive arcsin the tree.3 The ParsersULISSE was tested against the output of two reallydifferent data?driven parsers: the first?order Max-imum Spanning Tree (MST) parser (McDonald etal., 2006) and the DeSR parser (Attardi, 2006) usingSupport Vector Machine as learning algorithm.
The1We set r=0 in the in?domain experiments and r=2 in theout?of?domain experiment reported in Sec 5.3.117former is a graph?based parser (following the so?called ?all?pairs?
approach Buchholz et al (2006))where every possible arc is considered in the con-struction of the optimal parse tree and where depen-dency parsing is represented as the search for a max-imum spanning tree in a directed graph.
The latteris a Shift?Reduce parser (following a ?stepwise?
ap-proach, Buchholz et al (2006)), where the parseris trained and learns the sequence of parsing actionsrequired to build the parse tree.Although both parser models show a similar accu-racy, McDonald and Nivre (2007) demonstrate thatthe two types of models exhibit different behaviors.Their analysis exemplifies how different the twoparsers behave when their accuracies are comparedwith regard to some linguistic features of the ana-lyzed sentences.
To mention only a few, the Shift?Reduce parser tends to perform better on shortersentences, while the MST parser guarantees a higheraccuracy in identifying long distance dependencies.As regards the identification of dependency types,the MST parser shows a better ability to identifythe dependents of the sentences?
roots whereas theShift?Reduce tends to better recognize specific rela-tions (e.g.
Subject and Object).McDonald and Nivre (2007) describe how thesystems?
behavioral differences are due to the dif-ferent parsing algorithms implemented by the Shift?Reduce and the MST parsing models.
The ShiftReduce parser constructs a dependency tree by per-forming a sequence of parser actions or transitionsthrough a greedy parsing strategy.
As a result ofthis parsing procedure, a Shift Reduce parser cre-ates shorter arcs before longer arcs.
The latter couldbe the reason for the lower accuracy in identifyinglonger arcs when compared to the MST parser.
Thisalso influences a lower level of accuracy in the anal-ysis of longer sentences that usually contain longerarcs than shorter sentences.
The MST parser?s abil-ity to analyze both short and long arcs is invariantas it employs a graph-based parsing method whereevery possible arc is considered in the constructionof the dependency tree.4 The BaselinesThree different increasingly complex baseline mod-els were used to evaluate the performance ofULISSE.The first baseline is constituted by a Random Se-lection (RS ) of sentences from the test sets.
Thisbaseline is calculated in terms of the scores of theparser systems on the test set.The second baseline is represented by the Sen-tence Length (SL ), starting from the assumption,demonstrated by McDonald and Nivre (2007), thatlong sentences are harder to analyse using statisticaldependency parsers than short ones.
This is a strongunsupervised baseline based on raw text features,ranking the parser results from the shortest sentenceto the longest one.The third and most advanced baseline, exploitingparse features, is the PUPA algorithm (Reichart andRappoport, 2007a).
PUPA uses a set of parsed sen-tences to compute the statistics on which its scoresare based.
The PUPA algorithm operates on a con-stituency based representation and collects statisticsabout the POS tags of the words in the yield of theconstituent and of the words in the yields of neigh-boring constituents.
The sequences of POS tags thatare more frequent in target corpus receive higherscores after proper regularization is applied to pre-vent potential biases.
Therefore, the final score as-signed to a constituency tree results from a combina-tion of the scores of its extracted sequences of POSs.In order to use PUPA as a baseline, we imple-mented a dependency?based version, hencefoth re-ferred to as dPUPA.
dPUPA uses the same scorecomputation of PUPA and collects statistics aboutsequences of POS tags: the difference lies in the factthat in this case the POS sequences are not extractedfrom constituency trees but rather from dependencytrees.
To be more concrete, rather than represent-ing a sentence as a collection of constituency?basedsequences of POSs, dPUPA represents each sen-tence as a collection of sequences of POSs cov-ering all identified dependency subtrees.
In par-ticular, each dependency tree is represented as theset of all subtrees rooted by non?terminal nodes.Each subtree is then represented as the sequenceof POS tags of the words in the subtree (reflect-ing the word order of the original sentence) inte-grated with the POS of the leftmost and rightmostin the sentence (NULL when there are no neigh-bors).
Figure 1 shows the example of the depen-dency tree for the sentence I will give you the ball.118Figure 1: Example of dependency tree.If we consider the subtree rooted by give (in thedotted circle), the resulting POS sequence is asfollows: POS2 POS3 POS4 POS5 POS6 NULL,where POS3 POS4 POS5 POS6 is the sequence ofPOS tags in the subtree, POS2 is the left neighborPOS tag and NULL marks the absence of a rightneighbor.5 Experiments and ResultsThe experiments were organised as follows: a targetcorpus was automatically POS tagged (Dell?Orletta,2009) and dependency?parsed; the ULISSE andbaseline algorithms of reliable parse selection wererun on the POS?tagged and dependency?parsed tar-get corpus in order to identify high quality parses;results achieved by the selection algorithms wereevaluated with respect to a subset of the target cor-pus of about 5,000 word?tokens (henceforth referredto as ?test set?)
for which gold-standard annotationwas available.
Different sets of experiments weredevised to test the robustness of our algorithm.
Theywere performed with respect to i) the output of theparsers described in Section 3, ii) two different lan-guages, iii) different domains.For what concerns the languages, we chose Italianand English for two main reasons.
First of all, theypose different challenges to a parser since they arecharacterised by quite different syntactic features.For instance, Italian, as opposed to English, is char-acterised by a relatively free word order (especiallyfor what concerns subject and object relations withrespect to the verb) and by the possible absence ofan overt subject.
Secondly, as it is shown in Section5.1, Italian is a less resourced language with respectto English.
This is a key issue, since as demonstratedby Reichart and Rappoport (2007b) and McCloskyet al (2008), small and big treebanks pose differentproblems in the reliable parses selection.Last but not least, we aimed at demonstrating thatULISSE can be successfully used not only with textsbelonging to the same domain as the parser train-ing corpus.
For this purpose, ULISSE was testedon a target corpus of Italian legislative texts, whoseautomatic linguistic analysis poses domain?specificchallenges (Venturi, 2010).
Out?of?domain experi-ments are being carried out also for English.5.1 The CorporaThe Italian corpora Both parsers were trained onISST?TANL2, a dependency annotated corpus usedin Evalita?093, an evaluation campaign carried outfor Italian (Bosco et al, 2009).
ISST?TANL in-cludes 3,109 sentences (71,285 tokens) and consistsof articles from newspapers and periodicals.Two different target corpora were used for thein?domain and out?of?domain experiments.
Forthe former, we used a corpus of 1,104,237 sen-tences (22,830,739 word?tokens) of newspaperstexts which was extracted from the CLIC-ILC Cor-pus (Marinelli et al, 2003); for the legal domain,we used a collection of Italian legal texts (2,697,262word?tokens; 97,564 sentences) regulating a vari-ety of domains, ranging from environment, humanrights, disability rights, freedom of expression to pri-vacy, age disclaimer, etc.
In the two experiments,the test sets were represented respectively by: a) thetest set used in the Evalita?09 evaluation campaign,constituted by 260 sentences and 5,011 tokens fromnewpapers text; b) a set of 102 sentences (corre-sponding to 5,691 tokens) from legal texts.The English corpora For the training of parserswe used the dependency?based version of Sections2?11 of the Wall Street Journal partition of thePenn Treebank (Marcus et al, 2003), which was de-veloped for the CoNLL 2007 Shared Task on De-pendency Parsing (Nivre et al, 2007): it includes447,000 word tokens and about 18,600 sentences.As target data we took a corpus of news, specif-ically the whole Wall Street Journal Section of the2http://medialab.di.unipi.it/wiki/SemaWiki3http://evalita.fbk.eu/index.html119Penn Treebank4, from which the portion of text cor-responding to the training corpus was removed; theEnglish target corpus thus includes 39,285,425 to-kens (1,625,606 sentences).
For testing we usedthe test set of the CoNLL 2007 Shared Task, cor-responding to a subset of Section 23 of the WallStreet Journal partition of the Penn Treebank (5,003tokens, 214 sentences).5.2 Evaluation MethodologyPerformances of the ULISSE algorithm have beenevaluated i) with respect to the accuracy of rankedparses and ii) in terms of Precision and Recall.
First,for each experiment we evaluated how the ULISSEalgorithm and the baselines classify the sentences inthe test set with respect to the ?Labelled AttachmentScore?
(LAS) obtained by the parsers, i.e.
the per-centage of tokens for which it has predicted the cor-rect head and dependency relation.
In particular, wecomputed the LAS score of increasingly wider toplists of k tokens, where k ranges from 500 word to-kens to the whole size of the test set (with a step sizeof 500 word tokens, i.e.
k=500, k=1000, k=1500,etc.
).As regards ii), we focused on the set of rankedsentences showing a LAS ?
?.
Since imposinga 100% LAS was too restrictive, for each experi-ment we defined a different ?
threshold taking intoaccount the performance of each parser across thedifferent languages and domains.
In particular, wetook the top 25% and 50% of the list of ranked sen-tences and calculated Precision and Recall for eachof them.
To this specific end, a parse tree showinga LAS ?
?
is considered as a trustworthy analysis.Precision has been computed as the ratio of the num-ber of trustworthy analyses over the total number ofsentences in each top list.
Recall has been computedas the ratio of the number of trustworthy analyseswhich have been retrieved over the total number oftrustworthy analyses in the whole test set.In order to test how the ULISSE algorithm is ableto select reliable parses by relying on parse fea-tures rather than on raw text features, we computedthe accuracy score (LAS) of a subset of the top listof sentences parsed by both parsers and ranked by4This corpus represents to the unlabelled data set distributedfor the CoNLL 2007 Shared Task on Dependency Parsing, do-main adaptation track.ULISSE: in particular, we focused on those sen-tences which were not shared by the MST and DeSRtop lists.5.3 ResultsWe will refer to the performed experiments as fol-lows: ?IT in?domain?
and ?IT out?of?domain?
forthe Italian experiments using respectively the ISST?TANL test set (henceforth ISST TS) and the Legal-Corpus test set (henceforth Legal TS); ?EN in?domain?
for the English experiment using the PTBtest set (PTB TS).As a starting point let us consider the accuracyof DeSR and MST parsers on the whole test sets,reported in Table 1.
The accuracy has been com-puted in terms of LAS and of Unlabelled Attach-ment Score (UAS), i.e.
the percentage of tokens witha correctly identified syntactic head.
It can be no-ticed that the performance of the two parsers is quitesimilar for Italian (i.e.
wrt ISST TS and Legal TS),whereas there is a 2.3% difference between the MSTand DeSR accuracy as far as English is concerned.ISST TS Legal TS PTB TSParser LAS UAS LAS UAS LAS UASDeSR 80.22 84.96 73.40 76.12 85.95 87.25MST 79.52 85.43 73.99 78.72 88.25 89.55Table 1: Overall accuracy of DeSR and MST parsers.The plots in Figure 2 show the LAS of parsesranked by ULISSE and the baselines across the dif-ferent experiments.
Each plot reports the results of asingle experiment: plots in the same row report theLAS of DeSR and MST parsers with respect to thesame test set.
In all experiments, ULISSE turned outto be the best ranking algorithm since it appears toselect top lists characterised by higher LAS scoresthan the baselines.
As Figure 2 shows, all rankingalgorithms perform better than Random Selection(RS), i.e.
all top lists (for each k value) show a LAShigher than the accuracy of DeSR and MST parserson the whole test sets.
In the EN in?domain experi-ment, the difference between the results of ULISSEand the other ranking algorithms is smaller than inthe corresponding Italian experiment, a fact result-ing from the higher accuracy of DeSR and MSTparsers (i.e.
LAS 85.95% and 88.25% respectively)on the PTB TS.
It follows that, for example, thefirst top list (with k=500) of the SL baseline has a1201000 2000 3000 4000 5000808284868890ULISSELSdPUPARS(a) IT in?domain experiment (DeSR).1000 2000 3000 4000 50007980818283848586ULISSELSdPUPARS(b) IT in?domain experiment (MST).1000 2000 3000 4000 50007476788082ULISSELSdPUPARS(c) IT out?of?domain experiment (DeSR).1000 2000 3000 4000 500066687072747678808284ULISSELSdPUPARS(d) IT out?of?domain experiment (MST).1000 2000 3000 4000 50008688909294ULISSELSdPUPARS(e) EN in?domain experiment (DeSR).1000 2000 3000 4000 500088899091929394ULISSELSdPUPARS(f) EN in?domain experiment (MST).Figure 2: LAS of parses ranked by ULISSE algorithm and by the three baselines.LAS accuracy of 93.36% and 93.96% respectivelyfor DeSR and MST: even in this case, ULISSE out-performs all baselines.
This is also the case in theIT out?of?domain experiment.
As reported in Table1, parsing legal texts is a quite challenging task dueto a number of domain?specific peculiarities at thelevel of syntax: this is testified by the average sen-tence length which in the Legal TS is 56 word to-kens.
Nevertheless, ULISSE is able also in this caseto highly rank long sentences showing a high LAS.For example, while in the first top list of 500 wordtokens the sentences parsed by DeSR and ordered bySL have an average sentence length of 24 words anda LAS of 79.37%, ULISSE includes in the same toplist longer sentences (with average sentence length =29) with a higher LAS (82.72%).
Also dPUPA ranksin the same top list quite long sentences (with 27 av-erage sentence length), but compared to ULISSE itshows a lower LAS (i.e.
73.56%).IT in?domain IT out?of?domain EN in?domainDeSR MST DeSR MST DeSR MSTMST top?list 80.93 80.27 68.84 74.58 83.37 90.39DeSR top?list 82.46 77.82 75.47 74.88 86.50 86.74Table 3: LAS of not?shared sentences in the DeSR andMST top?lists.Results in Table 2 show that in the top 25% ofthe ranked sentences with a LAS ?
?
ULISSE hasthe highest Precision and Recall in all experiments.We believe that the low performance of dPUPA withrespect to all other ranking algorithms can be due to121DeSR MST25% 50% 25% 50%Prec Rec LAS AvgSL Prec Rec LAS AvgSL Prec Rec LAS AvgSL Prec Rec LAS AvgSLIT in?domain: LAS ?
85% (DeSR: 120 sentences; MST: 112 sentences)ULISSE 66.15 35.83 88.25 5.25 59.23 64.17 84.30 14.60 60 34.82 86.16 5.68 55.38 64.29 83.39 15.27LS 63.08 34.17 84.54 4.15 53.08 57.50 82.07 11.90 58.46 33.93 82.73 4.45 53.08 61.61 82.14 12.75dPUPA 61.54 33.33 86.89 6.68 59.23 64.17 84.36 14.82 53.85 31.25 82.26 8.61 50.00 58.04 79.94 17.04IT out?of?domain: LAS ?
75% (DeSR: 51 sentences; MST: 57 sentences)ULISSE 73.08 37.25 80.75 16.71 69.23 70.59 79.17 41.80 69.23 31.58 81.47 13.63 67.31 61.40 78.36 36LS 53.85 27.45 76.71 12.63 67.31 68.63 78.34 34.14 61.54 28.07 78.42 11.30 69.23 63.16 79.78 30.54dPUPA 57.69 29.41 73.97 15.67 61.54 62.74 75.24 40.39 46.15 21.05 72.08 22.56 57.69 52.63 74.86 42.91EN in?domain: LAS ?
90% (DeSR: 118 sentences; MST: 120 sentences)ULISSE 81.48 37.29 94.50 6.31 69.44 63.56 90.93 16.36 77.78 35 93.74 5.82 69.44 62.5 91.20 16.48LS 77.78 35.59 93.39 4.87 65.74 60.17 91.01 13.67 75.92 34.17 93.55 4.79 68.52 61.67 90.84 13.44dPUPA 74.07 33.90 89.76 7.95 65.74 60.17 88.37 18.14 77.78 35 93.43 5.08 68.52 61.67 91.03 14.49Table 2: In all Tables: the number of sentences with a LAS ?
?
parsed by DeSr and MST parsers (first row); Precision(Prec), Recall (Rec), the corresponding parser accuracy (LAS) of the top 25% and 50% of the list of sentences andranked by the ULISSE algorithm, Length of Sentence (LS) and dependency PUPA (dPUPA) and the correspondingaverage length in tokens of ranked sentence (AvgSL).the fact that PUPA is based on constituency?specificfeatures that once translated in terms of dependencystructures may be not so effective.In order to show that the ranking of sentencesdoes not follow from raw text features but ratherfrom parse features, we evaluated the accuracy ofparsed sentences that are not?shared by MST andDeSR top?lists selected by ULISSE.
For each testset we selected a different top list: a set of 100sentences in the IT and EN in?domain experimentsand of 50 sentences in the IT out?of?domain exper-iment.
For each of them we have a different numberof not?shared sentences: 24, 15 and 16 in the ITin?domain, IT out?of?domain and EN in?domainexperiments respectively.
Table 3 reports the LASof DeSR and MST for these sentences: it can beobserved that the LAS of not?shared sentences inthe DeSR top list is always higher than the LASassigned by the same parser to the not?shared sen-tences in the MST top list, and viceversa.
For in-stance, in the English experiment the LAS achievedby DeSR on the not?shared top list is higher (86.50)than the LAS of DeSR on the not?shared MST toplist (83.37); viceversa, the LAS of MST on the not?shared DeSR top list is higher (86.74) than the LASof MST on the not?shared MST top list (90.39).
Theunique exception is MST in the IT out?of?domainexperiment, but the difference in terms of LAS be-tween the parses is not statistically relevant (p?value< 0.05).
These results demonstrate that ULISSE isable to select parsed sentences on the basis of thereliability of the analysis produced by each parser.6 ConclusionULISSE is an unsupervised linguistically?drivenmethod to select reliable parses from the output ofdependency parsers.
To our knowledge, it repre-sents the first unsupervised ranking algorithm oper-ating on dependency representations which are moreand more gaining in popularity and are arguablymore useful for some applications than constituencyparsers.
ULISSE shows a promising performanceagainst the output of two supervised parsers se-lected for their behavioral differences.
In all experi-ments, ULISSE outperforms all baselines, includingdPUPA and Sentence Length (SL), the latter repre-senting a very strong baseline selection method in asupervised scenario, where parsers have a very highperformance with short sentences.
The fact of car-rying out the task of reliable parse selection in a su-pervised scenario represents an important novelty:however, the unsupervised nature of ULISSE couldalso be used in an unsupervised scenario (Reichartand Rappoport, 2010).
Current direction of researchinclude a careful study of a) the quality score func-tion, in particular for what concerns the combinationof individual feature weights, and b) the role and ef-fectivess of the set of linguistic features.
This studyis being carried out with a specific view to NLP taskswhich might benefit from the ULISSE algorithm.This is the case, for instance, of the domain adap-tation task in a self?training scenario (McClosky etal., 2006), of the treebank construction process byminimizing the human annotators?
efforts (Reichartand Rappoport, 2009b), of n?best ranking methodsfor machine translation (Zhang, 2006).122ReferencesGiuseppe Attardi.
2006.
Experiments with a multilan-guage non-projective dependency parser.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning (CoNLL-X ?06), New YorkCity, New York, pp.
166?170.Cristina Bosco, Simonetta Montemagni, AlessandroMazzei, Vincenzo Lombardo, Felice Dell?Orletta andAlessandro Lenci.
2009.
Parsing Task: comparingdependency parsers and treebanks.
In Proceedings ofEvalita?09, Reggio Emilia.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Felice Dell?Orletta.
2009.
Ensemble system for Part-of-Speech tagging.
In Proceedings of Evalita?09, Eval-uation of NLP and Speech Tools for Italian, ReggioEmilia, December.Lyn Frazier.
1985.
Syntactic complexity.
In D.R.Dowty, L. Karttunen and A.M. Zwicky (eds.
), NaturalLanguage Parsing, Cambridge University Press, Cam-bridge, UK.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
In Cognition, 68(1), pp.
1-76.Daisuke Kawahara and Kiyotaka Uchimoto.
2008.Learning Reliability of Parses for Domain Adaptationof Dependency Parsing.
In Proceedings of IJCNLP2008, pp.
709?714.Dekan Lin.
1996.
On the structural complexity of nat-ural language sentences.
In Proceedings of COLING1996, pp.
729?733.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of CoNLL2006.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the Errors of Data-Driven Dependency ParsingModels.
In Proceedings of EMNLP-CoNLL, 2007, pp.122-131.Mitchell P. Marcus, Mary Ann Marcinkiewicz and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the penn treebank.
In Comput.
Lin-guist.,vol.
19, issue 2, MIT Press, pp.
313?330.Rita Marinelli, et al 2003.
The Italian PAROLE cor-pus: an overview.
In A. Zampolli et al (eds.
), Compu-tational Linguistics in Pisa, XVI?XVII, Pisa?Roma,IEPI., I, 401?421.David McClosky, Eugene Charniak and Mark Johnson.2006.
Reranking and self?training for parser adap-tation.
In Proceedings of ICCL?ACL 2006, pp.
337?344.David McClosky, Eugene Charniak and Mark Johnson.2008.
When is Self?Trainig Effective for parsing?.
InProceedings of COLING 2008, pp.
561?568.Jim Miller and Regina Weinert.
1998.
Spontaneous spo-ken language.
Syntax and discourse.
Oxford, Claren-don Press.Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler.2010.
Structural Features for Predicting the LinguisticQuality of Text Applications to Machine Translation,Automatic Summarization and Human?Authored Text.In E. Krahmer, M. Theune (eds.
), Empirical Methodsin NLG, LNAI 5790, Springer-Verlag Berlin Heidel-berg, pp.
222241.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, Deniz Yuret.2007.
The CoNLL 2007 Shared Task on DependencyParsing.
In Proceedings of the EMNLP-CoNLL, pp.915?932.Sujith Ravi, Kevin Knight and Radu Soricut.
2008.
Auto-matic Prediction of Parser Accuracy.
In Proceedingsof the EMNLP 2008, pp.
887?896.Roi Reichart and Ari Rappoport.
2007a.
An ensemblemethod for selection of high quality parses.
In Pro-ceedings of ACL 2007, pp.
408?415.Roi Reichart and Ari Rappoport.
2007b.
Self?Trainingfor Enhancement and Domain Adaptation of Statisti-cal Parsers Trained on Small Datasets.
In Proceedingsof ACL 2007, pp.
616?623.Roi Reichart and Ari Rappoport.
2009a.
Automatic Se-lection of High Quality Parses Created By a Fully Un-supervised Parser.
In Proceedings of CoNLL 2009,pp.
156?164.Roi Reichart and Ari Rappoport.
2009b.
Sample Selec-tion for Statistical Parsers: Cognitively Driven Algo-rithms and Evaluation Measures.
In Proceedings ofCoNLL 2009, pp.
3?11.Roi Reichart and Ari Rappoport.
2010.
Improved FullyUnsupervised Parsing with Zoomed Learning.
In Pro-ceedings of EMNLP 2010.Brian Roark, Margaret Mitchell and Kristy Hollingshead.2007.
Syntactic complexity measures for detectingMild Cognitive Impairment.
In Proceedings of ACLWorkshop on Biological, Translational, and ClinicalLanguage Processing (BioNLP?07), pp.
1?8.Kenji Sagae and Junichi Tsujii.
2007.
DependencyParsing and Domain Adaptation with LR Models andParser Ensemble.
In Proceedings of the EMNLP?CoNLL 2007, pp.
1044?1050.Giulia Venturi.
2010.
Legal Language and Legal Knowl-edge Management Applications.
In E. Francesconi, S.Montemagni, W. Peters and D. Tiscornia (eds.
), Se-mantic Processing of Legal Texts, Lecture Notes inComputer Science, Springer Berlin / Heidelberg, vol.6036, pp.
3-26.123Alexander Yates, Stefan Schoenmackers and Oren Et-zioni.
2006.
Detecting Parser Errors Using Web?based Semantic Filters.
In Proceedings of the EMNLP2006, pp.
27?34.Victor H.A.
Yngve.
1960.
A model and an hypothesis forlanguage structure.
In Proceedings of the AmericanPhilosophical Society, pp.
444-466.Ying Zhang, Almut Hildebrand and Stephan Vogel.2006.
Distributed language modeling for N-best listre-ranking.
In Proceedings of the EMNLP 2006, pp.216?223.124
