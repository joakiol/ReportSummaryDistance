Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 511?520,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsIRT-based Aggregation Model of Crowdsourced Pairwise Comparisonsfor Evaluating Machine TranslationsNaoki Otani1 Toshiaki Nakazawa2 Daisuke Kawahara1 Sadao Kurohashi11Graduate School of Informatics, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, Japan2Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japanotani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp {dk,kuro}@i.kyoto-u.ac.jpAbstractRecent work on machine translation has usedcrowdsourcing to reduce costs of manual eval-uations.
However, crowdsourced judgmentsare often biased and inaccurate.
In this pa-per, we present a statistical model that ag-gregates many manual pairwise comparisonsto robustly measure a machine translationsystem?s performance.
Our method appliesgraded response model from item responsetheory (IRT), which was originally developedfor academic tests.
We conducted experi-ments on a public dataset from the Workshopon Statistical Machine Translation 2013, andfound that our approach resulted in highly in-terpretable estimates and was less affected bynoisy judges than previously proposed meth-ods.1 IntroductionManual evaluation is a primary means of interpret-ing the performance of machine translation (MT)systems and evaluating the accuracy of automaticevaluation metrics.
It is also essential for natural lan-guage processing tasks such as summarization anddialogue systems, where (1) the number of correctoutputs is unlimited, and (2) na?
?ve text matchingcannot judge the correctness, that is, an evaluatormust consider syntactic and semantic information.Recent work has used crowdsourcing to reducecosts of manual evaluations.
However, the judg-ments of crowd workers are often noisy and unre-liable because they are not experts.To maintain quality, evaluation tasks imple-mented using crowdsourcing should be simple.Thus, many previous studies focused on pairwisecomparisons instead of absolute evaluations.
Thesame task is given to multiple workers, and their re-sponses are aggregated to obtain a reliable answer.We must, therefore, develop methods that ro-bustly estimate the MT performance based on manypairwise comparisons.Some aggregation methods have been proposedfor MT competitions hosted by the Workshop onStatistical Machine Translation (WMT) (Bojar et al,2013; Hopkins and May, 2013; Sakaguchi et al,2014), where a ranking of the submitted systems isproduced by aggregating many manual judgments ofpairwise comparisons of system outputs.However, existing methods do not consider thefollowing important issues.Interpretability of the estimates: For the purposeof evaluation, their results must be interpretable sothat we could use the results to improve MT systemsand the next MT evaluation campaigns.
Existingmethods, however, only yield system-level scores.Judge sensitivity: Some judges can examine thequality of translations with consistent standards, butothers cannot (Graham et al, 2015).
Sensitivities tothe translation quality and judges?
own standards areimportant factors.Evaluation of a newly submitted system: Pre-vious approaches considered all pairwise combina-tions of systems and must compare a newly sub-mitted system with all the submitted systems.
Thismade it difficult to allow participants to submit theirsystems after starting the evaluation step.To address these issues, we use a model from511item response theory (IRT).
This theory was origi-nally developed for psychometrics, and has applica-tions to academic tests.
IRT models are highly in-terpretable and are supported by theoretical and em-pirical studies.
For example, we can estimate theinformativeness of a question in a test based on theresponses of examinees.We focused on aggregating many pairwise com-parisons with a baseline translation so that we coulduse the analogy of standard academic tests.
Figure 1shows our problem setting.
Each system of inter-est yields translations, and the translations are com-pared with a baseline translation by multiple humanjudges.
Each judge produces a preference judgment.The pairwise comparisons correspond to ques-tions in academic tests, a judge?s sensitivity to thetranslation quality is mapped to discrimination ofquestions, and the relative difficulty of winning thepairwise comparison is mapped to the difficulty ofquestions.
MT systems correspond to students thattake academic tests, and IRT models can be naturallyapplied to estimate the latent performance (ability)of MT systems (students).Additionally, our approach, fixing baseline trans-lations, can easily evaluate a newly submitted sys-tem.
We only need to compare the new system withthe baseline instead of testing all pairwise combina-tions of the submitted systems.Our contributions are summarized as follows.11.
We propose an IRT-based aggregation model ofpairwise comparisons with highly interpretableparameters.2.
We simulated noisy judges on the WMT13dataset and demonstrated that our model is lessaffected by the noisy judges than previouslyproposed methods.2 Related WorkThe WMT shared tasks have collected many manualjudgments of segment-level pairwise comparisonsand used them to produce system-level rankings forMT tasks.
Various methods has been proposed to ag-gregate the judgments to produce reliable rankings.1We also show that our method accurately replicated theWMT13 official system scores using a few comparisons.
How-ever, this is not the main focus of this paper.Figure 1: Illustration of manual pairwise comparison.Each system yields translations.
Judges compare themwith a baseline translation and report their preferences.Our goal is to aggregate the judgments to determine theperformance of each system.Frequency based approaches were used to pro-duce the WMT13 official rankings (Bojar et al,2013), considering statistical significance of the re-sults (Koehn, 2012).Hopkins and May (2013) noted that we shouldconsider the relative matchup difficulty, and pro-posed a statistical aggregation model.
Their modelassumes that the quality of each system can be rep-resented by a Gaussian distribution.Sakaguchi et al (2014) applied TrueSkill (Her-brich et al, 2006) to reduce the number of compar-isons to reach the final estimate based on an activelearning strategy.
The same model was recently usedfor grammatical error correction (Grundkiewicz etal., 2015; Napoles et al, 2015).These methods acquire the final system-levelscores, whereas our model also estimates segmentspecific and judge specific parameters.The Bradley?Terry (BT) model was the result ofa seminal study on aggregating pairwise compar-isons (Bradley and Terry, 1952; Chen et al, 2013;Dras, 2015).
Recently, Chen et al (2013) explic-itly incorporated the quality of judges into the BTmodel, and applied it to quality control in crowd-sourcing.The previously mentioned methods focused onpairwise comparisons of all combination of the MTsystems, and thus, the number of comparisons in-creases rapidly as the number of systems increases.512Our approach, however, only uses comparisons witha fixed baseline.
This approach enables to apply IRTmodels for academic tests and makes it easy to eval-uate a newly submitted system.The work most relevant to our model is the IRT-based crowdsourcing model proposed by Baba andKashima (2013).
Their goal was to estimate the truequality of artifacts such as design works based onratings assigned by reviewers.
They also applied agraded response model to incorporate the authors?latent abilities and the reviewers?
biases.Yet their setting differs from ours in that they fo-cused on the quality of the artifacts, whereas we areinterested in the authors.
Additionally, their modelmaps task difficulty and review bias to a difficultyparameter in IRT.
However, we naturally extendedthe model so that standard analysis approaches canbe applied to maintain interpretability.Some studies have focused on absolute evalua-tions (Goto et al, 2014; Graham et al, 2015).
Gra-ham et al (2015) gathered continuous scale evalu-ations in terms of adequacy and fluency for manysegments, and filtered out noisy judgments basedon their consistency.
The proposed pipeline resultsin very accurate evaluations, but 40-50% of all thejudgments were filtered out due to inconsistencies.This explains the difficulties of developing absoluteevaluation methods in crowdsourcing.3 Problem SettingWe first describe the problem setting, as shown inFigure 1.Assume that there are a group of systems I in-dexed by i, a set of segments J indexed by j, and aset of judges K indexed by k.Before a manual evaluation, we fix an arbitrarybaseline system and use it to translate the segmentsJ .
Then, each system i ?
I produces a transla-tion on segment j ?
J .
One of the judges k ?
Kcompares it with the baseline translation.
The judgeproduces a preference judgment.Let ui,j,k be the observed judgment that judge kassigns to a translation by system i on segment j,that is,ui,j,k =????
?1 (preference for baseline)2 (no preference)3 (preference for system i) ,3 2 1 0 1 2 3Latent performance of systems0.00.20.40.60.81.0Generative probability ofjudgment losetiewinFigure 2: ICC of graded response model for (b1, b2) =(?0.5, 0.5) and a = 1.7and let c ?
{1, 2, 3} be the judgment label.Each system i has its own latent performance ?i ?R.
Our goal is to estimate ?
by using the observedjudgments U = {ui,j,k}i?I,j?J ,k?K.4 Generative Judgment ModelWe describe a statistical model for pairwise compar-isons based on an IRT model.4.1 Modified Graded Response ModelBased on the graded response model (GRM) pro-posed by Samejima (1968), we define a generativemodel of judgments.
GRM deals with responses onordered categories including ratings such as A+, A,B+ and B, and partial credits in tests.
In our prob-lem setting, judgments can be seen as partial credits.When a system beats a baseline translation, the sys-tem receives c = 3 credit.
In the case of a tie, thesystem receives c = 2 credit.
The system receivesc = 1 credit when it lose to the baseline.Let P?jkc(?i) be the probability that judge k as-signs judgment pi > c to a comparison on segment jbetween system i and a baseline.P?jkc(?i) =11 + exp(?ak(?i ?
bjc)) ,where P?jk0(?i) = 1,P?jk3(?i) = 0.
Parameters a andb are called discrimination and difficulty parameters,respectively.
a represents the discriminablity or sen-sitivity of the judge, and b represents a segment-specific difficulty parameter.
The discrimination pa-rameter (a) is positive, and the difficulty parameter(b) satisfies b1 < b2, where b1 corresponds to the dif-ficulty of not losing to the baseline (c > 1), and b2513corresponds to the difficulty of beating the baseline(c > 2).The generative probability of judgment ui,j,k isdefined as the difference in the probabilities definedabove, that is,Pjkc(?i) = P(ui,j,k = c|?i, bj , ak)= P?jkc?1(?i)?
P?jkc(?i).This function is called item characteristic curve(ICC).
Figure 2 illustrates the ICC in the GRM.
Thehorizontal axis represents the latent performance ofsystems, and the vertical axis represents the genera-tive probability of the judgments.
This figure shows,for example, that the probability of the system with?
= 0 beating the baseline is 0.3, whereas the systemwith ?
= 1.0 is much more likely to win.
The dis-crimination parameter controls slope of the curves.If a is small, the probability drops a little when ?decreased.The model described above is different from theoriginal GRM, which assumed that the values of aare independent from question to question, and thateach a belongs to exactly one question.
However,in our problem setting, the judges evaluate multiplesegments, and discrimination parameter a is inde-pendent from segment j.
This modification meansthat the GRM can capture the judge?s sensitivity.4.2 PriorsWe assign prior distributions to the parameters toobtain estimates stably.
We assume Gaussian dis-tributions on ?
and b, that is, ?
?
N (0, ?2) andbc ?
N (?bc, ?2bc) (c = 1, 2).
The discriminationparameter is positive, so we assume a log Gaussiandistribution on a, i.e., log(a) ?
N (?a, ?2a).
Notethat ?, ?, and ?
are hyper parameters.5 Parameter EstimationWe find the values of the parameters to maximizethe log likelihood based on obtained judgments U :L(?, ?)
= logP(U, ?, ?
).We denote the parameters a = {ak}k?K and b ={bj1, bj2}j?J to be ?
in this section.5.1 Marginal Likelihood Maximization ofJudge Sensitivity and Matchup DifficultyEstimates are known to be inaccurate when all theparameters are optimized at once, so we first esti-mate the parameters ?
to maximize the marginal loglikelihood w.r.t.
the system performance ?.mL(?)
= logP(U, ?)=?i?Ilog?
???P(?
)P(Ui|?, ?)d?
+ logP(?
),where Ui is the set of judgments given to system iThe equation above can be approximated usingGauss-Hermite quadrature, i.e.,mL(?)
?
?i?IlogT?t=11?piwtP(Ui|?xt, ?)
+ logP(?
)wt =2T?1T !
?piT 2 (H(xt))2H(xt) =(2xt ?ddxt)T?1?
1,where a practically good approximation is obtainedby taking T ?
20.2We solve the optimization problem using the gra-dient descent methods to maximize the approxi-mated marginal likelihood.
The inequality con-straints on the parameters are handled by adding logbarrier functions to the objective function.5.2 Maximum A Posteriori (MAP) Estimationof System PerformanceGiven the estimates of ?, we estimate the system per-formance ?
= {?i}i?I by using MAP estimation.We maximize the objective function,L(?)
= logP(U, ?
; ?
)=?i?IlogP(?i) +?i?IlogP(Ui|?i; ?
).The estimates of ?
are obtained using the gradientdescent method.5.3 DiscussionSo far we have assumed that the estimate is basedon batch learning.
However, it is known that active2In this study, we set T = 21 to include x = 0.514learning can reduce the costs (i.e., the total numberof comparisons) (Sakaguchi et al, 2014).To extend our model to the active learning frame-work, one approach is to optimize the objectivefunction online and actively select the next systemto be compared based on criteria such as the uncer-tainty of the system?s performance.
We can applystochastic gradient descent to the online optimiza-tion, which updates the estimates of the parame-ters using the gradients calculated based on a singlecomparison.
This modification was left for futurework.6 ExperimentsWe conducted experiments on the WMT13 man-ual evaluation dataset for 10 language pairs.3 Fordetails of the evaluation data, see the overview ofWMT13 (Bojar et al, 2013).6.1 SetupModels: Our method (GRM) was initialized usinga = 1.7, b = (?0.5, 0.5), and a ?
value derivedby summing up the judgments for each system andscaling ?
to fit the prior distribution.
For the hyperparameters, we set ?
= ?2, ?a = log(1.7), ?a =1.0, ?b = (?0.5, 0.5), ?b = 2.0.To compare with our method, we trained Ex-pectedWins (EW) (Bojar et al, 2013), the modelby Hopkins and May (2013), (HM) and the two-stage crowdsourcing model proposed by Baba andKashima (2013) (TSt).
We also trained TrueSkill(TS) (Sakaguchi et al, 2014), which was used toproduce the gold score on this experiment.We followed Sakaguchi et al (2014), who alsoused the WMT13 datasets in their experiments, andinitialized the HM and TS parameters.
For TSt, wefollowed Baba and Kashima (2013).Pairwise comparisons: The WMT dataset con-tains five-way partial rankings, so we converted thefive-way partial rankings into pairwise comparisons.For example, given a five-way partial ranking A >B > C > D > E, we obtain ten pairwise compar-isons A > B, A > C, A > D, ?
?
?
, and D > E. Werandomly sampled 800, 1,600, 3,200 and 6,400 pair-wise comparisons from the whole dataset.3http://statmt.org/wmt13/results.htmlThe training data differs between the models.
ForGRM and TSt, we first sampled five-way rankingsthat contained a baseline translation for each base-line system and obtained pairwise comparisons.
ForEW and HM, we first converted five-way rankingsinto pairwise comparisons and selected them at ran-dom.4 TS first receives all the pairwise compar-isons and selects the training data based on the activelearning strategy, whereas we sampled the compar-isons before running the other methods.Gold scores: We followed the official evaluationprocedure of the WMT14-15 (Bojar et al, 2014; Bo-jar et al, 2015) and made gold scores with TS.
Weproduced 1,000 bootstrap-resampled datasets overall of the available comparisons.
We then ran TSand collected the system scores.
The gold score isthe mean of the scores.Evaluation metrics: We evaluated the models us-ing the Pearson correlation coefficient and the nor-malized discounted cumulative gain (nDCG), com-paring the estimated scores and gold scores.
Weused nDCG because we are often interested in ranksand scores, especially in MT competitions such asthe WMT translation task.5 These metrics were alsoused for experiments in Baba and Kashima (2013).6.2 ResultsFigure 3 shows the correlation and nDCG betweenthe estimated system performance and the goldscores for the WMT13 Spanish?English task.
Forthe GRM and TSt, the baselines used in the eval-uation are shown in parentheses in the labels.
Theother language pairs showed similar tendencies.
Thecomplete results for all language pairs can be foundin the supplementary data files.Note that the main contribution of our method isnot to perform better than other methods in terms ofcorrelation and nDCG to the gold scores, but to re-sult in highly interpretable and robust estimates dis-cussed later.TS resulted in the highest correlation and nDCG.It is reasonable because the gold scores themselveswere produced by TS, and because it estimates the4We also applied the sampling procedure of GRM and TStto EW and HM, but it made their estimation inaccurate.5We did not use Spearman?s rank correlation coefficient be-cause it does not consider a margin between ranks.5151000 2000 3000 4000 5000 6000 7000#comparisons0.30.40.50.60.70.80.91.0Corr.
TSHMEWTSt(CU-ZEMAN)TSt(SHEF-WPROA)TSt(mean)GRM(DCU)GRM(SHEF-WPROA)GRM(mean)(a) Correlation1000 2000 3000 4000 5000 6000 7000#comparisons0.30.40.50.60.70.80.91.0nDCG TSHMEWTSt(CU-ZEMAN)TSt(DCU-OKITA)TSt(mean)GRM(DCU)GRM(UEDIN-HEAFIELD)GRM(mean)(b) nDCGFigure 3: Correlation and nDCG comparing the estimated system performance and gold scores with the numberof comparisons for the WMT13 Spanish?English task.
The baseline system is shown in parenthesis for TSt andGRM.parameters using active learning, unlike the othermodels.The GRM with the best baseline system (DCU)achieved almost the same scores as the TS, in termsof correlation and nDCG.
Although the TSt with thebest baseline resulted in accurate estimates in termsof correlation, it did not in terms of nDCG.
Withthe worst baselines, the GRM and TSt both failedto replicate the gold scores, but the GRM was sur-prisingly accurate in terms of nDCG (even in theworst case).
This implies that the GRM can effec-tively predict the top ranked systems.6.3 Baseline SelectionIt is likely that single pairwise comparisons do notwork well if the baseline is very strong or weak.
Asshown in Figure 3, the baseline system influencesthe final result.
When we used SHEF-WPROAas baseline, the estimated system performance wasnot accurate.
This is because SHEF-WPROA loses69.4% of the pairwise comparisons and fails to dis-criminate between the other systems.
In contrast,DCU loses 34.5% and win 34.8% of the compar-isons and discriminate the other systems success-fully.
Thus, when we used DCU as baseline, the bestcorrelation and nDCG were achieved.
Therefore, wemust determine the appropriate baseline system be-fore the comparisons.One possible solution is to consider the system-Noise(%) 0 10 20 30 40 50CorrelationGRM .929 .917 .900 .879 .849 .807HM +.002 -.005 -.009 -.015 -.025 -.038EW -.025 -.028 -.035 -.038 -.040 -.046nDCGGRM .883 .867 .847 .822 .793 .752HM -.024 -.130 -.137 -.144 -.152 -.168EW -.035 -.054 -.064 -.060 -.060 -.069Table 1: Correlation and nDCG between the estimatedsystem performance and gold scores for the WMT13Spanish?English task, based on noisy judges.
The val-ues were averaged over all the datasets.
The GRM scoreswere averaged over all baselines.
The differences fromthe GRM are reported for the HM and EW.level scores yielded by automatic evaluation metricssuch as BLEU and METEOR.
Figure 4 shows thatwe obtained relatively good results when we useda system whose system-level BLEU score and ME-TEOR score6 were close to the mean of all the sys-tems.
76.4 Analysis of Judge SensitivityTo investigate the robustness of the GRM, we sim-ulated ?noisy?
judges.
We selected a subset of6BLEU and METEOR scores were given by the WMT13organizers.7The system-level scores can be found in the WMT13 Met-rics Task dataset.5160.15 0.10 0.05 0.00 0.05 0.10 0.150.50.40.30.20.10.0 cs-enen-csde-enen-dees-enen-esfr-enen-frru-enen-ru(a) BLEU vs. Correlation0.15 0.10 0.05 0.00 0.05 0.10 0.150.50.40.30.20.10.0 cs-enen-csde-enen-dees-enen-esfr-enen-frru-enen-ru(b) BLEU vs. nDCG0.15 0.10 0.05 0.00 0.05 0.10 0.150.50.40.30.20.10.0 cs-enen-csde-enen-dees-enen-esfr-enen-frru-enen-ru(c) METEOR vs. Correlation0.15 0.10 0.05 0.00 0.05 0.10 0.150.50.40.30.20.10.0 cs-enen-csde-enen-dees-enen-esfr-enen-frru-enen-ru(d) METEOR vs. nDCGFigure 4: Relationship between system-level BLEU/METEOR scores (horizontal) and correlation/nDCG scores(vertical).
The mean BLEU/METEOR was set to zero, and the best score was set to zero for each language pair.judges and randomly changed their decisions basedon a uniform distribution.
The percentage of noisyjudges varied between 10% and 50% (in incrementsof 10%).We trained HM and EW on the simulated datasets.We excluded TS because it assumes that we can ac-tively request more comparisons from judges whentheir decisions are ambiguous.As shown in Table 1, the accuracy of the GRMwas less affected by the noisy judges than HM andEW.
This is because our model estimates judge-specific sensitivities and automatically reduces theinfluence of the noisy judges.6.5 Analysis of the Interpretability of theEstimated Matchup DifficultyOur model is a natural extension of the GRM Same-jima (1968), so we can apply standard analyses forIRT models.
Item information is one of the standardanalysis methods and corresponds to sensitivity to alatent parameter of interest.
Based on the item infor-mation, we can find which segment was difficult tobe translated better than a baseline translation.The item information is calculated using the esti-1.0 0.5 0.0 0.5 1.0?0.500.550.600.650.700.750.800.850.90Iteminformation0.13-0.1118581818Figure 5: Item information for the WMT13 Spanish?English task.
The DCU was used as a baseline.
We usedthe averaged estimates of b on 100 sampled datasets with6,400 comparisons to calculate the item information forall segments.mated parameters ?
(Samejima, 1968), that is,Ij(?)
= ?E[?2L(?
; ?)??2]=3?c=1[??2logPjkc(?)??2]Pjkc=3?c=1[P??jkc?1(?)?
P??jkc(?)]2P?jkc?1(?)?
P?jkc(?)
,where P??
= ?P?/?
?.Because the item information is only determined517Segment 1858: Difficult to beat the baseline translation.Source Hasta 2007 los dos telescopios Keck situados en el volca?n hawaiano de Mauna Kea eranconsiderados los ma?s grandes del mundo.Reference Until 2007, the two Keck telescopes at the Hawaiian volcano, Mauna Kea, were thelargest in the world.DCU[baseline] Until 2007, the two Keck telescopes located on the Hawaiian volcano Mauna of KEAwere considered the largest in the world.ONLINE-B (?
=) 0.24 Until 2007 the two Keck telescopes located on the Hawaiian volcano Mauna Kea wereconsidered the largest in the world.UEDIN 0.12 Until 2007, the two Keck telescopes located on the Hawaiian volcano of Mauna Keawere considered the largest in the world.LIMSI-NCODE-SOUL 0.10 Until 2007 the two Keck telescopes in the Hawaiian Mauna Kea volcano were consid-ered the largest in the world.CU-ZEMAN -0.10 Until 2007, the two Keck telescope located in the volcano Mauna Kea hawaiano of wereregarded as the world?s largest.JHU -0.12 Until 2007, the two Telescope Keck located in the Kea volcano hawaiano of Mauna wereconsidered the world?s largest.SHEF-WPROA -0.92 Until 2007 the two telescope Keck located volcano hawaiano of Mauna KEA were re-garded larger of world.Segment 1818: Easy to beat the baseline translation.Source Dependiendo de las tonalidades, algunas ima?genes de galaxias espirales se convierten enverdaderas obras de arte.Reference Depending on the colouring, photographs of spiral galaxies can become genuine worksof art.DCU[baseline] Depending on the drink, some images of galaxias galaxies become true works of art.ONLINE-B 0.24 Depending on the shades, some images of spiral galaxies become true works of art.UEDIN 0.12 (Same as ONLINE-B)LIMSI-NCODE-SOUL 0.10 Depending on the color, some images of galaxies spirals become real works of art.CU-ZEMAN -0.10 Depending on the tonalidades, some images of spirals galaxies become true works of art.JHU -0.12 Depending on the tonalidades, some images of galaxies spirals become true works of art.SHEF-WPROA -0.92 Depending on the tonalidades, some images of galaxies spirals become real artwork.Table 2: Translation examples for the WMT13 Spanish?English task.
The reference is a correct translation givenby the WMT organizers and was shown to human judges.
Estimates of ?
(averaged over 100 sampled datasets with6,400 comparisons) are also reported in the table.by segments and is independent of the judges, we setak = 1 (k ?
K).Figure 5 gives two examples of the item infor-mation.
The horizontal axis corresponds to the sys-tem performance ?, and the vertical axis representsthe informativeness of a segment.
This figure in-dicates that segment 1858 (red line) can effectivelydiscriminate systems with ?
?
0.13, whereas seg-ment 1818 (blue dashed line) is sensitive to thosewith ?
?
?0.11.
This means that systems with low?
tend to lose to a baseline translation on segment1858, and the segment does not tell meaningful in-formation on performance of the systems.
However,they sometimes beat a baseline translation on seg-ment 1818, and the segment can measure their per-formance accurately.Table 2 shows translations for segments 1858 and1818.
We found that the baseline translation on seg-ment 1818 was relatively good, whereas the baselinetranslation on segment 1858 contained wrong wordssuch as ?drink?
and ?galaxias?.
Consequently, sys-tems with low ?
tended to lose to the baseline onsegment 1858 due to their wrong translation (see thetranslation of ?hawaiano de Mauna Kea?).
In con-trast, some of the low-ranked systems beat the base-line on segment 1818, and the segment contributedto discriminate them.The item information is used to design academictests that can effectively capture students?
abilities.It could analogously be used to preselect segmentsto be translated based on the item information in theMT evaluation.5187 ConclusionWe have addressed the task of manual judgment ag-gregation for MT evaluations.
Our motivation wasthree folded: (1) to incorporate a judge?s sensitivityto robustly measure a system?s performance, (2) tomaintain highly interpretable estimates, and (3) tohandle with a newly submitted system.To tackle these problems, we focused on pairwisecomparisons with a fixed baseline translation so thatwe could apply the GRM model in IRT by using theanalogy of standard academic tests.
Unlike testingall pairwise combinations of systems, fixing base-line translations makes it easy to evaluate a newlysubmitted system.
We demonstrated that our modelgave robust and highly interpretable estimates on theWMT13 datasets.In the future work, we will incorporate activelearning to the proposed method so that we couldreduce the total number of comparisons to obtain fi-nal results.
Although we evaluated the correlationbetween the estimated system performance scoresand the WMT official scores, other evaluation pro-cedures might also be considered.
For example,Hopkins and May (2013) considered model perplex-ity and Sakaguchi et al (2014) compared accuracy.However, we cannot directly compare other meth-ods to our method in terms of perplexity or accuracybecause our method focuses on comparisons with abaseline translation, whereas they do not.
It will berequired to investigate correlation between the esti-mates and expert decisions.AcknowledgmentsWe would like to thank Yukino Baba and HisashiKashima for providing an implementation of theirmethod.
We are also thankful for the useful com-ments from the anonymous reviewers.ReferencesYukino Baba and Hisashi Kashima.
2013.
Statisticalquality estimation for general crowdsourcing tasks.
InProceedings of the 19th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing (KDD), pages 554?562, New York, USA, August.ACM Press.Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 workshop onstatistical machine translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation(WMT), pages 1?44, Sofia, Bulgaria, August.
Associ-ation for Computational Linguistics.Ondr?ej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales?
Tam-chyna.
2014.
Findings of the 2014 workshop on statis-tical machine translation.
In Proceedings of the NinthWorkshop on Statistical Machine Translation (WMT),pages 12?58, Baltimore, Maryland, USA, June.
Asso-ciation for Computational Linguistics.Ondr?ej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp, PhilippKoehn, Varvara Logacheva, Christof Monz, MatteoNegri, Matt Post, Carolina Scarton, Lucia Specia, andMarco Turchi.
2015.
Findings of the 2015 workshopon statistical machine translation.
In Proceedings ofthe Tenth Workshop on Statistical Machine Translation(WMT), pages 1?46, Lisbon, Portugal, September.
As-sociation for Computational Linguistics.Ralph Allan Bradley and Milton E Terry.
1952.
Rankanalysis of incomplete block designs: I. the method ofpaired comparisons.
Biometrika, 39(3-4):324?345.Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson,and Eric Horvitz.
2013.
Pairwise ranking aggrega-tion in a crowdsourced setting.
In Proceedings of theSixth ACM International Conference on Web Searchand Data Mining (WSDM), pages 193?202, New York,New York, USA, February.
ACM Press.Mark Dras.
2015.
Evaluating human pairwise preferencejudgments.
Computational Linguistics, 41(2):337?345.Shinsuke Goto, Donghui Lin, and Toru Ishida.
2014.Crowdsourcing for evaluating machine translationquality.
In Proceedings of the Ninth InternationalConference on Language Resources and Evaluation(LREC), Reykjavik, Iceland, May.
European LanguageResources Association.Yvette Graham, Timothy Baldwin, Alistair Moffat, andJustin Zobel.
2015.
Can machine translation systemsbe evaluated by the crowd alone.
Natural LanguageEngineering, FirstView:1?28.Roman Grundkiewicz, Marcin Junczys-Dowmunt, andEdward Gillian.
2015.
Human evaluation of gram-matical error correction systems.
In Proceedings ofthe 2015 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 461?470,Lisbon, Portugal, June.
Association for ComputationalLinguistics.519Ralf Herbrich, Tom Minka, and Thore Graepel.
2006.TrueSkillTM: A bayesian skill rating system.
In Ad-vances in Neural Information Processing Systems 20(NIPS), pages 569?576, Vancouver, British Columbia,Canada, Demeber.
MIT Press.Mark Hopkins and Jonathan May.
2013.
Models oftranslation competitions.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 1416?1424, Sofia, Bulgaria,August.
Association for Computational Linguistics.Philipp Koehn.
2012.
Simulating human judg-ment in machine translation evaluation campaigns.In Proceedings of International Workshop on Spo-ken Language Translation (IWSLT), pages 179?184,Hongkong, China, December.
International SpeechCommunication Association.Courtney Napoles, Keisuke Sakaguchi, Matt Post, andJoel Tetreault.
2015.
Ground truth for grammaticalerror correction metrics.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing (ACL-IJCNLP),pages 588?593, Beijing, China, July.
Association forComputational Linguistics.Keisuke Sakaguchi, Matt Post, and Benjamin VanDurme.
2014.
Efficient elicitation of annotationsfor human evaluation of machine translation.
In Pro-ceedings of the Ninth Workshop on Statistical MachineTranslation (WMT), pages 1?11, Baltimore, Maryland,USA, June.
Association for Computational Linguis-tics.Fumiko Samejima.
1968.
Estimation of latent ability us-ing a response pattern of graded scores.
ETS ResearchBulletin Series, 1968(1):i?169, June.520
