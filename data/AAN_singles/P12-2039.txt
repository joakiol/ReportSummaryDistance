Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 198?202,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTense and Aspect Error Correction for ESL LearnersUsing Global ContextToshikazu Tajiri Mamoru Komachi Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japan{toshikazu-t, komachi, matsu}@is.naist.jpAbstractAs the number of learners of English is con-stantly growing, automatic error correction ofESL learners?
writing is an increasingly ac-tive area of research.
However, most researchhas mainly focused on errors concerning arti-cles and prepositions even though tense/aspecterrors are also important.
One of the mainreasons why tense/aspect error correction isdifficult is that the choice of tense/aspect ishighly dependent on global context.
Previousresearch on grammatical error correction typ-ically uses pointwise prediction that performsclassification on each word independently, andthus fails to capture the information of neigh-boring labels.
In order to take global infor-mation into account, we regard the task as se-quence labeling: each verb phrase in a doc-ument is labeled with tense/aspect dependingon surrounding labels.
Our experiments showthat the global context makes a moderate con-tribution to tense/aspect error correction.1 IntroductionBecause of the growing number of learners of En-glish, there is an increasing demand to help learn-ers of English.
It is highly effective for learners toreceive feedback on their essays from a human tu-tor (Nagata and Nakatani, 2010).
However, man-ual feedback needs a lot of work and time, and italso requires much grammatical knowledge.
Thus,a variety of automatic methods for helping Englishlearning and education have been proposed.The mainstream of English error detection andcorrection has focused on article errors (Knight andChander, 1994; Brockett et al, 2006) and preposi-tion errors (Chodorow et al, 2007; Rozovskaya andRoth, 2011), that commonly occur in essays by ESLlearners.
On the other hand, tense and aspect errorshave been little studied, even though they are alsocommonly found in learners?
essays (Lee and Sen-eff, 2006; Bitchener et al, 2005).
For instance, Lee(2008) corrects English verb inflection errors, butthey do not deal with tense/aspect errors because thechoice of tense and aspect highly depends on globalcontext, which makes correction difficult.
Considerthe following sentences taken from a corpus of aJapanese learner of English.
(1) I had a good time this Summer Vacation.First, I *go to KAIYUKAN 1 with my friends.In this example, go in the second sentence shouldbe written as went.
It is difficult to correct this typeof error because there are two choices for correc-tion, namely went and will go.
In this case, wecan exploit global context to determine which cor-rection is appropriate: the first sentence describes apast event, and the second sentence refers the firstsentence.
Thus, the verb should be changed to pasttense.
This deduction is easy for humans, but is dif-ficult for machines.One way to incorporate such global context intotense/aspect error correction is to use a machinelearning-based sequence labeling approach.
There-fore, we regard the task as sequence labeling:each verb phrase in the document is labeled withtense/aspect depending on surrounding labels.
Thismodel naturally takes global context into account.Our experiments show that global context makes amoderate contribution to tense/aspect correction.1Kaiyukan is an aquarium in Osaka, Japan.1982 Tense/Aspect Error CorpusDeveloping a high-quality tense and aspect errorcorrection system requires a large corpus annotatedwith tense/aspect errors.
However, existing anno-tated corpora are limited in size,2 which precludesthe possibility of machine learning-based approach.Therefore, we constructed a large-scale tense/aspectcorpus from Lang-8,3 a social networking servicefor learners of foreign languages.
ESL learners posttheir writing to be collaboratively corrected by na-tive speakers.
We leverage these corrections in creat-ing our tense/aspect annotation.
Lang-8 has 300,000users from 180 countries worldwide, with more than580,000 entries, approximately 170,000 of themin English.4 After cleaning the data, the corpusconsists of approximately 120,000 English entriescontaining 2,000,000 verb phrases with 750,000verb phrases having corrections.5 The annotatedtense/aspect labels include 12 combinations of tense(past, present, future) and aspect (nothing, perfect,progressive, perfect progressive).3 Error Correction Using Global ContextAs we described in Section 1, using only local in-formation about the target verb phrase may lead toinaccurate correction of tense/aspect errors.
Thus,we take into account global context: the relation be-tween target and preceding/following verb phrases.In this paper, we formulate the task as sequence la-beling, and use Conditional Random Fields (Laf-ferty, 2001), which provides state-of-the-art perfor-mance in sequence labeling while allowing flexiblefeature design for combining local and global fea-ture sets.3.1 Local FeaturesTable 1 shows the local features used to train the er-ror correction model.2Konan-JIEM Learner Corpus Second Edition (http://gsk.or.jp/catalog/GSK2011-B/catalog.html)contains 170 essays, and Cambridge English First Certificate inEnglish (http://www.cambridgeesol.org/exams/fce/index.html) contains 1244 essays.3http://lang-8.com/4As of January, 2012.
More details about the Lang-8 corpuscan be found in (Mizumoto et al, 2011).5Note that not all the 750,000 verb phrases were correcteddue to the misuse of tense/aspect.Table 1: Local features for a verb phrasename descriptiont-learn tense/aspect written by the learner(surface tense/aspect)bare the verb lemmaL the word to the leftR the word to the rightnsubj nominal subjectdobj direct objectaux auxiliary verbpobj object of a prepositionp-tmod temporal adverbnorm-p-tmod normalized temporal adverbadvmod other adverbconj subordinating conjunctionmain-clause true if the target VP is in main clausesub-clause true if the target VP is in subordinate clauseWe use dependency relations such as nsubj, dobj,aux, pobj, and advmod for syntactic features.
If asentence including a target verb phrase is a complexsentence, we use the conj feature and add either themain-clause or the sub-clause feature depending onwhether the target verb is in the main clause or in asubordinate clause.
For example, the following twosentences have the same features although they havedifferent structures.
(2) It pours when it rains.
(3) When it rains it pours.In both sentences, we use the feature main-clausefor the verb phrase pours, and sub-clause for theverb phrase rains along with the feature conj:whenfor both verb phrases.Regarding p-tmod, we extract a noun phrase in-cluding a word labeled tmod (temporal adverb).
Forinstance, consider the following sentence containinga temporal adverb:(4) I had a good time last night.In (4), the word night is the head of the noun phraselast night and is a temporal noun,6 so we add thefeature p-tmod:last night for the verb phrase had.Additionally, norm-p-tmod is a normalized formof p-tmod.
Table 2 shows the value of the fea-ture norm-p-tmod and the corresponding tempo-ral keywords.
We use norm-p-tmod when p-tmod6We made our own temporal noun list.199Table 2: The value of the feature norm-p-tmod and cor-responding temporal keywordstemporal keywords valueyesterday or last pastnow presenttomorrow or next futuretoday or this thisTable 3: Feature templatesLocal Feature Templates<head> <head, t-learn> <head, L, R> <L> <L, head><L, t-learn> <R> <R, head> <R, t-learn> <nsubj><nsubj, t-learn> <aux> <aux, head> <aux, t-learn><pobj> <pobj, t-learn> <norm-p-tmod><norm-p-tmod, t-learn> <advmod> <advmod, t-learn><tmod> <tmod, t-learn> <conj> <conj, t-learn><main-clause> <main-clause, t-learn><sub-clause> <sub-clause, t-learn><conj, main-clause> <conj, sub-clause>Global Context Feature Templates<p-tmod?> <p-tmod?, t-learn> <p-tmod?, t-learn?><p-tmod?, t-learn?, t-learn> <norm-p-tmod?><norm-p-tmod?, t-learn> <norm-p-tmod?, t-learn?><norm-p-tmod?, t-learn?, t-learn>includes any temporal keywords.
For instance, inthe sentence (4), we identify last night as temporaladverb representing past, and thus create a featuretime:past for the verb phrase had.3.2 Feature TemplateTable 3 shows feature templates.
<a> represents asingleton feature and <a, b> represents a combina-tion of features a and b.
Also, a?
means the featurea of the preceding verb phrase.
A local feature tem-plate is a feature function combining features in thetarget verb phrase, and a global context feature tem-plate is a feature function including features from anon-target verb phrase.
Suppose we have followinglearner?s sentences:(5) I went to Kyoto yesterday.I *eat yatsuhashi7 and drank green tea.In (5), the verb before eat is went, and p-tmod:yesterday and norm-p-tmod:past are addedto the feature set of verb went.
Accordingly,7Yatsuhashi is a Japanese snack.Table 4: Example of global context feature functions gen-erated by feature templates<p-tmod?:yesterday><p-tmod?
:yesterday, t-learn?
:simple past><p-tmod?
:yesterday, t-learn:simple present><p-tmod?
:yesterday, t-learn?
:simple past, t-learn:simple past><norm-p-tmod?:past><norm-p-tmod?
:past, t-learn?
:simple past><norm-p-tmod?
:past, t-learn:simple present><norm-p-tmod?
:past, t-learn?
:simple past, t-learn:simple present>the global context features p-tmod?
:yesterday andnorm-p-tmod?
:past are added to the verb eat.Table 4 lists all the global context features for theverb eat generated by the feature templates.3.3 Trade-off between Precision and RecallUse of surface tense/aspect forms of target verbs im-proves precision but harms recall.
This is becausein most cases the surface tense/aspect and the cor-rect tense/aspect form of a verb are the same.
It is,of course, desirable to achieve high precision, butvery low recall leads to the system making no cor-rections.
In order to control the trade-off betweenprecision and recall, we re-estimate the best outputlabel y?
based on the originally estimated label y asfollows:y?
= argmaxys(y)s(y) ={?c(y), if y is the same as learner?s tense/aspectc(y) otherwise.where c(y) is the confidence value of y estimatedby the originally trained model (explained in 4.3),and ?
(0 ?
?
< 1) is the weight of the surfacetense/aspect.We first calculate c(y) of all the labels, and dis-count only the label that is the same as learner?stense/aspect, and finally we choose the best outputlabel.
This process leads to an increase of recall.
Wecall this method T-correction.4 Experiments4.1 Data and Feature ExtractionWe used the Lang-8 tense/aspect corpus describedin Section 2.
We randomly selected 100,000 entriesfor training and 1,000 entries for testing.
The test20000.20.40.60.810 0.2 0.4 0.6PR(a) tense00.20.40.60.810 0.2 0.4 0.6PR(b) aspect00.20.40.60.810 0.2 0.4 0.6PR(c) tense/aspectFigure 1: Precision-Recall curve for error detection00.20.40.60.810 0.2 0.4 0.6 PR(a) tense00.20.40.60.810 0.2 0.4 0.6PR(b) aspect00.20.40.60.810 0.2 0.4 0.6PR(c) tense/aspectFigure 2: Precision-Recall curve for error correction0 0.2 4 .6 0 8 10 .2 0.4 0.6 0.8 1 SV M  M AXEN T  CRFdata includes 16,308 verb phrases, of which 1,072(6.6%) contain tense/aspect errors.
We used Stan-ford Parser 1.6.9 8 for generating syntactic featuresand tense/aspect tagging.4.2 ClassifiersBecause we want to know the effect of using globalcontext information with CRF, we trained a one-versus-rest multiclass SVM and a maximum entropyclassifier (MAXENT) as baselines.We built a SVM model with LIBLINEAR 1.89and a CRF and a MAXENT model with CRF++0.54.10 We use the default parameters for eachtoolkit.In every method, we use the same features andfeature described in Section 3, and use T-correctionfor choosing the final output.
The confidence mea-sure of the SVM is the distance to the separating hy-perplane, and that of the MAXENT and the CRF isthe marginal probability of the estimated label.8http://nlp.stanford.edu/software/lex-parser.shtml9http://www.csie.ntu.edu.tw/~cjlin/liblinear/10http://crfpp.sourceforge.net/5 ResultsFigures 1 and 2 show the Precision-Recall curvesof the error detection and correction performance ofeach model.
The figures are grouped by error types:tense, aspect, and both tense and aspect.
All figuresindicate that the CRF model achieves better perfor-mance than SVM and MAXENT.6 AnalysisWe analysed the results of experiments with the ?parameter of the CRF model set to 0.1.
The mostfrequent type of error in the corpus is using simplepresent tense instread of simple past, with 211 in-stances.
Of these our system detected 61 and suc-cessfully corrected 52 instances.
However, of thesecond most frequent error type (using simple pastinstead of simple present), with 94 instances in thecorpus, our system only detected 9 instances.
Onereason why the proposed method achieves high per-formance in the first type of errors is that tense errorswith action verbs written as simple present are rela-tively easy to detect.201ReferencesJohn Bitchener, Stuart Young, and Denise Cameron.2005.
The Effect of Different Types of CorrectiveFeedback on ESL Student Writing.
Journal of SecondLanguage Writing, 14(3):191?205.Chris Brockett, William B. Dolan, and Michael Gamon.2006.
Correcting ESL Errors Using Phrasal SMTTechniques.
In Proceedings of COLING-ACL, pages249?256.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of Grammatical Errors InvolvingPrepositions.
In Proceedings of ACL-SIGSEM, pages25?30.Kevin Knight and Ishwar Chander.
1994.
AutomatedPostediting of Documents.
In Proceedings of theAAAI?94, pages 779?784.John Lafferty.
2001.
Conditional Random Fields: Proba-bilistic Models for Segmenting and Labeling SequenceData.
In Proceedings of ICML, pages 282?289.John Lee and Stephanie Seneff.
2006.
Automatic Gram-mar Correction for Second-Language Learners.
InProceedings of the 9th ICSLP, pages 1978?1981.John Lee and Stephanie Seneff.
2008.
CorrectingMisuseof Verb Forms.
In Proceedings of the 46th ACL:HLT,pages 174?182.Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,and Yuji Matsumoto.
2011.
Mining Revision Log ofLanguage Learning SNS for Automated Japanese Er-ror Correction of Second Language Learners.
In Pro-ceedings of 5th IJCNLP, pages 147?155.Ryo Nagata and Kazuhide Nakatani.
2010.
EvaluatingPerformance of Grammatical Error Detection to Max-imize Learning Effect.
In Proceedings of COLING,pages 894?900.Alla Rozovskaya and Dan Roth.
2011.
Algorithm Selec-tion and Model Adaptation for ESL Correction Tasks.In Proceedings of the 49th ACL:HLT, pages 924?933.202
