Understanding Unsegmented User Utterances in Real-TimeSpoken Dialogue SystemsMikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,Kohji Dohsaka, Takeshi Kawabata*NTT Laboratories3-1 Morinosato-Wakamiya, Atsugi 243-0198, Japannakano @ atom.brl.ntt.co.jp, nmiya @ atom.brl.ntt.co.jp, jun @ idea.brl.ntt.co.jp,dohsaka@ atom.brl.ntt.co.jp, kaw @ nttspch.hil.ntt.co.jpAbstractThis paper proposes a method for incrementally un-derstanding user utterances whose semantic bound-aries are not known and responding in real timeeven before boundaries are determined.
It is anintegrated parsing and discourse processing methodthat updates the partial result of understanding wordby word, enabling responses based on the partialresult.
This method incrementally finds plausiblesequences of utterances that play crucial roles inthe task execution of dialogues, and utilizes beamsearch to deal with the ambiguity of boundaries aswell as syntactic and semantic ambiguities.
The re-sults of a preliminary experiment demonstrate hatthis method understands user utterances better thanan understanding method that assumes pauses to besemantic boundaries.1 IntroductionBuilding a real-time, interactive spoken dialoguesystem has long been a dream of researchers, and therecent progress in hardware technology and speechand language processing technologies i  making thisdream a reality.
It is still hard, however, for com-puters to understand unrestricted human utterancesand respond appropriately to them.
Consideringthe current level of speech recognition technology,system-initiative dialogue systems, which prohibitusers from speaking unrestrictedly, are preferred(Walker et al, 1998).
Nevertheless, we are stillpursuing techniques for understanding unrestricteduser utterances because, if the accuracy of under-standing can be improved, systems that allow usersto speak freely could be developed and these wouldbe more useful than systems that do not.
* Current address: N'I"F Laboratories, 1-1 Hikarino-oka, Yoko-suka 239-0847, JapanMost previous poken dialogue systems (e.g.
sys-tems by Allen et al (1996), Zue et al (1994) andPeckham (1993)) assume that the user makes oneutterance unit in each speechpush-to-talk method is used.unit we mean a phrase fromrepresentation is derived, andsentence in written language.act in this paper to mean ainterval, unless theHere, by utterancewhich a speech actit corresponds to aWe also use speechcommand that up-dates the hearer's belief state about the speaker'sintention and the context of the dialogue.
In thispaper, a system using this assumption is called aninterval-based system.The above assumption o longer holds when norestrictions are placed on the way the user speaks.This is because utterance boundaries (i.e., semanticboundaries) do not always correspond to pausesand techniques based on other acoustic informationare not perfect.
Utterance boundaries thus cannotbe identified prior to parsing, and so the timingof determining parsing results to update the beliefstate is unclear.
On the other hand, responding toa user utterance in real time requires understandingit and updating the belief state in real time; thus,it is impossible to wait for subsequent inputs todetermine boundaries.Abandoning full parsing and adopting keyword-based or fragment-based understanding could pre-vent this problem.
This would, however, sacri-fice the accuracy of understanding because phrasesacross the pauses could not be syntactically ana-lyzed.
There is, therefore, a need for a methodbased on full parsing that enables real-time un-derstanding of user utterances without boundaryinformation.This paper presents incremental significant-utterance-sequence search (ISSS), a method that200enables incremental understanding of user utter-ances word by word by finding plausible sequencesof utterances that play crucial roles in the task ex-ecution of dialogues.
The method utilizes beamsearch to deal with the ambiguity of boundaries aswell as syntactic and semantic ambiguities.
Since itoutputs the partial result of understanding that is themost plausible whenever a word hypothesis i in-putted, the response generation module can produceresponses at any appropriate time.
A comparisonof an experimental spoken dialogue system usingISSS with an interval-based system shows that themethod is effective.2 ProblemA dilemma is addressed in this paper.
First, it is diffi-cult to identify utterance boundaries in spontaneousspeech in real time using only pauses.
Observationof human-human dialogues reveals that humans of-ten put pauses in utterances and sometimes do notput pauses at utterance boundaries.
The followinghuman utterance shows where pauses might appearin an utterance.I'd like to make a reservation for a con-ference room (pause) for, uh (pause) thisafternoon (pause) at about (pause) say(pause) 2 or 3 o'clock (pause) for (pause)15 peopleAs far as Japanese is concerned, several studieshave pointed out that speech intervals in dialoguesare not always well-formed substrings (Seligman etal., 1997; Takezawa nd Morimoto, 1997).On the other hand, since parsing results can-not be obtained unless the end of the utterance isidentified, making real-time responses i  impossi-ble without boundary information.
For example,consider the utterance "I'd like to book MeetingRoom 1 on Wednesday".
It is expected that thesystem should infer the user wants to reserve theroom on 'Wednesday this week' if this utterance wasmade on Monday.
In real conversations, however,there is no guarantee that 'Wednesday' is the finalword of the utterance.
It might be followed by thephrase 'next week', in which case the system madea mistake in inferring the user's intention and mustbacktrack and re-understand.
Thus, it is not possibleto determine the interpretation u less the utteranceboundary is identified.
This problem is more seriousin head-final languages uch as Japanese becausefunction words that represent negation come aftercontent words.
Since there is no explicit clue in-dicating an utterance boundary in unrestricted userutterances, the system cannot make an interpretationand thus cannot respond appropriately.
Waiting fora long pause enables an interpretation, but preventsresponse in real time.
We therefore need a wayto reconcile real-time understanding and analysiswithout boundary clues.3 Previous WorkSeveral techniques have been proposed to segmentuser utterances prior to parsing.
They use into-nation (Wang and Hirschberg, 1992; Traum andHeeman, 1997; Heeman and Allen, 1997) and prob-abilistic language models (Stolcke et al, 1998;Ramaswamy and Kleindienst, 1998; Cettolo andFalavigna, 1998).
Since these methods are notperfect, the resulting segments do not always cor-respond to utterances and might not be parsablebecause of speech recognition errors.
In addition,since the algorithms of the probabilistic methods arenot designed to work in an incremental way, theycannot be used in real-time analysis in a straightfor-ward way.Some methods use keyword detection (Rose,1995; Hatazaki et al, 1994; Seto et al, 1994) andkey-phrase detection (Aust et al, 1995; Kawaharaet al, 1996) to understand speech mainly becausethe speech recognition score is not high enough.The lack of the full use of syntax in these ap-proaches, however, means user utterances might bemisunderstood even if the speech recognition gavethe correct answer.
Zechner and Waibel (1998) andWorm (1998) proposed understanding utterances bycombining partial parses.
Their methods, however,cannot syntactically analyze phrases across pausessince they use speech intervals as input units.
Al-though Lavie et al (1997) proposed a segmentationmethod that combines egmentation prior to parsingand segmentation during parsing, but it suffers fromthe same problem.In the parser proposed by Core and Schubert(1997), utterances interrupted by the other dialogueparticipant are analyzed based on recta-rules.
It isunclear, however, how this parser can be incorpo-201rated into a real-time dialogue system; it seems thatit cannot output analysis results without boundaryclues.4 Incremental Significant-Utterance-Sequence Search Method4.1 OverviewThe above problem can be solved by incremen-tal understanding, which means obtaining the mostplausible interpretation fuser utterances every timea word hypothesis inputted from the speech recog-nizer.
For incremental understanding, we proposeincremental significant-utterance-sequence search(ISSS), which is an integrated parsing and dis-course processing method.
ISSS holds multiplepossible belief states and updates those belief stateswhen a word hypothesis i inputted.
The responsegeneration module produces responses based on themost likely belief state.
The timing of responsesis determined according to the content of the beliefstates and acoustic lues such as pauses.In this paper, to simplify the discussion, we as-sume the speech recognizer incrementally outputselements of the recognized word sequence.
Need-less to say, this is impossible because the most likelyword sequence cannot be found in the midst of therecognition; only networks of word hypotheses canbe outputted.
Our method for incremental process-ing, however, can be easily generalized to deal withincremental network input, and our experimentalsystem utilizes the generalized method.4.2 Significant-Utterance SequenceA significant utterance (SU) in the user's speech isa phrase that plays a crucial role in performing thetask in the dialogue.
An SU may be a full sentenceor a subsentential phrase such as a noun phraseor a verb phrase.
Each SU has a speech act thatcan be considered a command to update the beliefstate.
SU is defined as a syntactic ategory by thegrammar for linguistic processing, which includessemantic inference rules.Any phrases that can change the belief stateshould be defined as SUs.
Two kinds of SUs canbe considered; domain-related ones that expressthe user's intention about the task of the dialogueand dialogue-related ones that express the user'sattitude with respect to the progress of the dia-logue such as confirmation and denial.
Consideringa meeting room reservation system, examples ofdomain-related SUs are "I need to book Room 2 onWednesday", I need to book Room 2", and "Room2" and dialogue-related ones are "yes", "no", and"Okay".User utterances are understood by finding a se-quence of SUs and updating the belief state basedon the sequence.
The utterances in the sequencedo not overlap.
In addition, they do not have tobe adjacent to each other, which leads to robustnessagainst speech recognition errors as in fragment-based understanding (Zechner and Waibel, 1998;Worm, 1998).The belief state can be computed at any pointin time if a significant-utterance sequence for userutterances up to that point in time is given.
Thebelief state holds not only the user's intention butalso the history of system utterances, so that alldiscourse information is stored in it.Consider, for example, the following user speechin a meeting room reservation dialogue.I need to, uh, book Room 2, and it's onWednesday.The most likely significant-utterance sequence con-sists of "I need to, uh, book Room 2" and "it's onWednesday".
From the speech act representation fthese utterances, the system can infer the user wantsto book Room 2 on Wednesday.4.3 Finding Significant-Utterance SequencesSUs are identified in the process of understanding.Unlike ordinary parsers, the understanding mod-ule does not try to determine whether the wholeinput forms an SU or not, but instead determineswhere SUs are.
Although this can be considered akind of partial parsing technique (McDonald, 1992;Lavie, 1996; Abney, 1996), the SUs obtained byISSS are not always subsentential phrases; they aresometimes full sentences.For one discourse, multiple significant-utterancesequences can be considered.
"Wednesday nextweek" above illustrates this well.
Let us assumethat the parser finds two SUs, "Wednesday" and"Wednesday next week".
Then three significant-utterance sequences are possible: one consisting of"Wednesday", one consisting of "Wednesday next202week", and one consisting of no SUs.
The secondsequence is obviously the most likely at this point,but it is not possible to choose only one sequenceand discard the others in the midst of a dialogue.We therefore adopt beam search.
Priorities areassigned to the possible sequences, and those withlow priorities are neglected uring the search.4.4 ISSS AlgorithmThe ISSS algorithm is based on shift-reduce parsing.The basic data structure is context, which representssearch information and is a triplet of the followingdata.stack: A push-down stack used in a shift-reduce parser.belief state: A set of the system's beliefsabout the user's intention with re-spect o the task of the dialogue anddialogue history.priority: A number assigned to the con-text.Accordingly, the algorithm is as follows.
(I) Create a context in which the stack and thebelief state are empty and the priority is zero.
(II) For each input word, perform the followingprocess.1.
Obtain the lexical feature structure forthe word and push it to the stacks of allexisting contexts.2.
For each context, apply rules as in ashift-reduce parser.
When a shift-reduceconflict or a reduce-reduce onflict occur,the context is duplicated and differentoperations are performed on them.
Whena reduce operation is performed, increasethe priority of the context by the priorityassigned to the rule used for the reduceoperation.3.
For each context, if the top of the stackis an SU, empty the stack and update thebelief state according to the content of theSU.
Increase the priority by the square ofthe length (i.e., the number of words) ofthis SU.
(I) SU \[day: ?x\] -~ NP \[sort: day, sem: ?x\](priority: 1)(11) NP\[sort: day\] :~ NP \[sort: day\] NP \[sort: week\](priority: 2)Figure 1: Rules used in the example..
Discard contexts with low priority so thatthe number of remaining contexts will bethe beam width or less.Since this algorithm is based on beam search, itworks in real time if Step (II) is completed quicklyenough, which is the case in our experimental sys-tem.The priorities for contexts are determined usinga general heuristics based on the length of SUs andthe kind of rules used.
Contexts with longer SUs arepreferred.
The reason we do not use the length of anSU, but its square instead, is that the system shouldavoid regarding an SU as consisting of several shortSUs.
Although this heuristics eems rather simple,we have found it works well in our experimentalsystems.Although some additional techniques, such asdiscarding redundant contexts and multiplying aweight w (w > 1) to the priority of each context afterthe Step 4, are effective, details are not discussedhere for lack of space.4.5 Response GenerationThe contexts created by the utterance understandingmodule can also be accessed by the response gener-ation module so that it can produce responses basedon the belief state in the context with the highestpriority at a point in time.
We do not discuss the tim-ing of the responses here, but, generally speaking,a reasonable strategy is to respond when the userpauses.
In Japanese dialogue systems, producing abackchannel is effective when the user's intentionis not clear at that point in time, but determining thecontent of responses in a real-time spoken dialoguesystem is also beyond the scope of this paper.4.6 A Simple ExampleHere we explain ISSS using a simple example.Consider again "Wednesday next week".
To sim-plify the explanation, we assume the noun phrase203InputsWednesday next week time(la) (2a) priority:0stack priority:0 no changes\[ NP(Wednesday) J ''''~'~ (2b) priority: 1belief state( )(2c) ~ priority:2I Iday:Wednesday "~this week j/(3a) priority:0I NP(Wednesday) I NP(next week)( ) (n)(3b) priority:2I NP(next week) I (" (day:Wednesday) ~this weekFigure 2: Execution of ISSS.
(4a) priority:0no changes(4b) priority:2\[ NP(WednesdaYnext w ek) ~ (4b) priority:2no changes ( )(1)(4c) priority:3 (4d) priority:7I I I I(~ay:Wednesdaynext week )(4e) priority:2no changes'next week' is one word.
The speech recognizerincrementally sends to the understanding modulethe word hypotheses 'Wednesday' and 'next week'.The rules used in this example are shown in Figure 1.They are unification-based rules.
Not all featuresand semantic onstraints are shown.
In this exam-ple, nouns and noun phrases are not distinguished.The ISSS execution is shown in Figure 2.When 'Wednesday' is inputted, its lexical featurestructure is created and pushed to the stack.
SinceRule (I) can be applied to this stack, (2b) in Figure 2is created.
The top of the stack in (2b) is an SU, thus(2c) is created, whose belief state contains the user'sintention of meeting room reservation on Wednes-day this week.
We assume that 'Wednesday' meansWednesday this week by default if this utterancewas made on Monday, and this is described in theadditional conditions in Rule (I).
After 'next week'is inputted, NP is pushed to the stacks of all con-texts, resulting in (3a) and (3b).
Then Rule (II) isapplied to (3a), making (4b).
Rule (I) can be appliedto (4b), and then (4c) is created and is turned into(4d), which has the highest priority.Before 'next week' is inputted, the interpretationthat the user wants to book a room on Wednesdaythis week has the highest priority, and then afterthat, the interpretation that the user wants to booka room on Wednesday next week has the highestDialogue ) C  s~,,~ Control ontextUtterance I Response Understanding(ISSS method) GenerationWor  /hypotheses/ ~ i o nI  peec "eco nition I I   eoc  o uction Il \User utterance System utteranceFigure 3: Architecture of the experimental systems.priority.
Thus, by this method, the most plausibleinterpretation can be obtained in an incrementalway.5 ImplementationUsing ISSS, we have developed several experimen-tal Japanese spoken dialogue systems, including ameeting room reservation system.The architecture of the systems is shown in Fig-ure 3.
The speech recognizer uses HMM-basedcontinuous speech recognition directed by a regular204grammar (Noda et al, 1998).
This grammar is weakenough to capture spontaneously spoken utterances,which sometimes include fillers and self-repairs, andallows each speech interval to be an arbitrary num-ber of arbitrary bunsetsu phrases.l The grammarcontains less than one hundred words for each task;we reduced the vocabulary size so that the speechrecognizer could output results in real time.
Thespeech recognizer incrementally outputs word hy-potheses as soon as they are found in the best-scoredpath in the forward search (Hirasawa et al, 1998;G6rz et al, 1996).
Since each word hypothesis isaccompanied by the pointer to its preceding word,the understanding module can reconstruct word se-quences.
The newest word hypothesis determinesthe word sequence that is acoustically most likelyat a point in time.
2The utterance understanding module works basedon ISSS and uses a domain-dependent unificationgrammar with a context-free backbone that is basedon bunsetsu phrases.
This grammar is more re-strictive than the grammar for speech recognition,but covers phenomena peculiar to spoken languagesuch as particle omission and self-repairs.
A be-lief state is represented by a frame (Bobrow etal., 1977); thus, a speech act representation is acommand for changing the slot value of a frame.Although a more sophisticated model would be re-quired for the system to engage in a complicateddialogue, frame representations are sufficient for ourtasks.
The response generation module is invokedwhen the user pauses, and plans responses basedon the belief state of the context with the highestpriority.
The response strategy is similar to thatof previous frame-based dialogue systems (Bobrowet al, 1977).
The speech production module out-puts speech according to orders from the responsegeneration module.Figure 4 shows the transcription of an exampledialogue of a reservation system that was recorded inthe experiment explained below.
As an example ofSUs across pauses, "gozen-jftji kara gozen-jaichijimade (from 10 a.m. to 11 a.m.)" in U5 and U7IA bunsetsu phrase is a phrase that consists of one contentword and a number (possibly zero) of function words.2A method for utilizing word sequences other than the mostlikely one and integrating acoustic scores and ISSS prioritiesremains as future work.SI: donoy6na goy6ken de sh6ka (May I 5.69-7.19help you?
)U2: kaigishitsu no yoyaku o onegaishimasu 7.79-9.66(I'd like to book a meeting room.
)\[hai s~desu gogoyoji made (That's right,to 4 p.m.)\]$3: hal (uh-huh) 10.06-10.32U4: e konshO no suiy6bi (Well, Wednesday 11.75-13.40this week)\[iie konsh~ no suiyObi (No, Wednesdaythis week)\]$5: hal (uh-huh) 14.04-14.31U5: gozen-jfiji kara (from 10 a.m.)\[gozen-jftji kara (from 10 a.m.)\] 15.13-16.30$6: hal (uh-huh) 17.15-17.42U7: gozen-jfiichiji made (to 11 a.m.) 18.00-19.46\[gozen-j~ichiji made (to 11 a.m. )\]$8: hai (uh-huh) 19.83-20.09U9: daisan- (three) 20.54-21.09\[daisan-kaigishitu (Meeting Room 3)\]S10: hal (uh-huh) 21.92-22.19U11: daisan-kaigishitu o onegaishimasu (I'd 21.52-23.59like to book Meeting Room 3)\[failure\]S12: hal (uh-huh) 24.05-24.32U13: yoyaku o onegaishimasu (Please book 25.26-26.52it)\[janiji (12 o 'clock)\]S14: hai (uh-huh) 27.09-27.36UI5: yoyaku shitekudasai (Please book it) 31.72-32.65\[yoyaku shitekudasai (Please book it)\]S16:konsh0 no suiybbi gozen-j0ji kara 33.62-39.04gozen-jOichiji made daisan-kaigi-shitu toyOkotode yoroshT-deshbka(Wednesday this week, from 10 a.m.to 11 a.m., meeting room 3, OK?
)U17: hai (yes) 40.85--41.10\[hai (yes)\]S18: kashikomarimashit& (All right) 41.95--43.00Figure 4: Example dialogue.S means a system utterance and U a user utterance.Recognition results are enclosed in square brackets.
Thefigures in the rightmost column are the start and end times(in seconds) of utterances.was recognized.
Although the SU '~ianiji yoyakushitekudasai (12 o'clock, please book it)" in U13and U15 was syntactically recognized, the systemcould not interpret it well enough to change theframe because of grammar limitations.
The reasonwhy the user hesitated to utter U15 is that S14 wasnot what the user had expected.We conducted a preliminary experiment to in-vestigate how ISSS improves the performance ofspoken dialogue systems.
Two systems were com-205pared: one that uses ISSS (system A), and onethat requires each speech interval to be an SU(an interval-based system, system B).
In system B,when a speech interval was not an SU, the framewas not changed.
The dialogue task was a meet-ing room reservation.
Both systems used the samespeech recognizer and the same grammar.
Therewere ten subjects and each carried out a task on thetwo systems, resulting in twenty dialogues.
Thesubjects were using the systems for the first time.They carried out one practice task with system Bbeforehand.
This experiment was conducted in acomputer terminal room where the machine noisewas somewhat adverse to speech recognition.
Ameaningful discussion on the success rate of utter-ance segmentation is not possible because of therecognition errors due to the small coverage of therecognition grammar.
3All subjects uccessfully completed the task withsystem A in an average of 42.5 seconds, and sixsubjects did so with system B in an average of55.0 seconds.
Four subjects could not completethe task in 90 seconds with system B.
Five subjectscompleted the task with system A 1.4 to 2.2 timesquicker than with system B and one subject com-pleted it with system B one second quicker thanwith system A.
A statistical hypothesis test showedthat times taken to carry out the task with systemA are significantly shorter than those with systemB (Z = 3.77, p < .0001).
4 The order in which thesubjects used the systems had no significant effect.In addition, user impressions of system A weregenerally better than those of system B. Althoughthere were some utterances that the system misun-derstood because of grammar limitations, excludingthe data for the three subjects who had made thoseutterances did not change the statistical results.The reason it took longer to carry out the tasks3About 50% of user speech intervals were not covered bythe recognition grammar due to the small vocabulary size of therecognition grammar.
For the remaining 50% of the intervals,the word error rate of recognition was about 20%.
The worderror rate is defined as 100 * ( substitutions + deletions+ insertions ) / ( correct + substitutions + deletions )(Zechner and Waibel, 1998).4In this test, we used a kind of censored mean which iscomputed by taking the mean of the logarithms of the ratios ofthe times only for the subjects that completed the tasks withboth systems.
The population distribution was estimated by thebootstrap method (Cohen, 1995).with system B is that, compared to system A, theprobability that it understood user utterances wasmuch lower.
This is because the recognition resultsof speech intervals do not always form one SU.About 67% of all recognition results of user speechintervals were SUs or fillers.
5Needless to say, these results depend on the recog-nition grammar, the grammar for understanding, theresponse strategy and other factors.
It has beensuggested, however, that assuming each speech in-terval to be an utterance unit could reduce systemperformance and that ISSS is effective.6 Concluding RemarksThis paper proposed ISSS (incremental significant-utterance-sequence search), an integrated incremen-tal parsing and discourse processing method that en-ables both the understanding of unsegmented userutterances and real-time responses.
This paper alsoreported an experimental result which suggestedthat ISSS is effective.
It is also worthwhile men-tioning that using ISSS enables building spoken di-alogue systems with less effort because it is possibleto define significant utterances without consideringwhere pauses might appear.AcknowledgmentsWe would like to thank Dr. Ken'ichiro Ishii, Dr. NorihiroHagita, and Dr. Kiyoaki Aikawa, and the members of theDialogue Understanding Research Group for their helpfulcomments.
We used the speech recognition engine REXdeveloped by NTI" Cyber Space Laboratories and wouldlike to thank those who helped us use it.
Thanks alsogo to the subjects of the experiment.
Comments by theanonymous reviewers were of great help.ReferencesSteven Abney.
1996.
Partial parsing via finite-state cas-cades.
In Proceedings of the ESSLLI '96 RobustParsing Workshop, pages 8-15.James E Allen, Bradford W. Miller, Eric K. Ringger, andTeresa Sikorski.
1996.
A robust system for naturalspoken dialogue.
In Proceedings of ACL-96, pages62-70.Harald Aust, Martin Oerder, Frank Seide, and VolkerSteinbiss.
1995.
The Philips automatic train timetableinformation system.
Speech Communication, 17:249-262.5Note that 91% of user speech intervals were well-formedsubstrings (not necessary SUs).206Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay,Donald A. Norman, Henry Thompson, and TerryWinograd.
1977.
GUS, a frame driven dialog system.Artificial Intelligence, 8:155-173.Mauro Cettolo and Daniele Falavigna.
1998.
Automaticdetection of semantic boundaries based on acousticand lexical knowledge.
In Proceedings of ICSLP-98,pages 1551-1554.Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
MIT Press.Mark G. Core and Lenhart K. Schubert.
1997.
Handlingspeech repairs and other disruptions through parsermetarules.
In Working Notes of AAA1 Spring Sympo-sium on Computational Models for Mixed InitiativeInteraction, pages 23-29.Gtinther G6rz, Marcus Kesseler, J6rg Spilker, and HansWeber.
1996.
Research on architectures for integratedspeech/language systems in Verbmobil.
In Proceed-ings of COLING-96, pages 484-489.Kaichiro Hatazaki, Farzad Ehsani, Jun Noguchi, andTakao Watanabe.
1994.
Speech dialogue systembased on simultaneous understanding.
Speech Com-munication, 15:323-330.Peter A. Heeman and James F. Allen.
1997.
Into-national boundaries, speech repairs, and discoursemarkers: Modeling spoken dialog.
In Proceedings ofACL/EACL-97.Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, andTakeshi Kawabata.
1998.
Implementation f coordi-native nodding behavior on spoken dialogue systems.In Proceedings oflCSLP-98, pages 2347-2350.Tatsuya Kawahara, Chin-Hui Lee, and Biing-HwangJuang.
1996.
Key-phrase detection and verificationfor flexible speech understanding.
In Proceedings ofICSLP-96, pages 861-864.Alon Lavie, Donna Gates, Noah Coccaro, and Lori Levin.1997.
Input segmentation of spontaneous speech inJANUS: A speech-to-speech translation system.
InElisabeth Maier, Marion Mast, and Susann LuperFoy,editors, Dialogue Processing in Spoken LanguageSystems, pages 86-99.
Springer-Verlag.Alon Lavie.
1996.
GLR* : A Robust Grammar-FocusedParser for Spontaneously Spoken Language.
Ph.D.thesis, School of Computer Science, Carnegie MellonUniversity.David D. McDonald.
1992.
An efficient chart-basedalgorithm for partial-parsing of unrestricted texts.
InProceedings of the Third Conference on Applied Nat-ural Language Processing, pages 193-200.Yoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu Ya-mada, Akihiro Imamura, Satoshi Takahashi, TomokoMatsui, and Kiyoaki Aikawa.
1998.
The developmentof speech recognition engine REX.
In Proceedings ofthe 1998 1EICE General Conference D-14-9, page220.
(in Japanese).Jeremy Peckham.
1993.
A new generation of spokenlanguage systems: Results and lessons from theSUNDIAL project.
In Proceedings of Eurospeech-93, pages 33-40.Ganesh N. Ramaswamy and Jan Kleindienst.
1998.Automatic identification of command boundaries ina conversational natural anguage user interface.
InProceedings of lCSLP-98, pages 401-404.R.
C. Rose.
1995.
Keyword detection in conversationalspeech utterances using hidden Markov model basedcontinuous speech recognition.
Computer Speech andLanguage, 9:309-333.Marc Seligman, Junko Hosaka, and Harald Singer.
1997.
"Pause units" and analysis of spontaneous Japanesedialogues: Preliminary studies.
In Elisabeth Maier,Marion Mast, and Susann LuperFoy, editors, DialogueProcessing in Spoken Language Systems, pages 100-112.
Springer-Verlag.Shigenobu Seto, Hiroshi Kanazawa, Hideaki Shinchi,and Yoichi Takebayashi.
1994.
Spontaneous speechdialogue system TOSBURG-II and its evaluation.Speech Communication, 15:341-353.Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,Mari Ostendorf, Dilek Hakkani, Madelaine Plauche,G6khan Ttir, and Yu Lu.
1998.
Automatic detectionof sentence boundaries and disfluencies based on rec-ognized words.
In Proceedings of ICSLP-98, pages2247-2250.Toshiyuki Takezawa and Tsuyoshi Morimoto.
1997.Dialogue speech recognition method using syntac-tic rules based on subtrees and preterminal bigrams.Systems and Computers inJapan, 28(5):22-32.David R. Traum and Peter A. Heeman.
1997.
Utteranceunits in spoken dialogue.
In Elisabeth Maier, MarionMast, and Susann LuperFoy, editors, Dialogue Pro-cessing in Spoken Language Systems, pages 125-140.Springer-Verlag.Marilyn A. Walker, Jeanne C. Fromer, and ShrikanthNarayanan.
1998.
Learning optimal dialogue strate-gies: A case study of a spoken dialogue agent foremail.
In Proceedings of COLING-A CL'98.Michelle Q. Wang and Julia Hirschberg.
1992.
Auto-matic classification of intonational phrase boundaries.Computer Speech and Language, 6:175-196.Karsten L. Worm.
1998.
A model for robust processingof spontaneous speech by integrating viable fragments.In Proceedings of COLING-ACL'98, pages 1403-1407.Klaus Zechner and Alex Waibel.
1998.
Using chunkbased partial parsing of spontaneous speech in unre-stricted omains for reducing word error ate in speechrecognition.
In Proceedings of COLING-ACL'98,pages 1453-1459.Victor Zue, Stephanie Seneff, Joseph Polifroni, MichaelPhillips, Christine Pao, David Goodine, David God-deau, and James Glass.
1994. PEGASUS: A spo-ken dialogue interface for on-line air travel planning.Speech Communication, 15:331-340.207
