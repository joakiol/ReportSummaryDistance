Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77?82,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsPower of Confidence:How Poll Scores Impact Topic Dynamics in Political DebatesVinodkumar PrabhakaranDept.
of Computer ScienceColumbia UniversityNew York, NYvinod@cs.columbia.eduAshima AroraDept.
of Computer ScienceColumbia UniversityNew York, NYaa3470@columbia.eduOwen RambowCCLSColumbia UniversityNew York, NYrambow@ccls.columbia.eduAbstractIn this paper, we investigate how topic dy-namics during the course of an interactioncorrelate with the power differences be-tween its participants.
We perform thisstudy on the US presidential debates andshow that a candidate?s power, modeledafter their poll scores, affects how oftenhe/she attempts to shift topics and whetherhe/she succeeds.
We ensure the validityof topic shifts by confirming, through asimple but effective method, that the turnsthat shift topics provide substantive topicalcontent to the interaction.1 IntroductionAnalyzing political speech has gathered great in-terest within the NLP community.
Researchershave analyzed political text to identify markers ofpersuasion (Guerini et al., 2008), predict votingpatterns (Thomas et al., 2006; Gerrish and Blei,2011), and detect ideological positions (Sim et al.,2013).
Studies have also looked into how per-sonal attributes of political personalities such ascharisma, confidence and power affect how theyinteract (Rosenberg and Hirschberg, 2009; Prab-hakaran et al., 2013b).
Our work belongs to thisgenre of studies.
We analyze how a presidentialcandidate?s power, modeled after his/her relativepoll standings, affect the dynamics of topic shiftsduring the course of a presidential debate.2 MotivationIn early work on correlating personal attributesto political speech, Rosenberg and Hirschberg(2009) analyzed speech transcripts in the con-text of 2004 Democratic presidential primary elec-tions, to identify prosodic and lexico-syntacticcues that signal charisma of political personalities.More recently, Prabhakaran et al.
(2013a) intro-duced the notion of power an election candidatehas at a certain point in the election campaign,modeled after the confidence that stems from theirrecent poll standings.
They analyzed the 2012 Re-publican presidential primary debates and foundthat the candidate?s power at the time of a de-bate impacts the structure of interactions (e.g., fre-quency of turns and interruption patterns).
Theyfollowed up their study with an automatic rankerto identify leading candidates based on the inter-action within a debate (Prabhakaran et al., 2013b).One of the interesting findings by Prabhakaranet al.
(2013a) was that candidates?
power corre-lates with the distribution of topics they speakabout in the debates.
They found that when can-didates have more power, they speak significantlymore about certain topics (e.g., economy) and lessabout certain other topics (e.g., energy).
However,these findings relate to the specific election cyclethey analyzed and will not carry over to all polit-ical debates in general.
A topical dimension withbroader relevance is how topics change during thecourse of an interaction (e.g., who introduces moretopics, who attempts to shift topics etc.).
For in-stance, Nguyen et al.
(2013) found that topic shiftswithin an interaction are correlated with the rolea participant plays in it (e.g., being a moderator).They also analyzed US presidential debates, butwith the objective of validating a topic segmenta-tion method they proposed earlier (Nguyen et al.,2012).
They do not study the topic shifting ten-dencies among the candidates in relation to theirpower differences.In this paper, we bring these two ideas together.We analyze the 2012 Republican presidential de-bates, modeling the power of a candidate basedon poll scores as proposed by Prabhakaran et al.
(2013a) and investigate various features that cap-ture the topical dynamics in the debates.
We showthat the power affects how often candidates at-77Turn # Speaker Turn Text Substantive?223 PAWLENTY (C) I support a constitutional amendment to define marriage between a man andwoman.
I was the co-author of the state ?
a law in Minnesota to define itand now we have courts jumping over this.
[S]224 KING (M) OK. Let?s just go through this.
[NS]225 PAUL (C) The federal government shouldn?t be involved.
I wouldn?t support anamendment.
[...] I don?t think government should give us a license toget married.
It should be in the church.
[S]226 KING (M) Governor Romney, constitutional amendment or state decision?
[NS]227 ROMNEY (C) Constitutional.
[NS]228 KING (M) Mr. Speaker?
[NS]229 GINGRICH (C) Well, I helped author the Defense of Marriage Act which the Obama ad-ministration should be frankly protecting in court.
[...][S][...]235 CAIN (C) If I had my druthers, I never would have overturned ?don?t ask/don?t tell?in the first place.
[...] Our men and women have too many other things tobe concerned about rather than have to deal with that as a distraction.[S][...
]240 KING (M) Leave it in place, [...] or overturn it?
[S]241 ROMNEY (C) Well, one, we ought to be talking about the economy and jobs.
But giventhe fact you?re insistent, the ?
the answer is, I believe that ?don?t ask/don?ttell?
should have been kept in place until conflict was over.
[S]Table 1: Excerpt from Goffstown, NH debate (06/13/11), discussing marriage equality and the ?Don?t Ask/Don?t Tell?
policy[S]/ [NS] denote substantiveness of turnstempt to shift topics and whether they succeed init or not.
In order to correctly model topic shifts,we ensure that the shifts happen in turns that con-tribute substantial topical content to the interac-tion.
We introduce the notion of a ?non-substantialturn?, and use a simple, but effective method to au-tomatically identify non-substantial turns.
This al-lows us to identify different topic segments withinthe interaction, while permitting (and capturing)interruptions within those segments.
We will com-pare the segments that we obtain with those byNguyen et al.
(2012) in future work.3 Domain and DataWe use the same corpus as Prabhakaran et al.(2013b).
The corpus contains manual transcriptsof 20 debates held between May 2011 and Febru-ary 2012 as part of the 2012 Republican pres-idential primaries.
The transcripts are obtainedfrom The American Presidency Project.1Eachturn is clearly demarcated in the transcripts andtheir speakers are identified.
The turns in the cor-pus are preprocessed using the Stanford CoreNLPpackage to perform basic NLP steps such as tok-enization, sentence segmentation, parts-of-speechtagging and lemmatization.
We show an excerpt1http://www.presidency.ucsb.edu/debates.phpfrom one of the debates in Table 1.
This segmentof the debate discusses marriage equality followedby the overturning of the ?Don?t Ask/Don?t Tell?policy prohibiting openly gay, lesbian, or bisexualpersons from US military service.Prabhakaran et al.
(2013b) added each candi-date?s power at the time of each debate to the cor-pus, computed based on their relative standing inrecent public polls.
We refer the reader to (Prab-hakaran et al., 2013b) for the detailed descriptionof how the relative standings in national and state-level polls from various sources are aggregated toobtain candidates?
power.
The poll numbers cap-ture how successful candidates are in convincingthe electorate of their candidature, which in turnaffects their confidence within the debates.
Thesedebates serve as a rich domain to explore manifes-tations of power since they are a medium throughwhich candidates pursue and maintain power overother candidates.4 Modeling TopicsPrabhakaran et al.
(2013a) model topics in the de-bates using Latent Dirichlet Allocation (LDA), as-signing topic probabilities to each turn.
The num-ber of topics was set to be 15 and the topic that wasassigned the highest probability for a turn was cho-78sen as its topic.
Assigning topics to each turn inthis manner, however, is problematic.
Not all turnsby themselves contribute to the conversational top-ics in an interaction.
A large number of turns,especially by the moderator, manage the conver-sation rather than contribute content to it.
Theseinclude turns redirecting questions to specific can-didates (e.g., turns 224, 226 and 228 in Table 1) aswell as moderator interruptions (e.g., ?Quickly.
?,?We have to save time?).
Furthermore, some otherturns address a topic only when considered to-gether with preceding turns, but not when read inisolation.
These include turns that are short one-word answers (e.g., turn 227) and turns that areuninterpretable without resolving anaphora (e.g.,?That?s right?).
While these turns are substantiveto human readers, topic modeling approaches suchas LDA cannot assign them topics correctly be-cause of their terseness.We define the turns that do not, in isolation, con-tribute substantially to the conversational topics asnon-substantive turns.
In order to obtain a goldstandard for non-substantivity, two of the authorsmanually annotated each turn in one entire debate(dated 06/13/11) as either substantive (S) or non-substantive (NS).
The annotators were instructednot to consider the identity of the speaker or thecontext of the turn (preceding/following turns) inmaking their assessment.
We obtained a highinter-annotator agreement (observed agreement =89.3%; Kappa = .76).
We took the assessmentsby one of the annotators as the gold standard, inwhich 108 (31.5%) of the 343 turns were identi-fied as non-substantive.
We show the S vs. NSassessments for each turn in column 4 of Table 1.Figure 1a shows the line graph of topic proba-bilities assigned by LDA to the sequence of turnsin Table 1.
As the graph shows, non-substantiveturns are assigned spurious topic probabilities byLDA.
For example, turn 224 by KING (?OK.
Letsjust go through this.?)
was assigned small prob-abilities for all topics; the highest of which waseconomy (probability of 0.12).
This error is prob-lematic when modeling topic shifts, since this turnand the next one by PAUL would have been incor-rectly identified as shifts in topic from their cor-responding previous turns.
Instead, if we assumethat the non-substantive turns follow the sametopic probabilities as the most recent substantiveturn, we obtain the line graph shown in Figure 1b.This topic assignment captures the topic dynam-(a) Topic Probabilities assigned by LDA(b) Topic Probabilities after ignoring non-substantive turnsFigure 1: Line graphs of topic probabilities for turns inTable 1 (legend shows only the top 5 topics in this segment)ics in the segment more accurately.
It identifiesGay Rights as the predominant topic until turn 234followed by a mix of Gay Rights and Military astopics while discussing the ?Don?t Ask/Don?t Tell?policy.
It also captures the attempt by ROMNEYin turn 242 to shift the topic to Economy.4.1 Identifying Non-substantive TurnsIn order to automatically detect non-substantiveturns, we investigate a few alternatives.
A simpleobservation is that many of the NS turns such asredirections of questions or short responses haveonly a few words.
We tried a word count thresh-old based method (WC Thresh) where we assigna turn to be NS if the number of tokens (words) inthe turn is less than a threshold.
Another intuitionis that for a non-substantive turn, it would be hardfor the LDA to assign topics and hence all topicswill get almost equal probabilities assigned.
In or-der to capture this, we used a method based on astandard deviation threshold (SD Thresh), wherewe assign a turn to be NS if the standard deviationof that turn?s topic probabilities is below a thresh-old.
We also used a combination system wherewe tag a turn to be NS if either system tags it tobe.
We tuned for the value of the thresholds andthe best performances obtained for each case areshown in Table 2.
We obtained the best resultsfor the WC Thresh method with a threshold of 28words, while for SD Thresh the optimal thresholdis .13 (almost twice the mean).79Method Accuracy (%) F-measureWC Thresh 82.6 73.7SD Thresh 76.2 64.7WC Thresh + SD Thresh 76.8 70.4Table 2: Accuracy and F-measure of different methods toidentify non-substantive turns4.2 Topic AssignmentsWe first ran the LDA at a turn-level for all debates,keeping the number of topics to be 15, and se-lected the best model after 2000 iterations.
Then,we ran the WC Thresh method described above todetect NS turns.
For all NS turns, we replace thetopic probabilities assigned by LDA with the lastsubstantive turn?s topic probabilities.
Note that anS turn coming after one or more NS turns couldstill be of the same topic as the last S turn, i.e.,non-substantivity of a turn is agnostic to whetherthe topic changes after that or not.
A topic shift (orattempt) happens only when LDA assigns a differ-ent topic to a substantive turn.5 Topical DimensionsWe now describe various features we use to cap-ture the topical dynamics within each debate, withrespect to each candidate.
When we compute afeature value, we use the topic probabilities as-signed to each turn as described in the previoussection.
For some features we only use the topicwith the highest probability, while for some oth-ers, we use the probabilities assigned to all topics.We consider features along four dimensions whichwe describe in detail below.5.1 Topic Shift PatternsWe build various features to capture how of-ten a candidate stays on the topic being dis-cussed.
We say a candidate attempted to shiftthe topic in a turn if the topic assigned to thatturn differs from the topic of the previous (sub-stantive) turn.
We use a feature to count thenumber of times a candidate attempts to shifttopics within a debate (TS Attempt#) and aversion of that feature normalized over the to-tal number of turns (TS Attempt#N).
We alsouse a variation of these features which consid-ers only the instances of topic shift attempts bythe candidates when responding to a questionfrom the moderator (TS AttemptAfterMod# andTS AttemptAfterMod#N).
We also compute asofter notion of topic shift where we measure theaverage Euclidean distance between topic proba-bilities of each of the candidate turns and turnsprior to them (EuclideanDist).
This feature inessence captures whether the candidate stayed ontopic, even if he/she did not completely switchtopics in a turn.5.2 Topic Shift Sustenance PatternsWe use a feature to capture the average numberof turns for which topic shifts by a candidate wassustained (TS SustTurns).
However, as discussedin Section 4, the turns vary greatly in terms oflength.
A more sensible measure is the time pe-riod for which a topic shift was sustained.
Weapproximate the time by the number of word to-kens and compute the average number of tokensin the turns that topic shifts by a candidate weresustained (TS SustTime).5.3 Topic Shift Success PatternsWe define a topic shift to be successful if it wassustained for at least three turns.
We computethree features ?
total number of successful topicshifts by a candidate (TS Success#), that numbernormalized over the total number of turns by thecandidate (TS Success#N), and the success rate ofcandidate?s topic shifts (TS SuccessRate)5.4 Topic Introduction PatternsWe also looked at cases where a candidate intro-duces a new topic, i.e., shifts to a topic whichis entirely new for the debate.
We use the num-ber of topics introduced by a candidate as a fea-ture (TS Intro#).
We also use features to cap-ture how important those topics were, measuredin terms of the number of turns about those top-ics in the en tire debate (TS IntroImpTurns) andthe time spent on those topics in the entire debate(TS IntroImpTime).6 Analysis and ResultsWe performed a correlation analysis on the fea-tures described in the previous section with re-spect to each candidate against the power he/shehad at the time of the debate (based on recent pollscores).
Figure 2 shows the Pearson?s product cor-relation between each topical feature and candi-date?s power.
Dark bars denote statistically signif-icant (p < 0.05) features.80Figure 2: Pearson Correlations for Topical FeaturesWe obtained significant strong positive correla-tion for TS Attempt# and TS AttemptAfterMod#.However, the normalized measure TS Attempt#Ndid not have any significant correlation, suggest-ing that the correlation obtained for TS Attempt#is mostly due to the fact that candidates withmore power have more turns, a finding that is al-ready established by Prabhakaran et al.
(2013b).However, interestingly, we obtained a weak,but statistically significant, negative correlationfor TS AttemptAfterMod#Nwhich suggests thatmore powerful candidates tend to stay on topicwhen responding to moderators.
We did not ob-tain any correlation for EuclideanDist.We did not obtain any significant correlationsbetween candidate?s power and their topic shiftsustenance features.
We obtained significant cor-relation for topic shift success (TS Success#),modeled based on the sustenance of topic shifts,suggesting that powerful candidates have a highernumber of successful topic shifts.
However,TS SuccessRate or TS Success#Ndid not obtainany significant correlation.
We also found thatpowerful candidates are more likely to introducenew topics (TS Intro#) and that the topics they in-troduce tend to be important (TS IntroImpTurnsand TS IntroImpTime).7 Related WorkStudies in sociolinguistics (e.g., (Ng et al., 1993;Ng et al., 1995)) have explored how dialog struc-ture in interactions relates to power and influence.Reid and Ng (2000) identified that factors such asfrequency of contribution, proportion of turns, andnumber of successful interruptions are importantindicators of influence.
Within the dialog commu-nity, researchers have studied notions of controland initiative in dialogs (Walker and Whittaker,1990; Jordan and Di Eugenio, 1997).
Walker andWhittaker (1990) define ?control of communica-tion?
in terms of whether the discourse partici-pants are providing new, unsolicited informationin their utterances.
Their notion of control dif-fers from our notion of power; however, the waywe model topic shifts is closely related to theirutterance level control assignment.
Within theNLP community, researchers have studied powerand influence in various genres of interactions,such as organizational email threads (Bramsen etal., 2011; Gilbert, 2012; Prabhakaran and Ram-bow, 2013), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012)and online chat dialogs (Strzalkowski et al., 2012).The correlates analyzed in these studies rangefrom word/phrase patterns, to derivatives of suchpatterns such as linguistic coordination, to deeperdialogic features such as argumentation and dialogacts.
Our work differs from these studies in thatwe study the correlates of power in topic dynam-ics.
Furthermore, we analyze spoken interactions.8 ConclusionWe studied the topical dynamics in the 2012 USpresidential debates and investigated their corre-lation with the power differences between candi-dates.
We showed that a candidate?s power, mod-eled after their poll scores, has significant correla-tion with how often he/she introduces new topics,attempts to shift topics, and whether they succeedin doing so.
In order to ensure the validity of ourtopic shifts we devised a simple yet effective wayto eliminate turns which do not provide substan-tial topical content to the interaction.
Furthermore,this allowed us to identify different topic segmentswithin the interaction.
In future work, we will ex-plore how our way of identifying segments com-pares to other approaches on topic segmentationin interactions (e.g., (Nguyen et al., 2012)).AcknowledgmentsThis paper is based upon work supported by theDARPA DEFT Program.
The views expressed arethose of the authors and do not reflect the officialpolicy or position of the Department of Defenseor the U.S. Government.
We also thank DebanjanGhosh and several anonymous reviewers for theirconstructive feedback.81ReferencesOr Biran, Sara Rosenthal, Jacob Andreas, KathleenMcKeown, and Owen Rambow.
2012.
Detectinginfluencers in written online conversations.
In Pro-ceedings of the Second Workshop on Language inSocial Media, pages 37?45, Montr?eal, Canada, June.Association for Computational Linguistics.Philip Bramsen, Martha Escobar-Molano, Ami Patel,and Rafael Alonso.
2011.
Extracting social powerrelationships from natural language.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 773?782, Portland, Oregon, USA,June.
Association for Computational Linguistics.Cristian Danescu-Niculescu-Mizil, Lillian Lee,Bo Pang, and Jon Kleinberg.
2012.
Echoes ofpower: language effects and power differences insocial interaction.
In Proceedings of the 21st in-ternational conference on World Wide Web, WWW?12, New York, NY, USA.
ACM.Sean Gerrish and David Blei.
2011.
Predicting legisla-tive roll calls from text.
In Lise Getoor and TobiasScheffer, editors, Proceedings of the 28th Interna-tional Conference on Machine Learning, ICML ?11,pages 489?496, New York, NY, USA, June.
ACM.Eric Gilbert.
2012.
Phrases that signal workplace hier-archy.
In Proceedings of the ACM 2012 conferenceon Computer Supported Cooperative Work, CSCW?12, pages 1037?1046, New York, NY, USA.
ACM.Marco Guerini, Carlo Strapparava, and Oliviero Stock.2008.
Corps: A corpus of tagged political speechesfor persuasive communication processing.
Journalof Information Technology & Politics, 5(1):19?32.Pamela W. Jordan and Barbara Di Eugenio.
1997.Control and initiative in collaborative problem solv-ing dialogues.
In Working Notes of the AAAI SpringSymposium on Computational Models for Mixed Ini-tiative, pages 81?84.Sik Hung Ng, Dean Bell, and Mark Brooke.
1993.Gaining turns and achieving high in influence rank-ing in small conversational groups.
British Journalof Social Psychology, pages 32, 265?275.Sik Hung Ng, Mark Brooke, and Michael Dunne.1995.
Interruption and in influence in discussiongroups.
Journal of Language and Social Psychol-ogy, pages 14(4),369?381.Viet-An Nguyen, Jordan Boyd-Graber, and PhilipResnik.
2012.
Sits: A hierarchical nonparametricmodel using speaker identity for topic segmentationin multiparty conversations.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),pages 78?87, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,Deborah A. Cai, Jennifer E. Midberry, and YuanxinWang.
2013.
Modeling topic control to detect in-fluence in conversations using nonparametric topicmodels.
Machine Learning, pages 1?41.Vinodkumar Prabhakaran and Owen Rambow.
2013.Written dialog and social power: Manifestations ofdifferent types of power in dialog behavior.
In Pro-ceedings of the IJCNLP, pages 216?224, Nagoya,Japan, October.
Asian Federation of Natural Lan-guage Processing.Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.Seligmann.
2013a.
Power dynamics in spoken in-teractions: a case study on 2012 republican primarydebates.
In Proceedings of the 22nd internationalconference on World Wide Web companion, pages99?100.
International World Wide Web ConferencesSteering Committee.Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.Seligmann.
2013b.
Who had the upper hand?
rank-ing participants of interactions based on their rela-tive power.
In Proceedings of the IJCNLP, pages365?373, Nagoya, Japan, October.
Asian Federationof Natural Language Processing.Scott A. Reid and Sik Hung Ng.
2000.
Conversation asa resource for in influence: evidence for prototypicalarguments and social identification processes.
Euro-pean Journal of Social Psych., pages 30, 83?100.Andrew Rosenberg and Julia Hirschberg.
2009.Charisma perception from text and speech.
SpeechCommunication, 51(7):640?655.Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, andNoah A. Smith.
2013.
Measuring ideological pro-portions in political speeches.
In Proceedings of the2013 Conference on EMNLP, pages 91?101, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Tomek Strzalkowski, Samira Shaikh, Ting Liu,George Aaron Broadwell, Jenny Stromer-Galley,Sarah Taylor, Umit Boz, Veena Ravishankar, andXiaoai Ren.
2012.
Modeling leadership and influ-ence in multi-party online discourse.
In Proceedingsof COLING, pages 2535?2552, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 327?335,Sydney, Australia, July.
Association for Computa-tional Linguistics.Marilyn Walker and Steve Whittaker.
1990.
Mixed ini-tiative in dialogue: An investigation into discoursesegmentation.
In Proceedings of the 28th annualmeeting on Association for Computational Linguis-tics, pages 70?78.
Association for ComputationalLinguistics.82
