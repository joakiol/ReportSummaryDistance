Exploiting Context for Biomedical Entity Recognition:From Syntax to the WebJenny Finkel,* Shipra Dingare,?
Huy Nguyen,*Malvina Nissim,?
Christopher Manning,* and Gail Sinclair?
*Department of Computer ScienceStanford UniversityStanford, CA 93405-9040United States{jrfinkel|htnguyen|manning}@cs.stanford.edu?Institute for Communicating andCollaborative SystemsUniversity of EdinburghEdinburgh EH8 9LWUnited Kingdom{sdingar1|mnissim|csincla1}@inf.ed.ac.ukAbstractWe describe a machine learning system for therecognition of names in biomedical texts.
The sys-tem makes extensive use of local and syntactic fea-tures within the text, as well as external resourcesincluding the web and gazetteers.
It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLPshared task of identifying five biomedical named en-tities in the GENIA corpus.1 IntroductionThe explosion of information in the fields of molec-ular biology and genetics has provided a uniqueopportunity for natural language processing tech-niques to aid researchers and curators of databasesin the biomedical field by providing text miningservices.
Yet typical natural language processingtasks such as named entity recognition, informa-tion extraction, and word sense disambiguation areparticularly challenging in the biomedical domainwith its highly complex and idiosyncratic language.With the increasing use of shared tasks and sharedevaluation procedures (e.g., the recent BioCreative,TREC, and KDD Cup), it is rapidly becoming clearthat performance in this domain is markedly lowerthan the field has come to expect from the standarddomain of newswire.
The Coling 2004 shared taskfocuses on the problem of Named Entity Recogni-tion, requiring participating systems to identify thefive named entities of protein, RNA, DNA, cell line,and cell type in the GENIA corpus of MEDLINEabstracts (Ohta et al, 2002).
In this paper we de-scribe a machine learning system incorporating a di-verse set of features and various external resourcesto accomplish this task.
We describe our system indetail and also discuss some sources of error.2 System DescriptionOur system is a Maximum Entropy Markov Model,which further develops a system earlier used for theCoNLL 2003 shared task (Klein et al, 2003) and the2004 BioCreative critical assessment of informationextraction systems, a task that involved identifyinggene and protein name mentions but not distinguish-ing between them (Dingare et al, 2004).
Unlikethe above two tasks, many of the entities in the cur-rent task do not have good internal cues for distin-guishing the class of entity: various systematic pol-ysemies and the widespread use of acronyms meanthat internal cues are lacking.
The challenge wasthus to make better use of contextual features, in-cluding local and syntactic features, and external re-sources in order to succeed at this task.2.1 Local FeaturesWe used a variety of features describing the imme-diate content and context of each word, includingthe word itself, the previous and next words, wordprefixes and suffix of up to a length of 6 characters,word shapes, and features describing the named en-tity tags assigned to the previous words.
Wordshapes refer to a mapping of each word onto equiva-lence classes that encodes attributes such as length,capitalization, numerals, greek letters, and so on.For instance, ?Varicella-zoster?
would become Xx-xxx, ?mRNA?
would become xXXX, and ?CPA1?would become XXXd.
We also incorporated part-of-speech tagging, using the TnT tagger(Brants, 2000)retrained on the GENIA corpus gold standard part-of-speech tagging.
We also used various interactionterms (conjunctions) of these base-level features invarious ways.
The full set of local features is out-lined in Table 1.2.2 External ResourcesWe made use of a number of external resources, in-cluding gazetteers, web-querying, use of the sur-rounding abstract, and frequency counts from theBritish National Corpus.88Word Features wi, wi?1, wi+1Disjunction of 5 prev wordsDisjunction of 5 next wordsTnT POS POSi, POSi?1, POSi+1Prefix/suffix Up to a length of 6Abbreviations abbriabbri?1 + abbriabbri + abbri+1abbri?1 + abbri + abbri+1Word Shape shapei, shapei?1, shapei+1shapei?1 + shapeishapei + shapei+1shapei?1 + shapei + shapei+1Prev NE NEi?1, NEi?2 + NEi?1NEi?3 + NEi?2 + NEi?1Prev NE + Word NEi?1 + wiPrev NE + POS NEi?1 + POSi?1 + POSiNEi?2 + NEi?1 + POSi?2 +POSi?1 + POSiPrev NE + Shape NEi?1 + shapeiNEi?1 + shapei+1NEi?1 + shapei?1 + shapeiNEi?2 + NEi?1 + shapei?2 +shapei?1 + shapeiParen-Matching Signals when one parenthesisin a pair has been assigned adifferent tag than the other in awindow of 4 wordsTable 1: Local Features (+ indicates conjunction)2.2.1 FrequencyMany entries in gazetteers are ambiguous words,occasionally used in the sense that the gazetteerseeks to represent, but at least as frequently not.So while the information that a token was seen ina gazetteer is an unreliable indicator of whether itis an entity, less frequent words are less likely to beambiguous than more frequent ones.
Additionally,more frequent words are likely to have been seenoften in the training data and the system should bebetter at classifying them, while less frequent wordsare a common source of error and their classifica-tion is more likely to benefit from the use of externalresources.
We assigned each word in the trainingand testing data a frequency category correspond-ing to its frequency in the British National Corpus,a 100 million word balanced corpus, and used con-junctions of this category and certain other features.2.2.2 GazetteersOur gazetteer contained only gene names and wascompiled from lists from biomedical websites (suchas LocusLink) as well as from the Gene Ontol-ogy and the data provided for the BioCreative 2004tasks.
The final gazetteer contained 1,731,496 en-tries.
Because it contained only gene names, and forthe reasons discussed earlier, we suspect that it wasnot terribly useful for identifying the presences ofentities, but rather that it mainly helped to establishthe exact beginning and ending point of multi-wordentities recognized mainly through other features.2.2.3 WebFor each of the named entity classes, we built in-dicative contexts, such as ?X mRNA?
for RNA, or?X ligation?
for protein.
For each entity X whichhad a frequency lower than 10 in the British Na-tional Corpus, we submitted instantiations of eachpattern to the web, using the Google API, and ob-tained the number of hits.
The pattern that returnedthe highest number of hits determined the featurevalue (e.g., ?web-protein?, or ?web-RNA?).
If nohits were returned by any pattern, a value ?O-web?was assigned.
This value was also assigned to allwords whose frequency was higher than 10 (usingyet another value for words with higher frequencydid not improve the tagger?s performance).2.2.4 AbstractsA number of NER systems have made effective useof how the same token was tagged in different partsof the same document (see (Curran and Clark, 2003)and (Mikheev et al, 1999)).
A token which appearsin an unindicative context in one sentence may ap-pear in a very obvious context in another sentencein the same abstract.
To leverage this we taggedeach abstract twice, providing for each token a fea-ture indicating whether it was tagged as an entityelsewhere in the abstract.
This information wasonly useful when combined with information on fre-quency.2.3 Deeper Syntactic FeaturesWhile the local features discussed earlier are allfairly surface level, our system also makes use ofdeeper syntactic features.
We fully parsed the train-ing and testing data using the Stanford Parser of(Klein and Manning, 2003) operating on the TnTpart-of-speech tagging ?
we believe that the un-lexicalized nature of this parser makes it a partic-ularly suitable statistical parser to use when thereis a large domain mismatch between the trainingmaterial (Wall Street Journal text) and the targetdomain, but have not yet carefully evaluated this.Then, for each word in the sentence which is in-side a noun phrase, the head and governor of thenoun phrase are extracted.
These features are notvery useful when identifying only two classes (suchas GENE and OTHER in the BioCreative task), butthey were quite useful for this task because of thelarge number of classes which the system needed todistinguish between.
Because the classifier is now89choosing between classes where members can lookvery similar, longer distance information can pro-vide a better representation of the context in whichthe word appears.
For instance, the word phospho-rylation occurs in the training corpus 492 times, 482of which it is was classified as other.
However,it is the governor of 738 words, of which 443 areprotein, 292 are other and only 3 are cellline.We also made use of abbreviation matching tohelp ensure consistency of labels.
Abbreviationsand long forms were extracted from the data usingthe method of (Schwartz and Hearst, 2003).
Thisdata was combined with a list of other abbreviationsand long forms extracted from the BioCreative 2004task.
Then all occurrences of either the long or shortforms in the data was labeled.
These labels were in-cluded in the system as features and helped to im-prove boundary detection.2.4 Adjacent EntitiesWhen training our classifier, we merged the B- andI- labels for each class, so it did not learn how todifferentiate between the first word of a class andinternal word.
There were several motivations fordoing this.
Foremost was memory concerns; our fi-nal system trained on just the six classes had 1.5million features ?
we just did not have the resourcesto train it over more classes without giving up manyof our features.
Our second motivation was that bymerging the beginning and internal labels for a par-ticular class, the classifier would see more examplesof that class and learn better how to identify it.
Thedrawback of this move is that when two entities be-longing to the same class are adjacent, our classifierwill automatically merge them into one entity.
Wedid attempt to split them back up using NP chunks,but this severely reduced performance.3 Results and DiscussionOur results on the evaluation data and a confusionmatrix are shown in Tables 2 and 4.
Table 4 sug-gests areas for further work.
Collapsing the B- andI- tags does cost us quite a bit.
Otherwise confusionsbetween some named entity and being nothing aremost of the errors, although protein/DNA and cell-line/cell-type confusions are also noticeable.Analysis of performance in biomedical NamedEntity Recognition tends to be dominated by theperceived poorness of the results, stemming fromthe twin beliefs that performance of roughly ninetypercent is the state-of-the-art and that performanceof 100% (or close to that) is possible and the goalto be aimed for.
Both of these beliefs are ques-tionable, as the top MUC 7 performance of 93.39%Entity Precision Recall F-ScoreFully Correctprotein 77.40% 68.48% 72.67%DNA 66.19% 69.62% 67.86%RNA 72.03% 65.89% 68.83%cell line 59.00% 47.12% 52.40%cell type 62.62% 76.97% 69.06%Overall 71.62% 68.56% 70.06%Left Boundary Correctprotein 82.89% 73.34% 77.82%DNA 68.47% 72.01% 70.19%RNA 75.42% 68.99% 72.06%cell line 63.80% 50.96% 56.66%cell type 63.93% 78.57% 70.49%Overall 75.72% 72.48% 74.07%Right Boundary Correctprotein 84.70% 74.96% 79.53%DNA 74.43% 78.29% 76.31%RNA 78.81% 72.09% 75.30%cell line 70.2% 56.07% 62.34%cell type 71.68% 88.10% 79.05%Overall 79.65% 76.24% 77.91%Table 2: Results on the evaluation data(Mikheev et al, 1998) in the domain of newswiretext used an easier performance metric where incor-rect boundaries were given partial credit, while boththe biomedical NER shared tasks to date have usedan exact match criterion where one is doubly penal-ized (both as a FP and as a FN) for incorrect bound-aries.
However, the difference in metric clearly can-not account entirely for the performance discrep-ancy between newswire NER and biomedical NER.Biomedical NER appears to be a harder task dueto the widespread ambiguity of terms out of con-text, the complexity of medical language, and theapparent need for expert domain knowledge.
Theseare problems that more sophisticated machine learn-ing systems using resources such as ontologies anddeep processing might be able to overcome.
How-ever, one should also consider the inherent ?fuzzi-ness?
of the classification task.
The few existingstudies of inter-annotator agreement for biomedi-cal named entities have measured agreement be-tween 87%(Hirschman, 2003) and 89%(Demetriousand Gaizauskas, 2003).
As far as we know thereare no inter-annotator agreement results for the GE-NIA corpus, and it is necessary to have such resultsbefore properly evaluating the performance of sys-tems.
In particular, the fact that BioNLP sought todistinguish between gene and protein names, whenthese are known to be systematically ambiguous,and when in fact in the GENIA corpus many enti-ties were doubly classified as ?protein molecule or90DNA RNA cell line cell type proteingold\ans B- I- B- I- B- I- B- I- B- I- OB-DNA 723 39 0 0 1 0 0 0 154 1 138I-DNA 52 1390 0 0 0 0 0 0 19 71 257B-RNA 1 0 89 3 0 0 0 0 14 0 11I-RNA 0 1 5 164 0 0 0 0 2 0 15B-cell line 3 0 0 0 319 41 37 5 12 1 82I-cell line 0 6 0 0 24 713 5 104 0 14 123B-cell type 1 0 0 0 164 22 1228 90 31 5 380I-cell type 0 0 0 0 13 383 88 2101 8 27 371B-protein 48 5 10 3 20 1 19 3 4200 192 566I-protein 6 66 0 11 0 10 2 25 245 3630 779O 170 240 25 26 85 142 184 132 1042 656 78945Table 3: Our confusion matrix over the evaluation datahuman B-cell typemonocytes I-cell typehuman Omonocytes B-cell typemacrophages B-cell typeprimary B-cell type JJT I-cell type NNlymphocytes I-cell type NNSprimary O JJperipheral B-cell type JJblood I-cell type NNlymphocytes I-cell type NNSTable 4: Examples of annotation inconsistenciesregion?
and ?DNA molecule or region?, suggeststhat inter-annotator agreement could be low, andthat many entities in fact have more than one classi-fication.One area where GENIA appears inconsistent isin the labeling of preceding adjectives.
The datawas selected by querying for the term human, yetthe term is labeled inconsistently, as is shown in Ta-ble 4.
Of the 1790 times the term human occurredbefore or at the beginning of an entity in the train-ing data, it was not classified as part of the entity110 times.
In the test data, there is only on instance(out of 130) where the term is excluded.
Adjectivesare excluded approximately 25% of the time in boththe training and evaluation data.
There are also in-consistencies when two entities are separated by theword and.4 AcknowledgementsThis paper is based on work supported in part by aScottish Enterprise Edinburgh-Stanford Link Grant(R36759), as part of the SEER project, and in partthe National Science Foundation under the Knowl-edge Discovery and Dissemination program.ReferencesThorsten Brants.
2000.
TnT ?
a statistical part-of-speechtagger.
In ANLP 6, pages 224?231.James R. Curran and Stephen Clark.
2003.
Languageindependent NER using a maximum entropy tagger.In Proceedings of the Seventh Conference on NaturalLanguage Learning (CoNLL-03), pages 164?167.George Demetrious and Rob Gaizauskas.
2003.
Corpusresources for development and evaluation of a biolog-ical text mining system.
In Proceedings of the ThirdMeeting of the Special Interest Group on Text Mining,Brisbane, Australia, July.Shipra Dingare, Jenny Rose Finkel, Christopher Man-ning, Malvina Nissim, and Beatrice Alex.
2004.
Ex-ploring the boundaries: Gene and protein identifica-tion in biomedical text.
In Proceedings of the BioCre-ative Workshop.Lynette Hirschman.
2003.
Using biological resources tobootstrap text mining.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL 41, pages 423?430.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In CoNLL 7, pages 180?183.Andrei Mikheev, Claire Grover, and Mark Moens.
1998.Description of the LTG system used for MUC-7.
InProceedings of MUC-7.Andrei Mikheev, Marc Moens, and Claire Grover.
1999.Named entity recognition without gazetteers.
In Pro-ceedings of the ninth conference on European chap-ter of the Association for Computational Linguistics,pages 1?8.
Association for Computational Linguis-tics.Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichiTsujii.
2002.
GENIA corpus: an annotated researchabstract corpus in molecular biology domain.
In Pro-ceedings of he Human Language Technology Confer-ence, pages 73?77.Ariel Schwartz and Marti Hearst.
2003.
A simple al-gorithm for identifying abbreviation definitions inbiomedical text.
In Pacific Symposium on Biocomput-ing, Kauai, Jan.91
