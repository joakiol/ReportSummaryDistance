Proceedings of the Second Workshop on Metaphor in NLP, pages 11?17,Baltimore, MD, USA, 26 June 2014.c?2014 Association for Computational LinguisticsDifferent Texts, Same Metaphors: Unigrams and BeyondBeata Beigman Klebanov, Chee Wee Leong, Michael Heilman, Michael FlorEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541{bbeigmanklebanov,cleong,mheilman,mflor}@ets.orgAbstractCurrent approaches to supervised learningof metaphor tend to use sophisticated fea-tures and restrict their attention to con-structions and contexts where these fea-tures apply.
In this paper, we describe thedevelopment of a supervised learning sys-tem to classify all content words in a run-ning text as either being used metaphori-cally or not.
We start by examining theperformance of a simple unigram baselinethat achieves surprisingly good results forsome of the datasets.
We then show howthe recall of the system can be improvedover this strong baseline.1 IntroductionCurrent approaches to supervised learning ofmetaphor tend to (a) use sophisticated featuresbased on theories of metaphor, (b) apply to cer-tain selected constructions, like adj-noun or verb-object pairs, and (c) concentrate on metaphorsof certain kind, such as metaphors about gover-nance or about the mind.
In this paper, we de-scribe the development of a supervised machinelearning system to classify all content words in arunning text as either being used metaphoricallyor not ?
a task not yet addressed in the literature,to our knowledge.
This approach would enable,for example, quantification of the extent to whicha given text uses metaphor, or the extent to whichtwo different texts use similar metaphors.
Both ofthese questions are important in our target appli-cation ?
scoring texts (in our case, essays writtenfor a test) for various aspects of effective use oflanguage, one of them being the use of metaphor.We start by examining the performance of asimple unigram baseline that achieves surprisinglygood results for some of the datasets.
We thenshow how the recall of the system can be improvedover this strong baseline.2 DataWe use two datasets that feature full text anno-tations of metaphors: A set of essays written fora large-scale assessment of college graduates andthe VUAmsterdam corpus (Steen et al., 2010),1containing articles from four genres sampled fromthe BNC.
Table 1 shows the sizes of the six sets,as well as the proportion of metaphors in them; thefollowing sections explain their composition.Data #Texts #NVAR #metaphorstokens (%)News 49 18,519 3,405 (18%)Fiction 11 17,836 2,497 (14%)Academic 12 29,469 3,689 (13%)Conversation 18 15,667 1,149 ( 7%)Essay Set A 85 21,838 2,368 (11%)Essay Set B 79 22,662 2,745 (12%)Table 1: Datasets used in this study.
NVAR =Nouns, Verbs, Adjectives, Adverbs, as tagged bythe Stanford POS tagger (Toutanova et al., 2003).2.1 VUAmsterdam DataThe dataset consists of 117 fragments sampledacross four genres: Academic, News, Conversa-tion, and Fiction.
Each genre is represented by ap-proximately the same number of tokens, althoughthe number of texts differs greatly, where the newsarchive has the largest number of texts.We randomly sampled 23% of the texts fromeach genre to set aside for a blind test to be carriedout at a later date with a more advanced system;the current experiments are performed using cross-validation on the remaining 90 fragments: 10-foldon News, 9-fold on Conversation, 11 on Fiction,and 12 on Academic.
All instances from the sametext were always placed in the same fold.1http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html11The data is annotated using MIP-VU proce-dure.
It is based on the MIP procedure (Prag-glejaz, 2007), extending it to handle metaphori-city through reference (such as marking did as ametaphor in As the weather broke up, so did theirfriendship) and allow for explicit coding of diffi-cult cases where a group of annotators could notarrive at a consensus.
The tagset is rich and isorganized hierarchically, detecting various typesof metaphors, words that flag the presense ofmetaphors, etc.
In this paper, we consider only thetop-level partition, labeling all content words withthe tag ?function=mrw?
(metaphor-related word)as metaphors, while all other content words are la-beled as non-metaphors.22.2 Essay DataThe dataset consists of 224 essays written for ahigh-stakes large-scale assessment of analyticalwriting taken by college graduates aspiring to en-ter a graduate school in the United States.
Out ofthese, 80 were set aside for future experiments andnot used for this paper.
Of the remaining essays,85 essays discuss the statement ?High-speed elec-tronic communications media, such as electronicmail and television, tend to prevent meaningfuland thoughtful communication?
(Set A), and 79discuss the statement ?In the age of television,reading books is not as important as it once was.People can learn as much by watching televisionas they can by reading books.?
(Set B).
Multipleessays on the same topic is a unique feature of thisdataset, allowing the examination of the effect oftopic on performance, by comparing performancein within-topic and across-topic settings.The essays were annotated using a protocolthat prefers a reader?s intuition over a formal de-finition, and emphasizes the connection betweenmetaphor and the arguments that are put forwardby the writer.
The protocol is presented in detailin Beigman Klebanov and Flor (2013).
All essayswere doubly annotated.
The reliability is ?
= 0.58for Set A and ?
= 0.56 for Set B.
We merge the twoannotations (union), following the observation ina previous study Beigman Klebanov et al.
(2008)that attention slips play a large role in accountingfor observed disagreements.We will report results for 10-fold cross-validation on each of sets A and B, as well as2We note that this top-level partition was used for manyof the analyses discussed in (Steen et al., 2010).across prompts, where the machine learner wouldbe trained on Set A and tested on Set B and viceversa.3 Supervised Learning of MetaphorFor this study, we consider each content-word to-ken in a text as an instance to be classified as ametaphor or non-metaphor.
We use the logisticregression classifier in the SKLL package (Blan-chard et al., 2013), which is based on scikit-learn(Pedregosa et al., 2011), optimizing for F1score(class ?metaphor?).
We consider the followingfeatures for metaphor detection.?
Unigrams (U): All content words from therelevant training data are used as features,without stemming or lemmatization.?
Part-of-Speech (P): We use Stanford POStagger 3.3.0 and the full Penn Treebank tagsetfor content words (tags starting with A, N, V,and J), removing the auxiliaries have, be, do.?
Concreteness (C): We use Brysbaert et al.
(2013) database of concreteness ratings forabout 40,000 English words.
The mean ra-tings, ranging 1-5, are binned in 0.25 incre-ments; each bin is used as a binary feature.?
Topic models (T): We use Latent Dirich-let Allocation (Blei et al., 2003) to derivea 100-topic model from the NYT corpusyears 2003?2007 (Sandhaus, 2008) to rep-resent common topics of public discussion.The NYT data was lemmatized using NLTK(Bird, 2006).
We used the gensim toolkit(?Reh?u?rek and Sojka, 2010) for building themodels, with default parameters.
The scoreassigned to an instance w on a topic t islogP (w|t)P (w)where P (w) were estimated fromthe Gigaword corpus (Parker et al., 2009).These features are based on the hypothesisthat certain topics are likelier to be used assource domains for metaphors than others.4 ResultsFor each dataset, we present the results for theunigram model (baseline) and the results for thefull model containing all the features.
For cross-validation results, all words from the same textwere always placed in the same fold, to ensure thatwe are evaluating generalization across texts.12M Unigram UPCTData F P R F P R FSet A .20 .72 .43 .53 .70 .47 .56Set B .22 .79 .54 .64 .76 .60 .67B-A .20 .58 .45 .50 .56 .50 .53A-B .22 .71 .28 .40 .72 .35 .47News .31 .62 .38 .47 .61 .43 .51Fiction .25 .54 .23 .32 .54 .24 .33Acad.
.23 .51 .20 .27 .50 .22 .28Conv.
.14 .39 .14 .21 .36 .15 .21Table 2: Summary of performance, in terms ofprecision, recall, and F1.
Set A, B, and VUAm-sterdam: cross-validation.
B-A and A-B: Trainingon B and testing on A, and vice versa, respectively.Column M: F1of a pseudo-system that classifiesall words as metaphors.4.1 Performance of the Baseline ModelFirst, we observe the strong performance of theunigram baseline for the cross-validation withinsets A and B (rows 1 and 2 in Table 2).
For anew essay, about half its metaphors will have beenobserved in a sample of a few dozen essays on thesame topic; these words are also consistently usedas metaphors, as precision is above 70%.
Once thesame-topic assumption is relaxed down to relatedtopics, the sharing of metaphor is reduced (com-pare rows 1 vs 3 and 2 vs 4), but still substantial.Moving to VUAmsterdam data, we observe thatthe performance of the unigram model on theNews partition is comparable to its performance inthe cross-prompt scenario in the essay data (com-pare row 5 to rows 3-4 in Table 2), suggesting thatthe News fragments tend to discuss a set of relatedtopics and exhibit substantial sharing of metaphorsacross texts.The performance of the unigram model is muchlower for the other VUAmsterdam partitions, al-though it is still non-trivial, as evidenced by itsconsistent improvement over a pseudo-baselinethat classifies all words as metaphor, attaining100% recall (shown in column M in Table 2).
Theweaker performance could be due to highly diver-gent topics between texts in each of the partitions.It is also possible that the number of differenttexts in these partitions is insufficient for coveringthe metaphors that are common in these kinds oftexts ?
recall that these partitions have small num-bers of long texts, whereas the News partition hasa larger number of short texts (see Table 1).4.2 Beyond BaselineThe addition of topic model, POS, and concrete-ness features produces a significant increase inrecall across all evaluations (p < 0.01), usingMcNemar?s test of the significance of differ-ences between correlated proportions (McNemar,1947).
Even for Conversations, where recallimprovement is the smallest and F1score doesnot improve, the UPCT model recovers all 161metaphors found by the unigrams plus 14 addi-tional metaphors, yielding a significant result onthe correlated test.We next investigate the relative contribution ofthe different types of features in the UPCT modelby ablating each type and observing the effect onperformance.
Table 3 shows ablation results foressay and News data, where substantial improve-ments over the unigram baseline were produced.We observe, as expected, that the unigram fea-tures contributed the most, as removing them re-sults in the most dramatic drop in performance,although the combination of concreteness, POS,and topic models recovers about one-fourth ofmetaphors with over 50% precision, showing non-trivial performance on essay data.The second most effective feature set for essaydata are the topic models ?
they are responsible formost of the recall gain obtained by the UPCT mo-del.
For example, one of the topics with a positiveweight in essays in set B deals with visual ima-gery, its top 5 most likely words in the NYT beingpicture, image, photograph, camera, photo.
Thistopic is often used metaphorically, with wordslike superficial, picture, framed, reflective, mirror,capture, vivid, distorted, exposure, scenes, face,background that were all observed as metaphors inSet B.
In the News data, a topic that deals with hur-ricane Katrina received a positive weight, as wordsof suffering and recovery from distaster are oftenused metaphorically when discussing other things:starved, severed, awash, damaged, relief, victim,distress, hits, swept, bounce, response, recovering,suffering.The part-of-speech features help improve recallacross all datasets in Table 3, while concretenessfeatures are effective only for some of the sets.5 Discussion: Metaphor & Word SenseThe classical ?one sense per discourse?
finding ofGale et al.
(1992) that words keep their senseswithin the same text 98% of the time suggests that13Set A cross-val.
Set B cross-val.
Train B : Test A Train A : Test B NewsP R F P R F P R F P R F P R FM .11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31U .72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47UPCT .70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51?
U .58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32?
P .71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49?
C .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50?
T .71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49Table 3: Ablation evaluations.
Model M is a pseudo-system that classifies all instances as metaphors.if a word is used as a metaphor once in a text, it isvery likely to be a metaphor if it is used again inthe same text.
Indeed, this is the reason for puttingall words from the same text in the same fold incross-validations, as training and testing on diffe-rent parts of the same text would produce inflatedestimates of metaphor classification performance.Koeling et al.
(2005) extend the notion of dis-course beyond a single text to a domain, such asarticles on Finance, Sports, and a general BNCdomain.
For a set of words that each have atleast one Finance and one Sports sense and notmore than 12 senses in total, guessing the pre-dominant sense in Finance and Sports yielded 77%and 76% precision, respectively.
Our results withthe unigram model show that guessing ?metaphor?based on a sufficient proportion of previously ob-served metaphorical uses in the given domainyields about 76% precision for essays on the sametopic.
Thus, metaphoricity distinctions in same-topic essays behave similarly to sense distinctionsfor polysemous words with a predominant sensein the Finance and Sports articles, keeping to theirdomain-specific predominant sense34of the time.Note that a domain-specific predominant sensemay or may not be the same as the most frequentsense overall; similarly, a word?s tendency to beused metaphorically might be domain specific orgeneral.
The results for the BNC at large are likelyto reflect general rather than domain-specific sensedistributions.
According to Koeling et al.
(2005),guessing the predominant sense in the BNC yields51% precision; our finding for BNC News is 62%precision for the unigram model.
The differencecould be due to the mixing of the BNC genres inKoeling et al.
(2005), given the lower precision ofmetaphoricity prediction in non-news (Table 2).In all, our results suggest that the pattern ofmetaphorical and non-metaphorical use is in linewith that of dominant word-sense for more andless topically restricted domains.6 Related WorkThe extent to which different texts use similarmetaphors was addressed by Pasanek and Scul-ley (2008) for corpora written by the same author.They studied metaphors of mind in the oeuvreof 7 authors, including John Milton and WilliamShakespeare.
They created a set of metaphori-cal and non-metaphorical references to the mindusing excerpts from various texts written by theseauthors.
Using cross-validation with unigramfeatures for each of the authors separately, theypresent very high accuracies (85%-94%), suggest-ing that authors are highly self-consistent in themetaphors of mind they select.
They also findgood generalizations between some pairs of au-thors, due to borrowing or literary allusion.Studies using political texts, such as speechesby politicians or news articles discussing politi-cally important events, documented repeated useof words from certain source domains, such asrejuvenation in Tony Blair?s speeches (Charteris-Black, 2005) or railroad metaphors in articles dis-cussing political integration of Europe (Musolff,2000).
Our results regarding settings with substan-tial topical consistency second these observations.According to the Conceptual Metaphor theory(Lakoff and Johnson, 1980), we expect certain ba-sic metaphors to be highly ubiquitous in any cor-pus of texts, such as TIME IS SPACE or UP ISGOOD.
To the extent that these metaphors arerealized through frequent content words, we ex-pect some cross-text generalization power for aunigram model.
Perhaps the share of these basicmetaphors in all metaphors in a text is reflectedmost faithfully in the peformance of the unigrammodel on the non-News partitions of the VUAms-14terdam data, where topical sharing is minimal.Approaches to metaphor detection are often ei-ther rule-based or unsupervised (Martin, 1990;Fass, 1991; Shutova et al., 2010; Shutova andSun, 2013; Li et al., 2013), although supervisedapproaches have recently been attempted with theadvent of relatively large collections of metaphor-annotated materials (Mohler et al., 2013; Hovy etal., 2013; Pasanek and Sculley, 2008; Gediganet al., 2006).
These approaches are difficult tocompare to our results, as these typically are notwhole texts but excerpts, and only certain kinds ofmetaphors are annotated, such as metaphors aboutgovernance or about the mind, or only words be-longing to certain syntactic or semantic class areannotated, such as verbs3or motion words only.Concreteness as a predictor of metaphoricitywas discussed in Turney et al.
(2011) in the contextof concrete adjectives modifying abstract nouns.The POS features are inspired by the discussionof the preference and aversion of various POStowards metaphoricity in Goatly (1997).
Heintzet al.
(2013) use LDA topics built on Wikipediaalong with manually constructed seed lists for po-tential source and target topics in the broad tar-get domain of governance, in order to identifysentences using lexica from both source and tar-get domains as potentially containing metaphors.Bethard et al.
(2009) use LDA topics built on BNCas features for classifying metaphorical and non-metaphorical uses of 9 words in 450 sentences thatuse these words, modeling metaphorical vs non-metaphorical contexts for these words.
In bothcases, LDA is used to capture the topical compo-sition of a sentence; in contrast, we use LDA tocapture the tendency of words belonging to a topicto be used metaphorically in a given discourse.Dunn (2013) compared algorithms based onvarious theories of metaphor on VUAmsterdamdata.
The evaluations were done at sentence level,where a sentence is metaphorical if it contains atleast one metaphorically used word.
In this ac-counting, the distribution is almost a mirror-imageof our setting, as 84% of sentences in News werelabeled as metaphorical, whereas 18% of contentwords are tagged as such.
The News partition wasvery difficult for the systems examined in Dunn(2013) ?
three of the four systems failed to pre-dict any non-metaphorical sentences, and the onesystem that did so suffered from a low recall of3as in Shutova and Teufel (2010)metaphors, 20%.
Dunn (2013) shows that thedifferent systems he compared had relatively lowagreement (?
< 0.3); he interprets this finding assuggesting that the different theories underlyingthe models capture different aspects of metapho-ricity and therefore detect different metaphors.
Itis therefore likely that features derived from thevarious models would fruitfully complement eachother in a supervised learning setting; our findingssuggest that the simplest building block ?
that ofa unigram model ?
should not be ignored in suchexperiments.7 ConclusionsWe address supervised learning of metaphoricityof words of any content part of speech in a runningtext.
To our knowledge, this task has not yet beenstudied in the literature.
We experimented with asimple unigram model that was surprisingly suc-cessful for some of the datasets, and showed howits recall can be further improved using topic mo-dels, POS, and concreteness features.The generally solid performance of the unigramfeatures suggests that these features should not beneglected when trying to predict metaphors in asupervised learning paradigm.
Inasmuch as me-taphoricity classification is similar to a coarse-grained word sense disambiguation, a unigrammodel can be thought of as a crude predominantsense model for WSD, and is the more effectivethe more topically homogeneous the data.By evaluating models with LDA-based topicfeatures in addition to unigrams, we showed thattopical homogeneity can be exploited beyond uni-grams.
In topically homogeneous data, certaintopics commonly discussed in the public spheremight not be addressed, yet their general fa-miliarity avails them as sources for metaphors.For essays on communication, topics like sportsand architecture are unlikely to be discussed; yetmetaphors from these domains can be used, suchas leveling of the playing field through cheap andfast communications or buildling bridges acrosscultures through the internet.In future work, we intend to add features thatcapture the relationship between the current wordand its immediate context, as well as add essaysfrom additional prompts to build a more topicallydiverse set for exploration of cross-topic generali-zation of our models for essay data.15ReferencesBeata Beigman Klebanov and Michael Flor.
2013.Argumentation-relevant metaphors in test-taker es-says.
In Proceedings of the First Workshop onMetaphor in NLP, pages 11?20, Atlanta, Georgia,June.
Association for Computational Linguistics.Beata Beigman Klebanov, Eyal Beigman, and DanielDiermeier.
2008.
Analyzing disagreements.
InCOLING 2008 workshop on Human Judgments inComputational Linguistics, pages 2?7, Manchester,UK.Steven Bethard, Vicky Tzuyin Lai, and James Martin.2009.
Topic model analysis of metaphor frequencyfor psycholinguistic stimuli.
In Proceedings of theWorkshop on Computational Approaches to Linguis-tic Creativity, CALC ?09, pages 9?16, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Steven Bird.
2006.
NLTK: The natural languagetoolkit.
In Proceedings of the ACL, Interactive Pre-sentations, pages 69?72.Daniel Blanchard, Michael Heilman,and Nitin Madnani.
2013.
SciKit-Learn Laboratory.
GitHub repository,https://github.com/EducationalTestingService/skll.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Marc Brysbaert, Amy Beth Warriner, and Victor Ku-perman.
2013.
Concreteness ratings for 40 thou-sand generally known english word lemmas.
Behav-ior Research Methods, pages 1?8.Jonathan Charteris-Black.
2005.
Politicians andrhetoric: The persuasive power of metaphors.
Pal-grave MacMillan, Houndmills, UK and New York.Jonathan Dunn.
2013.
What metaphor identificationsystems can tell us about metaphor-in-language.
InProceedings of the First Workshop on Metaphor inNLP, pages 1?10, Atlanta, Georgia, June.
Associa-tion for Computational Linguistics.Dan Fass.
1991.
Met*: A method for discriminatingmetonymy and metaphor by computer.
Computa-tional Linguistics, 17(1):49?90.William Gale, Kenneth Church, and David Yarowsky.1992.
One sense per discourse.
In Proceedings ofthe Speech and Natural Language Workshop, pages233?237.Matt Gedigan, John Bryant, Srini Narayanan, and Bra-nimir Ciric.
2006.
Catching metaphors.
In Pro-ceedings of the 3rd Workshop on Scalable NaturalLanguage Understanding, pages 41?48, New York.Andrew Goatly.
1997.
The Language of Metaphors.Routledge, London.Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, DaveBarner, Donald Black, Majorie Friedman, and RalphWeischedel.
2013.
Automatic Extraction of Lin-guistic Metaphors with LDA Topic Modeling.
InProceedings of the First Workshop on Metaphor inNLP, pages 58?66, Atlanta, Georgia, June.
Associa-tion for Computational Linguistics.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
In Pro-ceedings of the First Workshop on Metaphor in NLP,pages 52?57, Atlanta, GA. Association for Compu-tational Linguistics.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings of HLT-EMNLP, pages 419?426, Vancouver, Canada.
Asso-ciation for Computational Linguistics.George Lakoff and Mark Johnson.
1980.
Metaphorswe live by.
University of Chicago Press, Chicago.Hongsong Li, Kenny Q. Zhu, and Haixun Wang.
2013.Data-driven metaphor recognition and explanation.Transactions of the ACL, 1:379?390.James Martin.
1990.
A computational model ofmetaphor interpretation.
Academic Press Profes-sional, Inc., San Diego, CA, USA.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2):153?157.Michael Mohler, David Bracewell, Marc Tomlinson,and David Hinote.
2013.
Semantic signatures forexample-based linguistic metaphor detection.
InProceedings of the First Workshop on Metaphor inNLP, pages 27?35, Atlanta, GA. Association forComputational Linguistics.Andreas Musolff.
2000.
Mirror images of Eu-rope: Metaphors in the public debate aboutEurope in Britain and Germany.
Mu?nchen:Iudicium.
Annotated data is available athttp://www.dur.ac.uk/andreas.musolff/Arcindex.htm.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword FourthEdition LDC2009T13.
Linguistic Data Consortium,Philadelphia.Bradley Pasanek and D. Sculley.
2008.
Mining mil-lions of metaphors.
Literary and Linguistic Com-puting, 23(3):345?360.Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake VanderPlas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.16Group Pragglejaz.
2007.
MIP: A Method for Iden-tifying Metaphorically Used Words in Discourse.Metaphor and Symbol, 22(1):1?39.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.Evan Sandhaus.
2008.
The New York Times Anno-tated Corpus.
LDC Catalog No: LDC2008T19.Ekaterina Shutova and Lin Sun.
2013.
Unsu-pervised metaphor identification using hierarchicalgraph factorization clustering.
In Proceedings ofHLT-NAACL, pages 978?988.Ekaterina Shutova and Simone Teufel.
2010.Metaphor corpus annotated for source - target do-main mappings.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC?10), pages 3255?3261, Valletta, Malta, May.
European Language Re-sources Association (ELRA).Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COL-ING), pages 1002?1010.Gerard Steen, Aletta Dorst, Berenike Herrmann, AnnaKaal, Tina Krennmayr, and Trijntje Pasma.
2010.
AMethod for Linguistic Metaphor Identification.
Am-sterdam: John Benjamins.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of NAACL, pages 252?259.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identi-fication through concrete and abstract context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 680?690, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.17
