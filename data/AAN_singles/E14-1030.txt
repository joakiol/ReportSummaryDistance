Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 279?287,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIdentifying fake Amazon reviews as learning from crowdsTommaso FornaciariMinistero dell?InternoDipartimento della Pubblica SicurezzaSegreteria del DipartimentoComISSITtommaso.fornaciari@interno.itMassimo PoesioUniversity of EssexCSEEUniversity of TrentoCIMeCpoesio@essex.ac.ukAbstractCustomers who buy products such asbooks online often rely on other customersreviews more than on reviews found onspecialist magazines.
Unfortunately theconfidence in such reviews is often mis-placed due to the explosion of so-calledsock puppetry?authors writing glowingreviews of their own books.
Identifyingsuch deceptive reviews is not easy.
Thefirst contribution of our work is the cre-ation of a collection including a numberof genuinely deceptive Amazon book re-views in collaboration with crime writerJeremy Duns, who has devoted a greatdeal of effort in unmasking sock puppet-ing among his colleagues.
But there canbe no certainty concerning the other re-views in the collection: all we have is anumber of cues, also developed in collab-oration with Duns, suggesting that a re-view may be genuine or deceptive.
Thusthis corpus is an example of a collectionwhere it is not possible to acquire theactual label for all instances, and whereclues of deception were treated as anno-tators who assign them heuristic labels.
Anumber of approaches have been proposedfor such cases; we adopt here the ?learn-ing from crowds?
approach proposed byRaykar et al.
(2010).
Thanks to Duns?
cer-tainly fake reviews, the second contribu-tion of this work consists in the evaluationof the effectiveness of different methods ofannotation, according to the performanceof models trained to detect deceptive re-views.1 IntroductionCustomer reviews of books, hotels and other prod-ucts are widely perceived as an important rea-son for the success of e-commerce sites such asamazon.com or tripadvisor.com.
How-ever, customer confidence in such reviews is oftenmisplaced, due to the growth of the so-called sockpuppetry phenomenon: authors / hoteliers writingglowing reviews of their own works / hotels (andoccasionally also negative reviews of the competi-tors).1The prevalence of this phenomenon hasbeen revealed by campaigners such as crime writerJeremy Duns, who exposed a number of fellow au-thors involved in such practices.2A number ofsites have also emerged offering Amazon reviewsto authors for a fee.3Several automatic techniques for exposing suchdeceptive reviews have been proposed in recentyears (Feng et al., 2012; Ott et al., 2001).
But likeall work on deceptive language (computational orotherwise) (Newman et al., 2003; Strapparava andMihalcea, 2009), such works suffer from a seri-ous problem: the lack of a gold standard contain-ing ?real life?
examples of deceptive uses of lan-guage.
This is because it is very difficult to finddefinite proof that an Amazon review is either de-ceptive or genuine.
Thus most researchers recre-ate deceptive behavior in the lab, as done by New-man et al.
(2003).
For instance, Ott et al.
(2001),Feng et al.
(2012) and Strapparava and Mihalcea(2009) used crowdsourcing, asking turkers to pro-duce instances of deceptive behavior.
Finally, Liet al.
(2011) classify reviews as deceptive or truth-ful by hand on the basis of a series of heuristics:they start by excluding anonymous reviews, thenuse their helpfulness and other criteria to decide1The phenomenon predates Internet - see e.g., Amy Har-mon, ?Amazon Glitch Unmasks War Of Reviewers?, NewYork Times, February 14, 2004.2See Andrew Hough, ?RJ Ellory: fake book reviewsare rife on internet, authors warn?, telegraph.co.uk,September 3, 20123See Alison Flood, ?Sock puppetry and fake reviews:publish and be damned?, guardian.co.uk, September 4,2012 and David Streitfeld, ?Buy Reviews on Yelp, Get BlackMark?, nytimes.com, October 18, 2012.279whether they are deceptive or not.
Clearly a morerigorous approach to establishing the truth or oth-erwise of reviews on the basis of such heuristiccriteria would be useful.In this work we develop a system for identify-ing deceptive reviews in Amazon.
Our proposalmakes two main contributions:1. we identified in collaboration with JeremyDuns a series of criteria used by Duns andother ?sock puppet hunters?
to find suspiciousreviews / reviewers, and collected a dataset ofreviews some of which are certainly false asthe authors admitted so, whereas others maybe genuine or deceptive.2.
we developed an approach to the truthful-ness of reviews based on the notion that thetruthfulness of a review is a latent variablewhose value cannot be known, but can be es-timated using some criteria as potential indi-cators of such value?as annotators?and thenwe used the learning from crowds algorithmproposed by Raykar et al.
(2010) to assign aclass to each review in the dataset.The structure of the paper is as follows.
In Sec-tion 2 we describe how we collected our dataset;in Section 3 we show the experiments we carriedout and in Section 4 we discuss the results.2 Deception clues and dataset2.1 Examples of Unmasked Sock PuppetryAfter reading an article by Alison Flood on TheGuardian of September 4th, 20124, discussinghow crime writer Jeremy Duns had unmasked anumber of ?sock puppeteers,?
we contacted him.Duns was extremely helpful; he pointed us to theother articles on the topic, mostly on The New YorkTimes, and helped us create a set of deceptionclues and the dataset used in this work.On July 25th, 2011, an article appeared onwww.moneytalksnews.com, entitled ?3 Tipsfor Spotting Fake Product Reviews - From Some-one Who Wrote Them?.5Sandra Parker, authorof the text, in that page described her experienceas ?professional review writer?.
According to her4Sock puppetry and fake reviews: publish and be damned,http://www.guardian.co.uk/books/2012/sep/04/sock-puppetry-publish-be-damned5http://www.moneytalksnews.com/2011/07/25/3-tips-for-spotting-fake-product-reviews---from-someone-who-wrote-them/statements, advertising agencies were used to payher $10-20 for writing reviews on sites like Ama-zon.com.
She was not asked to lie, but ?if the re-view wasn?t five star, they didn?t pay?.
In an arti-cle of August 19th, written by David Streitfeld onwww.nytimes.com,6she actually denied thatpoint: ?We were not asked to provide a five-starreview, but would be asked to turn down an as-signment if we could not give one?.In any case, in her article Sandra Parker gavethe readers some common sense-based advices, inorder to help them to recognize possible fake re-views.
One of these suggestions were also usefulfor this study, as discussed in Section 2.3.
Fromour point of view, however, the most interestingaspect of the article relied in the fact that, lettingknow the name of an author of fake reviews, itmade possible to identify them in Amazon.com,with an high degree of confidence.A further article written on August 25thbyDavid Streitfeld gave us another similar opportu-nity.7In fact, thanks to his survey, it was possibleto come to know the titles of four books, whose theauthors paid an agency in order to receive reviews.2.2 The corpusUsing the suggestions of Jeremy Duns and the in-formation in these articles we built a corpus wecalled DEREV (DEception in REViews), consist-ing of clearly fake, possibly fake, and possiblygenuine book reviews posted on www.amazon.com.
The corpus, which will be freely availableon demand, consists of 6819 reviews downloadedfrom www.amazon.com, concerning 68 booksand written by 4811 different reviewers.
The 68books were chosen trying to balance the numberof reviews (our units of analysis) related to sus-pect books which probably or surely received fakereviews, with the number of reviews hypothesizedto be genuine in that we expected the authors ofthe books not to have bought reviews.
In partic-ular, we put into the group of the suspect books -henceforth SB - the reviews of the four books in-dicated by David Streitfeld.
To this first nucleus,we also added other four books, written by threeof the authors of the previous group.
We also in-6http://www.nytimes.com/2011/08/20/technology/finding-fake-reviews-online.html?_r=1&7http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html?pagewanted=all280cluded in the SB group the 22 books for whichSandra Parker wrote a review.
Lastly, we noticedthat some reviewers of the books pointed out byDavid Streitfeld tended to write reviews of thesame books: we identified 16 of them, and consid-ered suspect as well.
In total, on November 17th,2011 we downloaded the reviews of 46 books con-sidered as suspect, which received 2707 reviews.8We also collected the reviews of 22 so called ?in-nocent books?, for a total of 4112 reviews.
Thesebooks were mainly chosen among classic authors,such as Conan Doyle or Kipling, or among liv-ing writers who are so renowned that any reviews?purchase would be pointless: this is the case, forexample, of Ken Follett and Stephen King.
Asshown by the number of the reviews, the booksof these authors are so famous that they receive agreat amount of readers?
opinions.The size of DEREV is 1175410 tokens, con-sidering punctuation blocks as single token.
Themean size of the reviews is 172.37 tokens.
The ti-tles of the reviews were neither included in thesestatistics nor in the following analyses.2.3 Deception cluesOnce created the corpus, we identified a set ofclues, whose presence suggested the deceptivenessof the reviews.
These clues are:Suspect Book - SB The first clue of deceptive-ness was the reference of the reviews to a sus-pect book, identified as described above.
Thisis the only clue which is constant for all thereviews of the same book.Cluster - Cl The second clue comes from thesuggestions given by Sandra Parker in hermentioned article.
As she pointed out, theagencies she worked for were used to give her48 hours to write a review.
Being likely thatthe same deadline was given to other review-ers, Sandra Parker warns to pay attention ifthe books receive many reviews in a short pe-riod of time.
Following her advice, we con-sidered as positive this clue of deceptivenessif the review belonged to a group of at leasttwo reviews posted within 3 days.Nickname - NN A service provided by Amazonis the possibility for the reviewers to register8We specify the date of the download because, obviously,if the data collection would be repeated today, the overallnumber of reviews would be greater.in the website and to post comments usingtheir real name.
Since the real identity of thereviewers involves issues related to their rep-utation, we supposed it is less probable thatthe writers of fake reviews post their texts us-ing their true name.
Moreover, a similar as-sumption was probably accepted by Li et al.
(2011), who considered the profile features ofthe reviewers, and among them the use or notof their real name.Unknown Purchase - UP Lastly, the probablymost interesting information provided byAmazon is whether the reviewer bought thereviewed book through Amazon itself.
Itis reasonable to think that, if the reviewerbought the book, he also read it.
Therefore,the absence of information about the certifiedpurchase was considered a clue of deceptive-ness.2.4 Gold and silver standardThe clues of deception discussed above give usa heuristic estimate of the truthfulness of the re-views.
Such estimation represents a silver stan-dard of our classes, as these are not determinedthrough certain knowledge of the ground truth, butsimply thanks to hints of deceptiveness.
The meth-ods we used in order to assign the heuristic classesto the reviews are described in the next Section;however for our purposes we needed a gold stan-dard, that is at least a subset of reviews whoseground truth was known with a high degree of con-fidence.
This subset was identified as follows.First, we considered as false the 22 reviewspublished by Sandra Parker, even though not allher reviews are characterized by the presence ofall the deception clues.
Even though we cannotreally say whether her reviews reflect her opin-ion of the books in question or not, she explic-itly claimed to have been paid for writing them;and she only bought on Amazon three of these22 books.
This is the most accurate knowledgeabout fake reviews not artificially produced wehave found in literature.
Then we focused on thefour books whose authors admitted to have boughtthe reviews.9Three of them received many re-views, which made it difficult to understand ifthey were truthful or not.
However, one of these9http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html?pagewanted=all281Table 1: The distribution of deception clues in thereviewsNr.
clues Reviews Tot.
%False 4 903rev.
3 1913 2816 41.30%True 2 2528rev.
1 12100 265 4003 58.70%books (?Write your first book?, by Peter Biadasz)received only 20 reviews, which therefore couldbe considered as fake with high degree of proba-bility.
Even though we have no clear evidence thata small number of reviews correlates with a greaterlikelihood of deception, since we know this bookreceived fake reviews, and there are only few re-views for it, we felt it is pretty likely that thoseare fake.
Therefore we examined the reviews writ-ten by these twenty authors, and considered asfalse only those showing the presence of all thedeception clues described above.
In this way, wefound 96 reviews published by 14 reviewers, andwe added them to the 22 of Sandra Parker, for atotal of 118 reviews written by 15 authors.Once identified this subset of fake reviews, weselected other 118 reviews which did not showthe presence of any deception clue, that is chosenfrom books above any suspicion, written by au-thors who published the review having made useof their real name and having bought the bookthrough Amazon and so on.In the end, we identified a subset of DEREVconstituted by 236 reviews, whose class wasknown with high degree of confidence and con-sidered them as our gold standard.3 ExperimentsWe carried out two experiments, in which theclasses assigned to the reviews of DEREV werefound adopting two different strategies.
In the firstexperiment the classes of the reviews were de-termined using majority voting of our deceptionclues.
This experiment is thus conceptually simi-lar to those of Li et al.
(2011), who trained modelsusing supervised methods with the aim of identi-fying fake reviews.
We discuss this experiment inthe next Section.
In the second experiment, learn-ing from crowds was used (Raykar et al., 2010).This approach is discussed in Section 3.2.1.In both experiments we carried out a 10-foldcross-validation where in each iteration feature se-lection and training were carried out using 90% ofthe part of the corpus with only silver standard an-notation and 90% of the subset with gold.
The testset used in each iteration consisted of the remain-ing tenth of reviews with gold standard classes,which were employed in order to evaluate the pre-dictions of the models.
This allowed to estimatethe efficiency of the strategies we used to deter-mine our silver standard classes.3.1 Majority Voting3.1.1 Determining the class of reviews bymajority votingThe deception clues discussed in Section 2.3 wereused in our first experiment to identify the class ofeach review using majority voting.
In other words,those clues were considered as independent pre-dictors of the class; the class predicted by the ma-jority of the annotators/clues was assigned to thereview.
Specifically, if 0, 1 or 2 deception clueswere found, the review was classified as true; ifthere were 3 or 4, the review was considered false.Table 1 shows the distribution of the number ofdeception clues in the reviews in DEREV.3.1.2 Feature selectionIn both experiments each review was representedas feature vector.
The features were just of uni-grams, bigrams and trigrams of lemmas and part-of-speech (POS), as collected from the reviewsthrough TreeTagger10(Schmid, 1994).Since in each experiment we applied a 10-foldcross-validation, in every fold the features wereextracted from the nine-tenths of DEREV em-ployed as training set.
Once identified the train-ing set, we computed the frequency lists of then-grams of lemmas and POS.
The lists were col-lected separately from the reviews belonging tothe class ?true?
and to the class ?false?.
Such sep-aration was aimed to take into consideration themost highly frequent n-grams of both genuine andfake reviews.
However, for the following steps ofthe feature selection, only the n-grams which ap-peared more than 300 times in every frequency listwere considered: a threshold empirically chosenfor ease of calculations.
In fact, among the most10http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html282Table 2: The most frequent n-grams collectedN-grams Lemmas POS TotalUnigrams 34 21Bigrams 21 13Trigrams 13 8Total 68 42 110frequents, in order to identify the features mosteffective in discriminating the two classes of re-views, the Information Gain (IG) of the selected n-grams was computed (Kullback and Leibler, 1951;Yang and Pedersen, 1997).Then, after having found the Information Gainof the n-grams of lemmas and part-of-speech, afurther reduction of the features was realized.
Infact, we selected a relatively small amount of fea-tures, in order to facilitate the computation of theRaykar et al.
?s algorithm (discussed in Sub-section3.2.1), and only the n-grams with the highest IGvalues were selected to be taken as features of thevectors which represented the reviews.
In par-ticular, the n-grams were collected according tothe scheme shown in Table 2.
By the way, 8,13, 21 and 34 are numbers belonging to the Fi-bonacci series (Sigler, 2003).
They were chosenbecause they grow exponentially and are used, inour case, to give wider representation to the short-est n-grams.Lastly, two more features were added to the fea-ture set, that is the length of the review, consideredwith and without punctuation.
Therefore, in eachfold of the experiment, the vectors of the reviewswere constituted by 112 values: 2 correspondingto the length of the review, and 110 representingthe (not normalized) frequency, into the review it-self, of the selected n-grams of lemmas and POS.3.1.3 BaselinesThe best way to assess the improvement comingfrom the algorithm would have been with respectto a supervised baseline.
However this was notpossible as we could only be certain regarding theclassification of a fraction of the reviews (our goldstandard: 236 reviews, for a total of about 23,000tokens).
We felt such a small dataset could not beused for training, but only for evaluation; thereforewe used instead two simple heuristic baselines.Majority baseline.
The simplest metric for per-formance evaluation is the majority baseline: al-ways assign to a review the class most representedin the dataset.
Since in the subset of DEREV withgold standard we had 50% of true and false re-views, simply 50% is our majority baseline.Random baseline.
Furthermore, we estimated arandom baseline through a Monte Carlo simula-tion.
This kind of simulation allows to estimate theperformance of a classifier which performs severaltimes a task over random outputs whose distribu-tion reflects that of real data.In particular, for this experiment, since we had236 reviews whose 50% were labeled as false,100000 times we produced 236 random binomialpredictions, having p = .5.
In each simulation,the random prediction was compared with our realdata.
It turned out that in less than .01% of tri-als the level of 62.29% of correct predictions wasexceeded.
The thresholds for precision and recallin detecting deceptive reviews were 62.26% and66.95% respectively.3.1.4 ModelsWe tested a number of supervised learning meth-ods to learn a classifier using the classes deter-mined by majority voting, but the best resultswere obtained using Support Vector Machines(SVMs) (Cortes and Vapnik, 1995), already em-ployed in many applications involving text classi-fication (Yang and Liu, 1999).3.1.5 ResultsThe results obtained by training a supervised clas-sifier over the dataset with classes identified withmajority voting are shown in the Table 3.
Thehighest results are in bold.
The methodologicalapproach and performance achieved in this exper-iment seems to be comparable to that of Strappar-ava and Mihalcea (2009) and, more recently, of Liet al.
(2011).
However Li et al.
(2011) evaluate theeffectiveness of different kind of features with theaim of annotating unlabeled data, while we try toevaluate the reliability of heuristic classes in train-ing.3.2 Learning from Crowds3.2.1 The Learning from Crowds algorithmAs pointed out by Raykar et al.
(2010), major-ity voting is not necessarily the most effectiveway to determine the real classes in problems like283Table 3: The experiment with the majority voting classesCorrectly Incorrectly Precision Recall F-measureclassified reviews classified reviewsFalse reviews 75 43 83.33% 63.56% 72.12%True reviews 103 15Total 178 58Total accuracy 75.42%Random baseline 62.29% 62.26% 66.95%those of reviews where there is no gold standard.This is because annotators are not equally reli-able, and the reviews are not equally challenging.Hence the output of the majority voting may be af-fected by unevaluated biases.
To address this prob-lem, Raykar et al.
(2010) presented a maximum-likelihood estimator that jointly learns the classi-fier/regressor, the annotator accuracy, and the ac-tual true label.For ease of exposition, Raykar et al.
(2010) useas classifier the logistic regression, even thoughthey specify their algorithm would work with anyclassifier.
In case of logistic regression, the prob-ability for an entity x ?
X of belonging to a classy ?
Y with Y = {1, 0} is a sigmoid functionof the weight vector w of the features of each in-stance xi, that is p[y = 1|x,w] = ?
(w>x), where,given a threshold ?, the class y = 1 if w>x ?
?.Annotators?
performance, then, is evaluated ?interms of the sensitivity and specificity with respectto the unknown gold standard?
: in particular, in abinary classification problem, for the annotator jthe sensitivity ?jis the rate of positive cases iden-tified by the annotator ?i.e., the recall of positivecases?
while the specificity ?jis the annotator?srecall of negative cases.Given a dataset D constituted of indepen-dently sampled entities, a number of annotatorsR, and the relative parameters ?
= {w,?, ?
},the likelihood function which needs to be maxi-mized, according to Raykar et al.
(2010), wouldbe p[D|?]
=?Ni=1p[y1i, ...yRi|xi, ?
], and themaximum-likelihood estimator is obtained bymaximizing the log-likelihood, that is?
?ML= {??,?
?, w?}
= argmax?
{ln p[D|?]}.
(1)Raykar et al.
(2010) propose to solve this max-imization problem (Bickel and Doksum, 2000)through the technique of Expectation Maximiza-tion (EM) (Dempster et al., 1977).
The EM al-gorithm can be used to recover the parameters ofthe hidden distributions accounting for the distri-bution of data.
It consists of two steps, an Expecta-tion step (E-step) followed by a Maximization step(M-step), which are iterated until convergence.During the E-step the expectation of the term yiiscomputed starting from the current estimate of theparameters.
In the M-step the parameters ?
are up-dated by maximizing the conditional expectation.Regarding the third parameter, w, Raykar et al.
(2010) admit there is not a closed form solutionand suggest to use the Newton-Raphson method.3.2.2 Determining the class of reviews usingLearning from CrowdsIn order to apply Raykar?s algorithm, we pro-ceeded as follows.
First, we applied the procedurefor feature selection described in Subsection 3.1.2to create a single dataset: that is, the corpus wasnot divided in folds, but the feature selection in-volved all of DEREV.
This dataset was built usingthe classes resulting from the majority voting ap-proach and included these columns:?
The class assignments of the four clues dis-cussed in Sub-section 2.3 ?
SB, Cl, NN, UP;?
The majority voting class;?
The 112 features identified according to theprocedure presented in Sub-section 3.1.2.Then, we implemented the algorithm proposedby Raykar et al.
(2010) in R.11We computed a Lo-gistic Regression (Gelman and Hill, 2007) on thedataset to compute the weight vectorw, used to es-timate for each instance the probability pifor thereview of belonging to the class ?true?.
For the lo-gistic regression we used the 112 surface features11http://www.r-project.org/284Table 4: The experiment with Raykar et al.
?s algorithm classesCorrectly Incorrectly Precision Recall F-measureclassified reviews classified reviewsFalse reviews 85 33 78.70% 72.03% 75.22%True reviews 95 23Total 180 56Total accuracy 76.27%Random baseline 62.29% 62.26% 66.95%mentioned above, adopting as class the majorityvoting, as suggested by Raykar et al.
(2010).The parameters ?
and ?
were estimated regard-ing the three clues Cl - Cluster, NN - Nicknameand UP - Unknown Purchase.
The attribute SB -Suspect Book was not used, in order to carry outthe EM algorithm exclusively on heuristic data, re-moving the information obtained through sourcesexternal to the dataset.
The parameters ?
and ?of the three clues were obtained not from ran-dom classes, as the EM algorithm would allow, butagain comparing the clues?
labels with the major-ity voting class.
In fact, aware of the local maxi-mum problem of EM, in this way we tried to en-hance the reliability of the results posing a config-uration which could be, at least theoretically, bet-ter than a completely random one.Knowing these values for each instance of thedataset, we computed the E-step and we updatedour parameters in M-step.The E-step and the M-step were iterated 100times, in which the log-likelihood increases mono-tonically, indicating a convergence to a local max-imum.The final value of pidetermined the new class ofeach instance: if pi> .5 the review was labeled astrue, otherwise as false.
In the end, the EM clus-terization allowed to label 3267 reviews as falseand 3552 as true, that is 47.91% and 52.09% ofDEREV respectively.3.2.3 Feature selectionThe feature selection for this experiment was ex-actly the same presented for the previous one inSub-section 3.1.2; the only, fundamental differ-ence was that in the first experiment the classesderived from the majority voting rule, while inthe second experiment the classes were identifiedthrough the Raykar et al.
?s strategy.3.2.4 BaselinesAs in the first experiment, we compared the per-formance of the models with the same majorityand random baselines discussed in Sub-section3.1.3.3.2.5 ModelsWe used the classes determined through the Learn-ing by Crowds algorithm to train SVMs models,with the same settings employed in the first exper-iments.3.2.6 ResultsTable 4 shows the results of the classifier trainedover the dataset whose the classes were identifiedthrough the Raykar et al.
?s algorithm.4 Discussion4.1 Deceptive language in reviewsOf the 4811 reviewers who wrote reviews includedin our corpus, about 900 were anonymous, andonly 16 wrote 10 or more reviews.
If, in one hand,this prevented us from verifying the performanceof the models with respect to particular reviewers,on the other hand we had the opportunity of evalu-ating the style in writing reviews across many sub-jects.In our experiments, we extracted simple surfacefeatures constituted by short n-grams of lemmasand part-of-speech.
In literature there is evidencethat also other kinds of features are effective in de-tecting deception in reviews: for example, infor-mation about the syntactic structures of the texts(Feng et al., 2012).
In our pilot studies we did notobtain improvements using syntactic features.
Buteven the frequency of n-grams can provide someinsight regarding deceptive language in reviews;and with this aim we focused on the unigrams ap-pearing more than 50 times in the 236 reviews285constituting the gold standard of DEREV, whoseun/truthfulness is known.
The use of self-referredpronouns and adjectives is remarkably different intrue and fake reviews: in the genuine ones, the pro-nouns ?I?, ?my?
and ?me?
are found 371, 74 and 51times respectively, while in the fake ones the pro-noun ?I?
is present only 149 and ?me?
and ?my?less than 50 times.
This reduced number of self-references is coherent with the findings of otherwell-known studies regarding deception detection(Newman et al., 2003); however, while in truthfulreviews the pronoun ?you?
appears only 84 times,in the fake ones the frequency of ?you?
and ?your?is 151 and 75.
It seems that while the truth-tellerssimple state their opinions, the deceivers addressdirectly the reader.
Probably they tend to give ad-vice: after all, this is what they are paid for.
Thefrequency of the word ?read?
- that is the activ-ity simulated in fake reviews - is also quite imbal-anced: 137 in true reviews and 97 in the fake ones.Lastly, it is maybe surprising that in the false re-views terms related to positive feelings/judgmentsdo not have the highest frequency; instead in truth-ful reviews we found 52 times the term ?good?
(and 56 times the ambiguous term ?like?
): also thisoutcome is similar to that of the mentioned studyof Newman et al.
(2003).4.2 Estimating the gold standardThe estimation of the gold standard is a recur-rent problem in many tasks of text classificationand in particular with deceptive review identifica-tion, that is an application where the deceptivenessof the reviews cannot be properly determined butonly heuristically assessed.In this paper we introduced a new dataset forstudying deceptive reviews, constituted by 6819instances whose 236 (that is about 3.5% of the cor-pus) were labeled with the highest degree of confi-dence ever seen before.
We used this subset to testthe models that we trained on the other reviews ofDEREV, whose the class was heuristically deter-mined.With this purpose, we adopted two techniques.First, we simply considered the value of our cluesof deception as outputs of just as many annotators,and we assigned the classes to each review accord-ing to majority voting.
Then we clustered our in-stances using the Learning from Crowd algorithmproposed by Raykar et al.
(2010).
Lastly we car-ried out the two experiments of text classificationdescribed above.The results suggest that both methods achieveaccuracy well above the baseline.
However, themodels trained using Learning from Crowd classesnot only achieved the highest accuracy, but alsooutperformed the thresholds for precision and re-call in detecting deceptive reviews (Table 4), whilethe models trained with the majority voting classesshowed a very high precision, but at the expense ofthe recall, which was lower than the baseline (Ta-ble 3).Since the results even with simple majority vot-ing classes were positive, we carried out two moreexperiments, identical to those described aboveexcept that we included in the feature set the threedeception clues Cluster - Cl, Nickname - NN andUnknown Purchase - UP.
Both with majority vot-ing and with learning from Crowds classes, the ac-curacy of the models exceeded 97%.
This mightseem to suggest that those clues are very effective;but given that the deception clues were used to de-rive the silver standard, their use as features couldbe considered to some extent circular (Subsection2.4).
Moreover, not all of our non-linguistic cuesmay be found in all review scenarios, and thereforethe applicability of our methods to all review sce-narios will have to be investigated.
Specifically,Cluster is likely to be applicable to most reviewdomains, Nickname and Unknown Purchase areAmazon features that may or may not be adoptedby other services allowing users to provide re-views.
However, our main concern was not toevaluate the effectiveness of these specific clues ofdeception, but to investigate whether better strate-gies for labeling instances than simple majorityvoting could be found.In this perspective, the performance of oursecond experiment, in which the Learning fromCrowds algorithm was employed, stands out.
Infact in that case we tried to identify the classes ofthe instances abstaining from making use of anyexternal information regarding the reviews: in par-ticular, we ignored the Suspect Book - SB clue ofdeception which, by contrast, took part in the cre-ation of the majority voting classes.This outcome suggests that, even in scenarioswhere the gold standard is unknown, the Learningfrom Crowds algorithm is a reliable tool for label-ing the reviews, so that effective models can betrained in order to classify them as truthful or not.286ReferencesBickel, P. and Doksum, K. (2000).
Mathemati-cal statistics: basic ideas and selected topics.Number v. 1 in Mathematical Statistics: BasicIdeas and Selected Topics.
Prentice Hall.Cortes, C. and Vapnik, V. (1995).
Support-vectornetworks.
Machine Learning, 20.Dempster, A. P., Laird, N. M., and Rubin, D.
B.(1977).
Maximum Likelihood from IncompleteData via the EM Algorithm.
Journal of theRoyal Statistical Society.
Series B (Methodolog-ical), 39(1):1?38.Feng, S., Banerjee, R., and Choi, Y.
(2012).
Syn-tactic stylometry for deception detection.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Vol-ume 2: Short Papers), pages 171?175, JejuIsland, Korea.
Association for ComputationalLinguistics.Gelman, A. and Hill, J.
(2007).
Data AnalysisUsing Regression and Multilevel/HierarchicalModels.
Analytical Methods for Social Re-search.
Cambridge University Press.Kullback, S. and Leibler, R. A.
(1951).
On in-formation and sufficiency.
Ann.
Math.
Statist.,22(1):79?86.Li, F., Huang, M., Yang, Y., and Zhu, X.
(2011).Learning to identify review spam.
In Proceed-ings of the Twenty-Second international jointconference on Artificial Intelligence-VolumeVolume Three, pages 2488?2493.
AAAI Press.Newman, M. L., Pennebaker, J. W., Berry, D. S.,and Richards, J. M. (2003).
Lying Words:Predicting Deception From Linguistic Styles.Personality and Social Psychology Bulletin,29(5):665?675.Ott, M., Choi, Y., Cardie, C., and Hancock, J.(2001).
Finding deceptive opinion spam by anystretch of the imagination.
In Proceedings ofthe 49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies, pages 309?319, Portland, Ore-gon, USA.
Association for Computational Lin-guistics.Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H.,Florin, C., Bogoni, L., and Moy, L. (2010).Learning from crowds.
Journal of MachineLearning Research, 11:1297?1322.Schmid, H. (1994).
Probabilistic part-of-speechtagging using decision trees.
In Proceedingsof International Conference on New Methods inLanguage Processing.Sigler, L., editor (2003).
Fibonacci?s Liber Abaci:A Translation Into Modern English of LeonardoPisano?s Book of Calculation.
Sources andStudies in the History of Mathematics and Phys-ical Sciences.
Springer Verlag.Strapparava, C. and Mihalcea, R. (2009).
The LieDetector: Explorations in the Automatic Recog-nition of Deceptive Language.
In Proceed-ing ACLShort ?09 - Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.Yang, Y. and Liu, X.
(1999).
A re-examination oftext categorization methods.
In Proceedings ofthe 22nd annual international ACM SIGIR con-ference on Research and development in infor-mation retrieval, SIGIR ?99, pages 42?49, NewYork, NY, USA.
ACM.Yang, Y. and Pedersen, J. O.
(1997).
Acomparative study on feature selection intext categorization.
CiteSeerX - ScientificLiterature Digital Library and Search En-gine [http://citeseerx.ist.psu.edu/oai2] (UnitedStates).287
