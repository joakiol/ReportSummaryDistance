Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 30?34,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsConsensus versus Expertise : A Case Study of Word Alignment withMechanical TurkQin Gao and Stephan VogelLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh PA, 15213{qing,stephan.vogel}@cs.cmu.eduAbstractWord alignment is an important preprocessing stepfor machine translation.
The project aims at incorpo-rating manual alignments from Amazon MechanicalTurk (MTurk) to help improve word alignment qual-ity.
As a global crowdsourcing service, MTurk canprovide flexible and abundant labor force and there-fore reduce the cost of obtaining labels.
An easy-to-use interface is developed to simplify the labelingprocess.
We compare the alignment results by Turk-ers to that by experts, and incorporate the alignmentsin a semi-supervised word alignment tool to improvethe quality of the labels.
We also compared two pric-ing strategies for word alignment task.
Experimentalresults show high precision of the alignments pro-vided by Turkers and the semi-supervised approachachieved 0.5% absolute reduction on alignment errorrate.1 IntroductionWord alignment is used in various natural languageprocessing tasks.
Most state-of-the-art statistical machinetranslation systems rely on word alignment as a prepro-cessing step.
The quality of word alignment is usuallymeasured by AER, which is loosely related to BLEUscore (Lopez and Resnik, 2006).
There has been re-search on utilizing manually aligned corpus to assist auto-matic word alignment, and obtains encouraging results onalignment error rate.
(Callison-Burch et al, 2004; Blun-som and Cohn, 2006; Fraser and Marcu, 2006; Niehuesand Vogel, 2008; Taskar et al, 2005; Liu et al, 2005;Moore, 2005).
However, how to obtain large amount ofalignments with good quality is problematic.
Labelingword-aligned parallel corpora requires significant amountof labor.
In this paper we explore the possibility of us-ing Amazon Mechanical Turk (MTurk) to obtain manualword alignment faster, cheaper, with high quality.Crowdsourcing is a way of getting random labor forceon-line with low cost.
MTurk is one of the leadingproviders for crowdsourcing marketplace.
There havebeen several research papers on using MTurk to help nat-ural language processing tasks, Callison-Burch (2009)used MTurk to evaluate machine translation results.
Kit-tur et al (2008) showed the importance of validationdata set, the task is evaluating quality of Wikipedia arti-cles.
There are also experiments use the annotation fromMTurk in place of training data.
For example (Kaisser etal., 2008) and (Kaisser and Lowe, 2008) used MTurk tobuild question answering datasets and choose summarylengths that suite the need of the users.Word alignment is a relatively complicate task for in-experienced workers.
The fact puts us in a dilemma,we can either provide lengthy instructions and train theworkers, or we must face the problem that workers mayhave their own standards.
The former solution is im-practical in the context of crowdsourcing because heavilytrained workers will expect higher payment, which de-feats economical nature of crowdsourcing.
Therefore weare forced to face the uncertainty, and ask ourselves thefollowing questions: First, how consistent would the la-bels from random labelers be, given minimal or no in-structions?
Second, how consistent would these intuitivelabels be consistent with the labels from expert labelers?Third, if there is certain level of consistency between theintuitive labels and the labels from experts, can we extractmost reliable links from the former?
Last but not least,given the alignment links, can we utilize them to help au-tomatic word alignment without further human efforts?The statistics on the data we get shows the internalconsistency among multiple MTurk alignments is greaterthan 70%, and the precision is greater than 84% whenconsider all the links.
By applying majority vote andconsensus strategies, we can select links that have greaterthan 95% accuracy.
When applying the alignment linkson a new aligner that can perform constrained EM al-gorithm for IBM models we observe 0.5% absolute im-provements on alignment error rate.
The average per-word cost is about 2 cent per word.The paper will be organized as follows, first we willdiscuss the design principle of the task and the implemen-tation of the application for word alignment in section2.
Section 3 describes the algorithm used in utilizing themanual alignments.
Section 4 presents the analysis on theharvested data and the expert labels, and the the experi-ment results of semi-supervised word alignment.
Section5 concludes the paper.302 Design of the taskIn this task, we want to collect manual word alignmentdata from MTurk workers, Figure 2 shows an example ofword alignment.
There are two sentences which are trans-lation of each other.
There are links between words in twosentences, indicating the words are translation pairs.
No-tice that one word can be aligned to zero or more words,if a word is aligned to zero word, we can assume it isaligned to a virtual empty word.
Therefore, given a sen-tence pair, we want workers to link words in source sen-tence to one or more target words or the empty word.In our experiment, we use a Chinese-English parallelcorpus and ask workers to alignment the words in Chi-nese sentence to the words in English sentence.
We donot provide any alignment links from automatic aligner.2.1 Guidelines of designMTurk represents a new pattern of market that hasyet be thoroughly studied.
Mason and Watts (2009)shows that higher payment does not guarantee resultswith higher quality.
Also, one should be aware that theweb-based interface is vulnerable to automatic scriptsthat generate highly consistent yet meaningless results.To ensure a better result, several measures must be com-bined: 1) Require workers to take qualifications beforethey can accept the tasks.
2) Implement an interface lessvulnerable to automatic scripts.
3) Build quality controlmechanism that filters inaccurate results, and finally 4)Redesign the interface so that the time spent by carefuland careless workers does not differ too much, so thereis less incentives for workers to submit random results.With these guidelines in mind, we put together severalelements into the HIT.QualificationsWe require the workers to take qualifications, whichrequires them to pick correct translation of five Chinesewords.
The Chinese word is rendered in bitmap.Interface implementationWe implemented the word alignment interface ontop of Google Web Toolkit, which enables developingJavascript based Web application in Java.
Becauseall the content of the interface, including the content inthe final result, is generated dynamically in the run time,it is much more difficult to hack than plain HTML forms.Figure 1 shows a snapshot of the interface1.
The labelingprocedure requires only mouse click.
The worker needto label all the words with a golden background2.
Tocomplete the task, the worker needs to: 1) Click on the1A demo of the latest version can be found at http://alt-aligner.appspot.com, the source code of thealigner is distributed under Apache License 2.0 on http://code.google.com/p/alt-aligner/2If the document is printed in greyscale, the lightest background (ex-cept the white one) is actually golden, the second lightest one is red andthe darkest one is dark blue.Chinese word he want to label.
2) Click on the Englishwords he want the Chinese word to be linked, or click onthe empty word to the end of the sentence.
3) If he want todelete a link, he need to click on the English word again,otherwise he can move on to next unlabeled word, or tomodify links on another labeled word.
4) Only when allrequired words are labeled, the user would be allowed toclick on submit button.The interface has two more functionalities, first, it al-lows to specify a subset of words in the sentence for userto label, as shown in the snapshot, words with white back-ground are not required to label.
Secondly it supportsproviding initial alignment on the sentence.Quality controlQuality control is a crucial component of the system.For problems that have clear gold standard answers to aportion of data, the quality control can be done by min-gling the known into the unknown, and rejecting the sub-missions with low qualities on known samples.
Howeverin our situation it is not easy to do so because althoughwe have fully manual aligned sentences, we do not havecorpus in which the sentences are partially aligned, there-fore if we want to use the method we have to let workerlabel an additional sentence, which may double the effortfor the workers.
Also we do not provide thorough stan-dard for users, therefore before we know the divergenceof the alignments, we actually do not know how to set thethreshold, even with given gold standard labels.
In addi-tion, if the method will be applied on languages with lowresource, we cannot assume availability of gold standardanswers.
Therefore, we only try to filter out answers baseon the consensus.
The quality control works as follows.Firstly we assign an alignment task to 2n + 1 workers.For these submissions, we first try to build a majority an-swer from these assignments.
For each alignment link,if it appears in more than n submissions.
Then every in-dividual assignments will be compared to the majorityalignment, so we can get the precision and recall rates.If either precision or recall rate is lower than a threshold,we will reject the submission.Figure 1: A snapshot of the labeling interface.2.2 Pricing and worker baseWe tried two pricing strategies.
The first one fixes thenumber of words that a worker need to label for eachHIT, and fix the rate for each HIT.
The second one always31asks workers to label every word in the sentence, in themean time we vary the rate for each HIT according to thelengths of source sentences.
For each strategy we trieddifferent rates, starting from 10 words per cent.
Howeverwe did not get enough workers even after the price raisedto 2 words per cent.
The result indicates a limited workerbase of Chinese speakers.3 Utilizing the manual alignmentsAs we can expect, given no explicit guideline for wordalignments, the variance of different assignments can befairly large, a question will raise what can we do withthe disagreements?
As we will see later in the experi-ment part, the labels are more likely to be consistent withexpert labels if more workers agree on it.
Therefore, asimple strategy is to use only the links that more workershave consensus on them.2005?
?
?
?The   summer   of    2005Figure 2: Partial and full alignmentsHowever the method instantly gives rise to a prob-lem.
Now the alignment is not ?full alignments?, instead,they are ?partial?.
The claim seems to be trivial but theyhave completely different underlying assumptions.
Fig-ure 2 shows the comparison of partial alignments (thebold link) and full alignments (the dashed and the boldlinks).
In the example, if full alignment is given, we canassert 2005 is only aligned to 2005#, not to {or ,but we cannot do that if only partial alignment is given.In this paper we experiment with a novel method whichuses the partial alignment to constraint the EM algorithmin the parameter estimation of IBM models.IBM Models (Brown et.
al., 1993) are a series of gen-erative models for word alignment.
GIZA++ (Och andNey, 2003) is the most widely used implementation ofIBM models and HMM (Vogel et al, 1996) where EMalgorithm is employed to estimate the model parameters.In the E-step, it is possible to obtain sufficient statisticsfrom all possible alignments for simple models such asModel 1 and Model 2.
Meanwhile for fertility-basedmodels such as Model 3, 4, 5, enumerating all possiblealignments is NP-complete.
In practice, we use sim-pler models such as HMM or Model 2 to generate a?center alignment?
and then try to find better alignmentsamong the neighbors of it.
The neighbors of an alignmentaJ1 = [a1, a2, ?
?
?
, aJ ], aj ?
[0, I] is defined as align-ments that can be generated from aJ1 by one of the oper-ators: 1) Move operator m[i,j], that changes aj := i, i.e.arbitrarily set word fj in source sentence to align to wordei in target sentence; 2) Swap operator s[j1,j2] that ex-changes aj1 and aj2 .
The algorithm will update the centeralignment as long as a better alignment can be found, andfinally outputs a local optimal alignment.
The neighboralignments of the alignment are then used in collectingthe counts for the M Step.In order to use partial manual alignments to constrainthe search space, we separate the algorithm into twostages, first the seed alignment will be optimized towardsthe constraints.
Each iteration we only pick a new centeralignment with less inconsistent links than the originalone, until the alignment is consistent with all constraints.After that, in each iteration we pick the alignment withhighest likelihood but does not introduce any inconsistentlinks.
The algorithm will output a local optimal align-ment consistent with the partial alignment.
When col-lecting the counts for M-step, we also need to exclude allalignments that are not consistent with the partial man-ual alignment.
The task can also be done by skipping theinconsistent alignments in the neighborhood of the localoptimal alignment.4 Experiment and analysisIn this section we will show the analysis of the har-vested MTurk alignments and the results of the semi-supervised word alignment experiments.4.1 Consistency of the manual alignmentsWe first examine the internal consistency of the MTurkalignments.
We calculate the internal consistency ratein both results.
Because we requested three assignmentsfor every question, we classify the links in two differentways.
First, if a link appear in all three submissions, weclassify it as ?consensus link?.
Second, if a link appear inmore than one submissions, we classify it as ?majority?,otherwise it is classified as ?minority?.
Table 1 presentsthe statistics of partial alignment and full alignment tasks.Note that by spending the same amount of money, we getmore sentences aligned because for fixed rate partial sen-tence alignment tasks, sometimes we may have overlapsbetween tasks.
Therefore we also calculate a subset offull alignment tasks that consists of all the sentences inpartial alignment tasks.
The statistics shows that althoughgenerally full alignment tasks generates more links, thepartial alignment tasks gives denser alignments.
It is in-teresting to know whether the denser alignments lead tohigher recall rate or lower precision.4.2 Comparing MTurk and expert alignmentsTo exam the quality of alignments, we compared themwith expert alignments.
Table 2 lists the precision, recalland F-1 scores for partial and full alignment tasks.
Wecompare the consistency of all links, the links in majoritygroup and the consensus links.As we can observe from the results, the Turkers tendto label less links than the experts, Interestingly, the over-all quality of partial alignment tasks is significantly betterthan full alignment tasks.
Despite the lower recall rate, itis encouraging that the majority vote and consensus links32Partial Full Full-IntNumber of sentences 135 239 135Number of words 2,008 3,241 2,008Consensus words 13,03 2,299 1,426Consensus rate(%) 64.89 70.93 71.02Total Links 7,508 9,767 6,114Consensus Links 5,625 7,755 4,854Consensus Rate(%) 74.92 79.40 79.39Total Unique Links 3,186 3,989 2,506Consensus Links 1,875 2,585 1,618Consensus Rate(%) 58.85 64.80 64.54In majority group 2,447 3,193 1,426Majority rate(%) 76.80 80.04 71.06Table 1: Internal consistency of manual alignments, hereFull-Int means statistics of full alignment tasks on thesentences that also aligned using partial alignment taskAll Links Majority Links Consensus LinksP.
R. F. P. R. F. P. R. F.P 0.84 0.88 0.86 0.95 0.76 0.84 0.98 0.60 0.74F 0.88 0.70 0.78 0.96 0.61 0.75 0.99 0.51 0.68I 0.87 0.71 0.79 0.95 0.62 0.75 0.98 0.52 0.68Table 2: Consistency of MTurk alignments with expertalignments, showing precision (P), recall (R) and F1 (F)between MTurk and expert alignments.
P, F, and I corre-spond to Partial, Full and Full-Int in Table 1yield very high precisions against expert alignments.
Ta-ble 3 lists the words with most errors.
Most errors occuron function words.
A manual review shows that morethan 85% errors have function words on either Chineseside or English side.
The result, however, is as expectedbecause these words are hard to label and we did not pro-vide clear rule for function words.4.3 Results of semi-supervised word alignmentIn this experiment we try to use the alignment links inthe semi-supervised word alignment algorithm.
We useChinese-English manually aligned corpus in the exper-iments, which contains 21,863 sentence pairs, 424,683Chinese words and 524,882 English words.
First, we usethe parallel corpus to train IBM models without any man-ual alignments, we run 5 iterations of model 1 and HMM,Chinese EnglishFN FP FN FP64 { 16 , 122 the 15 ,26 4 11 ?
67 NULL 11 a19 , 9 4 43 of 6 the17 ?
3 ?
36 to 6 is16 ??
3 ?
24 a 4 toTable 3: Words that most errors occur, FN means a falsenegative error occurred on the word, i.e.
a link to thisword or from this word is missing.
FP means false pos-itive, accordingly.
The manual alignment links comesfrom majority vote.3 iterations of model 3 and 6 iterations of model 4.
Thenwe resume the training procedure from the third itera-tions of model 4.
This time we load the manual alignmentlinks and perform 3 iterations of constrained EM.
We alsoexperiment with 3 different sets of alignments.
Table 4presents the improvements on the alignment quality.UnsupervisedCh-En En-ChPrec.
Recall AER Prec.
Recall AER68.22 46.88 44.43 65.35 55.05 40.24All LinksPartial 68.28 47.09 44.26 65.86 55.63 39.68Full-Int 68.28 47.09 44.26 65.85 55.63 39.69Full 68.37 47.15 44.19 65.90 55.67 39.65Majority LinksPartial 68.28 47.08 44.27 65.84 55.62 39.70Full-Int 68.28 47.08 44.27 65.84 55.61 39.71Full 68.37 47.13 44.20 65.88 55.65 39.67Consensus LinksPartial 68.24 47.06 44.30 65.83 55.60 39.71Full-Int 68.25 47.06 44.29 65.83 55.60 39.72Full 68.31 47.10 44.25 65.86 55.63 39.68Table 4: The performance of using manual alignments insemi-supervised word alignmentFrom the result we can see that given the same amountof links the improvement of alignment error rate is gen-erally the same for partial and full alignment tasks, how-ever, if we consider the amount of money spent on thetask, the full alignment task collect much more data thanpartial alignments, we consider full sentence alignmentmore cost efficient in this sense.5 ConclusionIn this pilot experiment, we explore the possibility ofusing Amazon Mechanical Turk (MTurk) to collect bilin-gual word alignment data to assist automatic word align-ment.
We develop a system including a word align-ment interface based on Javascript and a quality controlscheme.
To utilize the manual alignments, we develop asemi-supervised word alignment algorithm that can per-form constrained EM with partial alignments.
The algo-rithm enables us to use only the most reliable links bymajority vote or consensus.
The effectiveness of thesemethods is proven by small-scale experiments.
The re-sults show the manual alignments from MTurk have highprecision with expert word alignment, especially whenfiltered by majority vote or consensus.
We get small im-provement on semi-supervised word alignment.
Giventhe promising results, it is interesting to see if the ten-dency will carry on when we scale up the experiments.However the experiment also shows some problems,first the coverage of worker base on MTurk is limited.Given small worker base for specific languages, the costefficiency for NLP tasks in those languages is question-able.33ReferencesP.
Blunsom and T. Cohn.
2006.
Discriminative word align-ment with conditional random fields.
In Proceedings of the21st International Conference on Computational Linguisticsand the 44th annual meeting of the Association for Compu-tational Linguistics, pages 65?72.P.
F. Brown et.
al.
1993.
The mathematics of statistical ma-chine translation: Parameter estimation.
In ComputationalLinguistics, volume 19(2), pages 263?331.C.
Callison-Burch, D. Talbot, and D. Osborne.
2004.
Statisticalmachine translation with word- and sentence-aligned parallelcorpora.
In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics (ACL-2004).C.
Callison-Burch.
2009.
Fast, cheap, and creative: Evaluat-ing translation quality using Amazon?s Mechanical Turk.
InProceedings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 286?295.
Associa-tion for Computational Linguistics.A.
Fraser and D. Marcu.
2006.
Semi-supervised training forstatistical word alignment.
In ACL-44: Proceedings of the21st International Conference on Computational Linguisticsand the 44th annual meeting of the Association for Compu-tational Linguistics, pages 769?776.M.
Kaisser and J.
B. Lowe.
2008.
A research collection ofquestion answer sentence pairs.
In Proceedings of The 6thLanguage Resources and Evaluation Conference.M.
Kaisser, M. Hearst, and J.B. Lowe.
2008.
Evidence forvarying search results summary lengths.
In Proceedings ofthe 46th Annual Meeting of the Association for Computa-tional Linguistics.A.
Kittur, E. H. Chi, and B Suh.
2008.
Crowdsourcing userstudies with mechanical turk.
In CHI ?08: Proceeding of thetwenty-sixth annual SIGCHI conference on Human factors incomputing systems, pages 453?456.Y.
Liu, Q. Liu, and S. Lin.
2005.
Log-linear models for wordalignment.
In ACL ?05: Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics, pages459?466.A.
Lopez and P. Resnik.
2006.
Word-based alignment, phrase-based translation: What?s the link?
with philip resnik.
InProceedings of the 7th Biennial Conference of the Associa-tion for Machine Translation in the Americas (AMTA-2006).W.
Mason and D. J. Watts.
2009.
Financial incentives and the?performance of crowds?.
In HCOMP ?09: Proceedings ofthe ACM SIGKDD Workshop on Human Computation, pages77?85.R.
C Moore.
2005.
A discriminative framework for bilingualword alignment.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methods in Natu-ral Language Processing, pages 81?88.J.
Niehues and S. Vogel.
2008.
Discriminative word alignmentvia alignment matrix modeling.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation, pages 18?25.F.
J. Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
In Computational Linguis-tics, volume 1:29, pages 19?51.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A discrim-inative matching approach to word alignment.
In Proceed-ings of the conference on Human Language Technology andEmpirical Methods in Natural Language Processing, pages73?80.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM based wordalignment in statistical machine translation.
In Proceedingsof 16th International Conference on Computational Linguis-tics), pages 836?841.34
