Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 103?108,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultilingual semantic parsing with a pipeline of linear classifiersOscar Ta?ckstro?mSwedish Institute of Computer ScienceSE-16429, Kista, Swedenoscar@sics.seAbstractI describe a fast multilingual parser for seman-tic dependencies.
The parser is implementedas a pipeline of linear classifiers trained withsupport vector machines.
I use only first or-der features, and no pair-wise feature combi-nations in order to reduce training and pre-diction times.
Hyper-parameters are carefullytuned for each language and sub-problem.The system is evaluated on seven differentlanguages: Catalan, Chinese, Czech, English,German, Japanese and Spanish.
An analysisof learning rates and of the reliance on syn-tactic parsing quality shows that only modestimprovements could be expected for most lan-guages given more training data; Better syn-tactic parsing quality, on the other hand, couldgreatly improve the results.
Individual tun-ing of hyper-parameters is crucial for obtain-ing good semantic parsing quality.1 IntroductionThis paper presents my submission for the seman-tic parsing track of the CoNLL 2009 shared task onsyntactic and semantic dependencies in multiple lan-guages (Hajic?
et al, 2009).
The submitted parser issimpler than the submission in which I participatedat the CoNLL 2008 shared task on joint learning ofsyntactic and semantic dependencies (Surdeanu etal., 2008), in which we used a more complex com-mittee based approach to both syntax and semantics(Samuelsson et al, 2008).
Results are on par withour previous system, while the parser is orders ofmagnitude faster both at training and prediction timeand is able to process natural language text in Cata-lan, Chinese, Czech, English, German, Japanese andSpanish.
The parser depends on the input to be anno-tated with part-of-speech tags and syntactic depen-dencies.2 Semantic parserThe semantic parser is implemented as a pipeline oflinear classifiers and a greedy constraint satisfactionpost-processing step.
The implementation is verysimilar to the best performing subsystem of the com-mittee based system in Samuelsson et al (2008).Parsing consists of four steps: predicate sensedisambiguation, argument identification, argumentclassification and predicate frame constraint satis-faction.
The first three steps are implemented us-ing linear classifiers, along with heuristic filteringtechniques.
Classifiers are trained using the sup-port vector machine implementation provided by theLIBLINEAR software (Fan et al, 2008).
MALLETis used as a framework for the system (McCallum,2002).For each classifier, the c-parameter of the SVMis optimised by a one dimensional grid search usingthreefold cross validation on the training set.
Forthe identification step, the c-parameter is optimisedwith respect to F1-score of the positive class, whilefor sense disambiguation and argument labelling theoptimisation is with respect to accuracy.
The regionsto search were identified by initial runs on the devel-opment data.
Optimising these parameters for eachclassification problem individually proved to be cru-cial for obtaining good results.2.1 Predicate sense disambiguationSince disambiguation of predicate sense is a multi-class problem, I train the classifiers using the methodof Crammer and Singer (2002), using the implemen-tation provided by LIBLINEAR.
Sense labels do notgeneralise over predicate lemmas, so one classifieris trained for each lemma occurring in the trainingdata.
Rare predicates are given the most commonsense of the predicate.
Predicates occurring less than1037 times in the training data were heuristically deter-mined to be considered rare.
Predicates with unseenlemmas are labelled with the most common sensetag in the training data.2.1.1 Feature templatesThe following feature templates are used for predi-cate sense disambiguation:PREDICATEWORDPREDICATE[POS/FEATS]PREDICATEWINDOWBAGLEMMASPREDICATEWINDOWPOSITION[POS/FEATS]GOVERNORRELATIONGOVERNOR[WORD/LEMMA]GOVERNOR[POS/FEATS]DEPENDENTRELATIONDEPENDENT[WORD/LEMMA]DEPENDENT[POS/FEATS]DEPENDENTSUBCAT.The *WINDOW feature templates extract featuresfrom the two preceding and the two following tokensaround the predicate, with respect to the linear order-ing of the tokens.
The *FEATS templates are basedon information in the PFEATS input column for thelanguages where this information is provided.2.2 Argument identification and labellingIn line with most previous pipelined systems, iden-tification and labelling of arguments are performedas two separate steps.
The classifiers in the identi-fication step are trained with the standard L2-lossSVM formulation, while the classifiers in the la-belling step are trained using the method of Cram-mer and Singer.In order to reduce the number of candidate argu-ments in the identification step, I apply the filter-ing technique of Xue and Palmer (2004), triviallyadopted to the dependency syntax formalism.
Fur-ther, a filtering heuristic is applied in which argu-ment candidates with rare predicate / argument part-of-speech combinations are removed; rare meaningthat the argument candidate is actually an argumentin less than 0.05% of the occurrences of the pair.These heuristics greatly reduce the number of in-stances in the argument identification step and im-prove performance by reducing noise from the train-ing data.Separate classifiers are trained for verbal pred-icates and for nominal predicates, both in orderto save computational resources and because theframe structures do not generalise between verbaland nominal predicates.
For Czech, in order to re-duce training time I split the argument identificationproblem into three sub-problems: verbs, nouns andothers, based on the part-of-speech of the predicate.In hindsight, after solving a file encoding related bugwhich affected the separability of the Czech dataset, a split into verbal and nominal predicates wouldhave sufficed.
Unfortunately I was not able to rerunthe Czech experiments on time.2.2.1 Feature templatesThe following feature templates are used both forargument identification and argument labelling:PREDICATELEMMASENSEPREDICATE[POS/FEATS]POSITIONARGUMENT[POS/FEATS]ARGUMENT[WORD/LEMMA]ARGUMENTWINDOWPOSITIONLEMMAARGUMENTWINDOWPOSITION[POS/FEATS]LEFTSIBLINGWORDLEFTSIBLING[POS/FEATS]RIGHTSIBLINGWORDRIGHTSIBLING[POS/FEATS]LEFTDEPENDENTWORDRIGHTDEPENDENT[POS/FEATS]RELATIONPATHTRIGRAMRELATIONPATHGOVERNORRELATIONGOVERNORLEMMAGOVERNOR[POS/FEATS]Most of these features, introduced by Gildea and Ju-rafsky (2002), belong to the folklore by now.
TheTRIGRAMRELATIONPATH is a ?soft?
version of theRELATIONPATH template, which treats the relationpath as a bag of triplets of directional labelled depen-dency relations.
Initial experiments suggested thatthis feature slightly improves performance, by over-coming local syntactic parse errors and data sparse-ness in the case of small training sets.2.2.2 Predicate frame constraintsFollowing Johansson and Nugues (2008) I imposethe CORE ARGUMENT CONSISTENCY and CON-104TINUATION CONSISTENCY constraints on the gen-erated semantic frames.
In the cited work, theseconstraints are used to filter the candidate framesfor a re-ranker.
I instead perform a greedy searchin which only the core argument with the highestscore is kept when the former constraint is violated.The latter constraint is enforced by simply droppingany continuation argument lacking its correspond-ing core argument.
Initial experiments on the de-velopment data indicates that these simple heuristicsslightly improves semantic parsing quality measuredwith labelled F1-score.
It is possible that the im-provement could be greater by using L2-regularisedlogistic regression scores instead of the SVM scores,since the latter can not be interpreted as probabili-ties.
However, logistic regression performed consis-tently worse than the SVM formulation of Crammerand Singer in the argument labelling step.2.2.3 Handling of multi-function argumentsIn Czech and Japanese an argument can have multi-ple relations to the same predicate, i.e.
the seman-tic structure needs sometimes be represented by amulti-graph.
I chose the simplest possible solutionand treat these structures as ordinary graphs withcomplex labels.
This solution is motivated by thefact that the palette of multi-function arguments issmall, and that the multiple functions mostly arehighly interdependent, such as in the ACT|PAT com-plex which is the most common in Czech.3 ResultsThe semantic parser was evaluated on in-domaindata for Catalan, Chinese, Czech, English, German,Japanese and Spanish, and on out-of-domain datafor Czech, English and German.
The respectivedata sets are described in Taule?
et al (2008), Palmerand Xue (2009), Hajic?
et al (2006), Surdeanu et al(2008), Burchardt et al (2006) and Kawahara et al(2002).My official submission scores are given in table1, together with post submission labelled and un-labelled F1-scores.
The official submissions wereaffected by bugs related to file encoding and hyper-parameter search.
After resolving these bugs, I ob-tained an improvement of mean F1-score of almost10 absolute points compared to the official scores.Lab F1 Lab F1 Unlab F1Catalan 57.11 67.14 93.31Chinese 63.41 74.14 82.57Czech 71.05 78.29 89.20English 67.64 78.93 88.70German 53.42 62.98 89.64Japanese 54.74 61.44 66.01Spanish 61.51 69.93 93.54Mean 61.27 70.41 86.14Czech?
71.59 78.77 87.13English?
59.82 68.96 86.23German?
50.43 47.81 79.52Mean?
60.61 65.18 84.29Table 1: Semantic labelled and unlabelled F1-scores foreach language and domain.
Left column: official labelledF1-score.
Middle column: post submission labelled F1-score.
Right column: post submission unlabelled F1-score.
?
indicates out-of-domain test data.Clearly, there is a large difference in performancefor the different languages and domains.
As couldbe expected the parser performs much better for thelanguages for which a large training set is provided.However, as discussed in the next section, simplyadding more training data does not seem to solve theproblem.Comparing unlabelled F1-scores with labelledF1-scores, it seems that argument identification andlabelling errors contribute almost equally to the totalerrors for Chinese, Czech and English.
For Catalan,Spanish and German argument identification scoresare high, while labelling scores are in the lowerrange.
Japanese stands out with exceptionally lowidentification scores.
Given that the quality of thepredicted syntactic parsing was higher for Japanesethan for any other language, the bottleneck whenperforming semantic parsing seems to be the limitedexpressivity of the Japanese syntactic dependencyannotation scheme.Interestingly, for Czech, the result on the out-of-domain data set is better than the result on the in-domain data set, even though the unlabelled resultis slightly worse.
For English, on the other handthe performance drop is in the order of 10 absolutelabelled F1 points, while the drop in unlabelled F1-score is comparably small.
The result on Germanout-of-domain data seems to be an outlier, with post-submission results even worse than the official sub-10510% 25% 50% 75% 100%Catalan 54.86 60.52 65.22 66.35 67.14Chinese 72.93 73.40 73.77 74.08 74.14Czech 75.42 76.90 77.69 78.00 78.29English 75.75 77.56 78.37 78.71 78.93German 47.77 54.74 58.94 61.02 62.98Japanese 59.82 60.34 60.99 61.37 61.44Spanish 58.80 64.32 68.35 69.34 69.93Mean 63.62 66.83 69.05 69.84 70.41Czech?
76.51 77.48 78.41 78.59 78.77English?
66.04 67.54 68.37 69.00 68.96German?
41.65 45.94 46.24 47.45 47.81Mean?
61.40 63.65 64.34 65.01 65.18Table 2: Semantic labelled F1-scores w.r.t.
training setsize.
?
indicates out-of-domain test data.mission results.
I suspect that this is due to a bug.3.1 Learning ratesIn order to assess the effect of training set size onsemantic parsing quality, I performed a learning rateexperiment, in which the proportion of the trainingset used for training was varied in steps between10% and 100% of the full training set size.Learning rates with respect to labelled F1-scoresare given in table 2.
The improvement in scores aremodest for Chinese, Czech, English and Japanese,while Catalan, German and Spanish stand out byvast improvements with additional training data.However, the improvement when going from 75% to100% of the training data is only modest for all lan-guages.
With the exception for English, for whichthe parser achieves the highest score, the relativelabelled F1-scores follow the relative sizes of thetraining sets.Looking at learning rates with respect to unla-belled F1-scores, given in table 3, it is evident thatadding more training data only has a minor effect onthe identification of arguments.From table 4, one can see that predicate sense dis-ambiguation is the sub-task that benefits most fromadditional training data.
This is not surprising, sincethe senses does not generalise, and hence we cannothope to correctly label the senses of unseen predi-cates; the only way to improve results with the cur-rent formalism seems to be by adding more trainingdata.The limited power of a pipeline of local classi-10% 25% 50% 75% 100%Catalan 93.12 93.18 93.28 93.35 93.31Chinese 82.37 82.45 82.54 82.55 82.57Czech 89.03 89.12 89.17 89.21 89.20English 87.96 88.38 88.52 88.67 88.70German 88.23 89.02 89.63 89.53 89.64Japanese 65.64 65.75 65.88 66.02 66.01Spanish 93.52 93.49 93.52 93.53 93.54Mean 85.70 85.91 86.08 86.12 86.14Czech?
86.76 87.02 87.16 87.08 87.13English?
85.67 86.14 86.22 86.20 86.23German?
77.35 78.31 79.09 79.10 79.52Mean?
83.26 83.82 84.16 84.13 84.29Table 3: Semantic unlabelled F1-scores w.r.t.
training setsize.
?
indicates out-of-domain test data.10% 25% 50% 75% 100%Catalan 30.61 40.29 53.83 55.83 58.95Chinese 94.06 94.37 94.71 95.10 95.26Czech 83.24 84.75 85.78 86.21 86.60English 92.18 93.68 94.83 95.35 95.60German 34.91 47.27 58.18 62.18 66.55Japanese 99.07 99.07 99.07 99.07 99.07Spanish 38.53 50.22 59.59 62.01 66.26Mean 67.51 72.81 78.00 79.39 81.18Czech?
89.05 89.88 91.06 91.38 91.56English?
83.64 84.27 84.83 85.70 85.94German?
33.64 43.36 42.59 44.44 45.22Mean?
68.78 72.51 72.83 73.84 74.24Table 4: Predicate sense disambiguation F1-scores w.r.t.training set size.
?
indicates out-of-domain test data.fiers shows itself in the exact match scores, givenin table 5.
This problem is clearly not remedied byadditional training data.3.2 Dependence on syntactic parsing qualitySince I only participated in the semantic parsingtask, the results reported above rely on the providedpredicted syntactic dependency parsing.
In order toinvestigate the effect of parsing quality on the cur-rent system, I performed the same learning curveexperiments with gold standard parse information.These results, shown in tables 6 and 7, give an upperbound on the possible improvement of the currentsystem by means of improved parsing quality, giventhat the same syntactic annotation formalism is used.Labelled F1-scores are greatly improved for alllanguages except for Japanese, when using gold10610% 25% 50% 75% 100%Catalan 6.77 9.08 11.39 11.17 12.24Chinese 17.02 17.33 17.61 17.76 17.68Czech 9.33 9.59 9.97 9.95 10.11English 12.01 12.76 12.96 13.13 13.17German 76.95 78.50 78.95 79.20 79.50Japanese 1.20 1.40 1.80 1.60 1.60Spanish 8.23 10.20 12.93 13.39 13.16Mean 18.79 19.84 20.80 20.89 21.07Czech?
2.53 2.79 2.79 2.87 2.87English?
19.06 19.53 19.76 20.00 20.00German?
15.98 19.24 17.82 19.94 20.08Mean?
12.52 13.85 13.46 14.27 14.32Table 5: Percentage of exactly matched predicate-argument frames w.r.t.
training set size.
?
indicates out-of-domain test data.10% 25% 50% 75% 100%Catalan 62.65 72.50 75.39 77.03 78.86Chinese 82.59 83.23 83.90 83.94 84.03Czech 79.15 80.62 81.46 81.91 82.24English 79.84 81.74 82.65 83.01 83.25German 52.15 60.66 65.12 65.71 68.36Japanese 60.85 61.76 62.55 62.85 63.23Spanish 66.40 72.47 75.70 77.73 78.38Mean 69.09 73.28 75.25 76.03 76.91Czech?
78.64 80.07 80.77 81.01 81.20English?
73.05 74.18 74.99 75.28 75.81German?
52.06 52.77 54.72 56.22 56.35Mean?
67.92 69.01 70.16 70.84 71.12Table 6: Semantic labelled F1-scores w.r.t.
training setsize, using gold standard syntactic and part-of-speech tagannotation.
?
indicates out-of-domain test data.standard syntactic and part-of-speech annotations.For Catalan, Chinese and Spanish the improvementis in the order of 10 absolute points.
For Japanesethe improvement is a meagre 2 absolute points.
Thisis not surprising given that the quality of the pro-vided syntactic parsing was already very high forJapanese, as discussed previously.Results with respect to unlabelled F1-scores fol-low the same pattern as for labelled F1-scores.Again, with Japanese the semantic parsing does notbenefit much from better syntactic parsing quality.For Catalan and Spanish on the other hand, the iden-tification of arguments is almost perfect with goldstandard syntax.
The poor labelling quality for theselanguages can thus not be attributed to the syntactic10% 25% 50% 75% 100%Catalan 99.94 99.98 99.99 99.99 99.99Chinese 92.55 92.67 92.72 92.63 92.62Czech 91.21 91.27 91.30 91.30 91.31English 92.34 92.61 92.85 92.89 92.95German 93.46 93.59 94.08 93.85 94.14Japanese 66.98 67.20 67.58 67.62 67.74Spanish 99.99 99.99 100.00 100.00 100.00Mean 90.92 91.04 91.22 91.18 91.25Czech?
89.00 89.22 89.34 89.38 89.36English?
92.71 92.56 92.91 93.06 93.04German?
90.54 90.23 90.77 90.86 90.99Mean?
90.75 90.67 91.01 91.10 91.13Table 7: Semantic unlabelled F1-scores w.r.t.
training setsize, using gold standard syntactic and part-of-speech tagannotation.
?
indicates out-of-domain test data.parse quality.3.3 Computational requirementsTraining and prediction times on a 2.3 GHz quad-core AMD OpteronTMsystem are given in table 8.Since only linear classifiers and no pair-wise featurecombinations are used, training and prediction timesare quite modest.
Verbal and nominal predicates aretrained in parallel, no additional parallelisation isemployed.
Most of the training time is spent on op-timising the c parameter of the SVM.
Training timesare roughly ten times as long as compared to trainingtimes with no hyper-parameter optimisation.
Czechstands out as much more computationally demand-ing, especially in the sense disambiguation trainingstep.
The reason is the vast number of predicates inCzech compared to the other languages.
The ma-jority of the time in this step is, however, spent onwriting the SVM training problems to disk.Memory requirements range between approxi-mately 1 Gigabytes for the smallest data sets and6 Gigabytes for the largest data set.
Memory us-age could be lowered substantially by using a morecompact feature dictionary.
Currently every featuretemplate / value pair is represented as a string, whichis wasteful since many feature templates share thesame values.4 ConclusionsI have presented an effective multilingual pipelinedsemantic parser, using linear classifiers and a simple107Sense ArgId ArgLab Tot PredCatalan 7m 11m 33m 51m 13sChinese 7m 13m 22m 42m 15sCzech 10h 1h 1.5h 12.5h 34.5mEnglish 16m 14m 28m 58m 14.5sGerman 4m 2m 5m 13m 3.5sJapanese 1s 1m 4m 5m 4sSpanish 10m 16m 40m 1.1h 13sTable 8: Training times for each language and sub-problem and approximate prediction times.
Columns:training times for sense disambiguation (Sense), ar-gument identification (ArgId), argument labelling (Ar-gLab), total training time (Tot), and total prediction time(Pred).
Training times are measured w.r.t.
to the unionof the official training and development data sets.
Predic-tion times are measured w.r.t.
to the official evaluationdata sets.greedy constraint satisfaction heuristic.
While thesemantic parsing results in these experiments fail toreach the best results given by other experiments, theparser quickly delivers quite accurate semantic pars-ing of Catalan, Chinese, Czech, English, German,Japanese and Spanish.Optimising the hyper-parameters of each of theindividual classifiers is essential for obtaining goodresults with this simple architecture.
Syntactic pars-ing quality has a large impact on the quality of thesemantic parsing; a problem that is not remedied byadding additional training data.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Koby Crammer and Yoram Singer.
2002.
On the learn-ability and design of output codes for multiclass prob-lems.
Machine Learning, 47(2):201?233, May.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
PragueDependency Treebank 2.0.
CD-ROM, Cat.
No.LDC2006T01, ISBN 1-58563-370-4, Linguistic DataConsortium, Philadelphia, Pennsylvania, USA.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In Proceedings of theShared Task Session of CoNLL-2008, Manchester,UK.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Yvonne Samuelsson, Oscar Ta?ckstro?m, SumithraVelupillai, Johan Eklund, Mark Fishel, and MarkusSaers.
2008.
Mixing and blending syntactic andsemantic dependencies.
In CoNLL 2008: Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, pages 248?252, Manchester,England, August.
Coling 2008 Organizing Committee.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008), Manchester, Great Britain.Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 88?94, Barcelona, Spain, July.
Association forComputational Linguistics.108
