Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 138?146,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Intrinsic Stopping Criterion for Committee-Based Active LearningFredrik OlssonSICSBox 1263SE-164 29 Kista, Swedenfredrik.olsson@sics.seKatrin TomanekJena University Language & Information Engineering LabFriedrich-Schiller-Universita?t JenaFu?rstengraben 30, D-07743 Jena, Germanykatrin.tomanek@uni-jena.deAbstractAs supervised machine learning methods areincreasingly used in language technology, theneed for high-quality annotated language databecomes imminent.
Active learning (AL) isa means to alleviate the burden of annotation.This paper addresses the problem of knowingwhen to stop the AL process without havingthe human annotator make an explicit deci-sion on the matter.
We propose and evaluatean intrinsic criterion for committee-based ALof named entity recognizers.1 IntroductionWith the increasing popularity of supervised ma-chine learning methods in language processing, theneed for high-quality labeled text becomes immi-nent.
On the one hand, the amount of readily avail-able texts is huge, while on the other hand the la-beling and creation of corpora based on such texts istedious, error prone and expensive.Active learning (AL) is one way of approachingthe challenge of classifier creation and data annota-tion.
Examples of AL used in language engineeringinclude named entity recognition (Shen et al, 2004;Tomanek et al, 2007), text categorization (Lewisand Gale, 1994; Hoi et al, 2006), part-of-speechtagging (Ringger et al, 2007), and parsing (Thomp-son et al, 1999; Becker and Osborne, 2005).AL is a supervised machine learning technique inwhich the learner is in control of the data used forlearning ?
the control is used to query an oracle, typ-ically a human, for the correct label of the unlabeledtraining instances for which the classifier learned sofar makes unreliable predictions.The AL process takes as input a set of labeled in-stances and a larger set of unlabeled instances, andproduces a classifier and a relatively small set ofnewly labeled data.
The overall goal is to obtainas good a classifier as possible, without having tomark-up and supply the learner with more than nec-essary data.
The learning process aims at keepingthe human annotation effort to a minimum, only ask-ing for advice where the training utility of the resultof such a query is high.The approaches taken to AL in this paper arebased on committees of classifiers with access topools of data.
Figure 1 outlines a prototypicalcommittee-based AL loop.
In this paper we focuson the question when AL-driven annotation shouldbe stopped (Item 7 in Figure 1).Usually, the progress of AL is illustrated bymeans of a learning curve which depicts how theclassifier?s performance changes as a result of in-creasingly more labeled training data being avail-able.
A learning curve might be used to addressthe issue of knowing when to stop the learning pro-cess ?
once the curve has leveled out, that is, whenadditional training data does not contribute (much)to increase the performance of the classifier, the ALprocess may be terminated.
While in a random se-lection scenario, classifier performance can be esti-mated by cross-validation on the labeled data, ALrequires a held-out annotated reference corpus.
InAL, the performance of the classifier cannot be re-liably estimated using the data labeled in the pro-cess since sampling strategies for estimating per-formance assume independently and identically dis-tributed examples (Schu?tze et al, 2006).
The wholepoint in AL is to obtain a distribution of instancesthat is skewed in favor of the base learner used.1381.
Initialize the process by applying EnsembleGeneration-Method using base learner B on labeled training data setDL to obtain a committee of classifiers C.2.
Have each classifier inC predict a label for every instancein the unlabeled data set DU , obtain labeled set DU ?.3.
From DU ?, select the most informative n instances tolearn from, obtaining DU ??.4.
Ask the teacher for classifications of the instances I inDU ??.5.
Move I , with supplied classifications, from DU to DL.6.
Re-train using EnsembleGenerationMethod and baselearner B on the newly extended DL to obtain a new com-mittee, C.7.
Repeat steps 2 through 6 until DU is empty or some stop-ping criterion is met.8.
Output classifier learned using EnsembleGeneration-Method and base learner B on DL.Figure 1: A prototypical query by committee algorithm.In practice, however, an annotated reference cor-pus is rarely available and its creation would be in-consistent with the goal of creating a classifier withas little human effort as possible.
Thus, other waysof deciding when to stop AL are needed.
In this pa-per, we propose an intrinsic stopping-criterion forcommittee-based AL of named entity recognizers.It is intrinsic in that it relies on the characteristics ofthe data and the base learner1 rather than on exter-nal parameters, i.e., the stopping criterion does notrequire any pre-defined thresholds.The paper is structured as follows.
Section 2sketches interpretations of ideal stopping points anddescribes the idea behind our stopping criterion.Section 3 outlines related work.
Section 4 describesthe experiments we have conducted concerning anamed entity recognition scenario, while Section 5presents the results which are then discussed in Sec-tion 6.
Section 7 concludes the paper.2 A stopping criterion for active learningWhat is the ideal stopping point for AL?
Obviously,annotation should be stopped at the latest when the1The term base learner (configuration) refers to the combi-nation of base learner, parameter settings, and data representa-tion.best classifier for a scenario is yielded.
However, de-pending on the scenario at hand, the ?best?
classifiercould have different interpretations.
In many paperson AL and stopping criteria, the best (or optimal)classifier is the one that yields the highest perfor-mance on a test set.
It is assumed that AL-basedannotation should be stopped as soon as this per-formance is reached.
This could be generalized asstopping criteria based on maximal classifier perfor-mance.
In practice, the trade-off between annota-tion effort and classifier performance is related to theachievable performance given the learner configura-tion and data under scrutiny: For instance, would weinvest many hours of additional annotation effort justto possibly increase the classifier performance by afraction of a percent?
In this context, a stopping cri-terion may be based on classifier performance con-vergence, and consequently, we can define the bestpossible classifier to be one which cannot learn morefrom the remaining pool of data.The intrinsic stopping criterion (ISC) we proposehere focuses on the latter aspect of the ideal stop-ping point described above ?
exhaustiveness of theAL pool.
We suggest to stop the annotation processof the data from a given pool when the base learnercannot learn (much) more from it.
The definition ofour intrinsic stopping criterion for committee-basedAL builds on the notions of Selection Agreement(Tomanek et al, 2007), and Validation Set Agree-ment (Tomanek and Hahn, 2008).The Selection Agreement (SA) is the agreementamong the members of a decision committee re-garding the classification of the most informative in-stance selected from the pool of unlabeled data ineach AL round.
The intuition underlying the SA isthat the committee will agree more on the hard in-stances selected from the remaining set of unlabeleddata as the AL process proceeds.
When the mem-bers of the committee are in complete agreement,AL should be aborted since it no longer contributesto the overall learning process ?
in this case, AL isbut a computationally expensive counterpart of ran-dom sampling.
However, as pointed out by Tomaneket al (2007), the SA hardly ever signals completeagreement and can thus not be used as the sole in-dicator of AL having reached the point at which itshould be aborted.The Validation Set Agreement (VSA) is the agree-139ment among the members of the decision commit-tee concerning the classification of a held-out, unan-notated data set (the validation set).
The validationset stays the same throughout the entire AL process.Thus, the VSA is mainly affected by the perfor-mance of the committee, which in turn, is groundedin the information contained in the most informativeinstances in the pool of unlabeled data.
Tomanekand colleagues argue that the VSA is thus a goodapproximation of the (progression of the) learningcurve and can be employed as decision support forknowing when to stop annotating ?
from the slope ofthe VSA curve one can read whether further annota-tion will result in increased classifier performance.We combine the SA and the VSA into a singlestopping criterion by relating the agreement of thecommittee on a held-out validation set with that onthe (remaining) pool of unlabeled data.
If the SAis larger than the VSA, it is a signal that the deci-sion committee is more in agreement concerning themost informative instances in the (diminishing) un-labeled pool than it is concerning the validation set.This, in turn, implies that the committee would learnmore from a random sample2 from the validation set(or from a data source exhibiting the same distribu-tion of instances), than it would from the unlabeleddata pool.
Based on this argument, a stopping crite-rion for committee-based AL can be formulated as:Active learning may be terminated whenthe Selection Agreement is larger than, orequal to, the Validation Set Agreement.In relation to the stopping criterion based solelyon SA proposed by Tomanek et al (2007), the abovedefined criterion comes into effect earlier in theAL process.
Furthermore, while it was claimed in(Tomanek and Hahn, 2008) that one can observe theclassifier convergence from the VSA curve (as it ap-proximated the progression of the learning curve),that requires a threshold to be specified for the ac-tual stopping point.
The ISC is completely intrinsicand does thus not require any thresholds to be set.3 Related workSchohn and Cohn (2000) report on document clas-sification using AL with Support Vector Machines.2The sample has to be large enough to mimic the distributionof instances in the original unlabeled pool.If the most informative instance is no closer to thedecision hyperplane than any of the support vectors,the margin has been exhausted and AL is terminated.Vlachos (2008) suggests to use classifier confi-dence to define a stopping criterion for uncertainty-based sampling.
The idea is to stop learning whenthe confidence of the classifier, on an external, pos-sibly unannotated test set, remains at the same levelor drops for a number of consecutive iterations dur-ing the AL process.
Vlachos shows that the criterionindeed is applicable to the tasks he investigates.Zhu and colleagues (Zhu and Hovy, 2007;Zhu et al, 2008a; Zhu et al, 2008b) introducemax-confidence, min-error, minimum expected er-ror strategy, overall-uncertainty, and classification-change as means to terminate AL.
They primar-ily use a single-classifier approach to word sensedisambiguation and text classification in their ex-periments.
Max-confidence seeks to terminate ALonce the classifier is most confident in its predic-tions.
In the min-error strategy, the learning is haltedwhen there is no difference between the classifier?spredictions and those labels provided by a humanannotator.
The minimum expected error strategyinvolves estimating the classification error on fu-ture unlabeled instances and stop the learning whenthe expected error is as low as possible.
Overall-uncertainty is similar to max-confidence, but unlikethe latter, overall-uncertainty takes into account alldata remaining in the unlabeled pool when estimat-ing the uncertainty of the classifier.
Classification-change builds on the assumption that the most in-formative instance is the one which causes the clas-sifier to change the predicted label of the instance.Classification-change-based stopping is realized byZhu and colleagues such that AL is terminated onceno predicted label of the instances in the unlabeledpool change during two consecutive AL iterations.Laws and Schu?tze (2008) investigate three waysof terminating uncertainty-based AL for named en-tity recognition ?
minimal absolute performance,maximum possible performance, and convergence.The minimal absolute performance of the systemis set by the user prior to starting the AL process.The classifier then estimates its own performanceusing a held-out unlabeled data set.
Once the per-formance is reached, the learning is terminated.
Themaximum possible performance strategy refers to140the optimal performance of the classifier given thedata.
Once the optimal performance is achieved, theprocess is aborted.
Finally, the convergence crite-rion aims to stop the learning process when the poolof available data does not contribute to the classi-fier?s performance.
The convergence is calculatedas the gradient of the classifier?s estimated perfor-mance or uncertainty.
Laws and Schu?tze concludethat both gradient-based approaches, that is, conver-gence, can be used as stopping criteria relative to theoptimal performance achievable on a given pool ofdata.
They also show that while their method lendsitself to acceptable estimates of accuracy, it is muchharder to estimate the recall of the classifier.
Thus,the stopping criteria based on minimal absolute ormaximum possible performance are not reliable.The work most related to ours is that of Tomanekand colleagues (Tomanek et al, 2007; Tomanek andHahn, 2008) who define and evaluate the SelectionAgreement (SA) and the Validation Set Agreement(VSA) already introduced in Section 2.
Tomanekand Hahn (2008) conclude that monitoring theprogress of AL should be based on a separate vali-dation set instead of the data directly affected by thelearning process ?
thus, VSA is preferred over SA.Further, they find that the VSA curve approximatesthe progression of the learning curve and thus clas-sifier performance convergence could be estimated.However, to actually find where to stop the annota-tion, a threshold needs to be set.Our proposed intrinsic stopping criterion isunique in several ways: The ISC is intrinsic, relyingonly on the characteristics of the base learner andthe data at hand in order to decide when the AL pro-cess may be terminated.
The ISC does not requirethe user to set any external parameters prior to ini-tiating the AL process.
Further, the ISC is designedto work with committees of classifiers, and as such,it is independent of how the disagreement betweenthe committee members is quantified.
The ISC doesneither rely on a particular base learner, nor on a par-ticular way of creating the decision committee.4 ExperimentsTo challenge the definition of the ISC, we conductedtwo types of experiments concerning named entityrecognition.
The primary focus of the first typeof experiment is on creating classifiers (classifier-centric), while the second type is concerned with thecreation of annotated documents (data-centric).
Inall experiments, the agreement among the decisioncommittee members is quantified by the Vote En-tropy measure (Engelson and Dagan, 1996):V E(e) = ?
1log k?lV (l, e)k logV (l, e)k (1)where k is the number of members in the committee,and V (l, e) is the number of members assigning la-bel l to instance e. If an instance obtains a low VoteEntropy value, it means that the committee membersare in high agreement concerning its classification,and thus also that it is less a informative one.4.1 Classifier-centric experimental settingsIn common AL scenarios, the main goal of us-ing AL is to create a good classifier with min-imal label complexity.
To follow this idea, weselect sentences that are assumed to be usefulfor classifier training.
We decided to selectcomplete sentences ?
instead of, e.g., single to-kens ?
as in practice annotators must see thecontext of words to decide on their entity labels.Our experimental setting is based on the AL ap-proach described by Tomanek et al (2007): Thecommittee consists of k = 3 Maximum Entropy(ME) classifiers (Berger et al, 1996).
In each ALiteration, each classifier is trained on a randomlydrawn (sampling without replacement) subset L?
?L with |L?| = 23L, L being the set of all instances la-beled so far (cf.
EnsembleGenerationMethod in Fig-ure 1).
Usefulness of a sentence is estimated as theaverage token Vote Entropy (cf.
Equation 1).
In eachAL iteration, the 20 most useful sentences are se-lected (n = 20 in Step 3 in Figure 1).
AL is startedfrom a randomly chosen seed of 20 sentences.While we made use of ME classifiers during theselection, we employed an NE tagger based on Con-ditional Random Fields (CRF) (Lafferty et al, 2001)during evaluation time to determine the learningcurves.
CRFs have a significantly higher taggingperformance, so the final classifier we are aimingat should be a CRF model.
We have shown be-fore (Tomanek et al, 2007) that MEs are well apt asselectors with the advantage of much shorter train-ing times than CRFs.
For both MEs and CRFs the141same features were employed which comprised or-thographical (based mainly on regular expressions),lexical and morphological (suffixed/prefixed, worditself), syntactic (POS tags), as well as contextual(features of neighboring tokens) ones.The experiments on classifier-centric AL havebeen performed on the English data set of cor-pus used in the CoNLL-2003 shared task (TjongKim Sang and Meulder, 2003).
This corpus con-sists of newspaper articles annotated with respect toperson, location, and organisation entities.
As ALpool we took the training set which consists of about14,000 sentences (?
200, 000 tokens).
As valida-tion set and as gold standard for plotting the learn-ing curve we used CoNLL?s evaluation corpus whichsums up to 3,453 sentences.4.2 Data-centric experimental settingsWhile AL is commonly used to create as goodclassifiers as possible, with the amount of humaneffort kept to a minimum, it may result in frag-mented and possibly non re-usable annotations (e.g.,a collection of documents in which only some ofthe names are marked up).
This experiment con-cerns a method of orchestrating AL in a way ben-eficial for the bootstrapping of annotated data (Ols-son, 2008).
The bootstrapping proper is realized bymeans of AL for selecting documents to annotate, asopposed to sentences.
This way the annotated dataset is comprised of entire documents thus promot-ing data creation.
As in the classifier-centric setting,the task is to recognize names ?
persons, organiza-tions, locations, times, dates, monetary expressions,and percentages ?
in news wire texts.
The textsused are part of the MUC-7 corpus (Linguistic DataConsortium, 2001) and consists of 100 documents,3,480 sentences, and 90,790 tokens.
The task is ap-proached using the IOB tagging scheme proposedby, e.g., Ramshaw and Marcus (1995), turning theoriginal 7-class task into a 15-class task.
Each to-ken is represented using a fairly standard menagerieof features, including such stemming from the sur-face appearance of the token (e.g., Contains dollar?Length in characters), calculated based on linguis-tic pre-processing made with the English FunctionalDependency Grammar (Tapanainen and Ja?rvinen,1997) (e.g., Case, Part-of-speech), fetched from pre-compiled lists of information (e.g., Is first name?
),and features based on predictions concerning thecontext of the token (e.g, Class of previous token).The decision committee is made up from 10boosted decision trees using MultiBoostAB (Webb,2000) (cf.
EnsembleGenerationMethod in Figure 1).Each classifier is created by the REPTree decisiontree learner described by Witten and Frank (2005).The informativeness of a document is calculated bymeans of average token Vote Entropy (cf.
Equa-tion 1).
The seed set of the AL process consists offive randomly selected documents.
In each AL iter-ation, one document is selected for annotation fromthe corpus (n = 1 in Step 3 in Figure 1).5 ResultsTwo different scenarios were used to illustrate theapplicability of the proposed intrinsic stopping cri-terion.
In the first scenario, we assumed that thepool of unlabeled data was static and fairly large.In the second scenario, we assumed that the unla-beled data would be collected in smaller batches asit was made available on a stream, for instance, froma news feed.
Both the classifier-centric and the data-centric experiments were carried out within the firstscenario.
Only the classifier-centric experiment wasconducted in the stream-based scenario.In the classifier-centric setting, the SA is definedas (1 ?
Vote Entropy) for the most informative in-stances in the unlabeled pool, that is, the per-tokenaverage Vote Entropy on the most informative sen-tences.
Analogously, in the data-centric setting, theSA is defined as (1 ?
Vote Entropy) for the most in-formative document ?
here too, the informativenessis calculated as the per-token average Vote Entropy.In both settings, the VSA is the per-token averageVote Entropy on the validation set.5.1 AL on static poolsThe intersection of the SA and VSA agreementcurves indicates a point at which the AL processmay be terminated without (a significant) loss inclassifier performance.
For both AL scenarios (data-and classifier-centric) we plot both the learningcurves for AL and random selection, as well as theSA and VSA curve for AL.
In both scenarios, these14200.050.10.150.20.250.30  25000  50000  75000  100000  125000  150000VoteEntropyNumber of tokens in the training setCSelection agreementValidation set agreement0.60.650.70.750.80.850  25000  50000  75000  100000  125000  150000F-scoreBaselineQuery by committeeFigure 2: Classifier-centric AL experiments on theCoNLL corpus.
The intersection, C, corresponds to thepoint where (almost) no further improvement in termsof classifier performance can be expected.
The baselinelearning curve shows the results of learning from ran-domly sampled data.curves are averages over several runs.3The results from the classifier-centric experimenton the CoNLL corpus are presented in Figure 2.AL clearly outperforms random selection.
The ALcurve converges at a maximum performance of F ?84% after about 125,000 tokens.
As expected, theSA curve drops from high values in the beginningdown to very low values in the end where hardlyany interesting instances are left in the pool.
Theintersection (C) with the VSA curve is very close tothe point (125,000 tokens) where no further increaseof performance can be reached by additional anno-tation making it a good stopping point.The results from the data-centric experiment areavailable in Figure 3.
The bottom part shows theSA and VSA curves.
The ISC occurs at the inter-section of the SA and VSA curves (C), which corre-sponds to a point well beyond the steepest part of thelearning curve.
While stopping the learning at C re-sults in a classifier with performance inferior what ismaximally achievable, stopping at C arguably corre-3The classifier-centric experiments are averages over threeindependent runs.
The data-centric experiments are averagesover ten independent runs.0.050.060.070.080.090.10.110.120.130.140  10  20  30  40  50  60  70  80  90VoteEntropyNumber of documents in the training setCSelection agreementValidation set agreement0.450.50.550.60.650.70.750.80.850  10  20  30  40  50  60  70  80  90F-scoreBaselineQuery by boostingFigure 3: Data-centric AL experiments on the MUC-7corpus.
The intersection, C, corresponds to a point atwhich the AL curve has almost leveled out.
The base-line learning curve shows the results of learning from ran-domly sampled data.sponds to a plausible place to abort the learning.
Theoptimal performance is F ?
83.5%, while the ISCcorresponds to F ?
82%.Keep in mind that the learning curves with whichthe ISC are compared are not available in a practicalsituation, they are included in Figures 2 and 3 for thesake of clarity only.5.2 AL on streamed dataOne way of paraphrasing the ISC is: Once the in-tersection between the SA and VSA curves has beenreached, the most informative instances remainingin the pool of unlabeled data are less informative tothe classifier than the instances in the held-out, unla-beled validation set are on average.
This means thatthe classifier would learn more from a sufficientlylarge sample taken from the validation set than itwould if the AL process continued on the remain-ing unlabeled pool.4As an illustration of the practical applicability ofthe ISC consider the following scenario.
Assume4Note however, that the classifier might still learn from theinstances in the unlabeled pool ?
applying the ISC only meansthat the classifier would learn more from a validation set-likedistribution of instances.1430.50.550.60.650.70.750.80  2000  4000  6000  8000  10000  12000  14000  16000  18000  20000F-scoreNumber of tokens in the training setC1C2C3Partition 1Partition 2Partition 3Partition 4Figure 4: AL curves for the four partitions used in the ex-periments on streamed data.
Ci denotes a point at whichthe AL is terminated for partition i and a new partition isemployed instead.
C1 corresponds to the ISC plotted inthe graph labeled Partition 1 in Figure 5, C2 to the ISC inPartition 2, and C3 to the ISC in Partition 3.that we are collecting data from a stream, for in-stance items taken from a news feed.
Thus, the datais not available on the form of a closed set, but ratheran open one which grows over time.
To make themost of the human annotators in this scenario, wewant them to operate on batches of data instead ofannotating individual news items as they are pub-lished.
The purpose of the annotation is to mark upnames in the texts in order to train a named entityrecognizer.
To do so, we wait until there has ap-peared a given number of sentences on the stream,and then collect those sentences.
The problem is,how do we know when the AL-based annotationprocess for each such batch should be terminated?We clearly do not want the annotators to annotateall sentences, and we cannot have the annotatorsset new thresholds pertaining to the absolute per-formance of the named entity recognizer for eachnew batch of data available.
By using the ISC, weare able to automatically issue a halting of the ALprocess (and thus also the annotation process) andproceed to the next batch of data without losing toomuch in performance, and without having the anno-tators mark up too much of the available data.
Tothis end, the ISC seems like a reasonable trade-offbetween annotation effort and performance gain.To carry out this experiment we took a sub sampleof 10% (1,400 sentences) from the original AL poolof the CoNLL corpus as validation set.5 The rest of5Note that the original CoNLL test set was not used in this1000 4000 70000.00.2Partition 1TokensVoteEntropySAVSA5000 80000.000.100.20Partition 2TokensVoteEntropySAVSA8000 140000.000.100.20Partition 3TokensVoteEntropySAVSA14000 180000.000.100.20Partition 4TokensVoteEntropySAVSAFigure 5: The SA and VSA curves for the four data par-titions used in the experiment on streamed data.
Eachintersection ?
ISC ?
corresponds to a point where AL isterminated.this pool was split into batches of about 500 con-secutive sentences.
Classifier-centric AL was nowrun taking the first batch as pool to select from.
Atthe point where the SA and VSA curve crossed, wecontinued AL selection from the next batch and soforth.
Figure 4 shows the learning curve for a simu-lation of the scenario described above.
The inter-section between the SA and VSA curves for par-tition 1 as depicted in Figure 5 corresponds to thefirst ?step?
(ending in C1) in the stair-like learningcurve in Figure 4.
The step occurs after 4,641 to-kens.
Analogously, the other steps (ending in C2 andC3, respectively) in the learning curve correspondsthe intersection between the SA and VSA curves forpartitions 2 and 3 in Figure 5.
The intersection forpartition 4 corresponds to the point were we wouldhave turned to the next partition.
This experimentwas stopped after 4 partitions.Table 1 shows the accumulated number of sen-tences and tokens (center columns) that required an-notation in order to reach the ISC for each partition.In addition, the last column in the table shows thenumber of sentences (of the 500 collected for inclu-experiment, thus the F-score reported in Figure 4 cannot becompared to that in Figure 2.144Partition Sents Toks Sentences per partition1 320 4,641 3202 580 7,932 2603 840 13,444 2604 1070 16,751 230Table 1: The number of tokens and sentences required toreach the ISC for each partition.sion in each partition) needed to reach the ISC ?
eachnew partition contributes less to the increase in per-formance than the preceding ones.6 DiscussionWe have argued that one interpretation of the ISCis that it constitutes the point where the informa-tiveness on the remaining part of the AL pool islower than the informativeness on a different andindependent data set with the same distribution.
Inthe first AL scenario where there is one static poolto select from, reaching this point can be inter-preted as an overall stopping point for annotation.Here, the ISC represents a trade-off between theamount of data annotated and the classifier perfor-mance obtained such that the resulting classifier isnearly optimal with respect to the data at hand.
Inthe second, stream-based AL scenario where severalsmaller partitions are consecutively made availableto the learner, the ISC serves as an indicator that theannotation of one batch should be terminated, andthat the mark-up should proceed with the next batch.The ISC constitutes an intrinsic way of determin-ing when to stop the learning process.
It does notrequire any external parameters such as pre-definedthresholds to be set, and it depends only on the char-acteristics of the data and base learner at hand.
TheISC can be utilized to relate the performance of theclassifier to the performance that is possible to ob-tain by the data and learner at hand.The ISC can not be used to estimate the perfor-mance of the classifier.
Consequently, it can not beused to relate the classifier?s performance to an ex-ternally set level, such as a particular F-score pro-vided by the user.
In this sense, the ISC may serve asa complement to stopping criteria requiring the clas-sifier to achieve absolute performance measures be-fore the learning process is aborted, for instance themax-confidence proposed by Zhu and Hovy (2007),and the minimal absolute performance introducedby Laws and Schu?tze (2008).7 Conclusions and Future WorkWe have defined and empirically tested an intrinsicstopping criterion (ISC) for committee-based AL.The results of our experiments in two named en-tity recognition scenarios show that the stopping cri-terion is indeed a viable one, which represents afair trade-off between data use and classifier perfor-mance.
In a setting in which the unlabeled pool ofdata used for learning is static, terminating the learn-ing process by means of the ISC results in a nearlyoptimal classifier.
The ISC can also be used for de-ciding when the pool of unlabeled data needs to berefreshed.We have focused on challenging the ISC with re-spect to named entity recognition, approached intwo very different settings; future work includes ex-periments using the ISC for other tasks.
We be-lieve that the ISC is likely to work in AL-based ap-proaches to, e.g., part-of-speech tagging, and chunk-ing as well.
It should be kept in mind that whilethe types of experiments conducted here concernthe same task, the ways they are realized differ inmany respects: the ways the decision committeesare formed, the data sets used, the representation ofinstances, the relation between the sample size andthe instance size, as well as the pre-processing toolsused.
Despite these differences, which outnumbersthe similarities, the ISC proves a viable stopping cri-terion.An assumption underlying the ISC is that the ini-tial distribution of instances in the pool of unlabeleddata used for learning, and the distribution of in-stances in the validation set are the same (or at leastvery similar).
Future work also includes investiga-tions of automatic ways to ensure that this assump-tion is met.AcknowledgementsThe first author was funded by the EC projectCOMPANIONS (IST-FP6-034434), the second au-thor was funded by the EC projects BOOTStrep(FP6-028099) and CALBC (FP7-231727).145ReferencesMarkus Becker and Miles Osborne.
2005.
A Two-StageMethod for Active Learning of Statistical Grammars.In Proc 19th IJCAI, Edinburgh, Scotland, UK.Adam Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
Computational Lin-guistics, 22(1):39?71.Sean P. Engelson and Ido Dagan.
1996.
MinimizingManual Annotation Cost In Supervised Training FromCorpora.
In Proc 34th ACL, Santa Cruz, California,USA.Steven C. H. Hoi, Rong Jin, and Michael R. Lyu.
2006.Large-Scale Text Categorization by Batch Mode Ac-tive Learning.
In Proc 15th WWW, Edinburgh, Scot-land.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProc 18th ICML, Williamstown, Massachusetts, USA.Florian Laws and Hinrich Schu?tze.
2008.
Stopping Cri-teria for Active Learning of Named Entity Recogni-tion.
In Proc 22nd COLING, Manchester, England.David D. Lewis and William A. Gale.
1994.
A Sequen-tial Algorithm for Training Text Classifiers.
In Proc17th ACM-SIGIR, Dublin, Ireland.Linguistic Data Consortium.
2001.
Message understand-ing conference (muc) 7.
LDC2001T02.
FTP FILE.Philadelphia: Linguistic Data Consortium.Fredrik Olsson.
2008.
Bootstrapping Named Entity An-notation by means of Active Machine Learning ?
AMethod for Creating Corpora.
Ph.D. thesis, Depart-ment of Swedish, University of Gothenburg.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
TextChunking using Transformation Based Learning.
InProc 3rd VLC, Massachusetts Institute of Technology,Cambridge, Massachusetts, USA.Eric Ringger, Peter McClanahan, Robbie Haertel, GeorgeBusby, Marc Carmen, James Carroll, Kevin Seppi, andDeryle Lonsdale.
2007.
Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation.
InProc Linguistic Annotation Workshop, Prague, CzechRepublic.Greg Schohn and David Cohn.
2000.
Less is More: Ac-tive Learning with Support Vector Machines.
In Proc17th ICML, Stanford University, Stanford, California,USA.Hinrich Schu?tze, Emre Velipasaoglu, and Jan O. Peder-sen. 2006.
Performance Thresholding in PracticalText Classification.
In Proc 15th CIKM, Arlington,Virginia, USA.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-Criteria-based Active Learn-ing for Named Entity Recognition.
In Proc 42nd ACL,Barcelona, Spain.Pasi Tapanainen and Timo Ja?rvinen.
1997.
A Non-Projective Dependency Parser.
In Proc 5th ANLP,Washington DC, USA.Cynthia A. Thompson, Mary Elaine Califf, and Ray-mond J. Mooney.
1999.
Active Learning for NaturalLanguage Parsing and Information Extraction.
In Proc16th ICML, Bled, Slovenia.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.
In Proc 7thCoNLL, Edmonton, Alberta, Canada.Katrin Tomanek and Udo Hahn.
2008.
ApproximatingLearning Curves for Active-Learning-Driven Annota-tion.
In Proc 6th LREC, Marrakech, Morocco.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An Approach to Text Corpus Constructionwhich Cuts Annotation Costs and Maintains Reusabil-ity of Annotated Data.
In Proc Joint EMNLP-CoNLL,Prague, Czech Republic.Andreas Vlachos.
2008.
A Stopping Criterion forActive Learning.
Computer, Speech and Language,22(3):295?312, July.Geoffrey I. Webb.
2000.
MultiBoosting: A Tech-nique for Combining Boosting and Wagging.
MachineLearning, 40(2):159?196, August.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools with Java Implementa-tions.
2nd Edition.
Morgan Kaufmann, San Fransisco.Jingbo Zhu and Eduard Hovy.
2007.
Active Learningfor Word Sense Disambiguation with Methods for Ad-dressing the Class Imbalance Problem.
In Proc JointEMNLP-CoNLL, Prague, Czech Republic.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008a.Learning a Stopping Criterion for Active Learning forWord Sense Disambiguation and Text Classification.In Proc 3rd IJCNLP, Hyderabad, India.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008b.Multi-Criteria-based Strategy to Stop Active Learningfor Data Annotation.
In Proc 22nd COLING, Manch-ester, England.146
