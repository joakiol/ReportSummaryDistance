Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63?71,Dublin, Ireland, August 24 2014.Wordsyoudontknow: Evaluation of lexicon-based decompoundingwith unknown handlingKarolina Owczarzak Ferdinand de Haan George Krupka Don HindleOracle Language Technology1111 19th Street NW #600, Washington, DC 20036, USA{karolina.owczarzak,ferdinand.de.haan,george.krupka,don.hindle}@oracle.comAbstractIn this paper we present a cross-linguistic evaluation of a lexicon-based decomposition methodfor decompounding, augmented with a ?guesser?
for unknown components.
Using a goldstandard test set, for which the correct decompositions are known, we optimize the method?sparameters and show correlations between each parameter and the resulting scores.
The resultsshow that even with optimal parameter settings, the performance on compounds with unknownelements is low in terms of matching the expected lemma components, but much higher interms of correct string segmentation.1 IntroductionCompounding is a productive process that creates new words by combining existing words together ina single string.
It is predominant in Germanic and Scandinavian languages, but is also present in otherlanguages, e.g.
Finnish, Korean, or Farsi.
Many languages that are not usually thought of as?compounding?
nevertheless display marginal presence of compounds, restricted, for instance, tonumerical expressions (e.g.
Polish czterogodzinny ?four-hour?).
Depending on a language,compounding can be a very frequent and productive process, in effect making it impossible to list allthe compound words in the dictionary.
This creates serious challenges for Natural LanguageProcessing in many areas, including search, Machine Translation, information retrieval and relateddisciplines that rely on matching multiple occurrences of words to the same underlying representation.In this paper, we present a cross-linguistic evaluation of a lexicon-based decomposition methodaugmented with a ?guesser?
for handling unknown components.
We use existing lexicons developedat Oracle Language Technology in combination with a string scanner parametrized with language-specific input/output settings.
Our focus is on the evaluation that tries to tease apart stringsegmentation (i.e.
finding boundaries between components) and morphological analysis (i.e.
matchingcomponent parts to known lemmas).The paper is organized as follows: Section 2 gives an overview of related research; Section 3describes the compound analyzer used in our experiments; Section 4 presents experimental results;Section 5 contains error analysis and discussion.
Section 6 concludes and suggests future research.2 Related researchCurrent research on compound splitting is predominantly lexicon-based, with a range of selectionmethods to choose the most likely decomposition.
The lexicons used to identify components areusually collected from large monolingual corpora (Larson et al., 2000; Monz and de Rijke, 2001;Alfonseca et al, 2008; Holz and Biemann, 2008; von Huyssteen and von Zaanen, 2004).The problem with pure lexicon-based approach without any constraints is that it will produce manyspurious decompositions, matching small substrings that happen to be legitimate words in theThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/63language.
Therefore, some approaches introduce maximizing component length (or, conversely,minimizing the number of components) as one of the selection factors (von Huyssteen and vonZaanen, 2004; Holz and Biemann, 2008; Macherey et al., 2011; Larson et al., 2000); others use part ofspeech to eliminate short components which tend to be function words (Koehn and Knight, 2003;Monz and de Rijke, 2001).
In other cases, Named Entity Recognition is used to filter out proper namesthat should not be decomposed but that can contain frequent short components like ?-berg?
or ?-dorf?
(Alfonseca et al., 2008).Even after removing unlikely small component candidates, there is enough ambiguity indecomposition to warrant further filtering methods.
And so, approaches related to Machine Translationuse bilingual parallel corpora to find the most likely components by checking whether theirtranslations match elements of the whole compound translation (Koehn and Knight, 2003; Machereyet al., 2011).
Other filtering methods are based on combined frequency of the components (Koehn andKnight, 2003; Holz and Biemann, 2008), point-wise mutual information of components, or occurrenceof components in related locations, such as anchor text (Alfonseca et al., 2008).
A very interestinglexicon-free approach is presented in Aussems et al.
(2013), which uses point-wise mutual informationto detect likely boundaries between characters that would identify a compound.A major issue with the current research is the absence of common training and testing data,particularly across multiple languages, which then translates into limited evaluations of presentedmethods.
Using pre-annotated frequency lists, we create gold standard test sets for 10 languages:Norwegian, Danish, Dutch, Estonian, Finnish, German, Hungarian, Korean, Farsi, Swedish, whichrange from around 600 to 15,000 compounds.
This allows a more thorough comparison of the analyserperformance across different languages.3 Lexicon-based analyzerOur approach follows the main line of research in that it uses lexicons to identify potential componentsin a compound; however, our lexicons contain lemmas rather than word forms, in contrast to lexiconsharvested from monolingual corpora.
However, the lexicons we use contain as well as partial lemmaswhose occurrences are restricted to compounds (e.g.
German verb forms without the final ?en; forexample schlie?-).
In addition, we use morphological rules to map recognized inflected forms to base(lexicon) lemmas.
Both the lexicons and the morphological rules have been previously created bycomputational linguists and native speakers for use in a variety of NLP applications at OracleLanguage Technology.On the most basic level, a compound can be explicitly added to the lexicon, with a specificdecomposition and appropriate part of speech and grammatical features; this option is used when thedecomposition is irregular or non-obvious, for instance when the component appears in a form that isnot directly analyzable to its lemma, as in the example below, which shows irregular plurals anddeletion of consonant:(1) a. Danish: barn ?child?
plural: b?rnb?rnebog barn+e+bog [child-connector-book] ?children?s book?b.
Norwegian Bokm?l: deletion of repeated consonantmusikkorps musikk+korps [music-band] ?music band?Lexicalized compounds are treated like any other words, and their inflected forms will berecognized.
Explicitly adding the compound to the lexicon is also useful when the compound can havemultiple decompositions, and we want to restrict the output only to the semantically correct analysis.In Dutch, for instance, the compound part stem can refer to the noun stem ?voice?
or to the root of theverb stemmen ?vote?.
These readings are distinguished in the lexicon by listing explicit decompositionsfor compounds that contain the part:(2) Dutch stem N vs. Va. stemband stem#band  [voice-cord]  ?vocal cord?
(N-N)b. stembureau stemmen#bureau [vote-station] ?polling station (V-N)64However, adding all compounds to the lexicon is simply unfeasible for many languages where thecompounding process is highly productive.
For this reason, we also use a compound analyser toidentify components in a dynamic manner, based on available component lemmas in the lexicon.Components are found by removing any recognizable inflections from the candidate string, scanning itleft-to-right, and looking for all matching lemmas, subject to constraints based on part of speech,length, number of components, and available features.
For speed reasons, we apply greedy matching,and prefer decompositions with the longest prefix and the smallest number of components.Since our goal is developing language processing systems that are as universal as possible, leavingcontext-dependent decisions to higher-level applications, we are not particularly concerned withalways selecting the single best decomposition for a compound, since in many cases is will bedependent on the domain and application.
However, it is useful to filter out decompositions that wouldbe highly unlikely in any context, for instance those containing small function words mentioned inprevious section.
For this purpose, we apply constraints described below.3.1 Rules for compound sequencesFor each language, we list the possible part of speech sequences that can appear in compounds.
Theserules serve not only to prevent the decompositions that would not appear in the language (for instance,noun-verb-particle), but also to restrict sequences that are fairly infrequent, but that would lead toconsiderable over-generation if they were added.
For example, in German, there are relatively fewcompounds that end with a verb, unless it is a combination of movable prefix particle (aus, an, ab, ein,etc.)
and the verb (aus+gehen, auf+stehen, um+steigen, etc.).
These verbs are functionally analyzed ascompounds, i.e.
a concatenation of two lemmas.
However, since sequences noun/adjective/verb + verbare much less productive (spazieren+gehen, auto+fahren), it is more efficient to restrict the verb-finalcompounds to particle-verb only, and add the exceptions to the lexicon.
A few examples of compoundpart of speech sequences for different languages are shown in (3).
(3) a. Dutch:cardinal_number + verb e.g., vier+en+delen ?quarter?b.
Estonian:noun+adjective  e.g.
silmi+pimestav ?eye-dazzling?c.
German:ordinal_number + adjective e.g.
zweit+gr?
?t ?second largest?d.
Swedish:noun + noun   e.g citron+saft ?lemon juice?Another issue is compounds of cardinal or ordinal numbers, which can also occur in somelanguages like Italian (cinquecento+sessanta+nove ?five hundred sixty nine?)
or Greek (??????????,????
+ ???????
?eight hundred?).
These number compounds can be very productive and are alsoincluded in the lists of allowed compound sequences.3.2 ConnectorsIn many compounding languages, the subparts of a compound can be connected with extra material, aconnector (or linking element).
These are semantically empty elements that have a mainlyphonological role in connecting the compound parts (Bauer, 2009).
In many Germanic languagesconnectors are derived from plural or genitive morphemes (such as ?er or ?s in German), but do nothave this role any more, as evidenced, among others, by the fact that in certain cases the connector isoptional and compounds with and without a connector co-exist (4a) or by the fact that there are caseswhere two different connectors co-occur (4b) (Krott et al., 2007):(4) a. Norwegian Bokm?l:rettssak rett + s + sak ?court case?rettsak rett + ?
+ sakb.
Dutch:paddestoel pad + e + stoel ?toadstool?paddenstoel pad + en + stoel65For each language, we create a set of allowed connectors, a few examples of which can be seen in(5).1 Note that it might be useful to restrict certain connectors to appear only in certain sequences (e.g.between noun and noun, but not adjective and verb); we plan to implement this restriction in futurework.
(5) Connector examplesa.
Dutch  s e.g.
water+s+nood ?flood?b.
German zu e.g.
to match auf+stehen and auf+zu+stehen ?stand up?c.
Swedish o e.g.
veck+o+slut ?weekend?3.3 Decompounding settingsAnother factor in successful dynamic decompounding is restrictions on possible number ofcomponents, and on length of candidate strings and candidate components.
Choosing to allow fewercomponents of longer length helps to prevent spurious over-analysis, where several short words canaccidentally match the string which is being analyzed.
However, setting the limits too high might alsoprevent legitimate decomposition, so this trade-off needs to be carefully balanced.
There are four basiclength settings, as shown in Table 1 below; the values are dependent on language.Maximum number of elements: Limits the number of components in a compound.
Low values helpprevent spurious decompositions into many small elements.Minimum length of compound: The minimum length of string that should be subject todecompounding; short strings are unlikely to be compounds, so for efficiency reasons, they are notdecompounded.Minimum length of component: Specifies the minimum length of potential compound elements;shorter substrings are excluded to avoid accidental matching of very short words.Minimum length of component with connector: A version of the above setting, it specifies theminimum length of potential element when this element is next to a connector; to avoid spuriousmatches of the short word + connector combination (e.g.
Dutch paspoort should be decomposed aspas+poort, not pa+s+poort).setting  valuemaximum number of elements 2-4minimum length of compound 4-11minimum length of component 2-4minimum length of component with connector 2-4Table 1.
Length settings for dynamic decompounding.The values for these settings are established manually and separately for each language, based onreview of top N most frequent compounds in the lexicon and the general knowledge of that language?sgrammar and patterns.4 Experimental resultsDespite all the constraints and settings described above, decompounding is still an imperfect process:there can be multiple competing (i.e.
overlapping) decompositions, and many decompositions that aretechnically possible are incorrect due to semantic reasons.
This problem becomes even morechallenging when some of the components are not present in the lexicon.
Since lexicons are limited,and real world text can contain misspellings, proper names, or obscure words, we need to address theissue of decompounding with unknown elements.
Therefore, we set out to evaluate the performance ofour lexicon-based method on a gold standard set of known compounds, and compare it to anaugmented version that also tries to construct potential components from unknown substrings.1 Note that for our purposes, particle zu in German is also treated as a connector, to match the movable particle verbs that canappear with and without zu: auf + zu + stehen and auf + stehen ?get up?.664.1 Test setFor our experiments, we collected compounds from the top 90% frequency lists based on largenews and Wikipedia corpora.
Each compound was annotated with the correct decomposition(s) by alinguist who was also a native speaker of the target language according to simple instructions: if themeaning of the word is compositional (i.e.
can be fully described by the component elements), treat itas a compound and provide component lemmas.Approximate sizes of source corpora per language are given in Table 2; column ?compounds?shows the count of compounds; column ?lexical?
shows how many of these are lexicalized compounds(i.e.
compounds that have been added to the lexicon for reasons of irregularity).
While two-partcompounds are by far the most frequent in all the languages we examined, there is also somepercentage of compounds with more than two parts; the distribution is shown in the last four columns.languagenewscorpusMBwikicorpusMBcompounds lexical 2-part 3-part 4-part 5-partDanish 335 154 1,982 1,326 1,856 122 4 0Dutch 512 103 3,439 1,909 3,186 245 8 0Estonian 204 41 2,343 562 2,166 169 8 0Farsi 512 244 648 340 635 13 0 0Finnish 512 78 1,868 1,665 1,703 154 11 0German 520 227 15,490 5,087 14,544 915 31 0Hungarian 512 257 1,841 1,537 1,794 45 2 0Korean 826 190 11,398 4,774 10,919 425 39 5Norwegian 512 88 3,582 1,106 3,405 175 2 0Swedish 512 204 9,677 5,608 8,901 744 31 5Table 2.
Size of corpora per language, count of compounds, distribution of parts.4.2 Dynamic decompounding with available lemmasAs mentioned before, it is not feasible to add all (or even the majority) of possible compounds, so weneed to examine our performance using only dynamic decompounding.
For this purpose, we removedall lexicalized compounds from the lexicon, and then ran the analyzer on the compound test setdescribed above.
This means that all the compound analysis was done dynamically, using only theavailable simple lemmas and compound rules and length restrictions.
Table 3 shows the results.
Thescores for lexicalized + dynamic decompounding are given only for reference; they are high but lessinteresting, since they reflect the fact that the lexicalized compounds were largely collected from thesame corpora (among other sources).
Our focus is on the dynamic scores, which show performance onunknown compounds assuming a nearly ?perfect?
lexicon that contains almost all the componentlemmas.
As such, these scores will serve as the upper bound for our next experiment, in which weremove at least one of the component lemmas from the lexicon and test the resulting performance.As can be seen in Table 3, for most languages recall decreases considerably ?
this suggests thatlexicalized compounds are of the kind that are not covered by the decompounding rules or whosecorrect analysis is blocked by another decomposition.4.3 Dynamic decompounding with missing lemmasWhile dynamic decompounding can handle the productive nature of compounds, it is still limited tofinding components that are already present in the lexicon.
However, in the real world compounds willcontain elements unknown to a lexicon-based analyzer, whether it is because they are domain-specificvocabulary, proper names, foreign borrowings, or misspellings.
In those cases, it is still useful toattempt analysis and return the known parts, with the option of returning the unknown substring as themissing lemma.67lexicalized + dynamic dynamic onlyprec rec f-score prec rec f-scoreDanish 98.18 99.6 98.88 87.99 66.9 76.01Dutch 98.84 100 99.42 84.46 80.49 82.43Estonian 98.25 99.83 99.03 95.69 90.27 92.9Farsi 92.9 100 96.32 65.75 72.84 69.11Finnish 98.74 100 99.37 84.55 68.63 75.76German 96.11 99.98 98.01 88.01 89.03 88.52Hungarian 90.44 99.84 94.91 77.42 72.19 74.71Korean 99.72 100 99.86 95.23 59.49 73.23Norwegian 99.6 100 99.8 93.25 86.32 89.65Swedish 96.35 99.88 98.08 86.67 75.75 80.84Table 3.
Precision, recall, and f-measure for dynamic decompounding.To evaluate the performance of our analyzer in case where some component lemmas are unknown,we applied a ?compound guesser?
function that tries to find known elements of unknown compounds,even if a complete decomposition to only known elements is impossible.
The guesser has its ownconstraints, independent of the main compound analyzer, which are shown in Table 4.setting  valuemaximum number of elements 2-20minimum length of compound 3-20minimum length of component 2-5minimum length of unknown element 1-5minimum percent of string covered 0-100%Table 4.
Settings for dynamic decompounding with unknown elements.The first three settings are parallel to the settings for regular dynamic decompounding; however, wealso add restrictions on length for unknown elements (minimum length of unknown element) and totalstring coverage (minimum percent of string covered).
Restriction on length of unknown element meanthat any unknown string shorter than the minimum length will be treated as a potentialconnector/suffix/prefix and will not be returned as a lemma:(6) German: assuming freundlicher ?friendlier?
is unknown:umweltfreundlicher -> umwelt + freundlich (!
+ er) [environment + friendly]The last setting allows a more fine-grained control over the proportion of known to unknown parts;however, since any value less than 100% will restrict the number of produced candidatedecompositions, resulting in no output if the unknown substring is too long, we do not test the impactof this setting.For this experiment, we collected all component lemmas from the test compounds, and removedfrom lexicon at least one component lemma per compound.
This renders the whole stringunanalyzable by regular means.
Then we ran the compound guesser with each combination of settingsfrom Table 4, to find the optimal set of values.Table 5 shows results obtained with the optimal guesser settings per language, compared to scoresfrom Table 3: a fully functional decomposition that has access to both dynamic decomposition andlexicalized compounds, and dynamic decomposition with near-perfect component lexicon.
It is clear68that even with optimal settings, the guesser performance falls well below the level of full functionality,even when we compare to a system that has no access to lexicalized compounds.
The highest scoreachieved by the guesser is 34 for the Hungarian test set, which includes mostly simple two-partcompounds, and where the lexicon does not provide too many spurious sub-matches.language lexical + dynamicdynamiconlydynamicguesserdynamic guesser -string segmentationDanish 98.88 76.01 25.93 51.25Dutch 99.42 82.43 27.13 64.01Estonian 99.03 92.9 9.56 53.89Farsi 96.32 69.11 27.16 78.68Finnish 99.37 75.76 19.49 51.6German 98.01 88.52 25.1 52.29Hungarian 94.91 74.71 34 53.5Korean 99.86 73.23 16.81 76.54Norwegian 99.8 89.65 22.56 49.74Swedish 98.08 80.84 25.56 54.18Table 5.
Dynamic decomposition with missing lemmas, optimal settings; string segmentation showsaccuracy score; remaining values are harmonic f-score of precision and recall.However, a major problem with this evaluation is that output of the regular decompounding processproduces lemmas in their dictionary form, without inflection, whereas the guesser can only returnsurface strings for the unknown elements which might carry grammatical inflection or stemalternations.
Therefore, it would be more fair to compare the guesser to dynamic decompounding interms of pure string segmentation ?
whether it finds the same boundaries between components,without concern for the form of the returned component.
This lets us tease apart the impact of findingcomponent elements from the impact of morphology.
The last column in Table 5 shows accuracy ofguesser string segmentation as compared to string segmentation performed by regular dynamicdecompounding; in this respect the guesser?s performance is indeed much better.
These results areencouraging, showing that we can recover correct components in up to 79% of cases, which is a veryuseful improvement for the purposes of information retrieval and search.
While some recall is lost byreturning strings instead of lemmas, we are planning to add a second step that would employ a lemma?guesser?, in order to produce the most likely dictionary form from the recovered unknown string.languagemaxelementscorr.withscoreminlength ofcompoundcorr.withscoreminlengthofelementcorr.withscoreminlength ofunknownelementcorr.withscoreDanish 2 -0.19 3-7 -0.46 4 0.43 3 -0.09Dutch 2 -0.21 8 -0.47 5 0.51 3 -0.06Estonian 2 -0.15 3-7 -0.57 4 0.33 3 -0.08Farsi 2 -0.17 3-5 -0.51 3 0.11 2 -0.24Finnish 2-10 -0.07 3-8 -0.49 5 0.41 3 0German 2 -0.26 8 -0.43 5 0.5 3 -0.06Hungarian 2-16 0 1-6 -0.58 4 0.39 2 -0.33Korean 2-10 -0.07 3 -0.14 2 -0.07 1 -0.11Norwegian 2-10 -0.18 3-8 -0.61 5 0.56 3 0.01Swedish 2 -0.23 7 -0.45 4 0.49 3 -0.04Average   -0.15   -0.47   0.37   -0.1Table 6.
Optimal guesser settings and their correlations of settings with the guesser score.69Finally, Table 6 shows the correlation (Pearson?s r) of guesser settings (or their ranges) and theresulting scores.
As can be seen, the strongest correlation holds for the minimum length of compound(average -0.47) and minimum length of element (0.37).
In the former case, the correlation is inverse,which means the higher the value, the lower the final score; this is caused by the fact that our test setcontains only compounds, so returning the whole unsplit string will never be the right result.
Thesecond correlation reflects the fact that it is safer to exclude very short elements from appearing ascomponents, a finding that confirms earlier research.5 Error analysisA considerable percentage of mismatch errors when guessing the unknown components ofcompounds is caused by the connectors.
Our current guesser settings return the whole unknown string,without attempting to identify any potential connectors on its edges.
This seems like an obvious areafor improvement, as it would let us return more correct decompositions for cases shown in Table 7(unknown strings are enclosed in square brackets and are currently returned whole).language token dynamic guesser translationNorwegian kj?rlighetsbrev kj?rlighet#brev kj?rlighet#[s + brev] love letterDanish ungdomshus ungdom#hus ungdom#[s + hus] youthGerman sklavenmoral sklave#moral sklave#[n + moral] slave moralitySwedish kvinnof?rbund kvinna#f?rbund kvinn#[o + f?rbund] women's allianceTable 7.
Examples of connector mismatches between dynamic decompounding and the guesser.As could be expected, most errors are nevertheless caused by the guesser splitting unknown stringsinto smaller known chunks; several typical examples are shown in Table 8.language token dynamic guesser translationDanish popul?rkulturen popul?r#kultur popul?r#kult#uren popular cultureDutch kunstschilders kunst#schilder kunst#schil#ders paintersFinnish rockmuusikot rock#muusiko rock#muusi#kot rock musicSwedish radioversion radio#version radio#vers#ion radio  versionTable 8.
Examples of incorrect splitting of unknown strings.6 Conclusion and future workIn this paper, we have shown a dictionary-based compound analyzer, augmented with the function tohandle unknown substrings.
A cross-linguistic evaluation against the gold standard containingcomponent lemmas shows that the correct handling of unknown compound elements is a difficult issueespecially if we try to match dictionary lemmas; however, a more detailed evaluation of the stringsegmentation and boundary detection shows fairly good results.
Being able to decompose unknowncompounds and match the components to known lemmas to increase recall is crucial to many NLPapplications, such as information retrieval or Machine Translation.
A correct segmentation is offundamental importance, but the question remains how we can match the unknown, possibly inflected,substring to known lemmas.
In the future, we plan to address this question by (1) adding the option toseparate out connectors from unknown strings, and (2) build a lemma ?guesser?
that would try toconstruct a probable dictionary representation for the unknown string, in effect building a pipeline thatwould more fully mirror the process of regular dynamic decompounding.AcknowledgementsWe would like to thank the rest of the Oracle Language Technology team, in particular Elena Spivakand Rattima Nitisaroj, for their help with compound examples.70ReferencesAlfonseca, Enrique, Slaven Bilac and Stefan Pharies.
2008.
German Decompounding in a Difficult Corpus.
InComputational Linguistics and Intelligent Text Processing, A. Gelbukh (ed.).
Springer Verlag, Berlin andHeidelberg, 128-139.Aussems, Suzanne., Bas Goris., Vincent Lichtenberg, Nanne van Noord, Rick Smetser, and Menno van Zaanen.2013.
Unsupervised identification of compounds.
In Proceedings of the 22nd Belgian-Dutch conference onmachine learning, A. van den Bosch, T. Heskes, & D. van Leeuwen (Eds.
), Nijmegen, 18-25.Bauer, Laurie.
2009.
Typology of Compounds.
In The Oxford Handbook of Compounding, Rochelle Lieber andPavol ?tekauer (eds.).
Oxford University Press, Oxford.343-356.Holz, Florian and Chris Biemann.
2008.
Unsupervised and Knowledge-Free Learning of Compound Splits andPeriphrases.
CICLing'08 Proceedings of the 9th international conference on Computational linguistics andintelligent text processing, A. Gelbukh (ed.).
Springer Verlag, Berlin and Heidelberg, 117-127.Koehn, Philipp and Kevin Knight.
2003.
Empirical Methods for Compound Splitting.
Proceedings of the 10thconference of the European Chapter of the Association for Computational Linguistics, Vol.
1, 187-193.Krott, Andrea, Robert Schreuder, R. Harald Baayen and Wolfgang U. Dressler.
2007 Analogical effects onlinking elements in German compounds.
Language and Cognitive Processes, 22(1):25-57.Larson, Martha, Daniel Willett, Joachin K?hler and Gerhard Rigoll.
2000.
Compound splitting and lexical unitrecombination for improved performance of a speech recognition system for German parliamentary speechesIn INTERSPEECH, 945-948.Macherey, Klaus, Andrew M. Dai, David Talbot, Ashok C. Popat and Franz Och.
2011.
Language-independentcompound splitting with Morphological Operations.
Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics, 1395-1404.Monz, Christof and Maarten de Rijke.
2002.
Shallow Morphological Analysis in Monolingual InformationRetrieval for Dutch, German and Italian.
In Evaluation of Cross-Language Information Retrieval Systems.Carol Peters, Martin Braschler, Julio Gonzalo and Michael Kluck (eds.).
Springer Verlag, Berlin andHeidelberg, 262-277.van Huyssteen, Gerhard and Menno van Zaanen.
2004.
Learning Compound Boundaries for Afrikaans SpellingChecking.
In Pre-Proceedings of the Workshop on International Proofing Tools and Language Technologies;Patras, Greece.
101?108.van Zaanen, Menno, Gerhard van Huyssteen, Suzanne Aussems, Chris Emmery, and Roald Eiselen.
2014.
TheDevelopment of Dutch and Afrikaans Language Resources for Compound Boundary Analysis.
In Proceedingof LREC 2014.71
