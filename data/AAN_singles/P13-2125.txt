Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714?718,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning Semantic Textual Similarity with Structural RepresentationsAliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2(1)DISI, University of Trento, 38123 Povo (TN), Italy{severyn,m.nicosia,moschitti}@disi.unitn.it(2)QCRI, Qatar Foundation, Doha, Qataramoschitti@qf.org.qaAbstractMeasuring semantic textual similarity(STS) is at the cornerstone of many NLPapplications.
Different from the major-ity of approaches, where a large numberof pairwise similarity features are used torepresent a text pair, our model featuresthe following: (i) it directly encodes inputtexts into relational syntactic structures;(ii) relies on tree kernels to handle featureengineering automatically; (iii) combinesboth structural and feature vector repre-sentations in a single scoring model, i.e.,in Support Vector Regression (SVR); and(iv) delivers significant improvement overthe best STS systems.1 IntroductionIn STS the goal is to learn a scoring model thatgiven a pair of two short texts returns a similar-ity score that correlates with human judgement.Hence, the key aspect of having an accurate STSframework is the design of features that can ade-quately represent various aspects of the similaritybetween texts, e.g., using lexical, syntactic and se-mantic similarity metrics.The majority of approaches treat input text pairsas feature vectors where each feature is a scorecorresponding to a certain type of similarity.
Thisapproach is conceptually easy to implement andthe STS shared task at SemEval 2012 (Agirre etal., 2012) (STS-2012) has shown that the best sys-tems were built following this idea, i.e., a num-ber of features encoding similarity of an input textpair were combined in a single scoring model, e.g.,SVR.
Nevertheless, one limitation of using onlysimilarity features to represent a text pair is that oflow representation power.The novelty of our approach is that we treat theinput text pairs as structural objects and rely on thepower of kernel learning to extract relevant struc-tures.
To link the documents in a pair we mark thenodes in the related structures with a special rela-tional tag.
This way effective structural relationalpatterns are implicitly encoded in the trees andcan be automatically learned by the kernel-basedmachines.
We combine our relational structuralmodel with the features from two best systems ofSTS-2012.
Finally, we use the approach of classi-fier stacking to combine several structural modelsinto the feature vector representation.The contribution of this paper is as follows: (i) itprovides a convincing evidence that adding struc-tural features automatically extracted by structuralkernels yields a significant improvement in accu-racy; (ii) we define a combination kernel that inte-grates both structural and feature vector represen-tations within a single scoring model, e.g., Sup-port Vector Regression; (iii) we provide a sim-ple way to construct relational structural modelsthat can be built using off-the-shelf NLP tools;(iv) we experiment with four structural representa-tions and show that constituency and dependencytrees represent the best source for learning struc-tural relationships; and (v) using a classifier stack-ing approach, structural models can be easily com-bined and integrated into existing feature-basedSTS models.2 Structural Relational SimilarityThe approach of relating pairs of input struc-tures by learning predictable syntactic transforma-tions has shown to deliver state-of-the-art resultsin question answering, recognizing textual entail-ment, and paraphrase detection, e.g.
(Wang et al2007; Wang and Manning, 2010; Heilman andSmith, 2010).
Previous work relied on fairly com-plex approaches, e.g.
applying quasi-synchronousgrammar formalism and variations of tree edit dis-tance alignments, to extract syntactic patterns re-lating pairs of input structures.
Our approachis conceptually simpler, as it regards the prob-lem within the kernel learning framework, wherewe first encode salient syntactic/semantic proper-714ties of the input text pairs into tree structures andrely on tree kernels to automatically generate richfeature spaces.
This work extends in several di-rections our earlier work in question answering,e.g., (Moschitti et al 2007; Moschitti and Quar-teroni, 2008), in textual entailment recognition,e.g., (Moschitti and Zanzotto, 2007), and more ingeneral in relational text categorization (Moschitti,2008; Severyn and Moschitti, 2012).In this section we describe: (i) a kernel frame-work to combine structural and vector models; (ii)structural kernels to handle feature engineering;and (iii) suitable structural representations for re-lational learning.2.1 Structural Kernel LearningIn supervised learning, given labeled data{(xi, y i)}ni=1, the goal is to estimate a decisionfunction h(x) = y that maps input examples totheir targets.
A conventional approach is to rep-resent a pair of texts as a set of similarity fea-tures {fi}, s.t.
the predictions are computed ash(x) = w ?
x = ?iwifi, where w is the modelweight vector.
Hence, the learning problem boilsdown to estimating individual weights of each ofthe similarity features fi.
One downside of suchapproach is that a great deal of similarity infor-mation encoded in a given text pair is lost whenmodeled by single real-valued scores.A more versatile approach in terms of the inputrepresentation relies on kernels.
In a typical kernellearning approach, e.g., SVM, the prediction func-tion for a test input x takes on the following formh(x) =?i ?iyiK(x,xi), where ?i are the modelparameters estimated from the training data, yi aretarget variables, xi are support vectors, andK(?, ?
)is a kernel function.To encode both structural representation andsimilarity feature vectors of a given text pair in asingle model we define each document in a pairto be composed of a tree and a vector: ?t, v?.To compute a kernel between two text pairs xiand xj we define the following all-vs-all kernel,where all possible combinations of components,x(1) and x(2), from each text pair are consid-ered: K(xi,xj) = K(x(1)i ,x(1)j )+K(x(1)i ,x(2)j )+K(x(2)i ,x(1)j ) + K(x(2)i ,x(2)j ).
Each of the ker-nel computations K can be broken down intothe following: K(x(1),x(2)) = KTK(t(1), t(2)) +Kfvec(v(1), v(2)), where KTK computes a struc-tural kernel and Kfvec is a kernel over feature vec-tors, e.g., linear, polynomial or RBF, etc.
Furtherin the text we refer to structural tree kernel modelsas TK and explicit feature vector representation asfvec.Having defined a way to jointly model text pairsusing structural TK representations along with thesimilarity features fvec, we next briefly reviewtree kernels and our relational structures.2.2 Tree KernelsWe use tree structures as our base representationsince they provide sufficient flexibility in repre-sentation and allow for easier feature extractionthan, for example, graph structures.
Hence, werely on tree kernels to compute KTK(?, ?).
Giventwo trees it evaluates the number of substructures(or fragments) they have in common, i.e., it is ameasure of their overlap.
Different TK functionsare characterized by alternative fragment defini-tions.
In particular, we focus on the Syntactic Treekernel (STK) (Collins and Duffy, 2002) and a Par-tial Tree Kernel (PTK) (Moschitti, 2006).STK generates all possible substructures rooted ineach node of the tree with the constraint that pro-duction rules can not be broken (i.e., any node in atree fragment must include either all or none of itschildren).PTK can be more effectively applied to both con-stituency and dependency parse trees.
It general-izes STK as the fragments it generates can containany subset of nodes, i.e., PTK allows for breakingthe production rules and generating an extremelyrich feature space, which results in higher gener-alization ability.2.3 Structural representationsIn this paper, we define simple-to-build relationalstructures based on: (i) a shallow syntactic tree,(ii) constituency, (iii) dependency and (iv) phrase-dependency trees.Shallow tree is a two-level syntactic hierarchybuilt from word lemmas (leaves), part-of-speechtags (preterminals) that are further organized intochunks.
It was shown to significantly outperformfeature vector baselines for modeling relationshipsbetween question answer pairs (Severyn and Mos-chitti, 2012).Constituency tree.
While shallow syntactic pars-ing is very fast, here we consider using con-stituency structures as a potentially richer sourceof syntactic/semantic information.Dependency tree.
We propose to use depen-dency relations between words to derive an alter-native structural representation.
In particular, de-715Figure 1: A phrase dependency-based structural representation of a text pair (s1, s2): A woman witha knife is slicing a pepper (s1) vs. A women slicing green pepper (s2) with a high semantic similarity(human judgement score 4.0 out of 5.0).
Related tree fragments are linked with a REL tag.pendency relations are used to link words in a waythat they are always at the leaf level.
This reorder-ing of the nodes helps to avoid the situation wherenodes with words tend to form long chains.
Thisis essential for PTK to extract meaningful frag-ments.
We also plug part-of-speech tags betweenthe word nodes and nodes carrying their grammat-ical role.Phrase-dependency tree.
We explore a phrase-dependency tree similar to the one defined in (Wuet al 2009).
It represents an alternative struc-ture derived from the dependency tree, where thedependency relations between words belonging tothe same phrase (chunk) are collapsed in a unifiednode.
Different from (Wu et al 2009), the col-lapsed nodes are stored as a shallow subtree rootedat the unified node.
This node organization is par-ticularly suitable for PTK that effectively runs asequence kernel on the tree fragments inside eachchunk subtree.
Fig 1 gives an example of our vari-ation of a phrase dependency tree.As a final consideration, if a document containsmultiple sentences they are merged in a single treewith a common root.
To encode the structuralrelationships between documents in a pair a spe-cial REL tag is used to link the related structures.We adopt a simple strategy to establish such links:words from two documents that have a commonlemma get their parents (POS tags) and grandpar-ents, non-terminals, marked with a REL tag.3 Pairwise similarity features.Along with the direct representation of input textpairs as structural objects our framework is alsocapable of encoding pairwise similarity featurevectors (fvec), which we describe below.Baseline features.
(base) We adopt similar-ity features from two best performing systemsof STS-2012, which were publicly released1:namely, the Takelab2 system (S?aric?
et al 2012)and the UKP Lab?s system3 (Bar et al 2012).Both systems represent input texts with similarityfeatures combining multiple text similarity mea-sures of varying complexity.UKP (U) provides metrics based on match-ing of character, word n-grams and commonsubsequences.
It also includes features derivedfrom Explicit Semantic Analysis (Gabrilovich andMarkovitch, 2007) and aggregation of word sim-ilarity based on lexical-semantic resources, e.g.,WordNet.
In total it provides 18 features.Takelab (T) includes n-gram matching of vary-ing size, weighted word matching, length differ-ence, WordNet similarity and vector space simi-larity where pairs of input sentences are mappedinto Latent Semantic Analysis (LSA) space.
Thefeatures are computed over several sentence rep-resentations where stop words are removed and/orlemmas are used in place of raw tokens.
The totalnumber of Takelab?s features is 21.
The combinedsystem consists of 39 features.Additional features.
We also augment the U andT feature sets, with an additional set of features (A)which includes: a cosine similarity scores com-puted over (i) n-grams of part-of-speech tags (upto 4-grams), (ii) SuperSense tags (Ciaramita and1Note that only a subset of the features used in the fi-nal evaluation was released, which results in lower accuracywhen compared to the official rankings.2http://takelab.fer.hr/sts/3https://code.google.com/p/dkpro-similarity-asl/wiki/SemEval2013716Altun, 2006), (iii) named entities, (iv) dependencytriplets, and (v) PTK syntactic similarity scorescomputed between documents in a pair, where asinput representations we use raw dependency andconstituency trees.
To alleviate the problem of do-main adaptation, where datasets used for trainingand testing are drawn from different sources, weinclude additional features to represent the com-bined text of a pair: (i) bags (B) of lemmas, de-pendency triplets, production rules (from the con-stituency parse tree) and a normalized length ofthe entire pair; and (ii) a manually encoded cor-pus type (M), where we use a binary feature witha non-zero entry corresponding to a dataset type.This helps the learning algorithm to learn implic-itly the individual properties of each dataset.Stacking.
To integrate multiple TK representa-tions into a single model we apply a classifierstacking approach (Fast and Jensen, 2008).
Eachof the learned TK models is used to generate pre-dictions which are then plugged as features intothe final fvec representation, s.t.
the final modeluses only explicit feature vector representation.
Toobtain prediction scores, we apply 5-fold cross-validation scheme, s.t.
for each of the held-outfolds we obtain independent predictions.4 ExperimentsWe present the results of our model tested on thedata from the Core STS task at SemEval 2012.4.1 SetupData.
To compare with the best systems of theSTS-2012 we followed the same setup used inthe final evaluation, where 3 datasets (MSRpar,MSRvid and SMTeuroparl) are used for trainingand 5 for testing (two ?surprise?
datasets wereadded: OnWN and SMTnews).
We use the entiretraining data to obtain a single model for makingpredictions on each test set.Software.
To encode TK models along with thesimilarity feature vectors into a single regressionscoring model, we use an SVR framework imple-mented in SVM-Light-TK4.
We use the follow-ing parameter settings -t 5 -F 1 -W A -C+, which specifies a combination of trees and fea-ture vectors (-C +), STK over trees (-F 1) (-F3 for PTK) computed in all-vs-all mode (-W A)and polynomial kernel of degree 3 for the featurevector (active by default).4http://disi.unitn.it/moschitti/Tree-Kernel.htmMetrics.
We report the following metrics em-ployed in the final evaluation: Pearson correlationfor individual test sets5 and Mean ?
an averagescore weighted by the test set size.4.2 ResultsTable 1 summarizes the results of combining TKmodels with a strong feature vector model.
Wetest structures defined in Sec.
2.3 when using STKand PTK.
The results show that: (i) combiningall three features sets (U, T, A) provides a strongbaseline system that we attempt to further improvewith our relational structures; (ii) the generality ofPTK provides an advantage over STK for learn-ing more versatile models; (iii) constituency anddependency representations seem to perform bet-ter than shallow and phrase-dependency trees; (iv)using structures with no relational linking does notwork; (v) TK models provide a far superior sourceof structural similarity than U + T + A that alreadyincludes PTK similarity scores as features, and fi-nally (vi) the domain adaptation problem can beaddressed by including corpus specific features,which leads to a large improvement over the pre-vious best system.5 Conclusions and Future WorkWe have presented an approach where text pairsare directly treated as structural objects.
This pro-vides a much richer representation for the learningalgorithm to extract useful syntactic and shallowsemantic patterns.
We have provided an exten-sive experimental study of four different structuralrepresentations, e.g.
shallow, constituency, de-pendency and phrase-dependency trees using STKand PTK.
The novelty of our approach is that itgoes beyond a simple combination of tree kernelswith feature vectors as: (i) it directly encodes inputtext pairs into relationally linked structures; (ii) thelearned structural models are used to obtain pre-diction scores thus making it easy to plug into ex-isting feature-based models, e.g.
via stacking; (iii)to our knowledge, this work is the first to applystructural kernels and combinations in a regres-sion setting; and (iv) our model achieves the stateof the art in STS largely improving the best pre-vious systems.
Our structural learning approachto STS is conceptually simple and does not re-quire additional linguistic sources other than off-the-shelf syntactic parsers.
It is particularly suit-able for NLP tasks where the input domain comes5we also report the results for a concatenation of all fivetest sets (ALL)717Experiment U T A S C D P STK PTK B M ALL Mean MSRp MSRv SMTe OnWN SMTnfvecmodel?
.7060 .6087 .6080 .8390 .2540 .6820 .4470?
.7589 .6863 .6814 .8637 .4950 .7091 .5395?
?
.8079 .7161 .7134 .8837 .5519 .7343 .5607?
?
?
.8187 .7137 .7157 .8833 .5131 .7355 .5809TKmodelswith STKand PTK?
?
?
?
?
.8261 .6982 .7026 .8870 .4807 .7258 .5333?
?
?
?
?
.8326 .6970 .7020 .8925 .4826 .7190 .5253?
?
?
?
?
.8341 .7024 .7086 .8921 .4671 .7319 .5495?
?
?
?
?
.8211 .6693 .6994 .8903 .2980 .7035 .5603?
?
?
?
?
.8362 .7026 .6927 .8896 .5282 .7144 .5485?
?
?
?
?
.8458 .7047 .6935 .8953 .5080 .7101 .5834?
?
?
?
?
.8468 .6954 .6717 .8902 .4652 .7089 .6133?
?
?
?
?
.8326 .6693 .7108 .8879 .4922 .7215 .5156REL tag ?
?
?
?
.8218 .6899 .6644 .8726 .4846 .7228 .5684?
?
?
?
.8250 .7000 .6806 .8822 .5171 .7145 .5769domainadaptation?
?
?
?
?
.8539 .7132 .6993 .9005 .4772 .7189 .6481?
?
?
?
?
.8529 .7249 .7080 .8984 .5142 .7263 .6700?
?
?
?
?
?
.8546 .7156 .6989 .8979 .4884 .7181 .6609?
?
?
?
?
?
.8810 .7416 .7210 .8971 .5912 .7328 .6778UKP (best system of STS-2012) .8239 .6773 .6830 .8739 .5280 .6641 .4937Table 1: Results on STS-2012.
First set of experiments studies the combination of fvec models fromUKP (U), Takelab (T) and (A).
Next we show results for four structural representations: shallow (S),constituency (C), dependency (D) and phrase-dependency (P) trees with STK and PTK; next row setdemonstrates the necessity of relational linking for two best structures, i.e.
C and D (empty circle denotesa structures with no relational linking.
); finally, domain adaptation via bags of features (B) of the entirepair and (M) manually encoded dataset type show the state of the art results.as pairs of objects, e.g., question answering, para-phrasing and recognizing textual entailment.6 AcknowledgementsThis research is supported by the EU?s SeventhFramework Program (FP7/2007-2013) under the#288024 LIMOSINE project.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre.
2012.
Semeval-2012 task 6: A pilot on se-mantic textual similarity.
In *SEM.Daniel Bar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In SemEval.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.
InEMNLP.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
InACL.Andrew S. Fast and David Jensen.
2008.
Why stackedmodels perform effective collective classification.In ICDM.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In IJCAI.Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In NAACL.Alessandro Moschitti and Silvia Quarteroni.
2008.Kernels on linguistic structures for answer extrac-tion.
In ACL.Alessandro Moschitti and Fabio Massimo Zanzotto.2007.
Fast and effective kernels for relational learn-ing from texts.
In ICML.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels for ques-tion/answer classification.
In ACL.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In SIGIR.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In SemEval.Mengqiu Wang and Christopher D. Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In ACL.Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
In EMNLP.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In EMNLP.718
