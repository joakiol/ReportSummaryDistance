In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 70?79,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsModeling covert event retrieval in logical metonymy:probabilistic and distributional accountsAlessandra Zarcone, Jason UttInstitut f?r Maschinelle SprachverarbeitungUniversit?t Stuttgart{zarconaa,uttjn}@ims.uni-stuttgart.deSebastian Pad?Institut f?r ComputerlinguistikUniversit?t Heidelbergpado@cl.uni-heidelberg.deAbstractLogical metonymies (The student finishedthe beer) represent a challenge to composi-tionality since they involve semantic contentnot overtly realized in the sentence (covertevents ?
drinking the beer).
We present acontrastive study of two classes of computa-tional models for logical metonymy in German,namely a probabilistic and a distributional,similarity-based model.
These are built usingthe SDEWAC corpus and evaluated against adataset from a self-paced reading and a proberecognition study for their sensitivity to the-matic fit effects via their accuracy in predictingthe correct covert event in a metonymical con-text.
The similarity-based models allow forbetter coverage while maintaining the accuracyof the probabilistic models.1 IntroductionLogical metonymies (The student finished the beer)require the interpretation of a covert event whichis not overtly realized in the sentence (?
drinkingthe beer).
Logical metonymy has received muchattention as it raises issues that are relevant to boththeoretical as well as cognitive accounts of language.On the theoretical side, logical metonymies consti-tute a challenge for theories of compositionality (Par-tee et al, 1993; Baggio et al, in press) since their in-terpretation requires additional, inferred information.There are two main accounts of logical metonymy:According to the lexical account, a type clash be-tween an event-subcategorizing verb (finish) and anentity-denoting object (beer) triggers the recovery ofa covert event from complex lexical entries, such asqualia structures (Pustejovsky, 1995).
The pragmaticaccount of logical metonymy suggests that covertevents are retrieved through post-lexical inferencestriggered by our world knowledge and communica-tion principles (Fodor and Lepore, 1998; Cartson,2002; De Almeida and Dwivedi, 2008).On the experimental side, logical metonymy leadsto higher processing costs (Pylkk?nen and McEl-ree, 2006; Baggio et al, 2010).
As to covertevent retrieval, it has been found that verbs cuefillers with a high thematic fit for their argumentpositions (e.g.
arrestagent????
cop, (Ferretti et al,2001)) and that verbs and arguments combined cuefillers with a high thematic fit for the remainingargument slots (e.g.
?journalist , check?patient????
?spelling but ?mechanic, check?patient?????
car (Bick-nell et al, 2010).
The interpretation of logicalmetonymy is also highly sensitive to context (e.g.
?confectioner , begin, icing?covertevent????????
spreadbut ?child , begin, icing?covertevent????????
eat (Zarconeand Pad?, 2011; Zarcone et al, 2012).
It thus pro-vides an excellent test bed for cognitively plausiblecomputational models of language processing.We evaluate two classes of computational mod-els for logical metonymy.
The classes represent thetwo main current approaches in lexical semantics:probabilistic and distributional models.
Probabilisticmodels view the interpretation as the assignment ofvalues to random variables.
Their advantage is thatthey provide a straightforward way to include con-text, by simply including additional random variables.However, practical estimation of complex modelstypically involves independence assumptions, which70may or may not be appropriate, and such modelsonly take first-order co-occurrence into account1.
Incontrast, distributional models represent linguisticentities as co-occurrence vectors and phrase interpre-tation as a vector similarity maximization problem.Distributional models typically do not require anyindependence assumptions, and include second-orderco-occurrences.
At the same time, how to integratecontext into the vector computation is essentially anopen research question (Mitchell and Lapata, 2010).In this paper, we provide the first (to our knowl-edge) distributional model of logical metonymyby extending the context update of Lenci?s ECUmodel (Lenci, 2011).
We compare this model toa previous probabilistic approach (Lapata and Las-carides, 2003a; Lapata et al, 2003b).
In contrastto most experimental studies on logical metonymy,which deal with English data (with the exception ofLapata et al (2003b)), we focus on German.
Weestimate our models on a large web corpus and eval-uate them on a psycholinguistic dataset (Zarconeand Pad?, 2011; Zarcone et al, 2012).
The taskwe use to evaluate our models is to distinguishcovert events with a high typicality / thematic fit(e.g.
The student finished the beer ??
drinking)from low typicality / thematic fit covert events(??
brewing).2 Probabilistic models of logical metonymyLapata et al (2003b; 2003a) model the interpretationof a logical metonymy (e.g.
The student finished thebeer) as the joint distribution P (s, v, o, e) of the vari-ables s (the subject, e.g.
student), v (the metonymicverb, e.g.
finish), o (the object, e.g.
beer), e (thecovert event, drinking).This model requires independence assumptionsfor estimation.
We present two models with differentindependence assumptions.1This statement refers to the simple probabilistic models weconsider, which are estimated directly from corpus co-occurrencefrequencies.
The situation is different for more complex prob-abilistic models, for example generative models that introducelatent variables, which can amount to clustering based on higher-order co-occurrences, as in, e.g., Prescher et al (2000).2.1 The SOVp modelLapata et al develop a model which we will referto as the SOVp model.2 It assumes a generative pro-cess which first generates the covert event e and thengenerates all other variables based on the choice of e:P (s, v, o, e) ?
P (e) P (o|e) P (v|e) P (s|e)They predict that the selected covert event e?
fora given context is the event which maximizesP (s, v, o, e):e?
= argmaxeP (e) P (o|e) P (v|e) P (s|e)These distributions are estimated as follows:P?
(e) =f(e)N, P?
(o|e) =f(eo??
o)f(eo??
?),P?
(v|e) =f(vc??
e)f(?c??
e), P?
(s|e) =f(es??
s)f(es??
?
),where N is the number of occurrences of full verbsin the corpus; f(e) is the frequency of the verb e;f(eo??
?)
and f(es??
?)
are the frequencies of ewith a direct object and subject, respectively; andf(ec??
?)
is number of times e is the complement ofanother full verb.2.2 The SOp modelIn Lapata et al?s covert event model, v, themetonymic verb, was used to prime different choicesof e for the same object (begin book??
writing;enjoy book??
reading).
In our dataset (Sec.
4), wekeep v constant and consider e only as a function ofs and o.
Thus, the second model we consider is theSOp model which does not consider v:P (s, v, o, e) ?
P (s, o, e) ?
P (e) P (o|e) P (s|e)Again, the preferred interpretation e?
is the one thatmaximizes P (s, v, o, e):e?
= argmaxeP (e) P (o|e) P (s|e)2In Lapata et al (2003b; 2003a), this model is called thesimplified model to distinguish it from a full model.
Since the fullmodel performs worse, we do not include it into considerationand use a more neutral name for the simplified model.713 Similarity-based models3.1 Distributional semanticsDistributional or vector space semantics (Turney andPantel, 2010) is a framework for representing wordmeaning.
It builds on the Distributional Hypothe-sis (Harris, 1954; Miller and Charles, 1991) whichstates that words occurring in similar contexts aresemantically similar.
In distributional models, themeaning of a word is represented as a vector whosedimensions represent features of its linguistic con-text.
These features can be chosen in different ways;popular choices are simple words (Sch?tze, 1992) orlexicalized dependency relations (Lin, 1998; Pad?and Lapata, 2007).
Semantic similarity can then beapproximated by vector similarity using a wide rangeof similarity metrics (Lee, 1999).3.1.1 Distributional MemoryA recent multi-purpose framework in distribu-tional semantics is Distributional Memory (DM, Ba-roni and Lenci (2010)).
DM does not immedi-ately construct vectors for words.
Instead, it ex-tracts a three-dimensional tensor of weighted word-link-word tuples each of which is mapped onto ascore by a function ?
: ?w1 l w2?
?
R+.
For ex-ample, ?pencil obj use?
has a higher weight than?elephant obj use?.
The set of links can be defined indifferent ways, yielding various DM instances.
Ba-roni and Lenci present DepDM (mainly syntacticlinks such as subj_tr ), LexDM (strongly lexicalizedlinks, e.g., such_as), or TypeDM (syntactic and lexi-calized links).3The benefit of the tensor-based representation isthat it is general, being applicable to many tasks.Once a task is selected, a dedicated semantic spacefor this task can be generated efficiently from thetensor.
For example, the word by link-word space(W1 ?
LW2) contains vectors for the words w1whose dimensions are labeled with ?l, w2?
pairs.
Theword-word by link space (W1W2 ?
L) contains co-occurrence vectors for word pairs ?w1, w2?
whosedimensions are labeled with l.3.2 Compositional Distributional SemanticsProbabilistic models can account for composition-ality by estimating conditional probabilities.
Com-3l?1 is used to denote the inverse link of l (i.e., exchangingthe positions of w1 and w2).positionality is less straightforward in a similarity-based distributional model, because similarity-baseddistributional models traditionally model meaningat word level.
Nevertheless, the last years haveseen a wave of distributional models which makeprogress at building compositional representationsof higher-level structures such as noun-adjective orverb-argument combinations (Mitchell and Lapata,2010; Guevara, 2011; Reddy et al, 2011).3.2.1 Expectation Composition and UpdateLenci (2011) presents a model to predict the degreeof thematic fit for verb-argument combinations: theExpectation Composition and Update (ECU) model.More specifically, the goal of ECU is explain how thechoice of a specific subject for a given verb impactsthe semantic expectation for possible objects.
Forexample, the verb draw alone might have fair, but notvery high, expectations for the two possible objectslandscape and card.
When it is combined with thesubject painter, the resulting phrase painter draw theexpectation for the object landscape should increase,while it should drop for card.The idea behind ECU is to first compute the verb?sown expectations for the object from a TypeDMW1 ?
LW2 matrix and then update it with the sub-ject?s expectations for the object, as mediated by theTypeDM verb link type.4 More formally, the verb?sexpectations for the object are defined asEXV (v) = ?o.
?
(?v obj?1 o?
)The subject?s expectations for the object areEXS(s) = ?o.
?
(?s verb o?
)And the updated expectation isEXSV (s, v) = ?o.EXV (v)(o) ?
EXS(s)(o)where ?
is a composition operation which Lenci in-stantiates as sum and product, following commonpractice in compositional distributional semantics(Mitchell and Lapata, 2010).
The product composi-tion approximates a conjunction, promoting objectsthat are strongly preferred by both verb and subject.It is, however, also prone to sparsity problems as well4In DM, verb directly connects the subject and the object oftransitive verb instances, e.g ?marine verb gun?.72shortcomings of the scoring function ?.
The sumcomposition is more akin to a disjunction where itsuffices that an object is strongly preferred by eitherthe verb or the subject.It would be possible to use these scores as directestimates of expectations, however, sinceEXSV con-tains three lexical variables, sparsity is a major issue.ECU thus introduces a distributional generalizationstep.
It only uses the updated expectations to identifythe 20 most expected nouns for the object position.It then determines the prototype of the updated ex-pectations as the centroid of their W1?LW2 vectors.Now, the thematic fit for any noun can be computedas the similarity of its vector to the prototype.Lenci evaluates ECU against a dataset from Bick-nell et al (2010), where objects (e.g.
spelling) arematched with a high-typicality subject-verb combi-nations (e.g.
?journalist, check?
- high thematic fit)and with a low-typicality subject-verb combination(e.g.
?mechanic, check?
- low thematic fit).
ECU isin fact able to correctly distinguish between the twocontexts differing in thematic fit with the object.3.3 Cognitive relevanceSimilarity-based models build upon the Distribu-tional Hypothesis, which, in its strong version, isa cognitive hypothesis about the form of semanticrepresentations (Lenci, 2008): the distributional be-havior of a word reflects its semantic behavior butis also a direct correlate of its semantic content atthe cognitive level.
Also, similarity-based modelsare highly compatible with known features of hu-man cognition, such as graded category member-ship (Rosch, 1975) or multiple sense activation (Erk,2010).
Their cognitive relevance for language hasbeen supported by studies of child lexical devel-opment (Li et al, 2004), category-related deficits(Vigliocco et al, 2004), selectional preferences (Erk,2007), event types (Zarcone and Lenci, 2008) andmore (see Landauer et al (2007) and Baroni andLenci (2010) for a review).3.4 Modeling Logical Metonymy with ECU3.4.1 Logical Metonymy as Thematic FitThe hypothesis that we follow in this paper is thatthe ECU model can also be used, with modifications,to predict the interpretation of logical metonymy.The underlying assumption is that the interpretationof logical metonymy is essentially the recovery ofa covert event with a maximal thematic fit (high-typicality) and can thus make use of ECU?s mech-anisms to treat verb-argument composition.
Strongevidence for this assumption has been found in psy-cholinguistic studies, which have established thatthematic fit dynamically affects processing, with on-line updates of expectations for typical fillers duringthe incremental processing of linguistic input (seeMcRae and Matsuki (2009) for a review).
Thus, wecan hope to transfer the benefits of similarity-basedmodels (notably, high coverage) to the interpretationof logical metonymy.3.4.2 Extending ECUThe ECU model nevertheless requires some modi-fications to be applicable to logical metonymy.
Boththe entity of interest and the knowledge sourceschange.
The entity of interest used to be the ob-ject of the sentence; now it is the covert event, whichwe will denote with e. As for knowledge sources,there are three sources in logical metonymy.
Theseare (a), the subject (compare the author began thebeer and the reader began the book)); (b), the objectthe reader began the book vs. the reader began thesandwich); and (c), the metonymic verb (comparePeter began the report vs. Peter enjoyed the report).The basic equations of ECU can be applied to thisnew scenario as follows.
We first formulate threebasic equations that express the expectations of thecovert event given the subject, object, and metonymicverb individually.
They are all derived from direct de-pendency relations in the DM tensor (e.g., the novelmetonymic verb?covert event relation from the ver-bal complement relation):EXS(s) = ?e.
?
(?s subj e?
)EXO(o) = ?e.
?
(?o obj e?
)EXV (v) = ?e.
?
(?v comp?1 e?
)To combine (or update) these basic expectations intoa final expectation, we propose two variants:ECU SOV In this model, we compose all threeexpectations:EXSOV (s, v, o) = ?e.EXS(s)(e) ?EXO(o)(e) ?
EXV (v)(e)73CEhigh thematic fit low thematic fitDerTheKonditorbakerbegann,starteddietheGlasuricingaufzutragen.to spread.zu essen.to eat.DasTheKindchildbegann,starteddietheGlasuricingzu essen.to eat.aufzutragen.to spread.Table 1: Example materials for the self-paced reading and probe recognition studiesWe will refer to this model as SOV?
when thecomposition function is sum, and as the SOV?
modelwhen the composition function is product.ECU SO Analogous to the SO probabilistic model,this model abstracts away from the metonymic verb.We assume most information about an event to bedetermined by the subject and object:EXSO(n, n?)
= ?e.EXS(n)(e) ?
EXO(n?
)(e)After the update, the prototype computation proceedsas defined in the original ECU.We will refer to this model as SO?
when the com-position function is sum, and as the SO?
model whenthe composition function is product.4 Experimental SetupWe evaluate the probabilistic models (Sec.
2) and thesimilarity-based models (Sec.
3) on a dataset con-structed from two German psycholinguistic studieson logical metonymy.
One study used self-pacedreading and the second one probe recognition.Dataset The dataset we use is composed of 96 sen-tences.
There are 24 sets of four ?s, v, o, e?
tuples,where s is the object, v the metonymic verb, o theobject and e the covert event.
The materials are illus-trated in Table 1.
As can be seen, all tuples withina set share the same metonymic verb and the sameobject.
Each of the two subject e is matched oncewith a high-typicality covert event and once with alow-typicality covert event.
This results in 2 high-typicality tuples and 2 low-typicality tuples in eachset.
Typical events (e) were elicited by 20 partici-pants given the corresponding object o, subjects wereelicited by 10 participants as the prototypical agentssubjects for each e, o combination.The experiments yielded a main effect of typicalityon self-paced reading times (Zarcone and Pad?, 2011)and on probe recognition latencies (Zarcone et al,2012): typical events involved in logical metonymyinterpretation are read faster and take longer to berejected as probe words after sentences which evokethem.
The effect is seen early on (after the patientposition in the self-paced reading and at short ISI forthe probe recognition), suggesting that knowledgeof typical events is quickly integrated in processingand that participants access a broader pool of knowl-edge than what has traditionally been argued to bein the lexical entries of nouns (Pustejovsky, 1995).The finding is in agreement with results of psycholin-guistic studies which challenge the very distinctionbetween world knowledge and linguistic knowledge(Hagoort et al, 2004; McRae and Matsuki, 2009).DM for German Since DM exists only for English,we constructed a German analog using the 884Mword SDEWAC web corpus (Faa?
et al, 2010) parsedwith the MATE German dependency parser (Bohnet,2010).From this corpus, we extract 55M instances ofsimple syntactic relations (subj_tr, subj_intr, obj,iobj, comp, nmod) and 104M instances of lexicalizedpatterns such as noun?prep?noun e.g.
?Recht aufAuskunft?
(?right to information?
), or adj?noun-(of)-noun such as ?strittig Entscheidung Schiedsrichter?
(?contested decision referee?).
These lexicalized pat-terns make our model roughly similar to the EnglishTypeDM model (Sec.
3.1.1).As for ?, we used local mutual information (LMI)as proposed by Baroni and Lenci (2010).
The LMIof a triple is defined as Ow1lw2 log(Ow1lw2/Ew1lw2),where Ow1lw2 is the observed co-occurrence fre-quency of the triple and Ew1lw2 its expected co-occurrence frequency (under the assumption of inde-pendence).
Like standard MI, LMI measures theinformativity or surprisal of a co-occurrence, but74weighs it by the observed frequency to avoid theoverestimation for low-probability events.4.1 TaskWe evaluate the models using a binary selectiontask, similar to Lenci (2011).
Given a triple ?s, v, o?and a pair of covert events e, e?
(cf.
rows inTab.
1), the task is to pick the high-typicality covertevent for the given triple: ?Chauffeur, vermeiden,Auto?
?
fahren/reparieren (?driver, avoid, car?
?drive/repair).
Since our dataset consists of 96 sen-tences, we have 48 such contexts.With the probabilistic models, we compare theprobabilities P (s, v, o, e) and P (s, v, o, e?)
(ignoringv in the SO model).
Analogously, for the similarity-based models, we compute the similarities of thevectors for e and e?
to the prototype vectors for the ex-pectations EXSOV (s, v, o) and predict the one withhigher similarity.
For the simplified ECU SO model,we use EXSO(s, o) as the point of comparison.4.2 BaselineFollowing the baseline choice in Lapata et al(2003b), we evaluated the probabilistic modelsagainst a baseline (Bp) which, given a ?s, v, o?triplet (e.g.
?Chauffeur, vermeiden, Auto?
), scoresa ?hit?
if the P?
(e|o) for the high-typicality e ishigher than the P?
(e|o) for the low-typicality e. Thesimilarity-based models were evaluated against abaseline (Bs) which, given an ?s, v, o?
triplet (e.g.
?Chauffeur, vermeiden, Auto?
), makes a correct pre-diction if the prototypical event vector for o has ahigher thematic fit (i.e.
similarity) with the high-typicality e than with the low-typicality e.Since our dataset is counterbalanced ?
that is, eachcovert event appears once as the high-typicality eventfor a given object (with a congruent subject) and onceas the low-typicality event ?
the baseline predictsthe correct covert event in exactly 50% of the cases.Note, however, that this is not a random baseline: thechoice of the covert event is made deterministicallyon the basis of the input parameters.4.3 Evaluation measuresWe evaluate the output of the model with the stan-dard measures coverage and accuracy.
Coverage isdefined as the percentage of datapoints for whicha model can make a prediction.
Lack of coveragearises primarily from sparsity, that is, zero counts forco-occurrences that are necessary in the estimationof a model.
Accuracy is computed on the coveredcontexts only, as the ratio of correct predictions tothe number of predictions of the model.
This allowsus to judge the quality of the model?s predictionsindependent of its coverage.We also consider a measure that combines cov-erage and accuracy, Backoff Accuracy, defined as:coverage?accuracy+((1?coverage)?0.5).
Back-off Accuracy emulates a backoff procedure: themodel?s predictions are adopted where they are avail-able; for the remaining datapoints, it assumes base-line performance (in the current setup, 50%).
TheBackoff Accuracy of low-coverage models tends todegrade towards baseline performance.We determine the significance of differences be-tween models with a ?2 test, applied to a 2?2 contin-gency matrix containing the number of correct andincorrect answers.
Datapoints outside a model?s cov-erage count half for each category, which correspondsexactly to the definition of Backoff Accuracy.5 ResultsThe results are shown in Table 2.
Looking at theprobabilistic models, we find SOp yields better cov-erage and better accuracy than SOVp (Lapata?s sim-plified model).
It is worth noting the large differ-ence in coverage, namely .75 as opposed to .44: TheSOVp model is unable to make a prediction for morethan half of all contexts.
This is due to the fact thatmany ?o, v?
combinations are unattested in the cor-pus.
Even on those contexts for which the proba-bilistic SOVp model can make a prediction, it is lessreliable than the more general SOp model (0.62 ver-sus 0.75 accuracy).
This indicates that, at least on ourdataset, the metonymic verb does not systematicallyhelp to predict the covert event; it rather harms perfor-mance by introducing noisy estimates.
As the lowerhalf of the Table shows, the SOVp model does notsignificantly outperform any other model (includingboth baselines Bp and Bs).The distributional models do not have such cover-age issues.
The main problematic combination forthe similarity model is ?Pizzabote hassen Pizza?
(i.e.
?Pizza delivery man hate pizza?)
which is pairedwith the covert events liefern (deliver) and backen(bake).
The computation of ECU predictions for75Probabilistic Models Similarity-based ModelsBp SOVp SOp Bs SOV?
SOV?
SO?
SO?Accuracy 0.50 0.62 0.75 0.50 0.68 0.56 0.68 0.70Coverage 1.00 0.44 0.75 1.00 0.98 0.94 0.98 0.98Backoff Accuracy 0.50 0.55 0.69 0.50 0.68 0.56 0.68 0.70Probabilistic Models Similarity-based ModelsBp SOVp SOp Bs SOV?
SOV?
SO?
SO?BpProb.
SOVp -SOp * -Bs - - *SOV?
* - - *SimilaritySOV?
- - - - -SO?
* - - * - -SO?
** ??
- ** - ??
-Table 2: Results (above) and significance levels for difference in backoff accuracy determined by ?2-test (below)for all probabilistic and similarity-based models (**: p<0.01, *: p?0.05, -: p>0.05).
For ??
(SO?
?
SOVp andSO?
?
SOV?)
p was just above 0.05 (p=0.053).this combination requires corpus transitive corpusconstructions for Pizzabote, in the corpus it is onlyattested once as the subject of the intransitive verbkommen (come).Among distributional models, the difference be-tween SO and SOV is not as clear-cut as on theprobabilistic side.
We observe an interaction with thecomposition operation.
Sum is less sensitive to com-plexity of updating: for sum models, the inclusionof the metonymic verb (SOV?
vs.
SOV?)
does notmake any difference.
On the side of the product mod-els, there is a major difference similar to the one forthe probabilistic models: SOV?
is the worst modelat near-baseline performance, and SO?
is the bestone.
This supports our interpretation from above thatthe metonymic model introduces noisy expectationswhich, in the product model, have the potential ofdisrupting the update process.Comparing the best models from the probabilisticand similarity-based classes (SOp and SO?
), we findthat both significantly outperform the baselines.
Thisshows that the subject contributes to the models witha significant improvement over the baseline models,which are only informed by the object.
Their back-off accuracies do not significantly differ from oneanother, which is not surprising given the small sizeof our dataset, however, the similarity-based modeloutperforms the probabilistic model by 1% BackoffAccuracy.
The two models have substantially differ-ent profiles: the accuracy of the probabilistic modelis 5% higher (0.70 vs. 0.75); at the same time, itscoverage is much lower.
It covers only 75% of thecontexts, while the distributional model SO?
coversall but one (98%).6 DiscussionAs mentioned above, the main issue with the proba-bilistic models is coverage.
This is due to the relianceof these models on first-order co-occurrence.For example, probabilistic models cannotassign a probability to any of the triples?Dieb/Juwelier schmuggeln/schleifen Diamant?
(?thief/jeweler smuggle/cut diamond?
), since thesubjects do not occur with either of the verbs incorpus, even though Diamant does occur as theobject of both.In contrast, the similarity-based models are able tocompute expectations for these triples from second-order co-occurrences by taking into account otherverbs that co-occur with Diamant.
The ECU modelis not punished by the extra context, as both Dieb andDiamant are associated with the verbs: stehlen (steal),76EXSO(?Chauffeur,Auto?)
EXSO(?Mechaniker,Auto?
)fahren (drive) bauen (build)parken (park) lassen (let/leave)lassen (let/leave) besitzen (own)geben (give) reparieren (repair)sehen (see) brauchen (need)bringen (bring) sehen (see)steuern (steer) benutzen (use)halten (keep/hold) stellen (put)Table 3: Updated expectations in SO?
for Chauffeur(chauffeur), Mechaniker (mechanic) and Auto (car).rauben (thieve), holen (get), entwenden (purloin),erbeuten (snatch), verkaufen (sell), nehmen (take),klauen (swipe).
We also note that these are typicalevents for a thief, which fits the intuition that Dieb ismore predictive of the event than Diamant.For both ?Chauffeur Auto?
and ?Mechaniker Auto?the probabilistic model predicts fahren due to thehigh overall frequency of fahren.5 The distributionalmodel, however, takes the mutual information intoaccount and is thus able to determine events thatare more strongly associated with Mechaniker (e.g.bauen, reparieren, etc.)
while at the same time dis-counting the uninformative verb fahren.There are, however, items that all models have dif-ficulty with.
Three such cases are due to a frequencydisparity between the high and low-typicality event.E.g.
for ?Lehrerin Klausur benoten/schreiben?
(?teacher exam grade/take?
), schreiben occurs muchmore frequently than benoten.
In the caseof ?Sch?ler Geschichte lernen/schreiben?
(?studentstory learn/write?
), none of the models or baselinescorrectly assigned lernen.
The probabilistic mod-els are influenced by the very frequent Geschichteschreiben which is part of an idiomatic expression (towrite history).
On the other hand, the distributionalmodels judge the story and history sense of the wordto have the most informative events, e.g.
erz?hlen(tell), lesen (read), h?ren (hear), erfinden (invent),and studieren (study), lehren (teach).The baselines were able to correctly chooseauspacken (unwrap) over einpacken (wrap) for?Geburtstagskind Geschenk?
(?birthday-boy/girlpresent?)
while the models were not.
The prob-5The combination Mechaniker fahren was seen once moreoften than Mechaniker reparieren.abilistic models lacked coverage and were notable to make a prediction.
For the distributionalmodels, while both auspacken and verpacken (wrap)are highly associated with Geschenk, the moststrongly associated actions of Geburtstagskind areextraordinarily diverse, e.g.
: bekommen (receive),sagen (say), auffuttern (eat up), herumkommandieren(boss around), ausblasen (blow out).
Neither of theevents of interest though were highly associated.7 Future WorkWe see a possible improvement in the choice of thenumber of fillers, with which we construct the pro-totype vectors.
A smaller number might lead to lessnoisy prototypes.It has been shown (Bergsma et al, 2010) that themeaning of the prefix verb can be accurately pre-dicted using the stem?s vector, when compositional-ity applies.
We suspect covert events that are prefixverbs to suffer from sparser representations than thevectors of their stem.
E.g., absaugen (vacuum off )is much less frequent than the semantically nearlyidentical saugen (vacuum).
Thus, by leveraging thericher representation of the stem, our distributionalmodels could more likely assign the correct event.8 ConclusionsWe have presented a contrastive study of two classesof computational models, probabilistic and distribu-tional similarity-based ones, for the prediction ofcovert events for German logical metonymies.We found that while both model classes modelsoutperform baselines which only take into accountinformation coming from the object, similarity-basedmodels rival and even outperform probabilistic mod-els.
The reason is that probabilistic models have torely on first-order co-occurrence information whichsuffers from sparsity issues even in large web corpora.This is particularly true for languages like Germanthat have a complex morphology, which tends to ag-gravate sparsity (e.g., through compound nouns).In contrast, similarity-based models can take ad-vantage of higher-order co-occurrences.
Providedthat some care is taken to identify reasonable vec-tor composition strategies, they can maintain the ac-curacy of probabilistic models while guaranteeinghigher coverage.77AcknowledgmentsWe would like to thank Alessandro Lenci, Siva Reddyand Sabine Schulte im Walde for useful feedbackand discussion.
The research for this paper hasbeen funded by the German Research Foundation(Deutsche Forschungsgemeinschaft) as part of theSFB 732 ?Incremental specification in context?
/project D6 ?Lexical-semantic factors in event inter-pretation?
at the University of Stuttgart.ReferencesGiosu?
Baggio, Travis Chroma, Michiel van Lambalgen,and Peter Hagoort.
2010.
Coercion and composition-ality.
Journal of Cognitive Neuroscience, 22(9):2131?2140.Giosu?
Baggio, Michiel van Lambalgen, and Peter Ha-goort.
in press.
The processing consequences of com-positionality.
In The Oxford Handbook of Composition-ality.
Oxford University Press.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):1?49.Shane Bergsma, Aditya Bhargava, Hua He, and GrzegorzKondrak.
2010.
Predicting the semantic composition-ality of prefix verbs.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 293?303, Cambridge, MA, October.Association for Computational Linguistics.Klinton Bicknell, Jeffrey L. Elman, Mary Hare, KenMcRae, and Marta Kutas.
2010.
Effects of eventknowledge in processing verbal arguments.
Journal ofMemory and Language, 63(4):489?505.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-guistics, pages 89?97, Beijing, China.Robyn Cartson.
2002.
Thoughts and utterances.
Black-well.Roberto G. De Almeida and Veena D. Dwivedi.
2008.
Co-ercion without lexical decomposition: Type-shiftingeffects revisited.
Canadian Journal of Linguistics,53(2/3):301?326.Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of ACL, Prague,Czech Republic.Katrin Erk.
2010.
What is word meaning, really?
(andhow can distributional models help us describe it?
).In Proceedings of the workshop on Geometrical Mod-els of Natural Language Semantics (GEMS), Uppsala,Sweden.Gertrud Faa?, Ulrich Heid, and Helmut Schmid.
2010.Design and Application of a Gold Standard for Mor-phological Analysis: SMOR as an Example of Mor-phological Evaluation.
In Proceedings of the SeventhInternational Conference on Language Resources andEvaluation (LREC?10), Valletta, Malta.T.
R. Ferretti, K. McRae, and A. Hatherell.
2001.
Integrat-ing verbs, situation schemas and thematic role concept.Journal of Memory and Language, 44:516?547.Jerry A. Fodor and Ernie Lepore.
1998.
The emptinessof the lexicon: Reflections on James Pustejovsky?s TheGenerative Lexicon.
Linguistic Inquiry, 29(2):269?288.Emiliano Raul Guevara.
2011.
Computing semantic com-positionality in distributional semantics.
In Proceed-ings of IWCS-2011, Oxford, UK.Peter Hagoort, Lea Hald, Marcel Bastiaansen, andKarl Magnus Petersson.
2004.
Integration of wordmeaning and world knowledge in language comprehen-sion.
Science, 304:438?441.Zelig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Thomas K. Landauer, Danielle S. McNamara, Simon Den-nis, and Walter Kintsch, editors.
2007.
Handbook ofLatent Semantic Analysis.
Lawrence Erlbaum Asso-ciates Publishers, Mahwah, NJ, US.Mirella Lapata and Alex Lascarides.
2003a.
A proba-bilistic account of logical metonymy.
ComputationalLinguistics, 29(2):263?317.Mirella Lapata, Frank Keller, and Christoph Scheepers.2003b.
Intra-sentential context effects on the inter-pretation of logical metonymy.
Cognitive Science,27(4):649?668.Lillian Lee.
1999.
Measures of Distributional Similarity.In Proceedings of the 37th annual meeting of the Associ-ation for Computational Linguistics on ComputationalLinguistics, College Park, MA.Alessandro Lenci.
2008.
Distributional semantics in lin-guistic and cognitive research.
From context to mean-ing: Distributional models of the lexicon in linguisticsand cognitive science.
Special issue of the Italian Jour-nal of Linguistics, 20(1):1?31.Alessandro Lenci.
2011.
Composing and updatingverb argument expectations: A distributional semanticmodel.
In Proceedings of the 2nd Workshop on Cog-nitive Modeling and Computational Linguistics, pages58?66, Portland, Oregon.Ping Li, Igor Farkas, and Brian MacWhinney.
2004.
Earlylexical development in a self-organizing neural network.Neural Networks, 17:1345?1362.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL, pages768?774, Montreal, QC.78Ken McRae and Kazunaga Matsuki.
2009.
People usetheir knowledge of common events to understand lan-guage, and do so as quickly as possible.
Language andLinguistics Compass, 3/6:1417?1429.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1):1?28.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Sebastian Pad?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33:161?199, June.Barbara H. Partee, Alice ter Meulen, and Robert E. Wall.1993.
Mathematical Methods in Linguistics.
Kluwer.Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000.Using a Probabilistic Class-Based Lexicon for LexicalAmbiguity Resolution.
In Proceedings of COLING2000, Saarbr?cken, Germany.James Pustejovsky.
1995.
The Generative Lexicon.
MITPress.Liina Pylkk?nen and Brian McElree.
2006.
The syntax-semantic interface: On-line composition of sentencemeaning.
In Handbook of Psycholinguistics, pages537?577.
Elsevier.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality in com-pound nouns.
In Proceedings of IJCNLP 2011, ChiangMai, Thailand.Eleanor Rosch.
1975.
Cognitive representations of seman-tic categories.
Journal of Experimental Psychology:General, 104:192?233.Hinrich Sch?tze.
1992.
Dimensions of meaning.
InProceedings of Supercomputing ?92, pages 787 ?796.Peter D. Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Gabriella Vigliocco, David P. Vinson, William Lewis,and Merrill F. Garrett.
2004.
Representing the mean-ings of object and action words: The featural and uni-tary semantic space hypothesis.
Cognitive Psychology,48(4):422?488.Alessandra Zarcone and Alessandro Lenci.
2008.
Compu-tational models for event type classification in context.In Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC?08),Marrakech, Morocco.
ELRA.Alessandra Zarcone and Sebastian Pad?.
2011.
General-ized event knowledge in logical metonymy resolution.In Proceedings of the 33rd Annual Conference of theCognitive Science Society, pages 944?949, Austin, TX.Alessandra Zarcone, Sebastian Pad?, and AlessandroLenci.
2012.
Inferring covert events in logicalmetonymies: a probe recognition experiment.
In Pro-ceedings of the 34th Annual Conference of the Cogni-tive Science Society, Austin, TX.79
