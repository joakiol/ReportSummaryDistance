Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 215?218,New York, June 2006. c?2006 Association for Computational LinguisticsSemantic Back-Pointers from GestureJacob EisensteinMIT Computer Science and Artificial Intelligence Laboratory77 Massachusetts Ave, MA 02139jacobe@csail.mit.edu1 IntroductionAlthough the natural-language processing commu-nity has dedicated much of its focus to text, face-to-face spoken language is ubiquitous, and offersthe potential for breakthrough applications in do-mains such as meetings, lectures, and presentations.Because spontaneous spoken language is typicallymore disfluent and less structured than written text,it may be critical to identify features from additionalmodalities that can aid in language understanding.However, due to the long-standing emphasis on textdatasets, there has been relatively little work on non-textual features in unconstrained natural language(prosody being the most studied non-textual modal-ity, e.g.
(Shriberg et al, 2000)).There are many non-verbal modalities that maycontribute to face-to-face communication, includ-ing body posture, hand gesture, facial expression,prosody, and free-hand drawing.
Hand gesture maybe more expressive than any non-verbal modalitybesides drawing, since it serves as the foundationfor sign languages in hearing-disabled communi-ties.
While non-deaf speakers rarely use any suchsystematized language as American Sign Language(ASL) while gesturing, the existence of ASL speaksto the potential of gesture for communicative expres-sivity.Hand gesture relates to spoken language in severalways:?
Hand gesture communicates meaning.
For ex-ample, (Kopp et al, 2006) describe a modelof how hand gesture is used to convey spatialproperties of its referents when speakers givenavigational directions.
This model both ex-plains observed behavior of human speakers,and serves as the basis for an implemented em-bodied agent.?
Hand gesture communicates discourse struc-ture.
(Quek et al, 2002) and (McNeill, 1992)describe how the structure of discourse is mir-rored by the the structure of the gestures, whenspeakers describe sequences of events in car-toon narratives.?
Hand gesture segments in unison with speech,suggesting possible applications to speechrecognition and syntactic processing.
(Morrel-Samuels and Krauss, 1992) show a strong cor-relation between the onset and duration of ges-tures, and their ?lexical affiliates?
?
the phrasethat is thought to relate semantically to the ges-ture.
Also, (Chen et al, 2004) show that gesturefeatures may improve sentence segmentation.These examples are a subset of a broad litera-ture on gesture that suggests that this modality couldplay an important role in improving the performanceof NLP systems on spontaneous spoken language.However, the existence of significant relationshipsbetween gesture and speech does not prove thatgesture will improve NLP; gesture features couldbe redundant with existing textual features, or theymay be simply too noisy or speaker-dependant to beuseful.
To test this, my thesis research will iden-tify specific, objective NLP tasks, and attempt toshow that automatically-detected gestural featuresimprove performance beyond what is attainable us-ing textual features.The relationship between gesture and meaning isparticularly intriguing, since gesture seems to offera unique, spatial representation of meaning to sup-215plement verbal expression.
However, the expressionof meaning through gesture is likely to be highlyvariable and speaker dependent, as the set of pos-sible mappings between meaning and gestural formis large, if not infinite.
For this reason, I take thepoint of view that it is too difficult to attempt to de-code individual gestures.
A more feasible approachis to identify similarities between pairs or groupsof gestures.
If gestures do communicate semantics,then similar gestures should predict semantic sim-ilarity.
Thus, gestures can help computers under-stand speech by providing a set of ?back pointers?between moments that are semantically related.
Us-ing this model, my dissertation will explore mea-sures of gesture similarity and applications of ges-ture similarity to NLP.A set of semantic ?back pointers?
decoded fromgestural features could be relevant to a number ofNLP benchmark problems.
I will investigate two:coreference resolution and disfluency detection.
Incoreference resolution, we seek to identify whethertwo noun phrases refer to the same semantic entity.A similarity in the gestural features observed duringtwo different noun phrases might suggest a similar-ity in meaning.
This problem has the advantage ofpermitting a quantitative evaluation of the relation-ship between gesture and semantics, without requir-ing the construction of a domain ontology.Restarts are disfluencies that occur when aspeaker begins an utterance, and then stops andstarts over again.
It is thought that the gesturemay return to its state at the beginning of the utter-ance, providing a back-pointer to the restart inser-tion point (Esposito et al, 2001).
If so, then a similartraining procedure and set of gestural features canbe used for both coreference resolution and restartcorrection.
Both of these problems have objective,quantifiable success measures, and both may playan important role in bringing to spontaneous spokenlanguage useful NLP applications such as summa-rization, segmentation, and question answering.2 Current StatusMy initial work involved hand annotation of ges-ture, using the system proposed in (McNeill, 1992).It was thought that hand annotation would identifyrelevant features to be detected by computer visionsystems.
However, in (Eisenstein and Davis, 2004),we found that the gesture phrase type (e.g., deic-tic, iconic, beat) could be predicted accurately bylexical information alone, without regard to handmovement.
This suggests that this level of annota-tion inherently captures a synthesis of gesture andspeech, rather than gesture alone.
This conclusionwas strengthened by (Eisenstein and Davis, 2005),where we found that hand-annotated gesture fea-tures correlate well with sentence boundaries, butthat the gesture features were almost completely re-dundant with information in the lexical features, anddid not improve overall performance.The corpus used in my initial research was notsuitable for automatic extraction of gesture featuresby computer vision, so a new corpus was gath-ered, using a better-defined experimental protocoland higher quality video and audio recording (Adleret al, 2004).
An articulated upper body tracker,largely based on the work of (Deutscher et al, 2000),was used to identify hand and arm positions, usingcolor and motion cues.
All future work will be basedon this new corpus, which contains six videos eachfrom nine pairs of speakers.
Each video is roughlytwo to three minutes in length.Each speaker was presented with three differentexperimental conditions regarding how informationin the corpus was to be presented: a) a pre-printeddiagram was provided, b) the speaker was allowedto draw a diagram using a tracked marker, c) no pre-sentational aids were allowed.
The first conditionwas designed to be relevant to presentations involv-ing pre-created presentation materials, such as Pow-erpoint slides.
The second condition was intended tobe similar to classroom lectures or design presenta-tions.
The third condition was aimed more at directone-on-one interaction.My preliminary work has involved data from thefirst condition, in which speakers gestured at pre-printed diagrams.
An empirical study on this partof the corpus has identified several gesture featuresthat are relevant to coreference resolution (Eisen-stein and Davis, 2006a).
In particular, gesture sim-ilarity can be measured by hand position and thechoice of the hand which makes the gesture; thesesimilarities correlate with the likelihood of coref-erence.
In addition, the likelihood of a gesturalhold ?
where the hand rests in place for a period of216time ?
acts as a meta-feature, indicating that gesturalcues are likely to be particularly important to disam-biguate the meaning of the associated noun phrase.In (Eisenstein and Davis, 2006b), these features arecombined with traditional textual features for coref-erence resolution, with encouraging results.
Thehand position gesture feature was found to be thefifth most informative feature by Chi-squared anal-ysis, and the inclusion of gesture features yielded astatistically significant increase in performance overthe textual features.3 Future DirectionsThe work on coreference can be considered prelimi-nary, because it is focused on a subset of our corpusin which speakers use pre-printed diagrams as an ex-planatory aide.
This changes their gestures (Eisen-stein and Davis, 2003), increasing the proportion ofdeictic gestures, in which hand position is the mostimportant feature (McNeill, 1992).
Hand positionis assumed to be less useful in characterizing thesimilarity of iconic gestures, which express meaningthrough motion or handshape.
Using the subsectionof the corpus in which no explanatory aids were pro-vided, I will investigate how to assess the similarityof such dynamic gestures, in the hope that corefer-ence resolution can still benefit from gestural cues inthis more general case.Disfluency repair is another plausible domain inwhich gesture might improve performance.
Thereare at least two ways in which gesture could be rel-evant to disfluency repair.
Using the semantic back-pointer model, restart repairs could be identified ifthere is a strong gestural similarity between the orig-inal start point and the restart.
Alternatively, gesturecould play a pragmatic function, if there are char-acteristic gestures that indicate restarts or other re-pairs.
In one case, we are looking for a similaritybetween the disfluency and the repair point; in theother case, we are looking for similarities across alldisfluencies, or across all repair points.
It is hopedthat this research will not only improve processingof spoken natural language, but also enhance our un-derstanding of how speakers use gesture to structuretheir discourse.4 Related WorkThe bulk of research on multimodality in the NLPcommunity relates to multimodal dialogue systems(e.g., (Johnston and Bangalore, 2000)).
This re-search differs fundamentally from mine in that it ad-dresses human-computer interaction, whereas I amstudying human-human interaction.
Multimodal di-alogue systems tackle many interesting challenges,but the grammar, vocabulary, and recognized ges-tures are often pre-specified, and dialogue is con-trolled at least in part by the computer.
In my data,all of these things are unconstrained.Another important area of research is the gen-eration of multimodal communication in animatedagents (e.g., (Cassell et al, 2001; Kopp et al, 2006;Nakano et al, 2003)).
While the models devel-oped in these papers are interesting and often well-motivated by the psychological literature, it remainsto be seen whether they are both broad and preciseenough to apply to gesture recognition.There is a substantial body of empirical work de-scribing relationships between non-verbal and lin-guistic phenomena, much of which suggests thatgesture could be used to improve the detection ofsuch phenomena.
(Quek et al, 2002) describe ex-amples in which gesture correlates with topic shiftsin the discourse structure, raising the possibilitythat topic segmentation and summarization could beaided by gesture features; Cassell et al (2001) makea similar argument using body posture.
(Nakano etal., 2003) describes how head gestures and eye gazerelate to turn taking and dialogue grounding.
Allof the studies listed in this paragraph identify rel-evant correlations between non-verbal communica-tion and linguistic phenomena, but none construct apredictive system that uses the non-verbal modali-ties to improve performance beyond a text-only sys-tem.Prosody has been shown to improve performanceon several NLP problems, such as topic and sentencesegmentation (e.g., (Shriberg et al, 2000; Kim etal., 2004)).
The prosody literature demonstrates thatnon-verbal features can improve performance on awide variety of NLP tasks.
However, it also warnsthat performance is often quite sensitive, both to therepresentation of prosodic features, and how they areintegrated with other linguistic features.217The literature on prosody would suggest paral-lels for gesture features, but little such work hasbeen reported.
(Chen et al, 2004) shows that ges-ture may improve sentence segmentation; however,in this study, the improvement afforded by gesture isnot statistically significant, and evaluation was per-formed on a subset of their original corpus that waschosen to include only the three speakers who ges-tured most frequently.
Still, this work provides avaluable starting point for the integration of gesturefeature into NLP systems.5 SummarySpontaneous spoken language poses difficult prob-lems for natural language processing, but these diffi-culties may be offset by the availability of additionalcommunicative modalities.
Using a model of handgesture as providing a set of semantic back-pointersto previous utterances, I am exploring whether ges-ture can improve performance on quantitative NLPbenchmark tasks.
Preliminary results on coreferenceresolution are encouraging.ReferencesAaron Adler, Jacob Eisenstein, Michael Oltmans, LisaGuttentag, and Randall Davis.
2004.
Building the de-sign studio of the future.
In Making Pen-Based Inter-action Intelligent and Natural, pages 1?7, Menlo Park,California, October 21-24.
AAAI Press.Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-more, Candace L. Sidner, and Charles Rich.
2001.Non-verbal cues for discourse structure.
In Proc.
ofACL, pages 106?115.Lei Chen, Yang Liu, Mary P. Harper, and Eliza-beth Shriberg.
2004.
Multimodal model integra-tion for sentence unit detection.
In Proceedings ofInternational Conference on Multimodal Interfaces(ICMI?04).
ACM Press.Jonathan Deutscher, Andrew Blake, and Ian Reid.
2000.Articulated body motion capture by annealed particlefiltering.
In IEEE Conference on Computer Vision andPattern Recognition, volume 2, pages 126?133.Jacob Eisenstein and Randall Davis.
2003.
Natural ges-ture in descriptive monologues.
In UIST?03 Supple-mental Proceedings, pages 69?70.
ACM Press.Jacob Eisenstein and Randall Davis.
2004.
Visual andlinguistic information in gesture classification.
In Pro-ceedings of International Conference on MultimodalInterfaces(ICMI?04).
ACM Press.Jacob Eisenstein and Randall Davis.
2005.
Gestural cuesfor sentence segmentation.
Technical Report AIM-2005-014, MIT AI Memo.Jacob Eisenstein and Randall Davis.
2006a.
Gesture fea-tures for coreference resolution.
In Workshop on Mul-timodal Interaction and Related Machine Learning Al-gorithms.Jacob Eisenstein and Randall Davis.
2006b.
Gestureimproves coreference resolution.
In Proceedings ofNAACL.Anna Esposito, Karl E. McCullough, and Francis Quek.2001.
Disfluencies in gesture: Gestural correlates tofilled and unfilled speech pauses.
In Proceedings ofIEEE Workshop on Cues in Communication.Michael Johnston and Srinivas Bangalore.
2000.
Finite-state multimodal parsing and understanding,.
In Pro-ceedings of COLING-2000, pages 369?375.Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.2004.
Detecting structural metadata with decisiontrees and transformation-based learning.
In Proceed-ings of HLT-NAACL?04.
ACL Press.Stefan Kopp, Paul Tepper, Kim Ferriman, and JustineCassell.
2006.
Trading spaces: How humans andhumanoids use speech and gesture to give directions.Spatial Cognition and Computation, In preparation.David McNeill.
1992.
Hand and Mind.
The Universityof Chicago Press.P.
Morrel-Samuels and R. M. Krauss.
1992.
Word fa-miliarity predicts temporal asynchrony of hand ges-tures and speech.
Journal of Experimental Psychol-ogy: Learning, Memory and Cognition, 18:615?623.Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-tine Cassell.
2003.
Towards a model of face-to-facegrounding.
In Proceedings of ACL?03.Francis Quek, David McNeill, Robert Bryll, Susan Dun-can, Xin-Feng Ma, Cemil Kirbas, Karl E. McCul-lough, and Rashid Ansari.
2002.
Multimodal humandiscourse: gesture and speech.
ACM Transactions onComputer-Human Interaction (TOCHI), pages 171?193.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,and Gokhan Tur.
2000.
Prosody-based automatic seg-mentation of speech into sentences and topics.
SpeechCommunication, 32.218
