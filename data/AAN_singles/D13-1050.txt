Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524?534,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImproving Pivot-Based Statistical Machine TranslationUsing Random WalkXiaoning Zhu1*Conghui Zhu1, and Tiejun Zhao1, Zhongjun He2, Hua Wu2, Haifeng Wang2,Harbin Institute of Technology, Harbin, China1Baidu Inc., Beijing, China2{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn{hezhongjun,wu_hua,wanghaifeng}@baidu.com* This work was done when the first author was visiting Baidu.AbstractThis paper proposes a novel approach that uti-lizes a machine learning method to improvepivot-based statistical machine translation(SMT).
For language pairs with few bilingualdata, a possible solution in pivot-based SMTusing another language as a "bridge" to gen-erate source-target translation.
However, oneof the weaknesses is that some useful source-target translations cannot be generated if thecorresponding source phrase and target phraseconnect to different pivot phrases.
To allevi-ate the problem, we utilize Markov randomwalks to connect possible translation phrasesbetween source and target language.
Experi-mental results on European Parliament data,spoken language data and web data show thatour method leads to significant improvementson all the tasks over the baseline system.1 IntroductionStatistical machine translation (SMT) uses bilin-gual corpora to build translation models.
Theamount and the quality of the bilingual datastrongly affect the performance of SMT systems.For resource-rich language pairs, such as Chinese-English, it is easy to collect large amounts of bi-lingual corpus.
However, for resource-poor lan-guage pairs, such as Chinese-Spanish, it is difficultto build a high-performance SMT system with thesmall scale bilingual data available.The pivot language approach, which performstranslation through a third language, provides apossible solution to the problem.
The triangulationmethod (Wu and Wang, 2007; Cohn and Lapata,2007) is a representative work for pivot-based ma-chine translation.
With a triangulation pivot ap-proach, a source-target phrase table can beobtained by combining the source-pivot phrasetable and the pivot-target phrase table.
However,one of the weaknesses is that some correspondingsource and target phrase pairs cannot be generated,because they are connected to different pivotphrases (Cui et al 2013).
As illustrated in Figure1, since there is no direct translation between ????
henkekou?
and ?really delicious?, the trian-gulation method is unable to establish a relationbetween ????
henkekou?
and the two Spanishphrases.To solve this problem, we apply a Markov ran-dom walk method to pivot-based SMT system.Random walk has been widely used.
For example,Brin and Page (1998) used random walk to dis-cover potential relations between queries and doc-uments for link analysis in information retrieval.Analogous to link analysis, the aim of pivot-basedtranslation is to discover potential translations be-tween source and target language via the pivotlanguage.524The goal of this paper is to extend the previoustriangulation approach by exploring implicit trans-lation relations using random walk method.
Weevaluated our approach in several translation tasks,including translations between European lan-guages; Chinese-Spanish spoken language transla-tion and Chinese-Japanese translation with Englishas the pivot language.
Experimental results showthat our approach achieves significant improve-ments over the conventional pivot-based method,triangulation method.The remainder of this paper is organized as fol-lows.
In section 2, we describe the related work.We review the triangulation method for pivot-based machine translation in section 3.
Section 4describes the random walk models.
In section 5and section 6, we describe the experiments andanalyze the performance, respectively.
Section 7gives a conclusion of the paper.2 Related WorkSeveral methods have been proposed for pivot-based translation.
Typically, they can be classifiedinto 3 kinds of methods:Transfer Method: Within the transfer frame-work (Utiyama and Isahara, 2007; Wang et al2008; Costa-juss?
et al 2011), a source sentenceis first translated to n pivot sentences via a source-pivot translation system, and then each pivot sen-tence is translated to m target sentences via a piv-ot-target translation system.
At each step (sourceto pivot and pivot to target), multiple translationoutputs will be generated, thus a minimum Bayes-risk system combination method is often used toselect the optimal sentence (Gonz?lez-Rubio et al2011; Duh et al 2011).
A problem with the trans-fer method is that it needs to decode twice.
On onehand, the time cost is doubled; on the other hand,the translation error of the source-pivot translationsystem will be transferred to the pivot-target trans-lation.Synthetic Method: A synthetic method createsa synthetic source-target corpus using source-pivottranslation model or pivot-target translation model(Utiyama et al 2008; Wu and Wang, 2009).
Forexample, we can translate each pivot sentence inthe pivot-target corpus to source language with apivot-source model, and then combine the translat-ed source sentence with the target sentence to ob-tain a synthetic source-target corpus, and viceversa.
However, it is difficult to build a high quali-ty translation system with a corpus created by amachine translation system.Triangulation Method: The triangulationmethod obtains source-target model by combiningsource-pivot and pivot-target translation models(Wu and Wang, 2007; Cohn and Lapata 2007),which has been shown to work better than the oth-er pivot approaches (Utiyama and Isahara, 2007).As we mentioned earlier, the weakness of triangu-lation is that the corresponding source and targetphrase pairs cannot be connected in the case thatthey connect to different pivot phrases.3 The Triangulation MethodIn this section, we review the triangulation methodfor pivot-based translation.With the two additional bilingual corpora, thesource-pivot and pivot-target translation modelscan be trained.
Thus, a pivot model can be ob-tained by merging these two models.
In the trans-lation model, the phrase translation probability andthe lexical weight are language dependent, whichwill be introduced in the next two sub-sections.Figure 1: An example of random walk on phrase table.
The dashed line indicates an implicit relationin the phrase table.???
?feichanghaochireally deliciousvery tasty??
?henkekourealmente deliciosoChinese English Spanishmuy delicioso5253.1 Phrase Translation ProbabilityThe triangulation method assumes that there existtranslations between phrases s  and phrase p  insource and pivot languages, and between phrasep  and phrase t  in pivot and target languages.The phrase translation probability ?
betweensource and target languages is determined by thefollowing model:( | ) ( | , ) ( | )( | ) ( | )pps t s p t p ts p p t?
?
??
?==??
(1)3.2 Lexical WeightGiven a phrase pair ( , )s t and a word alignmenta  between the source word positions 1, ,i n= ?and the target word positions 0,1, ,j m= ?
, thelexical weight of phrase pair ( , )s t  can be calcu-lated with the following formula (Koehn et al2003) :( , )11( | , ) ( | ){ | ( , ) }ni ji j aip s t a s tj i j a???
?==?
??
(2)In formula 2, the lexical translation probabilitydistribution ( | )s t?
between source word s  andtarget word t  can be estimated with formula 3.
''( , )( | )( , )scount s ts tcount s t?
=?
(3)Thus the alignment a  between the sourcephrase s  and target phrase t  via pivot phrase pis needed for computing the lexical weight.
Thealignment a  can be obtained as follows:1 2{( , ) | : ( , ) & ( , ) }a s t p s p a p t a= ?
?
?
(4)where 1a  and 2a  indicate the word alignment be-tween the phrase pair ( , )s p  and ( , )p t , respec-tively.The triangulation method requires that both thesource and target phrases connect to the same piv-ot phrase.
Otherwise, the source-target phrase paircannot be discovered.
As a result, some usefultranslation relations will be lost.
In order to allevi-ate this problem, we propose a random walk model,to discover the implicit relations among the source,pivot and target phrases.4 Random Walks on Translation GraphFor phrase-based SMT, all source-target phrasepairs are stored in a phrase table.
In our randomwalk approach, we first build a translation graphaccording to the phrase table.
A translation graphcontains two types of nodes: source phrase andtarget phrase.
A source phrase s  and a targetphrase t  are connected if exists a phrase pair( , )s t  in the phrase table.
The edge can beweighted according to translation probabilities oralignments in the phrase table.
For the pivot-basedtranslation, the translation graph can be derivedfrom the source-pivot phrase table and pivot-targetphrase table.Our random walk model is inspired by twoworks (Szummer and Jaakkola, 2002; Craswelland Szummer,2007).
The general process of ran-dom walk can be described as follows:Let ( , )G V E= be a directed graph with n  ver-tices and m  edges.
For a vertex v V?
, ( )v?
de-notes the set of neighbors of v  in G .
A randomwalk on G  follows the following process: start ata vertex 0v , chose and walk along a randomneighbor 1v , with 1 0( )v v??
.
At the second step,start from 1v  and chose a random neighbor 2v , andso on.Let S be the set of source phrases, and P be theset of pivot phrases.
Then the nodes V are the un-ion of S and P. The edges E correspond to the rela-tions between phrase pairs.Let R represent the binary relations betweensource phrases and pivot phrases.
Then the 1-steptranslation ikR from node i to node k can be direct-ly obtained in the phrase table.Define operator ?
to denote the calculation ofrelation R. Then 2-step translation ijR  from node ito node j can be obtained with the following for-mula.ij ik kjR R R= ?
(4)We use |0 ( | )tR k i  to denote a t-step translationrelation from node i to node k. In order to calculatethe translation relations efficiently, we use a ma-trix A to represent the graph.
A t step translationprobability can be denoted with the following for-mula.526|0 ( | ) [ ]tt ikP k i A=                         (5)where A is a matrix whose i,k-th element is ikR .4.1 Framework of Random Walk ApproachThe overall framework of random walk for pivot-based machine translation is shown in Figure 2.Before using random walk model, we have twophrase tables: source-pivot phrase table (SP phrasetable) and pivot-target phrase table (PT phrase ta-ble).
After applying the random walk approach, wecan achieve two extended phrase table: extendedsource-pivot phrase table (S?P?
phrase table) andextended pivot-target phrase table (P?T?
phrasetable).
The goal of pivot-based SMT is to get asource-target phrase table (ST phrase table) via SPphrase table and PT phrase table.Our random walk was applied on SP phrase ta-ble or PT phrase table separately.
In next 2 sub-sections, we will explain how the phrase transla-tion probabilities and lexical weight are obtainedwith random walk model on the phrase table.Figure 3 shows some possible decoding pro-cesses of random walk based pivot approach.
Infigure 3-a, the possible source-target phrase paircan be obtained directly via a pivot phrase, so itdoes not need a random walk model.
In figure 3-band figure 3-c, one candidate source-target phrasepair can be obtained by random walks on source-pivot side or pivot-target side.
Figure 3-d showsthat the possible source-target can only by ob-tained by random walks on source-pivot side andpivot-target side.4.2 Phrase Translation ProbabilitiesFor the translation probabilities, the binary relationR is the translation probabilities in the phrase table.The operator ?
is multiplication.
According toformula 5, the random walk sums up the probabili-ties of all paths of length t between the node i andk.Figure 2: Framework of random walk based pivot translation.
The ST phrase table was generated by combin-ing SP and PT phrase table through triangulation method.
The phrase table with superscript ???
means that itwas enlarged by random walk.S?P?Phrase TableP?T?Phrase TableSPPhrase TablePTPhrase TableSTPhrase TableS?T?Phrase TablePivot withoutrandom walkPivot withrandom walkrandom walkrandom walkFigure 3: Some possible decoding processes of random walk based pivot approach.
The ?
stands for thesource phrase (S); the ?
represents the pivot phrase (P) and the ?
stands for the target phrase (T).
(a) Pivot withoutrandom walkS P T(d) Random walk onboth sidesS P T(b) Random walk onsource-pivot sideS P T(c) Random walk onpivot-target sideS P T527Take source-to-pivot phrase graph as an exam-ple; denote matrix A contains s+p nodes (s sourcephrases and p pivot phrases) to represent the trans-lation graph.
( ) ( )ij s p s pA g+ ?
+?
?= ?
?
(6)where ijg  is the i,j-th elements of matrix A.We can split the matrix A into 4 sub-matrixes:00s s spps p pAAA???
?= ?
??
?
(7)where the sub-matrix [ ]sp ik s pA p ?=  represents thetranslation probabilities from source to pivot lan-guage, and psA  represents the similar meaning.Take 3 steps walks as an example:Step1:00s s spps p pAAA???
?= ?
??
?Step2:200sp ps s pp s ps spA AAA A????
?= ?
???
?Step3:300s s sp ps spps sp ps p pA A AAA A A???
??
?= ?
??
??
?For the 3 steps example, each step performs atranslation process in the form of matrix?s self-multiplication.1.
The first step means the translation fromsource language to pivot language.
The matrixA is derived from the phrase table directly andeach element in the graph indicates a transla-tion rule in the phrase table.2.
The second step demonstrates a procedure: S-P-S?.
With 2 steps random walks, we can findthe synonymous phrases, and this procedure isanalogous to paraphrasing (Bannard andCallison-Burch, 2005).
For the example shownin  figure 1 as an example, the hidden relationbetween ????
henkekou?
and ?????feichanghaochi?
can be found through Step 2.3.
The third step describes the following proce-dure: S-P-S?-P?.
An extended source-pivotphrase table is generated by 3-step randomwalks.
Compared with the initial phrase tablein Step1, although the number of phrases isnot increased, the relations between phrasepairs are increased and more translation rulescan be obtained.
Still for the example in Fig-ure 1 , the hidden relation between ????henkekou?
and ?really delicious?
can be gen-erated in Step 3.4.3 Lexical WeightsTo build a translation graph, the two sets of phrasetranslation probabilities are represented in thephrase tables.
However, the two lexical weightsare not presented in the graph directly.
To dealwith this, we should conduct a word alignmentrandom walk model to obtain a new alignment aafter t steps.
For the computation of lexicalweights, the relation R can be expressed as theword alignment in the phrase table.
The operator?
can be induced with the following formula.1 2{( , ) | : ( , ) & ( , ) }a x y p x z a z y a= ?
?
?
(8)where a1 and a2 represent the word alignmentinformation inside the phrase pairs ( , )x y  and( , )y z respectively.
An example of wordalignment inducing is shown in Figure 4.
With anew word alignment, the two lexical weights canbe calculated by formula 2 and formula 3.Figure 4: An example of word alignment induction with 3 steps random walks?
??
?
?
?could   you   fill   out   this   form ?
?
??
??
?
?please   fill   out   this   form?
??
?
?
?could   you   fill   out   this   formstep 1step 2step 35285 Experiments5.1 Translation System and Evaluation Met-ricIn our experiments, the word alignment was ob-tained by GIZA++ (Och and Ney, 2000) and theheuristics ?grow-diag-final?
refinement rule.
(Koehn et al 2003).
Our translation system is anin-house phrase-based system using a log-linearframework including a phrase translation model, alanguage model, a lexicalized reordering model, aword penalty model and a phrase penalty model,which is analogous to Moses (Koehn et al 2007).The baseline system is the triangulation methodbased pivot approach (Wu and Wang, 2007).To evaluate the translation quality, we usedBLEU (Papineni et al 2002) as our evaluationmetric.
The statistical significance using 95% con-fidence intervals were measured with paired boot-strap resampling (Koehn, 2004).5.2 Experiments on Europarl5.2.1.
Data setsWe mainly test our approach on Europarl1We perform our experiments on different trans-lation directions and via different pivot languages.As a most widely used language in the world(Mydans, 2011), English was used as the pivotlanguage for granted when carrying out experi-ments on different translation directions.
For trans-lating Portuguese to Swedish, we also tried toperform our experiments via different pivot lan-corpus,which is a multi-lingual corpus including 21 Euro-pean languages.
Due to the size of the data, weonly select 11 languages which were added toEuroparl from 04/1996 or 01/1997, including Dan-ish (da), German (de), Greek (el), English (en),Spanish (es), Finnish (fi), French (fr), Italian (it)Dutch (nl) Portuguese (pt) and Swedish (sv).
Inorder to avoid a trilingual scenario, we split thetraining corpus into 2 parts by the year of the data:the data released in odd years were used for train-ing source-pivot model and the data released ineven years were used for training pivot-targetmodel.1 http://www.statmt.org/europarl/guages.
Table 1 and Table 2 summarized the train-ing data.LanguagePairs(src-pvt)SentencePairs #LanguagePairs(pvt-tgt)SentencePairs #da-en 974,189 en-da 953,002de-en 983,411 en-de 905,167el-en 609,315 en-el 596,331es-en 968,527 en-es 961,782fi-en 998,429 en-fi 903,689fr-en 989,652 en-fr 974,637it-en 934,448 en-it 938,573nl-en 982,696 en-nl 971,379pt-en 967,816 en-pt 960,214sv-en 960,631 en-sv 869,254Table1.
Training data for experiments using English asthe pivot language.
For source-pivot (src-pvt; xx-en)model training, the data of odd years were used.
Insteadthe data of even years were used for pivot-target (pvt-src; en-xx) model training.LanguagePairs(src-pvt)SentencePairs #LanguagePairs(pvt-tgt)SentencePairs #pt-da 941,876 da-sv 865,020pt-de 939,932 de-sv 814,678pt-el 591,429 el-sv 558,765pt-es 934,783 es-sv 827,964pt-fi 950,588 fi-sv 872,182pt-fr 954,637 fr-sv 860,272pt-it 900,185 it-sv 813,000pt-nl 945,997 nl-sv 864,675Table2.
Training data for experiments via different piv-ot languages.
For source-pivot (src-pvt; pt-xx) modeltraining, the data of odd years were used.
Instead thedata of even years were used for pivot-target (pvt-src;xx-sv) model training.Test Set Sentence # Reference #WMT06 2,000 1WMT07 2,000 1WMT08 2,000 1Table3.
Statistics of test sets.529Several test sets have been released for theEuroparl corpus.
In our experiments, we usedWMT20062, WMT20073 and WMT20084 as ourtest data.
The original test data includes 4 lan-guages and extended versions with 11 languagesof these test sets are available by the EuroMatrix55.2.2.
Experiments on Different TranslationDirectionsproject.
Table 3 shows the test sets.We build 180 pivot translation systems6The baseline system was built following the tra-ditional triangulation pivot approach.
Table 4 liststhe results on Europarl training data.
Limited by(including90 baseline systems and 90 random walk basedsystems) using 10 source/target languages and 1pivot language (English).2 http://www.statmt.org/wmt06/shared-task/3 http://www.statmt.org/wmt07/shared-task.html4 http://www.statmt.org/wmt08/shared-task.html5 http://matrix.statmt.org/test_sets/list6 Given N languages, a total of N*(N-1) SMT systems shouldbe build to cover the translation between each language.the length of the paper, we only show the resultson WMT08, the tendency of the results onWMT06 and WMT07 is similar to WMT08.Several observations can be made from the table.1.
In all 90 language pairs, our method achievesgeneral improvements over the baseline system.2.
Among 90 language pairs, random walkbased approach is significantly better than thebaseline system in 75 language pairs.3.
The improvements of our approach are notequal in different translation directions.
The im-provement ranges from 0.06 (it-es) to 1.21 (pt-da).One possible reason is that the performance is re-lated with the source and target language.
For ex-ample, when using Finnish as the target language,the improvement is significant over the baseline.This may be caused by the great divergence be-tween Uralic language (Finnish) and Indo-European language (the other European languagein Table4).
From the table we can find that thetranslation between languages in different lan-guage family is worse than that in some languagefamily.
But our random walk approach can im-TGTSRCda de el es fi fr it nl pt svBaselineRWda -19.8320.15*20.4621.02*27.5928.29*14.7615.63*24.1124.71*20.4920.82*22.2622.57*24.3824.88*28.3328.87*BaselineRWde23.3523.69*-19.8320.0526.2126.70*12.7213.57*22.4322.78*18.8219.32*23.7424.11*23.0523.35*21.1721.27BaselineRWel23.2423.82*18.1218.49*-32.2832.4813.3114.08*27.3527.67*23.1923.63*20.8021.26*27.6227.8622.7023.15*BaselineRWes25.3426.07*19.6720.17*27.2427.52-13.9314.61*32.9133.1627.6727.9222.3722.85*34.7334.9324.8325.50*BaselineRWfi18.2918.63*13.2013.4014.7215.00*20.1720.48*-17.5217.84*14.7615.0115.5016.04*17.3017.68*16.6316.79BaselineRWfr25.6726.51*20.0220.45*26.5826.7537.5037.80*13.9014.75*-28.5128.7122.6523.33*33.8133.9324.6425.59*BaselineRWit22.6323.27*17.8118.40*24.2424.66*34.3635.42*13.2014.11*30.1630.48*-21.3721.81*30.8430.92*22.1222.64*BaselineRWnl22.4922.7619.8620.45*18.5619.10*24.6925.19*11.9612.63*21.4822.05*18.3618.67*-21.7122.13*19.8322.17*BaselineRWpt24.0825.29*19.1119.83*25.3026.20*36.5937.13*13.3314.21*32.4732.78*28.0828.44*21.5222.46*-22.9023.90*BaselineRWsv31.2431.75*20.2620.74*22.0622.59*29.2129.87*15.3916.13*25.6326.18*21.2521.81*22.3022.62*25.6026.09*-Table4.
Experimental results on Europarl with different translation directions (BLEU% on WMT08).RW=Random Walk.
* indicates the results are significantly better than the baseline (p<0.05).530prove the performance of translations between dif-ferent language families.5.2.3.
Experiments via Different Pivot Lan-guagesIn addition to using English as the pivot language,we also try some other languages as the pivotlanguage.
In this sub-section, experiments werecarried out from translating Portuguese to Swedishvia different pivot languages.Table 5 summarizes the BLEU% scores of dif-ferent pivot language when translating from Por-tuguese to Swedish.
Similar to Table 4, ourapproach still achieves general improvements overthe baseline system even if the pivot language hasbeen changed.
From the table we can see that formost of the pivot language, the random walk basedapproach gains more than 1 BLEU score over thebaseline.
But when using Finnish as the pivot lan-guage, the improvement is only 0.02 BLEU scoreson WMT08.
This phenomenon shows that the piv-ot language can also influence the performance ofrandom walk approach.
One possible reason forthe poor performance of using Finnish as the pivotlanguage is that Finnish belongs to Uralic lan-guage family, and the other languages belong toIndo-European family.
The divergence betweendifferent language families led to a poor perfor-mance.
Thus how to select a best pivot language isour future work.The problem with random walk is that it willlead to a larger phrase table with noises.
In thissub-section, a pre-pruning (before random walk)and a post-pruning (after random walk) methodwere introduced to deal with this problem.We used a naive pruning method which selectsthe top N phrase pairs in the phrase table.
In ourexperiments, we set N to 20.
For pre-pruning, weprune the SP phrase table and PT phrase table be-fore applying random walks.
Post-pruning meansthat we prune the ST phrase table after randomwalks.
For the baseline system, we also apply apruning method before combine the SP and PTphrase table.
We test our pruning method on pt-en-sv translation task.
Table 6 shows the results.With a pre- and post-pruning method, the ran-dom walk approach is able to achieve further im-provements.
Our approach achieved BLEU scoresof 25.11, 24.69 and 24.34 on WMT06, WMT07and WMT08 respectively, which is much betterthan the baseline and the random walk approachwith pruning.
Moreover, the size of the phrasetable is about half of the no-pruning method.When adopting a post-pruning method, the per-formance of translation did not improved signifi-cantly over the pre-pruning, but the scale of thephrase table dropped to 69M, which is only about2 times larger than the triangulation method.Phrase table pruning is a key work to improvethe performance of random walk.
We plan to ex-plore more approaches for phrase table pruning.E.g.
using significance test (Johnson et al 2007)or monolingual key phrases (He et al 2009) tofilter the phrase table.Table5.
Experimental results on translating from Portu-guese to Swedish via different pivot language.RW=Random Walk.
* indicates the results are signifi-cantly better than the baseline (p<0.05).Table6.
Results of Phrase Table FilteringtranslanguageWMT06WMT07WMT08BaselineRWpt-da-sv23.4024.47*22.8024.21*22.4923.75*BaselineRWpt-de-sv22.7223.12*22.2123.26*21.7622.35*BaselineRWpt-el-sv22.5323.75*22.1923.22*21.3722.40*BaselineRWpt-en-sv23.5424.66*23.2424.22*22.9023.90*BaselineRWpt-es-sv23.5824.65*23.3724.10*22.8023.77*BaselineRWpt-fi-sv21.0621.1720.0620.42*20.2620.28BaselineRWpt-fr-sv23.5524.75*23.0924.15*22.8923.96*BaselineRWpt-it-sv23.6524.74*22.9624.18*22.7924.02*BaselineRWpt-nl-sv21.8723.06*21.8322.76*21.3622.29*WMT06WMT07WMT08PhrasePairs #Baseline+pruning23.5424.05*23.2423.70*22.9023.59*46M32MRW+pre-pruning+post-pruning24.6625.1125.19*24.2224.6924.79*23.9024.3424.41*215M109M69M5315.3 Experiments on Spoken LanguageThe European languages show various degrees ofsimilarity to one another.
In this sub-section, weconsider translation from Chinese to Spanish withEnglish as the pivot language.
Chinese belongs toSino-Tibetan Languages and English/Spanish be-longs to Indo-European Languages, the gap be-tween two languages is wide.A pivot task was included in IWSLT 2008 inwhich the participants need to translate Chinese toSpanish via English.
A Chinese-English and anEnglish-Spanish data were supplied to carry outthe experiments.
The entire training corpus wastokenized and lowercased.
Table 7 and Table 8summarize the training data and test data.Table 9 shows the similar tendency with Table 4.The random walk models achieved BLEU% scores32.09, which achieved an absolute improvement of2.08 percentages points on BLEU over the base-line.CorpusSentencepair #Sourceword #Targetword #CE 20,000 135,518 182,793ES 19,972 153,178 147,560Table 7: Training Data of IWSLT2008Test Set Sentence # Reference #IWSLT08 507 16Table8.
Test Data of IWSLT2008System BLEU% phrase pairs #Baseline 30.01 143,790+pruning 30.25 108,407RW 31.57 2,760,439+pre-pruning 31.99 1,845,648+post-pruning 32.09* 1,514,694Table9.
Results on IWSLT20085.4 Experiments on Web DataThe setting with Europarl data is quite artificial asthe training data for directly translating betweensource and target actually exists in the originaldata sets.
The IWSLT data set is too small to rep-resent the real scenario.
Thus we try our experi-ment on a more realistic scenario: translating fromChinese to Japanese via English with web crawleddata.All the training data were crawled on the web.The scale of Chinese-English and English-Japanese is 10 million respectively.
The test setwas built in house with 1,000 sentences and 4 ref-erences.System BLEU% phrase pairs #Baseline 28.76 4.5G+pruning 28.90 273MRW 29.13 46G+pre-pruning 29.44 11G+post-pruning 29.51* 3.4GTable10.
Results on Web DataTable 10 lists the results on web data.
From thetable we can find that the random walk model canachieve an absolute improvement of 0.75 percent-ages points on BLEU over the baseline.In this subsection, the training data containsparallel sentences with different domains.
And thetwo training corpora (Chinese-English and Eng-lish-Japanese) are typically very different.
Itmeans that our random walk approach is robust inthe realistic scenario.6 DiscussionsThe random walk approach mainly improves theperformance of pivot translation in two aspects:reduces the OOVs and provides more hypothesisphrases for decoding.6.1 OOVOut-of-vocabulary (OOV 7We count the OOVs when decoding with trian-gulation model and random walk model onIWSLT2008 data.
The statistics shows that whenusing triangulation model, there are 11% OOVswhen using triangulation model, compared with9.6% when using random walk model.
Less OOVoften lead to a better result.)
terms cause seriousproblems for machine translation systems (Zhanget al 2005).
The random walk model can reducethe OOVs.
As illustrated in Figure 1, the Chinesephrase ????henkekou?
cannot be connected toany Spanish phrase, thus it is a OOV term.7 OOV refer to phrases here.5326.2 Hypothesis PhrasesTo illustrate how the random walk method helpsimprove the performance of machine translation,we illustrate an example as follows:- Source: ?
?
?
?
?wo xiang yao zhentou- Baseline trans: Quiero almohada- Random Walk trans: Quiero una almohadaFor translating a Chinese sentence ?????
?wo xiang yao zhentou?
to Spanish, we can get twocandidate translations.
In this case, the randomwalk translation is better than the baseline system.The key phrase in this sentence is ???
zhentou?,figure 5 shows the extension process.
In this case,the article ?a?
is hidden in the source-pivot phrasetable.
The same situation often occurs in articlesand prepositions.
Random walk is able to discoverthe hidden relations (hypothesis phrases) amongsource, pivot and target phrases.7 Conclusion and Future WorkIn this paper, we proposed a random walk methodto improve pivot-based statistical machine transla-tion.
The random walk method can find implicitrelations between phrases in the source and targetlanguages.
Therefore, more source-target phrasepairs can be obtained than conventional pivot-based method.
Experimental results show that ourmethod achieves significant improvements overthe baseline on Europarl corpus, spoken languagedata and the web data.A critical problem in the approach is the noisethat may bring in.
In this paper, we used a simplefiltering to reduce the noise.
Although the filteringmethod is effective, other method may work better.In the future, we plan to explore more approachesfor phrase table pruning.AcknowledgmentsWe would like to thank Jianyun Nie, Muyun Yangand Lemao Liu for insightful discussions, andthree anonymous reviewers for many invaluablecomments and suggestions to improve our paper.This work is supported by National Natural Sci-ence Foundation of China (61100093), and theKey Project of the National High Technology Re-search and Development Program of China(2011AA01A207).ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics, pages597-604Sergey Brin and Lawrence Page.
1998.
The Anatomy ofa Large-Scale Hypertextual Web Search Engine.
InProceedings of the Seventh International WorldWide Web ConferenceTrevor Cohn and Mirella Lapata.
2007.
Machine Trans-lation by Triangulation: Make Effective Use of Mul-ti-Parallel Corpora.
In Proceedings of 45th AnnualMeeting of the Association for Computational Lin-guistics, pages 828-735.Marta R.
Costa-juss?, Carlos Henr?quez, and Rafael E.Banchs.
2011.
Enhancing Scarce-Resource LanguageTranslation through Pivot Combinations.
In Proceed-ings of the 5th International Joint Conference onNatural Language Processing, pages 1361-1365Nick Craswell and Martin Szummer.
2007.
RandomWalks on the Click Graph.
In Proceedings of the30th annual international ACM SIGIR conference onResearch and development in information retrieval,pages 239-246Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhaoand Dequan Zheng.
2013.
Phrase Table CombinationDeficiency Analyses in Pivot-based SMT.
In Pro-ceedings of 18th International Conference on Appli-cation of Natural Language to Information Systems,pages 355-358.Kevin Duh, Katsuhito Sudoh, Xianchao Wu, HajimeTsukada and Masaaki Nagata.
2011.
GeneralizedMinimum Bayes Risk System Combination.
In Pro-ceedings of the 5th International Joint Conferenceon Natural Language Processing, pages 1356?1360Jes?s Gonz?lez-Rubio, Alfons Juan and FranciscoCasacuberta.
2011.
Minimum Bayes-risk SystemFigure 5: Phrase extension process.
The dotted lineindicates an implicit relation in the phrase table.??
?ge zhentou?
?zhentoupillowa pillowalmohadaunaalmohada533Combination.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics, pages 1268?1277Zhongjun He, Yao Meng, Yajuan L?, Hao Yu and QunLiu.
2009.
Reducing SMT Rule Table with Mono-lingual Key Phrase.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121-124Howard Johnson, Joel Martin, George Foster, and Ro-land Kuhn.
2007.
Improving  translation quality bydiscarding most of the phrase table.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 967?975.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In HLT-NAACL:Human Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 127-133Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 388?395.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit X, pages 79-86.Philipp Koehn, Hieu Hoang, Alexanda Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Rich-ard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics, demon-stration session, pages 177?180.Franz Josef Och and Hermann Ney.
2000.
A compari-son of alignment models for statistical machinetranslation.
In Proceedings of the 18th InternationalConference on Computational Linguistics, pages1086?1090Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputation Linguistics, pages 311-319Karl Pearson.
1905.
The Problem of the Random Walk.Nature, 27(1865):294Mydans, Seth.
2011.
Across cultures, English is theword.
New York Times.Martin Szummer and Tommi Jaakkola.
2002.
PartiallyLabeled Classification with Markov Random Walks.In Advances in Neural Information Processing Sys-tems, pages 945-952Kristina Toutanova, Christopher D. Manning and An-drew Y. Ng.
2004.
Learning Random Walk Modelsfor Inducting Word Dependency Distributions.
InProceedings of the 21st International Conference onMachine Learning.Masao Utiyama and Hitoshi Isahara.
2007.
A Compari-son of Pivot Methods for Phrase-Based StatisticalMachine Translation.
In Proceedings of HumanLanguage Technology: the Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 484-491Masao Utiyama, Andrew Finch, Hideo Okuma, MichaelPaul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-suda, and Eiichiro Sumita.
2008.
The NICT/ATRspeech Translation System for IWSLT 2008.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation, pages 77-84Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,Jianfeng Li, Dengjun Ren, and Zhengyu Niu.
2008.The TCH Machine Translation System for IWSLT2008.
In Proceedings of the International Workshopon Spoken Language Translation, pages 124-131Hua Wu and Haifeng Wang.
2007.
Pivot Language Ap-proach for Phrase-Based Statistical Machine Transla-tion.
In Proceedings of 45th Annual Meeting of theAssociation for Computational Linguistics, pages856-863.Hua Wu and Haifeng Wang.
2009.
Revisiting PivotLanguage Approach for Machine Translation.
InProceedings of the 47th Annual Meeting of the Asso-ciation for Computational Linguistics and the 4thIJCNLP of the AFNLP, pages 154-162Ying Zhang, Fei Huang, Stephan Vogel.
2005.
Miningtranslations of OOV terms from the web throughcross-lingual query expansion.
In Proceedings of the27th ACM SIGIR.
pages 524-525534
