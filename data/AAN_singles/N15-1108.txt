Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1030?1035,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsShift-Reduce Constituency Parsing with Dynamic Programmingand POS Tag LatticeHaitao Mi??T.J.
Watson Research CenterIBMhmi@us.ibm.comLiang Huang??
?Queens College & Graduate CenterCity University of New Yorkliang.huang.sh@gmail.comAbstractWe present the first dynamic programming(DP) algorithm for shift-reduce constituencyparsing, which extends the DP idea of Huangand Sagae (2010) to context-free grammars.To alleviate the propagation of errors frompart-of-speech tagging, we also extend theparser to take a tag lattice instead of a fixed tagsequence.
Experiments on both English andChinese treebanks show that our DP parsersignificantly improves parsing quality overnon-DP baselines, and achieves the best accu-racies among empirical linear-time parsers.1 IntroductionIncremental parsing has gained popularity in bothdependency (Nivre, 2004; Zhang and Clark, 2008)and constituency parsing (Zhu et al, 2013; Wangand Xue, 2014).
However, the greedy or beamsearch algorithms used in these parsers can only ex-plore a tiny fraction of trees among exponentiallymany candidates.
To alleviate this problem, Huangand Sagae (2010) propose a dynamic programming(DP) algorithm, reducing the search space to a poly-nomial size by merging equivalent states.
This ideahas been extended by Kuhlmann et al (2011) andCohen et al (2011) to other dependency parsingparadigms.In constituency parsing, however, DP has not yetbeen applied to incremental parsing, and the big-ger search space in constituency parsing suggests apotentially even bigger advantage by DP.
However,with unary rules and more-than-binary branchings,constituency parsing presents challenges not foundin dependency parsing that must be addressed be-fore applying DP.
Thus, we first present an odd-evenshift-reduce constituency parser which always fin-ishes in same number of steps, eliminating the com-plicated asynchronicity issue in previous work (Zhuet al, 2013; Wang and Xue, 2014), and then de-velop dynamic programming on top of that.
Sec-ondly, to alleviate the error propagation from POStagging, we also extends the algorithm to take a tag-ging sausage lattice as input, which is a compromisebetween pipeline and joint approaches (Hatori et al,2011; Li et al, 2011; Wang and Xue, 2014).Our DP parser achieves state-of-the-art perfor-mances on both Chinese and English treebanks (at90.8% on PTB and 83.9% on CTB, the latter beingthe highest in literature).2 Odd-Even Shift-Reduce CFG ParserOne major challenge in constituency parsing isunary rules.
Unlike dependency parsing where shift-reduce always finishes in 2n?1 steps, existing incre-mental constituency parsers (Zhu et al, 2013; Wangand Xue, 2014) reach the goal state (full parse tree)in different steps due to different number of unaryrules.
So we propose a new, synchronized, ?odd-even?
system to reach the goal in the same 4n ?
2steps.
A state is notated p = ?S,Q?, where S is astack of trees ..., s1, s0, and Q is a queue of word-tag pairs.
At even steps (when step index is even)we can choose one of the three standard actions?
sh: shift the head of Q, a word-tag pair (t, w),onto S as a singleton tree t(w);?
rexx: combine the top two trees on the stack andreplace them with a new tree x(s1, s0), x beingthe root nonterminal, headed on s0;?
rexy: similar to rexxbut headed on s1;and at odd steps we can choose two new actions:1030input (t1, w1) ... (tn, wn)axiom 0 : ?, (t1, w1) ... (tn, wn)?
: 0shl : ?S, (t, w)|Q?
: cl+1 : ?S|t(w), Q?
: c+cshl is evenrexxl : ?S|s1|s0, Q?
: cl+1 : ?S|x(s1, s0), Q?
: c+crexxl is evenunxl : ?S|s0, Q?
: cl+1 : ?S|x(s0), Q?
: c+cunxl is oddstl : ?S|s0, Q?
: cl+1 : ?S|s0, Q?
: c+cstl is oddgoal 2(2n?
1) : ?s0, ?
: cFigure 1: Shift-reduce system, omitting rexy.
c is themodel score, and csh, crexx, etc.
are the action scores.?
unx: replace s0with a new tree x(s0) with xbeing the root nonterminal;?
st: no action.Figure 1 shows the deductive system.
Note thatwe alternate between standard shift-reduce actionsin even steps and unary actions (unxor st) in oddsteps, and the first action must be sh, followed by aunxor st, and followed by another sh.
Continuingthis procedure, we can always achieve the goal in2(2n?
1) steps.In practice, we have larger than two-way rules andmulti-level unary rules, so we binarize them and col-lapse multi-level unary rules into one level, for ex-ample,NPSVPPPNPV=?NP+SVPPPVP?NPVFollowing Huang and Sagae (2010), we representfeature templates as functions f(?, ?)
on stack S andqueue Q.
Table 1 shows the 43 feature templates weuse in this paper, all adopted from Zhu et al (2013).They are combinations of the 32 atomic features?f(S,Q) (e.g.
s0.t and s0.c denote the head tag andshl : ?S, (t, w)|Q?
: (c, v)l+1 : ?S|t(w), Q?
: (c+csh, 0)l is evenrexxstate p:l?
:?S?|s?1, Q??
: (c?, v?
)state q:l :?S|s1|s0, Q?
: (c, v)l+1 : ?S?|x(s?1, s0), Q?
: (c?+v+?, v?+v+?
)l and l?are even, p ?
pi(q)unxl : ?S|s0, Q?
: (c, v)l+1 : ?S|x(s0), Q?
: (c+cunx, v + cunx)l is oddFigure 2: DP shift-reduce, omitting rexyand st. c and vare prefix and inside scores, and ?
= csh(p) + crexx(q).State equivalence is defined below in Section 3.syntactic category of tree s0, resp., and s0.lc.w is thehead word of its leftmost child).3 Dynamic ProgrammingThe key idea towards DP is the merging of equiva-lent states, after which the stacks are organized in a?graph-structured stack?
(GSS)(Tomita, 1988).
Fol-lowing Huang and Sagae (2010), ?equivalent states??
in a same beam are defined by the atomic features?f(S,Q) and the span of s0:?S,Q?
?
?S?, Q???
?f(S,Q) =?f(S?, Q?)
and s0.span = s?0.span.Similarly, for each state p, pi(p) is a set of predictorstates, each of which can be combined with p in arexxor rexyaction.
For each action, we have differ-ent operations on pi(p).
If a state pmakes a sh actionand generates a state p?, then pi(p?)
= {p}.
If twoshifted states p?and p?
?are equivalent, p??
p?
?, wemerge pi(p?)
and pi(p??).
If a state p makes a reduce(rexxor rexy) action, p tries to combine with everyp??
pi(p), and each combination generates a state rwith pi(r) = pi(p?).
If two reduced states are equiva-lent, we only keep one predictor states, as their pre-dictor states are identical.
If a state p fires an unxora st action resulting in a state u, we copy the predic-tor states pi(u) = pi(p).
Similar to reduce actions, iftwo resulting states after applying an unxor a st ac-tion are equivalent, we only keep the best one withhighest score (the recombined ones are only usefulfor searching k-best trees).1031feature templates f(S,Q)unigrams s0.t ?
s0.c s0.w ?
s0.c s1.t ?
s1.c s1.w ?
s1.c s2.t ?
s2.c s2.w ?
s2.cs3.t ?
s3.c q0.w ?
q0.t q1.w ?
q1.t q2.w ?
q2.t q3.w ?
q3.t s0.lc.w ?
s0.lc.cs0.rc.w ?
s0.rc.c s0.u.w ?
s0.u.c s1.lc.w ?
s1.lc.c s1.rc.w ?
s1.rc.c s1.u.w ?
s1.u.cbigrams s0.w ?
s1.w s0.w ?
s1.c s0.c ?
s1.w s0.c ?
s1.c s0.w ?
q0.w s0.w ?
q0.ts0.c ?
q0.w s0.c ?
q0.t q0.w ?
q1.w q0.w ?
q1.t q0.t ?
q1.w q0.t ?
q1.ts1.w ?
q0.w s1.w ?
q0.t s1.c ?
q0.w s1.c ?
q0.ts0.c ?
s0.lc.c ?
s0.rc.c ?
s1.c s0.c ?
s0.lc.c ?
s0.rc.c ?
s1.ctrigrams s0.c ?
s1.c ?
s2.c s0.w ?
s1.c ?
s2.c s0.c ?
s1.w ?
q0.ts0.c ?
s1.c ?
s2.w s0.c ?
s1.c ?
q0.t s0.w ?
s1.c ?
q0.ts0.c ?
s1.w ?
q0.t s0.c ?
s1.c ?
q0.wTable 1: All feature templates (43 templates based on 32 atomic features), taken from Zhu et al (2013).
si.c, si.w andsi.t denote the syntactic label, the head word, and the head tag of si.
si.lc.w means the head word of the left child ofsi.
si.u.w means the head word of the unary root si.
qi.w and qi.t denote the word and the tag of qi.input (T1, w1)...(Tn, wn)axioms 0 : ?, (t, w1)...(Tn, wn)({</s>}, </s>)?
: 0,?
t ?
T1shl : ?S, (t, w)|(T?, w?)|Q?
: (c, v)l+1 : ?S|t(w), (t?, w?)|Q?
: (c+csh, 0)t??
T?,l is evenFigure 3: Extended shift-reduce deductive system withtagging sausage lattice, only showing sh.In order to compute all the scores in GSS, for eachstate p, we calculate the prefix score, c, which is thetotal cost of the best action sequence from the initialstate to the end of state p, and the inside score v,which is the score since the last shift (Figure 2).The new mechanism beyond Huang and Sagae(2010) is the non-trivial dynamic programmingtreatment of unary actions (unxand st), which is notfound in dependency parsing.
Note that the scorecalculation is quite different from shift in the sensethat unary actions are more like reduces.4 Incorporating Tag LatticesIt is easy to extend our deductive system to take tag-ging sausage lattices as input.
The key differenceis that the tag t associated with each word in theinput sequence becomes a set of tags T .
Thus, inthe sh action, we split the state with all the possibletags t?in the tagset T?for the second word on thequeue.
Figure 3 shows the deductive system, wherewe only change the sh action, input and axiom.
Forsimplicity reasons we only present one word look87.58888.58989.5902  4  6  8  10  12  14  16  18F1onthedevsetiteration11th15thDPnon-DPFigure 4: The learning curves of non-DP and DP parserson the development set.
DP achieves the best perfor-mance at 11th iteration with 89.8%, while non-DP getsits optimal iteration at 15th with a lower F1 89.5%.ahead (we just need to know the tag of the first wordon the queue), but in practice, we use a look ahead of4 words (q0..q3, see Table 1), so each shift actuallysplits the tagset of the 5th word on the queue (q4).5 ExperimentsWe evaluate our parsers on both Penn English Tree-bank (PTB) and Chinese Treebank (CTB).
For PTB,we use sections 02-21 as the training, section 24 asthe dev set, and section 23 as the test.
For CTB,we use the version of 5.1, articles 001-270 and 440-1151 as the training data, articles 301-325 as the devset, and articles 271-300 as the test set.Besides training with gold POS tags, we addk-best automatic tagging results to the trainingset using a MaxEnt model with ten-way jackknif-ing (Collins, 2000).
And we automatically tag thedev and test sets with k-best tagging sequences us-10328686.58787.58888.58989.59016  32  48  64F1onthedevsetbeam sizeDP train, DP testnon-DP train, non-DP testFigure 5: The F1 curves of non-DP and DP parsers (trainand test consistently) on the dev set.ing the MaxEnt POS tagger (at 97.1% accuracy onEnglish, and 94.5% on Chinese) trained on the train-ing set.
We set k to 20 for English.
And we run twosets of experiments, 1-best vs. 20-best, for Chineseto address the tagging issue.
We train our parsers us-ing ?max-violation perceptron?
(Huang et al, 2012)(which has been shown to converge much faster than?early-update?
of Collins and Roark (2004)) withminibatch parallelization (Zhao and Huang, 2013)on the head-out binarized and unary-collapsed train-ing set.
We finally debinarize the trees to recover thecollapsed unary rules.We evaluate parser performance with EVALB in-cluding labeled precision (LP), labeled recall (LR),and bracketing F1.
We use a beam size of 32, andpick the optimal iteration number based on the per-formances on the dev set.Our baseline is the shift-reduce parser withoutstate recombination (henceforth ?non-DP?
), and ourdynamic programming parser (henceforth ?DP?)
isthe extension of the baseline.5.1 Learning Curves and Search QualityFigure 4 shows the learning curves on the PTB devset.
With a same beam width, DP parser achieves abetter performance (89.8%, peaking at the 11th it-eration) and converges faster than non-DP.
Pickingthe optimal iterations for DP and non-DP models,we test each with various beam size, and plot the F1curves in Figure 5.
Again, DP is always better thannon-DP, with 0.5% difference at beam of 64.LR LP F1 comp.Collins (1999) 88.1 88.3 88.2 O(n5)Charniak (2000) 89.5 89.9 89.5 O(n5)?Carreras (2008) 90.7 91.4 91.1 O(n4)?Petrov (2007) 90.1 90.2 90.1 O(n3)?Ratnaparkhi (1997) 86.3 87.5 86.9O(n)Sagae (2006) 87.8 88.1 87.9Zhu (2013) 90.2 90.7 90.4non-DP 90.3 90.4 90.3O(n)DP 90.7 90.9 90.8Table 2: Final Results on English (PTB) test set (sec23).
?The empirical complexities for Charniak and Petrov areO(n2.5) andO(n2.4), resp., ?but Carreras is exactO(n4).LR LP F1 POSCharniak (2000) 79.6 82.1 80.8 -Petrov (2007) 81.9 84.8 83.3 -Zhu (2013) 82.1 84.3 83.2 -Wang (2014) (1-best POS) 80.3 80.0 80.1 94.0Wang (2014) (joint) 82.9 84.2 83.6 95.5non-DP (1-best POS) 80.7 80.5 80.6 94.5non-DP (20-best POS) 83.3 83.2 83.2 95.5DP (20-best POS) 83.6 84.2 83.9 95.6Table 3: Results on Chinese (CTB) 5.1 test set.5.2 Final Results on EnglishTable 2 shows the final results on the PTB test set.The last column shows the empirical time com-plexity.
Our baseline parser achieves a competitivescore, which is higher than Berkeley even with a lin-ear time complexity, and is comparable to Zhu et al(2013).
Our DP parser improves the F1 score by0.5 points over the non-DP, and achieves the best F1score among empirical linear-time parsers.5.3 Sausage Lattice ParsingTo alleviate the propagation of errors from POS tag-ging, we run sausage lattice parsing on both Chineseand English, where Chinese tagging accuracy signif-icantly lag behind English.Table 3 shows the F1 score and POS tagging ac-curacy of all parsing models on the Chinese 5.1 testset.
Our MaxEnt POS tagger achieves an accuracyof 94.5% on 1-best outputs, and an oracle score of97.1% on 20-best results.
The average number of1033tags for each word in the 20-best list is 1.1.The joint tagging and parsing approach of Wangand Xue (2014) improves the F1 score from 80.1%to 83.6% (see lines 4 and 5).
We instead use sausagelattices, a much cheaper way.
The non-DP (1-bestPOS) and non-DP (20-best POS) lines show the ef-fectiveness of using sausage lattices (+1.1 for tag-ging and +2.6 for parsing).
As Wang and Xue (2014)is a non-DP model, it is comparable to our non-DPresults.
With the help of 20-best tagging lattices, weachieve the same tagging accuracy at 95.5%, but still0.4 worse on the F1 score than the joint model.
Itsuggests that we need a larger k to catch up the gap.But our DP model boosts the performance further tothe best score at 83.9% with a similar set of features.The last two lines (non-DP and DP) in Table 2show our English lattice parsing results.
So we runanother baseline with the non-DP English parser on1-best POS tags, and the baseline achieves a taggingaccuracy at 97.11 and an F1 score at 90.1.
Com-paring to the tagging accuracy (97.15) and F1 score(90.3) of our non-DP lattice parser, sausage latticeparsing doesn?t help the tagging accuracy, but helpsparsing a little by 0.2 points.
The statistics show that2 percent of POS tags in the lattice parsing resultare different from the baseline, and those differenceslead to a slight improvement on parsing.6 ConclusionsIn this paper, we present a dynamic programming al-gorithm based on graph-structured stack (GSS) forshift-reduce constituency parsing, and extend the al-gorithm to take tagging sausage lattices as input.
Ex-periments on both English and Chinese treebanksshow that our DP parser outperforms almost all otherparsers except of Carreras et al (2008), which runsin a much higher time complexity.AcknowledgmentWe thank the anonymous reviewers for comments.Haitao Mi is supported by DARPA HR0011-12-C-0015 (BOLT), and Liang Huang is supportedby DARPA FA8750-13-2-0041 (DEFT), NSF IIS-1449278, and a Google Faculty Research Award.The views and findings in this paper are those of theauthors and are not endorsed by the DARPA.ReferencesXavier Carreras, Michael Collins, and Terry Koo.
2008.Tag, dynamic programming, and the perceptron for ef-ficient, feature-rich parsing.
In Proceedings of CoNLL2008.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL.Shay B. Cohen, Carlos G?omez-Rodr?
?guez, and GiorgioSatta.
2011.
Exact inference for generative probabilis-tic non-projective dependency parsing.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof ACL.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML, pages175?182.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos taggingand dependency parsing in chinese.
In IJCNLP.Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In Proceed-ings of ACL 2010.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proceed-ings of NAACL.Marco Kuhlmann, Carlos Gmez-Rodrguez, and GiorgioSatta.
2011.
Dynamic programming algorithms fortransition-based dependency parsers.
In Proceedingsof ACL.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-liang Chen, and Haizhou Li.
2011.
Joint models forchinese pos tagging and dependency parsing.
In Pro-ceedings of EMNLP, pages 1180?1191.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together.
Workshop atACL-2004, Barcelona.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.
InProceedings of EMNLP, pages 1?10.Kenji Sagae and Alon Lavie.
2006.
A best-first prob-abilistic shift-reduce parser.
In Proceedings of ACL(poster).1034Masaru Tomita.
1988.
Graph-structured stack and natu-ral language parsing.
In Proceedings of the 26th an-nual meeting on Association for Computational Lin-guistics, pages 249?257, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Zhiguo Wang and Nianwen Xue.
2014.
Joint pos tag-ging and transition-based constituent parsing in chi-nese with non-local features.
In Proceedings of ACL.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP.Kai Zhao and Liang Huang.
2013.
Minibatch and paral-lelization for online large margin structured learning.In Proceedings of NAACL 2013.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, andJingbo Zhu.
2013.
Fast and accurate shift-reduce con-stituent parsing.
In Proceedings of ACL 2013.1035
