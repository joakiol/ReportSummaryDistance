Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 290?297,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsEvaluating automatic extraction of rules for sentence plan constructionAmanda StentAT&T Labs ?
ResearchFlorham Park, NJ, USAstent@research.att.comMartin MolinaDepartment of Artificial IntelligenceUniversidad Polite?cnica de Madrid, Spainmartin.molina@upm.esAbstractThe freely available SPaRKy sentenceplanner uses hand-written weighted rulesfor sentence plan construction, and a user-or domain-specific second-stage ranker forsentence plan selection.
However, comingup with sentence plan construction rulesfor a new domain can be difficult.
In thispaper, we automatically extract sentenceplan construction rules from the RST-DTcorpus.
In our rules, we use only domain-independent features that are available to asentence planner at runtime.
We evaluatethese rules, and outline ways in which theycan be used for sentence planning.
Wehave integrated them into a revised versionof SPaRKy.1 IntroductionMost natural language generation (NLG) systemshave a pipeline architecture consisting of four corestages: content selection, discourse planning, sen-tence planning, and surface realization (Reiter andDale, 2000; Rambow et al, 2001).
A sentenceplanner maps from an input discourse plan to anoutput sentence plan.
As part of this process itperforms several tasks, including sentence order-ing, sentence aggregation, discourse cue insertionand perhaps referring expression generation (Stentet al, 2004; Walker et al, 2007; Williams and Re-iter, 2003).The developer of a sentence planner must typ-ically write rules by hand (e.g.
(Stent et al,2004; Walker et al, 2007)) or learn a domain-specific model from a corpus of training data (e.g.
(Williams and Reiter, 2003)).
Unfortunately, thereare very few corpora annotated with discourseplans, and it is hard to automatically label a cor-pus for discourse structure.
It is also hard tohand-write sentence planning rules starting froma ?blank slate?, as it were.In this paper, we outline a method for ex-tracting sentence plan construction rules from theonly publicly available corpus of discourse trees,the RST Discourse Treebank (RST-DT) (Carl-son et al, 2002).
These rules use only domain-independent information available to a sentenceplanner at run-time.
They have been integratedinto the freely-available SPaRKy sentence plan-ner.
They serve as a starting point for a user ofSPaRKy, who can add, remove or modify rules tofit a particular domain.We also describe a set of experiments in whichwe look at each sentence plan construction task inorder, evaluating our rules for that task in termsof coverage and discriminative power.
We discussthe implications of these experiments for sentenceplanning.The rest of this paper is structured as follows: InSection 2 we describe the sentence planning pro-cess using SPaRKy as an example.
In Sections 3through 5 we describe how we obtain sentenceplan construction rules.
In Section 6, we evalu-ate alternative rule sets.
In Section 7, we describeour modifications to the SPaRKy sentence plannerto use these rules.
In Section 8 we conclude andpresent future work.2 Sentence Planning in SPaRKyThe only publicly available sentence planner fordata-to-text generation is SPaRKy (Stent et al,2004).
SPaRKy takes as input a discourse plan (atree with rhetorical relations on the internal nodesand a proposition representing a text span on eachleaf), and outputs one or more sentence plans290(each a tree with discourse cues and/or punctua-tion on the internal nodes).
SPaRKy is a two-stagesentence planner.
First, possible sentence plansare constructed through a sequence of decisionsmade using only local information about singlenodes in the discourse plan.
Second, the possiblesentence plans are ranked using a user- or domain-specific sentence plan ranker that evaluates theglobal quality of each sentence plan (Walker et al,2007).Sentence plan construction in SPaRKy involvesthree tasks: span ordering, sentence aggregation(deciding whether to realize a pair of propositionsas a single clause, a single sentence, or two sen-tences), and discourse cue selection1.
SPaRKyuses a single set of hand-written weighted rulesto perform these tasks.
In the current distributedversion of SPaRKy, there are 20 rules covering9 discourse cues (and, because, but, however, onthe other hand, since, while, with, and the default,period).
Each rule operates on the children ofone rhetorical relation, and may impose an order-ing, insert punctuation or merge two propositions,and/or insert a discourse cue.
During sentenceplan construction, SPaRKy walks over the inputdiscourse plan, at each node finding all matchingrules and applying one which it selects probabilis-tically according to the rule weights (with somerandomness to permit variation).While the developer of a NLG system will al-ways have to adapt the sentence planner to his orher domain, it is often hard to come up with sen-tence planning rules ?from scratch?.
As a result ofthe work described here a SPaRKy user will havea solid foundation for sentence plan construction.3 DataWe use the Wall Street Journal Penn Treebankcorpus (Marcus et al, 1993), which is a corpusof text annotated for syntactic structure.
We alsouse two additional annotations done on (parts of)that corpus: PropBank (Kingsbury and Palmer,2003), which consists of annotations for predicate-argument structure; and the RST-DT (Carlsonet al, 2002), which consists of annotations forrhetorical structure.We had to process this data into a form suitablefor feature extraction.
First, we produced a flat-tened form of the syntactic annotations, in which1SPaRKy also does some referring expression generation,in a single pass over each completed sentence plan.each word was labeled with its part-of-speech tagand the path to the root of the parse tree.
Eachword was also assigned indices in the sentence (sowe could apply the PropBank annotations) and inthe document (so we could apply the RST-DT an-notations)2.Second, we attach to each word one or morelabels from the PropBank annotations (each labelconsists of a predicate index, and either a predicatename or a semantic role type and index).Third, we extract relation information from theRST-DT.
For each relation, we extract the rela-tion name, the types of each child (?Nucleus?
or?Satellite?
), and the start and end word indices foreach child.
Finally, we extract from the word-level annotations the marked-up words for eachtext span in each rhetorical relation.4 FeaturesFeatures are individual rule conditions.
In thestandard NLG pipeline, no information about therealized text is available to the sentence planner.However, existing sentence planners use lexicaland word sequence information to improve per-formance for a particular domain.
Williams andReiter (2003) appear to do surface realization be-fore sentence planning, while Walker et al (2007)perform surface realization between sentence planconstruction and sentence plan ranking.
We areconcerned with sentence plan construction only;also, we want to produce sentence plan construc-tion rules that are as domain-independent as pos-sible.
So we use no features that rely on havingrealized text.
However, we assume that the inputpropositions have been fairly well fleshed-out, sothat one has information about predicate-argumentstructure, tense, and the information status of enti-ties to be realized.A relation has a label as well as one or morechild text spans.
The features we extract from ourdata include both per-span and per-relation fea-tures.
In our experiments we use a subset of thesefeatures which is fairly domain-independent anddoes not overly partition our data.
The completeset of features (full) is as well as our reduced setare given in Table 1.2The Penn Treebank and the RST-DT segment words andpunctuation slightly differently, which makes it hard to alignthe various annotations.291Feature type Full feature set Reduced feature setPer-relation relation, relation is leaf, parent relation, span coref, com-bined verb type class, combined verb type, identifier ofshortest span, temporal order of spansrelation, relation is leaf, parent rela-tion, span coref, combined verb typeclass, identifier of shortest span, tem-poral order of spansPer-span, span identifier span identifier span identifierPer-span, span length number of NPs in spanPer-span, span verb verb type class, verb type, verb part of speech, verb isnegated, verb has modalPer-span, arguments argument status for ARG0 to ARG5 plus ARGM-{EXT,DIR, LOC, TMP, REC, PRD, ADV, MNR, CAU, PNC}Table 1: Features used in evaluation4.1 Per-Span FeaturesWe extract per-span features from basic spans(leaves of the RST tree) and from complex spans(internal nodes of the RST tree).
For each span wecompute: identifier, text, length, verb information,span argument information, discourse cue infor-mation, and span-final punctuation.Identifier We need a way to refer to the child spansin the rules.
For relations having only one childspan of each type (Satellite or Nucleus), we orderthe spans by type.
Otherwise, we order the spansalphabetically by span text.
The span identifier foreach child span is the index of the span in the re-sulting list.TextWe extract the text of the span, and the indicesof its first and last words in the Penn Treebank.
Weonly use this information during data extraction.However, in a system like that of Williams and Re-iter (Williams and Reiter, 2003), where sentenceplanning is done after or with surface realization,these features could be used.
They could also beused to train a sentence plan ranker for SPaRKyspecific to the news domain.Length We use the number of base NPs in the span(as we cannot rely on having the complete realiza-tion during sentence planning).VerbWe extract verb type, which can beN/A (thereis no labeled predicate for the span), stat (thespan?s main verb is a form of ?to be?
), a singlePropBank predicate (e.g.
create.01), or mixed (thespan contains more than one predicate).
We thenabstract to get the verb type class: N/A, pb (a Prop-Bank predicate), stat, or mixed.If the span contains a single predicate or multi-ple predicates all having the same part-of-speechtag, we extract that (as an indicator of tense).We also extract information about negation andmodals (using the PropBank tags ARGM-NEGand ARGM-MOD).Arguments We extract the text of the argumentsof the predicate(s) in the span: ARG0 to ARG5,as well as ARGM-{EXT, DIR, LOC, TMP, REC,PRD, ADV, MNR, CAU, PNC}.
We then abstractto get an approximation of information status.An argument status feature covers zero or moreinstantiations of the argument and can have thevalue N/A (no instantiations), proper (proper nounphrase(s)), pro (pronoun(s)), def (definite nounphrase(s)), indef (indefinite noun phrase(s)), quant(noun phrase (s) containing quantifiers), other (wecannot determine a value), or mixed (the argumentinstantiations are not all of the same type).Discourse Cues We extract discourse cue informa-tion from basic spans and from the first basic spanin complex spans.
We identify discourse cue(s)appearing at the start of the span, inside the span,and at the end of the span.
PropBank includesthe argument label ARGM-DIS for discourse cues;however, we adopt a more expansive notion of dis-course cue.
We say that a discourse cue can be ei-ther: any sequence of words all labeled ARGM-DIS and belonging to the same predicate, any-where in the span; or any cue from a (slightlyexpanded version of) the set of cues studied byMarcu (Marcu, 1997), if it appears at the start of aspan, at the end of a span, or immediately before orafter a comma, and if its lowest containing phrasetag is one of {ADJP, ADVP, CONJP, FRAG, NP-ADV, PP, UCP, SBAR, WH} or its part of speechtag is one of {CC, WDT}3.Punctuation We extract punctuation (N/A or .
or ?or !
or ; or : or ,) at the end of the span.3We constructed these rules by extracting from the WSJPenn Treebank all instances of the cues in Marcu?s list, andthen examining instances where the word sequence was notactually a discourse cue.
Some mistakes still occur in cueextraction.2924.2 Per-Relation FeaturesFor each relation we compute: name, the com-bined verb type and verb class of the child spans,whether any argument instantiations in the childspans are coreferential, and which child span isshortest (or the temporal order of the child spans).Relation, Parent Relation The core relation labelfor the relation and its parent relation (e.g.
attri-bution for attribution-e and attribution-n).Relation is Leaf True if child spans of the relationare leaf spans (not themselves relations).Combined Verb The shared verb for the relation:the child spans?
verb type if there is only one non-N/A verb type among the child spans; otherwise,mixed.
We then abstract from the shared verb typeto the shared verb type class.Span Coreference We use the information Prop-Bank gives about intra-sentential coreference.
Wedo not employ any algorithm or annotation to iden-tify inter-sentential coreference.Shortest Span The identifier of the child span withthe fewest base NPs.Temporal Order of Spans For some relations (e.g.sequence, temporal-before, temporal-after), thetemporal order is very important.
For these rela-tions we note the temporal order of the child spansrather than the shortest span.5 Rule ExtractionEach rule we extract consists of a set of per-relation and per-span features (the conditions), anda pattern (the effects).
The conditions containeither: the relation only, features from the re-duced feature set, or features from the full fea-ture set.
The pattern can be an ordering of childspans, a set of between-span punctuation markers,a set of discourse cues, or an ordering of childspans mixed with punctuation markers and dis-course cues.
Each extracted rule is stored as XML.We only extract rules for relations having two ormore children.
We also exclude RST-DT?s spanand same-unit relations because they are not im-portant for our task.
Finally, because the accu-racy of low-level (just above the span) rhetoricalrelation annotation is greater than that of high-level relation annotation, we extract rules fromtwo data sets: one only containing first-level re-lations (those whose children are all basic spans),and one containing all relations regardless of levelin the RST tree.
The output from the rule ex-traction process is six alternative rule sets for eachConcession rule:conditions:type child=?0?
: nucleus, type child=?1?
: satellite, shortest: 0,isCoref: 0, isLeaf: 1, isSamePredClass: mixed,numChildren: 2, relation: concession, parentRel: antithesiseffects:order: 1 0, punc child=?1?
: comma, cues child=?1?
: whileexample:(1) While some automotive programs have been delayed,(0) they have n?t been canceledSequence rule:conditions:type child=?0?
: nucleus, type child=?1?
: nucleus,type child=?2?
: nucleus, type child=?3?
: nucleus,isCoref: 1, isLeaf: 1, isSamePredClass: mixed,numChildren: 4, relation: sequence, parentRel: circumstance,temporalOrder: 0 1 2 3effects:order: 0 1 2 3, punc child=?0?
: comma, punc child=?1?
:comma, punc child=?2?
: n/a, cues child=?3?
: andexample:(0) when you can get pension fund money, (1) buy a portfolio,(2) sell off pieces off it (3) and play your own gamePurpose rule:conditions:type child=?0?
: nucleus, type child=?1?
: satellite, shortest: 0,isCoref: 0, isLeaf: 0, isSamePredClass: shared,numChildren: 2, relation: purpose, parentRel: listeffects:order: 0 1, punc child=?0?
: n/a, cues child=?1?
: soexample:(0) In a modern system the government ?s role is to give thepeople as much choice as possible(1) so they are capable of making a choiceFigure 1: Glosses of extracted sentence planningrules for three relations (reduced feature set)sentence plan construction task: first-level or alldata, with either the relation condition alone, thereduced feature set, or the full feature set.The maximum number of patterns we couldhave is 7680 per relation, if we limit ourselvesto condition sets, relation instances with only twochild spans, and a maximum of one discoursecue to each span (two possible orderings for childspans * four possible choices for punctuation *480 choices for discourse cue on each span).
Bycontrast, for our all data set there are 5810 uniquerules conditioned on the reduced feature set (109.6per relation) and 292 conditioned on just the rela-tion (5.5 per relation).
Example rules are given inFigure 1.
Even though the data constrains sentenceplanning choices considerably, we still have manyrules (most differing only in discourse cues).2936 Rule Evaluation6.1 On EvaluationThere are two basic approaches to NLG, text-to-text generation (in which a model learned from atext corpus is applied to produce new texts fromtext input) and data-to-text generation (in whichnon-text input is converted into text output).
Intext-to-text generation, there has been consider-able work on sentence fusion and information or-dering, which are partly sentence planning tasks.For evaluation, researchers typically compare au-tomatically produced text to the original human-produced text, which is assumed to be ?correct?(e.g.
(Karamanis, 2007; Barzilay and McKeown,2005; Marsi and Krahmer, 2005)).
However, anevaluation that considers the only ?correct?
an-swer for a sentence planning task to be the an-swer in the original text is overly harsh.
First, al-though we assume that all the possibilities in thehuman-produced text are ?reasonable?, some maybe awkward or incorrect for particular domains,while other less frequent ones in the newspaperdomain may be more ?correct?
in another domain.Our purpose is to lay out sentence plan construc-tion possibilities, not to reproduce the WSJ au-thorial voice.
Second, because SPaRKy is a two-stage sentence planner and we are focusing hereon sentence plan construction, we can only evalu-ate the local decisions made during that stage, notthe overall quality of SPaRKy?s output.Evaluations of sentence planning tasks for data-to-text generation have tended to focus solelyon discourse cues (e.g.
(Eugenio et al, 1997;Grote and Stede, 1998; Moser and Moore, 1995;Nakatsu, 2008; Taboada, 2006)).
By contrast, wewant good coverage for all core sentence planningtasks.
Although Walker et al performed an eval-uation of SPaRKy (Stent et al, 2004; Walker etal., 2007), they evaluated the output from the sen-tence planner as a whole, rather than evaluatingeach stage separately.
Williams and Reiter, in thework most similar to ours, examined a subset ofthe RST-DT corpus to see if they could use it toperform span ordering, punctuation selection, anddiscourse cue selection and placement.
However,they assumed that surface realization was alreadycomplete, so they used lexical features.
Their sen-tence planner is not publicly available.In the following sections, we evaluate the infor-mation in our sentence plan construction rules interms of coverage and discriminative power.
Thefirst type of evaluation allows us to assess the de-gree to which our rules are general and providesystem developers with an adequate number ofchoices for sentence planning.
The second typeof evaluation allows us to evaluate whether our re-duced feature set helps us choose from the avail-able possibilities better than a feature set consist-ing simply of the relation (i.e.
is the complicatedfeature extraction necessary).
Because we includethe full feature set in this evaluation, it can alsobe seen as a text-to-text generation type of evalua-tion for readers who would like to use the sentenceplanning rules for news-style text generation.6.2 CoverageIn our evaluation of coverage, we count the num-ber of relations, discourse cues, and patterns wehave obtained, and compare against other data setsdescribed in the research literature.6.2.1 Relation CoverageThere are 57 unique core relation labels inthe RST-DT.
We exclude span and same-unit.Two others, elaboration-process-step and topic-comment, never occur with two or more childspans.
Our first-level and all rules cover all ofthe remaining 53.
The most frequently occurringrelations are elaboration-additional, list, attribu-tion, elaboration-object-attribute, contrast, cir-cumstance and explanation-argumentative.By contrast, the current version of SPaRKy cov-ers only 4 relations (justify, contrast, sequence,and infer)4.Mann and Thompson originally defined 24 re-lations (Mann and Thompson, 1987), while HovyandMaier listed about 70 (Hovy andMaier, 1992).6.2.2 Discourse Cue CoverageOur first-level rules cover 92 discourse cues,and our all rules cover 205 discourse cues.
Themost commonly occurring discourse cues in bothcases are and, but, that, when, as, who and which.By contrast, the current version of SPaRKy cov-ers only about 9 discourse cues.In his dissertation Marcu identified about 478discourse cues.
We used a modified version ofMarcu?s cue list to extract discourse cues from ourcorpus, but some of Marcu?s discourse cues do notoccur in the RST-DT.4Curiously, only two of these relations (contrast and se-quence) appear in the RST-DT data (although infer may beequivalent to span).2946.2.3 Sentence Plan Pattern CoverageFor the first-level data we have 140 unique sen-tence plan patterns using the relation conditionalone, and 1767 conditioning on the reduced fea-ture set.
For the all data we have 292 unique pat-terns with relation condition alone and 5810 withthe reduced feature set.
Most patterns differ onlyin choice of discourse cue(s).No system developer will want to examine all5810 rules.
However, she or he may wish to lookat the patterns for a particular relation.
In our useof SPaRKy, for example, we have extended thepatterns for the sequence relation by hand to covertemporal sequences of up to seven steps.6.3 Discriminative PowerIn this evaluation, we train decision tree classifiersfor each sentence plan construction task.
We ex-periment with both the first-level and all data setsand with both the reduced and full feature sets.For each experiment we perform ten-fold cross-validation using the J48 decision tree implemen-tation provided in Weka (Witten and Eibe, 2005)with its default parameters.
We also report perfor-mance for a model that selects a pattern condition-ing only on the relation.
Finally, we report perfor-mance of a baseline which always selects the mostfrequent pattern.We evaluate using 1-best classification accu-racy, by comparing with the choice made in thePenn Treebank for that task.
We test for signifi-cant differences between methods using Cochran?sQ, followed by post-hoc McNemar tests if signif-icant differences existed.
We also report the fea-tures with information gain greater than 0.1.6.3.1 Span OrderingWe have one input feature vector for each rela-tion instance that has two children5.
In the featurevector, child spans are ordered by their identifiers,and the pattern is either 0 1 (first child, then sec-ond child) or 1 0 (second child, then first child).Classification accuracy for all methods is re-ported in Table 2.
All methods perform signifi-cantly better than baseline (p < .001), and boththe reduced and full feature sets give results sig-nificantly better than using the relation alone (p <.001).
The full feature set performs significantly5The number of relation instances with three or more childspans is less than 2% of the data.
Removing these relationsmade it feasible for us to train classifiers without crashingWeka.First-level AllBaseline 71.8144 71.4356Per-relation 84.2707 82.3894Reduced 89.6092 90.3147Full 90.2129 91.9666Table 2: Span ordering classification accuracy.For first-level data, n = 3147.
For all data, n =10170.
Labels = {0 1, 1 0}.First-level AllBaseline 74.5154 50.4425Per-relation 74.5154 64.2773Reduced 77.8201 72.1731Full 74.3883 66.1357Table 3: Between-span punctuation classificationaccuracy.
For first-level data, n = 3147.
For alldata, n = 10170.
Labels = {semicolon, comma,full, N/A}.better than the reduced feature set for the all dataset (p < .001), but not for the first-level data set.Most of the relations have a strong preferencefor one ordering or the other.
Most mistakes aremade on those that don?t (e.g.
attribution, list).6.3.2 Punctuation InsertionWe have one input feature vector for each re-lation instance that has two children.
We assumethat span ordering is performed prior to punctu-ation insertion, so the child spans are ordered asthey appear in the data.
The pattern is the punc-tuation mark that should appear between the twochild spans (one of N/A or comma or semicolonor full6), which indicates whether the two childrenshould be realized as separate sentences, as sepa-rate clauses, or merged.Classification accuracy for all methods is re-ported in Table 3.
For the all data set, all meth-ods perform significantly better than baseline (p <.001), and both the reduced and full feature setsgive results significantly better than using the re-lation alone (p < .001).
Furthermore, the re-duced feature set performs significantly better thanthe full feature set (p < .001).
By contrast, forthe first-level data set, the reduced feature set per-forms significantly better than all the other datasets, while there are no statistically significant dif-ferences in performance between the baseline, per-relation and full feature sets.The most common type of error was misclas-sifying comma, semicolon or full as N/A: for the6full indicates a sentence boundary (.
or ?
or !
).295First-level AllBaseline 62.6629 68.4267Per-relation 68.605 70.1377Reduced 73.6257 73.9135Full 74.3565 74.5919Table 4: Discourse cue classification accuracy.For first-level data, n = 3147 and no.
labels = 92.For all data, n = 10170 and no.
labels = 203.first-level data this is what the models trained onthe per-relation and full feature sets do most of thetime.
The second most common type of error wasmisclassifying comma, semicolon or N/A as full.6.3.3 Discourse cue selectionWe have one input feature vector for each re-lation instance having two children.
We use thesame features as in the previous experiment, andas in the previous experiment, we order the childspans as they appear in the data.
The pattern is thefirst discourse cue appearing in the ordered childspans7.Classification accuracy for all methods is re-ported in Table 4.
All methods perform signifi-cantly better than baseline (p < .001), and boththe reduced and full feature sets give results sig-nificantly better than using the relation alone (p <.001).
The performance differences between thereduced and full feature sets are not statisticallysignificant for either data set.For this task, 44 of the 92 labels in the first-leveldata, and 97 of the 203 labels in the all data, oc-curred only once.
These cues were typically misla-beled.
Commonly occurring labels were typicallylabeled correctly.6.4 DiscussionOur methods for rule extraction are not general inthe sense that they rely on having access to particu-lar types of annotation which are not widely avail-able nor readily obtainable by automatic means.However, our extracted rules have quite broad cov-erage and will give NLG system developers a jumpstart when using and adapting SPaRKy.Our reduced feature set compares favorably indiscriminative power to both our full feature setand the per-relation feature set.
It achieves a very7Some relations have multiple cues, either independentcues such as but and also, or cues that depend on each othersuch as on the one hand and on the other hand.
Using allcues is infeasible, and there are too few span-internal andspan-final cues to break up the cue classification for this eval-uation.good fit to the input data for the span ordering taskand a good fit to the input data for the punctua-tion and discourse cue insertion tasks, especiallyfor the first-level data set.
Factors affect perfor-mance include: the punctuation insertion data ishighly imbalanced (by far the most common labelis N/A), while for the discourse cue insertion taskthere is a problem of data sparsity.7 Revised SPaRKyOne way to use these results would be to model thesentence planning task as a cascade of classifiers,but this method does not permit the system devel-oper to add his or her own rules.
So we continue touse SPaRKy, which is rule-based.
We have madeseveral changes to the Java version of SPaRKy tosupport application of our sentence plan construc-tion rules.
We modified the classes for storing andmanaging rules to read our XML rule format andprocess rule conditions and patterns.
We strippedout the dependence on RealPro and added hooksfor SimpleNLG (Gatt and Reiter, 2009).
We modi-fied the rule application algorithm so that users canchoose to use a single rule set with patterns cov-ering all three sentence planning tasks, or one ruleset for each sentence planning task.
Also, sincethere are now many rules, we give the user theoption to specify which relations jSPaRKy shouldload rules for at each run.Information about the revised jSparky, in-cluding how to obtain it, is available athttp://www.research.att.com/?stent/sparky2.0/or by contacting the first author.8 Conclusions and Future WorkIn this paper we described how we extractedless domain-dependent sentence plan constructionrules from the RST-DT corpus.
We presented eval-uations of our extracted rule sets and describedhow we integrated them into the freely-availableSPaRKy sentence planner.In future work, we will experiment with dis-course cue clustering.
We are also looking at alter-native ways of doing sentence planning that permita tighter interleaving of sentence planning and sur-face realization for improved efficiency and outputquality.296ReferencesR.
Barzilay and K. McKeown.
2005.
Sentence fusionfor multidocument news summarization.
Computa-tional Linguistics, 31(3):297?328.L.
Carlson, D. Marcu, and M. E. Okurowski.
2002.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
In Proceedingsof the SIGdial workshop on discourse and dialogue.B.
Di Eugenio, J. D. Moore, and M. Paolucci.
1997.Learning features that predict cue usage.
In Pro-ceedings of the EACL.A.
Gatt and E. Reiter.
2009.
SimpleNLG: A realisationengine for practical applications.
In Proceedings ofthe European Workshop on Natural Language Gen-eration.B.
Grote and M. Stede.
1998.
Discourse marker choicein sentence planning.
In Proceedings of the 9th In-ternational Workshop on Natural Language Gener-ation.E.
Hovy and E. Maier.
1992.
Parsimo-nious or profligate: how many and which dis-course structure relations?
Available fromhttp://handle.dtic.mil/100.2/ADA278715.N.
Karamanis.
2007.
Supplementing entity coherencewith local rhetorical relations for information order-ing.
Journal of Logic, Language and Information,16(4):445?464.P.
Kingsbury and M. Palmer.
2003.
PropBank: thenext level of TreeBank.
In Proceedings of the Work-shop on Treebanks and Lexical Theories.B.
Lavoi and O. Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proceedingsof ANLP.W.
Mann and S. Thompson.
1987.
Rhetorical structuretheory: A theory of text organization.
Technical Re-port ISI/RS-87-190, Information Sciences Institute,Los Angeles, CA.D.
Marcu.
1997.
The rhetorical parsing, summa-rization, and generation of natural language texts.Ph.D.
thesis, Department of Computer Science, Uni-versity of Toronto.M.
Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.E.
Marsi and E. Krahmer.
2005.
Explorations in sen-tence fusion.
In Proceedings of the European Work-shop on Natural Language Generation.M.
Moser and J. D. Moore.
1995.
Using discourseanalysis and automatic text generation to study dis-course cue usage.
In Proceedings of the AAAI 1995Spring Symposium on Empirical Methods in Dis-course Interpretation and Generation.C.
Nakatsu.
2008.
Learning contrastive connective insentence realization ranking.
In Proceedings of SIG-dial 2008.O.
Rambow, S. Bangalore, and M. A. Walker.
2001.Natural language generation in dialog systems.
InProceedings of HLT.E.
Reiter and R. Dale.
2000.
Building natural lan-guage generation systems.
Cambridge UniversityPress, Cambridge, UK.A.
Stent, R. Prasad, andM.
A.Walker.
2004.
Trainablesentence planning for complex information presen-tations in spoken dialog systems.
In Proceedings ofthe ACL.M.
Taboada.
2006.
Discourse markers as signals (ornot) of rhetorical relations.
Journal of Pragmatics,38(4):567?592.M.
A. Walker, A. Stent, F. Mairesse, and R. Prasad.2007.
Individual and domain adaptation in sentenceplanning for dialogue.
Journal of Artificial Intelli-gence Research, 30:413?456.S.
Williams and E. Reiter.
2003.
A corpus analysis ofdiscourse relations for natural language generation.In Proceedings of Corpus Linguistics.I.
Witten and F. Eibe.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.297
