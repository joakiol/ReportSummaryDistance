Identifying Parallel Documents from a Large Bilingual Collection of Texts:Application to Parallel Article Extraction in Wikipedia.Alexandre PatryKeaText845, Boulevard Dcarie, bureau 202Saint-Laurent, Canada H4L 3L7alexandre.patry@keatext.comPhilippe LanglaisDIRO/RALIUniversite?
de Montre?alMontre?al, Canada H3C3J7felipe@iro.umontreal.caAbstractWhile several recent works on dealing withlarge bilingual collections of texts, e.g.
(Smithet al, 2010), seek for extracting parallel sen-tences from comparable corpora, we presentPARADOCS, a system designed to recognizepairs of parallel documents in a (large) bilin-gual collection of texts.
We show that thissystem outperforms a fair baseline (Enrightand Kondrak, 2007) in a number of con-trolled tasks.
We applied it on the French-English cross-language linked article pairs ofWikipedia in order see whether parallel ar-ticles in this resource are available, and ifour system is able to locate them.
Accord-ing to some manual evaluation we conducted,a fourth of the article pairs in Wikipedia areindeed in translation relation, and PARADOCSidentifies parallel or noisy parallel article pairswith a precision of 80%.1 IntroductionThere is a growing interest within the MachineTranslation (MT) community to investigate compa-rable corpora.
The idea that they are available ina much larger quantity certainly contributes to fos-ter this interest.
Still, parallel corpora are playinga crucial role in MT.
This is therefore not surprisingthat the number of bitexts available to the commu-nity is increasing.Callison-Burch et al (2009) mined from institu-tional websites the 109 word parallel corpus1 whichgathers 22 million pairs of (likely parallel) French-English sentences.
Tiedemann (2009) created the1http://www.statmt.org/wmt10Opus corpus,2 an open source parallel corpus gath-ering texts of various sources, in several languagespairs.
This is an ongoing effort currently gatheringmore than 13 Gigabytes of compressed files.
TheEuroparl corpus3 (Koehn, 2005) gathers no less than2 Gigabytes of compressed documents in 20 lan-guage pairs.
Some other bitexts are more marginal innature.
For instance, the novel 1984 of George Or-wel has been organized into an English-Norvegianbitext (Erjavec, 2004) and Beyaz Kale of Orhan Pa-muk as well as Sofies Verden of Jostein Gaardnerare available for the Swedish-Turk language pair(Megyesi et al, 2006).A growing number of studies investigate the ex-traction of near parallel material (mostly sentences)from comparable data.
Among them, Munteanu etal.
(2004) demonstrate that a classifier can be trainedto recognize parallel sentences in comparable cor-pora mined from news collections.
A number ofrelated studies (see section 5) have also been pro-posed; some of them seeking to extract parallel sen-tences from cross-language linked article pairs inWikipedia4 (Adafre and de Rijke, 2006; Smithet al, 2010).
None of these studies addresses specif-ically the issue of discovering parallel pairs of arti-cles in Wikipedia.In this paper, we describe PARADOCS, a systemcapable of mining parallel documents in a collec-tion, based on lightweight content-based features ex-tracted from the documents.
On the contrary to othersystems designed to target parallel corpora (Chen2http://opus.lingfil.uu.se/3http://www.statmt.org/europarl/4http://fr.wikipedia.org/87Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 87?95,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsand Nie, 2000; Resnik and Smith, 2003), we donot assume any specific naming conventions on file-names or URLs.The reminder of this article is organized as fol-lows.
In the next section, we describe our ap-proach to mining parallel documents in a bilingualcollection of texts.
We test our approach on theEuroparl corpus in section 3.
We present in sec-tion 4 the application of our system to a subpartof the French-English articles of Wikipedia.
Wedescribe related work in section 5, summarize ourwork in section 6 and present future works in sec-tion 7.2 PARADOCSIn order to identify pairs of parallel documents in abilingual collection of texts, we designed a system,named PARADOCS, which is making as few assump-tions as possible on the language pair being consid-ered, while still making use of the content of the doc-uments in the collection.
Our system is built on threelightweight components.
The first one searches fortarget documents that are more likely parallel to agiven source document (section 2.1).
The secondcomponent classifies (candidate) pairs of documentsas parallel or not (section 2.2).
The third componentis designed to filter out some (wrongly) recognizedparallel pairs, making use of collection-level infor-mation (section 2.3).2.1 Searching Candidate PairsIn a collection containing n documents in a givenlanguage, and m in another one, scoring each of then?m potential pairs of source-target documents be-comes rapidly intractable.
In our approach, we re-sort to an information retrieval system in order toselect the target documents that are most likely par-allel to a given source one.
In order to do so, weindex target documents t in the collection thanks toan indexing strategy ?
that will be described shortly.Then, for a source document s, we first index it, thatis, we compute ?
(s), and query the retrieval enginewith ?
(s), which in turn returns the N most simi-lar target documents found in the collection.
In ourexperiments, we used the Lucene5 retrieval library.5http://lucene.apache.orgWe tested two indexing strategies: one reduces adocument to the sequence of hapax words it contains(?
?hap), the other one reduces it to its sequenceof numerical entities (?
?num).
Hapax words havebeen found very useful in identifying parallel pairsof documents (Enright and Kondrak, 2007) as wellas for word-aligning bitexts (Lardilleux and Lep-age, 2007).
Following Enright and Kondrak (2007),we define hapax words as blank separated strings ofmore than 4 characters that appear only once in thedocument being indexed.
Also, we define a numer-ical entity as a blank separated form containing atleast one digit.
It is clear from this description thatour indexing strategies can easily be applied to manydifferent languages.2.2 Identifying candidate pairsEach candidate pair delivered by Lucene, is classi-fied as parallel or not by a classifier trained in a su-pervised way to recognize parallel documents.
Hereagain, we want our classifier to be as agnostic aspossible to the pair of languages considered.
Thisis why we adopted very light feature extractors ?which are built on three types of entities in docu-ments: numerical entities (?
?num), hapax words(?
?hap) and punctuation marks6 (?
?punc).
Foreach sequence of entities ?
(s) and ?
(t) of a sourcedocument s and a target document t respectively, wecompute the three following features:?
the normalized edit-distance between the tworepresentations:?
= ed(?
(s), ?(t))/max(|?
(s)|, |?
(t)|)where |?
(d)| stands for the size of the sequenceof entities contained in d. Intuitively, ?
givesthe proportion of entities shared across docu-ments,?
the total number of entities in the representationof both documents:|?
(s)|+ |?
(t)|We thought this information might complementthe one of ?
which is relative to the document?ssequence length.6We only considered the 6 following punctuation marks thatare often preserved in translation: .!?():88?
A binary feature which fires whenever the pairof documents considered receives the smalleredit-distance among all the pairs of documentsinvolving this source document:?
(s, t) ={ 1 if ed (?
(s), ?
(t)) ?
ed (?
(s), ?(t?))
?
t?0 otherwiseIntuitively, the target document considered ismore likely the good one if it has with thesource document the smallest edit distance.Since we do compute edit-distance for all thecandidate documents pairs, this feature comesat no extra computational cost.We compute these three features for each se-quence of entities considered.
For instance, if werepresent a document according to its sequence ofnumerical entities and its hapax words, we do com-pute a total of 6 features.7It is fair to say that our feature extraction strat-egy is very light.
In particular, it does not capitalizeon an existing bilingual lexicon.
Preliminary exper-iments with features making use of such a lexiconturned out to be less successful, due to issues in thecoverage of the lexicon (Patry and Langlais, 2005).To create and put to the test our classifier, we usedthe free software package Weka (Hall et al, 2009),written in Java.8 This package allows the easy ex-perimentation of numerous families of classifiers.We investigated logistic regression (logit), naivebayes models (bayes), adaboost (ada), as well asdecision tree learning (j48).2.3 Post-treatmentsThe classifiers we trained label each pair of docu-ments independently of other candidate pairs.
Thisindependence assumption is obviously odd and leadsto situations where several target documents arepaired to a given source document and vice-versa.Several solutions can be applied; we considered twosimple ones in this work.
The first one, hereafternamed nop, consists in doing nothing; thereforeleaving potential duplicates source or target docu-ments.
The second solution, called dup, filters out7We tried with less success to compute a single set of fea-tures from a representation considering all entities.8www.cs.waikato.ac.nz/ml/weka/pairs sharing documents.
Another solution we didnot implement would require to keep from the set ofpairs concerning a given source document the onewith the best score as computed by our classifier.
Weleave this as future work.3 Controlled ExperimentsWe checked the good behavior of PARADOCSin a controlled experimental setting, using theEuroparl corpus.
This corpus is organized intobitexts, which means that we have a ground truthagainst which we can evaluate our system.3.1 CorpusWe downloaded version 5 of the Europarl cor-pus.9 Approximatively 6 000 documents are avail-able in 11 languages (including English), that is, wehave 6 000 bitexts in 10 language pairs where En-glish is one of the languages.
The average numberof sentences per document is 273.
Some documentscontain problems (encoding problems, files endingunexpectedly, etc.).
We did not try to cope with this.In order to measure how sensible our approach isto the size of the documents, we considered severalslices of them (from 10 to 1000 sentences).
103.2 ProtocolWe tested several experimental conditions, varyingthe language pairs considered (en-da, -de, -el, -es,-fi, -fr, -it, -nl, -pt and -sv) as well as the doc-ument length (10, 20, 30, 50, 70, 100 and 1 000sentences).
We also tested several system configu-rations, varying the indexing strategy (num, hap),the entities used for representing documents (hap,num, num+hap, num+punc), the classifier used(logit, ada, bayes, and j48), as well as thepost-filtering strategy (nop, dup).
This means thatwe conducted no less than 4 480 experiments.Because we know which documents are paral-lel, we can compute precision (percentage of iden-tified parallel pairs that are truly parallel) and recall(percentage of true parallel pairs identified) for eachconfiguration.9http://www.statmt.org/europarl10We removed the first sentences of each document, sincethey may contain titles or other information that may artificiallyease pairing.89Since our approach requires to train a classifier,we resorted in this experiment to a 5-fold cross-validation procedure where we trained our classifierson 4/5 of the corpus and tested on the remaining part.The figures reported in the reminder of this sectionare averaged over the 5 folds.
Also, all configura-tions tested in this section considered the N = 20most similar target documents returned by the re-trieval engine for each source document.3.3 Results3.3.1 Search errorsWe first measured search errors observed duringstep 1 of our system.
There are actually two typesof errors: one when no document is returned byLucene (nodoc) and one when none of the targetdocuments returned by the retrieval engine are sanc-tioned ones (nogood).
Figure 1 shows both errortypes for the Dutch-English language pair, as a func-tion of the document length.11 Clearly, search errorsare more important when documents are short.
Ap-proximatively a tenth of the source documents of (atmost) 100 sentences do not receive by Lucene anytarget document.
For smaller documents, this hap-pens for as much as a third of the documents.
Also,it is interesting to note that in approximatively 6% ofthe cases where Lucene returns target documents,the good one is not present.
Obviously we pay theprize of our lightweight indexation scheme.
In or-der to increase the recall of our system, nodoc er-rors could be treated by employing an indexing strat-egy which would use more complex features, suchas sufficiently rare words (possibly involving a key-word test, e.g.
tf.idf).
This is left as future work.3.3.2 Best System configurationIn order to determine the factors which influencethe most our system, we varied the language pairs(10 values) and the length of the documents (7 val-ues) and counted the number of times a given sys-tem configuration obtained the best f-measure overthe 70 tests we conducted.
We observed that mostof the time, the configurations recording the bestf-measure are those that exploit numerical entities(both at indexing time and feature extraction time).Actually, we observed that computing features on11Similar figures have been observed for other languagepairs.nb.
of sent.errors %10152025303540458  16  32  64  128  256  512  1024nodoc + nogoodnodocFigure 1: Percentage of Dutch documents for whichLucene returns no English document (nodoc), or nocorrect document (nodoc+nogood) as a function of thedocument size counted in sentences.hapax words or punctuation marks on top of nu-merical entities do not help much.
One possibleexplanation is that often, and especially within theEuroparl corpus, hapax words correspond to nu-merical entities.
Also, we noted that frequently, thewining configuration is the one embedding a logisticregression classifier, tightly followed by the decisiontree learner.3.3.3 Sensitivity to the language pairWe also tested the sensibility of our approach tothe language pair being considered.
Apart from thefact that the French-English pair was the easiest todeal with, we did not notice strong differences inperformance among language pairs.
For documentsof at most 100 sentences, the worst f-measure (0.93)is observed for the Dutch/English language pair,while the best one (0.95) is observed for the French-English pair.
Slightly larger differences were mea-sured for short documents.nb.
of sent.gain %05101520253035408  16  32  64  128  256  512  1024dadeelesfifritnlptsvFigure 2: Absolute gains of the best variant of our sys-tem over the approach described by Enright and Kon-drak (2007).903.3.4 Sanity checkWe conducted a last sanity check by comparingour approach to the one of (Enright and Kondrak,2007).
This approach simply ranks the candidatepairs in decreasing order of the number of hapaxwords they share.
The absolute gains of our ap-proach over theirs are reported in Figure 2, as afunction of the document length and the languagepair considered.
Our system systematically outper-forms the hapax approach of (Enright and Kondrak,2007) regardless of the length of the documents andthe language pairs considered.
An average absolutegain of 13.6% in f-measure is observed for long doc-uments, while much larger gains are observed forshorter ones.
It has to be noted, that our approachrequires to train a classifier, which makes it poten-tially less useful in some situations.
Also, we usedthe best of our system in this comparison.4 Experiments with WikipediaMany articles in Wikipedia are available inseveral languages.
Often, they are explicitlymarked as linked across languages.
For instance,the English article [Text corpus] is linked to theFrench one [Corpus], but they are not transla-tion of each other, while the English article [De-cline of the Roman Empire] and the French one[De?clin de l?empire romain d?Occident] are paral-lel.124.1 ResourceDuring summer 2009, we collected all French-English cross-language linked articles fromWikipedia.
A very straightforward pre-processing stage involving simple regular expres-sions removed part of the markup specific to thisresource.
We ended up with 537 067 articles ineach language.
The average length of the Englishpages is 711 words, while the average for French is445 words.
The difference in length among linkedarticles has been studied by Filatova (2009) on asmall excerpt of bibliographical articles describing48 persons listed in the biography generation task(Task 5) of DUC 2004.1312At least they were at the time of redaction.13http://duc.nist.gov/duc2004/tasks.html/4.2 Parallelness of cross-language linkedarticle pairs in FR-EN Wikipedia.In this experiment, we wanted to measure the pro-portion of cross-language linked article pairs inWikipedia that are in translation relation.
In or-der to do so, we manually evaluated 200 pairs of arti-cles in our French-English Wikipedia repository.A web interface was developed in order to anno-tate each pair, following the distinction introducedby Fung and Cheung (2004): parallel indicatessentence-aligned texts that are in translation relation;noisy characterizes two documents that are never-theless mostly bilingual translations of each other;topic corresponds to documents which share sim-ilar topics, but that are not translation of each oth-ers and very-non that stands for rather unrelatedtexts.The results of the manual evaluation are reportedin the left column of table 1.
We observe that afourth of the pairs of articles are indeed parallel ornoisy parallel.
This figure quantifies the observa-tion made by Adafre and de Rijke (2006) that whilesome articles in Wikipedia tend to be translationsof each other, the majority of the articles tend to bewritten independently of each other.
To the best ofour knowledge, this is the first time someone is mea-suring the degree of parallelness of Wikipedia atthe article level.If our sample is representative (something whichdeserves further investigations), it means that morethan 134 000 pairs of documents in the French-English Wikipedia are parallel or noisy parallel.We would like to stress that, while conductingthe manual annotation, we frequently found diffi-cult to label pairs of articles with the classes pro-posed by Fung and Cheung (2004).
Often, we couldspot a few sentences translated in pairs that we ratedvery-non or topic.
Also, it was hard to be con-sistent over the annotation session with the distinc-tion made between those two classes.
Many arti-cles are divided into sub-topics, some of which be-ing covered in the other article, some being not.4.3 Parallelness of the article pairs identifiedby PARADOCSWe applied PARADOCS to our Wikipedia collec-tion.
We indexed the French pages with the Lucene91Wikipedia PARADOCSType Count Ratio Count Ratiovery-non 92 46% 5 2.5%topic 58 29% 34 17%noisy 22 11% 39 19.5%parallel 28 14% 122 61%Total 200 200Table 1: Manual analysis of 200 pairs cross-languagelinked in Wikipedia (left) and 200 pairs of articlesjudged parallel by our system (right).toolkit using the num indexing scheme.
Each En-glish article was consequently transformed with thesame strategy before querying Lucene, which wasasked to return the N = 5 most similar French arti-cles.
We limited the retrieval to 5 documents in thisexperiment in order to reduce computation time.
Asa matter of fact, running our system on Wikipediatook 1.5 days of computation on 8 nodes of a pen-tium cluster.
Most of this time was devoted to com-pute edit-distance features.Each candidate pair of articles was then labeledas parallel or not by a classifier we trained to rec-ognize parallel documents in an in-house collectionof French-English documents we gathered in 2009from a website dedicated to Olympic games.14 Us-ing a classifier trained on a different task gives us theopportunity to see how our system would do if usedout-of-the-box.
A set of 1844 pairs of documentshave been automatically aligned (at the documentlevel) thanks to heuristics on URL names; then man-ually checked for parallelness.
The best classifierwe developed on this collection (thanks to a 5-foldcross-validation procedure) was a decision tree clas-sifier (j48) which achieves an average f-measure of90% (92.7% precision, and 87.4% recall).
This isthe classifier we used in this experiment.From the 537 067 English documents of our col-lection, 106 896 (20%) did not receive any answerfrom Lucene (nodoc).
A total of 117 032 pairs ofdocuments were judged by the classifier as parallel.The post-filtering stage (dup) eliminated slightlyless than half of them, leaving us with a total of14http://www.olympic.org61 897 pairs.
We finally eliminated those pairs thatwere not cross-language linked in Wikipedia.
Weended up with a set of 44 447 pairs of articles iden-tified as parallel by our system.Since there is no reference telling us which cross-language linked articles in Wikipedia are indeedparallel, we resorted to a manual inspection of a ran-dom excerpt of 200 pairs of articles identified as par-allel by our system.
The sampling was done in a waythat reflects the distribution of the scores of the clas-sifier over the pairs of articles identified as parallelby our system.The results of this evaluation are reported in theright column of table 1.
First, we observe that20% (2.5+17) of the pairs identified as parallel byour system are at best topic aligned.
One explana-tion for this is that topic aligned articles often sharenumbers (such as dates), sometimes in the same or-der, especially in bibliographies that are frequent inWikipedia.
Clearly, we are paying the prize ofa lightweight content-oriented system.
Second, weobserve that 61% of the annotated pairs were indeedparallel, and that roughly 80% of them were parallelor noisy parallel.
Although PARADOCS is not as ac-curate as it was on the Europarl corpus, it is stillperforming much better than random.4.4 Further analysisWe scored the manually annotated cross-languagelinked pairs described in section 4.2 with our clas-sifier.
The cumulative distribution of the scores isreported in table 2.
We observe that 64% (100-35.7%) of the parallel pairs are indeed rated as par-allel (p ?
0.5) by our classifier.
This percentage ismuch lower for the other types of article pairs.
Onthe contrary, for very non-parallel pairs, the classi-p ?
0.1 p ?
0.2 p < 0.5 avr.very-non 1.1% 91.4% 92.5% 0.25topic 1.7% 74.6% 78.0% 0.37noisy 13.6% 77.3% 90.9% 0.26parallel 7.1% 25.0% 35.7% 0.71Table 2: Cumulative distribution and average score givenby our classifier to the 200 manually annotated pairs ofarticles cross-language linked in Wikipedia.92fier assigns a score lower than 0.2 in more than 91%of the cases.
This shows that the score given by theclassifier correlates to some extent with the degreeof parallelness of the article pairs.Among the 28 pairs of cross-language linked arti-cle pairs manually labelled as parallel (see table 1),only 2 pairs were found parallel by PARADOCS,even if 18 of them received a score of 1 by the classi-fier.
This discrepancy is explained in part by the fil-ter (dup) which is too drastic since it removes all thepairs sharing one document.
We already discussedalternative strategies.
The retrieval stage of our sys-tem is as well responsible of some failures, espe-cially since we considered the 5 first French docu-ments returned by Lucene.
We further inspectedthe 10 (28-18) pairs judged parallel but scored byour classifier as non parallel.
We observed sev-eral problems; the most frequent one being a fail-ure of our pre-processing step which leaves unde-sired blocs of text in one of the article, but not inthe other (recall we kept the preprocessing very ag-nostic to the specificities of Wikipedia).
Theseblocs might be infoboxes or lists recapitulating im-portant dates, or even sometimes HTML markup.The presence of numerical entities in those blocs isconfounding the classifier.5 Related WorkPairing parallel documents in a bilingual collectionof texts has been investigated by several authors.Most of the previous approaches for tackling thisproblem capitalize on naming conventions (on fileURL names) for pairing documents.
This is for in-stance the case of PTMINER (Chen and Nie, 2000)and STRAND (Resnik and Smith, 2003), two sys-tems that are intended to mine parallel documentsover the Web.
Since heuristics on URL names doesnot ensure parallelness, other cues, such as the ratioof the length of the documents paired or their HTMLstructure, are further being used.
Others have pro-posed to use features computed after sentence align-ing a candidate pair of documents (Shi et al, 2006),a very time consuming strategy (that we tried with-out success).
Others have tried to use bilingual lex-icons in order to compare document pairs; this isfor instance the case of the BITS system (Ma andLiberman, 1999).
Also, Enright and Kondrak (2007)propose a very lightweight content-based approachto pairing documents, capitalizing on the number ofhapax words they share.
We show in this study, thatthis approach can easily be outperformed.Zhao and Vogel (2002) were among the first toreport experiments on harvesting comparable newscollections in order to extract parallel sentences.With a similar goal, Munteanu et al (2004) pro-posed to train in a supervised way (using some par-allel data) a classifier designed to recognize paral-lel sentences.
They applied their classifier on twomonolingual news corpora in Arabic and English,covering similar periods, and showed that the paral-lel material extracted, when added to an in-domainparallel training corpus of United Nation texts, im-proved significantly an Arabic-to-English SMT sys-tem tested on news data.
Still, they noted that theextracted material does not come close to the qual-ity obtained by adding a small out-domain parallelcorpus to the in-domain training material.
Differentvariants of this approach have been tried afterwards,e.g.
(Abdul-Rauf and Schwenk, 2009).To the best of our knowledge, Adafre and de Rijke(2006) where the first to look at the problem of ex-tracting parallel sentences from Wikipedia.
Theycompared two approaches for doing so that bothsearch for parallel sentence pairs in cross-languagelinked articles.
The first one uses an MT engine inorder to translate sentences of one document into thelanguage of the other article; then parallel sentencesare selected based on a monolingual similarity mea-sure.
The second approach represents each sentenceof a pair of documents in a space of hyperlink an-chored texts.
An initial lexicon is collected from thetitle of the articles that are linked across languages(they also used the Wikipedia?s redirect featureto extend the lexicon with synonyms).
This lexiconis used for representing sentences in both languages.Whenever the anchor text of two hyperlinks, one ina source sentence, and one in a target sentence issanctioned by the lexicon, the ID of the lexicon en-try is used to represent each hyperlink, thus makingsentences across languages sharing some representa-tion.
They concluded that the latter approach returnsfewer incorrect pairs than the MT based approach.Smith et al (2010) extended these previous linesof work in several directions.
First, by training aglobal classifier which is able to capture the ten-93dency of parallel sentences to appear in chunks.
Sec-ond, by applying it at large on Wikipedia.
Intheir work, they extracted a large number of sen-tences identified as parallel from linked pairs of arti-cles.
They show that this extra materiel, when addedto the training set, improves a state-of-the-art SMTsystem on out-domain test sets, especially when thein-domain training set is not very large.The four aforementioned studies implement someheuristics in order to limit the extraction of paral-lel sentences to some fruitful document pairs.
Fornews collections, the publication time can for in-stance be used for narrowing down the search; whilefor Wikipedia articles, the authors concentrateon document pairs that are linked across languages.PARADOCS could be used for narrowing the searchspace down to a set of parallel or closely paralleldocument pairs.
We see several ways this couldhelp the process of extracting parallel fragments.For one thing, we know that extracting parallelsentences from a parallel corpus is something wedo well, while extracting parallel sentences from acomparable corpus is a much riskier enterprise (noteven mentioning time issues).
As a matter of fact,Munteanu et al (2004) mentioned the inherent noisepresent in pairs of sentences extracted from com-parable corpora as a reason why a large set of ex-tracted sentence pairs does not contribute to improvean SMT system more that a small but highly specificparallel dataset.
Therefore, a system like ours couldbe used to decide which sort of alignment techniqueshould be used, given a pair of documents.
For an-other thing, one could use our system to delimit aset of fruitful documents to harvest in the first place.The material acquired this way could then be usedto train models that could be employed for extract-ing noisiest document pairs, hopefully for the sakeof the quality of the material extracted.6 ConclusionWe have described a system for identifying paral-lel documents in a bilingual collection.
This systemdoes not presume specific information, such as file(or URL) naming conventions, which can sometimebe useful for mining parallel documents.
Also, oursystem relies on a very lightweight set of content-based features (basically numerical entities and pos-sibly hapax words), therefore our claim of a lan-guage neutral system.We conducted a number of experiments on theEuroparl corpus in order to control the impactof some of its hyper-parameters.
We show thatour approach outperforms the fair baseline describedin (Enright and Kondrak, 2007).
We also con-ducted experiments in extracting parallel documentsin Wikipedia.
We were satisfied by the fact thatwe used a classifier trained on another task in thisexperiment, but still got good results (a precision of80% if we consider noisy parallel document pairsas acceptable).
We conducted a manual evalua-tion of some cross-language linked article pairs andfound that 25% of those pairs were indeed paral-lel or noisy parallel.
This manually annotated datathat can be downloaded at http://www.iro.umontreal.ca/?felipe/bucc11/.7 Future WorkIn their study on infobox arbitrage, Adar et al(2009) noted that currently, cross-language links inWikipedia are essentially made by volunteers,which explains why many such links are missing.Our approach lends itself to locate missing linksin Wikipedia.
Another extension of this line ofwork, admittedly more prospective, would be to de-tect recent vandalizations (modifications or exten-sions) operated on one language only of a parallelpair of documents.Also, we think that there are other kinds of dataon which our system could be invaluable.
This isthe reason why we refrained in this work to engi-neer features tailored for a specific data collection,such as Wikipedia.
One application of our sys-tem we can think of, is the organization of (pro-prietary) translation memories.
As a matter if fact,many companies do not organize the flow of the doc-uments they handle in a systematic way and there isa need for tools able to spot texts that are in transla-tion relation.AcknowledgmentsWe are grateful to Fabienne Venant who participatedin the manual annotation we conducted in this study.94ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
Onthe use of comparable corpora to improve smt per-formance.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, EACL ?09, pages 16?23.Sisay Fissaha Adafre and Maarten de Rijke.
2006.
Find-ing Similar Sentences across Multiple Languages inWikipedia.
In 11th EACL, pages 62?69, Trento, Italy.Eytan Adar, Michael Skinner, and Daniel S. Weld.
2009.Information arbitrage across multi-lingual wikipedia.In Proceedings of the Second ACM International Con-ference on Web Search and Data Mining, WSDM ?09,pages 94?103.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Jiang Chen and Jian-Yun Nie.
2000.
Parallel Web textmining for cross-language IR.
In RIAO, pages 62?67,Paris, France.Jessica Enright and Gregorz Kondrak.
2007.
A FastMethod for Parallel Document Identification.
InNAACL HLT 2007, Companion Volume, pages 29?32,Rochester, NY.Tomaz Erjavec.
2004.
MULTEXT-East Version 3:Multilingual Morphosyntactic Specifications, Lexi-cons and Corpora.
In LREC, Lisbon, Portugal.Elena Filatova.
2009.
Directions for exploiting asymme-tries in multilingual wikipedia.
In Third InternationalCross Lingual Information Access Workshop, pages30?37, Boulder, Colorado.Pascale Fung and Percy Cheung.
2004.
Mining very-non-parallel corpora: Parallel sentence and lexicon ex-traction via bootstrapping and EM.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 57?63, Barcelona, Spain, July.
Association forComputational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11, Issue 1(10?18).Philipp Koehn.
2005.
Europarl: A multilingual corpusfor evaluation of machine translation.
In 10th MachineTranslation Summit, Phuket, Thailand, sep.Adrien Lardilleux and Yves Lepage.
2007.
The con-tribution of the notion of hapax legomena to wordalignment.
In 3rd Language & Technology Conference(LTC?07), pages 458?462, Poznan?
Poland.Xiaoyi Ma and Mark Liberman.
1999.
Bits: A methodfor bilingual text search over the web.
In MachineTranslation Summit VII, Singapore, sep.Beata Bandmann Megyesi, Eva Csato Johansson, andAnna Sgvall Hein.
2006.
Building a Swedish-TurkishParallel Corpus.
In LREC, Genoa, Italy.Dragos Stefan Munteanu, Alexander Fraser, and DanielMarcu.
2004.
Improved machine translation perfor-mance via parallel sentence extraction from compara-ble corpora.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 265?272, Boston, Massachusetts, USA,May.
Association for Computational Linguistics.Alexandre Patry and Philippe Langlais.
2005.
Automaticidentification of parallel documents with light or with-out linguistic resources.
In 18th Annual Conference onArtificial Intelligence (Canadian AI), pages 354?365,Victoria, British-Columbia, Canada.Philip Resnik and Noah A. Smith.
2003.
The web as aparallel corpus.
Computational Linguistics, 29:349?380.
Special Issue on the Web as a Corpus.Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A dom tree alignment model for mining par-allel data from the web.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics (COLING) and the 44th annual meeting of the As-sociation for Computational Linguistics (ACL), pages489?496, Sydney, Australia.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from comparablecorpora using document level alignment.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the NAACL, HLT ?10, pages 403?411.Jo?rg Tiedemann.
2009.
News from OPUS ?
A Collectionof Multilingual Parallel Corpora with Tools and Inter-faces.
In N. Nicolov, K. Bontcheva, G. Angelova, andR.
Mitkov, editors, Recent Advances in Natural Lan-guage Processing, pages 237?248.
John Benjamins,Amsterdam/Philadelphia.Bing Zhao and Stephan Vogel.
2002.
Adaptive parallelsentences mining from web bilingual news collection.In Proceedings of the 2002 IEEE International Con-ference on Data Mining, ICDM ?02, pages 745?748,Maebashi City, Japan.95
