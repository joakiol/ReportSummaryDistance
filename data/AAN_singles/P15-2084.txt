Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511?517,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDependency Recurrent Neural Language Models for Sentence CompletionPiotr MirowskiGoogle DeepMindpiotr.mirowski@computer.orgAndreas VlachosUniversity College Londona.vlachos@cs.ucl.ac.ukAbstractRecent work on language modelling hasshifted focus from count-based models toneural models.
In these works, the wordsin each sentence are always considered ina left-to-right order.
In this paper we showhow we can improve the performance ofthe recurrent neural network (RNN) lan-guage model by incorporating the syntac-tic dependencies of a sentence, which havethe effect of bringing relevant contextscloser to the word being predicted.
Weevaluate our approach on the MicrosoftResearch Sentence Completion Challengeand show that the dependency RNN pro-posed improves over the RNN by about10 points in accuracy.
Furthermore, weachieve results comparable with the state-of-the-art models on this task.1 IntroductionLanguage Models (LM) are commonly used toscore a sequence of tokens according to its prob-ability of occurring in natural language.
They arean essential building block in a variety of applica-tions such as machine translation, speech recogni-tion and grammatical error correction.
The stan-dard way of evaluating a language model has beento calculate its perplexity on a large corpus.
How-ever, this evaluation assumes the output of the lan-guage model to be probabilistic and it has beenobserved that perplexity does not always correlatewith the downstream task performance.For these reasons, Zweig and Burges (2012)proposed the Sentence Completion Challenge, inwhich the task is to pick the correct word to com-plete a sentence out of five candidates.
Perfor-mance is evaluated by accuracy (how many sen-tences were completed correctly), thus both prob-abilistic and non-probabilistic models (e.g.
Roarket al.
(2007)) can be compared.
Recent approachesfor this task include both neural and count-basedlanguage models (Zweig et al., 2012; Gubbinsand Vlachos, 2013; Mnih and Kavukcuoglu, 2013;Mikolov et al., 2013).Most neural language models consider the to-kens in a sentence in the order they appear, andthe hidden state representation of the networkis typically reset at the beginning of each sen-tence.
In this work we propose a novel neu-ral language model that learns a recurrent neu-ral network (RNN) (Mikolov et al., 2010) ontop of the syntactic dependency parse of a sen-tence.
Syntactic dependencies bring relevant con-texts closer to the word being predicted, thus en-hancing performance as shown by Gubbins andVlachos (2013) for count-based language models.Our Dependency RNN model is published simul-taneously with another model, introduced in Tai etal.
(2015), who extend the Long-Short Term Mem-ory (LSTM) architecture to tree-structured net-work topologies and evaluate it at sentence-levelsentiment classification and semantic relatednesstasks, but not as a language model.Adapting the RNN to use the syntactic depen-dency structure required to reset and run the net-work on all the paths in the dependency parse treeof a given sentence, while maintaining a count ofhow often each token appears in those paths.
Fur-thermore, we explain how we can incorporate thedependency labels as features.Our results show that the dependency RNN lan-guage model proposed outperforms the RNN pro-posed by Mikolov et al.
(2011) by about 10 pointsin accuracy.
Furthermore, it improves upon thecount-based dependency language model of Gub-bins and Vlachos (2013), while achieving slightlyworse than the recent state-of-the-art results byMnih and Kavukcuoglu (2013).
Finally, we makethe code and preprocessed data available to facili-tate comparisons with future work.5112 Dependency Recurrent NeuralNetworkCount-based language models operate by assign-ing probabilities to sentences by factorizing theirlikelihood into n-grams.
Neural language mod-els further embed each word w(t) into a low-dimensional vector representation (denoted bys(t))1.
These word representations are learned asthe language model is trained (Bengio et al., 2003)and enable to define a word in relation to otherwords in a metric space.Recurrent Neural Network Mikolov et al.
(2010) suggested the use of Recurrent Neural Net-works (RNN) to model long-range dependenciesbetween words as they are not restricted to a fixedcontext length, like the feedforward neural net-work (Bengio et al., 2003).
The hidden representa-tion s(t) for the word in position t of the sentencein the RNN follows a first order auto-regressivedynamic (Eq.
1), where W is the matrix connect-ing the hidden representation of the previous words(t?
1) to the current one, w(t) is the one-hot in-dex of the current word (in a vocabulary of size Nwords) and U is the matrix containing the embed-dings for all the words in the vocabulary:s(t) = f (Ws(t?
1) +Uw(t)) (1)The nonlinearity f is typically the logistic sigmoidfunction f(x) =11+exp(?x).
At each time step, theRNN generates the word probability vector y(t)for the next word w(t+ 1), using the output wordembedding matrix V and the softmax nonlinearityg(xi) =exp(xi)?iexp(xi):y(t) = g (Vs(t)) (2)RNNwithMaximum EntropyModel Mikolovet al.
(2011) combined RNNs with a maximum en-tropy model, essentially adding a matrix that di-rectly connects the input words?
n-gram contextw(t ?
n + 1, .
.
.
, t) to the output word proba-bilities.
In practice, because of the large vocab-ulary size N , designing such a matrix is computa-tionally prohibitive.
Instead, a hash-based imple-mentation is used, where the word context is fedthrough a hash function h that computes the in-dex h(w(t ?
n + 1, .
.
.
, t)) of the context words1In our notation, we make a distinction between the wordtoken w(t) at position t in the sentence and its one-hot vectorrepresentation w(t).
We note withe i-th word token on abreadth-first traversal of a dependency parse tree.in a one-dimensional array d of size D (typically,D = 109).
Array d is trained in the same way asthe rest of the RNN model and contributes to theoutput word probabilities:y(t) = g(Vs(t) + dh(w(t?n+1,...,t)))(3)As we show in our experiments, this additionalmatrix is crucial to a good performance on wordcompletion tasks.Training RNNs RNNs are trained using maxi-mum likelihood through gradient-based optimiza-tion, such as Stochastic Gradient Descent (SGD)with an annealed learning rate ?.
The Back-Propagation Through Time (BPTT) variant ofSGD enables to sum-up gradients from consecu-tive time steps before updating the parameters ofthe RNN and to handle the long-range temporaldependencies in the hidden s and output y se-quences.
The loss function is the cross-entropybetween the generated word distribution y(t) andthe target one-hot word distribution w(t+ 1), andinvolves the log-likelihood terms log yw(t+1)(t).For speed-up, the estimation of the output wordprobabilities is done using hierarchical softmaxoutputs, i.e., class-based factorization (Mikolovand Zweig, 2012).
Each word wiis assigned toa class ciand the corresponding log-likelihood iseffectively log ywi(t) = log yci(t) + log ywj(t),where j is the index of word wiamong wordsbelonging to class ci.
In our experiments, webinned the words found in our training corpus into250 classes according to frequency, roughly corre-sponding to the square root of the vocabulary size.Dependency RNN RNNs are designed to pro-cess sequential data by iteratively presenting themwith word w(t) and generating next word?s proba-bility distribution y(t) at each time step.
They canbe reset at the beginning of a sentence by settingall the values of hidden vector s(t) to zero.Dependency parsing (Nivre, 2005) generates,for each sentence (which we note {w(t)}Tt=0), aparse tree with a single root, many leaves and anunique path (also called unroll) from the root toeach leaf, as illustrated on Figure 1.
We now note{wi}ithe set of word tokens appearing in the parsetree of a sentence.
The order in the notation de-rives from the breadth-first traversal of that tree(i.e., the root word is noted w0).
Each of the un-rolls can be seen as a different sequence of words512ROOT I saw the ship with very strong binocularsnsubjprepdobjdeppobjadvmodamodROOTFigure 1: Example dependency tree{wi}, starting from the single rootw0, that are vis-ited when one takes a specific path on the parsetree.
We propose a simple transformation to theRNN algorithm so that it can process dependencyparse trees.
The RNN is reset and independentlyrun on each such unroll.
As detailed in the nextparagraph, when evaluating the log-probability ofthe sentence, a word token wican appear in mul-tiple unrolls but its log-likelihood is counted onlyonce.
During training, and to avoid over-trainingthe network on word tokens that appear in morethan one unroll (words near the root appear inmore unrolls than those nearer the leaves), eachword token wiis given a weight discount ?i=1ni,based on the number niof unrolls the token ap-pears in.
Since the RNN is optimized using SGDand updated at every time-step, the contribution ofword token wican be discounted by multiplyingthe learning rate by the discount factor: ?i?.Sentence Probability in Dependency RNNGiven a word wi, let us define the ancestor se-quence A(wi) to be the subsequence of words,taken as a subset from {wk}i?1k=0and describingthe path from the root node w0to the parent of wi.For example, in Figure 1, the ancestors A(very)of word token very are saw, binoculars andstrong.
Assuming that each word wiis con-ditionally independent of the words outside ofits ancestor sequence, given its ancestor sequenceA(wi), Gubbins and Vlachos (2013) showed thatthe probability of a sentence (i.e., the probabilityof a lexicalized tree STgiven an unlexicalized treeT ) could be written as:P [ST|T ] =|S|?i=1P [wi|A(wi)] (4)This means that the conditional likelihood of aword given its ancestors needs to be counted onlyonce in the calculation of the sentence likelihood,even though each word can appear in multiple un-rolls.
When modeling a sentence using an RNN,the state sjthat is used to generate the distributionof words wi(where j is the parent of i in the tree),represents the vector embedding of the history ofthe ancestor words A(wi).
Therefore, we countthe term P [wi|sj] only once when computing thelikelihood of the sentence.3 Labelled Dependency RNNThe model presented so far does not usedependency labels.
For this purpose weadapted the context-dependent RNN (Mikolov andZweig, 2012) to handle them as additional M -dimensional label input features f(t).
These fea-tures require a matrix F that connects label fea-tures to word vectors, thus yielding a new dynam-ical model (Eq.
5) in the RNN, and a matrix Gthat connects label features to output word proba-bilities.
The full model becomes as follows:s(t) = f (Ws(t?
1) +Uw(t) + Ff(t))(5)y(t) = g(Vs(t) +Gf(t) + dh(wtt?n+1))(6)On our training dataset, the dependency parsingmodel found M = 44 distinct labels (e.g., nsubj,det or prep).
At each time step t, the context wordw(t) is associated a single dependency label f(t)(a one-hot vector of dimension M ).Let G(w) be the sequence of grammatical rela-tions (dependency tree labels) between successiveelements of (A(w), w).
The factorization of thesentence likelihood from Eq.
4 becomes:P [ST|T ] =|S|?i=1P [wi|A(wi), G(wi)] (7)4 Implementation and DatasetWe modified the Feature-Augmented RNNtoolkit2and adapted it to handle tree-structureddata.
Specifically, and instead of being run se-quentially on the entire training corpus, the RNNis run on all the word tokens in all unrolls of allthe sentences in all the books of the corpus.
TheRNN is reset at the beginning of each unroll of asentence.
When calculating the log-probability ofa sentence, the contribution of each word tokenis counted only once (and stored in a hash-tablespecific for that sentence).
Once all the unrollsof a sentence are processed, the log-probabilityof the sentence is the sum of the per-token log-probabilities in that hash-table.
We also further2http://research.microsoft.com/en-us/projects/rnn/513enhanced the RNN library by replacing somelarge matrix multiplication routines by calls to theCBLAS library, thus yielding a two- to three-foldspeed-up in the test and training time.3The training corpus consists of 522 19th cen-tury novels from Project Gutenberg (Zweig andBurges, 2012).
All processing (sentence-splitting,PoS tagging, syntactic parsing) was performed us-ing the Stanford CoreNLP toolkit (Manning et al.,2014).
The test set contains 1040 sentences to becompleted.
Each sentence consists of one groundtruth and 4 impostor sentences where a specificword has been replaced with a syntactically cor-rect but semantically incorrect impostor word.
De-pendency trees are generated for each sentencecandidate.
We split that set into two, using the first520 sentences in the validation (development) setand the latter 520 sentences in the test set.
Dur-ing training, we start annealing the learning rate ?with decay factor 0.66 as soon as the classificationerror on the validation set starts to increase.5 ResultsTable 1 shows the accuracy (validation and testsets) obtained using a simple RNN with 50, 100,200 and 300-dimensional hidden word represen-tation and 250 frequency-based word classes (vo-cabulary size N = 72846 words appearing at least5 times in the training corpus).
One notices thatadding the direct word context to target word con-nections (using the additional matrix described insection 2), enables to jump from a poor perfor-mance of about 30% accuracy to about 40% testaccuracy, essentially matching the 39% accuracyreported for Good-Turing n-gram language mod-els in Zweig et al.
(2012).
Modelling 4-gramsyields even better results, closer to the 45% accu-racy reported for RNNs in (Zweig et al., 2012).4As Table 2 shows, dependency RNNs (de-pRNN) enable about 10 point word accuracy im-provement over sequential RNNs.The best accuracy achieved by the depRNN onthe combined development and test sets used to re-port results in previous work was 53.5%.
The bestreported results in the MSR sentence completionchallenge have been achieved by Log-BiLinearModels (LBLs) (Mnih and Hinton, 2007), a vari-3Our code and our preprocessed datasets are avail-able from: https://github.com/piotrmirowski/DependencyTreeRnn4The paper did not provide details on the maximum en-tropy features or on class-based hierarchical softmax).Architecture 50h 100h 200h 300hRNN (dev) 29.6 30.0 30.0 30.6RNN (test) 28.1 30.0 30.4 28.5RNN+2g (dev) 29.6 28.7 29.4 29.8RNN+2g (test) 29.6 28.7 28.1 30.2RNN+3g (dev) 39.2 39.4 38.8 36.5RNN+3g (test) 40.8 40.6 40.2 39.8RNN+4g (dev) 40.2 40.6 40.0 40.2RNN+4g (test) 42.3 41.2 40.4 39.2Table 1: Accuracy of sequential RNN on the MSRSentence Completion Challenge.Architecture 50h 100h 200hdepRNN+3g (dev) 53.3 54.2 54.2depRNN+3g (test) 51.9 52.7 51.9ldepRNN+3g (dev) 48.8 51.5 49.0ldepRNN+3g (test) 44.8 45.4 47.7depRNN+4g (dev) 52.7 54.0 52.7depRNN+4g (test) 48.9 51.3 50.8ldepRNN+4g (dev) 49.4 50.0 (48.5)ldepRNN+4g (test) 47.7 51.4 (47.7)Table 2: Accuracy of (un-)labeled dependencyRNN (depRNN and ldepRNN respectively).ant of neural language models with 54.7% to55.5% accuracy (Mnih and Teh, 2012; Mnih andKavukcuoglu, 2013).
We conjecture that their su-perior performance might stem from the fact thatLBLs, just like n-grams, take into account the or-der of the words in the context and can thus modelhigher-order Markovian dynamics than the simplefirst-order autoregressive dynamics in RNNs.
ThedepRNN proposed ignores the left-to-right wordorder, thus it is likely that a combination of theseapproaches will result in even higher accuracies.Gubbins and Vlachos (2013) developed a count-based dependency language model achieving 50%accuracy.
Finally, Mikolov et al.
(2013) report thatthey achieved 55.4% accuracy with an ensemble ofRNNs, without giving any other details.6 DiscussionRelated work Mirowski et al.
(2010) incorpo-rated syntactic information into neural languagemodels using PoS tags as additional input to LBLsbut obtained only a small reduction of the worderror rate in a speech recognition task.
Similarly,Bian et al.
(2014) enriched the Continuous Bag-of-514Words (CBOW) model of Mikolov et al.
(2013)by incorporating morphology, PoS tags and en-tity categories into 600-dimensional word embed-dings trained on the Gutenberg dataset, increas-ing sentence completion accuracy from 41% to44%.
Other work on incorporating syntax into lan-guage modeling include Chelba et al.
(1997) andPauls and Klein (2012), however none of these ap-proaches considered neural language models, onlycount-based ones.
Levy and Goldberg (2014) andZhao et al.
(2014) proposed to train neural wordembeddings using skip-grams and CBOWs on de-pendency parse trees, but did not extend their ap-proach to actual language models such as LBL andRNN and did not evaluate the word embeddingson word completion tasks.Note that we assume that the dependency treeis supplied prior to running the RNN which limitsthe scope of the Dependency RNN to the scoringof complete sentences, not to next word prediction(unless a dependency tree parse for the sentenceto be generated is provided).
Nevertheless, it iscommon in speech recognition and machine trans-lation to use a conventional decoder to produce anN-best list of the most likely candidate sentencesand then re-score them with the language model.
(Chelba et al., 1997; Pauls and Klein, 2011)Tai et al.
(2015) propose a similar approach toours, learning Long Short-Term Memory (LSTM)(Hochreiter and Schmidhuber, 1997; Graves,2012) RNNs on dependency parse tree networktopologies.
Their architectures is not designed topredict next-word probability distributions, as ina language model, but to classify the input words(sentiment analysis task) or to measure the sim-ilarity in hidden representations (semantic relat-edness task).
Their relative improvement in per-formance (tree LSTMs vs standard LSTMs) onthese two tasks is smaller than ours, probably be-cause the LSTMs are better than RNNs at storinglong-term dependencies and thus do not benefitform the word ordering from dependency trees asmuch as RNNs.
In a similar vein to ours, Miceli-Barone and Attardi (2015) simply propose to en-hance RNN-based machine translation by permut-ing the order of the words in the source sentence tomatch the order of the words in the target sentence,using a source-side dependency parsing.Limitations of RNNs for word completionZweig et al.
(2012) reported that RNNs achievelower perplexity than n-grams but do not alwaysFigure 2: Perplexity vs. accuracy of RNNsoutperform them on word completion tasks.
Asillustrated in Fig.
2, the validation set perplex-ity (comprising all 5 choices for each sentence)of the RNN keeps decreasing monotonically (oncewe start annealing the learning rate), whereas thevalidation accuracy rapidly reaches a plateau andoscillates.
Our observation confirms that, once anRNN went through a few training epochs, changein perplexity is no longer a good predictor ofchange in word accuracy.
We presume that thelog-likelihood of word distribution is not a train-ing objective crafted for precision@1, and thatfurther perplexity reduction happens in the middleand tail of the word distribution.7 ConclusionsIn this paper we proposed a novel language model,dependency RNN, which incorporates syntacticdependencies into the RNN formulation.
We eval-uated its performance on the MSR sentence com-pletion task and showed that it improves overRNN by 10 points in accuracy, while achieving re-sults comparable with the state-of-the-art.
Furtherwork will include extending the dependency treelanguage modeling to Long Short-Term MemoryRNNs to handle longer syntactic dependencies.AcknowledgementsWe thank our anonymous reviewers for theirvaluable feedback.
PM also thanks GeoffreyZweig, Daniel Voinea, Francesco Nidito and Da-vide di Gennaro for sharing the original Feature-Augmented RNN toolkit on the Microsoft Re-search website and for insights about that code, aswell as Bhaskar Mitra, Milad Shokouhi and An-driy Mnih for enlighting discussions about wordembedding and sentence completion.515ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014.Knowledge-powered deep learning for word embed-ding.
In Machine Learning and Knowledge Discov-ery in Databases, Lecture Notes in Computer Sci-ence, volume 8724, pages 132?148.Ciprian Chelba, David Engle, Frederick Jelinek, VictorJimenez, Sanjeev Khudanpur, Lidia Mangu, HarryPrintz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-cke, et al.
1997.
Structure and performance of adependency language model.
In Proceedings of Eu-rospeech, volume 5, pages 2775?2778.Alex Graves.
2012.
Supervised Sequence Labellingwith Recurrent Neural Networks.
Studies in Com-putational Intelligence.
Springer.Joseph Gubbins and Andreas Vlachos.
2013.
Depen-dency language models for sentence completion.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing.Sepp Hochreiter and Jurgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9:17351780.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages302?308.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Antonio Valerio Miceli-Barone and Giuseppe At-tardi.
2015.
Non-projective dependency-based pre-reordering with recurrent neural network for ma-chine translation.
In The 53rd Annual Meeting ofthe Association for Computational Linguistics andThe 7th International Joint Conference of the AsianFederation of Natural Language Processing.Tomas Mikolov and Geoff Zweig.
2012.
Context de-pendent recurrent neural network language model.In Speech Language Technologies (SLT), 2012 IEEEWorkshop on.
IEEE.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH 2010, 11th Annual Conference of theInternational Speech Communication Association,Makuhari, Chiba, Japan, September 26-30, 2010,pages 1045?1048.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernocky.
2011.
Strategies fortraining large scale neural network language mod-els.
In Automatic Speech Recognition and Under-standing (ASRU), 2011 IEEE Workshop on, pages196?201.
IEEE.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Piotr Mirowski, Sumit Chopra, Suhrid Balakrishnan,and Srinivas Bangalore.
2010.
Feature-rich continu-ous language models for speech recognition.
In Spo-ken Language Technology Workshop (SLT), 2010IEEE, pages 241?246.
IEEE.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th International Conferenceon Machine Learning, page 641648.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 2265?2273.
Curran Associates, Inc.Andriy Mnih and Yee W Teh.
2012.
A fast and simplealgorithm for training neural probabilistic languagemodels.
In Proceedings of the 29th InternationalConference on Machine Learning (ICML-12), pages1751?1758.Joakim Nivre.
2005.
Dependency grammar and de-pendency parsing.
MSI report, 5133(1959):1?32.Adam Pauls and Dan Klein.
2011.
Faster and SmallerN-Gram Language Models.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 258?267.
Association for ComputationalLinguistics.Adam Pauls and Dan Klein.
2012.
Large-scale syntac-tic language modeling with treelets.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Long Papers-Volume 1,pages 959?968.
Association for Computational Lin-guistics.Brian Roark, Murat Saraclar, and Michael Collins.2007.
Discriminative n-gram language modeling.Computer Speech & Language, 21(2):373 ?
392.Kai Sheng Tai, Richard Socher, and Christopher Man-ning.
2015.
Improved semantic representationsfrom tree-structured long short-term memory net-works.
In The 53rd Annual Meeting of the Asso-ciation for Computational Linguistics and The 7thInternational Joint Conference of the Asian Federa-tion of Natural Language Processing.516Yinggong Zhao, Shujian Huang, Xinyu Dai, JianbingZhang, and Jiajun Chen.
2014.
Learning word em-beddings from dependency relations.
In In Proceed-ings of Asian Language Processing (IALP).Geoffrey Zweig and Christopher J. C. Burges.
2012.A challenge set for advancing language modeling.In Proceedings of the NAACL-HLT 2012 Workshop:Will We Ever Really Replace the N-gram Model?
Onthe Future of Language Modeling for HLT, pages29?36.
Association for Computational Linguistics.Geoffrey Zweig, John C Platt, Christopher Meek,Christopher J. C. Burges, Ainur Yessenalina, andQiang Liu.
2012.
Computational approaches to sen-tence completion.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, pages 601?610.517
