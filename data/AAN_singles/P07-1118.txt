Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 936?943,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMuch ado about nothing:A social network model of Russian paradigmatic gapsRobert Daland Andrea D. Sims Janet PierrehumbertDepartment of LinguisticsNorthwestern University2016 Sheridan RoadEvanston, IL 60208 USAr-daland, andrea-sims, jbp@northwestern.eduAbstractA number of Russian verbs lack 1sg non-past forms.
These paradigmatic gaps arepuzzling because they seemingly contradictthe highly productive nature of inflectionalsystems.
We model the persistence andspread of Russian gaps via a multi-agentmodel with Bayesian learning.
We ranthree simulations: no grammar learning,learning with arbitrary analogical pressure,and morphophonologically conditionedlearning.
We compare the results to theattested historical development of the gaps.Contradicting previous accounts, wepropose that the persistence of gaps can beexplained in the absence of synchroniccompetition between forms.1 IntroductionParadigmatic gaps present an interesting challengefor theories of inflectional structure and languagelearning.
Wug tests, analogical change andchildren?s overextensions of regular patternsdemonstrate that inflectional morphology is highlyproductive.
Yet lemmas sometimes have ?missing?inflected forms.
For example, in Russian themajority of verbs have first person singular (1sg)non-past forms (e.g., posadit?
?to plant?, posa?u ?Iwill plant?
), but no 1sg form for a number ofsimilar verbs (e.g., pobedit?
?to win?, *pobe?u ?Iwill win?).
The challenge lies in explaining thisapparent contradiction.
Given the highly produc-tive nature of inflection, why do paradigmatic gapsarise?
Why do they persist?One approach explains paradigmatic gaps as aproblem in generating an acceptable form.
Underthis hypothesis, gaps result from irreconcilableconflict between two or more inflectional patterns.For example, Albright (2003) presents an analysisof Spanish verbal gaps based on the MinimalGeneralization Learner (Albright and Hayes 2002).In his account, competition between mid-voweldiphthongization (e.g., s[e]ntir ?to feel?, s[je]nto ?Ifeel?)
and non-diphthongization (e.g., p[e]dir ?toask?, p[i]do ?I ask?)
leads to paradigmatic gaps inlexemes for which the applicability of diphthon-gization has low reliability (e.g., abolir ?to abolish,*ab[we]lo, *ab[o]lo ?I abolish?
).However, this approach both overpredicts andunderpredicts the existence of gaps cross-linguistically.
First, it predicts that gaps shouldoccur whenever the analogical forces determiningword forms are contradictory and evenly weighted.However, variation between two inflectionalpatterns seems to more commonly result from sucha scenario.
Second, the model predicts that if theform-based conflict disappears, the gaps shouldalso disappear.
However, in Russian and probablyin other languages, gaps persist even after the lossof competing inflectional patterns or othersynchronic form-based motivation (Sims 2006).By contrast, our approach operates at the levelof inflectional property sets (IPS), or moreproperly, at the level of inflectional paradigms.We propose that once gaps are established in alanguage for whatever reason, they persist becauselearners infer the relative non-use of a given1936combination of stem and IPS.1  Put differently, wehypothesize that speakers possess at least twokinds of knowledge about inflectional structure: (1)knowledge of how to generate the appropriate formfor a given lemma and IPS, and (2) knowledge ofthe probability with which that combination oflemma and property set is expressed, regardless ofthe form.
Our approach differs from previousaccounts in that persistence of gaps is attributed tothe latter kind of knowledge, and does not dependon synchronic morphological competition.We present a case study of the Russian verbalgaps, which are notable for their persistence.
Theyarose between the mid 19th and early 20th century(Baerman 2007), and are still strongly attested inthe modern language, but have no apparentsynchronic morphological cause.We model the persistence and spread of theRussian verbal gaps with a multi-agent model withBayesian learning.
Our model has two kinds ofagents, adults and children.
A model cycle consistsof two phases: a production-perception phase, anda learning-maturation phase.
In the production-perception phase, adults produce a batch oflinguistic data (verb forms), and children listen tothe productions from the adults they know.
In thelearning-maturation phase, children build agrammar based on the input they have received,then mature into adults.
The existing adults die off,and the next generation of children is born.Our model exhibits similar behavior to what isknown about the development of Russian gaps.2 The historical and distributional factsof Russian verbal gaps2.1 Traditional descriptionsGrammars and dictionaries of Russian frequentlycite paradigmatic gaps in the 1sg non-past.
Ninemajor dictionaries and grammars, including?vedova (1982) and Zaliznjak (1977), yielded acombined list of 96 gaps representing 68 distinctstems.
These verbal gaps fall almost entirely intothe second conjugation class, and theyoverwhelmingly affect the subgroup of dentalstems.
Commonly cited gaps include: *gal?u ?Imake a hubbub?
; *o?u?us?
?I come to be (REFL)?
;1SG *o??u?
?u ?I feel?
; *pobe?u ?I will win?
; and*ube?u ?I will convince?.21 Paradigmatic gaps also probably serve a sociolinguisticpurpose, for example as markers of education, but socio-linguistic issues are beyond the scope of this paper.There is no satisfactory synchronic reason forthe existence of the gaps.
The grouping of gapsamong 2nd conjugation dental stems is seeminglynon-arbitrary because these are exactly the formsthat would be subject to a palatalizing morphopho-nological alternation (tj ?
tS or Sj, dj ?
Z, sj ?
S, zj?
Z).
Yet the Russian gaps do not meet the criteriafor morphophonological competition as intendedby Albright?s (2003) model, because thealternations apply automatically in ContemporaryStandard Russian.
Analogical forces should thusheavily favor a single form, for example, pobe?u.Traditional explanations for the gaps, such ashomophony avoidance (?vedova 1982) are alsounsatisfactory since they can, at best, explain onlya small percentage of the gaps.Thus, the data suggest that gaps persist inRussian primarily because they are not uttered, andthis non-use is learned by succeeding generationsof Russian speakers.3  The clustering of the gapsamong 2nd conjugation dental stems most likely ispartially a remnant of their original causes, andpartially represents analogic extension of gapsalong morphophonological lines (see 2.3 below).2.2 Empirical evidence for and operationaldefinition of gapsWhen dealing with descriptions in semi-prescriptive sources such as dictionaries, we mustalways ask whether they accurately representlanguage use.
In other words, is there empiricalevidence that speakers fail to use these words?We sought evidence of gaps from the RussianNational Corpus (RNC).
4  The RNC is a balancedtextual corpus with 77.6 million words consistingprimarily of the contemporary Russian literarylanguage.
The content is prose, plays, memoirsand biographies, literary criticism, newspaper andmagazine articles, school texts, religious and2  We use here the standard Cyrillic transliteration used bylinguists.
It should not be considered an accuratephonological representation.
Elsewhere, when phonologicalissues are relevant, we use IPA.3 See Manning (2003) and Zuraw (2003) on learning fromimplicit negative evidence.4 Documentation: http://ruscorpora.ru/corpora-structure.htmlMirror site used for searching:http://corpus.leeds.ac.uk/ruscorpora.html.2937philosophical materials, technical and scientifictexts, judicial and governmental publications, etc.We gathered token frequencies for the six non-past forms of 3,265 randomly selected secondconjugation verb lemmas.
This produced 11,729inflected forms with non-zero frequency.
5   Asdescribed in Section 3 below, these 11,729 formfrequencies became our model?s seed data.To test the claim that Russian has verbal gaps,we examined a subsample of 557 2nd conjugationlemmas meeting the following criteria: (a) totalnon-past frequency greater than 36 raw tokens, and(b) 3sg and 3pl constituting less than 85% of totalnon-past frequency.
6   These constraints weredesigned to select verbs for which all six person-number combinations should be robustly attested,and to minimize sampling errors by removinglemmas with low attestation.We calculated the probability of the 1sginflection by dividing the number of 1sg forms bythe total number of non-past forms.
The subset wasbimodally distributed with one peak near 0%, atrough at around 2%, and the other peak at 13.3%.The first peak represents lemmas in which the 1sgform is basically not used ?
gaps.
Accordingly, wedefine gaps as second conjugation verbs whichmeet criteria (a) and (b) above, and for which the1sg non-past form constitutes less than 2% of totalnon-past frequency for that lemma (N=56).In accordance with the grammatical descrip-tions, our criteria are disproportionately likely toidentify dental stems as gaps.
Still, only 43 of 412dental stems (10.4%) have gaps, compared with 13gaps among 397 examples of other stems (3.3%).Second, not all dental stems are equally affected.There seems to be a weak prototypicality effectcentered around stems ending in /dj/, from which/tj/ and /zj/ each differ by one phonological feature.There may also be some weak semantic factors thatwe do not consider here./dj/ /tj/ /zj/ /sj/ /stj/13.3%(19/143)12.4%(14/118)11.9%(5/42)4.8%(3/62)4.3%(2/47)Table 1.
Distribution of Russian verbal gapsamong dental stems5  We excluded 29 high-frequency lemmas for which thecorpus did not provide accurate counts.6 Russian has a number of verbs for which only the 3sg and3pl are regularly used.2.3 Some relevant historical factsA significant difference between the morpho-logical competition approach and our statisticallearning approach is that the former attempts toprovide a single account for both the rise and theperpetuation of paradigmatic gaps.
By contrast,our statistical learning model does not require thatthe morphological system provide synchronicmotivation.
The following question thus arises:Were the Russian gaps originally caused by forceswhich are no longer in play in the language?Baerman and Corbett (2006) find evidence thatthe gaps began with a single root, -bed- (e.g.,pobedit?
?to win?
), and subsequently spreadanalogically within dental stems.
Baerman (2007)expands on the historical evidence, finding that aconspiracy of several factors provided the initialpush towards defective 1sg forms.
Most importantamong these, many of the verbs with 1sg gaps inmodern Russian are historically associated withaberrant morphophonological alternations.
Heargues that when these unusual alternations wereeliminated in the language, some of the wordsfailed to be integrated into the new morphologicalpatterns, which resulted in lexically specified gaps.Important to the point here is that theelimination of marginal alternations removed anearlier synchronic motivation for the gaps.
Yetgaps have persisted and new gaps have arisen (e.g.,pylesosit?
?to vacuum?).
This persistence is thebehavior that we seek to model.3 Formal aspects of the modelWe take up two questions: How much machinerydo we need for gaps to persist?
How muchmachinery do we need for gaps to spread to phono-logically similar words?
We model three scenarios.In the first scenario there is no grammar learning.Adult agents produce forms by random samplingfrom the forms that heard as children, and childagents hear those forms.
In the subsequentgeneration children become adults.
In this scenariothere is thus no analogical pressure.
Any perse-verance of gaps results from word-specific learning.The second scenario is similar to the first, exceptthat the learning process includes analogicalpressure from a random set of words.
Specifically,for a target concept, the estimated distribution ofits IPS is influenced by the distribution of knownwords.
This enables the learner to express a known3938concept with a novel IPS.
For example, imaginethat a learner hears the present tense verb formgoogles, but not the past tense googled.
By analogywith other verbs, learners can expect the past tenseto occur with a certain frequency, even if they havenot encountered it.The third scenario builds upon the second.
Inthis version, the analogical pressure is notcompletely random.
Instead, it is weighted bymorphophonological similarity ?
similar wordforms contribute more to the analogical force on atarget concept than do dissimilar forms.
Thisaddition to the model is motivated by the pervasiveimportance of stem shape in the Russianmorphological system generally, and potentiallyprovides an account for the phonologicalprototypicality effect among Russian gaps.The three scenarios thus represent increasingmachinery for the model, and we use them toexplore the conditions necessary for gaps to persistand spread.
We created a multi-agent networkmodel with Bayesian learning component.
In thefollowing sections we describe the model?sstructure, and outline the criteria by which weevaluate its output under the various conditions.3.1 Social structureOur model includes two generations of agents.Adult agents output linguistic forms, whichprovide linguistic input for child agents.Output/input occurs in batches.7  After each batchall adults die, all children mature into adults, and anew generation of children is born.
Each run of themodel included 10 generations of agents.We model the social structure with a randomnetwork.
Each adult produces 100,000 verb forms,and each child is exposed to every production fromevery adult to whom they are connected.
Eachgeneration consisted of 50 adult agents, and childagents are connected to adults with someprobability p.  On average, each child agent isconnected to 10 adult agents, meaning that eachchild hears, on average, 1,000,000 tokens.3.2 Linguistic eventsRussian gaps are localized to second conjugationnon-past verb forms, so productions of these formsare the focus of interest.
Formally, we define alinguistic event as a concept-inflection-form (C,I,F)triple.
The concept serves to connect the differentforms and inflections of the same lemma.7  See Niyogi (2006) for why batch learning is areasonable approximation in this context.3.3 Definition of grammarA grammar is defined as a probability distributionover linguistic events.
This gives rise to naturalformulations of learning and production asstatistical processes: learning is estimating aprobability distribution from existing data, andproduction is sampling from a probabilitydistribution.
The grammar can be factored intomodular components:p(C, I, F) = p(C) ?
p(I | C) ?
p(F | C, I)In this paper we focus on the probabilitydistribution of concept-inflection pairs.
In otherwords, we focus on the relative frequency ofinflectional property sets (IPS) on a lemma-by-lemma basis, represented by the middle term above.Accordingly, we made the simplest possibleassumptions for the first and last terms.
Tocalculate the probability of a concept, children usethe sample frequency (e.g., if they hear 10 tokensof the concept ?eat?, and 1,000 tokens total, thenp(?eat?)
= 10/1000 = .01).
Learning of forms isperfect.
That is, learners always produce thecorrect form for every concept-inflection pair.3.4 Learning modelAlthough production in the real world is governedby semantics, we treat it here as a statisticalprocess, much like rolling a six-sided die whichmay or may not be fair.
When producing a Russiannon-past verb, there are six possible combinationsof inflectional properties (3 persons * 2 numbers).In our model, word learning involves estimatingthe probability distribution over the frequencies ofthe six forms on a lemma-by-lemma basis.
Ahypothetical example that introduces our variables:jest?
1sg 2sg 3sg 1pl 2pl 3pl SUMD 15 5 45 5 5 25 100d 0.15 0.05 0.45 0.05 0.05 0.25 1Table 2.
Hypothetical probability distributionThe first row indicates the concept and theinflections.
The second row (D) indicates the4939hypothetical number of tokens of jest?
?eat?
that thelearner heard for each inflection (bolding indicatesa six-vector).
We use |D| to indicate the sum ofthis row (=100), which is the concept frequency.The third row (d) indicates the sample probabilityof that inflection, which is simply the second rowdivided by |D|.The learner?s goal is to estimate the distributionthat generated this data.
We assume themultinomial distribution, whose parameter issimply the vector of probabilities of each IPS.
Foreach concept, the learner?s task is to estimate theprobability of each IPS, represented by h in theequations below.
We begin with Bayes?
rule:p(h | D) ?
p(h) ?
multinom(D | h)The prior distribution constitutes the analogicalpressure on the lemma.
It is generated from the?expected?
behavior, h0, which is an average of theknown behavior from a random sample of otherlemmas.
The parameter ?
determines the numberof lemmas that are sampled for this purpose ?
itrepresents how many existing words affect a newword.
To model the effect of morphophonologicalsimilarity (mpSim), in one variant of the model weweight this average by the similarity of the stem-final consonant.8  For example, this has the effectthat existing dental stems have more of an effecton dental stems.
In this case, we defineh0 = ?c?
in sample d c?
?
mpSim(c, c?)/?
mpSim(c, c?
)We use a featural definition of similarity, so that ifthe stem-final consonants differ by 0, 1, 2, or 3 ormore phonological features, the resulting similarityis 1, 2/3, 1/3, or 0, respectively.The prior distribution should assign higherprobability to hypotheses that are ?closer?
to thisexpected behavior h0.
Since the hypothesis is itselfa probability distribution, the natural measure touse is the KL divergence.
We used anexponentially distributed prior with parameter ?
:p(h) ?
exp(-??
h0 || h)8  In Russian, the stem-final consonant is important formorphological behavior generally.
Any successful Russianlearner would have to extract the generalization, completelyapart from the issues posed by gaps.As will be shown shortly, ?
has a naturalinterpretation as the relative strength of the priorwith respect to the observed data.The learner calculates their final grammar bytaking the mode of the posterior distribution(MAP).
It can be shown that this value is given byarg max p(h | D) = (??
h0 + |D|?
d)/(?+|D|)Thus, the output of this learning rule is aprobability vector h that represents the estimatedprobability of each of the six possible IPS?s forthat concept.
As can be seen from the equationabove, this probability vector is an average of theexpected behavior h0 and the observed data d,weighted by ?
and the amount of observed data |D|,respectively.Our approach entails that from the perspectiveof a language learner, gaps are not qualitativelydistinct from productive forms.
Instead, 1sg non-past gaps represent one extreme of a range ofprobabilities that the first person singular will beproduced.
In this sense, ?gaps?
represent anartificial boundary which we place on a gradientstructure for the purpose of evaluating our model.The contrast between our learning model and theaccount of gaps presented in Albright (2003)merits emphasis at this point.
Generally speaking,learning a word involves at least two tasks:learning how to generate the appropriatephonological form for a given concept andinflectional property set, and learning theprobability that a concept and inflectional propertyset will be produced at all.
Albright?s modelfocuses on the former aspect; our model focuses onthe latter.
In short, our account of gaps lies in thelikelihood of a concept-IPS pair being expressed,not in the likelihood of a form being expressed.3.5 Production modelWe model language production as sampling fromthe probability distribution that is the output of thelearning rule.3.6 Seeding the modelThe input to the first generation was sampled fromthe verbs identified in the corpus search (see 2.2).Each input set contained 1,000,000 tokens, whichwas the average amount of input for agents in allsucceeding generations.
This made the first5940generation?s input as similar as possible to theinput of all succeeding generations.3.7 Parameter space in the three scenariosIn our model we manipulate two parameters ?
thestrength of the analogical force on a target conceptduring the learning process (?
), and the number ofconcepts which create the analogical force (?
),taken randomly from known concepts.As discussed above, we model three scenarios.In the first scenario, there is no grammar learning,so there is only one condition (?
= 0).
For thesecond and third scenarios, we run the model withfour values for ?, ranging from weak to stronganalogical force (0.05, 0.25, 1.25, 6.25), and twovalues for ?, representing influence from a small orlarge set of other words (30, 300).4 Evaluating the output of the modelWe evaluate the output of our model against thefollowing question: How well do gaps persist?We count as gaps any forms meeting the criteriaoutlined in 2.2 above, tabulating the number ofgaps which exist for only one generation, for twototal generations, etc.
We define ?
as the expectednumber of generations (out of 10) that a givenconcept meets the gap criteria.
Thus, ?
represents agap?s ?life expectancy?
(see Figure 1).We found that this distribution is exponential ?there are few gaps that exist for all ten generations,and lots of gaps that exist for only one, so wecalculated ?
with a log linear regression.
Eachvalue reported is an average over 10 runs.As discussed above, our goal was to discoverwhether the model can exhibit the same qualitativebehavior as the historical development of Russiangaps.
Persistence across a handful of generations(so far) and spread to a limited number of similarforms should be reflected by a non-negligible ?.5 ResultsIn this section we present the results of our modelunder the scenarios and parameter settings above.Remember that in the first scenario there is nogrammar learning.
This run of the model representsthe baseline condition ?
completely word-specificknowledge.
Sampling results in random walks onform frequencies, so once a word form disappearsit never returns to the sample.
Word-specificlearning is thus sufficient for the perseverance ofexisting paradigmatic gaps and the creation of newones.
With no analogical pressure, gaps arerobustly attested (?
= 6.32).
However, the newgaps are not restricted to the 1sg, and under thisscenario, learners are unable to generalize to anovel pairing of lexeme + IPS.The second scenario presents a morecomplicated picture.
As shown in Table 3, asanalogical pressure (?)
increases, gap lifeexpectancy (?)
decreases.
In other words, highanalogical pressure quickly eliminates atypicalfrequency distributions, such as those exhibited bygaps.
The runs with low values of ?
are particularlyinteresting because they represent an approximatebalance between elimination of gaps as a generalbehavior, and the short-term persistence and evenspread of gaps due to sampling artifacts and theinfluence of existing gaps.
Thus, although the limitbehavior is for gaps to disappear, this scenarioretains the ability to explain persistence of gapsdue to word-specific learning when there is weakanalogical force.At the same time, the facts of Russian differfrom the behavior of the model in that the Russiangaps spread to morphophonologically similarforms, not random ones.
The third version of ourmodel weights the analogical strength of differentconcepts based upon morphophonologicalsimilarity to the target.?
?
?
(random)?(phono.)
-- 0 6.3230 0.05 4.95 5.7730 0.25 3.46 5.2830 1.25 1.91 3.0730 6.25 2.59 1.87300 0.05 4.97 5.99300 0.25 3.72 5.14300 1.25 1.90 3.10300 6.25 2.62 1.84Table 3.
Life expectancy of gaps, as a function ofthe strength of random analogical forcesUnder these conditions we get two interestingresults, presented in Table 3 above.
First, gapspersist slightly better overall in scenario 3 than in6941scenario 2 for all levels of ?
and ?.
9  Compare the?
values for random analogical force (scenario 2)with the ?
values for morphophonologicallyweighted analogical force (scenario 3).Second, strength of analogical force matters.When there is weak analogical pressure, weightingfor morphophonological similarity has little effecton the persistence and spread of gaps.
However,when there is relatively strong analogical pressure,morphophonological similarity helps atypicalfrequency distributions to persist, as shown inFigure 1.
This results from the fact that there is aprototypicality effect for gaps.
Since dental stemsare more likely to be gaps, incorporating sensitivityto stem shape causes the analogical pressure ontarget dental stems to be relatively stronger fromwords that are gaps.
Correspondingly, theanalogical pressure on non-dental stems isrelatively stronger from words that are not gaps.The prototypical stem shape for a gap is therebyperpetuated and gaps spread to new dental stems.01234561 2 3 4 5 6 7 8 9 10# of generationslog(#ofgaps)random, ?
= 0.05 random, ?
= 1.25phonological, ?
= 0.05 phonological, ?
= 1.25Figure 1.
Gap life expectancy (?=0.05, ?=30)9 The apparent increase in gap half-life when ?=6.25 isan artifact of the regression model.
There were a fewwell-entrenched gaps whose high lemma frequencyenables them to resist even high levels of analogicalpressure over 10 generations.
These data points skewedthe regression, as shown by a much lower R2 (0.5 vs.0.85 or higher for all the other conditions).6 DiscussionIn conclusion, our model has in many respectssucceeded in getting gaps to perpetuate and spread.With word-specific learning alone, well-entrenched gaps can be maintained across multiplegenerations.
More significantly, weak analogicalpressure, especially if weighted for morpho-phonological similarity, results in the perseveranceand short-term growth of gaps.
This is essentiallythe historical pattern of the Russian verbal gaps.These results highlight several issues regardingboth the nature of paradigmatic gaps and thestructure of inflectional systems generally.We claim that it is not necessary to posit anirreconcilable conflict in the generation of inflectedforms in order to account for gaps.
Remember thatin our model, agents face no conflict in terms ofwhich form to produce ?
there is only onepossibility.
Yet the gaps persist in part because ofanalogical pressure from existing gaps.
Albright(2003) himself is agnostic on the issue of whetherform-based competition is necessary for theexistence and persistence of gaps, but Hudson(2000), among others, claims that gaps could notexist in the absence of it.
We have presentedevidence that this claim is unfounded.But why would someone assume that grammarcompetition is necessary?
Hudson?s claim arisesfrom a confusion of two issues.
Discussing theEnglish paradigmatic gap amn?t, Hudson statesthat ?a simple application of [the usage-basedlearning] principle would be to say that the gapexists simply because nobody says amn?t...  Butthis explanation is too simple...
There are manyinflected words that may never have been uttered,but which we can nevertheless imagine ourselvesusing, given the need; we generate them bygeneralization?
(Hudson 2000:300).
By his logic,there must therefore be some source of grammarconflict which prevents speakers from generalizing.However, there is a substantial differencebetween having no information about a word, andhaving information about the non-usage of a word.We do not dispute learners?
ability to generalize.We only claim that information of non-usage issufficient to block such generalizations.
Whenconfronted with a new word, speakers will happilygeneralize a word form, but this is not the sametask that they perform when faced with gaps.7942The perseverance of gaps in the absence ofform-based competition shows that a different,non-form level of representation is at issue.Generating inflectional morphology involves atleast two different types of knowledge: knowledgeabout the appropriate word form to express a givenconcept and IPS on the one hand, and knowledgeof how often that concept and IPS is expressed onthe other.
The emergence of paradigmatic gapsmay be closely tied to the first type of knowledge,but the Russian gaps, at least, persist because ofthe second type of knowledge.
We thereforepropose that morphology may be defective at themorphosyntactic level.This returns us to the question that we began thispaper with ?
how paradigmatic gaps can persist inlight of the overwhelming productivity ofinflectional morphology.
Our model suggests thatthe apparent contradiction is, at least in some cases,illusory.
Productivity refers to the likelihood of agiven inflectional pattern applying to a givencombination of stem and IPS.
Our account isbased in the likelihood of the stem and inflectionalproperty set being expressed at all, regardless ofthe form.
In short, the Russian paradigmatic gapsrepresent an issue which is orthogonal toproductivity.
The two issues are easily confused,however.
An unusual frequency distribution canmake it appear that there is in fact a problem at thelevel of form, even when there may not be.Finally, our simulations raise the question ofwhether the 1sg non-past gaps in Russian willpersist in the language in the long term.
In ourmodel, analogical forces delay convergence to themean, but the limit behavior is that all gapsdisappear.
Although there is evidence in Russianthat words can develop new gaps, we do not knowwith any great accuracy whether the set of gaps iscurrently expanding, contracting, or approximatelystable.
Our model predicts that in the long run, thegaps will disappear under general analogicalpressure.
However, another possibility is that ourmodel includes only enough factors (e.g.,morphophonological similarity) to approximate theshort-term influences on the Russian gaps and thatwe would need more factors, such as semantics, tosuccessfully model their long-term development.This remains an open question.ReferencesAlbright, Adam.
2003.
A quantitative study of Spanishparadigm gaps.
In West Coast Conference on FormalLinguistics 22 proceedings, eds.
Gina Garding andMimu Tsujimura.
Somerville, MA: Cascadilla Press,1-14.Albright, Adam, and Bruce Hayes.
2002.
ModelingEnglish past tense intuitions with minimalgeneralization.
In Proceedings of the Sixth Meeting ofthe Association for Computational LinguisticsSpecial Interest Group in Computational Phonologyin Philadelphia, July 2002, ed.
Michael Maxwell.Cambridge, MA: Association for ComputationalLinguistics, 58-69.Baerman, Matthew.
2007.
The diachrony ofdefectiveness.
Paper presented at 43rd AnnualMeeting of the Chicago Linguistic Society inChicago, IL, May 3-5, 2007.Baerman, Matthew, and Greville Corbett.
2006.
Threetypes of defective paradigms.
Paper presented at TheAnnual Meeting of the Linguistic Society of Americain Albuquerque, NM, January 5-8, 2006.Hudson, Richard.
2000.
*I amn?t.
Language 76 (2):297-323.Manning, Christopher.
2003.
Probabilistic syntax.
InProbabilistic linguistics, eds.
Rens Bod, Jennifer Hayand Stephanie Jannedy.
Cambridge, MA: MIT Press,289-341.Niyogi, Partha.
2006.
The computational nature oflanguage learning and evolution.
Cambridge, MA:MIT Press.Sims, Andrea.
2006.
Minding the gaps: Inflectionaldefectiveness in paradigmatic morphology.
Ph.D.thesis: Linguistics Department, The Ohio StateUniversity.
?vedova, Julja.
1982.
Grammatika sovremennogorusskogo literaturnogo jayzka.
Moscow: Nauka.Zaliznjak, A.A., ed.
1977.
Grammati?eskij slovar'russkogo jazyka: Slovoizmenenie.
Moskva: Russkijjazyk.Zuraw, Kie.
2003.
Probability in language change.
InProbabilistic linguistics, eds.
Rens Bod, Jennifer Hayand Stephanie Jannedy.
Cambridge, MA: MIT Press,139-176.8943
