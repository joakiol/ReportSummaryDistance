Proceedings of the ACL 2007 Demo and Poster Sessions, pages 89?92,Prague, June 2007. c?2007 Association for Computational LinguisticsTest Collection Selection and Gold Standard Generationfor a Multiply-Annotated Opinion CorpusLun-Wei Ku, Yong-Shen Lo and Hsin-Hsi ChenDepartment of Computer Science and Information EngineeringNational Taiwan University{lwku, yslo}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.twAbstractOpinion analysis is an important researchtopic in recent years.
However, there areno common methods to create evaluationcorpora.
This paper introduces a methodfor developing opinion corpora involvingmultiple annotators.
The characteristics ofthe created corpus are discussed, and themethodologies to select more consistenttesting collections and their correspondinggold standards are proposed.
Under thegold standards, an opinion extraction sys-tem is evaluated.
The experiment resultsshow some interesting phenomena.1 IntroductionOpinion information processing has been studiedfor several years.
Researchers extracted opinionsfrom words, sentences, and documents, and bothrule-based and statistical models are investigated(Wiebe et al, 2002; Pang et al, 2002).
Theevaluation metrics precision, recall and f-measureare usually adopted.A reliable corpus is very important for the opin-ion information processing because the annotationsof opinions concern human perspectives.
Thoughthe corpora created by researchers were analyzed(Wiebe et al, 2002), the methods to increase thereliability of them were seldom touched.
The strictand lenient metrics for opinions were mentioned,but not discussed in details together with the cor-pora and their annotations.This paper discusses the selection of testing col-lections and the generation of the correspondinggold standards under multiple annotations.
Thesetesting collections are further used in an opinionextraction system and the system is evaluated withthe corresponding gold standards.
The analysis ofhuman annotations makes the improvements ofopinion analysis systems feasible.2 Corpus AnnotationOpinion corpora are constructed for the research ofopinion tasks, such as opinion extraction, opinionpolarity judgment, opinion holder extraction,opinion summarization, opinion questionanswering, etc..
The materials of our opinioncorpus are news documents from NTCIR CIRB020and CIRB040 test collections.
A total of 32 topicsconcerning opinions are selected, and eachdocument is annotated by three annotators.Because different people often feel differentlyabout an opinion due to their own perspectives,multiple annotators are necessary to build areliable corpus.
For each sentence, whether it isrelevant to a given topic, whether it is an opinion,and if it is, its polarity, are assigned.
The holdersof opinions are also annotated.
The details of thiscorpus are shown in Table 1.Topics Documents SentencesQuantity 32 843 11,907Table 1.
Corpus size3 Analysis of Annotated CorpusAs mentioned, each sentence in our opinion corpusis annotated by three annotators.
Although this is amust for building reliable annotations, the incon-sistency is unavoidable.
In this section, all thepossible combinations of annotations are listed andtwo methods are introduced to evaluate the qualityof the human-tagged opinion corpora.3.1 Combinations of annotationsThree major properties are annotated for sen-tences in this corpus, i.e., the relevancy, the opin-ionated issue, and the holder of the opinion.
Thecombinations of relevancy annotations are simple,and annotators usually have no argument over theopinion holders.
However, for the annotation ofthe opinionated issue, the situation is more com-89plex.
Annotations may have an argument aboutwhether a sentence contains opinions, and theirannotations may not be consistent on the polaritiesof an opinion.
Here we focus on the annotations ofthe opinionated issue.
Sentences may be consid-ered as opinions only when more than two annota-tors mark them opinionated.
Therefore, they aretargets for analysis.
The possible combinations ofopinionated sentences and their polarity are shownin Figure 1.A BC EDPositive/Neutral/NegativeFigure 1.
Possible combinations of annotationsIn Figure 1, Cases A, B, C are those sentenceswhich are annotated as opinionated by all threeannotators, while cases D, E are those sentenceswhich are annotated as opinionated only by twoannotators.
In case A and case D, the polaritiesannotated by annotators are identical.
In case B,the polarities annotated by two of three annotatorsare agreed.
However, in cases C and E, the polari-ties annotated disagree with each other.
The statis-tics of these five cases are shown in Table 2.Case A B C D E AllNumber 1,660 1,076 124 2,413 1,826 7,099Table 2.
Statistics of cases A-E3.2 Inconsistency3P P PN N NX X X3Multiple annotators bring the inconsistency.
Thereare several kinds of inconsistency in annotations,for example, relevant/non-relevant, opinion-ated/non-opinionated, and the inconsistency of po-larities.
The relevant/non-relevant inconsistency ismore like an information retrieval issue.
For opin-ions, because their strength varies, sometimes it ishard for annotators to tell if a sentence is opinion-ated.
However, for the opinion polarities, the in-consistency between positive and negative annota-tions is obviously stronger than that between posi-tive and neutral, or neutral and negative ones.Here we define a sentence ?strongly inconsistent?if both positive and negative polarities are assignedto a sentence by different annotators.
The stronginconsistency may occur in case B (171), C (124),and E (270).
In the corpus, only about 8% sen-tences are strongly inconsistent, which shows theannotations are reliable.P PN NX X2 3P X NP N XN P XN X PX P NX N PP N3.3 Kappa value for agreementWe further assess the usability of the annotatedcorpus by Kappa values.
Kappa value gives aquantitative measure of the magnitude of inter-annotator agreement.
Table 3 shows a commonlyused scale of the Kappa values.Kappa value Meaning<0 less than change agreement0.01-0.20 slight agreement0.21-0.40 fair agreement0.41-0.60 moderate agreement0.61-0.80 substantial agreement0.81-0.99 almost perfect agreementTable 3.
Interpretation of Kappa valueThe inconsistency of annotations brings difficul-ties in generating the gold standard.
Sentencesshould first be selected as the testing collection,N PP XN XX PNX2P PN NX XP NX90and then the corresponding gold standard can begenerated.
Our aim is to generate testing collec-tions and their gold standards which agree mostlyto annotators.
Therefore, we analyze the kappavalue not between annotators, but between the an-notator and the gold standard.
The methodologiesare introduced in the next section.4 Testing Collections and Gold StandardsThe gold standard of relevance, the opinionatedissue, and the opinion holder must be generatedaccording to all the annotations.
Answers are cho-sen based on the agreement of annotations.
Con-sidering the agreement among annotations them-selves, the strict and the lenient testing collectionsand their corresponding gold standard are gener-ated.
Considering the Kappa values of each anno-tator and the gold standard, topics with high agree-ment are selected as the testing collection.
More-over, considering the consistency of polarities, thesubstantial consistent testing collection is gener-ated.
In summary, two metrics for generating goldstandards and four testing collections are adopted.4.1 Strict and lenientNamely, the strict metric is different from the leni-ent metric in the agreement of annotations.
For thestrict metric, sentences with annotations agreed byall three annotators are selected as the testing col-lection and the annotations are treated as the strictgold standard; for the lenient metric, sentenceswith annotations agreed by at least two annotatorsare selected as the testing collection and the major-ity of annotations are treated as the lenient goldstandard.
For example, for the experiments of ex-tracting opinion sentences, sentences in cases A, B,and C in Figure 1 are selected in both strict andlenient testing collections, while sentences in casesD and E are selected only in the lenient testing col-lection because three annotations are not totallyagreed with one another.
For the experiments ofopinion polarity judgment, sentences in case A inFigure 1 are selected in both strict and lenient test-ing collections, while sentences in cases B, C, Dand E are selected only in the lenient testing col-lection.
Because every opinion sentence should begiven a polarity, the polarities of sentences in casesB and D are the majority of annotations, while thepolarity of sentences in cases C are given the po-larity neutral in the lenient gold standard.
The po-larities of sentences in case E are decided by rulesP+X=P, N+X=N, and P+N=X.
As for opinionholders, holders are found in opinion sentences ofeach testing collection.
The strict and lenient met-rics are also applied in annotations of relevance.4.2 High agreementTo see how the generated gold standards agreewith the annotations of all annotators, we analyzethe kappa value from the agreements of each anno-tator and the gold standard for all 32 topics.
Eachtopic has two groups of documents from NTCIR:very relevant and relevant to topic.
However, onetopic has only the relevant type document, it re-sults in a total of 63 (2*31+1) groups of documents.Note that the lenient metric is applied for generat-ing the gold standard of this testing collection be-cause the strict metric needs perfect agreementwith each annotator?s annotations.
The distribu-tion of kappa values of 63 groups is shown in Ta-ble 4 and Table 5.
The cumulative frequency bargraphs of Table 4 and Table 5 are shown in Figure2 and Figure 3.Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99Number 1 2 12 14 33 1Table 4.
Kappa values for opinion extractionKappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99Number 9 0 7 21 17 9Table 5.
Kappa values for polarity judgmentFigure 2.
Cumulative frequency of Table 41 3152962 63010203040506070<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.999 916375463010203040506070<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99Figure 3.
Cumulative frequency of Table 5According to Figure 2 and Figure 3, documentgroups with kappa values above 0.4 are selected as91the high agreement testing collection, that is,document groups with moderate agreement in Ta-ble 3.
A total of 48 document groups are collectedfor opinion extraction and 47 document groups arecollected for opinion polarity judgment.4.3 Substantial ConsistencyIn Section 3.2, sentences which are ?strongly in-consistent?
are defined.
The substantial consis-tency test collection expels strongly inconsistentsentences to achieve a higher consistency.
Noticethat this test collection is still less consistent thanthe strict test collection, which is perfectly consis-tent with annotators.
The lenient metric is appliedfor generating the gold standard for this collection.5 An Opinion System -- CopeOpiA Chinese opinion extraction system for opinion-ated information, CopeOpi, is introduced here.
(Kuet al, 2007)  When judging the opinion polarity ofa sentence in this system, three factors are consid-ered: sentiment words, negation operators andopinion holders.
Every sentiment word has its ownsentiment score.
If a sentence consists of morepositive sentiments than negative sentiments, itmust reveal something good, and vice versa.
How-ever, a negation operator, such as ?not?and ?never?, may totally change the sentiment po-larity of a sentiment word.
Therefore, when a nega-tion operator appears together with a sentimentword, the opinion score of the sentiment word Swill be changed to -S to keep the strength but re-verse the polarity.
Opinion holders are also consid-ered for opinion sentences, but how they influenceopinions has not been investigated yet.
As a result,they are weighted equally at first.
A word is con-sidered an opinion holder of an opinion sentence ifeither one of the following two criteria is met:1.
The part of speech is a person name, organi-zation name or personal.2.
The word is in class A (human), type Ae (job)of the Cilin Dictionary (Mei et al, 1982).6 Evaluation Results and DiscussionsExperiment results of CopeOpi using four designedtesting collections are shown in Table 6.
Under thelenient metric with the lenient test collection, f-measure scores 0.761 and 0.383 are achieved byCopeOpi.
The strict metric is the most severe, andthe performance drops a lot under it.
Moreover,when using high agreement (H-A) and substantialconsistency (S-C) test collections, the performanceof the system does not increase in portion to theincrease of agreement.
According to the agree-ment of annotators, people should perform best inthe strict collection, and both high agreement andsubstantial consistency testing collections are eas-ier than the lenient one.
This phenomenon showsthat though this system?s performance is satisfac-tory, its behavior is not like human beings.
For acomputer system, the lenient testing collection isfuzzier and contains more information for judg-ment.
However, this also shows that the systemmay only take advantage of the surface informa-tion.
If we want our systems really judge like hu-man beings, we should enhance the performanceon strict, high agreement, and substantial consis-tency testing collections.
This analysis gives us, orother researchers who use this corpus for experi-ments, a direction to improve their own systems.Opinion Extraction Opinion + PolarityMeasure P R F P R FLenient 0.664 0.890 0.761 0.335 0.448 0.383Strict 0.258 0.921 0.404 0.104 0.662 0.180H-A 0.677 0.885 0.767 0.339 0.455 0.388S-C    0.308 0.452 0.367Table 6.
Evaluation resultsAcknowledgmentsResearch of this paper was partially supported by Excel-lent Research Projects of National Taiwan University,under the contract 95R0062-AE00-02.ReferencesMei, J., Zhu, Y. Gao, Y. and Yin, H.. tong2yi4ci2ci2lin2.Shanghai Dictionary Press, 1982.Pang, B., Lee, L., and Vaithyanathan, S. (2002).Thumbs up?
Sentiment classification using machinelearning techniques.
Proceedings of the 2002 Confer-ence on EMNLP, pages 79-86.Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P.,Fraser, B., Litman, D., Pierce, D., Riloff, E., andWilson, T. (2002).
NRRC summer workshop onmulti-perspective question answering, final report.ARDA NRRC Summer 2002 Workshop.Ku, L.-W., Wu, T.-H., Li, L.-Y.
and Chen., H.-H.(2007).
Using Polarity Scores of Words for Sentence-level Opinion Extraction.
Proceedings of the SixthNTCIR Workshop.92
