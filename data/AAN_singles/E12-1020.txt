Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 194?203,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCompensating for Annotation Errors in Training a Relation ExtractorBonan Min Ralph GrishmanNew York University New York University715 Broadway, 7th floor 715 Broadway, 7th floorNew York, NY 10003 USA New York, NY 10003 USAmin@cs.nyu.edu grishman@cs.nyu.eduAbstractThe well-studied supervised RelationExtraction algorithms require trainingdata that is accurate and has goodcoverage.
To obtain such a gold standard,the common practice is to do independentdouble annotation followed byadjudication.
This takes significantlymore human effort than annotation doneby a single annotator.
We do a detailedanalysis on a snapshot of the ACE 2005annotation files to understand thedifferences between single-passannotation and the more expensive nearlythree-pass process, and then propose analgorithm that learns from the muchcheaper single-pass annotation andachieves a performance on a par with theextractor trained on multi-pass annotateddata.
Furthermore, we show that giventhe same amount of human labor, thebetter way to do relation annotation is notto annotate with high-cost qualityassurance, but to annotate more.1.
IntroductionRelation Extraction aims at detecting andcategorizing semantic relations between pairs ofentities in text.
It is an important NLP task thathas many practical applications such asanswering factoid questions, building knowledgebases and improving web search.Supervised methods for relation extractionhave been studied extensively since richannotated linguistic resources, e.g.
the AutomaticContent Extraction1 (ACE) training corpus, werereleased.
We will give a summary of relatedmethods in section 2.
Those methods rely onaccurate and complete annotation.
To obtain highquality annotation, the common wisdom is to let1 http://www.itl.nist.gov/iad/mig/tests/ace/two annotators independently annotate a corpus,and then asking a senior annotator to adjudicatethe disagreements 2 .
This annotation procedureroughly requires 3 passes3 over the same corpus.Therefore it is very expensive.
The ACE 2005annotation on relations is conducted in this way.In this paper, we analyzed a snapshot of ACEtraining data and found that each annotatormissed a significant fraction of relation mentionsand annotated some spurious ones.
We foundthat it is possible to separate most missingexamples from the vast majority of true-negativeunlabeled examples, and in contrast, most of therelation mentions that are adjudicated asincorrect contain useful expressions for learninga relation extractor.
Based on this observation,we propose an algorithm that purifies negativeexamples and applies transductive inference toutilize missing examples during the trainingprocess on the single-pass annotation.
Resultsshow that the extractor trained on single-passannotation with the proposed algorithm has aperformance that is close to an extractor trainedon the 3-pass annotation.
We further show thatthe proposed algorithm trained on a single-passannotation on the complete set of documents hasa higher performance than an extractor trained on3-pass annotation on 90% of the documents inthe same corpus, although the effort of doing asingle-pass annotation over the entire set costsless than half that of doing 3 passes over 90% ofthe documents.
From the perspective of learninga high-performance relation extractor, it suggeststhat a better way to do relation annotation is notto annotate with a high-cost quality assurance,but to annotate more.2 The senior annotator also found some missing examples asshown in figure 1.3 In this paper, we will assume that the adjudication pass hasa similar cost compared to each of the two first-passes.
Theadjudicator may not have to look at as many sentences as anannotator, but he is required to review all instances found byboth annotators.
Moreover, he has to be more skilled andmay have to spend more time on each instance to be able toresolve disagreements.1942.
Background2.1 Supervised Relation ExtractionOne of the most studied relation extraction tasksis the ACE relation extraction evaluationsponsored by the U.S. government.
ACE 2005defined 7 major entity types, such as PER(Person), LOC (Location), ORG (Organization).A relation in ACE is defined as an ordered pairof entities appearing in the same sentence whichexpresses one of the predefined relations.
ACE2005 defines 7 major relation types and morethan 20 subtypes.
Following previous work, weignore sub-types in this paper and only evaluateon types when reporting relation classificationperformance.
Types include General-affiliation(GEN-AFF), Part-whole (PART-WHOLE),Person-social (PER-SOC), etc.
ACE provides alarge corpus which is manually annotated withentities (with coreference chains between entitymentions annotated), relations, events andvalues.
Each mention of a relation is tagged witha pair of entity mentions appearing in the samesentence as its arguments.
More details about theACE evaluation are on the ACE official website.Given a sentence s and two entity mentionsarg1 and arg2 contained in s, a candidate relationmention r with argument arg1 preceding arg2 isdefined as r=(s, arg1, arg2).
The goal of RelationDetection and Classification (RDC) is todetermine whether r expresses one of the typesdefined.
If so, classify it into one of the types.Supervised learning treats RDC as aclassification problem and solves it withsupervised Machine Learning algorithms such asMaxEnt and SVM.
There are two commonlyused learning strategies (Sun et al 2011).
Givenan annotated corpus, one could apply a flatlearning strategy, which trains a single multi-class classifier on training examples labeled asone of the relation types or not-a-relation, andapply it to determine its type or output not-arelation for each candidate relation mentionduring testing.
The examples of each type are therelation mentions that are tagged as instances ofthat type, and the not-a-relation examples areconstructed from pairs of entities that appear inthe same sentence but are not tagged as any ofthe types.
Alternatively, one could apply ahierarchical learning strategy, which trains twoclassifiers, a binary classifier RD for relationdetection and the other a multi-class classifier RCfor relation classification.
RD is trained bygrouping tagged relation mentions of all types aspositive instances and using all the not-a-relationcases (same as described above) as negativeexamples.
RC is trained on the annotatedexamples with their tagged types.
During testing,RD is applied first to identify whether anexample expresses some relation, then RC isapplied to determine the most likely type only ifit is detected as correct by RD.State-of-the-art supervised methods forrelation extraction also differ from each other ondata representation.
Given a relation mention,feature-based methods (Miller et al 2000;Kambhatla, 2004; Boschee et al 2005;Grishman et al 2005; Zhou et al 2005; Jiangand Zhai, 2007; Sun et al 2011) extract a richlist of structural, lexical, syntactic and semanticfeatures to represent it; in contrast, the kernelbased methods (Zelenko et al 2003; Bunescuand Mooney, 2005a; Bunescu and Mooney,2005b; Zhao and Grishman, 2005; Zhang et al2006a; Zhang et al 2006b; Zhou et al 2007;Qian et al 2008) represent each instance with anobject such as augmented token sequences or aparse tree, and used a carefully designed kernelfunction, e.g.
subsequence kernel (Bunescu andMooney, 2005b) or convolution tree kernel(Collins and Duffy, 2001),  to calculate theirsimilarity.
These objects are usually augmentedwith features such as semantic features.In this paper, we use the hierarchical learningstrategy since it simplifies the problem by lettingus focus on relation detection only.
The relationclassification stage remains unchanged and wewill show that it benefits from improveddetection.
For experiments on both relationdetection and relation classification, we useSVM4 (Vapnik 1998) as the learning algorithmsince it can be extended to support transductiveinference as discussed in section 4.3.
However,for the analysis in section 3.2 and the purificationpreprocess steps in section 4.2, we use aMaxEnt5 model since it outputs probabilities6 forits predictions.
For the choice of features, we usethe full set of features from Zhou et al(2005)since it is reported to have a state-of-the-artperformance (Sun et al 2011).2.2 ACE 2005 annotationThe ACE 2005 training data contains 599 articles4 SVM-Light is used.
http://svmlight.joachims.org/5 OpenNLP MaxEnt package is used.http://maxent.sourceforge.net/about.html6 SVM also outputs a value associated with each prediction.However, this value cannot be interpreted as probability.195from newswire, broadcast news, weblogs, usenetnewsgroups/discussion forum, conversationaltelephone speech and broadcast conversations.The annotation process is conducted as follows:two annotators working independently annotateeach article and complete all annotation tasks(entities, values, relations and events).
After twoannotators both finished annotating a file, alldiscrepancies are then adjudicated by a seniorannotator.
This results in a high-qualityannotation file.
More details can be found in thedocumentation of ACE 2005 MultilingualTraining Data V3.0.Since the final release of the ACE trainingcorpus only contains the final adjudicatedannotations, in which all the traces of the twofirst-pass annotations are removed, we use asnapshot of almost-finished annotation, ACE2005 Multilingual Training Data V3.0, for ouranalysis.
In the remainder of this paper, we willcall the two independent first-passes ofannotation fp1 and fp2.
The higher-quality datadone by merging fp1 and fp2 and then havingdisagreements adjudicated by the seniorannotator is called adj.
From this corpus, weremoved the files that have not been completedfor all three passes.
On the final corpusconsisting of 511 files, we can differentiate theannotations on which the three annotators haveagreed and disagreed.A notable fact of ACE relation annotation isthat it is done with arguments from the list ofannotated entity mentions.
For example, in arelation mention tyco's ceo and president denniskozlowski which expresses an EMP-ORGrelation, the two arguments tyco and denniskozlowski must have been tagged as entitymentions previously by the annotator.
Since fp1and fp2 are done on all tasks independently, theirdisagreement on entity annotation will bepropagated to relation annotation; thus we needto deal with these cases specifically.3.
Analysis of data annotation3.1 General statisticsAs discussed in section 2, relation mentions areannotated with entity mentions as arguments, andthe lists of annotated entity mentions vary in fp1,fp2 and adj.
To estimate the impact propagatedfrom entity annotation, we first calculate the ratioof overlapping entity mentions between entitiesannotated in fp1/fp2 with adj.
We found thatfp1/fp2 each agrees with adj on around 89% ofthe entity mentions.
Following up, we checkedthe relation mentions7 from fp1 and fp2 againstthe adjudicated list of entity mentions from adjand found that 682 and 665 relation mentionsrespectively have at least one argument whichdoesn?t appear in the list of adjudicated entitymentions.Given the list of relation mentions with botharguments appearing in the list of adjudicatedentity mentions, figure 1 shows the inter-annotator agreement of the ACE 2005 relationannotation.
In this figure, the three circlesrepresent the list of relation mentions in fp1, fp2and adj, respectively.30651486 1525645 53847383fp1 fp2adjFigure 1.
Inter-annotator agreement of ACE 2005 relationannotation.
Numbers are the distinct relation mentionswhose both arguments are in the list of adjudicated entitymentions.It shows that each annotator missed asignificant number of relation mentionsannotated by the other.
Considering that weremoved 682/665 relation mentions from fp1/fp2because we generate this figure based on the listof adjudicated entity mentions, we estimate thatfp1 and fp2 both missed around 18.3-28.5%8 ofthe relation mentions.
This clearly shows thatboth of the annotators missed a significantfraction of the relation mentions.
They alsoannotated some spurious relation mentions (asadjudicated in adj), although the fraction issmaller (close to 10% of all relation mentions inadj).ACE 2005 relation annotation guidelines(ACE English Annotation Guidelines forRelations, version 5.8.3) defined 7 syntacticclasses and the other class.
We plot thedistribution of syntactic classes of the annotated7 This is done by selecting the relation mentions whose botharguments are in the list of adjudicated entity mentions.8 We calculate the lower bound by assuming that the 682relation mentions removed from fp1 are found in fp2,although with different argument boundary and headwordtagged.
The upper bound is calculated by assuming that theyare all irrelevant and erroneous relation mentions.196relations in figure 2 (3 of the classes, accountingtogether for less than 10% of the cases, areomitted) and the other class.
It seems that it isgenerally easier for the annotators to find andagree on relation mentions of the typePreposition/PreMod/Possessives but harder tofind and agree on the ones belonging to Verbaland Other.
The definition and examples of thesesyntactic classes can be found in the annotationguidelines.In the following sections, we will show theanalysis on fp1 and adj since the result is similarfor fp2.Figure 2.
Percentage of examples of major syntactic classes.3.2 Why the differences?To understand what causes the missingannotations and the spurious ones, we needmethods to find how similar/different the falsepositives are to true positives and also howsimilar/different the false negatives (missingannotations) are to true negatives.
If we adopt agood similarity metric, which captures thestructural, lexical and semantic similaritybetween relation mentions, this analysis will helpus to understand the similarity/difference from anextraction perspective.We use a state-of-the-art feature space (Zhouet al 2005) to represent examples (including allcorrect examples, erroneous ones and untaggedexamples) and use MaxEnt as the weightlearning model since it shows competitiveperformance in relation extraction (Jiang andZhai, 2007) and outputs probabilities associatedwith each prediction.
We train a MaxEnt modelfor relation detection on true positives and truenegatives, which respectively are the subset ofcorrect examples annotated by fp1 (andadjudicated as correct ones) and negativeexamples that are not annotated in adj, and use itto make predictions on the mixed pool of correctexamples, missing examples and spurious ones.To illustrate how distinguishable the missingexamples (false negatives) are from the truenegative ones, 1) we apply the MaxEnt model onboth false negatives and true negatives, 2) putthem together and rank them by the model-predicted probabilities of being positive, 3)calculate their relative rank in this pool.
We plotthe Cumulative distribution of frequency (CDF)of the ranks (as percentages in the mixed pools)of false negatives in figure 3.
We took similarsteps for the spurious ones (false positives) andplot them in figure 3 as well (However, they areranked by model-predicted probabilities of beingnegative).Figure 3: cumulative distribution of frequency (CDF) of therelative ranking of model-predicted probability of beingpositive for false negatives in a pool mixed of falsenegatives and true negatives; and the CDF of the relativeranking of model-predicted probability of being negative forfalse positives in a pool mixed of false positives and truepositives.For false negatives, it shows a highly skeweddistribution in which around 75% of the falsenegatives are ranked within the top 10%.
Thatmeans the missing examples are lexically,structurally or semantically similar to correctexamples, and are distinguishable from the truenegative examples.
However, the distribution offalse positives (spurious examples) is close touniform (flat curve), which means they aregenerally indistinguishable from the correctexamples.3.3 Categorize annotation errorsThe automatic method shows that the errors(spurious annotations) are very similar to thecorrect examples but provides little clue as towhy that is the case.
To understand their causes,we sampled 65 examples from fp1 (10% of the645 errors), read the sentences containing these197Category PercentageExampleRelationTypeSampled text of spurious examples in fp1Notes (examples are similarones in adj for comparison)Duplicaterelationmention forcoreferentialentity mentions49.2% ORG-AFF?
his budding friendship with US      PresidentGeorge W. Bush in the face of ??
his budding friendshipwith US      President GeorgeW.
Bush in the face of ?Correct 20%PHYSHundreds of thousands of demonstrators took tothe streets in Britain?PER-SOCThe dead included the quack doctor, 55-year-oldNityalila Naotia, his teenaged son and?
(Symmetric relation)The dead included the quackdoctor, 55-year-old NityalilaNaotia, his teenaged sonArgument notin list15.4%PER-SOCPutin had even secretly invited British PrimeMinister Tony Blair, Bush's staunchest backerin the war on Iraq?Violatereasonablereader rule6.2% PHYS"The      amazing thing is they are going to turnSan Francisco into ground zero for every criminalwho wants to profit at their chosen profession",Paredes said.Errors 6.1%PART-WHOLE?a likely candidate to run Vivendi Universal'sentertainment unit in the United States?Arguments are taggedreversedPART-WHOLEKhakamada argued that the UnitedStates would also need Russia's help "to make thenew Iraqi government seem legitimate.Relation type errorillegalpromotionthrough?blocked?categories3%PHYSUp to 20,000 protesters thronged the plazas andstreets of San Francisco, where?Up to 20,000 protestersthronged the plazas andstreets of San Francisco,where?Table 1.
Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentageof the examples in each category.
In the sample text, red text (also marked with dotted underlines) shows head words of thefirst arguments and the underlined text shows head words of the second arguments.erroneous relation mentions and compared themto the correct relation mentions in the samesentence; we categorized these examples andshow them in table 1.
The most common type oferror is duplicate relation mention forcoreferential entity mentions.
The first row intable 1 shows an example, in which there is arelation ORG-AFF tagged between US andGeorge W. Bush in adj.
Because President andGeorge W. Bush are coreferential, the example<US, President > from fp1 is adjudicated asincorrect.
This shows that if a relation isexpressed repeatedly across relation mentionswhose arguments are coreferential, theadjudicator only tags one of the relation mentionsas correct, although the other is correct too.
Thisshared the same principle with another type oferror illegal promotion through ?blocked?categories 9  as defined in the annotationguideline.
The second largest category is correct,by which we mean the example is a correctrelation mention and the adjudicator made a9 For example, in sentence Smith went to a hotel in Brazil,(Smith, hotel) is a taggable PHYS Relation but (Smith,Brazil) is not, because to get the second relationship, onewould have to ?promote?
Brazil through hotel.
For theprecise definition of annotation rules, please refer to ACE(Automatic Content Extraction) English AnnotationGuidelines for Relations, version 5.8.3.mistake.
The third largest category is argumentnot in list, by which we mean that at least one ofthe arguments is not in the list of adjudicatedentity mentions.Based on Table 1, we can see that as many as72%-88% of the examples which are adjudicatedas incorrect are actually correct if viewed from arelation learning perspective, since most of themcontain informative expressions for taggingrelations.
The annotation guideline is designedto ensure high quality while not imposing toomuch burden on human annotators.
To reduceannotation effort, it defined rules such as illegalpromotion through ?blocked?
categories.
Theannotators?
practice suggests that they arefollowing another rule not to annotate duplicaterelation mention for coreferential entitymentions.
This follows the similar principle ofreducing annotation effort but is not explicitlystated in the guideline: to avoid propagation of arelation through a coreference chain.
However,these examples are useful for learning more waysto express a relation.
Moreover, even for theerroneous examples (as shown in table 1 asviolate reasonable reader rule and errors), mostof them have some level of similar structures orsemantics to the targeted relation.
Therefore, it isvery hard to distinguish them without humanproofreading.198Exp # TrainingdataTestingdataDetection (%) Classification (%)Precision Recall F1 Precision Recall F11 fp1 adj 83.4 60.4 70.0 75.7 54.8 63.62 fp2 adj 83.5 60.5 70.2 76.0 55.1 63.93 adj adj 80.4 69.7 74.6 73.4 63.6 68.2Table 2.
Performance of RDC trained on fp1/fp2/adj, and tested on adj.3.4 Why missing annotations and howmany examples are missing?For the large number of missing annotations,there are a couple of possible reasons.
Onereason is that it is generally easier for a humanannotator to annotate correctly given a well-defined guideline, but it is hard to ensurecompleteness, especially for a task like relationextraction.
Furthermore, the ACE 2005annotation guideline defines more than 20relation subtypes.
These many subtypes make ithard for an annotator to keep all of them in mindwhile doing the annotation, and thus it isinevitable that some examples are missed.Here we proceed to approximate the numberof missing examples given limited knowledge.Let each annotator annotate n examples andassume that each pair of annotators agrees on acertain fraction p of the examples.
Assuming theexamples are equally likely to be found by anannotator, therefore the total number of uniqueexamples found by ?
annotators is ?
(1???=0?)??.
If we had an infinite number of annotators(?
?
?
), the total number of unique exampleswill be?
?, which is the upper bound of the totalnumber of examples.
In the case of the ACE2005 relation mention annotation, since the twoannotators annotate around 4500 examples andthey agree on 2/3 of them, the total number of allpositive examples is around 6750.
This is closeto the number of relation mentions in theadjudicated list: 6459.
Here we assume theadjudicator is doing a more complex task than anannotator, resolving the disagreements andcompleting the annotation (as shown in figure 1).The assumption of the calculation is a littlecrude but reasonable given the limited number ofpasses of annotation we have.
Recent research (Jiet al2010) shows that, by adding annotators forIE tasks, the merged annotation tends toconverge after having 5 annotators.
Tounderstand the annotation behavior better, inparticular whether annotation will converge afteradding a few annotators, more passes ofannotation need to be collected.
We leave this asfuture work.4.
Relation extraction with low-costannotation4.1 Baseline algorithmTo see whether a single-pass annotation is usefulfor relation detection and classification, we did5-fold cross validation (5-fold CV) with each offp1, fp2 and adj as the training set, and tested onadj.
The experiments are done with the same 511documents we used for the analysis.
As shown intable 2, we did 5-fold CV on adj for experiment3.
For fairness, we use settings similar to 5-foldCV for experiment 1 and 2.
Take experiment 1 asan example: we split both of fp1 and adj into 5folds, use 4 folds from fp1 as training data, and 1fold from adj as testing data and does one train-test cycle.
We rotate the folds (both training andtesting) and repeat 5 times.
The final results areaveraged over the 5 runs.
Experiment 2 wasconducted similarly.
In the reminder of the paper,5-fold CV experiments are all conducted in thisway.Table 2 shows that a relation tagger trained onthe single-pass annotated data fp1 performsworse than the one trained on merged andadjudicated data adj, with 4.6 points lower Fmeasure in relation detection, and 4.6 pointslower relation classification.
For detection,precision on fp1 is 3 points higher than on adjbut recall is much lower (close to 10 points).
Therecall difference shows that the missingannotations contain expressions that can help tofind more correct examples during testing.
Thesmall precision difference indirectly shows thatthe spurious ones in fp1 (as adjudicated) do nothurt precision.
Performance on classificationshows a similar trend because the relationclassifier takes the examples predicted by thedetector as correct as its input.
Therefore, if thereis an error, it gets propagated to this stage.
Table2 also shows similar performance differencesbetween fp2 and adj.In the remainder of this paper, we will discussa few algorithms to improve a relation taggertrained on single-pass annotated data10.
Since we10 We only use fp1 and adj in the following experimentsbecause we observed that fp1 and fp2 are similar in generalin the analysis, though a fraction of the annotation in fp1199already showed that most of the spuriousannotations are not actually errors from anextraction perspective and table 2 shows thatthey do not hurt precision, we will only focus onutilizing the missing examples, in other words,training with an incomplete annotation.4.2 Purify the set of negative examplesAs discussed in section 2, traditional supervisedmethods find all pairs of entity mentions thatappear within a sentence, and then use the pairsthat are not annotated as relation mentions as thenegative examples for the purpose of training arelation detector.
It relies on the assumption thatthe annotators annotated all relation mentionsand missed no (or very few) examples.
However,this is not true for training on a single-passannotation, in which a significant portion ofrelation mentions are left not annotated.
If thisscheme is applied, all of the correct pairs whichthe annotators missed belong to this ?negative?category.
Therefore, we need a way to purify the?negative?
set of examples obtained by thisconventional approach.Li and Liu (2003) focuses on classifyingdocuments with only positive examples.
Theiralgorithm initially sets all unlabeled data to benegative and trains a Rocchio classifier, selectsnegative examples which are closer to thenegative centroid than positive centroid as thepurified negative examples, and then retrains themodel.
Their algorithm performs well for textclassification.
It is based on the assumption thatthere are fewer unannotated positive examplesthan negative ones in the   unlabeled set, so truenegative examples still dominate the set of noisy?negative?
examples in the purification step.Based on the same assumption, our purificationprocess consists of the following steps:1) Use annotated relation mentions aspositive examples; construct all possiblerelation mentions that are not annotated, andinitially set them to be negative.
We call thisnoisy data set D.2) Train a MaxEnt relation detection modelMdet on D.3) Apply Mdet  on all unannotatedexamples, and rank them by the model-predicted probabilities of being positive,4) Remove the top N examples from D.These preprocessing steps result in a purifieddata set  ?????.
We can use ?????
for the normaland fp2 is different.
Moreover, algorithms trained on themshow similar performance.training process of a supervised relationextraction algorithm.The algorithm is similar to Li and Liu 2003.However, we drop a few noisy examples insteadof choosing a small purified subset since we haverelatively few false negatives compared to theentire set of unannotated examples.
Moreover,after step 3, most false negatives are clusteredwithin the small region of top ranked exampleswhich has a high model-predicated probability ofbeing positive.
The intuition is similar to whatwe observed from figure 3 for false negativessince we also observed very similar distributionusing the model trained with noisy data.Therefore, we can purify negatives by removingexamples in this noisy subset.However, the false negatives are still mixedwith true negatives.
For example, still slightlymore than half of the top 2000 examples are truenegatives.
Thus we cannot simply flip theirlabels and use them as positive examples.
In thefollowing section, we will use them in the formof unlabeled examples to help train a bettermodel.4.3 Transductive inference on unlabeledexamplesTransductive SVM (Vapnik, 1998; Joachims,1999) is a semi-supervised learning methodwhich learns a model from a data set consistingof both labeled and unlabeled examples.Compared to its popular antecedent SVM, it alsolearns a maximum margin classificationhyperplane, but additionally forces it to separatea set of unlabeled data with large margin.
Theoptimization function of Transductive  SVM(TSVM) is the following:Figure 4.
TSVM optimization function for non-separablecase (Joachims, 1999)TSVM can leverage an unlabeled set ofexamples to improve supervised learning.
Asshown in section 3, a significant number ofrelation mentions are missing from the single-pass annotation data.
Although it is not possibleto find all missing annotations without humaneffort, we can improve the model by further200utilizing the fact that some unannotated examplesshould have been annotated.The purification process discussed in theprevious section removes N examples whichhave a high density of false negatives.
We furtherutilize the N examples as follows:1) Construct a training corpus ???????
from?????
by taking a random sample11 of N*(1-p)/p (p is the ratio of annotated examples toall examples; p=0.05 in fp1) negativelylabeled examples in ?????
and setting them tobe unlabeled.
In addition, the N examplesremoved by the purification process are addedback as unlabeled examples.2) Train TSVM on ??????
?.The second step trained a model whichreplaced the detection model in the hierarchicaldetection-classification learning scheme we used.We will show in the next section that thisimproves the model.5.
ExperimentsExperiments were conducted over the same set ofdocuments on which we did analysis: the 511documents which have completed annotation inall of the fp1, fp2 and adj from the ACE 2005Multilingual Training Data V3.0.
Toreemphasize, we apply the hierarchical learningscheme and we focus on improving relationdetection while keeping relation classificationunchanged (results show that its performance isimproved because of the improved detection).We use SVM as our learning algorithm with thefull feature set from Zhou et al(2005).Baseline algorithm: The relation detector isunchanged.
We follow the common practice,which is to use annotated examples as positiveones and all possible untagged relation mentionsas negative ones.
We sub-sampled the negativedata by ?
since that shows better performance.+purify:  This algorithm adds an additionalpurification preprocessing step (section 4.2)before the hierarchical learning RDC algorithm.After purification, the RDC algorithm is trainedon the positive examples and purified negativeexamples.
We set N=200012 in all experiments.11 We included this large random sample so that the balanceof positive to negative examples in the unlabeled set wouldbe similar to that of the labeled data.
The test data is notincluded in the unlabeled set.12 We choose 2000 because it is close to the number ofrelations missed from each single-pass annotation.
Inpractice, it contains more than 70% of the false negatives,and it is less than 10% of the unannotated examples.
Toestimate how many examples are missing (section 3.4), one+tSVM: First, the same purification process of+purify is applied.
Then we follow the stepsdescribed in section 4.3 to construct the set ofunlabeled examples, and set althe rest ofpurified negative examples to be negative.Finally, we train TSVM on both labeled andunlabeled data and replace the relation detectionin the RDC algorithm.
The relation classificationis unchanged.Table 3 shows the results.
All experiments aredone with 5-fold cross validation13 using testingdata from adj.
The first three rows showexperiments trained on fp1, and the last row(ADJ) shows the unmodified RDC algorithmtrained on adj for comparison.
The purificationof negative examples shows significantperformance gain, 3.7% F1 on relation detectionand 3.4% on relation classification.
The precisiondecreases but recall increases substantially sincethe missing examples are not treated asnegatives.
Experiment shows that the purificationprocess removes more than 60% of the falsenegatives.
Transductive SVM further improvedperformance by a relatively small margin.
Thisshows that the latent positive examples can helprefine the model.
Results also show thattransductive inference can find around 17% ofmissing relation mentions.
We notice that theperformance of relation classification isimproved since by improving relation detection,some examples that do not express a relation areremoved.
The classification performance onsingle-pass annotation is close to the one trainedon adj due to the help from a better relationdetector trained with our algorithm.We also did 5-fold cross validation with amodel trained on a fraction of the 4/5 (4 folds) ofadj data (each experiment shown in table 4 uses4 folds of adj documents for training since onefold is left for cross validation).
The documentsare sampled randomly.
Table 4 shows results forvarying training data size.
Compared to theresults shown in the ?+tSVM?
row of table 3, wecan see that our best model trained on single-passannotation outperforms SVM trained on 90% ofthe dual-pass, adjudicated data in both relationdetection and classification, although it costs lessthan half the 3-pass annotation.
This suggeststhat given the same amount of human effort forshould perform multiple passes of independent annotationon a small dataset and measure inter-annotator agreements.13 Details about the settings for 5-fold cross validation are insection 4.1.201AlgorithmDetection (%) Classification (%)Precision Recall F1 Precision Recall F1Baseline 83.4 60.4 70.0 75.7 54.8 63.6+purify 76.8 70.9 73.7 69.8 64.5 67.0+tSVM 76.4 72.1 74.2 69.4 65.2 67.2ADJ (on adj) 80.4 69.7 74.6 73.4 63.6 68.2Table 3.
5-fold cross-validation results.
All are trained on fp1 (except the last row showing the unchanged algorithm trainedon adj for comparison), and tested on adj.
McNemar's test show that the improvement from +purify to +tSVM, and from+tSVM to ADJ are statistically significant (with p<0.05).Percentage ofadj usedDetection (%) Classification (%)Precision Recall F1 Precision Recall F160% ?
4/5 86.9 41.2 55.8 78.6 37.2 50.570% ?
4/5 85.5 51.3 64.1 77.7 46.6 58.280% ?
4/5 83.3 58.1 68.4 75.8 52.9 62.390% ?
4/5 82.0 64.9 72.5 74.9 59.4 66.2Table 4.
Performance with SVM trained on a fraction of adj.
It shows 5 fold cross validation results.relation annotation, annotating more documentswith single-pass offers advantages overannotating less data with high quality assurance(dual passes and adjudication).6.
Related workDligach et al(2010) studied WSD annotationfrom a cost-effectiveness viewpoint.
Theyshowed empirically that, with same amount ofannotation dollars spent, single-annotation isbetter than dual-annotation and adjudication.
Thecommon practice for quality control of WSDannotation is similar to Relation annotation.However, the task of WSD annotation is verydifferent from relation annotation.
WSD requiresthat every example must be assigned some tag,whereas that is not required for relation tagging.Moreover, relation tagging requires identifyingtwo arguments and correctly categorizing theirtypes.The purified approach applied in this paper isrelated to the general framework of learning frompositive and unlabeled examples.
Li and Liu(2003) initially set alunlabeled data to benegative and train a Rocchio classifier, thenselect negative examples which are closer to thenegative centroid than positive centroid as thepurified negative examples.
We share a similarassumption with Li and Liu (2003) but we use adifferent method to select negative examplessince the false negative examples show a veryskewed distribution, as described in section 5.2.Transductive SVM was introduced by Vapnik(1998) and later refined in Joachims (1999).
Afew related methods were studied on the subtaskof relation classification (the second stage of thehierarchical learning scheme) in Zhang (2005).Chan and Roth (2011) observed the similarphenomenon that ACE annotators rarelyduplicate a relation link for coreferentialmentions.
They use an evaluation scheme toavoid being penalized by the relation mentionswhich are not annotated because of this behavior.7.
ConclusionWe analyzed a snapshot of the ACE 2005relation annotation and found that each single-pass annotation missed around 18-28% ofrelation mentions and contains around 10%spurious mentions.
A detailed analysis showedthat it is possible to find some of the falsenegatives, and that most spurious cases areactually correct examples from a systembuilder?s perspective.
By automatically purifyingnegative examples and applying transductiveinference on suspicious examples, we can train arelation classifier whose performance iscomparable to a classifier trained on the dual-annotated and adjudicated data.
Furthermore, weshow that single-pass annotation is more cost-effective than annotation with high qualityassurance.AcknowledgmentsSupported by the Intelligence AdvancedResearch Projects Activity (IARPA) via AirForce Research Laboratory (AFRL) contractnumber FA8650-10-C-7058.
The U.S.Government is authorized to reproduce anddistribute reprints for Governmental purposesnotwithstanding any copyright annotationthereon.
The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing theofficial policies or endorsements, eitherexpressed or implied, of IARPA, AFRL, or theU.S.
Government.202ReferencesACE.
http://www.itl.nist.gov/iad/mig/tests/ace/ACE (Automatic Content Extraction) EnglishAnnotation Guidelines for Relations, version 5.8.3.2005.  http://projects.ldc.upenn.edu/ace/.ACE 2005 Multilingual Training Data V3.0.
2005.LDC2005E18.
LDC Catalog.Elizabeth Boschee, Ralph Weischedel, and AlexZamanian.
2005.
Automatic information extraction.In Proceedings of the International Conference onIntelligence Analysis.Razvan C. Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kenrel for relationextraction.
In Proceedings of HLT/EMNLP-2005.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
InProceedings of NIPS-2005.Yee Seng Chan and Dan Roth.
2011.
ExploitingSyntactico-Semantic Structures for RelationExtraction.
In Proceedings of ACL-2011.Michael Collins and Nigel Duffy.
ConvolutionKernels for Natural Language.
In Proceedings ofNIPS-2001.Dmitriy Dligach, Rodney D. Nielsen and MarthaPalmer.
2010.
To annotate more accurately or toannotate more.
In Proceedings of Fourth LinguisticAnnotation Workshop at ACL 2010Ralph Grishman, David Westbrook and AdamMeyers.
2005.
NYU?s English ACE 2005 SystemDescription.
In Proceedings of ACE 2005Evaluation WorkshopScott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statisticalparsing to extract information from text InProceedings of NAACL-2010.Heng Ji, Ralph Grishman, Hoa Trang Dang and KiraGriffitt.
2010.
An Overview of the TAC2010Knowledge Base Population Track.
In Proceedingsof TAC-2010Jing  Jiang  and ChengXiang Zhai.
2007.
A systematicexploration of the feature space for relationextraction.
In Proceedings of HLT-NAACL-2007.Thorsten Joachims.
1999.
Transductive Inference forText Classification using Support VectorMachines.
In Proceedings of ICML-1999.Nanda Kambhatla.
2004.
Combining lexical,syntactic, and semantic features with maximumentropy models for information extraction.
InProceedings of ACL-2004Xiao-Li Li and Bing Liu.
2003.
Learning to classifytext using positive and unlabeled data.
InProceedings of IJCAI-2003.Longhua Qian,  Guodong Zhou,  Qiaoming Zhu andPeide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semanticrelation extraction .
In Proc.
of COLING-2008.Ang Sun, Ralph Grishman and Satoshi Sekine.
2011.Semi-supervised Relation Extraction with Large-scale Word Clustering.
In Proceedings of ACL-2011.Vladimir N. Vapnik.
1998.
Statistical LearningTheory.
John Wiley.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research.Min Zhang, Jie Zhang and Jian Su.
2006a.
Exploringsyntactic features for relation extraction using aconvolution tree kernel, In Proceedings of HLT-NAACL-2006.Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.2006b.
A composite kernel to extract relationsbetween entities with both flat and structuredfeatures.
In Proceedings of COLING-ACL-2006.Zhu Zhang.
2005.
Mining Inter-Entity SemanticRelations Using Improved Transductive Learning.In Proceedings of ICJNLP-2005.Shubin Zhao and Ralph Grishman, 2005.
ExtractingRelations with Integrated Information Using Kernel Methods.
In Proceedings of ACL-2005.Guodong Zhou, Jian Su, Jie Zhang and Min Zhang.2005.
Exploring various knowledge in relationextraction.
In Proceedings of ACL-2005.Guodong Zhou, Min Zhang, DongHong Ji, andQiaoMing Zhu.
2007.
Tree kernel-based relationextraction with context-sensitive structured parsetree information.
In Proceedings ofEMNLP/CoNLL-2007.203
