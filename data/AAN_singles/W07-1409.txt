Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 54?59,Prague, June 2007. c?2007 Association for Computational LinguisticsOn the Role of Lexical and World Knowledge in RTE3Peter Clark1, William R. Murray1, John Thompson1, Phil Harrison1,Jerry Hobbs2, Christiane Fellbaum31Boeing Phantom Works, The Boeing Company, Seattle, WA 981242USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 902923Princeton University, NJ 08544{peter.e.clark,william.r.murray,john.a.thompson,philip.harrison}@boeing.com,hobbs@isi.edu, fellbaum@clarity.princeton.eduAbstractTo score well in RTE3, and even more soto create good justifications for entailments,substantial lexical and world knowledge isneeded.
With this in mind, we present ananalysis of a sample of the RTE3 positiveentailment pairs, to identify where andwhat kinds of world knowledge are neededto fully identify and justify the entailment,and discuss several existing resources andtheir capacity for supplying that knowledge.We also briefly sketch the path we are fol-lowing to build an RTE system (Our im-plementation is very preliminary, scoring50.9% at the time of RTE).
The contribu-tion of this paper is thus a framework fordiscussing the knowledge requirementsposed by RTE and some exploration ofhow these requirements can be met.1 IntroductionThe Pascal RTE site defines entailment betweentwo texts T and H as holding "if, typically, a hu-man reading T would infer that H is most likelytrue" assuming "common human understanding oflanguage as well as common background knowl-edge."
While a few RTE3 entailments can be rec-ognized using simple syntactic matching, the ma-jority rely on significant amounts of this "commonhuman understanding" of lexical and world knowl-edge.
Our goal in this paper is to analyze what thatknowledge is, create a preliminary framework forit, and explore a few available sources for it.
In theshort term, such knowledge can be (and has been)used to drive semantic matching of the T and Hdependency/parse trees and their semantic repre-sentations, as many prior RTE systems perform,e.g., (Hickl et al, 2006).
In the long term, com-puters should be able to perform deep languageunderstanding to build a computational model ofthe scenario being described in T, to reason aboutthe entailment, answer further questions, and createmeaningful justifications.
With this longer termgoal in mind, it is useful to explore the types ofknowledge required.
It also gives a snapshot of thekinds of challenges that RTE3 poses.The scope of this paper is to examine the underly-ing lexical/world knowledge requirements of RTE,rather than the more syntactic/grammatical issuesof parsing, coreference resolution, named entityrecognition, punctuation, coordination, typographi-cal errors, etc.
Although there is a somewhat blurryline between the two, this separation is useful forbounding the analysis.
It should be noted that themore syntactic issues are themselves vast in RTE,but here we will not delve into them.
Instead, wewill perform a thought experiment in which theyhave been handled correctly.2 AnalysisBased on an analysis of 100 (25%) of the positiveentailments in the RTE3 test set, we have dividedthe knowledge requirements into several roughcategories, which we now present.
We then sum-marize the frequency with which examples in thissample fell into these categories.
The examplesbelow are fragments of the original test questions,abbreviated and occasionally simplified.2.1 Syntactic MatchingIn a few cases, entailment can be identified by syn-tactic matching of T and H, for example:54489.T "The Gurkhas come from Nepal and?
?489.H "The Gurkhas come from Nepal.
"Other examples include 299, 489, and 456.
Insome cases, the syntactic matching can be verycomplex, e.g., examples 152, 724.2.2 SynonymsSynonymy is often needed to recognize entailment,648.T "?go through ?
licencing procedures..."648.H "?go through the licencing processes.
"Other examples include 286 ("dismiss"/"throwout"), 37 ("begin?/"start"), 236 ("wildfire"/"bushfire"), and, arguably, 462 ("revenue"/"proceeds").2.3 Generalizations (Hypernyms)Similarly, subsumption (generalization) relation-ships between word senses need to be recognized(whether or not a fixed set of senses are used), eg.148.T "Beverly served...at WEDCOR"148.H "Beverly worked for WEDCOR.
"Others include 178 ("succumbed" as a kind of"killed"), and 453 ("take over" as a kind of "buy").2.4 Noun RedundancySometimes a noun in a compound can be dropped:607.T "single-run production process..."607.H "Single-run production..."Other examples include 269 ("increasing preva-lence of" ?
"increasing"), 604 ("mini-mill proc-ess" ?
"mini-mill"), and (at the phrase level) 668("all segments of the public" ?
"the public").2.5 Noun-Verb RelationsOften derivationally related nouns and verbs occurin the pairs.
To identify and justify the entailment,the relationship and its nature is needed, as in:480 "Marquez is a winner..." ?
"Marquez won..."Other examples include 286 ("pirated", "piracy"),and 75 ("invent", "invention").
In some cases, thedeverbal noun denotes the verb's event, in othercases it denotes one of the verb?s arguments (e.g.,"winner" as the subject/agent of a "win" event).2.6 Compound NounsSome examples require inferring the semantic rela-tion between nouns in a compound, e.g.,168 "Sirius CEO Karmazin" ?
"Karmazin is anexecutive of Sirius"583 "physicist Hawking" ?
"Hawking is a physi-cist"In some cases this is straightforward, others requiremore detailed knowledge of the entities involved.2.7 DefinitionsAlthough there is somewhat of a fuzzy boundarybetween word and world knowledge, we draw thisdistinction here.
Some examples of RTE pairswhich require knowing word meanings are:667 "?
found guilty..." ?
"?convicted..."328 "sufferers of coeliac disease..." ?
"coeliacs..."The second example is particularly interesting asmany readers (and computers) will not have en-countered the word "coeliacs" before, yet a personcan reasonably infer its meaning on the fly fromcontext and morphology - something challengingfor a machine to do.
Definitions of compoundnouns are also sometimes needed, e.g., ?familyplanning?
(612) and ?cough syrup?
(80).2.8 World Knowledge: GeneralA large number of RTE pairs require non-definitional knowledge about the way the world(usually) is, e.g.,:273 "bears kill people"  ?
"bears attack people"People recognize this entailment as they know(have heard about) how people might be killed bya bear, and hence can justify why the entailment isvalid.
(They know that the first step in the bearkilling a person is for the bear to attack that person.
)Some other examples are:499 "shot dead" ?
"murder"705 "under a contract with" ?
"cooperates with"721 "worked on the law" ?
"discussed the law"731 "cut tracts of forest" ?
"cut trees in the forest"732 "establishing tree farms"?
"trees were planted"639 "X's plans for reorganizing"?
"X intends to reorganize"328 "the diets must be followed by <person>"?
"the diets are for <person>"722 X and Y vote for Z ?
X and Y agree to Z.All these cases appeal to a person's world knowl-edge concerning the situation being described.552.9 World Knowledge: Core TheoriesIn addition to this more specific knowledge of theworld, some RTE examples appeal to more general,fundamental knowledge (e.g., space, time, plans,goals).
For example6.T "Yunupingu is one of the clan of..."6.H "Yunupingu is a member of..."appeals to a basic rule of set inclusion, and 10 (anegative entailment: "unsuccessfully sought elec-tion" ?
not elected) appeals to core notions ofgoals and achievement.
Several examples appeal tocore spatial knowledge, e.g.
:491.T "...come from the high mountains of Nepal.
"491.H "...come from Nepal.
"178.T "...3 people in Saskatchewan succumbed tothe storm.
"178.H "...a storm in Saskatchewan.
"491 appeals to regional inclusion (if X location Y,and Y is in Z, then X location Z), and 178 appealsto colocation (if X is at Y, and X physically inter-acts with Z, then Z is at Y).
Other spatial examplesinclude 236 ("around Sydney" ?
"near Sydney"),and 129 ("invasion of" ?
"arrived in").2.10 World Knowledge: Frames and ScriptsAlthough loosely delineated, another category ofworld knowledge concerns stereotypical places,situations and the events which occur in them, withvarious representational schemes proposed in theAI literature, e.g., Frames (Minsky 1985), Scripts(Schank 1983).
Some RTE examples require rec-ognizing the implicit scenario ("frame", "script",etc.)
which T describes to confirm the new facts orrelationships introduced in H are valid.
A first ex-ample is:358.T "Kiesbauer was target of a letter bomb..."358.H "A letter bomb was sent to Kiesbauer.
"A person recognizes H as entailed by T becausehe/she knows the "script" for letter bombing,which includes sending the bomb in the mail.
Thusa person could also recognize alternative verbs in358.H as valid (e.g., "mailed", "delivered") or in-valid (e.g., "thrown at", "dropped on"), eventhough these verbs are all strongly associated withwords in T. For a computer to fully explain theentailment, it would need similar knowledge.As a second example, consider:538.T "...the O. J. Simpson murder trial..."538.H "O. J. Simpson was accused of murder.
"Again, this requires knowing about trials: Thatthey involve charges, a defendant, an accusation,etc., in order to validate H as an entailed expansionof T. In this example, there is also a second twist toit as the noun phrase in 538.T equally supports thehypothesis "O. J. Simpson was murdered."
(e.g.,consider replacing "O.
J."
with "Nicole").
It is onlya reference elsewhere in the T sentence to "Simp-son's attorneys" that suggests Simpson is still alive,and hence couldn't have been the victim, and hencemust be the accused, that clarifies 538.H as beingcorrect, a highly complex chain of reasoning.As a third example, consider:736.T "In a security fraud case, Milken was sen-tenced to 10 years in prison.
"736.H "Milken was imprisoned for security fraud.
"This example is particularly interesting, as oneneeds to recognize security fraud as Milken's crime,a connection which not stated in T. A humanreader will recognize the notion of sentencing, andthus expect to see a convict and his/her crime, andhence can align these expectations with T, validat-ing H. Thus again, deep knowledge of sentencingis needed to understand and justify the entailment.Some other examples requiring world knowledgeto validate their expansions, include 623 ("narcot-ics-sniffing dogs" ?
"dogs are used to sniff outnarcotics"), and 11 ("the Nintendo release of thegame" ?
"the game is produced by Nintendo").2.11 Implicative VerbsSome RTE3 examples contain complement-takingverbs that make an implication (either positive ornegative) about the complement.
For example:668 "A survey shows that X..." ?
"X..."657 "...X was seen..." ?
"...X..."725 ?...decided to X..." ?
"...X..."716 "...have been unable to X..." ?
"...do not X"56Table 1: Frequency of different entailmentphenomena from a sample of 100 RTE3 pairs.In the first 3 the implication is positive, but in thelast the implication is negative.
(Nairn et al 2006)provide a detailed analysis of this type of behavior.In fact, this notion of implicature (one part of asentence making an implication about another part)extends beyond single verbs, and there are somemore complex examples in RTE3, e.g.
:453 "...won the battle to X..." ?
"...X..."784.T "X  reassures Russia it has nothing to fear..."784.H "Russia fears..."In this last example the implication behavior isquite complex: (loosely) If X reassures Y of Z,then Y is concerned about not-Z.2.12 Metonymy/TransferIn some cases, language allows us to replace aword (sense) with a closely related word (sense):708.T "Revenue from stores funded..."708.H "stores fund..."Rules for metonymic transfer, e.g., (Fass 2000),can be used to define which transfers are allowed.Another example is 723 ?
?pursue its drive to-wards X?
?
?
?pursue X?.2.13 Idioms/Protocol/SlangFinally, some RTE pairs rely on understandingidioms, slang, or special protocols used in lan-guage, for example:12 "Drew served as Justice.
Kennon returned toclaim Drew's seat"  ?
"Kennon served as Justice.
"486 "name, 1890-1970" ?
"name died in 1970"408 "takes the title of" ?
"is"688 "art finds its way back" ?
"art gets returned"The phrases in these examples all have specialmeaning which cannot be easily derived composi-tionally from their words, and thus require specialhandling within an entailment system.2.14 Frequency StatisticsTable 1 shows the number of positive entailmentsin our sample of 100 that fell into the differentcategories (some fell into several).
While there is acertain subjectivity in the boundaries of the catego-ries, the most significant observation is that veryfew entailments depend purely on syntactic ma-nipulation and simple lexical knowledge (syno-nyms, hypernyms), and that the vast majority ofentailments require significant world knowledge,highlighting one of the biggest challenges of RTE.In addition, the category of general world knowl-edge -- small, non-definitional facts about the waythe world (usually) is -- is the largest, suggestingthat harvesting and using this kind of knowledgeshould continue to be a priority for improving per-formance on RTE-style tasks.3 Sources of World KnowledgeWhile there are many potential sources of theknowledge that we have identified, we describethree in a bit more detail and how they relate to theearlier analysis.3.1 WordNetWordNet (Fellbaum, 1998) is one of the most ex-tensively used resources in RTE already and incomputational linguistics in general.
Despite somewell-known problems, it provides broad coverageof several key relationships between word senses(and words), in particular for synonyms, hy-pernyms (generalizations), meronyms (parts), andsemantically (?morphosemantically?)
relatedforms.
From the preceding analysis, WordNet doescontain the synonyms {"procedure","process"},{"dismiss","throw out"}, {"begin","start"}, butdoes not contain the compound "wild fire" and(strictly correctly) does not contain {"reve-nue","proceeds"} as synonyms.
In addition, the57three hypernyms mentioned in the earlier analysisare in WordNet.
WordNet alo links (via the ?mor-phosemantic?
link) the 3 noun-verb pairs men-tioned earlier (win/winner, pirated/piracy, in-vent/invention) ?
however it does not currentlydistinguish the nature of that link (e.g., agent, re-sult, event).
WordNet is currently being expandedto include this information, as part of theAQUAINT program.Two independently developed versions of theWordNet glosses expressed in logic are also avail-able: Extended WordNet (Moldovan and Rus,2001) and a version about to be released withWordNet3.0 (again developed under AQUAINT).These in principle can help with definitionalknowledge.
From our earlier analysis, it turns out"convicted" is conveniently defined in WordNet as"pronounced or proved guilty" potentially bridgingthe gap for pair 667, although there are problemswith the logical interpretation of this particulargloss in both resources mentioned.
WordNet doeshave "coeliac", but not in the sense of a personwith coeliac disease1.In addition, several existing ?core theories?
(e.g.,TimeML, IKRIS) that can supply some of the fun-damental knowledge mentioned earlier (e.g., space,time, goals) are being connected to WordNet underthe AQUAINT program.3.2 The DIRT Paraphrase DatabaseParaphrases have been used successfully by severalRTE systems (e.g., Hickl et al, 2005).
With re-spect to our earlier analysis, we examined Lin andPantel's (2001) paraphrase database built with theirsystem DIRT, containing 12 million entries.
Whilethere is of course noise in the database, it containsa remarkable amount of sensible world knowledgeas well as syntactic rewrites, albeit encoded asshallow rules lacking word senses.Looking at the examples from our earlier analysisof general world knowledge, we find that three aresupported by paraphrase rules in the database:273: X kills Y ?
X attacks Y499: X shoots Y ?
X murders Y1  This seems to be an accidental gap; WordNet containsmany interlinked disease-patient noun pairs, incl.
"dia-betes-diabetic," "epilepsy-eplileptic," etc.721: X works on Y ?
X discusses YAnd one that could be is not, namely:705: X is under a contract with Y ?
X coop-erates with Y (not in the database)Other examples are outside the scope of DIRT'sapproach (i.e., ?X pattern1 Y?
?
?X pattern2 Y?
),but nonetheless the coverage is encouraging.3.3 FrameNetIn our earlier analysis, we identified knowledgeabout stereotypical situations and their events asimportant for RTE.
FrameNet (Baker et al 1998)attempts to encode this knowledge.
FrameNet wasused with some success in RTE2 by Burchardt andFrank (2005).
FrameNet's basic unit - a Frame - isa script-like conceptual schema that refers to asituation, object, or event along with its partici-pants (Frame Elements), identified independent oftheir syntactic configuration.We earlier discussed how 538.T "...the O. J. Simp-son murder trial..." might entail 538.H "O. J. Simp-son was accused of murder."
This case applies toFrameNet?s Trial frame, which includes the FrameElements Defendant and Charges, with Chargesbeing defined as "The legal label for the crime thatthe Defendant is accused of.
", thus stating the rela-tionship between the defendant and the charges,unstated in T but made explicit in H, validating theentailment.
However,  the lexical units instantiat-ing the Frame Elements are not yet disambiguatedagainst a lexical database, limiting full semanticunderstanding.
Moreover, FrameNet's  worldknowledge is stated informally in English descrip-tions, though it may be possible to convert these toa more machine-processable form either manuallyor automatically.3.4 Other ResourcesWe have drawn attention to these three resourcesas they provide some answers to the requirementsidentified earlier.
Several other publicly availableresources could also be of use, including VerbNet(Univ Colorado at Boulder), the Component Li-brary (Univ Texas at Austin), OpenCyc (Cycorp),SUMO, Stanford's additions to WordNet, and theTuple Database (Boeing, following Schubert's2002 approach), to name but a few.584 Sketch of our RTE SystemAlthough not the primary purpose of this paper, webriefly sketch the path we are following to build anRTE system able to exploit the above resources.Our implementation is very preliminary, scoring50.9% at the time of RTE and 52.6% now (55.0%on the 525 examples it is able to analyze, guessing"no entailment" for the remainder).
Nevertheless,the following shows the direction we are moving inLike several other groups, our basic approach is togenerate logic for the T and H sentences, and thenexplore the application of inference rules to elabo-rate T, or transform H, until H matches T. Parsingis done using a broad coverage chart parser.
Sub-sequently, an abstracted form of the parse tree isconverted into a logical form, for example:299.H "Tropical storms cause severe damage.
"subject(_Cause1, _Storm1)sobject(_Cause1, _Damage1)mod(_Storm1, _Tropical1)mod(_Damage1, _Severe1)input-word(_Storm1, "storm", noun)[same for other words]Entailment is determined if every clause in the se-mantic representation of H semantically matches(subsumes) some clause in T. Two variables in aclause match if their input words are the same, orsome WordNet sense of one is the same as or ahypernym of the other.
In addition, the systemsearches for DIRT paraphrase rules that can trans-form the sentences into a form which can thenmatch.
The explicit use of WordNet and DIRT re-sults in comprehensible, machine-generated justifi-cations when entailments are found, , e.g.,:T: "The Salvation Army operates the shelter undera contract with the county.
"H: "The Salvation Army collaborates with thecounty."Yes!
Justification: I have general knowledge that:IF X works with Y THEN X collaborates with YHere: X = the Salvation Army, Y = the countyThus, here:I can see from T: the Salvation Army workswith the county (because "operate" and "work"mean roughly the same thing)Thus it follows that:The Salvation Army collaborates with the county.We are continuing to develop our system and ex-pand the number of knowledge sources it uses.5 SummaryTo recognize and justify textual entailments, andultimately understand language, considerable lexi-cal and world knowledge is needed.
We have pre-sented an analysis of some of the knowledge re-quirements of RTE3, and commented on someavailable sources of that knowledge.
The analysisserves to highlight the depth and variety of knowl-edge demanded by RTE3, and contributes a roughframework for organizing these requirements.
Ul-timately, to fully understand language, extensiveknowledge of the world (either manually or auto-matically acquired) is needed.
From this analysis,RTE3 is clearly providing a powerful driving forcefor research in this direction.AcknowledgementsThis work was performed under the DTOAQUAINT program, contract N61339-06-C-0160.ReferencesCollin Baker, Charles Fillmore, and John Lowe.
1998.
"The Berkeley FrameNet Project".
Proc 36th ACLAljoscha Burchardt and Anette Frank.
2005.
Approach-ing Textual Entailment with LFG and FrameNetFrames.
in 2nd PASCAL RTE Workshop, pp 92-97.Dan Fass.
1991.
"Met*: A Method for DiscriminatingMetonymy and Metaphor by Computer".
In Compu-tational Linguistics 17 (1), pp 49-90.Christiane Fellbaum.
1998.
?WordNet: An ElectronicLexical Database.?
The MIT Press.Andrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink, and Ying Shi.
?RecognizingTextual Entailment with LCC?s Groundhog System?,in Proc 2nd PASCAL RTE Workshop, pp 80-85.Dekang Lin and Patrick Pantel.
2001.
"Discovery ofInference Rules for Question Answering".
NaturalLanguage Engineering 7 (4) pp 343-360.Marvin Minsky.
1985.
"A Framework for RepresentingKnowledge".
In Readings in Knowledge Repn.Dan Moldovan and Vasile Rus, 2001.
Explaining An-swers with Extended WordNet, ACL 2001.Rowan Nairn, Cleo Condoravdi and Lauri Karttunen.2006.
Computing Relative Polarity for Textual Infer-ence.
In the Proceedings of ICoS-5.Len Schubert.
2002.
"Can we Derive General WorldKnowledge from Texts?
", Proc.
HLT'02, pp84-87.59
