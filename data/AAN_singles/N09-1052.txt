Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 459?467,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTied?Mixture Language Modeling in Continuous SpaceRuhi SarikayaIBM T.J. Watson Research CenterYorktown Heights, NY 10598sarikaya@us.ibm.comMohamed AfifyOrange Labs.Cairo, Egyptmohamed afify2001@yahoo.comBrian KingsburyIBM T.J. Watson Research CenterYorktown Heights, NY 10598bedk@us.ibm.comAbstractThis paper presents a new perspective to thelanguage modeling problem by moving theword representations and modeling into thecontinuous space.
In a previous work we in-troduced Gaussian-Mixture Language Model(GMLM) and presented some initial experi-ments.
Here, we propose Tied-Mixture Lan-guage Model (TMLM), which does not havethe model parameter estimation problems thatGMLM has.
TMLM provides a great deal ofparameter tying across words, hence achievesrobust parameter estimation.
As such, TMLMcan estimate the probability of any word thathas as few as two occurrences in the train-ing data.
The speech recognition experimentswith the TMLM show improvement over theword trigram model.1 IntroductionDespite numerous studies demonstrating the seriousshort-comings of the n?gram language models, ithas been surprisingly difficult to outperform n?gramlanguage models consistently across different do-mains, tasks and languages.
It is well-known that n?gram language models are not effective in modelinglong range lexical, syntactic and semantic dependen-cies.
Nevertheless, n?gram models have been veryappealing due to their simplicity; they require onlya plain corpus of data to train the model.
The im-provements obtained by some more elaborate lan-guage models (Chelba & Jelinek, 2000; Erdogan etal., 2005) come from the explicit use of syntactic andsemantic knowledge put into the annotated corpus.In addition to the mentioned problems above, tra-ditional n?gram language models do not lend them-selves easily to rapid and effective adaptation anddiscriminative training.
A typical n?gram modelcontains millions of parameters and has no structurecapturing dependencies and relationships betweenthe words beyond a limited local context.
These pa-rameters are estimated from the empirical distribu-tions, and suffer from data sparseness.
n?gram lan-guage model adaptation (to new domain, speaker,genre and language) is difficult, simply because ofthe large number of parameters, for which largeamount of adaptation data is required.
Instead of up-dating model parameters with an adaptation method,the typical practice is to collect some data in the tar-get domain and build a domain specific languagemodel.
The domain specific language model is in-terpolated with a generic language model trainedon a larger domain independent data to achieve ro-bustness.
On the other hand, rapid adaptation foracoustic modeling, using such methods as Maxi-mum Likelihood Linear Regression (MLLR) (Leg-etter & Woodland, 1995), is possible using verysmall amount of acoustic data, thanks to the inher-ent structure of acoustic models that allow large de-grees of parameter tying across different words (sev-eral thousand context dependent states are sharedby all the words in the dictionary).
Likewise,even though discriminatively trained acoustic mod-els have been widely used, discriminatively trainedlanguages models (Roark et al, 2007) have notwidely accepted as a standard practice yet.In this study, we present a new perspective to thelanguage modeling.
In this perspective, words arenot treated as discrete entities but rather vectors ofreal numbers.
As a result, long?term semantic re-lationships between the words could be quantifiedand can be integrated into a model.
The proposedformulation casts the language modeling problem as459an acoustic modeling problem in speech recognition.This approach opens up new possibilities from rapidand effective adaptation of language models to usingdiscriminative acoustic modeling tools and meth-ods, such as Minimum Phone Error (MPE) (Povey& Woodland, 2002) training to train discriminativelanguage models.We introduced the idea of language modeling incontinuous space from the acoustic modeling per-spective and proposed Gaussian Mixture LanguageModel (GMLM) (Afify et al, 2007).
However,GMLM has model parameter estimation problems.In GMLM each word is represented by a specific setof Gaussian mixtures.
Robust parameter estimationof the Gaussian mixtures requires hundreds or eventhousands of examples.
As a result, we were ableto estimate the GMLM probabilities only for wordsthat have at least 50 or more examples.
Essentially,this was meant to estimate the GMLM probabilitiesfor only about top 10% of the words in the vocab-ulary.
Not surprisingly, we have not observed im-provements in speech recognition accuracy (Afify etal., 2007).
Tied-Mixture Language Model (TMLM)does not have these requirements in model estima-tion.
In fact, language model probabilities can be es-timated for words having as few as two occurrencesin the training data.The concept of language modeling in continuousspace was previously proposed (Bengio et al, 2003;Schwenk & Gauvain, 2003) using Neural Networks.However, our method offers several potential advan-tages over (Schwenk & Gauvain, 2003) includingadaptation, and modeling of semantic dependenciesbecause of the way we represent the words in thecontinuous space.
Moreover, our method also al-lows efficient model training using large amounts oftraining data, thanks to the acoustic modeling toolsand methods which are optimized to handle largeamounts of data efficiently.It is important to note that we have to realize thefull potential of the proposed model, before investi-gating the potential benefits such as adaptation anddiscriminative training.
To this end, we proposeTMLM, which does not have the problems GMLMhas and, unlike GMLM we report improvements inspeech recognition over the corresponding n?grammodels.The rest of the paper is organized as follows.
Sec-tion 2 presents the concept of language modelingin continuous space.
Section 3 describes the tied?mixture modeling.
Speech recognition architectureis summarized in Section 4, followed by the experi-mental results in Section 5.
Section 6 discusses var-ious issues with the proposed method and finally,Section 7 summarizes our findings.2 Language Modeling In ContinuousSpaceThe language model training in continuous spacehas three main steps; namely, creation of a co?occurrence matrix, mapping discrete words into acontinuous parameter space in the form of vectorsof real numbers and training a statistical parametricmodel.
Now, we will describe each step in detail.2.1 Creation of a co?occurrence MatrixThere are many ways that discrete words canbe mapped into a continuous space.
The ap-proach we take is based on Latent Semantic Analy-sis (LSA) (Deerwester et al, 1990), and beginswith the creation of a co?occurrence matrix.
Theco?occurrence matrix can be constructed in sev-eral ways, depending on the morphological com-plexity of the language.
For a morphologicallyimpoverished language, such as English the co?occurrence matrix can be constructed using word bi-gram co?occurrences.
For morphologically rich lan-guages, there are several options to construct a co?occurrence matrix.
For example, the co?occurrencematrix can be constructed using either words (word?word co?occurrences) or morphemes (morpheme?morpheme co?occurrences), which are obtained af-ter morphologically tokenizing the entire corpus.In addition to word?word or morpheme?morphemeco?occurrence matrices, a word?morpheme co?occurrence matrix can also be constructed.
A wordw can be decomposed into a set of prefixes, stemand suffixes: w = [pfx1 + pfx2 + pfxn + stem+sfx1+sfx2+sfxn].
The columns of such a matrixcontain words and the rows contain the correspond-ing morphological decomposition (i.e.
morphemes)making up the word.
The decomposition of this ma-trix (as will be described in the next sub-section) canallow joint modeling of words and morphemes inone model.460In this study, we use morpheme level bigram co?occurrences to construct the matrix.
All the mor-pheme1 bigrams are accumulated for the entire cor-pus to fill in the entries of a co?occurrence matrix,C, where C(wi, wj) denotes the counts for whichword wi follows word wj in the corpus.
This is alarge, but very sparse matrix, since typically a smallnumber of words follow a given word.
Because ofits large size and sparsity, Singular Value Decom-position (SVD) is a natural choice for producing areduced-rank approximation of this matrix.The co?occurrence matrices typically contain asmall number of high frequency events and a largenumber of less frequent events.
Since SVD derivesa compact approximation of the co?occurrence ma-trix that is optimal in the least?square sense, it bestmodels these high-frequency events, which may notbe the most informative.
Therefore, the entries ofa word-pair co?occurrence matrix are smoothed ac-cording to the following expression:C?
(wi, wj) = log(C(wi, wj) + 1) (1)Following the notation presented in (Bellegarda,2000) we proceed to perform the SVD as follows:C?
?
USV T (2)where U is a left singular matrix with row vectorsui (1 ?
i ?
M) and dimension M ?
R. S is adiagonal matrix of singular values with dimensionR?R.
V is a right singular matrix with row vectorsvj (1 ?
j ?
N) and dimension N ?
R. R is theorder of the decomposition and R ?
min(M,N).M and N are the vocabulary sizes on the rowsand columns of the co?occurrence matrix, respec-tively.
For word?word or morpheme?morphemeco?occurrence matrices M = N , but for word?morpheme co?occurrence matrix, M is the numberof unique words in the training corpus and N is thenumber of unique morphemes in morphologicallytokenized training corpus.2.2 Mapping Words into Continuous SpaceThe continuous space for the words listed on therows of the co?occurrence matrix is defined as thespace spanned by the column vectors of AM?R =1For the generality of the notation, from now on we use?word?
instead of ?morpheme?.US.
Similarly, the continuous space for the wordson the columns are defined as the space spannedby the row vectors of BR?N = SV T .
Here, fora word?word co?occurrence matrix, each of thescaled vectors (by S) in the columns of A and rowsof B are called latent word history vectors for theforward and backward bigrams, respectively.
Now,a bigram wij = (wi, wj) (1 ?
i, j ?
M ) is repre-sented as a vector of dimension M ?
1, where theith entry of wij is 1 and the remaining ones are zero.This vector is mapped to a lower dimensional vectorw?ij by:w?ij = ATwij (3)where w?ij has dimension of R ?
1.
Similarly, thebackward bigram wji (1 ?
j, i ?
N ) is mapped to alower dimensional vector w?ji by:w?ji = Bwji (4)where w?ji has dimension of R ?
1.
Note that for aword?morpheme co?occurrence matrix the rows ofB would contain latent morpheme vectors.Since a trigram history consists of two bigram his-tories, a trigram history vector is obtained by con-catenating two bigram vectors.
Having generatedthe features, now we explain the structure of theparametric model and how to train it for languagemodeling in continuous space.2.3 Parametric Model Training in ContinuousSpaceRecalling the necessary inputs to train an acousticmodel for speech recognition would be helpful toexplain the new language modeling method.
Theacoustic model training in speech recognition needsthree inputs: 1) features (extracted from the speechwaveform), 2) transcriptions of the speech wave-forms and 3) baseforms, which show the pronuncia-tion of each word in the vocabulary.
We propose tomodel the language model using HMMs.
The HMMparameters are estimated in such way that the givenset of observations is represented by the model inthe ?best?
way.
The ?best?
can be defined in vari-ous ways.
One obvious choice is to use MaximumLikelihood (ML) criterion.
In ML, we maximize theprobability of a given sequence of observations O,belonging to a given class, given the HMM ?
of theclass, with respect to the parameters of the model ?.461This probability is the total likelihood of the obser-vations and can be expressed mathematically as:Ltot = p(O|?)
(5)However, there is no known way to analyticallysolve for the model ?
= {A,B, pi} , which max-imize the quantity Ltot, where A is the transi-tion probabilities, B is the observation probabili-ties, and pi is the initial state distribution.
But wecan choose model parameters such that it is locallymaximized, using an iterative procedure, like Baum-Welch method (Baum et al, 1970).The objective function given in Eq.
5 is the sameobjective function used to estimate the parametersof an HMM based acoustic model.
By drawing ananalogy between the acoustic model training andlanguage modeling in continuous space, the historyvectors are considered as the acoustic observations(feature vectors) and the next word to be predicted isconsidered as the label the acoustic features belongto, and words with their morphological decomposi-tions can be considered as the lexicon or dictionary.Fig.
1 presents the topology of the model for model-ing a word sequence of 3 words.
Each word is mod-eled with a single state left?to?right HMM topology.Using a morphologically rich language (or a char-acter based language like Chinese) to explain howHMMs can be used for language modeling will behelpful.
In the figure, let the states be the words andthe observations that they emit are the morphemes(or characters in the case of Chinese).
The sametopology (3 states) can also be used to model a sin-gle word, where the first state models the prefixes,the middle state models the stem and the final statemodels the suffixes.
In this case, words are repre-sented by network of morphemes.
Each path in aword network represents a segmentation (or ?pro-nunciation?)
of the word.The basic idea of the proposed modeling is to cre-ate a separate model for each word of the languageand use the language model corpus to estimate theparameters of the model.
However, one could arguethat the basic model could be improved by takingthe contexts of the morphemes into account.
Insteadof building a single HMM for each word, severalmodels could be trained according to the context ofthe morphemes.
These models are called context?<s> </s>s1 s2 s3a11a1222aa23a33ObservationSequenceO1 O2 O3 O4 O5Figure 1: HMM topology for language modeling in con-tinuous space.dependent morphemes.
The most obvious choice isto use both left and right neighbor of a morpheme ascontext, and creating, what we call tri?morphemes.In principal even if context-dependent morphemescould improve the modeling accuracy, the numberof models increase substantially.
For a vocabularysize of V , the number of tri?morpheme could be ashigh as V 3.
However, most of the tri?morphemesare either rare or will not be observed in the trainingdata altogether.Decision tree is one approach that can solve thisproblem.
The main idea is to find similar tri?morphemes and share the parameters between them.The decision tree uses a top-down approach to splitthe samples, which are in a single cluster at the rootof the tree, into smaller clusters by asking questionsabout the current morpheme and its context.
In ourcase, the questions could be syntactic and/or seman-tic in nature.What we hope for is that in the new continuousspace there is some form of distance or similaritybetween histories such that histories not observed inthe data for some words are smoothed by similar ob-served histories.2.4 Summary of the Continuous LanguageModel Training and Using it for DecodingIn the upper part of Fig.
2 the language model train-ing steps are shown.
The training process starts withthe language model training corpus.
From the sen-tences a bigram word co?occurrence matrix is con-structed.
This is a square matrix where the num-ber of rows (columns) equal to the vocabulary sizeof the training data.
The bigram co?occurrence ma-462trix is decomposed using SVD.
The columns of theleft?singular matrix obtained from SVD is used tomap the bigram word histories into a lower dimen-sional continuous parameter space.
The projectedword history vectors are stacked together dependingon the size of the n?gram.
For example, for trigrammodeling two history vectors are stacked together.Even though, we have not done so, at this stage onecould cluster the word histories for robust parame-ter estimation.
Now, the feature vectors, their corre-sponding transcriptions and the lexicon (baseforms)are ready to perform the ?acoustic model training?.One could use maximum likelihood criterion or anyother objective function such as Minimum Phone Er-ror (MPE) training to estimate the language modelparameters in the continuous space.The decoding phase could employ an adaptationstep, if one wants to adapt the language model toa different domain, speaker or genre.
Then, givena hypothesized sequence of words the decoder ex-tracts the corresponding feature vectors.
The fea-ture vectors are used to estimate the likelihood ofthe word sequence using the HMM parameters.
Thislikelihood is used to compute the probability of theword sequence.
Next, we introduce Tied?MixtureModeling, which is a special HMM structure to ro-bustly estimate model parameters.3 Tied?Mixture ModelingHidden Markov Models (HMMs) have been exten-sively used virtually in all aspects of speech andlanguage processing.
In speech recognition areacontinuous-density HMMs have been the standardfor modeling speech signals, where several thousandcontext?dependent states have their own Gaussiandensity functions to model different speech sounds.Typically, speech data have hundreds of millions offrames, which are sufficient to robustly estimate themodel parameters.
The amount of data for languagemodeling is orders of magnitude less than that ofthe acoustic data in continuous space.
Tied?MixtureHidden Markov Models (TM?HMMs) (Bellegarda& Nahamoo, 1989; Huang & Jack, 1988) have a bet-ter decoupling between the number of Gaussians andthe number of states compared to continuous den-sity HMMs.
The TM?HMM is useful for languagemodeling because it allows us to choose the num-HMM Training(e.g.
ML, MMIE,MPE)Model AdaptationDecoding/SearchModel TrainingWord History ClusteringAdaptation DataProbability ComputationNw1to Create Feature Vector,...,2ww DimensionalityReduction (e.g.,SVDDecomposition)Create BigramCo-occurance MatrixMap Word Historiesinto Continuous Spacei-2i-1i-1if(w     |   w    )f(w    |   w     )Stack the History VectorsFigure 2: Language Model Training and Adaptation inContinuous Space.ber of Gaussian densities and the number of mixtureweights independently.
Much more data is requiredto reliably estimate Gaussian densities than to esti-mate mixture weights.The evaluation of the observation density func-tions for TM?HMMs can be time consuming due tothe large mixture weight vector and due to the factthat for each frame all Gaussians have to be evalu-ated.
However, there are a number of solutions pro-posed in the past that significantly reduces the com-putation (Duchateau et al, 1998).The function p(w | h), defined in a continu-ous space, represents the conditional probability ofthe word w given the history h. In general, hcontains previous words and additional information(e.g.
part-of-speech (POS) tags for the previouswords) that may help to the prediction of the nextword.
Unlike TM?HMMs, using a separate HMMfor each word as in the case of Gaussian MixtureModels (GMMs), to represent the probability distri-bution functions results in the estimation problemsfor the model parameters since each n?gram doesnot have hundreds of examples.
TM?HMMs useGaussian mixture probability density functions per463state in which a single set of Gaussians is sharedamong all states:p(o|w) =J?jcw,jNj(o, ?w,j ,?w,j) (6)where w is the state, Nj is the jth Gaussian, and ois the observation (i.e.
history) vectors.
and J is thenumber of component mixtures in the TM-HMM.In order to avoid zero variance in word mappinginto continuous space, all the latent word vectors areadded a small amount of white noise.The TM?HMM topology is given in Fig.
3.
Eachstate models a word and they all share the same set ofGaussian densities.
However, each state has a spe-cific set of mixture weights associated with them.This topology can model a word?sequence that con-sist of three words in them.
The TM?HMM esti-mates the probability of observing the history vec-tors (h) for a given word w. However, what we needis the posterior probability p(w | h) of observing was the next word given the history, h. This can beobtained using the Bayes rule:p(w|h) = p(h|w)p(w)p(h) (7)= p(h|w)p(w)?Vv=1 p(h|v)p(v)(8)where p(w) is the unigram probability of the wordw.
The unigram probabilities can also be substitutedfor more accurate higher order n?gram probabilities.If this n?gram has an order that is equal to or greaterthan the one used in defining the continuous contextsh, then the TMLM can be viewed as performing akind of smoothing of the original n?gram model:Ps(w | h) = P (w | h)p(h | w)?Vv=1 P (v | h)p(h | v)(9)where Ps(w | h) and P (w | h) are the smoothedand original n?grams.The TM?HMM parameters are estimated throughan iterative procedure called the Baum-Welch, orforward-backward, algorithm (Baum et al, 1970).The algorithm locally maximizes the likelihoodfunction via an iterative procedure.
This type of33<s> </s>s1 s2 s3a11a1222aa23aFigure 3: Tied-Mixture HMM topology for languagemodeling in continuous space.
The mixtures are tiedacross states.
Each state represents a word.
The TM-HMM is completely defined with the mixture weights,mixture densities and transition probabilities.training is identical to training continuous densityHMMs except the Gaussians are tied across all arcs.For the model estimation equations the readers arereferred to (Bellegarda & Nahamoo, 1989; Huang &Jack, 1988).Next, we introduce the speech recognition systemused for the experiments.4 Speech Recognition ArchitectureThe speech recognition experiments are carried outon the Iraqi Arabic side of an English to Iraqi Ara-bic speech-to-speech translation task.
This task cov-ers the military and medical domains.
The acousticdata has about 200 hours of conversational speechcollected in the context of a DARPA supportedspeech-to-speech (S2S) translation project (Gao etal., 2006).The feature vectors for training acoustic modelsare generated as follows.
The speech data is sampledat 16kHz and the feature vectors are computed every10ms.
First, 24-dimensional MFCC features are ex-tracted and appended with the frame energy.
Thefeature vector is then mean and energy normalized.Nine vectors, including the current vector and fourvectors from its right and left contexts, are stackedleading to a 216-dimensional parameter space.
Thefeature space is finally reduced from 216 to 40 di-mensions using a combination of linear discriminantanalysis (LDA), feature space MLLR (fMLLR) andfeature space MPE (fMPE) training (Povey et al,4642005).
The baseline speech recognition system usedin our experiments is the state?of?the?art and pro-duces a competitive performance.The phone set consists of 33 graphemes represent-ing speech and silence for acoustic modeling.
Thesegraphemes correspond to letters in Arabic plus si-lence and short pause models.
Short vowels are im-plicitly modeled in the neighboring graphemes.
Fea-ture vectors are first aligned, using initial models,to model states.
A decision tree is then built foreach state using the aligned feature vectors by ask-ing questions about the phonetic context; quinphonequestions are used in this case.
The resulting tree hasabout 3K leaves.
Each leaf is then modeled usinga Gaussian mixture model.
These models are firstbootstrapped and then refined using three iterationsof forward?backward training.
The current systemhas about 75K Gaussians.The language model training data has 2.8M wordswith 98K unique words and it includes acousticmodel training data as a subset.
The morpholog-ically analyzed training data has 58K unique vo-cabulary items.
The pronunciation lexicon consistsof the grapheme mappings of these unique words.The mapping to graphemes is one-to-one and thereare very few pronunciation variants that are sup-plied manually mainly for numbers.
A statistical tri-gram language model using Modified Kneser-Neysmoothing (Chen& Goodman, 1996) has been builtusing the training data, which is referred to as Word-3gr.For decoding a static decoding graph is com-piled by composing the language model, the pro-nunciation lexicon, the decision tree, and the HMMgraphs.
This static decoding scheme, which com-piles the recognition network off?line before decod-ing, is widely adopted in speech recognition (Ri-ley et al, 2002).
The resulting graph is further op-timized using determinization and minimization toachieve a relatively compact structure.
Decoding isperformed on this graph using a Viterbi beam search.5 Experimental ResultsWe used the following TMLM parameters to buildthe model.
The SVD projection size is set to 200(i.e.
R = 200) for each bigram history.
This re-sults into a trigram history vector of size 400.
This?8 ?7 ?6 ?5 ?4 ?3 ?2 ?1 0?9?8?7?6?5?4?3?2?10N?gram ProbabilityTMLM ProbabilityFigure 4: Scatter plot of the n?gram and TMLM proba-bilities.vector is further projected down to a 50 dimensionalfeature space using LDA transform.
The total num-ber of Gaussian densities used for the TM?HMM isset to 1024.
In order to find the overall relationshipbetween trigram and TMLM probabilities we showthe scatter plot of the trigram and TMMT probabili-ties in Fig.
4.
While calculating the TMLM score theTMLM likelihood generated by the model is dividedby 40 to balance its dynamic range with that of then?gram model.
Most of the probabilities lie alongthe diagonal line.
However, some trigram proba-bilities are modulated making TMLM probabilitiesquite different than the corresponding trigram prob-abilities.
Analysis of TMLM probabilities with re-spect to the trigram probabilities would be an inter-esting future research.We conducted the speech recognition languagemodeling experiments on 3 testsets: TestA, TestBand TestC.
All three test sets are from July?07official evaluations of the IBM?s speech-to-speechtranslation system by DARPA.
TestA consists ofsentences spoken out in the field to the IBM?s S2Ssystem during live evaluation.
TestB contains sen-tences spoken in an office environment to the liveS2S system.
Using on-the-spot speakers for TestAand TestB meant to have shorter and clean sentences.Finally TestC contains pre-recorded sentences withmuch more hesitations and more casual conversa-tions compared to the other two testsets.
TestA,TestB and TestC have 309, 320 and 561 sentences,respectively.465LM TestA TestB TestC AllWord-3gr 18.7 18.6 38.9 32.9TMLM 18.8 18.9 38.2 32.5TMLM + Word-3gr 17.6 18.0 37.4 31.9Table 1: Speech Recognition Language Model RescoringResults.In order to evaluate the performance of theTMLM, a lattice with a low oracle error rate wasgenerated by a Viterbi decoder using the word tri-gram model (Word-3gr) model.
From the lattice atmost 30 (N=30) sentences are extracted for each ut-terance to form an N-best list.
The N?best error ratefor the combined test set (All) is 22.7%.
The N?best size is limited (it is not in the hundreds), simplybecause of faster experiment turn-around.
These ut-terances are rescored using TMLM.
The results arepresented in Table 1.
The first two rows in the ta-ble show the baseline numbers for the word trigram(Word?3gr) model.
TestA has a WER of 18.7% sim-ilar to that of TestB (18.6%).
The WER for TestCis relatively high (38.9%), because, as explainedabove, TestC contains causal conversation with hes-itations and repairs, and speakers do not necessar-ily stick to the domain.
Moreover, when users arespeaking to a device, as in the case of TestA andTestB, they use clear and shorter sentences, whichare easier to recognize.
The TMLM does not pro-vide improvements for TestA and TestB but it im-proves the WER by 0.7% for TestC.
The combinedoverall result is a 0.4% improvement over base-line.
This improvement is not statistically signifi-cant.
However, interpolating TMLM with Word-3grimproves the WER to 31.9%, which is 1.0% betterthan that of the Word-3gr.
Standard p-test (MatchedPairs Sentence-Segment Word Error test availablein standard SCLITEs statistical system comparisonprogram from NIST) shows that this improvementis significant at p < 0.05 level.
The interpolationweights are set equally to 0.5 for each LM.6 DiscussionsDespite limited but encouraging experimental re-sults, we believe that the proposed perspective is aradical departure from the traditional n?gram basedlanguage modeling methods.
The new perspectiveopens up a number of avenues which are impossibleto explore in one paper.We realize that there are a number of outstand-ing issues with the proposed perspective that re-quire a closer look.
We make a number of deci-sions to build a language model within this perspec-tive.
The decisions are sometimes ad hoc.
The de-cisions are made in order to build a working sys-tem and are by no means the best decisions.
Infact, it is quite likely that a different set of de-cisions may result into a better system.
Using aword?morpheme co?occurrence matrix instead of amorpheme?morpheme co?occurrence matrix is onesuch decision.
Another one is the clustering/tyingof the rarely observed events to achieve robust para-meter estimation both for the SVD and TMLM pa-rameter estimation.
We also use a trivial decisiontree to build the models where there were no con-text questions.
Clustering morphemes with respectto their syntactic and semantic context is anotherarea which should be explored.
In fact, we are inthe process of building these models.
Once we haverealized the full potential of the baseline maximumlikelihood TMLM, then we will investigate the dis-criminative training methods such as MPE (Povey& Woodland, 2002) to further improve the languagemodel performance and adaptation to new domainsusing MLLR (Legetter & Woodland, 1995).We also realize that different problems such assegmentation (e.g.
Chinese) of words or morpholog-ical decomposition of words into morphemes can beaddressed within the proposed perspective.7 ConclusionsWe presented our progress in improving continuous-space language modeling.
We proposed the Tied-Mixture Language Model (TMLM), which allowsfor robust parameter estimation through the useof tying and improves on the previously presentedGMLM.
The new formulation lets us train a para-metric language model using off?the?shelf acousticmodel training tools.
Our initial experimental resultsvalidated the proposed approach with encouragingresults.466ReferencesM.Afify, O. Siohan and R. Sarikaya.
2007.
GaussianMixture Language Models for Speech Recognition,ICASSP, Honolulu, Hawaii.C.J.
Legetter and P.C.
Woodland.
1995.
Maximum like-lihood linear regression for speaker adaptation of con-tinuous density hidden Markov models, ComputerSpeech and Language, vol.9, pp.
171-185.J.
Bellegarda.
2000.
Large Vocabulary Speech Recogni-tion with Multispan Language Models, IEEE Trans-actions on Speech and Audio Processing, vol.
8, no.
1,pp.
76-84.H.
Schwenk, and J.L.
Gauvain.
2003.
Using ContinuousSpace Language Models for Conversational TelephonySpeech Recognition, IEEE Workshop on SpontaneousSpeech Processing and Recognition, Tokyo, Japan.J.
Duchateau, K. Demuynck, D.V.
Compernolle and P.Wambacq.
1998.
Improved Parameter Tying for Ef-ficient Acoustic Model Evaluation in Large Vocabu-lary Continuous Speech Recognition.
Proc.
of ICSLP,Sydney, Australia.Y.
Gao, L. Gu, B. Zhou, R. Sarikaya, H.-K. Kuo.
A.-V.I.
Rosti, M. Afify, W. Zhu.
2006.
IBM MASTOR:Multilingual Automatic Speech-to-Speech Translator.Proc.
of ICASSP, Toulouse, France.S.
Chen, J. Goodman.
1996.
An Empirical Study ofSmoothing Techniques for Language Modeling, ACL,Santa Cruz, CA.J.
Bellagarda and D. Nahamoo.
1989.
Tied mixture con-tinuous parameter models for large vocabulary isolatedspeech recognition, Proc.
of ICASSP, pp.
13-16.X.D.
Huang and M.A.
Jack.
1988.
Hidden Markov Mod-elling of Speech Based on a Semicontinuous Model,Electronic Letters, 24(1), pp.
6-7, 1988.D.
Povey and P.C.
Woodland.
2002.
Minimum phone er-ror and I-smoothing for improved discriminative train-ing, Proc.
of ICASSP, pp.
105?108, Orlando, Florida.D.
Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,G.
Zweig.
2005. fMPE: Discriminatively TrainedFeatures for Speech Recognition, Proc.
of ICASSP,pp.
961?964, Philadelphia, PA.C.
Chelba and F. Jelinek.
2000.
Structured languagemodeling, Computer Speech and Language, 14(4),283?332, 2000.H.
Erdogan, R. Sarikaya, S.F.
Chen, Y. Gao and M.Picheny.
2005.
Using Semantic Analysis to ImproveSpeech Recognition Performance, Computer Speech& Language Journal, vol.
19(3), pp: 321?343.B.
Roark, M. Saraclar, M. Collins.
2007.
Using Seman-tic Analysis to Improve Speech Recognition Perfor-mance, Computer Speech & Language, vol.
21(2), pp:373?392.M.
Riley, E. Bocchieri, A. Ljolje and M. Saraclar.
2007.The AT&T 1x real-time Switchboard speech-to-textsystem, NIST RT02 Workshop, Vienna, Virginia.Y.
Bengio, R. Ducharme, P. Vincent and C. Jauvin.
2003.A Neural Probabilistic Language Model.
Journal ofMachine Learning Research, vol.
3, 11371155.S.
Deerwester, Susan Dumais, G. W. Furnas, T. K. Lan-dauer, R. Harshman.
1990.
Indexing by Latent Se-mantic Analysis, Journal of the American Society forInformation Science, 41 (6): 391?407.L.
E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970.
AMaximization Techniques Occurring in the StatisticalAnalysis of Probabilistic Functions of Markov Chains,The Annals of Mathematical Statistics, 41(1):164?171.467
