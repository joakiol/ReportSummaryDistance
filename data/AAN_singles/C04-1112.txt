A Lemma-Based Approach to a Maximum EntropyWord Sense Disambiguation System for DutchTanja GaustadAlfa-Informatica, University of GroningenPostbus 7169700 AS Groningen,The Netherlands,T.Gaustad@let.rug.nlAbstractIn this paper, we present a corpus-based super-vised word sense disambiguation (WSD) sys-tem for Dutch which combines statistical classi-fication (maximum entropy) with linguistic in-formation.
Instead of building individual clas-sifiers per ambiguous wordform, we introducea lemma-based approach.
The advantage ofthis novel method is that it clusters all inflec-ted forms of an ambiguous word in one classi-fier, therefore augmenting the training materialavailable to the algorithm.
Testing the lemma-based model on the Dutch SENSEVAL-2 testdata, we achieve a significant increase in accur-acy over the wordform model.
Also, the WSDsystem based on lemmas is smaller and morerobust.1 Introduction: WSD for DutchA major problem in natural language processing(NLP) for which no satisfactory solution has beenfound to date is word sense disambiguation (WSD).WSD refers to the resolution of lexical semanticambiguity and its goal is to attribute the correctsense(s) to words in a certain context.
For instancemachine translation, information retrieval or docu-ment extraction could all benefit from the accuratedisambiguation of word senses.The WSD system for Dutch1 presented here is acorpus-based supervised algorithm combining stat-istical classification with various kinds of linguisticinformation.
The intuition behind the system is thatlinguistic information is beneficial for WSD whichmeans that it will improve results over purely stat-istical approaches.
The linguistic information in-cludes lemmas, part-of-speech (PoS), and the con-text around the ambiguous word.In this paper, we focus on a lemma-based ap-proach to WSD for Dutch.
So far, systems builtindividual classifiers for each ambiguous wordform1The interest in Dutch lies grounded in the factthat we are working in the context of a project con-cerned with developing NLP tools for Dutch (seehttp://www.let.rug.nl/?vannoord/alp).
(Hendrickx et al, 2002; Hoste et al, 2002).
Inthe system presented here, the classifiers built foreach ambiguous word are based on its lemma in-stead.
Lemmatization allows for more compact andgeneralizable data by clustering all inflected formsof an ambiguous word together, an effect alreadycommented on by Yarowsky (1994).
The more in-flection in a language, the more lemmatization willhelp to compress and generalize the data.
In the caseof our WSD system this means that less classifiershave to be built therefore adding up the training ma-terial available to the algorithm for each ambiguouswordform.
Accuracy is expected to increase for thelemma-based model in comparison to the wordformmodel.The paper is structured as follows: First, we willpresent the dictionary-based lemmatizer for Dutchwhich was used to lemmatize the data, followed bya detailed explanation of the lemma-based approachadopted in our WSD system.
Next, the statisticalclassification algorithm, namely maximum entropy,and Gaussian priors (used for smoothing purposes)are introduced.
We will then proceed to describethe corpus, the corpus preparation, and the systemsettings.
We conclude the paper with results on theDutch SENSEVAL-2 data, their evaluation and ideasfor future work.2 Dictionary-Based Lemmatizer for DutchStatistical classification systems, like our WSD sys-tem, determine the most likely class for a given in-stance by computing how likely the words or lin-guistic features in the instance are for any givenclass.
Estimating these probabilities is difficult, ascorpora contain lots of different, often infrequent,words.
Lemmatization2 is a method that can be usedto reduce the number of wordforms that need to betaken into consideration, as estimation is more reli-able for frequently occurring data.2We chose to use lemmatization and not stemming becausethe lemma (or canonical dictionary entry form) can be used tolook up an ambiguous word in a dictionary or an ontology likee.g.
WordNet.
This is not the case for a stem.Lemmatization reduces all inflected forms of aword to the same lemma.
The number of differentlemmas in a training corpus will therefore in generalbe much smaller than the number of different word-forms, and the frequency of lemmas will thereforebe higher than that of the corresponding individualinflected forms, which in turn suggests that probab-ilities can be estimated more reliably.For the experiments in this paper, we used a lem-matizer for Dutch with dictionary lookup.
Diction-ary information is obtained from Celex (Baayen etal., 1993), a lexical database for Dutch.
Celex con-tains 381,292 wordforms and 124,136 lemmas forDutch.
It also contains the PoS associated withthe lemmas.
This information is useful for disam-biguation: in those cases where a particular word-form has two (or more) possible corresponding lem-mas, the one matching the PoS of the wordformis chosen.
Thus, in a first step, information aboutwordforms, their respective lemmas and their PoSis extracted from the database.Dictionary lookup can be time consuming, espe-cially for large dictionaries such as Celex.
To guar-antee fast lookup and a compact representation, theinformation extracted from the dictionary is storedas a finite state automaton (FSA) using Daciuk?s(2000) FSA morphology tools.3 Given a wordform,the compiled automaton provides the correspond-ing lemmas in time linear to the length of the inputword.
Contrasting this dictionary-based lemmat-izer with a simple suffix stripper, such as the DutchPorter Stemmer (Kraaij and Pohlman, 1994), ourlemmatizer is more accurate, faster and more com-pact (see (Gaustad and Bouma, 2002) for a moreelaborate description and evaluation).During the actual lemmatization procedure, theFSA encoding of the information in Celex assignsevery wordform all its possible lemmas.
For am-biguous wordforms, the lemma with the same PoSas the wordform in question is chosen.
All word-forms that were not found in Celex are processedwith a morphological guessing automaton.4The key features of the lemmatizer employed arethat it is fast, compact and accurate.3 Lemma-Based ApproachAs we have mentioned in the previous section, lem-matization collapses all inflected forms of a givenword to the same lemma.
In our system, separateclassifiers are built for every ambiguous wordform.3Available at http://www.eti.pg.gda.pl/?jandac/fsa.html4Also available from the FSA morphology tools (Daciuk,2000).Normally, this implies that the basis for groupingoccurrences of particular ambiguous words togetheris that their wordform is the same.
Alternatively, wechose for a model constructing classifiers based onlemmas therefore reducing the number of classifiersthat need to be made.As has already been noted by Yarowsky (1994),using lemmas helps to produce more concise andgeneric evidence than inflected forms.
Thereforebuilding classifiers based on lemmas increases thedata available to each classifier.
We make use ofthe advantage of clustering all instances of e.g.
oneverb in a single classifier instead of several classifi-ers (one for each inflected form found in the data).In this way, there is more training data per ambigu-ous wordform available to each classifier.
The ex-pectation is that this should increase the accuracy ofour maximum entropy WSD system in comparisonto the wordform-based model.Figure 1 shows how the system works.
Duringtraining, every wordform is first checked for ambi-guity, i.e.
whether it has more than one sense associ-ated with all its occurrences.
If the wordform is am-biguous, the number of lemmas associated with it islooked up.
If the wordform has one lemma, all oc-currences of this lemma in the training data are usedto make the classifier for that particular wordform?and others with the same lemma.
If a wordformhas more than one lemmas, a classifier based on thewordform is built.
This strategy has been decidedon in order to be able to treat all ambiguous words,notwithstanding lemmatization errors or wordformsthat can genuinely be assigned two or more lemmas.An example of a word that has two different lem-mas depending on the context is boog: it can eitherbe the past tense of the verb buigen (?to bend?)
or thenoun boog (?arch?).
Since the Dutch SENSEVAL-2data is not only ambiguous with regard to meaningbut also with regard to PoS, both lemmas are sub-sumed in the wordform classifier for boog.During testing, we check for each word whetherthere is a classifier available for either its wordformor its lemma and apply that classifier to the test in-stance.4 Maximum Entropy Word SenseDisambiguation SystemOur WSD system is founded on the idea of combin-ing statistical classification with linguistic sourcesof knowledge.
In order to be able to take full advant-age of the linguistic information, we need a classi-fication algorithm capable of incorporating the in-formation provided.
The main advantage of max-imum entropy modeling is that heterogeneous andnon-ambiguous psensepwordform LEMMAMODELpsenseambiguousWORDFORMMODELpsense1 senseX senses1 lemmaX lemmasFigure 1: Schematic overview of the lemma-based approach for our WSD System for Dutchoverlapping information can be integrated into asingle statistical model.
Other learning algorithms,like e.g.
decision lists, only take the strongest fea-ture into account, whereas maximum entropy com-bines them all.
Also, no independence assumptionsas in e.g.
Naive Bayes are necessary.We will now describe the different steps in put-ting together the WSD system we used to incor-porate and test our lemma-based approach, start-ing with the introduction of maximum entropy, themachine learning algorithm used for classification.Then, smoothing with Gaussian priors will be ex-plained.4.1 Maximum Entropy ClassificationSeveral problems in NLP have lent themselves tosolutions using statistical language processing tech-niques.
Many of these problems can be viewed asa classification task in which linguistic classes haveto be predicted given a context.The statistical classifier used in the experimentsreported in this paper is a maximum entropy classi-fier (Berger et al, 1996; Ratnaparkhi, 1997b).
Max-imum entropy is a general technique for estimatingprobability distributions from data.
A probabilitydistribution is derived from a set of events based onthe computable qualities (characteristics) of theseevents.
The characteristics are called features, andthe events are sets of feature values.If nothing about the data is known, estimating aprobability distribution using the principle of max-imum entropy involves selecting the most uniformdistribution where all events have equal probability.In other words, it means selecting the distributionwhich maximises the entropy.If data is available, a number of features extrac-ted from the labeled training data are used to de-rive a set of constraints for the model.
This set ofconstraints characterises the class-specific expecta-tions for the distribution.
So, while the distributionshould maximise the entropy, the model should alsosatisfy the constraints imposed by the training data.A maximum entropy model is thus the model withmaximum entropy of all models that satisfy the setof constraints derived from the training data.The model consists of a set of features which oc-cur on events in the training data.
Training itselfamounts to finding weights for each feature usingthe following formula:p(c|x) = 1Z exp(?i=1n?if i(x, c))where the property function f i(x, c) representsthe number of times feature i is used to find classc for event x, and the weights ?i are chosen to max-imise the likelihood of the training data and, at thesame time, maximise the entropy of p. Z is a nor-malizing constant, constraining the distribution tosum to 1 and n is the total number of features.This means that during training the weight ?i foreach feature i is computed and stored.
During test-ing, the sum of the weights ?i of all features i foundin the test instances is computed for each class c andthe class with the highest score is chosen.A big advantage of maximum entropy modelingis that the features include any information whichmight be useful for disambiguation.
Thus, dissim-ilar types of information, such as various kinds oflinguistic knowledge, can be combined into a singlemodel for WSD without having to assume inde-pendence of the different features.
Furthermore,good results have been produced in other areas ofNLP research using maximum entropy techniques(Berger et al, 1996; Koeling, 2001; Ratnaparkhi,1997a).4.2 Smoothing: Gaussian PriorsSince NLP maximum entropy models usually havelots of features and lots of sparseness (e.g.
featuresseen in testing not occurring in training), smoothingis essential as a way to optimize the feature weights(Chen and Rosenfeld, 2000; Klein and Manning,2003).
In the case of the Dutch SENSEVAL-2, formany ambiguous words there is little training dataavailable, therefore making smoothing essential.The intuition behind Gaussian priors is that theparameters in the maximum entropy model shouldnot be too large because of optimization problemswith infinite feature weights.
In other words: weenforce that each parameter will be distributed ac-cording to a Gaussian prior with mean ?
and vari-ance ?2 .
This prior expectation over the distributionof parameters penalizes parameters for drifting toofar from their mean prior value which is ?
= 0.Using Gaussian priors has a number of effects onthe maximum entropy model.
We trade off someexpectation-matching for smaller parameters.
Also,when multiple features can be used to explain adata point, the more common ones generally receivemore weight.
Last but not least accuracy generallygoes up and convergence is faster.In the current experiments the Gaussian prior wasset to ?2 = 1000 (based on preliminary experi-ments) which led to an overall increase of at least0.5% when compared to a model which was builtwithout smoothing.5 Corpus Preparation and BuildingClassifiersIn the context of SENSEVAL-25 , the first sense-tagged corpus for Dutch was made available (see(Hendrickx and van den Bosch, 2001) for a de-tailed description).
The training section of theDutch SENSEVAL-2 dataset contains approximately120,000 tokens and 9,300 sentences, whereas thetest section consists of ca.
40,000 tokens and 3,000sentences.In contrast to the English WSD data availablefrom SENSEVAL-2, the Dutch WSD data is not onlyambiguous in word senses, but also with regard toPoS.
This means that accurate PoS information isimportant in order for the WSD system to accur-ately achieve morpho-syntactic as well as semanticdisambiguation.5See http://www.senseval.org/ for more inform-ation on SENSEVAL and for downloads of the data.First, the corpus is lemmatized (see section 2) andpart-of-speech-tagged.
We used the Memory-Basedtagger MBT (Daelemans et al, 2002a; Daelemanset al, 2002b) with the (limited) WOTAN tag set(Berghmans, 1994; Drenth, 1997) to PoS tag ourdata (see (Gaustad, 2003) for an evaluation of dif-ferent PoS-taggers on this task).
Since we are onlyinterested in the main PoS-categories, we discardedall additional information from the assigned PoS.This resulted in 12 different tags being kept.
Inthe current experiments, we included the PoS of theambiguous wordform (important for the morpho-syntactic disambiguation) and also the PoS of thecontext words or lemmas.After the preprocessing (lemmatization and PoStagging), for each ambiguous wordform6 all in-stances of its occurrence are extracted from the cor-pus.
These instances are then transformed into fea-ture vectors including the features specified in a par-ticular model.
The model we used in the reportedexperiments includes information on the wordform,its lemma, its PoS, contextwords to the left and rightas well as the context PoS, and its sense/class.
(1) Nunowgingwenthijhebloemenflowersplukkenpickenandmaaktemadeeriteenakranscrownvan.of?Now he went to pick flowers and made acrown of it.
?Below we show an example of a feature vectorfor the ambiguous word bloem (?flower?/?flour?)
insentence 1:bloemen bloem N nu gaan hij AdvV Pron plukken en maken V Conj Vbloem plantThe first slot represents the ambiguous wordform,the second its lemma, the third the PoS of the am-biguous wordform, the fourth to twelfth slots con-tain the context lemmas and their PoS (left beforeright), and the last slot represents the sense or class.Various preliminary experiments have shown a con-text size of ?3 context words, i.e.
3 words to theleft and 3 words to the right of the ambiguous word,to achieve the best and most stable results.
Onlycontext words within the same sentence as the am-biguous wordform were taken into account.Earlier experiments showed that using lemmasas context instead of wordforms increases accuracy6A wordform is ?ambiguous?
if it has two or more differentsenses/classes in the training data.
The sense ?=?
is seen asmarking the basic sense of a word/lemma and is therefore alsotaken into account.due to the compression achieved through lemmat-ization (as explained earlier in this paper and putto practice in the lemma-based approach).
Withlemmas, less context features have to be estimated,therefore counteracting data sparseness.In the experiments presented here, no thresholdwas used.
Experiments have shown that build-ing classifiers even for wordforms with very fewtraining instances yields better results than apply-ing a frequency threshold and using the baselinecount (assigning the most frequent sense) for word-forms with an amount of training instances belowthe threshold.
It has to be noted, though, that theeffect of applying a threshold may depend on thechoice of learning algorithm.6 Results and EvaluationIn order to be able to evaluate the results fromthe lemma-based approach, we also include resultsbased on wordform classifiers.
During training withwordform classifiers, 953 separate classifiers werebuilt.With the lemma-based approach, 669 classifierswere built in total during training, 372 based onthe lemma of an ambiguous word (subsuming 656wordforms) and 297 based on the wordform.
A totalof 512 unique ambiguous wordforms was found inthe test data.
438 of these were classified using theclassifiers built from the training data, whereas only410 could be classified using the wordform model(see table 1 for an overview).We include the accuracy of the WSD system onall words for which classifiers were built (ambig) aswell as the overall performance on all words (all),including the non-ambiguous ones.
This makes ourresults comparable to other systems which use thesame data, but maybe a different data split or a dif-ferent number of classifiers (e.g.
in connection witha frequency threshold applied).
The baseline hasbeen computed by always choosing the most fre-quent sense of a given wordform in the test data.The results in table 2 show the average accuracyfor the two different approaches.
The accuracy ofboth approaches improves significantly (when ap-plying a paired sign test with a confidence level of95%) over the baseline.
This demonstrates that thegeneral idea of the system, to combine linguisticfeatures with statistical classification, works well.Focusing on a comparison of the two approaches,we can clearly see that the lemma-based approachworks significantly better than the wordform onlymodel, thereby verifying our hypothesis.Another advantage of the approach proposed, be-sides increasing the classification accuracy, is thatless classifiers need to be built during training andtherefore the WSD system based on lemmas issmaller.
In an online application, this might be animportant aspect of the speed and the size of the ap-plication.
It should be noted here that the degree ofgeneralization through lemmatization strongly de-pends on the data.
Only inflected wordforms oc-curring in the corpus are subsumed in one lemmaclassifier.
The more different inflected forms thetraining corpus contains, the better the ?compres-sion rate?
in the WSD model.
Added robustness is afurther asset of our system.
More wordforms couldbe classified with the lemma-based approach com-pared to the wordform-based one (438 vs. 410).In order to better assess the real gain in accur-acy from the lemma-based model, we also evalu-ated a subpart of the results for the lemma-basedand the wordform-based model, namely the accur-acy of those wordforms which were classified basedon their lemma in the former approach, but basedon their wordform in the latter case.
The compar-ison in table 3 clearly shows that there is much to begained from lemmatization.
The fact that inflectedwordforms are subsumed in lemma classifiers leadsto an error rate reduction of 8% and a system withless than half as many classifiers.In table 4, we see a comparison with anotherWSD systems for Dutch which uses Memory-Basedlearning (MBL) in combination with local context(Hendrickx et al, 2002).
A big difference withthe system presented in this article is that extensiveparameter optimization for the classifier of each am-biguous wordform has been conducted for the MBLapproach.
Also, a frequency threshold of minimally10 training instances was applied, using the baselineclassifier for all words below that threshold.
As wecan see, our lemma-based WSD system scores thesame as the Memory-Based WSD system, withoutextensive ?per classifier?
parameter optimization.According to Daelemans and Hoste (2002), differ-ent machine learning results should be comparedonce all parameters have been optimized for all clas-sifiers.
This is not the case in our system, andyet it achieves the same accuracy as an optimizedmodel.
Optimization of parameters for each am-biguous wordform and lemma classifier might helpincrease our results even further.7 Conclusion and Future WorkIn this paper, we have introduced a lemma-basedapproach for a statistical WSD system using max-imum entropy and a number of linguistic sourcesof information.
This novel approach uses the ad-vantage of more concise and more generalizable in-lemma-based wordformsTraining # classifiers built 669 953based on wordforms 297 953based on lemmas 372 na# wordforms subsumed 656 naTesting # unique ambiguous wordforms 512 512# classifiers used 387 410based on wordforms 230 410based on lemmas 70 na# wordforms subsumed 208 na# wordforms seen 1st time 74 102Table 1: Overview of classifiers built during training and used in testing with the lemma-based and thewordform-based approachModel ambig allbaseline all ambiguous words 78.47 89.44wordform classifiers 83.66 92.37lemma-based classifiers 84.15 92.45Table 2: WSD Results (in %) with the lemma-based approach compared to classifiers based on wordformsformation contained in lemmas as key feature: clas-sifiers for individual ambiguous words are built onthe basis of their lemmas, instead of wordforms ashas traditionally been done.
Therefore, more train-ing material is available to each classifier and theresulting WSD system is smaller and more robust.The lemma-based approach has been tested onthe Dutch SENSEVAL-2 data set and resulted in asignificant improvement of the accuracy achievedover the system using the traditional wordformbased approach.
In comparison to earlier resultswith a Memory-Based WSD system, the lemma-based approach performs the same, involving lesswork (no parameter optimization).A possible extension of the present approach is toinclude more specialized feature selection and alsoto optimize the settings for each ambiguous word-form instead of adopting the same strategy for allwords in the corpus.
Furthermore, we would like totest the lemma-based approach in a multi-classifiervoting scheme.AcknowledgmentsThis research was carried out within the frameworkof the PIONIER Project Algorithms for LinguisticProcessing.
This PIONIER Project is funded byNWO (Dutch Organization for Scientific Research)and the University of Groningen.
We are grateful toGertjan van Noord and Menno van Zaanen for com-ments and discussions.ReferencesR.
Harald Baayen, Richard Piepenbrock, and Afkevan Rijn.
1993.
The CELEX lexical database(CD-ROM).
Linguistic Data Consortium, Uni-versity of Pennsylvania, Philadelphia.Adam Berger, Stephen Della Pietra, and Vin-cent Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Johan Berghmans.
1994.
WOTAN?een automat-ische grammaticale tagger voor het Nederlands.Master?s thesis, Nijmegen University, Nijmegen.Stanley Chen and Ronald Rosenfeld.
2000.
A sur-vey of smoothing techniques for ME models.IEEE Transactions on Speech and Audio Pro-cessing, 8(1):37?50.Jan Daciuk.
2000.
Finite state tools for natural lan-guage processing.
In Proceedings of the COL-ING 2000 Workshop ?Using Toolsets and Archi-tectures to Build NLP Systems?, pages 34?37,Centre Universitaire, Luxembourg.Walter Daelemans and Ve?ronique Hoste.
2002.Evaluation of machine learning methods for nat-ural language processing tasks.
In Proceedings ofthe Third International Conference on LanguageResources and Evaluation (LREC 2002), pages755?760, Las Palmas, Gran Canaria.Walter Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
2002a.
MBT:Memory-Based tagger, reference guide.
Tech-Model ambig #classifiersbaseline 76.77 192wordform classifiers 78.66 192lemma-based classifiers 80.39 70Table 3: Comparison of results (in %) for wordforms with different classifiers in the lemma-based andwordform-based approachModel allbaseline all words 89.4wordform classifiers 92.4lemma-based classifiers 92.5Hendrickx et al (2002) 92.5Table 4: Comparison of results (in %) on the Dutch SENSEVAL-2 Data with different WSD systemsnical Report ILK 02-09, Induction of LinguisticKnowledge, Computational Linguistics, TilburgUniversity, Tilburg.
version 1.0.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2002b.
TiMBL:Tilburg Memory-Based learner, reference guide.Technical Report ILK 02-10, Induction of Lin-guistic Knowledge, Computational Linguistics,Tilburg University, Tilburg.
version 4.3.Erwin Drenth.
1997.
Using a hybrid approach to-wards Dutch part-of-speech tagging.
Master?sthesis, Alfa-Informatica, University of Gronin-gen, Groningen.Tanja Gaustad and Gosse Bouma.
2002.
Accur-ate stemming of Dutch for text classification.
InMarie?t Theune, Anton Nijholt, and Hendri Hon-dorp, editors, Computational Linguistics in theNetherlands 2001, Amsterdam.
Rodopi.Tanja Gaustad.
2003.
The importance of high qual-ity input for WSD: An application-oriented com-parison of part-of-speech taggers.
In Proceedingsof the Australasian Language Technology Work-shop (ALTW 2003), pages 65?72, Melbourne.Iris Hendrickx and Antal van den Bosch.
2001.Dutch word sense disambiguation: Data and pre-liminary results.
In Proceedings of Senseval-2,Second International Workshop on EvaluatingWord Sense Disambiguation Systems, pages 13?16, Toulouse.Iris Hendrickx, Antal van den Bosch, Ve?roniqueHoste, and Walter Daelemans.
2002.
Dutchword sense disambiguation: Optimizing the loc-alness of context.
In Proceedings of the ACL2002 Workshop on Word Sense Disambiguation:Recent Successes and Future Directions, Phil-adelphia.Ve?ronique Hoste, Iris Hendrickx, Walter Daele-mans, and Antal van den Bosch.
2002.
Para-meter optimization for machine-learning of wordsense disambiguation.
Natural Language Engin-eering, Special Issue on Word Sense Disambigu-ation Systems, 8(4):311?325.Dan Klein and Christopher Manning.
2003.
Max-ent models, conditional estimation, and optim-ization without the magic.
ACL 2003 TutorialNotes.
Sapporo.Rob Koeling.
2001.
Dialogue-Based Disambigu-ation: Using Dialogue Status to Improve SpeechUnderstanding.
Ph.D. thesis, Alfa-Informatica,University of Groningen, Groningen.Wessel Kraaij and Rene?e Pohlman.
1994.
Porter?sstemming algorithm for Dutch.
In L.G.M.Noordman and W.A.M.
de Vroomen, editors,Informatiewetenschap 1994: Wetenschapelijkebijdragen aan de derde STINFON Conferentie,pages 167?180, Tilburg.Adwait Ratnaparkhi.
1997a.
A linear observedtime statistical parser based on maximum entropymodels.
In Proceedings of the 2nd Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-97), Providence.Adwait Ratnaparkhi.
1997b.
A simple introduc-tion to maximum entropy models for natural lan-guage processing.
Technical Report IRCS Report97-08, IRCS, University of Pennsylvania, Phil-adelphia.David Yarowsky.
1994.
Decision lists for lexicalambiguity resolution: Application to accent res-toration in Spanish and French.
In 32th AnnualMeeting of the Association for ComputationalLinguistics (ACL 1994), Las Cruces.
