Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10?18,COLING 2010, Beijing, August 2010.A Systematic Comparison between Inversion Transduction Grammarand Linear Transduction Grammar for Word AlignmentMarkus Saers and Joakim NivreDept.
of Linguistics & PhilologyUppsala Universityfirst.last@lingfil.uu.seDekai WuHKUSTHuman Language Technology CenterDept.
of Computer Science & EngineeringHong Kong Univ.
of Science & Technologydekai@cs.ust.hkAbstractWe present two contributions to gram-mar driven translation.
First, since bothInversion Transduction Grammar andLinear Inversion Transduction Gram-mars have been shown to produce bet-ter alignments then the standard wordalignment tool, we investigate how thetrade-off between speed and end-to-endtranslation quality extends to the choiceof grammar formalism.
Second, weprove that Linear Transduction Gram-mars (LTGs) generate the same transduc-tions as Linear Inversion TransductionGrammars, and present a scheme for ar-riving at LTGs by bilingualizing LinearGrammars.
We also present a method forobtaining Inversion Transduction Gram-mars from Linear (Inversion) Transduc-tion Grammars, which can speed upgrammar induction from parallel corporadramatically.1 IntroductionIn this paper we introduce Linear TransductionGrammars (LTGs), which are the bilingual caseof Linear Grammars (LGs).
We also show thatLTGs are equal to Linear Inversion TransductionGrammars (Saers et al, 2010).
To be able to in-duce transduction grammars directly from par-allel corpora an approximate search for parses isneeded.
The trade-off between speed and end-to-end translation quality is investigated and com-pared to Inversion Transduction Grammars (Wu,1997) and the standard tool for word alignment,GIZA++ (Brown et al, 1993; Vogel et al, 1996;Och and Ney, 2003).
A heuristic for convertingstochastic bracketing LTGs into stochastic brack-eting ITGs is presented, and fitted into the speed?quality trade-off.In section 3 we give an overview of transduc-tion grammars, introduce LTGs and show thatthey are equal to LITGs.
In section 4 we givea short description of the rational for the trans-duction grammar pruning used.
In section 5 wedescribe a way of seeding a stochastic bracketingITG with the rules and probabilities of a stochas-tic bracketing LTG.
Section 6 describes the setup,and results are given in section 7.
Finally, someconclusions are offered in section 82 BackgroundAny form of automatic translation that relies ongeneralizations of observed translations needs toalign these translations on a sub-sentential level.The standard way of doing this is by aligningwords, which works well for languages that usewhite space separators between words.
The stan-dard method is a combination of the family ofIBM-models (Brown et al, 1993) and HiddenMarkov Models (Vogel et al, 1996).
Thesemethods all arrive at a function (A) from lan-guage 1 (F ) to language 2 (E).
By running theprocess in both directions, two functions can beestimated and then combined to form an align-ment.
The simplest of these combinations are in-tersection and union, but usually, the intersectionis heuristically extended.
Transduction gram-mars on the other hand, impose a shared struc-ture on the sentence pairs, thus forcing a consis-tent alignment in both directions.
This method10has proved successful in the settings it has beentried (Zhang et al, 2008; Saers and Wu, 2009;Haghighi et al, 2009; Saers et al, 2009; Saerset al, 2010).
Most efforts focus on cutting downtime complexity so that larger data sets than toy-examples can be processed.3 Transduction GrammarsTransduction grammars were first introduced inLewis and Stearns (1968), and further devel-oped in Aho and Ullman (1972).
The origi-nal notation called for regular CFG-rules in lan-guage F with rephrased E productions, either incurly brackets, or comma separated.
The bilin-gual version of CFGs is called Syntax-DirectedTransduction Grammars (SDTGs).
To differenti-ate identical nonterminal symbols, indices wereused (the bag of nonterminals for the two pro-ductions are equal by definition).A ?
B(1) a B(2) {x B(1) B(2)}= A ?
B(1) a B(2), x B(1) B(2)The semantics of the rules is that one nontermi-nal rewrites into a bag of nonterminals that is dis-tributed independently in the two languages, andinterspersed with any number of terminal sym-bols in the respective languages.
As with CFGs,the terminal symbols can be factored out intopreterminals with the added twist that they areshared between the two languages, since preter-minals are formally nonterminals.
The aboverule can thus be rephrased asA ?
B(1) Xa/x B(2), Xa/x B(1) B(2)Xa/x ?
a, xIn this way, rules producing nonterminals andrules producing terminals can be separated.Since only nonterminals are allowed to move,their movement can be represented as the orig-inal sequence of nonterminals and a permutationvector as follows:A ?
B Xa/x B ; 1, 0, 2Xa/x ?
a, xTo keep the reordering as monotone as possible,the terminals a and x can be produced separately,but doing so eliminates any possibility of param-eterizing their lexical relationship.
Instead, theindividual terminals are pair up with the emptystring (?
).A ?
Xx B Xa B ; 0, 1, 2, 3Xa ?
a, ?Xx ?
?, xLexical rules involving the empty string are re-ferred to as singletons.
Whenever a preterminalis used to pair up two terminal symbols, we referto that pair of terminals as a biterminal, whichwill be written as e/f .Any SDTG can be rephrased to contain per-muted nonterminal productions and biterminalproductions only, and we will call this the nor-mal form of SDTGs.
Note that it is not possi-ble to produce a two-normal form for SDTGs,as there are some rules that are not binarizable(Wu, 1997; Huang et al, 2009).
This is animportant point to make, since efficient parsingfor CFGs is based on either restricting parsingto only handle binary grammars (Cocke, 1969;Kasami, 1965; Younger, 1967), or rely on on-the-fly binarization (Earley, 1970).
When trans-lating with a grammar, parsing only has to bedone in F , which is binarizable (since it is aCFG), and can therefor be computed in polyno-mial time (O(n3)).
Once there is a parse treefor F , the corresponding tree for E can be eas-ily constructed.
When inducing a grammar fromexamples, however, biparsing (finding an anal-ysis that is consistent across a sentence pair) isneeded.
The time complexity for biparsing withSDTGs is O(n2n+2), which is clearly intractable.Inversion Transduction Grammars or ITGs(Wu, 1997) are transduction grammars that havea two-normal form, thus guaranteeing binariz-ability.
Defining the rank of a rule as the numberof nonterminals in the production, and the rankof a grammar as the highest ranking rule in therule set, ITGs are a) any SDTG of rank two, b)any SDTG of rank three or c) any SDTG where norule has a permutation vector other than identitypermutation or inversion permutation.
It followsfrom this definition that ITGs have a two-normalform, which is usually expressed as SDTG rules,11with brackets around the production to distin-guish the different kinds of rules from each other.A ?
B C ; 0, 1 = A ?
[ B C ]A ?
B C ; 1, 0 = A ?
?
B C ?A ?
e/f = A ?
e/fBy guaranteeing binarizability, biparsing timecomplexity becomes O(n6).There is an even more restricted version ofSDTGs called Simple Transduction Grammar(STG), where no permutation at all is allowed,which can also biparse a sentence pair in O(n6)time.A Linear Transduction Grammar (LTG) is abilingual version of a Linear Grammar (LG).Definition 1.
An LG in normal form is a tupleGL = ?N,?, R, S?Where N is a finite set of nonterminal symbols,?
is a finite set of terminal symbols, R is a finiteset of rules and S ?
N is the designated startsymbol.
The rule set is constrained so thatR ?
N ?
(?
?
{?})N(?
?
{?})
?
{?
}Where ?
is the empty string.To bilingualize a linear grammar, we will takethe same approach as taken when a finite-stateautomaton is bilingualized into a finite-statetransducer.
That is: to replace all terminal sym-bols with biterminal symbols.Definition 2.
An LTG in normal form is a tupleT GL = ?N,?,?, R, S?Where N is a finite set of nonterminal symbols,?
is a finite set of terminal symbols in languageE, ?
is a finite set of terminal symbols in lan-guage F , R is a finite set of linear transductionrules and S ?
N is the designated start symbol.The rule set is constrained so thatR ?
N ??N?
?
{?
?, ??
}Where ?
= ??{?}???{?}
and ?
is the emptystring.Graphically, we will represent LTG rules as pro-duction rules with biterminals:?A, ?x, p?B?y, q??
= A ?
x/p B y/q?A, ?
?, ???
= B ?
?/?Like STGs, LTGs do not allow any reordering,and are monotone, but because they are linear,this has no impact on expressiveness, as we shallsee later.Linear Inversion Transduction Grammars(LITGs) were introduced in Saers et al (2010),and represent ITGs that are allowed to have atmost one nonterminal symbol in each produc-tion.
These are attractive because they can bi-parse a sentence pair in O(n4) time, which canbe further reduced to linear time by severelypruning the search space.
This makes themtractable for large parallel corpora, and a viableway to induce transduction grammars from largeparallel corpora.Definition 3.
An LITG in normal form is a tupleT GLI = ?N,?,?, R, S?Where N is a finite set of nonterminal symbols,?
is a finite set of terminal symbols from lan-guage E, ?
is a finite set of terminal symbolsfrom language F , R is a set of rules and S ?
Nis the designated start symbol.
The rule set isconstrained so thatR ?
N ?
{[], ??}
?
?N ?N?
?
{?
?, ??
}Where [] represents identity permutation and ?
?represents inversion permutation, ?
= ??{?}??
?
{?}
is a possibly empty biterminal, and ?
isthe empty string.Graphically, a rule will be represented as an ITGrule:?A, [], B?e, f??
= A ?
[ B e/f ]?A, ?
?, ?e, f?B?
= A ?
?
e/f B ?
?A, [], ?
?, ???
= A ?
?/?As with ITGs, productions with only biterminalswill be represented without their permutation, asany such rule can be trivially rewritten into in-verted or identity form.12Definition 4.
An ?-free LITG is an LITG whereno rule may rewrite one nonterminal into anothernonterminal only.
Formally, the rule set is con-strained so thatR ?N ?
{[], ??}
?
({?
?, ??
}B ?B{?
?, ??})
= ?The LITG presented in Saers et al (2010) isthus an ?-free LITG in normal form, since it hasthe following thirteen rule forms (of which 8 aremeaningful, 1 is only used to terminate genera-tion and 4 are redundant):A ?
[ e/f B ]A ?
?
e/f B ?A ?
[ B e/f ]A ?
?
B e/f ?A ?
[ e/?
B ] | A ?
?
e/?
B ?A ?
[ B e/? ]
| A ?
?
B e/?
?A ?
[ ?/f B ] | A ?
?
B ?/f ?A ?
[ B ?/f ] | A ?
?
?/f B ?A ?
?/?All the singleton rules can be expressed either instraight or inverted form, but the result of apply-ing the two rules are the same.Lemma 1.
Any LITG in normal form can be ex-pressed as an LTG in normal form.Proof.
The above LITG can be rewritten in LTGform as follows:A ?
[ e/f B ] = A ?
e/f BA ?
?
e/f B ?
= A ?
e/?
B ?/fA ?
[ B e/f ] = A ?
B e/fA ?
?
B e/f ?
= A ?
?/f B e/?A ?
[ e/?
B ] = A ?
e/?
BA ?
[ B e/? ]
= A ?
B e/?A ?
[ ?/f B ] = A ?
?/f BA ?
[ B ?/f ] = A ?
B ?/fA ?
?/?
= A ?
?/?To account for all LITGs in normal form, the fol-lowing two non-?-free rules also needs to be ac-counted for:A ?
[ B ] = A ?
BA ?
?
B ?
= A ?
BLemma 2.
Any LTG in normal form can be ex-pressed as an LITG in normal form.Proof.
An LTG in normal form has two rules,which can be rewritten in LITG form, either asstraight or inverted rules as followsA ?
x/p B y/q = A ?
[ x/p B?
]B?
?
[ B y/q ]= A ?
?
x/q B?
?B?
?
?
B y/p ?A ?
?/?
= A ?
?/?Theorem 1.
LTGs in normal form and LITGs innormal form express the same class of transduc-tions.Proof.
Follows from lemmas 1 and 2.By theorem 1 everything concerning LTGs is alsoapplicable to LITGs, and an LTG can be expressedin LITG form when convenient, and vice versa.4 Pruning the Alignment SpaceThe alignment space for a transduction grammaris the combinations of the parse spaces of thesentence pair.
Let e be the E sentence, and fbe the F sentence.
The parse spaces would beO(|e|2) and O(|f |2) respectively, and the com-bination of these spaces would be O(|e|2?|f |2),or O(n4) if we assume n to be proportionalto the sentence lengths.
In the case of LTGs,this space is searched linearly, giving time com-plexity O(n4), and in the case of ITGs thereis branching within both parse spaces, addingan order of magnitude each, giving a total timecomplexity of O(n6).
There is, in other words,a tight connection between the alignment spaceand the time complexity of the biparsing al-gorithm.
Furthermore, most of this alignmentspace is clearly useless.
Consider the case wherethe entire F sentence is deleted, and the entire Esentence is simply inserted.
Although it is pos-sible that it is allowed by the grammar, it shouldhave a negligible probability (since it is clearly atranslation strategy that generalize poorly), andcould, for all practical reasons, be ignored.13Language pair Bisentences TokensSpanish?English 108,073 1,466,132French?English 95,990 1,340,718German?English 115,323 1,602,781Table 1: Size of training data.Saers et al (2009) present a scheme for prun-ing away most of the points in the alignmentspace.
Parse items are binned according to cov-erage (the total number of words covered), andeach bin is restricted to carry a maximum of bitems.
Any items that do not fit in the bins areexcluded from further analysis.
To decide whichitems to keep, inside probability is used.
Thispruning scheme effectively linearizes the align-ment space, as is will be of size O(nb), regard-less of what type grammar is used.
An ITG canthus be biparsed in cubic time, and an LTG in lin-ear time.5 Seeding an ITG with an LTGSince LTGs are a subclass of ITGs, it would bepossible to convert an LTG to a ITG.
This couldsave a lot of time, since LTGs are much faster toinduce from corpora than ITGs.Converting a BLTG to a BITG is fairly straightforward.
Consider the BLTG ruleX ?
[ e/f X ]To convert it to BITG in two-normal form, thebiterminal has to be factored out.
Replacingthe biterminal with a temporary symbol X?
, andintroducing a rule that rewrites this temporarysymbol to the replaced biterminal produces tworules:X ?
[ X?
X ]X?
?
e/fThis is no longer a bracketing grammar sincethere are two nonterminals, but equating X?
to Xrestores this property.
An analogous procedurecan be applied in the case where the nonterminalcomes before the biterminal, as well as for theinverting cases.When converting stochastic LTGs, the proba-bility mass of the SLTG rule has to be distributedto two SITG rules.
The fact that the LTG ruleX ?
?/?
lacks correspondence in ITGs has tobe weighted in as well.
In this paper we took themaximum entropy approach and distributed theprobability mass uniformly.
This means defin-ing the probability mass function p?
for the newSBITG from the probability mass function p ofthe original SBLTG such that:p?
(X ?
[ X X ]) =?e/f?????p(X?
[ e/f X ])1?p(X??/?)+?p(X?
[ X e/f ])1?p(X??/?)????p?
(X ?
?
X X ?)
=?e/f?????p(X??
e/f X ?)1?p(X??/?)+?p(X??
X e/f ?)1?p(X??/?)????p?
(X ?
e/f) =???????????????p(X?
[ e/f X ])1?p(X??/?)+?p(X?
[ X e/f ])1?p(X??/?)+?p(X??
e/f X ?)1?p(X??/?)+?p(X??
X e/f ?)1?p(X??/?)?????????????
?6 SetupThe aim of this paper is to compare the align-ments from SBITG and SBLTG to those fromGIZA++, and to study the impact of pruningon efficiency and translation quality.
Initialgrammars will be estimated by counting cooc-currences in the training corpus, after whichexpectation-maximization (EM) will be used torefine the initial estimate.
At the last iteration,the one-best parse of each sentence will be con-sidered as the word alignment of that sentence.In order to keep the experiments comparable,relatively small corpora will be used.
If largercorpora were used, it would not be possible to getany results for unpruned SBITGs because of theprohibitive time complexity.
The Europarl cor-pus (Koehn, 2005) was used as a starting point,and then all sentence pairs where one of the sen-tences were longer than 10 tokens were filtered14Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (inseconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination.Beam sizeSystem 1 10 25 50 75 100 ?BLEUSBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628GIZA++ 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597NISTSBITG 3.9705 6.6439 6.7312 6.7101 6.7329 6.7445 6.6793SBLTG 6.6023 6.6800 6.6657 6.6637 6.6714 6.6863 6.6765GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464Training timesSBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59Table 2: Results for the Spanish?English translation task.out (see table 1).
The GIZA++ system was builtaccording to the instructions for creating a base-line system for the Fifth Workshop on StatisticalMachine Translation (WMT?10),1 but the abovecorpora were used instead of those supplied bythe workshop.
This includes word alignmentwith GIZA++, a 5-gram language model builtwith SRILM (Stolcke, 2002) and parameter tun-ing with MERT (Och, 2003).
To carry out the ac-tual translations, Moses (Koehn et al, 2007) wasused.
The SBITG and SBLTG systems were builtin exactly the same way, except that the align-ments from GIZA++ were replaced by those fromthe respective grammars.In addition to trying out exhaustive biparsing1http://www.statmt.org/wmt10/for SBITGs and SBLTGs on three different trans-lation tasks, several different levels of pruningwere tried (1, 10, 25, 50, 75 and 100).
We alsoused the grammar induced from SBLTGs with abeam size of 25 to seed SBITGs (see section 5),which were then run for an additional iterationof EM, also with beam size 25.All systems are evaluated with BLEU (Pap-ineni et al, 2002) and NIST (Doddington, 2002).7 ResultsThe results for the three different translationtasks are presented in Tables 2, 3 and 4.
It isinteresting to note that the trend they portray isquite similar.
When the beam is very narrow,GIZA++ is better, but already at beam size 10,both transduction grammars are superior.
Con-15Beam sizeSystem 1 10 25 50 75 100 ?BLEUSBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603NISTSBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151SBLTG 6.6814 6.7608 6.7656 6.7992 6.8020 6.7925 6.7784GIZA++ 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907Training timesSBITG 03:25 17:00 42:00 1:25:00 2:10:00 2:45:00 3:10:00SBLTG 31 1:41 3:25 7:06 9:35 13:56 10:52Table 3: Results for the French?English translation task.Beam sizeSystem 1 10 25 50 75 100 ?BLEUSBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059NISTSBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668Training timesSBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32Table 4: Results for the German?English translation task.sistent with Saers et al (2009), SBITG has a sharprise in quality going from beam size 1 to 10,and then a gentle slope up to beam size 25, af-ter which it levels out.
SBLTG, on the other handstart out at a respectable level, and goes up a gen-tle slope from beam size 1 to 10, after which islevel out.
This is an interesting observation, as itsuggests that SBLTG reaches its optimum with alower beam size (although that optimum is lowerthan that of SBITG).
The trade-off between qual-ity and time can now be extended beyond beamsize to include grammar choice.
In Figure 1, runtimes are plotted against BLEU scores to illus-trate this trade-off.
It is clear that SBLTGs areindeed much faster than SBITGs, the only excep-tion is when SBITGs are run with b = 1, but thenthe BLEU score is so low that is is not worth con-sidering.The time may seem inconsistent between b =100 and b = ?
for SBLTG, but the extra timefor the tighter beam is because of beam manage-ment, which the exhaustive search doesn?t botherwith.In table 5 we compare the pure approachesto one where an LTG was trained during 10 it-erations of EM and then used to seed (see sec-16Translation task System BLEU NIST Total timeSBLTG 0.2631 6.6657 36:40Spanish?English SBITG 0.2655 6.7312 6:20:00Both 0.2660 6.7124 1:14:40SBLTG 0.2651 6.7656 34:10French?English SBITG 0.2654 6.7913 7:00:00Both 0.2625 6.7609 1:16:10SBLTG 0.2066 5.8882 48:52German?English SBITG 0.2091 5.9292 6:50:00Both 0.2095 5.9224 1:29:40Table 5: Results for seeding an SBITG with an SBLTG (Both) compared to the pure approach.
Totaltime refers to 10 iterations of EM training for SBITG and SBLTG respectively, and 10 iterations ofSBLTG and one iteration of SBITG training for the combined system.tion 5) an SBITG, which was then trained forone iteration of EM.
Although the differencesare fairly small, German?English and Spanish?English seem to reach the level of SBITG,whereas French?English is actually hurt.
Thebig difference is in time, since the combined sys-tem needs about a fifth of the time the SBITG-based system needs.
This phenomenon needs tobe more thoroughly examined.It is also worth noting that GIZA++ was beatenby an aligner that used less than 20 minutes (lessthan 2 minutes per iteration and at most 10 itera-tions) to align the corpus.8 ConclusionsIn this paper we have introduced the bilingualversion of linear grammar: Linear Transduc-tion Grammars, and found that they generate thesame class of transductions as Linear InversionTransduction Grammars.
We have also com-pared Stochastic Bracketing versions of ITGs andLTGs to GIZA++ on three word alignment tasks.The efficiency issues with transduction gram-mars have been addressed by pruning, and theconclusion is that there is a trade-off betweenrun time and translation quality.
A part of thetrade-off is choosing which grammar frameworkto use, as LTGs are faster but not as good as ITGs.It also seems possible to take a short-cut in thistrade-off by starting out with an LTG and convert-ing it to an ITG.
We have also showed that it ispossible to beat the translation quality of GIZA++with a quite fast transduction grammar.AcknowledgmentsThis work was funded by the Swedish Na-tional Graduate School of Language Technol-ogy (GSLT), the Defense Advanced ResearchProjects Agency (DARPA) under GALE Con-tracts No.
HR0011-06-C-0022 and No.
HR0011-06-C-0023, and the Hong Kong ResearchGrants Council (RGC) under research grantsGRF621008, DAG03/04.EG09, RGC6256/00E,and RGC6083/99E.
Any opinions, findings andconclusions or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the views ofthe Defense Ad-vanced Research Projects Agency.
The computa-tions were performed on UPPMAX resources un-der project p2007020.ReferencesAho, Alfred V. Ullman, Jeffrey D. 1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice-Halll, Inc., Upper Saddle River, NJ.Brown, Peter F., Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Cocke, John.
1969.
Programming languages andtheir compilers: Preliminary notes.
Courant Insti-17tute of Mathematical Sciences, New York Univer-sity.Doddington, George.
2002.
Automatic eval-uation of machine translation quality using n-gram co-occurrence statistics.
In Proceedings ofHuman Language Technology conference (HLT-2002), San Diego, California.Earley, Jay.
1970.
An efficient context-free parsingalgorithm.
Communications of the Association forComuter Machinery, 13(2):94?102.Haghighi, Aria, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with super-vised ITG models.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Nat-ural Language Processing of the AFNLP, pages923?931, Suntec, Singapore, August.Huang, Liang, Hao Zhang, Daniel Gildea, and KevinKnight.
2009.
Binarization of synchronouscontext-free grammars.
Computational Linguis-tics, 35(4):559?595.Kasami, Tadao.
1965.
An efficient recognitionand syntax analysis algorithm for context-free lan-guages.
Technical Report AFCRL-65-00143, AirForce Cambridge Research Laboratory.Koehn, Philipp, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics Companion Volume Proceedingsof the Demo and Poster Sessions, pages 177?180,Prague, Czech Republic, June.Koehn, Philipp.
2005.
Europarl: A parallel cor-pus for statistical machine translation.
In MachineTranslation Summit X, Phuket, Thailand, Septem-ber.Lewis, Philip M. and Richard E. Stearns.
1968.Syntax-directed transduction.
Journal of the Asso-ciation for Computing Machinery, 15(3):465?488.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Och, Franz Josef.
2003.
Minimum error rate trainingin statistical machine translation.
In 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 160?167, Sapporo, Japan, July.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of 40th Annual Meeting of the Associ-ation for Computational Linguistics, pages 311?318, Philadelphia, Pennsylvania, July.Saers, Markus and Dekai Wu.
2009.
Improvingphrase-based translation via word alignments fromStochastic Inversion Transduction Grammars.
InProceedings of the Third Workshop on Syntaxand Structure in Statistical Translation (SSST-3)at NAACL HLT 2009, pages 28?36, Boulder, Col-orado, June.Saers, Markus, Joakim Nivre, and Dekai Wu.
2009.Learning Stochastic Bracketing Inversion Trans-duction Grammars with a cubic time biparsing al-gorithm.
In Proceedings of the 11th InternationalConference on Parsing Technologies (IWPT?09),pages 29?32, Paris, France, October.Saers, Markus, Joakim Nivre, and Dekai Wu.
2010.Word alignment with Stochastic Bracketing LinearInversion Transduction Grammar.
In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics,Los Angeles, California, June.Stolcke, Andreas.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In International Con-ference on Spoken Language Processing, Denver,Colorado, September.Vogel, Stephan, Hermann Ney, and Christoph Till-mann.
1996.
Hmm-based word alignment in sta-tistical translation.
In Proceedings of the 16th con-ference on Computational linguistics, pages 836?841, Morristown, New Jersey.Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403.Younger, Daniel H. 1967.
Recognition and parsingof context-free languages in time n3.
Informationand Control, 10(2):189?208.Zhang, Hao, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.In Proceedings of ACL-08: HLT, pages 97?105,Columbus, Ohio, June.18
