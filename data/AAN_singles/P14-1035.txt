Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370?379,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Bayesian Mixed Effects Model of Literary CharacterDavid BammanSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdbamman@cs.cmu.eduTed UnderwoodDepartment of EnglishUniversity of IllinoisUrbana, IL 61801, USAtunder@illinois.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe consider the problem of automaticallyinferring latent character types in a collec-tion of 15,099 English novels publishedbetween 1700 and 1899.
Unlike priorwork in which character types are assumedresponsible for probabilistically generat-ing all text associated with a character,we introduce a model that employs mul-tiple effects to account for the influenceof extra-linguistic information (such as au-thor).
In an empirical evaluation, we findthat this method leads to improved agree-ment with the preregistered judgments of aliterary scholar, complementing the resultsof alternative models.1 IntroductionRecent work in NLP has begun to exploit thepotential of entity-centric modeling for a vari-ety of tasks: Chambers (2013) places entities atthe center of probabilistic frame induction, show-ing gains over a comparable event-centric model(Cheung et al, 2013); Bamman et al (2013) ex-plicitly learn character types (or ?personas?)
in adataset of Wikipedia movie plot summaries; andentity-centric models form one dominant approachin coreference resolution (Durrett et al, 2013;Haghighi and Klein, 2010).One commonality among all of these very dif-ferent probabilistic approaches is that each learnsstatistical regularities about how entities are de-picted in text (whether for the sake of learninga set of semantic roles, character types, or link-ing anaphora to the entities to which they refer).In each case, the text we observe associated withan entity in a document is directly dependent onthe class of entity?and only that class.
This re-lationship between entity and text is a theoreti-cal assumption, with important consequences forlearning: entity types learned in this way willbe increasingly similar the more similar the do-main, author, and other extra-linguistic effects arebetween them.1While in many cases the topi-cally similar types learned under this assumptionmay be desirable, we explore here the alterna-tive, in which entity types are learned in a waythat controls for such effects.
In introducing amodel based on different assumptions, we providea method that complements past work and pro-vides researchers with more flexible tools to inferdifferent kinds of character types.We focus here on the literary domain, exploringa large collection of 15,099 English novels pub-lished in the 18th and 19th centuries.
By account-ing for the influence of individual authors while in-ferring latent character types, we are able to learnpersonas that cut across different authors more ef-fectively than if we learned types conditioned onthe text alone.
Modeling the language used to de-scribe a character as the joint result of that charac-ter?s latent type and of other formal variables al-lows us to test multiple models of character andassess their value for different interpretive prob-lems.
As a test case, we focus on separating char-acter from authorial diction, but this approach canreadily be generalized to produce models that pro-visionally distinguish character from other factors(such as period, genre, or point of view) as well.2 Literary BackgroundInferring character is challenging from a liter-ary perspective partly because scholars have notreached consensus about the meaning of the term.It may seem obvious that a ?character?
is a repre-sentation of a (real or imagined) person, and manyhumanists do use the term that way.
But there is1For example, many entities in Early Modern Englishtexts may be judged to be more similar to each other thanto entities from later texts simply by virtue of using hath andother archaic verb forms.370an equally strong critical tradition that treats char-acter as a formal dimension of narrative.
To de-scribe a character as a ?blocking figure?
or ?first-person narrator,?
for instance, is a statement lessabout the attributes of an imagined person thanabout a narrative function (Keen, 2003).
Charac-ters are in one sense collections of psychologicalor moral attributes, but in another sense ?word-masses?
(Forster, 1927).
This tension between?referential?
and ?formalist?
models of characterhas been a centrally ?divisive question in .
.
.
liter-ary theory?
(Woloch, 2003).Considering primary source texts (as distinctfrom plot summaries) forces us to confront newtheoretical questions about character.
In a plotsummary (such as those explored by Bamman etal., 2013), a human reader may already have usedimplicit models of character to extract high-levelfeatures.
To infer character types from raw narra-tive text, researchers need to explicitly model therelationship of character to narrative form.
This isnot a solved problem, even for human readers.For instance, it has frequently been remarkedthat the characters of Charles Dickens sharecertain similarities?including a reliance on tagphrases and recurring tics.
A referential modelof character might try to distinguish this commonstylistic element from underlying ?personalities.
?A strictly formalist model might refuse to separateauthorial diction from character at all.
In prac-tice, human readers can adopt either perspective:we recognize that characters have a ?Dickensian?quality but also recognize that a Dickens villain is(in one sense) more like villains in other authorsthan like a Dickensian philanthropist.
Our goal isto show that computational methods can supportthe same range of perspectives?allowing a provi-sional, flexible separation between the referentialand formal dimensions of narrative.3 DataThe dataset for this work consists of 15,099 dis-tinct narratives drawn from HathiTrust Digital Li-brary.2From an initial collection of 469,200 vol-umes written in English and published between1700 and 1899 (including poetry, drama, and non-fiction as well as prose narrative), we extract32,209 volumes of prose fiction, remove dupli-cates and fuse multi-volume works to create the fi-nal dataset.
Since the original texts were produced2http://www.hathitrust.orgby scanning and running OCR on physical books,we automatically correct common OCR errors andtrim front and back matter from the volumes usingthe page-level classifiers and HMM of Underwoodet al (2013)Many aspects of this process would be sim-pler if we used manually-corrected texts, such asthose drawn from Project Gutenberg.
But we hopeto produce research that has historical as well ascomputational significance, and doing so dependson the provenance of a collection.
Gutenberg?sdecentralized selection process tends to produceexceptionally good coverage of currently-populargenres like science fiction, whereas HathiTrust ag-gregates university libraries.
Library collectionsare not guaranteed to represent the past perfectly,but they are larger, and less strongly shaped bycontemporary preferences.The goal of this work is to provide a method toinfer a set of character types in an unsupervisedfashion from the data.
As with prior work (Bam-man et al, 2013), we define this target, a characterpersona, as a distribution over several categoriesof typed dependency relations:31. agent: the actions of which a character isthe agent (i.e., verbs for which the characterholds an nsubj or agent relation).2. patient: the actions of which a character isthe patient (i.e., verbs for which the characterholds a dobj or nsubjpass relation).3. possessive: the objects that a character pos-sesses (i.e., all words for which the characterholds a poss relation).4. predicative: attributes predicated of a char-acter (i.e., adjectives or nouns holding annsubj relation to the character, with an inflec-tion of be as a child).This set captures the constellation of what acharacter does and has done to them, what theypossess, and what they are described as being.While previous work uses the StanfordCoreNLP toolkit to identify characters and extracttyped dependencies for them, we found thisapproach to be too slow for the scale of our data (atotal of 1.8 billion tokens); in particular, syntacticparsing, with cubic complexity in sentence length,and out-of-the-box coreference resolution (withthousands of potential antecedents) prove to be3All categories are described using the Stanford typed de-pendencies (de Marneffe and Manning, 2008), but any syn-tactic formalism is equally applicable.371the biggest bottlenecks.Before addressing character inference, wepresent here a prerequisite NLP pipeline thatscales well to book-length documents.4Thispipeline uses the Stanford POS tagger (Toutanovaet al, 2003), the linear-time MaltParser (Nivre etal., 2007) for dependency parsing (trained on Stan-ford typed dependencies), and the Stanford namedentity recognizer (Finkel et al, 2005).
It includesthe following components for clustering charac-ter name mentions, resolving pronominal corefer-ence, and reducing vocabulary dimensionality.3.1 Character ClusteringFirst, let us terminologically distinguish between acharacter mention in a text (e.g., the token Tom onpage 141 of The Adventures of Tom Sawyer) and acharacter entity (e.g., TOM SAWYER the character,to which that token refers).
To resolve the formerto the latter, we largely follow Davis et al (2003)and Elson et al (2010): we define a set of initialcharacters corresponding to each unique charac-ter name that is not a subset of another (e.g., Mr.Tom Sawyer) and deterministically create a set ofallowable variants for each one (Mr. Tom Sawyer?
Tom, Sawyer, Tom Sawyer, Mr. Sawyer, andMr.
Tom); then, from the beginning of the bookto the end, we greedily assign each mention to themost recently linked entity for whom it is a vari-ant.
The result constitutes our set of characters,with all mentions partitioned among them.3.2 Pronominal Coreference ResolutionWhile the character clustering stage is essentiallyperforming proper noun coreference resolution,approximately 74% of references to characters inbooks come in the form of pronouns.5To resolvethis more difficult class at the scale of an entirebook, we train a log-linear discriminative classifieronly on the task of resolving pronominal anaphora(i.e., ignoring generic noun phrases such as thepaint or the rascal).For this task, we annotated a set of 832 coref-erence links in 3 books (Pride and Prejudice, TheTurn of the Screw, and Heart of Darkness) and fea-turized coreference/antecedent pairs with:4All code is available at http://www.ark.cs.cmu.edu/literaryCharacter5Over all 15,099 narratives, the average number of char-acter proper name mentions is 1,673; the average number ofgendered singular pronouns (he, she, him, his, her) is 4,641.1.
The syntactic dependency path from apronoun to its potential antecedent (e.g.,dobj?pred?
?pred?nsubj (where ?
de-notes movement across sentence boundaries).2.
The salience of the antecedent character (de-fined as the count of that character?s namedmentions in the previous 500 words).3.
The antecedent part of speech.4.
Whether or not the pronoun and antecedentappear in the same quotation scope (false ifone appears in a quotation and one outside).5.
Whether or not the two agree for gender.6.
The syntactic tree distance between the two.7.
The linear (word) distance between the two.With this featurization and training data, we traina binary logistic regression classifier with `1regu-larization (where negative examples are comprisedof all character entities in the previous 100 wordsnot labeled as the true antecedent).
In a 10-foldcross-validation on predicting the true nearest an-tecedent for a pronominal anaphor, this methodachieves an average accuracy of 82.7%.With this trained model, we then select thehighest-scoring antecedent within 100 words foreach pronominal anaphor in our data.3.3 Dimensionality ReductionTo manage the degrees of freedom in the modeldescribed in ?4, we perform dimensionality reduc-tion on the vocabulary by learning word embed-dings with a log-linear continuous skip-gram lan-guage model (Mikolov et al, 2013) on the entirecollection of 15,099 books.
This method learns alow-dimensional real-valued vector representationof each word to predict all of the words in a win-dow around it; empirically, we find that with a suf-ficient window size (we use n = 10), these wordembeddings capture semantic similarity (placingtopically similar words near each other in vectorspace).6We learn a 100-dimensional embeddingfor each of the 512,344 words in our vocabulary.To create a partition over the vocabulary, weuse hard K-means clustering (with Euclidean dis-tance) to group the 512,344 word types into 1,000clusters.
We then agglomeratively cluster those1,000 groups to assign bitstring representations toeach one, forming a balanced binary tree by onlymerging existing clusters at equal levels in the hi-6In comparison, Brown et al (1992) clusters learned fromthe same data capture syntactic similarity (placing function-ally similar words in the same cluster).3720101010111001110: hat coat cap cloak handkerchief0111001111: pair boots shoes gloves leather0111001100: dressed costume uniform clad clothed0111001101: dress clothes wore worn wear01110011 ?Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in theprefix 01110011.
Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1)branches of the merge tree created through agglomerative clustering.erarchy.
We use Euclidean distance as a funda-mental metric and a group-average similarity func-tion for calculating the distance between groups.Fig.
1 illustrates four of the 1,000 learned clusters.4 ModelIn order to separate out the effects that a charac-ter?s persona has on the words that are associatedwith them (as opposed to other factors, such astime period, genre, or author), we adopt a hierar-chical Bayesian approach in which the words weobserve are generated conditional on a combina-tion of different effects captured in a log-linear (or?maximum entropy?)
distribution.Maximum entropy approaches to languagemodeling have been used since Rosenfeld (1996)to incorporate long-distance information, such aspreviously-mentioned trigger words, into n-gramlanguage models.
This work has since been ex-tended to a Bayesian setting by applying botha Gaussian prior (Chen and Rosenfeld, 2000),which dampens the impact of any individual fea-ture, and sparsity-inducing priors (Kazama andTsujii, 2003; Goodman, 2004), which can drivemany feature weights to 0.
The latter have beenapplied specifically to the problem of estimatingword probabilities with sparse additive generative(SAGE) models (Eisenstein et al, 2011), wheresparse extra-linguistic effects can influence a wordprobability in a larger generative setting.In contrast to previous work in which the prob-ability of a word linked to a character is depen-dent entirely on the character?s latent persona, inour model, we see the probability of a word asdependent on: (i) the background likelihood ofthe word, (ii) the author, so that a word becomesmore probable if a particular author tends to use itmore, and (iii) the character?s persona, so that aword is more probable if appearing with a partic-ular persona.
Intuitively, if the author Jane Austenis associated with a high weight for the word man-ners, and all personas have little effect for thisword, then manners will have little impact on de-ciding which persona a particular Austen characterembodies, since its presence is explained largelyby Austen having penned the word.
While we ad-dress only the author as an observed effect, thismodel is easily extended to other features as well,including period, genre, point of view, and others.The generative story runs as follows (Figure 2depicts the full graphical model): Let there beM unique authors in the data, P latent personas(a hyperparameter to be set), and V words inthe vocabulary (in the general setting these maybe word types; in our data the vocabulary is theset of 1,000 unique cluster IDs).
Each role typer ?
{agent,patient,possessive,predicative}and vocabulary word v (here, a cluster ID)is associated with a real-valued vector ?r,v=[?metar,v, ?persr,v, ?0r,v] of length M + P + 1.
The firstM + P elements are drawn from a Laplace priorwith mean ?
= 0 and scale ?
= 1; the last el-ement ?0r,vis an unregularized bias term account-ing for the background.
Each element in this vec-tor captures the log-additive effect of each author,persona, and the background distribution on theword?s probability (Eq.
1, below).Much like latent Dirichlet alocation (Blei et al,2003), each document d in our dataset draws amultinomial distribution ?dover personas from ashared Dirichlet prior ?, which captures the pro-portion of each character type in that particulardocument.
Every character c in the documentdraws its persona p from this document-specificmultinomial.
Given document metadata m (here,one of a set of M authors) and persona p, each tu-ple of a role r with word w is assumed to be drawnfrom Eq.
1 in Fig.
3.
This SAGE model can beunderstood as a log-linear distribution with threekinds of features (metadata, persona, and back-373P (w | m, p, r, ?)
= exp(?metar,w[m] + ?persr,w[p] + ?0r,w)/V?v=1exp(?metar,v[m] + ?persr,v[p] + ?0r,v)(1)P (b | m, p, r, ?)
=n?1?j=0??
?logit?1(?metar,b1:j[m] + ?persr,b1:j[p] + ?0r,b1:j)if bj+1= 11?
logit?1(?metar,b1:j[m] + ?persr,b1:j[p] + ?0r,b1:j)otherwise(2)Figure 3: Parameterizations of the SAGE word distribution.
Eq.
1 is a ?flat?
multinomial logistic regression with one ?-vectorper role and word.
Eq.
2 uses the hierarchical softmax formulation, with one ?-vector per role and node in the binary tree ofword clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq.
1.ground bias).4.1 Hierarchical SoftmaxThe partition function in Eq.
1 can lead to slowinference for any reasonably-sized vocabulary.
Toaddress this, we reparameterize the model by ex-ploiting the structure of the agglomerative clus-tering in ?3.3 to perform a hierarchical softmax,following Goodman (2001), Morin and Bengio(2005) and Mikolov et al (2013).The bitstring representations by which we en-code each word in the vocabulary serve as natural,and inherently meaningful, intermediate classesthat correspond to semantically related subsets ofthe vocabulary, with each bitstring prefix denotingone such class.
Longer bitstrings correspond tomore fine-grained classes.
In the example shownin Figure 1, 011100111 is one such intermediateclass, containing the union of pair, boots, shoes,gloves leather and hat, coat, cap cloak, handker-chief.
Because these classes recursively partitionthe vocabulary, they offer a convenient way toreparameterize the model through the chain ruleof probability.Consider, for example, a word represented asthe bitstring c = 01011; calculating P (c =01011)?we suppress conditioning variables forclarity?involves the product: P (c1= 0) ?P (c2= 1 | c1= 0) ?
P (c3= 0 | c1:2=01) ?
P (c4= 1 | c1:3= 010) ?
P (c5= 1 |c1:4= 0101).Since each multiplicand involves a binary pre-diction, we can avoid partition functions and usethe classic binary logistic regression.7We haveconverted the V -way multiclass logistic regressionproblem of Eq.
1 into a sequence of log V evalua-tions (assuming a perfectly balanced tree).
Given7Recall that logistic regression lets PLR(y = 1 | x, ?)
=logit?1(x>?)
= 1/(1 + exp?x>?)
for binary dependentvariable y, independent variables x, and coefficients ?.m, p, and r (as above) we let b = b1b2?
?
?
bnde-note the bitstring representation of a word cluster,and the distribution is given by Eq.
2 in Fig.
3.In this paramaterization, rather than one ?-vector for each role and vocabulary term, we haveone ?-vector for each role and conditional binarydecision in the tree (each bitstring prefix).
Sincethe tree is binary with V leaves, this yields thesame total number of parameters.
As Goodman(2001) points out, while this reparameterization isexact for true probabilities, it remains an approx-imation for estimated models (with generalizationbehavior dependent on how well the class hierar-chy is supported by the data).
In addition to en-abling faster inference, one advantage of the bit-string representation and the hierarchical softmaxparameterization is that we can easily calculateprobabilities of clusters at different granularities.4.2 InferenceOur primary quantities of interest in this modelare p (the personas for each character) and ?, theeffects that each author and persona have on theprobability of a word.
Rather than adopting a fullyBayesian approach (e.g., sampling all variables),we infer these values using stochastic EM, alter-nating between collapsed Gibbs sampling for eachp and maximizing with respect to ?.Collapsed Gibbs for personas.8At each step,the required quantity is the probability that char-acter c in document d has persona z, given ev-erything else.
This is proportional to the numberof other characters in document d who also (cur-rently) have that persona (plus the Dirichlet hy-perparameter which acts as a smoother) times theprobability (under pd,c= z) of all of the words8We assume the reader is familiar with collapsed Gibbssampling as used in latent-variable NLP models.374??pwrm??
?WCDP Number of personas (hyperparameter)D Number of documentsCdNumber of characters in document dWd,cNumber of (cluster, role) tuples for character cmdMetadata for document d (ranges over M authors)?dDocument d?s distribution over personaspd,cCharacter c?s personaj An index for a ?r, w?
tuple in the datawjWord cluster ID for tuple jrjRole for tuple j ?
{agent, patient, poss, pred}?
Coefficients for the log-linear language model?, ?
Laplace mean and scale (for regularizing ?)?
Dirichlet concentration parameterFigure 2: Above: Probabilistic graphical model.
Observedvariables are shaded, latent variables are clear, and collapsedvariables are dotted.
Below: Definition of variables.observed in each role r for that character:(count(z; pd,?c) + ?z)?R?r=1?j:rj=rP (bj| m, p, r, ?
)(3)The metadata features (like author, etc.)
influencethis probability by being constant for all choicesof z; e.g., if the coefficient learned for Austen forvocabulary term manners is high and all coeffi-cients for all z are close to zero, then the proba-bility of manners will change little under differentchoices of z. Eq.
3 contains one multiplicand forevery word associated with a character, and onlyone term reflecting the influence of the shared doc-ument multinomial.
The implication is that, formajor characters with many observed words, thewords will dominate the choice of persona; wherethe document influence would have a bigger effectis with characters for whom we don?t have muchdata.
In that case, it can act as a kind of informedbackground; given what little data we have for thatcharacter, it would nudge us toward the charactertypes that the other characters in the book embody.Given an assignment of all p, we choose ?to maximize the conditional log-likelihood of thewords, as represented by their bitstring cluster IDs,given the observed author and background effectsand the sampled personas.
This equates to solving4V `1-regularized logistic regressions (see Eq.
2in Figure 3), one for each role type and bitstringprefix, each with M + P + 1 parameters.
We ap-ply OWL-QN (Andrew and Gao, 2007) to mini-mize the `1-regularized objective with an absoluteconvergence threshold of 10?5.5 EvaluationWhile standard NLP and machine learning prac-tice is to evaluate the performance of an algorithmon a held-out gold standard, articulating what atrue ?persona?
might be for a character is inher-ently problematic.
Rather, we evaluate the perfor-mance and output of our model by preregisteringa set of 29 hypotheses of varying scope and diffi-culty and comparing the performance of differentmodels in either confirming, or failing to confirm,those hypotheses.
This kind of evaluation was pre-viously applied to a subjective text measurementproblem by Sim et al (2013).All hypotheses were created by a literaryscholar with specialization in the period to notonly give an empirical measure of the strengthsand weaknesses of different models, but also tohelp explore exactly what the different modelsmay, or may not, be learning.
All preregistered hy-potheses establish the degrees of similarity amongthree characters, taking the form: ?character X ismore similar to character Y than either X or Y isto a distractor character Z?
; for a given model anddefinition of distance under that model, each hy-pothesis yields two yes/no decisions that we canevaluate:?
distance(X,Y ) < distance(X,Z)?
distance(X,Y ) < distance(Y,Z)To tease apart the different kinds of similaritieswe hope to explore, we divide the hypotheses intofour classes:375A.
This class constitutes sanity checks: charac-ter X and Y are more similar to each otherin every way than to character Z.
E.g.
: Eliz-abeth Bennet in Pride and Prejudice resem-bles Elinor Dashwood in Sense and Sensibil-ity (Jane Austen) more than either characterresembles Allen Quatermain in Allen Quater-main (H. Rider Haggard).
(Austenian protag-onists should resemble each other more thanthey resemble a grizzled hunter.)B.
This class captures our ability to identify twocharacters in the same author as being moresimilar to each other than to a closely re-lated character in a different author.
E.g.
:Wickham in Pride and Prejudice resemblesWilloughby in Sense and Sensibility (JaneAusten) more than either character resem-bles Mr. Rochester in Jane Eyre (CharlotteBront?e).C.
This class captures our ability to discrimi-nate among similar characters in the same au-thor.
In these hypotheses, two characters Xand Y from the same author are more simi-lar to each other than to a third character Zfrom that same author.
E.g.
: Wickham inPride and Prejudice (Jane Austen) resemblesWilloughby in Sense and Sensibility morethan either character resembles Mr. Darcy inPride and Prejudice.D.
This class constitutes more difficult, ex-ploratory hypotheses, including differencesamong point of view.
E.g.
: Montoni inMysteries of Udolpho (Radcliffe) resem-bles Heathcliff in Wuthering Heights (EmilyBront?e) more than either resembles Mr. Ben-net in Pride and Prejudice.
(Testing ourmodel?s ability to discern similarities in spiteof elapsed time.
)All 29 hypotheses can be found in a supplemen-tary technical report (Bamman et al, 2014).
Weemphasize that the full set of hypotheses waslocked before the model was estimated.6 ExperimentsPart of the motivation of our mixed effects modelis to be able to tackle hypothesis class C?by fac-toring out the influence of a particular author onthe learning of personas, we would like to be ableto discriminate between characters that all havea common authorial voice.
In contrast, the Per-sona Regression model of Bamman et al (2013),which uses metadata variables (like authorship)to encourage entities with similar covariates tohave similar personas, reflects an assumption thatmakes it likely to perform well at class B.To judge their respective strengths on differenthypothesis classes, we evaluate three models:1.
The mixed-effects Author/Persona model(described above), which includes author in-formation as a metadata effect; here, each?-vector (of length M + P + 1) contains aparameter for each of the distinct authors inour data, a parameter for each persona, and abackground parameter.2.
A Basic persona model, which ablates au-thor information but retains the same log-linear architecture; here, the ?-vector is ofsize P +1 and does not model author effects.3.
The Persona Regression model of Bam-man et al (2013).All models are run with P ?
{10, 25, 50, 100,250} personas; Persona Regression addition-ally uses K = 25 latent topics.
All configura-tions use the full dataset of 15,099 novels, and allcharacters with at least 25 total roles (a total of257,298 entities).
All experiments are run with50 iterations of Gibbs sampling to collect samplesfor the personas p, alternating with maximizationsteps for ?.
The value of ?
is optimized using slicesampling (with a non-informative prior) every 5iterations.
The value of ?
is held constant at 1.At the end of inference, we calculate the posteriordistributions over personas for all characters as thesampling probability of the final iteration.To formally evaluate ?similarity?
between twocharacters, we measure the Jensen-Shannon diver-gence between personas (calculated as the averageJS distance over the cluster distributions for eachrole type), marginalizing over the characters?
pos-terior distributions over personas; two characterswith a lower JS divergence are judged to be moresimilar than two characters with a higher one.As a Baseline, we also evaluate all hypotheseson a model with no latent variables whatsoever,which instead measures similarity as the averageJS divergence between the empirical word distri-butions over each role type.Table 1 presents the results of this compari-son; for all models with latent variables, we re-port the average of 5 sampling runs with differentrandom initializations.
Figure 4 provides a syn-376P ModelHypothesis ClassA B C D250Author/Persona 1.00 0.58 0.75 0.42Basic Persona 1.00 0.73 0.58 0.53Persona Reg.
0.90 0.70 0.58 0.44100Author/Persona 0.98 0.68 0.70 0.46Basic Persona 0.95 0.73 0.53 0.47Persona Reg.
0.93 0.78 0.63 0.4950Author/Persona 0.95 0.73 0.63 0.50Basic Persona 0.98 0.75 0.48 0.53Persona Reg.
1.00 0.75 0.65 0.3825Author/Persona 1.00 0.63 0.65 0.50Basic Persona 1.00 0.63 0.50 0.50Persona Reg.
0.90 0.78 0.60 0.3910Author/Persona 0.95 0.63 0.70 0.51Basic Persona 0.78 0.80 0.48 0.46Persona Reg.
0.90 0.73 0.43 0.41Baseline 1.00 0.63 0.58 0.37Table 1: Agreement rates with preregistered hypotheses, av-eraged over 5 sampling runs with different initializations.0255075100A B C DHypothesis classAccuracyAuthor/Persona  Basic  Persona Reg.
BaselineFigure 4: Synopsis of table 1: average accuracy across all P .Persona regression is best able to judge characters in oneauthor to be more similar to each other than to characters inanother (B), while our mixed-effects Author/Persona modeloutperforms other models at discriminating characters in thesame author (C).opsis of this table by illustrating the average ac-curacy across all choice of P .
All models, in-cluding the baseline, perform well on the sanitychecks (A).
As expected, the Persona Regres-sion model performs best at hypothesis class B(correctly judging two characters from the sameauthor to be more similar to each other than to acharacter from a different author); this behavior isencouraged in this model by allowing an author (asan external metadata variable) to directly influencethe persona choice, which has the effect of push-ing characters from the same author to embodythe same character type.
Our mixed effects Au-thor/Persona model, in contrast, outperforms theother models at hypothesis class C (correctly dis-criminating different character types present in thesame author).
By discounting author-specific lexi-cal effects during persona inference, we are betterable to detect variation among the characters of asingle author that we are not able to capture oth-erwise.
While these different models complementeach other in this manner, we note that there isno absolute separation among them, which may besuggestive of the degree to which the formal andreferential dimensions are fused in novels.
Nev-ertheless, the strengths of these different modelson these different hypothesis classes gives us flex-ible alternatives to use depending on the kinds ofcharacter types we are looking to infer.7 AnalysisThe latent personas inferred from this model willsupport further exploratory analysis of literary his-tory.
Figure 2 illustrates this with a selection ofthree character types learned, displaying charac-teristic clusters for all role types, along with thedistribution of that persona?s use across time andthe gender distribution of characters embodyingthat persona.
In general, the personas learned sofar do not align neatly with character types knownto literary historians.
But they do have legible as-sociations both with literary genres and with socialcategories.
Even though gender is not an observ-able variable known to the model during inference,personas tend to be clearly gendered.
This is notin itself surprising (since literary scholars knowthat assumptions about character are strongly gen-dered), but it does suggest that diachronic analysisof latent character types might cast new light onthe history of gender in fiction.
This is especiallytrue since the distribution of personas across thetime axis similarly reveals coherent trends.Table 3 likewise illustrates what our modellearns by presenting a sample of the fixed effectslearned for a set of five major 19th-century au-thors.
These are clusters that are conditionallymore likely to appear associated with a characterin a work by the given author than they are in theoverall data; by factoring this information out ofthe inference process for learning character types(by attributing its presence in a text to the author3771800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900Agentcarried ran threw sent received arrived turns begins returnsrose fell suddenly appeared struck showed thinks loves callsis seems returned immediately waiting does knows comesPatientwounded killed murdered wounded killed murdered thinks loves callssuffer yield acknowledge destroy bind crush love hope truefree saved unknown attend haste proceed turn hold showPossdeath happiness future army officers troops lips cheek browlips cheek brow soldiers band armed eyes face eyemouth fingers tongue party join camp table bed chairPredcrime guilty murder king emperor throne beautiful fair fineyouth lover hers general officer guard good kind illdead living died soldier knight hero dead living died% Female 12.2 3.7 54.7Table 2: Snapshots of three personas learned from the P = 50, Author/Persona model.
Gender and time proportions arecalculated by summing and normalizing the posterior distributions over all characters with that feature.
We truncate time seriesat 1800 due to data sparsity before that date; the y-axis illustrates the frequency of its use in a given year, relative to its lifetime.Author clustersJane Austenpraise gift consolationletter read writecharacter natural tasteCharlotte Bront?elips cheek browbook paper bookshat coat capCharles Dickenshat coat captable bed chairhand head handsHerman Melvilleboat ship boardhat coat capfeet ground footJules Vernejourney travel voyagemaster company presencesuccess plan progressTable 3: Characteristic possessive clusters in a sample ofmajor 19th-century authors.rather than the persona), we are able to learn per-sonas that cut across different topics more effec-tively than if a character type is responsible forexplaining the presence of these terms as well.8 ConclusionOur method establishes the possibility of repre-senting the relationship between character and nar-rative form in a hierarchical Bayesian model.
Pos-tulating an interaction between authorial dictionand character allows models that consider the ef-fect of the author to more closely reproduce a hu-man reader?s judgments, especially by learning todistinguish different character types within a sin-gle author?s oeuvre.
This opens the door to con-sidering other structural and formal dimensions ofnarration.
For instance, representation of charac-ter is notoriously complicated by narrative point ofview (Booth, 1961); and indeed, comparisons be-tween first-person narrators and other charactersare a primary source of error for all models testedabove.
The strategy we have demonstrated sug-gests that it might be productive to address this bymodeling the interaction of character and point ofview as a separate effect analogous to authorship.It is also worth noting that the models testedabove diverge from many structuralist theories ofnarrative (Propp, 1998) by allowing multiple in-stances of the same persona in a single work.Learning structural limitations on the number of?protagonists?
likely to coexist in a single story,for example, may be another fruitful area to ex-plore.
In all cases, the machinery of hierarchicalmodels gives us the flexibility to incorporate sucheffects at will, while also being explicit about thetheoretical assumptions that attend them.9 AcknowledgmentsWe thank the reviewers for their helpful com-ments.
The research reported here was supportedby a National Endowment for the Humanitiesstart-up grant to T.U., U.S. National Science Foun-dation grant CAREER IIS-1054319 to N.A.S., andan ARCS scholarship to D.B.
This work was madepossible through the use of computing resourcesmade available by the Pittsburgh SupercomputingCenter.
Eleanor Courtemanche provided adviceabout the history of narrative theory.378ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of l1-regularized log-linear models.
In Proc.
ofICML.David Bamman, Brendan O?Connor, and Noah A.Smith.
2013.
Learning latent personas of film char-acters.
Proc.
of ACL.David Bamman, Ted Underwood, and Noah A. Smith.2014.
Appendix to ?A Bayesian mixed effectsmodel of literary character?.
Technical report,Carnegie Mellon University, University of Illinois-Urbana Champaign.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Wayne Booth.
1961.
The Rhetoric of Fiction.
Univer-sity of Chicago Press, Chicago.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Nathanael Chambers.
2013.
Event schema inductionwith a probabilistic entity-driven model.
In Proc.
ofEMNLP, Seattle, Washington, USA.Stanley F. Chen and Roni Rosenfeld.
2000.
Asurvey of smoothing techniques for me models.IEEE Transactions on Speech and Audio Process-ing, 8(1):37?50.Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic frame induction.
InProc.
of NAACL.Peter T. Davis, David K. Elson, and Judith L. Klavans.2003.
Methods for precise named entity matching indigital collections.
In Proc.
of JCDL, Washington,DC, USA.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical report, Stanford University.Greg Durrett, David Hall, and Dan Klein.
2013.Decentralized entity-level modeling for coreferenceresolution.
In Proc.
of ACL.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse additive generative models of text.
In Proc.of ICML.David K. Elson, Nicholas Dames, and Kathleen R.McKeown.
2010.
Extracting social networks fromliterary fiction.
In Proc.
of ACL, Stroudsburg, PA,USA.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proc.
of ACL.E.
M. Forster.
1927.
Aspects of the Novel.
Harcourt,Brace & Co.Joshua Goodman.
2001.
Classes for fast maximumentropy training.
In Proc.
of ICASSP.Joshua Goodman.
2004.
Exponential priors for maxi-mum entropy models.
In Proc.
of NAACL.Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In Proc.of NAACL.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proc.
of EMNLP.Suzanne Keen.
2003.
Narrative Form.
PalgraveMacmillan, Basingstoke.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
Proc.
of ICLR.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProc.
of AISTATS.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13:95?135, 5.Vladimir Propp.
1998.
Morphology of the Folktale.University of Texas Press, 2nd edition.Roni Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modelling.
Com-puter Speech and Language, 10(3):187 ?
228.Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, andNoah A. Smith.
2013.
Measuring ideological pro-portions in political speeches.
In Proc.
of EMNLP,Seattle, Washington, USA.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proc.
of NAACL.Ted Underwood, Michael L Black, Loretta Auvil, andBoris Capitanu.
2013.
Mapping mutable genres instructurally complex volumes.
In Proc.
of IEEE In-ternational Conference on Big Data.Alex Woloch.
2003.
The One vs. the Many: MinorCharacters and the Space of the Protagonist in theNovel.
Princeton University Press, Princeton NJ.379
