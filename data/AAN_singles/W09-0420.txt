Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 115?119,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsExperiments in morphosyntactic processing for translating to and fromGermanAlexander FraserInstitute for Natural Language ProcessingUniversity of Stuttgartfraser@ims.uni-stuttgart.deAbstractWe describe two shared task systems andassociated experiments.
The German toEnglish system used reordering rules ap-plied to parses and morphological split-ting and stemming.
The English to Ger-man system used an additional translationstep which recreated compound words andgenerated morphological inflection.1 IntroductionThe Institute for Natural Language Processing(IfNLP), Stuttgart, participated in the WMT-2009shared tasks for German to English and Englishto German translation with constrained systemswhich employed morphological and syntactic pro-cessing techniques.
The systems were based onthe open source Moses docoder (Koehn et al,2007).
We combined IfNLP tools for syntactic andmorphological analysis (which are publicly avail-able and widely used) with preprocessing tech-niques that were successfully used by other groupsin WMT-2008, and extended these.
For English toGerman translation, we additionally performed astep which recreated compound words and gener-ated morphological inflection.1.1 BaselineThe baseline is the standard system supplied forthe shared task.
We used the default parametersof the Moses toolkit, except for a small differencein the generation of the word alignments, see sec-tion 3.2 Improvements2.1 Character NormalizationWe normalize both the English and German byconverting all characters to their nearest equivalentin Latin-1 (ISO 8859-1) encoding1, except for theeuro sign, which is handled specially.
We did notmodify the SGML files used for calculating BLEUand METEOR scores in any way.2.2 German Writing ReformGerman underwent a writing reform from the alteRechtschreibung (old spelling rules/orthography)to the neue Rechtschreibung (gloss: new spellingrules/orthography) recently.
Early Europarldata are written using the alte Rechtschreibungand hence need to be converted to the neueRechtschreibung in order to match the news data,which is in the new form.We began the process by mapping all cased vari-ants of a particular word to a single class (suchas by mapping two words which are written withue and u?, but are otherwise identical, to a singleclass).
We then tried to automatically identify thecorrect variant under the writing reform for eachclass.
Initially we tried the linux tool aspell butfound that its coverage (the recall of its lexicon)was poor.We used a simple technique for finding the bestvariant.
We separated the Europarl corpus intoportions written using the old and new forms.
Weused the incidence of the word dass (the comple-mentizer meaning that) and its old rules variantda?.
We used a chunk size of 70 sentences tosegment Europarl into old and new by countingwhether there were more instances of da?
or dass,respectively, in each chunk.
We added the newscorpora to the new portion.
For each variant wecounted the number of times it occurred in thenew data and subtracted the number of times it oc-curred in the old data; the variant with the highestadjusted count was selected.1Latin-1 is an 8-bit encoding which has the common ac-cented characters used in Western European languages.
Areviewer pointed out that ISO 8859-15 has superseded ISO8859-1.1152.3 Reordering GermanGerman word order differs from English substan-tially.
Preprocessing approaches involving theuse of a syntactic parse of the source sentence tochange the word order to more closely match theword order of the target language have been stud-ied by Niessen and Ney (2004), Xia and McCord(2004), Dra?bek and Yarowsky (2004), Collins etal.
(2005), Popovic?
and Ney (2006), Wang et al(2007) and many others.To obtain a parse of each German sentence inthe training, dev and test corpora, we employed theIfNLP BitPar probabilistic parser (Schmid, 2004),using models learned from the Tiger Treebank forGerman.Dealing with morphological productivity is im-portant in the syntactic parsing of German.
Bit-Par has been designed with this in mind.
IfNLP?sSMOR analyzer is used for morphological analy-sis (Schmid et al, 2004).
SMOR is run over a listof types in each German sentence, and outputs alist of analyses for each type, each of which corre-sponds to a POS tag.
BitPar is limited to choosingone of these POS tags for this type.
Words whichSMOR fails to analyze are allowed to occur withany POS tag.We reimplemented the syntactic preprocessingapproach of Collins et al (2005), with modifica-tions.
Reordering rules are applied to a Germanparse tree (generated by BitPar), and focus on re-ordering the words in the German clause structureto more closely resemble English clause structure.The rules are applied to both the training data forthe SMT system, and the input (the dev and testsets).
We previously performed an error analysisof this approach and for the work described herewe addressed some of the shortcomings identifiedthrough the analysis.
The analysis was performedon the Europarl dev2006 set.The first error that we noticed occurring fre-quently was that some large clausal units whichwere labeled as subjects were being moved for-ward in the sentence.
We modified the rule movingsubjects forward to not apply to the constituents S,CS, VP and CVP.
See the first part of table 3 for anexample.
The phrase ?dass der Balkan ist kein Ge-biet?
is moved under the original rules, and withthe modification is no longer moved2.2Note that there is an unrelated reordering error at the endof the sentence for both BEFORE and AFTER, gibt (gloss:gives) should have moved to follow das (gloss: that).System BLEU METEOR LRno processing 18.91 49.50 1.0097c+w 19.37 49.69 1.0067c+w, s/s 19.18 51.13 1.0035c+w, old reordering 19.61 50.44 1.0092c+w, new reordering 19.91 50.84 1.0059c+w, new reordering, s/s(submitted, bug)19.65 51.57 1.0093* c+w, new reordering, s/s 19.73 51.59 1.0062as * IRSTLM quantized 19.52 51.33 1.0003as * IRSTLM 19.75 51.61 1.0013as * IRSTLM 21.2 quan-tized19.52 51.51 1.0095as * RANDLM 19.67 51.73 1.0067as * RANDLM 21.2 21.03 51.96 1.0111Table 1: German to English, dev-2009b (casesensitive), c+w = char+word normalization, s/s =splitting/stemming, 21.2 = larger LMSystem BLEU METEOR LRno processing 13.55 38.31 0.9910c+w (no second step) 14.11 38.27 0.9991c+w, s/s, second step(submitted, bug)12.34 37.89 1.0338c+w, s/s, second step 13.05 37.94 1.0157Table 2: English to German, dev-2009b (casesensitive), c+w = char+word normalization, s/s =splitting/stemmingThe second error that we handled was that S-RCconstituents which do not have a complementizerare reordered incorrectly.
We modified the orig-inal verb 2nd rule, so that if there is no comple-mentizer in a S-RC constituent, then the head ismoved to the second position, see the second partof table 3 for an example.
Using the original rules,the verb 2nd rule fails to fire, incorrectly leavinghaben (gloss: have) at the end of the clause.2.4 Morphological DecompositionWe implemented the frequency-based word split-ting approach of Koehn and Knight (2003), andmade modifications, including some similar tothose described by Stymne et al (2008).
Thiswell-known technique splits compound words.
Inaddition, we performed simple suffix elimination,aimed at removing inflection marking featuressuch as gender and case that are not necessary fortranslation to English.
We took the stem combi-nation with the highest geometric mean of the fre-quencies of the stems, but following Stymne et al(2008), we restricted stems to minimum length 4,and we allowed an extended list of infixes: s, n,en, nen, es, er and ien.
For suffixes, we allowed:e, en, n, es, s, em and er, which is more aggressive116INPUT Mir ist bewusst , dass der Balkan keinGebiet ist , das Anlass zu Optimismusgibt .gloss me is clear , that the Balkans not area is, that opportunity for optimism gives .BEFORE Mir dass der Balkan ist kein Gebiet istbewusst , , das Anlass zu Optimismusgibt .gloss me that the Balkans is not area is clear ,that opportunity for optimism gives .AFTER Mir ist bewusst , dass der Balkan ist keinGebiet , das Anlass zu Optimismus gibt.gloss me is clear , that the Balkans is not area, that opportunity for optimism gives .REF I am aware that the Balkans are not themost promising area for optimism .INPUT Am 23.
November 1999 hat ein Partner-schaftstag stattgefunden , an dem vielevon uns teilgenommen haben .gloss on 23 November 1999 have apartnership-day took-place , in whichmany of us participated have .BEFORE Am 23.
November 1999 ein Partner-schaftstag hat stattgefunden , an demteilgenommen viele von uns haben .gloss on 23 November 1999 a partnership-dayhave took-place , in which participatedmany of us have .AFTER Am 23.
November 1999 ein Partner-schaftstag hat stattgefunden , an demviele von uns haben teilgenommen .gloss on 23 November 1999 a partnership-dayhave took-place , in which many of ushave participated .REF A partnership day was held on 23November 1999 , in which many of usparticipated .Table 3: Differences in reordering: BEFORE is re-ordering using rules in (Collins et al, 2005), AF-TER is our modified reorderingthan used in previous work (and therefore gener-alizes more but at the same time causes some er-roneous conflation).
We stripped e, en and n fromall stems (but remembered the most frequent vari-ant, so that applying the procedure to Kirchturmresults in Kirche Turm (gloss: church tower)).
Westore an alignment from the original German to thesimplified German which we will use in the nextsection.2.5 Morphological GenerationFor translation from English to German, we firsttranslated from English to the simplified Germanpresented in the previous section, and then per-formed an independent translation step from sim-plified German to fully inflected German.Two processes are handled by this step.
First,series of stems corresponding to compound wordsare recomposed (along with infixes which are notpresent in the simplified German form) into com-pound words.
Second, inflection is added (e.g.,case and gender agreement is handled).
Both ofthese processes are implemented using a Mosessystem trained on a parallel corpus where thesource language is simplified German and the tar-get language is fully inflected German.
The align-ment is error-free as it was generated as a sideeffect of the splitting and stemming process de-scribed in the previous section.
In translation, re-ordering is not allowed, but we otherwise use stan-dard Moses settings.3 Experiments3.1 German to EnglishWe trained our German to English system on theconstrained parallel data.
The English data wasprocessed using character normalization.
The Ger-man data was first processed using character andword (writing reform) normalization.
We thenparsed the German data using BitPar and appliedthe modified reordering rules.
After this the split-ting and stemming process was applied.
Finally,we lowercased the data.Word alignments were generated using Model4 (Brown et al, 1993) using the multi-threadedimplementation of GIZA++ (Och and Ney, 2003;Gao and Vogel, 2008).
We first trained Model 4with English as the source language, and then withGerman as the source language, resulting in twoViterbi alignments3.
The resulting Viterbi align-ments were combined using the Grow Diag FinalAnd symmetrization heuristic (Koehn et al, 2003).We estimated a standard Moses system using de-fault settings.
MERT was run until convergenceusing dev-2009a (separately for each experiment).One limitation of our German to English systemis that we were unable to scale to the full languagemodeling data using SRILM (Stolcke, 2002), 5-grams and modified Kneser-Ney with no single-ton deletion4.
The language model in our sub-mitted system is based on all of the available En-glish data, but news-train08 is truncated to the first10193376 lines, meaning that we did not train onthe remaining 11038787 lines, so we used a littleless than half of the data.
We converted the lan-3We used 5 iterations of Model 1, 4 iterations of HMM(Vogel et al, 1996) and 4 iterations of Model 4.4SRILM failed when trained on the full data, even when amachine with 32 GB RAM and 48 GB swap was used.117guage model trained using SRILM to the binaryformat using IRSTLM.Experiments are presented in table 1, usingBLEU (Papineni et al, 2001) and METEOR5(Banerjee and Lavie, 2005), and we also showthe length ratio (ratio of hypothesized tokens toreference tokens).
For translation into EnglishMETEOR had superior correlation with humanrankings to BLEU at WMT 2008 (Callison-Burchet al, 2008).
Our submitted system had a bugwhere the environment variable LC ALL was setto en US when creating the binarized filtered lex-icalized reordering table for the test set (and forthe blindtest set, but not for the dev set used forMERT).
This caused minor degradation, see thesystem marked (*) for the system with the bug cor-rected.Each system increases in both BLEU and ME-TEOR as improvements are added.
An exceptionis that splitting/stemming decreases BLEU some-what.
However, we trust the METEOR resultsmore due to their better correlation with humanjudgements.We also compared using a different languagemodel instead of the SRILM model (the bottomhalf of table 1).
These used either the reducedEnglish language modeling data or the full data(21.2 M segments, marked 21.2 in the results).RANDLM (Talbot and Osborne, 2007) performswell and scaled to the full data with improvement(resulting in our best overall system).
IRSTLM(Federico and Cettolo, 2007) also performs well,but the quantized model on the 21.2 data didnot improve over the smaller quantized model6.IRSTLM uses an approximation of Witten-Bellsmoothing, our results support that this is compet-itive.3.2 English to GermanWe trained our English to German system on theconstrained parallel data.
The first SMT systemtranslates from lowercased English to lowercasedsimplified German, which is then recased.
Thesyntactic reordering process is not used, but other-wise the German data is processed identically.
Thealignment from simplified German to English isgenerated as described in the previous section.
Weused all of the German data to train the language5METEOR used default weights, stemming and Wordnetsynsets.6After speaking with the authors, we plan to try IRSTLMon the full data using memory mapping for binarization.model on simplified German.
The second SMTsystem translates mixed case simplified German tomixed case unsimplified German.
The translationmodel is built only on the simplified German fromthe parallel text, and the language model is trainedon all German data.We present the results in table 2.
METEOR7 didnot correlate as well as BLEU for translation out ofEnglish in WMT 2008.
The BLEU score of our fi-nal system is worse than the baseline.
We had cho-sen to submit this system as we found it more in-teresting than submitting a vanilla system.
In addi-tion, the system of Stymne et al (2008) received agood human evaluation despite having a relativelylow BLEU score, and we hoped we were perform-ing similar morphological generalization.
We ex-pect to be able to improve this system through er-ror analysis.
In an initial inspection we found casemismatching problems between step one and steptwo.4 ConclusionWe presented our German to English systemwhich employed character normalization, com-pensated for problems caused by the German writ-ing reform, used modified syntactic reorderingrules (in combination with morphologically awareparsing), and employed substring-based morpho-logical analysis.
Our best system improves by2.46 METEOR and 1.12 BLEU over a standardMoses system.
Our English to German sys-tem used the same two normalizations and thesubstring-based morphological analysis, and addi-tionally implemented a second translation step forrecreating compound words and generating caseand gender inflection.
We will improve this sys-tem in future work.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarizationat the 43th Annual Meeting of the Association ofComputational Linguistics (ACL-2005), Ann Arbor,Michigan.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and R. L. Mercer.
1993.
The mathe-matics of statistical machine translation: parameter7METEOR for this task is calculated using defaultweights but no Wordnet synsets.118estimation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InACL Third Workshop on Statistical Machine Trans-lation, Columbus, Ohio.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In ACL, pages 531?540, Ann Arbor, MI.Elliott F. Dra?bek and David Yarowsky.
2004.
Improv-ing bitext word alignments via syntax-based reorder-ing of English.
In The Companion Volume to theProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages 146?149, Barcelona, Spain.Marcello Federico and Mauro Cettolo.
2007.
Efficienthandling of n-gram language models for statisticalmachine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages88?95, Prague, Czech Republic.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In EACL, pages187?193, Morristown, NJ.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL,pages 127?133, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, Demonstration Program, Prague, Czech Re-public.Sonja Niessen and Hermann Ney.
2004.
Statisti-cal machine translation with scarce resources usingmorpho-syntactic information.
Computational Lin-guistics, 30(2):181?204.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Kishore A. Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
BLEU: a method for auto-matic evaluation of machine translation.
TechnicalReport RC22176 (W0109-022), IBM Research Di-vision, Thomas J. Watson Research Center, York-town Heights, NY.Maja Popovic?
and Hermann Ney.
2006.
POS-basedword reorderings for statistical machine translation.In LREC, pages 1278?1283, Genoa, Italy.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
SMOR: a German computational morphologycovering derivation, composition, and inflection.
InLREC, pages 1263?1266, Lisbon, Portugal.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InCOLING, Geneva, Switzerland.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2008.
Effects of morphological analysis in transla-tion between German and English.
In Proceedingsof the Third Workshop on Statistical Machine Trans-lation, pages 135?138, Columbus, Ohio.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In ACL, pages 512?519, Prague, Czech Re-public.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In COLING, pages 836?841, Copen-hagen, Denmark.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In EMNLP-CONLL, pages737?745.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In COLING.119
