Learning with Unlabeled Data for Text Categorization Using Bootstrappingand Feature Projection TechniquesYoungjoong KoDept.
of Computer Science, Sogang Univ.Sinsu-dong 1, Mapo-guSeoul, 121-742, Koreakyj@nlpzodiac.sogang.ac.krJungyun SeoDept.
of Computer Science, Sogang Univ.Sinsu-dong 1, Mapo-guSeoul, 121-742, Koreaseojy@ccs.sogang.ac.krAbstractA wide range of supervised learningalgorithms has been applied to TextCategorization.
However, the supervisedlearning approaches have some problems.
Oneof them is that they require a large, oftenprohibitive, number of labeled trainingdocuments for accurate learning.
Generally,acquiring class labels for training data is costly,while gathering a large quantity of unlabeleddata is cheap.
We here propose a newautomatic text categorization method forlearning from only unlabeled data using abootstrapping framework and a featureprojection technique.
From results of ourexperiments, our method showed reasonablycomparable performance compared with asupervised method.
If our method is used in atext categorization task, building textcategorization systems will becomesignificantly faster and less expensive.1 IntroductionText categorization is the task of classifyingdocuments into a certain number of pre-definedcategories.
Many supervised learning algorithmshave been applied to this area.
These algorithmstoday are reasonably successful when providedwith enough labeled or annotated trainingexamples.
For example, there are Naive Bayes(McCallum and Nigam, 1998), Rocchio (Lewis etal., 1996), Nearest Neighbor (kNN) (Yang et al,2002), TCFP (Ko and Seo, 2002), and SupportVector Machine (SVM) (Joachims, 1998).However, the supervised learning approach hassome difficulties.
One key difficulty is that itrequires a large, often prohibitive, number oflabeled training data for accurate learning.
Since alabeling task must be done manually, it is apainfully time-consuming process.
Furthermore,since the application area of text categorization hasdiversified from newswire articles and web pagesto E-mails and newsgroup postings, it is also adifficult task to create training data for eachapplication area (Nigam et al, 1998).
In this light,we consider learning algorithms that do not requiresuch a large amount of labeled data.While labeled data are difficult to obtain,unlabeled data are readily available and plentiful.Therefore, this paper advocates using abootstrapping framework and a feature projectiontechnique with just unlabeled data for textcategorization.
The input to the bootstrappingprocess is a large amount of unlabeled data and asmall amount of seed information to tell the learnerabout the specific task.
In this paper, we considerseed information in the form of title wordsassociated with categories.
In general, sinceunlabeled data are much less expensive and easierto collect than labeled data, our method is usefulfor text categorization tasks including online datasources such as web pages, E-mails, andnewsgroup postings.To automatically build up a text classifier withunlabeled data, we must solve two problems; howwe can automatically generate labeled trainingdocuments (machine-labeled data) from only titlewords and how we can handle incorrectly labeleddocuments in the machine-labeled data.
This paperprovides solutions for these problems.
For the firstproblem, we employ the bootstrapping framework.For the second, we use the TCFP classifier withrobustness from noisy data (Ko and Seo, 2004).How can labeled training data be automaticallycreated from unlabeled data and title words?Maybe unlabeled data don?t have any informationfor building a text classifier because they do notcontain the most important information, theircategory.
Thus we must assign the class to eachdocument in order to use supervised learningapproaches.
Since text categorization is a taskbased on pre-defined categories, we know thecategories for classifying documents.
Knowing thecategories means that we can choose at least arepresentative title word of each category.
This isthe starting point of our proposed method.
As wecarry out a bootstrapping task from these titlewords, we can finally get labeled training data.Suppose, for example, that we are interested inclassifying newsgroup postings about specially?Autos?
category.
Above all, we can select?automobile?
as a title word, and automaticallyextract keywords (?car?, ?gear?, ?transmission?,?sedan?, and so on) using co-occurrenceinformation.
In our method, we use context (asequence of 60 words) as a unit of meaning forbootstrapping from title words; it is generallyconstructed as a middle size of a sentence and adocument.
We then extract core contexts thatinclude at least one of the title words and thekeywords.
We call them centroid-contexts becausethey are regarded as contexts with the coremeaning of each category.
From the centroid-contexts, we can gain many words contextually co-occurred with the title words and keywords:?driver?, ?clutch?, ?trunk?, and so on.
They arewords in first-order co-occurrence with the titlewords and the keywords.
To gather morevocabulary, we extract contexts that are similar tocentroid-contexts by a similarity measure; theycontain words in second-order co-occurrence withthe title words and the keywords.
We finallyconstruct context-cluster of each category as thecombination of centroid-contexts and contextsselected by the similarity measure.
Using thecontext-clusters as labeled training data, a NaiveBayes classifier can be built.
Since the NaiveBayes classifier can label all unlabeled documentsfor their category, we can finally obtain labeledtraining data (machine-labeled data).When the machine-labeled data is used to learn atext classifier, there is another difficult in that theyhave more incorrectly labeled documents thanmanually labeled data.
Thus we develop andemploy the TCFP classifiers with robustness fromnoisy data.The rest of this paper is organized as follows.Section 2 reviews previous works.
In section 3 and4, we explain the proposed method in detail.Section 5 is devoted to the analysis of theempirical results.
The final section describesconclusions and future works.2 Related WorksIn general, related approaches for using unlabeleddata in text categorization have two directions;One builds classifiers from a combination oflabeled and unlabeled data (Nigam, 2001; Bennettand Demiriz, 1999), and the other employsclustering algorithms for text categorization(Slonim et al, 2002).Nigam studied an Expected Maximization (EM)technique for combining labeled and unlabeleddata for text categorization in his dissertation.
Heshowed that the accuracy of learned text classifierscan be improved by augmenting a small number oflabeled training data with a large pool of unlabeleddata.Bennet and Demiriz achieved smallimprovements on some UCI data sets using SVM.It seems that SVMs assume that decisionboundaries lie between classes in low-densityregions of instance space, and the unlabeledexamples help find these areas.Slonim suggested clustering techniques forunsupervised document classification.
Given acollection of unlabeled data, he attempted to findclusters that are highly correlated with the truetopics of documents by unsupervised clusteringmethods.
In his paper, Slonim proposed a newclustering method, the sequential InformationBottleneck (sIB) algorithm.3 The Bootstrapping Algorithm for CreatingMachine-labeled DataThe bootstrapping framework described in thispaper consists of the following steps.
Each moduleis described in the following sections in detail.1.
Preprocessing: Contexts are separated fromunlabeled documents and content words areextracted from them.2.
Constructing context-clusters for training:- Keywords of each category are created- Centroid-contexts are extracted and verified- Context-clusters are created by a similaritymeasure3.
Learning Classifier: Naive Bayes classifier arelearned by using the context-clusters3.1 PreprocessingThe preprocessing module has two main roles:extracting content words and reconstructing thecollected documents into contexts.
We use the BrillPOS tagger to extract content words (Brill, 1995).Generally, the supervised learning approach withlabeled data regards a document as a unit ofmeaning.
But since we can use only the title wordsand unlabeled data, we define context as a unit ofmeaning and we employ it as the meaning unit tobootstrap the meaning of each category.
In oursystem, we regard a sequence of 60 content wordswithin a document as a context.
To extract contextsfrom a document, we use sliding windowtechniques (Maarek et al, 1991).
The window is aslide from the first word of the document to the lastin the size of the window (60 words) and theinterval of each window (30 words).
Therefore, thefinal output of preprocessing is a set of contextvectors that are represented as content words ofeach context.3.2 Constructing Context-Clusters forTrainingAt first, we automatically create keywords from atitle word for each category using co-occurrenceinformation.
Then centroid-contexts are extractedusing the title word and keywords.
They contain atleast one of the title and keywords.
Finally, we cangain more information of each category byassigning remaining contexts to each context-cluster using a similarity measure technique; theremaining contexts do not contain any keywords ortitle words.3.2.1 Creating Keyword ListsThe starting point of our method is that we havetitle words and collected documents.
A title wordcan present the main meaning of each category butit could be insufficient in representing anycategory for text categorization.
Thus we need tofind words that are semantically related to a titleword, and we define them as keywords of eachcategory.The score of semantic similarity between a titleword, T, and a word, W, is calculated by the cosinemetric as follows:???===?
?=ni ini ini iiwtwtWTsim12121),(               (1)where ti and wi represent the occurrence (binaryvalue: 0 or 1) of words T and W in i-th documentrespectively, and n is the total number ofdocuments in the collected documents.
Thismethod calculates the similarity score betweenwords based on the degree of their co-occurrencein the same document.Since the keywords for text categorization musthave the power to discriminate categories as wellas similarity with the title words, we assign a wordto the keyword list of a category with themaximum similarity score and recalculate the scoreof the word in the category using the followingformula:)),(),((),(),( maxsecmaxmaxmax WTsimWTsimWTsimcWScore ond?+=  (2)where Tmax is the title word with the maximumsimilarity score with a word W, cmax is the categoryof the title word Tmax, and Tsecondmax is other titleword with the second high similarity score with theword W.This formula means that a word with highranking in a category has a high similarity scorewith the title word of the category and a highsimilarity score difference with other title words.We sort out words assigned to each categoryaccording to the calculated score in descendingorder.
We then choose top m words as keywords inthe category.
Table 1 shows the list of keywords(top 5) for each category in the WebKB data set.Table 1.
The list of keywords in the WebKB data setCategory Title Word Keywordscourse course assignments, hours, instructor, class, fallfaculty professor associate, ph.d, fax, interests, publicationsproject project system, systems, research, software, informationstudent student graduate, computer, science, page, university3.2.2 Extracting and Verifying Centroid-ContextsWe choose contexts with a keyword or a title wordof a category as centroid-contexts.
Amongcentroid-contexts, some contexts could not havegood features of a category even though theyinclude the keywords of the category.
To rank theimportance of centroid-contexts, we compute theimportance score of each centroid-context.
First ofall, weights (Wij) of word wi in j-th category arecalculated using Term Frequency (TF) within acategory and Inverse Category Frequency (ICF)(Cho and Kim, 1997) as follows:))log()(log( iijiijij CFMTFICFTFW ?
?=?=     (3)where CFi is the number of categories that containwi and M is the total number of categories.Using word weights (Wij) calculated by formula3, the score of a centroid-context (Sk) in j-thcategory (cj) is computed as follows:NWWWcSScore Njjjjk+++= ...),( 21            (4)where N is the  number of words in the centroid-context.As a result, we obtain a set of words in first-order co-occurrence from centroid-contexts of eachcategory.3.2.3 Creating Context-ClustersWe gather the second-order co-occurrenceinformation by assigning remaining contexts to thecontext-cluster of each category.
For the assigningcriterion, we calculate similarity betweenremaining contexts and centroid-contexts of eachcategory.
Thus we employ the similarity measuretechnique by Karov and Edelman (1998).
In ourmethod, a part of this technique is reformed for ourpurpose and remaining contexts are assigned toeach context-cluster by that revised technique.1) Measurement of word and context similaritiesAs similar words tend to appear in similar contexts,we can compute the similarity by using contextualinformation.
Words and contexts playcomplementary roles.
Contexts are similar to theextent that they contain similar words, and wordsare similar to the extent that they appear in similarcontexts (Karov and Edelman, 1998).
Thisdefinition is circular.
Thus it is applied iterativelyusing two matrices, WSM and CSM.Each category has a word similarity matrixWSMn and a context similarity matrix CSMn.
Ineach iteration n, we update WSMn, whose rows andcolumns are labeled by all content wordsencountered in the centroid-contexts of eachcategory and input remaining contexts.
In thatmatrix, the cell (i,j) holds a value between 0 and 1,indicating the extent to which the i-th word iscontextually similar to the j-th word.
Also, we keepand update a CSMn, which holds similaritiesamong contexts.
The rows of CSMn correspond tothe remaining contexts and the columns to thecentroid-contexts.
In this paper, the number ofinput contexts of row and column in CSM islimited to 200, considering execution time andmemory allocation, and the number of iterations isset as 3.To compute the similarities, we initialize WSMnto the identity matrix.
The following steps areiterated until the changes in the similarity valuesare small enough.1.
Update the context similarity matrix CSMn,using the word similarity matrix WSMn.2.
Update the word similarity matrix WSMn, using thecontext similarity matrix CSMn.2) Affinity formulaeTo simplify the symmetric iterative treatment ofsimilarity between words and contexts, we definean auxiliary relation between words and contextsas affinity.Affinity formulae are defined as follows (Karovand Edelman, 1998):),(max),( inXWn WWsimXWaff i?=   (5)(6)                    ),(max),( jnXWn XXsimWXaff j?=In the above formulae, n denotes the iterationnumber, and the similarity values are defined byWSMn and CSMn.
Every word has some affinity tothe context, and the context can be represented bya vector indicating the affinity of each word to it.3) Similarity formulaeThe similarity of W1 to W2 is the average affinity ofthe contexts that include W1 to W2, and thesimilarity of a context X1 to X2 is a weightedaverage of the affinity of the words in X1 to X2.Similarity formulae are defined as follows:),(),(),( 212111XWaffXWweightXXsim nXWn ?= ?
?+ (7)(8)),(),(),(1),(21211211211WXaffWXweightWWsimelseWWsimWWifnXWnn?===?
?++The weights in formula 7 are computed asreflecting global frequency, log-likelihood factors,and part of speech as used in (Karov and Edelman,1998).
The sum of weights in formula 8, which is areciprocal number of contexts that contain W1, is 1.4) Assigning remaining contexts to a categoryWe decided a similarity value of each remainingcontext for each category using the followingmethod:),(),(   ??????=?
?jCCSiCcSXsimavercXsimicji(9)In formula 9, i) X is a remaining context, ii){ }mcccC ,...,, 21= is a category set, and iii) { }nc SSi ,...,1=CC isa controid-contexts set of category ci.Each remaining context is assigned to a categorywhich has a maximum similarity value.
But theremay exist noisy remaining contexts which do notbelong to any category.
To remove these noisyremaining contexts, we set up a dropping thresholdusing normal distribution of similarity values asfollows (Ko and Seo, 2000):} ),( max{Cci???
+??
icXsim (10)where i) X is a remaining context, ii) ?
is anaverage of similarity values , iii) ?
is astandard deviation of similarity values, and iv) ?
isa numerical value corresponding to the threshold(%) in normal distribution table.
),( iCc cXsimi?Finally, a remaining context is assigned to thecontext-cluster of any category when the categoryhas a maximum similarity above the droppingthreshold value.
In this paper, we empirically use a15% threshold value from an experiment using avalidation set.3.3 Learning the Naive Bayes Classifier UsingContext-ClustersIn above section, we obtained labeled training data:context-clusters.
Since training data are labeled asthe context unit, we employ a Naive Bayesclassifier because it can be built by estimating theword probability in a category, but not in adocument.
That is, the Naive Bayes classifier doesnot require labeled data with the unit of documentsunlike other classifiers.We use the Naive Bayes classifier with minormodifications based on Kullback-LeiblerDivergence (Craven et al, 2000).
We classify adocument di according to the following formula:??==????????+??=||1||1),()?;|()?;|(log)?;|()?;(log)?;|()?|()?|()?;|()?|()?
;|(Vt itjtitjVtdwNjtjijijijdwPcwPdwPncPcwPcPdPcdPcPdcP i??????????
(11)where i) n is the number of words in document di,ii) wt is the t-th word in the vocabulary, iii) N(wt,di)is the frequency of word wt in document di.Here, the Laplace smoothing is used to estimatethe probability of word wt in class cj and theprobability of class cj as follows:?=++=||1),(||),(1)?
;|(Vt ctctjtjjGwNVGwNcwP ?
(12)?++=iijc ccjGCGcP||||||1)?|( ?
(13)where  is the count of the number of timesword w),(jct GwNt occurs in the context-cluster ( ) ofcategory cjcGj.4 Using a Feature Projection Technique forHandling Noisy Data of Machine-labeledDataWe finally obtained labeled data of a documentsunit, machine-labeled data.
Now we can learn textclassifiers using them.
But since the machine-labeled data are created by our method, theygenerally include far more incorrectly labeleddocuments than the human-labeled data.
Thus weemploy a feature projection technique for ourmethod.
By the property of the feature projectiontechnique, a classifier (the TCFP classifier) canhave robustness from noisy data (Ko and Seo,2004).
As seen in our experiment results, TCFPshowed the highest performance amongconventional classifiers in using machine-labeleddata.The TCFP classifier with robustness from noisydataHere, we simply describe the TCFP classifier usingthe feature projection technique (Ko and Seo,2002; 2004).
In this approach, the classificationknowledge is represented as sets of projections oftraining data on each feature dimension.
Theclassification of a test document is based on thevoting of each feature of that test document.
Thatis, the final prediction score is calculated byaccumulating the voting scores of all features.First of all, we must calculate the voting ratio ofeach category for all features.
Since elements witha high TF-IDF value in projections of a featuremust become more useful classification criteria forthe feature, we use only elements with TF-IDFvalues above the average TF-IDF value for voting.And the selected elements participate inproportional voting with the same importance asthe TF-IDF value of each element.
The voting ratioof each category cj in a feature tm is calculated bythe following formula:????
?=mmmmjjIltlmIltmlmm dtwltcydtwtcr)()(),())(,(),(),(rr(14)In formula 14, w ),( dtmris the weight of term tm indocument d, Im denotes a set of elements selectedfor voting and  is a function; if thecategory for an element t  is equal to c , theoutput value is 1.
Otherwise, the output value is 0.
{ }1.0?
)(lm))(,( ltcy mjjNext, since each feature separately votes onfeature projections, contextual information ismissing.
Thus we calculate co-occurrencefrequency of features in the training data andmodify TF-IDF values of two terms ti and tj in atest document by co-occurrence frequency betweenthem; terms with a high co-occurrence frequencyvalue have higher term weights.Finally, the voting score of each category c inthe m-th feature tjm of a test document d iscalculated by the following formula:))(1log(),(),(),( 2 mmmm ttcrdttwtcvs jj ?+?
?=r(15)where tw(tm,d) denotes a modified term weight bythe co-occurrence frequency and denotesthe calculated ?
)(2 mt?m2 statistics value of .
tTable 2.
The top micro-avg F1 scores and  precision-recall breakeven points of each method.OurMethod(basis)OurMethod(NB)OurMethod(Rocchio)OurMethod(kNN)OurMethod(SVM)OurMethod(TCFP)Newsgroups 79.36 83.46 83 79.95 82.49 86.19WebKB 73.63 73.22 75.28 68.04 73.74 75.47Reuters 88.62 88.23 86.26 85.65 87.41 89.09The outline of the TCFP classifier is as follow:5 Empirical Evaluation5.1 Data Sets and Experimental SettingsTo test our method, we used three different kindsof data sets: UseNet newsgroups (20 Newsgroups),web pages (WebKB), and newswire articles(Reuters 21578).
For fair evaluation inNewsgroups and WebKB, we employed the five-fold cross-validation method.The Newsgroups data set, collected by KenLang, contains about 20,000 articles evenlydivided among 20 UseNet discussion groups(McCallum and Nigam, 1998).
In this paper, weused only 16 categories after removing 4categories: three miscellaneous categories(talk.politics.misc, talk.religion.misc, andcomp.os.ms-windows.misc) and one duplicatemeT Bpro etcon tycomT etcon esfro in(N uscatAeacappstaclaAstameacross categories, we used the micro-averagingmethod (Yang et al, 2002).
Results on Reuters arereported as precision-recall breakeven points,which is a standard information retrieval measurefor binary classification (Joachims, 1998).1. input : test document: dr=<t1,t2,?,tn>2.
main processFor each feature titw(ti,d) is calculatedFor each feature tiFor each category cjvote[cj]=vote[cj]+vs(cj,ti) by Formula 15prediction = ][maxarg jccvotejTitle words in our experiment are selectedaccording to category names of each data set (seeTable 1 as an example).5.2 Experimental Results5.2.1 Observing the Performance According tothe Number of KeywordsFirst of all, we determine the number of keywordsin our method using the validation set.
Thenumber of keywords is limited by the top m-thkeyword from the ordered list of each category.Figure 1 displays the performance at differentnumber of keywords (from 0 to 20) in each data set.404550556065707580850 1 2 3 4 5 8 10 13 15 18 20The number of keywordsMicro-avg.F1Newsgroups WebKB ReutersFigure 1.
The comparison of performance according tothe number of keywordsWe set the number of keywords to 2 inNewsgroups, 5 in WebKB, and 3 in Reutersempirically.
Generally, we recommend that thenumber of keywords be between 2 and 5.5.2.2 Comparing our Method Using TCFP withthose Using other ClassifiersIn this section, we prove the superiority of TCFPover the other classifiers (SVM, kNN, Naive Bayes aning category (comp.sys.
ibm.pc.hardware).he second data set comes from the WebKject at CMU (Craven et al, 2000).
This data stains web pages gathered from universiputer science departments.he Reuters 21578 Distribution 1.0 data ssists of 12,902 articles and 90 topic categorim the Reuters newswire.
Like other studyigam, 2001), we used the ten most populoegories to identify the news topic.
bout 25% documents from training data ofh data set are selected for a validation set.
Welied a statistical feature selection method (?2tistics) to a preprocessing stage for eachssifier (Yang and Pedersen, 1997).s performance measures, we followed thendard definition of recall, precision, and F1asure.
For evaluation performance average(NB), Roccio) in training data with much noisydata such as machine-labeled data.
As shown inTable 2, we obtained the best performance in usingTCFP at all three data sets.Let us define the notations.
OurMethod(basis)denotes the Naive Bayes classifier using labeledcontexts and OurMethod(NB) denotes the NaiveBayes classifier using machine-labeled data astraining data.
The same manner is applied for otherclassifiers.OurMethod(TCFP) achieved more advancedscores than OurMethod(basis): 6.83 inNewsgroups, 1.84 in WebKB, and  0.47 in Reuters.5.2.3 Comparing with the Supervised NaiveBayes ClassifierFor this experiment, we consider two possiblecases for labeling task.
The first task is to label apart of collected documents and the second is tolabel all of them.
As the first task, we built up anew training data set; it consists of 500 differentdocuments randomly chosen from appropriatecategories like the experiment in (Slonim et al,2002).
As a result, we report performances fromtwo kinds of Naive Bayes classifiers which arelearned from 500 training documents and thewhole training documents respectively.Table 3.
The comparison of our method and thesupervised NB classifierOurMethod(TCFP)NB(500)NB(All)Newsgroups 86.19 72.68 91.72WebKB 75.47 74.1 85.29Reuters 89.09 82.1 91.64In Table 3, the results of our method are higherthan those of NB(500) and are comparable to thoseof NB(All) in all data sets.
Especially, the result inReuters reached 2.55 close to that of NB(All)though it used the whole labeled training data.5.2.4 Enhancing our Method from ChoosingKeywords by HumanThe main problem of our method is that theperformance depends on the quality of thekeywords and title words.
As we have seen inTable 3, we obtained the worst performance in theWebKB data set.
In fact, title words and keywordsof each category in the WebKB data set alo havehigh frequency in other categories.
We think thesefactors contribute to a comparatively poorperformance of our method.
If keywords as well astitle words are supplied by humans, our methodmay achieve higher performance.
However,choosing the proper keywords for each category isa much difficult task.
Moreover, keywords fromdevelopers, who have insufficient knowledge aboutan application domain, do not guarantee highperformance.
In order to overcome this problem,we propose a hybrid method for choosingkeywords.
That is, a developer obtains 10candidate keywords from our keyword extractionmethod and then they can choose proper keywordsfrom them.
Table 4 shows the results from threedata sets.Table 4.
The comparison of our method and enhancingmethodOurMethod(TCFP)Enhancing(TCFP)) ImprovementNewsgroups 86.19 86.23 +0.04WebKB 75.47 77.59 +2.12Reuters 89.09 89.52 +0.43As shown in Table 4, especially we could achievesignificant improvement in the WebKb data set.Thus we find that the new method for choosingkeywords is more useful in a domain withconfused keywords between categories such as theWebKB data set.5.2.5 Comparing with a Clustering TechniqueIn related works, we presented two approachesusing unlabeled data in text categorization; oneapproach combines unlabeled data and labeled data,and the other approach uses the clusteringtechnique for text categorization.
Since our methoddoes not use any labeled data, it cannot be fairlycompared with the former approaches.
Therefore,we compare our method with a clusteringtechnique.
Slonim et al (2002) proposed a newclustering algorithm (sIB) for unsuperviseddocument classification and verified the superiorityof his algorithm.
In his experiments, the sIBalgorithm was superior to other clusteringalgorithms.
As we set the same experimentalsettings as in Slonim?s experiments and conductexperiments, we verify that our methodoutperforms ths sIB algorithm.
In our experiments,we used the micro-averaging precision asperformance measure and two revised data sets:revised_NG, revised_Reuters.
These data sets wererevised in the same way according to Slonim?spaper as follows:In revised_NG, the categories of Newsgroups wereunited with respect to 10 meta-categories: five compcategories, three politics categories, two sportscategories, three religions categories, and twotransportation categories into five big meta-categories.The revised_Reuters used the 10 most frequentcategories in the Reuters 21578 corpus under theModApte split.As shown in Table 5, our method shows 6.65advanced score in revised_NG and 3.2 advancedscore in revised_Reuters.Table 5.
The comparison of our method and sIBsIB OurMethod(TCFP) Improvementrevised_NG 79.5 86.15 +6.65revised_Reuters 85.8 89 +3.26 Conclusions and Future WorksThis paper has addressed a new unsupervised orsemi-unsupervised text categorization method.Though our method uses only title words andunlabeled data, it shows reasonably comparableperformance in comparison with that of thesupervised Naive Bayes classifier.
Moreover, itoutperforms a clustering method, sIB.
Labeled dataare expensive while unlabeled data are inexpensiveand plentiful.
Therefore, our method is useful forlow-cost text categorization.
Furthermore, if sometext categorization tasks require high accuracy, ourmethod can be used as an assistant tool for easilycreating labeled training data.Since our method depends on title words andkeywords, we need additional studies about thecharacteristics of candidate words for title wordsand keywords according to each data set.AcknowledgementThis work was supported by grant No.
R01-2003-000-11588-0 from the basic Research Program ofthe KOSEFReferencesK.
Bennett and A. Demiriz, 1999, Semi-supervisedSupport Vector Machines, Advances in NeuralInformation Processing Systems 11, pp.
368-374.E.
Brill, 1995, Transformation-Based Error-drivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging, ComputationalLinguistics, Vol.21, No.
4.K.
Cho and J. Kim, 1997, Automatic TextCategorization on Hierarchical Category Structure byusing ICF (Inverse Category Frequency) Weighting,In Proc.
of KISS conference, pp.
507-510.M.
Craven, D. DiPasquo, D. Freitag, A. McCallum, T.Mitchell, K. Nigam, and S. Slattery, 2000, Learningto construct knowledge bases from the World WideWeb, Artificial Intelligence, 118(1-2), pp.
69-113.T.
Joachims, 1998, Text Categorization with SupportVector Machines: Learning with Many RelevantFeatures.
In Proc.
of ECML, pp.
137-142.Y.
Karov and S. Edelman, 1998, Similarity-based WordSense Disambiguation, Computational Linguistics,Vol.
24, No.
1, pp.
41-60.Y.
Ko and J. Seo, 2000, Automatic Text Categorizationby Unsupervised Learning, In Proc.
ofCOLING?2000, pp.
453-459.Y.
Ko and J. Seo, 2002, Text Categorization usingFeature Projections, In Proc.
of COLING?2002, pp.467-473.Y.
Ko and J. Seo, 2004, Using the Feature ProjectionTechnique based on the Normalized Voting Methodfor Text Classification, Information Processing andManagement, Vol.
40, No.
2, pp.
191-208.D.D.
Lewis, R.E.
Schapire, J.P. Callan, and R. Papka,1996, Training Algorithms for Linear TextClassifiers.
In Proc.
of SIGIR?96, pp.289-297.Y.
Maarek, D. Berry, and G. Kaiser, 1991, AnInformation Retrieval Approach for AutomaticallyConstruction Software Libraries, IEEE Transactionon Software Engineering, Vol.
17, No.
8, pp.
800-813.A.
McCallum and K. Nigam, 1998, A Comparison ofEvent Models for Naive Bayes Text Classification.AAAI ?98 workshop on Learning for TextCategorization, pp.
41-48.K.
P. Nigam, A. McCallum, S. Thrun, and T. Mitchell,1998, Learning to Classify Text from Labeled andUnlabeled Documents, In Proc.
of AAAI-98.K.
P. Nigam, 2001, Using Unlabeled Data to ImproveText Classification, The dissertation for the degree ofDoctor of Philosophy.N.
Slonim, N. Friedman, and N. Tishby, 2002,Unsupervised Document Classification usingSequential Information Maximization, In Proc.
ofSIGIR?02, pp.
129-136.Y.
Yang and J. P. Pedersen.
1997, Feature selection instatistical leaning of text categorization.
In Proc.
ofICML?97, pp.
412-420.Y.
Yang, S. Slattery, and R. Ghani.
2002, A study ofapproaches to hypertext categorization, Journal ofIntelligent Information Systems, Vol.
18, No.
2.
