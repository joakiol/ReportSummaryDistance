Proceedings of the ACL Student Research Workshop, pages 139?144,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsCorpus-Oriented Development of Japanese HPSG ParsersKazuhiro YoshidaDepartment of Computer Science,University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033kyoshida@is.s.u-tokyo.ac.jpAbstractThis paper reports the corpus-oriented de-velopment of a wide-coverage JapaneseHPSG parser.
We first created an HPSGtreebank from the EDR corpus by us-ing heuristic conversion rules, and thenextracted lexical entries from the tree-bank.
The grammar developed using thismethod attained wide coverage that couldhardly be obtained by conventional man-ual development.
We also trained a statis-tical parser for the grammar on the tree-bank, and evaluated the parser in terms ofthe accuracy of semantic-role identifica-tion and dependency analysis.1 IntroductionIn this study, we report the corpus-oriented de-velopment of a Japanese HPSG parser using theEDR Japanese corpus (2002).
Although several re-searchers have attempted to utilize linguistic gram-mar theories, such as LFG (Bresnan and Kaplan,1982), CCG (Steedman, 2001) and HPSG (Pollardand Sag, 1994), for parsing real-world texts, such at-tempts could hardly be successful, because manualdevelopment of wide-coverage linguistically moti-vated grammars involves years of labor-intensive ef-fort.Corpus-oriented grammar development is a gram-mar development method that has been proposed asa promising substitute for conventional manual de-velopment.
In corpus-oriented methods, a treebankof a target grammar is constructed first, and variousgrammatical constraints are extracted from the tree-bank.
Previous studies reported that wide-coveragegrammars can be obtained at low cost by using thismethod.
(Hockenmaier and Steedman, 2002; Miyaoet al, 2004) The treebank can also be used for train-ing statistical disambiguation models, and hence wecan construct a statistical parser for the extractedgrammar.The corpus-oriented method enabled us to de-velop a Japanese HPSG parser with semantic infor-mation, whose coverage on real-world sentences is95.3%.
This high coverage allowed us to evaluatethe parser in terms of the accuracy of dependencyanalysis on real-world texts, the evaluation measurethat is previously used for more statistically-orientedparsers.2 HPSGHead-Driven Phrase Structure Grammar (HPSG) isclassified into lexicalized grammars (Schabes et al,1988).
It attempts to model linguistic phenomenaby interactions between a small number of grammarrules and a large number of lexical entries.
Figure1 shows an example of an HPSG derivation of aJapanese sentence ?kare ga shinda,?
which means,?He died.?
In HPSG, linguistic entities such as wordsand phrases are represented by typed feature struc-tures called signs, and the grammaticality of a sen-tence is verified by applying grammar rules to a se-quence of signs.
The sign of a lexical entry encodesthe type and valence (i.e.
restriction on the types ofphrases that can appear around the word) of a corre-sponding word.
Grammar rules of HPSG consist of139RULE complement_headSIGNHEAD verbSPRCOMPSHEAD verbSPRCOMPS 2 PP"ga""shinda"diedRULE specifier_headSIGN 2HEAD PP"ga"SPRCOMPSHEAD PP"ga"SPR 1 nounCOMPS"ga"NOM1HEAD nounSPRCOMPS"kare"heFigure 1: Example of HPSG analysis.schemata and principles, the former enumerate pos-sible patterns of phrase structures, and the latter arebasically for controlling the inheritance of daugh-ters?
features to the parent.In the current example, the lexical entry for?shinda?
is of the type verb, as indicated in itsHEAD, and its COMPS feature restricts its preced-ing phrase to be of the type PP?ga?.
The HEADfeature of the root node of the derivation is inher-ited from the lexical entry for ?shinda?, becausecomplement-head structures are head-final, and thehead feature principle states that the HEAD featureof a phrase must be inherited from its head daughter.There are several implementations of JapaneseHPSG grammars.
JACY (Siegel and Bender, 2002)is a hand-crafted Japanese HPSG grammar that pro-vides semantic information as well as linguisticallymotivated analysis of complex constructions.
How-ever, the evaluation of the grammar has not beendone on domain-independent real-world texts suchas newspaper articles.
Although Bond et al (2004)attempted to improve the coverage of the JACYgrammar through the development of an HPSG tree-bank, they limited the target of their treebank an-notation to short sentences from dictionary defini-tions.
SLUNG (Mitsuishi et al, 1998) is an HPSGgrammar whose coverage on real-world sentencesis about 99%, but the grammar is underspecified,which means that the constraints of the grammar arenot sufficient for conducting semantic analysis.
Byemploying corpus-oriented development, we aim todevelop a wide-coverage HPSG parser that enablessignSYNSEMsynsemLOCALlocalCATcatHEADheadMOD RIGHT synsemLEFT synsemBAR phrase/chunkVALSPR localCOMPSAGENT localOBJECT localGOAL localCONT contentFigure 2: Sign of the grammar.semantic analysis of real-word texts.3 Grammar DesignFirst, we provide a brief description of some char-acteristics of Japanese.
Japanese is head final, andphrases are typically headed by function words.
Ar-guments of verbs usually have no fixed order (thisphenomenon is called scrambling) and are freelyomitted.
Arguments?
semantic relations to verbsare chiefly determined by their head postpositions.For example, ?boku/I ga/NOM kare/he wo/ACC ko-roshi/kill ta/DECL?
(I killed him) can be paraphrasedas ?kare wo boku ga koroshi ta,?
without changingthe meaning.The case alternation phenomenon must also betaken into account.
Case alternation is caused byspecial auxiliaries ?(sa)se?
and ?(ra)re,?
which arecausative and passive auxiliaries, respectively, andthe verbs change their subcategorization behaviorwhen they are combined with these auxiliaries.The following sections describe the design of ourgrammar.
Especially, treatment of the scramblingand case alternation phenomena is provided in de-tail.3.1 Fundamental Phrase StructuresFigure 2 presents the basic structure of signs of ourgrammar.
The HEAD feature specifies phrasal cat-egories, the MOD feature represents restrictions onthe left and right modifiees, and the VAL feature en-codes valence information.
(For the explanation ofthe BAR feature, see the description of the promo-140Table 1: Schemata and their uses.schema name common use of the rulespecifier-head PP or NP + postpositionVP + verbal endingNP + suffixcomplement-head argument (PP/NP) + verbcompound-noun NP + NPmodifier-head modifier + headhead-modifier phrase + punctuationpromotion promotes chunks to phrasestion schema below.)
1 For some types of phrases,additional features are specified as HEAD features.Now, we provide a detailed explanation of the de-sign of the schemata and how the features in Figure2 work.
The following descriptions are also summa-rized in Table 1.specifier-head schema Words are first concate-nated by this schema to construct basic word chunks.Postpositional phrases (PPs), which consist of post-positions and preceding phrases, are the most typi-cal example of specifier-head structures.
For post-positions, we specify a head feature PFORM, withthe postposition?s surface string as its value, in addi-tion to the features in Figure 2, because differencesof postpositions play a crucial role in disambiguat-ing semantic-structures of Japanese.
For example,the postposition ?wo?
has a PFORM feature whosevalue is ?wo,?
and it accepts an NP as its specifier.As a result, a PP such as ?kare wo?
inherits the valueof PFORM feature ?wo?
from ?wo.
?The schema is also used when VPs are con-structed from verbs and their endings (or, sometimesauxiliaries.
See also Section 3.2).complement-head schema This schema is usedfor combining VPs with their subcategorized argu-ments (see Section 3.2 for details).compound-noun schema Because nouns can befreely concatenated to form compound nouns, a spe-cial schema is used for compound nouns.modifier-head schema This schema is for modi-fiers and their heads.
Binary structures that cannotbe captured by the above three schemata are also1The CONTENT feature, which should contain informationabout the semantic contents of syntactic entities, is ignored inthe current implementation of the grammar.considered to be modifier-head structures.2head-modifier schema This schema is used whenthe modifier-head schema is not appropriate.
In thecurrent implementation, it is used for a phrase andits following punctuation.promotion schema This unary schema changesthe value of the BAR feature from chunk to phrase.The distinction between these two types of con-stituents is for prohibiting some kind of spuriousambiguities.
For example, ?kinou/yesterday ko-roshi/kill ta/DECL?
can be analyzed in two differ-ent ways, i.e.
?
(kinou (koroshi ta))?
and ?
((kinoukoroshi) ta).?
The latter analysis is prevented byrestricting ?kinou?
?s modifiee to be a phrase, and?ta?
?s specifier to be a chunk, and by assuming ?ko-roshi?
to be a chunk.3.2 Scrambling and Case AlternationScrambling causes problems in designing a JapaneseHPSG grammar, because original HPSG, designedfor English, specifies the subcategorization frame ofa verb as an ordered list, and the semantic roles ofarguments are determined by their order in the com-plement list.Our implementation treats the complement fea-ture as a list of semantic roles.
Semantic roles forwhich verbs subcategorize are agent, object, andgoal.3 Correspondingly, we assume three subtypesof the complement-head schema: the agent-head,object-head, and goal-head schemata.
When verbstake their arguments, arguments receive semanticroles which are permitted by the subcategorizationof verbal signs.
We do not restrict the order ofapplication of the three types of complement-headschemata, so that a single verbal lexical entry canaccept arguments that are scrambled in arbitrary or-der.
In Figure 3, ?kare ga?
is a ga-marked PP, so it isanalyzed as an agent of ?koro(su).?
4Case alternation is caused by special auxiliaries?(sa)se?
and ?(ra)re.?
For instance, in ?boku/I2Current implementation of the grammar treats complexstructures such as relative clause constructions and coordina-tions just the same as simple modification.3These are the three roles most commonly found in EDR.4We assume that a single semantic role cannot be occupiedby more than one syntactic entities.
This assumption is some-times violated in EDR?s annotation, causing failures in grammarextraction.141comp_headHEAD verbAGENT 1 PP"ga"OBJECT PP"wo""korosu"kill1 HEAD PP"ga""kare ga"he-NOMFigure 3: Verb and its argument.HEAD verbSPRverbHEAD PASSIVE plusCOMPS 1COMPS 1Figure 4: Lexical sign of ?
(ra)re?.ga/NOM kare/he ni/DAT korosa/kill re/PASSIVEta/DECL?
(I was killed by him), ?korosa?
takes a?ga?-marked PP as an object and a ?ni?-marked PPas an agent, though without ?(sa)re,?
it takes a ?ga?-marked PP as an agent and a ?wo?-marked PP as anobject.We consider auxiliaries as a special type ofverbs which do not have their own subcategoriza-tion frames.
They inherit the subcategorizationframes of verbs.5 To capture the case alternationphenomenon, each verb has distinct lexical entriesfor its passive and causative uses.
This distinc-tion is made by binary valued HEAD features, PAS-SIVE and CAUSATIVE.
The passive (causative) aux-iliary restricts the value of its specifier?s PASSIVE(CAUSATIVE) feature to be plus, so that it can onlybe combined with properly case-alternated verballexical entries.Figure 4 presents the lexical sign of the passiveauxiliary ?(ra)re.?
Our analysis of an example sen-tence is presented in Figure 5.
Note that the passiveauxiliary ?re(ta)?
requires the value of the PASSIVEfeature of its specifier be plus, and hence ?koro(sa)?cannot take the same lexical entry as in Figure 3.4 Grammar Extraction from EDRThe EDR Japanese corpus consists of 207802 sen-tences, mainly from newspapers and magazines.The annotation of the corpus includes word segmen-5The control phenomena caused by auxiliaries are currentlyunsupported in our grammar.comp_headHEAD verbAGENT PP"ni"OBJECT 3 PP"ga"HEAD verbSPRverbHEAD PASSIVE plusAGENT 1OBJECT 2AGENT 1OBJECT 2"reta"PASSIVEHEADverbPASSIVE plusAGENT 1 PP"ni"OBJECT 2 PP"ga""korosa"kill3 HEAD PP"ga""kare ga"he-NOMFigure 5: Example of passive construction.tation, part-of-speech (POS) tags, phrase structureannotation, and semantic information.The heuristic conversion of the EDR corpus intoan HPSG treebank consists of the following steps.
Asentence ?
((kare/NP-he wo/PP-ACC) (koro/VP-killshi/VP-ENDING ta/VP-DECL))?
([I] killed him yes-terday) is used to provide examples in some steps.Phrase type annotation Phrase type labels suchas NP and VP are assigned to non-terminal nodes.Because Japanese is head final, the label of the right-most daughter of a phrase is usually percolated to itsparent.
After this step, the example sentence will be?
((PP kare/NP wo/PP) (VP koro/VP shi/VP ta/VP)).
?Assign head features The types of head featuresof terminal nodes are determined, chiefly from theirphrase types.
Features specific to some categories,such as PFORM, are also assigned in this step.Binarization Phrases for which EDR employs flatannotation are converted into binary structures.
Thebinarized phrase structure of the example sentencewill be ?
((kare wo) ((koro shi) ta)).
?Assign schema names Schema names are as-signed according to the patterns of phrase structures.For instance, a phrase structure which consists ofPP and VP is identified as a complement-head struc-ture, if the VP?s argument and the PP are coindexed.In the example sentence, ?kare wo?
is annotated as?koro?s object in EDR, so the object-head schema isapplied to the root node of the derivation.Inverse schema application The consistency ofthe derivation of the obtained HPSG treebank is ver-142ified by applying the schemata to each node of thederivation trees in the treebank.Lexicon Extraction Lexical entries are extractedfrom the terminal nodes of the obtained treebank.5 Disambiguation ModelWe also train disambiguation models for the gram-mar using the obtained treebank.
We employ log-linear models (Berger et al, 1996) for the disam-biguation.
The probability of a parse   of a sentenceis calculated as follows:  ff   flfiwhere   are feature functions,are strengths of thefeature functions, and  fispans all possible parses of.
We employ Gaussian MAP estimation (Chen andRosenfeld, 1999) as a criterion for optimizing   .An algorithm proposed by Miyao et.
al.
(2002) pro-vides an efficient solution to this optimization prob-lem.6 ExperimentsBecause the aim of our research is to construct aJapanese parser that can extract semantic informa-tion from real-world texts, we evaluated our parserin terms of its coverage and semantic-role identifica-tion accuracy.
We also compare the accuracy of ourparser with that of an existing statistical dependencyanalyzer, in order to investigate the necessity of fur-ther improvements to our disambiguation model.The following experiments were conducted usingthe EDR Japanese corpus.
An HPSG grammar wasextracted from 519516 sentences of the corpus, andthe same set of sentences were used as a trainingset for the disambiguation model.
47767 sentences(91.9%) of the training set were successfully con-verted into an HPSG treebank, from which we ex-tracted lexical entries.When we construct a lexicon from the extractedlexical entries, we reserved lexical entry templatesfor infrequent words as default templates for un-known words of each POS, in order to achieve suffi-cient coverage.
The threshold for ?infrequent?
words6We could not use the entire corpus for the experiments, be-cause of the limitation of computational resources.were determined to be 30 from the results of prelim-inary experiments.We used 2079 EDR sentences as a test set.
(An-other set of 2078 sentences were used as a devel-opment set.)
The test set is also converted into anHPSG treebank, and the conversion was successfulfor 1913 sentences.
(We will call the obtained HPSGtreebank the ?test treebank.?
)As features of the log-linear model, we extractedthe POS of the head, template name of the head,surface string and its ending of the head, punctua-tion contained in the phrase, and distance betweenheads of daughters, from each sign in derivationtrees.
These features are used in combinations.The coverage of the parser7 on the test set was95.3% (1982/2079).
Though it is still below the cov-erage achieved by SLUNG (Mitsuishi et al, 1998),our grammar has richer information that enables se-mantic analysis, which is lacking in SLUNG.We evaluated the parser in terms of its accuracyin identifying semantic roles of arguments of verbs.For each phrase which is in complement-head rela-tion with some VP, a semantic role is assigned ac-cording to the type8 of the complement-head struc-ture.
The performance of our parser on the test tree-bank was 63.8%/57.8% in precision/recall of seman-tic roles.As most studies on syntactic parsing of Japanesehave focused on bunsetsu-based dependency analy-sis, we also attempted an evaluation in this frame-work.9 In order to evaluate our parser by bunsetsudependency, we converted the phrase structures ofEDR and the output of our parser into dependencystructures of the right-most content word of eachbunsetsu.
Bunsetsu boundaries of the EDR sen-tences were determined by using simple heuristicrules.
The dependency accuracies and the senten-tial accuracies of our parser and Kanayama et.
al.
?sanalyzer are shown in Table 2.
(failure sentencesare not counted for calculating accuracies.)
Ourresults were still significantly lower than those of7Coverage of the parser can be somewhat lower than that ofthe grammar, because we employed a beam thresholding tech-nique proposed by Tsuruoka et al (Tsuruoka et al, 2004).8As described in Section 3.2, there are three types ofcomplement-head structures.9Bunsetsu is a widely accepted syntactic unit of Japanese,which usually consists of a content word followed by a functionword.143accuracy (dependency) accuracy (sentence) # failure(Kanayama et al, 2000) 88.6% (23078/26062) 46.9% (1560/3326) 1.4% (46/3372)This paper 85.0% (13201/15524) 37.4% (705/1887) 1.4% (26/1913)Table 2: Accuracy of dependency analysis.Kanayama et.
al., which are the best reported de-pendency accuracies on EDR.This experiment revealed that the accuracy of ourparser requires further improvement, although ourgrammar achieved high coverage.
Our expectation isthat incorporating grammar rules for complex struc-tures which is ignored in the current implementation(e.g.
control, relative clause, and coordination con-structions) will improve the accuracy of the parser.In addition, we should investigate whether the se-mantic analysis our parser provides can contributethe performance of more application-oriented taskssuch as information extraction.7 ConclusionWe developed a Japanese HPSG grammar by meansof the corpus-oriented method, and the grammarachieved the high coverage, which we consider to benearly sufficient for real-world applications.
How-ever, the accuracy of the parser in terms of depen-dency analysis was significantly lower than that ofthe existing parser.
We expect that the accuracycan be improved through further elaboration of thegrammar design and disambiguation method.ReferencesAdam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
Computational Lin-guistics, 22(1).Francis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, AkiraOhtani, Takaaki Tanaka, and Shigeaki Amano.
2004.The Hinoki Treebank: A Treebank for Text Under-standing.
In Proc.
of IJCNLP-04.J.
Bresnan and R. M. Kaplan.
1982.
Introduction:Grammars as mental representations of language.
InThe Mental Representation of Grammatical Relations.MIT Press.S.
Chen and R. Rosenfeld.
1999.
A Gaussian prior forsmoothing maximum entropy models.
In TechnicalReport CMUCS.Julia Hockenmaier and Mark Steedman.
2002.
Acquir-ing Compact Lexicalized Grammars from a CleanerTreebank.
In Proc.
of Third LREC.Hiroshi Kanayama, Kentaro Torisawa, Mitsuishi Yutaka,and Jun?ichi Tsujii.
2000.
A Hybrid Japanese Parserwith Hand-crafted Grammar and Statistics.
In Proc.
ofthe 18th COLING, volume 1.Yutaka Mitsuishi, Kentaro Torisawa, and Jun?ichi Tsujii.1998.
HPSG-Style Underspecified Japanese Grammarwith Wide Coverage.
In Proc.
of the 17th COLING?ACL.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum En-tropy Estimation for Feature Forests.
In Proc.
of HLT2002.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2004.
Corpus-oriented Grammar Development forAcquiring a Head-driven Phrase Structure Grammarfrom the Penn Treebank.
In Proc.
of IJCNLP-04.National Institute of Information and CommunicationsTechnology.
2002.
EDR Electronic Dictionary Ver-sion 2.0 Technical Guide.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
The University of Chicago Press.Y.
Schabes, A. Abeille, and A. K. Joshi.
1988.
Pars-ing Strategies with ?Lexicalized?
Grammars: Applica-tion to Tree Adjoining Grammars.
In Proc.
of the 12thCOLING.Melanie Siegel and Emily M. Bender.
2002.
Ef-ficient Deep Processing of Japanese.
In Proc.
ofthe 3rd Workshop on Asian Language Resources andInternational Standardization.
COLING 2002 Post-Conference Workshop, August 31.Mark Steedman.
2001.
The Syntactic Process.
MITPress.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsu-jii.
2004.
Towards efficient probabilistic HPSG pars-ing: integrating semantic and syntactic preference toguide the parsing.
In Proc.
of IJCNLP-04 Workshop:Beyond shallow analyses - Formalisms and statisticalmodeling for deep analyses.144
