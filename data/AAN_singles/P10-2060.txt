Proceedings of the ACL 2010 Conference Short Papers, pages 325?330,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsOptimizing Informativeness and Readabilityfor Sentiment SummarizationHitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro KikuiNTT Cyber Space Laboratories, NTT Corporation1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan{nishikawa.hitoshi, hasegawa.takaakimatsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jpAbstractWe propose a novel algorithm for senti-ment summarization that takes account ofinformativeness and readability, simulta-neously.
Our algorithm generates a sum-mary by selecting and ordering sentencestaken from multiple review texts accordingto two scores that represent the informa-tiveness and readability of the sentence or-der.
The informativeness score is definedby the number of sentiment expressionsand the readability score is learned fromthe target corpus.
We evaluate our methodby summarizing reviews on restaurants.Our method outperforms an existing al-gorithm as indicated by its ROUGE scoreand human readability experiments.1 IntroductionThe Web holds a massive number of reviews de-scribing the sentiments of customers about prod-ucts and services.
These reviews can help the userreach purchasing decisions and guide companies?business activities such as product improvements.It is, however, almost impossible to read all re-views given their sheer number.These reviews are best utilized by the devel-opment of automatic text summarization, partic-ularly sentiment summarization.
It enables us toefficiently grasp the key bits of information.
Senti-ment summarizers are divided into two categoriesin terms of output style.
One outputs lists ofsentences (Hu and Liu, 2004; Blair-Goldensohnet al, 2008; Titov and McDonald, 2008), theother outputs texts consisting of ordered sentences(Carenini et al, 2006; Carenini and Cheung, 2008;Lerman et al, 2009; Lerman and McDonald,2009).
Our work lies in the latter category, anda typical summary is shown in Figure 1.
Althoughvisual representations such as bar or rader chartsThis restaurant offers customers delicious foods and arelaxing atmosphere.
The staff are very friendly but theprice is a little high.Figure 1: A typical summary.are helpful, such representations necessitate somesimplifications of information to presentation.
Incontrast, text can present complex information thatcan?t readily be visualized, so in this paper we fo-cus on producing textual summaries.One crucial weakness of existing text-orientedsummarizers is the poor readability of their results.Good readability is essential because readabilitystrongly affects text comprehension (Barzilay etal., 2002).To achieve readable summaries, the extractedsentences must be appropriately ordered (Barzilayet al, 2002; Lapata, 2003; Barzilay and Lee, 2004;Barzilay and Lapata, 2005).
Barzilay et al (2002)proposed an algorithm for ordering sentences ac-cording to the dates of the publications from whichthe sentences were extracted.
Lapata (2003) pro-posed an algorithm that computes the probabilityof two sentences being adjacent for ordering sen-tences.
Both methods delink sentence extractionfrom sentence ordering, so a sentence can be ex-tracted that cannot be ordered naturally with theother extracted sentences.To solve this problem, we propose an algorithmthat chooses sentences and orders them simulta-neously in such a way that the ordered sentencesmaximize the scores of informativeness and read-ability.
Our algorithm efficiently searches for thebest sequence of sentences by using dynamic pro-gramming and beam search.
We verify that ourmethod generates summaries that are significantlybetter than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability mea-sures.
As far as we know, this is the first work to325simultaneously achieve both informativeness andreadability in the area of multi-document summa-rization.This paper is organized as follows: Section 2describes our summarization method.
Section 3reports our evaluation experiments.
We concludethis paper in Section 4.2 Optimizing Sentence SequenceFormally, we define a summary S?
=?s0, s1, .
.
.
, sn, sn+1?
as a sequence consist-ing of n sentences where s0 and sn+1 are symbolsindicating the beginning and ending of the se-quence, respectively.
Summary S?
is also definedas follows:S?
= argmaxS?T[Info(S) + ?Read(S)] (1)s.t.
length(S) ?
Kwhere Info(S) indicates the informativenessscore of S, Read(S) indicates the readabilityscore of S, T indicates possible sequences com-posed of sentences in the target documents, ?is a weight parameter balancing informativenessagainst readability, length(S) is the length of S,and K is the maximum size of the summary.We introduce the informativeness score and thereadability score, then describe how to optimize asequence.2.1 Informativeness ScoreSince we attempt to summarize reviews, we as-sume that a good summary must involve as manysentiments as possible.
Therefore, we define theinformativeness score as follows:Info(S) =?e?E(S)f(e) (2)where e indicates sentiment e = ?a, p?
as the tu-ple of aspect a and polarity p = {?1, 0, 1}, E(S)is the set of sentiments contained S, and f(e) is thescore of sentiment e. Aspect a represents a stand-point for evaluating products and services.
Withregard to restaurants, aspects include food, atmo-sphere and staff.
Polarity represents whether thesentiment is positive or negative.
In this paper, wedefine p = ?1 as negative, p = 0 as neutral andp = 1 as positive sentiment.Notice that Equation 2 defines the informative-ness score of a summary as the sum of the scoreof the sentiments contained in S. To avoid du-plicative sentences, each sentiment is counted onlyonce for scoring.
In addition, the aspects are clus-tered and similar aspects (e.g.
air, ambience) aretreated as the same aspect (e.g.
atmosphere).
Inthis paper we define f(e) as the frequency of e inthe target documents.Sentiments are extracted using a sentiment lex-icon and pattern matched from dependency treesof sentences.
The sentiment lexicon1 consists ofpairs of sentiment expressions and their polarities,for example, delicious, friendly and good are pos-itive sentiment expressions, bad and expensive arenegative sentiment expressions.To extract sentiments from given sentences,first, we identify sentiment expressions amongwords consisting of parsed sentences.
For ex-ample, in the case of the sentence ?This restau-rant offers customers delicious foods and a relax-ing atmosphere.?
in Figure 1, delicious and re-laxing are identified as sentiment expressions.
Ifthe sentiment expressions are identified, the ex-pressions and its aspects are extracted as aspect-sentiment expression pairs from dependency treeusing some rules.
In the case of the example sen-tence, foods and delicious, atmosphere and relax-ing are extracted as aspect-sentiment expressionpairs.
Finally extracted sentiment expressions areconverted to polarities, we acquire the set of sen-timents from sentences, for example, ?
foods, 1?and ?
atmosphere, 1?.Note that since our method relies on only senti-ment lexicon, extractable aspects are unlimited.2.2 Readability ScoreReadability consists of various elements such asconciseness, coherence, and grammar.
Since itis difficult to model all of them, we approximatereadability as the natural order of sentences.To order sentences, Barzilay et al (2002)used the publication dates of documents to catchtemporally-ordered events, but this approach is notreally suitable for our goal because reviews focuson entities rather than events.
Lapata (2003) em-ployed the probability of two sentences being ad-jacent as determined from a corpus.
If the cor-pus consists of reviews, it is expected that this ap-proach would be effective for sentiment summa-rization.
Therefore, we adopt and improve Lap-ata?s approach to order sentences.
We define the1Since we aim to summarize Japanese reviews, we utilizeJapanese sentiment lexicon (Asano et al, 2008).
However,our method is, except for sentiment extraction, language in-dependent.326readability score as follows:Read(S) =n?i=0w>?
(si, si+1) (3)where, given two adjacent sentences si andsi+1, w>?
(si, si+1), which measures the connec-tivity of the two sentences, is the inner product ofw and ?
(si, si+1), w is a parameter vector and?
(si, si+1) is a feature vector of the two sentences.That is, the readability score of sentence sequenceS is the sum of the connectivity of all adjacent sen-tences in the sequence.As the features, Lapata (2003) proposed theCartesian product of content words in adjacentsentences.
To this, we add named entity tags (e.g.LOC, ORG) and connectives.
We observe that thefirst sentence of a review of a restaurant frequentlycontains named entities indicating location.
Weaim to reproduce this characteristic in the order-ing.We also define feature vector ?
(S) of the entiresequence S = ?s0, s1, .
.
.
, sn, sn+1?
as follows:?
(S) =n?i=0?
(si, si+1) (4)Therefore, the score of sequence S is w>?
(S).Given a training set, if a trained parameter w as-signs a score w>?
(S+) to an correct order S+that is higher than a score w>?(S?)
to an incor-rect order S?, it is expected that the trained pa-rameter will give higher score to naturally orderedsentences than to unnaturally ordered sentences.We use Averaged Perceptron (Collins, 2002) tofind w. Averaged Perceptron requires an argmaxoperation for parameter estimation.
Since we at-tempt to order a set of sentences, the operation isregarded as solving the Traveling Salesman Prob-lem; that is, we locate the path that offers maxi-mum score through all n sentences as s0 and sn+1are starting and ending points, respectively.
Thusthe operation is NP-hard and it is difficult to findthe global optimal solution.
To alleviate this, wefind an approximate solution by adopting the dy-namic programming technique of the Held andKarp Algorithm (Held and Karp, 1962) and beamsearch.We show the search procedure in Figure 2.
Sindicates intended sentences and M is a distancematrix of the readability scores of adjacent sen-tence pairs.
Hi(C, j) indicates the score of thehypothesis that has covered the set of i sentencesC and has the sentence j at the end of the path,Sentences: S = {s1, .
.
.
, sn}Distance matrix: M = [ai,j ]i=0...n+1,j=0...n+11: H0({s0}, s0) = 02: for i : 0 .
.
.
n ?
13: for j : 1 .
.
.
n4: foreach Hi(C\{j}, k) ?
b5: Hi+1(C, j) = maxHi(C\{j},k)?bHi(C\{j}, k)6: +Mk,j7: H?
= maxHn(C,k) Hn(C, k) +Mk,n+1Figure 2: Held and Karp Algorithm.i.e.
the last sentence of the summary being gener-ated.
For example, H2({s0, s2, s5}, s2) indicatesa hypothesis that covers s0, s2, s5 and the last sen-tence is s2.
Initially, H0({s0}, s0) is assigned thescore of 0, and new sentences are then added oneby one.
In the search procedure, our dynamic pro-gramming based algorithm retains just the hypoth-esis with maximum score among the hypothesesthat have the same sentences and the same last sen-tence.
Since this procedure is still computationallyhard, only the top b hypotheses are expanded.Note that our method learns w from texts auto-matically annotated by a POS tagger and a namedentity tagger.
Thus manual annotation isn?t re-quired.2.3 OptimizationThe argmax operation in Equation 1 also involvessearch, which is NP-hard as described in Section2.2.
Therefore, we adopt the Held and Karp Algo-rithm and beam search to find approximate solu-tions.
The search algorithm is basically the sameas parameter estimation, except for its calculationof the informativeness score and size limitation.Therefore, when a new sentence is added to a hy-pothesis, both the informativeness and the read-ability scores are calculated.
The size of the hy-pothesis is also calculated and if the size exceedsthe limit, the sentence can?t be added.
A hypoth-esis that can?t accept any more sentences is re-moved from the search procedure and preservedin memory.
After all hypotheses are removed,the best hypothesis is chosen from among the pre-served hypotheses as the solution.3 ExperimentsThis section evaluates our method in terms ofROUGE score and readability.
We collected 2,940reviews of 100 restaurants from a website.
The327R-2 R-SU4 R-SU9Baseline 0.089 0.068 0.062Method1 0.157 0.096 0.089Method2 0.172 0.107 0.098Method3 0.180 0.110 0.101Human 0.258 0.143 0.131Table 1: Automatic ROUGE evaluation.average size of each document set (corresponds toone restaurant) was 5,343 bytes.
We attemptedto generate 300 byte summaries, so the summa-rization rate was about 6%.
We used CRFs-based Japanese dependency parser (Imamura etal., 2007) and named entity recognizer (Suzuki etal., 2006) for sentiment extraction and construct-ing feature vectors for readability score, respec-tively.3.1 ROUGEWe used ROUGE (Lin, 2004) for evaluating thecontent of summaries.
We chose ROUGE-2,ROUGE-SU4 and ROUGE-SU9.
We preparedfour reference summaries for each document set.To evaluate the effects of the informativenessscore, the readability score and the optimization,we compared the following five methods.Baseline: employs MMR (Carbonell and Gold-stein, 1998).
We designed the score of a sentenceas term frequencies of the content words in a doc-ument set.Method1: uses optimization without the infor-mativeness score or readability score.
It also usedterm frequencies to score sentences.Method2: uses the informativeness score andoptimization without the readability score.Method3: the proposed method.
FollowingEquation 1, the summarizer searches for a se-quence with high informativeness and readabilityscore.
The parameter vector w was trained on thesame 2,940 reviews in 5-fold cross validation fash-ion.
?
was set to 6,000 using a development set.Human is the reference summaries.
To com-pare our summarizer to human summarization, wecalculated ROUGE scores between each referenceand the other references, and averaged them.The results of these experiments are shown inTable 1.
ROUGE scores increase in the order ofMethod1, Method2 and Method3 but no methodcould match the performance of Human.
Themethods significantly outperformed Baseline ac-NumbersBaseline 1.76Method1 4.32Method2 10.41Method3 10.18Human 4.75Table 2: Unique sentiment numbers.cording to the Wilcoxon signed-rank test.We discuss the contribution of readability toROUGE scores.
Comparing Method2 to Method3,ROUGE scores of the latter were higher for all cri-teria.
It is interesting that the readability criterionalso improved ROUGE scores.We also evaluated our method in terms of sen-timents.
We extracted sentiments from the sum-maries using the above sentiment extractor, andaveraged the unique sentiment numbers.
Table 2shows the results.The references (Human) have fewer sentimentsthan the summaries generated by our method.
Inother words, the references included almost asmany other sentences (e.g.
reasons for the senti-ments) as those expressing sentiments.
Careniniet al (2006) pointed out that readers wanted ?de-tailed information?
in summaries, and the reasonsare one of such piece of information.
Includingthem in summaries would greatly improve sum-marizer appeal.3.2 ReadabilityReadability was evaluated by human judges.Three different summarizers generated summariesfor each document set.
Ten judges evaluated thethirty summaries for each.
Before the evalua-tion the judges read evaluation criteria and gavepoints to summaries using a five-point scale.
Thejudges weren?t informed of which method gener-ated which summary.We compared three methods; Ordering sen-tences according to publication dates and posi-tions in which sentences appear after sentenceextraction (Method2), Ordering sentences us-ing the readability score after sentence extrac-tion (Method2+) and searching a document setto discover the sequence with the highest score(Method3).Table 3 shows the results of the experiment.Readability increased in the order of Method2,Method2+ and Method3.
According to the328Readability pointMethod2 3.45Method2+ 3.54Method3 3.74Table 3: Readability evaluation.Wilcoxon signed-rank test, there was no signifi-cance difference between Method2 and Method2+but the difference between Method2 and Method3was significant, p < 0.10.One important factor behind the higher read-ability of Method3 is that it yields longer sen-tences on average (6.52).
Method2 and Method2+yielded averages of 7.23 sentences.
The differenceis significant as indicated by p < 0.01.
That is,Method2 and Method2+ tended to select short sen-tences, which made their summaries less readable.4 ConclusionThis paper proposed a novel algorithm for senti-ment summarization that takes account of infor-mativeness and readability, simultaneously.
Tosummarize reviews, the informativeness score isbased on sentiments and the readability score islearned from a corpus of reviews.
The preferredsequence is determined by using dynamic pro-gramming and beam search.
Experiments showedthat our method generated better summaries thanthe baseline in terms of ROUGE score and read-ability.One future work is to include important infor-mation other than sentiments in the summaries.We also plan to model the order of sentences glob-ally.
Although the ordering model in this paper islocal since it looks at only adjacent sentences, amodel that can evaluate global order is importantfor better summaries.AcknowledgmentsWe would like to sincerely thank Tsutomu Hiraofor his comments and discussions.
We would alsolike to thank the reviewers for their comments.ReferencesHisako Asano, Toru Hirano, Nozomi Kobayashi andYoshihiro Matsuo.
2008.
Subjective Information In-dexing Technology Analyzing Word-of-mouth Con-tent on the Web.
NTT Technical Review, Vol.6, No.9.Regina Barzilay, Noemie Elhadad and Kathleen McK-eown.
2002.
Inferring Strategies for Sentence Or-dering in Multidocument Summarization.
Journal ofArtificial Intelligence Research (JAIR), Vol.17, pp.35?55.Regina Barzilay and Lillian Lee.
2004.
Catching theDrift: Probabilistic Content Models, with Applica-tions to Generation and Summarization.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL),pp.
113?120.Regina Barzilay and Mirella Lapata.
2005.
ModelingLocal Coherence: An Entity-based Approach.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pp.141?148.Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-ald, Tyler Neylon, George A. Reis and Jeff Rey-nar.
2008.
Building a Sentiment Summarizer for Lo-cal Service Reviews.
WWW Workshop NLP Chal-lenges in the Information Explosion Era (NLPIX).Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedings ofthe 21st annual international ACM SIGIR confer-ence on Research and development in informationretrieval (SIGIR), pp.
335?356.Giuseppe Carenini, Raymond Ng and Adam Pauls.2006.
Multi-Document Summarization of Evalua-tive Text.
In Proceedings of the 11th EuropeanChapter of the Association for Computational Lin-guistics (EACL), pp.
305?312.Giuseppe Carenini and Jackie Chi Kit Cheung.
2008.Extractive vs. NLG-based Abstractive Summariza-tion of Evaluative Text: The Effect of Corpus Con-troversiality.
In Proceedings of the 5th InternationalNatural Language Generation Conference (INLG),pp.
33?41.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods onNatural Language Processing (EMNLP), pp.
1?8.Michael Held and Richard M. Karp.
1962.
A dy-namic programming approach to sequencing prob-lems.
Journal of the Society for Industrial and Ap-plied Mathematics (SIAM), Vol.10, No.1, pp.
196?210.Minqing Hu and Bing Liu.
2004.
Mining and Summa-rizing Customer Reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD), pp.
168?177.329Kenji Imamura, Genichiro Kikui and Norihito Yasuda.2007.
Japanese Dependency Parsing Using Sequen-tial Labeling for Semi-spoken Language.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL) Com-panion Volume Proceedings of the Demo and PosterSessions, pp.
225?228.Mirella Lapata.
2003.
Probabilistic Text Structuring:Experiments with Sentence Ordering.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL), pp.
545?552.Kevin Lerman, Sasha Blair-Goldensohn and Ryan Mc-Donald.
2009.
Sentiment Summarization: Evalu-ating and Learning User Preferences.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pp.
514?522.Kevin Lerman and Ryan McDonald.
2009.
ContrastiveSummarization: An Experiment with Consumer Re-views.
In Proceedings of Human Language Tech-nologies: the 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-HLT), Companion Vol-ume: Short Papers, pp.
113?116.Chin-Yew Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out,pp.
74?81.Jun Suzuki, Erik McDermott and Hideki Isozaki.
2006.Training Conditional Random Fields with Multi-variate Evaluation Measures.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL(COLING-ACL), pp.
217?224.Ivan Titov and Ryan McDonald.
2008.
A Joint Modelof Text and Aspect Ratings for Sentiment Summa-rization.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT),pp.
308?316.330
