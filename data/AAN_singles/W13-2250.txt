Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398?404,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsLIMSI Submission for the WMT?13 Quality Estimation Task: anExperiment with n-gram PosteriorsAnil Kumar SinghLIMSIOrsay, Franceanil@limsi.frGuillaume WisniewskiUniversite?
Paris SudLIMSIOrsay, Francewisniews@limsi.frFranc?ois YvonUniversite?
Paris SudLIMSIOrsay, Franceyvon@limsi.frAbstractThis paper describes the machine learningalgorithm and the features used by LIMSIfor the Quality Estimation Shared Task.Our submission mainly aims at evaluatingthe usefulness for quality estimation of n-gram posterior probabilities that quantifythe probability for a given n-gram to bepart of the system output.1 IntroductionThe dissemination of statistical machine transla-tion (SMT) systems in the professional translationindustry is still limited by the lack of reliability ofSMT outputs, the quality of which varies to a greatextent.
In this context, a critical piece of informa-tion would be for MT systems to assess their out-put translations with automatically derived qualitymeasures.
This problem is the focus of a sharedtask, the aim of which is to predict the qualityof a translation without knowing any human ref-erence(s).To the best of our knowledge, all approachesso far have tackled quality estimation as a super-vised learning problem (He et al 2010; Soricutand Echihabi, 2010; Specia et al 2010; Specia,2011).
A wide variety of features have been pro-posed, most of which can be described as loosely?linguistic?
features that describe the source sen-tence, the target sentence and the association be-tween them (Callison-Burch et al 2012).
Sur-prisingly enough, information used by the decoderto choose the best translation in the search space,such as its internal scores, have hardly been con-sidered and never proved to be useful.
Indeed, it iswell-known that these scores are hard to interpretand to compare across hypotheses.
Furthermore,mapping scores of a linear classifier (such as thescores estimated by MERT) into consistent prob-abilities is a difficult task (Platt, 2000; Lin et al2007).This work aims at assessing whether informa-tion extracted from the decoder search space canhelp to predict the quality of a translation.
Ratherthan using directly the decoder score, we proposeto consider a finer level of information, the n-gramposterior probabilities that quantifies the probabil-ity for a given n-gram to be part of the systemoutput.
These probabilities can be directly inter-preted as the confidence the system has for a givenn-gram to be part of the translation.
As they aredirectly derived from the number of hypotheses inthe search space that contains this n-gram, theseprobabilities might be more reliable than the onesestimated from the decoder scores.We first quickly review, in Section 2, the n-gramposteriors introduced by (Gispert et al 2013) andexplain how they can be used in the QE task; wethen describe, in Section 3 the different systemsthat have developed for our participation in theWMT?13 shared task on Quality Estimation andassess their performance in Section 4.2 n-gram Posterior Probabilities in SMTOur contribution to the WMT?13 shared task onquality estimation relies on n-gram posteriors.
Forthe sake of completeness, we will quickly formal-ize this notion and summarize the method pro-posed by (Gispert et al 2013) to efficiently com-pute them.
We will then describe preliminary ex-periments to assess their usefulness for predictingthe quality of a translation hypothesis.2.1 Computing n-gram PosteriorsFor a given source sentence F , the n-gram pos-terior probabilities quantifies the probability fora given n-gram to be part of the system output.Their computation relies on all the hypothesesconsidered by a SMT system during decoding: in-tuitively, the more hypotheses a n-gram appearsin, the more confident the system is that this n-gram is part of the ?correct?
translation, and the398higher its posterior probability is.
Formally, theposterior of a given n-gram u is defined as:P (u|E) =?
(A,E)?E?u(E) ?
P (E,A|F )where the sum runs over the translation hypothe-ses contained in the search space E (generally rep-resented as a lattice); ?u(E) has the value 1 if uoccurs in the translation hypothesis E and 0 oth-erwise and P (E,A|F ) is the probability that thesource sentence F is translated by the hypothesisE using a derivation A.
Following (Gispert et al2013), this probability is estimated by applying asoft-max function to the score of the decoder:P (A,E|F ) = exp (?
?H(E,A, F ))?(A?,E?
)?E exp (H(E?, A?, F ))where the decoder score H(E,A, F ) is typicallya linear combination of a handful of features, theweights of which are estimated by MERT (Och,2003).n-gram posteriors therefore aggregate twopieces of information: first, the number of paths inthe lattice (i.e.
the number of translation hypothe-ses of the search path) the n-gram appears in; sec-ond, the decoder scores of these paths that can beroughly interpreted as a quality of the path.Computing P (u|E) requires to enumerate all n-gram contained in E and to count the number ofpaths in which this n-gram appears at least once.An efficient method to perform this computationin a single traversal of the lattice is describedin (Gispert et al 2013).
This algorithm has beenreimplemented1 to generate the posteriors used inthis work.2.2 Analysis of n-gram PosteriorsFigure 1 represents the distribution of n-gram pos-teriors on the training set of the task 1-1.
This dis-tribution is similar to the ones observed for task 1-3 and for higher n-gram orders.
It appears that, thedistribution is quite irregular and has two modes.The minor modes corresponds to n-grams that ap-pear in almost every translation hypotheses andhave posterior probability close to 1.
Further anal-yses show that these n-grams are mainly made ofstop words and of out-of-vocabulary words.
Themajor mode corresponds to very small n-gramposteriors (less than 10?1) that the system has only1Our implementation can be downloaded from http://perso.limsi.fr/Individu/wisniews/.a very small confidence in producing.
The num-ber of n-grams that have such a small posteriorsuggests that most n-grams occur only in a smallnumber of paths.0 5 10 15 20 250.000.050.100.150.20?
logP (u|E)DensityFigure 1: Distribution of the unigram posteriorsobserved on the training set of the task 1-1Using n-gram posteriors to predict the qualityof translation raises a representation issue: thenumber of n-grams contained in a sentence varieswith the sentence length (and hence with the num-ber of posteriors) but this information needs to berepresented in a fixed-length vector describing thesentence.
Similarly to what is usually done in thequality estimation task, we chose to represent pos-teriors probability by their histogram: for a givenn-gram order, each posterior is mapped to a bin;each bin is then represented by a feature equal tothe number of n-gram posteriors it contains.
Toaccount for the irregular distribution of posteriors,bin breaks are chosen on the training set so as toensure that each bin contains the same number ofexamples.
In our experiments, we considered apartition of the training data into 20 bins.3 Systems DescriptionLIMSI has participated to the tasks 1-1 (predic-tion of the hTER) and 1-3 (prediction of the post-edition time).
Similar features and learning algo-rithms have been considered for the two tasks.
Wewill first quickly describe them before discussingthe specific development made for task 1-3.3993.1 FeaturesIn addition to the features described in the previ-ous section, 176 ?standard?
features for quality es-timation have been considered.
The full list of fea-tures we have considered is given in (Wisniewskiet al 2013) and the features set can be down-loaded from our website.2 These features can beclassified into four broad categories:?
Association Features: Measures of the qual-ity of the ?association?
between the sourceand the target sentences like, for instance,features derived from the IBM model 1scores;?
Fluency Features: Measures of the ?fluency?or the ?grammaticality?
of the target sentencesuch as features based on language modelscores;?
Surface Features: Surface features extractedmainly from the source sentence such asthe number of words, the number of out-of-vocabulary words or words that are notaligned;?
Syntactic Features: some simple syntacticfeatures like the number of nouns, modifiers,verbs, function words, WH-words, numberwords, etc., in a sentence;These features sets differ, in several ways, fromthe baseline feature set provided by the shared taskorganizers.
First, in addition to features derivedfrom a language model, it also includes severalfeatures based on large span continuous space lan-guage models (Le et al 2011).
Such languagemodels have already proved their efficiency bothfor the translation task (Le et al 2012) and thequality estimation task (Wisniewski et al 2013).Second, each feature was expanded into two ?nor-malized forms?
in which their value was dividedeither by the source length or the target lengthand, when relevant, into a ?ratio form?
in whichthe feature value computed on the target sentenceis divided by its value computed in the source sen-tence.
At the end, when all possible feature expan-sions are considered, each example is described by395 features.2http://perso.limsi.fr/Individu/wisniews/3.2 Learning MethodsThe main focus of this work is to study the rel-evance of features for quality estimation; there-fore, only very standard learning methods wereused in our work.
For this year submissionboth random forests (Breiman, 2001) and elas-tic net regression (Zou and Hastie, 2005) havebeen used.
The capacity of random forests to takeinto account complex interactions between fea-tures has proved to be a key element in the re-sults achieved in our experiments with last yearcampaign datasets (Zhuang et al 2012).
As weare considering a larger features set this year andthe number of examples is comparatively quitesmall, we also considered elastic regression, a lin-ear model trained with L1 and L2 priors as regu-larizers, hoping that training a sparse model wouldreduce the risk of overfitting.In this study, we have used the implementationprovided by scikit-learn (Pedregosa et al2011).
As detailed in Section 4.1, cross-validationhas been used to choose the hyper-parameters ofall regressors, namely the number of estimators,the maximal depth of a tree and the minimumnumber of examples in a leaf for the randomforests and the importance of the L1 and the L2regularizers for the elastic net regressor.3.3 System for Task 1-3Like task 1-1, task 1-3 is a regression task thataims at predicting the time needed to post-edit atranslation hypothesis.
From a machine learningpoint of view, this task differs from task 1-1 inthree aspects.
First, the distributed training setis much smaller: it is made of only 803 exam-ples, which increases the risk of overfitting.
Sec-ond, contrary to hTER scores, post-edition time isnot normalized and the label of this task can takeany positive value.
Finally and most importantly,as shown in Figure 2, the label distributions es-timated on the training set has a long tail whichindicates the presence of several outliers: in theworse case, it took more than 18 minutes to cor-rect a single sentence made of 35 words!
Sucha long post-edition time most certainly indicatesthat the corrector has been distracted when post-editing the sentence rather than a true difficulty inthe post-edition.These outliers have a large impact on trainingand on testing, as their contributions to both MAE4000 200 400 600 800 1000 12000.0000.0020.0040.0060.008Post-edition time (s)DensityFigure 2: Kernel density estimate of the post-edition time distribution used as label in task 1-3.and MSE,3 directly depends on label values andcan therefore be very large in the case of outliers.For instance, a simple ridge regression with thebaseline features provided by the shared task or-ganizer achieves a MAE of 42.641 ?
2.126 onthe test set.
When all the examples having a la-bel higher than 300 are removed from the trainingset, the MAE drops to 41.843?
4.134.
When out-liers are removed from both the training and thetest sets, the MAE further drops to 32.803?1.673.These observations indicate that special care mustbe taken when collecting the data and that, maybe,post-edition times should be clipped to provide amore reliable estimation of the predictor perfor-mance.In the following (and in our submission) onlyexamples for which the post-edition time was lessthan 300 seconds were considered.4 Results4.1 Experimental SetupWe have tested different combinations of featuresand learning methods using a standard metric forregression: Mean Absolute Error (MAE) definedby:MAE = 1nn?i=1|y?i ?
yi|3The two standard loss functions used to train and evalu-ate a regressorwhere n is the number of examples, yi and y?ithe true label and predicted label of the ith exam-ple.
MAE can be understood as the averaged errormade in predicting the quality of a translation.Performance of both task 1-1 and task 1-34 wasalso evaluated by the Spearman rank correlationcoefficient ?
that assesses how well the relation-ship between two variables can be described usinga monotonic function.
While the value of the cor-relation coefficient is harder to interpret as it notdirectly related to the value to predict, it can beused to compare the performance achieved whenpredicting different measures of the post-editingeffort.
Indeed, several sentence-level (or docu-ment level) annotation types can be used to reflecttranslation quality (Specia, 2011), such as the timeneeded to post-edit a translation hypothesis, thehTER, or qualitative judgments as it was the casefor the shared task of WMT 2012.
Comparing di-rectly these different settings is complicated, sinceeach of them requires to optimize a different loss,and even if the losses are the same, their actualvalues will depend on the actual annotation to bepredicted (refer again to the discussion in (Specia,2011, p5)).
Using a metric that relies on the pre-dicted rank of the example rather than the actualvalue predicted allows us to directly compare theperformance achieved on the two tasks.As the labels for the different tasks were not re-leased before the evaluation, all the reported re-sults are obtained on an ?internal?
test set, made of20% of the data released by the shared task or-ganizers as ?training?
data.
The remaining datawere used to train the regressor in a 10 folds cross-validation setting.
In order to get reliable estimateof our methods performances, we used bootstrapresampling (Efron and Tibshirani, 1993) to com-pute confidence intervals of the different scores:10 random splits of the data into a training andsets were generated; a regressor was then trainedand tested for each of these splits and the resultingconfidence intervals at 95% computed.4.2 ResultsTable 1and Table 2 contain the results achieved byour different conditions.
We used, as a baseline,the set of 17 features released by the shared taskorganizers.It appears that the differences in MAE between4The Spearman ?
was an official metric only for task 1-1.
For reasons explained in this paragraph, we also used it toevaluate our results for task 1-3.401the different configurations are always very smalland hardly significant.
However, the variation ofthe Spearman ?
are much larger and the differenceobserved are practically significant when the inter-pretation scale of (Landis and Koch, 1977) is used.We will therefore mainly consider ?
in our discus-sion.For the two tasks 1-1 and 1-3, the features wehave designed allow us to significantly improveprediction performance in comparison to the base-line.
For instance, for task 1-1, the correlationis almost doubled when the features described inSection 3.1 are used.
As expected, random forestsare overfitting and did not manage to outperforma simple linear classifier.
That is why we onlyused the elastic net method for our official submis-sion.
Including posterior probabilities in the fea-ture set did not improve performance much (ex-cept when only the baseline features are consid-ered) and sometimes even hurt performance.
Thismight be caused by an overfitting problem, thetraining set becoming too small when new featuresare added.
We are conducting further experimentsto explain this paradoxical observation.Another interesting observation that can bemade looking at the results of Table 1 and Ta-ble 2 is that the prediction of the post-edition timeseems to be easier than the prediction of the hTER:using the same classifiers and the same features,the performance for the former task is always farbetter than the performance for the latter.5 ConclusionIn this paper, we described our submission to theWMT?13 shared task on quality estimation.
Wehave explored the use of posteriors probability,hoping that information about the search spacecould help in predicting the quality of a transla-tion.
Even if features derived from posterior prob-abilities have shown to have only a very limitedimpact, we managed to significantly improve thebaseline with a standard learning method and sim-ple features.
Further experiments are required tounderstand the reasons of this failure.Our results also highlight the need to continuegathering high-quality resources to train and in-vestigate quality estimation systems: even whenconsidering few features, our systems were proneto overfitting.
Developing more elaborated sys-tems will therefore only be possible if more train-ing resource is available.
Our experiments alsostress that both the choice of the quality measure(i.e.
the quantity to predict) and of the evaluationmetrics for quality estimation are still open prob-lems.6 AcknowledgmentsThis work was partly supported by ANRprojects Trace (ANR-09-CORD-023) and Tran-sread (ANR-12-CORD-0015).ReferencesLeo Breiman.
2001.
Random forests.
Mach.
Learn.,45(1):5?32, October.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada, June.
Association forComputational Linguistics.B.
Efron and R. Tibshirani.
1993.
An Introductionto the Bootstrap.
Chapman and Hall/CRC Mono-graphs on Statistics and Applied Probability Series.Chapman & Hall.Adria` Gispert, Graeme Blackwood, Gonzalo Iglesias,and William Byrne.
2013.
N-gram posterior prob-ability confidence measures for statistical machinetranslation: an empirical study.
Machine Transla-tion, 27(2):85?114.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging smt and tm with translationrecommendation.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 622?630, Uppsala, Sweden, July.Association for Computational Linguistics.R.
J. Landis and G. G. Koch.
1977.
The measurementof observer agreement for categorical data.
Biomet-rics, 33(1):159?174.Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
StructuredOutput Layer Neural Network Language Model.In Proceedings of IEEE International Conferenceon Acoustic, Speech and Signal Processing, pages5524?5527, Prague, Czech Republic.Hai-Son Le, Thomas Lavergne, Alexandre Al-lauzen, Marianna Apidianaki, Li Gong, Aure?lienMax, Artem Sokolov, Guillaume Wisniewski, andFranc?ois Yvon.
2012.
Limsi @ wmt12.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 330?337, Montre?al,Canada, June.
Association for Computational Lin-guistics.402MAE ?train test train testBaseline FeaturesRandomForest 0.109?
0.013 0.130?
0.004 0.405?
0.008 0.314?
0.016Elastic 0.127?
0.001 0.129?
0.003 0.336?
0.004 0.319?
0.015?Linguistic?
FeaturesRandomForest 0.082?
0.019 0.118?
0.003 0.689?
0.003 0.625?
0.009Elastic 0.107?
0.004 0.115?
0.003 0.705?
0.009 0.660?
0.009?Linguistic?
Features + posteriorsRandomForest 0.088?
0.017 0.116?
0.003 0.694?
0.003 0.615?
0.014Elastic 0.105?
0.006 0.114?
0.002 0.699?
0.007 0.662?
0.011Table 1: Results for the task 1-1MAE ?train test train testBaseline FeaturesRandomForest 25.145?
3.745 33.279?
1.687 0.669?
0.007 0.639?
0.017Elastic 32.776?
0.795 33.702?
2.328 0.678?
0.006 0.657?
0.018Baseline Features + PosteriorsRandomForest 33.707?
0.309 35.646?
0.889 0.674?
0.004 0.637?
0.017Elastic 31.487?
0.261 32.922?
0.789 0.698?
0.004 0.681?
0.016?Linguistic?
FeaturesRandomForest 25.236?
4.400 33.017?
1.582 0.735?
0.007 0.666?
0.023Elastic 28.706?
1.273 31.630?
1.612 0.760?
0.006 0.701?
0.017?Linguistic?
Features + PosteriorsRandomForest 22.951?
3.903 33.013?
1.514 0.741?
0.003 0.695?
0.013Elastic 28.911?
1.020 31.865?
1.636 0.761?
0.008 0.710?
0.017Table 2: Results for the task 1-3403Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.2007.
A note on platt?s probabilistic outputs forsupport vector machines.
Mach.
Learn., 68(3):267?276, October.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine Learn-ing in Python .
Journal of Machine Learning Re-search, 12:2825?2830.John C. Platt, 2000.
Probabilities for SV Machines,pages 61?74.
MIT Press.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine translation evaluation versus quality esti-mation.
Machine Translation, 24(1):39?50, March.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of the 15th conference of EAMT, pages 73?80, Leuven, Belgium.Guillaume Wisniewski, Anil Kumar Singh, andFranc?ois Yvon.
2013.
Quality estimation for ma-chine translation: Some lessons learned.
MachineTranslation.
accepted for publication.Yong Zhuang, Guillaume Wisniewski, and Franc?oisYvon.
2012.
Non-linear models for confidence es-timation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 157?162,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Hui Zou and Trevor Hastie.
2005.
Regularization andvariable selection via the elastic net.
Journal of theRoyal Statistical Society, Series B, 67:301?320.404
