Meta-Learning Orthographic and Contextual Models for LanguageIndependent Named Entity RecognitionRobert Munro and Daren Ler and Jon PatrickLanguage Technology Research GroupCapital Markets Co-operative Research CentreUniversity of Sydney{rmunro,ler,jonpat}@it.usyd.edu.auAbstractThis paper presents a named entity classifica-tion system that utilises both orthographic andcontextual information.
The random subspacemethod was employed to generate and refine at-tribute models.
Supervised and unsupervisedlearning techniques used in the recombinationof models to produce the final results.1 IntroductionThere are commonly considered to be two main tasks innamed entity recognition, recognition (NER) and classi-fication (NEC).
As the features that best classify wordsaccording to the two tasks are somewhat disparate, thetwo are often separated.
Attribute sets may be furtherdivided into subsets through sub-grouping of attributes,sub-grouping of instances and/or the use of multiple clas-sifying processes.
While the use of multiple subsets canincrease overall accuracy, the recombination of modelshas been shown to propagate errors (Carreras et al, 2002;Patrick et al, 2002).
More importantly, the decision re-garding the separation of attributes into various subsets isoften a manual task.
As it is reasonable to assume that thesame attributes will have different relative levels of sig-nificance in different languages, using the same divisionof attributes across languages will be less than optimal,while a manual redistribution across different languagesis limited by the users knowledge of those languages.
Inthis paper, the division and subsequent recombination ofsubgroups is treated as a meta-learning task.2 Feature RepresentationIt has been our intention to create linguistically drivenmodel of named entity composition, and to search forthe attribute representations of these linguistic phenom-ena that best suit inference by a machine learning algo-rithm.It is important to note that a named entity is a labelthat has been consciously granted by some person or per-sons, and as these names are chosen rather than assignedrandomly or evolved gradually, there are generalisationsthat may be inferred about the words that may be used fornaming certain entity types (Allan, 2001; Kripke, 1972).While generalisations relating to abstract connotationsof a word may be difficult to infer, generalisations aboutthe structure of the words are more emergent.
As theuse of a name stretches back in time, it stretches backto a different set of etymological constraints.
It mayalso stem from another language, with a different ortho-graphic structure, possibly representing a different under-lying phonology.
Foreign words are frequently namedentities, especially in the domain of a newswire such asReuters.
In language in general it is also reasonable to as-sume that a foreign word is more likely to be an entity, aspeople are more likely to migrate between countries thanprepositions.
It is these generalisations of the structure ofwords that we have attempted to represent in the n-gramfeatures.Another emergent structural generalisation is that ofcapitalisation, as named entities are commonly expressedin title-case in European Languages.
In this work it hasbeen investigated as a preprocessing step.The other features used were contextual features, suchas observed trigger words, and the given part-of-speechand chunking tags.In total, we selected 402 attributes from which themodels were built.2.1 Character N-Gram ModellingThe fundamental attribute of character n-gram modellingis the observed probability of a collocation of charactersoccurring as each of the category types.
Individual n-grams, or aggregations of them, may be used as attributesin part of a larger data set for machine learning.Modeling at the orthographic level has been shownto be a successful method of named entity recogni-tion.
Orthographic Tries (Cucerzan and Yarowsky, 1999;Whitelaw and Patrick, 2003; Whitelaw and Patrick, 2002)and character n-gram modelling (Patrick et al, 2002) aretwo methods for capturing orthographic features.
WhileTries give a rich representation of a word, they are fixed toone boundary of a word and cannot extend beyond unseencharacter sequences.
As they are also a classifying toolin themselves, their integration with a machine learningalgorithm is problematic, as evidenced by reduction ofoverall accuracy when processing a Trie output through amachine learner in Patrick et al (2002).
As such, Trieshave not been used here.
Although n-gram modelling hasnot always been successful as a lone method of classifica-tion (Burger et al, 2002), for the reasons outlined aboveit is a more flexible modelling technique than Tries.To capture affixal information, we used N-Grams mod-elling to extract features for the suffixes and prefixes of allwords for all categories.For general orthographic information we used the av-erage probability of all bi-grams occurring in a wordfor each category, and the value of the maximum andminimum probability of all bi-grams in a word for eachcategory.
To capture contextual information, these bi-gram attributes was also extracted across word bound-aries, both pre/post and exclusive/inclusive of the currentword, for different context windows.All n-grams were extracted for the four entity types, lo-cation, person, organisation and miscellaneous, with theword level n-grams also extracted for NE recognition at-tributes using a IOE2 model.The aggregate n-gram attributes (for example, the av-erage probability of all the n-grams in a word belongingto a category), act as a memory based attribute, clusteringforms with less then random variance.
These most bene-fit agglutinative structures, such as the compound wordscommon to German, as well as morphologically disparateforms, for example, ?Australia?
and ?Australian?.
Here, ofall the n-grams, only the final one differs.
While a stem-ming algorithm would also match the two words, stem-ming algorithms are usually based on language specificaffixal rules and are therefore inappropriate for a lan-guage independent task.
Furthermore, the difference be-tween the words may be significant.
The second of thetwo words, used adjectively, would most likely belongto the miscellaneous category, while the former is mostlikely to be a location.2.2 Contextual FeaturesOther than the contextual n-gram attributes, contextualfeatures used were: a bag of words, both pre and postan entity, the relative sentence position of the word, com-monly observed forms, and observed collocational triggerwords for each category.2.3 Other FeaturesThe part-of-speech and chunking tags were used with acontext window of three, both before and after each word.For the German data, an attribute indicating whetherthe word matched its lemma form or was unknown wasalso included.An attribute indicating both the individual and sequen-tial existence of a words in the gazetteer was included forboth sets.No external sources were used.3 Normalising Case InformationAs well as indicating a named entity, capitalisation mayindicate phenomenon such as the start of a sentence, a ti-tle phrase or the start of reported speech.
As orthographicmeasures such as n-grams are case sensitive, both in thebuilding of the model and in classification, a preprocess-ing step to correctly reassign the case information wasused to correct alternations caused by these phenomenon.To the best knowledge of the authors, the only other at-tempt to use computational inference methods for thistask is Whitelaw and Patrick (2003).
Here we assumedall words in the training and raw data sets that were notsentence initial, did not occur in a title sentence, and didnot immediately follow punctuation were in the correctcase.
This amounted to approximately 10,000,000 words.From these, we extracted the observed probability of aword occurring as lowercase, all capitals, initial capital,or internal capital; the bi-gram distribution across thesefour categories; and the part-of-speech and chunking tagsof the word.
Using a decision graph (Patrick and Goyal,2001), all words from the test and training sets were theneither recapitalised or decapitalised according to the out-put.
The results were 97.8% accurate, as indicated by thenumber of elements in the training set that were correctlyre-assigned their original case.The benefit of case-restoration for the English develop-ment set was F?=1 1.56.
Case-restoration was not under-taken on the English test set or German sets.
For consis-tency, the English development results reported in table1 are for processing without case restoration.
We leave amore thorough investigation of case restoration as futurework.4 ProcessingIn order to make classifications, we employ a meta-learning strategy that is a variant of stacking (Wolpert,1992) and cascading (Gama and Brazdil, 2000) over anensemble of classifiers.
This classifier is described in twophases.In the first phase, an ensemble of classifiers is producedby combining both the random subspace method (Ho,1998) and bootstrap aggregation or bagging (Breiman,1996).In the random subspace method, subspaces of the fea-ture space are formed, with each subspace trained to pro-duce a classifier.
Given that with n features, 2n differ-ent subsets of features can be generated, not all possiblesubsets are created.
Ho (1998) suggests that the randomsubspace method is best suited for problems with highdimensionality.
Furthermore, he finds that the methodworks well where there exists a high degree of redun-dancy across attributes, and where the prior knowledgeabout the significance of various attributes is unknown.It is also a useful method for limiting the impact of at-tributes that may cause the learner to overfit the data.
Thisis especially important in the domain of newswires wherethe division between training and test sets is temporal, astopic shift is likely to occur.From a different prespective, bagging produces differ-ent subsets or bootstrap replicates by randomly drawingwith replacement, m instances from the original trainingset.
Once again, each bag is used to produce a differentclassifier.Both techniques share the same fundamental idea offorming multiple training sets from a single original train-ing set.
An unweighted or weighted voting scheme isthen typically adopted to make the ultimate classifica-tion.
However, in this paper, as the second phase of ourclassifier, an additional level of learning is performed.For each training instance, the class or category proba-bility distributions produced by the underlying ensembleis used in conjunction with the correct classification totrain a new final classifier.
The category probability dis-tributions may be seen as meta-data that is used to train ameta-learner.Specifically, given n features A1, A2, ..., An andm training instances I1, I2, ..., Im,we may then ran-domly form l different training subsets S1, S2, ..., Sl,with each Si containing a random subset of both at-tributes and training instances.
Given a learning al-gorithm L, each Si is used to train L to producel different classifiers C1, C2, ..., Cl.
When tested,each Ci will produce a category probability distribu-tion Ci(D1), Ci(D2), ..., Ci(Dg) where g is the to-tal number of categories.
Then for each traininginstance h, the unified category probability distribu-tion?lr=1 Cr(D1),?lr=1 Cr(D2), ...,?lr=1 Cr(Dg) inconjunction with the correct category for that instanceCLh is used to train L to produce the final classifier C ?.In our experimentation, we divided each data set intosubsets containing approximately 65% of the originaltraining set (with replication) and with 50 of the total 402attributes.
In total, the meta-learner utilised data gen-erated from the combined output of 150 sub-classifiers.The choices regarding the number of subsets and their re-spective attribute and instance populations were made inconsideration of both processing constraints and the min-imum requirements in terms of the required original data.While increasing the number of subsets will generally in-crease the overall accuracy, obtaining an optimal subsetsize through automated experimentation would have beena preferable method, especially as the optimal size maydiffer between languages.To eliminate subsets that were unlikely to produce ac-curate results across any language, we identified eightsubtypes of attributes, and considered only those sets withat least one attribute from each.
These were:1. prefixal n-grams2.
suffixal n-grams3.
n-grams specifically modelling IOE2 categories4.
trigger forms occurring before a word5.
trigger forms occurring after a word6.
sentence and clausal positions7.
collocating common forms8.
the observed probability of the word and surround-ing words belonging to each category typeTo classify the various subsets generated as well asto train the final meta-learner, a boosted decision graph(Patrick and Goyal, 2001) is used.5 ResultsN-Grams, in various instances, were able to capture in-formation about various structural phenomenon.
For ex-ample, the bi-gram ?ae?
occurred in an entity in approx-imately 96% of instances in the English training set and91% in the German set, showing that the compulsion tonot assimilate old forms of names ?Israel?
and ?Michael?to something like ?Israil?
and ?Michal?
is more emergentthan the constraint to maintain form.
An example bi-gramindicating a word from a foreign language with a dif-ferent phonology is ?cz?, representing the voiced palatalfricative, which is not commonly used in English or Ger-man.
The fact two characters were needed to representone underlying phoneme in itself suggests this.
WithinEnglish, the suffix ?gg?
always indicates a named entity,with the exception of the word ?egg?, which has retainedboth g?s in accordance with the English constraint of con-tent words being three or more letters long.
All otherword?s with an etymological history of a ?gg?
suffix suchas ?beg?
have assimilated to the shorter form.The meta-learning strategy improved the German testset results by F?=1 9.06 over a vote across the classifiers.For English test set, this improvement was F?=1 0.40.6 DiscussionThe methodology employed was significantly more suc-cessful at identifying location and person entities (see ta-ble 1).
The recalls for these values for English are es-pecially high considering that precision is typically thehigher value in named entity recognition.
Although thelower value for miscellaneous entities was expected, dueto the relatively smaller number of items and idiosyn-crasies of the category membership, the significantly lowvalues for organisations was surprising.
There are threepossible reasons for this: organisations are more likelythan people or places to take their names from the con-temporary lexicon, and are therefore less likely to containorthographic structures able to be exploited by n-grammodelling; in the training set, organisations were rela-tively over represented in the errors made in the normal-ising of case information, most likely due to the previousreason; and organisations may be represented metonymi-cally, creating ambiguity about the entity class.As the difference that meta-learning made to Germanwas very large, but to English very small (see Results), itis reasonable to assume that the individual English classi-fiers were much more homogeneous, indicating both thatthe attribute space for the individual classifiers for En-glish were very successful, but only certain classifiers orcombinations of them were beneficial for German.
Theflexibility of the strategy as a whole was successful whengeneralising across languagesReferencesK.
Allan.
2001.
Natural Language Semantics.
Black-well Publishers, Oxford, UK.L.
Breiman.
1996.
Bagging predictors.
In MachineLearning, 24(2), pages 123?140.J.
D. Burger, J. C. Henderson, and W. T. Morgan.
2002.Statistical Named Entity Recognizer Adaptation.
InProceedings of CoNLL-2002.
Taipei, Taiwan.X.
Carreras, L. Marques, and L. Padro.
2002.
NamedEntity Extraction using AdaBoost.
In Proceedings ofCoNLL-2002.
Taipei, Taiwan.S.
Cucerzan and D. Yarowsky.
1999.
Language indepen-dent named entity recognition combining morphologi-cal and contextual evidence.J.
Gama and P. Brazdil.
2000.
Cascade generalization.In Machine Learning, 41(3), pages 315?343.T.
K. Ho.
1998.
The Random Subspace Method for Con-structing Decision Forests.
In IEEE Transactions onPattern Analysis and Machine Intelligence, 20(8).S.
Kripke.
1972.
Naming and necessity.
In Semantics ofNatural Language, pages 253?355.English devel.
precision recall F?=1LOC 90.02% 92.76% 91.37MISC 89.05% 78.52% 83.46ORG 77.61% 82.48% 79.97PER 88.48% 93.38% 90.86Overall 86.49% 88.42% 87.44English test precision recall F?=1LOC 84.74% 88.25% 86.46MISC 80.13% 70.66% 75.09ORG 75.20% 79.77% 77.42PER 82.98% 90.48% 86.57Overall 80.87% 84.21% 82.50German devel.
precision recall F?=1LOC 74.70% 68.76% 71.60MISC 76.07% 59.80% 66.96ORG 71.14% 63.17% 66.92PER 76.87% 76.87% 76.87Overall 74.75% 67.80% 71.11German test precision recall F?=1LOC 67.73% 69.76% 68.73MISC 64.38% 57.46% 60.73ORG 60.54% 54.98% 57.63PER 78.95% 75.31% 77.09Overall 69.37% 66.21% 67.75Table 1: Results for English and German sets.J.
Patrick and I. Goyal.
2001.
Boosted Decision Graphsfor NLP Learning Tasks.
In Walter Daelemans andRe?mi Zajac, editors, Proceedings of CoNLL-2001,pages 58?60.
Toulouse, France.J.
Patrick, C. Whitelaw, and R. Munro.
2002.
SLIN-ERC: The Sydney Language-Independent Named En-tity Recogniser and Classifier.
In Proceedings ofCoNLL-2002, pages 199?202.
Taipei, Taiwan.C.
Whitelaw and J. Patrick.
2002.
Orthographic triesin language independent named entity recognition.
InProceedings of ANLP02, pages 1?8.
Centre for Lan-guage Technology, Macquarie University.C.
Whitelaw and J. Patrick.
2003.
Named EntityRecognition Using a Character-based Probabilistic Ap-proach.
In Proceedings of CoNLL-2003.
Edmonton,Canada.D.
Wolpert.
1992.
Stacked generalization.
In NeuralNetworks, 5(2), pages 241?260.
