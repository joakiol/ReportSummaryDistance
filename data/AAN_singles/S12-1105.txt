First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701?705,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsFBK: Cross-Lingual Textual Entailment Without TranslationYashar MehdadFBK-irstTrento , Italymehdad@fbk.euMatteo NegriFBK-irstTrento , Italynegri@fbk.euJose?
Guilherme C. de SouzaFBK-irst & University of TrentoTrento, Italydesouza@fbk.euAbstractThis paper overviews FBK?s participationin the Cross-Lingual Textual Entailmentfor Content Synchronization task organizedwithin SemEval-2012.
Our participation ischaracterized by using cross-lingual matchingfeatures extracted from lexical and semanticphrase tables and dependency relations.
Thefeatures are used for multi-class and binaryclassification using SVMs.
Using a combi-nation of lexical, syntactic, and semantic fea-tures to create a cross-lingual textual entail-ment system, we report on experiments overthe provided dataset.
Our best run achievedan accuracy of 50.4% on the Spanish-Englishdataset (with the average score and the me-dian system respectively achieving 40.7% and34.6%), demonstrating the effectiveness of a?pure?
cross-lingual approach that avoids in-termediate translations.1 IntroductionSo far, cross-lingual textual entailment (CLTE)(Mehdad et al, 2010) has been applied to: i)available TE datasets (?YES?/?NO?
uni-directionalrelations between monolingual pairs) transformedinto their cross-lingual counterpart by translatingthe hypotheses into other languages (Negri andMehdad, 2010), and ii) machine translation evalu-ation datasets (Mehdad et al, 2012b).
The contentsynchronization task represents a challenging appli-cation scenario to test the capabilities of CLTE sys-tems, by proposing a richer inventory of phenomena(i.e.
?Bidirectional?/?Forward?/?Backward?/?Noentailment?
multi-directional entailment relations).Multi-directional CLTE recognition can be seenas the identification of semantic equivalence and in-formation disparity between two topically relatedsentences, at the cross-lingual level.
This is a coreaspect of the multilingual content synchronizationtask, which represents a challenging application sce-nario for a variety of NLP technologies, and a sharedresearch framework for the integration of semanticsand MT technology.The CLTE methods proposed so far adopt eithera ?pivoting approach?
(translation of the two in-put texts into the same language, as in (Mehdad etal., 2010)), or an ?integrated solution?
that exploitsbilingual phrase tables to capture lexical relationsand contextual information (Mehdad et al, 2011).The promising results achieved with the integratedapproach still rely on phrasal matching techniquesthat disregard relevant semantic aspects of the prob-lem.
By filling this gap integrating linguisticallymotivated features, in our participation, we proposean approach that combines lexical, syntactic and se-mantic features within a machine learning frame-work (Mehdad et al, 2012a).Our submitted runs have been produced by train-ing and optimizing multiclass and binary SVM clas-sifiers, over the Spanish-English (Spa-Eng) devel-opment set.
In both cases, our results were posi-tive, showing significant improvements over the me-dian systems and average scores obtained by partic-ipants.
The overall results confirm the difficulty ofthe task, and the potential of our approach in com-bining linguistically motivated features in a ?pure?cross-lingual approach that avoids the recourse toexternal MT components.7012 ExperimentsIn our experiment we used the Spa-Eng portion ofthe dataset described in (Negri et al, 2012; Negriet al, 2011), consisting of 500 multi-directional en-tailment pairs which was provided to train the sys-tems and 500 pairs for the submission.
Each pair inthe dataset is annotated with ?Bidirectional?, ?For-ward?, ?Backward?
or ?No entailment?
judgements.2.1 ApproachOur system builds on the integration of lexical,syntactic and semantic features in a supervisedlearning framework.
Our model builds on threemain feature sets, respectively derived from: i)phrase tables, ii) dependency relations, and iii)semantic phrase tables.1.
Phrase Table (PT) matching: throughthese features, a semantic judgement about entail-ment is made exclusively on the basis of lexicalevidence.
The matching features are calculatedwith a phrase-to-phrase matching process.
A phrasein our approach is an n-gram composed of oneor more (up to 5) consecutive words, excludingpunctuation.
Entailment decisions are assignedcombining phrasal matching scores calculated foreach level of n-grams (i.e.
considering the numberof 1-grams, 2-grams,..., 5-grams extracted from Hthat match with n-grams in T).
Phrasal matches,performed either at the level of tokens, lemmas, orstems, can be of two types:1.
Exact: in the case that two phrases are identicalat one of the three levels (token, lemma, stem).2.
Lexical: in the case that two different phrasescan be mapped through entries of the resourcesused to bridge T and H (i.e.
phrase tables).For each phrase in H, we first search for exactmatches at the level of token with phrases in T. Ifno match is found at a token level, the other levels(lemma and stem) are attempted.
Then, in case offailure with exact matching, lexical matching is per-formed at the same three levels.
To reduce redun-dant matches, the lexical matches between pairs ofphrases which have already been identified as exactmatches are not considered.Once the matching phase for each n-gramlevel has been concluded, the number of matchesMatchn and the number of phrases in the hypoth-esis H(n) is used to estimate the portion of phrasesin H that are matched at each level n (Equation 1).1Since languages can express the same meaning withdifferent amounts of words, a phrase with length nin H can match a phrase with any length in T.Matchn =Matchn|H(n)|(1)In order to build English-Spanish phrase tablesfor our experiments, we used the freely availableEuroparl V.4, News Commentary and UnitedNations Spanish-English parallel corpora releasedfor the WMT10 Shared Translation Task.2 Werun the TreeTagger (Schmid, 1995) and Snowballstemmer (Porter, 2001) for preprocessing, and usedthe Giza++ (Och and Ney, 2000) toolkit to align thetokenized corpora at the word level.
Subsequently,we extracted the bi-lingual phrase table from thealigned corpora using the Moses toolkit (Koehn etal., 2007).2.
Dependency Relation (DR) matching tar-gets the increase of CLTE precision.
By addingsyntactic constraints to the matching process,DR features aim to reduce wrong matches oftenoccurring at the lexical level.
For instance, the con-tradiction between ?Yahoo acquired Overture?
and?Overture compro?
Yahoo?
is evident when syntax(in this case subject-object inversion) is taken intoaccount, but can not be caught by bag-of-wordsmethods.We define a dependency relation as a triple thatconnects pairs of words through a grammatical rela-tion.
For example, ?nsubj (loves, John)?
is a depen-dency relation with head loves and dependent Johnconnected by the relation nsubj, which means that?John?
is the subject of ?loves?.
DR matching cap-tures similarities between dependency relations, bycombining the syntactic and lexical level.
In a validmatch, while the relation has to be the same (?exact?1When checking for entailment from H to T, the normaliza-tion is carried out dividing the number of n-grams in H by thenumber of n-grams in T. The same holds for dependency rela-tion and semantic phrase table matching.2http://www.statmt.org/wmt10/702match), the connected words must be either the sameor semantically equivalent in the two languages.
Forexample, ?nsubj (loves, John)?
can match ?nsubj(ama, John)?
and ?nsubj (quiere, John)?
but not?dobj (quiere, John)?.Given the dependency tree representations of Tand H, for each grammatical relation (r) we calcu-late a DR matching score (Matchr, see Equation 2)as the number of matching occurrences of r in T andH (respectively DRr(T ) and DRr(H)), divided bythe number of occurrences of r in H.matchr =|match(DRr(T ), DRr(H))||DRr(H)|(2)In our experiments, in order to extract de-pendency relation (DR) matching features, thedependency tree representations of English andSpanish texts have been produced with DepPattern(Otero and Lopez, 2011).
We then mapped thesets of dependency relation labels for the English-Spanish parser output into: Adjunct, Determiner,Object, Subject and Preposition.
The dictionary,containing about 9M bilingual word pairs, createdduring the alignment of the English-Spanish parallelcorpora provided the lexical knowledge to performmatches when the connected words are different.3.
Semantic Phrase Table (SPT) matching:represents a novel way to leverage the integrationof semantics and MT-derived techniques.
To thisaim, SPT improves CLTE methods relying on purelexical match, by means of ?generalized?
phrasetables annotated with shallow semantic labels.Semantically enhanced phrase tables, with entries inthe form ?
[LABEL] word1...wordn [LABEL]?
(e.g.?
[ORG] acquired [ORG]?
), are used as a recall-oriented complement to the lexical phrase tablesused in machine translation (token-based entries like?Yahoo acquired Overture?).
The main motivationfor this augmentation is that word replacement withsemantic tags allows to match T-H tokens that donot occur in the original bilingual parallel corporaused for phrase table extraction.
Our hypothesisis that the increase in recall obtained from relaxedmatches through semantic tags in place of ?out ofvocabulary?
terms (e.g.
unseen person, location, ororganization names) is an effective way to improveCLTE performance, even at the cost of some loss inprecision.
Semantic phrase tables, however, havetwo additional advantages.
The first is related totheir smaller size and, in turn, its positive impacton system?s efficiency, due to the considerablesearch space reduction.
Semantic tags allow tomerge different sequences of tokens into a single tagand, consequently, different phrase entries can beunified to one semantic phrase entry.
As a result, forinstance, the SPT used in our experiments is morethan 30% smaller than the original token-based one.The second advantage relates to their potential im-pact on the confidence of CLTE judgements.
Sincea semantic tag might cover more than one tokenin the original entry phrase, SPT entries are oftenshort generalizations of longer original phrases.Consequently, the matching process can benefitfrom the increased probability of mapping higherorder n-grams (i.e.
those providing more contextualinformation) from H into T and vice-versa.Like lexical phrase tables, SPTs are extractedfrom parallel corpora.
As a first step, we annotatethe corpora with named-entity taggers (FreeLing inour case (Carreras et al, 2004)) for the source andtarget languages, replacing named entities with gen-eral semantic labels chosen from a coarse-grainedtaxonomy including the categories: person, location,organization, date and numeric expression.
Then,we combine the sequences of unique labels into onesingle token of the same label, and we run Giza++(Och and Ney, 2000) to align the resulting seman-tically augmented corpora.
Finally, we extract thesemantic phrase table from the augmented alignedcorpora using the Moses toolkit (Koehn et al, 2007).For the matching phase, we first annotate T andH in the same way we labeled our parallel corpora.Then, for each n-gram order (n=1 to 5, excludingpunctuation), we use the SPT to calculate a matchingscore (SPT matchn, see Equation 3), as the num-ber of n-grams in H that match with phrases in Tdivided by the number of n-grams in H. The match-ing algorithm is same as the phrase table matchingone.SPT matchn =|SPTn(H) ?
SPT (T )||SPTn(H)|(3)703Run Features Classification Parameter selection Result1 PT+SPT+DR Multiclass Entire training set 0.5022 PT+SPT+DR Multiclass 2-fold cross validation 0.4903 PT+SPT+DR Binary Entire training set 0.5044 PT+SPT+DR Binary 2-fold cross validation 0.500Table 1: Summary of the submitted runs and results for Spa-Eng dataset.Forward Backward No entailment BidirectionalP R F1 P R F1 P R F1 P R F10.515 0.704 0.595 0.546 0.568 0.557 0.447 0.304 0.362 0.482 0.440 0.460Table 2: Best run?s Precision/Recall/F1 scores.In our supervised learning framework, the com-puted PT, SPT and DR scores are used as sepa-rate features, giving to an SVM classifier, LIBSVM(Chang and Lin, 2011), the possibility to learn opti-mal feature weights from training data.2.2 Submitted runsIn order to test our models under different condi-tions, we set the CLTE problem both as two-way andmulticlass classification tasks.Two-way classification casts multidirectional en-tailment as a unidirectional problem, where eachpair is analyzed checking for entailment both fromleft to right and from right to left.
In this condi-tion, each original test example is correctly clas-sified if both pairs originated from it are correctlyjudged (?YES-YES?
for bidirectional, ?YES-NO?for forward, ?NO-YES?
for backward entailment,and ?NO-NO?
for no entailment).
Two-way clas-sification represents an intuitive solution to capturemultidirectional entailment relations but, at the sametime, a suboptimal approach in terms of efficiencysince two checks are performed for each pair.Multiclass classification is more efficient, but atthe same time more challenging due to the higherdifficulty of multiclass learning, especially withsmall datasets.
We also tried to use the parameter se-lection tool for C-SVM classification using the RBF(radial basis function) kernel, available in LIBSVMpackage.
Our submitted runs and results have beenobtained with the settings summarized in table 1.As can be seen from the table, our best result hasbeen achieved by Run 3 (50.4% accuracy), whichis significantly higher than the average and medianscore over the best runs obtained by participants(44.0% and 40.7% respectively).
The detailed re-sults achieved by the best run are reported in Table2.
We can observe that our system is performingwell for recognizing the unidirectional entailment(i.e.
forward and backward), while the performancedrops over no entailment pairs.
The low results forbidirectional cases also reflect the difficulty of dis-criminating the no entailment pairs from the bidi-rectional ones.
Looking at the detailed results, wecan observe a high recall in the forward and back-ward entailment cases, which could be explained bythe effectiveness of the semantic phrase table match-ing features aiming at coverage increase over lexi-cal methods.
Adding more linguistically motivatedfeatures and weighting the non-matched phrases canbe a starting point to improve the overall results forother cases (bidirectional and no entailment).3 ConclusionIn this paper we described our participation to thecross-lingual textual entailment for content synchro-nization task at SemEval-2012.
We approached thistask by combining lexical, syntactic and semanticfeatures, at the cross-lingual level without recourseto intermediate translation steps.
In spite of thedifficulty and novelty of the task, our results onthe Spanish-English dataset (0.504) prove the effec-tiveness of the approach with significant improve-ments over the reported average and median accu-racy scores for the 29 submitted runs (respectively40.7% and 34.6%).AcknowledgmentsThis work has been partially supported by the EU-funded project CoSyne (FP7-ICT-4-248531).704ReferencesX.
Carreras, I. Chao, L.
Padro?, and M. Padro?.
2004.FreeLing: An Open-Source Suite of Language An-alyzers.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC?04).C.C.
Chang and C.J.
Lin.
2011.
LIBSVM: A Libraryfor Support Vector Machines.
ACM Transactions onIntelligent Systems and Technology (TIST), 2(3).P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceedings of the45th Annual Meeting of the ACL on Interactive Posterand Demonstration Sessions (ACL 2007).Y.
Mehdad, M. Negri, and M. Federico.
2010.
TowardsCross-Lingual Textual Entailment.
In Proceedings ofthe 11th Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL HLT 2010).Y.
Mehdad, M. Negri, and M. Federico.
2011.
UsingBilingual Parallel Corpora for Cross-Lingual TextualEntailment.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies (ACL HLT 2011).Y.
Mehdad, M. Negri, and M. Federico.
2012a.
Detect-ing Semantic Equivalence and Information Disparityin Cross-lingual Documents.
In Proceedings of theACL?12.Y.
Mehdad, M. Negri, and M. Federico.
2012b.
Matchwithout a Referee: Evaluating MT Adequacy withoutReference Translations.
In Proceedings of the Ma-chine Translation Workshop (WMT2012).M.
Negri and Y. Mehdad.
2010.
Creating a Bi-lingualEntailment Corpus through Translations with Mechan-ical Turk: $100 for a 10-day rush.
In Proceedings ofthe NAACL HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk,pages 212?216.
Association for Computational Lin-guistics.M.
Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, andA.
Marchetti.
2011.
Divide and conquer: Crowd-sourcing the creation of cross-lingual textual entail-ment corpora.
In Proceedings of EMNLP 2011.M.
Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, andD.
Giampiccolo.
2012.
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchroniza-tion.
In Proceedings of the 6th International Workshopon Semantic Evaluation (SemEval 2012).F.J.
Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2000).P.G.
Otero and I.G.
Lopez.
2011.
A Grammatical For-malism Based on Patterns of Part-of-Speech Tags.
In-ternational journal of corpus linguistics, 16(1).M.
Porter.
2001.
Snowball: A language for stemmingalgorithms.H.
Schmid.
1995.
Treetaggera language indepen-dent part-of-speech tagger.
Institut fu?r MaschinelleSprachverarbeitung, Universita?t Stuttgart.705
