IIOLiNI)EI) CONH'XT PARSING AND FASY I.I'AI+.NAIIII.ITYRobert C. IlcrwickRoom 820.
MH" Artificial Intelligence I ~lbCambridge.
MA 02139AIISTRACI"Natural angt~ages are often assumed to be constrained so that theyare either easily learnable or parsdble, but few studies haveinvestigated the conrtcction between these two "'functional'"demands, Without a fonnal model of pamtbility or learnability, it isdifficult to determine which is morc "dominant" in fixing theproperties of natural anguages.
In this paper we show that if weadopt one precise model of "easy" parsability, namely, that ofboumled context parsabilio,, and a precise model of "easy"learnability, namely, that of degree 2 learnabilio" then we can showthat certain families of grammars that meet the bounded contextparsability ct~ndition will also be degree 2 learnable.
Someimplications of this result for learning in other subsystems oflinguistic knowledge are suggested.
1I INTRODUCTIONNatural languages are usually assumed to be constrained so thatthey arc both learnable and par'sable.
But how are these twofunctional demands related computationally?
With someexceptions, 2 there has been little or no work connecting these twokey constraints on natural languages, even though linguisticresearchers conventionally assume that learnability somehow playsa dominant role in "shaping" language, while eomputationalistsusually assume that efficient prncessability is dominant.
Can thesetwo functional demands be recrtnciled?
There is in fact no a priorireason to believe that the demands of learnability and parsabilityare necessarily compatible.
After all.
learuability has to do with thescattering of possible grammars with respect tu evidence input to alearning procedure.
This is a property of a family of grammars.Efficient parsability, on the other hand.
is a property of a singlegrammar.
A family of grammars could be easily learnable but noteasily parsable, or vice-versa.
It is easy to provide xamples of bothsorts.
For example, there are finite collections of grammarsgenerating non-rccursivc languages that are easily learnable (justuse a disjoint vocabulary as triggering evidcncc to distinguishamong them), Yet by dcfinition these languages cannot be easilyparsable.
On the other hand as is wcll known even the class of all1.
This v,'ork has h~n ~rried out at the MIT Artificial Intelliger.
?e I,aboratory.Support for the l.aborator3"s artificial intdligenc?
research ~sprovided mpart by theDcf~:nse Advanced Research Projects Agency.2.
See Ik~r~iek 1980 for a sketch of the connections between learnability andparsability.Iinite languages plus the tmiver~d inlirtite language coxcring themall is not learnable from just positive evidence (Gold 1967).
Yeteach of these languages is linite state and hence efficientlyanalyzable.
'lhis paper establishes tile first known resolts lbnnally linkingefficient par~tbility to efficient Icarnability.
It connects a particularmodel of efficient parsing, namely, bounded context pal.
'sing withlookahead as developed by Marcus 1980. to a particular model oflanguage acqnisilitm, the Bounded l)egree of Error (Ill)E) model ofWexlcr and Culicovcr 1980.
The key result: bounded contextparsability implies "'easy" learnability.
Here, "easily learnable"means "'learnable from simple, positive (grammatical) sentences ofbounded cgrec of embedding."
In this case then, the constraintsrequired to guarantee easy parsability, as enforced by the boundedcontext eortstraJllt, are at least as strong as those required for easylearnability.
This means that if we have a language and associatedgrammar that is known to be parsable by a Marcus-type machine.then we already know that it meets the constraints of boundeddegree learning, as defined by Wcxler and Culicover.A number of extensions to the learnability-parsabilityconnection are also suggested.
One is to apply the result to otherlinguistic subsystems, notably, morphological nd phonological rulesystems.
Although these subsystems are finite state, this does notautomatically imply easy learnability, as Gold (1967) shows.
In fact,identification is still computationally intractable -- it is NP-hard(Gold 1978), taking an amount of evidence exponentiallyproportional to the number of states in the target finite state system.Since a given natural anguage could have a morphological systemof a few hundred or even a few thousand states (Kimmn 1983, forFinnish), this is a serious problem, Thus we must find additionalconstraints to make natural morphological systems tractablylearnable.
An analog of the bounded context model formorphological systems may suffice.
If we require that such systemsbe k-reversible, as defined by Angluin (in press), then art efficientpolynomial time induction algorithm exists.To summarize, what is the importance of this result forcomputational linguistics?o It shows for the first time thatparsability is stronger constraint itanlearnability, at least given this particularway of defining the comparison.
Thuscomputationalists may have been rightin tbcusing on efficient parsability as ametric for comparing theories.20o It provides an explicit criterion forlearnability.
This criterion can bc tied toknown grammar and language classresults.
For example, we can .say that thelanguage anbncn will be easily learnable,since it is hounded context parsablc (inan extended sense).u It Ibrlnall.~ cnnnects the Marcus modelfi~r p.nsing to a model of acquisition.
Itpinf~oints he rcl,ttionship of tile Marcusparser ~o the 1.1~,( k I and btmndcd contextp,trsmg models.o It suggests criteria fi~r tile learnability~f phomflogical and rnorphulugicalsystems.
In particular, fl~c notitm ofk-reversibility, the anah~g of boundedcontext par.~d'~ility Ibr Iinite slaues3,stems, may play a key nile here.
Thereversibility constraint thus lendslearnahilit.v support to computationalframeworks that propose "'reversible"rules (such as that of Koskcnnicmi 1983)versus those that do not (such asstandard generative approaches).This paper is organized as follows.
Section l reviews the basicdefinitions of the bounded context model for parsing and thebounded egree of error model for learning.
Section 2 sketches themain result, leaving aside the details of certain lemmas.
Section 3extends the bounded context--bounded degree of error model tomorphological nd phtmological systems, and advances the notionof k.reversibility as the analog of bounded context parsability forsuch finite state sysiems.1I IIOUNDED CONTEXT PARSAIflI.ITY ANDI)OUNDED DEGREE OF EI~,ROR I.EARNINGTo begin, we define the models of parsing and learning that will beused in the sequel.
The parsing model is a variant of the Marcusparser.
"I11e learning theory is the Degree 2 theory of Wexler andCulicover (1980).
The Marcus parser defines a class of languages(and associated grammars) that are easily pa~able; Degree 2 theory,a class of languages (and asstx:iated grammars) that is easilylearnable.To begin our comparison, We must say what class of "easilylearnable" languages l)egrec 2 theory defines.
The aim of thetheory is to define constraints such that a family of transfonnationalgrammars will be learnable from "'simple" data; the learningprocedure can get positive (grammatical) example sentences ofdepth of embedding of two or tess (sentences up to two embeddedsentences, but no more).
The key property of the translbrmationalfamily that establishes learnability is dubbed Bounded Degree ofI?rror.
Roughly and intuitively.
BI)E is a property related to the"separability" of langu:tges and grammars given simple data: ifthere is a way for the learner to tell that a currently hypnthesizedlanguage {and grammar) is incorrect, then there must be somesimple scntc'~ce that reveals this -- all languages in the family mustbe separable b',' simple sentences.The wa.~ that the learner can tell that a currentl~ I1H~othesizcdgrammar is wrong given some sample sentence is by trying to seewhether the current granlmar can nl~lp from a deep structure for thesentence to the observed ~mple sentence.
That is, we imagine thelearner being li~d with a series of hase (deep structnre)-st, rfacesentence (denoted "'b, s") pairs.
(See Wexler and Culicover 1980 furdetails and justification of this approach, as well as a weakening ofthe requirement that base structures be available: see Berwick 19801982 for an independently developed conlputational version.)
Ifthelearner's current ransformational component.
'1I, can map from bto s. then all is well.
If not.
and Tl(b)=s does not equal s. then adetectable error has been uncovered.With this background we can provide a precise definition of theBI)E property:A family of transrormationally-generated l nguages kpossesses the BI)t- property iff for any base grammar B(fur languages in 13 there exists a finite integer U. suchthat for an).
possible adult transformational componentA and learner component C, if A and C disagree on anyphrase-marker b generated by B. then they disagree onsome phrase-marker b generated by B, with b' ofdegreeat most U. Wexler and Culicover 1980 page 108.If we substitute 2 for U in the theorem, we get the Degree 2constraint.Once IIDE is established for some family of languages, thenconvergence of a learning procedure iseasy to proved.
Wexler andCulicover 1980 have the details, but the key insight is that thenumber of possible rrors is now bounded from above.The BDE property can be defined in any grammaticalframework, and this is what we shall do here.
We retain the idea ofmapping from some underlying "base" structure to the surfacesentence.
(If we are parsing, we must map from the surfacesentence to this underlying structure.)
The mapping is notnecessarily transformational, however; for example, a set ofcontext-free rules could carry it out.
In this paper w?
assume thatthe mapping from surface sentences to underlying structures icarried out by a Marcus-type parser.
The mapping from structureto sentence is then defined by the inverse of the operation of thismachine.
This fixes one possible target language.
(The full versionof this paper defines this mapping in full.
)Note further that the BDE property is defined not just withrespect to possible adult target languages, but also with respect tothe distribution of the learner's possible guesses.
So for example,even if there were just ten target languages (defining 10 underlyinggrammars), the BDE property must hold with respect o thoselanguages and any intervening learner languages (grammars).
Sowe must also define a family of languages to be acquired.
This isdone in the next section.BI)E, then, is our criterial property for easy learnability.
Justthose lhmilies of grammars that possess the BI)E property (withrespect to a learner's guesses) are easily learnable.Now let us I11rn to bounded context parsal)ilit).
(llCl>).
Thedefinition ~)1" IICI ) used here an extension t)f the standard elinitionas in Aht)and Lillmall 1972 p. 427.
Intuitively.
a grammar is IICP ifit is "'backwards deterministic" given a radius nf k tokens around21cvcry parsing decision.
That is.
it is possible to finddcte.rmiuistically the production that vpplied at a given step in aderivation by examining just a btnmded mnuber of tokens (fixed inadvance) to the left and right at that point in the derivation.Following Aho and UIIman we have this definition for boundedright-context grammars:G is bounded right-context if the following four conditions:(1) S=:'aA,~=:'a#~ and(2) S=%,Bx=~-~,~x = a'B,bare rightmost derivations in the grammar;(3) the length ofx is less than or equal to the length of,/,and(4) the last m symbols of a and a' coincide,and the first n symbols of,., and ~, coincideimply that A=B, a'=v, and ,/' = x.We will u~ the term "bounded context" instead of "boundedright-context."
To extend the definition we drop the requirementthat the derivation is rightmost and use instead non-canonicalderivation sequences a  defined by Szymanski and Williams (1976).This model corresponds toMarcus's (1980) use of attention shi.Bs topostpone parsing decisions until more right context is examined.The effect is to have a lookahead that can include nonterminainames like NP or VP.
For example, in order to successfully parseHave the students take the exam, the Marcus parser must delayanalyzing hare until the full NP the students is processed.
Thus acanonical (rightmost) parse is not produced, and the lookahead forthe parser includes the sequence NP--take, successfullydistinguishing this parse from the NP--taken sequence for a yes-noquestion.
This extension was first proposed by Knuth (1965) anddeveloped by Szymanski and Williams (1976).
In this model we canpostpone a canonical rightmost derivation some fixed number ofthnes t. This corresponds to building t complete subtrees andmaking these part of the lookahead before we return to thepostponed analysis.The Marcus machine (and the model we adopt here) is not asgeneral as an l.R(k) type parser in one key respect.
An I.R(k)parser can use the entire left context m making its parsing decisions.
(It alst) uses a bounded right context, its h)okahead.
)The 1.R(k),nachine can do this because the entire left context can be stored asa regular set in the finite control of the parsing machine (see Knuth1965).
That is, l.R(k) parsers make use uf an encoding of the leftcontext in order to keep track of what to do.
The Marcus machineis much mure limited than this.
l.ocal parsing decisions arc madeby examining strictly litend contexts an)und file current locus ofparsing contexts.
A finite state encoding of left context is notpermitted.The BCP class also makes sense its a pn)xy for "'efficientlyparsable" because all its members are analyzable in time linear inthe length t)\[" their input sentences, at least if file associatedgr~lllllllars are COlttext-fiee.
If die ~r~lllllTlars are nol etmtext-free.then BCP members are parsahle in at ~orst quadratic (n squared)time.
(See Szymanski and Williams 1976 fur proofs of theseresults.
)III CONNIT_q'ING PARSABII.ITY AND I.EARNABII.ITYWe can now at least furmalize our problem of comparinglearnability and parsability.
The question now becomes: What isthe relationship between the Ill)t" property and the BCP property?Intuitively, a grammar is BCP if we can always tell which of tworules applied in a given bounded context.
Also intuitively, a familyof grammars i  III)E il: given any two grammars in the family G andG" with different roles R and R" say.
we can tell which rule is thecorrect one by looking at two derivations ofbotmded egree, with Rapplying in one and yielding surface string s, and R" applying in theudder yielding surface string s'.
with s not equal to s'.
This propertymust hold with respect o all possible adult and learner grammars.So a space of possible target grammars must be considered.
Theway we do this is by considering some '*fixed" grammar G andpossible variants of G formed by substituting the production rulesin G with hypothesized alternatives.The theorem we want to now prove is:If the grammars formed by augmenting G with possiblehypothesized grammar rules arc BCP.
then that family isalso BDE.The theorem is established by using the BCP property to directlyconstruct a small-degree phrase marker that meets the BDEcondition.
We select two grammars G, G' from the family ofgrammars.
Both are BCP, by definition.
By assumption, there is adetectable error that distinguishes G with rule R from G' with ruleR'.
Letus .say that Rule R is of the form A~a;  R' is B=*'a'.Since R' determines a detectable rror, there must be aderivation with a common sentential form ,t, such that R applies to,I, and eventually derives sentence s, while R' applies to ?, andeventually derives ' different from s. The number of steps in thederivation of the the two sentences may be arbitrary, however.What we must show is that there are two derivations bounded inadvance by some constant that yield two different sentences.The BCP conditions tate that identical (re.n) contexts implythat A and B are equal.
Taking the contrapositive, if A and B areunequal, then the 0n,n) context must be nonidentical.
Thisestablishes that BCP implies (re.n) context error detectability.
3We are not yet done though.
An (Ul.U) context detectable errorcould consist of tenninal and nonterminal elements, not justterminals (words) as required by the detectable error condition.
Wemust show that we can extend such a detectable rror to a surfacesentence detectable rror with an underlying structure of boundeddegree.
An easy lemma establishes this.If R' is an (m.n) context detectable rror, then R' isbounded egree of error detectable.The proof (by induction) is omitted: only a sketch will be givenhere.
Intuitively.
the reason is that ~e can extend any nonterminalsin the error-detectable (m,n) context o some valid surface sentenceand bound this derivation by some constant fixed in advance anddepending only on the grammar.
This is because unboundedderivations are possible only by the repetitiort of nontermirmls viarecursion: since there are only a finite number of distinctnonterminals, it is only via recursion that wc can obtain a derivationchain that is arbitrarily deep.
But.
as is well knuwn (compare theproof of the pumping lemma for context-free grammars), any sucharbitrarily deep derivation producing a valid surface sentence alsohas an associated truncated derivation, bounded by a constant22dependent on the grammar, that yields a valid sentcnce of thelanguage.
Thus we can convert any (re.n) context detectable errorto a bounded egree of error sentence.
This proves the basic result.As an application, consider the strictly context-sensitivelanguage anbnc n. This language has a grammar that is BCP in theextended sense (Szymanski and Williams 1976).
The family ofgrammars obtained by replacing the rules of this IICP grammar byalternative rules that are also 11CP (including the original grammar)meets the BDE condition.
This result was establishedindependently b Wexler 1982.IV EXTENSIONS OF THE BASIC RESULTIn the domain of syntax, we have seen that constraints ensuringefficiem parsability also guarantee asy lcarnability.
This resultsuggests an extension to other domains of linguistic knowledge.Consider morphological rule systems.
Several recent modelssuggest finite state transducers a a way to pair lexical (surface) andunderlying titans of words (Koskenniemi 1983: Kaplan and Kay1983).
While such systems may well be efficiently analyzable, it isnot so ~ell known that easy learnability does not follow directlyfrom this adopted formalism.
To learn even a finite state systemone must examine all possible state-transition combinations.
This iscombinatorially explosive, as Gold 1978 proves.
Without additionalconstraints, finite trzmsducer induction is intractable.What is needed is some way to localize errors: this is what thebounded egree ofern)r condition does.Is there ill) an;dog tlf the the IICP condition for finite statesystems that also implies easy learnahility?
The answer is yes.
Theessence of BCP is that derivations are backwards and forwardsdeterministic within local (m.n) contexts.
But this is precisely thenotion of k-reversibilit.I; as defined by Angluin (in press).
Angluinshows that k-reversible automata have polynomial time inductionalgorithms, in contrast to the result for general finite state automata.It then becomes important to .see if k-reversibility holds for currenttheories of morphological rule systems.
The fifll paper analyzesbt)th "'classical" generative theories (that do not seem to meet thetest of reversibility) and recent transducer theories.
Sincek-reversibility is a sufficient, but evidently not a necessaryconstraint fi,r Icarnability.
there could be other conditionsguaranteeing the Ic;,rnability of finite state systems.
For instance.One of the~, the strict cycle condition in phonology, is alsoexamined in the full paper.
We show that the strict cycle alsost, flices to meet he III)E condition.In short, it eppcars that .
".t Icz:st in terms of one framework in whicha fontal comparison can bc made, the same constraints hat forgeefficient parsability also ensure asy learnability.V REFERENCESAho, J. and Ullman, J.
1972.
The Theory of Parsh~g, Translation,and Compiling, vol.
1., Englewood-Cliffs, N J: Prentice-Hall.Angluin, D. 1982.
Induction of k-reversible languages.
In press,JACM.Berwiek, R. 1980.
Computational analogs of constraints ongrammars.
Proceedings of the 18th Annual Meeting of theAssociation for Computational Linguistics.Berwick, R. 1982.
Locality Principles and the Acquisition ofSyntactic Knowledge, PhD dissertation, MIT Department ofElectrical Engineering and Computer Science.Gold, E. 1967.
Language identification i  the limit.
Informationand Control, 10.Gold, E. 1978.
On the complexity of minimum inference of regularsets.
h~fonnation a d Control 39, 337-350.Kaplan, R. and Kay, M. 1983.
Word recognition.
Xerox Palo AltoResearch Center.Koskennicmi, K. 1983.
Two-Level Morphology: A GeneralComputational Model for Word Form Recognition and Production,Phi) dissc~ltion, University ofl lelsinki.Knuth.
D. 1965.
On the translation of languages from left  to right.In.fimnathm and ('ontroL 8.Marcus.
M. 1980.
A Model of Syntactic Recognition for NaturalLanguage.
Cambridge MA: MIT Press.Szymanski.
T. and Williams.
J.
1976.
Noncanonical extensions ofbottomup arsing techniques.
SIAM .1.
Computing, 5.Wexler, K. 1982.
Some isst,es in the formal theory of learnability.in C. Baker and J. McCarthy (eds.).
The Logical Problem ofl,anguage Acquisition.Wexler, K. and P. Culicover 1980.
Formal Principles of LanguageAcquisition, Cambridge, MA: Mrr  Press.3 One of lhe nlh,,'r ~hJee nCP ~mdilions could al.~ be ~ioldle.d, bu!
ll'lcs~ atea::~:un.ed t.~e .~)) ~,~Ud,nlic::, W;" ."..
',~Jme (h~' existence of dcd,.ali~,ns meeting,"(mdh!
(m.~ t l ).rod L",) ~n Ihc cxlet:,l..'d !:?n,.u.
i!s v.cJl as ccmdi!ion (3).23
