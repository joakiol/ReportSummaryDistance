Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 387?392,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGeneralized Reordering Rules for Improved SMTFei Huang Cezar PendusIBM T. J. Watson Research Center IBM T. J. Watson Research Centerhuangfe@us.ibm.com cpendus@us.ibm.comAbstractWe present a simple yet effectiveapproach to syntactic reordering forStatistical Machine Translation (SMT).Instead of solely relying on the top-1best-matching rule for source sentencepreordering, we generalize fullylexicalized rules into partially lexicalizedand unlexicalized rules to broaden therule coverage.
Furthermore, , we considermultiple permutations of all the matchingrules, and select the final reordering pathbased on the weighed sum of reorderingprobabilities of these rules.
Ourexperiments in English-Chinese andEnglish-Japanese translationsdemonstrate the effectiveness of theproposed approach: we observeconsistent and significant improvementin translation quality across multiple testsets in both language pairs judged byboth humans and automatic metric.1 IntroductionLanguages are structured data.
The properhandling of linguistic structures (such as wordorder) has been one of the most important yetmost challenging tasks in statistical machinetranslation (SMT).
It is important because it hassignificant impact on human judgment ofMachine Translation (MT) quality: an MT outputwithout structure is just like a bag of words.
It isalso very challenging due to the lack of effectivemethods to model the structural differencebetween source and target languages.A lot of research has been conducted in thisarea.
Approaches include distance-based penaltyfunction (Koehn et.
al.
2003) and lexicalizeddistortion models such as (Tillman 2004), (Al-Onaizan and Papineni 2006).
Because thesemodels are relatively easy to compute, they arewidely used in phrase-based SMT systems.Hierarchical phrase-based system (Hiero,Chiang, 2005) utilizes long range reorderinginformation without syntax.
Other models usemore syntactic information (string-to-tree, tree-to-string, tree-to-tree, string-to-dependency etc.
)to capture the structural difference betweenlanguage pairs, including (Yamada and Knight,2001), (Zollmann and Venugopal, 2006), (Liu et.al.
2006), and (Shen et.
al.
2008).
These modelsdemonstrate better handling of sentencestructures, while the computation is moreexpensive compared with the distortion-basedmodels.In the middle of the spectrum, (Xia andMcCord 2004), (Collins et.
al 2005),  (Wang  et.al.
2007), and (Visweswariah et.
al.
2010)combined the benefits of the above twostrategies: their approaches  reorder an inputsentence  based  on  a set  of  reordering  rulesdefined  over  the  source sentence?s  syntaxparse  tree.
As  a  result,  the  re-ordered  sourcesentence  resembles  the  word  order of  itstarget  translation.
The  reordering  rules  areeither hand-crafted or automatically learned fromthe training  data  (source  parse  trees  andbitext  word alignments).
These rules can beunlexicalized (only including the constituentlabels) or fully lexicalized (including both theconstituent labels and their head words).
Theunlexicalized  reordering  rules are more  generaland  can be applied  broadly, but sometimes  theyare  not  discriminative  enough.
In the followingEnglish-Chinese reordering rules,0.44  NP PP ?
0 10.56  NP PP ?
1 0the NP and PP nodes are reordered with close torandom probabilities.
When the constituents areattached with their headwords, the reorderingprobability is much higher than that of theunlexicalized rules.0.20 NP:testimony PP:by --> 0 10.80  NP:testimony PP:by --> 1 0Unfortunately, the application of lexicalizedreordering rules is constrained by datasparseness: it is unlikely to train the NP:<noun>387PP:<prep> reordering rules for every noun-preposition combination.
Even  for the  learntlexicalized  rules,  their  counts  are  alsorelatively  small,  thus  the  reorderingprobabilities may  not  be  estimated  reliably,which  could lead   to incorrect reorderingdecisions.To alleviate this problem, we generalize fullylexicalized rules into partially lexicalized rules,which are further generalized into unlexicalizedrules.
Such generalization allows partial matchwhen the fully lexicalized rules can not be found,thus achieving broader rule coverage.Given a node of a source parse tree, we find allthe matching rules and consider all their possiblereorder permutations.
Each permutation has areordering score, which is the weighted sum ofreordering probabilities of all the matching rules.We reorder the child nodes based on thepermutation with the highest reordering score.Finally we translate the reordered sentence in aphrase-based SMT system.
Our experiments inEnglish to Chinese (EnZh) and English toJapanese (EnJa) translation demonstrate theeffectiveness of the proposed approach: weobserve consistent improvements across multipletest sets in multiple language pairs andsignificant gain in human judgment of the MTquality.This paper is organized as follows: in section 2we briefly introduce the syntax-based reorderingtechnique.
In section 3, we describe ourapproach.
In section 4, we show the experimentresults, which is followed by conclusion insection 5.2 Baseline Syntax-based ReorderingIn the general syntax-based reordering,reordering is achieved by permuting the childrenof any interior node in the source parse tree.Although there are cases where reordering isneeded across multiple constituents, this still is asimple and effective technique.Formally, the reordering rule is a triple {p, lhs,rhs}, where p is the reordering probability, lhs isthe left hand side of the rule, i.e., the constituentlabel sequence of a parse tree node, and rhs is thereordering permutation derived either from hand-crafted rules as in (Collins et.
al 2005) and(Wang  et.
al.
2007), or from training data as in(Visweswariah et.
al.
2010).The training data includes bilingual sentencepairs with word alignments, as well as the sourcesentences' parse trees.
The children?s relativeorder of each node is decided according to theiraverage alignment position in the target sentence.Such relative order is a permutation of theinteger sequence [0, 1, ?
N-1], where N is thenumber of children of the given parse node.
Thecounts of each permutation of each parse labelsequence will be collected from the training dataand converted to probabilities as shown in theexamples in Section 1.
Finally, only thepermutation with the highest probability isselected to reorder the matching parse node.
TheSMT system is re-trained on reordered trainingdata to translate reordered input sentences.Following the above approach, only thereordering rule [0.56 NP PP  1 0] is kept in theabove example.
In other words, all the NP PPphrases will be reordered, even though thereordering is only slightly preferred in all thetraining data.3 Generalized Syntactic ReorderingAs shown in the previous examples, reorderingdepends not only on the constituents?
parselabels, but also on the headwords of theconstituents.
Such fully lexicalized rules sufferfrom data sparseness: there is either no matchinglexicalized rule for a given parse node or thematching rule?s reordering probability isunreliable.
We address the above issues withrule generalization, then consider all thepermutations from multi-level rule matching.3.1 Rule GeneralizationLexicalized rules are applied only when both theconstituent labels and headwords match.
Whenonly the labels match, these reordering rules arenot used.
To increase the rule coverage, wegeneralize the fully lexicalized rules intopartially lexicalized and unlexicalized rules.We notice that many lexicalized rules sharesimilar reordering permutations, thus it ispossible to merge them to form a partiallylexicalized rule, where lexicalization onlyappears at selected constituent?s headword.Although it is possible to have multiplelexicalizations in a partially lexicalized rule(which will exponentially increase the totalnumber of rules), we observe that most of thetime reordering is triggered by a singleconstituent.
Therefore we keep one lexicalizationin the partially lexicalized rules.
For example, thefollowing lexicalized rule:VB:appeal PP-MNR:by PP-DIR:to --> 1 2 0388will be converted into the following 3 partiallylexicalized rules:VB:appeal PP-MNR PP-DIR --> 1 2 0VB PP-MNR:by PP-DIR    --> 1 2 0VB PP-MNR PP-DIR:to    --> 1 2 0The count of each rule will be the sum of thefully lexicalized rules which can derive the givenpartially lexicalized rule.
In the abovepreordering rules, ?MNR?
and ?DIR?
arefunctional labels, indicating the semantic labels(?manner?, ?direction?)
of the parse node.We could go even further, converting thepartially lexicalized rules into unlexicalizedrules.
This is similar to the baseline syntaxreordering model, although we will keep all theirpossible permutations and counts for rulematching, as shown below.5   VB PP-MNR PP-DIR --> 2 0 122  VB PP-MNR PP-DIR --> 2 1 021  VB PP-MNR PP-DIR --> 0 1 241  VB PP-MNR PP-DIR --> 1 2 035  VB PP-MNR PP-DIR --> 1 0 2Note that to reduce the noise from paring andword alignment errors, we only keep thereordering rules that appear at least 5 times.
Thenwe convert the counts into probabilities:?= )(*,),()|(iiiiii lhsClhsrhsClhsrhspwhere },,{ upfi ?
represents the fully, partiallyand un-lexicalized rules, and ),( ii lhsrhsC  is thecount of rule (lhsi  rhs) in type i rules.When we convert the most specific fullylexicalized rules to the more general partiallylexicalized rules and then to the most generalunlexicalized rules, we increase the rule coveragewhile keep their discriminative power at differentlevels as much as possible.3.2 Multiple Permutation Multi-level RuleMatchingWhen applying the three types of reorderingrules to reorder a parse tree node, we find all thematching rules and consider all possiblepermutations.
As multiple levels of rules can leadto the same permutation with differentprobabilities, we take the weighted sum ofprobabilities from all matching rules (with thesame rhs).
Therefore, the permutation decision isnot based on any particular rule, but thecombination of all the rules matching differentlevels of context.
As opposed to the generalsyntax-based reordering approaches, this strategyachieves a desired balance between broad rulecoverage and specific rule match: when a fullylexicalized rule matches, it has strong influenceon the permutation decision given the richercontext.
If such specific rule is unavailable or haslow probability, more general (partial andunlexicalized) rules will have higher weights.
Foreach permutation we compute the weightedreordering probability, then select thepermutation that has the highest score.Formally, given a parse tree node T, let lhsf bethe label:head_word sequence of the fullylexicalized rules matching T. Similarly, lhsp andlhsu are the sequences of the matching partiallylexicalized and unlexicalized rules, respectively,and let rhs be their possible permutations.
Thetop-score permutation is computed as:?
?=},,{* )|(maxargupfiiiirhs lhsrhspwrhswhere wi?s are the weights of different kind ofrules and pi is reordering probability of each rule.The weights are chosen empirically based on theperformance on a held-out tuning set.
In ourexperiments, wf=1.0, wp=0.5, and wu=0.2, wherehigher weights are assigned to more specificrules.For each parse tree node, we identify the toppermutation choice and reorder its childrenaccordingly.
The source parse tree is traversedbreadth-first.4 ExperimentsWe applied the generalized syntax-basedreordering on both English-Chinese (EnZh) andEnglish-Japanese (EnJa) translations.
OurEnglish parser is IBM?s maximum entropyconstituent parser (Ratnaparkhi 1999) trained onPenn Treebank.
Experiments in (Visweswariahet.
al.
2010) indicated that minimal differencewas observed using Berkeley?s parser or IBM?sparser for reordering.Our EnZh training data consists of 20 millionsentence pairs (~250M words), half of which arefrom LDC released bilingual corpora and theother half are from technical domains (e.g.,software manual).
We first trained automaticword alignments (HMM alignments in bothdirections and a MaxEnt alignment (Ittycheriahand Roukos, 2005)), then parsed the Englishsentences with the IBM parser.
We extracteddifferent reordering rules from the wordalignments and the English parse trees.
After389frequency-based pruning, we obtained 12Mlexicalized rules, 13M partially lexicalized rulesand 600K unlexicalized rules.
Using these rules,we applied preordering on the English sentencesand then built an SMT system with the reorderedtraining data.
Our decoder is a phrase-baseddecoder (Tillman 2006), where various featuresare combined within the log-linear framework.These features include source-to-target phrasetranslation score based on relative frequency,source-to-target and target-to-source word-to-word translation scores, a 5-gram languagemodel score, distortion model scores and wordcount.Tech1 Tech2 MT08# of sentences 582 600 1859PBMT  33.08 31.35 36.81UnLex 33.37 31.38 36.39FullLex  34.12 31.62 37.14PartLex 34.13 32.58 37.60MPML 34.34 32.64 38.02Table 1: MT experiment comparison using differentsyntax-based reordering techniques on English-Chinese test sets.We selected one tuning set from softwaremanual domain (Tech1), and used PRO tuning(Hopkins and May 2011) to select decoderfeature weights.
Our test sets include one fromthe online technical support domain (Tech2) andone from the news domain: the NIST MT08English-Chinese evaluation test data.
Thetranslation quality is measured by BLEU score(Papineni et.
al., 2001).
Table 1 shows the BLEUscore of the baseline phrase-based system(PBMT) thatuses lexicalized reordering at decoding timerather than preordering.
Next, Table 1 shows thetranslation results with several preorderedsystems that use unlexicalized (UnLex), fullylexicalized (FullLex) and partially lexicalized(PartLex) rules, respectively.
The lexicalizedreordering model is still applicable forpreordered systems so that some preorderingerrors can be recovered at run time.First we observed that the UnLex preorderingmodel on average does not improve over thetypical phrase-based MT baseline due to itslimited discriminative power.
When thepreordering decision is conditioned on the headword, the FullLex model shows some gains(~0.3 pt) thanks to the richer matching context,while the PartLex model improves further overthe FullLex model because of its broadercoverage.
Combining all three with multi-permutation, multi-level rule matching (MPML)brings the most gains, with consistent (~1.3 Bleupoints) improvement over the baseline system onall the test sets.
Note that the Bleu scores on thenews domain (MT08) are higher than those onthe tech domain.
This is because the Tech1 andTech2 have one reference translation whileMT08 has 4 reference translations.In addition to the automatic MT evaluation, wealso used human judgment of quality of the MTtranslation on a set of randomly selected 125sentences from the baseline and improvedreordering systems.
The human judgment scoreis 2.82 for the UnLex system output, and 3.04for the improved MPML reordering output.
The0.2 point improvement on the 0-5 scale isconsidered significant.Tech1 Tech2 News# of sentences 1000 600 600PBMT 56.45 35.45 21.70UnLex 59.22 38.36 23.08FullLex 57.55 36.56 22.23PartLex 59.80 38.47 23.13MPML 59.94 38.62 23.31Table 2: MT experiment comparison usinggeneralized syntax-based reordering techniques onEnglish-Japanese test sets.We also apply the same generalized reorderingtechnique on English-Japanese (EnJa)translation.
As there is very limited publiclyavailable English-Japanese parallel data, mostour training data (20M sentence pairs) is fromthe in-house software manual domain.
We usethe same English parser and phrase-baseddecoder as in EnZh experiment.
Table 2 showsthe translation results on technical and newsdomain test sets.
All the test sets have singlereference translation.First, we observe that the improvement frompreordering is larger than that in EnZh MT (1.6-3pts vs. 1 pt).
This is because the word orderdifference between English and Japanese islarger than that between English and Chinese(Japanese is a SOV language while both Englishand Chinese are SVO languages).
Withoutpreordering, correct word orders are difficult toobtain given the typical skip-window beamsearch in the PBMT.
Also, as in EnZh, thePartLex model outperforms the UnLex model,both of which being significantly better than theFullLex model due to the limited rule coveragein the later model: only 50% preordering rules390are applied in the FullLex model.
Tech1 test setis a very close match to the training data thus itsBLEU score is much higher.5 Conclusion and Future WorkTo summarize, we made the followingimprovements:1.
We generalized fully lexicalizedreordering rules to partially lexicalizedand unlexicalized rules for broader rulecoverage and reduced data sparseness.2.
We allowed multiple permutation, multi-level rule matching to select the bestreordering path.Experiment results show consistent andsignificant improvements on multiple English-Chinese and English-Japanese test sets judged byboth automatic and human judgments.In future work we would like to explore newmethods to prune the phrase table withoutdegrading MT performance and to make ruleextraction and reordering more robust to parsingerrors.AcknowledgementThe authors appreciate helpful comments fromanonymous reviewers as well as fruitfuldiscussions with Karthik Visweswariah andSalim Roukos.ReferencesYaser Al-Onaizan , Kishore Papineni, Distortionmodels for statistical machine translation,Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for ComputationalLinguistics, p.529-536, July 17-18, 2006, Sydney,AustraliaDavid Chiang, A hierarchical phrase-based model forstatistical machine translation, Proceedings of the43rd Annual Meeting on Association forComputational Linguistics, p.263-270, June 25-30,2005, Ann Arbor, MichiganMichael Collins , Philipp Koehn , Ivona Kucerov,Clause restructuring for statistical machinetranslation, Proceedings of the 43rd AnnualMeeting on Association for ComputationalLinguistics, p.531-540, June 25-30, 2005, AnnArbor, MichiganMark Hopkins, Jonathan May, Tuning as ranking, InProceedings of the Conference on EmpiricalMethods in Natural Language Processing 2011, pp.1352-1362.
Association for ComputationalLinguistics.Abraham Ittycheriah , Salim Roukos, A maximumentropy word aligner for Arabic-English machinetranslation, Proceedings of the conference onHuman Language Technology and EmpiricalMethods in Natural Language Processing, p.89-96,October 06-08, 2005, Vancouver, BritishColumbia, CanadaPhilipp Koehn , Franz Josef Och , Daniel Marcu,Statistical phrase-based translation, Proceedings ofthe 2003 Conference of the North AmericanChapter of the Association for ComputationalLinguistics on Human Language Technology, p.48-54, May 27-June 01, 2003, Edmonton, CanadaYang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proceedings of COLING/ACL2006, pages 609-616, Sydney, Australia, July.Libin Shen, Jinxi Xu and Ralph Weischedel 2008.
ANew String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
in Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics(ACL).
Columbus, OH, USA, June 15 - 20, 2008.Christoph Tillmann, A unigram orientation model forstatistical machine translation, Proceedings ofHLT-NAACL 2004: Short Papers, p.101-104, May02-07, 2004, Boston, MassachusettsChristoph Tillmann.
2006.
Efficient DynamicProgramming Search Algorithms for Phrase-basedSMT.
In Proc.
of the Workshop CHPSLP atHLT'06.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proceedings of EMNLP-CoNLL.Karthik Visweswariah , Jiri Navratil , JeffreySorensen , Vijil Chenthamarakshan , NandaKambhatla, Syntax based reordering withautomatically derived rules for improved statisticalmachine translation, Proceedings of the 23rdInternational Conference on ComputationalLinguistics, p.1119-1127, August 23-27, 2010,Beijing, ChinaAdwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3).Fei Xia , Michael McCord, Improving a statistical MTsystem with automatically learned rewrite patterns,Proceedings of the 20th international conference onComputational Linguistics, p.508-es, August 23-27, 2004, Geneva, Switzerland391Kenji Yamada , Kevin Knight, A syntax-basedstatistical translation model, Proceedings of the39th Annual Meeting on Association forComputational Linguistics, p.523-530, July 06-11,2001, Toulouse, FranceAndreas Zollmann , Ashish Venugopal, Syntaxaugmented machine translation via chart parsing,Proceedings of the Workshop on StatisticalMachine Translation, June 08-09, 2006, New YorkCity, New YorkAlfred.
V. Aho and Jeffrey D.Ullman.
1972.
The Theory of Parsing, Translationand Compiling, volume 1.
Prentice-Hall,Englewood Cliffs, NJ.392
