Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 248?251,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsHUMB: Automatic Key Term Extraction from Scientific Articlesin GROBIDPatrice LopezINRIABerlin, Germanypatrice lopez@hotmail.comLaurent RomaryINRIA & HUB-IDSLBerlin, Germanylaurent.romary@inria.frAbstractThe Semeval task 5 was an opportunityfor experimenting with the key term ex-traction module of GROBID, a system forextracting and generating bibliographicalinformation from technical and scientificdocuments.
The tool first uses GROBID?sfacilities for analyzing the structure of sci-entific articles, resulting in a first set ofstructural features.
A second set of fea-tures captures content properties based onphraseness, informativeness and keyword-ness measures.
Two knowledge bases,GRISP and Wikipedia, are then exploitedfor producing a last set of lexical/semanticfeatures.
Bagged decision trees appearedto be the most efficient machine learningalgorithm for generating a list of rankedkey term candidates.
Finally a post rank-ing was realized based on statistics of co-usage of keywords in HAL, a large OpenAccess publication repository.1 IntroductionKey terms (or keyphrases or keywords) are meta-data providing general information about the con-tent of a document.
Their selection by authorsor readers is, to a large extent, subjective whichmakes automatic extraction difficult.
This is, how-ever, a valuable exercise, because such key termsconstitute good topic descriptions of documentswhich can be used in particular for informationretrieval, automatic document clustering and clas-sification.
Used as subject headings, better key-words can lead to higher retrieval rates of an arti-cle in a digital library.We view automatic key term extraction as a sub-task of the general problem of extraction of tech-nical terms which is crucial in technical and scien-tific documents (Ahmad and Collingham, 1996).Among the extracted terms for a given scientificdocument in a given collection, which key termsbest characterize this document?This article describes the system realized forthe Semeval 2010 task 5, based on GROBID?s(GeneRation Of BIbilographic Data) module ded-icated to key term extraction.
GROBID is a toolfor analyzing technical and scientific documents,focusing on automatic bibliographical data extrac-tion (header, citations, etc.)
(Lopez, 2009) andstructure recognition (section titles, figures, etc).As the space for the system description is verylimited, this presentation focuses on key aspects.We present first an overview of our approach, thenour selection of features (section 3), the differenttested machine learning models (section 4) and thefinal post-ranking (section 5).
We briefly describeour unsuccessful experiments (section 6) and weconclude by discussing future works.2 BasesPrinciple As most of the successful works forkeyphrase extraction, our approach relies on Ma-chine Learning (ML).
The following steps are ap-plied to each document to be processed:1.
Analysis of the structure of the article.2.
Selection of candidate terms.3.
Calculation of features.4.
Application of a ML model for evaluatingeach candidate term independently.5.
Final re-ranking for capturing relationshipsbetween the term candidates.For creating the ML model, steps 1-3 are appliedto the articles of the training set.
We view steps 1and 5 as our main novel contributions.
The struc-ture analysis permits the usage of reliable featuresin relation to the logical composition of the arti-cle to be processed.
The final re-ranking exploits248general relationships between the set of candidateswhich cannot be captured by the ML models.Candidate term selection In the following,word should be understood as similar to token inthe sense of MAF1.
Step 2 has been implementedin a standard manner, as follows:1.
Extract all n-grams up to 5 words,2.
Remove all candidate n-grams starting orending with a stop word,3.
Filter from these candidates terms havingmathematical symbols,4.
Normalize each candidate by lowercasingand by stemming using the Porter stemmer.Training data The task?s collection consists ofarticles from the ACM (Association for Computa-tional Machinery) in four narrow domains (C.2.4Distributed Systems, H.3.3 Information Searchand Retrieval, I.2.6 Learning and J.4 Social andBehavioral Sciences).
As training data, we usedthis task?s training resources (144 articles fromACM) and the National University of Singapore(NUS) corpus2(156 ACM articles from all com-puting domains).
Adding the additional NUStraining data improved our final results (+7.4%for the F-score at top 15, i.e.
from 25.6 to 27.5).3 Features3.1 Structural featuresOne of the goals of GROBID is to realize reli-able conversions of technical and scientific docu-ments in PDF to fully compliant TEI3documents.This conversion implies first the recognition ofthe different sections of the document, then theextraction of all header metadata and references.The analysis is realized in GROBID with Condi-tional Random Fields (CRF) (Peng and McCal-lum, 2004) exploiting a large amount of trainingdata.
We added to this training set a few ACM doc-uments manually annotated and obtained a veryhigh performance for field recognitions, between97% (section titles, reference titles) and 99% (ti-tle, abstract) accuracy for the task?s collection.Authors commonly introduce the main conceptsof a written communication in the header (title,abstract, table of contents), the introduction, the1Morpho-syntactic Annotation Framework, seehttp://pauillac.inria.fr/ clerger/MAF/2http://wing.comp.nus.edu.sg/downloads/keyphraseCorpus3Text Encoding Initiative (TEI), http://www.tei-c.org.section titles, the conclusion and the reference list.Similarly human readers/annotators typically fo-cus their attention on the same document parts.We introduced thus the following 6 binary fea-tures characterizing the position of a term with re-spect to the document structure for each candidate:present in the title, in the abstract, in the introduc-tion, in at least one section titles, in the conclusion,in at least one reference or book title.In addition, we used the following standard fea-ture: the position of the first occurrence, calcu-lated as the number of words which precede thefirst occurrence of the term divided by the num-ber of words in the document, similarly as, for in-stance, (Witten et al, 1999).3.2 Content featuresThe second set of features used in this work triesto captures distributional properties of a term rel-atively to the overall textual content of the docu-ment where the term appears or the collection.Phraseness The phraseness measures the lexicalcohesion of a sequence of words in a given docu-ment, i.e.
the degree to which it can be consid-ered as a phrase.
This measure is classically usedfor term extraction and can rely on different tech-niques, usually evaluating the ability of a sequenceof words to appear as a stable phrase more oftenthan just by chance.
We applied here the Gen-eralized Dice Coeficient (GDC) as introduced by(Park et al, 2002), applicable to any arbitrary n-gram of words (n ?
2).
For a given term T , | T |being the number of words in T , freq(T ) the fre-quency of occurrence of T and freq(wi) the fre-quency of occurrence of the word wi, we have:GDC(T ) =| T | log10(freq(T ))freq(T )?wi?Tfreq(wi)We used a default value for a single word, because,in this case, the association measure is not mean-ingful as it depends only on the frequency.Informativeness The informativeness of a termis the degree to which the term is representative ofa document given a collection of documents.
Onceagain many measures can be relevant, and we optfor the standard TF-IDF value which is used inmost of the keyphrase extraction systems, see forinstance (Witten et al, 1999) or (Medelyan and249Witten, 2008).
The TF-IDF score for a Term T indocument D is given by:TF-IDF(T,D) =freq(T,D)| D |?
?log2count(T )Nwhere | D | is the number of words in D,count(T ) is the number of occurrence of the termT in the global corpus, and N is the number of doc-uments in the corpus.Keywordness Introduced by (Witten et al,1999), the keywordness reflects the degree towhich a term is selected as a keyword.
In prac-tice, it is simply the frequency of the keyword inthe global corpus.
The efficiency of this featuredepends, however, on the amount of training dataavailable and the variety of technical domains con-sidered.
As the training set of documents for thistask is relatively large and narrow in term of tech-nical domains, this feature was relevant.3.3 Lexical/Semantic featuresGRISP is a large scale terminological databasefor technical and scientific domains resulting fromthe fusion of terminological resources (MeSH, theGene Ontology, etc.
), linguistic resources (part ofWordNet) and part of Wikipedia.
It has been cre-ated for improving patent retrieval and classifica-tion (Lopez and Romary, 2010).
The assumptionis that a phrase which has been identified as con-trolled term in these resources tend to be a moreimportant keyphrase.
A binary feature is used toindicate if the term is part of GRISP or not.We use Wikipedia similarly as the Wikipediakeyphraseness in Maui (Medelyan, 2009).
TheWikipedia keyphraseness of a term T is the prob-ability of an appearance of T in a document beingan anchor (Medelyan, 2009).
We use WikipediaMiner4for obtaining this value.Finally we introduced an additional featurecommonly used in keyword extraction, the lengthof the term candidate, i.e.
its number of words.4 Machine learning modelWe experimented different ML models: Decisiontree (C4.5), Multi-Layer perceptron (MLP) andSupport Vector Machine (SVM).
In addition, wecombined these models with boosting and baggingtechniques.
We used WEKA (Witten and Frank,2005) for all our experiments, except for SVM4http://wikipedia-miner.sourceforge.netwhere LIBSVM (Chang and Lin, 2001) was used.We failed to obtain reasonable results with SVM.Our hypothesis is that SVM is sensitive to the verylarge number of negative examples compared tothe positive ones and additional techniques shouldbe used for balancing the training data.
Resultsfor decision tree and MLP were similar but the lat-ter is approx.
57 times more time-consuming fortraining.
Bagged decision tree appeared to per-form constantly better than boosting (+8,4% forF-score).
The selected model for the final run was,therefore, bagged decision tree, similarly as, forinstance, in (Medelyan, 2009).5 Post-rankingPost-ranking uses the selected candidates as awhole for improving the results, while in the pre-vious step, each candidate was selected indepen-dently from the other.
If we have a ranked list ofterm T1?N, each having a score s(Ti), the newscore s?for the term Tiis obtained as follow:s?
(Ti) = s(Ti) + ?
?1?j 6=iP (Tj|Ti)s(Tj)where ?
is a constant in [0 ?
1] for control-ling the re-ranking factor.
?
has been set ex-perimentally to 0.8.
P (Tj|Ti) is the probabilitythat the keyword Tjis chosen by the author whenthe keyword Tihas been selected.
For obtain-ing these probabilities, we use statistics for theHAL5research archive.
HAL contains approx.139,000 full texts articles described by a rich set ofmetadata, often including author?s keywords.
Weuse the keywords appearing in English and in theComputer Science domain (a subset of 29,000 ar-ticles), corresponding to a total of 16,412 differentkeywords.
No smoothing was used.
The usage ofopen publication repository as a research resourceis in its infancy and very promising.6 ResultsOur system was ranked first of the competitionamong 19 participants.
Table 1 presents our offi-cial results (Precision, Recall, F-score) for com-bined keywords and reader keywords, togetherwith the scores of the systems ranked second(WINGNUS and KX FBK).5HAL (Hyper Article en Ligne) is the French Institutionalrepository for research publications: http://hal.archives-ouvertes.fr/index.php?langue=en250Set System top 5 top 10 top 15Comb.
HUMB P:39.0 R:13.3 F:19.8 F:32.0 R:21.8 F:25.9 P:27.2 R:27.8 F:27.5WINGNUS P:40.2 R:13.7 F:20.5 P:30.5 R:20.8 F:24.7 P:24.9 R:25.5 F:25.2Reader HUMB P:30.4 R:12.6 F:17.8 P:24.8 R:20.6 F:22.5 P:21.2 R:26.4 F:23.5KX FBK P:29.2 R:12.1 F:17.1 P:23.2 R:19.3 F:21.1 P:20.3 R:25.3 F:22.6Table 1: Performance of our system (HUMB) and of the systems ranked second.7 What did not workThe previously described features were selectedbecause they all had a positive impact on the ex-traction accuracy based on our experiments on thetask?s collection.
The following intuitively perti-nent ideas appeared, however, to deteriorate or tobe neutral for the results.Noun phrase filtering We applied a filtering ofnoun phrases based on a POS tagging and extrac-tion of all possible NP based on typical patterns.This filtering lowered both the recall and the pre-cision (?7.6% for F-score at top 15).Term variants We tried to apply a post-rankingby conflating term variants using FASTR6, result-ing in a disappointing ?11.5% for the F-score.Global keywordness We evaluated the key-wordness using also the overall HAL keyword fre-quencies rather than only the training corpus.
Ithad no impact on the results.Language Model deviation We experimentedthe usage of HMM deviation using LingPipe7asalternative informativeness measure, resulting in?3.7% for the F-score at top 15.Wikipedia term Relatedness Using WikipediaMiner, we tried to apply as post-ranking a boostingof related terms, but saw no impact on the results.8 Future workWe think that automatic key term extraction canbe highly valuable for assisting self-archiving ofresearch papers by authors in scholarly reposito-ries such as arXiv or HAL.
We plan to experimentkeyword suggestions in HAL based on the presentsystem.
Many archived research papers are cur-rently not associated with any keyword.We also plan to adapt our module to a large col-lection of approx.
2.6 million patent documents in6http://perso.limsi.fr/jacquemi/FASTR7http://alias-i.com/lingpipethe context of CLEF IP 2010.
This will be the op-portunity to evaluate the relevance of the extractedkey terms for large scale topic-based IR.ReferencesK.
Ahmad and S. Collingham.
1996.
Pointer projectfinal report.
Technical report, University of Surrey.http://www.computing.surrey.ac.uk/ai/pointer/report.C.-C. Chang and C.-J.
Lin.
2001.
Libsvm: a libraryfor support vector machines.
Technical report.P.
Lopez and L. Romary.
2010.
GRISP: A MassiveMultilingual Terminological Database for Scientificand Technical Domains.
In Seventh internationalconference on Language Resources and Evaluation(LREC), Valletta, Malta.P.
Lopez.
2009.
GROBID: Combining AutomaticBibliographic Data Recognition and Term Extrac-tion for Scholarship Publications.
In Proceedings ofECDL 2009, 13th European Conference on DigitalLibrary, Corfu, Greece.O.
Medelyan and I.H.
Witten.
2008.
Domain-independent automatic keyphrase indexing withsmall training sets.
Journal of the AmericanSociety for Information Science and Technology,59(7):1026?1040.O.
Medelyan.
2009.
Human-competitive automatictopic indexing.
Ph.D. thesis.Y.
Park, R.J. Byrd, and B.K.
Boguraev.
2002.
Auto-matic glossary extraction: beyond terminology iden-tification.
In Proceedings of the 19th internationalconference on Computational linguistics-Volume 1,pages 1?7.
Association for Computational Linguis-tics.F.
Peng and A. McCallum.
2004.
Accurate infor-mation extraction from research papers using con-ditional random fields.
In Proceedings of HLT-NAACL, Boston, USA.I.H.
Witten and E. Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition edition.I.H.
Witten, G.W.
Paynter, E. Frank, C. Gutwin, andC.G.
Nevill-Manning.
1999.
KEA: Practical auto-matic keyphrase extraction.
In Proceedings of thefourth ACM conference on Digital libraries, page255.
ACM.251
