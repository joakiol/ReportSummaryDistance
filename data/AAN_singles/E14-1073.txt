Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 692?701,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsFast Recursive Multi-class Classification of Pairs of Text Entitiesfor Biomedical Event ExtractionXiao Liu, Antoine Bordes, Yves GrandvaletUniversit?
de Technologie de Compi?gne & CNRSHeudiasyc UMR 725360200 Compi?gne Cedex, Francefirstname.lastname@hds.utc.frAbstractBiomedical event extraction from arti-cles has become a popular research topicdriven by important applications, such asthe automatic update of dedicated knowl-edge base.
Most existing approaches areeither pipeline models of specific classi-fiers, usually subject to cascading errors,or joint structured models, more efficientbut also more costly and more involved totrain.
We propose here a system based ona pairwise model that transforms event ex-traction into a simple multi-class problemof classifying pairs of text entities.
Suchpairs are recursively provided to the classi-fier, so as to extract events involving otherevents as arguments.
Our model is moredirect than the usual pipeline approaches,and speeds up inference compared to jointmodels.
We report here the best resultsreported so far on the BioNLP 2011 and2013 Genia tasks.1 IntroductionHuge amounts of biomedical documents, suchas molecular biology reports or genomic papersare generated daily.
Automatically organizingtheir content in dedicated databases enables ad-vanced search and eases information retrieval forresearchers in biology, medicine or other relatedfields.
Nowadays, these data sources are mostlyin the form of unstructured free text, which iscomplex to incorporate into databases.
Hence,many text-mining research initiatives are orga-nized around the issue of automatically extract-ing information from biomedical text.
Effortsspecifically dedicated to biomedical text are nec-essary because standard Natural Language Pro-cessing tools cannot be readily applied to extractbiomedical events since such texts, articles or re-ports involve highly domain-specific jargon, syn-tax and dependencies (Kim et al., 2011a).This paper tackles the problem of event extrac-tion from biomedical documents.
Building on pre-vious advances in named entity recognition (fordetecting gene or protein mentions for instance),this task consists in associating to these entities therelated events expressed in the text.
Such eventsare of multiple types and involve at least one textentity as argument and another one as trigger; theycan be quite complex since some events have sev-eral arguments, and recursive in the sense that ar-guments can themselves be events.
An example ofevent is given in Figure 1.Biomedical event extraction is attracting moreand more attention, especially thanks to the or-ganization of recurrent dedicated BioNLP chal-lenges (Kim et al., 2009; Kim et al., 2011b; Kimet al., 2013).
We propose here a new approachwhich relies on a single multi-class classifier forrecursively detecting events from (trigger, argu-ment) pairs.
Compared to standard pipeline ap-proaches based on sequences of classifiers (Bj?rneand Salakoski, 2013; Hakala et al., 2013), weavoid the intermediate problem of associating iso-lated triggers to event types, relying on a trickymulti-label classification problem.
Instead, we di-rectly extract compounds of events in the formof (trigger, argument) pairs, simply relying ona multi-class problem, whereby (trigger, argu-ment) pairs are associated to event types.
Con-sidering pairs of words also allows us to char-acterize examples by sophisticated joint featuressuch as shortest path in the dependency parse tree,and hence to achieve much accurate trigger de-tection than pipeline models.
Besides, comparedto Markov random fields (Riedel and McCallum,2011a), our discriminant model does not repre-sent the full joint distribution of words and events.We thus have a simpler inference process, whichresults into drastically reduced training times (15692!!!!!!allows!Tax!to!recruit!coac/vator!proteins!CBP/p300!to!!!Binding!Binding!
Theme!2Theme!2ThemeThemeFigure 1: Part of a sentence and correspondingextracted events for the BioNLP 2013 Genia task.times faster for processing about 800 training doc-uments).
In short, we propose in this work ahappy medium between pipeline and joint mod-els.
Our approach builds on our previous proposal(Liu et al., 2013), where we detected triggers di-rectly from (trigger, argument) pairs.
Here, weupgrade our scheme by adding a recursive clas-sification process that considerably improves thedetection of complex events.This paper is organized as follows: Section 2 in-troduces the problem of biomedical event extrac-tion and discusses related works.
Section 3 de-scribes our recursive model and its training pro-cess.
The post-processing procedures and the fea-tures used are detailed in Sections 4 and 5.
Sec-tion 6 shows that our method achieves excellentempirical results, with the best performance re-ported so far on the BioNLP 2011 and 2013 Geniatasks, and a reduced training duration compared tothe previously state-of-the-art models.2 Context and Related WorksBiomedical event extraction aims at extractingevent formulas from sentences, defined as se-quences of tokens (words, numbers, or symbols).2.1 Task DefinitionTerminology regarding biomedical events, trig-gers, etc.
varies from one task or data set to an-other; in the following, we use the definitions usedby the Genia (GE) task 1 of the BioNLP chal-lenges.
An event is constituted of two kinds ofelements: an event trigger and one or several ar-guments.
The event trigger is an entity, that is,a sequence of consecutive tokens which indicatesthat an event is mentioned in the text.
The argu-ments of an event are participants, which can beproteins, genes or other biomedical events.In the data settings of the GE task, proteinmentions are already annotated in the text.
Fig-ure 1 illustrates biomedical event extraction inthe GE task framework: given 3 proteins ?Tax?,?CBP?
and ?p300?, one must detect ?recruit?as an event trigger for two events of the Bind-ing category , encoded by the formulas: (?re-Class Type Principal arg Optional argGene_expression theme (P)S Transcription theme (P)V Protein_catabolism theme (P)T Phosphorylation theme (P)Localization theme (P)BI Binding theme (P) theme_2 (P)NR Regulation theme (P/E) cause (P/E)E Positive_regulation theme (P/E) cause (P/E)G Negative_regulation theme (P/E) cause (P/E)Table 1: Classes and types of events with theirarguments (P stands for Protein, E for Event).cruit?, theme:?Tax?, theme_2:?CBP?)
and (?re-cruit?, theme:?Tax?, theme_2:?p300?
).A key part of the task is to detect the trigger en-tities among the candidates sequences of tokens.The BioNLP GE task considers 9 types of events.1Table 1 lists these events.
The 9 event types maybe merged into three broader categories: the first5 (termed SVT) have a single theme argument; theBinding event (or BIN) can accept up to two themearguments; the last 3 types (termed REG) also ac-cept up to two arguments, a theme and an optionalcause.
REG events can be recursive because theirarguments can be proteins or events.2.2 Related WorksCurrent approaches fall into two main cate-gories: pipeline incremental models and globaljoint methods.
Pipeline approaches (S?tre et al.,2009; Cohen et al., 2009; Quirk et al., 2011)are the simplest way to tackle the problem ofevent extraction.
A sequence of specific classi-fiers are ran on the text to successively (P1) de-tect event triggers, (P2) assign them arguments,(P3) detect event triggers whose arguments canbe events, and (P4) assign them arguments (steps(P3) and (P4) can be ran multiple times).
Suchsystems are relatively easy to set up and experi-enced many successes: the TEES system (Bj?rneet al., 2009; Bj[Pleaseinsertintopreamble]rne etal., 2012; Bj?rne and Salakoski, 2013) won theBioNLP GE task in 2009 and ranked 2ndin 2013,whereas the EVEX system won in 2013 (Van Lan-deghem et al., 2011; Hakala et al., 2013).
How-ever, all these methods suffer from error cascad-ing.
Besides, prediction must be formalized as1The BioNLP 2013 challenge considered 13 types ofevents, but we only dealt with the 9 types defined in the pre-vious challenges, because there was not enough data on thenewly defined types for proper training or model selection.693a multi-label classification problem because somewords can participate in the definition of severalevents of different types.
Detecting triggers in iso-lation of their arguments in steps (P1) and (P3) areill-posed intermediate problems, since the notionof trigger is intrinsically tied to its argument.
Thelatter brings contextual information that is indis-putably relevant for detection.
Besides, rich fea-tures coding for (trigger, argument) pairs (Miwa etal., 2010) are only used by pipeline models for as-signing arguments, whereas they could be usefulfor trigger detection as well.Global joint approaches (Riedel et al., 2009;McClosky et al., 2011) aim at solving the eventextraction task at once, so as to resolve thedrawbacks of pipeline models.
In (McClosky etal., 2011), event annotations are converted intopseudo-syntactic representations and the task issolved as a syntactic extraction problem by tradi-tional parsing methods.
In (Riedel et al., 2009;Riedel and McCallum, 2011a; Riedel et al., 2011;Riedel and McCallum, 2011b), some models areproposed based on the maximization of a globalscore taking into account the annotations of nodesand edges in a graph representing each sentence.This maximization problem is formalized as an in-teger linear program with consistency constraints,and solved via dual decomposition.
Such jointmodels perform very well (winner of the BioNLP2011 GE task), but suffer from consequential com-puting costs, as all possible combinations of wordsare considered as potential events.
In the follow-ing, we show that our model is able to reach betteraccuracies than joint models while being compu-tationally much cheaper.A method based on the search-based structuredprediction paradigm (Vlachos and Craven, 2012)has been proposed as an intermediate step betweenjoint and pipeline approaches, by turning the struc-tured prediction problem into a sequence of multi-class classification tasks.
Our experiments demon-strate that, despite being conceptually simpler, ourrecursive pairwise model outperforms it.3 Recursive Pairwise ModelIn this section, we present our recursive pair-wise model.
It directly extracts pairwise inter-actions between entities, thereby contrasting withthe usual pipeline approaches, which require de-tecting triggers as an intermediate problem.
Ourapproach proceeds in two steps:1.
Main (trigger, theme) pair extraction:main event extraction step that detects thetriggers with one of their arguments;2.
Post-processing: step that adds extra-arguments to BIN and REG events.Step 1 is the main innovative part of our sys-tem, and is detailed in the remainder of this sec-tion.
Step 2, which relies on more establishedtechniques, is described in Section 4.3.1 Direct Extraction of Simple EventsWe process entities differently depending onwhether they are marked as proteins in the anno-tation or not; the latter are termed candidate en-tities.
We denote CS= {ci}ithe set of candi-date entities, which is built from the sentence to-kens (see Section 5 for details on its construction),AS= {aj}jthe set of candidate arguments (thatis, the proteins identified by a named-entity recog-nizer beforehand) in a given sentence S, and theset of event types (augmented by None) is Y .The first steps of a pipeline model consist in pre-dicting whether candidate entities ci?
CSare trig-gers or not and then, whether arguments aj?
AScan participate to a subset of events from Y .
In-stead, our pairwise model directly addresses theproblem of classifying the (candidate, argument)pairs pij= (ci, aj) as events of type from Y .This classification is based on Support VectorMachines (SVMs), where the multi-class problemis broken down in a series of one-vs-rest binaryproblems, one for each event type.
The final de-cision associated to each pair pijis simply takenas the event (including None) whose score is max-imal.
Classifying a pair pijas not-None jointlydetects the event trigger ciand its argument aj.3.2 Recursive Extraction of Complex EventsFor simple SVT and BIN events, the set ASofpossible arguments is restricted to proteins, but theevents of class REG may have other events as ar-guments, thus AShas to be enriched.
Consider-ing all possible events would be intractable, so thatthe set of possible arguments is updated dynami-cally in the process of extracting events.
As herepossible arguments are exclusive to event types, inpractice it is simpler to update the set of pairs thatremain to be assessed.Assume that an event has been actually pre-dicted, that is, that p?
?= (c?, a?)
has been clas-694Algorithm 1 Recursive pairwise event extractioninput sentence S, candidate entities CS= {ci}iand labeled proteins AS= {aj}j1: initialize candidate pairsPS= {(ci, aj), ci?
CS, aj?
AS}2: initialize extracted events ES= ?3: score and label the pairs in PS4: while PS6= ?
do5: select the pair p???
PSwith highest score6: update PS?
PS?
{p??
}7: if y??
?6= None then8: create event e??
?= (c?, a?, y???
)9: update ES?
ES?
{e???
}10: update PS?
PS?
{(ci, e???)|ci?
CS}11: censor pairs PSto avoid cycles12: score and label the new {(ci, e???)}
pairs13: end if14: end while15: return extracted events ESsified as y??
?6= None; the predicted event is de-noted e??
?= (c?, a?, y???).
We create all pairswith it as argument, {(ci, e???)|ci?
CS}, and addthem to PS, so as to allow for the detection of re-cursive events.
We assume that recursive eventsconstitute a directed acyclic graph, where the an-cestor of a candidate entity cannot be used as itsargument.
The dynamic updating process is thusconstrained to prevent the creation of cycles.Algorithm 1 summaries our event extractionalgorithm.
For all events with a single argu-ment, predicting y?
variables directly responds tothe event extraction problem.
When appropriate,additional optional arguments are added after allpairwise events have been extracted, by the post-processing described in Section 4.3.3 Fitting the Pairwise ModelThe prediction process described above relies ona multi-class classifier.
We stress again that, sincepairs are assigned to a single class, there is no needto address the more difficult multi-label problemencountered in standard pipeline approaches.
Anentity may still be assigned to several events, pos-sibly of different types, through the allocation oflabels to several pairs comprising this entity.Training SVMs For each event type, a seriesof binary linear SVMs is fitted to the avail-able training data, using the implementation fromscikit-learn.org.
As events are rare, eachbinary classification problem is highly unbal-anced.
We thus use different losses for posi-tive and negative examples (Morik et al., 1999;Veropoulos et al., 1999), resulting in two hyper-parameters that are set by cross-validation, so as tomaximize the F-score of the corresponding eventtype taken in isolation.For the SVT and BIN events, the training setsare all composed of the possible (candidate, argu-ment) pairs PS= {pij= (ci, aj)|ci?
CS, aj?AS} readily extracted from all training sentences,and they only differ in the definition of the posi-tive and negative class, according to the true labelassociated to each pair.Creating the training sets for REG events ismore complicated: since they can take events asarguments, new pairs are added to PSby consid-ering all the events already detected, as sketchedin Algorithm 1.
Hence, the sets of training exam-ples are not deterministically known before train-ing, but depend on predictions of all other clas-sifiers.
Training directly on them requires to useeither online algorithms or complex search-basedstructured prediction procedures as in (Vlachosand Craven, 2012).
In this paper, we prefer touse instead the true labels y?
?during the trainingphase of REG and None classifiers: the trainingsets are then the enriched sets of possible (candi-date, argument) pairs PS= {pij= (ci, aj)|ci?CS, aj?
AS} ?
{pi?= (ci, e??)|ci?
CS, ??
:y?
?6= None}.
This allows to know all train-ing examples beforehand and hence to use stan-dard batch SVM algorithms.
The drawback isthat, since extracted events in test are imperfect,this creates a divergence between training and test-ing scenarios, which can lead to degraded perfor-mance.
However, as our experiments show, thiseffect is marginal compared to the advantages ofusing fast reliable batch training algorithms.Score Combination As said earlier, the decisionrule simply consists in predicting the class corre-sponding to the highest SVM score.
This simplescheme could be improved, either by using multi-class classifiers or by using more refined combi-nations optimizing a global criterion as proposedin (Liu et al., 2013).
Though this route deservesto be thoroughly tested, we conjecture that onlymarginal gains should be expected since the vastmajority of errors are due to the detection of anevent when there is none or to the absence of de-tection of an existing event: when an event is de-695tected, its correct type is predominantly predicted.3.4 Computational ConsiderationsThe pairwise structure leads to a simple inferenceprocedure, with a slight increase in computationalcomplexity compared to pipeline models.
We de-note m = card(CS), the number of candidate enti-ties, n = card(AS), the number of annotated pro-teins and m?the number of detected triggers.
Thecomplexity of a pipeline model isO(m?(n+m?
)),whereas that of our approach is O(m(n + m?
)).This implies more calls to the classifying mech-anism, but this is not too penalizing, since SVM-based classification scales well with the number ofexamples.
Besides, this is still cheaper than jointmodels such as (Riedel and McCallum, 2011a),whose complexity is O(m(n2+m)).4 Post-ProcessingThis section describes the post-processing car-ried out once the (trigger, theme) pairs are de-tected and labeled as events.
The goal is to lookwhether extra-arguments should be added to theseextracted events.4.1 Binding Theme FusionThis step attempts to merge several pairs la-beled as Binding to create multiple argumentsevents.
We take the set of extracted Binding events{(c?, a?)}
that share the same trigger c?, and allcombinations {(c?, a?, a?)|?
6= ?}
are classifiedby a binary SVM.
Once a combination (c?, a?, a?
)is predicted as a correct merge, it is added to pre-dicted events while both pairs (c?, a?)
and (c?, a?
)are removed.4.2 Regulation Cause AssignmentThis step looks for optional cause arguments thatmay be added to the extracted REG events.
Givenan extracted event (c?, a?)
and a candidate argu-ment set AS= {a?}
containing all the proteins ofthe sentence S as well as all events extracted bythe classifier, all combinations {(c?, a?, a?)|?
6=?}
are classified by a binary SVM.
Since causeargument could be another event, we extract themincrementally in a dynamic process alike (trigger,theme) pair extraction, also with constraints avoid-ing the creation of cycles.5 FeaturesThis section details our features as well as the datapreprocessing used by our system.Pre-processing Tokenization and sentence split-ting have an important impact on the quality of thedependency parse trees as well as the way we han-dle compound words that contain protein names.Data is split in sentences using both the nltktoolkit (nltk.org) and the sentence splittingprovided for the BioNLP GE task.
High qualitydependency parse trees require a fine grained tok-enization, whereas coarse tokenization conservessome biomedical jargon that could also provideessential information.
Hence, two tokenizationsare used for different features.
Tokenization1, pro-vided by the organizers of the BioNLP GE task,is a coarse tokenization that is used to character-ize when a candidate entity and a protein are inthe same token.
Tokenization2 is fined grained,based on the Stanford parser (McClosky et al.,2011) that is slightly modified for primary tok-enization.
It supplies the dependency parse, can-didate entity match and most of our features.
Thedependency parse trees are finally obtained us-ing a phrase structure parser (McClosky et al.,2010), using the post-processing of the Stanfordcorenlp package (De Marneffe et al., 2006).
Weused stems (obtained by Snowball stemmer pro-vided in nltk) as base forms for the tokens.Candidate set For each sentence S, the set CSis built with a gazetteer: candidate entities arerecursively added by searching first the longesttoken sequences (from Tokenization2) from thegazetteer.
For entities with several tokens, a rep-resentative head token is selected by a heuristicbased on the dependency parse.Candidate entities Three types of tokens areconsidered: the head token, its parent and childnodes in the dependency tree, and the tokens be-longing to a neighboring window of the entity.
Thesize k of the word window is a hyper-parameter ofour model.
Table 2 lists all features which includestems, part-of-speech (POS) tags, etc.
Special carewas taken to design the feature for head tokensince it plays an extremely important role in can-didate entities.
We hence employed features andheuristics to deal with compound-words, hyphensand prefixes, inspired by such tools developed inthe code of the UCLEED system (based on Tok-696Base form (stem) of the head token.Base form of the head token without?-?
or ?/?
before of after.Sub-string after ?-?
in the head token.POS of the head token.First token of the entity is after ?-?
or ?/?.Last token of the entity is before ?-?
or ?/?.Head token has a special prefix: "over","up", "down", "co"Candidate Concat.
of base form and POS of parentsentity of the head token in dependency parse.features Concat.
of base form and POS of childrenof the head token in dependency parse.Base forms of k neighboring tokensaround the entity.POS of k neighboring tokens around theentity.Neighborhood of the entity has ?-?
or ?/?.Sentence has "mRNA".Entity is connected with another stringusing Tokenization1.Argument is a protein.POS of the head token.Features extracted from IntAct when theargument is a protein.Argument Base forms of k neighboring tokensfeatures around the argument.POS of k neighboring tokens around theargument.Concat.
of base form and POS of childrenof the head token in dependency parse.Token sequence between candidate andargument has proteins.V-walk features between candidate andargument with base forms.Pairwise E-walk features between candidate andfeatures argument with base forms.V-walk features between candidate andargument with POS.E-walk features between candidate andargument with POS.Candidate and the argument share a tokenusing Tokenization1.Table 2: Features used by our system.
Most arebased on Tokenization2 except when specified.enization2).2Protein names and POS in tokens aresubstituted by the token PROT, e.g.
transforming"LPS-activated" into "PROT-activated".
There istotal of a 35,365 candidate features.Arguments Table 2 also lists the argument fea-tures, which are a subset of those for candidate en-tities.
Most head word features are removed, butbase forms and POS of the neighboring tokens andof the parent node in the dependency tree are stillincluded.
Assigning label from SVT or BIN eventclasses to a (ci, e??)
pair should never occur, be-cause only regulation events could have anotherevent as argument.
Therefore, we add a featurethat indicates whether the argument is a protein2See github.com/riedelcastro/ucleed.Figure 2: Example of E-walks and V-walksfeatures for encoding a dependency parse tree.or a trigger entity.
Proteins are also described us-ing features extracted from the Uniprot knowledgebase (uniprot.org).
There is total of 4,349 ar-gument features.Pairwise relations Our pairwise approach isable to take advantage of features that code inter-actions between candidate triggers and arguments,such as those listed in Table 2.
Hence, we have afeature indicating if both elements of a pair belongto the same token (based on Tokenization1).But the most important pairwise features comefrom the shortest path linking two candidate andarguments in the dependency parse tree of the sen-tence.
Incorporating such dependency informationinto the pairwise model relies on the process en-coding the path into feature vectors.
Many for-matting methods have been proposed in previousworks.
Following (Miwa et al., 2010), our sys-tem use a combination of E-walks, that encode thepath into triplets (dep-tag, token, dep-tag), and V-walks that encode it into triplets (token, dep-tag,token), where tokens are encoded via stem andPOS tags, and dep-tags are the dependency labels.Figure 2 illustrates this formatting: from the de-pendency parse given on top, three V-walk and twoE-walk features are defined.
These are insertedin the feature vector using a bag-of-words pro-cess, thus losing any relative ordering information.These imperfect representations lose a lot of infor-mation and can even add noise, especially whenthe path is long.
Therefore, we applied heuris-tics from the UCLEED system to remove someuninformative edges from the dependency parse.Moreover, dependency parse features are addedonly for pairs for which the (candidate, argument)path length is below a threshold whose value is ahyper-parameter.
There is a total of 176,106 pair-wise features.697Event Type TEES 2.1 EVEX Pipeline Our approachor Class counterpartGene_expression 82.7 82.7 83.9 85.1Transcription 55.0 55.0 61.7 62.8Protein_catabol 56.3 56.3 66.7 68.8Phosphorylation 72.6 71.5 81.8 81.8Localization 63.3 60.7 56.9 57.7SVT TOTAL 74.9 74.5 79.0 79.6BIN TOTAL 43.3 42.9 41.6 42.4Regulation 23.0 23.4 23.1 31.8Positive_regul 38.7 39.2 36.5 46.3Negative_regul 43.7 43.9 38.1 43.6REG TOTAL 38.1 38.4 35.1 43.2ALL TOTAL 50.7 51.0 50.8 54.4Table 3: F-scores on the test set of the BioNLP2013 GE task.6 ExperimentsIn this section, we evaluate empirically our sys-tem in the framework (data, annotations and eval-uation) of biomedical event extraction defined inthe GE tasks of the BioNLP challenges.
More pre-cisely, we present results on the test sets of thefresh 2013 GE task, and of the 2011 edition tocompare to joint methods.In order to assess the efficiency of our mod-eling choices, we also implemented a pipelinecounterpart system, following the structure ofthe TEES approach (Bj?rne et al., 2009;Bj[Pleaseinsertintopreamble]rne et al., 2012;Bj?rne and Salakoski, 2013) but using the samefeature set, pre-processing and a similar post-processing as our system.
This pipeline systemcomprises four steps: (1) trigger classification,which assigns event types from Y to candidateentities ci?
CSusing a multi-class SVM classi-fier; (2) edge detection, which identifies the edgesbetween extracted triggers and proteins and be-tween REG triggers and all the triggers; labelsfrom Yedge= {theme, cause,None} are as-signed to those pairs; (3) binding theme fusion,identical to as in Section 4.1; (4) theme-cause fu-sion, as in Section 4.2, given two predicted pairs(ci, theme : a?
), (ci, cause : a?
), this step de-cides whether they should be merged into a singleevent (ci, theme : a?, cause : a?
).6.1 Genia Shared Task 2013For the BioNLP 2013 GE task, the hyper-parameters of our system have been optimized onthe GE task development set (except for the regu-larization parameters of the SVMs, which are se-lected by cross-validation), after training on thecorresponding training sets: token window size is2 for candidate entities and 1 for arguments, thethreshold for dependency path is 4.
Using thesehyper-parameter values, the final model submit-ted for test evaluation on the GE task server hasbeen trained on all documents from training anddevelopment sets of BioNLP 2011 and 2013 GEtasks.
Detailed descriptions of the BioNLP 2011and 2013 GE data are respectively given in (Kimet al., 2011b) and (Kim et al., 2013).Table 3 lists the detailed test F-scores, asreturned by the official challenge test server(using the default approximate span & recur-sive matching evaluation setting).
We com-pare our model to the winner of the challenge,EVEX (Hakala et al., 2013), and of the bestrunner-up, TEES 2.1 (Bj?rne and Salakoski,2013), which are both pipeline approaches.Our approach is slightly below TEES 2.1 onBIN events, but overall, it outperforms all com-petitors significantly (by more than 3%), witha wide margin on REG events.
Our pipelinecounterpart has an overall performance similar toEVEX and TEES 2.1, while being better for SVTand worse for BIN and REG events.
These dis-parities are due to the differences in features andin processing details.
The benefits of the pairwisestructure and the recursive process are demon-strated by the considerable improvement upon thepipeline counterpart (using the same features, pre-and post-processing).
In particular, the recursiveprediction process run on REG events brings abouta very substantial improvement (more than 8%).6.2 Genia Shared Task 2011The best performing methods on the BioNLP2013 GE task were pipeline approaches, but thejoint models that were performing better in theprevious challenge were not competing in 2013.As these joint models are quite tricky to train,we compare our system with joint models onthe BioNLP 2011 GE task, where trustworthyperformances have been publicly released.
Wetrain our model using the training and develop-ment sets available at the time of the challengeand we then get an evaluation on the same testdata using the official test server maintained on-line by BioNLP organizers.
Table 4 lists the re-sults of our approach, its pipeline counterpart, andthose of UCLEED (Riedel and McCallum, 2011a)and TEES (Bj[Pleaseinsertintopreamble]rne et al.,698Event UCLEED SEARN TEES Pipeline Our approachClass counterpartSVT 73.5 71.8 n/a 71.8 74.0BIN 48.8 45.8 n/a 40.0 50.5REG 43.8 43.0 n/a 35.7 45.1ALL 55.2 53.5 53.3 50.0 55.6Table 4: F-scores on the test set of the BioNLP2011 GE task.2012), which are respectively the best performingjoint model and best pipeline on this task.
Wealso added SEARN (Vlachos and Craven, 2012),which is a hybrid between them.3As for 2013 data, our system achieves a higherF-score on all event classes compared to itspipeline counterpart.
The benefits of the pair-wise structure and the recursive process are largerhere, thereby outperforming the overall F-score ofTEES (no detailed results available), which itselfperforms better than our pipeline counterpart.
Sys-tematic improvements on all event classes are alsoobserved compared to the joint model UCLEEDand to the search-based structured prediction ap-proach of SEARN.
To our knowledge, our modelthus reaches the best overall performance reportedso far on this data set for a single model.4By combining the use of the simple pair struc-ture between triggers and arguments with a recur-sive prediction process, our approach is able tooutperform pipeline models and to be at least atpar with models relying on much more sophisti-cated structures.
For this task, it is thus highlybeneficial to consider pairwise interactions frombeginning to end, but more complex dependen-cies seem not to be essential, especially since theycome at a higher computational cost.6.3 Training DurationsIn this last section, we propose to illustrate thelower complexity of our approach compared toUCLEED by providing durations for training bothsystems on BioNLP 2011 GE.
These timings donot involve preprocessing but only running cross-validation on the training set and evaluation onthe development and test sets.
For UCLEED,3The results for UCLEED, TEES and SEARN mod-els are reproduced from (Riedel and McCallum, 2011a;Bj[Pleaseinsertintopreamble]rne et al., 2012; Vlachos andCraven, 2012) respectively.4We do not compare with the results of FAUST (Riedel etal., 2011), which achieved the best F-score on this task (56.0)because this is an ensemble of various models of UCLEEDand of the Stanford system (McClosky et al., 2011), whichmakes it an unfair comparison.we used the code (in java & scala) provided bythe authors5and we chose BioNLP 2011 GE be-cause this code was primarily designed to runon it.
Our code, in python, is publicly availablefrom github.com/XiaoLiuAI/RUPEE.
Ex-periments were conducted on the same computer,with a quad-core Intel Xeon CPU and 16GB ofRAM.
Both codes are multi-threaded and usedall 4 threads simultaneously.
Under these condi-tions, UCLEED requires around 8h30min to runits 10 epochs,6while our code completes trainingin about 30min.
Some of these differences may bedue to implementation choices, but we believe thatthe 15 fold speed increase (for around 800 trainingdocuments) is at least partially due to the lowercomplexity of our approach.7 ConclusionWe introduced a recursive pairwise model de-signed for biomedical event extraction.
This pair-wise model improves on the best current ap-proaches of the BioNLP 2013 and 2011 GE tasks.Our system breaks down the overall event extrac-tion task into the classification of (trigger, theme)pairs, assigned to event types.
These (trigger,theme) pairs enable to use joint features in off-the-shelf classifiers, without resorting to costly globalinference models.
We also implemented a recur-sive procedure that deals with regulation events,which may include other events in their definition.All operations are run in a unified framework, us-ing a single event classifier.Our system is fast and more accurate than theavailable pipeline models or joint models.
Givenits simplicity and scalability, we believe that ourmodel is a strong basis for large-scale event extrac-tion projects.
Several refinements are possible, forexample by exploring other types features, or byenabling the direct processing of triplets that maybe encountered in binding or regulation events.AcknowledgmentsThis work was carried out in the framework ofthe Labex MS2T funded by the French NationalAgency for Research through the program ?Invest-ments for the future?
(ANR-11-IDEX-0004-02),and supported by the ?young researchers?
pro-gram (EVEREST-12-JS02-005-01).5See github.com/riedelcastro/ucleed .6UCLEED might be faster by using feature caching, butwe had to disable it because it was taking up too much RAM.699ReferencesJ.
Bj?rne and T. Salakoski.
2013.
TEES 2.1: Auto-mated annotation scheme learning in the BioNLP2013 shared task.
In Proceedings of BioNLP SharedTask 2013 Workshop, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.J.
Bj?rne, J. Heimonen, F. Ginter, A. Airola,T.
Pahikkala, and T. Salakoski.
2009.
Extract-ing complex biological events with rich graph-based feature sets.
In Proceedings of the BioNLP2009 Workshop Companion Volume for Shared Task,pages 10?18, Boulder, Colorado, June.
Associationfor Computational Linguistics.J.
Bj?rne, F. Ginter, and T. Salakoski.
2012.
Universityof turku in the bionlp?11 shared task.
BMC Bioinfor-matics, 13(Suppl 11):S4.K.
B. Cohen, K. Verspoor, H. Johnson, C. Roeder,P.
Ogren, W. Baumgartner, E. White, and L. Hunter.2009.
High-precision biological event extractionwith a concept recognizer.
In Proceedings of theBioNLP 2009 Workshop Companion Volume forShared Task, pages 50?58, Boulder, Colorado, June.Association for Computational Linguistics.M.-C. De Marneffe, B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedings ofLREC, volume 6, pages 449?454.K.
Hakala, S. Van Landeghem, T. Salakoski, Y.
Van dePeer, and F. Ginter.
2013.
EVEX in ST?13: Appli-cation of a large-scale text mining resource to eventextraction and network construction.
In Proceedingsof BioNLP Shared Task 2013 Workshop, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.J.-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-jii.
2009.
Overview of BioNLP?09 shared taskon event extraction.
In Proceedings of the BioNLP2009 Workshop Companion Volume for Shared Task,pages 1?9, Boulder, Colorado, June.
Association forComputational Linguistics.J.-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.2011a.
Extracting bio-molecular events from litera-ture.
Computational Intelligence, 27(4):513?540.J.-D. Kim, Y. Wang, T. Takagi, and A. Yonezawa.2011b.
Overview of genia event task in bionlpshared task 2011.
In Proceedings of BioNLP SharedTask 2011 Workshop, pages 7?15, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.J.-D. Kim, Y. Wang, and Y. Yasunori.
2013.
Thegenia event extraction shared task, 2013 edition -overview.
In Proceedings of the BioNLP SharedTask 2013 Workshop, pages 8?15, Sofia, Bulgaria,August.
Association for Computational Linguistics.X.
Liu, A. Bordes, and Y. Grandvalet.
2013.
Biomed-ical event extraction by multi-class classification ofpairs of text entities.
In Proceedings of the BioNLPShared Task 2013 Workshop, pages 45?49, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.D.
McClosky, E. Charniak, and M. Johnson.
2010.
Au-tomatic domain adaptation for parsing.
In HumanLanguage Technologies: The 2010 Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics, HLT ?10,pages 28?36, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.D.
McClosky, M. Surdeanu, and C. D. Manning.
2011.Event extraction as dependency parsing.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages1626?1635, Stroudsburg, PA, USA.
Association forComputational Linguistics.M.
Miwa, R. S?tre, J.-D. Kim, and J. Tsujii.
2010.Event extraction with complex event classificationusing rich features.
J. Bioinformatics and Computa-tional Biology, 8(1):131?146.K.
Morik, P. Brockhausen, and T. Joachims.
1999.Combining statistical learning with a knowledge-based approach - a case study in intensive care moni-toring.
In Proceedings of the Sixteenth InternationalConference on Machine Learning (ICML 1999).C.
Quirk, P. Choudhury, M. Gamon, and L. Vander-wende.
2011.
MSR-NLP entry in BioNLP sharedtask 2011.
In Proceedings of BioNLP Shared Task2011 Workshop, pages 155?163, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.S.
Riedel and A. McCallum.
2011a.
Fast and robustjoint models for biomedical event extraction.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 1?12, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.S.
Riedel and A. McCallum.
2011b.
Robust biomed-ical event extraction with dual decomposition andminimal domain adaptation.
In Proceedings ofBioNLP Shared Task 2011 Workshop, pages 46?50,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.S.
Riedel, H.-W. Chun, T. Takagi, and J. Tsujii.
2009.A Markov logic approach to bio-molecular event ex-traction.
In Proceedings of the BioNLP 2009 Work-shop Companion Volume for Shared Task, pages 41?49, Boulder, Colorado, June.
Association for Com-putational Linguistics.S.
Riedel, D. McClosky, M. Surdeanu, A. McCallum,and C. D. Manning.
2011.
Model combination forevent extraction in BioNLP 2011.
In Proceedings700of BioNLP Shared Task 2011 Workshop, pages 51?55, Portland, Oregon, USA, June.
Association forComputational Linguistics.R.
S?tre, M. Miwa, K. Yoshida, and J. Tsujii.
2009.From protein-protein interaction to molecular eventextraction.
In Proceedings of the BioNLP 2009Workshop Companion Volume for Shared Task,pages 103?106, Boulder, Colorado, June.
Associa-tion for Computational Linguistics.S.
Van Landeghem, F. Ginter, Y.
Van de Peer, andT.
Salakoski.
2011.
Evex: A pubmed-scale resourcefor homology-based generalization of text miningpredictions.
In Proceedings of BioNLP 2011 Work-shop, pages 28?37, Portland, Oregon, USA, June.Association for Computational Linguistics.K.
Veropoulos, C. Campbell, and N. Cristianini.
1999.Controlling the sensitivity of support vector ma-chines.
In T. Dean, editor, Proceedings of the Inter-national Joint Conference on Artificial Intelligence,pages 55?60.A.
Vlachos and M. Craven.
2012.
Biomedicalevent extraction from abstracts and full papers usingsearch-based structured prediction.
BMC bioinfor-matics, 13(Suppl 11):S5.701
