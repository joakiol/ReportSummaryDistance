Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 169?174,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSyntactic Annotations for the Google Books Ngram CorpusYuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,Jon Orwant, Will Brockman and Slav Petrov?Google Inc.{yurilin,jbmichel,drerez,orwant,brockman,slav}@google.comAbstractWe present a new edition of the Google BooksNgram Corpus, which describes how oftenwords and phrases were used over a periodof five centuries, in eight languages; it reflects6% of all books ever published.
This new edi-tion introduces syntactic annotations: wordsare tagged with their part-of-speech, and head-modifier relationships are recorded.
The an-notations are produced automatically with sta-tistical models that are specifically adapted tohistorical text.
The corpus will facilitate thestudy of linguistic trends, especially those re-lated to the evolution of syntax.1 IntroductionThe Google Books Ngram Corpus (Michel et al,2011) has enabled the quantitative analysis of lin-guistic and cultural trends as reflected in millionsof books written over the past five centuries.
Thecorpus consists of words and phrases (i.e., ngrams)and their usage frequency over time.
The data isavailable for download, and can also be viewedthrough the interactive Google Books Ngram Viewerat http://books.google.com/ngrams.The sheer quantity of and broad historical scopeof the data has enabled a wide range of analyses(Michel et al, 2011; Ravallion, 2011).
Of course,examining raw ngram frequencies is of limited util-ity when studying many aspects of linguistic change,particularly the ones related to syntax.
For instance,most English verbs are regular (their past tense isformed by adding -ed), and the few exceptions,known as irregular verbs, tend to regularize over the?
Corresponding author.1800 1850 1900 1950 2000Relative Frequencyburntburnt_VERBburnt_ADJburnedburned_VERBburned_ADJFigure 1: Usage frequencies of burned and burnt overtime, showing that burned became the dominant spellingaround 1880.
Our new syntactic annotations enable amore refined analysis, suggesting that the crossing-pointfor the verb usage (burned VERB vs. burnt VERB) wasdecades earlier.centuries (Lieberman et al, 2007).
Figure 1 illus-trates how burned gradually overtook burnt, becom-ing more frequent around 1880.
Unfortunately, as astudy of verb regularization, this analysis is skewedby a significant confound: both words can serveas either verbs (e.g., the house burnt) or adjectives(e.g., the burnt toast).
Because many words havemultiple syntactic interpretations, such confoundsoften limit the utility of raw ngram frequency data.In this work we provide a new edition of theGoogle Books Ngram Corpus that contains over 8million books, or 6% of all books ever published (cf.Section 3).
Moreover, we include syntactic anal-ysis in order to facilitate a fine-grained analysis ofthe evolution of syntax.
Ngrams are annotated withpart-of-speech tags (e.g., in the phrase he burnt thetoast, burnt is a verb; in the burnt toast, burnt is anadjective) and head-modifier dependencies (e.g., inthe phrase the little black book, little modifies book).The annotated ngrams are far more useful for ex-169amining the evolution of grammar and syntax.
Forour study of the regularization of the verb burn,the availability of syntactic annotations resolves theverb vs. adjective ambiguity in the original data, al-lowing us to only examine instances where burntand burned appear as verbs.
This more refined anal-ysis suggests a crossover date for the frequency ofthe verb forms that is several decades earlier thanthe overall (verbs and adjectives) crossover.We use state-of-the-art statistical part-of-speechtaggers and dependency parsers to produce syntac-tic annotations for eight languages in the GoogleBooks collection.
The annotations consist of 12 lan-guage universal part-of-speech tags and unlabeledhead-modifier dependencies.
Section 4 describes themodels that we used and the format of the annota-tions in detail.
We assess the expected annotationaccuracies experimentally and discuss how we adaptthe taggers and parsers to historical text in Section 5.The annotated ngrams are available as a new editionof the Google Books Ngram Corpus; we providesome examples from the new corpus in Figure 3.2 Related WorkMichel et al (2011) described the construction ofthe first edition of the Google Books Ngram Corpusand used it to quantitatively analyze a variety of top-ics ranging from language growth to public health.The related Ngram Viewer has become a populartool for examining language trends by experts andnon-experts alike.In addition to studying frequency patterns in thedata, researchers have also attempted to analyze thegrammatical function of the ngrams (Davies, 2011).Such endeavors are hampered by the fact that theNgram Corpus provides only aggregate statistics inthe form of ngram counts and not the full sen-tences.
Furthermore, only ngrams that pass certainoccurrence thresholds are publicly available, makingany further aggregation attempt futile: in heavy taildistributions like the ones common in natural lan-guages, the counts of rare events (that do not passthe frequency threshold) can have a large cumula-tive mass.In contrast, because we have access to the fulltext, we can annotate ngrams to reflect the particu-lar grammatical functions they take in the sentencesLanguage #Volumes #TokensEnglish 4,541,627 468,491,999,592Spanish 854,649 83,967,471,303French 792,118 102,174,681,393German 657,991 64,784,628,286Russian 591,310 67,137,666,353Italian 305,763 40,288,810,817Chinese 302,652 26,859,461,025Hebrew 70,636 8,172,543,728Table 1: Number of volumes and tokens for each lan-guage in our corpus.
The total collection contains morethan 6% of all books ever published.they were extracted from, and can also account forthe contribution of rare ngrams to otherwise frequentgrammatical functions.3 Ngram CorpusThe Google Books Ngram Corpus has been avail-able at http://books.google.com/ngramssince 2010.
This work presents new corpora thathave been extracted from an even larger book collec-tion, adds a new language (Italian), and introducessyntactically annotated ngrams.
The new corporaare available in addition to the already existing ones.3.1 Books DataThe new edition of the Ngram Corpus supports theeight languages shown in Table 1.
The book vol-umes were selected from the larger collection of allbooks digitized at Google following exactly the pro-cedure described in Michel et al (2011).
The newedition contains data from 8,116,746 books, or over6% of all books ever published.
The English cor-pus alone comprises close to half a trillion words.This collection of books is much larger than anyother digitized collection; its generation required asubstantial effort involving obtaining and manuallyscanning millions of books.3.2 Raw NgramsWe extract ngrams in a similar way to the first edi-tion of the corpus (Michel et al, 2011), but withsome notable differences.
Previously, tokenizationwas done on whitespace characters and all ngramsoccurring on a given page were extracted, includ-ing ones that span sentence boundaries, but omitting170Tag English Spanish French German Russian1 Italian Chinese HebrewADJ other, such mayor, gran tous, me?me anderen, ersten vse,to stesso, grande ?,?
!??
?, !???
?ADP of, in de, en de, a` in, von v, na di, in ?,?
!
?, !
?ADV not, when no, ma?s ne, plus auch, so tak, bolee non, piu?
?,?
!?
?, !?
?CONJ and, or y, que et, que und, da?
i, qto che, ed ?,?
!
?, !?
?DET the, a la, el la, les der, die - la, il ?,?
!
?NOUN time, people parte, an?os temps, partie Zeit, Jahre ego, on parte, tempo ?,?
!??
?, !????
?PRON it, I que, se qui, il sich, die - che, si ?,?
!??
?, !?
?VERB is, was es, ha est, sont ist, werden bylo, byl e?, sono ?,?
!N?
?, !??
?Table 2: The two most common words for some POS tags in the new Google Books NGram Corpus for all languages.ngrams that span page boundaries.Instead, we perform tokenization and sentenceboundary detection by applying a set of manuallydevised rules (except for Chinese, where a statisticalsystem is used for segmentation).
We capture sen-tences that span across page boundaries, and thenextract ngrams only within sentences.
As is typicallydone in language model estimation, we add sentencebeginning ( START ) and end tokens ( END ) thatare included in the ngram extraction.
This allows usto distinguish ngrams that appear in sentence-medialpositions from ngrams that occur at sentence bound-aries (e.g., START John).3.3 Differences to the First EditionThe differences between this edition and the firstedition of the Ngram Corpus are as follows: (i) theunderlying book collection has grown substantiallyin the meantime; (ii) OCR technology and metadataextraction have improved, resulting in higher qual-ity digitalization; (iii) ngrams spanning sentenceboundaries are omitted, and ngrams spanning pageboundaries are included.
As a result, this new edi-tion is not a superset of the first edition.4 Syntactic AnnotationsIn addition to extracting raw ngrams, we part-of-speech tag and parse the entire corpus and extractsyntactically annotated ngrams (see Figure 2).
Weuse manually annotated treebanks of modern text(often newswire) as training data for the POS tag-ger and parser models.
We discuss our approach toadapting the models to historical text in Section 5.1Pronouns and determiners are not explicitly annotated inthe Russian treebank.
As a result, the most common Russiannouns in the table are pronouns.4.1 Part-of-Speech TaggingPart-of-speech tagging is one of the most funda-mental disambiguation steps in any natural lan-guage processing system.
Over the years, POS tag-ging accuracies have steadily improved, appearingto plateau at an accuracy level that approaches hu-man inter-annotator agreement (Manning, 2011).
Aswe demonstrate in the next section, these numbersare misleading since they are computed on test datathat is very close to the training domain.
We there-fore need to specifically adapt our models to handlenoisy and historical text.We perform POS tagging with a state-of-the-art2Conditional Random Field (CRF) based tagger (Laf-ferty et al, 2001) trained on manually annotatedtreebank data.
We use the following fairly standardfeatures in our tagger: current word, suffixes andprefixes of length 1, 2 and 3; additionally we useword cluster features (Uszkoreit and Brants, 2008)for the current word, and transition features of thecluster of the current and previous word.To provide a language-independent interface, weuse the universal POS tagset described in detail inPetrov et al (2012).
This universal POS tagset de-fines the following twelve POS tags, which existin similar form in most languages: NOUN (nouns),VERB (verbs), ADJ (adjectives), ADV (adverbs),PRON (pronouns), DET (determiners and articles),ADP (prepositions and postpositions), NUM (nu-merals), CONJ (conjunctions), PRT (particles), ?.?
(punctuation marks) and X (a catch-all for other cat-egories such as abbreviations or foreign words).Table 2 shows the two most common words for2On a standard benchmark (training on sections 1-18 of thePenn Treebank (Marcus et al, 1993) and testing on sections 22-24) our tagger achieves a state-of-the-art accuracy of 97.22%.171John has blackshort hair_START_ _END_NOUN VERB ADJADJ NOUN_ROOT_JohnJohn has...Raw Ngramsshortshort black hair..._START_ Johnhair _END_...John_NOUNJohn has_VERBJohn _VERB_ shortAnnotated Ngramshair=>shorthair=>black..._NOUN_<=hashair=>short_ADJ_ROOT_=>hasFigure 2: An English sentence and its part-of-speech tags and dependency parse tree.
Below are some of the rawngrams available in the first release of the Ngram Corpus, as well as some of the new, syntactically annotated ngrams.some POS tag categories.
It is interesting to see thatthere is overlap between the most frequent contentwords across language boundaries.
In general, func-tion words are more frequent than content words,resulting in somewhat less interesting examples forsome POS tags.
More typical examples might be bigfor adjectives, quickly for adverbs or read for verbs.As suggested in Petrov et al (2012), we train onthe language-specific treebank POS tags, and thenmap the predicted tags to the universal tags.
Table 3shows POS tagging accuracies on the treebank eval-uation sets using the 12 universal POS tags.4.2 Syntactic ParsingWe use a dependency syntax representation, sinceit is intuitive to work with and can be predicted ef-fectively.
Additionally, dependency parse tree cor-pora exist for several languages, making the repre-sentation desirable from a practical standpoint.
De-pendency parse trees specify pairwise relationshipsbetween words in the same sentence.
Directed arcsspecify which words modify a given word (if any),or alternatively, which head word governs a givenword (there can only be one).
For example, in Fig-ure 2, hair is the head of the modifier short.We use a deterministic transition-based depen-dency parsing model (Nivre, 2008) with an arc-eagertransition strategy.
A linear kernel SVM with thefollowing features is used for prediction: the part-of-speech tags of the first four words on the bufferand of the top two words on the stack; the wordidentities of the first two words on the buffer andof the top word on the stack; the word identity ofthe syntactic head of the top word on the stack (ifavailable).
All non-lexical feature conjunctions areincluded.
For treebanks with non-projective trees weuse the pseudo-projective parsing technique to trans-form the treebank into projective structures (Nivreand Nilsson, 2005).
To standardize and simplify thedependency relations across languages we use unla-beled directed dependency arcs.
Table 3 shows un-labeled attachment scores on the treebank evaluationsets with automatically predicted POS tags.4.3 Syntactic NgramsAs described above, we extract raw ngrams (n ?
5)from the book text.
Additionally, we provide ngramsannotated with POS tags and dependency relations.The syntactic ngrams comprise words (e.g.,burnt), POS-annotated words (e.g.
burnt VERB),and POS tags (e.g., VERB ).
All of these formscan be mixed freely in 1-, 2- and 3-grams (e.g.,the ADJ toast NOUN).
To limit the combinatorialexplosion, we restrict the forms that can be mixedin 4- and 5-grams.
Words and POS tags cab bemixed freely (e.g., the house is ADJ ) and we alsoallow every word to be annotated (e.g., the DEThouse NOUN is VERB red ADJ).
However, we donot allow annotated words to be mixed with otherforms (e.g., both the house NOUN is ADJ andthe house NOUN is red are not allowed).
Head-modifier dependencies between pairs of words canbe expressed similarly (we do not record chains ofdependencies).
Both the head and the modifier cantake any of the forms described above.
We use anarrow that points from the head word to the modifierword (e.g., head=>modifier or modifier<=head) toindicate a dependency relation.
We use the desig-nated ROOT for the root of the parse tree (e.g.,ROOT =>has).172Language POS Tags DependenciesEnglish 97.9 90.1Spanish 96.9 74.5German 98.8 83.1French 97.3 84.7Italian 95.6 80.0Russian 96.8 86.2Chinese 92.6 73.2Hebrew 91.3 76.2Table 3: Part-of-speech and unlabeled dependency arcprediction accuracies on in-domain data.
Accuracies onthe out-of-domain book data are likely lower.Figure 2 shows an English sentence, its POS tagsand dependency parse tree, and some concrete ex-amples of ngrams that are extracted.
Note the flex-ibility and additional possibilities that the depen-dency relations provide.
Using the raw ngrams itis not possible to accurately estimate how frequentlyhair is described as short, as there are often interven-ing words between the head and the modifier.
Be-cause dependency relations are independent of wordorder, we are able to calculate the frequency of bothhair=>black and hair=>short.Similarly, there are many ways to express thatsomebody is reading a book.
The first plot inFigure 3 shows multiple related queries.
The 3-gram read DET book aggregates several more spe-cific 3-grams like read a book, read the book, etc.The dependency representation read=>book is evenmore general, enforcing the requirement that the twowords obey a specific syntactic configuration, but ig-noring the number of words that appear in between.5 Domain AdaptationThe results on the treebank evaluation sets need tobe taken with caution, since performance often suf-fers when generalized to other domains.
To geta better estimate of the POS tagging and parsingaccuracies we conducted a detailed study for En-glish.
We chose English since it is the largest lan-guage in our corpus and because labeled treebankdata for multiple domains is available.
In addition tothe WSJ (newswire) treebank (Marcus et al, 1993),we use: the Brown corpus (Francis and Kucera,1979), which provides a balanced sample of textfrom the early 1960s; the QuestionBank (Judge etPOS Tags DependenciesDomain base adapted base adaptedNewswire 97.9 97.9 90.1 90.1Brown 96.8 97.5 84.7 87.1Questions 94.2 97.5 85.3 91.2Historical 91.6 93.3 - -Table 4: English tagging and parsing accuracies on vari-ous domains for baseline and adapted models.al., 2006), which consists entirely of questions; andthe PPCMBE corpus (Kroch et al, 2010), whichcontains modern British English from 1700 to 1914and is perhaps most close to our application domain.Since the English treebanks are in constituencyformat, we used the StanfordConverter (de Marn-effe et al, 2006) to convert the parse trees to de-pendencies and ignored the arc labels.
The depen-dency conversion was unfortunately not possible forthe PPCMBE corpus since it uses a different set ofconstituency labels.
The tagset of PPCMBE is alsounique and cannot be mapped deterministically tothe universal tagset.
For example the string ?one?has its own POS tag in PPCMBE, but is ambigu-ous in general ?
it can be used either as a number(NUM), noun (NOUN) or pronoun (PRON).
We didour best to convert the tags as closely as possible,leaving tags that cannot be mapped untouched.
Con-sequently, our evaluation results underestimate theaccuracy of our tagger since it might correctly dis-ambiguate certain words that are not disambiguatedin the PPCMBE evaluation data.Table 4 shows the accuracies on the different do-mains for our baseline and adapted models.
Thebaseline model is trained only on newswire text andhence performs best on the newswire evaluation set.Our final model is adapted in two ways.
First, weadd the the Brown corpus and QuestionBank to thetraining data.
Second, and more importantly, we es-timate word cluster features on the books data anduse them as features in the POS tagger.The word cluster features group words determin-istically into clusters that have similar distributionalproperties.
When the model encounters a word thatwas never seen during training, the clusters allow themodel to relate it to other, potentially known words.This approach improves the accuracy on rare words,and also makes our models robust to scanning er-1731800 1850 1900 1950 2000Relative Frequencyread=>bookread _DET_ bookread the bookread a bookread this book1800 1850 1900 1950 2000Relative Frequencytackle_NOUNtackle_VERBfootball1800 1850 1900 1950 2000Relative Frequency_NOUN__PRON__NUM_Figure 3: Several queries expressing that somebody is reading a book (left).
Frequencies of tackle used as noun vs.verb compared to the frequency of football (middle).
Relative frequencies of all nouns, pronouns and numbers (right).rors.
For example, in older books the medial-s (?
)is often incorrectly recognized as an ?f?
by the OCRsoftware (e.g., ?beft?
instead of ?best?).
Such sys-tematic scanning errors will produce spurious wordsthat have very similar co-occurrence patterns as thecorrect spelling of the word.
In fact, a manual exam-ination reveals that words with systematic scanningerrors tend to be in the same cluster as their correctlyspelled versions.
The cluster feature thus provides astrong signal for determining the correct POS tag.While the final annotations are by no means per-fect, we expect that in aggregate they are accurateenough to be useful when analyzing broad trends inthe evolution of grammar.6 ConclusionsWe described a new edition of the GoogleBooks Ngram Corpus that provides syntacti-cally annotated ngrams for eight languages.The data is available for download and view-able through an interactive web application athttp://books.google.com/ngrams.
Wediscussed the statistical models used to produce thesyntactic annotations and how they were adapted tohandle historical text more robustly, resulting in sig-nificantly improved annotation quality.
Analyzingthe resulting data is beyond the scope of this paper,but we show some example plots in Figure 3.ReferencesM.
Davies.
2011.
Google Books (American En-glish) Corpus (155 billion words, 1810-2009).
Inhttp://googlebooks.byu.edu/.M.-C. de Marneffe, B. MacCartney, and C. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC.W.
N. Francis and H. Kucera.
1979.
Manual of infor-mation to accompany a standard corpus of present-dayedited American English.
Technical report, BrownUniversity.J.
Judge, A. Cahill, and J. van Genabith.
2006.
Question-bank: Creating a corpus of parse-annotated questions.In Proc.
of ACL.A.
Kroch, B. Santorini, and A. Diertani.
2010.
Pennparsed corpus of modern british english.
Technical re-port, LDC.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.E.
Lieberman, J.-B.
Michel, J. Jackson, T. Tang, andM.
A. Nowak.
2007.
Quantifying the evolutionarydynamics of language.
Nature.C.
Manning.
2011.
Part-of-speech tagging from 97%to 100%: is it time for some linguistics?
Proc.
ofCICLing.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:the Penn treebank.
Computational Linguistics, 19.J.-B.
Michel, Y. K. Shen, A. P. Aiden, A. Veres,M.
K. Gray, The Google Books Team, J. P. Pickett,D.
Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker,M.
A. Nowak, and E. Lieberman Aiden.
2011.
Quan-titative analysis of culture using millions of digitizedbooks.
Science.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective depen-dency parsing.
In Proc.
of ACL.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4):513?553.S.
Petrov, D. Das, and R. McDonald.
2012.
A universalpart-of-speech tagset.
In Proc.
of LREC.M.
Ravallion.
2011.
The two poverty enlightenments:Historical insights from digitized books spanning threecenturies.
Poverty And Public Policy.J.
Uszkoreit and T. Brants.
2008.
Distributed word clus-tering for large scale class-based language modeling inmachine translation.
In Proc.
of ACL.174
