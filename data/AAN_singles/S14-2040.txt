Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 247?251,Dublin, Ireland, August 23-24, 2014.Duluth : Measuring Cross?Level Semantic Similaritywith First and Second?Order Dictionary OverlapsTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812tpederse@d.umn.eduAbstractThis paper describes the Duluth systemsthat participated in the Cross?Level Se-mantic Similarity task of SemEval?2014.These three systems were all unsupervisedand relied on a dictionary melded togetherfrom various sources, and used first?order(Lesk) and second?order (Vector) over-laps to measure similarity.
The first?orderoverlaps fared well according to Spear-man?s correlation (top 5) but less so rela-tive to Pearson?s.
Most systems performedat comparable levels for both Spearman?sand Pearson?s measure, which suggeststhe Duluth approach is potentially uniqueamong the participating systems.1 IntroductionCross?Level Semantic Similarity (CLSS) is anovel variation on the problem of semantic simi-larity.
As traditionally formulated, pairs of words,pairs of phrases, or pairs of sentences are scoredfor similarity.
However, the CLSS shared task(Jurgens et al., 2014) included 4 subtasks wherepairs of different granularity were measured forsemantic similarity.
These included : word-2-sense (w2s), phrase-2-word (p2w), sentence-2-phrase (s2p), and paragraph-2-sentence (g2s).
Inaddition to different levels of granularity, thesepairs included slang, jargon and other examples ofnon?standard English.We were drawn to this task because of our long?standing interest in semantic similarity.
We havepursued approaches ranging from those that relyon structured knowledge sources like WordNet(e.g., WordNet::Similarity) (Pedersen et al., 2004)to those that use distributional information foundThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/in raw text (e.g., SenseClusters) (Purandare andPedersen, 2004).
Our approach in this shared taskis a bit of both, but relies on using definitions foreach item in a pair so that similarity can be mea-sured using first or second?order overlaps.A first?order approach finds direct matches be-tween the words in a pair of definitions.
In asecond?order approach each word in a definitionis replaced by a vector of the words it co?occurswith, and then the vectors for all the words in adefinition are averaged together to represent thedefinition.
Then, similarity can be measured byfinding the cosine between pairs of these vectors.We decided on a definition based approach sinceit had the potential to normalize the differences ingranularity of the pairs.The main difficulty in comparing definitions isthat they can be very brief or may not even ex-ist at all.
This is why we combined various dif-ferent kinds of resources to arrive at our dictio-nary.
While we achieved near total coverage ofwords and senses, phrases were sparsely covered,and sentences and paragraphs had no coverage.
Inthose cases we used the text of the phrase, sentenceor paragraph to serve as its own definition.The Duluth systems were implemented usingthe UMLS::Similarity package (McInnes et al.,2009) (version 1.35)1, which includes support foruser?defined dictionaries, first?order Lesk meth-ods, and second?order Vector methods.
As a resultthe Duluth systems required minimal implementa-tion, so once a dictionary was ready experimentscould begin immediately.This paper is organized as follows.
First, thefirst?order Lesk and second?order Vector mea-sures are described.
Then we discuss the detailsof the three Duluth systems that participated inthis task.
Finally, we review the task results andconsider future directions for this problem and oursystem.1http://umls-similarity.sourceforge.net2472 MeasuresThe Duluth systems use first?order Lesk meth-ods (Duluth1 and Duluth3) and second?order Vec-tor methods (Duluth2).
These require that defini-tions be available for both items in a pair, with thecaveat that we use the term definition somewhatloosely to mean both traditional dictionary defini-tions as well as various proxies when those are notavailable.2.1 First?order Overlaps : LeskThe Lesk measure (Lesk, 1986) was originally amethod of word sense disambiguation that mea-sured the overlap among the definitions of thepossible senses of an ambiguous word with thoseof surrounding words (Lesk, 1986).
The senseswhich have the largest number of overlaps are pre-sumed to be the correct or intended senses for thegiven context.
A modified approach compares theglosses of an ambiguous word with the surround-ing context (Kilgarriff and Rosenzweig, 2000).These are both first?order methods where defini-tions are directly compared with each other, orwith the surrounding context.In the Duluth systems, we measure overlaps bysumming the number of words shared betweendefinitions.
Sequences of words that match areweighted more heavily and contribute the squareof their length, while individual matching wordsjust count as one.
For example, given the defini-tions a small noisy collie and a small noisy bor-der collie the stop word a would not be matched,and then small noisy would match (and be givena score of 4) and then collie would also match(receiving a score of 1).
So, the total Lesk scorewould be 5.
The scores of the Duluth systems werenormalized by dividing by the maximum Leskscore for any pair in a subtask.
This moves thescores to a 0?1 scale, where 1.00 means the def-initions are exactly the same, and where 0 meansthey share no words.One of the main drawbacks of the original Leskmethod is that glosses tend to be very short.
Vari-ous methods have been proposed to overcome this.For example, (Banerjee and Pedersen, 2003) intro-duced the Extended Gloss Overlap measure whichcreates super?glosses by augmenting the glossesof the senses to be measured with the glosses ofsemantically related senses (which are connectedvia relation links in WordNet).
This adaptationof the Lesk measure was first implemented inWordNet::Similarity (Pedersen et al., 2004) andthen later in UMLS::Similarity (McInnes et al.,2009).
It has been applied to both word sensedisambiguation and semantic similarity, and gen-erally found to improve on original Lesk (Baner-jee, 2002; Banerjee and Pedersen, 2002; Patward-han et al., 2003; McInnes and Pedersen, 2013).However, the Duluth systems do not build super?glosses in this way since many of the items in thepairs are not found in WordNet.
However, def-initions are expanded in a simpler way, by merg-ing together various different resources to increaseboth coverage and the length of definitions.2.2 Second?order Overlaps : VectorThe main limitation of first?order Lesk ap-proaches is that if terminology differs from onedefinition to another, then meaningful matchesmay not be found.
For example, consider the def-initions a small noisy collie and a dog that barksa lot.
A first?order overlap approach would findno similarity (other than the stop word a) betweenthese definitions.In cases like this some form of term expansioncould improve the chances of matching.
Synonymexpansion is a well?known possibility, although inthe Duluth systems we opted to expand words withtheir co?occurrence vectors.
This follows from anapproach to word sense discrimination developedby (Schu?tze, 1998).
Once words are expandedthen all the vectors in a definition are averaged to-gether and this averaged vector becomes the rep-resentation of the definition.
This idea was firstimplemented in WordNet::Similarity (Pedersen etal., 2004) and then later in UMLS::Similarity(McInnes et al., 2009), and has been applied toword sense disambiguation and semantic similar-ity (Patwardhan, 2003; Patwardhan and Pedersen,2006; Liu et al., 2012).The co?occurrences for the words in the defi-nitions can come from any corpus of text.
Oncea co?occurrence matrix is constructed, then eachword in each definition is replaced by its vectorfrom that matrix.
If no such vector is found theword is removed from the definition.
Then, all thevectors representing a definition are averaged to-gether, and this vector is used to measure againstother vectors created in the same way.
The scoresreturned by the Vector measure are between 0 and1 (inclusive) where 1.00 means exactly the sameand 0 means no similarity.2483 Duluth SystemsThere were three Duluth systems.
Duluth1 andDuluth3 use first?order Lesk, and Duluth2 usessecond?order Vector.
Duluth3 was an ensemblemade up of Duluth1 and a close variant of it (Du-luth1a, where the only difference was the stop listemployed).Duluth1 and Duluth2 use the NSP stoplist2which includes approximately 390 words andcomes from the SMART stoplist.
Duluth1a treatedany word with 4 or fewer characters as a stopword.
Stemming was performed by all Duluth sys-tems using the Porter algorithm as implemented inthe Lingua::Stem::en Perl module.Before processing, all of the similarity pairs andthe dictionary entries were converted to lower caseand any non alpha-numeric characters were re-placed with spaces.
Also, any stop listed wordswere removed.3.1 Dictionary CreationThe key step for all the Duluth systems is thecreation of the dictionary.
We elected to treatsenses as word forms, and so our dictionary didnot make sense distinctions (and would include allthe senses of a word or phrase in its entry).Since the words and phrases used in some pairsare slang or non?standard English, traditional lex-ical resources like WordNet do not provide ad-equate coverage.
However, WordNet providesa good foundation for coverage of standard En-glish, so we began by extracting the glosses fromWordNet v3.0 using the WordNet::QueryData Perlmodule.Wiktionary is a crowd sourced lexical resourcethat includes more slang and jargon, so we also ex-tracted entries from it using the Wiktionary::ParserPerl module.
In hopes of increasing our coverageof phrases in particular, we looked up words andphrases in Wikipedia using the WWW::WikipediaPerl module and used the first paragraph of an en-try (up to the first heading) as a definition.
Finally,we also used the dict program in Linux whichwe configured to use the following resources :the Collaborative International Dictionary of En-glish v.0.48 (gcide), Moby Thesaurus II by GradyWard, 1.0 (moby-thes), V.E.R.A.
?
Virtual Entityof Relevant Acronyms (June 2006) (vera), the Jar-gon File (version 4.4.7, 29 Dec 2003) (argon), the2http://cpansearch.perl.org/src/TPEDERSE/Text-NSP-1.27/bin/utils/stoplist-nsp.regexFree On-line Dictionary of Computing (26 July2010) (foldoc), and the CIA World Factbook 2002(world02).The most obvious question that arises aboutthese resources is how much coverage they pro-vide for the pairs in the task.
Based on experi-ments on the trial data, we found that none of theresources individually provided satisfactory cov-erage, but if they were all combined then coveragewas reasonably good (although still not complete).In the test data, it turned out there were only 20items in the w2s subtask for which we did not havea dictionary entry (out of 1000).
However, for p2w(phrase-2-word) there were 407 items not includedin the dictionary (most of which were phrases).In the s2p (sentence-2-phrase) subtask there wereonly 15 phrases which had definitions, so for thissubtask and also for g2s (paragraph-2-sentence)the items themselves were the definitions for es-sentially all the pairs.Also of interest might be the total size of thedictionaries created.
The number of tokens ing2s (paragraph-2-sentence) was 46,252, and in s2p(sentence-2-phrase) it was 12,361.
This is simplythe token count for the pairs included in each sub-task.
However, the dictionaries were much largerfor p2w (phrase-2-word), where the token countwas 262,876, and for w2s (word-2-sense) where itwas 499,767.3.2 Co?occurrence Matrix for VectorIn the Duluth systems, the co?occurrence matrixcomes from treating the WordNet glosses as a cor-pus.
Any pair of words that occur together in aWordNet gloss are considered a co?occurrence.There are 117,659 glosses, made up of1,460,921 words.
This resulted in a matrix of90,516 rows and 99,493 columns, representing708,152 unique bigrams.
The matrix is not sym-metric since the co?occurrences are bigrams, sodog house is treated differently than house dog.The WordNet glosses were extracted from ver-sion 3.0 using the glossExtract Perl program3.4 ResultsResults for the CLSS task were ranked bothby Pearson?s and Spearman?s Correlation coeffi-cients.
Duluth system results are shown in Tables1 and 2.
These tables also include the results of3http://www.d.umn.edu/?tpederse/Code/glossExtract-v0.03.tar.gz249Table 1: Spearman?s Resultsrankg2s s2p p2w w2s (of 38)Top .801 .728 .424 .343 1Duluth3 .725 .660 .399 .327 3Duluth1 .726 .658 .385 .316 5Duluth2 .553 .473 .235 .231 21Baseline .613 .626 .162 .128Table 2: Pearson?s Resultsrankg2s s2p p2w w2s (of 38)Top .811 .742 .415 .355 1Duluth2 .501 .450 .241 .224 23Duluth1 .458 .440 .075 .076 30Duluth3 .455 .426 .075 .080 31Baseline .527 .562 .165 .110the top ranked system (which was the same sys-tem according to both measures) and results froma baseline system that measures the Least Com-mon Substring between the terms in a pair, exceptin the w2s subtask, where it measured the LCS be-tween the associated WordNet glosses.Table 1 shows that the Duluth3 system offers aslight improvement upon Duluth1.
Recall that Du-luth3 is an ensemble that includes Duluth1 and itsminor variant Duluth1a.
Both of these are first?order methods, and significantly outperform thesecond?order method Duluth2.However, Table 2 tells a completely differentstory.
There the second?order system Duluth2performs better, although overall rankings sufferaccording to Pearson?s measure.
It is also very ap-parent that the ranks between Pearson?s and Spear-man?s for Duluth1 and Duluth3 differ significantly(from 3 to 30 and 5 to 31).
This is very atypical,and most systems maintained approximately thesame rankings between the two correlation mea-sures.
Note that Duluth2 behaves in this way,where the relative ranking is 21 and 23.Table 3 shows the number of pairs in each sub-task which returned a score of 0.
This could be dueto missing definitions, or no matches occurring be-tween the definitions.
Interestingly Duluth2 has amuch smaller number of 0 valued scores, whichshows the second?order method provides greatercoverage due to its more flexible notion of match-ing.
However, despite much higher numbers ofTable 3: Number of Pairs with Score of 0g2s s2p p2w w2sDuluth1 107 197 211 23Duluth2 9 101 40 15Duluth3 101 196 205 230s, Duluth1 and Duluth3 perform much better withSpearman?s rank correlation coefficient.
This sug-gests that there is a kind of precision?recall trade-off between these systems, where Duluth2 hashigher recall and Duluth1 and Duluth3 have higherprecision.5 Future DirectionsThe relatively good performance of the first?orderDuluth systems (at least with respect to rank cor-relation) shows again the important role of lexicalresources.
Our first?order method was not appre-ciably more complex than the baseline method, yetit performed significantly better (especially for thep2w and w2s tasks).
This is no doubt due to themore extensive dictionary that we employed.That said, our approach to building the dictio-nary was relatively crude, and could be substan-tially improved.
For example, we could be moreselective in the content we add to the entries forwords or phrases.
We could also do more thansimply use the sentences and paragraphs as theirown definitions.
For example, we could replacewords or phrases in sentences and paragraphs withtheir definitions, and then carry out first or second?order matching.Second?order matching did not perform as wellas we had hoped.
We believe this is due to thesomewhat noisy nature of the dictionaries we con-structed, and expanding those definitions by re-placing words with vectors created even morenoise.
We believe that a more refined approachto creating dictionaries would certainly improvethese results, as would a more selective method ofcombining the co?occurrence vectors (rather thansimply averaging them).AcknowledgmentsThe Duluth systems relied heavily on the freelyavailable software package UMLS::Similarity.
Weare grateful to Bridget McInnes and Ying Liu fortheir work in developing this package, and in par-ticular for the --dict functionality.250ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
Anadapted Lesk algorithm for word sense disambigua-tion using WordNet.
In Proceedings of the Third In-ternational Conference on Intelligent Text Process-ing and Computational Linguistics, pages 136?145,Mexico City, February.Satanjeev Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semantic re-latedness.
In Proceedings of the Eighteenth Inter-national Joint Conference on Artificial Intelligence,pages 805?810, Acapulco, August.Satanjeev Banerjee.
2002.
Adapting the Lesk algo-rithm for word sense disambiguation to WordNet.Master?s thesis, University of Minnesota, Duluth,December.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
Semeval-2014 task 3:Cross?level semantic similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval 2014), Dublin, August.Adam Kilgarriff and Joseph Rosenzweig.
2000.
Spe-cial issue on SENSEVAL: Framework and resultsfor english SENSEVAL.
Computers and the Hu-manities, 34(1?2):15?48.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedings ofthe 5th annual international conference on Systemsdocumentation, pages 24?26.
ACM Press.Ying Liu, Bridget McInnes, Ted Pedersen, GenevieveMelton-Meaux, and Serguei Pakhomov.
2012.
Se-mantic relatedness study using second?order co?occurrence vectors computed from biomedical cor-pora, UMLS, and WordNet.
In Proceedings of the2nd ACM SIGHIT International Health InformaticsSymposium, pages 363?371, Miami, FL.Bridget McInnes and Ted Pedersen.
2013.
Evaluatingmeasures of semantic similarity and relatedness todisambiguate terms in biomedical text.
Journal ofBiomedical Informatics, 46:1116?1124.Bridget McInnes, Ted Pedersen, and Serguei Pakho-mov.
2009.
UMLS-Interface and UMLS-Similarity: Open source software for measuring paths andsemantic similarity.
In Proceedings of the AnnualSymposium of the American Medical Informatics As-sociation, pages 431?435, San Francisco.Siddharth Patwardhan and Ted Pedersen.
2006.
Us-ing WordNet-based Context Vectors to Estimate theSemantic Relatedness of Concepts.
In Proceed-ings of the EACL 2006 Workshop on Making Senseof Sense: Bringing Computational Linguistics andPsycholinguistics Together, pages 1?8, Trento, Italy,April.Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2003.
Using measures of semantic re-latedness for word sense disambiguation.
In Pro-ceedings of the Fourth International Conference onIntelligent Text Processing and Computational Lin-guistics, pages 241?257, Mexico City, February.Siddharth Patwardhan.
2003.
Incorporating dictionaryand corpus information into a context vector mea-sure of semantic relatedness.
Master?s thesis, Uni-versity of Minnesota, Duluth, August.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::Similarity - Measuring therelatedness of concepts.
In Proceedings of Fifth An-nual Meeting of the North American Chapter of theAssociation for Computational Linguistics, pages38?41, Boston, MA.Amruta Purandare and Ted Pedersen.
2004.
Wordsense discrimination by clustering contexts in vectorand similarity spaces.
In Proceedings of the Confer-ence on Computational Natural Language Learning,pages 41?48, Boston, MA.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.251
