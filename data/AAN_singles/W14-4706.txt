Zock/Rapp/Huang (eds.
): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 46?49,Dublin, Ireland, August 23, 2014.Using Significant Word Co-occurences for the Lexical Access ProblemRico Feist and Daniel Gerighausen and Manuel Konrad and Georg Richter andDepartment of Computer Science,University of Leipzig,Germanyrf@ricofeist.de, daniel@bioinf.uni-leipzig.demanuel.konrad, georg.richter @studserv.uni-leipzig.deThomas Eckart and Dirk Goldhahn and Uwe QuasthoffNatural Language Processing Group,University of Leipzig,Germanyteckart, dgoldhahn, quasthoff@informatik.uni-leipzig.deAbstractOne way to analyse word relations is to examine their co-occurrence in the same context.
Thisallows for the identification of potential semantic or lexical relationships between words.
Asprevious studies showed word co-occurrences often reflect human stimuli-response pairs.
In thispaper significant sentence co-occurrences on word level were used to identify potential responsesfor word stimuli based on three automatically generated text corpora of the Leipzig CorporaCollection.1 IntroductionConventional dictionaries have very limited possibilities for retrieving information.
By contrast elec-tronic dictionaries offer a much wider and more dynamic range of access strategies.
One important taskin dictionary lookup is to retrieve a word starting just with the corresponding meaning.
For this purposethe flexibility of electronic dictionaries should be advantageous.
In the following the related task of re-trieving a word based just on a stimulus of five related input words is addressed.
Based on the assumptionthat word co-occurrences in the same context can be used to analyse word relations and to identify poten-tial semantic or lexical relationships between words an automatic system is built based on an electronicdictionary extracted from Web corpora.
As previous studies showed word co-occurrences often reflecthuman stimuli-response pairs (Spence, 1990; Schulte im Walde, 2008).
In this paper significant sentenceco-occurrences on word level were used to identify potential responses for word stimuli based on threeautomatically generated text corpora of the Leipzig Corpora Collection (LCC).2 Used Methods and Resources2.1 Used CorporaThe text corpora of the Leipzig Corpora Collection (Biemann, 2007; Goldhahn, 2012) were used asdata basis.
As the origin of the stimuli data was unknown corpora based on different text material wereexploited:?
eng wikipedia 2010: a corpus based on the English Wikipedia generated in 2010 containing 23million sentences?
eng news 2008: 49 million sentences from English newspaper articles collected in 2008?
eng web 2002: 57 million sentences of English Web text crawled in 2002This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/46All of these corpora were generated by the standard preprocessing toolchain of the LCC.
Thistoolchain contains different procedures to ensure corpus quality like language identification and patternbased removal of invalid text material (Goldhahn, 2012).
Furthermore all corpora were annotated withstatistical information about word co-occurrences based on co-occurrence in the same sentence or directneighbourhood.
These word relations were generated by using the log-likelihood ratio (Buechler, 2006)as measure of significance.
Complete sentences were used as co-occurrence window.2.2 Raw Results GenerationFor each of the five stimulus words and every corpus all co-occurrent words were extracted.
For extractedterms that significantly co-occurred with more than one of the stimulus words the significance of co-occurrence were combined.
Based on the sum of the significance values a ranking of the most relevantterms for every stimulus was created for every corpus.
The most significant 15 words were consideredas raw result for every corpus and stimulus 5-tuple.2.3 PostprocessingThe raw results were combined by replacing the result ranks in the three intermediate result lists li(1 <= i <= 3) with a weight (weighti(w) = 16 ?
ranki(w)).
These weights were merged bygenerating the combined weight for all three corpora c weight(w) =3?i=1weighti(w).
The word withthe highest combined weight was chosen as response for a stimulus tupel.The same procedure was used in two variations:?
Rankings were generated based on the combination of all inflected terms of the same word stem (byusing the Porter stemmer(Porter, 1980)).?
Stop words were removed from the result lists to reduce the influence of high frequent functionwords1.For some stimuli only stop words were extracted as response.
Here not only the 15 most significant termswere extracted from every corpus but the 45 most significant terms.
This lead to more useful results inmost cases.2.4 ResultsAll three variants were evaluated on the training data set.
The evaluation lead to the conclusion that astop word filter is a useful preprocessing step, whereas stemming lead to unsatisfactory results (cp.
table1).
As a consequence only the stop word filter (without stemming) was used for the test data set where281 (14.05%) of the responses were correctly predicted.Used Data Correctly Predicted PercentageOriginal corpus data (incl.
stop words, unstemmed) 61 3.05%Removed stop words 262 13.1%Stemmed 43 2.15%Table 1: Evaluation of the different approaches based on the training data set3 ConclusionIt is noteworthy that corpora where solely stop words were removed yielded better results than corporawhere additional stemming took place.
One reason for this observation is most likely that by using thePorter algorithm an overstemming occurred.
Some word pairs were reduced to identical word stems1For this purpose the stop word list of the database management system MySQL was used(https://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html).47Figure 1: Histogramm of the combined weights for the training data setFigure 2: Histogramm of the combined weights for the test data setalthough having no semantic relationship.The final results also contained a disproportionately high number of specific terms.
As an example theword ?god?
was chosen 38 times as response.
An analysis of the corpora showed that the word ?god?was especially frequent in theWeb corpus (330,276 of 56,523,369 sentences (0.59%)) and theWikipedia-based corpus (58,605 of 22,675,331 sentences (0.26%)).
In contrast, the newspaper corpus had only 29occurrences of the term ?god?
(in 48,903,372 sentences (0.00006%)).The evaluation for both the training (figure 1) and the test data set (figure 2) shows that there is a peakfor the combined weight of 15.
This behaviour originates in terms that have the maximum rank in one ofthe three corpora but being no significant co-occurrent term in the other two.4 Further ImprovementsThe evaluation showed that the used corpora generated results of different quality.
This was especiallydemonstrated at the example of the term ?god?.
As a consequence a stricter selection of the corpus ma-terial combined with a weighting of the specific results from each corpus could improve the predictions.48The used corpora reflect a specific selection of input material (in this case written text material fromdifferent sources of the years 2002 to 2010).
A corpus that reflects more of the details of the test data(most notably being significantly older) would very likely enhance the results.
This is especially the caseas words that became prominent over the last decades (like technical terms or words strongly related tomore recent political developments) would not have occurred in the generated results.
A deeper analysisof the input material and the availability of a comparable corpus would have been the prerequisites.An examination of the results also showed that in many cases a synonym of the correct response wasidentified.
Hence the usage of a synonym database could also lead to further improvements.
Furthermoreusing part of speech information could be beneficial for the weighting of intermediate results.
The basicidea is that part of speech of stimulus and response are very likely to be the same.
This would haveeliminated parts of the generated result sets.
Furthermore a deeper analysis of the ranking proceduremay have reduced the effect which manifests in many terms having a weight of 15 in the results.ReferencesChris Biemann, Gerhard Heyer, Uwe Quasthoff, and Matthias Richter.
2007.
The Leipzig Corpora Collection -Monolingual corpora of standard size.
Proceedings of Corpus Linguistic 2007, Birmingham, UK.Marco Buechler.
2006.
Flexibles Berechnen von Kookkurrenzen auf strukturierten und unstrukturierten Daten.Diploma Thesis, University of Leipzig.Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.
2012.
Building Large Monolingual Dictionaries at theLeipzig Corpora Collection: From 100 to 200 Languages.
Proceedings of the Eighth International Conferenceon Language Resources and Evaluation (LREC 2012).Martin F. Porter.
1980.
An algorithm for suffix stripping.
electronic library and information systems, 14.3 (1980):130-137.Donald P. Spence, and Kimberley C. Owens.
1990.
Lexical co-occurrence and association strength.
Journal ofPsycholinguistic Research, Volume 19(5):317?330.Sabine Schulte im Walde, and Alissa Melinger.
2008.
An in-depth look into the co-occurrence distribution ofsemantic associates.
Italian Journal of Linguistics, Special Issue on From Context to Meaning: DistributionalModels of the Lexicon in Linguistics and Cognitive Science.49
