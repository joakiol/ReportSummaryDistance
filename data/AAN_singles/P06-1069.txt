Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 545?552,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Comparison and Semi-Quantitative Analysis of Words andCharacter-Bigrams as Features in Chinese Text CategorizationJingyang Li                       Maosong Sun                     Xian ZhangNational Lab.
of Intelligent Technology & Systems, Department of Computer Sci.
& Tech.Tsinghua University, Beijing 100084, Chinalijingyang@gmail.com  sms@tsinghua.edu.cn  kevinn9@gmail.comAbstractWords and character-bigrams are bothused as features in Chinese text process-ing tasks, but no systematic comparisonor analysis of their values as features forChinese text categorization has been re-ported heretofore.
We carry out here afull performance comparison betweenthem by experiments on various docu-ment collections (including a manuallyword-segmented corpus as a golden stan-dard), and a semi-quantitative analysis toelucidate the characteristics of their be-havior; and try to provide some prelimi-nary clue for feature term choice (in mostcases, character-bigrams are better thanwords) and dimensionality setting in textcategorization systems.1 Introduction1Because of the popularity of the Vector SpaceModel (VSM) in text information processing,document indexing (term extraction) acts as apre-requisite step in most text information proc-essing tasks such as Information Retrieval(Baeza-Yates and Ribeiro-Neto, 1999) and TextCategorization (Sebastiani, 2002).
It is empiri-cally known that the indexing scheme is a non-trivial complication to system performance, es-pecially for some Asian languages in which thereare no explicit word margins and even no naturalsemantic unit.
Concretely, in Chinese Text Cate-gorization tasks, the two most important index-1 This research is supported by the National Natural ScienceFoundation of China under grant number 60573187 and60321002, and the Tsinghua-ALVIS Project co-sponsoredby the National Natural Science Foundation of China undergrant number 60520130299 and EU FP6.ing units (feature terms) are word and character-bigram, so the problem is: which kind of terms2should be chosen as the feature terms, words orcharacter-bigrams?To obtain an all-sided idea about featurechoice beforehand,  we review here the possiblefeature variants (or, options).
First, at the wordlevel, we can do stemming, do stop-word prun-ing, include POS (Part of Speech) information,etc.
Second, term combinations (such as ?word-bigram?, ?word + word-bigram?, ?character-bigram + character-trigram?3, etc.)
can also beused as features (Nie et al, 2000).
But, for Chi-nese Text Categorization, the ?word or bigram?question is fundamental.
They have quite differ-ent characteristics (e.g.
bigrams overlap eachother in text, but words do not) and influence theclassification performance in different ways.In Information Retrieval, it is reported that bi-gram indexing schemes outperforms wordschemes to some or little extent (Luk and Kwok,1997; Leong and Zhou 1998; Nie et al, 2000).Few similar comparative studies have been re-ported for Text Categorization (Li et al, 2003) sofar in literature.Text categorization and Information Retrievalare tasks that sometimes share identical aspects(Sebastiani, 2002) apart from term extraction(document indexing), such as tfidf term weight-ing and performance evaluation.
Nevertheless,they are different tasks.
One of the generally ac-cepted connections between Information Re-trieval and Text Categorization is that an infor-mation retrieval task could be partially taken as abinary classification problem with the query asthe only positive training document.
From this2 The terminology ?term?
stands for both word and charac-ter-bigram.
Term or  combination of terms (in word-bigramor other forms) might be chosen as ?feature?.3 The terminology ?character?
stands for Chinese character,and ?bigram?
stands for character-bigram in this paper.545viewpoint, an IR task and a general TC task havea large difference in granularity.
To better illus-trate this difference, an example is present here.The words ????
(film producer)?
and ????
(dubbed film)?
should be taken as differentterms in an IR task because a document with onewould not necessarily be a good match for aquery with the other, so the bigram ???(filmproduction)?
is semantically not a shared part ofthese two words, i.e.
not an appropriate featureterm.
But in a Text Categorization task, bothwords might have a similar meaning at the cate-gory level (?film?
category, generally), whichenables us to regard the bigram ????
as a se-mantically acceptable representative word snip-pet for them, or for the category.There are also differences in some other as-pects of IR and TC.
So it is significant to make adetailed comparison and analysis here on therelative value of words and bigrams as featuresin Text Categorization.
The organization of thispaper is as follows: Section 2 shows some ex-periments on different document collections toobserve the common trends in the performancecurves of the word-scheme and bigram-scheme;Section 3 qualitatively analyses these trends;Section 4 makes some statistical analysis to cor-roborate the issues addressed in Section 3; Sec-tion 5 summarizes the results and concludes.2 Performance ComparisonThree document collections in Chinese languageare used in this study.The electronic version of Chinese Encyclo-pedia (?CE?
): It has 55 subject categories and71674 single-labeled documents (entries).
It israndomly split by a proportion of 9:1 into a train-ing set with 64533 documents and a test set with7141 documents.
Every document has the full-text.
This data collection does not have much ofa sparseness problem.The training data from a national Chinesetext categorization evaluation4 (?CTC?
): It has36 subject categories and 3600 single-labeled5documents.
It is randomly split by a proportionof 4:1 into a training set with 2800 documentsand a test set with 720 documents.
Documents inthis data collection are from various sources in-cluding news websites, and some documents4 The Annual Evaluation of  Chinese Text Categorization2004, by 863 National Natural Science Foundation.5 In the original document collection, a document mighthave a secondary category label.
In this study, only the pri-mary category label is reserved.may be very short.
This data collection has amoderate sparseness problem.A manually word-segmented corpus fromthe State Language Affairs Commission(?LC?
): It has more than 100 categories andmore than 20000 single-labeled documents6.
Inthis study, we choose a subset of 12 categorieswith the most documents (totally 2022 docu-ments).
It is randomly split by a proportion of 2:1into a training set and a test set.
Every documenthas the full-text and has been entirely word-segmented7 by hand (which could be regarded asa golden standard of segmentation).All experiments in this study are carried out atvarious feature space dimensionalities to showthe scalability.
Classifiers used in this study areRocchio and SVM.
All experiments here aremulti-class tasks and each document is assigneda single category label.The outline of this section is as follows: Sub-section 2.1 shows experiments based on the Roc-chio classifier, feature selection schemes besidesChi and term weighting schemes besides tfidf tocompare the automatic segmented word featureswith bigram features on CE and CTC, and bothdocument collections lead to similar behaviors;Subsection 2.2 shows experiments on CE by aSVM classifier,  in which, unlike with the Roc-chio method, Chi feature selection scheme andtfidf term weighting scheme outperform otherschemes; Subsection 2.3 shows experiments by aSVM classifier with Chi feature selection andtfidf term weighting on LC (manual word seg-mentation) to compare the best word featureswith bigram features.2.1 The Rocchio Method and Various Set-tingsThe Rocchio method is rooted in the IR tradition,and is very different from machine learning ones(such as SVM) (Joachims, 1997; Sebastiani,2002).
Therefore, we choose it here as one of therepresentative classifiers to be examined.
In theexperiment, the control parameter of negativeexamples is set to 0, so this Rocchio based classi-fier is in fact a centroid-based classifier.Chimax is a state-of-the-art feature selectioncriterion for dimensionality reduction (Yang andPeterson, 1997; Rogati and Yang, 2002).
Chi-max*CIG (Xue and Sun, 2003a) is reported to bebetter in Chinese text categorization by a cen-6 Not completed.7 And POS (part-of-speech) tagged as well.
But POS tagsare not used in this study.546troid based classifier, so we choose it as anotherrepresentative feature selection criterion besidesChimax.Likewise, as for term weighting schemes, inaddition to tfidf, the state of the art (Baeza-Yatesand Ribeiro-Neto, 1999), we also choosetfidf*CIG (Xue and Sun, 2003b).Two word segmentation schemes are used forthe word-indexing of documents.
One is themaximum match algorithm (?mmword?
in thefigures), which is a representative of simple andfast word segmentation algorithms.
The other isICTCLAS8 (?lqword?
in the figures).
ICTCLASis one of the best word segmentation systems(SIGHAN 2003) and reaches a segmentationprecision of more than 97%, so we choose it as arepresentative of state-of-the-art schemes forautomatic word-indexing of document).For evaluation of single-label classifications,F1-measure, precision, recall and accuracy(Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani,2002) have the same value by microaveraging9,and are labeled with ?performance?
in the fol-lowing figures.1 2 3 4 5 6 7 8x 1040.50.60.70.8performancemmwordchi-tfidfchicig-tfidfcig1 2 3 4 5 6 7 8x 1040.50.60.70.8lqwordperformancechi-tfidfchicig-tfidfcig1 2 3 4 5 6 7 8x 1040.50.60.70.8bigramperformancedimensionalitychi-tfidfchicig-tfidfcigFigure 1. chi-tfidf and chicig-tfidfcig on CEFigure 1 shows the performance-dimensionality curves of the chi-tfidf approachand the approach with CIG, by mmword, lqwordand bigram document indexing, on the CEdocument collection.
We can see that the originalchi-tfidf approach is better at low dimensional-ities (less than 10000 dimensions), while the CIGversion is better at high dimensionalities andreaches a higher limit.108 http://www.nlp.org.cn/project/project.php?proj_id=69 Microaveraging is more prefered in most cases thanmacroaveraging (Sebastiani 2002).10 In all figures in this paper, curves might be truncated dueto the large scale of dimensionality, especially the curves of1 2 3 4 5 6 7 8x 1040.50.60.70.8performancemmwordchi-tfidfchicig-tfidfcig1 2 3 4 5 6 7 8x 1040.50.60.70.8lqwordperformancechi-tfidfchicig-tfidfcig1 2 3 4 5 6 7 8x 1040.50.60.70.8bigramperformancedimensionalitychi-tfidfchicig-tfidfcigFigure 2. chi-tfidf and chicig-tfidfcig on CTCFigure 2 shows the same group of curves forthe CTC document collection.
The curves fluctu-ate more than the curves for the CE collectionbecause of sparseness; The CE collection is moresensitive to the additions of terms that come withthe increase of dimensionality.
The CE curves inthe following figures show similar fluctuationsfor the same reason.For a parallel comparison among mmword,lqword and bigram schemes, the curves in  Fig-ure 1 and Figure 2 are regrouped and shown inFigure 3 and Figure 4.2 4 6 8x 1040.50.550.60.650.70.750.80.85performancedimensionalitychi-tfidfmmwordlqwordbigram2 4 6 8x 1040.50.550.60.650.70.750.80.85dimensionalitychicig-tfidfcigmmwordlqwordbigramFigure 3. mmword, lqword and bigram on CE1 2 3 4 5x 1040.50.550.60.650.70.750.80.85performancedimensionalitychi-tfidfmmwordlqwordbigram1 2 3 4 5x 1040.50.550.60.650.70.750.80.85dimensionalitychicig-tfidfcigmmwordlqwordbigramFigure 4. mmword, lqword and bigram on CTCbigram scheme.
For these kinds of figures, at least one ofthe following is satisfied: (a) every curve has shown itszenith; (b) only one curve is not complete and has shown ahigher zenith than other curves; (c) a margin line is shownto indicate the limit of the incomplete curve.547We can see that the lqword scheme outper-forms the mmword scheme at almost any dimen-sionality, which means the more precise the wordsegmentation the better the classification per-formance.
At the same time, the bigram schemeoutperforms both of the word schemes on a highdimensionality, wherea the word schemes mightoutperform the bigram scheme on a low dimen-sionality.Till now, the experiments on CE and CTCshow the same characteristics despite the per-formance fluctuation on CTC caused by sparse-ness.
Hence in the next subsections CE is usedinstead of both of them because its curves aresmoother.2.2 SVM on Words and BigramsAs stated in the previous subsection, the lqwordscheme always outperforms the mmword scheme;we compare here only the lqword scheme withthe bigram scheme.Support Vector Machine (SVM) is one of thebest classifiers at present (Vapnik, 1995;Joachims, 1998), so we choose it as the mainclassifier in this study.
The SVM implementationused here is LIBSVM (Chang, 2001); the type ofSVM is set to ?C-SVC?
and the kernel type is setto linear, which means a one-with-one scheme isused in the multi-class classification.Because the CIG?s effectiveness on a SVMclassifier is not examined in Xue and Sun (2003a,2003b)?s report, we make here the four combina-tions of schemes with and without CIG in featureselection and term weighting.
The experimentresults are shown in Figure 5.
The collectionused is CE.1 2 3 4 5 6 7x 1040.60.650.70.750.80.850.9performancedimensionalitylqwordchi-tfidfchi-tfidfcigchicig-tfidfchicig-tfidfcig1 2 3 4 5 6 7x 1040.60.650.70.750.80.850.9dimensionalitybigramchi-tfidfchi-tfidfcigchicig-tfidfchicig-tfidfcigFigure 5. chi-tfidf and cig-involved approacheson lqword and bigramHere we find that the chi-tfidf combinationoutperforms any approach with CIG, which is theopposite of the results with the Rocchio method.And the results with SVM are all better than theresults with the Rocchio method.
So we find thatthe feature selection scheme and the termweighting scheme are related to the classifier,which is worth noting.
In other words, no featureselection scheme or term weighting scheme isabsolutely the best for all classifiers.
Therefore, areasonable choice is to select the best performingcombination of feature selection scheme, termweighting scheme and classifier, i.e.
chi-tfidf andSVM.
The curves for the lqword scheme and thebigram scheme are redrawn in Figure 6 to makethem clearer.1 2 3 4 5 6 7x 1040.750.80.850.9performancedimensionalitylqwordbigramFigure 6. lqword and bigram on CEThe curves shown in Figure 6 are similar tothose in Figure 3.
The differences are: (a) a lar-ger dimensionality is needed for the bigramscheme to start outperforming the lqword scheme;(b) the two schemes have a smaller performancegap.The lqword scheme reaches its top perform-ance at a dimensionality of around 40000, andthe bigram scheme reaches its top performanceat a dimensionality of around 60000 to 70000,after which both schemes?
performances slowlydecrease.
The reason is that the low ranked termsin feature selection are in fact noise and do nothelp to classification, which is why the featureselection phase is necessary.2.3 Comparing Manually SegmentedWords and Bigrams0 1 2 3 4 5 6 7 8 9 10x 104727476788082848688dimansionalityperformancewordbigrambigram limitFigure 7. word and bigram on LC548Up to now, bigram features seem to be betterthan word ones for fairly large dimensionalities.But it appears that word segmentation precisionimpacts classification performance.
So wechoose here a fully manually segmented docu-ment collection to detect the best performance aword scheme could  reach and compare it withthe bigram scheme.Figure 7 shows such an experiment result onthe LC document collection (the circles indicatethe maximums and the dash-dot lines indicate thesuperior limit and the asymptotic interior limit ofthe bigram scheme).
The word scheme reaches atop performance around the dimensionality of20000, which is a little higher than the bigramscheme?s zenith around 70000.Besides this experiment on 12 categories ofthe LC document collection, some experimentson fewer (2 to 6) categories of this subset werealso done, and showed similar behaviors.
Theword scheme shows a better performance thanthe bigram scheme and needs a much lower di-mensionality.
The simpler the classification taskis, the more distinct this behavior is.3 Qualitative AnalysisTo analyze the performance of words and bi-grams as feature terms in Chinese text categori-zation, we need to investigate two aspects as fol-lows.3.1 An Individual Feature PerspectiveThe word is a natural semantic unit in Chineselanguage and expresses a complete meaning intext.
The bigram is not a natural semantic unitand might not express a complete meaning intext, but there are also reasons for the bigram tobe a good feature term.First, two-character words and three-characterwords account for most of all multi-characterChinese words (Liu and Liang, 1986).
A two-character word can be substituted by the samebigram.
At the granularity of most categorizationtasks, a three-character words can often be sub-stituted by one of its sub-bigrams (namely the?intraword bigram?
in the next section)  withouta change of meaning.
For instance, ????
is asub-bigram of the word ????
(tournament)?and could represent it without ambiguity.Second, a bigram may overlap on two succes-sive words (namely the ?interword bigram?
inthe next section), and thus to some extent fills therole of a word-bigram.
The word-bigram as amore definite (although more sparse) featuresurely helps the classification.
For instance, ????
is a bigram overlapping on the two succes-sive words ?
?
?
(weather)?
and ?
?
?
(forecast)?, and could almost replace the word-bigram (also a phrase) ?????
(weather fore-cast)?, which is more likely to be a representativefeature of the category ????
(meteorology)?than either word.Third, due to the first issue, bigram featureshave some capability of identifying OOV (out-of-vocabulary) words 11 , and help improve therecall of classification.The above issues state the advantages of bi-grams compared with words.
But in the first andsecond issue, the equivalence between bigramand word or word-bigram is not perfect.
For in-stance, the word ???(literature)?
is a also sub-bigram of the word ????
(astronomy)?, buttheir meanings are completely different.
So theloss and distortion of semantic information is adisadvantage of bigram features over word fea-tures.Furthermore, one-character words cover about7% of words and more than 30% of word occur-rences in the Chinese language; they are effev-tive in the word scheme and are not involved inthe above issues.
Note that the impact of effec-tive one-character words on the classification isnot as large as their total frequency, because thehigh frequency ones are often too common tohave a good classification power, for instance,the word ??
(of, ?s)?.3.2 A Mass Feature PerspectiveFeatures are not independently acting in textclassification.
They are assembled together toconstitute a feature space.
Except for a few mod-els such as Latent Semantic Indexing (LSI)(Deerwester et al, 1990), most models assumethe feature space to be orthogonal.
This assump-tion might not affect the effectiveness of themodels, but the semantic redundancy and com-plementation among the feature terms do impacton the classification efficiency at a given dimen-sionality.According to the first issue addressed in theprevious subsection, a bigram might cover formore than one word.
For instance, the bigram????
is a sub-bigram of the words ???(fabric)?,????
(cotton fabric)?, ????
(knitted fabric)?, and also a good substitute of11 The ?OOV words?
in this paper stand for the words thatoccur in the test documents but not in the training document.549them.
So, to a certain extent, word features areredundant with regard to the bigram features as-sociated to them.
Similarly, according to the sec-ond issue addressed, a bigram might cover formore than one word-bigram.
For instance, thebigram ????
is a sub-bigram of the word-bigrams (phrases) ?????
(short story)?, ?????
(novelette)?, ?????(novel)?
and alsoa good substitute for them.
So, as an addition tothe second issue stated in the previous subsection,a bigram feature might even cover for more thanone word-bigram.On the other hand, bigrams features are alsoredundant with regard to word features associ-ated with them.
For instance, the ????
and ????
are both sub-bigrams of the previously men-tioned word ?????.
In some cases, more thanone sub-bigram can be a good representative of aword.We make a word list and a bigram list sortedby the feature selection criterion in a descendingorder.
We now try to find how the relative re-dundancy degrees of the word list and the bigramlist vary with the dimensionality.
Following is-sues are elicited by an observation on the twolists (not shown here due to space limitations).The relative redundancy rate in the word listkeeps even while the dimensionality varies to acertain extent, because words that share a com-mon sub-bigram might not have similar statisticsand thus be scattered in the word feature list.Note that these words are possibly ranked lowerin the list than the sub-bigram because featureselection criteria (such as Chi) often preferhigher frequency terms to lower frequency ones,and every word containing the bigram certainlyhas a lower frequency than the bigram itself.The relative redundancy in the bigram listmight be not as even as in the word list.
Good(representative) sub-bigrams of a word are quitelikely to be ranked close to the word itself.
Forinstance, ????
and ????
are sub-bigrams ofthe word ????
(music composer)?, both thebigrams and the word are on the top of the lists.Theretofore, the bigram list has a relatively largeredundancy rate at low dimensionalities.
Theredundancy rate should decrease along with theincreas of dimensionality for: (a) the relative re-dundancy in the word list counteracts the redun-dancy in the bigram list, because the words thatcontain a same bigram are gradually included asthe dimensionality increases; (b) the proportionof interword bigrams increases in the bigram listand there is generally no redundancy betweeninterword bigrams and intraword bigrams.Last, there are more bigram features than wordfeatures because bigrams can overlap each otherin the text but words can not.
Thus the bigramsas a whole should theoretically contain more in-formation than the words as a whole.From the above analysis and observations, bi-gram features are expected to outperform wordfeatures at high dimensionalities.
And word fea-tures are expected to outperform bigram featuresat low dimensionalities.4 Semi-Quantitative AnalysisIn this section, a preliminary statistical analysisis presented to corroborate the statements in theabove qualitative analysis and expected to beidentical with the experiment results shown inSection 1.
All statistics in this section are basedon the CE document collection and the lqwordsegmentation scheme (because the CE documentcollection is large enough to provide good statis-tical characteristics).4.1 Intraword Bigrams and Interword Bi-gramsIn the previous section, only the intraword bi-grams were discussed together with the words.But every bigram may have both intraword oc-currences and interword occurrences.
Thereforewe need to distinguish these two kinds of bi-grams at a statistical level.
For every bigram, thenumber of intraword occurrences and the numberof interword occurrences are counted and we canuse1log1interword#intraword#+?
??
?+?
?as a metric to indicate its natual propensity to bea intraword bigram.
The probability density ofbigrams about on this metric is shown in Figure8.-12 -10 -8 -6 -4 -2 0 2 4 6 8 1000.050.10.150.20.25log(intraword#/interword#)probabilitydensityFigure 8.
Bigram Probability Density onlog(intraword#/interword#)550The figure shows a mixture of two Gaussiandistributions, the left one for ?natural interwordbigrams?
and the right one for ?natural intrawordbigrams?.
We can moderately distinguish thesetwo kinds of bigrams by a division at -1.4.4.2 Overall Information Quantity of a Fea-ture SpaceThe performance limit of a classification is re-lated to the quantity of information used.
So aquantitative metric of the information a featurespace can provide is need.
Feature Quantity (Ai-zawa, 2000) is suitable for this purpose becauseit comes from information theory and is additive;tfidf was also reported as an appropriate metric offeature quantity (defined as ?probability ?
infor-mation?).
Because of the probability involved asa factor, the overall information provided by afeature space can be calculated on training databy summation.The redundancy and complementation men-tioned in Subsection 3.2 must be taken into ac-count in the calculation of overall informationquantity.
For bigrams, the redundancy with re-gard to words associated with them between twointraword bigrams is given by{ }1,21 2( ) min ( ), ( )b wtf w idf b idf b??
?in which b1 and b2 stand for the two bigrams andw stands for any word containing both of them.The overall information quantity is obtained bysubtracting the redundancy between each pair ofbigrams from the sum of all features?
featurequantity (tfidf).
Redundancy among more thantwo bigrams is ignored.
For words, there is onlycomplementation among words but not redun-dancy, the complementation with regard to bi-grams associated with them is given by{ } if  exists;if  does not exists.
( ) min ( ) ,( ) ( ),b wbbtf w idf btf w idf w???????
?in which b is an intraword bigram contained byw.
The overall information is calculated bysumming the complementations of all words.4.3 Statistics and DiscussionFigure 9 shows the variation of these overall in-formation metrics on the CE document collection.It corroborates the characteristics analyzed inSection 3 and corresponds with the performancecurves in Section 2.Figure 10 shows the proportion of interwordbigrams at different dimensionalities, which alsocorresponds with the analysis in Section 3.0 2 4 6 8 10 12 14 16x 1040246810121416x 107dimensionalityoverallinformationquantitywordbigramFigure 9.
Overall Information Quantity on CEThe curves do not cross at exactly the samedimensionality as in the figures in Section 1, be-cause other complications impact on the classifi-cation performance: (a) OOV word identifyingcapability, as stated in Subsection 3.1; (b) wordsegmentation precision; (c) granularity of thecategories (words have more definite semanticmeaning than bigrams and lead to a better per-formance for small category granularities); (d)noise terms, introduced in the feature space dur-ing the increase of dimensionality.
With thesefactors, the actual curves would not keep increas-ing as they do in Figure 9.0 2 4 6 8 10 12 14 16x 10400.10.20.30.40.50.60.70.80.91dimensionalityinterwordbigramproportionFigure 10.
Interword Bigram Proportion on CE5 ConclusionIn this paper, we aimed to thoroughly comparethe value of words and bigrams as feature termsin text categorization, and make the implicitmechanism explicit.Experimental comparison showed that the Chifeature selection scheme and the tfidf termweighting scheme are still the best choices for(Chinese) text categorization on a SVM classifier.In most cases, the bigram scheme outperformsthe word scheme at high dimensionalities andusually reaches its top performance at a dimen-551sionality of around 70000.
The word scheme of-ten outperforms the bigram scheme at low di-mensionalities and reaches its top performance ata dimensionality of less than 40000.Whether the best performance of the wordscheme is higher than the best performancescheme depends considerably on the word seg-mentation precision and the number of categories.The word scheme performs better with a higherword segmentation precision and fewer (<10)categories.A word scheme costs more document indexingtime than a bigram scheme does; however a bi-gram scheme costs more training time and classi-fication time than a word scheme does at thesame performance level due to its higher dimen-sionality.
Considering that the document index-ing is needed in both the training phase and theclassification phase, a high precision wordscheme is more time consuming as a whole thana bigram scheme.As a concluding suggestion: a word scheme ismore fit for small-scale tasks (with no more than10 categories and no strict classification speedrequirements) and needs a high precision wordsegmentation system; a bigram scheme is morefit for large-scale tasks (with dozens of catego-ries or even more) without too strict trainingspeed requirements (because a high dimensional-ity and a large number of categories lead to along training time).ReferenceAkiko Aizawa.
2000.
The Feature Quantity: An In-formation Theoretic Perspective of Tfidf-likeMeasures, Proceedings of ACM SIGIR 2000, 104-111.Ricardo Baeza-Yates, Berthier Ribeiro-Neto.
1999.Modern Information Retrieval, Addison-WesleyChih-Chung Chang, Chih-Jen Lin.
2001.
LIBSVM: ALibrary for Support Vector Machines, Softwareavailable at http://www.csie.ntu.edu.tw/~cjlin/libsvmSteve Deerwester, Sue T. Dumais, George W. Furnas,Richard Harshman.
1990.
Indexing by Latent Se-mantic Analysis, Journal of the American Societyfor Information Science, 41:391-407.Thorsten Joachims.
1997.
A Probabilistic Analysis ofthe Rocchio Algorithm with TFIDF for Text Cate-gorization, Proceedings of 14th International Con-ference on Machine Learning (Nashville, TN,1997), 143-151.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machine: Learning with ManyRelevant Features, Proceedings of the 10th Euro-pean Conference on Machine Learning, 137-142.Mun-Kew Leong, Hong Zhou.
1998.
PreliminaryQualitative Analysis of Segmented vs. Bigram In-dexing in Chinese, The 6th Text Retrieval Confer-ence (TREC-6), NIST Special Publication 500-240,551-557.Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu.2003.
Experimental Study on Representing Units inChinese Text Categorization, Proceedings of the4th International Conference on  ComputationalLinguistics and Intelligent Text Processing (CI-CLing 2003), 602-614.Yuan Liu, Nanyuan Liang.
1986.
Basic Engineeringfor Chinese Processing ?
Contemporary ChineseWords Frequency Count, Journal of Chinese In-formation Processing, 1(1):17-25.Robert W.P.
Luk, K.L.
Kwok.
1997.
Comparing rep-resentations in Chinese information retrieval.
Pro-ceedings of ACM SIGIR 1997, 34-41.Jianyun Nie, Fuji Ren.
1999.
Chinese InformationRetrieval: Using Characters or Words?
Informa-tion Processing and Management, 35:443-462.Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou.2000.
On the Use of Words and N-grams for Chi-nese Information Retrieval, Proceedings of 5th In-ternational Workshop on Information Retrievalwith Asian LanguagesMonica Rogati, Yiming Yang.
2002.
High-performingFeature Selection for Text Classification, Proceed-ings of ACM Conference on Information andKnowledge Management 2002, 659-661.Gerard Salton, Christopher Buckley.
1988.
TermWeighting Approaches in Automatic Text Retrieval,Information Processing and Management,24(5):513-523.Fabrizio Sebastiani.
2002.
Machine Learning inAutomated Text Categorization, ACM ComputingSurveys, 34(1):1-47Dejun Xue, Maosong Sun.
2003a.
Select Strong In-formation Features to Improve Text CategorizationEffectiveness, Journal of Intelligent Systems, Spe-cial Issue.Dejun Xue, Maosong Sun.
2003b.
A Study on FeatureWeighting in Chinese Text Categorization, Pro-ceedings of the 4th International Conference onComputational Linguistics and Intelligent TextProcessing (CICLing 2003), 594-604.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory, Springer.Yiming Yang, Jan O. Pederson.
1997.
A ComparativeStudy on Feature Selection in Text Categorization,Proceedings of ICML 1997, 412-420.552
