Bayesian Network, a model for NLP?Davy WeissenbacherLaboratoire d?Informatique de Paris-NordUniversite Paris-NordVilletaneuse, FRANCEdavy.weissenbacher@lipn.univ-paris13.frAbstractThe NLP systems often have low perfor-mances because they rely on unreliableand heterogeneous knowledge.
We showon the task of non-anaphoric it identifi-cation how to overcome these handicapswith the Bayesian Network (BN) formal-ism.
The first results are very encourag-ing compared with the state-of-the-art sys-tems.1 IntroductionWhen a pronoun refers to a linguistic expressionpreviously introduced in the text, it is anaphoric.In the sentence Nonexpression of the locus evenwhen it is present suggests that these chromo-somes[...], the pronoun it refers to the referentdesignated as ?the locus?.
When it does not re-fer to any referent, as in the sentence Thus, it isnot unexpected that this versatile cellular... thepronoun is semantically empty or non-anaphoric.Any anaphora resolution system starts by identi-fying the pronoun occurrences and distinguishingthe anaphoric and non-anaphoric occurrences of it.The first systems that tackled this classificationproblem were based either on manually writtenrules or on the automatic learning of relevant sur-face clues.
Whatever strategy is used, these sys-tems see their performances limited by the qualityof knowledge they exploit, which is usually onlypartially reliable and heterogeneous.This article describes a new approach to go be-yond the limits of traditional systems.
This ap-proach stands on the formalism, still little ex-ploited for NLP, of Bayesian Network (BN).
Asa probabilistic formalism, it offers a great expres-sion capacity to integrate heterogeneous knowl-edge in a single representation (Peshkin, 2003)as well as an elegant mechanism to take into ac-count an a priori estimation of their reliability inthe classification decision (Roth, 2002).
In orderto validate our approach we carried out various ex-periments on a corpus made up of abtsracts of ge-nomic articles.Section 2 presents the state of the art for theautomatic recognition of the non-anaphoric oc-curences of it.
Our BN-based approach is exposedin section 3.
The experiments are reported in sec-tion 4, and results are discussed in section 5.2 Identification of Non-anaphoric itoccurencesThe decisions made by NLP systems depend onthe available knowledge.
However this informa-tion is often weakly reliable and leads to erroneousor incomplete results.One of first pronoun classifier system is pre-sented by (Paice, 1987).
It relies on a set of logicalfirst order rules to distinguish the non-anaphoricoccurences of the pronoun it.
Non-anaphoric se-quences share remarkable forms (they start with anit and end with a delimiter like to, that, whether...).The rules expresses some constraints which varyaccording to the delimiter.
They concern the leftcontext of the pronoun (it should not be immedi-ately preceded by certain words like before, fromto), the distance between the pronoun and the de-limiter (it must be shorter than 25 words long), andfinally the lexical items occurring between the pro-noun and the delimiter (the sequence must or mustnot contain certain words belonging to specificsets, such as words expressing modality over thesentence content, e.g.
certain, known, unclear...).Tests performed by Paice show good results with19591.4%Accuracy1 on a technical corpus.
Howeverthe performances are degraded if one applies themto corpora of different natures: the number of falsepositive increases.In order to avoid this pitfall, (Lappin, 1994) pro-poses some more constrained rules in the form offinite state automata.
Based on linguistic knowl-edge the automata recognize specific sequenceslike It is not/may be<Modaladj>; It is <Cogv-ed> that <Subject> where <Modaladj> and<Cogv> are modal adjective and cognitive verbsclasses known to introduce non-anaphoric it (e.g.necessary, possible and recommend, think).
Thissystem has a good precision (few false positivecases), but has a low recall (many false negativecases).
Any sequence with a variation is ignoredby the automata and it is difficult to get exhaustiveadjective and verb semantic classes2.
In the nextparagraphs we refer to Lappin rules?
as HighlyConstraint rules (HC rules) and Paice rules?
asLightly Constraint rules (LC rules).
(Evans, 2001) gives up the constraints broughtinto play by these rules and proposes a machinelearning approach based on surface clues.
Thetraining determines the relative weight of the vari-ous corpus clues.
Evans considers 35 syntactic andcontextual surface clues (e.g.
pronoun position inthe sentence, lemma of the following verb) on amanually annotated sample.
The system classifiesthe new it occurences by the k-nearest neighbormethod metric.
The first tests achieve a satisfac-tory score: 71.31%Acc on a general language cor-pus.
(Clement, 2004) carries out a similar test inthe genomic domain.
He reduces the number ofEvans?s surface clues to the 21 most relevant onesand classifies the new instances with a SupportVector Machine(SVM).
It obtains 92.71%Acc tobe compared with a 90.78%Acc score for the LCrules on the same corpus.
The difficulty, however,comes from the fact that the information on which1Accuracy(Acc) is a classifi cation measure:Acc= P+NP+N+p+n where p is the number of anaphoricpronoun occurences tagged as non-anaphoric, which wecall the false positive cases, n the number of non-anaphoricpronoun ocurrences tagged as anaphoric, the false negativecases.
P and N are the numbers of correctly taggednon-anaphoric and anaphoric pronoun occurences, the truepositive and negative cases respectively.2For instance in the sentences It is well documented thattreatment of serum-grown... and It is generally accepted thatBcl-2 exerts... the it occurences are not classifi ed as non-anaphorics because documented does not belong to the origi-nal verb class <Cogv> and generally does not appear in theprevious automaton.the systems are built is often diverse and hetero-geneous.
This system is based on atomic surfaceclues only and does not make use of the linguisticknowledge or the relational information that theconstraints of the previous systems encode.
We ar-gue that these three types of knowledge that are theHC rules, the LC rules, and the surfaces clues areall relevant and complementary for the task andthat they must be unified in a single representation.3 A Bayesian Network Based SystemContainNo?ContainContain?Known?NounAnaphoric?ItNon?anaphoric?ItPronounStarNo?StartStart?PropositionStartNo?StartStart?SentenceStartNo?StartStart?AbstractNo?matchMatchLeft?Context?ConstraintsContainNo?ContainContain?Known?AdjectiveMatchNo?matchSuperior?eleven...threeInferior?threeContainNo?ContainMoreThreeTwoOneOtherPrepositionObjectSubjectGrammatical?RoleMatchNo?matchToThatWhether?ifWhich?WhoOtherSequence?LengthLCR?AutomataContain?Known?VerbHCR?AutomataUnknown?WordsDelimitorFigure 1: A Bayesian Network for identificationofnon-anaphoric it occurrencesNeither the surface clues nor the surface cluesare reliable indicators of the pronoun status.
Theyencode heterogeneous pieces of information andconsequently produce different false negative andpositive cases.
The HC rules have a good precisionbut tag only few pronouns.
On the opposite, theLC rules, which have a good recall, are not preciseenough to be exploited as such and the additionalsurface clues must be checked.
Our model com-bines these clues and take their respective reliabil-ity in to account.
It obtains better results than thoseobtained from each clue exploited separately.The BN is a model designed to deal with dubi-ous pieces of information.
It is based on a qualita-tive description of their dependancy relationships,a directed acyclic graph, and a set of condition-nal probablities, each node being represented asa Random Variable (RV).
Parametrizing the BNassociates an a priori probability distribution to196the graph.
Exploiting the BN (inference stage)consists in propagating new pieces of informa-tion through the network edges and updating themaccording to observations (a posteriori probabili-ties).We integrated all the clues exploted by of theprevious methods within the same BN.
We use de-pendancy relationships to express the fact that twoclues are combined.
The BN is manually designed(choice of the RV values and graph structure).
Onthe Figure1, the nodes associated with the HCrules method are marked in grey, white is forthe LC rules method and black for the Clement?smethod3.
The Pronoun node estimates the de-cision probability for a given it occurence to benon-anaphoric.The parameterising stage establishes the a pri-ori probability values for all possible RV by sim-ple frequency counts in a training corpus.
Theyexpress the weight of each piece of information inthe decision, its a priori reliability in the classifi-cation decision4 .
The inference stage exploits therelationships for the propagation of the informa-tion and the BN operates by information reinforce-ment to label a pronoun.
We applied all precedentrules and checked surface clues on the sequencecontaining the it occurrence and set observationvalues to the correspondant RV probabilities.
Anew probability is computed for the node?s vari-able Pronoun: if it is superior or equal to 50%the pronoun is labeled non-anaphoric, anaphoricotherwise.Let us consider the sentence extracted from ourcorpus: It had previously been thought that ZE-BRA?s capacity to disrupt EBV latency.... No HCrule recognizes the sequence even by tolerating 3unknown words 5, but a LC rule matches it with4 words between the pronoun and the delimiterthat6.
Among the surface clues, we checked thatthe sequence is at the beginning of the sentence3Only signifi cant surface clues for our modelisation havebeen added to the BN.4Among the 2000 it occurences of our training cor-pus (see next section), the HC rules recognized 649of the 727 non-anaphoric pronouns and they have er-roneously recognized as non-anaphoric 17 pronouns, sowe set the HCR-rules node probabilities as P(HCR-rules=Match|Pronoun=Non-Anaphoric)=89.2% and P(HCR-rules=Match|Pronoun=Anaphoric)=1.3% which expressesthe expected value for the false negative cases and the falsepositive cases produced by the HC rules respectively.5So we set P(HC-rules = No-match)=1 and P(Unknown-Words = More)=1.6We set P(LC-rules = Match)=1, P(Sequence-Length =four)=1 and P(Delimitor = That)=1.Table 1: Prediction Results (Accuracy/False Posi-tive Cases/False Negatives Cases)Method ResultsHighly Constraint Rules 88.11% / 12.8 / 169.1Lightly Constraint Rules 88.88% / 123.6 / 24.2Support Vector Machine 92.71% / - / -Naive Bayesian Classifier 92.58% / 74.1 / 19.5Bayesan Network 95.91% / 21.0 / 38.2(1) but that the sentence is not the first of the ab-stract (2).
The sentence also contains the adverbpreviously (3) and the verb think (4), which wordsbelong to our semantic classes7.
The a prioriprobability for the pronoun to be non-anaphoric is36.2%.
After modifying the probabilities of thenodes of the BN according to the corpus obser-vations, the a posteriori probability computed forthis occurence is 99.9% and the system classifiesit as non-anaphoric.4 Experiments and DiscussionMedline is a database specialized in genomic re-search articles.
We extracted from it 11966 ab-stracts with keywords bacillus subtilis, transcrip-tion factors, Human, blood cells, gene and fu-sion.
Among these abstracts, we isolated 3347occurences of the pronoun it and two human an-notators tagged it occurences as either anaphoricor non-anaphoric8 .
After discussion, the two an-notators achieved a total agreement.We implemented the HC rules, LC rules andsurface clues using finite transducers and extractedthe pronoun syntactic role from the results ofthe Link Parser analysis of the corpus (Aubin,2005).
As a working approximation, we automati-caly generated the verb, adjective and noun classesfrom the training corpus: among all it occurencestagged as non-anaphoric, we selected the verbs,adjectives and nouns occurring between the delim-iter and the pronoun.
We considered a third of thecorpus for training and the remaining for testing.Our experiment was performed using 20-cross val-idation.Table1 summarizes the average results reached7Others node values are set consequently.8Corpus is available at http://www-lipn.univ-paris13.fr/?weissenbacher/197by the state-of-the-art methods described above9.The BN system achieved a better classificationthan other methods.In order to neutralize and comparatively quan-tify the contribution in the decision of the depen-dancy relationships between the factors, we haveimplemented a Naive Bayesian Classifier (NBC)which exploits the same pieces of knowledge andthe same parameters as the BN but it does notprofit from reinforcement mechanism, which leadsto a rise in the number of false positive cases.Our BN, which has a good precision, never-theless tags as non-anaphoric some occurrenceswhich are not.
The most recurrent error corre-sponds to the sequences ending with a delimiterto recognized by some LC rules.
Although noneHC rule matches the sequence, its minimal lengthand the fact that it contains particular adjectivesor verbs like assumed or shown, makes this con-figuration caracteristic enough to tag the pronounas non-anaphoric.
When the delimiter is that, thisclassification is correct 10 but it is always incorrectwhen the delimiter is to11.
For the delimiter to, therules must be more carefully designed.Three different factors explain the false nega-tive cases.
Firstly, some sequences were ignoredbecause the delimiter remained implicit12.
Sec-ondly, the presence of apposition clauses increasesthe sequence length and decreases the confidence.Dedicated algorithms taking advantage of a deepersyntactic analysis could resolve these cases.
Thelast cause is the non-exhaustiveness of the verb,adjective and noun classes.
It should be possibleto enrich them automatically.
In our experimentswe have noticed that if a LC rule matches a se-quence in the first clause of the first sentence in theabstract then the pronoun is non-anaphoric.
Wecould automatically extract from Medline a largenumber of such sentences and extend our classesby selecting the verbs, adjectives and nouns occur-ing between the pronoun and the delimiter in thesesentences.5 ConclusionOur system can of course be enhanced along theprevious axes.
However, it is interesting to note9We have completed the Clement?s SVM score for thesame biological corpus to compare its results with ours.10Like in the sentence It is assumed that the SecY proteinof B. subtilis has multiple roles...11Like in the sentence It is assumed to play a role in ...12For example Thus, it appears T3SO4 has no intrinsic...that it achieves better results than the comparablestate-of-the art systems, although it relies on thesame set of rules and surface clues.
This com-parison confirms the fact that the BN model pro-poses an interesting way to combine the variousclues, some of then being only partially reliable.We are continuing our work and expect to confirmthe contribution of BN to NLP problems on a taskwhich is more complex than the classification of itoccurences: the resolution of anaphora.ReferencesS.
Aubin, A. Nazarenko and C. Nedellec.
2005.Adapting a General Parser to a Sublanguage.
Pro-ceedings of the International Conference on Re-cent Advances in Natural Language Processing(RANLP?05), 1:89?93.L.
Clemente, K. Satou and K. Torisawa.
2004.
Im-proving the Identification of Non-anaphoric It Us-ing Support Vector Machines.
Actes d?InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications, 1:58?61.I.
Dagan and A. Itai.
1990.
Automatic Processingof Large Corpora for the Resolution of AnaphoraReferences.
Proceedings of the 13th InternationalConference on Computational Linguistics (COL-ING?90), 3:1?3.R.
Evans.
2001.
Applying Machine Learning Towardan Automatic Classification of it.
Literary and lin-guistic computing, 16:45?57.S.
Lappin and H.J.
Leass.
1994.
An Algorithm forPronominal Anaphora Resolution.
ComputationalLinguistics, 20(4):535?561.C.D.
Paice and G.D. Husk.
1987.
Towards the Auto-matic Recognition of Anaphoric Features in EnglishText: the Impersonal Pronoun It.
Computer Speechand Language, 2:109?132.L.
Peshkin and A. Pfeffer 2003.
Bayesian InformationExtraction Network.
In Proc.18th Int.
Joint Conf.Artifical Intelligence, 421?426.D.
Roth and Y. Wen-tau.
2002.
Probalistic Reasoningfor Entity and Relation Recognition.
Proceedings ofthe 19th InternationalConference on ComputationalLinguistics (COLING?02), 1:1?7.198
