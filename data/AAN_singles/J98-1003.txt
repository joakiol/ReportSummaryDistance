Topical Clustering of MRD Senses Basedon Information Retrieval TechniquesJen Nan ChewNational Tsing Hua UniversityJason  S. Chang*National Tsing Hua UniversityThis paper describes aheuristic approach capable of automatically clustering senses in a machine-readable dictionary (MRD).
Including these clusters in the MRD-based lexical database offersseveral positive benefits for word sense disambiguation (WSD).
First, the clusters can be usedas a coarser sense division, so unnecessarily fine sense distinction can be avoided.
The clusteredentries in the MRD can also be used as materials for supervised training to develop a WSDsystem.
Furthermore, if the algorithm is run on several MRDs, the clusters also provide a meansof linking different senses across multiple MRDs to create an integrated lexical database.
Animplementation f the method for clustering definition sentences in the Longman Dictionaryof Contemporary English (LDOCE) is described.
To this end, the topical word lists and topicalcross-references in the Longman Lexicon of Contemporary English (LLOCE) are used.
Nearlyhalf of the senses in the LDOCE can be linked precisely to a relevant LLOCE topic using a simpleheuristic.
With the definitions of senses linked to the same topic viewed as a document, opicalclustering of the MRD senses bears a striking resemblance to retrieval of relevant documents fora given query in information retrieval (IR) research.
Relatively well-established IR techniques ofweighting terms and ranking document relevancy are applied to find the topical clusters that aremost relevant o the definition of each MRD sense.
Finally, we describe an implemented versionof the algorithms for the LDOCE and the LLOCE and assess the performance of the proposedapproach in a series of experiments and evaluations.1.
IntroductionWord sense disambiguation (WSD) has been found useful in many natural anguageprocessing (NLP) applications, including information retrieval (Krovetz and Croft 1992;McRoy 1992), machine translation (Brown et al 1991; Dagan, Itai, and Schwall 1991;Dagan and Itai 1994), and speech synthesis (Yarowsky 1992).
WSD has received in-creasing attention in recent literature on computational linguistics (Lesk 1986; Schi.itze1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995;Luk 1995; Ng and Lee 1996; Chang et al 1996).
Given a polysemous word in runningtext, the task of WSD involves examining contextual information to determine the in-tended sense from a set of predetermined candidates.
It is a nontrivial task to dividethe senses of a word and determine this set, for word sense is an abstract conceptfrequently based on subjective and subtle distinctions in topic, register, dialect, collo-cation, part of speech, and valency (McRoy 1992).
Various approaches to word sensedivision have been proposed in the literature on WSD, including (1) sense numbers inevery-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automaticor hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk* Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC.
E-mail:dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw.
(~ 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 11995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translationin another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwall 1991;Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation(Schiitze 1992), and (6) hand-crafted lexicons (McRoy 1992).This paper is motivated by the observation that directly using dictionary sensesfor sense division offers several advantages.
Sense distinction according to a dictionaryis readily available from machine-readable dictionaries (MRDs) such as the LongmanDictionary of Contemporary English (LDOCE) (Proctor 1978).
A dictionary such as theLDOCE has broad coverage of word senses, useful for WSD.
Furthermore, indicativewords and concepts for each sense are directly available in numbered efinitions andexamples.
Lesk (1986) describes the first MRD-based WSD method that relies on theextent of overlap between words in a dictionary definition and words in the localcontext of the word to be disambiguated.
The author eports that WSD performanceranges from 50% to 70% and his method works well for senses trongly associatedwith specific collocations, uch as ice-cream cone and pine cone.Unfortunately, using MRDs as the knowledge source for sense division and disam-biguation leads to some problems.
Zernik (1992) notes that the dictionary dichotomyof senses is inadequate for WSD, because it is defined along grammatical, not seman-tic, lines.
Furthermore, as pointed out in Dolan (1994), the sense division in an MRDis frequently too fine-grained for the purpose of WSD.
A WSD system based on dic-tionary senses often faces unnecessary and difficult "forced-choices."
Dolan proposesa heuristic algorithm for forming unlabeled clusters of closely related senses in theLDOCE to eliminate distinctions that are unnecessarily fine for WSD.
Regrettably, theproposed algorithm was only described in a few examples and was not developedfurther.
Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk1995; Yarowsky 1995) still resort o human intervention to identify and group closelyrelated senses in an MRD.Using thesaurus categories directly as a coarse sense division may seem to be a vi-able alternative (Yarowsky 1995).
However, typical thesauri, such as Roget's Thesaurus(1987), suffer sense gaps and, occasionally, are too fine-grained.
Yarowsky (1992) re-ports that there are uses not listed in Roget's for 3 of 12 nouns in his WSD study, whileuses which a native speaker might consider as a single sense are often encoded inseveral Roget's categories.As an alternative approach to word sense division, this paper presents an algo-rithm capable of automatically clustering senses in an MRD based on topical informa-tion in a thesaurus.
We refer to the algorithm as TopSense (Topical clustering of Senses).The current implementation f TopSense uses the topical information in the LongmanLexicon of Contemporary English (LLOCE) (McArthur 1992) to cluster LDOCE senses.
Themethod makes use of none of the idiosyncratic nformation i  either the LLOCE or theLDOCE.
Therefore, the TopSense algorithm is quite general and is expected to producecomparable r sults for other MRDs and thesauri.
TopSense is tested on 20 words exten-sively investigated in recent WSD literature (Schi~tze 1992; Yarowsky 1992; Luk 1995).According to the experimental results, the automatically derived topical clusters canbe used to good effect without any human intervention as a coarse sense division forWSD.The rest of the paper is organized as follows.
Section 2 starts out with a descriptionof the MRDs and thesauri used in the computational lexicography and WSD literature,followed by some observations to justify the topic-based approach to word sensedivision.
Section 3 describes the LinkSense algorithm for linking senses between anMRD and a thesaurus.
Section 4 shows how the TopSense algorithm based on the IRmodel may be used to cluster the senses in an MRD.
Examples are given in both62Chen and Chang Topical ClusteringSections 3 and 4 to illustrate how the algorithms work.
Section 4 also describes animplementation f the algorithms for the LDOCE and the LLOCE and reports theevaluation results for both algorithms based on a 20-word test set.
Section 5 analyzesthe experimental results to demonstrate he strengths and limitations of the method.The implication of TopSense to WSD and other issues related to lexical semantics arealso touched upon.
Section 6 compares the proposed method with other approaches inthe computational linguistics literature.
Finally, conclusions are made and directionsfor further esearch are pointed out in Section 7.2.
Word Senses in Machine-Readable Dict ionaries and ThesauriIn this section, we look at two knowledge sources of word sense division, which arecurrently widely available, namely, the dictionary and the thesaurus.
A good-sizeddictionary usually has a large vocabulary and broad coverage of word senses, both ofwhich are useful for WSD.
However, a dictionary's division of senses for a given wordis often too fine for the task of WSD.
On the other hand, a thesaurus organizes wordsenses into a fixed set of coarse semantic ategories, making it more appropriate forWSD.
However, thesauri tend to have a smaller vocabulary and a narrower coverage ofword senses.
To get the best of both worlds of dictionary and thesaurus, we propose tocluster MRD definitions to yield a broad-coverage s nse division with the granularityof a thesaurus.
Therefore, a short description of MRDs and thesauri is in order.2.1 Fine-Grained Senses in an MRDInterest in MRD-based research as increased over the years; in particular, the LDOCEand Webster's Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention.Much of the MRD-based research as focused on the analysis and exploitation ofthe sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi,Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989).
In these works, thedefinitions are analyzed using either a parser (Montemagni and Vanderwende 1992) ora pattern matcher (Ahlswede and Evens 1988) into semantic relations.
These relationsare then used for various tasks, ranging from the interpretation f a noun sequence(Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structuralambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan1994).
Besides the definition itself, there is an abundance of information listed in adictionary entry, including part of speech, subcategory, examples, collocations, andtypical arguments, which is potentially useful for WSD.
In this regard, the LDOCE isparticularly appropriate since it uses a reduced, controlled vocabulary of some 2,000words to define over 60,000 word senses representing a comprehensive ocabularyand broad coverage of word senses.It is arguable that the dictionary division of senses for a given word is too fine-grained, thus inadequate for WSD.
For instance, it might not always be necessary oreasy to distinguish between two LDOCE senses bank.l.n.1 (river bank) and bank.l.n.5(sandbank) shown in Table 1.
Hence, dictionary senses can be used to good effectfor WSD only if such closely related senses are merged and treated as one.
There ismore than one way to merge dictionary senses.
In the following sections, we describeone such approach, under which MRD senses are merged according to the sensegranularity of a typical thesaurus.2.2 Coarse Senses in Thesauri: WordNet, Roget's, and LLOCEOne of the most potentially valuable aspects of the thesaurus, as a knowledge sourcefor word sense division, is the organization of word senses into a limited number of63Computational Linguistics Volume 24, Number 1Table 1The sense entries for bank in the LDOCE.Sense ID Sense Entriesbank.l.n.1bank.l.n.2bank.l.n.3bank.l.n.4bank.l.n.5bank.2.v.1bank.3.n.1bank.4.n.1bank.4.n.2bank.4.n.3bank.5.v.1bank.5.v.2land along the side of a river, lake, etc.earth which is heaped up in a field or garden, often making a border or division.a mass of snow, clouds, mud, etc.a slope made at bends in a road or race-track, so that they are safer for cars to goround.= SANDBANK.
(a high underwater bank of sand in a river, harbour, etc.
)(of a car or aircraft) to move with one side higher than the other, esp.
whenmaking a turna row, esp.
of OARs in an ancient boat or KEYs on a TYPEWRITER.a place in which money is kept and paid out on demand, and where relatedactivities go on.(usu.
in comb.)
a place where something is held ready for use, esp.
ORGANICproducts of human origin for medical use.
(a person who keeps) a supply of money or pieces for payment or use in a gameof chance.to put or keep (money) in a bank.\[esp.
with\] to keep one's money (esp.
in the stated bank)Note: Sense ID = Root + Homonym No.
+ Part-of-speech + Sense No.Table 2Roget's emantic lasses and categories.Class Categories Gloss for ClassesA 1-182 Abstract relationsB 183-318 SpaceC 319-446 MatterD 447-594 Intellect: the exercise of the mindE 595-816 Volition: the exercise of the willF 817-990 Emotion, religion and moralitycoarse semantic ategories.
We briefly describe the on-line thesauri, WordNet (Milleret al 1993), Roget's Thesaurus, and LLOCE, which have been used as word sense di-visions in the computational linguistics literature.
WordNet is organized as a set ofhierarchical, conceptual taxonomies of nouns, verbs, adjectives, and adverbs calledsynsets.
The synsets are too fine-grained from the WSD perspective; WordNet con-tains 24,825 noun synsets for 32,264 distinct nouns with a total of 43,136 senses in itsnoun taxonomy alone.
It would be difficult to acquire WSD knowledge for makingsuch fine distinctions even from a substantial body of training materials.Roget's Thesaurus arranges words in a three-layer hierarchy and organizes over30,000 distinct words into some 1,000 categories on the bottom layer.
These categoriesare divided into 39 middle-layer sections that are further organized as 6 top-layerclasses.
Each category is given a three-digit reference code.
To make the hierarchicalstructure xplicit, an uppercase letter from A to F is added to the reference code to de-note the top-layer class for each category, as indicated in Table 2.
Similarly, the middlelayer is denoted with a lowercase reference letter.
The sections related to class B (Space)are shown in Table 3.
Therefore, the reference code for each category is denoted byan uppercase class letter, a lowercase section letter, and a three-digit category number.64Chen and Chang Topical ClusteringTable 3Sections related to the Space class in Roget's.Class Sections Gloss of Section ExamplesB 183-194 a Space in generalB 195-242 b DimensionsB 243-264 c FormB 265-318 d Motionsurface, heavens, room, kitchen, abodeweight, proximity, clothes, wear, hallidea, distortion, flat, plug, yawn, subwayrocket, transposition, carrier, entrance?class?sectionCZ~category~ ~  Emotion,Abstract / " J \[ ~ ~ ~ religionre la t io~ S ~ ~l~r  ~n~t~i t ion  -"--,aa..d m_~alityDimen I ~ i o n  ~ Nre\]ations.
.
.
.
.
.Edge ~Obl iq u it yI,,~2~-~-~L,,~ bank.
bank bankS uppork,,,~r),~,,~ .
.
.
.
.
.
?
...Height k,,,,.___..J?
bank ..Figure 1Roget's categorization scheme?For instance, the word bank listed under Category 209 in Roget's will be prefixed anadditional etter B to denote the class Space, plus a lowercase letter b to denote thesection Dimensions; the reference code 209 is replaced with Bb209.
Figure 1 shows theinformation for the word bank in Roget's.WordNet and Roget's to a lesser degree present word senses that are too fine-grained for WSD.
Often, uses that a native speaker might consider as a single sense areencoded in several Roget's categories or WordNet synsets.
For instance, a single LDOCEsense bank.4.n.1 shown in Table 1 corresponds to two WordNet synsets Depositoryfinancial institution and Bank building and two Roget's categories, 799 (Treasurer) and784 (Lending).
Similarly, the Roget's lists two categories 234 (Edge) and 344 (Land) for aconcept reated as one word sense, bank.l.n.1 in the LDOCE.
Table 4 provides furtherdetails.The LLOCE is a hierarchical thesaurus that organizes word senses primarily ac-cording to subject matter.
The LLOCE contains over 23,000 different senses for some15,000 distinct words.
The coarser senses in LLOCE are organized into approximately2,500 topical word sets.
These sets are divided into 129 topics and these topics arefurther organized as fourteen subjects.
The subjects are denoted with alphabetical ref-erence letters from A to N (see Table 5).
Thus the LLOCE subject, topic, and topical setconstitutes a three-level hierarchy, in which each subject contains 7 to 12 topics andeach topic contains 10 to 50 sets of related words.
Table 6 displays the topics related65Computational Linguistics Volume 24, Number 1Table 4Comparison of MRD and thesaurus treatment of bank senses.LDOCE WordNet Sense Roget's Sense LLOCE Sensebank.l.n.1 Ridge 234 (Edge)/344 (Land) Ld099 (River bank)bank.l.n.2 Ridge 234 (Edge) - -bank.l.n.3 Array - -  - -bank.l.n.4 Slope 239 (Laterality) - -bank.l.n.5 Ridge 344 (Land) Ld099 (River bank)bank.2.v.1 Tip laterally 239 (Laterality) Nj295 (To bend)bank.3.n.1 Array - -  - -bank.4.n.1 Depository/Bank building 799 (Treasurer)/784 (Lending) Jel04 (Finance)bank.4.n.2 Supply 632 (Storage) - -bank.4.n.3 Supply - -  - -bank.5.v.1 Deposit 799 (Treasurer) Jel06 (Deposit)bank.5.v.2 Keep money/Deposit 799 (Treasurer) Jel06 (Deposit)Table 5LLOCE subjects and their reference l tters.Subject Set Gloss for SubjectsA 1-158B 1-181C 1-357D 1-186E 1-143F 1-283G 1-293H 1-252I 1-148J 1-240K 1-207L 1-273M 1-225N 1-367Life and living thingsBody; its function and welfarePeople and familyBuildings, houses, home, clothes, belongings, personal careFood, drink, and farmingFeeling, emotions, attitudes, and sensationsThought, communication, language, and grammarSubstance, materials, objects, and equipmentArts/Crafts, science/technology, industry/educationNumbers, measurement, money, and commerceEntailment, sports and gamesSpace and timeMovement, location, travel, and transportationGeneral and abstract termsto subject L (Space and time).
Each topical set is given a three-digit reference code;however, this code does not explicitly reflect the topic.
To make use of the informa-tion related to a topic, we have designated a lowercase letter to each topic.
Therefore,each set is denoted by an uppercase "subject" letter, a lowercase "topic" letter, and athree-digit "topical set" number.
For instance, the word bank listed under L99 in theLLOCE will be given an additional reference l tter d to denote the topic Geography; thereference code L99 is replaced with Ld099.
The LLOCE also provides cross-referencesbetween sets and topics to indicate various intersense relations not captured withinthe same topic.
For instance, topic Ld (Geography) has a cross-reference to topic Me(Place).
Figure 2 shows LLOCE's topical classification and cross-references related tothe word bank.The LLOCE, and, to a lesser degree, Roget's, are based on coarse, topical seman-tic classes, making them more appropriate for WSD than the finer-grained WordNetsynsets.
The 129 topics in the LLOCE or 990 categories in Roget's appear to be suffi-66Chen and Chang Topical ClusteringTable 6Topics related to subject L in LLOCE.Subject Range Gloss ExamplesL 001-019 a The universeL 020-039 b Light and colorL 040-079 c Weather and temperatureL 080-129 d GeographyL 130-169 e Time generallyL 170-199 f Beginning and endingL 200-219 g Old, new, and youngL 220-249 h Period/Measure of timeL 250-273 i Function words (time)sun, moon, star, left, right, etc.light, dark, ray, color, white, black, etc.weather, sky, rain, snow, rain, ice, etc.stream, sea, lake, flood, to flow, etc.time, history, frequent, permanent, etc.start, stop, late, last, etc.ancient, modem, future, age, etc.day, night, second, minute, etc.now, soon, always, ever, after, etc.
"L9 >O,o , .
.
.
.
.
.
.
.
.
?
-  ....... .
.
bank  .
.
.
.
.
.
bank  ..- - - ~ cross-referenceFigure 2LLOCE's topical organization of word sense.cient for representing the distinction we would want to make for the task of WSD.Roget's has been used as the sense division in two recent WSD works (Yarowsky 1992;Luk 1995) more or less as is, except for a small number  of senses added to fill gaps.We contend that a sense division based on the LLOCE topics will offer more or lessthe same kind of granularity, suitable for WSD.
For instance, in Yarowsky (1992), thesenses of star are divided into three Roget's categories, which roughly correspond tofive LDOCE star senses labeled with LLOCE topics.
In the same study, six Roget's cat-egories are sufficient o distinguish the senses of slug.
These six categories correspondto five relevant LLOCE topics.
Table 7 provides further details.2.3 Combining Word Sense Information from an MRD and a ThesaurusIt should be clear by now that combining a dictionary and a thesaurus leads to abroad-coverage s nse division with a suitable granularity for WSD.
The obvious wayto combine the two would be to disambiguate and link a sense definition D of aheadword h in the dictionary to an entry relevant o D in the thesaurus.
This amountsto a special case of WSD with respect o thesaurus enses.
There is no simple solution67Computational Linguistics Volume 24, Number 1Table 7Roget's and LLOCE classifiers for two sample words.Word Roget's (three-layer representation) LLOCE (two-layer representation)star 321 (Universe) La (Universe)594 (Entertainer) Kd (Drama)729 (Insignia) Jb (Mathematics)- -  Dg (Personal belongings)- -  Nb (Chance)slug 365 (Animal~Insect) Ag (Insect)587 (Printing) Gd (Communication)359 (Impulse~Impact)797 (Money) Jd (Money)723 (Arms) Hh (Weapon)322 (Weight) Hc (SpeciJi'c substances)to the general WSD problem for unrestricted text, but we will show that this specialcase of disambiguating MRD definitions is significantly easier, for several reasons.First, the words used in a definition sentence are limited primarily to a small set;in the case of the LDOCE, the controlled vocabulary consists of some 2,000 words.For instance, in the first five LDOCE senses of bank shown in Table 1, all definingwords are in the controlled vocabulary, except for the word SANDBANK, shown incapital letters.
Obtaining WSD information for this small set of words obviously ismuch easier than it would be for a large, open set.Second, dictionary definitions adhere to rather rigid patterns under which onlywords with predictable semantic relations how up.
A dictionary definition, in general,begins with a genus term (that is, conceptual ancestor of the sense), followed by a setof differentiae that are words semantically related to the sense to provide the specifics.The semantic relations between the sense, the genus, and differentiae are reflected inwhat are termed categorical, functional, and situational clusters in McRoy (1992).The semantic relations and clusters have been shown to be very effective knowledgesources for such NLP tasks as WSD (McRoy 1992) and interpretation ofnoun sequences(Vanderwende 1994).
For instance, in the first four definitions of bank in Table 1, thegenus terms land, earth, mass, and slope are categorically related to the respective banksenses.
On the other hand, the differentiae river, lake, field, garden, bend, road, and race-track have a LocationOf situational relation with bank.
Other differentiae, snow, cloud,and mud, are related functionally to bank.l .n.3 through the MakeOf relation.Third, for the most part these relations are captured implicitly in a typical the-saurus.
The LLOCE and Roget's conveniently contain information on the relations inthe form of word lists under a topic (category) or cross-referencing to other topics.Therefore, an MRD sense definition can be effectively disambiguated based on theword lists and cross-references in a thesaurus.
A simple heuristic relying on the sim-ilarity between a sense's defining keywords and thesaurus word lists suffices to linkan MRD sense to its relevant sense in the thesaurus.
For instance, the differentiae(land, side, river, lake) of bank.l .n.1 is sufficiently similar to the word list of Ld-topic(Geography) to warrant the link between LDOCE sense bank.l .n.1 and LLOCE sensebank-Ld099.The topics and cross-references of LLOCE in general capture the Generic~Specificrelation; therefore, a sense definition is often disambiguated through the genus.
Thus,the task of linking MRD and thesaurus enses is closely related to the extraction and68Chen and Chang Topical Clusteringdisambiguation f the genus.
For instance, in the above example, linking bank.l.n.1 tobank-Ld099 has, as a by-product, he disambiguation f the genus land to land-Ld084(Geography) rather than land-Ce078 (Social organization in groups and place).
Details ofextraction and disambiguation f the genus can be found in previous works (Guthrieet al 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al1992).
Disambiguated genus and differentiae t rms can be used to construct a bettertaxonomy of word senses.Since the dictionary usually has broader coverage of word senses than the the-saurus, not all MRD senses of a headword h correspond to one of h's predefinedsenses in the thesaurus.
For instance, LDOCE sense bank.l.n.3 (a mass of cloud, snow,or mud, etc.)
corresponds to LLOCE topic Hb (Object generally) rather than any of thepredefined LLOCE senses for bank.
Therefore, such entries represent sense gaps in thethesaurus and should be left unlinked.
Nevertheless, the linked entries are enoughtraining material for topical clustering of MRD senses, as described in Section 4.3.
Linking an MRD to a ThesaurusThis section describes how to establish a link between an MRD sense and its relevantword sense in a thesaurus, if such a link exists.
We start with the preprocessing stepsfor the sense definition, which are necessary for the algorithm to obtain good results.Then we describe the linking algorithm step by step.
Finally, we show illustrativeexamples to give some idea how the proposed algorithm works for the LLOCE andRoget's.3.1 Preprocessing StepsAlthough only simple words are usually used in sense definitions, most of these wordsare also highly ambiguous.
For instance, the two instances of lies listed in the twofollowing LDOCE sense definitions differ in meaning:couch.2.n.2 a bed-like piece of furniture on which a person lies when being ex-amined by a doctor.lie detector an instrument that is supposed to show when a person is telling lies.Notably, their parts-of-speech are also different.
Determining the part of speech ofeach instance allows us to limit the range of possible meanings.
The first instanceof lies is a verb that means "to be in a flat resting position" or "to tell a lie."
Onthe other hand, the second instance is a nominal with a unique meaning "a falsestatement purposely made to deceive."
By tagging the definition with part-of-speechinformation, the degree of sense ambiguity in the definition can be reduced, therebyincreasing the chance of successful linking.Part-of-Speech Tagging.
Various methods for POS tagging have been proposed in re-cent years.
For simplicity, we adopted the method proposed by Church (1988) to tagdefinition sentences.
Experiments indicated an average rror rate for tagging of lessthan 10%.
Tagging errors have limited negative impact, because words in the LLOCEare organized primarily according to topic, not part of speech.
The POS informationis used to remove function words, as well as to look up words in the LLOCE withmatching POS.
The part-of-speech preprocessing phase is mandatory for the algorithmto exclude some inappropriate candidates for topics.
See Table 8 for some examplesof tagged LDOCE definition sentences.69Computational Linguistics Volume 24, Number 1Table 8Some tagged LDOCE definition sentences for the headword bank.bank.l.n.1bank.l.n.2bank.l.n.3bank.l.n.4land/n along/prep the/det side/n of/prep a/det river/n ,/, lake/n ,/, etc/advearth/n which/det is/v heaped/v up/adv in/prep a/det field/n or/conj gar-den/n ,/, often/adv making/v a/det border/n or/conj division/na/det mass/n of/prep snow/n ,/, clouds/n ,/, mud/n ,/, etc/adva/det slope/n made/v at/prep bends/n in/prep a/det road/n or/conj race-track/n ,/, so/conj that/conj they/pron are/v safer/adj for/conj cars/n to/* go/vround/advTable 9Examples of keywords extracted from tagged efinition sentences.bank.l.n.1bank.l.n.2bank.l.n.3bank.l.n.4land/n side/n river/n lake/nearth/n heap/v field/n garden/n border/n division/nmass/n snow/n clouds/n mud/nslope/n bend/n road/n race-track/n cars/nRemoval of Stopwords.
In general, function words in the definition are only marginallyrelevant o the meaning being defined.
This is also true of words used in many defi-nitions.
For this reason, IR systems commonly exclude stopwords from the process ofindexing and query.
This also applies to our situation of retrieving topics relevant othe meaning of a sense based on the words in its definition.
The list of all the stop-words is specifically designed to remove pronouns, determiners, prepositions, andconjunctions.
Table 9 shows that the meaning of some definitions of bank is found tobe quite intact, even after stopwords are removed.Calculating Similarity between Definition and Thesaurus Class.
When viewing the definitionof a headword h as a set of words, it becomes easy to compare and measure theirsimilarity to thesaurus word classes containing h. By word classes, we mean anysupersets of synonym sets in a thesaurus that capture the semantic relations andsemantic lusters that are effective for disambiguation as described in Section 2.3.
Theword classes are so chosen that they contain enough words to overlap with the sensedefinition in question.
But each class should not be so big as to cover more than onethesaurus sense for h, blurring the distinction we want to make in the first place.
Topicsin the LLOCE and categories or sections in Roget's are good choices for such classes.Similarity between the defining keywords and a class of words reflects how closelythe definition is related to the class.
As a simple heuristic, the intended meaning of adictionary definition D for h is disambiguated in favor of a relevant sense T for h in athesaurus class C with the highest similarity to D. When such a sense T is found, wesay that the dictionary sense D is linked to the thesaurus ense T or that D is linkedto the thesaurus class C (containing T.)For a headword h, let DEFh denote the definitions of h and let CLASSh be the wordclasses in a thesaurus that contain h. For a definition D E DEFh, our problem amountsto finding C E CLASSh that is relevant o D. With these terms, the unweighted Dicecoefficient can be adopted to measure similarity between a definition D and a class Cas follows:Sim(D, C) = ZdEKEYD 2 X W d X In(d, C)\]KEYD\[ + \[C\[ 'where KEYD = the set of words in definition D E DEFh, \[KEYD\[ = number of words in70Chen and Chang Topical Clustering1 KEYD, C E CLASSh -= a relevant class to h in the thesaurus, w k ~-  degree ofambiguity ofk'In(a, B) ---- 1, when a E B, and In(a, B) = 0, when a ?
B.The above similarity measure may be improved by taking into consideration spe-cific features of a particular thesaurus.
For instance, the cross-reference features in theLLOCE or the intersense relations in Roget's are very effective in reflecting semantic re-latedness; thus, they should be included in this similarity measure.
Let REFc representthe cross-referenced classes for the word class C in the thesaurus.
Thus, we haveSim'(D,C) = ~dEKEYD 2 X W d X (In(d,C) + "I, In(d, REFc) )IKEYDI + ICI + '~IREFclwhere-y = relevancy of cross-references to a class 1, and IREFcl = the number of classesin REFc.3.2 The LinkSense AlgorithmWe sum up the above description and outline the procedure for labeling senses on adictionary entry as follows:Algorithm LinkSenseLinking fine-grained MRD senses to their relevant hesaurus classes.Step 1: Given a head word h, read its definition, DEFh, from the MRD.Step 2: For each definition D in DEFh, tag each word in D with POS in format ion .
.Step 3: Remove all stop words in D to obtain a list of keyword-POS pairs, KEYD.Step 4: Look up the headword h in the thesaurus to obtain CLASSh.Step 5: Compute Sire(D, C) for all C E CLASSh.Step 6: Link D to C such that Sim(D, C) is the largest and Sim(D, C) is greaterthan a preset hreshold, 0.3.3 Illustrative Examples: Linking LDOCE to LLOCE and Roget'sTwo examples are given in this subsection to illustrate how LinkSense works to establishlinkage between a typical dictionary and thesaurus.
Example 1 shows, step by step,how LinkSense links up an LDOCE sense, interest.l.n.2 (a share in a company businessetc.)
with the relevant LLOCE sense interest-Je (Banking).
2 Example 2 is intended toshow that LinkSense is quite general and applies to thesauri other than the LLOCE.The same LDOCE senses will be shown to links to a relevant Roget's ense interest-Ei(Possessive r lation).Example 1Linking an LDOCE sense interest.l.n.3 to its relevant LLOCE sense.Step 1: D -- "a share in a company, business, etc.
"Step 2: POSD -~ {a/det, share/n, in/prep,  a/det,  company/n ,  business/n, etc./adv}1 For simplicity, the parameter 3' is set to 1 in our experiment.2 Je represents the class of words related to the topic of Banking, Wealth, and Investment l isted underLLOCE topical sets Jel00 through 127.
The reference code e is added in accordance with the codingscheme described in Section 2.2.71Computational Linguistics Volume 24, Number 1Step 3: KEYD = {share/n, company/n,  business/n}, IKEYDI = 3.Step 4: Using LLOCE topics as word classes in LinkSense, we haveCLASSinterest = {Fj (Excitement), Fb (Liking), Je (Banking), Ka (Entertainment)},The LLOCE lists the following cross references relevant o CLASSinterest:REFFjREFFBREFKa= {Ka (Entertainment), Kb (Musicand related activity),...,Kh (Outdoor games)},= {Cc (Friendship)},REFje = {De (Getting and giving)},= {Fj (Excitement)}, IFjl = 1, IFbl = 1, IJel = 1, IKal = 1,IREFFjl = 8, IREFFbl = 1, IREFjel = 1, IREFKal = 1.All three keywords appear in three different opics but only the followingclasses are relevant o ccaSSinterest: De (share), Je (share), Cc (company)Thus, we haveIn(share, De) = 1, In(share, Je) = 1, In(company, Cc) = 1.Wshare/n -~ Wcompany/n = Wbusiness/n = 1/3.Step 5: Similarity values are calculated as follows:Sim'(D, Je)2 x Wshar e X (In(share, Je) + In(share, REFje))I{share, company, business}l + I{Je}l + I aEFjel2 x Wcompany X (In(company, Je) + In(company, REFje)) +I{share, company, business}l + I{Je}l + IREFiel2 x Wbusiness X (In(business, Je) + In(business, REFje)) +I{share, company, business}l + I{Je}l + IREFjel2x  ?x(1+1)+2x ?x(0+0)+ax ?
x(0+0)3+1+11.33 - -  - 0.267,5Sim'(D, Fb) = 2 x ?
x (0 + 1)3+1+1Sim'(D, Fj) = O,Sim'(D, Ka) = 0.= 0.133,Step 6: The LDOCE sense, interest.l.n.3 is linked to the LLOCE sense, interest-Je.Example 2Linking an LDOCE sense interest.l.n.3 to its relevant Roget's ense.Step 1-3: The first three steps are independent of the thesaurus used, thereforethe same results as in Example 1 should be obtained.72Chen and Chang Topical ClusteringStep 4: Using Roget's categories as word classes in LinkSense, we have:CLaSSinterest = {Ab (Dimensions), Cb (Inorganic matter),Eb (Prospective volition), Ei (Possessive r lations) },The keywords hare, company, and business appear in many Roget's ections,but only the following sections are relevant o CLASSinterest:  Ei (share andbusiness), Eb (business)Therefore we have:Wshare/n = 1/4, Wcompany/n  = 1/5, Wbusiness/n -~ 1/7.Step 5: For simplicity, we ignore the cross-reference information in Roget's andbase our similarity calculation solely on the CLASS information.
Thus, wehave:Sim(D, Bb) = 0,Sim(D, Cb) = 0,1 2Sim(D, Eb) -  2x  ~ _ y3+1 42x(?+?
)Sim(D, Ei) - 3 + 12-- - 0.071,2811 _ ~ 114 56 0.196.Step 6: The LDOCE sense interest.l.n.3 is linked to Roger's sense interest-Ei.3.4 Performance Evaluation of LinkSenseAn experiment involving the LDOCE and the LLOCE was carried out to assess theeffectiveness of the LinkSense algorithm (see Table 10).
To evaluate the performance ofalgorithms, we define the ratios of applicability A and precision P as follows:#(all labeled efinitions) A = # (all definitions)# (correct labeled efinitions) P = # (all labeled efinitions)Nearly half of the nominal LDOCE senses for a set of highly polysemous words arelinked to their relevant LLOCE sense and topics, with a surprisingly high precisionrate of 93%.
For the other hai l  LinkSense does not find sufficiently high similarity towarrant a link.
That is due primarily (approximately two-thirds) to sense gaps in theLLOCE, rather than inconsistency among the LDOCE definitions.4.
Topical Clustering of MRD Senses as Information RetrievalIn this section, we will describe TopSense, an algorithm for clustering dictionary senses.TopSense clusters closely related senses by applying IR techniques on the results ofrunning LinkSense on an MRD.
After LinkSense links a substantial portion of MRDsenses to thesaurus ense classes, we put all definitions of the senses linked to aparticular class together in a document.
With such a document of collective definitions,topical clustering of all MRD senses bears a striking resemblance to the IR task of73Computational Linguistics Volume 24, Number 1Table 10Performance of LinkSense algorithm.# of DefinitionsHeadword in LDOCE Linking to the LLOCECorrect Incorrect Unknown Applicability Precisionbass 5 2 1 2 67% 100%bow 5 4 0 1 80% 100%cone 3 3 0 0 100% 100%country 5 5 0 0 100% 100%crane 2 2 0 0 100% 100%duty 2 1 0 1 50% 100%galley 4 2 0 2 50% 100%interest 6 4 0 2 67% 100%issue 8 2 1 5 38% 50%mole 3 2 0 1 67% 100%plant 6 1 0 5 17% 100%position 10 7 0 3 70% 100%sentence 2 2 0 0 100% 100%slug 5 2 0 3 40% 100%space 8 2 0 6 25% 100%star 9 3 2 4 56% 60%suit 6 3 0 3 50% 100%table 7 2 0 5 29% 100%tank 3 1 0 2 33% 100%taste 6 1 0 5 17% 100%total 105 51 4 50 52% 93%retrieving relevant documents for a given query.
We observe that the defining wordsof a sense S frequently recur in documents relevant o S. For instance, consider thefollowing LDOCE sense:star.
l .n.5 a piece of metal in this shape for wearing as a mark of office, rank,honour, etc.We observe that most entries in the LDOCE are like star.l.n.5, in that they containdefining words that are also recurring terms in a relevant document.
For instance,headwords uch as apron, bracelet, necklace, and tie are defined by using terms in adocument corresponding the LLOCE word class for the topic Dg (Clothes and personalbelongings).
Table 11 shows the Dg class, including such senses as apron, bracelet, neck-lace, and tie, which are indeed relevant o star.l.n.5.4.1 The  C lustered Senses  as DocumentsWith topical clustering of MRD senses cast as an IR task, a wealth of well-understoodIR techniques can be utilized, including stopword removal, case folding, stemming,term weighting, and document ranking (Witten, Moffat, and Bell 1994).
Using the IRanalogy, topical clustering of an MRD sense S is finding relevant documents (topicalclusters), given a query (S, the sense definition).
With this in mind, we treat the col-lective definitions of each topical cluster as a virtual document (VD) and reduce theclustering task to ranking relevancy based on terms in the sense definition as well asthose in the VDs.
For simplicity, we adopt a common scheme of tf x idf to weight termsin the documents.
Each defining term in a VD is associated with a term frequency (tf)74Chen and Chang Topical Clusteringand document frequency (dr).
Let t~j represent the frequency of term tj in documentVDi, and d~ represent the number of VDs where term tj appears.
The relevancy of VDito the sense S according to term tj is therefore given by the following weight:wi j  = x = ?
log(N/dR) ,where N is the number of documents in the collection.
The relevancy of a VDi to aquery Q is obtained by summing up the weights of all terms t) in Q:t~j x log(N/dJ~).tjTable 11 shows the LDOCE senses and their definitions that are linked to relevantLLOCE senses under a certain topic.
An implementation f LinkSense links 455 LDOCEsenses including accessory, bracelet, and tie to a Dg class in the LLOCE.
The definitionsof these senses in the Dg class form the virtual document DDg.
As shown in Table 12,significantly topical terms are used within a VD consistently.
For instance, the termangle appears consistently in 10 senses in Djb with a weight of 23.82.
Table 12 displaysheavily weighted terms in some VDs and their associated values of tf, df, and weight.4.2 The TopSense AlgorithmWe sum up the above descriptions and outline the TopSense algorithm here.Algorithm TopSense: Topical clustering of MRD senses.Step 1:Step 2:Step 3:Step 4:Step 5:Step 6:Run LinkSense on the MRD and thesaurus and collect erms to form VDs.Read sense definition S from the MRD.Remove all stopwords in S and produce a list, Q, of stemmed keywordswith part of speech.For each term tj in Q, look up the corresponding Wij for all virtual docu-ments Do C E CLASS, the set of all word classes in the thesaurus.For C E CLASS, calculate Sire(Q, Dc) = ~tj Wij ' where Wij = t~j xlog(N/dJ~).Assign S to the class C such that Sim(Q, Dc) is the largest for all C E CLASSand passes a preset hreshold 0.4.3 Illustrative Examples: Topical Clustering of LDOCE SensesTwo examples are given in this subsection to illustrate how TopSense works.
Example 3shows a calculation done in TopSense to find the most relevant topics for another starsense (a 5-or more pointed figure).
Example 4 shows the same calculation done for thesense of star (a piece of metal in this shape for wearing as a mark of office, rank, honour etc.
)discussed above at the beginning of Section 4.
Despite very ambiguous terms such asto wear (to dress or to rub) and figure (body, shape, or number) present in both definitions,the weighting scheme of TopSense seems to work well enough to determine the relevanttopics, Dg (Clothes and personal belongings) and Jb (Mathematics), respectively.75Computat ional  Linguistics Volume 24, Number  1Table 11Partial list of LDOCE senses l inked to LLOCE classes by LinkSense.Clus ter /S i ze  Headword  Sense DefinitionDg / 455 ?
accessory ?
(Clothes and per- ?
apronsonal belongings)?
bracelet?
coat?
necklace?
fiesomething which is not a necessary part of somethinglarger but which makes it more beautiful, useful, effec-tive etc.?
a simple garment worn  over the front part of one'sclothes to keep them clean whi le work ing or doingsomething dirty or esp.
whi le cooking.?
a band or ring, usu.
of metal,  worn  round the wrist orarm as an ornament.?
an outer garment with long SLEEVEs, often fastened atthe front with buttons, and usu.
worn  to keep warm orfor protection.?
a string of jewels, BEADs, PEARLs, etc., or a chain ofgold, silver, etc., worn  around the neck as an ornamentesp.
by women.?
a band of cloth worn  round the neck, usu.
inside a shirtcollar and tied in a knot at the front.Jb / 212(Math)?
cross?
d iameter?
pyramid?
rectangle?
square?
triangle?
a figure or mark formed by one straight line crossinganother, as X, often used.?
a straight line going from side to side through the centreof a circle or other curved figure.?
a solid figure with a flat usu.
square base and straightflat 3-angled sides that slope upwards to meet at a point.?
a figure with 4 straight sides forming 4 right angles.?
a figure with 4 equal sides and 4 right angles.?
a flat figure with 3 straight sides and 3 angles.Ld / 524(Geography)?
bank?
bay?
beach?
lake?
cascade?
land along the side of a river, lake, etc.?
a wide opening along a coast; part of the sea or of alarge lake enclosed in a curve of the land.?
a shore of an ocean, sea, or lake or the bank of a rivercovered by sand, smooth stones, or larger pieces of rock.?
a large mass of water surrounded by land.?
a steep high usu.
small waterfall, esp.
one part of abigger waterfall.Je / 181(Banking)?
account?
asset?
bank?
capital?
stock?
a record or statement of money received and paid out,as by bank or business, esp.
for a particular period orat a particular date.?
something such as a house or furniture, that has valueand that may be sold to pay a debt.?
a place in which money is kept and paid out on demand,and where related activities go on.?
wealth, esp.
when used to produce more wealth.?
money lent to a government  at a fixed rate of interest.Example 3Clus ter ing  an LDOCE sense star.l.n.3.Step 1:Step 2:Step 3:Refer  to Table 11 for some of the  resu l ts  of  runn ing  LinkSense.S = "a 5-or more pointed figure.
"Q = {po in ted /a ,  f igure /n}76Chen and Chang Topical ClusteringTable 12Some examples of virtual documents, terms, tf, dr, and weight.VD Terms tf dff Weight VD Terms tf df WeightDg garment 43 12 102.45 Ld sea 38 23 65.81wear 60 27 94.30 land 47 40 55.39dress 21 5 68.42 mountain 16 7 46.74woman 49 36 62.91 water 44 47 44.76coat 19 7 55.51 river 17 15 36.71trouser 11 2 45.91 tide 7 1 34.07shirt 12 3 45.22 valley 8 2 33.39undergarment 8 2 33.39 ocean 9 4 31.33shoe 12 11 29.63 lake 10 8 27.88skirt 6 1 29.20 shore 8 4 27.84cloth 16 25 26.37 earth 16 26 25.75waist 8 5 26.06 island 6 2 25.04jacket 6 2 25.04 rock 11 14 24.51sleeve 5 1 24.33 wave 9 13 20.72glove 5 2 20.87 hill 8 10 20.51sock 5 2 20.87 deep 12 25 19.78neck 10 17 20.34 coast 6 5 19.54underpants 4 1 19.47 slope 7 8 19.51woolen 5 4 17.40 cliff 5 3 18.84tie 7 14 15.59 map 7 9 18.69Jb mathematics 15 4 52.21 Je money 42 39 50.56multiply 8 3 30.15 account 16 9 42.72figure 13 19 25.00 bank 16 12 38.12straight 12 17 24.41 pay 17 31 24.37angle 10 12 23.82 lend 7 5 22.80line 21 44 22.75 interest 12 25 19.78circle 9 11 22.22 debt 5 3 18.84geometry 5 3 18.84 sum 6 10 15.38calculate 7 9 18.69 wealth 5 6 15.37add 8 13 18.42 credit 3 1 14.60subtract 3 1 14.60 property 5 9 13.35curved 7 5 11.54 deposit 2 1 9.73perpendicular 2 1 9.73 savings 2 1 9.73proportion 2 1 9.73 payment 4 14 8.91right-angled 2 1 9.73 share 4 15 8.63triangle 2 1 9.73 record 4 16 8.37edge 6 26 9.65 spend 3 11 7.40arc 2 2 8.34 business 6 40 7.07curve 3 9 8.01 supply 4 25 6.59cross 3 11 7.40 amount 6 46 6.23Step 4:Step 5:For each term in Q, we have:Wpointed,Jb = 0, Wpointed,Kf = 0, Wpointed,Gd = 0,Wfigure, jb z 25.00, Wfigure, Kf -~ 9.62, Wfigure, Gd = 7.69,Add ing  up the weights for each VD, we getSim(Q, Jb) = 25.00,Sim(Q, Kf) = 9.62,Sim(Q, Gd) = 7.69,Wpointed,Hd ~ 0,Wfigure,nd = 5.77.77Computational Linguistics Volume 24, Number 1Sim(Q, Hd) = 5.77.Step 6: For the most relevant opics to S, we get the following ranked listJb (Mathematics),Kf (Indoor games),Gd (Communicating),Hd (Equipment, machines, and instruments).Example 4Clustering an LDOCE sense star.l.n.4.Step 1:Step 2:Step 3:Step 4:See Table 11.S = "a piece of metal in this shape for wearing as a mark of office, rank, honouretc.
"Q = {metal /n,  shape/n ,  wear /v ,  mark /n ,  office/n, rank/n ,  honour /n}For each term in Q, we have:Wmetal,Dg = 3.77,Wshapel, Dg = 5.98,Wwear, Dg = 94.30,Wmark,Dg = 1.11,Woffice,Dg = 0,Wrank,Dg = 1.69,Whonour, Dg = 0,Wmetal,Hc : 62.83,Wspape,Hc : 8.97,Wwear, H c : 0,Wmark,H c : 3.32,Woffice,m = 1.61,Wrank,H c : 0,Wh~nour, Hc = GWmetal,Ci ~ 0,Wshape,Ci -~ 0,Wwear, C i = 0,Wmark,C i = 0,Woffice,Ci = 3.22,Wrank,C i ---- 72.65,Wh .
.
.
.
r, Ci = 2.30,Wmetal,Hb = 22.62,Wshape,Hb = 7.97,Wwear, H b ~- 4.72,Wmark,H b = 6.64,Woffice,Hb = 1.61,Wrank,H b = 0,Wh .
.
.
.
r, Hb = 2.30.Step 5: Adding up the weights for each VD, we getSim(Q, Dg) = 115.16,Sim(Q, Hc) = 100.29,Sim(Q, Hb) = 88.83,Sim(Q, Ci) -- 78.17.Step 6: For the most relevant opics to S, we get the following ranked list:Dg (Clothes and personal belongings),Hc (Specific substances and materials),Hb (Object generally),Ci (Social classifications and situations).4.4 Experimental ResultsAn experiment was conducted to assess the effectiveness of the LinkSense and TopSensealgorithms.
The experimental results show that the LinkSense links nearly 11,045 ofsome 39,000 nominal LDOCE senses to a topical sense in the LLOCE.
Evaluationbased on a 20-word test set shows that, on the average, 50% of the LDOCE instanceslinked to an LLOCE sense, and, of these links, 95% are correct.
These linked LDOCEsenses establish 129 topical clusters, one for each LLOCE topic.
When the proposed78Chen and Chang Topical ClusteringLinkSense algorithm is applied to assign sense definitions in LDOCE with relevanttopical labels, it obtains very high precision but low coverage.
TopSense is designspecifically to improve coverage by providing a reliable method for clustering MRDentries left unlabeled by LinkSense.
3 A document of defining terms is then formed fromMRD senses in each of these clusters.
Subsequently, TopSense runs on the nominalLDOCE sense, attempting to merge it to one of the topical clusters.The thresholds for LinkSense and TopSense are selected according to random sam-piing from definitions in the LDOCE.
Assume 0 is the threshold and 0 is an estimatorof 8, and B is the bound on the error of estimation.
The problem is to limit the error ofestimation below B with probability 1- o~.
This can be stated as P(I~ - ~1 < B) = 1 - o~,since the number of definitions is large enough to permit estimation of populationparameter 8.
Considering Central Limit Theory, the parameter ~ tends to have ap-proximately a normal distribution.
We will usually select B = 2cr6, and hence 1 - o~will be approximately 0.95 for normal distribution.
To estimate ~, a simple randomsample of 100 definitions (about 350 senses) is used.
Thus, the estimate of thresholdis 0.12 for LinkSense.
Similar estimation was done for the threshold used in TopSense.Evaluation was done on a set of 20 polysemous words that have been used inrecent literature on WSD.
These words focus on the more difficult cases of senseambiguity, as can be seen by the degree of ambiguity as recorded in the LDOCE.These words have 5.3 senses on the average, as opposed to the average of 2.6 sensesfor all words in the LDOCE.The evaluation is based on the relevancy assessment by two human judges.
TheAppendix gives a sense-by-sense rundown of all senses tested and evaluated.
Table 13summarizes the word-by-word applicability and precision of TopSense.
Although notall senses are clustered and not all clustered senses are correct, applicability and pre-cision are rather high, which seems to indicate that the resulting sense division isdirectly usable in WSD, and thus, eliminates the need for human intervention.5.
DiscussionIn this section, we thoroughly analyze the experimental results, in particular, the casesfor which TopSense fails.
These cases reveal the strengths and limitations of TopSenseand hint at possible improvements to the algorithm.
In addition, we also point outseveral uses of the topical clusters.5.1 Failure of the TopSense AlgorithmFailure of TopSense can be attributed to a number of factors, including vagueness ofdefinitions, inappropriate definition lengths (too short or too long), metaphoric ormetonymic senses, and deictic references.
Table 14 shows some examples of the failedcases.
For instance, the sense interest.l.n.3 (a readiness togive attention) is too vague andshort for correct clustering to occur.
On the other hand, long definitions including toomany non-essential differentiae also give rise to erroneous clustering.
We notice thatthe definitions of such senses have been radically changed and made more specificin the third edition of the LDOCE.
The reason behind the changes may be that thesesense definitions are also difficult for humans to grasp.Metonymic senses sometimes lead to problems for the proposed algorithms.TopSense successfully puts start.l.n.3 (a piece of metal in this shape for wearing as a mark3 It seems that precision may be lower if TopSense is run on the unlabeled entries, but we suspect thedifference isvery small.79Computational Linguistics Volume 24, Number 1Table 13Evaluation of the TopSense algorithm.Headword #of Definitions Labeling with Expanded Candidate Setin LDOCE Correct Incorrect Unknown Applicability Precisionbass 5 5 0 0 100% 100%bow 5 5 0 0 100% 100%cone 3 3 0 0 100% 100%country 5 5 0 0 100% 100%crane 2 2 0 0 100% 100%duty 2 2 0 0 100% 100%galley 4 4 0 0 100% 100%interest 6 4 2 0 100% 67%issue 8 3 2 3 63% 60%mole 3 3 0 0 100% 100%plant 6 5 1 0 100% 83%position 10 9 1 0 100% 90%sentence 2 2 0 0 100% 100%slug 5 5 0 0 100% 100%space 8 7 1 0 100% 88%star 9 8 1 0 100% 90%suit 6 5 1 0 100% 83%table 7 6 1 0 100% 86%tank 3 3 0 0 100% 100%taste 6 5 0 1 83% 100%total 105 91 10 4 96% 90%of office, rank, honour, etc.)
in the Dg class (Clothes and personal belongings).
On the otherhand, the metonymic meaning, Nb (Chance) of another star sense (a heavenly body re-garded as determining one's fate) comes out second to the "primary" sense, La (Heavenlybody).
By considering cue phrases uch as regarded as or as a mark of, we might be ableto handle metaphoric and metonymic senses more successfully.Krovetz (1992) observes that the LDOCE indicates explicit sense shifts via the de-ictic reference, which is a link to the previous ense created by such terms as this, these,that, those, its, itself, such a, and such an.
The author identifies many systematic senseshifts indicated by such references including Substance/Product (lemon, tree or fruit),Substance/Color (jade, amber), Object/Shape (pyramid), Animal/Food (chicken), Count-noun/Mass-noun (blasphemy), Language/People (Spanish), Animal/Skin-fur (crocodile),and Music/Dance (waltz).
Such shifts indicated through a deictic reference are so per-vasive in the MRD that they show up more than once in our small 20-word testset.
For instance, the LDOCE sense issue.l .n.2 (an example of this) indicates a Count-noun/Mass-noun shift from its previous ense issue.l .n.1 (the act of coming out) throughthe deictic reference of this.
Since these specific patterns of definition are not taken intoconsideration i TopSense, the algorithm often fails in such cases.
Further work mustbe undertaken to cope with direct and deictic references, o that such definitions canbe appropriately clustered.5.2 Clustered Definit ions and Examples as a Knowledge Source for WSDMany studies have shown that MRD definitions and example sentences are a goodknowledge source for WSD.
As described in the introduction, Lesk (1986) shows thatdefining words are especially effective for disambiguating senses strongly associated80Chen and Chang Topical ClusteringTable  14Analysis of failure by error types.Error Type TopSense Output Sense Definitionvague definitionlong definitionmetonynymshort, vague definition*Gd (communicating)*Ca (people)* La (universe)*Ge (communication)*Bj (medicine)- (unknown)- (unknown)deictic reference *Hb (object)- (unknown)i n te res t  - an activity, subject, etc., which onegives time and attention totab le  - also multiplication table; a list whichyoung children repeat o learn what numberresults when a number from 1 to 12 is mul-tiplied by any of the numbers from 1 to 12star - a heavenly body regarded as determin-ing one's fatesu i t  - a set (of armour)i n te res t  - a readiness to give attentioni s sue  - the act of coming outi s sue  - something which comes or is givenoutspace  - a quantity or bit of this for a particularpurposei s sue  - an example of thiswith specific collocations, such as cone in ice-cream cone and pine cone.
Wilks et al(1990) call the defining words in the LDOCE definition semant ic  p r imi t ives  (SP) andsuggest hat a semantic network constructed on the strength of co-occurrence of SPsin definitions can be useful for a variety of NLP tasks, ranging from WSD, to machinetranslation, to message understanding.
Along the same lines, Luk (1995) terms SPthe def in i t ion -based  concept  (DBC) and proposes using DBC co-occurrence (DBCC)trained on a large corpus to disambiguate word senses.
However, the effectivenessof SPs or DBCs to represent a word sense and its indicative context is hamperedby ambiguity and data sparseness.
For instance, earth, one of the SPs in bank.
l .n.2is ambiguous (either as the planet Earth or soil) thus possibly leading to problems inWSD.
Although these SPs are drawn from a small, controlled vocabulary in mostMRDs, nevertheless, it is difficult to find SPs of a polysemous ense overlapping theSPs of its context.
For instance, consider the problem of disambiguating the word bankin the context of an LDOCE example, He sat down and rested on a mossy bank in the woods.When working on the level of the SPs of an individual MRD sense, we are hardpressed to find a match between the SPs of the intended sense:SP(bank.l .n.2) = {earth, heap, field, garden, make, border, division}and the SPs of its context:SP(sit)SP(rest)SP(moss)SP(wood.l .n.1)SP(wood.l .n.2)= {rest, position, upper, body, upright, support, bottom, chair, seat},= {take, rest},= {small,flat, green, yellow,flowerless, plants, grow, thick,furry, wet,soil, surface},= {material, trunk, branch, tree, cut, dry,form, burn, paper,furniture},= {place, tree, grow, small,forest}.The clusters of MRD senses produced by TopSense give us an advantage in thisrespect.
By matching the context against he clustered semantic primitives (CSP) of the81Computational Linguistics Volume 24, Number 1related senses, we have a better chance of a match.
For instance, the following CSPsof the relevant bank senses contains more words, therefore are more likely to recur inthe SPs of contextual words:CSP(bank-Ld) = SP(bank.l.n.1) USP(bank.l.n.2) USP(bank.l.n.4)U SP(bank.l.n.5)= {hand, side, river, stream, lake, earth, heap, field, make, border,division, slope, bend, road, race-track, safe, car, go round,sandbank}If data sparseness till gets in the way, as in the case of this example, one can goone step further and adopt a class-based approach.
Under such an approach, the SPs ofthe context are matched against he SPs of a class of senses related to the polysemoussense in question.
To this end, we can make use of the topical clusters of MRD sensesproduced by TopSense.
By taking the collective defining terms of all the senses in atopical cluster, we obtain the virtual document of SPs described in Section 4.1.
To copewith the problem caused by ambiguous SPs, it is a good idea to weight erms accordingto tf and idf, as in the TopSense algorithm.
Under such a class-based approach, we willbe matching the contextual information against he unweighted or weighted terms ina class relevant to the intended sense.
For instance, to resolve the sense of bank in theabove example to the Ld sense, we look for a match of contextual information withVLd.VLd = CSP(bank-Ld) t3 CSP(forest-Ld) UCSP(valley-Ld) U.--= {land, side, river, stream, lake, earth, heap, field, make, border, division,slope, bend, road, race-track, safe, car, go round, sandbank, large,area, land, thick, cover, tree, bush, grow, wild, plant,purpose .
.
.
.
}Vcd = {sea (65.81),land (55.39),mountain (46.74),water (44.76),river (36.71),lake (27.88), earth (25.76), tree (21.87) .
.
.
.
}Notice that for this example, the relevant VD is now large enough to overlap thecontextual information; the term tree appears in SP (wood.l.n.1) as well as the relevantdocument VLd.
Although the relevant VLd is very large, it contains mostly words thatare nevertheless consistently related to geography.5.3 Systematic Sense ShiftOstler and Atkins (1991) contend that there is strong evidence to suggest that a largepart of word sense ambiguity is not arbitrary but follows regular patterns.
Moreover,gaps frequently arise in dictionaries and thesauri in specifying this kind of polysemy.Encoding regularity of the extended usage of a sense makes it possible to resolveword sense ambiguity for word entries that are underspecified in this respect.
This so-called virtual polysemy can be illustrated through some examples.
For instance, manyverbs for moving and action, such as move and strike, can be used polysemously in thesense of emotion.
Chodorow, Byrd, and Heidom (1985) observe that many instancesof intersense relations can be found in W7 that are not idiosyncratic, but rather existamong senses of many words.
Those relations include Process/Result, Food/Plant, andContainer/Volume.
Virtual polysemy and recurring intersense relations are closelyrelated to polymorphic senses that can support coercion in semantic typing underPutstejovsky's (1991) theory of the generative l xicon.82Chen and Chang Topical ClusteringDolan (1994) maintains the position that intersense relations are mostly idiosyn-cratical, thereby making it difficult to characterize them in a general way so as toidentify them.
The author cites the example of two senses of to moult, one a bird be-havior and the other an animal behavior, to stress that polysemy primarily reflects finedistinctions that do not recur systematically throughout the English lexicon.
However,our experimental results indicate that (a) it is exactly senses with fine distinction thatare merged together and (b) there is a greater concentration of recurring intersenserelations emerging from condensed senses.
For instance, the distinction between thebird and animal behavior of moulting would be eliminated, since both are likely to beclustered and labeled as Ha (Making things) by TopSense.
Relations among senses inthe same topical clusters are mostly systematic.
Many of those relations are reflectedin the cross-reference information in the LLOCE.
For instance, the LLOCE lists thefollowing cross-references for the topic of Eb (Food):Ac (Animals~Mammals),Ad (Birds),Af (Fish and other water creatures),Ah (Parts of animal),Ai (Kinds of parts of plants),Aj (Plant in general),Jg (Shopkeepers and shops elling food).Most of those cross-references are systematic intersense relations imilar to theabovementioned Food/Plant relation.
Indeed, words involved in such intersense r la-tions are frequently underspecified.
For instance, chicken is listed under both topic Eband topic Ad, while duck is listed under Ad but not Eb.By characterizing some 200 cross-references in LLOCE, most systematic sense shiftscan be easily identified among the senses across topical clusters.
The topical clusters ofMRD senses, coupled with the topical description of sense-shift knowledge, can sup-port and realize automatic sense extension, as advocated in Putstejovsky and Bouillon(1994), and prevent a proliferation of senses in the semantic lexicon.
For instance, thesense of duck in the Ad cluster can be coerced into an Eb sense, in some context, basedon the knowledge of a systematic sense shift from Ad (Birds) to Eb (Food).6.
Other ApproachesSanfilippo and Poznanski (1992) propose a so-called ictionary correlation kit (DCK)in a dialogue-based nvironment for correlating word senses across a pair of MRDssuch as the LDOCE and the LLOCE.
The approach taken in DCK is essentially aheuristic one, based on a correlation i  the headwords, grammar codes, definition, andexamples between the senses in LDOCE and LLOCE.
The authors indicate that for theheuristics to yield optimum results, the degree of overlap in the examples hould beweighted twice as heavily as all other factors.
However, they do not elaborate on howthe comparisons are done, or on how effective the program is.Dolan (1994) describes a heuristic approach to forming unlabeled clusters of closelyrelated senses in an MRD.
The clustering program relies on LDOCE domain code,grammar code, and 25 types of semantic relations extracted from definitions uch asHypernym, Location, Manner, Purpose, PartOf, and IngredientOf.
Matching two senses83Computational Linguistics Volume 24, Number 1involves comparing any values that have been identified for each of the semantic re-lation types.
The author reports that straightforwardly comparing the values of thesame semantic relation types, particularly the Hypernym relation, for two senses wouldbe quite effective.
In addition to such a comparison, a number of "scrambled" com-parisons between values of different ypes of semantic relations are also helpful.
Forinstance, in comparing the two senses of coffee, the value "drink" in the sense, "thecoffee as a drink" is compared with that of the IngredientOf relation in another sense,"the powder as an ingredient of the drink.
"Yarowsky (1992) describes a WSD method and an implementation based on Ro-get's Thesaurus and a very large corpus, the 10-million-word Grolier's Encyclopedia.
Hesuggests that the method can be applied to disambiguation a d merging of MRD deft-nitions as well, and gives the results of applying the method to the senses of the wordcrane for the COBUILD and Collins dictionaries using Roget's categories as an example.It is not known how the method fares for words other than crane.
Contrary to ourapproach, the method requires ubstantial data for training.In most of the above-mentioned works, experimental results are reported only forsome senses of a few words.
In this study, we have evaluated our method using allsenses for 20 words that have been studied in WSD literature.
This evaluation providesan overall picture of the expected success rate of the method when applied to all wordsenses in the MRD.
Direct comparison of methods is often difficult, but it is clear that,as compared to other methods discussed above, our algorithm is very simple, requiresminimal preprocessing, and does not rely on information idiosyncratic to the MRD,such as the LDOCE subject code or grammar code.
Thus, the algorithm describedin this paper can be readily applied to other MRDs besides LDOCE.
Although ouralgorithm makes use of defining words in various semantic relations with the sense,those relations need not be explicitly computed through an elaborated parsing andextraction process.Finally, it is interesting to compare our method with some aspects of the programfor induction of sense division of Sch/.itze (1992).
As mentioned in the introduction,the program uses distributional similarity of lexical co-occurrence to partition wordinstances into clusters that are likely to be related to sense division.
Drawing on thework of latent semantic indexing in IR research, words and contexts are represented asvectors in a multidimensional space.
Regression techniques of singular value decom-position are used to reduce the representation to a lower dimensional space.
After that,sense division is derived through unsupervised clustering of these word instances.
Ourmethod, on the other hand, relies primarily on co-occurrence in an existing set of top-ical clusters, the topics in LLOCE or Roget's.
The sense in question is simply mergedto the nearest opical cluster.
Low-cost distance calculation is done according to theoverlap between words in a definition and a topical cluster.7.
Conclusions and Future WorkThis paper presents the issues of WSD using machine-readable dictionaries.
It de-scribes simple but effective algorithms for disambiguating and clustering dictionarysenses to create a sense division for WSD.
The proposed algorithms are effective forspecific linguistic reasons.
Although word sense is an abstract concept hat relies onthe subjective and subtle distinction of many factors, coarse word sense division canbe attributed primarily to the subject and topic.
This is evident from the observationthat very topical genus and differentiae show up in dictionary definitions in ratherrigid patterns.
Therefore, an MRD coupled with a thesaurus organized according tosubjects and topics is very effective for acquisition of sense division for WSD.84Chen and Chang Topical ClusteringIn a broader context, this paper presents an approach to automatic onstruction ofsemantic lexicons through integration of lexicographic resources uch as MRDs andthesauri.
As noted in Dolan (1994), it is possible to run a sense-clustering algorithmon several MRDs to build an integrated lexical database with more complete coverageof word senses.
If TopSense is run on several bilingual MRDs, there is a potentialfor creating an integrated multilingual exicon enriched with thesaurus concepts aslanguage-neutral signs to support knowledge-based machine translation.
A similaridea has been put forward by Okumura and Hovy (1994).The TopSense algorithm's performance could definitely be improved by handlingdeictic, metonymic, and metaphoric sense definitions more appropriately.
Neverthe-less, the algorithm already produces clustered MRD sense entries that not only areexploitable as a workable sense division but also are likely to be an effective knowl-edge source for many NLP tasks related to semantic processing, such as WSD.
Insummary, this paper presents a functional core for automatic onstruction of the se-mantic lexicon.AppendixThe following table shows the experimental results of running TopSense on the LDOCEsenses in a test set of 20 highly polysemous words.bassTopical Clustering Definition Sentences Applicability Precision?
Eb (food) ?
any of many kinds of fresh-water 100% 100%or salt-water fish that have pricklyskins and that can be eaten.?
the lowest part in written music.
?
Gd(communicating)?
Kb (music)?
Kb (music)?
Kb (music)?
the lowest male singing voice.?
a deep voice.?
= DOUBLE BASS.bowTopical Clustering Definition Sentences Applicability Precision?
Dg (clothes ?
a knot formed by doubling a line 100% 100%and personal into 2 or more round or curvedbelongings) pieces, and used for ornament in?
Hc (substancesand materials)?
Kb (music)?
Ma (moving)?
Mf (shipping)the hair, in tying shoes, etc.?
a piece of wood held in a curveby a tight string and used forshooting arrows.?
a long thin piece of wood with atight string fastened along it,used for playing musical?
instruments that have strings.?
a bending forward of the upperpart of the body to show respector yielding.?
the forward part of a ship.85Computational Linguistics Volume 24, Number 1coneTopical Clustering Definition Sentences Applicability Precision?
Aj (plants)?
Hb (objects)?
Hf (containers)?
the fruit of a PINE or FIR,consisting of several partlyseparate seed-containing pieceslaid over each other, shapedrather like this.?
a hollow or solid objectshaped like this.?
a solid object with a round baseand a point at the top.100% 100%countryTopical Clustering Definition Sentences Applicability Precision?
Ld (geography)?
Ce (organization)?
Ce (organization)?
Ld (geography)?
Ld (geography)?
a nation or state with its land orpopulation.?
the nation or state of one's birth orcitizenship.?
the people of a nation or state.?
land with a special natureor character?
the land outside cities or towns;land used for farming or left unused.100% 100%craneTopical Clustering Definition Sentences Applicability Precision?
Hd (equipments)?
Ad (birds)?
a machine for lifting andmoving heavy objects by means of avery strong rope or wire fastened to amovable arm (JIB).?
a type of large tall bird withvery long legs and neck, whichspends much time walking in watercatching fish in its very long beak.100% 100%dutyTopical Clustering Definition Sentences Applicability Precision?
Jf (commerce)?
Jh (work)?
any of various types of tax.?
what one must do either becauseof one's job or because one thinksit right100% 100%86Chen and Chang Topical ClusteringgalleyTopical Clustering Definition Sentences Applicability Precision?
Gd ?
a long flat container used by a 100% 100%(communicating) printer to hold the letters (TYPE)which have been arranged forthe first stage of printing.?
=GALLEY PROOF.
?
Gd(communicating)?
Mf (shipping) ?
a ship which was rowed alongby slaves.?
Mf (shipping) ?
a ship's kitchen.interestTopical Clustering Definition Sentences Applicability Precision?
*Bj (medicine) ?
a readiness to give attention.
100% 67%?
*Gd ?
an activity, subject, etc., which(communicating) one gives time and attention to.?
Je (banking) ?
advantage, advancement, or favour(esp.
in the phrs.
in the interest of(something)/ in someone's interest).?
money paid for the use of money.?
a share (in a company, business, etc.?
a quality of causing attention tobe given.?
Je (banking)?
Jf (commerce)?
Na (being,becoming)issueTopical Clustering Definition Sentences Applicability Precision?
- (unknown) ?
the act of coming out.
63% 60%?
- (unknown) ?
something which comes or isgiven out.?
an important point.?
old use and law children (esp.in the phr.
die without issue).?
the act of bringing out somethingin a new form.?
an example of this.?
- (unknown)?
Ca (people)?
*Ck(courts of law)?
*C1 (police,crime)?
Gd(communicating) ?
something, esp.
something printed,brought out again or in a new form.?
Nf (causing) ?
the result.87Computational Linguistics Volume 24, Number 1moleTopical Clustering Definition Sentences Applicabil ity Precision?
Ac (animals) ?
a small, dark brown, slightly raised 100% 100%mark on a person's kin, usu.
theresince birth.?
Hc (specificsubstances andmaterials)?
Ld (geography)?
a type of small insect-eating animalwith very small eyes and soft darkfur, which digs holes and passage under-ground and makes its home in them.?
a stone wall of great strength builtout into the sea from the land as adefense against the force of the waves,or to act as a road.plantTopical Clustering Definition Sentences Applicabil ity Precision?
Ai (plants) ?
a living thing that has leaves and 100% 83%roots, and grows usu.
in earth, esp.the kind smaller than trees.?
a machine; apparatus.?
a factory 0.?
machinery.?
Hd (equipment)?
Id (industry)?
*Md (vehicles)?
Ce (organization ingroups)?
C1 (crime)?
a person who is placed in a groupof people thought to be criminals inorder to discover facts about them.?
a thing, esp.
stolen goods,hidden on a person so that hewill seem guilty.138Chen and Chang Topical ClusteringpositionTopical Clustering Definition Sentences Applicabil ity Precision?
Me (places) ?
the place where someone or 100% 90%something is or stands, esp.
inrelation to other objects,places, etc.?
the place where someone orsomething is (in the phr.
inposition).?
the place where someone orsomething is supposed to be; theproper place.the place of advantage in astruggle (in the phrs.
manoeuvre /jockey for position).the way or manner in whichsomeone or something is placed ormoves, stands, sits, etc.?
a condition or state, esp.
in relationto that of someone or something else.?
a particular place or rank in a group.?
high rank in society, government,  orbusiness.?
a job; employment.?
an opinion or judgment on a matter.?
Me (places)?
Me (places)?
Cn (fighting) ??
Ma (moving) ??
*Ca (people)?
Ci (classifications)?
Cf (government)?
Jh (work)?
Ga (thinking)sentenceTopical Clustering Definition Sentences Applicability Precision?
Ck (courts of law) ?
a punishment for a criminal found 100% 100%guilty in court.?
Gd ?
a group of words that forms a(communicating) statement, command,  EXCLAMATION,or question, usu.
contains a subjectand a verb, and (in writing) beginswith a capital letter and ends withone of the marks ".!?
"89Computational Linguistics Volume 24, Number 1slugTopical Clustering Definition Sentences Applicability Precision?
Ab (living ?
any of several types of small 100% 100%creatures) limbless plant-eating creature, relatedto the SNAIL but with no shell,that often do damage to gardens.?
Gd ?
a machine-made piece of metal(communicating) with a row of letters along the edgefor printing.?
Hc (specific ?
a machine-made piece of metalsubstances) with a row of letters along the edgefor printing.?
Hd (equipment) ?
a coin-shaped object unlawfullyput into a machine in placeof a coin.?
Hh (weapons) ?
a bullet.spaceTopical Clustering Definition Sentences Applicability Precision?
Jc (measurement) ?
something limited and measurable 100% 88%?
*Hb (objects)?
La (universe)?
La (universe)?
Ld (geography)?
Le (time)?
Gd(communicating)?
Gd(communicating)in length, width, or depth and regardedas not filled up; distance, area, orVOLUME (3); room.?
a quantity or bit of this for aparticular purpose.?
that which surrounds all objects andcontinues outward in all directions.?
what is outside the earth's air; whereother heavenly bodies move.?
land not built on (esp.
in the phr.
openspace).?
a period of time.?
an area or distance left betweenwritten or printed words, lines etc.?
the width of a letter on aTYPEWRITER.90Chen and Chang Topical ClusteringstarTopical Clustering Definition Sentences Applicabil ity Precision?
Dg (personal ?
a piece of metal in this shape for 100% 90%belongings) wearing as a mark of office, rank,honour, etc.?
a 5- or more-pointed figure.?
a famous or very skillful performer.?
STARS.?
a brightly-burning heavenly bodyof great size, such as the sun but esp.one very far away.?
any heavenly body (such as aPLANET) that appears as a brightpoint in the sky.?
a heavenly body regarded asdetermining one's fate.?
a sign used with numbers from usu.1 to 5 in various systems, and in theimagination, to judge quality.?
one's success or fame or chance ofgetting it.?
Jb (mathematics)?
Kd (drama)?
La (universe)?
La (universe)?
La (universe)?
*La (universe)?
La (universe)?
Nb (possibility)suitTopical Clustering Definition Sentences Applicability Precision?
Dg (clothes) ?
a set of outer clothes which 100% 83%match, usu.
including a shortcoat (JACKET) with trousers orskirt.?
a garment or set of garmentsfor a special purpose.?
a set (of armour) (in the phrs.suit of armour/mai l ) .?
one of the 4 sets of cards usedin games.?
fml a request 0.?
Dg (clothes)?
*Ge(communication)?
Kf (indoor games)?
Gc(communicating)?
Cb (courting) old use the act of asking awoman to marry (esp.
in thephrs.
p lead/press  one's suit).91Computational Linguistics Volume 24, Number 1tankTopical Clustering Definition Sentences Applicability Precision?
Hf (containers) ?
a large container for storing 100% 100%liquid or gas.?
Md(vehicles) ?
an enclosed heavily armedarmoured vehicle that moves on2 endless metal belts.?
esp.
Ind & PakE a large man-made pool for storing water.?
Ld(geography)tableTopical Clustering Definition Sentences Applicability Precision?
Hb (objects) ?
a piece of furniture with a flat top 100% 86%supported by one or more uprightlegs.?
made to be placed and used onsuch a piece of furniture.?
such a piece of furniture speciallymade for the playing of variousgames.?
the food served at a meal.?
the people sitting at a table.?
a printed or written collectionof figures, facts, or informationarranged in orderly rows across anddown the page.?
also multiplication table a list whichyoung children repeat o learn whatnumber results when a number from1 to 12 is multiplied by any of thenumbers from 1 to 12.?
Df (furniture)?
Kf (indoor games)?
Ea (food)?
Ca (people)?
Gd(communicating)o *Ca (people)92Chen and Chang Topical ClusteringtasteTopical Clustering Definition Sentences Applicability Precision?
Bg (bodily states) ?
an experience.
83% 100%?
Ea (food generally) ?
a small quantity of food or drink.?
Eb (food) ?
the special sense by which a personor animal knows one food fromanother by its sweet, bitter,salty, etc.?
the sensation that is producedwhen food or drink is put in themouth and that makes it differentfrom other foods or drinks by itssalty, sweet, bitter, etc.?
the ability to enjoy and judgebeauty, style, art, music, etc.
;ability to choose and use the bestmanners, behaviour, fashions, etc.?
a personal iking for something.?
Eb (food)?
Kb(music)?
- (unknown)AcknowledgmentsThis work is partially supported by ROCNSC grants 84-2213-E-007-023 and NSC85-2213-E-007-042.
We are grateful to BettyTeng and Nora Liu from Longman AsiaLimited for the permission to use theirlexicographical resources for researchpurposes.
Finally, we would like to thankthe anonymous reviewers for manyconstructive and insightful suggestions.ReferencesAgeno, A., I. Castellon, M. A. Marti,G.
Rigau, E Ribas, H. Rodriguez, M.Taule and E Verdejo.
1992.
SEISD: Anenvironment for extraction of semanticinformation from on-line dictionaries.
InProceedings ofthe 3rd Conference on AppliedNatural Language Processing, pages253-254, Trento, Italy.Ahlswede, Thomas and Martha Evens.
1988.Parsing vs. text processing in the analysisof dictionary definitions.
In Proceedings ofthe 26th Annual Meeting, pages 217-224.Association for ComputationalLinguistics.Alshawi, H. 1987.
Processing dictionarydefinitions with phrasal patternhierarchies.
American Journal ofComputational Linguistics, 13(3):195-202.Alshawi, H., B. Boguraev, and D. Carter.1989.
Placing the dictionary on-line.
InB.
Boguraev and Briscoe, editors,Computational Lexicography for NaturalLanguage Processing, Longman, London,pages 41-63.Amsler, Robert A.
1984a.
Machine-readabledictionaries.
Annual Review of InformationScience and Technology, 19:161-209.Amsler, Robert A.
1984b.
Lexical knowledgebases, Panel session on machine-readabledictionaries.
In Proceedings ofthe TenthInternational Congress on ComputationalLinguistics, pages 458--459, Stanford, CA.Amsler, Robert A.
1987.
Words and words.In Proceedings ofthe Third Workshop onTheoretical Issues in Natural LanguageProcessing, pages 7-9, New Mexico StateUniversity at Las Cruces, NM.Brown, P. F., S. A. Della Pietra, V. J. DellaPietra, and R. L. Mercer.
1991.
Word sensedisambiguation using statistical methods.In Proceedings ofthe 29th Annual Meeting,pages 264-270.
Association forComputational Linguistics.Bruce, Rebecca nd Janyce Wiebe.
1995.Word sense disambiguation usingdecomposable models.
In Proceedings ofthe33rd Annual Meeting, pages 139-145.Association for ComputationalLinguistics.Chang, Jason S., J. N. Chen, H. H. Sheng,and S. J. Ker.
1996.
Combining machinereadable lexical resources and bilingualcorpora for broad word sensedisambiguation.
In Proceedings oftheSecond Conference ofthe Association for93Computational Linguistics Volume 24, Number 1Machine Translation, pages 115-124,Montreal, Quebec, Canada.Chen, J. N. and Jason S. Chang.
1994.Towards generality and modularity instatistical word sense disambiguation.
IProceedings ofthe 8th Asian Conference onLanguage, Information and Computation,pages 45--48.Chodorow, Martin S., Roy J. Byrd andGeorge E. Heidom.
1985.
Extractingsemantic hierarchies from a large on-linedictionary.
In Proceedings ofthe 23rd AnnualMeeting, pages 299-304.
Association forComputational Linguistics.Church, K. W. 1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings ofthe 2ndConference on Applied Natural LanguageProcessing, pages 136-143, Austin, TX.Copestake, A.
1990.
An approach tobuilding the hierarchical element of alexical knowledge base from a machinereadable dictionary.
In Proceedings oftheFirst International Workshop on Inheritance inNatural Language Processing, pages 19-29,Tilburg, The Netherlands.Cowie, Jim, Joe Guthrie and Louise Guthrie.1992.
Lexical disambiguation usingsimulated annealing.
In Proceedings ofthe14th International Conference onComputational Linguistics, pages 359-365.Dagan, Ido and Alon Itai.
1994.
Word sensedisambiguation using a second languagemonolingual corpus.
ComputationalLinguistics, 20(4):563-596.Dagan, Ido, Alon Itai, and Ulrike Schwall.1991.
Two languages are more informativethan one.
In Proceedings ofthe 29th AnnualMeeting, pages 130-137.
Association forComputational Linguistics.Dolan, William B.
1994.
Word sensedisambiguation: Clustering related senses.In Proceedings ofthe 15th InternationalConference on Computational Linguistics,pages 712-716.Gale, W., K. W. Church, and D. Yarowsky.1992.
Using bilingual materials to developword sense disambiguation methods.
InProceedings ofthe 4th International Conferenceon Theoretical nd Methodological Issues inMachine Translation, pages 101-112.Guthrie, Louise, Brian M. Slator, YorickWilks, and Rebecca Bruce.
1990.
Is therecontents in empty heads?
In Proceedings ofthe 13th International Conference onComputational Linguistics, pages 138-143.Jenson, Karen and Jean Louis Binot.
1987.Disambiguating prepositional phraseattachments by using on-line dictionarydefinitions.
Computational Linguistics,13(3-4):251-260.Klavans, J. L., M. S. Chodorow, and N.Wacholder.
1990.
From dictionary toknowledge via taxonomy.
In Proceedings ofthe Sixth Conference ofthe University ofWaterloo Centre for the New Oxford EnglishDictionary and Text Research: Electronic TextResearch, University of Waterloo,Waterloo, Canada.Krovetz, Robert.
1992.
Sense-linking in amachine readable dictionary.
InProceedings ofthe 30th Annual Meeting,pages 330-332.
Association forComputational Linguistics.Krovetz, R. and W. B. Croft.
1992.
Lexicalambiguity and information retrieval.ACM Transaction on Information Systems,pages 115-141.Lesk, Michael E. 1986.
Automatic sensedisambiguation using machine-readabledictionaries: How to tell a pine cone froman ice-cream cone.
In Proceedings oftheACM SIGDOC Conference, Toronto,Ontario, pages 24-26.Proctor, P., editor.
1978.
Longman Dictionaryof Contemporary English.
Longman Group,London.Luk, Alpha K. 1995.
Statistical sensedisambiguation with relatively smallcorpora using dictionary definitions.
InProceedings ofthe 33rd Annual Meeting,pages 181-188.
Association forComputational Linguistics.McArthur, Tom.
1992.
Longman Lexicon ofContemporary English.
Longman Group(Far East) Ltd., Hong Kong.McRoy, S. 1992.
Using multiple knowledgesources for word sense discrimination.Computational Linguistics, 18(1):1-30.Miller, G. A., R. Beckwith, C. Fellbaum,D.
Gross, and K. Miller.
1993.
Introductionto WordNet: An on-line lexical database.CSL 43, Cognitive Science Laboratory,Princeton University, Princeton, NJ.Montemagni, S. and L. Vanderwende.
1992.Structural pattern vs. string pattern forextracting semantic information fromdictionaries.
In Proceedings ofthe FifteenthInternational Conference on ComputationalLinguistics, pages 546-552.Ng, Hwee Tou and Hian Beng Lee.
1996.Integrating multiple knowledge sourcesto disambiguate word sense: Anexamplar-based approach.
In Proceedingsof the 34th Annual Meeting, pages 40-47,Santa Cruz, CA.
Association forComputational Linguistics.Okumura, A. and Eduard Hovy.
1994.Lexicon-to-ontology concept associationusing a bilingual dictionary.
In Proceedingsof the First Conference ofthe Association forMachine Translation in the Americans,94Chen and Chang Topical Clusteringpages 177-184.
Columbia, MD.Ostler, Nicholas and B. T. S. Atkins.
1991.Predictable meaning shift: Some linguisticproperties of lexical implication rules.
InProceedings ofthe 1991 ACL Workshop onLexical Semantics and KnowledgeRepresentation, pages 76-87.Putstejovsky, James.
1991.
The generativelexicon.
Computational Linguistics,(17)4:409-441.Putstejovsky, James and Pierrette Bouillon.1994.
On the proper role of coercion insemantic typing.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics, pages 706-711.Ravin, Yael.
1990.
Disambiguating andinterpreting verb definitions.
InProceedings ofthe 28th Annual Meeting,pages 260-267.
Association forComputational Linguistics.Roget's Thesaurus of English Words and Phrases.1987.
Longman Group UK Limited,London.Sanfilippo, A. and V. Poznanski.
1992.
Theacquisition of lexical knowledge fromcombined machine-readable dictionarysources.
In Proceedings ofthe 3rd Conferenceon Applied Natural Language Processing(ANLP-92), pages 80-87, Trento, Italy.Sch~tze, Hinrich.
1992.
Word sensedisambiguation with sublexicalrepresentations.
In Proceedings ofthe 1992AAAI Workshop on Statistically-basedNatural Language Programming Techniques,pages 100-104.Vanderwende, L. 1994.
Interpretation ofnoun sequences.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics, pages 454-460.Vossen, P., W. Meijs, and M. den Broeder.1989.
Meaning and structure in dictionarydefinitions.
In Computational Lexicographyfor Natural Language Processing.
BranimirBoguraev and Ted Briscoe, editors,Longman Group UK Limited, London,pages 171-190.Webster's Seventh New Collegiate Dictionary.1967.
C. and C. Merriam Company,Springfield, MA.Wilks, Y.
A., D. C. Fass, C. Ming Guo,J.
E. McDonald, T. Plate, and B. M. Slator.1990.
Providing tractable dictionary tools.Machine Translation, 5:99-154.Witten, Ian H., Alistair Moffat, and TimothyC.
Bell.
1994.
Managing Gigabytes.
VanNostrand Reinhold, New York.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoget's categories'trained on largecorpora.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics, pages 454-460, Nantes, France.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting, pages 189-196.
Association forComputational Linguistics.Zernik, Uri.
1992.
Train1 vs. Train2: Taggingword senses In corpus, hi U. Zernik,editor, Lexical Acquisition: ExploitingOn-line Resources to Build a Lexicon.Lawrence Erlbaum Associates,pages 91-112.95
