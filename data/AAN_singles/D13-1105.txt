Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032?1043,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatic Extraction of Morphological Lexiconsfrom Morphologically Annotated CorporaRamy Eskander, Nizar Habash, Owen RambowCenter for Computational Learning SystemsColumbia University{reskander,habash,rambow}@ccls.columbia.eduAbstractWe present a method for automatically learn-ing inflectional classes and associated lem-mas from morphologically annotated corpora.The method consists of a core language-independent algorithm, which can be opti-mized for specific languages.
The method isdemonstrated on Egyptian Arabic and Ger-man, two morphologically rich languages.Our best method for Egyptian Arabic pro-vides an error reduction of 55.6% over a sim-ple baseline; our best method for Germanachieves a 66.7% error reduction.1 IntroductionMorphological lexicons specify all inflected formsfor each lexeme; in a language with rich morphol-ogy, such a resource can be important for natural lan-guage processing (NLP) tasks in order to limit datasparseness.
For example, a morphological lexicon isan important component of a morphological taggeror of a part-of-speech (POS) tagger for languageswith rich morphology.1 Traditionally, a morpholog-ical lexicon has been created through painstakinglexicographic and morphological analysis of the lan-guage, drawing on unannotated corpora.
Recently,new approaches have emerged.
Fully or largely un-supervised approaches cannot link surface forms tomorphosyntactic features and are thus not suited forbuilding morphological lexicons.
This problem isovercome by approaches that use explicit linguisticknowledge.
In this paper, we investigate using exist-ing morphologically annotated corpora.
In a mor-phologically annotated corpus, the words in natu-rally occurring texts or transcribed speech are anno-tated for the correct morphological analysis (includ-1Note that a full-form morphological lexicon is functionallyequivalent to a morphological analyzer-generator, since one canbe used to create the other.ing, of course, core POS) in context.
While therehas been much work on computational morphology,to our knowledge this is the first paper to study thequestion of how to extract morphological lexiconsfrom morphologically annotated corpora, and howto determine how much annotation is needed.
Inthis paper, we assume a corpus with each word an-notated with morphosyntactic features and with alemma which tells us what lexeme the word formis part of.
The task is to predict the correspondencebetween a word form and its lemma and morpholog-ical features.This paper makes two contributions.
First, weintroduce an algorithm that learns unseen forms byanalogy.
This algorithm is language-independent.
Itincrementally merges complementary paradigm in-formation about different lexemes into more abstractand more informative inflectional classes.
Second,we explore how to model stems, and we proposea generalization of the Semitic root-and-templatemodeling.
We use Egyptian Arabic (EGY), and Ger-man (GER) as our test languages.
We test on corpusdata, in order to simulate a standard real-world ap-plication.
The baseline just uses the word forms seenin training and does not predict any unseen forms.Our language-independent algorithm improves theperformance for both EGY and GER, with error re-ductions over the baseline of 44.4% for EGY and of66.7% for GER.
By adding language-specific mod-eling of the stem using templates, we obtain furthererror reductions for EGY (up to 55.6%) but not forGER.Next, we review related work (Section 2) andintroduce the key linguistic concepts we use (Sec-tion 3).
We present our basic language-independentmethod in Section 4, and our language-specific mod-eling of stem variation in Section 5.10322 Related WorkApproaches to Morphological Modeling Muchwork has been done in the area of computationalmorphology ranging from systems painstakingly de-signed by hand (Koskenniemi, 1983; Buckwalter,2004; Habash and Rambow, 2006; De?trez andRanta, 2012) to unsupervised methods that learnmorphology models from unannotated data (Creutzand Lagus, 2007; Monson et al 2008; Ham-marstro?m and Borin, 2011; Dreyer and Eisner,2011).
There is a large continuum between these twoapproaches.
Closer to one end, we find work on min-imally supervised methods for morphology learningthat make use of available resources such as paral-lel data, dictionaries or some additional morpholog-ical annotations (Yarowsky and Wicentowski, 2000;Cucerzan and Yarowsky, 2002; Neuvel and Fulop,2002; Snyder and Barzilay, 2008).
Closer to theother end, we find work that focuses on definingmorphological models with limited lexicons that arethen extended using raw text (Cle?ment et al 2004;Forsberg et al 2006).
The work presented in thispaper falls in the middle of this continuum: we areinterested in learning complete morphological mod-els using rich morphological annotations and, op-tionally, limited linguistic knowledge.
We comparethe value of different amounts of annotation and howthey relate to additional linguistic knowledge.Morphological Paradigms Many traditional andmodern theories of inflectional morphology orga-nize natural language morphology by paradigms(Stump, 2001; Walther, 2011; Camilleri, 2011).Within the continuum we discussed above, we findhierarchical representations of paradigm knowledgethat have been used in manually constructed mor-phological models (Finkel and Stump, 2002; Habashet al 2005).
Furthermore, De?trez and Ranta (2012)introduce an implementation of Smart Paradigms?
heuristically organized paradigms minimizing thenumber of forms needed to predict the full paradigmof a particular lexeme.Both Forsberg et al(2006) and Cle?ment et al(2004) describe methods for automatically populat-ing a lexicon from raw data given a set of morpho-logical inflectional classes in a language.
Our workdiffers in that we use annotated data, but do not startwith a complete set of inflectional classes; thus, ourwork is exactly complementary to this work.The concept of a paradigm is also used in manypublished efforts on unsupervised learning of mor-phology, although not always in a way consistentwith its use in linguistics.
For instance, Snover etal.
(2002) (and later on Can and Manandhar (2012))define a paradigm as ?a set of suffixes and the stemsthat attach to those suffixes and no others?.
Thisdefinition is quite limited since it is not modelingthe notion of lexeme.
Chan (2006) defines a simplerconcept of paradigms in his probabilistic paradigmmodel, which has many limitations, such as not han-dling syncretism or irregular morphology, nor dis-tinguishing inflection and derivation.Dreyer and Eisner (2011) learn complete Ger-man verb paradigms from a small set of completeseed paradigms (50 or 100), which they choose ran-domly from all verbs in the language.
They modelstem changes using letter-based models, and usea large unannotated corpus in addition to the seedparadigms.
Durrett and DeNero (2013) attack thesame problem as Dreyer and Eisner (2011).
Insteadof using unannotated text, they model explicit rulesfor affixes and stem changes.
The major differencebetween these two efforts and our work is that theproblem is defined differently: we assume that thetraining and test data is defined by a corpus, notby complete paradigms.
Our methods therefore aremore sensitive to frequency effects of tokens.
Webelieve that our way of stating the problem is morerelevant to actual computational challenges for lan-guages with limited morphological resources.
Weempirically compare our approach to that of Durrettand DeNero (2013) in Section 5.None of the unsupervised approaches mentionedmodel inflectional classes, i.e., meta-paradigmaticrepresentation that cluster the various paradigms ofdifferent lexemes into a set of general classes ofparadigms.
There is no explicit notion of mor-phosyntactic features.
In this paper we target thelearning and completion of inflectional classes frommorphologically annotated data.
Our approach doesnot sacrifice details of what paradigms should in-clude: we handle syncretism and stem changes, andallow for the prediction of new word forms frommorphosyntactic features and lemmas, unlike thelargely unsupervised work.
Our work also differsfrom most previous work in that we investigate howto model stem change explicitly.
Whereas otherapproaches model stem syncretism through letter-1033based models (Yarowsky and Wicentowski, 2000;Neuvel and Fulop, 2002; Dreyer and Eisner, 2011),we explore the use of abstract stems.In our previous work on the EGY morphologicalanalyzer CALIMA, we similarly used a lexicon ofannotated morphological forms and extended it au-tomatically using a simpler approach to paradigmcompletion (Habash et al 2012).3 Linguistic TerminologyIn this section, we review key concepts from mor-phology, and introduce the terminology we will usein this paper.2 We then introduce our own formal-ization of stems using vocalic templates.Morphology is the study of word forms and theirdecomposition into elementary morphemes, whichare the smallest meaning-bearing units of a lan-guage.
There are two types of morphological pro-cesses: inflectional and derivational morphology.
Ininflectional morphology, a core meaning is retainedand different word forms reflect different types ofmorphosyntactic features such as person, number, ortense.
In derivational morphology, the core meaningof a word is changed, and perhaps even its part-of-speech (POS).
In this paper, we restrict our interestto inflectional morphology.
Furthermore, we takethe written form of the word to be primary, and baseall morphological analyses on the written form.We will refer to the set of all word forms thatare related through inflectional morphology alone aslexeme.
We can refer to a lexeme with a lemma,which we take to be a language-specific and conven-tionalized choice of one of the inflected forms.
Forexample, in English the verb is conventionally citedin the infinitive (often with to, which can be omit-ted), while in Arabic it is conventionally cited in per-fective third person masculine singular.
The lemmais sometimes referred to as a ?citation form?.
Aparadigm of a lexeme is a list of cells, where a cellis a combination of a complete set of morphosyn-tactic features (properties) and the corresponding in-flected form of the lexeme.
A paradigm is com-plete if there are cells for all possible morphosyn-tactic features (the list of possible morphosyntacticfeatures is of course language-dependent).We can divide the word forms into affixes (i.e.,prefixes and suffixes) and the stem.
There is no sin-2We (roughly) base our terminology and our conceptualiza-tion on the inferential-realizational theory of Stump (2001).gle correct way to do this for the words of a lan-guage.
Each lexeme has its own paradigm.
Wecan abstract from paradigms by grouping togetherparadigms which share the same affixes in corre-sponding cells, and where stems in correspondingcells differ in some restricted manner.
We can de-fine the inflectional class (IC) more formally as aset of abstract cells, where an abstract cell is a com-bination of a complete set of morphosyntactic fea-tures (properties) and an abstract representation of astem (an abstract stem) along with fully specifiedaffixes.
The abstract stems (and thus the abstractcells) must have the property that, given a single in-stantiated word form of a lexeme along with its as-sociated IC, we can derive the complete paradigmof the lexeme deterministically.
In the first resultswe present, we simply assume that the stem is ei-ther shared entirely with other abstract cells, or it isentirely lexically instantiated.
We explore language-specific approaches to defining an abstract stem inSection 5, where we also discuss relevant morpho-logical facts of EGY and GER.Prefixes, suffixes, and stems can be the same fordifferent cells of a single paradigm or IC.
This iscalled syncretism.
We will refer to the cells whichshare a stem as a stem syncretism zone or ?zone?for short.4 Language-Independent InflectionalClass Construction AlgorithmIn this section, we present our language-independentIC construction algorithm (LICA).
LICA consists ofa core algorithm for building ICs from seen data andmodels of soft-stem syncretism and affix prediction.4.1 Problem DefinitionStarting with a corpus of words annotated as triplesof ?prefix+stem+suffix, lemma, features?, we wantto create a lexicon of complete ICs, with eachIC having an associated set of lemmas.
Thefollowing is an input example specifying the in-flected form of the 3rd person plural imperfec-tive inflection for the EGY lemma katab3 ?write?
:?y+iktib+uwA, katab, I3UP?.3Arabic transliteration throughout the paper is presented inthe Habash-Soudi-Buckwalter scheme (Habash et al 2007).1034IC1IC3IC2IC4CS = 3CS = 2CS = 1IC1,3IC2IC4CS = 2IC1,2,3IC4IC1L1 ?
L2 ?
L3IC2L4 ?
L5IC3L6 ?
L7IC4L8IC1,3L1 ?
L2 ?
L3L6 ?
l7IC2L4 ?
L5IC4L8IC1,2,3L1 ?
L2 ?
L3L4 ?
L5L6 ?
l7IC4L82 31Figure 1: This graph illustrates two merges that result in combining three ICs in two steps.
The IC cells are representedas solid (filled cells) or blank (empty cells) circles.
CS is the compatibility score marking the number of matchingfilled cells in an IC.
The boxes to the right of the graph represent the lexicon which associates lemmas (L*) with ICs.IC4 is not connected with any of the other ICs, because it has cell values that are incompatible with them.4.2 Core AlgorithmBuilding the Initial Inflectional Classes An ini-tial IC is constructed for each lemma found in theinput corpus using all the triplets involving saidlemma.
Since the lemma itself is an inflected formwith an a priori fixed feature combination, we alsouse the lemma to construct the initial IC, even if itdid not occur as a word form in the corpus.
If allinflected forms of a lemma appear in the trainingdata, the IC will be complete.
However, typicallythere are many unseen forms.
If all seen forms re-lated to one lexeme have the same stem, the abstractstem chosen for the IC is a single stem variable;the abstract cells of the new IC can of course dif-fer in terms of affixes (which are always fully spec-ified rather than represented as variables).
In caseswhen the seen forms of one lemma have more thanone stem, the IC created will simply be the same asthe paradigm, i.e., we have fully instantiated stemsas our abstract stems.
We call such ICs suppletiveICs.
At this point, we have a repository of numerousincomplete ICs and a lexicon consisting of a one-to-one mapping between lemmas and these new ICs.We then identify all ICs that are exactly the same (bydefinition, these are not suppletive ICs), and mergethe associated sets of lemmas.
We now have a newlexicon in which some ICs are associated with morethan one lemma.Constructing the Inflectional Class Graph Weconstruct an IC graph that connects all mergeableICs.
Two ICs are mergeable if neither of the ICsis suppletive, if they share at least one abstract cellfor some morphological feature, and if no morpho-logical features are associated with different abstractcells, i.e., there are no incompatibilities.
Each pointin the graph represents a specific IC, while the edgescarry the following four scores that we use to deter-mine mergeability order:?
The compatibility score is the number of non-empty intersections between the two ICs.?
The originality score is the larger number ofprevious merges of the two ICs.
At the begin-ning, this number is 0 for all edges.?
The completeness score is the size of the unionof non-empty rows from the two ICs?
The lexical size score is the sum of the numberof lemmas associated with the two ICs.The edges of the graph are ranked according to thecompatibility score first (more is better).
Any tiesare broken using the originality score (lower is bet-ter), any remaining ties by the completeness score(more is better) and then by the lexical size score(more is better).
We explored all possible orders oftie breaking using EGY data, and determined the or-der of the listing above to be best.
We use it in allexperiments reported in this paper.Merging the Inflectional Classes While the ICgraph is still connected, we repeat the followingmerging procedure.
Starting with the highest rankededge in the graph, we merge the two ICs connectedby the edge.
In case of multiple edges that areequally highly ranked (after tie breaking), we selectrandomly among them.
The merge creates a new ICthat has the union of the cells in the two ICs.
Thelexicon is adjusted accordingly by associating withthe new IC the union of all the lemmas originally as-sociated with the two ICs.
The new IC inherits theunion of all the IC graph connections of its prede-cessors.
The IC graph edge scores between the new1035IC and other ICs are recalculated.
Graph edges thatbecome incompatible after the merge are removed.The two original ICs are removed from the graphand lexicon.
See Figure 1 for an illustration of themerge process.4.3 Completing Stems and AffixesSoft Stem Syncretism Zones We extend the con-cept of stem-syncretism zones into a statisticalmodel that computes the probability that the abstractstems in two abstract cells are the same given theirfeature combinations, i.e., that they belong to thesame syncretism zone.
We refer to this approachas ?soft?
stem-syncretism zones or ?soft zones?
forshort.
The probabilities are computed for eachfeature-combination pair as the ratio of the times theabstract stem for the feature-combination pair areequal divided by the times the abstract stem for thefeature-combination pair are not empty.
The prob-abilities can be computed after the initial IC graphconstruction or after the merging process has con-cluded; they can also be based on IC type countsor weighted by the number of associated lemmas.Applying soft zones to fully complete the ICs canonly be done after merging is completed.
We try allthe possible alternatives for learning the SZs on theEGY data, including experiments with small train-ing data sizes.
The best accuracy is obtained usingIC type weights learned after the merge.
We use thissetting for all experiments.When applying the soft zones to determine theabstract stems of empty cells, we consider all filledcells in the same IC and select the abstract stem fromthe cell of the feature combination that has the high-est soft-zone probability with the feature combina-tion of the empty cell.
Note that the copied abstractstem can be either a stem variable, or, for a supple-tive IC, a lexical form.Predicting Affixes In building the ICs mentionedabove, some feature combinations and their corre-sponding affixes (prefixes and suffixes) are missingsince they were not in the training data.
We fill themissing affixes in a particular IC i by copying themfrom other ICs that we rank by overall seen affixsimilarity to IC i. ICs with conflicting affixes forany feature combinations are excluded completely.For any remaining missing affix-feature combina-tion, we use the most common affix for that featurecombination over all ICs.4.4 Model ApplicationThe complete ICs produced by the completion algo-rithm, associated with their lemmas in the lexicon,can now be used to predict the surface form of agiven lemma and morphological features.
We usethe following procedure: If the given features arethe same as those used to define the lemma, then thesurface form is the same as the lemma form.
If thelemma was seen in training, we select its IC from themodel and generate the inflection associated with thefeatures.
This may require instantiating a stem vari-able (in case the IC is not suppletive), but this can bedone deterministically from the lemma.If the lemma has not been seen, then we buildan IC on the fly using the lemma, i.e., an IC witha single cell.
We then pick an IC from the modelthat does not conflict with that IC.
The priority isgiven to the IC with the largest lexicon size, whichis likely to be a result of several merges.
If such anIC does not exist, a backup mode returns the stem ofthe lemma associated with the most frequent affixesof the queried features.4.5 Results on Egyptian ArabicData and Metrics We use a morphologically an-notated EGY corpus based on the CALLHOMEEGY (CHE) corpus (Gadalla et al 1997).4 We di-vide the corpus into three parts: training, develop-ment and test, of about 75K, 36K and 41K words,respectively.
We conduct our experiments on verbsonly since they have a large number of possible mor-phosyntactic feature combinations.
The verbs inthese experiments are uncliticized.
Clitics are eas-ily handled using a few orthographic rules (El Kholyand Habash, 2010).
On average, uncliticized verbsare about 12% of all words in our corpus.
The datais represented in triplets as described in Section 4.1.There is a total of 19 feature combinations of as-pect/mood (perfective,imperfective and imperative),person (first, second and third), gender (masculine,feminine and neutral) and number (singular and plu-ral).
Some combinations are invalid such as the firstand third persons with the imperative form.
Thelemma we use is the Arabic citation form for verbs,which is the perfective third person masculine sin-4The corpus was automatically annotated using informationfrom the CHE transcripts (Gadalla et al 1997) and the Egyp-tian Colloquial Arabic Lexicon (Kilany et al 2002).
For moredetails, see Habash et al(2012) and Eskander et al(2013).1036gular (P3MS) inflection.
We use this fact to fill theP3MS cells when building the initial ICs.We evaluate the accuracy of the automaticallygenerated bidirectional lexicon by generating sur-face forms from lemmas and morphosyntactic fea-tures.
We use the model application method de-scribed in Section 4.4.Baseline Our baseline system consists of twosteps.
First, we check the features.
All cases of cita-tion form features (in the case of EGY, P3MS) returnthe lemma as the inflected form.
Otherwise, we lookup the lemma and feature pair among all triplets inthe training data and return the inflected form if sucha triplet was found.
If not, the baseline does not re-turn an answer.Results The results on tokens are summarized inTable 2 under the column heading BL (baseline) andNOTMP (LICA with no stem template).
Our sys-tem consistently improves over the baseline for alltraining data sizes explored.Error Analysis We performed an error analysisusing the best settings found on the developmentset.
We found that in 46% of the error types thelemma is unseen.
Additional 35% of the cases aredue to stem templates that are unseen in the com-plete ICs although their corresponding lemmas areseen.
About one tenth of the cases are because ofthe existence of multiple forms of the same lemmaand feature combinations, where our system assignsa form that is different from the gold form.
Finally,gold errors contribute to about 9% of all errors.4.6 Results on GermanData and Metrics For our experiments, we usethe TIGER corpus (Brants and Hansen, 2002).
Wedivide the corpus into three parts: training, develop-ment and test, of about 709K, 143K and 37K words,respectively.
However, we use the first 75K wordsin training and the first 36K words in developmentto have the results comparable to EGY.
We conductour experiments on verbs only.
We disregard anyverbs with separable prefixes (as is common in workin morphology learning).
On average, verbs with-out separable prefixes are about 9% of all words inour corpus.
The data is represented in triplets as de-scribed in Section 4.1.
There are 28 feature com-binations of tense (present, past), person (first, sec-ond and third), number (singular, plural), and mood(indicative, subjunctive, imperative, past participle,infinitive).
Some combinations are invalid such asany tense with the non-tensed participle or infinitive.The infinitive is the lemma.
We use the same metricsas for EGY (Section 4.5).Results The results are summarized in Table 5,with the relevant results in the column NOTMP.
Wesee that our algorithm performs substantially betterthan the baseline at all training set sizes, with greaterrelative error reductions at smaller training sizes.Error Analysis We inspected all error types in thedevelopment set and found that nearly 8% of all er-ror types are errors in the gold annotation of the de-velopment set, and in 4% of cases, our predictedform is correct because a lemma is shared by twoverbal paradigms (for example, werden has differentpast participles depending on whether it is the pas-sive auxiliary or the verb for ?become?
).5 Language-Specific Modeling of Stems5.1 General ApproachWe model the abstract stems using the notions of or-thographic template and orthographic root.
Tomeet our definition of IC given above, we definethese two notions so that the root and template canalways be extracted deterministically.
We definethem simply in terms of sets of letters: the set ofletters of the alphabet used to write the languagewe are modeling is partitioned into the root let-ters and the pattern letters.
The orthographic tem-plate is a string that specifies the template letters andthat has placeholders for the root letters, which wewrite as ?2?.
The orthographic root is a sequenceof strings that specifies the root letters that can filla vocalic template?s placeholders, in the order spec-ified.
Note that an IC along with a root is equiv-alent to a paradigm, since the root can be inserteddeterministically into the abstract stem.
This givesus another way to specify a lexeme: since a com-plete paradigm enumerates all inflected forms of alexeme, and since a complete IC along with a rootdefines a complete paradigm, we can specify a lex-eme to be a pair consisting of an IC along with aroot.
This pair determines a complete mapping frommorphosyntactic features to surface word forms forthe lexeme.The basic algorithm discussed earlier is modifiedas follows: when we read in the training data, the1037EGY Inflectional Class (IC) RepositoryIC-1 IC-2 IC-3P1US 2a2a2+t 2a2?+ayt 2u2+tP1UP 2a2a2+nA 2a2?+aynA 2u2+nAP2MS 2a2a2+t 2a2?+ayt 2u2+tP2FS 2a2a2+tiy 2a2?+aytiy 2u2+tiyP2UP 2a2a2+tuwA 2a2?+aytuwA 2u2+tuwAP3MS 2a2a2 2a2?
2A2P3FS 2a2a2+it 2a2?+it 2A2+itP3UP 2a2a2+uwA 2a2?+uwA 2A2+uwAI1US Aa+22i2 Aa+2i2?
Aa+2uw2I1UP ni+22i2 ni+2i2?
ni+2uw2I2MS ti+22i2 ti+2i2?
ti+2uw2I2FS ti+22i2+iy ti+2i2?+iy ti+2uw2+iyI2UP ti+22i2+uwA ti+2i2?+uwA ti+2uw2+uwAI3MS yi+22i2 yi+2i2?
yi+2uw2I3FS ti+22i2 ti+2i2?
ti+2uw2I3UP yi+22i2+uwA yi+2i2?+uwA yi+2uw2+uwAC2MS Ai22i2 2i2?
2uw2C2FS Ai22i2+iy 2i2?+iy 2uw2+iyC2UP Ai22i2+uwA 2i2?+uwA 2uw2+uwAEGY LexiconIC-1 IC-2 IC-3katab faraD mad?
Hal?
s?Af qAl?k,t,b?
?f,r,D?
?m,d?
?H,l?
?s?,f?
?q,l??write?
?suppose?
?extend?
?solve?
?see?
?say?Table 1: Example of three inflectional classes and associ-ated lemmas with their roots in EGY.
The various blocksin the IC table specify different syncretism zones.root of the lemma is used to decompose the stem ofthe inflected form into a root and a template.5.2 Choosing Root and Pattern LettersDetermining which letters count as root letters andwhich count as pattern letters is language-specific.In this paper, we use three approaches to choosingroot and pattern letters.No Template (NOTMP) In this approach, we donot model the stem at all, i.e., there are no patternletters.
As a result, the ICs cannot express stemchanges; any verb with a stem change will be anirregular verb and it will be modeled with a sup-pletive IC.
Note that in EGY, every verb manifestsa stem change between the perfective and imperfec-tive forms, while in GER many verbs in fact do havethe same stem for all inflected forms.
The results ofthe NOTMP approach are the same basic approachresults already presented in Section 4.Scholar (SCHLR) In this approach, we use lin-guistic scholarship and intuition to choose a set ofletters to be the pattern letters.
These are letterswhich we know can change in stems within thesame IC.
We discuss this approach for EGY in Sec-tion 5.3.1 and for GER Section 5.4.1.Empirical (EMPR) In this approach, we obtainthe set of pattern letters empirically.
We align theletters of all the stems in the development corpuswith the stem letters of their corresponding lemmas.This allows us to identify which letters change be-tween lemma stem and inflected stem.
We order theletters by the probability that a letter changes givenall occurrences of that letter, and by the probabil-ity that a letter changes given all changes that oc-cur.
This gives us two rankings.
For both rank-ings and for each letter that changes, we constructa set of letters by including all letters from the mosthighly ranked letter down to the letter under con-sideration.
This gives us per ranking as many setsas there are letters that change.
We then choose thebest performing set among all sets generated by bothrankings.
We present the results of this approach forEGY in Section 5.3.1 and for GER in Section 5.4.1.5.3 Egyptian Arabic5.3.1 Choice of Pattern LettersScholar-based Pattern Letters For the EGYSCHLR approach, we selected the following patternletters:  Z????
?@ @@ @ AA?A?A?ww?yy?y??aui?.
Theseletters cover all the so-called weak root radicals,Hamzated forms and diacritics that are often usedto discuss different verbal paradigms in Arabic(Gadalla, 2000).
Table 1 shows three EGY inflec-tional classes.
We follow the standard practice inmodeling Arabic morphology of assuming that eachplaceholder in an orthographic template correspondsto exactly one letter from the orthographic root (i.e.,the root is a sequence of strings each of length one).We would like to stress that our orthographic tem-plate and root differ from the notions of pattern androot in Semitic morphology (for an overview, see(Habash, 2010)): while for katab ?write?
the or-thographic root as well as the ?real?
root is ?k,t,b?,the orthographic root for xal?ay?
?let?
is ?x,l?
(asopposed to the real root ?x,l,y?).
Henceforth, wewill omit the adjective ?orthographic?, since we willnot talk about the ?real?
root.
To inflect a lemmafor a particular set of features, we combine the rootwith the 2 slots in the corresponding inflectionalclass feature row, e.g., Hal?
+ P2FS ?
?H,l?
+10382a2?+aytiy?
Hal?aytiy.
Note that in this paper,we do not use any rules; all regular phonological andorthographic variation is ?compiled?
into the ICs.We also see examples of stem syncretism zones inTable 1; they are marked with horizontal lines.
Thestems associated with P3MS, P3FS and P3UP hap-pen to be the same within each IC (although differ-ent across ICs).
Different ICs have different zones:e.g., IC-1 has one zone for the P*** features, whileIC-2 and IC-3 have two identical zones for P3** andP[12]**.Empirically Determined Pattern Letters ForEGY, the EMPR approach, using the algorithmdescribed in Section 5.2, yields the followingoptimal set of pattern letters:   HZ????
@@ @AA?A?wyy?y???aui.
This set is very similar to theSCHLR set except in that it omits the lexicallyconstant (per lexeme) Shadda diacrtic  ?
(ortho-graphic gemination marker), and some very infre-quent Hamzated forms; and it also adds one letterH ?
as a result of orthographic inconsistency in thetraining data.Corpus VerbBL NOTMP SCHLR EMPREMPRSize Count IIC+HZ0K 0 19.3 20.0 20.0 20.0 58.81K 95 51.8 64.2 68.3 68.1 83.35K 630 64.2 77.0 83.9 83.9 91.110K 1,201 70.4 84.3 89.4 89.1 92.425K 2,994 79.6 91.5 94.4 94.8 95.650K 5,966 84.5 94.2 96.9 96.7 97.275K 8,690 85.6 94.9 97.5 97.6 97.2Table 2: Learning curve comparing system performancefor EGY on tokens (development set).Results The template approaches SCHLR andEMPR consistently beat NOTMP which does notmake use of any templatic stem modeling (see Ta-ble 2).
However, SCHLR and EMPR perform aboutequally.
This is not surprising since the two sets ofpattern letters are quite similar.Error Analysis We conducted an error analysis ofEMPR using the best settings found on the develop-ment set.
About 86% of the error types in NOTMPwhere the lemma is unseen are solved after intro-ducing EMPR.
Additionally, 71% of the cases inNOTMP where the stem template is unseen and thelemma is seen are solved.
However, 7% of the errortypes in EMPR are not present in NOTMP, and theyare all cases where the stem template is unseen whilethe lemma is seen.
Errors due to multiple stem formsand gold errors remain the same in both NOTMP andEMPR, contributing to 26% and 23%, respectively,of all the error types in EMPR.5.3.2 Other Enhancements with LinguisticKnowledgeOther linguistic knowledge can also help enrichthe process of IC learning, especially under limitedannotation conditions.
We use the following twotypes of linguistic knowledge, which seamlessly in-tegrate into the core merging algorithm describedabove.Iconic Inflectional Classes (IICs) are ICs that aremanually fully annotated, i.e., they have all the tem-plate cells for all morphosyntactic features specified.IICs are treated like any other ICs when constructingthe initial IC graph.
They are different from otherICs in that they initially have no lemmas associatedwith them in the lexicon.Hard Stem-Syncretism Zones (HZ) are stem-syncretism zones determined manually by linguiststo hold for all ICs.
As such they can be more fine-grained than is needed to describe individual ICs.
Ahard zone is not applied in case of any partial dis-agreement within it.
Unlike soft zones, they couldbe applied before or after the merge process, andthey do not guarantee that the ICs will be completelyfilled.We conducted experiments where we addedexternal linguistic knowledge to our training data.We added 112 IICs which are extracted from allEGY verb inflections listed in a reference grammarof EGY (Gadalla, 2000).
Also we added thefollowing eight HZs for EGY (with reference tofeatures <tense, person, gender, number>):(P1US-P1UP-P2MS-P2FS-P2UP), (I1UP-I2MS-I3MS-I3FS), (I2FS-I2UP-I3UP), (P3FS-P3UP),(C2FS-C2UP), (P3MS), (I1US), and (C2MS).Applying the HZs on the completed ICs (before softzone application) gives higher results than applyingthem on initial ICs.
We only report below on thesetting of applying HZs after IC merge completion.We present the accuracies of IC learning for thebaseline, and for the best setup with and withoutIICs and HZs, for different training sizes in Table 2.The baseline with no training data is at 19.3%1039because of all the cases with verbs appearing inthe citation form.
As expected, IICs always helpimprove accuracy, especially under limited (andno) data conditions.
However the benefits diminishrapidly for larger training sets.
When evaluating ontypes only (results not presented in this paper), wefind that using IICs in our system with no data isbetter than using the baseline with 75K words.5.3.3 Blind Test SetTable 3 shows the accuracies the different sys-tems on our blind test set.
The results for the testset are lower than those of the development set, butthe trends are the same.
We also compare our re-sults to those obtained using the system of Durrettand DeNero (2013) on the same test data.
Note thatwe apply their system to our problem ?
predictingunseen forms from annotated corpora (i.e., incom-plete paradigms), not to the problem for which theycreated their system ?
predicting unseen forms fromcomplete paradigms.
Our best system outperformstheirs by 2.8% absolute in accuracy.System Accuracy Error ReductionBaseline 84.7NOTMP 91.5 44.4SCHLR 93.2 55.6EMPR 93.2 55.6Durrett & DeNero 90.4 37.3Table 3: Results for EGY on tokens using a blind test set.5.4 German5.4.1 Choice of Pattern LettersScholar-based Pattern Letters We now discussour scholarship-based choice of pattern letters forGerman.
Like Arabic, German verb paradigmscan show stem changes which are typically vowelchanges.
Furthermore, like Arabic, German hasprefixes, suffixes, and circumfixes.
However, un-like Arabic, German has many verbs (called ?weakverbs?)
which are regular in the sense that theyshow no stem change at all.
The irregular verbs,or ?strong verbs?, show many different patterns ofstem changes.
Another difference to Arabic is thatthe affixes are not the same for all verb paradigms.In particular, the weak verbs form several inflec-tional classes (which, of course, differ only in af-fixes).
Finally, unlike Arabic, in the strong verbsthe orthographic root does not necessarily consistGER Inflectional Class (IC) RepositoryIC-1 IC-2 IC-3PI1S 2o2 +e 2e2 +e 2e2 +ePI2S 2o2 +st 2ie2 +st 2i2 +stPI3S 2o2 +t 2ie2 +t 2i2 +tPI1P 2o2 +en 2e2 +en 2e2 +enPI2P 2o2 +t 2e2 +t 2e2 +tPI3P 2o2 +en 2e2 +en 2e2 +enPS1S 2o2 +e 2e2 +e 2e2 +ePS2S 2o2 +est 2e2 +est 2e2 +estXI1S 2o2 +te 2a2 + 2a2 +XI2S 2o2 +est 2a2 +st 2a2 +stXS1S 2o2 +te 2a?2 +e 2a?2 +eXS2S 2o2 +test 2a?2 +est 2a?2 +estPP ge+ 2o2 +t ge+ 2e2 +en 2e2 +enINF 2o2 +en 2e2 +en 2e2 +enGER LexiconIC-1 IC-2 IC-3holen sohlen sehen lesen vergeben begeben?h,l?
?s,hl?
?s,h?
?l,s?
?verg,b?
?beg,b??fetch?
?sole?
?see?
?read?
?forgive?
?occur?Table 4: Example of three inflectional classes (some cellsomitted in the interest of space economy) and associatedlemmas with their roots in German (?X?
stands for pasttense).
The different blocks in the IC table specify differ-ent stem syncretism zones.of sequences of single letters: the strong verbs havemonosyllabic stems (plus perhaps derivational mor-phology), with the vowel in this stem potentially un-dergoing changes.
However, the onset and coda ofthe stem syllable can be any consonant cluster al-lowed by German phonology.
Thus, in German, wemodel roots as pairs of strings of any length (whichrepresent the onset and coda of the stem syllable).Table 4 shows a weak IC and two strong ICs.Since German strong verbs can have any stemvowel, we assume that all eight vowel letters of Ger-man (aeioua?o?u?)
are pattern letters, and all other con-sonant letters are root letters.
German weak verbsshow no stem changes at all; if we tailored our tem-plates to them, we would define all letters to be rootletters, and there would be no pattern letters at all.This is in fact the experiment we reported on in Sec-tion 4.6, and whose results are shown as NOTMPin Table 5.
However, since we do not know dur-ing training time whether a verb is weak or strong,all verbs will be modeled with an orthographic tem-plate, even though there is no stem change at all.Empirically Determined Pattern Letters ForGER, the EMPR approach, using the algorithm de-scribed in Section 5.2, yields oaa??
as the optimal1040set of pattern letters.
The EMPR pattern letters dif-fer from the SCHLR pattern letters by omitting fivevowels, but including the ?
variant of the s.Results In table 5 we see that using all eight vow-els as pattern letters (SCHLR column) in fact de-creases performance at every training size (and rel-atively more at smaller training sizes).
However,if we use the empirically obtained pattern letter setoaa?
?, we see that we perform better than SCHLRat almost all training sizes, and slightly better thanNOTMP at larger training sizes.Error Analysis A manual inspection of all devel-opment error types again revealed 8% developmentset annotation errors and 4% acceptable variations.To investigate why NOTMP outperforms SCHLRfor German on the development set, we performedan oracle experiment: we assumed we knew for eachseen verb in training whether it is a weak or a strongverb.
If it is weak, we model it using NOTMP, and ifit is strong, using SCHLR.
We observe as expectedthat the performance on weak verbs is very similar tothat obtained using NOTMP on all verbs.
However,for the strong verbs, it is only when we have morethan 75,000 words of training data that the oracleoutperforms NOTMP.
We assume that the reason isthat the highly frequent verbs are strong, but occurfrequently enough so that their IC can be learned di-rectly from the training data.
Using SCHLR simplyadds noise for these very frequent verbs.
It is onlyfor the less frequent strong verbs that SCHLR cancontribute, and then only when a large amount oftraining data is available.5.4.2 Blind Test SetTable 6 shows the accuracies the different sys-tems on our blind test set.
The results for the testset are lower than those of the development set, andNOTMP, SCHLR,and EMPR produce very simi-lar accuracy results.
We also compare our resultsto those obtained by running the system of Durrettand DeNero (2013) on the same training and testdata.
Our system outperforms Durrett and DeNero(2013)?s system reducing the error of by 5%.55We also tested our system on Durrett and DeNero (2013)?sproblem definition and data, training on 200 GER paradigms,and testing on 200 unseen paradigms.
This is the case of testingfor unseen lemmas in our system.
Our system gives an accu-racy of 88.4% as opposed to 91.8% as reported by Durrett andDeNero (2013).
Our system was not designed for this task.Corpus VerbBL NOTMP SCHLR EMPRSize Count0K 0 13.7 25.2 25.2 25.21K 81 43.3 72.0 67.0 69.45K 461 64.8 83.5 83.0 83.110K 929 72.1 89.0 87.4 88.625K 2,362.0 79.5 93.6 93.0 92.450K 4,527 86.2 95.6 95.1 95.675K 6,728 88.9 96.8 96.2 97.0Table 5: Learning curve comparing system performancefor GER on tokens on DEV corpus.
BL=BaselineSystem Accuracy Error ReductionBaseline 89.5NOTMP 96.5 66.7SCHLR 96.5 66.7EMPR 96.5 66.7Durrett 96.3 64.8Table 6: Results for GER on tokens using a blind test set.6 Conclusion and Future WorkWe presented a method for automatically learn-ing inflectional classes and associated lemmas frommorphologically annotated corpora.
In the future,we plan to improve several aspects of our mod-els, in particular, using more powerful language-independent template transformations to automati-cally optimize for stem and affix modeling.
Weplan to take the insights from this paper and ap-ply them to new dialects and languages with lim-ited resources.
We are interested in extending ourapproach to languages with different morphologicalsystems, e.g., agglutinative or reduplicative.
We willexplore ideas from unsupervised morphology learn-ing to minimize the need for morphological annota-tions.AcknowledgmentThis paper is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-12-C-0014.Any opinions, findings and conclusions or recom-mendations expressed in this paper are those of theauthors and do not necessarily reflect the views ofDARPA.1041ReferencesSabine Brants and Silvia Hansen.
2002.
Developmentsin the tiger annotation scheme and their realization inthe corpus.
In In Proceedings of the Third Conferenceon Language Resources and Evaluation LREC-02.
LasPalmas de Gran Canaria, pages 1643?1649.Tim Buckwalter.
2004.
Buckwalter Arabic Morpho-logical Analyzer Version 2.0.
LDC catalog numberLDC2004L02, ISBN 1-58563-324-0.Maris Camilleri.
2011.
Island morphology: Morphol-ogy?s interactions in the study of stem patterns.
Lin-guistica, 51:65?84.
Internal and External Boundariesof Morphology.Burcu Can and Suresh Manandhar.
2012.
Probabilistichierarchical clustering of morphological paradigms.EACL 2012, page 654.Erwin Chan.
2006.
Learning probabilistic paradigms formorphology in a latent class model.
In Proceedings ofthe Eighth Meeting of the ACL Special Interest Groupon Computational Phonology at HLT-NAACL, pages69?78.Lionel Cle?ment, Beno?
?t Sagot, and Bernard Lang.
2004.Morphology based automatic acquisition of large-coverage lexica.
In LREC 04, pages 1841?1844.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing (TSLP), 4(1).Silviu Cucerzan and David Yarowsky.
2002.
Boot-strapping a multilingual part-of-speech tagger in oneperson-day.
In Proceedings of the 6th conference onNatural language learning-Volume 20, pages 1?7.Gre?goire De?trez and Aarne Ranta.
2012.
Smartparadigms and the predictability and complexity of in-flectional morphology.
EACL 2012, page 645.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text using adirichlet process mixture model.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 616?627.Greg Durrett and John DeNero.
2013.
Supervised Learn-ing of Complete Morphological Paradigms.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACL-HLT), Atlanta, GA.Ahmed El Kholy and Nizar Habash.
2010.
Techniquesfor Arabic morphological detokenization and ortho-graphic denormalization.
In Proceedings of LREC-2010, May.Ramy Eskander, Nizar Habash, Ann Bies, Seth Kulick,and Mohamed Maamouri.
2013.
Automatic correc-tion and extension of morphological annotations.
InProceedings of the 7th Linguistic Annotation Work-shop and Interoperability with Discourse, pages 1?10, Sofia, Bulgaria, August.
Association for Compu-tational Linguistics.Raphael Finkel and Gregory Stump.
2002.
GeneratingHebrew Verb Morphology by Default Inheritance Hi-erarchies.
In Proceedings of the Workshop on Compu-tational Approaches to Semitic Languages, pages 9?18.Markus Forsberg, Harald Hammarstro?m, and AarneRanta.
2006.
Morphological lexicon extraction fromraw text data.
Advances in Natural Language Process-ing, pages 488?499.Hassan Gadalla, Hanaa Kilany, Howaida Arram, AshrafYacoub, Alaa El-Habashi, Amr Shalaby, KrisjanisKarins, Everett Rowson, Robert MacIntyre, PaulKingsbury, David Graff, and Cynthia McLemore.1997.
CALLHOME Egyptian Arabic Transcripts.
InLinguistic Data Consortium, Philadelphia.Hassan Gadalla.
2000.
Comparative Morphology ofStandard and Egyptian Arabic.
LINCOM EUROPA.Nizar Habash and Owen Rambow.
2006.
MAGEAD: Amorphological analyzer and generator for the Arabicdialects.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 681?688, Sydney, Australia.Nizar Habash, Owen Rambow, and George Kiraz.
2005.Morphological Analysis and Generation for ArabicDialects.
In Proceedings of the ACL Workshopon Computational Approaches to Semitic Languages,pages 17?24, Ann Arbor, Michigan.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.Nizar Habash, Ramy Eskander, and Abdelati Hawwari.2012.
A Morphological Analyzer for Egyptian Ara-bic.
In Proceedings of the Twelfth Meeting of the Spe-cial Interest Group on Computational Morphology andPhonology, pages 1?9, Montre?al, Canada.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Harald Hammarstro?m and Lars Borin.
2011.
Unsuper-vised learning of morphology.
Computational Lin-guistics, 37(2):309?350.H.
Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-Habashi, and C. McLemore.
2002.
EgyptianColloquial Arabic Lexicon.
LDC catalog numberLDC99L22.Kimmo Koskenniemi.
1983.
Two-Level Model for Mor-phological Analysis.
In Proceedings of the 8th In-1042ternational Joint Conference on Artificial Intelligence,pages 683?685.Christian Monson, Jaime Carbonell, Alon Lavie, and LoriLevin.
2008.
Paramor: Finding paradigms acrossmorphology.
Advances in Multilingual and Multi-modal Information Retrieval, pages 900?907.Sylvain Neuvel and Sean A Fulop.
2002.
Unsuper-vised learning of morphology without morphemes.
InProceedings of the ACL-02 workshop on Morphologi-cal and phonological learning-Volume 6, pages 31?40.Association for Computational Linguistics.Matthew G Snover, Gaja E Jarosz, and Michael R Brent.2002.
Unsupervised learning of morphology using anovel directed search algorithm: Taking the first step.In Proceedings of the ACL-02 workshop on Morpho-logical and phonological learning-Volume 6, pages11?20.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of ACL-08: HLT, pages 737?745, Columbus, Ohio, June.Gregory T. Stump.
2001.
Inflectional Morphology.
ATheory of Paradigm Structure.
Cambridge Studies inLinguistics.
Cambridge University Press.Ge?raldine Walther.
2011.
Measuring morphologicalcanonicity.
Linguistica, 51:157?180.
Internal and Ex-ternal Boundaries of Morphology.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In Proceedings of the 38th AnnualMeeting on Association for Computational Linguis-tics, pages 207?216.1043
