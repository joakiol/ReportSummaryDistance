AbstractMost NLG systems generate texts for readers withgood reading ability, but SkillSum adapts its outputfor readers with poor literacy.
Evaluation with low-skilled readers confirms that SkillSum?s knowl-edge-based microplanning choices enhance read-ability.
We also discuss future readability im-provements.1 IntroductionMost existing NLG systems assume that generated texts areread by proficient readers with good literacy levels.
How-ever, many people in the UK and elsewhere are not profi-cient readers; indeed, according to a UK Government survey[Moser, 1999], twenty percent of the UK adult populationhave problems with reading (and an even greater numberhave problems with simple maths).
Some of these individu-als have physical or cognitive disabilities (such as dyslexia),but many have no such problems; their poor basic skills arebecause of factors such as social deprivation and attendinglow-quality schools.
NLG systems that generate personal-ised health information, for example Cawsey et al [2000]and Reiter et al [2003a], would probably be more effectiveif they could generate appropriate texts for poor readers aswell as good readers.
Certainly real world NLG applicationsshould at least consider such readers; otherwise there is adanger that many readers will not understand the texts wegenerate.Generating appropriate texts for poor readers is a multifac-eted problem.
At a content level, texts should be short, ex-plicit, and clearly useful to the reader  [Sripada et al, 2003],so that he or she is willing to make the effort required toread it.
At a linguistic level, texts should use simple andeasy-to-understand words and short sentences with simplesyntactic structures [Harley, 2001].
At a presentation level,texts should have an easy-to-understand layout [Bouayad-Agha et al, 2001] and be communicated in clear fonts par-ticularly for dyslexic readers (e.g.
K-type fonts, www.k-type.com) and for readers with visual impairment (e.g.tiresias font, www.tiresias.org).The focus of our research is on the linguistic level, and todate we have looked at choices related to the expression ofdiscourse structure, such as the order in which phrases re-lated by a discourse relation are expressed.
Our hope wasthat rules for linguistic choices at least would be generic andeasy to ?plug in?
to NLG systems intended for poor readers.Future work in the project will look at lexical choice andalso at improved content selection and personalisation.1.1 The SkillSum projectSkillSum is an on-going collaborative project betweenCambridge Training and Development Ltd. (CTAD), whobuild educational resources, and NLG researchers at Aber-deen University.
The project is developing a web-basedapplication that assesses adult basic skills in literacy (read-ing and writing skills) or numeracy (maths skills) and gen-erates feedback reports.
Users of SkillSum take a test devel-oped by CTAD that assesses their literacy or numeracy, andthen SkillSum generates reports that summarise theirperformance.
SkillSum is being developed in a user-centredmanner involving rapid prototyping and frequent evalua-tions with users.The ultimate goal of the SkillSum project is to build a sys-tem that allows people who are concerned about their liter-acy or numeracy to assess their skills with minimal supportfrom others, and that encourages people with poor skills totake steps to improve them.
Currently most people withpoor skills do not in fact enrol in courses to improve theirskills, and our hope is that making the assessment process aseasy (and private) as possible will encourage more peoplewho need help to seek it out.The SkillSum project originally used detailed diagnosticliteracy and numeracy assessments developed by CTAD.However, in pilots with users, we found that these took toolong to complete and it seemed unlikely that people wouldbe able to use them in an unsupported environment.
Ourcurrent solution uses modified versions of CTAD?s shorterliteracy and numeracy screeners, i.e.
tests that identify in abroader sense whether a user has problems with literacy ornumeracy, but without a detailed analysis.
These administertwenty-seven questions graded according to the Adult BasicSkills Core Curriculum for England and Wales [Steeds,2001] and covering a broad range of skills from simplerlevels to higher levels in this curriculum.
The tests adminis-ter the easiest questions first.
Ideally, the difficulty of thequestions that are administered should change according toGenerating readable texts for readers with low basic skillsSandra Williams and Ehud ReiterDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, U.K.{ swilliam,ereiter }@csd.abdn.ac.uka user?s ability to answer correctly, but at present if a userhas difficulties with the questions, the test simply ends.The reports generated by SkillSum are, of course, tailored toindividuals, but this tailoring is in terms of content, ratherthan language.
The focus of research to date has been onhow to generate appropriate texts for readers with belowaverage literacy and numeracy; i.e.
language tailoring forthe group as a whole, not for individuals.1.2 Related workUsing NLG in educational applications is not new, but thetype of text generated is different from SkillSum?s reports.For example, generating turns in intelligent tutoring system(ITS) dialogues (e.g.
[Di Eugenio et al, 2001; [Moore et al,2004]).
Although turns can give feedback, the kind of feed-back differs in that it attempts to teach a student about animmediate domain-specific learning problem, rather than tosummarise his/her overall skills.With regard to tailoring texts for different readers, a numberof previous researchers have looked at tailoring generatedtexts according to whether the reader is a domain expert or anovice (for example [Paris, 1988; McKeown et al, 1993;Milosavljevic and Oberlander, 1998]).
Less work has beendone on tailoring texts according to the reader?s literacy.Perhaps the best-known previous work in this area is PSET[Devlin et al, 1999], which focused on syntactic and lexicalchoices in texts intended for aphasic readers.
Unfortunatelymost of PSET?s adaptation rules were not experimentallyvalidated.
Siddharthan [2003] similarly proposed and im-plemented a system for simplifying texts, but did not evalu-ate how readable his generated texts were for poor readers.Scott and de Souza [1990] suggested some psycholinguisti-cally-motivated rules for expressing discourse relations, butdid not evaluate them at all.2 Linguistic choices investigatedFigure 1 ?
Extract from typical content planThe document (content) planners of our system produce asoutput a tree, where core messages are related by discourserelations such as explanation or concession; this basicallyfollows the architecture described by Reiter and Dale[2000].
Discourse relations are essentially rhetorical struc-ture theory (RST) relations [Mann and Thompson, 1987],and messages are represented using a deep-syntactic repre-sentation, which is loosely based on RealPro [Lavoie andRambow, 1997].
An example of an extract from a typicalcontent plan, with messages shown as text glosses instead ofdeep syntactic structures, is shown in Figure 1.Our focus to date has been on how discourse relations suchas Concession and Condition in Figure 1 are expressed, inparticular:?
cue phrases: should a cue phrase (or multiple cuephrases) be used to express a discourse relation?
If so,which one(s)?
For example, should we generate:?
If you practise reading, your skills will improve(one cue, If)?
If you practise reading, then your skills will im-prove (two cues, If and then)?
ordering: which order should the constituents relatedby a discourse relation be expressed in?
Should thenucleus (core) be first or second?
For example, shouldwe generate:?
Your skills will improve if you practise reading(nucleus first)?
If you practise reading, your skills will improve(nucleus second)?
punctuation (sentence structure): should constituentsbe expressed in separate paragraphs, separate sen-tences, in a single sentence with punctuation separat-ing them, or in a single sentence without punctuation?For example, should we generate (just showing two ofthese options):?
Many people find reading hard, but your skillswill improve if you practise reading (single sen-tence, comma separation)?
Many people find reading hard.
But your skillswill improve if you practise reading (two sen-tences)These choices are inter-dependent.
For example, we cannotsay, ?Then your skills will improve, if you practise reading?
(both if and then cue phrases, nucleus first).This problem is related to the document structuring task ofPower et al [2002].
Power et al?s approach is essentiallyalgorithmic; whereas our approach is centred on the knowl-edge required to make the choices, and the algorithm used isless important.
Their task is how to map an input RST treeto a set of output trees representing possible alternativedocument structures and then choose the best; whereasSkillSum?s microplanning task is to map an input RST treeto flat, ordered lists of syntactic structures representing pos-sible lists of alternative sentences and then pick the best.Power et al include document layout in their task, whereasSkillSum makes layout decisions later, during the final re-alisation stage.3 Choice rules and the microplannerWe created a microplanner (developed from the one de-scribed in [Williams, 2004]) that made the above choicesbased on hard constraints and optimisation rules; the hardConcessionCondition [Many people findreading hard][Your skills will improve] [You practise reading]constraints forbade illegal combinations, and the optimisa-tion rules expressed readability preferences.3.1 Hard constraintsThe hard constraints were intended to forbid combinationsof choices that led to ungrammatical texts, such as ?Thenyour skills will improve, if you practise reading?.
We cre-ated these by analyzing the RST Discourse Treebank Cor-pus (RST-DTC) [Carlson et al 2002]; this is a corpus ofWall Street Journal texts that have been annotated with dis-course relations.
For each discourse relation of the type thatoccurs in SkillSum texts, we extracted 200 instances of therelation from the RST-DTC (or as many as possible if thecorpus contained fewer than 200 instances), and analysedwhat combination of the above choices were instantiated ineach of these instances.
We then created hard constraintsthat forbade any pair of choices which was not present inany of the RST-DTC instances that we analysed.
These con-straints were specified on pairs of choices (for example,ordering and punctuation), not a complete choice set (order-ing, punctuation, cue phrases), because we did not haveenough instances to make rules for complete choice sets.The RST-DTC corpus was not ideal for this exercise, as it isbased on texts (Wall Street Journal articles) that are in-tended for good readers and written in U.S. English.
Itwould have been preferable to use a corpus of U.K. Englishtexts intended for low-skilled readers.
Unfortunately there isno such corpus that includes discourse relation annotations.3.2 Optimisation rulesThe optimisation rules expressed preferences between legalsets of choices.
We created two sets of rules: control andenhanced-readability (ER).
The control rules were based onthe most common choices observed in the RST-DTC; theyalso penalised cue phrases which were highly ambiguous(could be used for many discourse relations).
The ER rulesexpressed a set of preferences for the above choices whichwe hypothesized would result in more readable texts forlow-skilled readers.The ER model was based on a literature review of relevantpsycholinguistic findings (such as [Millis and Just, 1994;Degand et al, 1999; Harley, 2001]) and also on a series ofpilot experiments that we performed with low-skilled read-ers [Williams et al, 2003].
Essentially, it prefers that?
each discourse relation should be expressed by a cuephrase.
Only a single cue phrase should be used (forexample, not both if and then for condition);?
lexically common cue phrases are preferred, even ifthey are ambiguous; for example but instead of how-ever for concession;?
a cue phrase should be placed between the constituentsif possible, and the nucleus (core) should come first ifpossible.
For example, ?Your skills will improve if youpractise reading?
is preferred over ?If you practisereading, your skills will improve?;?
constituents should preferably be in separate sentences;if they are in the same sentence, they should be sepa-rated by a comma.With regard to the choice of cue phrases, obviously cuephrases do not have identical meanings; even if in a broadsense they express the same discourse relation, they havedifferent connotations and applicability constraints [Knott,1996].
Hence the choice of cue phrases should be influencedFigure 2 ?
Example text produced by SkillSum and generated with enhanced readability (ER) model.						 !"!
!#!#$!$%by how well the cue matches the context as well as howcommon (readable) it is.
The issue is finessed in SkillSumby essentially hard-coding which cue phrases can be used toexpress a particular instance of a relation; this is clearly nota satisfactory long-term solution.3.3 MicroplannerOur microplanner treats the decision-making problem as aconstraint-satisfaction problem (CSP), using the above con-straints and optimisation rules.
It is in general terms similarto the CSP system that Power [2000] and Power et al[2002] used to make expression choices about rhetoricalstructures, as discussed above.
However, we make differentchoices.
For example, our microplanner decides whether 0,1, or 2 cue phrases should be used to express a discourserelation, whereas Power et al?s assumes that a single cuephrase is always used.
Power et al?s microplanner makeschoices about indentation, which our microplanner does not.Power et al ?s microplanner also attempts to optimize thedocument as a whole, whereas ours processes each relationseparately, and does not consider interactions between theexpressions of different relations.Our microplanner is implemented in Java, and uses a JavaConstraint Library (http://liawww.epfl.ch/JCL) for CSPrepresentation and solving.
While we have not explicitlymeasured the amount of computation time needed by themicroplanner, certainly the SkillSum system as a wholeproduces texts within a few seconds, which is acceptable tousers.3.4 ExampleAn example of a text produced by SkillSum is shown inFigure 2.
This text was generated using the ER model.
Someexamples of differences in expression resulting from usingthe ER model over the control model for the same contentare?
cue phrase choice: the control model would choose un-common, unambiguous thus to express evaluation;whereas the ER model chooses common but ambigu-ous and (first paragraph in Figure 2);?
ordering: the control model would generate, if you im-prove your skills, it could help you do a coursewhereas the ER model generates it could help you doa course, if you improve your skills (sixth paragraphin Figure 2);?
punctuation and sentence structure: the control modelwould choose to express the first three paragraphs inFigure 2 joined by commas and as a single sentence;whereas the same information is expressed in threedifferent sentences (and indeed three different para-graphs) by the ER model.Some colleagues have asked why the ER model allows sen-tences to start with And (such as And you did well in Figure2).
This is because its hard constraints are based on corpusanalysis, and sentences beginning with And do in fact occurin corpora.4 Evaluation4.1 PilotsWe conducted several pilot studies of the readability ofSkillSum texts, generally in settings where subjects com-pleted the literacy assessment and then were asked to readthe summary texts that described their performance on theassessment.
Perhaps the most important decision we madeon the basis of these pilots was to evaluate readability byasking subjects to read texts aloud, and timing how long ittook them to do so.
We initially wanted to measure readabil-ity by asking comprehension questions and by timing silentreading, but this proved problematical.
The problem withcomprehension questions was that subjects responded tothem based on their beliefs about their literacy, not on thecontent of the report.
For example, if we showed a subject atext similar to Figure 2 and asked her ?What does the reportsay you did well on?, she might respond ?I didn?t do well onanything?
instead of ?It says I did well on grammar?.We also tried measuring reading rate using self-timed silentreading (subjects were asked to read text on a screen, andpress a button when they finished), which is a commontechnique in psycholinguistics.
Again, this did not workwell with poor readers because they tended to skim-read oreven simply press the button without reading the text at all.Therefore we decided to ask subjects to read texts aloud,and measure their reading rate.
This also allowed us tomeasure reading errors, which cannot be measured withsilent reading.
Oral reading rates and reading errors arecommonly used by psychologists [Kintsch and Vipond1979] and educationalists [ARCS, 2005] to measure readingdifficulty.Generally in pilots, the ER model texts were read faster thancontrol model texts by poor readers but the increase in read-ing rate was not statistically significant.
Good readers?
read-ing rates were not affected.
We used these results to calcu-late the sample size (number of subjects) required for theexperiment in section 5.2, which did show a statisticallysignificant increase in reading rate for poor readers.Other findings of the pilots included:?
Texts needed to be short, no longer than the exampleshown in Figure 2.
We tried giving people more de-tailed feedback about their assessments, but they didnot wish to read longer texts.?
We got better results if we focused on subgroups withsimilar skills profiles, rather then trying to get sub-jects with a wide range of reading (dis)abilities.
Inpart this is because low-skilled readers have very dif-ferent ability profiles; a dyslexic is quite differentfrom someone who never learnt to read because shemissed school, for example.
Also, from a statisticalperspective, a varied subject group meant high stan-dard deviations in reading speed, which made it diffi-cult to obtain statistically significant results.User preferences elicited in early pilots showed variationacross good and poor readers.
Some preferred the shortersentences and simpler cue phrases of the ER model, whileothers preferred the control model output, describing itstexts as ?more flowing?.4.2 ExperimentIn this experiment, we focused specifically on people withmoderately poor skills but not people with severe learningdifficulties.Goal.
To test the readability of texts generated by SkillSum,our hypothesis was that participants would make fewerreading errors and have faster reading rates on a text gener-ated using ER rules than on a text generated using controlrules.
We also took the opportunity to trial the latest versionof SkillSum with real users.Materials.
To measure readability, we showed participantsreports generated for someone else (not themselves), in or-der to de-personalise the experiment; in fact the reports usedwere the ER version shown in Figure 2 and text with thesame content but generated using the control model.
Col-leagues have suggested that we could have shown a numberof different texts tailored for different people, rather thanshowing everyone the same individual?s texts.
This is agood idea and we will try it in the future.Participants.
60 students aged sixteen to twenty-eight yearsat a UK Further Education college (similar to an Americancommunity college), all of whom were enrolled in voca-tional courses (e.g.
Hairdressing, Sport and Travel andTourism).
The participants were selected by staff from thecollege?s basic skills department who knew their approxi-mate levels of literacy and numeracy; they selected peoplewho were known to have problems with basic skills, but didnot have severe skills deficits.Ideally the SkillSum application requires users who are notalready enrolled at college.
In practice, however, we hadsome difficulty in finding such people.
Community workerswe contacted are protective of the people they work withand wary about involving them in experiments.
To getaround this problem, we work with people who are enrolledin community or FE college courses where we have existingcontacts and have built up trust.
We feel that although this isnot ideal, the people we work with do, in fact have problemswith basic skills and are thus well-placed to test SkillSum;to comment on any difficulties they have with using it andto make suggestions for improvements.Method.
Each participant answered some questions abouthis/her background (e.g.
age and college course) and com-pleted one of the on-line SkillSum assessments; half didliteracy and half did numeracy.
SkillSum then generated areport and participants were asked (a) some simple ques-tions about it to check comprehension (e.g.
?What was yourscore??)
and (b) for their comments on SkillSum in general.Participants were then asked to read aloud the two texts; thetexts were presented in random order.
We recorded thespeech digitally using a high-quality but unobtrusive lapelmicrophone.Analysis of recordings: Recordings were analysed accu-rately using speech signal processing and annotation soft-ware to mark beginning and end of reading the entire textand the beginnings and ends of reading errors.
Reading timeand rate for the entire text was calculated as well as totaltime spent making reading errors.We found a very strong practice effect on reading rate (thesecond text was almost always read significantly morequickly than the first) even when we attempted to factor thisout using repeated measures statistics, so we decided to onlyuse data from the first text read by each subject.Following Sanders and Noordman [2000], we decided toexclude as outliers subjects whose reading speed was morethan twice the standard deviation from the mean; in practicethis meant that we excluded three very poor readers (readingspeed less than 110 words/min) from the analysis.
We alsoexcluded six sets of recordings that were too noisy to ana-lyse.We were interested in overall numbers of errors and particu-larly in errors that caused increases in reading times.
Thesewould indicate an increase in reading difficulty of the textbeing read.
We identified insertion errors and pause errorsthat both increase reading times, omission errors that de-crease reading times and substitutions (miscues) where an-other word or mispronunciation replaces the target word.Our classification was similar to van Hasselt?s [2002], butwhereas her study only measured numbers of errors, oursmeasured error numbers and time spent making errors.Results: Oral reading rate results for the remaining fifty-onepeople are shown in Table 1.
The ER version was read onaverage 16 words per minute faster than the control text (9%faster) and this result is statistically significant at p<0.05.Text n Mean oral readingrate (words/minute)Sig.(indep.
samp t-test)control  25 173 0.040ER  26 189Table 1 ?
Results for oral reading rateAn analysis of reading error times gave the results shown inTable 2.
Types of errors made were typically substitutionerrors, where one word had been substituted for another e.g.correct for contact, pauses (hesitations) that were not atphrase boundaries and insertion errors, where readers haduttered words, or parts of words that were not present in thetexts, e.g.
par before parliament.Text n Mean error time(milliseconds)Sig.(indep.
samp t-test)control  25 1588 0.058ER  26 874Table 2 ?
Results for reading error timesThe table shows that subjects spent on average an extra714ms making errors on the control text, that is they spent82% more time making errors on the control text than on theER text; this is weakly significant at p = 0.058.5 Discussion and future workOur evaluation experiment suggests that the ER choice rulesat the discourse level do enhance the readability of gener-ated texts for low-skilled and moderate-skilled readers.However, the effect is not as large as we believe SkillSum iscapable of.
We suspect this is partly because we only lookedat a few linguistic choices, and in particular because wehave not yet included lexical choice.
This will be addressedin our future work, see below.
Our reviewers pointed outthat layout could also affect readability.
This is another fac-tor to investigate in future work.Another reason why the effect of our ER models on read-ability is not so large is because we are using only onechoice model for people with a wide range of skills and(dis)abilities.
Our experiences suggest that there are majordifferences between moderately-low-skilled readers (such asmost subjects in our evaluation experiment) and very-low-skilled readers (such as the outlier subjects excluded fromthe experiment); and between people who are dyslexic, non-native speakers, or simply have not had the chance to learnto read well.
More generally, adults with poor reading skillsare often said to have ?spiky?
ability profiles; for exampleone person may have good vocabulary but poor grammar,and another may have the reverse.
ARCS [2005] dividespoor readers into 10 subgroups; building models for each ofthe ARCS subgroups would be one way of making ourmodels less generic and more focused.Ultimately, we would like to explore building choice mod-els for individuals, that is groups of 1.
Building such modelsof course requires spending considerable effort in acquiringdata about individual readers.
However, if such models aresignificantly more effective than generic or subgroup mod-els in terms of generating readable texts, they may be worthexploring, since the benefits of making health information(for example) more accessible to people with limited read-ing skills could be very large.5.1 Future work on lexical choiceIncluding lexical choice in our models will not be easy be-cause true synonyms and paraphrases are rare [Edmondsand Hirst, 2002].
Hence we cannot simply select between Ndifferent lexicalisations that have exactly the same meaning,instead we have to determine which synonyms or para-phrases are appropriate given the text?s content and the sys-tem?s goals (as well as readability preferences).For example, since many people have problems with thetechnical term grammar, in our pilots we tried paraphrasingproblems with grammar as problems writing good sen-tences.
However, many subjects interpreted the latter phraseas saying that they had problems with spelling (not gram-mar), and this confused students whose spelling was in factfine.
Hence this paraphrase is probably not appropriate, atleast for students with poor grammar but good spelling.
Weinitially avoided lexical choice because of this issue (exceptfor choice of cue phrases, where this problem in fact arose,as mentioned in Section 3.2).Recently, we carried out a small study on technical terms inEnglish and maths (such as grammar) to find out what kindsof explanation basic skills students would understand.
Weelicited their own explanations of the terms as well as tryingout uses of the terms themselves, paraphrases of the termsand illustrative examples of the terms.
We asked basic skillsstudents to think aloud while solving simple English andmaths problems and then explained their errors to them us-ing technical terms.
In think-aloud, they varied a great deal:?
Some people did not use technical terms at all.?
Some people used technical terms inaccurately,  (e.g.spelling or capital letters were described as gram-mar).?
Some people used technical terms correctly.We concluded that it is dangerous to use technical termswhen they are likely to be misunderstood.
When explainingterms to people, however, they seemed to comprehend theillustrative examples best of all and we will try these in thenext version of SkillSum.5.2 Future work on content selectionOther future work will be to improve content selection andpersonalisation in SkillSum to help people understand theirstrengths and weaknesses.
This poses a challenge becausethe topic of SkillSum?s reports is very sensitive indeed, e.g.telling vulnerable people that they have problems with theirliteracy and/or numeracy can be hurtful!
To date, one of ourbiggest difficulties has been to generate feedback for peoplewho did not answer any of the questions correctly.
Whatshould SkillSum say to them?
This is particularly difficultbecause SkillSum does not know what might have gonewrong during the test: perhaps there was a problem withusing the computer or the mouse; or perhaps the user haddifficulties with reading the screen because of poor eye-sight; or perhaps he/she has severe learning difficulties.Knowledge acquisition for SkillSum is difficult because thetask of generating adult basic skills summary reports isnovel, poorly-understood and complex.
See Reiter et al2003b] for a detailed review of KA problems specific toNLG.
Like the smoking cessation letters generated by theSTOP system [Reiter et al, 2003a], adult basic skills reportsdid not previously occur naturally.
That is, tutors do nottend to write down feedback and advice for basic learners.
Arelated text type is school reports.
However, Education lit-erature on writing school reports is unhelpful because adultbasic learners have often had bad experiences with schooland school reports in the past [FENTO, 2004].We are currently involved in knowledge acquisition to de-rive improved content selection rules.
We elicited tutor-authored reports for ten case studies in literacy and ten innumeracy.
We gave tutors test results and a short user pro-file containing background material, e.g.
age, gender, courseenrolled in and ambitions (these were built using ano-nymised data from actual people who took part in earlierpilots and from [Swain et al, 2004]).
An analysis of thetutor-authored reports demonstrated that they have similari-ties in high-level content structures but individual authordifferences in lower-level content.
Some issues that we arecurrently considering include:?
Should reports mention students?
mistakes as well astheir correct answers??
Should reports congratulate students for doing well,when perhaps their performance was worse than nor-mal, or, on the other hand, should they commiseratewith students when perhaps their performance wasbetter than normal??
Should reports refer to students?
ambitions (e.g.
interms of qualifications or career)??
How much advice should be given??
How much motivational content should be included?We need to reconcile what tutors tell us with what studentstell us.
Tutors tend to agree that reports should be encourag-ing, focus on positive aspects and not mention mistakes.
Onthe other hand, when we talk to basic skills students, theyoften want to know what their mistakes were (it can be frus-trating to score twenty-six out of twenty-seven and notknow which one was wrong!
).Not knowing an individual and how much effort he/she hasput into the test is another problem.
Evaluative comments ina report such as ?this is very good?
are meaningless withoutsuch knowledge and, indeed they are meaningless withoutreference to some scale (but tutors have advised againstmentioning the core curriculum scale as students are notfamiliar with it).We propose to elicit self-assessments and some backgroundinformation about users?
ambitions from an initial question-naire in SkillSum.
The former might help with choice ofevaluative comments.
But combining the latter with adviceand motivational content e.g.
?you may be able to get aplumbing qualification if you do a course to improve yourEnglish?
could be dangerous because the system has noway of knowing what an individual?s potential might be andthe resulting content could be highly inaccurate.5.3 SkillSum final experimentLastly, we are planning our final experiment in which wewill evaluate SkillSum to find out if it meets the commercialand research goals of the project partners.
More specifically,these goals are:?
Basic research: Does SkillSum generate texts that canbe read and understood by people who have someproblems with literacy but not severe learning diffi-culties?
In particular, is the ER linguistic choicemodel making the right microplanning choices forsuch readers?
We will show low-skilled readers re-ports generated with SkillSum using different linguis-tic choice models, and see how the choice models af-fect readability.?
Applied research: Do NLG reports help students tounderstand their strengths and weaknesses, andwhether their skills are adequate?
We will test thiswith two versions of SkillSum (with and without theNLG component) with students who wish to do acourse at an FE college; hence ?whether skills areadequate?
will be judged relative to the needs of thestudent?s intended course.6 ConclusionSkillSum is an on-going project that is just starting to makesome progress on generating readable texts for low-skilledreaders and there is much more that can and should be done.Nevertheless, we would like to think that the choice prefer-ences we used (Section 3.2) could be practically useful topeople building systems that produce texts for low-skilledreaders, and indeed systems that produce texts for the gen-eral public (since good readers don?t seem to be affected bythese choices, there is no harm in using low-skilled prefer-ences for all readers).
We also hope that our work encour-ages other researchers to think about this topic, as makinginformation more accessible to low skill readers would havemajor benefits for society.AcknowledgmentsThis work is supported by U.K. PACCIT-LINK GrantESRC RES-328-25-0026.
We thank Liesl M. Osman, De-partment of Medicine and Therapeutics; members of theAberdeen Natural Language Generation Group and theanonymous reviewers.References[Bouayad-Agha et al, 2001] N. Bouayad-Agha, D. Scott,and R. Power.
The influence of layout on the interpreta-tion of referring expressions.
Multidisciplinary Ap-proaches to Discourse.
L. Degand, Y. Bestgen, W.Spooren and L. van Waes (eds.
), Stichting Neerlan-distiek VU Amsterdam and Nodus Publikationen M?n-ster, 2001.
[ARCS, 2005] Adult Reading Components Study.www.nifl.gov/readingprofiles/FT_ARCS.htm[Carlson et al 2002] Lynn Carlson, Daniel Marcu, andMary Ellen Okurowski.
Building a Discourse-TaggedCorpus in the Framework of Rhetorical Structure The-ory.
In Current Directions in Discourse and Dialogue, J.van Kuppevelt and R. Smith eds., Kluwer AcademicPublishers, 2002.
[Cawsey et al 2000] Alison Cawsey, Ray Jones, and JannePearson.
The Evaluation of a Personalised InformationSystem for Patients with Cancer.
User Modeling andUser-Adapted Interaction, 10(1):47-72, 2000.
[Degand et al 1999] L. Degand, N. Lef?vre, and Y. Best-gen.
The impact of connectives and anaphoric expres-sions on expository discourse comprehension.
DocumentDesign: Journal of Research and Problem Solving inOrganizational Communication, 1:39-51, 1999.
[Devlin et al 1999] S. Devlin, J. Tait, Y.Canning, J. Carroll,G.
Minnen and D. Pearce.
The Application of AssistiveTechnology in Facilitating the Comprehension of News-paper Text by Aphasic People.
In C. Buhler and H.Knops (eds.)
Assistive Technology on the Threshold ofthe New Millennium.
IOS Press.
1999.
[Di Eugenio et al 2001] Barbara Di Eugenio, MichaelGlass, Michael J. Trolio and Susan Haller.
Simple Natu-ral Language Generation and Intelligent Tutoring Sys-tems.
Proc.
of Artificial Intelligence in Education, 2001.
[Edmonds and Hirst.
2002] Philip Edmonds and GraemeHirst.
Near-Synonymy and Lexical Choice.
Computa-tional Linguistics 28(2):105-144, 2002.
[FENTO, 2004].
Further Education National Training Or-ganisation (FENTO).
Including Language, Literacy andNumeracy Learning in all Post-16 Education.
Guidanceon Curriculum and Methodology for generic initialteacher education programmes, www.nrdc.org.uk, 2004.
[Harley, 2001] Trevor Harley.
The psychology of languagefrom data to theory.
Psychology Press Ltd, 2001.
[Kintsch and Vipond, 1979] Walter Kintsch and DouglasVipond.
Reading Comprehension and Readability inEducational Practice and Psychological Theory.
In L.G.Nilsson (ed.)
Perspectives on Memory Research.
Law-rence Erlbaum, 1979.
[Knott 1996] Alistair Knott.
A Data-Driven Methodologyfor Motivating a Set of Coherence Relations.
PhD The-sis, University of Edinburgh, 1996.
[Lavoie and Rambow, 1997] Benoit Lavoie and OwenRambow.
A Fast and Portable Realizer for Text Genera-tion.
Proc.
of ANLP-1997, pages 265-268, 1997.
[Mann and Thompson, 1987] W. Mann and S. Thompson.Rhetorical Structure Theory: A Theory of Text Organi-zation.
Information Sciences Institute, Reprint Series no.ISI/RS-87-190, 1987.
[McKeown et al 1993] Kathleen McKeown, Jacques Robin,and Michael Tanenblatt.
Tailoring Lexical Choice to theUser's Vocabulary in Multimedia Explanation Genera-tion.
Proc of ACL, pages 226-234, 1993.
[Millis and Just, 1994] Keith Millis and Marcel Just.
TheInfluence of Connectives on Sentence Comprehension.Journal of Memory and Language.
33:128-147, 1994.
[Milosavljevic and Oberlander, 1998] Maria Milosavljevicand Jon Oberlander.
Dynamic Hypertext Catalogues:Helping Users to Help Themselves.
Proc.
of Hypertext1998, pages 123-131, 1998.
[Moore et al, 2004] Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian Varges, and Claus Zinn, GeneratingTutorial Feedback with Affect, in Proc.
of the 17th In-ternational Florida Artificial Intelligence Research So-ciety Conference, AAAI Press, 2004.
[Moser 1999].
Claus Moser.
Improving literacy and nu-meracy: a fresh start (report of working group).http://www.lifelonglearning.co.uk/mosergroup/.
1999[Paris 1988] C?cile Paris.
Tailoring Object Descriptions to aUser's Level of Expertise.
Computational Linguistics14(3):64-78, 1988.
[Power, 2000] Richard Power: Planning texts by constraintsatisfaction.
Proc.
of COLING 2000, pp.
642-648, 2000.
[Power et al 2002] Richard Power, Donia Scott, and NadjetBouayad-Agha.
Document Structure.
ComputationalLinguistics 29(2):211-260, 2002.
[Reiter and Dale 2000].
Ehud Reiter and Robert Dale.
Build-ing Natural Language Generation Systems.
CambridgeUniversity Press, 2000.
[Reiter et al, 2003a].
Ehud Reiter, Roma Robertson, andLiesl Osman.
Lessons from a Failure: Generating Tai-lored Smoking Cessation Letters.
Artificial Intelligence144:41-58, 2003.
[Reiter et al, 2003b].
E. Reiter, S. Sripada, and R. Robert-son.
Acquiring Correct Knowledge for Natural LanguageGeneration.
Journal of Artificial Intelligence Research18:491-516., 2003.
[Sanders and Noordman, 2000] Ted Sanders and LeoNoordman.
The Role of Coherence Relations and TheirLinguistic Markers in Text Processing.
Discourse Proc-esses, 29(1):37 ?
60, 2000.
[Scott and de Souza, 1990] Donia Scott and Clarisse deSouza.
Getting the Message Across in RST-Based TextGeneration.
In R Dale et al (eds), Current Research inNatural Language Generation.
Academic Press, 1990.
[Siddharthan 2003] Advaith Siddharthan.
SyntacticSimplification and Text Cohesion.
PhD Thesis,University of Cambridge Computing Lab, 2003.
[Sripada et al, 2003].
Somayajulu G. Sripada, Ehud Reiter,Jim Hunter and Jin Yu.
Generating English Summariesof Time Series Data using the Gricean Maxims.
Proc.
ofKDD 2003, pp.
187-196, 2003.
[Steeds, 2001] Steeds, Andrew (Ed.).
Adult Literacy corecurriculum including Spoken Communication.
Producedby Cambridge Training and Development Ltd. on behalfof The Basic Skills Agency.
ISBN 1-85990-127-1, 2001.
[Swain et al, 2004] Swain, Jon., Elizabeth Baker, DebbieHolder, Barbara Newmarch and Diana Coben.
Beyondthe daily application: Making numeracy teaching mean-ingful to adult learners.
National Research and Devel-opment Center for adult literacy and numeracy.www.nrdc.org.uk.
2004.
[van Hasselt, 2002] van Hasselt, Clare.
Oral ReadingAchievements, Strategies and Personal Characteristicsof New Zealand Primary School Students Reading Be-low Normal Expectation.
Probe Study, National Educa-tion Monitoring Project (NEMP), University of Otago,New Zealand, 2002.
[Williams, 2004] Williams, Sandra, H. Natural LanguageGeneration of discourse relations for different readinglevels.
PhD Thesis, University of Aberdeen, 2004.
[Williams et al 2003] Sandra Williams, Ehud Reiter andLiesl Osman.
Experiments with discourse-level choicesand readability.
Proc.
of the 9th European Workshop onNatural Language Generation, pp.
127-134, 2003.
