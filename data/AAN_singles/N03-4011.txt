Automatically Discovering Word SensesPatrick Pantel and Dekang LinDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta T6G 2E8 Canada{ppantel, lindek}@cs.ualberta.caAbstractWe will demonstrate the output of a distribu-tional clustering algorithm called Clusteringby Committee that automatically discoversword senses from text1.1 IntroductionUsing word senses versus word forms is useful in manyapplications such as information retrieval (Voorhees1998), machine translation (Hutchins and Sommers1992), and question-answering (Pasca and Harabagiu2001).The Distributional Hypothesis (Harris 1985) statesthat words that occur in the same contexts tend to besimilar.
There have been many approaches to computethe similarity between words based on their distributionin a corpus (Hindle 1990; Landauer and Dumais 1997;Lin 1998).
The output of these programs is a ranked listof similar words to each word.
For example, Lin?sapproach outputs the following similar words for wineand suit:wine: beer, white wine, red wine,Chardonnay, champagne, fruit, food,coffee, juice, Cabernet, cognac,vinegar, Pinot noir, milk, vodka,?suit: lawsuit, jacket, shirt, pant, dress,case, sweater, coat, trouser, claim,business suit, blouse, skirt, litiga-tion, ?The similar words of wine represent the meaning ofwine.
However, the similar words of suit represent amixture of its clothing and litigation senses.
Such listsof similar words do not distinguish between themultiple senses of polysemous words.1 The demonstration is currently available online atwww.cs.ualberta.ca/~lindek/demos/wordcluster.htm.We will demonstrate the output of a distributionalclustering algorithm called Clustering by Committee(CBC) that discovers word senses automatically fromtext.
Each cluster that a word belongs to corresponds toa sense of the word.
The following is a sample outputfrom our algorithm:(suit0.39 (blouse, slack, legging, sweater)0.20 (lawsuit, allegation, case, charge))(plant0.41 (plant, factory, facility,refinery)0.20 (shrub, ground cover, perennial,bulb))(heart0.27 (kidney, bone marrow, marrow,liver)0.17 (psyche, consciousness, soul, mind))Each entry shows the clusters to which the head-word belongs along with its similarity to the cluster.The lists of words are the top-4 most similar membersto the cluster centroid.
Each cluster corresponds to asense of the headword.2 Feature RepresentationFollowing (Lin 1998), we represent each word by afeature vector.
Each feature corresponds to a context inwhich the word occurs.
For example, ?sip __?
is a verb-object context.
If the word wine occurred in thiscontext, the context is a feature of wine.
These featuresare obtained by parsing a large corpus using Minipar(Lin 1994), a broad-coverage English parser.
The valueof the feature is the pointwise mutual information(Manning and Sch?tze 1999) between the feature andthe word.
Let c be a context and Fc(w) be the frequencycount of a word w occurring in context c. The pointwisemutual information, miw,c, between c and w is definedas:Edmonton, May-June 2003Demonstrations , pp.
21-22Proceedings of HLT-NAACL 2003( )( ) ( )NjFNwFNwFcwjciicmi ??
?=,where N is the total frequency counts of all words andtheir contexts.
We compute the similarity between twowords wi and wj using the cosine coefficient (Salton andMcGill 1983) of their mutual information vectors:( ) ????
?=ccwccwccwcwjijijimimimimiw,wsim223 Clustering by CommitteeCBC finds clusters by first discovering the underlyingstructure of the data.
It does this by searching for setsof representative elements for each cluster, which werefer to as committees.
The goal is to find committeesthat unambiguously describe the (unknown) targetclasses.
By carefully choosing committee members, thefeatures of the centroid tend to be the more typicalfeatures of the target class.
For example, our systemchose the following committee members to computethe centroid of the state cluster: Illinois, Michigan,Minnesota, Iowa, Wisconsin, Indiana, Nebraska andVermont.
States like Washington and New York are notpart of the committee because they are polysemous.The centroid of a cluster is constructed by averagingthe feature vectors of the committee members.CBC consists of three phases.
Phase I computeseach element?s top-k similar elements.
In Phase II, wedo a first pass through the data and discover thecommittees.
The goal is that we form tight committees(high intra-cluster similarity) that are dissimilar fromone another (low inter-cluster similarity) and that coverthe whole similarity space.
The method is based onfinding sub-clusters in the top-similar elements of everygiven element.In the final phase of the algorithm, each word isassigned to its most similar clusters (represented by acommittee).
Suppose a word w is assigned to a clusterc.
We then remove from w its features that intersectwith the features in c. Intuitively, this removes the csense from w, allowing CBC to discover the lessfrequent senses of a word and to avoid discoveringduplicate senses.
The word w is then assigned to itsnext most similar cluster and the process is repeated.4 ConclusionWe will demonstrate the senses discovered by CBC for54,685 words on the 3GB ACQUAINT corpus.
CBCdiscovered 24,497 polysemous words.ReferencesHarris, Z.
1985.
Distributional structure.
In: Katz, J.
J.(ed.)
The Philosophy of Linguistics.
New York:Oxford University Press.
pp.
26?47.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90.
pp.268?275.
Pittsburgh, PA.Hutchins, J. and Sommers, H. 1992.
Introduction toMachine Translation.
Academic Press.Landauer, T. K., and Dumais, S. T. 1997.
A solution toPlato's problem: The Latent Semantic Analysistheory of the acquisition, induction, and representa-tion of knowledge.
Psychological Review, 104:211?240.Lin, D. 1994.
Principar - an efficient, broad-coverage,principle-based parser.
In Proceedings of COLING-94.
pp.
42?48.
Kyoto, Japan.Lin, D. 1998.
Automatic retrieval and  clustering ofsimilar words.
In Proceedings of COLING/ACL-98.pp.
768?774.
Montreal, Canada.Manning, C. D. and Sch?tze, H. 1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.Pasca, M. and Harabagiu, S. 2001.
The informative roleof WordNet in Open-Domain Question Answering.In Proceedings of NAACL-01 Workshop on WordNetand Other Lexical Resources.
pp.
138?143.Pittsburgh, PA.Salton, G. and McGill, M. J.
1983.
Introduction toModern Information Retrieval.
McGraw Hill.Voorhees, E. M. 1998.
Using WordNet for textretrieval.
In WordNet: An Electronic LexicalDatabase, edited by C. Fellbaum.
pp.
285?303.
MITPress.
