Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 19?28,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsHierarchical Alignment Decomposition Labels for Hiero Grammar RulesGideon Maillette de Buy WennigerInstitute for Logic,Language and ComputationUniversity of AmsterdamScience Park 904, 1098 XH AmsterdamThe Netherlandsgemdbw AT gmail.comKhalil Sima?anInstitute for Logic,Language and ComputationUniversity of AmsterdamScience Park 904, 1098 XH AmsterdamThe Netherlandsk.simaan AT uva.nlAbstractSelecting a set of nonterminals for the syn-chronous CFGs underlying the hierarchicalphrase-based models is usually done on thebasis of a monolingual resource (like a syntac-tic parser).
However, a standard bilingual re-source like word alignments is itself rich withreordering patterns that, if clustered some-how, might provide labels of different (pos-sibly complementary) nature to monolinguallabels.
In this paper we explore a first ver-sion of this idea based on a hierarchical de-composition of word alignments into recursivetree representations.
We identify five clus-ters of alignment patterns in which the chil-dren of a node in a decomposition tree arefound and employ these five as nonterminal la-bels for the Hiero productions.
Although thisis our first non-optimized instantiation of theidea, our experiments show competitive per-formance with the Hiero baseline, exemplify-ing certain merits of this novel approach.1 IntroductionThe Hiero model (Chiang, 2007; Chiang, 2005)formulates phrase-based translation in terms of asynchronous context-free grammar (SCFG) limitedto the inversion transduction grammar (ITG) (Wu,1997) family.
While the original Hiero approachworks with a single nonterminal label (X) (besidesthe start nonterminal S ), more recent work is dedi-cated to devising methods for extracting more elab-orate labels for the phrase-pairs and their abstrac-tions into SCFG productions, e.g., (Zollmann andVenugopal, 2006; Li et al 2012; Almaghout et al2011).
All labeling approaches exploit monolin-gual parsers of some kind, e.g., syntactic, seman-tic or sense-oriented.
The rationale behind mono-lingual labeling is often to make the probability dis-tributions over alternative synchronous derivationsof the Hiero model more sensitive to linguisticallyjustified monolingual phrase context.
For example,syntactic target-language labels in many approachesare aimed at improved target language modeling(fluency, cf.
Hassan et al(2007); Zollmann andVenugopal (2006)), whereas source-language labelsprovide suitable context for reordering (see Mylon-akis and Sima?an (2011)).
It is usually believedthat the monolingual labels tend to stand for clus-ters of phrase pairs that are expected to be inter-substitutable, either syntactically or semantically(see Marton et al(2012) for an illuminating discus-sion).While we believe that monolingual labelingstrategies are sound, in this paper we explore thecomplementary idea that the nonterminal labelscould also signify bilingual properties of the phrasepair, particularly its characteristic word alignmentpatterns.
Intuitively, an SCFG with nonterminal la-bels standing for alignment patterns should put morepreference on synchronous derivations that mimicthe word alignment patterns found in the trainingcorpus, and thus, possibly allow for better reorder-ing.
It is important to stress the fact that these wordalignment patterns are complementary to the mono-lingual linguistic patterns and it is conceivable thatthe two can be combined effectively, but this remainsbeyond the scope of this article.The question addressed in this paper is how to se-lect word alignment patterns and cluster them intobilingual nonterminal labels?
In this paper we ex-plore a first instantiation of this idea starting outfrom the following simplifying assumptions:19?
The labels come from the word alignmentsonly,?
The labels are coarse-grained, pre-defined clus-ters and not optimized for performance,?
The labels extend the binary set of ITG oper-ators (monotone and inverted) into five suchlabels in order to cover non-binarizable align-ment patterns.Our labels are based on our own tree decomposi-tions of word alignments (Sima?an and Maillette deBuy Wenniger, 2011), akin to Normalized Decom-position Trees (NDTs) (Zhang et al 2008).
In thisfirst attempt we explore a set of five nonterminal la-bels that characterize alignment patterns found di-rectly under nodes in the NDT projected for everyword alignment in the parallel corpus during train-ing.
There is a range of work that exploits the mono-tone and inverted orientations of binary ITG withinhierarchical phrase-based models, either as featurefunctions of lexicalized Hiero productions (Chiang,2007; Zollmann and Venugopal, 2006), or as labelson non-lexicalized ITG productions, e.g., (Mylon-akis and Sima?an, 2011).
As far as we are aware,this is the first attempt at exploring a larger set ofsuch word alignment derived labels in hierarchicalSMT.
Therefore, we expect that there are many vari-ants that could improve substantially on our strongset of assumptions.2 Hierarchical SMT modelsHierarchical SMT usually works with weighted in-stantiations of Synchronous Context-Free Gram-mars (SCFGs) (Aho and Ullman, 1969).
SCFGsare defined over a finite set of nonterminals (startincluded), a finite set of terminals and a finite setof synchronous productions.
A synchronous pro-duction in an SCFG consists of two context-freeproductions (source and target) containing the samenumber of nonterminals on the right-hand side, witha bijective (1-to-1 and onto) function between thesource and target nonterminals.
Like the standardHiero model (Chiang, 2007), we constrain our workto SCFGs which involve at most two nonterminalsin every lexicalized production.Given an SCFG G, a source sentence s is trans-lated into a target sentence t by synchronous deriva-tions d, each is a finite sequence of well-formedsubstitutions of synchronous productions from G,see (Chiang, 2006).
Standardly, for complexity rea-sons, most models used make the assumption thatthe probability P(t | s) can be optimized through assingle best derivation as follows:arg maxtP(t | s) = arg maxt?d?GP(t,d | s) (1)?
arg maxd?GP(t,d | s) (2)This approximation can be notoriously problematicfor labelled Hiero models because the labels tendto lead to many more derivations than in the orig-inal model, thereby aggravating the effects of thisassumption.
This problem is relevant for our workand approaches to deal with it are Minimum Bayes-Risk decoding (Kumar and Byrne, 2004; Tromble etal., 2008), Variational Decoding (Li et al 2009) andsoft labeling (Venugopal et al 2009; Marton et al2012; Chiang, 2010).Given a derivation d, most existing phrase-based models approximate the derivation probabil-ity through a linear interpolation of a finite set offeature functions (?
(d)) of the derivation d, mostlyworking with local feature functions ?i of individ-ual productions, the target side yield string t of d(target language model features) and other heuristicfeatures discussed in the experimental section:arg maxd?GP(t,d | s) ?
arg maxd?G|?
(d)|?i=1?i ?
?i (3)Where ?i is the weight of feature ?i optimized overa held-out parallel corpus by some direct error-minimization procedure like MERT (Och, 2003).3 Baseline: Hiero Grammars (single label)Hiero Grammars (Chiang, 2005; Chiang, 2007) area particular form of SCFGs that generalize phrase-based translation models to hierarchical phrase-based Translation models.
They allow only up totwo (pairs of) nonterminals on the right-hand-side ofrules.
Hierarchical rules are formed from fully lex-icalized base rules (i.e.
phrase pairs) by replacing asub-span of the phrase pair that corresponds itself toa valid phrase pair with variable X called ?gap?.
Two20gaps may be maximally introduced in this way1, la-beled as X1 and X2 respectively for distinction.
Thetypes of permissible Hiero rules are:X ?
?
?, ??
(4a)X ?
??
X1 ?, ?
X1 ??
(4b)X ?
??
X1 ?
X2 ?
, ?
X1 ?
X2 ?
?
(4c)X ?
??
X1 ?
X2 ?
, ?
X2 ?
X1 ?
?
(4d)Here ?, ?, ?, ?, ?, ?
are terminal sequences, pos-sibly empty.
Equation 4a corresponds to a normalphrase pair, 4b to a rule with one gap and 4c and 4dto the monotone- and inverting rules respectively.An important extra constraint used in the originalHiero model is that rules must have at least one pairof aligned words, so that translation decisions are al-ways based on some lexical evidence.
Furthermorethe sum of terminals and nonterminals on the sourceside may not be greater than five, and nonterminalsare not allowed to be adjacent on the source side.4 Alignment Labeled GrammarsLabeling the Hiero grammar productions makesrules with gaps more restricted about what broadcategories of rules are allowed to substitute for thegaps.
In the best case this prevents overgeneraliza-tion, and makes the translation distributions moreaccurate.
In the worst case, however, it can also leadto too restrictive rules, as well as sparse translationdistributions.
Despite these inherent risks, a numberof approaches based on syntactically inspired labelshas succeeded to improve the state of the art byusing monolingual labels, e.g., (Zollmann andVenugopal, 2006; Zollmann, 2011; Almaghout etal., 2011; Chiang, 2010; Li et al 2012).Unlabeled Hiero derivations can be seen as recur-sive compositions of phrase pairs.
A single transla-tion may be generated by different derivations (seeequation 1), each standing for a choice of com-position rules over a choice of a segmentation ofthe source-target sentence pair into a bag of phrasepairs.
However, a synchronous derivation also in-duces an alignment between the different segments1The motivation for this restriction to two gaps is mainly apractical computational one, as it can be shown that translationcomplexity grows exponentially with the number of gaps.that it composes together.
Our goal here is to la-bel the Hiero rules in order to exploit aspects of thealignment that a synchronous derivation induces.We exploit the idea that phrase pairs can be ef-ficiently grouped into maximally decomposed trees(normalized decomposition trees ?
NDTs) (Zhanget al 2008).
In an NDT every phrase pair is re-cursively decomposed at every level into the mini-mum number of its phrase constituents, so that theresulting structure is maximal in that it contains thelargest number of nodes.
In Figure 1 left we showan example alignment and in Figure 1 right its as-sociated NDT.
The NDT shows pairs of source andtarget spans of (sub-) phrase pairs, governed at dif-ferent levels of the tree by their parent node.
Inour example the root node splits into three phrasepairs, but these three phrase pairs together do notmanage to cover the entire phrase pair of the par-ent because of the discontinuous translation struc-ture ?owe, sind ... schuldig?.
Consequently, a par-tially lexicalized structure with three children corre-sponding to phrase pairs and lexical items coveringthe words left by these phrase pairs is required.During grammar extraction we determine anAlignment Label for every left-hand-side and gap ofevery rule we extract.
This is done by looking at theNDT that decomposes their corresponding phrasepairs, and determining the complexity of the rela-tion with their direct children in this tree.
Complex-ity cases are ordered by preference, where the moresimple label corresponding to the choice of maximaldecomposition is preferred.
We distinguish the fol-lowing five cases, ordered by increasing complexity:1.
Monotonic: If the alignment can be split intotwo monotonically ordered parts.2.
Inverted: If the alignment can be split into twoinverted parts.3.
Permutation: If the alignment can be factoredas a permutation of more than 3 parts.24.
Complex: If the alignment cannot be factoredas a permutation of parts, but the phrase doescontain at least one smaller phrase pair (i.e., itis composite).5.
Atomic: If the alignment does not allow the ex-istence of smaller (child) phrase pairs.2Permutations of just 3 parts never occur in a NDT, as theycan always be further decomposed as a sequence of two binarynodes.211we2owe3this4to5our6citizensdas1sind2wir3unsern4burgern5schuldig6([1, 6], [1, 6])([5, 6], [4, 5])([6, 6], [5, 5])([5, 5], [4, 4])([3, 3], [1, 1])([1, 1], [3, 3])Figure 1: Example of complex word alignment, taken from Europarl data English-German (left) and its associatedNormalized Decomposition Tree (Zhang et al 2008) (right).We show examples of each of these cases in Figure2.
Furthermore, in Figure 3 we show an exampleof an alignment labeled Hiero rule based on one ofthese alignment examples.Our kind of labels has a completely different fla-vor from monolingual labels in that they cannot beseen as identifying linguistically meaningful clus-ters of phrase pairs.
These labels are mere latentbilingual clusters and the translation model mustmarginalize over them (equation 1) or use MinimumBayes-Risk decoding.4.1 Features : Relations over labelsIn this section we describe the features we use inour experiments.
To be unambiguous we first needto introduce some terminology.
Let r be a transla-tion rule.
We use p?
to denote probabilities estimatedusing simple relative frequency estimation from theword aligned sentence pairs of the training corpus.Then src(r) is the source side of the rule, includ-ing the source side of the left-hand-side label.
Simi-larly tgt(r) is the target side of the rule, including thetarget side of the left-hand-side label.
Furthermoreun(src(r)) is the source side without any nontermi-nal labels, and analogous for un(tgt(r)).4.1.1 Basic FeaturesWe use the following phrase probability features:?
p?
(tgt(r)|src(r)): Phrase probability target sidegiven source side?
p?
(src(r)|tgt(r)): Phrase probability source sidegiven target sideWe reinforce those by the following phrase prob-ability smoothing features:?
p?(tgt(r)|un(src(r)))?
p?(un(src(r))|tgt(r))?
p?(un(tgt(r))|src(r))?
p?(src(r)|un(tgt(r)))?
p?(un(tgt(r))|un(src(r)))?
p?
(un(src(r))|un(tgt(r)))We also add the following features:?
p?w(tgt(r)|src(r)), p?w(src(r)|tgt(r)): Lexicalweights based on terminal symbols as forphrase-based and hierarchical phrase-basedMT.?
p?
(r|lhs(r)) : Generative probability of a rulegiven its left-hand-side labelWe use the following set of basic binary features,with 1 values by default, and a value exp(1) if thecorresponding condition holds:?
?Glue(r): exp(1) if rule is a glue rule?
?lex(r): exp(1) if rule has only terminals onright-hand side?
?abs(r): exp(1) if rule has only nonterminals onright-hand side?
?st w tt(r): exp(1) if rule has terminals on thesource side but not on the target side?
?tt w st(r): exp(1) if rule has terminals on thetarget side but not on the source side?
?mono(r): exp(1) if rule has no inverted pair ofnonterminalsFurthermore we use the :?
?ra(r): Phrase penalty, exp(1) for all rules.?
exp(?wp(r)): Word penalty, exponent of thenumber of terminals on the target side?
?rare(r): exp( 1#(?r?
?C ?rr? )
) : Rarity penalty, with#(?r?
?C ?rr?)
being the count of rule r in the cor-pus.4.1.2 Binary Reordering FeaturesBesides the basic features we want to use extrasets of binary features that are specially designedto directly learn the desirability of certain broadclasses of reordering patterns, beyond the way thisis already implicitly learned for particular lexical-ized rules by the introduction of reordering labels.3These features can be seen as generalizations of themost simple feature that penalizes/rewards mono-3We did some initial experiments with such features inJoshua, but haven?t managed yet to get them working in Moseswith MBR.
Since these experiments are inconclusive withoutMBR we leave them out here.22this is an important matterdas ist ein wichtige angelegenheit1122Monotonewe all agree on thisdas sehen wir alle1122Inversioni want to stress two pointsauf zwei punkte mo?chte ich hinweisen11223344Permutationwe owe this to our citizensdas sind wir unsern burgern schuldig112233Complexit would be possiblekann mann11AtomicFigure 2: Different types of Alignment Labelstone order ?mono(r) from our basic feature set.
Thenew features we want to introduce ?fire?
for a spe-cific combination of reordering labels on the lefthand side and one or both gaps, plus optionally theinformation whether the rule itself invert its gaps andwhether or not it is abstract.5 ExperimentsWe evaluate our method on one language pair usingGerman as source and English as target.
The data isderived from parliament proceedings sourced fromthe Europarl corpus (Koehn, 2005), with WMT-07development and test data.
We used a maximumsentence length of 40 for filtering.
We employ ei-ther 200K or (approximately) 1000K sentence pairsfor training, 1K for development and 2K for test-ing (single reference per source sentence).
Both thebaseline and our method decode with a 3-gram lan-guage model smoothed with modified Knesser-Neydiscounting (Chen and Goodman, 1998), trained onthe target side of the full original training set (ap-proximately 1000K sentences).We compare against state-of-the-art hierarchi-cal translation (Chiang, 2005) baselines, based onthe Joshua (Ganitkevitch et al 2012) and Moses(Hoang et al 2007) translation systems with defaultdecoding settings.
We use our own grammar extrac-we owe this to our citizensdas sind wir unsern burgern schuldigX ComplexX Atomic1X Atomic1X Monotone2X Monotone2X ComplexFigure 3: Example of a labeled Hiero ruleX Complex?
?we owe X Atomic1 to X Monotone2 ,X Atomic1 sind wir X Monotone2 schuldig ?extracted from the Complex example in Figure 2 by re-placing the phrase pairs ?this, das?
and ?our citizens , un-sern burgern?
with (labeled) variables.tor for the generation of all grammars, including thebaseline Hiero grammars.
This enables us to use thesame features (as far as applicable given the gram-mar formalism) and assure true comparability of thegrammars under comparison.5.1 Training and Decoding DetailsIn this section we discuss the choices and settingswe used in our experiments.
Our initial experiments4We later discovered we needed to add the flag ??return-best-dev?
in Moses to actually get the weights from the bestdevelopment run, our initial experiments had not used this.
Thisexplains the somewhat unfortunate drop in performance in ourAnalysis Experiments.23DecodingTypeSystemName 200KLatticeMBRHiero 26.44Hiero-RL 26.72Viterbi Hiero 26.23Hiero-RL-PPL 26.16Table 1: Initial Results.
Lowercase BLEU results forGerman-English trained on 200K sentence pairs.4Top rows display results for our experiments using Moses(Hoang et al 2007) with Lattice Minimum Bayes-RiskDecoding5 (Tromble et al 2008) in combination withBatch Mira (Cherry and Foster, 2012) for tuning.
Beloware results for experiments with Joshua (Ganitkevitch etal., 2012) using Viterbi decoding (i.e.
no MBR) and PRO(Hopkins and May, 2011) for tuning.were done on Joshua (Ganitkevitch et al 2012),using the Viterbi best derivation.
The second setof experiments was done on Moses (Hoang et al2007) using Lattice Minimum Bayes-Risk Decod-ing5 (Tromble et al 2008) to sum over derivations.5.1.1 General SettingsTo train our system we use the following settings.We use the standard Hiero grammar extractionconstraints (Chiang, 2007) but for our reorderinglabeled grammars we use them with some modifi-cations.
In particular, while for basic Hiero onlyphrase pairs with source spans up to 10 are allowed,and abstract rules are forbidden, we allow extractionof fully abstract rules, without length constraints.Furthermore we allow their application withoutlength constraints during decoding.
Followingcommon practice, we use simple relative frequencyestimation to estimate the phrase probabilities,lexical probabilities and generative rule probabilityrespectively.65After submission we were told by Moses support that infact neither normal Minimum Bayes-Risk (MBR) nor LatticeMBR are operational in Moses Chart.6Personal correspondence with Andreas Zollmann furtherreinforced the authors appreciation of the importance of thisfeature introduced in (Zollmann and Venugopal, 2006; Zoll-mann, 2011).
Strangely enough this feature seems to be un-available in the standard Moses (Hoang et al 2007) and Joshua(Ganitkevitch et al 2012) grammar extractors, that also imple-ment SAMT grammar extraction5.1.2 Specific choices and settings JoshuaViterbi experimentsBased on experiments reported in (Mylonakis andSima?an, 2011; Mylonakis, 2012) we opted to notlabel the (fully lexicalized) phrase pairs, but insteadlabel them with a generic PhrasePair label and usea set of switch rules from all other labels to thePhrasePair label to enable transition between Hierorules and phrase pairs.We train our systems using PRO (Hopkins andMay, 2011) implemented in Joshua by Ganitkevitchet al(2012).
We use the standard tuning, where allfeatures are treated as dense features.We allow up to30 tuning iterations.
We further follow the PRO set-tings introduced in (Ganitkevitch et al 2012) butuse 0.5 for the coefficient ?
that interpolates theweights learned at the current with those from theprevious iteration.
We use the final learned weightsfor decoding with the log-linear model and reportLowercase BLEU scores for the tuned test set.5.1.3 Specific choices and settings MosesLattice MBR experimentsAs mentioned before we use Moses (Hoang etal., 2007) for our second experiment, in combina-tion with Lattice Minimum Bayes-Risk Decoding5(Tromble et al 2008).
Furthermore we use BatchMira (Cherry and Foster, 2012) for tuning with max-imum 10 tuning iterations of the 200K training set,and 30 for the 1000K training set.7For our Moses experiments we mainly workedwith a uniform labeling policy, labeling phrase pairsin the same way with alignment labels as normalrules.
This is motivated by the fact that since we areusing Minimum Bayes-Risk decoding, the risks ofsparsity from labeling are much reduced.
And label-ing everything does have the advantage that reorder-7We are mostly interested in the relative performance of oursystem in comparison to the baseline for the same settings.
Nev-ertheless, it might be that the labeled systems, which have moresmoothing features, are relatively suffering more from too lit-tle tuning iterations than the baseline which does not have theseextra features and thus may be easier to tune.
This was one ofthe reasons to increase the number of tuning iterations from 10to 30 in our later experiments on 1000K.
Usage of MinimumBayes-Risk decoding or not is crucial as we have explained be-fore in section 1.
The main reason we opted for Batch Mira overPRO is that it is more commonly used in Moses systems, and inany case at least superior to MERT (Och, 2003) in most cases.24ing information can be fully propagated in deriva-tions starting from the lowest (phrase) level.
We alsoran experiments with the generic phrase pair label-ing, since there were reasons to believe this coulddecrease sparsity and potentially lead to better re-sults.85.2 Initial ResultsWe report Lowercase BLEU scores for experi-ments with and without Lattice Minimum Bayes-Risk (MBR) decoding (Tromble et al 2008).
Ta-ble 1 bottom shows the results of our first experi-ments with Joshua, using the Viterbi derivation andno MBR decoding to sum over derivations.
Wedisplay scores for the Hiero baseline (Hiero) andthe (partially) alignment labeled system (Hiero-AL-PPL) which uses alignment labels for Hiero rulesand PhrasePair to label all phrase pairs.
Scores arearound 26.25 BLEU for both systems, with onlymarginal differences.
In summary our labeled sys-tems are at best comparable to the Hiero baseline.Table 1 top shows the results of our second ex-periments with Moses and Lattice MBR5.
Hereour (fully) alignment labeled system (Hiero-AL)achieves a score of 26.72 BLEU, in comparison to26.44 BLEU for the Hiero baseline (Hiero).
A smallimprovement of 0.28 BLEU point.5.3 Advanced experimentsWe now report Lowercase BLEU scores for moredetailed analysis experiments with and without Lat-tice Minimum Bayes-Risk5 (MBR) decoding, wherewe varied other training and decoding parameters inthe Moses environment.
Particularly, in this set ofexperiments we choose the best tuning parametersettings over 30 Batch Mira iterations (as opposedto the weights returned by default ?
used in the pre-vious experiments).
We also explore varieties in tun-ing with a decoder that works with Viterbi/MBR,and final testing with Viterbi/MBR.In Table 2, the top rows show the results of our ex-periments using MBR decoding.
We display scores8We discovered that the Moses chart decoder does not allowfully abstract unary rules in the current implementation, whichmakes direct usage of unary (switch) rules not possible.
Switchrules and other unaries can still be emulated though, by adapt-ing the grammar, using multiple copies of rules with differentlabels.
This blows up the grammar a bit, but at least works inpractice.DecodingTypeSystemName 200K 1000KLatticeMBRHiero 27.19 28.39Hiero-AL 26.61 28.32Hiero-AL-PPL 26.89 28.41Viterbi Hiero 26.80 28.57Hiero-AL 28.36Table 2: Analysis Results.
Lowercase BLEU results forGerman-English trained on 200K and 1000K sentencepairs using Moses (Hoang et al 2007) in combinationwith Batch Mira (Cherry and Foster, 2012) for tuning.Top rows display results for our experiments with LatticeMinimum Bayes-Risk Decoding5 (Tromble et al 2008).Below are results for experiments using Viterbi decoding(i.e.
no MBR) for tuning.
Results on 200K were run with10 tuning iterations, results on 1000K with 30 tuning it-erations.for the Hiero baseline (Hiero) and the fully/partiallyalignment labeled systems Hiero-AL and Hiero-AL-PPL.
In the preceding set of experiments MBR de-coding clearly showed improved performance overViterbi, particularly for our labelled system.On the small training set of 200K we observethat the Hiero baseline achieves 27.19 BLEU andthus beats the labeled systems Hiero-AL with 26.61BLEU and 26.89 BLEU by a good margin.
On thebigger dataset of 1000K and with more tuning iter-ations (3), all systems perform better.
When usingLattice MBR Hiero achieving 28.39 BLEU, Hiero-AL 28.32 BLEU and finally Hiero-AL-PPL achieves28.41.
These are insignificant differences in perfor-mance between the labelled and unlabeled systems.Table 1 bottom also shows the results of oursecond set of experiments with Viterbi decoding.Here, the baseline Hiero system for 200K trainingset achieves a score of 26.80 BLEU on the smalltraining set.
We also conducted another set ofexperiments on the larger training set of 1000K, thistime with Viterbi decoding.
The Hiero baseline withViterbi scores 28.57 BLEU while Hiero-AL scores28.36 BLEU under the same conditions.It is puzzling that Hiero Viterbi (for 1000k) per-forms better than the same system with MBR decod-ing systems.
But after submission we were told byMoses support that neither normal MBR nor LatticeMBR are operational in Moses Chart.
This meansthat in fact the effect of MBR on our labels remainsstill undecided, and more work is still needed in thisdirection.
The small decrease in performance for the25labelled system relative to Hiero (in Viterbi) is possi-bly the result of the labelled system being more brit-tle and harder to tune than the Hiero system.
Thishypothesis needs further exploration.While a whole set of experimental questions re-mains open, we think that based on this preliminarybut nevertheless considerable set of experiments, itseems that our labels do not always improve perfor-mance compared with the Hiero baseline.
It is possi-ble that these labels, under a more advanced imple-mentation via soft constraints (as opposed to hard la-beling), could provide the empirical evidence to ourtheoretical choices.
A further concern regarding thelabels is that our current choice (5 labels) is heuristicand not optimized for the training data.
It remains tobe seen in the future if proper learning of these labelsas latent variables optimized for the training data orthe use of soft constraints can shed more light on theuse of alignment labels in hierarchical SMT.5.4 AnalysisWhile we did not have time to do a deep compara-tive analysis of the properties of the grammars, a fewthings can be said based on the results.
First of allwe have seen that alignment labels do not always im-prove over the Hiero baseline.
In earlier experimentswe observed some improvement when the labelledgrammar was used in combination with MinimumBayes-Risk Decoding but not without it.
In later ex-periments with different tuning settings (Mira), theimprovements evaporated and in fact, the Viterbi Hi-ero baseline turned out, surprisingly, the best of allsystems.Our use of MBR is theoretically justified by theimportance of aggregating over the derivations of theoutput translations when labeling Hiero variables:statistically speaking, if the labels split the occur-rences of the phrase pairs, they will lead to multiplederivations per Hiero derivation with fractions of thescores.
This is in line with earlier work on the ef-fect of spurious ambiguity, e.g.
Variational Decod-ing (Li et al 2009).
Yet, in the case of our model,there is also a conceptual explanation for the need toaggregate over different derivations of the same sen-tence pair.
The decomposition of a word alignmentinto hierarchical decomposition trees has a interest-ing property: the simpler (less reordering) a wordalignment, the more (binary) decomposition trees ?and in our model derivations ?
it will have.
Hence,aggregating over the derivations is a way to gatherevidence for the complexity of alignment patternsthat our model can fit in between a given source-target sentence pair.
However, in the current exper-imental setting, where final tuning with Mira is cru-cial, and where the use of MBR within Moses is stillnot standard, we cannot reap full benefit of our the-oretical analysis concerning the fit of MBR for ourmodels?
alignment labels.6 ConclusionWe presented a novel method for labeling Hierovariables with nonterminals derived from the hierar-chical patterns found in recursive decompositions ofword alignments into tree representations.
Our ex-periments based on a first instantiation of this ideawith a fixed set of labels, not optimized to the train-ing data, show promising performance.
Our earlyexperiments suggested that these labels have merit,whereas later experiments with more varied trainingand decoder settings showed these results to be un-stable.Empirical results aside, our approach opens up awhole new line of research to improve the state ofthe art of hierarchical SMT by learning these la-tent alignment labels directly from standard wordalignments without special use of syntactic or otherparsers.
The fact that such labels are in principlecomplementary with monolingual information is anexciting perspective which we might explore in fu-ture work.AcknowledgementsThis work is supported by The Netherlands Organi-zation for Scientific Research (NWO) under grant nr.612.066.929.
This work was sponsored by the BIGGrid project for the use of the computing and storagefacilities, with financial support from the Nether-lands Organization of Scientific Research (NWO)under grant BG-087-12.
The authors would like tothank the people from the Joshua team at John Hop-kins University, in particular Yuan Cao, JonathanWeese, Matt Post and Juri Ganitkevitch, for theirhelpful replies to questions regarding Joshua and itsPRO and Packing implementations.26ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1969.
Syntaxdirected translations and the pushdown assembler.
J.Comput.
Syst.
Sci., 3(1):37?56.Hala Almaghout, Jie Jiang, and Andy Way.
2011.
Ccgcontextual labels in hierarchical phrase-based smt.
InProceedings of the 15th Annual Conference of the Eu-ropean Association for Machine Translation (EAMT-2011), May.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, Computer Sci-ence Group, Harvard University.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In HLT-NAACL, pages 427?436.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the ACL, pages 263?270,June.David Chiang.
2006.
An introduction to synchronousgrammars.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452.Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,and Chris Callison-Burch.
2012.
Joshua 4.0: Pack-ing, pro, and paraphrases.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,pages 283?291, Montre?al, Canada, June.
Associationfor Computational Linguistics.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proceedings of ACL 2007, page 288295.Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, andOndrej Bojar.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, pages 177?180.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1352?1362.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In HLT-NAACL, page 16917.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 2 - Volume 2, pages 593?601.Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef vanGenabith.
2012.
Using syntactic head information inhierarchical phrase-based translation.
In Proceedingsof the Seventh Workshop on Statistical Machine Trans-lation, pages 232?242.Yuval Marton, David Chiang, and Philip Resnik.
2012.Soft syntactic constraints for arabic?english hierar-chical phrase-based translation.
Machine Translation,26(1-2):137?157.Markos Mylonakis and Khalil Sima?an.
2011.
Learninghierarchical translation structure with linguistic anno-tations.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 642?652.Markos Mylonakis.
2012.
Learning the Latent Struc-ture of Translation.
Ph.D. thesis, University of Ams-terdam.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, pages 160?167.Khalil Sima?an and Gideon Maillette de Buy Wenniger.2011.
Hierarchical translation equivalence over wordalignments.
Technical Report PP-2011-38, Institutefor Logic, Language and Computation.Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice minimum bayes-riskdecoding for statistical machine translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 620?629.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: soft-ening syntactic constraints to improve statistical ma-chine translation.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 236?244.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.Hao Zhang, Daniel Gildea, and David Chiang.
2008.
Ex-tracting synchronous grammar rules from word-levelalignments in linear time.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics - Volume 1, pages 1081?1088.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
In27NAACL 2006 - Workshop on statistical machine trans-lation, June.Andreas Zollmann.
2011.
Learning Multiple-Nonterminal Synchronous Grammars for StatisticalMachine Translation.
Ph.D. thesis, Carnegie MellonUniversity.28
