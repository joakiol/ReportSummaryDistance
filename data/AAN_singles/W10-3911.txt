Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,Beijing, August 2010Adverse?Effect Relations Extraction fromMassive Clinical RecordsYasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe ca  Fuji Xerox Co., Ltd.b Center for Knowledge Structuring, University of Tokyoc University of Tokyo Hospitalyasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com,{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara,hiroshi.masuichi}@fujixerox.co.jp,kohe@hcc.h.u-tokyo.ac.jpAbstractThe rapid spread of electronic healthrecords raised an interest to large-scaleinformation extraction from clinicaltexts.
Considering such a background,we are developing a method that canextract adverse drug event and effect(adverse?effect) relations from massiveclinical records.
Adverse?effect rela-tions share some features with relationsproposed in previous relation extrac-tion studies, but they also have uniquecharacteristics.
Adverse?effect rela-tions are usually uncertain.
Not evenmedical experts can usually determinewhether a symptom that arises after amedication represents an adverse?effect relation or not.
We propose amethod to extract adverse?effect rela-tions using a machine-learning tech-nique with dependency features.
Weperformed experiments to extract ad-verse?effect relations from 2,577 clini-cal texts, and obtained F1-score of37.54 with an optimal parameters andF1-score of 34.90 with automaticallytuned parameters.
The results alsoshow that dependency features increasethe extraction F1-score by 3.59.1 IntroductionThe widespread use of electronic health rec-ords (EHR) made clinical texts to be stored ascomputer processable data.
EHRs contain im-portant information about patients?
health.However, extracting clinical information fromEHRs is not easy because they are likely to bewritten in a natural language.We are working on a task to extract adversedrug event and effect relations from clinicalrecords.
Usually, the association between adrug and its adverse?effect relation is investi-gated using numerous human resources, cost-ing much time and money.
The motivation ofour task comes from this situation.
An exampleof the task is presented in Figure 1.
We definedan adverse?effect relation as a relation thatholds between a drug entity and a symptomentity.
The sentence illustrates the occurrenceof the adverse?effect hepatic disorder by theSingulair medication.Figure 1.
Example of an adverse?effect relation.A hepatic disorder found was suspected drug-induced and the Singulair was stopped.adverse?effect relationsymptom drug75A salient characteristic of adverse?effect re-lations is that they are usually uncertain.
Thesentence in the example states that the hepaticdisorder is suspected drug-induced, whichmeans the hepatic disorder is likely to presentan adverse?effect relation.
Figure 2 presents anexample in which an adverse?effect relation issuspected, but words to indicate the suspicionare not stated.
The two effects of the drug?
?therecovery of HbA1c and the appearance of theedema?
?are expressed merely as observationresults in this sentence.
The recovery ofHbA1c is an expected effect of the drug andthe appearance of the edema probably repre-sents an adverse?effect case.
The uncertainnature of adverse?effect relations often engen-ders the statement of an adverse?effect rela-tion as an observed fact.
A sentence includ-ing an adverse?effect relation occasionally be-comes long to list all observations that ap-peared after administration of a medication.Whether an interpretation that expresses anadverse?effect relation, such as drug-inducedor suspected to be an adverse?effect, exists in aclinical record or not depends on a person whowrites it.
However, an adverse?effect relationis associated with an undesired effect of amedication.
Its appearance would engender anextra action (e.g.
stopped in the first example)or lead to an extra indication (e.g.
but ?
ap-peared in the second example).
Proper han-dling of this extra information is likely to boostthe extraction accuracy.The challenge of this study is to capture re-lations with various certainties.
To establishthis goal, we used a dependency structure forthe adverse?effect relation extraction method.Adverse?effect statements are assumed toshare a dependency structure to a certaindegree.
For example, if we obtain the depend-ency structures as shown in Figure 3, then wecan easily determine that the structures aresimilar.
Of course, obtaining such perfect pars-ing results is not always possible.
A statisticalsyntactic parser is known to perform badly if atext to be parsed belongs to a domain whichdiffers from a domain on which the parser istrained (Gildea, 2001).
A statistical parser willlikely output incomplete results in these textsand will likely have a negative effect on rela-tion extraction methods which depend on it.The specified research topic of this study is toinvestigate whether incomplete dependencystructures are effective and how they behave inthe extraction of uncertain relations.Figure 2.
The example of an adverse-effect relation where the suspicion is not stated.Figure 3.
The example of a similarity within dependency structures.ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.A suspected drug-induced hepatic disorder found and the Singulair was stopped.conjunctnominal subject nominal subjectnominal subject nominal subjectconjunctwasACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.adverse-effect relationdrug symptom762 Related WorksVarious studies have been done to extract se-mantic information from texts.
SemEval-2007Task:04 (Girju et al, 2007) is a task to extractsemantic relations between nominals.
The taskincludes ?Cause?Effect?
relation extraction,which shares some similarity with a task thatwill be presented herein.
Saeger et al (2008)presented a method to extract potential trou-bles or obstacles related to the use of a givenobject.
This relation can be interpreted as amore general relation of the adverse?effectrelation.
The protein?protein interaction (PPI)annotation extraction task of BioCreative II(Krallinger et al, 2008) is a task to extract PPIfrom PubMed abstracts.
BioNLP?09 SharedTask on Event Extraction (Kim et al, 2009) isa task to extract bio-molecular events (bio-events) from the GENIA event corpus.Similar characteristics to those of the ad-verse?effect relation are described in previousreports in the bio-medical domain.
Friedman etal.
(1994) describes the certainty in findings ofclinical radiology.
Certainty is also known inscientific papers of biomedical domains asspeculation (Light et al, 2004).
Vincze et al(2008) are producing a freely available corpusincluding annotations of uncertainty along withits scope.Dependency structure feature which we uti-lized to extract adverse?effect relations arewidely used in relation extraction tasks.
Wepresent previous works which used syntac-tic/dependency information as a feature of astatistical method.
Beamer et al (2007), Giuli-ano et al (2007), and Hendrickx et al (2007)all used syntactic information with machinelearning techniques in SemEval-2007 Task:04and achieved good performance.
Riedel et al(2009) used dependency path features with astatistical relational learning method in Bi-oNLP?09 Shared Task on Event Extraction andachieved the best performance in the event en-richment subtask.
Miyao et al (2008) com-pared syntactic information of various statisti-cal parsers on PPI.3 CorpusWe produced an annotated corpus of adverse?effect relations to develop and test an adverse?effect relation extraction method.
This sectionpresents a description of details of the corpus.3.1 Texts Comprising the CorpusWe used a discharge summary among variousdocuments in a hospital as the source data ofthe task.
The discharge summary is a docu-ment created by a doctor or another medicalexpert at the conclusion of a hospital stay.Medications performed during a stay are writ-ten in discharge summaries.
If adverse?effectrelations were observed during the stay, theyare likely to be expressed in free text.
Textswritten in discharge summaries tend to be writ-ten more roughly than texts in newspaper arti-cles or scientific papers.
For example, theamounts of medications are often written in aname-value list as shown below:?When admitted to the hospital, Artist 6 mg1x,Diovan 70 mg1x, Norvasac 5 mg1x and BPwas 145/83, but after dialysis, BP showed adecreasing tendency and in 5/14 Norvasac wasreduced to 2.5 mg1x.
?3.2 Why Adverse?Effect Relation Extrac-tion from Discharge Summaries isImportantIn many countries, adverse?effects are investi-gated through multiple phases of clinical trials,but unexpected adverse?effects occur in actualmedications.
One reason why this occurs isthat drugs are often used in combination withothers in actual medications.
Clinical trialsusually target single drug use.
For that reason,the combinatory uses of drugs occasionallyengender unknown effects.
This situation natu-rally motivates automatic adverse?effect rela-tion extraction from actual patient records.773.3 Corpus SizeWe collected 3,012 discharge summaries1 writ-ten in Japanese from all departments of a hos-pital.
To reduce a cost to survey the occurrenceof adverse?effects in the summaries, we firstsplit the summaries into two sets: SET-A,which contains keywords related to adverse?effects and SET-B, which do not contain thekeywords.
The keywords we used were ?stop,change, adverse effect?, and they were chosenbased on a heuristic.
The keyword filteringresulted to SET-A with 435 summaries andSET-B with 2,577 summaries.
Regarding SET-A, we randomly sampled 275 summaries andfour annotators annotated adverse?effect in-formation to these summaries to create the ad-verse?effect relation corpus.
For SET-B, thefour annotators checked the small portion ofthe summaries.
Cases of ambiguity were re-solved through discussion, and even suspiciousadverse?effect relations were annotated in thecorpus as positive data.
The overview of thesummary selection is presented in Figure 4.1 All private information was removed from them.The definition of private information was referredfrom the HIPAA guidelines.3.4 Quantities of Adverse?Effects in Clin-ical Texts55.6% (=158/275) of the summaries in SET-Acontained adverse?effects.
11.3% (=6/53) ofthe summaries in SET-B contained adverse?effects.
Since the ratio of SET-A:SET-B is14.4:85.6, we estimated that about 17.7%(=0.556?0.144+0.113?0.856) of the summar-ies contain adverse?effects.
Even consideringthat a summary may only include suspectedadverse?effects, we think that discharge sum-maries are a valuable resource to explore ad-verse?effects.3.5 Annotated InformationWe annotated information of two kinds to thecorpus: term information and relation infor-mation.
(1) Term AnnotationTerm annotation includes two tags: a tag toexpress a drug and a tag to express a drug ef-fect.
Table 1 presents the definition.
In thecorpus, 2,739 drugs and 12,391 effects wereannotated.
(2) Relation AnnotationAdverse?effect relations are annotated as the?relation?
attribute of the term tags.
We repre-sent the effect of a drug as a relation between adrug tag and a symptom tag.
Table 2 presentsTable 2.
Annotation examples.Figure 4.
The overview of the summaryselection.Table 1.
Markup scheme.The expression of a disease orsymptom: e.g.
endometrial cancer,headache.
This tag covers not only anoun phrase but also a verb phrasesuch as ?<symptom>feels a pain infront of the head</symptom>?.symptomThe expression of an administrateddrug: e.g.
Levofloxacin, Flexeril.drugDefinition and Examplestag<drug relation=?1?>ACTOS(30)</drug> broughtboth <symptom relation=?1?>headache<symptom>and <symptom relation=?1?>insomnia</symptom>.<drug relation=?1?>Ridora</drug> resumedbecause it is associated with an <symptomrelation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects,symptoms take a same relation ID.3,012dischargesummaries435summariesw/ keywords2,577summariesw/o keywords275summaries53summaries153summariesw/ adverse?effects122summariesw/o adverse?effects6summariesw/ adverse?effects47summariesw/o adverse?effectsYES NOContain keywords?Random samplingRandom samplingContain adverse?effects?Contain adverse?effects?YES YESNO NOSET-A (annotated corpus) SET-B78several examples, wherein ?relation=1?
de-notes the ID of a adverse?effect relation.
In thecorpus, 236 relations were annotated.4 Extraction MethodWe present a simple adverse?effect relationextraction method.
We extract drug?symptompairs from the corpus and discriminate themusing a machine-learning technique.
Featuresbased on morphological analysis and depend-ency analysis are used in discrimination.
Thisapproach is similar to the PPI extraction ap-proach of Miyao et al (2008), in which webinary classify pairs whether they are in ad-verse?effect relations or not.
A pattern-basedsemi-supervised approach like Saeger et al(2008), or more generally Espresso (Pantel andPennacchiotti, 2006), can also be taken, but wechose a pair classification approach to avoidthe effect of seed patterns.
To capture a viewof an adverseness of a drug, a statistic of ad-verse?effect relations is important.
We do notwant to favor certain patterns and chose a pairclassification approach to equally treat everyrelation.
Extraction steps of our method are aspresented below.STEP 1: Pair ExtractionAll combinations of drug?symptom pairs thatappear in a same sentence are extracted.
Pairs<drug relation=?1?>Lasix</drug> for<symptom>hyperpiesia</symptom> hasbeen suspended due to the appearance ofa <symptom relation=?1?>headache</symptom>.headacheLasixpositivehyperpiesiaLasixnegativesymptomdruglabelID Feature Definition and Examples1 Character Distance The number of characters between members of a pair.2 Morpheme Distance The number of morpheme between members of a pair.3 Pair Order Order in which a drug and a symptom appear in a text;?drug?symptom?
or ?symptom?drug?.4 Symptom Type The type of symptom: ?disease name?, ?medical test name?,or ?medical test value?.5 Morpheme Chain Base?forms of morphemes that appear between a pair.6 Dependency Chain Base?forms of morphemes included in the minimaldependency path of a pair.7 Case Frame Chain Verb, case frame, and object triples that appear between apair: e.g.
?examine?
??de?
(case particle) ?
?inhalation?,?begin?
??wo?
(case particle) ?
?medication?.8 Case FrameDependency ChainVerb, case frame, and object triples included in the minimaldependency path of a pair.Figure 6.
Dependency chain example.Figure 5.
Pair extraction example.hyperpiesia no-PPfor no-PPLasix wo-PPheadache no-PPappear niyori-PPsuspend ta-AUXLasix, wo-PP, headache, no-PP,appear, niyori-PP, suspend, ta-AUXminimal pathTable 3.
Features used in adverse-effect extraction.79with the same relation ID become positivesamples; pairs with different relation IDs be-come negative samples.
Figure 5 shows exam-ples of positive and negative samples.STEP 2: Feature ExtractionFeatures presented in Table 3 are extracted.The text in the corpus is in Japanese.
Somefeatures assume widely known characteristicsof Japanese.
For example, the dependency fea-ture allows a phrase to depend on only onephrase that appears after a dependent phrase.Figure 6 portrays an example of a dependencychain feature.
In the example, most terms weretranslated into English, excluding postpositions(PP) and auxiliaries (AUX), which are ex-pressed in italic.
To reduce the negative effectof feature sparsity, features which appeared inmore than three summaries are used for fea-tures with respective IDs 5?8.STEP 3: Machine LearningThe support vector machine (SVM) (Vapnik,1995) is trained using positive/negative labelsand features extracted in prior steps.
In testing,an unlabeled pair is given a positive or nega-tive label with the trained SVM.5 ExperimentWe performed two experiments to evaluate theextraction method.5.1 Experiment 1Experiment 1 aimed to observe the effects ofthe presented features.
Five combinations ofthe features were evaluated with a five-foldcross validation assuming that an optimal pa-rameter combination was obtained.
The exper-iment conditions are described below:A. Data7,690 drug?symptom pairs were extractedfrom the corpus.
Manually annotated infor-mation was used to identify drugs and symp-toms.
Within 7,690 pairs, 149 pairs failed toextract the dependency chain feature.
We re-moved these 149 pairs and used the remaining7,541 pairs in the experiment.
The 7,541 pairsconsisted of 367 positive samples and 7,174negative samples.B.
Feature CombinationsWe tested the five combinations of features inthe experiment.
Manually annotated infor-mation was used for the symptom type feature.Features related to morphemes are obtained byprocessing sentences with a Japanese mor-phology analyzer (JUMAN2 ver.
6.0).
Featuresrelated to dependency and case are obtained byprocessing sentences using a Japanese depend-ency parser (KNP ver.
3.0; Kurohashi and Na-gao, 1994).C.
EvaluationsWe evaluated the extraction method with allcombinations of SVM parameters in certain2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/juman-e.htmlEDCBAID35.4535.0134.3933.3026.72Precision41.0540.6743.0642.4346.21Recalllog(c)=1.0, log(g)=-5.0, p=0.10log(c)=1.0, log(g)=-5.0, p=0.10log(c)=1.0, log(g)=-5.0, p=0.10log(c)=1.0, log(g)=-5.0, p=0.10log(c)=3.0, log(g)=-5.0, p=0.10Parameters37.181,2,3,4,5,6,7,836.781,2,3,4,5,6,837.541,2,3,4,5,6,736.641,2,3,4,5,633.051,2,3,4,5F1-scoreFeatureCombinationTable 4.
Best F1-scores and their parameters.Figure 7.
Precision?recall distribution.80ranges.
We used LIBSVM3 ver.
2.89 as an im-plementation of SVM.
The radial basis func-tion (RBF) was used as the kernel function ofSVM.
The probability estimates option ofLIBSVM was used to obtain the confidencevalue of discrimination.The gamma parameter of the RBF kernelwas chosen from the range of [2-20, 20].
The Cparameter of SVM was chosen from the rangeof [2-10, 210].
The SVM was trained and testedon 441 combinations of gamma and C. In test-ing, the probability threshold parameter p be-tween [0.05, 0.95] was also chosen, and the F1-scores of all combination of gamma, C, and pwere calculated with five-fold cross validation.The best F1-scores and their parameter valuesfor each combination of features (optimal F1-scores in this setting) are portrayed in Table 4.The precision?recall distribution of F1-scoreswith feature combination C is presented inFigure 7.5.2 Experiment 2Experiment 2 aimed to observe the perfor-mance of our extraction method when SVMparameters were automatically tuned.
In thisexperiment, we performed two cross valida-tions: a cross validation to tune SVM parame-ters and another cross validation to evaluatethe extraction method.
The experiment condi-tions are described below:A. DataThe same data as Experiment 1 were used.B.
Feature CombinationFeature combination C, which performed bestin Experiment 1, was used.C.
Evaluation3 http://www.csie.ntu.edu.tw/~cjlin/libsvmTwo five-fold cross validations were per-formed.
The first cross validation divided thedata to 5 sets (A, B, C, D, and E) each consist-ing of development set and test set with theratio of 4:1.
The second cross validation trainand test all combination of SVM parameters (C,gamma, and p) in certain ranges and decide theoptimal parameter combination(s) for  the de-velopment sets of A, B, C, D, and E. The se-cond cross validation denotes the execution ofExperiment 1 for each development set.
Foreach optimal parameter combination of A, B,C, D, and E, the corresponding developmentset was trained and the trained model was test-ed on the corresponding test set.
The averageF1-score on five test sets marked 34.90, whichis 2.64 lower than the F1-score of Experiment 1with the same feature combination.6 DiscussionThe result of the experiment reveals the effec-tiveness of the dependency chain feature andthe case-frame chain feature.
This section pre-sents a description of the effects of several fea-tures in detail.
The section also mentions re-maining problems in our extraction method.6.1 Effects of the Dependency Chain Fea-ture and Case-frame FeaturesA.
Dependency Chain FeatureThe dependency chain features improved theF1-score by 3.59 (the F1-score difference be-tween feature combination A and B).
This in-crease was obtained using 260 improved pairsand 127 deproved pairs.
Improved pairs con-Figure 8.
Relation between the number ofpairs and the morpheme distance.Figure 9.
Number of dependency errorsin the improved pairs sentences.259323sentencewith no errorsentencewith 1?3errorssentencewith 4 ormore errors01020304050distance lessthan 40distance larger thanor equal to 40frequencyimproveddeproved81tribute to the increase of a F1-score.
Deprovedpairs have the opposite effect.We observed that improved pairs tend tohave longer morpheme distance compared todeproved pairs.
Figure 8 shows the relationbetween the number of pairs and the mor-pheme distance of improved pairs and de-proved pairs.
The ratio between the improvedpairs and the deproved pairs is 11:1 when thedistance is greater than 40.
In contrast, theratio is 2:1 when the distance is smaller than40.
This observation suggests that adverse?effect relations share dependency structures toa certain degree.We also observed that in improved pairs,dependency errors tended to be low.
Figure 9presents the manually counted number of de-pendency errors in the 141 sentences in whichthe 260 improved pairs exist: 65.96 % of thesentences included 1?3 errors.
The result sug-gests that the dependency structure is effectiveeven if it includes small errors.B.
Case-frame FeaturesThe effect of the case-frame dependency chainfeature differed with the effect of the depend-ency chain feature.
The case-frame chain fea-ture improved the F1-score by 0.90 (the F1-score difference between feature combinationB and C), but the case-frame dependency chainfeature decreased the F1-score by 0.36 (the F1-score difference between feature combinationC and E).
One reason for the negative effect ofthe case-frame dependency feature might befeature sparsity, but no clear evidence of it hasbeen found.6.2 Remaining ProblemsA.
Imbalanced DataThe adverse?effect relation pairs we used inthe experiment were not balanced.
Low valuesof optimal probability threshold parameter psuggest the degree of imbalance.
We are con-sidering introduction of some kind of method-ology to reduce negative samples or to use amachine learning method that can accommo-date imbalanced data well.B.
Use of Medical ResourcesThe extraction method we propose uses nomedical resources.
Girju et al (2007) indicatethe effect of WordNet senses in the classifica-tion of a semantic relation between nominals.Krallinger et al (2008) report that top scoringteams in the interaction pair subtask used so-phisticated interactor protein normalizationstrategies.
If medical terms in texts can bemapped to a medical terminology or ontology,it would likely improve the extraction accuracy.C.
Fully Automated ExtractionIn the experiments, we used the manuallyannotated information to extract pairs and fea-tures.
This setting is, of course, not real if weconsider a situation to extract adverse?effectrelations from massive clinical records, but wechose it to focus on the relation extractionproblem.
We performed an event recognitionexperiment (Aramaki et al, 2009) andachieved F1-score of about 80.
We assume thatdrug expressions and symptom expressions tobe automatically recognized in a similar accu-racy.We are planning to perform a fully automat-ed adverse?effect relations extraction from alarger set of clinical texts to see the perfor-mance of our method on a raw corpus.
Theextraction F1-score will likely to decrease, butwe intend to observe the other aspect of theextraction, like the overall tendency of extract-ed relations.7 ConclusionWe presented a method to extract adverse?effect relations from texts.
One importantcharacteristic of adverse?effect relations is thatthey are uncertain in most cases.
We per-formed experiments to extract adverse?effectrelations from 2,577 clinical texts, and ob-tained F1-score of 37.54 with optimal SVMparameters and F1-score of 34.90 with auto-matically tuned SVM parameters.
Results alsoshow that dependency features increase theextraction F1-score by 3.59.
We observed thatan increased F1-score was obtained using theimprovement of adverse?effects with longmorpheme distance, which suggests that ad-verse?effect relations share dependency struc-tures to a certain degree.
We also observed thatthe increase of the F1-score was obtained withdependency structures that include small errors,which suggests that the dependency structureis effective even if it includes small errors.82ReferencesAramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike,Tomoko Ohkuma, Hiroshi Masuichi, andKazuhiko Ohe.
2009.
TEXT2TABLE: MedicalText Summarization System Based on NamedEntity Recognition and Modality Identification.In Proceedings of the BioNLP 2009 Workshop,pages 185-192.Beamer, Brandon, Suma Bhat, Brant Chee, AndrewFister, Alla Rozovskaya, and Roxana Girju.2007.
UIUC: A Knowledge-rich Approach toIdentifying Semantic Relations between Nomi-nals.
In Proceedings of Fourth InternationalWorkshop on Semantic Evaluations, pages 386-389.Friedman, Carol, Philip O. Alderson, John H. M.Austin, James J. Cimino, and Stephen B. John-son.
1994.
A General Natural-language TextProcessor for Clinical Radiology.
Journal of theAmerican Medical Informatics Association, 1(2),pages 161-174.Gildea, Daniel.
2001.
Corpus Variation and ParserPerformance.
In Proceedings of the 2001 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 1-9.Girju, Roxana, Preslav Nakov, Vivi Nastase,  StanSzpakowicz, Peter Turney, and Deniz Yuret.2007.
SemEval-2007 task 04: Classification ofSemantic Relations between Nominals.
In Pro-ceedings of Fourth International Workshop onSemantic Evaluations, pages 13-18.Giuliano, Claudio, Alberto Lavelli, Daniele Pighin,and Lorenza Romano.
2007.
FBK-IRST: KernelMethods for Semantic Relation Extraction.
InProceedings of the 4th International Workshopon Semantic Evaluations, pages 141-144.Hendrickx , Iris, Roser Morante, Caroline Sporleder,and Antal van den Bosch.
2007.
ILK: Machinelearning of semantic relations with shallow fea-tures and almost no data.
In Proceedings of the4th International Workshop on Semantic Evalua-tions, 187-190.Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo,Yoshinobu Kano, and Jun?ichi Tsujii.
2009.Overview of  BioNLP?09 Shared Task on EventExtraction.
In Proceedings of the BioNLP 2009Workshop Companion Volume for Shared Task,pages 1-9.Krallinger, Martin, Florian Leitner, CarlosRodriguez-Penagos, and Alfonso Valencia.
2008.Overview of the protein-protein interaction an-notation extraction task of BioCreative II.
Ge-nome Biology 2008, 9(Suppl 2):S4.Kurohashi, Sadao and Makoto Nagao.
1994.
KNParser : Japanese Dependency/Case StructureAnalyzer.
In Proceedings of The InternationalWorkshop on Sharable Natural Language Re-sources, pages 22-28.
Software available athttp://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html.Light, Marc, Xin Ying Qiu, and Padmini Srinivasan.2004.
The Language of Bioscience: Facts, Spec-ulations, and Statements in Between.
In Pro-ceedings of HLT/NAACL 2004 Workshop: Bi-oLINK 2004, Linking Biological Literature, On-tologies and Databases, pages 17-24.Miyao, Yusuke, Rune S?tre, Kenji Sagae, TakuyaMatsuzaki, and Jun'ichi Tsujii.
2008.
Task-oriented Evaluation of Syntactic Parsers andTheir Representations.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies, pages 46-54.Pantel, Patrick and Marco Pennacchiotti.
2006.
Es-presso: Leveraging Generic Patterns for Auto-matically Harvesting Semantic Relations.
InProceedings of the 21st International Confer-ence on Computational Linguistics and 44th An-nual Meeting of the Association for Computa-tional Linguistics, pages 113-120.Riedel, Sebastian, Hong-Woo Chun, ToshihisaTakagi, and Jun'ichi Tsujii.
2009.
A MarkovLogic Approach to Bio-Molecular Event Extrac-tion.
In Proceedings of the BioNLP 2009 Work-shop Companion Volume for Shared Task, pages41-49.Saeger, Stijn De, Kentaro Torisawa, and Jun?ichiKazama.
2008.
Looking for Trouble.
In Proceed-ings of the 22nd International Conference onComputational Linguistics, pages 185-192.Vapnik, Vladimir N.. 1995.
The Nature of Statisti-cal Learning Theory.
Springer-Verlag New York,Inc..Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for un-certainty, negation and their scopes.
BMC Bioin-formatics 2008, 9(Suppl 11):S9.83
