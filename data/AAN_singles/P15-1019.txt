Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 188?197,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGenerative Event Schema Induction with Entity DisambiguationKiem-Hieu Nguyen1, 2Xavier Tannier3, 1Olivier Ferret2Romaric Besanc?on2(1) LIMSI-CNRS(2) CEA, LIST, Laboratoire Vision et Ingnierie des Contenus, F-91191, Gif-sur-Yvette(3) Univ.
Paris-Sud{nguyen,xtannier}@limsi.fr, {olivier.ferret,romaric.besancon}@cea.frAbstractThis paper presents a generative model toevent schema induction.
Previous meth-ods in the literature only use head wordsto represent entities.
However, elementsother than head words contain useful in-formation.
For instance, an armed manis more discriminative than man.
Ourmodel takes into account this informationand precisely represents it using proba-bilistic topic distributions.
We illustratethat such information plays an importantrole in parameter estimation.
Mostly, itmakes topic distributions more coherentand more discriminative.
Experimentalresults on benchmark dataset empiricallyconfirm this enhancement.1 IntroductionInformation Extraction was initially defined (andis still defined) by the MUC evaluations (Grish-man and Sundheim, 1996) and more specificallyby the task of template filling.
The objective ofthis task is to assign event roles to individual tex-tual mentions.
A template defines a specific typeof events (e.g.
earthquakes), associated with se-mantic roles (or slots) hold by entities (for earth-quakes, their location, date, magnitude and thedamages they caused (Jean-Louis et al, 2011)).Schema induction is the task of learning thesetemplates with no supervision from unlabeled text.We focus here on event schema induction and con-tinue the trend of generative models proposed ear-lier for this task.
The idea is to group togetherentities corresponding to the same role in an eventtemplate based on the similarity of the relationsthat these entities hold with predicates.
For ex-ample, in a corpus about terrorist attacks, enti-ties that are objects of verbs to kill, to attack canbe grouped together and characterized by a rolenamed VICTIM.
The output of this identificationoperation is a set of clusters of which membersare both words and relations, associated with theirprobability (see an example later in Figure 4).These clusters are not labeled but each of themrepresents an event slot.Our approach here is to improve this initial ideaby entity disambiguation.
Some ambiguous enti-ties, such as man or soldier, can match two differ-ent slots (victim or perpetrator).
An entity such asterrorist can be mixed up with victims when arti-cles relate that a terrorist has been killed by police(and thus is object of to kill).
Our hypothesis isthat the immediate context of entities is helpful fordisambiguating them.
For example, the fact thatman is associated with armed, dangerous, heroicor innocent can lead to a better attribution and def-inition of roles.
We then introduce relations be-tween entities and their attributes in the model bymeans of syntactic relations.The document level, which is generally a cen-ter notion in topic modeling, is not used in ourgenerative model.
This results in a simpler, moreintuitive model, where observations are generatedfrom slots, that are defined by probabilistic dis-tributions on entities, predicates and syntactic at-tributes.
This model offers room for further exten-sions since multiple observations on an entity canbe represented in the same manner.Model parameters are estimated by Gibbs sam-pling.
We evaluate the performance of this ap-proach by an automatic and empiric mapping be-tween slots from the system and slots from the ref-erence in a way similar to previous work in thedomain.The rest of this paper is organized as follows:Section 2 briefly presents previous work; in Sec-tion 3, we detail our entity and relation represen-tation; we describe our generative model in Sec-tion 4, before presenting our experiments and eval-uations in Section 5.1882 Related WorkDespite efforts made for making template fill-ing as generic as possible, it still depends heav-ily on the type of events.
Mixing genericprocesses with a restrictive number of domain-specific rules (Freedman et al, 2011) or exam-ples (Grishman and He, 2014) is a way to reducethe amount of effort needed for adapting a sys-tem to another domain.
The approaches of On-demand information extraction (Hasegawa et al,2004; Sekine, 2006) and Preemptive InformationExtraction (Shinyama and Sekine, 2006) tried toovercome this difficulty in another way by exploit-ing templates induced from representative docu-ments selected by queries.Event schema induction takes root in workon the acquisition from text of knowledge struc-tures, such as the Memory Organization Pack-ets (Schank, 1980), used by early text under-standing systems (DeJong, 1982) and more re-cently by Ferret and Grau (1997).
First attemptsfor applying such processes to schema induc-tion have been made in the fields of Informa-tion Extraction (Collier, 1998), Automatic Sum-marization (Harabagiu, 2004) and event Question-Answering (Filatova et al, 2006; Filatova, 2008).More recently, work after (Hasegawa et al,2004) has developed weakly supervised formsof Information Extraction including schema in-duction in their objectives.
However, they havebeen mainly applied to binary relation extractionin practice (Eichler et al, 2008; Rosenfeld andFeldman, 2007; Min et al, 2012).
In parallel,several approaches were proposed for perform-ing specifically schema induction in already ex-isting frameworks: clause graph clustering (Qiuet al, 2008), event sequence alignment (Reg-neri et al, 2010) or LDA-based approach relyingon FrameNet-like semantic frames (Bejan, 2008).More event-specific generative models were pro-posed by Chambers (2013) and Cheung et al(2013).
Finally, Chambers and Jurafsky (2008),Chambers and Jurafsky (2009), Chambers and Ju-rafsky (2011), improved by Balasubramanian et al(2013), and Chambers (2013) focused specificallyon the induction of event roles and the identifica-tion of chains of events for building representa-tions from texts by exploiting coreference resolu-tion or the temporal ordering of events.
All thiswork is also linked to work about the induction ofscripts from texts, more or less closely linked toAttributes Head Triggers#1 [armed:amod] man [attack:nsubj,kill:nsubj]#2 [police:nn] station [attack:dobj]#3 [] policeman [kill:dobj]#4 [innocent:amod, man [wound:dobj]young:amod]Figure 1: Entity representation as tuples of ([at-tributes], head, [triggers]).events, such as (Frermann et al, 2014), (Pichottaand Mooney, 2014) or (Modi and Titov, 2014).The work we present in this article is in linewith Chambers (2013), which will be described inmore details in Section 5, together with a quanti-tative and qualitative comparison.3 Entity RepresentationAn entity is represented as a triple containing: ahead word h, a list A of attribute relations and alist T of trigger relations.
Consider the followingexample:(1) Two armed men attacked the police stationand killed a policeman.
An innocent youngman was also wounded.As illustrated in Figure 1, four entities, equiva-lent to four separated triples, are generated fromthe text above.
Head words are extracted fromnoun phrases.
A trigger relation is composedof a predicate (attack, kill, wound) and a depen-dency type (subject, object).
An attribute rela-tion is composed of an argument (armed, police,young) and a dependency type (adjectival, nomi-nal or verbal modifier).
In the relationship to trig-gers, a head word is argument, but in the relation-ship to attributes, it is predicate.
We use StanfordNLP toolkit (Manning et al, 2014) for parsing andcoreference resolution.A head word is extracted if it is a nominal orproper noun and it is related to at least one trig-ger; pronouns are omitted.
A trigger of an headword is extracted if it is a verb or an eventive nounand the head word serves as its subject, object, orpreposition.
We use the categories noun.EVENTand noun.ACT in WordNet as a list of eventivenouns.
A head word can have more than one trig-ger.
These multiple relations can come from a syn-tactic coordination inside a single sentence, as itis the case in the first sentence of the illustratingexample.
They can also represent a coreference189ht??uni(1,K)#tuplesa?sdir(?)dir(?)dir(?
)ATFigure 2: Generative model for event induction.chain across sentences, as we use coreference res-olution to merge the triggers of mentions corefer-ing to the same entity in a document.
Coreferencesare useful sources for event induction (Chambersand Jurafsky, 2011; Chambers, 2013).
Finally, anattribute is extracted if it is an adjective, a noun ora verb and serves as an adjective, verbal or nom-inal modifier of a head word.
If there are severalmodifiers, only the closest to the head word is se-lected.
This ?best selection?
heuristic allows toomit non-discriminative attributes for the entity.4 Generative Model4.1 Model DescriptionFigure 2 shows the plate notation of our model.For each triple representing an entity e, the modelfirst assigns a slot s for the entity from an uni-form distribution uni(1,K).
Its head word h isthen generated from a multinominal distributionpis.
Each tiof event trigger relations Teis gen-erated from a multinominal distribution ?s.
Eachajof attribute relations Aeis similarly generatedfrom a multinominal distribution ?s.
The distri-butions ?, pi, and ?
are generated from Dirichletpriors dir(?
), dir(?)
and dir(?)
respectively.Given a set of entities E, our model (pi, ?, ?)
isdefined byPpi,?,?
(E) =?e?EPpi,?,?
(e) (2)where the probability of each entity e is defined byPpi,?,?
(e) = P (s)?
P (h|s)?
?t?TeP (t|s)?
?a?AeP (a|s) (3)The generative story is as follows:for slot s?
1 to K doGenerate an attribute distribution ?sfrom aDirichlet prior dir(?
);Generate a head distribution pisfrom a Dirichletprior dir(?
);Generate a trigger distribution ?sfrom a Dirichletprior dir(?
);endfor entity e ?
E doGenerate a slot s from a uniform distributionuni(1,K);Generate a head h from a multinominal distributionpis;for i?
1 to |Te| doGenerate a trigger tifrom a multinominaldistribution ?s;endfor j ?
1 to |Ae| doGenerate an attribute ajfrom a multinominaldistribution ?s;endend4.2 Parameter EstimationFor parameter estimation, we use the Gibbs sam-pling method (Griffiths, 2002).
The slot variables is sampled by integrating out all the other vari-ables.Previous models (Cheung et al, 2013; Cham-bers, 2013) are based on document-level topicmodeling, which originated from models such asLatent Dirichlet Allocation (Blei et al, 2003).Our model is, instead, independent from docu-ment contexts.
Its input is a sequence of entitytriples.
Document boundary is only used in a post-processing step of filtering (see Section 5.3 formore details).
There is a universal slot distribu-tion instead of each slot distribution for one doc-ument.
Furthermore, slot prior is ignored by us-ing a uniform distribution as a particular case ofcategorical probability.
Sampling-based slot as-signment could depend on initial states and ran-dom seeds.
In our implementation of Gibbs sam-pling, we use 2,000 burn-in of overall 10,000 it-erations.
The purpose of burn-in is to assure thatparameters converge to a stable state before esti-mating the probability distributions.
Moreover, aninterval step of 100 is applied between consecutivesamples in order to avoid too strong coherence.Particularly, for tracking changes in probabili-ties resulting from attribute relations, we ran inthe first stage a specific burn-in with only headsand trigger relations.
This stable state was thenused as initialization for the second burn-in in1900.00050.0010.00150.0020.00250.0030.00350.00410  20  30  40  50  60  70  80  90  100P(terrorist|ATTACK victim)BURN_IN iterations (x20)Using attributesNo attribute(a) P (terrorist|ATTACK victim)00.0050.010.0150.020.0250.030.0350.0410  20  30  40  50  60  70  80  90  100P(terrorist|ATTACK perpetrator)BURN_IN iterations (x20)Using attributesNo attribute(b) P (terrorist|ATTACK perp)00.050.10.150.20.250.310  20  30  40  50  60  70  80  90  100P(kill:dobj|ATTACK victim)BURN_IN iterations (x20)Using attributesNo attribute(c) P (kill : dobj|ATTACK victim)00.0050.010.0150.0210  20  30  40  50  60  70  80  90  100P(kill:dobj|ATTACK perpetrator)BURN_IN iterations (x20)Using attributesNo attribute(d) P (kill : dobj|ATTACK perp)Figure 3: Probability convergence when using attributes in sampling.
The use of attributes is startedat point 50 (i.e., 50% of burn-in phase).
The dotted line shows convergence without attributes; thecontinuous line shows convergence with attributes.which attributes, heads, and triggers were used al-together.
This specific experimental setting madeus understand how the attributes modified distri-butions.
We observed that non-ambiguous wordsor relations (i.e.
explode, murder:nsubj) were onlyslightly modified whereas probabilities of ambigu-ous words such as man, soldier or triggers such askill:dobj or attack:nsubj converged smoothly to adifferent stable state that was semantically morecoherent.
For instance, the model interestingly re-alized that even if a terrorist was killed (e.g.
bypolice), he was not actually a real victim of an at-tack.
Figure 3 shows probability convergences ofterrorist and kill:dobj given ATTACK victim andATTACK perpetrator.5 EvaluationsIn order to compare with related work, we eval-uated our method on the Message UnderstandingConference (MUC-4) corpus (Sundheim, 1991)using precision, recall and F-score as conventionalmetrics for template extraction.In what follows, we first introduce the MUC-4 corpus (Section 5.1.1), we detail the mappingtechnique between learned slots and referenceslots (5.1.2) as well as the hyper-parameters ofour model (5.1.3).
Next, we present a first exper-iment (Section 5.2) showing how using attributerelations improves overall results.
The second ex-periment (Section 5.3) studies the impact of doc-ument classification.
We then compare our re-sults with previous approaches, more particularlywith Chambers (2013), from both quantitative andqualitative points of view (Section 5.4).
Finally,Section 5.5 is dedicated to error analysis, with aspecial emphasis on sources of false positives.5.1 Experimental Setups5.1.1 DatasetsThe MUC-4 corpus contains 1,700 news articlesabout terrorist incidents happening in Latin Amer-ica.
The corpus is divided into 1,300 documents191for the development set and four test sets, eachcontaining 100 documents.We follow the rules in the literature to guaranteecomparable results (Patwardhan and Riloff, 2007;Chambers and Jurafsky, 2011).
The evaluation fo-cuses on four template types ?
ARSON, ATTACK,BOMBING, KIDNAPPING ?
and four slots ?
Perpe-trator, Instrument, Target, and Victim.
Perpetratoris merged from Perpetrator Individual and Perpe-trator Organization.
The matching between sys-tem answers and references is based on head wordmatching.
A head word is defined as the right-most word of the phrase or as the right-most wordof the first ?of?
if the phrase contains any.
Op-tional templates and slots are ignored when calcu-lating recall.
Template types are ignored in eval-uation: this means that a perpetrator of BOMBINGin the answers could be compared to a perpetratorof ARSON, ATTACK, BOMBING or KIDNAPPING inthe reference.5.1.2 Slot MappingThe model learns K slots and assigns each entityin a document to one of the learned slots.
Slotmapping consists in matching each reference slotto an equivalent learned slot.Note that among the K learned slots, some areirrelevant while others, sometimes of high quality,contain entities that are not part of the reference(spatio-temporal information, protagonist context,etc.).
For this reason, it makes sense to have muchmore learned slots than expected event slots.Similarly to previous work in the literature, weimplemented an automatic empirical-driven slotmapping.
Each reference slot was mapped tothe learned slot that performed the best on thetask of template extraction according to the F-score metric.
Here, two identical slots of twodifferent templates, such as ATTACK victim andKIDNAPPING victim, must to be mapped sepa-rately.
Figure 4 shows the most common words oftwo learned slots which were mapped to BOMB-ING instrument and KIDNAPPING victim.
Thismapping is then kept for testing.5.1.3 Parameter TuningWe first tuned hyper-parameters of the models onthe development set.
The number of slots was setto K = 35.
Dirichlet priors were set to ?
= 0.1,?
= 1 and ?
= 0.1.
The model was learned fromthe whole dataset.
Slot mapping was done on tst1and tst2.
Outputs from tst3 and tst4 were eval-BOMBING instrumentAttributes Heads Triggerscar:nn bomb explode:nsubjpowerful:amod fire hear:dobjexplosive:amod explosion place:dobjdynamite:nn blow cause:nsubjheavy:amod charge set:dobjKIDNAPPING victimAttributes Heads Triggersseveral:amod people arrest:dobjother:amod person kidnap:dobjresponsible:amod man release:dobjmilitary:amod member kill:dobjyoung:amod leader identify:prep asFigure 4: Attribute, head and trigger distributionslearned by the model HT+A for learned slots thatwere mapped to BOMBING instrument and KID-NAPPING victim.uated using references and were averaged acrossten runs.5.2 Experiment 1: Using Entity AttributesIn this experiment, two versions of our model arecompared: HT+A uses entity heads, event triggerrelations and entity attribute relations.
HT usesonly entity heads and event triggers and omits at-tributes.We studied the gain brought by attribute re-lations with a focus on their effect when coref-erence information was available or was miss-ing.
The variations on the model input are namedsingle, multi and coref.
Single input has onlyone event trigger for each entity.
A text likean armed man attacked the police station andkilled a policeman results in two triples for theentity man: (armed:amod, man, attack:nsubj) and(armed:amod, man, kill:nsubj).
In multi input, oneentity can have several event triggers, leading forthe text above to the triple (armed:amod, man, [at-tack:nsubj, kill:nsubj]).
The coref input is richerthan multi in that, in addition to triggers from thesame sentence, triggers linked to the same coref-ered entity are merged together.
For instance, ifman in the above example corefers with he inHe was arrested three hours later, the mergedtriple becomes (armed:amod, man, [attack:nsubj,kill:nsubj, arrest:dobj]).
The plate notations ofthese model+data combinations are given in Fig-ure 5.Table 1 shows a consistent improvement whenusing attributes, both with and without corefer-ences.
The best performance of 40.62 F-score isobtained by the full model on inputs with coref-192ht??uni(1,K)#tupless(a)ht??uni(1,K)#tuplessa(b)ht??uni(1,K)#tuplesa?sd(c)ht?
?uni(1,K)#tuplesa?sdr(d)Figure 5: Model variants (Dirichlet priors are omitted for simplicity): 5a) HT model ran on single data.This model is equivalent to 5b) with T=1; 5b) HT model ran on multi data; 5c) HT+A model ran onsingle data; 5d) HT+A model ran on multi data.Data HT HT+AP R F P R FSingle 29.59 51.17 37.48 30.22 52.41 38.33Multi 29.32 52.21 37.52 30.82 51.68 38.55Coref 39.99 53.53 40.01 32.42 54.59 40.62Table 1: Improvement from using attributes.erences.
Using both attributes in the model andcoreference to generate input data results in a gainof 3 F-score points.5.3 Experiment 2: Document ClassificationIn the second experiment, we evaluated our modelwith a post-processing step of document classifi-cation.The MUC-4 corpus contains many ?irrelevant?documents.
A document is irrelevant if it containsno template.
Among 1,300 documents in the de-velopment set, 567 are irrelevant.
The most chal-lenging part is that there are many terrorist entities,e.g.
bomb, force, guerrilla, occurring in irrelevantdocuments.
That makes filtering out those docu-ments important, but difficult.
As document clas-sification is not explicitly performed by our model,a post-processing step is needed.
Document clas-sification is expected to reduce false positives in ir-relevant documents while not dramatically reduc-ing recall.Given a document d with slot-assigned entitiesand a set of mapped slots Smresulting from slotmapping, we have to decide whether this docu-ment is relevant or not.
We define the relevancescore of a document as:relevance(d) =?e?d:se?Sm?t?TeP (t|se)?e?d?t?TeP (t|se)(4)where e is an entity in the document d; seis theslot value assigned to e; and t is an event trigger inthe list of triggers Te.The equation (4) defines the score of an entity asthe sum of the conditional probabilities of triggersgiven a slot.
The relevance score of the documentis proportional to the score of the entities assignedto mapped slots.
If this relevance score is higherthan a threshold ?, then the document is consid-ered as relevant.
The value of ?
= 0.02 was tuned193System P R FHT+A 32.42 54.59 40.62HT+A + doc.
classification 35.57 53.89 42.79HT+A + oracle classification 44.58 54.59 49.08Table 2: Improvement from document classifica-tion as post-processing.on the development set by maximizing the F-scoreof document classification.Table 2 shows the improvement when applyingdocument classification.
The precision increasesas false positives from irrelevant documents are fil-tered out.
The loss of recall comes from relevantdocuments that are mistakenly filtered out.
How-ever, this loss is not significant and the overall F-score finally increases by 5%.
We also compareour results to an ?oracle?
classifier that would re-move all irrelevant documents while preserving allrelevant ones.
The performance of this oracle clas-sification shows that there are some room for fur-ther improvement from document classification.Irrelevant document filtering is a technique ap-plied by most supervised and unsupervised ap-proaches.
Supervised methods prefer relevancedetection at sentence or phrase-level (Patwardhanand Riloff, 2009; Patwardhan and Riloff, 2007).As for several unsupervised methods, Chambers(2013) includes document classification in histopic model.
Chambers and Jurafsky (2011) andCheung et al (2013) use the learned clusters toclassify documents by estimating the relevance ofa document with respect to a template from post-hoc statistics about event triggers.5.4 Comparison to State-of-the-ArtFor comparing in more depth our results to thestate-of-the-art in the literature.
we reimple-mented the method proposed in Chambers (2013)and integrated our attribute distributions into hismodel (as shown in Figure 6).The main differences between this model andours are the following:1.
The full template model of Chambers (2013)adds a distribution ?
linking events to docu-ments.
This makes the model more complexand maybe less intuitive since there is no rea-son to connect documents and slots (a docu-ment may contain references to several tem-plates and slot mapping does not depend ondocument level).
A benefit of this documentSystem P R FCheung et al (2013) 32 37 34Chambers and Jurafsky (2011) 48 25 33Chambers (2013) (paper values) 41 41 41HT+A + doc.
classification 36 54 43Table 3: Comparison to state-of-the-art unsuper-vised systems.distribution is that it leads to a free classifi-cation of irrelevant documents, thus avoid-ing a pre- or post-processing for classifica-tion.
However, this issue of document rel-evance is very specific to the MUC corpusand the evaluation method; In a more generaluse case, there would be no ?irrelevant?
doc-uments, only documents on various topics.2.
Each entity is linked to an event variable e.This event generates a predicate for eachentity mention (recall that mentions of anentity are all occurrences of this entity inthe documents, for example in a corefer-ence chain).
Our work instead focus onthe fact that a probabilistic model couldhave multiple observations at the same po-sition.
Multiple triggers and multiple at-tributes are treated equally.
The sourcesof multiple attributes and multiple triggersare not only from document-level corefer-ences but also from dependency relations (oreven from domain-level entity coreferences ifavailable).
Hence, our model arguably gener-alizes better in terms of both modeling andinput data.3.
Chambers (2013) applies a heuristic con-straint during the sampling process, impos-ing that subject and object of the same predi-cate (e.g.
kill:nsubj and kill:dobj) are not dis-tributed into the same slot.
Our model doesnot require this heuristic.Some details concerning data preprocessing andmodel parameters are not fully specified by Cham-bers (2013); for this reason, our implementationof the model (applied on the same data) leadsto slightly different results than those published.That is why we present the two results here (pa-per values in Table 3, reimplementation values inTable 4).Table 3 shows that our model outperforms theothers on recall by a large margin.
It achieves the194ht??
unti(1,KK) ,(n#plKe(a)ht??
unti(1,K)#Kpl ,(nesaK?
(b)Figure 6: Variation of Chambers (2013) model: 6a) Original model; 6b) Original model + attributedistributions.Chambers (2013) P R FOriginal reimpl.
38.65 42.68 40.56Original reimpl.
+ Attribute 39.25 43.68 41.31Table 4: Performance on reimplementation ofChambers (2013).best overall F-score.
In addition, as stated by ourexperiments, precision could be further improvedby more sophisticated document classification.
In-terestingly, using attributes also proves to be use-ful in the model proposed by Chambers (2013) (asshown in Table 4).5.5 Error AnalysisWe performed an error analysis on the output ofHT+A + doc.
classification to detect the originof false positives (FPs).
38% of FPs are mentionsthat never occur in the reference.
Within this 38%,attacker and killer are among the most frequent er-rors.
These words could refer to a perpetrator of anattack.
These mentions, however, do not occur inthe reference, possibly because human annotatorsconsider them as too generic terms.
Apart fromsuch generic terms, other assignments are obviouserrors of the system, e.g.
window, door or wall asphysical target; action or massacre as perpetrator;explosion or shooting as instrument.
These kindsof errors are due to the fact that in our model, as inthe one of Chambers (2013), the number of slotsis fixed and is not equivalent to the real number ofreference slots.On the other hand, 62% of FPs are mentions ofentities that occur at least once in the reference.On top of the list are perpetrators such as guer-rilla, group and rebel.
The model is capable of as-signing guerrilla to attribution slot if it is accom-panied by a trigger like announce:nsubj.
How-ever, triggers that describe quasi-terrorism events(e.g.
menace, threatening, military conflict) arealso grouped into perpetrator slots.
Similarly,mentions of frequent words such as bomb (instru-ment), building, house, office (targets) tend to besystematically grouped into these slots, regardlessof their relations.
Increasing the number of slots(to sharpen their content) does not help overall.This is due to the fact that the MUC corpus isvery small and is biased towards terrorism events.Adding a higher level of template type as in Cham-bers (2013) partially solves the problem but makesrecall decrease (as shown in Table 3).6 Conclusions and PerspectivesWe presented a generative model for representingthe roles played by the entities in an event tem-plate.
We focused on using immediate contexts ofentities and proposed a simpler and more effectivemodel than those proposed in previous work.
Weevaluated this model on the MUC-4 corpus.Even if our results outperform other unsuper-vised approaches, we are still far from results ob-tained by supervised systems.
Improvements canbe obtained by several ways.
First, the character-istics of the MUC-4 corpus are a limiting factor.The corpus is small and roles are similar from atemplate to another, which does not reflect reality.195A bigger corpus, even partially annotated but pre-senting a better variety of templates, could lead tovery different approaches.As we showed, our model comes with a unifiedrepresentation of all types of relations.
This opensthe way to the use of multiple types of relations(syntactic, semantic, thematic, etc.)
to refine theclusters.Last but not least, the evaluation protocol, thatbecame a kind of de facto standard, is very muchimperfect.
Most notably, the way of finally map-ping with reference slots can have a great influenceon the results.AcknowledgmentThis work was partially financed by the Foun-dation for Scientific Cooperation ?Campus Paris-Saclay?
(FSC) under the project Digiteo ASTRENo.
2013-0774D.ReferencesNiranjan Balasubramanian, Stephen Soderland,Mausam, and Oren Etzioni.
2013.
GeneratingCoherent Event Schemas at Scale.
In 2013 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP 2013), pages 1721?1731,Seattle, Washington, USA, October.Cosmin Adrian Bejan.
2008.
Unsupervised Discoveryof Event Scenarios from Texts.
In Twenty-First In-ternational Florida Artificial Intelligence ResearchSociety Conference (FLAIRS 2008), pages 124?129,Coconut Grove, Florida.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022, March.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised Learning of Narrative Event Chains.
InACL-08: HLT, pages 789?797, Columbus, Ohio,June.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised Learning of Narrative Schemas and theirParticipants.
In Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP (ACL-IJCNLP?09), pages 602?610, Suntec,Singapore, August.Nathanael Chambers and Dan Jurafsky.
2011.Template-Based Information Extraction without theTemplates.
In 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL 2011), pages 976?986,Portland, Oregon, USA, June.Nathanael Chambers.
2013.
Event Schema Inductionwith a Probabilistic Entity-Driven Model.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1797?1807, Seattle, Washington, USA, October.Kit Jackie Chi Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic Frame Induction.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 837?846.R.
Collier.
1998.
Automatic Template Creation forInformation Extraction.
Ph.D. thesis, University ofSheffield.Gerald DeJong.
1982.
An overview of the FRUMPsystem.
In W. Lehnert and M. Ringle, editors,Strategies for natural language processing, pages149?176.
Lawrence Erlbaum Associates.Kathrin Eichler, Holmer Hemsen, and G?unter Neu-mann.
2008.
Unsupervised Relation ExtractionFrom Web Documents.
In 6thConference on Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco.Olivier Ferret and Brigitte Grau.
1997.
An Aggre-gation Procedure for Building Episodic Memory.In 15thInternational Joint Conference on ArtificialIntelligence (IJCAI-97), pages 280?285, Nagoya,Japan.Elena Filatova, Vasileios Hatzivassiloglou, and Kath-leen McKeown.
2006.
Automatic Creation ofDomain Templates.
In 21stInternational Confer-ence on Computational Linguistics and 44thAnnualMeeting of the Association for Computational Lin-guistics (COLING-ACL 2006), pages 207?214, Syd-ney, Australia.Elena Filatova.
2008.
Unsupervised Relation Learningfor Event-Focused Question-Answering and DomainModelling.
Ph.D. thesis, Columbia University.Marjorie Freedman, Lance Ramshaw, ElizabethBoschee, Ryan Gabbard, Gary Kratkiewicz, Nico-las Ward, and Ralph Weischedel.
2011.
Ex-treme Extraction ?
Machine Reading in a Week.
In2011 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2011), pages 1437?1446, Edinburgh, Scotland, UK., July.Lea Frermann, Ivan Titov, and Manfred Pinkal.
2014.A Hierarchical Bayesian Model for UnsupervisedInduction of Script Knowledge.
In 14th Conferenceof the European Chapter of the Association for Com-putational Linguistics (EACL 2014), pages 49?57,Gothenburg, Sweden, April.Tom Griffiths.
2002.
Gibbs sampling in the genera-tive model of Latent Dirichlet Allocation.
Technicalreport, Stanford University.196Ralph Grishman and Yifan He.
2014.
An Informa-tion Extraction Customizer.
In Petr Sojka, Ale Hork,Ivan Kopeek, and Karel Pala, editors, 17th Inter-national Conference on Text, Speech and Dialogue(TSD 2014), volume 8655 of Lecture Notes in Com-puter Science, pages 3?10.
Springer InternationalPublishing.Ralph Grishman and Beth Sundheim.
1996.
Mes-sage Understanding Conference-6: A Brief History.In 16thInternational Conference on Computationallinguistics (COLING?96), pages 466?471, Copen-hagen, Denmark.Sanda Harabagiu.
2004.
Incremental Topic Repre-sentation.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COL-ING?04), Geneva, Switzerland, August.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discovering Relations among NamedEntities from Large Corpora.
In 42ndMeetingof the Association for Computational Linguistics(ACL?04), pages 415?422, Barcelona, Spain.Ludovic Jean-Louis, Romaric Besanon, and OlivierFerret.
2011.
Text Segmentation and Graph-basedMethod for Template Filling in Information Extrac-tion.
In 5thInternational Joint Conference on Nat-ural Language Processing (IJCNLP 2011), pages723?731, Chiang Mai, Thailand.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP Natural Lan-guage Processing Toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60, Baltimore, USA, jun.Bonan Min, Shuming Shi, Ralph Grishman, and Chin-Yew Lin.
2012.
Ensemble Semantics for Large-scale Unsupervised Relation Extraction.
In 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2012, pages1027?1037, Jeju Island, Korea.Ashutosh Modi and Ivan Titov.
2014.
Inducing neuralmodels of script knowledge.
In Eighteenth Confer-ence on Computational Natural Language Learning(CoNLL 2014), pages 49?57, Ann Arbor, Michigan.Siddharth Patwardhan and Ellen Riloff.
2007.
Ef-fective Information Extraction with Semantic Affin-ity Patterns and Relevant Regions.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL2007), pages 717?727, Prague, Czech Republic,June.Siddharth Patwardhan and Ellen Riloff.
2009.
A Uni-fied Model of Phrasal and Sentential Evidence forInformation Extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2009), pages 151?160.Karl Pichotta and Raymond Mooney.
2014.
Statisticalscript learning with multi-argument events.
In 14thConference of the European Chapter of the Associ-ation for Computational Linguistics (EACL 2014),pages 220?229, Gothenburg, Sweden.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2008.Modeling Context in Scenario Template Creation.In Third International Joint Conference on NaturalLanguage Processing (IJCNLP 2008), pages 157?164, Hyderabad, India.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning Script Knowledge with WebExperiments.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2010),pages 979?988, Uppsala, Sweden, July.Benjamin Rosenfeld and Ronen Feldman.
2007.
Clus-tering for unsupervised relation identification.
InSixteenth ACM conference on Conference on in-formation and knowledge management (CIKM?07),pages 411?418, Lisbon, Portugal.Roger C. Schank.
1980.
Language and memory.
Cog-nitive Science, 4:243?284.Satoshi Sekine.
2006.
On-demand informationextraction.
In 21stInternational Conference onComputational Linguistics and 44thAnnual Meet-ing of the Association for Computational Linguis-tics (COLING-ACL 2006), pages 731?738, Sydney,Australia.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive Information Extraction using Unrestricted Rela-tion Discovery.
In HLT-NAACL 2006, pages 304?311, New York City, USA.Beth M. Sundheim.
1991.
Third Message Understand-ing Evaluation and Conference (MUC-3): Phase 1Status Report.
In Proceedings of the Workshop onSpeech and Natural Language, HLT ?91, pages 301?305.197
