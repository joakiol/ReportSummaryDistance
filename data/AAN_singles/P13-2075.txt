Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424?428,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMinimum Bayes Risk based Answer Re-ranking for Question AnsweringNan DuanNatural Language ComputingMicrosoft Research Asiananduan@microsoft.comAbstractThis paper presents two minimum Bayesrisk (MBR) based Answer Re-ranking(MBRAR) approaches for the questionanswering (QA) task.
The first approachre-ranks single QA system?s outputs byusing a traditional MBR model, by mea-suring correlations between answer can-didates; while the second approach re-ranks the combined outputs of multipleQA systems with heterogenous answer ex-traction components by using a mixturemodel-based MBR model.
Evaluation-s are performed on factoid questions se-lected from two different domains: Jeop-ardy!
and Web, and significant improve-ments are achieved on all data sets.1 IntroductionMinimum Bayes Risk (MBR) techniques havebeen successfully applied to a wide range of nat-ural language processing tasks, such as statisti-cal machine translation (Kumar and Byrne, 2004),automatic speech recognition (Goel and Byrne,2000), parsing (Titov and Henderson, 2006), etc.This work makes further exploration along thisline of research, by applying MBR technique toquestion answering (QA).The function of a typical factoid question an-swering system is to automatically give answers toquestions in most case asking about entities, whichusually consists of three key components: ques-tion understanding, passage retrieval, and answerextraction.
In this paper, we propose two MBR-based Answer Re-ranking (MBRAR) approaches,aiming to re-rank answer candidates from eithersingle and multiple QA systems.
The first onere-ranks answer outputs from single QA systembased on a traditional MBR model by measuringthe correlations between each answer candidatesand all the other candidates; while the second onere-ranks the combined answer outputs from multi-ple QA systems based on a mixture model-basedMBR model.
The key contribution of this work isthat, our MBRAR approaches assume little aboutQA systems and can be easily applied to QA sys-tems with arbitrary sub-components.The remainder of this paper is organized as fol-lows: Section 2 gives a brief review of the QA taskand describe two types of QA systems with differ-ent pros and cons.
Section 3 presents twoMBRARapproaches that can re-rank the answer candidatesfrom single and multiple QA systems respectively.The relationship between our approach and pre-vious work is discussed in Section 4.
Section 5evaluates our methods on large scale questions s-elected from two domains (Jeopardy!
and Web)and shows promising results.
Section 6 concludesthis paper.2 Question Answering2.1 OverviewFormally, given an input question Q, a typical fac-toid QA system generates answers on the basis ofthe following three procedures:(1) Question Understanding, which determinesthe answer type and identifies necessory informa-tion contained in Q, such as question focus andlexical answer type (LAT).
Such information willbe encoded and used by the following procedures.
(2) Passage Retrieval, which formulates queriesbased on Q, and retrieves passages from offlinecorpus or online search engines (e.g.
Google andBing).
(3) Answer Extraction, which first extracts an-swer candidates from retrieved passages, and thenranks them based on specific ranking models.4242.2 Two Types of QA SystemsWe present two different QA sysytems, which aredistinguished from three aspects: answer typing,answer generation, and answer ranking.The 1st QA system is denoted as Type-Dependent QA engine (TD-QA).
In answer typingphase, TD-QA assigns the most possible answertype T?
to a given question Q based on:T?
= argmaxTP (T |Q)P (T |Q) is a probabilistic answer-typing mod-el that is similar to Pinchak and Lin (2006)?swork.
In answer generation phase, TD-QA usesa CRF-based Named Entity Recognizer to detectall named entities contained in retrieved passageswith the type T?
, and treat them as the answer can-didate space H(Q):H(Q) =?kAkIn answer ranking phase, the decision rule de-scribed below is used to rank answer candidate s-pace H(Q):A?
= argmaxA?H(Q)P (A|T?
, Q)= argmaxA?H(Q)?i?i ?
hi(A, T?
, Q)where {hi(?)}
is a set of ranking features thatmeasure the correctness of answer candidates, and{?i} are their corresponding feature weights.The 2ed QA system is denoted as Type-Independent QA engine (TI-QA).
In answer typ-ing phase, TI-QA assigns top N , instead of thebest, answer types TN (Q) for each question Q.The probability of each type candidate is main-tained as well.
In answer generation phase, TI-QA extracts all answer candidates from retrievedpassages based on answer types in TN (Q), by thesame NER used in TD-QA.
In answer rankingphase, TI-QA considers the probabilities of differ-ent answer types as well:A?
= argmaxA?H(Q)P (A|Q)= argmaxA?H(Q)?T?TN (Q)P (A|T,Q) ?
P (T |Q)On one hand, TD-QA can achieve relative highranking precision, as using a unique answer typegreatly reduces the size of the candidate list forranking.
However, as the answer-typing model isfar from perfect, if prediction errors happen, TD-QA can no longer give correct answers at all.On the other hand, TI-QA can provide higheranswer coverage, as it can extract answer candi-dates with multiple answer types.
However, moreanswer candidates with different types bring moredifficulties to the answer ranking model to rank thecorrect answer to the top 1 position.
So the rank-ing precision of TI-QA is not as good as TD-QA.3 MBR-based Answering Re-ranking3.1 MBRAR for Single QA SystemMBR decoding (Bickel and Doksum, 1977) aimsto select the hypothesis that minimizes the expect-ed loss in classification.
In MBRAR, we replacethe loss function with the gain function that mea-sure the correlation between answer candidates.Thus, the objective of the MBRAR approach forsingle QA system is to find the answer candidatethat is most supported by other candidates underQA system?s distribution, which can be formallywritten as:A?
= argmaxA?H(Q)?Ak?H(Q)G(A,Ak) ?
P (Ak|H(Q))P (Ak|H(Q)) denotes the hypothesis distribu-tion estimated on the search space H(Q) based onthe following log-linear formulation:P (Ak|H(Q)) =exp(?
?
P (Ak|Q))?A?
?H exp(?
?
P (A?
|Q))P (Ak|Q) is the posterior probability of the answercandidate Ak based on QA system?s ranking mod-el, ?
is a scaling factor which controls the distri-bution P (?)
sharp (when ?
> 1) or smooth (when?
< 1).G(A,Ak) is the gain function that denotes thedegree of how Ak supports A.
This function canbe further expanded as a weighted combination ofa set of correlation features as: ?j ?j ?hj(A,Ak).The following correlation features are used inG(?):?
answer-level n-gram correlation feature:hanswer(A,Ak) =???A#?
(Ak)where ?
denotes an n-gram in A, #?
(Ak)denotes the number of times that ?
occurs inAk.425?
passage-level n-gram correlation feature:hpassage(A,Ak) =???PA#?
(PAk)where PA denotes passages from which Aare extracted.
This feature measures the de-gree of Ak supports A from the context per-spective.?
answer-type agreement feature:htype(A,Ak) = ?
(TA, TAi)?
(TA, TAk) denotes an indicator function thatequals to 1 when the answer types of A andAk are the same, and 0 otherwise.?
answer-length feature that is used to penalizelong answer candidates.?
averaged passage-length feature that is usedto penalize passages with a long averagedlength.3.2 MBRAR for Multiple QA SystemsAiming to apply MBRAR to the outputs from NQA systems, we modify MBR components as fol-lows.First, the hypothesis space HC(Q) is built bymerging answer candidates of multiple QA sys-tems:HC(Q) =?iHi(Q)Second, the hypothesis distribution is definedas a probability distribution over the combinedsearch space of N component QA systems andcomputed as a weighted sum of component modeldistributions:P (A|HC(Q)) =N?i=1?i ?
P (A|Hi(Q))where ?1, ..., ?N are coefficients with followingconstraints holds1: 0 ?
?i ?
1 and?Ni=1 ?i = 1,P (A|Hi(Q)) is the posterior probability ofA esti-mated on the ith QA system?s search spaceHi(Q).Third, the features used in the gain function G(?
)can be grouped into two categories, including:?
system-independent features, which includesall features described in Section 3.1 for singlesystem based MBRAR method;1For simplicity, the coefficients are equally set: ?i =1/N .?
system-dependent features, which measurethe correctness of answer candidates basedon information provided by multiple QA sys-tems:?
system indicator feature hsys(A, QAi),which equals to 1 when A is generatedby the ith system QAi, and 0 otherwise;?
system ranking feature hrank(A, QAi),which equals to the reciprocal of therank position of A predicted by QAi.
IfQAi fails to generate A, then it equalsto 0;?
ensemble feature hcons(A), which e-quals to 1 when A can be generated byall individual QA system, and 0 other-wise.Thus, the MBRAR for multiple QA systems canbe finally formulated as follows:A?
= argmaxA?HC(Q)?Ai?HC(Q)G(A,Ai) ?
P (Ai|HC(Q))where the training process of the weights in thegain function is carried out with Ranking SVM2based on the method described in Verberne et al(2009).4 Related WorkMBR decoding have been successfully applied tomany NLP tasks, e.g.
machine translation, pars-ing, speech recognition and etc.
As far as weknow, this is the first work that applies MBR prin-ciple to QA.Yaman et al (2009) proposed a classifica-tion based method for QA task that jointly usesmultiple 5-W QA systems by selecting one opti-mal QA system for each question.
Comparing totheir work, our MBRAR approaches assume fewabout the question types, and all QA systems con-tribute in the re-ranking model.
Tellez-Valero etal.
(2008) presented an answer validation methodthat helps individual QA systems to automatical-ly detect its own errors based on information frommultiple QA systems.
Chu-Carroll et al (2003) p-resented a multi-level answer resolution algorithmto merge results from the answering agents at thequestion, passage, and answer levels.
Grappy et al2We use SVMRank (Joachims, 2006) that can be found-ed at www.cs.cornell.edu/people/tj/svm light/svm rank.html/426(2012) proposed to use different score combina-tions to merge answers from different QA system-s.
Although all methods mentioned above leverageinformation provided by multiple QA systems, ourwork is the first time to explore the usage of MBRprinciple for the QA task.5 Experiments5.1 Data and MetricQuestions from two different domains are usedas our evaluation data sets: the first data set in-cludes 10,051 factoid question-answer pairs se-lected from the Jeopardy!
quiz show3; while thesecond data set includes 360 celebrity-asking webquestions4 selected from a commercial search en-gine, the answers for each question is labeled byhuman annotators.The evaluation metric Succeed@n is defined asthe number of questions whose correct answersare successfully ranked to the top n answer can-didates.5.2 MBRAR for Single QA SystemWe first evaluate the effectiveness of our MBRARfor single QA system.
Given the N-best answeroutputs from each single QA system, together withtheir ranking scores assigned by the correspondingranking components, we further perform MBRARto re-rank them and show resulting numbers on t-wo evaluation data sets in Table 1 and 2 respec-tively.Both Table 1 and Table 2 show that, by lever-aging our MBRAR method on individual QA sys-tems, the rankings of correct answers are consis-tently improved on both Jeopardy!
and web ques-tions.Joepardy!
Succeed@1 Succeed@2 Succeed@3TD-QA 2,289 2,693 2,885MBRAR 2,372 2,784 2,982TI-QA 2,527 3,397 3,821MBRAR 2,628 3,500 3,931Table 1: Impacts of MBRAR for single QA systemon Jeopardy!
questions.We also notice TI-QA performs significantlybetter than TD-QA on Jeopardy!
questions, butworse on web questions.
This is due to fac-t that when the answer type is fixed (PERSON for3http://www.jeopardy.com/4The answers of such questions are person names.Web Succeed@1 Succeed@2 Succeed@3TD-QA 97 128 146MBRAR 99 130 148TI-QA 95 122 136MBRAR 97 126 143Table 2: Impacts of MBRAR for single QA systemon web questions.celebrity-asking questions), TI-QA will generatecandidates with wrong answer types, which willdefinitely deteriorate the ranking accuracy.5.3 MBRAR for Multiple QA SystemsWe then evaluate the effectiveness of our MBRARfor multiple QA systems.
The mixture model-based MBRAR method described in Section 3.2is used to rank the combined answer outputs fromTD-QA and TI-QA, with ranking results shown inTable 3 and 4.From Table 3 and Table 4 we can see that, com-paring to the ranking performances of single QAsystems TD-QA and TI-QA, MBRAR using twoQA systems?
outputs shows significant improve-ments on both Jeopardy!
and web questions.
Fur-thermore, comparing to MBRAR on single QAsystem, MBRAR onmultiple QA systems can pro-vide extra gains on both questions sets as well.Jeopardy!
Succeed@1 Succeed@2 Succeed@3TD-QA 2,289 2,693 2,885TI-QA 2,527 3,397 3,821MBRAR 2,891 3,668 4,033Table 3: Impacts of MBRAR for multiple QA sys-tems on Jeopardy!
questions.Web Succeed@1 Succeed@2 Succeed@3TD-QA 97 128 146TI-QA 95 122 136MBRAR 108 137 152Table 4: Impacts of MBRAR for multiple QA sys-tems on web questions.6 Conclusions and Future WorkIn this paper, we present two MBR-based answerre-ranking approaches for QA.
Comparing to pre-vious methods, MBRAR provides a systematicway to re-rank answers from either single or multi-ple QA systems, without considering their hetero-geneous implementations of internal components.427Experiments on questions from two different do-mains show that, our proposed method can sig-nificantly improve the ranking performances.
Infuture, we will add more QA systems into our M-BRAR framework, and design more features forthe MBR gain function.ReferencesP.
J. Bickel and K. A. Doksum.
1977.
MathematicalStatistics: Basic Ideas and Selected Topics.
Holden-Day Inc.Jennifer Chu-Carroll, Krzysztof Czuba, John Prager,and Abraham Ittycheriah.
2003.
In Question An-swering, Two Heads Are Better Than One.
In pro-ceeding of HLT-NAACL.Vaibhava Goel and William Byrne.
2000.
Minimumbayes-risk automatic speech recognition, ComputerSpeech and Language.Arnaud Grappy, Brigitte Grau, and Sophie Ros-set.
2012.
Methods Combination and ML-basedRe-ranking of Multiple Hypothesis for Question-Answering Systems, In proceeding of EACL.Thorsten Joachims.
2006.
Training Linear SVMs inLinear Time, In proceeding of KDD.Shankar Kumar and William Byrne.
2004.
Mini-mum Bayes-Risk Decoding for Statisti-cal MachineTranslation.
In proceeding of HLT-NAACL.Christopher Pinchak and Dekang Lin.
2006.
A Prob-abilistic Answer Type Model.
In proceeding of EA-CL.Ivan Titov and James Henderson.
2006.
Bayes RiskMinimization in Natural Language Parsing.
Techni-cal report.Alberto Tellez-Valero, Manuel Montes-y-Gomez, LuisVillasenor-Pineda, and Anselmo Penas.
2008.
Im-proving Question Answering by Combining MultipleSystems via Answer Validation.
In proceeding of CI-CLing.Suzan Verberne, Clst Ru Nijmegen, Hans Van Hal-teren, Clst Ru Nijmegen, Daphne Theijssen, Ru Ni-jmegen, Stephan Raaijmakers, Lou Boves, and ClstRu Nijmegen.
2009.
Learning to rank qa data.
e-valuating machine learning techniques for rankinganswers to why-questions.
In proceeding of SIGIRworkshop.Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, RalphGrishman, Mary Harper, Kathleen R. McKe-own, Adam Meyers, Kartavya Sharma.
2009.Classification-Based Strategies for Combining Mul-tiple 5-W Question Answering Systems.
In proceed-ing of INTERSPEECH.428
