Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1188?1198,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsJoint Syntactic and Semantic Parsing withCombinatory Categorial GrammarJayant KrishnamurthyCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213jayantk@cs.cmu.eduTom M. MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213tom.mitchell@cmu.eduAbstractWe present an approach to training a jointsyntactic and semantic parser that com-bines syntactic training information fromCCGbank with semantic training informa-tion from a knowledge base via distant su-pervision.
The trained parser produces afull syntactic parse of any sentence, whilesimultaneously producing logical formsfor portions of the sentence that have a se-mantic representation within the parser?spredicate vocabulary.
We demonstrate ourapproach by training a parser whose se-mantic representation contains 130 pred-icates from the NELL ontology.
A seman-tic evaluation demonstrates that this parserproduces logical forms better than bothcomparable prior work and a pipelinedsyntax-then-semantics approach.
A syn-tactic evaluation on CCGbank demon-strates that the parser?s dependency F-score is within 2.5% of state-of-the-art.1 IntroductionIntegrating syntactic parsing with semantics haslong been a goal of natural language processingand is expected to improve both syntactic and se-mantic processing.
For example, semantics couldhelp predict the differing prepositional phrase at-tachments in ?I caught the butterfly with the net?and ?I caught the butterfly with the spots.?
A jointanalysis could also avoid propagating syntacticparsing errors into semantic processing, therebyimproving performance.We suggest that a large populated knowledgebase should play a key role in syntactic and se-mantic parsing: in training the parser, in resolv-ing syntactic ambiguities when the trained parseris applied to new text, and in its output semanticrepresentation.
Using semantic information fromthe knowledge base at training and test time willideally improve the parser?s ability to solve diffi-cult syntactic parsing problems, as in the exam-ples above.
A semantic representation tied to aknowledge base allows for powerful inference op-erations ?
such as identifying the possible entityreferents of a noun phrase ?
that cannot be per-formed with shallower representations (e.g., framesemantics (Baker et al, 1998) or a direct conver-sion of syntax to logic (Bos, 2005)).This paper presents an approach to training ajoint syntactic and semantic parser using a largebackground knowledge base.
Our parser producesa full syntactic parse of every sentence, and fur-thermore produces logical forms for portions ofthe sentence that have a semantic representationwithin the parser?s predicate vocabulary.
For ex-ample, given a phrase like ?my favorite town inCalifornia,?
our parser will assign a logical formlike ?x.CITY(x) ?
LOCATEDIN(x,CALIFORNIA)to the ?town in California?
portion.
Additionally,the parser uses predicate and entity type informa-tion during parsing to select a syntactic parse.Our parser is trained by combining a syntacticparsing task with a distantly-supervised relationextraction task.
Syntactic information is providedby CCGbank, a conversion of the Penn Treebankinto the CCG formalism (Hockenmaier and Steed-man, 2002a).
Semantics are learned by trainingthe parser to extract knowledge base relation in-stances from a corpus of unlabeled sentences, ina distantly-supervised training regime.
This ap-proach uses the knowledge base to avoid expen-sive manual labeling of individual sentence se-mantics.
By optimizing the parser to perform bothtasks simultaneously, we train a parser that pro-duces accurate syntactic and semantic analyses.We demonstrate our approach by training a jointsyntactic and semantic parser, which we call ASP.ASP produces a full syntactic analysis of everysentence while simultaneously producing logicalforms containing any of 61 category and 69 re-1188lation predicates from NELL.
Experiments withASP demonstrate that jointly analyzing syntaxand semantics improves semantic parsing perfor-mance over comparable prior work and a pipelinedsyntax-then-semantics approach.
ASP?s syntacticparsing performance is within 2.5% of state-of-the-art; however, we also find that incorporatingsemantic information reduces syntactic parsing ac-curacy by ?
0.5%.2 Prior WorkThis paper combines two lines of prior work:broad coverage syntactic parsing with CCG andsemantic parsing.Broad coverage syntactic parsing with CCG hasproduced both resources and successful parsers.These parsers are trained and evaluated usingCCGbank (Hockenmaier and Steedman, 2002a),an automatic conversion of the Penn Treebankinto the CCG formalism.
Several broad cover-age parsers have been trained using this resource(Hockenmaier and Steedman, 2002b; Hocken-maier, 2003b).
The parsing model in this paper isloosely based on C&C (Clark and Curran, 2007b;Clark and Curran, 2007a), a discriminative log-linear model for statistical parsing.
Some workhas also attempted to automatically derive logi-cal meaning representations directly from syntac-tic CCG parses (Bos, 2005; Lewis and Steedman,2013).
However, these approaches to semantics donot ground the text to beliefs in a knowledge base.Meanwhile, work on semantic parsing has fo-cused on producing semantic parsers for answer-ing simple natural language questions (Zelle andMooney, 1996; Ge and Mooney, 2005; Wong andMooney, 2006; Wong and Mooney, 2007; Lu etal., 2008; Kate and Mooney, 2006; Zettlemoyerand Collins, 2005; Kwiatkowski et al, 2011).
Thisline of work has typically used a corpus of sen-tences with annotated logical forms to train theparser.
Recent work has relaxed the requisite su-pervision conditions (Clarke et al, 2010; Liang etal., 2011), but has still focused on simple ques-tions.
Finally, some work has looked at applyingsemantic parsing to answer queries against largeknowledge bases, such as YAGO (Yahya et al,2012) and Freebase (Cai and Yates, 2013b; Caiand Yates, 2013a; Kwiatkowski et al, 2013; Be-rant et al, 2013).
Although this work considersa larger number (thousands) of predicates than wedo, none of these systems are capable of parsingopen-domain text.
Our approach is most closelyrelated to the distantly-supervised approach of Kr-ishnamurthy and Mitchell (2012).The parser presented in this paper can be viewedas a combination of both a broad coverage syn-tactic parser and a semantic parser trained usingdistant supervision.
Combining these two linesof work has synergistic effects ?
for example, ourparser is capable of semantically analyzing con-junctions and relative clauses based on the syn-tactic annotation of these categories in CCGbank.This synergy gives our parser a richer semanticrepresentation than previous work, while simulta-neously enabling broad coverage.3 Parser DesignThis section describes the Combinatory CategorialGrammar (CCG) parsing model used by ASP.
Theinput to the parser is a part-of-speech tagged sen-tence, and the output is a syntactic CCG parse tree,along with zero or more logical forms representingthe semantics of subspans of the sentence.
Theselogical forms are constructed using category andrelation predicates from a broad coverage knowl-edge base.
The parser also outputs a collection ofdependency structures summarizing the sentence?spredicate-argument structure.
Figure 1 illustratesASP?s input/output specification.3.1 Knowledge BaseThe parser uses category and relation predicatesfrom a broad coverage knowledge base both toconstruct logical forms and to parametrize theparsing model.
The knowledge base is assumedto have two kinds of ontological structure: a gen-eralization/subsumption hierarchy and argumenttype constraints.
This paper uses NELL?s ontology(Carlson et al, 2010), which, for example, speci-fies that the category ORGANIZATION is a general-ization of SPORTSTEAM, and that both argumentsto the LOCATEDIN relation must have type LOCA-TION.
These type constraints are enforced duringparsing.
Throughout this paper, predicate namesare shown in SMALLCAPS.3.2 SyntaxASP uses a lexicalized and semantically-typed Combinatory Categorial Grammar(CCG) (Steedman, 1996).
Most gram-matical information in CCG is encoded ina lexicon ?, containing entries such as:1189area / NNN?x.LOCATION(x)that / WDT(N1\N1)/(S[dcl]\NP1)2?f.?g.
?z.g(z) ?
f(?y.y = z)includes / VBZ(S[dcl]\NP1)/NP2?f.?g.
?x, y.g(x) ?
f(y)?
LOCATEDIN(y, x)beautiful / JJN1/N1?f.fLondon / NNPN?x.M(x, ?london?, CITY)N : ?x.M(x, ?london?, CITY)(S[dcl]\NP1) :?g.
?x, y.g(x) ?
M(y, ?london?, CITY) ?
LOCATEDIN(y, x)N1\N1: ?g.?z.
?x, y.g(z) ?
x = z ?
M(y, ?london?, CITY) ?
LOCATEDIN(y, x)N : ?z.
?x, y.LOCATION(z) ?
x = z ?
M(y, ?london?, CITY) ?
LOCATEDIN(y, x)Head Argumentword POS semantic type index syntactic category arg.
num.
word POS semantic type indexthat WDT ?
1 (N1\N1)/(S\NP1)21 area NN LOCATION 0that WDT ?
1 (N1\N1)/(S\NP1)22 includes VBZ LOCATEDIN?12includes VBZ LOCATEDIN?12 (S[dcl]\NP1)/NP21 area NN LOCATION 0includes VBZ LOCATEDIN?12 (S[dcl]\NP1)/NP22 ENTITY:CITY NNP CITY 4beautiful JJ ?
3 N1/N11 ENTITY:CITY NNP CITY 4Figure 1: Example input and output for ASP.
Given a POS-tagged sentence, the parser produces a CCGsyntactic tree and logical form (top), and a collection of dependency structures (bottom).person := N : PERSON : ?x.PERSON(x)London := N : CITY : ?x.M(x, ?london?, CITY)great := N1/N1: ?
: ?f.
?x.f(x)bought :=(S[dcl]\NP1)/NP2: ACQUIRED :?f.?g.
?x, y.f(y) ?
g(x) ?
ACQUIRED(x, y)Each lexicon entry maps a word to a syntacticcategory, semantic type, and logical form.
CCGhas two kinds of syntactic categories: atomic andfunctional.
Atomic categories include N for nounand S for sentence.
Functional categories arefunctions constructed recursively from atomic cat-egories; these categories are denoted using slashesto separate the category?s argument type from itsreturn type.
The argument type appears on theright side of the slash, and the return type on theleft.
The direction of slash determines where theargument must appear ?
/ means an argument onthe right, and \ means an argument on the left.Syntactic categories in ASP are annotated withtwo additional kinds of information.
First, atomiccategories may have associated syntactic featuresgiven in square brackets.
These features are usedin CCGbank to distinguish variants of atomic syn-tactic categories, e.g., S[dcl] denotes a declara-tive sentence.
Second, each category is anno-tated with head and dependency information us-ing subscripts.
These subscripts are used to pop-ulate predicate-argument dependencies (describedbelow), and to pass head information using unifi-cation.
For example, the head of the parse in Fig-ure 1 is ?area,?
due to the coindexing of the argu-ment and return categories in the categoryN1\N1.In addition to the syntactic category, each lexi-con entry has a semantic type and a logical form.The semantic type is a category or relation pred-icate that concisely represents the word?s seman-tics.
The semantic type is used to enforce typeconstraints during parsing and to include seman-tics in the parser?s parametrization.
The logi-cal form gives the full semantics of the word inlambda calculus.
The parser also allows lexiconentries with the semantic type ??
?, representingwords whose semantics cannot be expressed usingpredicates from the ontology.Parsing in CCG combines adjacent categoriesusing a small number of combinators, such asfunction application:X/Y : f Y : g =?
X : f(g)Y : g X\Y : f =?
X : f(g)The first rule states that the category X/Y canbe applied to the category Y , returning categoryX , and that the logical form f is applied to g toproduce the logical form for the returned category.Head words and semantic types are also propa-gated to the returned category based on the anno-tated head-passing markup.3.3 Dependency StructuresParsing a sentence produces a collection of depen-dency structures which summarize the predicate-argument structure of the sentence.
Dependencystructures are 10-tuples, of the form:< head word, head POS, head semantic type, head wordindex, head word syntactic category, argument number, ar-gument word, argument POS, argument semantic type, argu-ment word index >A dependency structure captures a relationshipbetween a head word and its argument.
Duringparsing, whenever a subscripted argument of asyntactic category is filled, a dependency structure1190is created between the head of the applied func-tion and its argument.
For example, in Figure 1,the first application fills argument 1 of ?beautiful?with ?London,?
creating a dependency structure.3.4 Logical FormsASP performs a best-effort semantic analysis ofevery parsed sentence, producing logical forms forsubspans of the sentence when possible.
Logicalforms are designed so that the meaning of a sen-tence is a universally- and existentially-quantifiedconjunction of predicates with partially shared ar-guments.
This representation allows the parser toproduce semantic analyses for a reasonable subsetof language, including prepositions, verbs, nouns,relative clauses, and conjunctions.Figure 1 shows a representative sample of a log-ical form produced by ASP.
Generally, the parserproduces a lambda calculus statement with sev-eral existentially-quantified variables ranging overentities in the knowledge base.
The only excep-tion to this rule is conjunctions, which are rep-resented using a scoped universal quantifier overthe conjoined predicates.
Entity mentions appearin logical forms via a special mention predicate,M, instead of as database constants.
For exam-ple, ?London?
appears as M(x, ?london?, CITY),instead of as a constant like LONDON.
The mean-ing of this mention predicate is that x is an en-tity which can be called ?london?
and belongs tothe CITY category.
This representation propagatesuncertainty about entity references into the logicalform where background knowledge can be usedfor disambiguation.
For example, ?London, Eng-land?
is assigned a logical form that disambiguates?London?
to a ?London?
located in ?England.
?1Lexicon entries without a semantic type are au-tomatically assigned logical forms based on theirhead passing markup.
For example, in Figure 1,the adjective ?beautiful?
is assigned ?f.f .
Thisapproach allows a logical form to be derived formost sentences, but (somewhat counterintuitively)can lose interesting logical forms from constituentsubspans.
For example, the preposition ?in?
hassyntactic category (N1\N1)/N2, which results inthe logical form ?f.?g.g.
This logical form dis-cards any information present in the argument f .We avoid this problem by extracting a logical formfrom every subtree of the CCG parse.1Specifically, ?x.
?y.CITYLOCATEDINCOUNTRY(x, y) ?M(x, ?london?, CITY) ?
M(y, ?england?, COUNTRY)3.5 ParametrizationThe parser ?
is trained as a discriminative linearmodel of the following form:?
(`, d, t|s; ?)
= ?T?
(d, t, s)Given a parameter vector ?
and a sentence s, theparser produces a score for a syntactic parse treet, a collection of dependency structures d and alogical form `.
The score depends on features ofthe parse produced by the feature function ?.?
contains four classes of features: lexiconfeatures, combinator features, dependency fea-tures and dependency distance features (Table 1).These features are based on those of C&C (Clarkand Curran, 2007b), modified to include seman-tic types.
The features are designed to share syn-tactic information about a word across its distinctsemantic realizations in order to transfer syntacticinformation from CCGbank to semantic parsing.The parser also includes a hard type-checkingconstraint to ensure that logical forms are well-typed.
This constraint states that dependencystructures with a head semantic type only acceptarguments that (1) have a semantic type, and (2)are within the domain/range of the head type.4 Parameter EstimationThis section describes the training procedure forASP.
Training is performed by minimizing a jointobjective function combining a syntactic parsingtask and a distantly-supervised relation extractiontask.
The input training data includes:1.
A collection L of sentences siwith annotatedsyntactic trees ti(e.g., CCGbank).2.
A corpus of sentences S (e.g., Wikipedia).3.
A knowledge base K (e.g., NELL), contain-ing relation instances r(e1, e2) ?
K.4.
A CCG lexicon ?
(see Section 5.2).Given these resources, the algorithm describedin this section produces parameters ?
for a se-mantic parser.
Our parameter estimation proce-dure constructs a joint objective functionO(?)
thatdecomposes into syntactic and semantic compo-nents: O(?)
= Osyn(?)
+ Osem(?).
The syntac-tic component Osynis a standard syntactic pars-ing objective constructed using the syntactic re-source L. The semantic component Osemis adistantly-supervised relation extraction task basedon the semantic constraint from Krishnamurthyand Mitchell (2012).
These components are de-scribed in more detail in the following sections.1191Lexicon features: word, POS := X : t : `Word/syntactic category word, XPOS/syntactic category POS, XWord semantics word, X, tCombinator features: X Y ?
Z or X ?
ZBinary combinator indicator X Y ?
ZUnary combinator indicator X ?
ZRoot syntactic category ZDependency Features: < hw, hp, ht, hi, s, n, aw, ap, at, ai>Predicate-Argument Indicator < hw,?, ht,?, s, n, aw,?, at,?
>Word-Word Indicator < hw,?,?,?, s, n, aw,?,?,?
>Predicate-POS Indicator < hw,?, ht,?, s, n,?, ap,?,?
>Word-POS Indicator < hw,?,?,?, s, n,?, ap,?,?
>POS-Argument Indicator < ?, hp,?,?, s, n, aw,?, at,?
>POS-Word Indicator < ?, hp,?,?, s, n, aw,?,?,?
>POS-POS Indicator < ?, hp,?,?, s, n,?, ap,?,?
>Dependency Distance Features:Token distance hw, ht,?, s, n, d d = Number of tokens between hiand ai: 0, 1, 2 or more.Token distance word backoff hw,?, s, n, d d = Number of tokens between hiand ai: 0, 1, 2 or more.Token distance POS backoff ?,?, hp, s, n, d d = Number of tokens between hiand ai: 0, 1, 2 or more.
(The above distance features are repeated using the number of intervening verbs and punctuation marks.
)Table 1: Listing of parser feature templates used in the feature function ?.
Each feature template repre-sents a class of indicator features that fire during parsing when lexicon entries are used, combinators areapplied, or dependency structures are instantiated.4.1 Syntactic ObjectiveThe syntactic objective is the structured percep-tron objective instantiated for a syntactic parsingtask.
This objective encourages the parser to accu-rately reproduce the syntactic parses in the anno-tated corpus L = {(si, ti)}ni=1:Osyn(?)
=n?i=1|max?`,?d,?t?
(?`,?d,?t|si; ?)?max`?,d??
(`?, d?, ti|si; ?
)|+The first term in the above expression representsthe best CCG parse of the sentence siaccording tothe current model.
The second term is the bestparse of siwhose syntactic tree equals the truesyntactic tree ti.
In the above equation | ?
|+de-notes the positive part of the expression.
Minimiz-ing this objective therefore finds parameters ?
thatreproduce the annotated syntactic trees.4.2 Semantic ObjectiveThe semantic objective corresponds to a distantly-supervised relation extraction task that constrainsthe logical forms produced by the semantic parser.Distant supervision is provided by the followingconstraint: every relation instance r(e1, e2) ?
Kmust be expressed by at least one sentence inS(e1,e2), the set of sentences that mention both e1and e2(Hoffmann et al, 2011).
If this constraintis empirically true and sufficiently constrains theparser?s logical forms, then optimizing the seman-tic objective produces an accurate semantic parser.A training example in the semantic objectiveconsists of the set of sentences mentioning a pairof entities, S(e1,e2)= {s1, s2, ...}, paired with abinary vector representing the set of relations thatthe two entities participate in, y(e1,e2).
The distantsupervision constraint ?
forces the logical formspredicted for the sentences to entail the relationsin y(e1,e2).
?
is a deterministic OR constraint thatchecks whether each logical form entails the re-lation instance r(e1, e2), deterministically settingyr= 1 if any logical form entails the instance andyr= 0 otherwise.Let (`,d, t) represent a collection of seman-tic parses for the sentences S = S(e1,e2).
Let?
(`,d, t|S; ?)
=?|S|i=1?
(`i, di, ti|si; ?)
representthe total weight assigned by the parser to a collec-tion of parses for the sentences S. For the pair ofentities (e1, e2), the semantic objective is:Osem(?)
= |max?`,?d,?t?
(?`,?d,?t|S; ?)?
max`?,d?,t?(?
(y(e1,e2), `?,d?, t?)
+ ?
(`?,d?, t?|S; ?
))|+4.3 OptimizationTraining minimizes the joint objective using thestructured perceptron algorithm, which can beviewed as the stochastic subgradient method(Ratliff et al, 2006) applied to the objectiveO(?).
We initialize the parameters to zero, i.e.,?0= 0.
On each iteration, we sample either asyntactic example (si, ti) or a semantic example(S(e1,e2), y(e1,e2)).
If a syntactic example is sam-pled, we apply the following parameter update:?`,?d,?t ?
arg max`,d,t?
(`, d, t|si; ?t)`?, d??
arg max`,d?
(`, d, ti|si; ?t)?t+1?
?t+ ?
(d?, ti, si)?
?
(?d,?t, si)This update moves the parameters toward the fea-tures of the best parse with the correct syntacticderivation, ?
(d?, ti, si).
If a semantic example is1192Labeled Dependencies Unlabeled DependenciesP R F P R F CoverageASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63C&C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63(Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83Table 2: Syntactic parsing results for Section 23 of CCGbank.
Parser performance is measured usingprecision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies.sampled, we instead apply the following update:?`,?d,?t?
arg max`,d,t?
(`,d, t|S(e1,e2); ?t)`?,d?, t??
arg max`,d,t?
(`,d, t|S(e1,e2); ?t)+ ?
(y(e1,e2), `,d, t)?t+1?
?t+ ?
(d?, t?,S(e1,e2))?
?
(?d,?t,S(e1,e2))This update moves the parameters toward the fea-tures of the best set of parses that satisfy the distantsupervision constraint.
Training outputs the aver-age of each iteration?s parameters,??
=1n?nt=1?t.In practice, we train the parser by performing asingle pass over the examples in the data set.All of the maximizations above can be per-formed exactly using a CKY-style chart parsingalgorithm, except for the last one.
This maxi-mization is intractable due to the coupling betweenlogical forms in ` caused by enforcing the dis-tant supervision constraint.
We approximate thismaximization in two steps.
First, we perform abeam search to produce a list of candidate parsesfor each sentence s ?
S(e1,e2).
We then extractrelation instances from each parse and apply thegreedy inference algorithm from Hoffmann et al,(2011) to identify the best set of parses that satisfythe distant supervision constraint.
The procedureskips any examples with sentences that cannot beparsed (due to beam search failures) or where thedistant supervision constraint cannot be satisfied.5 ExperimentsThe experiments below evaluate ASP?s syntacticand semantic parsing ability.
The parser is trainedon CCGbank and a corpus of Wikipedia sentences,using NELL?s predicate vocabulary.
The syntacticanalyses of the trained parser are evaluated againstCCGbank, and its logical forms are evaluated onan information extraction task and against an an-notated test set of Wikipedia sentences.5.1 Data SetsThe data sets for the evaluation consist of CCG-bank, a corpus of dependency-parsed Wikipediasentences, and a logical knowledge base derivedfrom NELL and Freebase.
Sections 02-21 ofCCGbank were used for training, Section 00 forvalidation, and Section 23 for the final results.
Theknowledge base?s predicate vocabulary is takenfrom NELL, and its instances are taken from Free-base using a manually-constructed mapping be-tween Freebase and NELL.
Using Freebase rela-tion instances produces cleaner training data thanNELL?s automatically-extracted instances.Using the relation instances and Wikipedia sen-tences, we constructed a data set for distantly-supervised relation extraction.
We identified men-tions of entities in each sentence using simplestring matching, then aggregated these sentencesby entity pair.
20% of the entity pairs were setaside for validation.
In the remaining trainingdata, we downsampled entity pairs that did notparticipate in at least one relation.
We furthereliminated sentences containing more than 30 to-kens.
The resulting training corpus contains 25kentity pairs (half of which participate in a relation),41k sentences, and 71 distinct relation predicates.5.2 Grammar ConstructionThe grammar for ASP contains the annotated lex-icon entries and grammar rules in Sections 02-21of CCGbank, and additional semantic entries pro-duced using a set of dependency parse heuristics.The lexicon ?
contains all words that occur atleast 20 times in CCGbank.
Rare words are re-placed by their part of speech.
The head pass-ing and dependency markup was generated usingthe rules of the C&C parser (Clark and Curran,2007b).
These lexicon entries are also annotatedwith logical forms capturing their head passing re-lationship.
For example, the adjective categoryN1/N1is annotated with the logical form ?f.f .These entries are all assigned semantic type ?.We augment this lexicon with additional entries1193Sentence Extracted Logical FormSt.
John, a Mexican-American born in San Francisco, Califor-nia, her family comes from Zacatecas, Mexico.?x.
?y, z.M(x, ?st.
john?)
?
M(y, ?san francisco?)
?PERSONBORNINLOCATION(x, y) ?CITYLOCATEDINSTATE(y, z) ?
M(z, ?california?
)The capital and largest city of Laos is Vientiane and other majorcities include Luang Prabang, Savannakhet and Pakse.
?x, y.M(x, ?vientiane?)
?
CITY(x) ?CITYCAPITALOFCOUNTRY(x, y) ?
M(y, ?laos?
)Gellar next played a lead role in James Toback ?s criticallyunsuccessful independent ?Harvard Man?
(2001), where sheplayed the daughter of a mobster.?x.
?y.M(y, ?james toback?)
?DIRECTORDIRECTEDMOVIE(y, x) ?M(x, ?harvard man?
)Figure 2: Logical forms produced by ASP for sentences in the information extraction corpus.
Eachlogical form is extracted from the underlined sentence portion.ASPPIPELINEK&M-20120 300 600 90000.20.40.60.81.0Figure 3: Logical form precision as a function ofthe expected number of correct extracted logicalforms.
ASP extracts more correct logical formsbecause it jointly analyzes syntax and semantics.mapping words to logical forms with NELL pred-icates.
These entries are instantiated using a setof dependency parse patterns, listed in an onlineappendix.2These patterns are applied to the train-ing corpus, heuristically identifying verbs, prepo-sitions, and possessives that express relations, andnouns that express categories.
The patterns alsoinclude special cases for forms of ?to be.?
Thisprocess generates ?4000 entries (not counting en-tity names), representing 69 relations and 61 cate-gories from NELL.
Section 3.2 shows several lex-icon entries generated by this process.The parser?s combinators include function ap-plication, composition, and crossed composition,as well as several binary and unary type-changingrules that occur in CCGbank.
All combinatorswere restricted to only apply to categories thatcombine in Sections 02-21.
Finally, the grammarincludes a number of heuristically-instantiated bi-nary rules of the form , N ?
N\N that instanti-ate a relation between adjacent nouns.
These rulescapture appositives and some other constructions.5.3 SupertaggingParsing in practice can be slow because theparser?s lexicalized grammar permits a large num-ber of parses for a sentence.
We improve parserperformance by performing supertagging (Banga-2http://rtw.ml.cmu.edu/acl2014_asp/lore and Joshi, 1999; Clark and Curran, 2004).We trained a logistic regression classifier to pre-dict the syntactic category of each token in a sen-tence from features of the surrounding tokens andPOS tags.
Subsequent parsing is restricted to onlyconsider categories whose probability is within afactor of ?
of the highest-scoring category.
Theparser uses a backoff strategy, first attempting toparse with the supertags from ?
= 0.01, backingoff to ?
= 0.001 if the initial parsing attempt fails.5.4 Syntactic EvaluationThe syntactic evaluation measures ASP?s abilityto reproduce the predicate-argument dependenciesin CCGbank.
As in previous work, our evalu-ation uses labeled and unlabeled dependencies.Labeled dependencies are dependency structureswith both words and semantic types removed,leaving two word indexes, a syntactic category,and an argument number.
Unlabeled dependen-cies further eliminate the syntactic category andargument number, leaving a pair of word indexes.Performance is measured using precision, recall,and F-measure against the annotated dependencystructures in CCGbank.
Precision is the fractionof predicted dependencies which are in CCGbank,recall is the fraction of CCGbank dependenciesproduced by the parser, and F-measure is the har-monic mean of precision and recall.For comparison, we also trained a syntactic ver-sion of our parser, ASP-SYN, using only the CCG-bank lexicon and grammar.
Comparing againstthis parser lets us measure the effect of the rela-tion extraction task on syntactic parsing.Table 2 shows the results of our evaluation.For comparison, we include results for two ex-isting syntactic CCG parsers: C&C, the currentstate-of-the-art CCG parser (Clark and Curran,2007b), and the next best system (Hockenmaier,2003a).
Both ASP and ASP-SYN perform rea-sonably well, within 2.5% of the performance ofC&C at the same coverage level.
However, ASP-1194Logical Form Extraction ExtractionAccuracy Precision RecallASP 0.28 0.90 0.32K&M-2012 0.14 1.00 0.06PIPELINE 0.2 0.63 0.17Table 3: Logical form accuracy and extraction pre-cision/recall on the annotated test set.
The highextraction recall for ASP shows that it producesmore complete logical forms than either baseline.SYN outperforms ASP by around 0.5%, suggestingthat ASP?s additional semantic knowledge slightlyhurts syntactic parsing performance.
This perfor-mance loss appears to be largely due to poor en-tity mention detection, as we found that not us-ing entity mention lexicon entries at test time im-proves ASP?s labeled and unlabeled F-scores by0.3% on Section 00.
The knowledge base containsmany infrequently-mentioned entities with com-mon names; these entities contribute incorrect se-mantic type information that confuses the parser.5.5 Semantic EvaluationWe performed two semantic evaluations to bet-ter understand ASP?s ability to construct logicalforms.
The first evaluation emphasizes precisionover recall, and the second evaluation accuratelymeasures recall using a manually labeled test set.5.5.1 BaselinesFor comparison, we also trained two base-line models.
The first baseline, PIPELINE, isa pipelined syntax-then-semantics approach de-signed to mimic Boxer (Bos, 2005).
This base-line first syntactically parses each sentence usingASP-SYN, then produces a semantic analysis byassigning a logical form to each word.
We trainthis baseline using the semantic objective (Section4.2) while holding fixed the syntactic parse of eachsentence.
Note that, unlike Boxer, this baselinelearns which logical form to assign each word, andits logical forms contain NELL predicates.The second baseline, K&M-2012, is the ap-proach of Krishnamurthy and Mitchell (2012),representing the state-of-the-art in distantly-supervised semantic parsing.
This approach trainsa semantic parser by combining distant seman-tic supervision with syntactic supervision fromdependency parses.
The best performing vari-ant of this system also uses dependency parsesat test time to constrain the interpretation oftest sentences ?
hence, this system also uses apipelined syntax-then-semantics approach.
To im-prove comparability, we reimplemented this ap-proach using our parsing model, which has richerfeatures than were used in their paper.5.5.2 Information Extraction EvaluationThe information extraction evaluation uses eachsystem to extract logical forms from a large cor-pus of sentences, then measures the fraction ofextracted logical forms that are correct.
The testset consists of 8.5k sentences sampled from theheld-out Wikipedia sentences.
Each system wasrun on this data set, extracting all logical formsfrom each sentence that entailed at least one cat-egory or relation instance.
We ranked these ex-tractions using the parser?s inside chart score, thenmanually annotated a sample of 250 logical formsfrom each system for correctness.
Logical formswere marked correct if all category and relationinstances entailed by the logical form were ex-pressed by the sentence.
Note that a correct logicalform need not entail all of the relations expressedby the sentence, reflecting an emphasis on preci-sion over recall.
Figure 2 shows some examplelogical forms produced by ASP in the evaluation.The annotated sample of logical forms allowsus to estimate precision for each system as a func-tion of the number of correct extractions (Figure3).
The number of correct extractions is directlyproportional to recall, and was estimated from thetotal number of extractions and precision at eachrank in the sample.
All three systems initiallyhave high precision, implying that their extractedlogical forms express facts found in the sentence.However, ASP produces 3 times more correct log-ical forms than either baseline because it jointlyanalyzes syntax and semantics.
The baselines suf-fer from reduced recall because they depend on re-ceiving an accurate syntactic parse as input; syn-tactic parsing errors cause these systems to fail.Examining the incorrect logical forms producedby ASP reveals that incorrect mention detection isby far the most common source of mistakes.
Ap-proximately 50% of errors are caused by markingcommon nouns as entity mentions (e.g., marking?coin?
as a COMPANY).
These errors occur be-cause the knowledge base contains many infre-quently mentioned entities with relatively com-mon names.
Another 30% of errors are caused byassigning an incorrect type to a common propernoun (e.g, marking ?Bolivia?
as a CITY).
Thisanalysis suggests that performing entity linkingbefore parsing could significantly reduce errors.1195Sentence: De Niro and Joe Pesci in ?Goodfellas?
offered a virtuoso display of the director?s bravura cinematictechnique and reestablished, enhanced, and consolidated his reputation.Annotation:LF: ?x.
?p ?
{?d.M(d, ?de niro?
), ?j.M(j, ?joe pesci?
)}?y.p(x) ?
STARREDINMOVIE(x, y) ?
M(y, ?goodfellas?
)Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)Prediction:LF: ?x.
?p ?
{?d.M(d, ?de niro?
), ?j.M(j, ?joe pesci?
)}?y.p(x) ?
STARREDINMOVIE(x, y) ?
M(y, ?goodfellas?
)Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)Logical form accuracy: 1 / 1 Extraction Precision: 2 / 2 Extraction Recall: 2 / 2Sentence: In addition to the University of Illinois, Champaign is also home to Parkland College.Annotation:LF: ?c, p.M(c, ?champaign?)
?
CITY(c) ?
M(p, ?parkland college?)
?
UNIVERSITYINCITY(p, c)Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign)Prediction:LF 1: ?x.
?yM(y, ?illinois?)
?
M(x, ?university?)
?
CITYLOCATEDINSTATE(x, y)LF 2: ?c, p.M(c, ?champaign?)
?
CITY(c) ?
M(p, ?parkland college?)
?
UNIVERSITYINCITY(p, c)Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign),CITYLOCATEDINSTATE(university, illinois)Logical form accuracy: 1 / 1 Extraction Precision: 2 / 3 Extraction Recall: 2 / 2Figure 4: Two test examples with ASP?s predictions and error calculations.
The annotated logical formsare for the italicized sentence spans, while the extracted logical forms are for the underlined spans.5.5.3 Annotated Sentence EvaluationA limitation of the previous evaluation is that itdoes not measure the completeness of predictedlogical forms, nor estimate what portion of sen-tences are left unanalyzed.
We conducted a secondevaluation to measure these quantities.The data for this evaluation consists of sen-tences annotated with logical forms for subspans.We manually annotated Wikipedia sentences fromthe held-out set with logical forms for the largestsubspans for which a logical form existed.
Toavoid trivial cases, we only annotated logicalforms containing at least one category or relationpredicate and at least one mention.
We also chosenot to annotate mentions of entities that are not inthe knowledge base, as no system would be ableto correctly identify them.
The corpus contains 97sentences with 100 annotated logical forms.We measured performance using two met-rics: logical form accuracy, and extraction preci-sion/recall.
Logical form accuracy examines thepredicted logical form for the smallest subspan ofthe sentence containing the annotated span, andmarks this prediction correct if it exactly matchesthe annotation.
A limitation of this metric is that itdoes not assign partial credit to logical forms thatare close to, but do not exactly match, the anno-tation.
The extraction metric assigns partial creditby computing the precision and recall of the cat-egory and relation instances entailed by the pre-dicted logical form, using those entailed by the an-notated logical form as the gold standard.
Figure4 shows the computation of both error metrics ontwo examples from the test corpus.Table 3 shows the results of the annotated sen-tence evaluation.
ASP outperforms both baselinesin logical form accuracy and extraction recall, sug-gesting that it produces more complete analysesthan either baseline.
The extraction precision of90% suggests that ASP rarely extracts incorrect in-formation.
Precision is higher in this evaluationbecause every sentence in the data set has at leastone correct extraction.6 DiscussionWe present an approach to training a joint syntac-tic and semantic parser.
Our parser ASP producesa full syntactic parse of any sentence, while simul-taneously producing logical forms for sentencespans that have a semantic representation withinits predicate vocabulary.
The parser is trainedby jointly optimizing performance on a syntac-tic parsing task and a distantly-supervised rela-tion extraction task.
Experimental results demon-strate that jointly analyzing syntax and semanticstriples the number of extracted logical forms overapproaches that first analyze syntax, then seman-tics.
However, we also find that incorporating se-mantics slightly reduces syntactic parsing perfor-mance.
Poor entity mention detection is a majorsource of error in both cases, suggesting that fu-ture work should consider integrating entity link-ing with joint syntactic and semantic parsing.AcknowledgmentsThis work was supported in part by DARPA underaward FA8750-13-2-0005.
We additionally thankJamie Callan and Chris R?e?s Hazy group for col-lecting and processing the Wikipedia corpus.1196ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 17th International Conference on Com-putational Linguistics - Volume 1.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2):237?265.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing.Johan Bos.
2005.
Towards wide-coverage seman-tic interpretation.
In In Proceedings of Sixth In-ternational Workshop on Computational SemanticsIWCS-6.Qingqing Cai and Alexander Yates.
2013a.
Large-scale Semantic Parsing via Schema Matching andLexicon Extension.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics (ACL).Qingqing Cai and Alexander Yates.
2013b.
SemanticParsing Freebase: Towards Open-domain SemanticParsing.
In Proceedings of the Second Joint Con-ference on Lexical and Computational Semantics(*SEM).Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth AAAI Conference on Artificial Intel-ligence.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of the 20th International Con-ference on Computational Linguistics.Stephen Clark and James R. Curran.
2007a.
Per-ceptron training for a wide-coverage lexicalized-grammar parser.
In Proceedings of the Workshop onDeep Linguistic Processing.Stephen Clark and James R. Curran.
2007b.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Ruifang Ge and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proceedings of the Ninth Conference onComputational Natural Language Learning.Julia Hockenmaier and Mark Steedman.
2002a.Acquiring compact lexicalized grammars from acleaner treebank.
In Proceedings of Third Interna-tional Conference on Language Resources and Eval-uation.Julia Hockenmaier and Mark Steedman.
2002b.
Gen-erative models for statistical parsing with combina-tory categorial grammar.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics.Julia Hockenmaier.
2003a.
Data and Models for Sta-tistical Parsing with Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.Julia Hockenmaier.
2003b.
Parsing with generativemodels of predicate-argument structure.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics - Volume 1.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke S. Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In The 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
In21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, Proceedingsof the Conference.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in CCG grammar induction for semantic pars-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of the Association for Compu-tational Linguistics, Portland, Oregon.
Associationfor Computational Linguistics.1197Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.Zinkevich.
2006.
(online) subgradient methodsfor structured prediction.
Artificial Intelligence andStatistics.Mark Steedman.
1996.
Surface Structure and Inter-pretation.
The MIT Press, Cambridge, MA, USA.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL.Yuk Wah Wong and Raymond J. Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural language questions forthe web of data.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the thirteenth na-tional conference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: struc-tured classification with probabilistic categorialgrammars.
In UAI ?05, Proceedings of the 21st Con-ference in Uncertainty in Artificial Intelligence.1198
