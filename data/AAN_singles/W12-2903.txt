NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 9?18,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsDiscourse-Based Modeling for AACMargaret Mitchell Richard SproatCenter for Spoken Language UnderstandingOregon Health & Science Universitym.mitchell@abdn.ac.uk, rws@xoba.comAbstractThis paper presents a method for an AAC sys-tem to predict a whole response given featuresof the previous utterance from the interlocu-tor.
It uses a large corpus of scripted dialogs,computes a variety of lexical, syntactic andwhole phrase features for the previous utter-ance, and predicts features that the responseshould have, using an entropy-based measure.We evaluate the system on a held-out portionof the corpus.
We find that for about 3.5% ofcases in the held-out corpus, we are able topredict a response, and among those, over halfare either exact or at least reasonable substi-tutes for the actual response.
We also presentsome results on keystroke savings.
Finallywe compare our approach to a state-of-the-artchatbot, and show (not surprisingly) that a sys-tem like ours, tuned for a particular style ofconversation, outperforms one that is not.Predicting possible responses automaticallyby mining a corpus of dialogues is anovel contribution to the literature on wholeutterance-based methods in AAC.
Also useful,we believe, is our estimate that about 3.5-4.0%of utterances in dialogs are in principle pre-dictable given previous context.1 IntroductionOne of the overarching goals of Augmentative andAlternative Communication technology is to helpimpaired users communicate more quickly and morenaturally.
Over the past thirty years, solutionsthat attempt to reduce the amount of effort neededto input a sentence have include semantic com-paction (Baker, 1990), and lexicon- or language-model-based word prediction (Darragh et al, 1990;Higginbotham, 1992; Li and Hirst, 2005; Trost etal., 2005; Trnka et al, 2006; Trnka et al, 2007;Wandmacher and Antoine, 2007), among others.
Inrecent years, there has been an increased interestin whole utterance-based and discourse-based ap-proaches (see Section 2).
Such approaches havebeen argued to be beneficial in that they can speed upthe conversation, thus making it appear more felici-tous (McCoy et al, 2007).
Most commercial tabletssold as AAC devices contain an inventory of cannedphrases, comprising such items as common greet-ings, polite phrases, salutations and so forth.
Userscan also enter their own phrases, or indeed entire se-quences of phrases (e.g., for a prepared talk).The work presented here attempts to take wholephrase prediction one step further by automaticallypredicting appropriate responses to utterances bymining conversational text.
In an actual deploy-ment, one would present a limited number of pre-dicted phrases in a prominent location on the user?sdevice, along with additional input options.
The usercould then select from these phrases, or revert toother input methods.
In actual use, one would alsowant such a system to incorporate speech recogni-tion (ASR), but for the present we restrict ourselvesto typed text ?
which is perfectly appropriate forsome modes of interaction such as on-line social me-dia domains.
Using a corpus of 72 million wordsfrom American soap operas, we isolate features use-ful in predicting an appropriate set of responses forthe previous utterance of an interlocutor.
The mainresults of this work are a method that can automati-9cally produce appropriate responses to utterances insome cases, and an estimate of what percentage ofdialog may be amenable to such techniques.2 Previous WorkAlm et al (1992) discuss how AAC technology canincrease social interaction by having the utterance,rather than the letter or word, be the basic unitof communication.
Findings from conversationalanalysis suggest a number of utterances common toconversation, including short conversational openersand closers (hello, goodbye), backchannel responses(yeah?
), and quickfire phrases (That?s too bad.).
In-deed ?small talk?
is central to smooth-flowing con-versation (King et al, 1995).
Many modern AACsystems therefore provide canned small-talk phrases(Alm et al, 1993; Todman et al, 2008).More complex conversational utterances are chal-lenging to predict, and recent systems have useda variety of approaches to generate longer phrasesfrom minimal user input.
One approach relies ontelegraphic input, where full sentences are con-structed from a set of uninflected words, as in theCompansion system (McCoy et al, 1998).
Thissystem employs a semantic parser to capture themeaning of the input words and generates usingthe Functional Unification Formalism (FUF) system(Elhadad, 1991).
One of the limitations of this ap-proach is that information associated with each wordis primarily hand-coded on the basis of intuition; asa result, the system cannot handle the problem of un-restricted vocabulary.
Similar issues arise in seman-tic authoring systems (Netzer and Elhadad, 2006),where at each step of the sentence creation process,the system offers possible symbols for a small set ofconcepts, and the user can select which is intended.Recent work has also tried to handle the complex-ity of conversation by providing full sentences withslots that can be filled in by the user.
Dempster etal.
(2010) define an ontology where pieces of hand-coded knowledge are stored and realized within sev-eral syntactic templates.
Users can generate utter-ances by entering utterance types and topics, andthese are filled into the templates.
The Frametalkersystem (Higginbotham et al, 1999) uses contextualframes ?
basic sentences for different contexts ?with a set vocabulary for each.
The intuition be-hind this system is that there are typical linguisticstructures for different situations and the kinds ofwords that the user will need to fill in will be se-mantically related to the context.
Wisenburn andHigginbotham (2008) extend this technology usingASR on the speech of the interlocutor.
The systemextracts noun phrases from the speech and presentsthose noun phrases on the AAC device, with framesentences that the user can then select.
Thus, if theinterlocutor says Paris, the AAC user will be able toselect from phrases like Tell me more about Paris orI want to talk about Paris.Other approaches provide a way for users toquickly find canned utterances.
WordKeys (Langerand Hickey, 1998) allows users to access storedphrases by entering key words.
This system ap-proaches generation as a text retrieval task, using alexicon derived from WordNet to expand user inputto find possible utterances.
Dye et al (1998) intro-duce a system that utilizes scripts for specific situa-tions.
Although pre-stored scripts work reasonablywell for specific contexts, the authors find (not unex-pectedly) that a larger number of scripts are neededfor the system to be generally effective.3 The Soap Opera CorpusIn this work we attempt a different approach, devel-oping a system that can learn appropriate responsesto utterances given a corpus of conversations.Part of the difficulty in automatically generatingconversational utterances is that very large corporaof naturally occurring dialogs are non-existent.
Theclosest such corpus is Switchboard (Godfrey andHolliman, 1997), which contains 2,400 two-sidedconversations with about 1.4 million words.
The in-terlocutors in Switchboard are not acquainted witheach other and they are instructed to discuss a par-ticular topic.
While the dialogs are ?natural?
to apoint, because they involve people who have neverpreviously met, they are not particularly reflective ofthe kinds of conversations between intimates that weare interested in helping impaired users with.We thus look instead to a corpus of scripted di-alogs taken from American soap operas.
The web-site tvmegasite.net contains soap opera scriptsthat have been transcribed by aficionados of the var-ious series.
The scripts include utterances marked10with information on which character is speaking,and a few dramatic cues.
We downloaded 72 mil-lion words of text, with 5.5 million utterances.
Soapopera series downloaded were: All my Children, Asthe World Turns, The Bold and the Beautiful, Daysof our Lives, General Hospital, Guiding Light, OneLife to Live and The Young and the Restless.
The textwas cleaned to remove HTML markup and other ex-traneous material, and the result was a set of 550,000dialogs, with alternating utterances by (usually) twospeakers.
These dialogs were split 0.8/0.1/0.1 intotraining, development testing and testing portions,respectively.
All results reported in this paper are onthe development test set.While soap operas may not be very representativeof most people?s lives, the corpus nonetheless hasthree advantages.
First of all, the corpus is large.Second, the language tends to be fairly colloquial.Third, many of the dialogs take place between char-acters who are supposed to know each other well,often intimately; thus the topics might be more re-flective of casual conversation between friends andintimates than the dialogs one finds in Switchboard.4 Data Analysis, Feature Extraction andUtterance PredictionEach dialog was processed using the Stanford CoreNLP tools.
The Stanford tools perform part ofspeech tagging (Toutanova et al, 2003), constituentand dependency parsing (Klein and Manning, 2003),named entity recognition (Finkel et al, 2005), andcoreference resolution (Lee et al, 2011).
Fromthe output of the Stanford tools, the following fea-tures were extracted for each utterance: word bi-grams (pairs of adjacent words); dependency-headrelations, along with the type of dependency rela-tion (basically, governors ?
e.g., verbs ?
and theirdependents ?
e.g., nouns); named entities (per-sons, organizations, etc.
); and the whole utterance.Extracted named entities include noun phrases thatwere explicitly tagged as named entities, as well asany phrases that were marked as coreferential withnamed entities.
Thus if the pronoun she occurred inan utterance, and was marked as coreferential with aprevious or following named entity Amelia, then thefeature Amelia as a named entity was added for thisutterance.
We also include the whole utterance as afeature, which turns out to be the most useful predic-tor for an appropriate response to an input utterance.The dialogs were divided into turns, with eachturn consisting of one or more utterances.
For ourexperiments, we are interested in predicting the firstutterance of a turn (which in many cases may be thewhole turn) given features of all the utterances ofthe previous turns ?
the exception being that forthe whole sentence feature, only the last sentence ofthe previous turn is used.
The method of using fea-tures of a turn to predict features of the next turn isrelated to the work reported in Purandare and Lit-man (2008), though their goal was to analyze dialogcoherence rather than to predict the next utterance.We are particularly interested in feature valuesthat are highly skewed in their predictions, mean-ing that if the turn has a given value, then the firstsentence of the next utterance is much more likelyto have some values than others.
A useful measureof this is the difference between the entropy of thepredicted feature values fi of a feature g:H(g) = ?n?i=0log(p(fi)) ?
p(fi) (1)and the maximum possible entropy of g given n pre-dicted features, namely:Hmax(g) = ?log(1n) (2)The larger the difference Hmax(g)?H(g), the moreskewed the distribution.For the purposes of this experiment and to keepthe computation reasonably tractable, we computedthe entropic values described above for like features:thus we used bigram features to predict bigram fea-tures, dependency features to predict dependencyfeatures, and so forth.
We also filtered the output ofthe process so that each feature of the prior contexthad a minimum of 10 occurrences, and the entropyof the feature was no greater than 0.9 of the max-imum entropy as defined above.
For each featurevalue, the 2 most strongly associated values for thepredicted utterance were stored.To take a simple example (Figure 1) the bigram ?mfine has a strong association with the bigrams you ?reand , I, these co-occurring 486 and 464 times in thetraining corpus, respectively.
For this feature, the11?m fine 8.196261 9.406976 you ?re 486?m fine 8.196261 9.406976 , i 464you?re kidding .
__SENT 4.348040 4.852030no.
.
__SENT 32you?re kidding .
__SENT 4.348040 4.852030i wish .
__SENT 7Figure 1: Examples of bigram and full-sentence features.entropy is 8.20 and the maximum entropy is 9.41.Or consider a full-sentence feature You?re kidding.This is strongly associated with the predicted sen-tence features no.. and I wish..Utterances in the training data were stored and as-sociated with predicted features.
In order to pro-duce a rank-ordered list of possible responses to atest utterance, the features of the test utterance areextracted.
For each of these features, the predictedfeatures and their entropies are retrieved.
Thosetraining data utterances that match on one or moreof these predicted features are retrieved in this step,and a score is assigned which is simply the sum ofthe predicted feature entropies.
However, since wewant to favor full-sentence matches, entropies forfull-sentence matches are multiplied by a positivenumber (currently set to 100).5 Experimental Results5.1 Whole sentence predictionThe first question we were interested in is how of-ten, based on the approach described here, one couldpredict a sentence that is close to what the speakeractually intended to say.
For this purpose, we sim-ply took as the gold standard the utterance that waswritten in the script for the speaker, and consideredthe prediction of the system described above, whenit was able to make one.
The prediction could bean exact match to what was actually said, somethingclose enough to be a reasonable substitute, some-thing appropriate given the context but not the oneintended, or something that is wholly inappropriate.In the ensuing discussion we will focus on wholesentence features, since these were the most usefulfor predicting reasonable whole sentences.
We re-turn to the use of other features in Section 5.2.Some examples can be found in Figure 2.
Ineach case, we give the final sentence of the previousturn, the actual utterance, and the two predicted ut-PREV really ?ACTUAL yeah .PRED 232.3099 yeah .
__SENT 4PRED 230.9528 mm-hmm .
__SENT 3PREV love you .ACTUAL i love you , too , baby doll .PRED 83.4519 i love you , too .
__SENT 3PRED 74.1185 love you .
__SENT 3PREV ok ?ACTUAL i?m sorry , laurie , about j.r. ,about everything .PRED 86.2623 yeah .
__SENT 2PRED 86.2623 ok .
__SENT 2Figure 2: Whole sentence prediction examples.terances, along with the predicted utterances?
scoresand the counts with which they co-occurred in thetraining data with the previous utterance in question.For the first example Really?, the actual responsewas Yeah, and this was also the highest ranked re-sponse of the system.
In the second example, the ac-tual response was I love you, too, baby doll, whereasa response of the system was I love you too.
Whilenot exact, this is arguably close enough, and couldbe selected by an impaired user who did not wish totype the whole message.
In the third example, thepredictions Yeah.
and Ok. do not substitute at all forthe actual response.Of the 276,802 utterance-response pairs in the de-velopment test data, the system was able to makepredictions for 9,794 cases, or 3.5%.
Evaluating9,794 responses is labor intensive, so two evalua-tions based on random samples were performed.In the first, the authors evaluated a random sam-ple of 455 utterance pairs, assigning the followingscores to each response: 4 exact match; 3 equiva-lent meaning; 2 good answer but not the right one;1 inappropriate.
The results are given in Table 1, forthe best score of the pair of responses generated.
Inother words, if the first response has a score of 2 andthe second a score of 3, then the pair of responseswill receive a score of 3: in that pair, there was onegenerated response that was close enough to use.From Table 1, we see that between 38% to 40.7%of the response pairs contained a response that wasexact, or close enough to have the same meaning.59.3% to 62% had at best a reasonable answer, butnot the one intended.
Finally, none contained only12Score Judge 1 Judge 2Exact match 110 24.2% 109 24.0%Equivalent meaning 63 13.8% 76 16.7%Good answer (but wrong) 282 62.0% 270 59.3%Inappropriate 0 0.0% 0 0.0%Table 1: Judgments of a sample of 455 utterance pairs bythe authors.inappropriate answers: this is not surprising, giventhat all of the predicted responses were based onwhat was found in the training data, which one mayassume involved largely felicitous interactions.We also used Amazon?s Mechanical Turk (AMT)to collect judgements from unbiased judges.
Basedon our previous evaluation, we expanded the equiv-alent meaning category into two more fine-grainedcategories, essentially the same and similar mean-ing, in order to capture phrases with slightly differ-ent connotations.
This results in the 4-point scalein Table 2.
Exact matches were found automaticallybefore giving response pairs to Turkers, and accountfor a large portion of the data ?
2,330 of the 9,794response pairs, or 23.8%.
For the remaining 76.2%,138 participants were asked to judge how close thepredicted response was to the actual response.Each AMT participant was presented with sixprompts (three entropy-based conversational turnsand three chatbot-based conversational turns, dis-cussed below).
Each prompt listed the utterance,actual response, and predicted response.
Two ad-ditional prompts with known answers were includedto automatically flag participants who were not fo-cusing on the task.
Evaluation results are given in4 Essentiallythe same:They?re pretty close, and meanbasically the same thing.3 Similarmeaning:They?re similar, but the pre-dicted response has a slightlydifferent connotation from theactual response.2 Good answer,but not theright one:They?re different, but the pre-dicted response is still a reason-able response to the comment.1 Inappropriate: Different, and the predicted re-sponse is a totally unreasonableresponse to the comment.Table 2: Four-point scale for AMT evaluation.
Exactmatches were found automatically.Essentially the same 89 16.4%Similar meaning 81 14.9%Good answer (but wrong) 165 30.4%Inappropriate 79 14.5%Table 3: Evaluation results from AMT on a randomsample of 414 predicted utterances (excluding exactmatches).Table 3.
Percentages are multiplied by the propor-tion of results they represent (.762).
Of the evalu-ated cases, we find that 31.3% of the predicted re-sponses were judged to be essentially the same orsimilar to the actual response.
30.4% were judgedto be a reasonable answer, and the remaining 14.5%were judged to be inappropriate.Evaluation by AMT judges was thus much morefavorable towards the prediction-based system thanthe authors?
evaluation.
Where the authors found13.8%-16.7% to be essentially the same or similar,unbiased judges found just under a third of the datato meet these criteria.
Coupled with the automati-cally detected exact matches, 55.1% of the predictedresponses were found to be a reasonable approxima-tion of (or exactly) the intended response.
A smallerportion of the data was thought to be a good answer(but wrong), or wholly inappropriate.5.2 Prediction with features plus a prefix of theintended utteranceIt is of course not necessary for the system to predictthe whole response without any input from the user.As with word prediction, the user might type a pre-fix of the intended utterance, and the system couldthen produce a small set of corresponding responses,among which would often be the one desired.In order to evaluate such a scenario, we consid-ered the shortest prefix of the actual intended re-sponse that would be consistent with a maximumof five sentences predicted from the features of theprevious turn.
Thus, we gathered the entire set ofsentences from the training data that matched one ormore of the predicted features, then began (virtually)typing the actual response.
There are two possibleoutcomes.
If the actual response is not in the set,then at some point the typed prefix will be consistentwith none of the sentences in the set.
In this worstcase, the user would simply have to type the wholesentence (possibly using whatever word-completion13technology is already available on the device).
Butif the intended response is in the set, then at somepoint the set consistent with the prefix will be win-nowed down to at most five members.
The length ofthe prefix at that point, subtracted from the length ofthe intended sentence, is the keystroke savings.Of the 276,802 utterances in the development testresponses, 11,665 (4.2%) had a keystroke savingsof greater than zero: thus, in 4.2% of cases, the in-tended utterance was to be found among the set ofsentences consistent with the predicted features.
Thetotal keystroke savings was 102,323 characters outof a total of 8,725,508, or about 1%.
While this isclearly small, note that it is over and above whateverkeystroke savings one would gain by other methods,such as language modeling.5.3 ALICEA final experiment involved using a chatbot to gen-erate responses.
Previous approaches have usedstored sentence templates that are generated basedon keyword input from the user; a similar approachis used in a chatbot, where the input utterances arethemselves triggers for the generated content.
Forthis experiment, we used the publicly available AL-ICE (Wallace, 2012), which won the Loebner Prize(a Turing test) in 2000, 2001, and 2004.
ALICEmakes use of a large library of pattern-action pairswritten in AIML (Artificial Intelligence MarkupLanguage): if an input sentence matches a partic-ular pattern, a response is generated by a rule that isassociated with that pattern.
ALICE follows conver-sational context by using a notion of TOPIC (whatthe conversation is currently about, based on key-words) and of THAT (the bot?s previous utterance).Both are used along with the input utterance whenselecting what next to say.
In essence, ALICE is amuch more sophisticated version of the 1960s Elizaprogram (Weizenbaum, 1966).In order to use the chatbot for this task, we use anAIML interpreter (Stratton, 2010) on the most recentset of ALICE knowledge.1 ALICE was given theutterances for each conversation in our developmenttesting set, which allows the system to store someof the dialogue context under its THAT and TOPIC1http://code.google.com/p/aiml-en-us-foundation-alice/, re-trieved February 2012.Essentially the same 45 10.7%Similar meaning 96 22.9%Good answer (but wrong) 135 32.1%Inappropriate 138 32.9%Table 4: Evaluation results from AMT on a random sam-ple of 414 chatbot utterances (excluding exact matches).variables.Example responses are given in Figure 3.
As withthe previous experiments, some responses are closeto the actual intended message (first example in Fig-ure 3).
In some other cases (second example), theresponse is reasonable, though not the one intended.But in many cases, the response is too ?cute?, as inthe examples on the righthand side.Evaluation with AMT is given in Table 4, us-ing the same scoring criteria as in Table 3.
Exactmatches are again automatically removed, and ac-count for 142 responses (1.5%).
For the remainingdata, participants were asked to judge how close thechatbot response was to the actual response.
Per-centages are multiplied by the proportion of resultsthey represent (.9855).The chatbot is judged to produce less acceptableutterances than the entropy-based approach, with10.7% essentially the same as the actual response,and 22.9% with similar meaning.
Coupled with theautomatically detected exact matches, 35% of thechatbot responses were found to be a reasonable ap-proximation of (or exactly) the intended response.Over half of the data (65%) was thought to be a goodanswer (but wrong), or wholly inappropriate.6 Discussion and Future WorkOf the two different methods for automatically gen-erating responses, the entropy-based approach inparticular produces responses ranging from similarto exact in a majority of the cases for which it canmake a prediction.
These are promising findingsfor expanding a system that populates possible re-sponses from an input utterance.An obvious limitation of the work presented hereis that it is based on scripted data.
More to the point,to build an effective dialog-based system for an im-paired user, the best possible data to use would bedata involving the user him/herself along with hisor her interlocutors.
Indeed, we are in the process14PREV okay .ACTUAL okay .ALICE Great.PREV please don?t leave .ACTUAL i?m right here .ALICE I?m not going anywhere.PREV you need to drink water .ACTUAL no , stay .ALICE I need to wait and see if theopportunity presents itself.PREV okay ?ACTUAL you were so mad at melately , dixie .ALICE Dude!Figure 3: Some responses generated by ALICE.of collecting a corpus of such interactions from asmall number of AAC users in the Portland, Ore-gon area.
But the resulting corpora will obviouslybe tiny in comparison with the data used in the ex-periments here, in no small measure because of theextreme slowness with which most AAC users areable to communicate.
What can be done about this?One thing would be to use the results of this workdirectly even if it does not model the particular user:even if it comes from soap opera dialogs, Are youmad at me?
No, I?m not mad at you, still makesfor a perfectly reasonable utterance/response pair.This, to some extent, counters potential objectionsthat soap opera dialogs are not reflective of naturalinteractions.
These kinds of pairs could be supple-mented by whatever data we are able to learn from aparticular user.Even better, though, would be to collect largeamounts of data from users before they become im-paired.
Many disorders, such as ALS, are often de-tected early, before they start to impair communi-cation.
In such cases, one could consider language-banking the user?s interactions, and building a modelof the ways in which the user interacts with otherspeakers, in order to get a good model of that par-ticular user.
While there are obviously privacy con-cerns, a person who knows that they will lose theability to speak over time will likely be very moti-vated to try to preserve samples of their speech andlanguage, assuming there exists technology that canuse those samples to provide more sophisticated as-sistance when it becomes needed.It may also be possible to use features from thetext to generate utterances, similar to the telegraphicapproaches to generation discussed in Section 2, butautomatically learning words that can be used togenerate appropriate responses to an utterance.
Asa first look at the feasibility of this approach, we usethe Midge generator (Mitchell et al, 2012), rebuild-ing its models from the soap dialogues.
Midge re-quires as input a set of nouns and then builds likelysyntactic structure around them, and so we use thedialogues to predict possible nouns in response toan input utterance.
For each <utterance, response>pair in the dialogues, we gather all utterance nounsnu and all response nouns nr.
We then compute nor-malized pointwise mutual information (nPMI) foreach nu, nr pair type in the corpus.
Given a novel in-put utterance, we tag it to extract the nouns and cre-ate the set of highest nPMI nouns from the model.This is then input to Midge, which uses the set togenerate present-tense declarative sentences.
Someexamples are given in Figure 4.
We hope to expandon this approach in future work.A further improvement is to take advantage ofsynonymy.
There are many ways to convey the samebasic message: i am sick, i am not feeling well, i?munder the weather, are all ways for a speaker to con-vey that he or she is not in the best of health.
Inthe current system, these are all treated separately.Clearly what is needed is a way of recognizing thatthese are all paraphrases of each other.
Fortunately,there has been a lot of progress in recent years onparaphrasing ?
see Ganitkevitch et al (2011) for arecent example ?
and such work could in princi-ple be adapted to the problem here.
Indeed it seemslikely that incorporating paraphrasing into the sys-tem will be a major source of improved coverage.A limitation of the work described here is thatit only models turn-to-turn interactions.
Clearlydiscourse models need to have more memory thanthis, so features that relate to earlier turns would beneeded.
The downside is that this would quicklylead to data sparsity.There are a variety of machine learning tech-niques that could also be tried, beyond the rather15Input: this is n?t the same .
this is not like anything i have beenthrough before .
i mean , how am i supposed to make it work withsomebody who ...Pred.
nouns: strength, somebodyOutput: strength comes with somebodyInput: i ?ve been a little bit too busy to socialize .
i did have aninteresting conversation with your sister , however .Pred.
nouns: bit, conversation, sisterOutput: a bit about this conversation with sisterFigure 4: Generating with nPMI: Creating syntactic structure around likely nouns.simple methods employed in this work.
For exam-ple, particular classes of response types, comprisinga variety of related utterances, may be predictableusing the extracted features.Finally, we have assumed for this discussion thatthe AAC system is only within the control of the im-paired user.
There is no reason to make that assump-tion in general: many AAC situations in real life in-volve a helper who will often co-construct with theimpaired user.
Such helpers usually know the im-paired user very well and can often make reasonableguesses as to the whole utterance intended by theimpaired user.
Recent work reported in Roark et al(2011) suggests one way in which the results of alanguage modeling system and those of a human co-constructor may be integrated into a single system,and such an approach could easily be applied here.7 ConclusionsWe have proposed and evaluated an approach towhole utterance prediction for AAC.
While the ap-proach is fairly simple, it is able to generate corrector at least reasonable responses in some cases.
Sucha system could be used in conjunction with othertechniques, such as language-model-based predic-tion, or co-construction.
One of the potentially use-ful side-effects of this work is an estimate of whatpercentage of interactions in a dialog are likely to beeasily handled by such techniques.
In other words,how many interactions in dialog are sufficiently pre-dictable that a system could have a reasonable guessas to what a speaker is going to say given the pre-vious context?
A rough estimate based on what wehave found here is something on the order of 3.5%-4.0%.
Obviously this does not mean that the sys-tem will always make the right prediction: a reason-able response to congratulations on your promotionwould often be thank you, but a speaker may wishto say something else.
But what it does mean is thatin about 3.5%-4.0% of cases, one has a reasonablechance of being able to guess.
This percentage iscertainly small, and one might be inclined to con-clude that the approach does not work.
On the otherhand, it is important to bear in mind that not all per-centages are created equal.
Rapid responses to ba-sic phrases (e.g.
Are you mad at me?
?
No, I?mnot mad at you), could help with the perceived flowof conversation, even if they do not occur that fre-quently.As we noted at the outset, whole utterance pre-diction is an area that has received increased inter-est in recent years, because of its potential to speedcommunication, and its contribution to increasingthe naturalness of conversational interactions.
Whencoupled with gains in utterance generation achievedby other methods, automatically generating utter-ances can further the range of comments and re-sponses available to AAC users.
The work reportedhere is a small contribution towards this goal.AcknowledgmentsThis work was supported under grant NIH-K25DC011308.
Sproat thanks his K25 mentor,Melanie Fried-Oken, for discussion and support.
Wealso thank four anonymous reviewers, as well as theaudience at a Center for Spoken Language Under-standing seminar, for their comments.16ReferencesN.
Alm, J. L. Arnott, and A. F. Newell.
1992.
Predic-tion and conversational momentum in an augmentativecommunication system.
Communications of the ACM,35(5):46?57.N.
Alm, J. Todman, Leona Elder, and A. F. Newell.
1993.Computer aided conversation for severely physicallyimpaired non-speaking people.
Proceedings of IN-TERCHI ?93, pages 236?241.Bruce Baker.
1990.
Semantic compaction: a basic tech-nology for artificial intelligence in AAC.
In 5th An-nual Minspeak Conference.J.
J. Darragh, I. H. Witten, and M. L. James.
1990.
Thereactive keyboard: A predictive typing aid.
Computer,23(11):41?49.Martin Dempster, Norman Alm, and Ehud Reiter.
2010.Automatic generation of conversational utterances andnarrative for augmentative and alternative communi-cation: A prototype system.
Proceedings of the Work-shop on Speech and Language Processing for AssistiveTechnologies (SLPAT), pages 10?18.R.
Dye, N. Alm, J. L. Arnott, G. Harper, and A Morrison.1998.
A script-based AAC system for transactionalinteraction.
Natural Language Engineering, 4(1):57?71.Michael Elhadad.
1991.
FUF: The universal unifer-usermanual version 5.0.
Technical report.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages363?370.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
Proceedings of EmpiricalMethods in Natural Language Processing (EMNLP).John Godfrey and Edward Holliman.
1997.Switchboard-1 release 2.
Linguistic Data Con-sortium, Philadelphia.D.
J. Higginbotham, D. P. Wilkins, G. W. Lesher, andB.
J. Moulton.
1999.
Frametalker: A communicationframe and utterance-based augmentative communica-tion device.
Technical Report.D.
Jeffery Higginbotham.
1992.
Evaluation of keystrokesavings across five assistive communication technolo-gies.
Augmentative and Alternative Communication,8:258?272.Julia King, Tracie Spoeneman, Sheela Stuart, and DavidBeukelman.
1995.
Small talk in adult conversations:Implications for AAC vocabulary selection.
Augmen-tative and Alternative Communication, 11:260?264.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
Proceedings of the 41st Meet-ing of the Association for Computational Linguistics(ACL), pages 423?430.S.
Langer and M. Hickey.
1998.
Using semantic lexiconsfor full text message retrieval in a communication aid.Natural Language Engineering, 4(1):41?55.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the conll-2011 shared task.
Proceedings of theCoNLL-2011 Shared Task.J.
Li and G. Hirst.
2005.
Semantic knowledge in wordcompletion.
In Proceedings of the 7th InternationalACM Conference on Computers and Accessibility.K.
McCoy, C. A. Pennington, and A. L. Badman.
1998.Compansion: From research prototype to practical in-tegration.
Natural Language Engineering, 4(1):73?95.Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,and Dallas E. Johnson.
2007.
Brevity and speed ofmessage delivery trade-offs in augmentative and alter-native communication.
Augmentative and AlternativeCommunication, 23(1):76?88.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Sratos, Xufeng Han, Alysssa Mensch,Alex Berg, Tamara L. Berg, and Hal Daume?
III.
2012.Midge: Generating image descriptions from computervision detections.
Proceedings of EACL 2012.Y.
Netzer and M. Elhadad.
2006.
Using semantic author-ing for Blissymbols communication boards.
Proceed-ings of HLT 2006, pages 105?108.Amruta Purandare and Diane Litman.
2008.
Analyzingdialog coherence using transition patterns in lexicaland semantic features.
In FLAIRS Conference, pages195?200.Brian Roark, Andrew Fowler, Richard Sproat, Christo-pher Gibbons, and Melanie Fried-Oken.
2011.
To-wards technology-assisted co-construction with com-munication partners.
Proceedings of the Workshop onSpeech and Language Processing for Assistive Tech-nologies (SLPAT).Cort Stratton.
2010.
PyAIML, a Python AIML inter-preter.
http://pyaiml.sourceforge.net/.J.
Todman, A. Norman, J. Higginbotham, and P. File.2008.
Whole utterance approaches in AAC.
Augmen-tative and Alternative Communication, 24(3):235?254.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
Proceed-ings of HLT-NAACL, pages 252?259.17K.
Trnka, D. Yarrington, K.F.
McCoy, and C. Pennington.2006.
Topic modeling in fringe word prediction forAAC.
In Proceedings of the International Conferenceon Intelligent User Interfaces, pages 276?278.K.
Trnka, D. Yarrington, J. McCaw, K.F.
McCoy, andC.
Pennington.
2007.
The effects of word predic-tion on communication rate for AAC.
In Proceed-ings of HLT-NAACL; Companion Volume, Short Pa-pers, pages 173?176.H.
Trost, J. Matiasek, and M. Baroni.
2005.
The lan-guage component of the FASTY text prediction sys-tem.
Applied Artificial Intelligence, 19(8):743?781.Richard Wallace.
2012.
A.L.I.C.E.
(Artificial LinguisticInternet Computer Entity).
http://www.alicebot.org/.T.
Wandmacher and J.Y.
Antoine.
2007.
Methods to in-tegrate a language model with semantic informationfor a word prediction component.
In Proceedings ofEmpirical Methods in Natural Language Processing(EMNLP), pages 506?513.Joseph Weizenbaum.
1966.
Eliza ?
a computer programfor the study of natural language communication be-tween man and machine.
Proceedings of the ACM,9(1).Bruce Wisenburn and D. Jeffery Higginbotham.
2008.An AAC application using speaking partner speechrecognition to automatically produce contextually rel-evant utterances: Objective results.
Augmentative andAlternative Communication, 24(2):100?109.18
