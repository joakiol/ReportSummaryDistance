Enriching Automated Essay Scoring Using Discourse MarkingJill Burstein, Karen Kukich, Susanne Wolff, Chi Lu+ and Martin Chodorow~.
"~ Educational Testing Service, Princeton NJHunter College.
New York.
New York?
AbstractElectronic Essay Rater (e-rater) is a prototype automated essay scoring system built at Educational Testing Service(ETS) that uses discourse marking, in addition to syntactic information and topical content vector analyses toautomatically assign essay scores.
This paper gives a general description ore-rater as a whole, but its emphasis is onthe importance of discourse marking and argument partitioning for annotating the argument structure of an essay.We show comparisons between two content vector analysis programs used to predict scores .
EsscQ/'Content andArgContent.
EsscnContent assigns cores to essays by using a standard cosine correlation that treats the essay like a"'bag of words."
in that it does not consider word order.
Ark, Content employs a novel content vector analysisapproach for score assignment based on the individual arguments in an essay.
The average agreement betweenArgContent scores and human rater scores is 82%.
as compared to 69% agreement between EssavContent and thehuman raters.
These results suggest hat discourse marking enriches e-rater's coring capability.
When e-rater usesits whole set of predictive features, agreement with human rater scores ranges from 87?,/o - 94% across the 15 sets ofessa5 responses used in this study1.
IntroductionThe development of Electronic Essay Rater (e-rater).an automated prototype essay scoring system, wasmotivated by practical concerns of time and costs thatlimit the number of essay questions on currentstandardized tests.
Literature on automated essayscoring shows that reasonably high agreement can beachieved between a machine score and a human raterscore simply by doing analyses based on the numbero f  words in an essay (Page and Peterson (1995)).Scoring an essay based on the essay length is not acriterion that can be used to define competentwriting.
In addition, from a practical standpoint.essay length is a highly coachable feature.
It doesn'ttake examinees long to figure out that a computerwill assign a high score on an essay based on a pre-specified number of words.E-rater's modules extract syntactic and discoursestructure information from essays, as well asinformation about vocabulary content in order topredict the score.
The 57 features included in e-rater15are based on writing characteristics specified at eachof the six score points in the scoring guide used byhuman raters for manual scoring (also available athttp://www.gmat.org;).
For example, the scoringguide indicates that an essay that stays on the topicof the test question, has a strong, coherent and well-organized argument structure, and displays a varietyof word use and syntactic structure will receive ascore at the higher end of the six-point scale (5 or 6}.Lower scores are assigned to essays as thesecharacteristics diminish.Included in e-rater's feature set are features derivedfrom discourse structure, syntactic structure, andtopical analysis as they relate to the human scoringguide.
For each essay question, e-rater is run on aset of training data (human-scored essay responses)to extract t~.atures.
A stepwise linear regressionanalysis is performed on the features extracted fromthe training set to determine which ones havesignificant weights (the predictive features).
Finalscore prediction for cross-validation sets is performedusing these predictive features identified in thetraining sets.
Accuracy is determined by measuringagreement between human rater assigned scores andmachine predicted scores, which are considered to"agree" if there is no greater than a single pointdifference on the six-point scale.
This is the samecriterion used to measure agreement between twohuman raters.Among the strongest predictive features across theessay questions used in this study are the scoresgenerated from ArgContent (a content vector analysisapplied to discourse chunked text), and discourse-related surface cue word and non-lexical features.
Onaverage, ArgContent alone has 82% agreement withthe human rater score as compared to EssavContent's69%.
EssayContent is a content vector analysisprogram that treats an essay like a "'bag of words.
"This suggests two things.
First, the discoursemarkers detected by the argument annotation andpartitioning program.
APA.
are helpful foridentification of relevant units of discourse in essayresponses.
Second.
the application of content vectoranalysis to those text units appears to increasescoring performance.
Overall, it appears thatdiscourse marking provides feature information thatis useful in e-rater's essay score predictions.A long-term goal of automated essay scoring is to beable to generate diagnostic or instructionalinformation, along with a numeric score to a test-taker or instructor.
Information about the discoursestructure of essays brings us closer to being able togenerate informative feedback to test-takers about theessay's cohesion.We report on the overall evaluation results from c-rater's scoring performance on 13 sets of essay datafrom the Analytical Writing Assessments of theGraduate Management Admissions Test (GMAT)(see http://www.gmat.org/) and 2 sets of essay datafrom the Test of Written English (TWE) (seehttp://w.w.w.toefl.or~tstprpmt.html for sample TWEquestions).
The paper devotes pecial attention to e-rater's discourse marking and analysis components.2.
Hybrid Feature MethodologyE-rater uses a hybrid feature approach in that itincorporates several variables that are derivedstatistically, or extracted through NLP techniques.The following sections describe the features used inthis study.2.1 Syntactic FeaturesThe scoring guides indicate that one feature used toevaluate an essay is syntactic variety.
Syntacticstructures in essays are identified using NLPtechniques.
All sentences are parsed with theMicrosoft Natural Language Processing tool(MSNLP) (see MSNLP(1997)).
Examination of  theparse trees yields information about syntactic varietywith regard to what kinds of clauses or verb typeswere used by a test-taker.A program was implemented to identify the numberof complement clauses, subordinate clauses,infinitive clauses, relative clauses and occurrences oftile subjunctive modal auxiliary, verbs, would, could,.~'hould.
might and may, tbr each sentence in an essay.Ratios of syntactic structure types per essay and persentence were calculated as possible measures ofsyntactic variety.2.2 Discourse Structure AnalysisGMAT essay questions are of two types: Analysis ofan Issue (issue) and Analysis of  an Argument(argument).
The issue essay asks the writer torespond to a general question and to provide "reasonsand'or examples" to support his or her position on anissue introduced by the test question.
The argumentessay tbcuses the writer on the argument in a givenpiece of text.
using the term argument in tile sense ofa rational presentation of points with the purpose ofpersuading the reader.
The scoring guides used formanual scoring indicate that an essay will receive ascore based on the examinee's demonstration of  awell-developed essay.
For the argument essay', forinstance, tile scoring guide states that a "'6"" essay"'develops ideas cogently, organizes them logically,and connects them with clear transitions."
Thecorrelate to this for the issue essay would appear tobe that a "'6"" essay "'...develops a position on theissue with insightful reasons..." and that the essay "'isclearly well-organized.'"
Nolan (I 997) points out thatterms in holistic scoring guides, such as "'cogent.""'logical."
"'insightful."
and "'well-organized" have"'fuzzy" meaning, since they are based on impreciseobservation.
Nolan uses methods of"fuzzy, logic"to automatically assign these kinds of "fuzzy"classifications to essays.
In this study, we try, toidentify organization of  an essay through automatedanalysis and identification of the essay's argumentstructure through discourse marking.16Since there is no particular text unit that reliablyCorresponds to the stages, steps, or passages of anargument, readers of an essay must rely on otherthings such as surface cue words to identifyindividual arguments.
We found that it was useful toidentify rhetorical relations such as Parallelism andContrast.
and content or coherence relations thathave more to do with the discourse involved.
Theserelations can appear at almost any level -- phrase,sentence, a chunk consisting of several sentences, orparagraph.
Therefore, we developed a program toautomatically identify the discourse unit of text usingsurface cue words and non-lexical cues.arg_init#PARALLEL = alsoarg_init#CLAIM_THAT = thatarg_aux#SPECULATE =mavSentence 3: It is conceivuble that other programssuch us arts.
music or social sciences will be mostUffected by this drop in high school population.arg_dev#SAME_TOPlC = Itarg_dev#CLA\[M_YHAY = thatarg_dev# D ETA \[ L = such_asFigure I: APA Output for 2 Essay SentencesAs literature in the field of  discourse analysis pointsout.
surface cue words and structures can beidentified and used for computer-based discourseanalysis (Cohen (1984), (Mann and Thompson(1988), Hovy.
et al(1992) Hirschberg and Litman(1993), Vander Linden and Martin (1995), Knott(1996) and Litman (1996)).
E-rater's AP.4 moduleuses surface cue words and non-lexical cues (i.e.,syntactic structures) to denote discourse structure inessays.
We adapted the conceptual framework ofconjunctive relations from Quirk.
et al(1985) in~hich terms, such as "'In summary" and "'Inconclusion," which we consider to be surface cueterms, are classified as conjuncts used totsummarizing.
Cue words such as "'perhaps" and"'possibly" are considered to be Belief words used bythe writer to express a belief with regard to argumentdevelopment in essays.
Words like "'this" and"'these" may often be used to flag that the writer isdeveloping on the same topic (Sidner (1986)).
Wealso observed that.
in certain discourse contexts, non-lexicat, syntactic structure cues, such as infinitive orcomplement clauses, may characterize the beginningof  a new argument.The automated argument partitioning and annotationprogram (APA) was implemented to output adiscourse-marked annotated version of each essay inwhich the discourse marking is used to indicate newarguments (arg_init), or development of an argument(arg_dev).
An example of APA annotations i shownin Figure I.New Paragraph:.
.
.Sentence I: fl is also assumed that shrinking highschool enrollment ram, lead to a shortage of qual(fiedengineers.AP.4"s heuristic rules for discourse marker annotationand argument partitioning are based on syntactic andparagraph-based distribution of surface cue words.phrases and non-lexical cues corresponding todiscourse structure.
Relevant cue words and terms arecontained in a specialized surface cue word and phraselexicon.
In Figure 1, the annotations.arg_init#PARALLEL, and arg_dev#DETAIL indicatethe rhetorical relations of Parallel structure and Detailintbrmation, respectively, in arguments.
Thearg_dev~-SAME_TOPIC label denotes the pronoun "'it"as indicating the writer has not changed topics.
Tilelabels arg_in it-C LA I M_THAT andarg_dev=CLAIM_THAT indicate that a complementclause was used to flag a new argument, or argumentdevelopment.
Arg_aux=SPECULATE flags subjunctivemodals that are believed to indicate a writer'sspeculation.
Preliminary analysis of these rulesindicates that some rule refinements might be useful;however, more research needs to be done on this.
~Based on the arg_init flags in the annotated essays,.4P.q outputs a version of the essay partitioned "byargument".
The argument-partitioned versions ofessays are input to .4rgContent.
tile discourse-driven,topical analysis program described below.2.3 Topical AnalysisGood essays are relevant o the assigned topic.
The}'also tend to use a more specialized and precisevocabulary in discussing the topic than poorer essaysdo.
We should therefore expect a good essay toresemble other good essays in its choice of wordsand.
conversely, a poor essay to resemble other poorones.
E-rater evaluates tile topical content of  an' We thank Mary Dee Harris for her analysis of  APAannotated outputs.17essay bx comparing the words it contains to thewords tbund in manually, graded training examplestbr each of the six score categories.
Two measures ofcontent similarity are computed, one based on wordfrequency and the other on word weight, as ininformation retrieval applications (Salton.
1988).
Forthe former application (EssayContent).
contentsimilarit2, is computed over the essay as a ~hole.while in the latter application (ArgComem) contentsimilarities are computed for each argument in anessay.For the frequency based measure (the EssavComentprogram), the content of each score category isconverted to a single vector whose elementsrepresent he total frequency of each word in thetraining essays for that category.
In effect, thismerges the essays for each score.
(A stop list of somefunction words is removed prior to vectorconstruction.)
The system computes cosinecorrelations between the vector for a given test essayand the six vectors representing the trainedcategories: the category that is most similar to the testessay is assigned as the evaluation of its content.
Anadvantage of using the cosine correlation is that it isnot sensitive to essay length, which ma3 vary'considerably.The other content similarity measure..4rgContem, iscomputed separatel3, for each argument in the testessay and is based on the kind of term weightingused in information retrieval.
For this purpose, theword frequency vectors for the six score categories.described above, are converted to vectors of wordweights.
The ~eight for word i in score category s is:W,,  =(freq,. '
max freq,) * Iog(n essays:,~,,,,'nessays, lwhere freq,~ is the frequency of word i in category s.max_freq~ is the frequency of the most frequent wordin x (after a stop list of  words has been removed).n_essays,o,., is the total number of training essaysacross all six categories, and n_essays, is the numberof  training essays containing word i.The first part of the weight formula represents theprominence of word i in the score category, and thesecond part is the log of the word's inverse documentfrequency (IDF).
For each argument a in the testessay, a vector of  word weights is also constructed.The weight for ~ord i in argument a is(freq,,;max_freq,0 * Iog(n_essays,o,,,/n_essays,)where freq, ~ is the frequency of  word i in argument a.and max_freq, is the frequency of  the most frequentword in a (once again, after a stop list of  words hasbeen removed).
Each argument (as it has beenpartitioned by APA) is evaluated by computingcosine correlations between its weighted vector andthose of the six score categories, and the most similarcategory is assigned to the argument.
As a result ofthis analysis, e-rater has a set of scores (one perargument) for each test essay.We were curious to find out if an essay containingseveral good arguments (each with scores of  5 or 6)and several poor arguments (each with scores o f  1 or2) produced a different overall judgment by thehuman raters than an essay consisting of  uniformlymediocre arguments (3"s or 4"s).
or if perhapshumans were most influenced by the best or poorestargument in the essay.
In a preliminary study, welooked at how well the minimum, maximum, mode.median, and mean of the set of  argument scoresagreed with the judgments of  human raters for theessay as a whole.
The mode and the mean showedgood agreement with human raters, but the greatestagreernent was obtained from an adjusted mean ofthe argument scores which compensated for an effectof the number of arguments in the essay.
Forexample, essays which contained only one or twoarguments tended to receive slightly lower scoresfrom the human raters than the mean of the argumentscores, and essays which contained many argumentstended to receive slightly higher scores than the meanof tile argument scores.
To compensate for this, anadjusted mean is used as e-rater's ArgContent..qri~C'otllent =((arg_scores +n_args) / (n_args + I)3.
Training and TestingIn all.
e-rater's syntactic, discourse, and topicalanalyses yielded a total of 57 features for each essay.The majority of the features in the overall feature setare discourse-related (see Table 3 for someexamples).
To predict the score assigned by humanraters, a stepwise linear regression analysis was usedto compute the optimal weights for these predictorsbased on manually scored training essays.
Thetraining sets for each test question consisted of  a total18of 270 essays.
5 essays for score 0:, 15 essays forscore I (a rating infrequently used by the humanraters) and 50 essays each for scores 2 through 6.After training, e-rater analyzed new test essays, andthe regression weights were used to combine themeasures into a predicted score for each one.
E-toterpredictions were compared to the two human raterscores to measure xact and adjacent agreement (seeTable 1).
Figure 2 shows the predictive feature setidentified by the regression analysis for one of theexample test questions.
ARG I, in Tables !
and 2.I.
ArgContent Score2.
EssayContent Score3.
Total Argument DevelopmentWords/Phrases4.
Total Pronouns Beginning Arguments5.
Total Complement Clauses BeginningArguments6.
Total Summary Words BeginningArguments7.
Total Detail Words Beginning Arguments8.
Total Rhetorical Words DevelopingArguments9.
Subjunctive Modal VerbsFigure 2: Predictive Feature Set for ARG !
TestQuestion3.1 ResultsTable I shows the overall results for 8 GMATargument questions, 5 GMAT issue questions and 2TWE questions.
The level of agreement between e-rater and the human raters ranged from 87% to 94%across the 15 tests.
Agreement appears to becomparable to that found between the human raters.Table I: E-rater (E) and Human Rater (HR)Percentage Agreement & Human InterraterPercentage Agreement For Cross-Validation TestsQuestionArglArg2Arg3Arg4n = HR - HR1 HR2HR2 ~ E E552 92 87 89517 93 91 89577 87 87 89592 '91 92 93: O's either contain no text or the response ts off-topic.Arg5 634 02 9 IArg6 706 87 87Arg7 "1 q 90 91Arg8 684 89 89lssuel 700 90 89Issue2 747 92 89Issue3 795 88 87Issue4 879 92 87Issue5 915 93 89TWEI 260 ... .
.
.
93TWE2 287 ... .
.
.
0419018888909090868789Table 2 shows that scores generated by ArgContenthave higher agreement with human raters than doscores generated by Essaa'Content.
This suggests thatthe discourse structures generated by APA are usefulfor score prediction, and that the application ofcontent vector analysis to text partitioned into smallerunits of discourse might improxe e-rater's overallstoring accuracy.Table 2: Percentage Agreement BetweenEssayContent (EC) or .4rgContent (,4C) andHuman Rater ScoreQuestionArglA rg2Arg3A rg4Arg5Arg6?
x, rg 7\rg8Issue lIssue2Issue3Issue4Issue5TWEITWE2AverageI f=552517577502634706719684709747\] 795879915260287638HRI- HR2 EC AC92 69 7393 68 7587 72 7691 70 8192 72 8187 67 8290 68 8089 62 80O0 67 8292 65 8388 64 8492 69 8393 69 85. .
.
.
.
.
77 88. .
.
.
.
.
77 9100 69 82Results tbr the essay questions in Tables I and 2represent a wide variety of topics.
(Sample questionsthat show topical variety in GMAT essays can beviewed at http://www.gmat.org/.
Topical variety inTWE questions can be reviewed athttp://www.toefl.org/tstprpmt.html.)
The data alsorepresented a wide range of English writingcompetency.
The majority of test-takers from thetwo TWE data sets were nonnative English speakers.Despite these differences in topic and writing skill, e-rater, as well as EssayContenr and ArgContentperformed consistently across items.
In fact.
over the15 essay questions, the discourse t~atures output byAPA and scores output by ArgContent (based ondiscourse-chunked text) account for the majority ofthe most frequently occurring predictive features.These are shown in Table 3.We believe that the discourse related features used bye-rater might be the most useful building blocks forautomated generation of diagnostic and instructionalsumnaaries about essays.
For example, sentencesindicated as "'the beginning of an argument" could beused to flag main points of an essay (Marcu (1997)).ArgContent's ability to generate "'scores" for eachargument could provide information about therelevance of individual arguments in an essay, whichin turn could be used to generate helpful diagnosticor instructional information.Table 3: Most Frequently Occurring PredictiveFeatures Across 15 Essay QuestionsFeatu~'e Feature Feature, Class CountsAr,.,Content ~ i'opical/ 15:15DiscourseEssavContent \] Topical 14 15Total Argument Discourse 14/15Development Words \]Auxiliary Modals S\ ntactic' 12'15Arg lnit: Discourse 7/15Complement ClausesArg Development: Discourse 6, 15Rhetorical QuestionWordsArg Development: Discourse 6 15Evidence WordsSubordinate Clauses Syntactic 2 15Relative Clauses Syntactic 4'154.
Discussion and ConclusionsThe study indicates that discourse, syntactic, andtopical information can be reliably used for machineprediction of essay scores.
The results uggest that e-ruter's discourse marking is informative to thescoring process.
ArgContenr the statistical, topicaldiscourse analyzer, appears to be the most predictivefeature.
Other highly ranked features include surfacecue words and non-lexical discourse cues.5.
ReferencesCohen.
Robin (I 984).
"'A computational theory of thethnction of clue words in argument understanding.
"In Proceedings of 1984 International ComputationalLinguistics Cont'erence.
California.
251-255..Hirschberg.
Julia and Diane Litman (1993).
"'Empirical Studies on the Disambiguation of CuePhrases."
Computational Linguistics ( 19)3, 501-530.Hovy, Eduard.
Julia Lavid, Elisabeth Maier,"'Employing Knowledge Resources in a New TextPlanner Architecture," In Aspects of Automated NLGeneration.
Dale.
Hovy, Rosner and Stoch (Eds),Springer-Verlag Lecture Notes in AI no.
587.57-72.GMAT (1997).
http:,:www.gmat.org/Knott.
Alistair.
(1996).
"'A Data-DrivenMethodology for Motivating a Set of CoherenceRelations."
Ph.D. Dissertation.
available at~vww,co~dsci.edu.ac.uk/-alik/pub!ications ..html.
under the Heading, Unpublished Stuff.Litman.
Diane, J.
(1996).
"'Cue Phrase ClassificationUsing Machine Learning."
Artificial Intelligence.
5.53-94.Mann.
William C. and Sandra A. Thompson (1988).
"'Rhetorical Structure Theory: Toward a functionaltheory of text organization."
Text 8(3).
243-28 I.One line of future research will examine the effectsof various term weighting schemes on theperformance of both ArgContent and EssavContenrAnother study will compare the argumentboundaries assigned by APA and the positions whichhuman readers judge to be beginnings and ends ofarguments.Marcu, Daniel.
(1997).
"'From Discourse Structuresto Text Summaries.
", In Proceedings of theIntelligent Scalable Text Summarization Workshop,Association for Computational Linguistics.Universidad Nacional de Educacion a Distancia,Madrid.
Spain.M SN L P (1997) http://research.m icrosoft.com/n Ip/20Nolan.
James (1997).
The Architecture of a HybridKnowledge-Based System for Evaluating WritingSamples.
In A. Niku-Lari (Ed.)
Expert SystemsApplications and Artificial Intelligence TechnologyTransfer Series, EXPERSYS-97.
Gournay S,M,France: ilTT International.Page, E.B.
and Peterson.
N. (1995).
The computermoves into essay grading: updating the ancient est.Phi Delta Kappan.
March, 561-565.Qui(k, Randolph.
Sidney Greenbaum.
GeoffreyLeech, and Jan Svartik (1985).
A ComprehensiveGrammar of the English Language.
Longman.
NewYork.Sidner.
Candace.
(1986).
Focusing in theComprehension f Definite Anaphora.
In Readings inNatural Language Processing.
Barbara Grosz.
KarenSparck Jones.
and Bonnie Lynn Webber (Eds.
),Morgan Kaufmann Publishers.
Los Altos, California,363-394.Salton.
Gerard.
(1988).
Automatic text processing ?the transformation, analysis, and retrieval ofinllbrmation by computer.
Addison-Wesley, Reading.Mass.TOEFL (1997).
http://www.toefl.org/tstprpmt.htmlVander Linden, Keith and James H. Martin (1995).
"'Expressing Rhetorical Relations in InstructionalText: A Case Study in Purpose Relation.
"Computational Linguistics 2I( I ), 29-57.21
