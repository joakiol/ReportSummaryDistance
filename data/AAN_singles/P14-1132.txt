Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403?1414,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsIs this a wampimuk?Cross-modal mapping between distributional semanticsand the visual worldAngeliki Lazaridou and Elia Bruni and Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{angeliki.lazaridou|elia.bruni|marco.baroni}@unitn.itAbstractFollowing up on recent work on estab-lishing a mapping between vector-basedsemantic embeddings of words and thevisual representations of the correspond-ing objects from natural images, we firstpresent a simple approach to cross-modalvector-based semantics for the task ofzero-shot learning, in which an imageof a previously unseen object is mappedto a linguistic representation denoting itsword.
We then introduce fast mapping, achallenging and more cognitively plausi-ble variant of the zero-shot task, in whichthe learner is exposed to new objects andthe corresponding words in very limitedlinguistic contexts.
By combining priorlinguistic and visual knowledge acquiredabout words and their objects, as well asexploiting the limited new evidence avail-able, the learner must learn to associatenew objects with words.
Our results onthis task pave the way to realistic simula-tions of how children or robots could useexisting knowledge to bootstrap groundedsemantic knowledge about new concepts.1 IntroductionComputational models of meaning that rely oncorpus-extracted context vectors, such as LSA(Landauer and Dumais, 1997), HAL (Lund andBurgess, 1996), Topic Models (Griffiths et al,2007) and more recent neural-network approaches(Collobert and Weston, 2008; Mikolov et al,2013b) have successfully tackled a number of lex-ical semantics tasks, where context vector sim-ilarity highly correlates with various indices ofsemantic relatedness (Turney and Pantel, 2010).Given that these models are learned from natu-rally occurring data using simple associative tech-niques, various authors have advanced the claimthat they might be also capturing some crucial as-pects of how humans acquire and use language(Landauer and Dumais, 1997; Lenci, 2008).However, the models induce the meaning ofwords entirely from their co-occurrence with otherwords, without links to the external world.
Thisconstitutes a serious blow to claims of cogni-tive plausibility in at least two respects.
Oneis the grounding problem (Harnad, 1990; Searle,1984).
Irrespective of their relatively high per-formance on various semantic tasks, it is debat-able whether models that have no access to visualand perceptual information can capture the holis-tic, grounded knowledge that humans have aboutconcepts.
However, a possibly even more seriouspitfall of vector models is lack of reference: natu-ral language is, fundamentally, a means to commu-nicate, and thus our words must be able to refer toobjects, properties and events in the outside world(Abbott, 2010).
Current vector models are purelylanguage-internal, solipsistic models of meaning.Consider the very simple scenario in which visualinformation is being provided to an agent aboutthe current state of the world, and the agent?s taskis to determine the truth of a statement similar toThere is a dog in the room.
Although the agentis equipped with a powerful context vector model,this will not suffice to successfully complete thetask.
The model might suggest that the conceptsof dog and cat are semantically related, but it hasno means to determine the visual appearance ofdogs, and consequently no way to verify the truthof such a simple statement.Mapping words to the objects they denote issuch a core function of language that humans arehighly optimized for it, as shown by the so-calledfast mapping phenomenon, whereby children canlearn to associate a word to an object or prop-erty by a single exposure to it (Bloom, 2000;Carey, 1978; Carey and Bartlett, 1978; Heibeckand Markman, 1987).
But lack of reference is not1403only a theoretical weakness: Without the ability torefer to the outside world, context vectors are ar-guably useless for practical goals such as learningto execute natural language instructions (Brana-van et al, 2009; Chen and Mooney, 2011), thatcould greatly benefit from the rich network of lex-ical meaning such vectors encode, in order to scaleup to real-life challenges.Very recently, a number of papers have ex-ploited advances in automated feature extractionform images and videos to enrich context vectorswith visual information (Bruni et al, 2014; Fengand Lapata, 2010; Leong and Mihalcea, 2011;Regneri et al, 2013; Silberer et al, 2013).
Thisline of research tackles the grounding problem:Word representations are no longer limited to theirlinguistic contexts but also encode visual informa-tion present in images associated with the corre-sponding objects.
In this paper, we rely on thesame image analysis techniques but instead focuson the reference problem: We do not aim at en-riching word representations with visual informa-tion, although this might be a side effect of ourapproach, but we address the issue of automati-cally mapping objects, as depicted in images, tothe context vectors representing the correspond-ing words.
This is achieved by means of a simpleneural network trained to project image-extractedfeature vectors to text-based vectors through a hid-den layer that can be interpreted as a cross-modalsemantic space.We first test the effectiveness of our cross-modal semantic space on the so-called zero-shotlearning task (Palatucci et al, 2009), which has re-cently been explored in the machine learning com-munity (Frome et al, 2013; Socher et al, 2013).
Inthis setting, we assume that our system possesseslinguistic and visual information for a set of con-cepts in the form of text-based representations ofwords and image-based vectors of the correspond-ing objects, used for vision-to-language-mappingtraining.
The system is then provided with visualinformation for a previously unseen object, and thetask is to associate it with a word by cross-modalmapping.
Our approach is competitive with re-spect to the recently proposed alternatives, whilebeing overall simpler.The aforementioned task is very demanding andinteresting from an engineering point of view.However, from a cognitive angle, it relies onstrong, unrealistic assumptions: The learner isasked to establish a link between a new object anda word for which they possess a full-fledged text-based vector extracted from a billion-word cor-pus.
On the contrary, the first time a learner isexposed to a new object, the linguistic informa-tion available is likely also very limited.
Thus, inorder to consider vision-to-language mapping un-der more plausible conditions, similar to the onesthat children or robots in a new environment arefaced with, we next simulate a scenario akin to fastmapping.
We show that the induced cross-modalsemantic space is powerful enough that sensibleguesses about the correct word denoting an objectcan be made, even when the linguistic context vec-tor representing the word has been created from aslittle as 1 sentence containing it.The contributions of this work are three-fold.First, we conduct experiments with simple image-and text-based vector representations and comparealternative methods to perform cross-modal map-ping.
Then, we complement recent work (Fromeet al, 2013) and show that zero-shot learningscales to a large and noisy dataset.
Finally, we pro-vide preliminary evidence that cross-modal pro-jections can be used effectively to simulate a fastmapping scenario, thus strengthening the claimsof this approach as a full-fledged, fully inductivetheory of meaning acquisition.2 Related WorkThe problem of establishing word reference hasbeen extensively explored in computational sim-ulations of cross-situational learning (see Fazly etal.
(2010) for a recent proposal and extended re-view of previous work).
This line of research hastraditionally assumed artificial models of the ex-ternal world, typically a set of linguistic or logi-cal labels for objects, actions and possibly otheraspects of a scene (Siskind, 1996).
Recently,Yu and Siskind (2013) presented a system thatinduces word-object mappings from features ex-tracted from short videos paired with sentences.Our work complements theirs in two ways.
First,unlike Yu and Siskind (2013) who considered alimited lexicon of 15 items with only 4 nouns, weconduct experiments in a large search space con-taining a highly ambiguous set of potential targetwords for every object (see Section 4.1).
Most im-portantly, by projecting visual representations ofobjects into a shared semantic space, we do notlimit ourselves to establishing a link between ob-1404jects and words.
We induce a rich semantic rep-resentation of the multimodal concept, that canlead, among other things, to the discovery of im-portant properties of an object even when we lackits linguistic label.
Nevertheless, Yu and Siskind?ssystem could in principle be used to initialize thevision-language mapping that we rely upon.Closer to the spirit of our work are two veryrecent studies coming from the machine learningcommunity.
Socher et al (2013) and Frome et al(2013) focus on zero-shot learning in the vision-language domain by exploiting a shared visual-linguistic semantic space.
Socher et al (2013)learn to project unsupervised vector-based imagerepresentations onto a word-based semantic spaceusing a neural network architecture.
Unlike us,Socher and colleagues train an outlier detectorto decide whether a test image should receive aknown-word label by means of a standard super-vised object classifier, or be assigned an unseenlabel by vision-to-language mapping.
In our zero-shot experiments, we assume no access to an out-lier detector, and thus, the search for the correctlabel is performed in the full concept space.
Fur-thermore, Socher and colleagues present a muchmore constrained evaluation setup, where only 10concepts are considered, compared to our experi-ments with hundreds or thousands of concepts.Frome et al (2013) use linear regression totransform vector-based image representations ontovectors representing the same concepts in linguis-tic semantic space.
Unlike Socher et al (2013) andthe current study that adopt simple unsupervisedtechniques for constructing image representations,Frome et al (2013) rely on a supervised state-of-the-art method: They feed low-level features to adeep neural network trained on a supervised objectrecognition task (Krizhevsky et al, 2012).
Fur-thermore, their text-based vectors encode very richinformation, such as~king ?
~man + ~woman =~queen (Mikolov et al, 2013c).
A natural ques-tion we aim to answer is whether the success ofcross-modal mapping is due to the high-qualityembeddings or to the general algorithmic design.If the latter is the case, then these results could beextended to traditional distributional vectors bear-ing other desirable properties, such as high inter-pretability of dimensions.
(a) (b)Figure 1: A potential wampimuk (a) together withits projection onto the linguistic space (b).3 Zero-shot learning and fast mapping?We found a cute, hairy wampimuk sleeping be-hind the tree.?
Even though the previous state-ment is certainly the first time one hears aboutwampimuks, the linguistic context already createssome visual expectations: Wampimuks probablyresemble small animals (Figure 1a).
This is thescenario of zero-shot learning.
Moreover, if this isalso the first linguistic encounter of that concept,then we refer to the task as fast mapping.Concretely, we assume that concepts, denotedfor convenience by word labels, are represented inlinguistic terms by vectors in a text-based distri-butional semantic space (see Section 4.3).
Objectscorresponding to concepts are represented in vi-sual terms by vectors in an image-based semanticspace (Section 4.2).
For a subset of concepts (e.g.,a set of animals, a set of vehicles), we possess in-formation related to both their linguistic and visualrepresentations.
During training, this cross-modalvocabulary is used to induce a projection func-tion (Section 4.4), which ?
intuitively ?
representsa mapping between visual and linguistic dimen-sions.
Thus, this function, given a visual vector,returns its corresponding linguistic representation.At test time, the system is presented with a previ-ously unseen object (e.g., wampimuk).
This objectis projected onto the linguistic space and associ-ated with the word label of the nearest neighbor inthat space (degus in Figure 1b).The fast mapping setting can be seen as a spe-cial case of the zero-shot task.
Whereas for the lat-ter our system assumes that all concepts have richlinguistic representations (i.e., representations es-timated from a large corpus), in the case of the for-mer, new concepts are assumed to be encounted ina limited linguistic context and therefore lackingrich linguistic representations.
This is operational-ized by constructing the text-based vector for these1405Figure 2: Images of chair as extracted fromCIFAR-100 (left) and ESP (right).concepts from a context of just a few occurrences.In this way, we simulate the first encounter of alearner with a concept that is new in both visualand linguistic terms.4 Experimental Setup4.1 Visual DatasetsCIFAR-100 The CIFAR-100 dataset(Krizhevsky, 2009) consists of 60,000 32x32colour images (note the extremely small size)representing 100 distinct concepts, with 600images per concept.
The dataset covers a widerange of concrete domains and is organized into20 broader categories.
Table 1 lists the conceptsused in our experiments organized by category.ESP Our second dataset consists of 100K im-ages from the ESP-Game data set, labeled througha ?game with a purpose?
(Von Ahn, 2006).1TheESP image tags form a vocabulary of 20,515unique words.
Unlike other datasets used for zero-shot learning, it covers adjectives and verbs in ad-dition to nouns.
On average, an image has 14tags and a word appears as a tag for 70 images.Unlike the CIFAR-100 images, which were cho-sen specifically for image object recognition tasks(i.e., each image is clearly depicting a single ob-ject in the foreground), ESP contains a random se-lection of images from the Web.
Consequently,objects do not appear in most images in their pro-totypical display, but rather as elements of com-plex scenes (see Figure 2).
Thus, ESP constitutesa more realistic, and at the same time more chal-lenging, simulation of how things are encounteredin real life, testing the potentials of cross-modalmapping in dealing with the complex scenes thatone would encounter in event recognition and cap-tion generation tasks.1http://www.cs.cmu.edu/?biglou/resources/4.2 Visual Semantic SpacesImage-based vectors are extracted using the unsu-pervised bag-of-visual-words (BoVW) represen-tational architecture (Sivic and Zisserman, 2003;Csurka et al, 2004), that has been widely and suc-cessfully applied to computer vision tasks such asobject recognition and image retrieval (Yang et al,2007).
First, low-level visual features (Szeliski,2010) are extracted from a large collection of im-ages and clustered into a set of ?visual words?.The low-level features of a specific image are thenmapped to the corresponding visual words, and theimage is represented by a count vector recordingthe number of occurrences of each visual word init.
We do not attempt any parameter tuning of thepipeline.As low-level features, we use Scale InvariantFeature Transform (SIFT) features (Lowe, 2004).SIFT features are tailored to capture object partsand to be invariant to several image transfor-mations such as rotation, illumination and scalechange.
These features are clustered into vocab-ularies of 5,000 (ESP) and 4,096 (CIFAR-100) vi-sual words.2To preserve spatial information in theBoVW representation, we use the spatial pyramidtechnique (Lazebnik et al, 2006), which consistsin dividing the image into several regions, comput-ing BoVW vectors for each region and concatenat-ing them.
In particular, we divide ESP images into16 regions and the smaller CIFAR-100 images into4.
The vectors resulting from region concatenationhave dimensionality 5000 ?
16 = 80, 000 (ESP)and 4, 096 ?
4 = 16, 384 (CIFAR-100), respec-tively.
We apply Local Mutual Information (LMI,(Evert, 2005)) as weighting scheme and reduce thefull co-occurrence space to 300 dimensions usingthe Singular Value Decomposition.For CIFAR-100, we extract distinct visual vec-tors for single images.
For ESP, given the sizeand amount of noise in this dataset, we build vec-tors for visual concepts, by normalizing and sum-ming the BoVW vectors of all the images that havethe relevant concept as a tag.
Note that relevantliterature (Pereira et al, 2010) has emphasizedthe importance of learners self-generating multi-ple views when faced with new objects.
Thus, ourmultiple-image assumption should not be consid-ered as problematic in the current setup.2For selecting the size of the vocabulary size, we relied onstandard settings found in the relevant literature (Bruni et al,2014; Chatfield et al, 2011).1406Category Seen Concepts Unseen (Test) Conceptsaquatic mammals beaver, otter, seal, whale dolphinfish ray, trout sharkflowers orchid, poppy, sunflower, tulip rosefood containers bottle, bowl, can ,plate cupfruit vegetable apple, mushroom, pear orangehousehold electrical devices keyboard, lamp, telephone, television clockhousehold furniture chair, couch, table, wardrobe bedinsects bee, beetle, caterpillar, cockroach butterflylarge carnivores bear, leopard, lion, wolf tigerlarge man-made outdoor things bridge, castle, house, road skyscraperlarge natural outdoor scenes cloud, mountain, plain, sea forestlarge omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephantmedium-sized mammals fox, porcupine, possum, skunk raccoonnon-insect invertebrates crab, snail, spider, worm lobsterpeople baby, girl, man, woman boyreptiles crocodile, dinosaur, snake, turtle lizardsmall mammals hamster, mouse, rabbit, shrew squirrelvehicles 1 bicycle, motorcycle, train busvehicles 2 rocket, tank, tractor streetcarTable 1: Concepts in our version of the CIFAR-100 data setWe implement the entire visual pipeline withVSEM, an open library for visual seman-tics (Bruni et al, 2013).34.3 Linguistic Semantic SpacesFor constructing the text-based vectors, we fol-low a standard pipeline in distributional semantics(Turney and Pantel, 2010) without tuning its pa-rameters and collect co-occurrence statistics fromthe concatenation of ukWaC4and the Wikipedia,amounting to 2.7 billion tokens in total.
Seman-tic vectors are constructed for a set of 30K targetwords (lemmas), namely the top 20K most fre-quent nouns, 5K most frequent adjectives and 5Kmost frequent verbs, and the same 30K lemmas arealso employed as contextual elements.
We collectco-occurrences in a symmetric context window of20 elements around a target word.
Finally, simi-larly to the visual semantic space, raw counts aretransformed by applying LMI and then reduced to300 dimensions with SVD.54.4 Cross-modal MappingThe process of learning to map objects to the theirword label is implemented by training a projec-tion function fprojv?wfrom the visual onto the lin-guistic semantic space.
For the learning, we usea set of Nsseen concepts for which we have bothimage-based visual representations Vs?
RNs?dv3http://clic.cimec.unitn.it/vsem/4http://wacky.sslmit.unibo.it5We also experimented with the image- and text-basedvectors of Socher et al (2013), but achieved better perfor-mance with the reported setup.and text-based linguistic representations Ws?RNs?dw.
The projection function is subject toan objective that aims at minimizing some costfunction between the induced text-based represen-tations?Ws?
RNs?dwand the gold ones Ws.The induced fprojv?wis then applied to the image-based representations Vu?
RNu?dvof Nuun-seen objects to transform them into text-based rep-resentations?Wu?
RNu?dw.
We implement 4alternative learning algorithms for inducing thecross-modal projection function fprojv?w.Linear Regression (lin) Our first model is a verysimple linear mapping between the two modali-ties estimated by solving a least-squares problem.This method is similar to the one introduced byMikolov et al (2013a) for estimating a translationmatrix, only solved analytically.
In our setup, wecan see the two different modalities as if they weredifferent languages.
By using least-squares regres-sion, the projection function fprojv?wcan be de-rived asfprojv?w= (VTsVs)?1VTsWs(1)Canonical Correlation Analysis (CCA)CCA (Hardoon et al, 2004; Hotelling, 1936)and variations thereof have been successfully usedin the past for annotation of regions (Socher andFei-Fei, 2010) and complete images (Hardoon etal., 2006; Hodosh et al, 2013).
Given two pairedobservation matrices, in our case Vsand Ws,CCA aims at capturing the linear relationshipthat exists between these variables.
This isachieved by finding a pair of matrices, in our1407case CV?
Rdv?dand CW?
Rdw?d, such thatthe correlation between the projections of thetwo multidimensional variables into a common,lower-rank space is maximized.
The resultingmultimodal space has been shown to provide agood approximation to human concept similarityjudgments (Silberer and Lapata, 2012).
In oursetup, after applying CCA on the two spaces Vsand Ws, we obtain the two projection mappingsonto the common space and thus our projectionfunction can be derived as:fprojv?w= CVCW?1(2)Singular Value Decomposition (SVD) SVD isthe most widely used dimensionality reductiontechnique in distributional semantics (Turney andPantel, 2010), and it has recently been exploitedto combine visual and linguistic dimensions inthe multimodal distributional semantic model ofBruni et al (2014).
SVD smoothing is also a wayto infer values of unseen dimensions in partiallyincomplete matrices, a technique that has been ap-plied to the task of inferring word tags of unanno-tated images (Hare et al, 2008).
Assuming that theconcept-representing rows of Vsand Wsare or-dered in the same way, we apply the (k-truncated)SVD to the concatenated matrix [VsWs], suchthat [?Vs?Ws] = Uk?kZTkis a k-rank approxima-tion of the original matrix.6The projection func-tion is then:fprojv?w= ZkZTk(3)where the input is appropriately padded with 0s([Vu0Nu?W]) and we discard the visual block ofthe output matrix [?Vu?Wu].Neural Network (NNet) The last model that weintroduce is a neural network with one hiddenlayer.
The projection function in this model canbe described as:fprojv?w= ?v?w(4)where ?v?wconsists of the model weights ?
(1)?Rdv?hand ?(2)?
Rh?dwthat map the in-put image-based vectors Vsfirst to the hid-den layer and then to the output layer in or-der to obtain text-based vectors, i.e.,?Ws=?(2)(?(1)(Vs?(1))?
(2)), where ?
(1)and ?
(2)are6We denote the right singular vectors matrix by Z insteadof the customaryV to avoid confusion with the visual matrix.the non-linear activation functions.
We experi-mented with sigmoid, hyperbolic tangent and lin-ear; hyperbolic tangent yielded the highest perfor-mance.
The weights are estimated by minimizingthe objective functionJ(?v?w) =12(1?
sim(Ws,?Ws)) (5)where sim is some similarity function.
In our ex-periments we used cosine as similarity function,so that sim(A,B) =AB?A?
?B?, thus penalizing pa-rameter settings leading to a low cosine betweenthe target linguistic representations Wsand thoseproduced by the projection function?Ws.
The co-sine has been widely used in the distributional se-mantic literature, and it has been shown to out-perform Euclidean distance (Bullinaria and Levy,2007).7Parameters were estimated with standardbackpropagation and L-BFGS.5 ResultsOur experiments focus on the tasks of zero-shotlearning (Sections 5.1 and 5.2) and fast mapping(Section 5.3).
In both tasks, the projected vector ofthe unseen concept is labeled with the word asso-ciated to its cosine-based nearest neighbor vectorin the corresponding semantic space.For the zero-shot task we report the accuracyof retrieving the correct label among the top kneighbors from a semantic space populated withthe union of seen and unseen concepts.
For fastmapping, we report the mean rank of the correctconcept among fast mapping candidates.5.1 Zero-shot Learning in CIFAR-100For this experiment, we use the intersection ofour linguistic space with the concepts present inCIFAR-100, containing a total of 90 concepts.
Foreach concept category, we treat all concepts butone as seen concepts (Table 1).
The 71 seen con-cepts correspond to 42,600 distinct visual vectorsand are used to induce the projection function.
Ta-ble 2 reports results obtained by averaging the per-formance on the 11,400 distinct vectors of the 19unseen concepts.Our 4 models introduced in Section 4.4 arecompared to a theoretically derived baselineChance simulating selecting a label at random.
Forthe neural network NN, we use prior knowledge7We also experimented with the same objective func-tion as Socher et al (2013), however, our objective functionyielded consistently better results in all experimental settings.1408PPPPPPModelk1 2 3 5 10 20Chance 1.1 2.2 3.3 5.5 11.0 22.0SVD 1.9 5.0 8.1 14.5 29.0 48.6CCA 3.0 6.9 10.7 17.9 31.7 51.7lin 2.4 6.4 10.5 18.7 33.0 55.0NN 3.9 6.6 10.6 21.9 37.9 58.2Table 2: Percentage accuracy among top k nearestneighbors on CIFAR-100.about the number of concept categories to set thenumber of hidden units to 20 in order to avoidtuning of this parameter.
For the SVD model, weset the number of dimensions to 300, a commonchoice in distributional semantics, coherent withthe settings we used for the visual and linguisticspaces.First and foremost, all 4 models outperformChance by a large margin.
Surprisingly, the verysimple lin method outperforms both CCA and SVD.However, NN, an architecture that can capturemore complex, non-linear relations in featuresacross modalities, emerges as the best performingmodel, confirming on a larger scale the recent find-ings of Socher et al (2013).5.1.1 Concept CategorizationIn order to gain qualitative insights into the perfor-mance of the projection process of NN, we attemptto investigate the role and interpretability of thehidden layer.
We achieve this by looking at whichvisual concepts result in the highest hidden unitactivation.8This is inspired by analogous quali-tative analysis conducted in Topic Models (Grif-fiths et al, 2007), where ?topics?
are interpretedin terms of the words with the highest probabilityunder each of them.Table 3 presents both seen and unseen con-cepts corresponding to visual vectors that triggerthe highest activation for a subset of hidden units.The table further reports, for each hidden unit, the?correct?
unseen concept for the category of thetop seen concepts, together with its rank in termsof activation of the unit.
The analysis demon-strates that, although prior knowledge about cat-egories was not explicitly used to train the net-work, the latter induced an organization of con-cepts into superordinate categories in which the8For this post-hoc analysis, we include a sparsity param-eter in the objective function of Equation 5 in order to getmore interpretable results; hidden units are therefore maxi-mally activated by a only few concepts.Unseen Concept Nearest Neighborstiger cat, microchip, kitten, vet, petbike spoke, wheel, brake, tyre, motorcycleblossom bud, leaf, jasmine, petal, dandelionbakery quiche, bread, pie, bagel, curryTable 4: Top 5 neighbors in linguistic space aftervisual vector projection of 4 unseen concepts.hidden layer acts as a cross-modal concept cate-gorization/organization system.
When the inducedprojection function maps an object onto the lin-guistic space, the derived text vector will inherita mixture of textual features from the conceptsthat activated the same hidden unit as the object.This suggests a bias towards seen concepts.
Fur-thermore, in many cases of miscategorization, theconcepts are still semantically coherent with theinduced category, confirming that the projectionfunction is indeed capturing a latent, cross-modalsemantic space.
A squirrel, although not a ?largeomnivore?, is still an animal, while butterflies arenot flowers but often feed on their nectar.5.2 Zero-shot Learning in ESPFor this experiment, we focus on NN, the best per-forming model in the previous experiment.
Weuse a set of approximately 9,500 concepts, the in-tersection of the ESP-based visual semantic spacewith the linguistic space.
For tuning the numberof hidden units of NN, we use the MEN-concretedataset of Bruni et al (2014).
Finally, we ran-domly pick 70% of the concepts to induce the pro-jection function fprojv?wand report results on theremaining 30%.
Note that the search space for thecorrect label in this experiment is approximately95 times larger than the one used for the experi-ment presented in Section 5.1.Although our experimental setup differs fromthe one of Frome et al (2013), thus preventing adirect comparison, the results reported in Table 5are on a comparable scale to theirs.
We note thatprevious work on zero-shot learning has used stan-dard object recognition benchmarks.
To the bestof our knowledge, this is the first time this task hasbeen performed on a dataset as noisy as ESP.
Over-all, the results suggest that cross-modal mappingcould be applied in tasks where images exhibit amore complex structure, e.g., caption generationand event recognition.1409Seen Concepts Unseen Concept Rank of Correct CIFAR-100 CategoryUnseen ConceptUnit 1 sunflower, tulip, pear butterfly 2 (rose) flowersUnit 2 cattle, camel, bear squirrel 2 (elephant) large omnivores and herbivoresUnit 3 castle, bridge, house bus 4 (skyscraper) large man-made outdoor thingsUnit 4 man, girl, baby boy 1 peopleUnit 5 motorcycle, bicycle, tractor streetcar 2 (bus) vehicles 1Unit 6 sea, plain, cloud forest 1 large natural outdoor scenesUnit 7 chair, couch, table bed 1 household furnitureUnit 8 plate, bowl, can clock 3 (cup) food containersUnit 9 apple, pear, mushroom orange 1 fruit and vegetablesTable 3: Categorization induced by the hidden layer of the NN; concepts belonging in the same CIFAR-100 categories, reported in the last column, are marked in bold.
Example: Unit 1 receives the highestactivation during training by the category flowers and at test time by butterfly, belonging to insects.
Thesame unit receives the second highest activation by the ?correct?
test concept, the flower rose.PPPPPPModelk1 2 5 10 50Chance 0.01 0.02 0.05 0.10 0.5NN 0.8 1.9 5.6 9.7 30.9Table 5: Percentage accuracy among top k nearestneighbors on ESP.5.3 Fast Mapping in ESPIn this section, we aim at simulating a fast map-ping scenario in which the learner has been justexposed to a new concept, and thus has limited lin-guistic evidence for that concept.
We operational-ize this by considering the 34 concrete conceptsintroduced by Frassinelli and Keller (2012), andderiving their text-based representations from justa few sentences randomly picked from the corpus.Concretely, we implement 5 models: context 1, con-text 5, context 10, context 20 and context full, wherethe name of the model denotes the number of sen-tences used to construct the text-based representa-tions.
The derived vectors were reduced with thesame SVD projection induced from the completecorpus.
Cross-modal mapping is done via NN.The zero-shot framework leads us to frame fastmapping as the task of projecting visual represen-tations of new objects onto language space for re-trieving their word labels (v?
w).
This mappingfrom visual to textual representations is arguablya more plausible task than vice versa.
If we thinkabout how linguistic reference is acquired, a sce-nario in which a learner first encounters a new ob-ject and then seeks its reference in the language ofthe surrounding environment (e.g., adults having aconversation, the text of a book with an illustrationof an unknown object) is very natural.
Further-more, since not all new concepts in the linguisticenvironment refer to new objects (they might de-note abstract concepts or out-of-scene objects), itseems more reasonable for the learner to be morealerted to linguistic cues about a recently-spottednew object than vice versa.
Moreover, once thelearner observes a new object, she can easily con-struct a full visual representation for it (and theacquisition literature has shown that humans arewired for good object segmentation and recogni-tion (Spelke, 1994)) ?
the more challenging task isto scan the ongoing and very ambiguous linguisticcommunication for contexts that might be relevantand informative about the new object.
However,fast mapping is often described in the psycholog-ical literature as the opposite task: The learneris exposed to a new word in context and has tosearch for the right object referring to it.
We im-plement this second setup (w?
v) by training theprojection function fprojw?vwhich maps linguis-tic vectors to visual ones.
The adaptation of NN isstraightforward; the new objective function is de-rived asJ(?w?v) =12(1?
sim(Vs,?Vs)) (6)where?Vs= ?(2)(?(1)(Ws?(1))?
(2)), ?
(1)?Rdw?hand ?(2)?
Rh?dv.Table 7 presents the results.
Not surprisingly,performance increases with the number of sen-tences that are used to construct the textual repre-sentations.
Furthermore, all models perform bet-ter than Chance, including those that are based onjust 1 or 5 sentences.
This suggests that the systemcan make reasonable inferences about object-wordconnections even when linguistic evidence is veryscarce.Regarding the sources of error, a qualitativeanalysis of predicted word labels and objects as1410v?w w?vcooker?potato dishwasher?
corkscrewclarinet?
drum potato?
corngorilla?
elephant guitar?
violinscooter?
car scarf?
trouserTable 6: Top-ranked concepts in cases where thegold concepts received numerically high ranks.XXXXXXXXContextMappingv?
w w?
vChance 17 17context 1 12.6 14.5context 5 8.08 13.29context 10 7.29 13.44context 20 6.02 12.17context full 5.52 5.88Table 7: Mean rank results averaged across 34concepts when mapping an image-based vectorand retrieving its linguistic neighbors (v?
w) aswell as when mapping a text-based vector andretrieving its visual neighbors (w?
v).
Lowernumbers cue better performance.presented in Table 6 suggests that both textualand visual representations, although capturing rel-evant ?topical?
or ?domain?
information, are notenough to single out the properties of the targetconcept.
As an example, the textual vector of dish-washer contains kitchen-related dimensions suchas ?fridge, oven, gas, hob, ..., sink?.
After projectingonto the visual space, its nearest visual neighboursare the visual ones of the same-domain conceptscorkscrew and kettle.
The latter is shown in Figure3a, with a gas hob well in evidence.
As a furtherexample, the visual vector for cooker is extractedfrom pictures such as the one in Figure 3b.
Notsurprisingly, when projecting it onto the linguis-tic space, the nearest neighbours are other kitchen-related terms, i.e., potato and dishwasher.6 ConclusionAt the outset of this work, we considered theproblem of linking purely language-based distri-(a) A kettle(b) A cookerFigure 3: Two images from ESP.butional semantic spaces with objects in the vi-sual world by means of cross-modal mapping.
Wecompared recent models for this task both on abenchmark object recognition dataset and on amore realistic and noisier dataset covering a widerange of concepts.
The neural network architec-ture emerged as the best performing approach, andour qualitative analysis revealed that it induced acategorical organization of concepts.
Most impor-tantly, our results suggest the viability of cross-modal mapping for grounded word-meaning ac-quisition in a simulation of fast mapping.Given the success of NN, we plan to experi-ment in the future with more sophisticated neuralnetwork architectures inspired by recent work inmachine translation (Gao et al, 2013) and mul-timodal deep learning (Srivastava and Salakhut-dinov, 2012).
Furthermore, we intend to adoptvisual attributes (Farhadi et al, 2009; Silbereret al, 2013) as visual representations, since theyshould allow a better understanding of how cross-modal mapping works, thanks to their linguisticinterpretability.
The error analysis in Section 5.3suggests that automated localization techniques(van de Sande et al, 2011), distinguishing an ob-ject from its surroundings, might drastically im-prove mapping accuracy.
Similarly, in the textualdomain, models that extract collocates of a wordthat are more likely to denote conceptual proper-ties (Kelly et al, 2012) might lead to more infor-mative and discriminative linguistic vectors.
Fi-nally, the lack of large child-directed speech cor-pora constrained the experimental design of fastmapping simulations; we plan to run more realis-tic experiments with true nonce words and usingsource corpora (e.g., the Simple Wikipedia, childstories, portions of CHILDES) that contain sen-tences more akin to those a child might effectivelyhear or read in her word-learning years.AcknowledgmentsWe thank Adam Li?ska for helpful discussions andthe 3 anonymous reviewers for useful comments.This work was supported by ERC 2011 StartingIndependent Research Grant n. 283554 (COM-POSES).ReferencesBarbara Abbott.
2010.
Reference.
Oxford UniversityPress, Oxford, UK.1411Paul Bloom.
2000.
How Children Learn the Meaningsof Words.
MIT Press, Cambridge, MA.S.
R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,and Regina Barzilay.
2009.
Reinforcement learningfor mapping instructions to actions.
In Proceedingsof ACL/IJCNLP, pages 82?90.Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-jlings, and Irina Sergienya.
2013.
Vsem: An openlibrary for visual semantics representation.
In Pro-ceedings of ACL, Sofia, Bulgaria.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.John Bullinaria and Joseph Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39:510?526.Susan Carey and Elsa Bartlett.
1978.
Acquiring a sin-gle new word.
Papers and Reports on Child Lan-guage Development, 15:17?29.Susan Carey.
1978.
The child as a word learner.
InM.
Halle, J. Bresnan, and G. Miller, editors, Linguis-tics Theory and Psychological Reality.
MIT Press,Cambridge, MA.Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, andAndrew Zisserman.
2011.
The devil is in the de-tails: an evaluation of recent feature encoding meth-ods.
In Proceedings of BMVC, Dundee, UK.David Chen and Raymond Mooney.
2011.
Learningto interpret natural language navigation instructionsfrom observations.
In Proceedings of AAAI, pages859?865, San Francisco, CA.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167, Helsinki, Fin-land.Gabriella Csurka, Christopher Dance, Lixin Fan, JuttaWillamowski, and C?edric Bray.
2004.
Visual cate-gorization with bags of keypoints.
In In Workshopon Statistical Learning in Computer Vision, ECCV,pages 1?22, Prague, Czech Republic.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Ph.D dissertation, Stuttgart University.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their at-tributes.
In Proceedings of CVPR, pages 1778?1785, Miami Beach, FL.Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-son.
2010.
A probabilistic computational model ofcross-situational word learning.
Cognitive Science,34:1017?1063.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedingsof HLT-NAACL, pages 91?99, Los Angeles, CA.Diego Frassinelli and Frank Keller.
2012.
The plausi-bility of semantic properties generated by a distribu-tional model: Evidence from a visual world experi-ment.
In Proceedings of CogSci, pages 1560?1565.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semanticembedding model.
In Proceedings of NIPS, pages2121?2129, Lake Tahoe, Nevada.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2013.
Learning semantic representationsfor the phrase translation model.
arXiv preprintarXiv:1312.0482.Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.2007.
Topics in semantic representation.
Psycho-logical Review, 114:211?244.David R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis:An overview with application to learning methods.Neural Computation, 16(12):2639?2664.David R Hardoon, Craig Saunders, Sandor Szedmak,and John Shawe-Taylor.
2006.
A correlation ap-proach for automatic image annotation.
In Ad-vanced Data Mining and Applications, pages 681?692.
Springer.Jonathon Hare, Sina Samangooei, Paul Lewis, andMark Nixon.
2008.
Semantic spaces revisited: In-vestigating the performance of auto-annotation andsemantic retrieval using semantic spaces.
In Pro-ceedings of CIVR, pages 359?368, Niagara Falls,Canada.Stevan Harnad.
1990.
The symbol grounding problem.Physica D: Nonlinear Phenomena, 42(1-3):335?346.Tracy Heibeck and Ellen Markman.
1987.
Word learn-ing in children: an examination of fast mapping.Child Development, 58:1021?1024.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
Journal of Ar-tificial Intelligence Research, 47:853?899.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Colin Kelly, Barry Devereux, and Anna Korhonen.2012.
Semi-supervised learning for automatic con-ceptual property extraction.
In Proceedings of the3rd Workshop on Cognitive Modeling and Computa-tional Linguistics, pages 11?20, Montreal, Canada.1412Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Proceedings of NIPS,pages 1106?1114.Alex Krizhevsky.
2009.
Learning multiple layers offeatures from tiny images.
Master?s thesis.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching for recognizing natural scene categories.In Proceedings of CVPR, pages 2169?2178, Wash-ington, DC.Alessandro Lenci.
2008.
Distributional approaches inlinguistic and cognitive research.
Italian Journal ofLinguistics, 20(1):1?31.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407.David Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2).Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013a.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119, LakeTahoe, Nevada.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,and Tom Mitchell.
2009.
Zero-shot learning withsemantic output codes.
In Proceedings of NIPS,pages 1410?1418, Vancouver, Canada.Alfredo F Pereira, Karin H James, Susan S Jones,and Linda B Smith.
2010.
Early biases and de-velopmental changes in self-generated object views.Journal of vision, 10(11).Michaela Regneri, Marcus Rohrbach, Dominikus Wet-zel, Stefan Thater, Bernt Schiele, and ManfredPinkal.
2013.
Grounding action descriptions invideos.
Transactions of the Association for Com-putational Linguistics, 1:25?36.John Searle.
1984.
Minds, Brains and Science.
Har-vard University Press, Cambridge, MA.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof EMNLP, pages 1423?1433, Jeju, Korea.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of semantic representation with visualattributes.
In Proceedings of ACL, pages 572?582,Sofia, Bulgaria.Jeffrey Siskind.
1996.
A computational study of cross-situational techniques for learning word-to-meaningmappings.
Cognition, 61:39?91.Josef Sivic and Andrew Zisserman.
2003.
VideoGoogle: A text retrieval approach to object match-ing in videos.
In Proceedings of ICCV, pages 1470?1477, Nice, France.Richard Socher and Li Fei-Fei.
2010.
Connectingmodalities: Semi-supervised segmentation and an-notation of images using unaligned text corpora.
InProceedings of CVPR, pages 966?973.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, Nevada.Elizabeth Spelke.
1994.
Initial knowledge: Six sug-gestions.
Cognition, 50:431?445.Nitish Srivastava and Ruslan Salakhutdinov.
2012.Multimodal learning with deep boltzmann ma-chines.
In Proceedings of NIPS, pages 2231?2239.Richard Szeliski.
2010.
Computer Vision : Algorithmsand Applications.
Springer, Berlin.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Koen van de Sande, Jasper Uijlings, Theo Gevers, andArnold Smeulders.
2011.
Segmentation as selec-tive search for object recognition.
In Proceedings ofICCV, pages 1879?1886, Barcelona, Spain.Luis Von Ahn.
2006.
Games with a purpose.
Com-puter, 29(6):92?94.Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, andChong-Wah Ngo.
2007.
Evaluating bag-of-visual-words representations in scene classification.
InJames Ze Wang, Nozha Boujemaa, Alberto DelBimbo, and Jia Li, editors, Multimedia InformationRetrieval, pages 197?206.
ACM.1413Haonan Yu and Jeffrey Siskind.
2013.
Grounded lan-guage learning from video described with sentences.In Proceedings of ACL, pages 53?63, Sofia, Bul-garia.1414
