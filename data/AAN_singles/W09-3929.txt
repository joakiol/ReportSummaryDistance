Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 196?205,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsClarification Potential of InstructionsLuciana BenottiTALARIS Team - LORIA (Universite?
Henri Poincare?, INRIA)BP 239, 54506 Vandoeuvre-le`s-Nancy, FranceLuciana.Benotti@loria.frAbstractOur hypothesis is that conversational im-plicatures are a rich source of clarificationquestions.
In this paper we do two things.First, we motivate the hypothesis in theo-retical, practical and empirical terms.
Sec-ond, we present a framework for generat-ing the clarification potential of an instruc-tion by inferring its conversational impli-catures with respect to a particular con-text.
General means-ends inference, be-yond classical planning, turns out to becrucial.1 IntroductionPractical interest in clarification requests (CRs)no longer needs to be awakened in dialoguesystem designers (Gabsdil, 2003; Purver, 2004;Rodr?
?guez and Schlangen, 2004; Rieser andMoore, 2005; Skantze, 2007).
In sociolinguisticsand discourse analysis, repair has been an evenmore favored theme for almost three decades now;see (Schegloff, 1987) as a representative example.However, the theoretical scope of the phenomenaand its implications for a theory of meaning arestill being delineated.
Recently, it has been pro-posed that clarification should be a basic compo-nent in an adequate theory of meaning:The basic criterion for adequacy of a theory ofmeaning is the ability to characterize for any ut-terance type the update that emerges in the after-math of successful mutual understanding and thefull range of possible clarification requests other-wise ?
this is the early 21st century analogue oftruth conditions.
(Ginzburg, 2009, p.4)In this view, repairs are not a necessary evil butan intrinsic mechanism of language.
In fact, inter-preting an utterance centrally involves characteriz-ing the space of possible requests of clarificationof the utterance, that is its clarification potential.We believe that Ginzburg?s comment points in theright direction; we discuss the motivations froma theoretical perspective in Section 2.1.
In Sec-tion 2.2 we review a state-of-the-art definition ofthe notion of clarification from the perspective ofdialogue system designers.
This review makes ev-ident the necessity of further refining the notionof clarification if it is going to play such a cen-tral role in a theory of meaning.
In Section 2.3 wepresent our findings in the corpus SCARE (Stoia etal., 2008) which empirically motivates our work.We believe that it is crucial to redefine the no-tion of clarification in functional terms.
Becausewe know that the task is difficult, we restrict our-selves to one utterance type, instructions, and toa particular interaction level, the task-level.
In therest of the paper (Sections 3 and 4), we presenta framework that generates the task-level clarifi-cation potential of an instruction by inferring itsparticularized conversational implicatures.The following exchange illustrate the kinds ofinteractions our framework models:(1) A(1): Turn it on.B(2): By pushing the red button?(Rodr?
?guez and Schlangen, 2004, p.102)Roughly speaking, our framework takes as in-put sentences like A(1) and explains how B(2)can be generated.
In particular, the framework in-dicates what kinds of information resources andwhat kind of inferences are involved in the processof generating utterances like B(2).
In other words,the goal of the framework is to explain why A(1)and B(2) constitute a coherent dialogue by sayinghow B(2) is relevant to A(1).1962 Background and motivationIn this section, we motivate our framework fromthe theoretical perspective of pragmaticists inter-ested in the relevance of clarifications for a theoryof meaning, from the practical perspective of di-alogue system designers, and from the empiricalperspective of a human-human corpus that pro-vides evidence for the necessity of such a frame-work.2.1 Theoretical: Relevance of clarificationsModeling how listeners draw inferences fromwhat they hear, is a basic problem for theoriesof understanding natural language.
An importantpart of the information conveyed is inferred in con-text, given the nature of conversation as a goal-oriented enterprise; as illustrated by the followingclassical example by Grice:(2) A: I am out of petrol.B: There is a garage around the corner.
; B thinks that the garage is open.
(Grice, 1975, p.311)B?s answer conversationally implicates (;) in-formation that is relevant to A.
In Grice?s terms, Bmade a relevance implicature: he would be flout-ing the conversational maxim of relevance unlesshe believes that it?s possible that the garage isopen.
A conversational implicature (CI) is dif-ferent from an entailment in that it is cancelablewithout contradiction.
B can append material thatis inconsistent with the CI ?
?but I don?t knowwhether it?s open?.
Since the CI can be canceled,B knows that it does not necessarily hold and thenboth B or A are able to reinforce or clarify it with-out repetition.It is often controversial whether something isactually a CI or not (people have different intu-itions, which is not surprising given that peoplehave different background assumptions).
In dia-logue, CRs provide good evidence of the impli-catures that have been made simply because theymake implicatures explicit.
Take for instance theclarification request which can naturally followGrice?s example.
(3) A: and you think it?s open?B will have to answer and support the impli-cature (for instance with ?yes, it?s open till mid-night?)
if he wants to get it added to the commonground; otherwise, if he didn?t mean it, he can wellreject it without contradiction with ?well, you havea point there, they might have closed?.Our hypothesis is that CIs are a rich source ofclarification requests.
And our method for gener-ating the potential CRs of an utterance will be thento infer (some of) the CIs of that utterance with re-spect to a particular context.2.2 Practical: Kinds of clarificationsGiving a precise definition of a clarification re-quest is more difficult than might be thought atfirst sight.
Rodr?
?guez and Schlangen (2004) rec-ognize this problem by saying:Where we cannot report reliability yet is for thetask of identifying CRs in the first place.
This isnot a trivial problem, which we will address in fu-ture work.
As far as we can see, Purver, Ginzburgand Healey have not tested for reliability for doingthis task either.
(Rodr?
?guez and Schlangen, 2004,p.107)One of the most developed classifications ofCRs is the one presented in (Purver, 2004).
How-ever, Purver?s classification relies mainly on thesurface form of the CRs.
The attempts found in theliterature to give a classification of CRs accord-ing to their functions (Rodr?
?guez and Schlangen,2004; Rieser and Moore, 2005) are based on thefour-level model of communication independentlydeveloped by Clark (1996) and Allwood (1995).The model is summarized in Figure 1 (from thepoint of view of the hearer).Level Clark Allwood4 consideration reaction3 understanding understanding2 identification perception1 attention contactFigure 1: The four levels of communicationMost of the previous work on clarifications hasconcentrated on levels 1 to 3 of communication.For instance, Schlangen (2004) proposed a fined-grained classification of CRs but only for level3.
Gabsdil (2003) proposes a test for identifyingCRs.
The test says that CRs cannot be precededby explicit acknowledgements.
But in the follow-ing example, presented by Gabsdil himself, the CRuttered by F can well start with an explicit ?ok?.197(4) G: I want you to go up the left hand side of ittowards the green bay and make it a slightlydiagonal line, towards, sloping to the right.F: So you want me to go above the carpen-ter?
(Gabsdil, 2003, p.30)The kind of CR showed in 4, also called clarifi-cation of intentions or task level clarifications,are in fact very frequent in dialogue; they havebeen reported to be the second or third most com-mon kind of CR (the most common being ref-erence resolution).
(Rodr?
?guez and Schlangen,2004) reports that 22% of the CRs found by themin a German task-oriented spoken dialogue be-longed to level 4, while (Rieser and Moore, 2005)reports 8% (a high percentage considering that thechannel quality was poor and caused a 31% ofacoustic problems).Fourth level CRs are not only frequent butthere are studies that show that the hearer in factprefers them.
That is, if the dialogue shows ahigher amount of task related clarifications (in-stead of, conventional CRs such as ?what??)
hear-ers qualitative evaluate the task as more success-ful (Skantze, 2007).
(Gabsdil, 2003) and (Rieserand Moore, 2005) also agree that for task-orienteddialogues the hearer should present a task-level re-formulation to be confirmed rather than asking forrepetition, thereby showing his subjective under-standing to the other dialogue participants.
Gabs-dil briefly suggests a step in this direction:Task-level reformulations might benefit from sys-tems that have access to effects of action opera-tors or other ways to compute task-level implica-tions.
(Gabsdil, 2003, p.29 and p.34)In the rest of the paper we propose a frameworkthat formalizes how to compute task-level impli-catures and that suggests a finer-grained classifi-cation for CRs in level 4.
But first, in Section 2.3we present empirical findings that motivate such aframework.2.3 Empirical: The SCARE corpusThe SCARE corpus (Stoia et al, 2008) consistsof fifteen English spontaneous dialogues situatedin an instruction giving task1.
It was collectedusing the Quake environment, a first-person vir-tual reality game.
The task consists of a directiongiver (DG) instructing a direction follower (DF)1The corpus is freely available for research inhttp://slate.cse.ohio-state.edu/quake-corpora/scare/on how to complete several tasks in a simulatedgame world.
The corpus contains the collected au-dio and video, as well as word-aligned transcrip-tions.The DF had no prior knowledge of the worldmap or tasks and relied on his partner, the DG, toguide him on completing the tasks.
The DG hada map of the world and a list of tasks to complete(detailed in Appendix A.3).
The partners spoketo each other through headset microphones; theycould not see each other.
As the participants col-laborated on the tasks, the DG had instant feed-back of the DF?s location in the simulated world,because the game engine displayed the DF?s firstperson view of the world on both the DG?s andDF?s computer monitors.We analyzed the 15 transcripts that constitutethe SCARE corpus while watching the associatedvideos to get familiarized with the experiment andevaluate its suitability for our purposes.
Then, werandomly selected one dialogue; its transcript con-tains 449 turns and its video lasts 9 minutes and 12seconds.
Finally, we classified the clarification re-quests according to the levels of communication(see Figure 1).
We found 29 clarification requests;so 6.5% of the turns are CRs.
From these 29 CRs,65% belong to the level 4 of Table 1, and 31% be-longed to level 3 (most of them related to referenceresolution).
Only 4% of the CRs were acoustic(level 2) since the channel used was very reliable.In fact we only found one CR of the form?what??
and it was a signal of incredulity of theeffect of an action as can be seen below:DG(1): and then cabinet should openDF(2): did itDF(3): nothing in itDG(4): what?DG(5): There should be a silencer thereInterestingly, the ?what??
form of CR was re-ported as the most frequently found in ?ordinary?dialogue in (Purver et al, 2003).
This is not thecase in the SCARE corpus.
Furthermore, ?what?
?is usually assumed to be a CR that indicates a lowlevel of coordination and is frequently classified asbelonging to level 1 or 2.
However, this is not thecase in our example in which the CR is evidentlyrelated to the task structure and thus belongs tolevel 4.
This is an example of why surface form isnot reliable when classifying CRs.1982.4 Preliminary conclusionsIn this preliminary study, the SCARE corpusseems to present more CRs than the corpus ana-lyzed by previous work (which reports that 4% ofthe dialogue turns are CR).
Furthermore, in dis-tinction to results reported in Ginzburg (2009),most CRs occur at level 4.
We believe this is nat-urally explained in politeness theory (Brown andLevinson, 1987).The participants were punished if they per-formed steps of the task that they were not sup-posed to (see the instructions in Appendix A.1).This punishment might take precedence over thedispreference for CRs that is universal in dialoguedue to politeness.
CRs are perceived as a form ofdisagreement which is universally dispreferred ac-cording to politeness theory.
The pairs of partici-pants selected were friends so the level of intimacyamong them was high, lowering the need of polite-ness strategies; a behavior that is also predictedby politeness theory.
Finally, the participants re-ceived a set of instructions before the task started(see Appendix A) that includes information on theavailable actions in the simulated world and theirexpected effects.
The participants make heavy useof this to produce high level clarification requests,instead of just signaling misunderstanding.From these observations we draw the prelim-inary conclusion that clarification strategies de-pend on the information that is available to thedialogue participants (crucially including the in-formation available before the dialogue starts) andon the constraints imposed on the interaction, suchas politeness constraints.
In Section 3 we describethe four information resources of our frameworkwhose content depends on the information avail-able to the dialogue participants.
In Section 4 weintroduce the reasoning tasks that use the informa-tion resources to infer the clarification potential ofinstructions.
The study of the interaction betweenpoliteness constraints and clarification strategiesseems promising, and we plan to address it in fu-ture work.3 The information resourcesThe inference framework uses four information re-sources whose content depends on the informationavailable to the dialogue participants.
We describeeach of them in turn and we illustrate their contentusing the SCARE experimental setup.3.1 The world modelSince the kind of utterance that the frameworkhandles are instructions that are supposed to beexecuted in a simulated world, the first requiredinformation resource is a model of this world.
Theworld model is a knowledge base that representsthe physical state of the simulated world.
Thisknowledge base has complete and accurate infor-mation about the world that is relevant for com-pleting the task at hand.
It specifies properties ofparticular individuals (for example, an individualcan be a button or a cabinet).
Relationships be-tween individuals are also represented here (suchas the relationship between an object and its loca-tion).
Such a knowledge base can be thought as afirst-order model.The content of the world model for the SCAREsetup is a representation of the factual informationprovided to the DG before the experiment started,namely, a relational model of the map he received(see Figure 3 in Appendix A.3).
Crucially, sucha model contains all the functions associated withthe buttons in the world and the contents of thecabinets (which are indicated on the map).3.2 The dialogue modelUsually, this knowledge base starts empty; it is as-sumed to represent what the DF knows about theworld.
The information learned, either throughthe contributions made during the dialogue or bynavigating the simulated world, are incrementallyadded to this knowledge base.
The knowledge isalso represented as a relational model and in factthis knowledge base will usually (but not neces-sarily) be a submodel of the world model.The DF initial instructions in the SCARE setupinclude almost no factual information (as youcan verify looking at his instructions in Ap-pendix A.2).
The only factual information thathe received were pictures of some objects in theworld so that he is able to recognize them.
Suchinformation is relevant mainly for referent resolu-tion and this is not the focus of the current paper.Therefore, for our purposes we can assume that thedialogue model of the SCARE experiment startsempty.3.3 The world actionsCrucially, the framework also includes the defi-nitions of the actions that can be executed in theworld (such as the actions take or open).
Each ac-199tion is specified as a STRIPS-like operator (Fikeset al, 1972) detailing its arguments, preconditionsand effects.
The preconditions indicate the condi-tions that the world scenario must satisfy so thatthe action can be executed; the effects determinehow the action changes the world when it is exe-cuted.
These actions specify complete and accu-rate information about how the world behaves andtogether with the world model is assumed to rep-resent what the DG knows about the world.The SCARE world action database will containa representation of the specification of the quakecontrols (see Appendix A.1) received by both par-ticipants and the extra action information that theDG received.
First, he received a specification ofthe action hide that was not received by the DF.Second, if the DG read the instructions carefully,he knows that pressing a button can also causethings to move.
The representation of this last ac-tion schema is shown in Appendix A.3.1.3.4 The potential actionsThe potential actions include representation of ac-tions that the DF learned from the instructions hereceived before beginning the task.
This includesthe quake controls (see Appendix A.1) and alsothe action knowledge that he acquired during hislearning phase (see appendix A.2).
In the learningphase the direction follower learned that the effectof pressing a button can open a cabinet (if it wasclosed) or close it (if it was opened).
Such knowl-edge is represented as a STRIPS-like operator likeone showed in Appendix A.2.1.3.5 Preliminary conclusionsAn action language like PDDL (Gerevini andLong, 2005) can be used to specify the two actiondatabases introduced above (in fact, the STRIPSfragment is enough).
PDDL is the official lan-guage of the International Conference on Auto-mated Planning and Scheduling since 1998.
Thismeans that most off-the-shelf planners that areavailable nowadays support this language, such asFF (Hoffmann and Nebel, 2001) and SGPlan (Hsuet al, 2006).As we said in the previous section, the worldmodel and the dialogue model are just relationalstructures like the one showed in Figure 3 (in theappendix).
These relational structures can be di-rectly expressed as a set of literals which is theformat used to specify the initial state of a plan-ning problem.The information resources then constitute al-most everything that is needed in order to specify acomplete planning problem, as expected by cur-rent planners, the only element that the frameworkis missing is the goal.
With a set of action schemas(i.e.
action operators), an initial state and a goal asinput, a planner is able to return a sequence of ac-tions (i.e.
a plan) that, when executed in the initialstate, achieves the goal.Planning is a means-end inference task, akind of practical inference as defined by Kennyin (Kenny, 1966); and is a very popular inferencetask indeed as evidenced by the amount of workdone in the area in the last two decades.
However,planning is not the only interesting means-end in-ference task.
One of the goals of the next sectionis to show exactly this: there is more to practicalinference than planning.4 The inference tasksIn this section we do two things.
First, we say howcurrent off-the-shelf planners can be used to inferpart of the clarification potential of instructions.In particular we define what the missing element,the goal, is and we illustrate this with fragments ofhuman-human dialogue of the SCARE corpus.
In-cidentally, we also show that clarification potentialcan not only be used for generating and interpret-ing CRs but also for performing acceptance andrejection acts.
Second, we motivate and start todefine one means-ends inference task that is notcurrently implemented, but that is crucial for in-ferring the clarification potential of instructions.In order to better understand the examples be-low you may want to read the Appendix A first.The information in the Appendix was available tothe participants when they performed the experi-ments and it?s heavily used in the inferences theydraw.4.1 Planning: A means-end inference taskShared-plan recognition ?and not artificial intel-ligence planning?
has been used for utterance in-terpretation (Lochbaum, 1998; Carberry and Lam-bert, 1999; Blaylock and Allen, 2005).
In suchplan recognition approaches each utterance addsa constraint to the plan that is partially filled out,and the goal of the conversation has to be inferredduring the dialogue; that is, a whole dialogue ismapped to one shared plan.
In our approach, eachinstruction is interpreted as a plan instead; that is,200we use planning at the utterance level and not atdialogue level.Artificial intelligence planning has been used atutterance level (called micro-planning) for gener-ation (Koller and Stone, 2007).
We use artificialintelligence planning for interpretation of instruc-tions instead.In our framework, the goal of the planningproblem are the preconditions of instruction forwhich the clarification potential is being calcu-lated.
Now, the planning problem has a goal,but there are two action databases and two initialstates.
Which one will be used for finding the clar-ification potential?
In fact, all four.When the DG gives an instruction, the DF hasto interpret it in order to know what actions he hasto perform (step 1 of the inference).
The interpre-tation consists in trying to construct a plan that,when executed in the current state of the gameworld, achieves the goals of the instruction.
Thespecification of such planning problem is as fol-lows.
The preconditions of the instruction are thegoal of the planning problem, the dialogue modelis the initial state and the potential actions are theaction operators.
With this information the off-the-shelf planner will find a plan, a sequence ofactions that are the implicatures of the instruction.Then (step 2 of the inference), an attempt to ex-ecute the plan on the the world model and usingthe world actions occurs.
Whenever the plan fails,there is a potential clarification.Using clarification potential to clarify: In thedialogue below, the participants are trying to movea picture from a wall to another wall (task 1 in Ap-pendix A.3).
The instruction that is being inter-preted is the one uttered by the DG in (1).
Usingthe information in the potential action database,the DF infers a plan that involves two implicatures,namely picking up the picture (in order to achievethe precondition of holding the picture), and goingto the wall (inference step 1).
However, this planwill fail when executed on the world model be-cause the picture is not takeable and thus it cannotbe picked, resulting in a potential clarification (in-ference step 2).
This potential clarification, fore-shadowed by (3), is finally made explicit by theCR in (4).DG(1): well, put it on the opposite wallDF(2): ok, control picks the .DF(3): control?s supposed to pick things up and .DF(4): am I supposed to pick this thing?A graphical representation of both steps of in-ference involved in this example is shown in Sec-tion B of the Appendix2.But also to produce evidence of rejection: Inthe dialogue below, the DG utters the instruction(1) knowing that the DF will not be able to followit; the DG is just thinking aloud.
If taken seriously,this instruction would involve the action resolvethe reference ?cabinet nine?.
A precondition ofthis action is that the DF knows the numbers of thecabinets, but both participants know this is not thecase, only the DG can see the map.
That?s why therejection in (2) is received with laughs and the DGcontinues his loud thinking in (3) while looking atthe map.DG(1): we have to put it in cabinet nine .DF(2): yeah, they?re not numbered [laughs]DG(3): [laughs] where is cabinet nine .And to produce evidence of acceptance: Thefollowing dialogue fragment continues the frag-ment above.
Now, the DG finally says where cab-inet nine is in (4).
And the DF comes up with theplan that he incrementally grounds making it ex-plicit in (5), (7), and (9) while he is executing it;the plan achieves the precondition of the instruc-tion put of being near the destination of the action,in this case ?near cabinet nine?.
Uttering the stepsof the plan that were not made explicit by the in-struction is indeed a frequently used method forperforming acceptance acts.DG(4): it?s .
kinda like back where you started .soDF(5): ok .
so I have to go back through here .DG(6): yeahDF(7): and around the corner .DG(8): rightDF(9): and then do I have to go back up the stepsDG(10): yeahDF(11): alright, this is where we startedDG(12): ok .
so your left ca- .
the left oneDF(13): alright, so how do I open it?In (13) the DF is not able to find a plan thatachieves another precondition of the action put,namely that the destination container is opened, sohe directly produces a CR about the precondition.2The correct plan to achieve (1) involves pressing button12, as you (and the DG) can verify on the map (in the Ap-pendix).2014.2 Beyond classical planning: Otherimportant means-end inference tasksConsider the following example, here the DG justtold the DF to press a button, in turn (1), with nofurther explanation.
As a result of the action a cab-inet opened, and the DF predicted that the follow-ing action requested would be (5).
In (6) the DGconfirms this hypothesis.DG(1): press the button on the left [pause]DG(2): and .
uh [pause]DF(3): [pause]DG(4): [pause]DF(5): put it in this cabinet?DG(6): put it in that cabinet, yeahThe inference that the DF did in order to pro-duce (5) can be defined as another means-end in-ference task which involves finding the next rele-vant actions.
The input of such task would alsoconsist of an initial state, a set of possible ac-tions but it will contain one observed action (inthe example, action (1)).
Inferring the next rele-vant action consists in inferring the affordabilities(i.e.
the set of executable actions) of the initialstate and the affordabilities of the state after theobserved action was executed.
The next relevantactions will be those actions that were activatedby the observed action.
In the example above, thenext relevant action that will be inferred is ?putthe thing you are carrying in the cabinet that justopened?, just what the DF predicted in (5).The definition of this inference task needs refin-ing but it already constitutes an interesting exam-ple of a new form of means-ends reasoning.There are further examples in the corpus thatsuggest the need for means-end inferences in situ-ations in which a classical planner would just say?there is no plan?.
These are cases in which nocomplete plan can be found but the DF is anywayable to predict a possible course of action.
For in-stance, in the last dialogue of Section 4.1, the DFdoes not stops in (13) and waits for an answer buthe continues with:DF(14): one of the buttons?DG(15): yeah, it?s the left oneOther CRs similar to this one, where a param-eter of the action is ambiguous, is missing or isredundant, were also found in the corpus.4.3 Preliminary ConclusionsThe inference-tasks we discussed or just hinted toin this paper do not give a complete characteriza-tion of the kinds of clarification requests of level4.
It covers 14 of the 19 CRs in the SCARE di-alogue analyzed in Section 2.3.
CRs not coveredat all have to do mainly with the fact that peopledo not completely remember (or trust) the instruc-tions during the experiments or what themselves(or their partner) said a few turns before, such asthe following one:DG(1): you?ve to .
like jump on it or something .DF(2): I don?t know if I can jumpHere, the DF does not remember that he canjump using the Spacebar as stated in the instruc-tions he received (Appendix A.1).In order to account for these cases it is nec-essary to consider how conversation is useful forovercoming also this issue.
The fact that people?smemory is non reliable is intrinsic to communica-tion and here again, communication must provideintrinsic mechanisms to deal with it.
Modelingsuch things are challenges that a complete theoryof communication will have to face.5 ConclusionsConversational implicatures are negotiable, thisis the characteristic that distinguishes them fromother kinds of meanings (like entailments).
Dia-logue provides an intrinsic mechanism for carry-ing out negotiations of meaning, namely clarifi-cations.
So our hypothesis is that conversationalimplicatures are a rich source of clarification re-quests.In order to investigate this hypothesis, we re-viewed theoretical work from pragmatics, prac-tical work from the dialogue system communityand we presented empirical evidence from spon-taneous dialogues situated in an instruction givingtask.
Also, we presented a framework in which(part of) the clarification potential of an instruc-tion is generated by inferring its conversationalimplicatures.
We believe that this is a step towardsdefining a clear functional criteria for identifyingand classifying the clarification requests at level 4of communication.But much more remains to be done.
The empir-ical results we present here are suggestive but pre-liminary; we are currently in the process of eval-uating their reliability measuring inter-annotator202agreement.
Moreover, in the course of this workwe noticed a promising link between clarifica-tion strategies and politeness constraints which weplan to develop in future work.
Also, we are par-ticularly interested in means-ends reasoning otherthan planning, something we have merely hintedat in this paper; these tasks still need to be for-mally defined, implemented and tested.
Finally,we are considering the GIVE challenge (Byron etal., 2009) as a possible setting for evaluating ourwork (our framework could predict potential clar-ification requests from the users).There is lot to do yet, but we believe that theinterplay between conversational implicatures andclarification mechanisms will play a crucial role infuture theories of communication.ReferencesJens Allwood.
1995.
An activity based approachto pragmatics.
In Abduction, Belief and Contextin Dialogue: Studies in Computational Pragmatics,pages 47?80.
University of Go?teborg.Nate Blaylock and James Allen.
2005.
A collaborativeproblem-solving model of dialogue.
In Proceedingsof the 6th SIGdial Workshop on Discourse and Dia-logue, pages 200?211, Lisbon, Portugal.Penelope Brown and Stephen Levinson.
1987.
Polite-ness: Some universals in language usage.
Studies inInteractional Sociolinguistics.Donna Byron, Alexander Koller, Kristina Striegnitz,Justine Cassell, Robert Dale, Johanna Moore, andJon Oberlander.
2009.
Report on the First NLGChallenge on Generating Instructions in Virtual En-vironments (GIVE).
In Proc.
of the 12th EuropeanWorkshop on Natural Language Generation, pages165?173, Athens, Greece.
ACL.Sandra Carberry and Lynn Lambert.
1999.
A processmodel for recognizing communicative acts and mod-eling negotiation subdialogues.
Computational Lin-guistics, 25(1):1?53.Herbert Clark.
1996.
Using Language.
CambridgeUniversity Press, New York.Richard Fikes, Peter Hart, and Nils Nilsson.
1972.Learning and executing generalized robot plans.
Ar-tificial Intelligence, 3:251?288.Malte Gabsdil.
2003.
Clarification in spoken dialoguesystems.
In Proc of the AAAI Spring Symposium.Workshop on Natural Language Generation in Spo-ken and Written Dialogue, pages 28?35.Alfonso Gerevini and Derek Long.
2005.
Plan con-straints and preferences in PDDL3.
Technical Re-port R.T. 2005-08-47, Brescia University, Italy.Jonathan Ginzburg.
2009.
The interactive Stance:Meaning for Conversation.
CSLI Publications.Paul Grice.
1975.
Logic and conversation.
In P. Coleand J. L. Morgan, editors, Syntax and Semantics:Vol.
3: Speech Acts, pages 41?58.
Academic Press.Jo?rg Hoffmann and Bernhard Nebel.
2001.
TheFF planning system: Fast plan generation throughheuristic search.
JAIR, 14:253?302.Chih-Wei Hsu, Benjamin W. Wah, Ruoyun Huang,and Yixin Chen.
2006.
New features in SGPlanfor handling soft constraints and goal preferences inPDDL3.0.
In Proc of ICAPS.Anthony Kenny.
1966.
Practical inference.
Analysis,26:65?75.Alexander Koller and Matthew Stone.
2007.
Sentencegeneration as planning.
In Proc.
of ACL-07, Prague.Karen E. Lochbaum.
1998.
A collaborative planningmodel of intentional structure.
Comput.
Linguist.,24(4):525?572.Matthew Purver, Jonathan Ginzburg, and PatrickHealey.
2003.
On the means for clarification indialogue.
In Current and New Directions in Dis-course and Dialogue, pages 235?255.
Kluwer Aca-demic Publishers.Matthew Purver.
2004.
The Theory and Use of Clari-fication Requests in Dialogue.
Ph.D. thesis, King?sCollege, University of London.Verena Rieser and Johanna Moore.
2005.
Implicationsfor generating clarification requests in task-orienteddialogues.
In Proc of ACL, pages 239?246.Kepa Rodr?
?guez and David Schlangen.
2004.
Form,intonation and function of clarification requests ingerman task oriented spoken dialogues.
In Proc ofSEMDIAL, pages 101?108.Emanuel Schegloff.
1987.
Some sources of misunder-standing in talk-in-interaction.
Linguistics, 8:201?218.David Schlangen.
2004.
Causes and strategies for re-questing clarification in dialogue.
In Proc of SIG-DIAL.Gabriel Skantze.
2007.
Error Handling in Spoken Di-alogue Systems.
Ph.D. thesis, KTH - Royal Instituteof Technology, Sweden.Laura Stoia, Darla Shockley, Donna Byron, and EricFosler-Lussier.
2008.
SCARE: A situated corpuswith annotated referring expressions.
In Proc ofLREC.Laura Stoia.
2007.
Noun Phrase Generation for Sit-uated Dialogs.
Ph.D. thesis, Ohio State University,USA.203A Instructions for the DG and DFIn this section, we specify the information thatwas available to the DG and the DF before theSCARE experiment started (adapted from (Stoia,2007)).
These instructions are crucial for ourstudy since they define the content of the infor-mation resources of the inference framework de-scribed in this paper.A.1 Instructions for bothThe following specification of the Quake controls,that is, the possible actions in the simulated world,were received by all participants.1.
Use the arrow keys for movement:?
Walk forward: ??
Walk backward: ??
Turn right: ??
Turn left: ?2.
To jump: use Spacebar.3.
To press a button: Walk over the button.You will see it depress.4.
To pick up an object: Step onto the itemthen press Ctrl (Control key).5.
To drop an object: Hit TAB to see the list ofitems that you are currently carrying.
Pressthe letter beside the item you wish to drop.Press TAB again to make the menu go away.The participants also received the following pic-tures of possible objects in the simulated world sothat they are able to recognize them.Buttons CabinetThe following things were indicated as beingobjects that the DF can pick up and move:Quad damage Rebreather SilencerThey also received the following warning: Youwill not be timed, but penalty points will be takenfor pushing the wrong buttons or placing things inthe wrong cabinets.A.2 Instructions for the Direction FollowerOnly the DF received the following information:Phase 1: Learning the controls First you willbe put into a small map with no partner, to get ac-customed to the quake controls (detailed in Sec-tion A.1).
Practice moving around using the arrowkeys.
Practice these actions:1.
Pick up the Rebreather or the Quad Damage.2.
Push the blue button to open the cabinet.3.
Drop the Quad Damage or the Rebreather in-side the cabinet and close the door by pushingthe button again.Phase 2: Completing the task In this phase youwill be put in a new location.
Your partner willdirect you in completing 5 tasks.
He will see thesame view that you are seeing, but you are the onlyone that can move around and act in the world.A.2.1 Implications for the Potential ActionsIn phase 1, when the DF is learning the con-trols, he learns that buttons can have the effectof opening closed cabinets and closing open cab-inets.
Such action is formalized as follows inPDDL (Gerevini and Long, 2005) and is includedin the possible action database:(:action press_button:parameters (?x ?y):precondition(button ?x)(cabinet ?y)(opens ?x ?y):effects(when (open ?y) (closed ?y))(when (closed ?y) (open ?y)))Notice that this action operator has conditionaleffects in order to specify the action more suc-cinctly.
However, it is not mandatory for the actionlanguage to support conditional effects.
This ac-tion could be specified with two actions in whichthe antecedent of the conditional effect is now aprecondition.A.3 Instructions for the Direction GiverOnly the DG received the following information:Phase 1: Planning the task Your packet con-tains a map of the quake world with 5 objectivesthat you have to direct your partner to perform.Read the instructions and take your time to planthe directions you want to give to your partner.204Figure 2: Map received by the DG (upper floor)Phase 2: Directing the follower In this phaseyour partner will be placed into the world in thestart position.
Your monitor will show his/herview of the world as he/she moves around.
He/shehas no knowledge of the tasks, and has not re-ceived a map.
You have to direct him/her throughspeech in order to complete the tasks.
The objec-tive is to complete all 5 tasks, but the order doesnot matter.The tasks are:1.
Move the picture to the other wall.2.
Move the boxes on the long table so that thefinal configuration matches the picture below.Picture Long table3.
Hide the Rebreather in Cabinet9.
To hide anitem you have to find it, pick it up, drop it inthe cabinet and close the door.4.
Hide the Silencer in Cabinet4.5.
Hide the Quad Damage in Cabinet14.6.
At the end, return to the starting point.A.3.1 Implications for the World ActionsThe functions of the buttons that can movethings can be represented in the following actionschema.
If the thing is in it?s original location (itslocation when the game starts), we say that is thingis not-moved.
If the thing is in the goal positionthen we say that the thing is moved.
(:action press_button:parameters (?x ?y):precondition(button ?x)(thing ?y)(moves ?x ?y):effects(when (moved ?y) (not-moved ?y))(when (not-moved ?y) (moved ?y)))A.3.2 Implications for the World ModelThe world model is a relational model that rep-resents the information provided by the map, in-cluding the functions of the buttons and the con-tents of the cabinets.Figure 3: Fragment of the SCARE world modelB Clarification Potential Inference StepsThe following pictures illustrate how the impli-catures of the instruction ?put the picture on theopposite wall?
are calculated using the dialoguemodel (Figure 4) and used to predict the CR ?AmI supposed to pick up this thing??
(Figure 5).Figure 4: Step 1 - Calculating the implicaturesFigure 5: Step 2 - Predicting the CR205
