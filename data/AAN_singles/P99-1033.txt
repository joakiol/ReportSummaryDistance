Dependency  Parsing with an Extended Finite State ApproachKemal  Of lazerDepar tment  of Computer  EngineeringBilkent UniversityAnkara, 06533,Turkeyko?cs, b i l kent ,  edu.
t rComput ing  Research LaboratoryNew Mexico State UniversityLas Cruces, NM, 88003 USAko@cr l ,  nmsu.
eduAbstractThis paper presents a dependency parsing schemeusing an extended finite state approach.
The parseraugments input representation with "channels" sothat links representing syntactic dependency rela-tions among words can be accommodated, and it-erates on the input a number of times to arrive ata fixed point.
Intermediate configurations violatingvarious constraints of projective dependency repre-sentations such as no crossing links, no independentitems except sentential head, etc, are filtered via fi-nite state filters.
We have applied the parser to de-pendency parsing of Turkish.1 IntroductionRecent advances in the development of sophisticatedtools for building finite state systems (e.g., XRCEFinite State Tools (Karttunen et al, 1996), ATgzTTools (Mohri et al, 1998)) have fostered the develop-ment of quite complex finite state systems for natu-ral language processing.
In the last several years,there have been a number of studies on develop-ing finite state parsing systems, (Koskenniemi, 1990;Koskenniemi et al, 1992; Grefenstette, 1996; Ait-Mokhtar and Chanod, 1997).
There have also beena number of approaches to natural anguage pars-ing using extended finite state approaches in whicha finite state engine is applied multiple times to theinput, or various derivatives thereof, until some stop-ping condition is reached.
Roche (1997) presentsan approach for parsing in which the input is itera-tively bracketed using a finite state transducer.
Ab-ney(1996) presents a finite state parsing approachin which a tagged sentence is parsed by transducerswhich progressively transform the input to sequencesof symbols representing phrasal constituents.
Thispaper presents an approach to dependency parsingusing an extended finite state model resembling theapproaches of Roche and Abney.
The parser pro-duces outputs that encode alabeled ependency treerepresentation f the syntactic relations between thewords in the sentence.We assume that the reader is familiar with thebasic concepts of finite state transducers (FST here-after), finite state devices that map between two reg-ular languages U and L (Kaplan and Kay, 1994).2 Dependency  SyntaxDependency approaches to syntactic representationuse the notion of syntactic relation to associate sur-face lexical items.
The book by Mel~uk (1988)presents a comprehensive exposition of dependencysyntax.
Computational pproaches to dependencysyntax have recently become quite popular (e.g.,a workshop dedicated to computational pproachesto dependency grammars has been held at COL-ING/ACL'98 Conference).
J~irvinen and Tapana-ninen have demonstrated anefficient wide-coveragedependency parser for English (Tapanainen andJ~irvinen, 1997; J?rvinen and Tapanainen, 1998).The work of Sleator and Temperley(1991) on linkgrammar, an essentially exicalized variant of depen-dency grammar, has also proved to be interesting ina number of aspects.
Dependency-based statisticallanguage modeling and analysis have also becomequite popular in statistical natural language process-ing (Lafferty et al, 1992; Eisner, 1996; Chelba andet al, 1997).Robinson(1970) gives four axioms for well-formeddependency structures, which have been assumed inalmost all computational pproaches.
In a depen-dency structure of a sentence (i) one and only oneword is independent, i.e., not linked to some otherword, (ii) all others depend directly on some word,(iii) no word depends on more than one other, and,(iv) if a word A depends directly on B, and someword C intervenes between them (in linear order),then C depends directly on A or on B, or on someother intervening word.
This last condition of pro-jectivity (or various extensions of it; see e.g., Lauand Huang (1994)) is usually assumed by most com-putational approaches to dependency grammars asa constraint for filtering configurations, and has alsobeen used as a simplifying condition in statisticalapproaches for inducing dependencies from corpora(e.g., Yiiret(1998).
)3 TurkishTurkish is an agglutinative language where a se-quence of inflectional and derivational morphemesget affixed to a root (Oflazer, 1993).
Derivations arevery productive, and the syntactic relations that aword is involved in as a dependent or head element,are determined by the inflectional properties of the254"~41~tCJ  f ru i t  DopeDdoDt:g L:Lnk t:o Head1 IIIG3 io,  }1Figure h Links and Inflectional Groupsone or more (intermediate) derived forms.
In thiswork, we assume that a Turkish word is representedas a sequence of inflectional groups (IGs hereafter),separated by "DBs denoting derivation boundaries,in the following general form:root+Infl1"DB+Infl2"DB+.
?
.
'DB+Infl.where Infli denote relevant inflectional featuresincluding the part-of-speech for the root, or anyof the derived forms.
For instance, the deriveddeterminer saglamla?tlrdzgzmzzdaki I would berepresented as:2s aglam+hdj "DB+Verb+Be come "DB+Verb+Caus+Po s"DB+Adj +PastPart+P i sg* DB+Noun+Zero+A3sg+Pnon+Loc'DB+DetThis word has 6 IGs:I. sa~lam+Adj 2.
+Verb+Become3.
+Verb+Caus+Pos 4.
+Adj+PastPart+Plsg5.
+Noun+Zero+A3sg 6.
+Det+Pnon+LocA sentence would then be represented as a sequenceof the IGs making up the words.An interesting observation that we can makeabout Turkish is that, when a word is consideredas a sequence of IGs, syntactic relation links onlyemanate from the last IG of a (dependent) word,and land on one of the IG's of the (head) word onthe right (with minor exceptions), as exemplified inFigure 1.
A second observation is that, with minorexceptions, the dependency links between the IGs,when drawn above the IG sequence, do not cross.Figure 2 shows a dependency tree for a sentence laidon top of the words segmented along IG boundaries.4 F in i te  S ta te  Dependency  Pars ingThe approach relies on augmenting the input with"channels" that (logically) reside above the IG se-quence and "laying" links representing dependencyrelations in these channels, as depicted Figure 3 a).The parser operates in a number of iterations: Ateach iteration of the parser, an new empty channel1Literally, "(the thing existing) at the time we caused(something) to become strong".
Obviously this is not a wordthat one would use everyday.
Turkish words found in typicaltext average about 3-4 morphemes including the stem.2 The morphological features other than the obvious POSeare: +Become: become verb, +Caus: causative verb, PastPart:Derived past participle, Ptsg: leg possessive agreement,A3sg: 3sg number-person agreement,+Zero: Zero derivationwith no overt morpheme, +Pnon: No possessive agreement,+Loc:Locative case, +Poe: Positive Polarity.a) Input sequence ofIGs am augmented with symbols to represent Channels.
(IGl) (IG2) (IG3)... (IGi)... (IGn_{) (IG,)b) Links are embedded in channels., .
.
- .
.
.
, .
, ,% , , , : .
.
.
, .
.
.
r ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~ , , .
.
.
.
.~  .
.
.
.
.
.
(IGl) (IG2) (IG3)... (IGi)... (IG._l) (IG.
)c) New channels are "stacked on top of each other".?
u .
.
.
.
.
.
~ .
.
.T  . '
, .
.
, L .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.~  .
.
.
.
..n .
.
.
.
.
.
.
r .
.
, .
: , .
.
.
.
.
.~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~ .
.
.
.
, .
.~  .
.
.
.
.
.
(IGI) (IG2) (IG3)... (IGi)...
(IG..I) (IG.
)d) So that links that can not be accommodated in lower channels can be established.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
l  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
; .
.
.
.
.
.
(IGl) (IG2) (IG3)... (IGi)... (IG,.l) (1G,)?
.~ .
- - .
- -  ~-  "A ' " "  ~ .
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
~"""1~ .
.
.
.
.
(IG,) (IG,) (IG0... (IG~)...
(IG?.,) 0G,)Figure 3: Channels and Linksis "stacked" on top of the input, and any possiblelinks are established using these channels, until nonew links can be added.
An abstract view of this ispresented in parts b) through e) of Figure 3.4.1 Represent ing  Channe ls  and  Syntact i cRe la t ionsThe sequence (or the chart) of IGs is produced bya a morphological nalyzer FST, with each IG be-ing augmented by two pairs of delimiter symbols, as<(IG)>.
Word final IGs, IGs that links will emanatefrom, are further augmented with a special marker ?.Channels are represented by pairs of matching sym-bols that surround the <.
.
.
( and the ) .
.
.>  pairs.Symbols for new channels (upper channels in Figure3) are stacked so that the symbols for the topmostchannels are those closest o the ( .
.
. )
.a  The chan-nel symbol 0 indicates that the channel segment isnot used while 1 indicates that the channel is usedby a link that starts at some IG on the left andends at some IG on the right, that is, the link isjust crossing over the IG.
If a link starts from anIG (ends on an IG), then a start (stop) symbol de-noting the syntactic relation is used on the right(left) side of the IG.
The syntactic relations (alongwith symbols used) that we currently encode in ourparser are the following: 4 S (Subject), 0 (Object),M (Modifier, adv/adj), P (Possessor), C (Classifier),D (Determiner), T (Dative Adjunct), L ( LocativeAdjunct), A: (Ablative Adjunct) and I (Instrumen-tal Adjunct).
For instance, with three channels, thetwo IGs of bahgedeki in Figure 2, would be repre-sented as <MD0(bah~e+Noun+h3sg+Pnon+Loc)000><000(+Det?)00d>.
The M and the D to the left of3 At any time, the number  of channel symbols on both sidesof an IG are the same.4We use the lower case symbol to mark the start of thelink and the upper case symbol to encode the end of the link.255Det Pos SubjD ADJ N D N ADV V N PN ADV VLast line shows the final POS for each word.Figure 2: Dependency Links in an example Turkish Sentencethe first IG indicate the incoming modifier and de-terminer links, and the d on the right of the secondIG indicates the outgoing determiner link.4.2 Components  o f  a Parser  StageThe basic strategy of a parser stage is to recognize bya rule (encoded as a regular expression) a dependentIG and a head IG, and link them by modifying the"topmost" channel between those two.
To achievethis:1. we put temporary brackets to the left of thedependent IG and to the right of the head IG,making sure that (i) the last channel in thatsegment is free, and (ii) the dependent is notalready linked (at  one of the lower channels),2. we mark the channels of the start, intermediateand ending IGs with the appropriate symbolsencoding the relation thus established by thebrackets,3.
we remove the temporary brackets.A typical linking rule looks like the following: 5\[LL IGI LR\] \[ML IG2 MR\]* \[RL IG3 RR\] (->)"{s" .
.
.
"s}"This rule says: (optionally) bracket (with {S andS}), any occurrence of morphological pattern IG1(dependent), skipping over any number of occur-rences of pattern IG2, finally ending with a pat-tern IG3 (governor).
The symbols L(eft)L(eft),LR, ML, MR, RL and RR are regular expressionsthat encode constraints on the bounding chan-nel symbols.
For instance, LI~ is the pattern"?
.. .
.  )
.
.
.
.
0" \["0" I 1\]* ">" which checks that(i) this is a word-final IG (has a "?
"), (ii) the rightside "topmost" channel is empty (channel symbolnearest o ")" is "0"), and (iii) the IG is not linkedto any other in any of the lower channels (the onlysymbols on the right side are 0s and ls.
)For instance the example rule\[LL NominativeNominalA3pl LR\] \[ML AnyIG MR\]*\[RL \[FiniteVerbA3sg I FiniteVerbl3pl\] RR \](->) "{s .
.
.
.
s}"SWe use the XRCE Regular Expression LanguageSyntax; see ht tp  ://www.
xrce.
xerox,  com/resea.vch/taltt/fst/fssyntax.htral for details.is used to bracket a segment starting with a pluralnominative nominal, as subject of a finite verb onthe right with either +A3sg or +A3pl number-personagreement (allowed in Turkish.)
The regular expres-sion NominativeNominalA3pl matches any nomi-nal IG with nominative case and A3pl agreement,while the regular expression \[F initeVerbA3sg JF in i teVerbA3pl \ ]  matches any finite verb IG witheither A3sg or A3pl agreement.
The regular expres-sion AnyIG matches any IG.All the rules are grouped together into a parallelbracketing rule defined as follows:Bracket = \[Pat tern l  (->) "{Re l l "  ... "Re l l}" ,Pat tern2  (->) "{Re l2"  ... "Re l2}" ,\ ] ;which will produce all possible bracketing of the in-put IG sequence.
64.3 F i l te r ing  Cross ing L ink  Conf igurat ionsThe bracketings produced by Bracket  contain con-figurations that may have crossing links.
This hap-pens when the left side channel symbols of the IGimmediately right of a open bracket contains thesymbol 1 for one of the lower channels, indicatinga link entering the region, or when the right sidechannel symbols of the IG immediately to the leftof a close bracket contains the symbol 1 for one ofthe lower channels, indicating a link exiting the seg-ment, i.e., either or both of the following patternsappear in the bracketed segment:(i) {S < .
.
.
1 .
.
.
0 ( .
.
.  )
.
.
.
(ii) ... ( ... ) 0 ... 1 ... > S}Configurations generated by bracketing are filteredby FSTs implementing suitable regular expressionsthat reject inputs having crossing links.A second configuration that may appear is the fol-lowing: A rule may attempt to put a link in thetopmost channel even though the corresponding seg-ment is not utilized in a previous channel, e.g., thecorresponding segment one of the previous channelsmay be all Os.
This constraint filters such cases to6{Reli and Roli} are pairs of brackets; there is a distinctpair for each syntactic relation to be identified by these rules.256prevent redundant configurations from proliferatingfor later iterations of the parser.
7 For these two con-figuration constraints we define F i l te raonf igs  as sFilterConfigs = \[ Fi lterCrossingLinks .o.Filt erEmptySegment s\] ;We can now define one phase (of one iteration) ofthe parser as:Phase  = Bracket  .o .
F i l te rCon2 igs  .o .MarkChanne ls  .o .
RemoveTempBrackets ;The transducer MarkChannels modifies the chan-nel symbols in the bracketed segments to eitherthe syntactic relation start or end symbol, or a1, depending on the IG.
Finally, the transducerRemoveTempBrackets ,  removes  the brackets.
9The formulation up to now does not allow us tobracket an IG on two consecutive non-overlappinglinks in the same channel.
We would need a brack-eting configuration like... {S < ... > {H < ... > S} ... < ... > M} ...but this would not be possible within Bracket, aspatterns check that no other brackets are withintheir segment of interest.
Simply composing thePhase transducer with itself without introducing anew channel solves this problem, giving us a one-stage parser, i.e.,Parse  = Phase  .
o .
Phase ;4.4 Enforc ing  Syntact i c  Const ra in tsThe rules linking the IGs are overgenerating in thatthey may generate configurations that may vio-late some general or language specific constraints.For instance, more than one subject or one ob-ject may attach to a verb, or more that one deter-miner or possessor may attach to a nominal, an ob-ject may attach to a passive verb (conjunctions arehandled in the manner described in J?rvinen andTapanainen(1998)), or a nominative pronoun maybe linked as a direct object (which is not possiblein Turkish), etc.
Constraints preventing these maycan be encoded in the bracketing patterns, but do-ing so results in complex and unreadable rules.
In-stead, each can be implemented as a finite state filterwhich operate on the outputs of Parse  by checkingthe symbols denoting the relations.
For instance wecan define the following regular expression for fil-tering out configurations where two determiners areattached to the same IG: l?7This constra int  is a bit  tr ickier since one has to check thatthe same number  of channels on both sides are empty;  we l imitourselves to the last 3 channels in the implementat ion.8.
o. denotes the t ransducer  composit ion operator.
Wealso use, for exposit ion purposes, =, instead of the XRCEdef ine  command.9 The detai ls  of these regular expressions are quite uninter-esting.l?LeftChannelSymbols and RightChannelSymbols denotethe sets of symbols that can appear on the left and right sidechannels.AtMost0neDet =\[ "<" \[ ~ \[\[$"D"\]'I\] & LeftCharmelSymbols* \]"(" AnyIG ("@") ")"RightChannelSymbols* ">" \]*;The FST for this regular expression makes sure thatall configurations that are produced have at mostone D symbol among the left channel symbols, nMany other syntactic onstraints (e.g., only one ob-ject to a verb) can be formulated similar to above.All such constraints Consl ,  Cons2 .. .ConsN, canthen be composed to give one FST that enforces allof these:Syntact i cF i l te r  = \[ Cons l  .o .
Cons2 .o .Cons3 .o .
.
.
.
.
o.  ConsN\]4.5 I te ra t lve  app l i ca t ion  of  the  parserFull parsing consists of iterative applications of theParser and Syntact icF i l ter  FSTs.
Let Input bea transducer that represents the word sequence.
LetLastChannelNotEmpt y =\["<" Lef tChannelSymbels+"(" AnyIG ("@") ")"RightCharmelSymbols+ ">"\]* -\["<" LeftChannelSymbols* 0"(" AnyIG ("@") ")"0 RightChannelSymbols* ">"\]*;be a transducer which detects if any configurationhas at least one link established in the last channeladded (i.e., not all of the "topmost" channel sym-bols are O's.)
Let Morpholog ica lD isambiguatorbe a reductionistic finite state disambiguator whichperforms accurate but very conservative local dis-ambiguation and multi-word construct coalescing, toreduce morphological mbiguity without making anyerrors.The iterative applications of the parser can nowbe given (in pseudo-code) as:# Map sentence to a transducer representinga chart of IGsM = \[Sentence .o.
MorphologicalAnalyzer\] .o.MorphologicalDisambi~nlat or;repeat {M = M .o.
AddChannel .o.
Parse .o.Synt act icFilter ;}unti l  ( \[M .o.
LastChannelNotEmpty\].
l  == { })M = M .o.
0nly0neUnlinked ;Parses = M.I;This procedure iterates until the most recentlyadded channel of every configuration generated isunused (i.e., the (lower regular) language recognizedby M .o.
LastChannelNotEmpty is empty.
)The step after the loop, M = M .o.0nly0neUnlinked, enforces the constraint that11 The crucial  port ion at the beginning says "For any IG it isnot the case that  there is more than one subst r ing  conta in ingD among the left channel  symbols of that  IG.
"257in a correct dependency parse all except one ofthe word final IGs have to link as a dependentto some head.
This transduction filters all thoseconfigurations (and usually there are many of themdue to the optionality in the bracketing step.
)Then, Parses defined as the (lower) language of theresulting FST has all the strings that encode theIGs and the links.4.6 Robust  Pars ingIt is possible that either because of grammar cover-age, or ungrammatical input, a parse with only oneunlinked word final IG may not be found.
In suchcases Parses above would be empty.
One may how-ever opt to accept parses with k > 1 unlinked wordfinal IGs when there are no parses with < k un-linked word final IGs (for some small k.) This can beachieved by using the lenient composition operator(Karttunen, 1998).
Lenient composition, notatedas .
0., is used with a generator-filter combination.When a generator transducer Gis leniently composedwith a filter transducer, F, the resulting transducer,G .
0.
F, has the following behavior when an inputis applied: If any of the outputs of G in response tothe input string satisfies the filter F, then G .0.
Fproduces just these as output.
Otherwise, G .0.
Foutputs what G outputs.Let Unlinked_i denote a regular expression whichaccepts parse configurations with less than or equali unlinked word final IGs.
For instance, for i = 2,this would be defined as follows:- \ [ \ [$\ [  "<" LeftChannelSymbols* "(" AnyIG "@ .
.
.
.
)"E"0" I 13.
">"\]3" > 2 \];which rejects configurations having more than 2word final IGs whose right channel symbols containonly 0s and is, i.e., they do not link to some otherIG as a dependent.Replacing line M = H .o.
Only0neUnlinked,with, for instance, M = M .0.
Unlinked_l .0.Unlinked_2 .0.
Unlinked_3; will have the parserproduce outputs with up to 3 unlinked word finalIGs, when there are no outputs with a smaller num-ber of unlinked word final IGs.
Thus it is possible torecover some of the partial dependency structureswhen a full dependency structure is not availablefor some reason.
The caveat would be however thatsince Unlinked_l is a very strong constraint, anyrelaxation would increase the number of outputssubstantially.5 Exper iments  w i th  dependencypars ing  o f  Turk i shOur work to date has mainly consisted of developingand implementing the representation a d finite statetechniques involved here, along with a non-trivialgrammar component.
We have tested the resultingsystem and grammar on a corpus of 50 Turkish sen-tences, 20 of which were also used for developing andtesting the grammar.
These sentences had 4 to 24words with an average 10 about 12 words.The grammar has two major components.
Themorphological analyzer is a full coverage analyzerbuilt using XRCE tools, slightly modified to gen-erate outputs as a sequence of IGs for a sequenceof words.
When an input sentence (again repre-sented as a transducer denoting a sequence of words)is composed with the morphological analyzer (seepseudo-code above), a transducer for the chart rep-resenting all IGs for all morphological ambiguities(remaining after morphological disambiguation) isgenerated.
The dependency relations are describedby a set of about 30 patterns much like the onesexemplified above.
The rules are almost all non-lexical establishing links of the types listed earlier.Conjunctions are handled by linking the left con-junct to the conjunction, and linking the conjunctionto the right conjunct (possibly at a different chan-nel).
There are an additional set of about 25 finitestate constraints that impose various syntactic andconfigurational constraints.
The resulting Parsertransducer has 2707 states 27,713 transitions whilethe Syntact i cConst ra in ts  transducer has 28,894states and 302,354 transitions.
The combined trans-ducer for morphological nalysis and (very limited)disambiguation has 87,475 states and 218,082 arcs.Table 1 presents our results for parsing this set of50 sentences.
The number of iterations also countthe last iteration where no new links are added.
In-spired by Lin's notion of structural complexity (Lin,1996), measured by the total length of the links ina dependency parse, we ordered the parses of a sen-tence using this measure.
In 32 out of 50 sentences(64%), the correct parse was either the top rankedparse or among the top ranked parses with the samemeasure.
In 13 out of 50 parses (26%) the correctparse was not among the top ranked parses, but wasranked lower.
Since smaller structural complexityrequires, for example, verbal adjuncts, etc.
to attachto the nearest verb wherever possible, topicalizationof such items which brings them to the beginning ofthe sentence, will generate a long(er) link to the verb(at the end) increasing complexity.
In 5 out of 50sentences (5%), the correct parse was not availableamong the parses generated, mainly due to grammarcoverage.
The parses generated in these cases usedother (morphological) ambiguities of certain lexicalitems to arrive at some parse within the confines ofthe grammar.The finite state transducers compile in about2 minutes on Apple Macintosh 250 Mhz Power-book.
Parsing is about a second per iteration in-cluding lookup in the morphological nalyzer.
Withcompletely (and manually) morphologically disam-biguated input, parsing is instantaneous.
Figure 4presents the input and the output of the parser for asample Turkish sentence.
Figure 5 shows the output258Input Sentence: Diinya Bankas~T/irkiye Direkthdi English: World Bank Turkey Director said that as a re-h/ikfimetin izledi~i ekonomik programln sonucunda sult of the economic program fonowed by the government,5nemfi achmlann atflchg\]m s6yledi, important steps were taken.Parser Output after 3 iterations:Parsel:<O00(dUnya+Noun+A3sg+Pnon+Nom@)OOc><COO(banka+Noun+A3sg+P3sg+Bom~)OcO> <OlO(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)Olc><CC~(direkt~r+N~un+A3sg+~3sg+N~m@)s~><~1(hUkUmet+B~un+A3sg+~n~n+Gen@)1~s><~1(iz1e+verb+p~s)1~><~(+Adj+Past~art+p3sg@)1m~><~11(ek~n~mik+Adj@)1~m><MM1(pr~gram+B~un+A3sg+~n~n+Gen~)~p><P~(s~nuC+N~un+A3sg+P3s~?L~c@)~1~><~(~nem+N~un)~><~11(+Adj+with@)1~m><M1~(adIm+N~un+A3p1+Pn~n+Gen~)1~s><S~(at+Verb)~><~(+verb+~ass+P~s)~><~(+I~un+~ast~art+A3sg+~3s~Acc@)~1~><~L~(s~y1e+verb+p~s+~ast+A3sg@)~>Parse2:<~(dUnya+I~un+A3sg+~n~n+N~m@)~c><C~(banka+N~un+A3sg+~3sg+I~m~)~c~><~1~(tUrkiye+N~un+pr~p+A3sg+pn~n+l~m@)~c><CC~(direkt~r+N~un+A3sg+p3sg+N~m@)s~><~(hUkUmet+l~un+A3sg+pn~n+Gen@)1~s><~(iz1e+Verb+p~s)1~><~(+Adj+Past~art+~3sg@)~m~><~(ek~n~mik+AdjQ)~m><RM1(pr~ram+N~un+A3s~+pn~n+GenQ)~p><p~(s~nuC+N~un+A3sg+~3sg+L~)~1~><~1~(~nem+|~un)~><~1(+Adj+with@)1~m><M~1(adIm+N~un+A3p1+~n~n+Gen~)1~s><SL1(at+Verb)1~><~1(+Verb+~ass+~s)1~><~(+N~un+~astpart+A3sg+~3sg+Acc@)1~><~(s~y1e+verb+p~s+~ast+A3sg@)~>The only difference in the two are parses are in the locative adjunct attachment (to verbs at and sSyle,highlighted with ***).Figure 4: Sample Input and Output of the parserAvg.
Words /Sentence:Avg.
IGs /Sentence:Avg.
Parser  Iterations:Avg.
Parses /Sentence:11.7 (4 - 24)16.4 (5 - 36)5.2 (3 - 8)23.9 (1 - 132)Table 1: Statistics from Parsing 50 Turkish Sen-tencesof the parser processed with a Perl script to providea more human-consumable presentation:6 D iscuss ion  and  Conc lus ionsWe have presented the architecture and implemen-tation of novel extended finite state dependencyparser, with results from Turkish.
We have formu-lated, but not yet implemented at this stage, twoextensions.
Crossing dependency links are very rarein Turkish and almost always occur in Turkish whenan adjunct of a verb cuts in a certain position of a(discontinuous) noun phrase.
We can solve this byallowing such adjuncts to use a special channel "be-low" the IG sequence so that limited crossing linkconfigurations can be allowed.
Links where the de-pendent is to the right of its head, which can happenwith some of the word order variations (with back-grounding of some dependents of the main verb) cansimilarly be handled with a right-to-left version ofParser  which is applied during each iteration, butthese cases are very rare.In addition to the reductionistic disambiguatorthat we have used just prior to parsing, we have im-plemented a number of heuristics to limit the num-ber of potentially spurious configurations that re-sult because of optionality in bracketing, mainly byenforcing obligatory bracketing for immediately se-quential dependency configurations (e.g., the com-plement of a postposition is immediately before it.
)Such heuristics force such dependencies to appear inthe first channel and hence prune many potentiallyuseless configurations popping up in later stages.The robust parsing technique has been very instru-mental during the process mainly in the debuggingof the grammar, but we have not made any substan-tial experiments with it yet.7 AcknowledgmentsThis work was partially supported by a NATOScience for Stability Program Project Grant, TU-LANGUAGE made to Bilkent University.
A portionof this work was done while the author was visit-ing Computing Research Laboratory at New MexicoState University.
The author thanks Lauri Kart-tunen of Xerox Research Centre Europe, Grenoblefor making available XRCE Finite State Tools.Re ferencesSteven Abney.
1996.
Partial parsing via finite statecascades.
In Proceedings of the ESSLLI'96 RobustParsing Workshop.Salah Ait-Mokhtar and Jean-Pierre Chanod.
1997.Incremental finite-state parsing.
In Proceedings ofANLP'97, pages 72 - 79, April.Ciprian Chelba and et al 1997.
Structure and esti-mation of a dependency language model.
In Pro-cessings of Eurospeech '97.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Pro-ceedings of the 16th International Conference onComputational Linguistics (COLING-96), pages340-345, August.259S .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.c .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
C s s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Rc - - -C  c c - - -CC s s - - -S  m s - - -NR p -dUnya banka  tUrk iye  d i rektOr  hUkUmet  i z le  ekonomik  programNoun Noun Noun Noun Noun Verb AdS AdS@ NounA3sg A3sg Prop A3sg A3sg Pos PastPar t  A3sgPnon P3sg A3sg P3sg Pnon P3sg@ PnonNos@ Sos@ Pnon Nom@ Gen@ Gen@Nom@.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
S1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
L S- -P  1 m---R s - - -SL  o - - -0  SsonuC Ones adIs at sOyleNoun Noun Adj Noun Verb Verb Noun VerbA3sg With@ A3pI Pass PastPart PosP3sg Pnon Pos A3sg PastLoc@ Gen@ P3sg A3sg@Acc@Figure 5: Dependency tree for the second parseGregory Grefenstette.
1996.
Light parsing as finite-state filtering.
In ECAI '96 Workshop on Ex-tended finite state models of language.
August.Timo J~irvinen and Pasi Tapanainen.
1998.
Towardsan implementable dependency grammar.
In Pro-ceedings of COLING/ACL'98 Workshop on Pro-cessing Dependency-based Grammars, pages 1-10.Ronald M. Kaplan and Martin Kay.
1994.
Regularmodels of phonological rule systems.
Computa-tional Linguistics, 20(3):331-378, September.Lauri Karttunen, Jean-Pierre Chanod, GregoryGrefenstette, and Anne Schiller.
1996.
Regu-lar expressions for language ngineering.
NaturalLanguage Engineering, 2(4):305-328.Lauri Karttunen.
1998.
The proper treatment ofoptimality theory in computational linguistics.
InLauri Karttunen and Kemal Oflazer, editors, Pro-ceedings of the International Workshop on FiniteState Methods in Natural Language Processing-FSMNLP, June.Kimmo Koskenniemi, Pasi Tapanainen, and AtroVoutilainen.
1992.
Compiling and using finite-state syntactic rules.
In Proceedings of the 14thInternational Conference on Computational Lin-guistics, COLING-92, pages 156-162.Kimmo Koskenniemi.
1990.
Finite-state parsingand disambiguation.
In Proceedings of the 13thInternational Conference on Computational Lin-guistics, COLING'90, pages 229 - 233.John Lafferty, Daniel Sleator, and Davy Temper-ley.
1992.
Grammatical trigrams: A probabilis-tic model of link grammars.
In Proceedings of the1992 AAAI Fall Symposium on Probablistic Ap-proaches to Natural Language.Bong Yeung Tom Lai and Changning Huang.
1994.Dependency grammar and the parsing of Chinesesentences.
In Proceedings of the 1994 Joint Con-ference of 8th ACLIC and 2nd PaFoCol.Dekang Lin.
1996.
On the structural complexity ofnatural anguage sentences.
In Proceedings of the16th International Conference on ComputationalLinguistics (COLING-96).Igor A. Mel~uk.
1988.
Dependency Syntax: Theoryand Practice.
State University of New York Press.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
1998.
A rational design for a weighted finite-state transducer library.
In Lecture Notes in Com-puter Science, 1.~36.
Springer Verlag.Kemal Oflazer.
1993.
Two-level description of Turk-ish morphology.
In Proceedings of the Sixth Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, April.
A fullversion appears in Literary and Linguistic Com-puting, Vol.9 No.2, 1994.Jane J. Robinson.
1970.
Dependency structures andtransformational rules.
Language, 46(2):259-284.Emmanuel Roche.
1997.
Parsing with finite statetransducers.
In Emmanuel Roche and Yves Sch-abes, editors, Finite-State Language Processing,chapter 8.
The MIT Press.Daniel Sleator and Davy Temperley.
1991.
ParsingEnglish with a link grammar.
Technical ReportCMU-CS-91-196, Computer Science Department,Carnegie Mellon University.Pasi Tapanainen and Timo J~rvinen.
1997.
A non-projective dependency parser.
In Proceedings ofANLP'97, pages 64 - 71, April.Deniz Y/iret.
1998.
Discovery of Linguistic Rela-tions Using Lexical Attraction.
Ph.D. thesis, De-partment of Electrical Engineering and ComputerScience, Massachusetts Institute of Technology.260
