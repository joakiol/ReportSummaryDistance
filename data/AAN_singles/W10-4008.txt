Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 43?51,Beijing, August 2010A Voting Mechanism for Named Entity Translation in English?Chinese Question AnsweringLing-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu11Faculty of Science and TechnologyQueensland University of Technology{l4.tang,s.geva,yue.xu}@qut.edu.au2Department of Computer ScienceUniversity of Otagoandrew@cs.otago.ac.nzAbstractIn this paper, we describe a votingmechanism for accurate named entity(NE) translation in English?Chinesequestion answering (QA).
This mecha-nism involves translations from threedifferent sources: machine translation,online encyclopaedia, and web docu-ments.
The translation with the highestnumber of votes is selected.
We evalu-ated this approach using test collection,topics and assessment results from theNTCIR-8 evaluation forum.
Thismechanism achieved 95% accuracy inNEs translation and 0.3756 MAP inEnglish?Chinese cross-lingual infor-mation retrieval of QA.1  IntroductionNowadays, it is easy for people to accessmulti-lingual information on the Internet.
Keyterm searching on an information retrieval (IR)system is common for information lookup.However, when people try to look for answersin a different language, it is more natural andcomfortable for them to provide the IR systemwith questions in their own natural languages(e.g.
looking for a Chinese answer with anEnglish question: ?what is Taiji??).
Cross-lingual question answering (CLQA) tries tosatisfy such needs by directly finding the cor-rect answer for the question in a different lan-guage.In order to return a cross-lingual answer, aCLQA system needs to understand the ques-tion, choose proper query terms, and then ex-tract correct answers.
Cross-lingual informa-tion retrieval (CLIR) plays a very importantrole in this process because the relevancy ofretrieved documents (or passages) affects theaccuracy of the answers.A simple approach to achieving CLIR is totranslate the query into the language of the tar-get documents and then to use a monolingualIR system to locate the relevant ones.
How-ever, it is essential but difficult to translate thequestion correctly.
Currently, machine transla-tion (MT) can achieve very high accuracywhen translating general text.
However, thecomplex phrases and possible ambiguities pre-sent in a question challenge general purposeMT approaches.
Out-of-vocabulary (OOV)terms are particularly problematic.
So the keyfor successful CLQA is being able to correctlytranslate all terms in the question, especiallythe OOV phrases.In this paper, we discuss an approach foraccurate question translation that targets theOOV phrases and uses a translation votingmechanism.
This mechanism involves transla-tions from three different sources: machinetranslation, online encyclopaedia, and webdocuments.
The translation with the highestnumber of votes is selected.
To demonstratethis mechanism, we use Google Translate43(GT)1 as the MT source, Wikipedia as the en-cyclopaedia source, and Google web searchengine to retrieve Wikipedia links and relevantWeb document snippets.English questions on the Chinese corpus forCLQA are used to illustrate of this approach.Finally, the approach is examined and evalu-ated in terms of translation accuracy and re-sulting CLIR performance using the test col-lection, topics and assessment results fromNTCIR-82.English Question Templates (QTs)who [is | was | were | will], what is the definition of,what is the [relationship | interrelationship | inter-relationship]  [of | between], what links are there,what link is there, what [is | was | are | were | does |happened], when [is | was | were |  will | did | do],where [will | is | are | were], how [is | was | were |did], why [does | is | was | do | did | were | can |had], which [is | was | year], please list, describe[relationship | interrelationship | inter-relationship][of | between], could you [please | EMPTY] giveshort description[s] to, who, where, what, which,how, describe, explainChinese QT Counterparts??????
?, ?????
?, ?????
?, ?????
?, ?????,????
?, ????
?, ?????,????
?, ????
?, ???
?, ????,????,???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ???
?, ????,???
?, ???
?, ???
?, ???
?, ???
?, ???,??
?, ??
?, ??
?, ??
?, ???,??
?, ??
?, ??
?, ???,??
?, ??
?, ???,???,??
?, ???,???,???,??,?
?, ??,?
?, ?
?, ?
?, ?
?, ??,?
?, ?
?, ?
?, ?
?, ?
?, ?
?, ?
?, ?
?, ??,?
?, ?
?Table 1.
Question templates2 CLIR Issue and Related WorkIn CLIR, retrieving documents with a cross-lingual query with out-of-vocabulary phraseshas always been difficult.
To resolve this prob-lem, an external resource such as Web orWikipedia is often used to discover the possi-ble translation for the OOV term.
Wikipediaand other Web documents are thought of astreasure troves for OOV problem solving be-cause they potentially cover the most recentOOV terms.1 http://translate.google.com.2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html.The Web-based translation method wasshown to be an effective way to solve the OOVphrase problem (Chen et al, 2000; Lu et al,2007; Zhang & Vines, 2004; Zhang et al,2005).
The idea behind this method is that aterm/phrase and its corresponding translationnormally co-exist in the same document be-cause authors often provide the new terms?translation for easy reading.In Wikipedia the language links providedfor each entry cover most popular written lan-guages, therefore, it was used to solve a lowcoverage issue on named entities in Eu-roWordNet (Ferr?ndez et al, 2007); a numberof research groups (Chan et al, 2007; Shi etal., 2008; Su et al, 2007; Tatsunori Mori,2007) employed Wikipedia to tackle OOVproblems in the NTCIR evaluation forum.3 CLQA Question AnalysisQuestions for CLQA can be very complex.
Forexample, ?What is the relationship between themovie "Riding Alone for Thousands of Miles"and ZHANG Yimou??.
In this example, it isimportant to recognise two named entities("Riding Alone for Thousands of Miles" and?ZHANG Yimou?)
and to translate them pre-cisely.In order to recognise the NEs in the ques-tion, first, English question template phrases inTable 1 are removed from question; next, weuse the Stanford NLP POS tagger (The Stan-ford Natural Language Processing Group,2010) to identify the named entities; thentranslate them accordingly.
Chinese questiontemplate phrases are also pruned from thetranslated question at the end to reduce thenoise words in the final query.There are three scenarios in which a term orphrase is considered a named entity.
First, it isconsecutively labelled NNP or NNPS (Univer-sity of Pennsylvania, 2010).
Second, term(s)are grouped by quotation marks.
For example,to extract a named entity from the examplequestion above, three steps are needed:1.
Remove the question template phrase?What is the relationship between?
fromthe question.2.
Process the remaining using the POS tag-ger, giving ?the_DT movie_NN ``_`` Rid-ing_NNP Alone_NNP for_IN Thou-44sands_NNS of_IN Miles_NNP``_``and_CC ZHANG_NNP Yimou_NNP?_.?3.
?Riding Alone for Thousands of Miles?
isbetween two tags (``) and so is an entity,and the phrase ?ZHANG Yimou?, as indi-cated by two consecutive NNP tags is alsoa named entity.Third, if a named entity recognised in the twoscenarios above is followed in the question bya phrase enclosed in bracket pairs, this phrasewill be used as a tip term providing additionalinformation about this named entity.
For in-stance, in the question ?Who is David Ho (Da-iHo)?
?, ?Da-i Ho?
is the tip term of the namedentity ?David Ho?.4 A Voting Mechanism for NamedEntity Translation (VMNET)Observations have been made:?
Wikipedia has over 100,000 Chinese en-tries describing various up-to-date events,people, organizations, locations, and facts.Most importantly, there are links betweenEnglish articles and their Chinese counter-parts.?
When people post information on theInternet, they often provide a translation(where necessary) in the same document.These pages contain bilingual phrase pairs.For example, if an English term/phrase isused in a Chinese article, it is often fol-lowed by its Chinese translation enclosedin parentheses.?
A web search engine such as Google canidentify Wikipedia entries, and returnpopular bi-lingual web document snippetsthat are closely related to the query.?
Statistical machine translation relying onparallel corpus such as Google Translatecan achieve very high translation accuracy.Given these observations, there could be upto three different sources from which we canobtain translations for a named entity; the taskis to find the best one.4.1 VMNET AlgorithmA Google search on the extracted named entityis performed to return related Wikipedia linksand bilingual web document snippets.
Thenfrom the results of Web search and MT, threedifferent translations could be acquired.Wikipedia TranslationThe Chinese equivalent Wikipedia pagescould be found by following the language linksin English pages.
The title of the discoveredChinese Wikipedia page is then used as theWikipedia translation.Bilingual Clue Text TranslationThe Chinese text contained in the snippetsreturned by the search engine is processed forbilingual clue text translation.
The phrase in adifferent language enclosed in parentheseswhich come directly after the named entity isused as a candidate translation.
For example,from a web document snippet, ?YouTube -Sean Chen (???)
dunks on Yao Ming?
?, ?????
can be extracted and used as a candi-date translation of ?Sean Chen?, who is a bas-ket ball player from Taiwan.Machine TranslationIn the meantime, translations for the namedentity and its tip term (if there is one) are alsoretrieved using Google Translate.Regarding the translation using Wikipedia,the number of results could be more than onebecause of ambiguity.
So for a given namedentity, we could have at least one, but possiblymore than three candidate translations.With all possible candidate translations, thebest one then can be selected.
Translationsfrom all three sources are equally weighted.Each translation contributes one vote, and thevotes for identical translation are cumulated.The best translation is the one with the highestnumber of votes.
In the case of a tie, the firstchoice of the best translation is the Wikipediatranslation if only one Wiki-entry is found;otherwise, the priority for choosing the best isbilingual clue text translation, then machinetranslation.4.2 Query Generation with VMNETBecause terms can have multiple meanings,ambiguity often occurs if only a single term isgiven in machine translation.
A state-of-the-artMT toolkit/service could perform better ifmore contextual information is provided.
So abetter translation is possible if the whole sen-tence is given (e.g.
the question).
For this rea-45son, the machine translation of the question isthe whole query and not with the templatesremoved.However, issues arise: 1) how do we knowif all the named entities in question are trans-lated correctly?
2) if there is an error in namedentity translation, how can it be fixed?
Particu-larly for case 2, the translation for the wholequestion is considered acceptable, except forthe named entity translation part.
We intend tokeep most of the translation and replace thebad named entity translation with the goodone.
But finding the incorrect named entitytranslation is difficult because the translationfor a named entity can be different in differentcontexts.
The missing boundaries in Chinesesentences make the problem harder.
To solvethis, when a translation error is detected, thequestion is reformatted by replacing all thenamed entities with some nonsense stringscontaining special characters as place holders.These place holders remain unchanged duringthe translation process.
The good NE transla-tions then can be put back for the nearly trans-lated question.Given an English question Q, the detailedsteps for the Chinese query generation are asfollowing:1.
Retrieve machine translation Tmt for thewhole question from Google Translate.2.
Remove question template phrase fromquestion.3.
Process the remaining using the POS tag-ger.4.
Extract the named entities from the taggedwords using the method discussed in Sec-tion 3.5.
Replace each named entity in question Qwith a special string Si,(i =0,1,2,..) whichmakes nonsense in translation and isformed by a few non-alphabet characters.In our experiments, Si is created by joininga double quote character with a ^ characterand the named entity id (a number, startingfrom 0, then increasing by 1 in order ofoccurrence of the named entity) followedby another double quote character.
The fi-nal Si, becomes ?^id?.
The resulting ques-tion is used as Qs.6.
Retrieve machine translation Tqs for Qsfrom Google Translate.
Since Si consistsof special characters, it remains unchangedin Tqs.7.
Start the VMNET loop for each namedentity.8.
With an option set to return both Englishand Chinese results, Google the named en-tity and its tip term (if there is one).9.
If there are any English Wikipedia links inthe top 10 search results, then retrievethem all.
Else, jump to step 12.10.
Retrieve all the corresponding ChineseWikipedia articles by following the lan-guages links in the English pages.
If none,then jump to step 12.11.
Save the title NETwiki(i) of each ChineseWikipedia article  Wiki(i).12.
Process the search results again to locate abilingual clue text translation candidate -NETct, as discussed in Section 4.1.13.
Retrieve machine translation NETmt, andNETtip for this named entity and its tip term(if there is one).14.
Gather all candidate translations: NET-wiki(*), NETct, NETtip, and NETmt  for vot-ing.
The translation with the highest num-ber of votes is considered the best(NETbest).
If there is a tie, NETbest is thenassigned the translation with the highestpriority.
The priority order of candidatetranslation is NETwiki(0) (ifsizeof(NETwiki(*))=1)  >  NETct  > NETmt.
Itmeans when a tie occurs and if there aremore than one Wikipedia translation, allthe Wikipedia translations are skipped.15.
If Tmt does not contain NETbest, it is thenconsidered a faulty translation.16.
Replace Si  in Tqs with NETbest.17.
If NETbest is different from any NETwiki(i)but can be found in the content of aWikipedia article (Wiki(i)), then the corre-sponding NETwiki(i)  is used as an addi-tional query term, and appended to the fi-nal Chinese query.18.
Continue the VMNET loop and jump backto step 8 until no more named entities re-main in the question.19.
If Tmt was considered a faulty translation,use Tqs as the final translation of Q. Other-wise, just use Tmt.
The Chinese questiontemplate phrases are pruned from thetranslation for the final query generation.46A short question translation example isgiven below:?
For the question ?What is the relationshipbetween the movie "Riding Alone forThousands of Miles" and ZHANG Yi-mou?
?, retrieving its Chinese translationfrom a MT service, we get the following:???????????????????????.?
The translation for the movie name "Rid-ing Alone for Thousands of Miles" of?ZHANG Yimou?
is however incorrect.?
Since the question is also reformatted into?What is the relationship between themovie "^0" and ?^1??
?, machine transla-tion returns a second translation:  ??????????
?^ 0??
?^ 1???
VMNET obtains the correct translations:?????
and ??
?, for two named en-tities "Riding Alone for Thousands ofMiles" and ?ZHANG Yimou?
respectively.?
Replace the place holders with the correcttranslations in the second translation andgive the final Chinese translation: ???????????????????????
?5 Information Retrieval5.1 Chinese Document ProcessingApproaches to Chinese text indexing vary:Unigrams, bigrams and whole words are allcommonly used as tokens.
The performance ofvarious IR systems using different segmenta-tion algorithms or techniques varies as well(Chen et al, 1997; Robert & Kwok, 2002).
Itwas seen in prior experiments that using anindexing technique requiring no dictionary canhave similar performance to word-based index-ing (Chen, et al, 1997).
Using bigrams thatexhibit high mutual information and unigramsas index terms can achieve good results.
Moti-vated by indexing efficiency and without theneed for Chinese text segmentation, we useboth bigrams and unigrams as indexing unitsfor our Chinese IR experiments.5.2 Weighting ModelA slightly modified BM25 ranking functionwas used for document ordering.When calculating the inverse document fre-quency, we use:(1)where N is the number of documents in thecorpus, and n is the document frequency ofquery term  .
The retrieval status value of adocument d with respect to queryis given as:?
(2)where          is the term frequency of termin document d;        is the length ofdocument d in words and avgdl is the meandocument length.
The number of bigrams isincluded in the document length.
The values ofthe tuneable parameters    and b used in ourexperiments are 0.7 and 0.3 respectively.6 CLIR Experiment6.1 Test Collection and TopicsTable 2 gives the statistics of the test collectionand the topics used in our experiments.
Thecollection contains 308,845 documents in sim-plified Chinese from Xinhua News.
There arein total 100 topics consisting of both Englishand Chinese questions.
This is a NTCIR-8 col-lection for ACLIA task.Corpus #docs #topicsXinhua Chinese (simplified) 308,845 100Table 2.
Statistics of test corpus and topics6.2 Evaluation MeasuresThe evaluation of VMNET performance cov-ers two main aspects: translation accuracy andCLIR performance.As we focus on named entity translation, thetranslation accuracy is measured using the pre-cision of translated named entities at the topiclevel.
So the translation precision -P is definedas:(3)where c is the number of topics in which allthe named entities are correctly translated; N isthe number of topics evaluated.47The effectiveness of different translationmethods can be further measured by the result-ing CLIR performance.
In NTCIR-8, CLIRperformance is measured using the mean aver-age precision.
The MAP values are obtainedby running the ir4qa_eval2 toolkit with theassessment results 3  on experimental runs(NTCIR Project, 2010).
MAP is computedusing only 73 topics due to an insufficientnumber of relevant document found for theother 27 topics (Sakai et al, 2010).
This is thecase for all NTCIR-8 ACLIA submissions andnot our decision.It also must be noted that there are five top-ics that have misspelled terms in their Englishquestions.
The misspelled terms in those 5 top-ics are given in Table 3.
It is interesting to seehow different translations cope with misspelledterms and how this affects the CLIR result.Topic ID Misspelling CorrectionACLIA2-CS-0024 Qingling QinlingACLIA2-CS-0035 Initials D Initial DACLIA2-CS-0066 Kasianov KasyanovACLIA2-CS-0074NorthernTerritoriesnorthernterritoriesACLIA2-CS-0075 Kashimir KashmirTable 3.
The misspelled terms in topics6.3 CLIR Experiment runsA few experimental runs were created forVMNET and CLIR system performanceevaluation.
Their details are listed in Table 7.Those with name *CS-CS* are the Chinesemonolingual IR runs; and those with the name*EN-CS* are the English-to-Chinese CLIRruns.
Mono-lingual IR runs are used forbenchmarking our CLIR system performance.7 Results and Discussion7.1 Translation EvaluationThe translations in our experiments usingGoogle Translate reflect only the results re-trieved at the time of the experiments becauseGoogle Translate is believed to be improvedover time.The result of the final translation evaluationon the 100 topics is given in Table 4.
GoogleTranslate had difficulties in 13 topics.
If all3 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html.thirteen named entities in those topics whereGoogle Translate failed are considered OOVterms, the portion of topics with OOV phrasesis relatively small.
Regardless, there is an 8%improvement achieved by VMNET reaching95% precision.Method c N PGoogle Translate 87 100 87%VMNET 95 100 95%Table 4.
Translation Evaluation ResultsThere are in total 14 topics in which GoogleTranslate or VMNET failed to correctly trans-late all named entities.
These topics are listedin Table 8.
Interestingly, for topic (ACLIA2-CS-0066) with the misspelled term?Kasianov?, VMNET still managed to find acorrect translation (?????????????????).
This has to be attributed tothe search engine?s capability in handling mis-spellings.
On the other hand, Google Translatewas correct in its translation of ?Northern Ter-ritories?
of Japan, but VMNET incorrectlychose ?Northern Territory?
(of Australia).
Forthe rest of the misspelled phrases (Qingling,Initials D, Kashimir), neither Google Translatenor VMNET could pick the correct translation.7.2 IR EvaluationThe MAP values of all experimental runs cor-responding to each query processing techniqueand Chinese indexing strategy are given in Ta-ble 5.
The results of mono-lingual runs givebenchmarking scores for CLIR runs.As expected, the highest MAP 0.4681 isachieved by the monolingual run VMNET-CS-CS-01-T, in which the questions were manu-ally segmented and all the noise words wereremoved.It is encouraging to see that the automaticrun VMNET-CS-CS-02-T with only questiontemplate phrase removal has a slightly lowerMAP 0.4419 than that (0.4488) of the best per-formance CS-CS run in the NTCIR-8 evalua-tion forum (Sakai, et al, 2010).If unigrams were used as the only indexingunits, the MAP of VMNET-CS-CS-04-Tdropped from 0.4681 to 0.3406.
On the otherhand, all runs using bigrams as indexing unitseither exclusively or jointly performed verywell.
The MAP of run VMNET-CS-CS-05-Tusing bigrams only is 0.4653, which is slightly48lower than that of the top performer runVMNET-CS-CS-01-T, which used two formsof indexing units.
However, retrieval perform-ance could be maximised by using both uni-grams and bigrams as indexing units.The highest MAP (0.3756) of a CLIR run isachieved by run VMNET-EN-CS-03-T, whichused VMNET for translation.
Comparing it toour manual run VMNET-CS-CS-01-T, there isaround 9% performance degradation as a resultof the influence of noise words in the ques-tions, and the possible information loss oradded noise due to English-to-Chinese transla-tion, even though the named entities translationprecision is relatively high.The best EN-CS CLIR run (MAP 0.4209)in all submissions to the NTCIR-8 ACLIA taskused the same indexing technique (bigramsand unigrams) and ranking function (BM25) asrun VMNET-EN-CS-03-T but with ?queryexpansion based on RSV?
(Sakai, et al, 2010).The MAP difference 4.5% between the forumbest run and our CLIR best run could suggestthat using query expansion is an effective wayto improve the CLIR system performance.Runs VMNET-EN-CS-01-T and VMNET-EN-CS-04-T, that both used Google Translateprovide direct comparisons with runsVMNET-EN-CS-02-T and VMNET-EN-CS-03-T, respectively, which employed VMNETfor translation.
All runs using VMNET per-formed better than the runs using GoogleTranslate.Run Name MAPNTCIR-8 CS-CS BEST 0.4488VMNET-CS-CS-01-T 0.4681VMNET-CS-CS-02-T 0.4419VMNET-CS-CS-03-T 0.4189VMNET-CS-CS-04-T 0.3406VMNET-CS-CS-05-T 0.4653NTCIR-8 EN-CS BEST 0.4209VMNET-EN-CS-01-T 0.3161VMNET-EN-CS-02-T 0.3408VMNET-EN-CS-03-T 0.3756VMNET-EN-CS-04-T 0.3449Table 5.
Results of all experimental runsThe different performances between CLIRruns using Google Translate and VMENT isthe joint result of the translation improvementand other translation differences.
As shown inTable 8, VMNET found the correct transla-tions for 8 more topics than Google Translate.It should be noted that there are two topics(ACLIA2-CS-0008 and ACLIA2-CS-0088)not included in the final CLIR evaluation (Sa-kai, et al, 2010).
Also, there is one phrase,?Kenneth Yen (K. T. Yen) (???
)?, whichVMNET couldn?t find the correct translationfor, but it detected a highly associated term?Yulon - ????
?, an automaker company inTaiwan; Kenneth Yen is the CEO of Yulon.Although Yulon is not a correct translation, it isstill a good query term because it is then possi-ble to find the correct answer for the question:?Who is Kenneth Yen??.
However, this topicwas not included in the NTCIR-8 IR4QAevaluation.Moreover, it is possible to have multipleexplanations for a term.
In order to discover asmany question-related documents as possible,alternative translations found by VMNET arealso used as additional query terms.
They areshown in Table 6.
For example, ??
is theChinese term for DINK in Mainland China,but ???
is used in Taiwan.
Furthermore,because VMNET gives the Wikipedia transla-tion the highest priority if only one entry isfound, a person?s full name is used in personname translation rather than the short com-monly used name.
For example, Cheney (for-mer vice president of U.S.) is translated into?????
rather than just?
?.NE VMNET Wiki TitlePrincess Nori ????
???
?DINK ??
??
?BSE ???
?????
?Three Gorges Dam ????
???
?Table 6.
Alternative translationsThe biggest difference, 3.07%, betweenruns that used different translation is from runsVMNET-EN-CS-03-T and VMNET-EN-CS-04-T, which both pruned the question templatephrase for simple query processing.
Althoughthe performance improvement is not obvious,the correct translations and the additionalquery terms found by VMNET are still veryvaluable.8 ConclusionsGeneral machine translation can alreadyachieve very good translation results, but withour proposed approach we can further improvethe translation accuracy.
With a proper adjust-49ment of this approach, it could be used in asituation where there is a need for higher pre-cision of complex phrase translation.The results from our CLIR experiments in-dicate that VMNET is also capable of provid-ing high quality query terms.
A CLIR systemcan achieve good results for answer finding byusing the VMNET for translation, simple in-dexing technique (bigrams and unigrams), andplain question template phrase pruning.Run Name IndexingUnitsQuery ProcessingVMNET-CS-CS-01-T U + B Manually segment the question and remove all the noise wordsVMNET-CS-CS-02-T U + B Prune the question template phraseVMNET-CS-CS-03-T U + B Use the whole question without doing any extra processing workVMNET-CS-CS-04-T U As VMNET-CS-CS-01-TVMNET-CS-CS-05-T B As VMNET-CS-CS-01-TVMNET-EN-CS-01-T U + B Use Google Translate on the whole question and use the entire translationas queryVMNET-EN-CS-02-T U + B Use VMNET translation result without doing any further processingVMNET-EN-CS-03-T U + B As above, but prune the Chinese question template from translationVMNET-EN-CS-04-T U + B Use Google Translate  on  the whole question and prune the Chinese ques-tion template phrase from the translationTable 7.
The experimental runs.
For indexing units, U means unigrams; B means bigrams.Topic ID Question with OOV Phrases  Correct  GT VMNETACLIA2-CS-0002 What is the relationship between the movie"Riding Alone for Thousands of Miles"and ZHANG Yimou?????????????
????
?ACLIA2-CS-0008 Who is LI Yuchun?
???
???
??
?ACLIA2-CS-0024 Why does Qingling build "panda corridorzone"??
???
??
?ACLIA2-CS-0035 Please list the events related to the movie"Initials D".???
D ??
D???
??
D??
?ACLIA2-CS-0036 Please list the movies in which Zhao Weiparticipated.??
??
?
?ACLIA2-CS-0038 What is the relationship between Xia Yuand Yuan Quan.??
???
?
?ACLIA2-CS-0048 Who is Sean Chen(Chen Shin-An)?
???
????????
??
?ACLIA2-CS-0049 Who is Lung Yingtai?
???
???
??
?ACLIA2-CS-0057 What is the disputes between China andJapan for the undersea natural gas field inthe East China Sea???
?????
?
?ACLIA2-CS-0066 What is the relationship between two Rus-sian politicians, Kasianov and Putin?????
?Kasianov ????????????????
?ACLIA2-CS-0074 Where are Japan's Northern Territorieslocated?????
????
??
?ACLIA2-CS-0075 Which countries have borders in the Ka-shimir region?????
Kashimir KashimirACLIA2-CS-0088 What is the relationship between theGolden Globe Awards and Broken-backMountain????
?????
??
?ACLIA2-CS-0089 What is the relationship between KennethYen(K. T. Yen) and China????
??????????????
?Table 8.
The differences between Google Translate and VMNET translation of OOVphrases in which GT or VMNET was wrong.50ReferencesChan, Y.-C., Chen, K.-H., & Lu, W.-H. (2007).Extracting and Ranking Question-FocusedTerms Using the Titles of Wikipedia Articles.Paper presented at the NTCIR-6.Chen, A., He, J., Xu, L., Gey, F. C., & Meggs, J.
(1997, 1997).
Chinese text retrieval withoutusing a dictionary.
Paper presented at the SIGIR'97: Proceedings of the 20th annual internationalACM SIGIR conference on Research anddevelopment in information retrieval.Chen, A., Jiang, H., & Gey, F. (2000).
Combiningmultiple sources for short query translation inChinese-English cross-language informationretrieval.
17-23.Ferr?ndez, S., Toral, A., Ferr?ndez, ?., Ferr?ndez,A., & Mu?oz, R. (2007).
Applying Wikipedia?sMultilingual Knowledge to Cross?LingualQuestion Answering Natural LanguageProcessing and Information Systems (pp.
352-363).Lu, C., Xu, Y., & Geva, S. (2007).
Translationdisambiguation in web-based translationextraction for English?Chinese CLIR.
819-823.NTCIR Project.
(2010).
Tools.
fromhttp://research.nii.ac.jp/ntcir/tools/tools-en.htmlRobert, W. P. L., & Kwok, K. L. (2002).
Acomparison of Chinese document indexingstrategies and retrieval models.
ACMTransactions on Asian Language InformationProcessing (TALIP), 1(3), 225-268.Sakai, T., Shima, H., Kando, N., Song, R., Lin, C.-J., Mitamura, T., et al (2010).
Overview ofNTCIR-8 ACLIA IR4QA.
Paper presented at theProceedings of NTCIR-8, to appear.Shi, L., Nie, J.-Y., & Cao, G. (2008).
RALIExperiments in IR4QA at NTCIR-7.
Paperpresented at the NTCIR-7.Su, C.-Y., Lin, T.-C., & Wu, S.-H. (2007).
UsingWikipedia to Translate OOV Terms on MLIR.Paper presented at the NTCIR-6.Tatsunori Mori, K. T. (2007).
A method of Cross-Lingual Question-Answering Based on MachineTranslation and Noun Phrase Translation usingWeb documents.
Paper presented at the NTCIR-6.The Stanford Natural Language Processing Group.(2010).
Stanford Log-linear Part-Of-SpeechTagger.
fromhttp://nlp.stanford.edu/software/tagger.shtmlUniversity of Pennsylvania.
(2010).
POS tags.
fromhttp://bioie.ldc.upenn.edu/wiki/index.php/POS_tagsZhang, Y., & Vines, P. (2004).
Using the web forautomated translation extraction in cross-language information retrieval.
Paper presentedat the Proceedings of the 27th annualinternational ACM SIGIR conference onResearch and development in informationretrieval.Zhang, Y., Vines, P., & Zobel, J.
(2005).
ChineseOOV translation and post-translation queryexpansion in Chinese?English cross-lingualinformation retrieval.
ACM Transactions onAsian Language Information Processing(TALIP), 4(2), 57-77.51
