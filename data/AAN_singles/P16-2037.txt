Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 225?230,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsDimensional Sentiment Analysis Using a Regional CNN-LSTM ModelJin Wang1,3,4, Liang-Chih Yu2,4, K. Robert Lai3,4 and Xuejie Zhang11School of Information Science and Engineering, Yunnan University, Yunnan, P.R.
China2Department of Information Management, Yuan Ze University, Taiwan3Department of Computer Science & Engineering, Yuan Ze University, Taiwan4Innovation Center for Big Data and Digital Convergence Yuan Ze University, TaiwanContact: lcyu@saturn.yzu.edu.twAbstractDimensional sentiment analysis aims torecognize continuous numerical values inmultiple dimensions such as the valence-arousal (VA) space.
Compared to the cate-gorical approach that focuses on sentimentclassification such as binary classification(i.e., positive and negative), the dimensionalapproach can provide more fine-grainedsentiment analysis.
This study proposes aregional CNN-LSTM model consisting oftwo parts: regional CNN and LSTM to pre-dict the VA ratings of texts.
Unlike a con-ventional CNN which considers a wholetext as input, the proposed regional CNNuses an individual sentence as a region, di-viding an input text into several regionssuch that the useful affective information ineach region can be extracted and weightedaccording to their contribution to the VAprediction.
Such regional information is se-quentially integrated across regions usingLSTM for VA prediction.
By combining theregional CNN and LSTM, both local (re-gional) information within sentences andlong-distance dependency across sentencescan be considered in the prediction process.Experimental results show that the proposedmethod outperforms lexicon-based, regres-sion-based, and NN-based methods pro-posed in previous studies.1 IntroductionSentiment analysis has been useful in the devel-opment of online applications for customer re-views and public opinion analysis (Pang and Lee2008; Calvo and D'Mello 2010; Liu 2012; Feld-man 2013).
In sentiment representation, the cate-gorical approach represents emotional states asseveral discrete classes such as binary (i.e., posi-tive and negative) or as multiple categories suchas Ekman?s (1992) six basic emotions (anger,happiness, fear, sadness, disgust, and surprise).Classification algorithms can then be used toidentify sentiment categories from texts.The dimensional approach represents emo-tional states as continuous numerical values inmultiple dimensions such as the valence-arousal(VA) space (Russell, 1980).
The dimension ofvalence refers to the degree of positive and nega-tive sentiment, whereas the dimension of arousalrefers to the degree of calm and excitement.
Bothdimensions range from 1 (highly negative orcalm) to 9 (highly positive or excited) based onthe self-assessment manikin (SAM) annotationscheme (Bradley et al 1994).
For example, thefollowing passage consisting of three sentencesis associated with a valence-arousal rating of (2.5,7.8), which displays a high degree of negativityand arousal.
(r1) A few days ago I checked into a franchisehotel.
(r2) The front desk service was terrible, andthey didn?t know much about local attrac-tions.
(r3) I would not recommend this hotel to afriend.Such high-arousal negative (or high-arousal posi-tive) texts are usually of interest and could priori-tized in product review systems.
Dimensionalsentiment analysis can accomplish this by recog-nizing the VA ratings of texts and rank them ac-cordingly, thus providing more intelligent and fi-ne-grained sentiment applications.225Region rk(xrk)Text vector?
?Linear decoder?
?Convolutional layer Max pooling layer???L??L?
?L?1irwirIw2jrwjrJw1krw2krwkrKwLSTMLSTMLSTMVASequential layer??1jrw?
?2irwRegion ri(xri)Word vectorText vectorRegion rj(xrj)Figure 1: System architecture of the proposed regional CNN-LSTM model.Research on dimensional sentiment analysishas addressed VA recognition at both the word-level (Wei et al, 2011; Malandrakis et al, 2011;Yu et al, 2015) and the sentence-level (Paltoglouet al, 2013; Malandrakis et al, 2013).
At theword-level, Wei et al (2011) used linear regres-sion to transfer VA ratings from English affec-tive words to Chinese words.
Malandrakis et al(2011) used a kernel function to combine thesimilarity between words for VA prediction.
Yuet al (2015) used a weighted graph model to it-eratively determine the VA ratings of affectivewords.
At the sentence level, Paltoglou et al(2013) adopted a lexicon-based method to calcu-late the VA ratings of texts by averaging the VAratings of affective words in the texts using aweighted arithmetic/geometric mean.Malandrakis et al (2013) proposed a regressionmethod that extracted n-gram with affective rat-ings as features to predict VA values for texts.Recently, word embedding (Mikolov et al,2013a; Mikolov et al, 2013b) and deep neuralnetworks (NN) such as convolutional neuralnetworks (CNN) (Kim, 2014; Kalchbrenner et al,2014), recurrent neural networks (RNN) (Graves,2012; Irsoy and Cardie, 2014) and long short-term memory (LSTM) (Wang et al, 2015; Liu etal., 2015) have been successfully employed forcategorical sentiment analysis.
In general, CNNis capable of extracting local information butmay fail to capture long-distance dependency.LSTM can address this limitation by sequentiallymodeling texts across sentences.
Such NN-basedand word embedding methods have not been wellexplored for dimensional sentiment analysis.This study proposes a regional CNN-LSTMmodel consisting of two parts, regional CNN andLSTM, to predict the VA ratings of texts.
Wefirst construct word vectors for vocabulary wordsusing word embedding.
The regional CNN isthen used to build text vectors for the given textsbeing predicted based on the word vectors.
Un-like a conventional CNN which considers awhole text as input, the proposed regional CNNuses individual sentences as regions, dividing aninput text into several regions such that the use-ful affective information in different regions canbe extracted and weighted according to their con-tribution to the VA prediction.
For example, inthe aforementioned example text, it would beuseful for the system to emphasize the two sen-tences/regions (r2) and (r3) containing negativeaffective information.
Finally, such regional in-formation is sequentially integrated across re-gions using LSTM for VA prediction.
By com-bining the regional CNN and LSTM, both local226(regional) information within sentences and long-distance dependency across sentences can beconsidered in the prediction process.The rest of this paper is organized as follows.Section 2 describes the proposed regional CNN-LSTM model.
Section 3 reports the evaluationresults of the proposed method against lexicon-based, regression-based, and NN-based methods.Conclusions are finally drawn in Section 4.2 Regional CNN-LSTM ModelFigure 1 shows the overall framework of theproposed regional CNN-LSTM model.
First, theword vectors of vocabulary words are trainedfrom a large corpus using the word2vec toolkit.For each given text, the regional CNN model us-es a sentence as a region to divide the given textinto R regions, i.e.
r1,?, ri, rj, rk,?, rR.
In eachregion, useful affective features can be extractedonce the word vectors sequentially pass througha convolutional layer and max pooling layer.Such local (regional) features are then sequen-tially integrated across regions using LSTM tobuild a text vector for VA prediction.2.1 Convolutional LayerIn each region, a convolutional layer is first usedto extract local n-gram features.
All word em-beddings are stacked in a region matrixd VM ???
, where |V| is the vocabulary size of aregion, and d is the dimensionality of word vec-tors.
For example, in Fig.1, the word vectors inthe regions ri={wri 1 , wri2 ,?,wriI }, rj={wrj1 ,wrj2 ,?, wrjJ }and rk={wrk 1 ,wrk2 ,?, wrkK } are combined to formthe region matrices xri, xrj, and xrk.
In each region,we use L convolutional filters to learn local n-gram features.
In a window of ?
words xn:n+?-1, afilter Fl (1?l?L) generates the feature map yl n asfollows,: 1( )l l ln n ny f W b?+ ?= +x?
(1)where ?
is a convolutional operator, dW ???
?and b respectively denote the weight matrix andbias, ?
is the length of the filter, d is the dimen-sion of the word vector, and f is the ReLU func-tion.
When a filter gradually traverses from x1:?-1to xN+?-1:N, we get the output feature maps1 2 1, , ,l l l lNy y y ??
+= ?y of filter Fl.
Given varyingtext lengths in the regions, yl may have differentdimensions for different texts.
Therefore, we de-fine the maximum length of the CNN input in thecorpora as the dimension N. If the input length isshorter than N, then several random vectors witha uniform distribution U(-0.25, 0.25) will be ap-pended.2.2 Max-pooling LayerMax-pooling subsamples the output of the con-volutional layer.
The most common way to dopooling it to apply a max operation to the resultof each filter.
There are two reasons to use amax-pooling layer here.
First, by eliminatingnon-maximal values, it reduces computation forupper layers.
Second, it can extract the local de-pendency within different regions to keep themost salient information.
The obtained regionvectors are then fed to a sequential layer.2.3 Sequential LayerTo capture long-distance dependency across re-gions, the sequential layer sequentially integrateseach region vector into a text vector.
Due to theproblem of gradients vanishing or exploding inRNN (Bengio et al, 1994), LSTM is introducedin the sequential layer for vector composition.After the LSTM memory cell sequentiallytraverses through all regions, the last hidden stateof the sequential layer is regarded as the text rep-resentation for VA prediction.2.4 Linear DecoderSince the values in both the valence and arousaldimensions are continuous, the VA predictiontask requires a regression.
Instead of using asoftmax classifier, a linear activation function(also known as a linear decoder) is used in theoutput layer, defined as,d t dy W b= +x   (2)where xt is the text vector learned from the se-quential layer, y is the degree of valence orarousal of the target text, and Wd and bd respec-tively denote the weight and bias associated withthe linear decoder.The regional CNN-LSTM model is trained byminimizing the mean squared error between thepredicted y and actual y.
Given a training set oftext matrix X={x(1), x(2),?, x(m)}, and their VAratings set y={y(1), y(2), ?, y(m)}, the loss functionis defined as2( ) ( )11( , ) ( )2mi iiL h ym == ?
?X y x   (3)227In the training phase, a back propagation (BP)algorithm with stochastic gradient descent (SGD)is used to learn model parameters.
Details of theBP algorithm can be found in (LeCun et al,2012).3 ExperimentsThis section evaluates the performance of theproposed regional CNN-LSTM model againstlexicon-based, regression-based, and NN-basedmethods.Datasets.
This experiment used two affectivecorpora.
i) Stanford Sentiment Treebank (SST)(Socher et al, 2013) contains 8,544 training texts,2,210 test texts, and 1,101 validation texts.
Eachtext was rated with a single dimension (valence)in the range of (0, 1).
ii) Chinese Valence-Arousal Texts (CVAT) (Yu et al, 2016) consistsof 2,009 texts collected from social forums,manually rated with both valence and arousaldimensions in the range of (1, 9) using the SAMannotation scheme (Bradley et al 1994).
Theword vectors for English and Chinese were re-spectively trained using the Google News andChinese wiki dumps (zhwiki) datasets.
The di-mensionality for both word vectors are 300.Experimental Settings.
Two lexicon-basedmethods were used for comparison: weightedarithmetic mean (wAM) and weighted geometricmean (wGM) (Paltoglou et al, 2013), along withtwo regression-based methods: average valuesregression (AVR) and maximum values regres-sion (MVR) (Malandrakis et al, 2013).
The va-lence ratings of English and Chinese words wererespectively taken from the Extended ANEW(Warriner et al, 2013) and Chinese Valence-Arousal Words (CVAW) lexicons (Yu et al,2016).
A conventional CNN, RNN and LSTMwere also implemented for comparison.Metrics.
Performance was evaluated using theroot mean square error (RMSE), mean absoluteerror (MAE), and Pearson correlation coefficient(r), defined as?
Root mean square error (RMSE)( )21ni iiRMSE A P n== ??
(4)?
Mean absolute error (MAE)11| |ni iiMAE A Pn == ??
(5)?
Pearson correlation coefficient (r)11( )( )1ni ii A PA A P Prn ?
?=?
?=?
?
(6)where Ai is the actual value, Pi is the predictedvalue, n is the number of test samples, A  and Prespectively denote the arithmetic mean of A andP, and ?
is the standard deviation.
A lowerRMSE or MAE and a higher r value indicatesbetter prediction performance.
A t-test was usedto determine whether the performance differencewas statistically significant.SST (English)Valence RMSE MAE rLexicon-wAM 2.018 1.709 0.350Lexicon-wGM 1.985 1.692 0.385Regression-AVR 1.856 1.542 0.455Regression-MVR 1.868 1.551 0.448CNN 1.489 1.184 0.706RNN 1.976 1.715 0.401LSTM 1.444 1.151 0.717Regional CNN-LSTM 1.341* 0.987* 0.778** Regional CNN-LSTM vs LSTM significantly different (p<0.05)Table 1: Comparative results of different methods in SST.CVAT (Chinese)Valence RMSE MAE rLexicon - wAM 1.884 1.632 0.406Lexicon - wGM 1.843 1.597 0.418Regression-AVR 1.685 1.374 0.476Regression-MVR 1.697 1.392 0.468CNN 1.093 0.880 0.645RNN 1.424 1.262 0.493LSTM 1.135 0.939 0.641Regional CNN-LSTM 1.026* 0.842* 0.781*Arousal RMSE MAE rLexicon-wAM 1.232 0.985 0.268Lexicon-wGM 1.243 0.996 0.263Regression-AVR 1.154 0.862 0.286Regression-MVR 1.128 0.842 0.289CNN 0.991 0.788 0.453RNN 1.024 0.816 0.290LSTM 0.945 0.751 0.472Regional CNN-LSTM 0.874* 0.689* 0.557** Regional CNN-LSTM vs LSTM significantly different (p<0.05)Table 2.
Comparative results of different methods in CVAT.228Comparative Results.
Tables 1 and 2 respec-tively present the comparative results of the re-gional CNN-LSTM against several methods forVA prediction of texts in both English and Chi-nese corpora.
For the lexicon-based methods,wGM outperformed wAM, which is consistentwith the results presented in (Paltoglou et al,2013).
Instead of using the VA ratings of wordsto directly measure those of texts, the regression-based methods learned the correlations betweenthe VA ratings of words and texts, thus yieldingbetter performance.
Once the word embeddingand deep learning techniques were introduced,the performance of NN-based methods (exceptRNN) jumped dramatically.
In addition, the pro-posed regional CNN-LSTM outperformed theother NN-based methods, indicating the effec-tiveness of sequentially integrating the regionalinformation across regions.
Another observationis that the Pearson correlation coefficient of pre-diction in arousal is lower than that for the va-lence prediction, indicating that arousal is moredifficult to predict.4 ConclusionThis study presents a regional CNN-LSTM mod-el to predict the VA ratings of texts.
By capturingboth local (regional) information within sentenc-es and long-distance dependency across sentenc-es, the proposed method outperformed regres-sion- and conventional NN-based methods pre-sented in previous studies.
Future work will fo-cus on the use of a parser to identify regions sothat the structural information can be further in-corporated to improve the prediction perfor-mance.AcknowledgmentsThis work was supported by the Ministry of Sci-ence and Technology, Taiwan, ROC, underGrant No.
MOST 102-2221-E-155-029-MY3 andMOST 104-3315-E-155-002.
The authors wouldlike to thank the anonymous reviewers and thearea chairs for their constructive comments.ReferencesYoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-dient descent is difficult.
IEEE Trans.
Neural Net-works, 5(21):57-166.Margaret M. Bradley, and Peter J. Lang.
1994.
Meas-uring emotion: the self-assessment manikin and thesemantic differential.
Journal of Behavior Therapyand Experimental Psychiatry, 25 (1): 49-59.Rafael A. Calvo and Sidney D'Mello.
2010.
Affect de-tection: An interdisciplinary re-view of models,methods, and their applications.
IEEE Trans.
Affec-tive Computing, 1(1): 18-37.Paul Ekman.
1992.
An argument for basic emotions.Cognition and Emotion, 6:169-200.Ronen Feldman.
2013.
Techniques and applicationsfor sentiment analysis.
Communications of theACM, 56(4):82-89.Alex Graves.
2012.
Supervised sequence labellingwith recurrent neural networks.
Vol.
385, Springer.Ozan Irsoy and Claire Cardie.
2014.
Opinion miningwith deep recurrent neural networks.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP-14),pages 720-728.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods on Natural Lan-guage Processing (EMNLP-14), pages 1746-1751.Nal Kalchbrenner, Edward Grefenstette, and PhilBlunsom.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL-14), pages 655-665.Yann LeCun, Leon Bottou, Genevieve B. Orr andKlaus-Robert Muller.
2012.
Efficient backprop.Neural networks: Tricks of the trade.
Springer Ber-lin Heidelberg, 2012.
9-48.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool, Chicago, IL.Pengfei Liu, Shafiq Joty and Helen Meng.
2015.
Fine-grained opinion mining with recurrent neural net-works and word embeddings.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-15), pages 1433-1443.Nikos Malandrakis, Alexandros Potamianos, EliasIosif, Shrikanth Narayanan.
2011.
Kernel modelsfor affective lexicon creation.
In Proceedings ofINTERSPEECH, pages 2977-2980.Nikos Malandrakis, Alexandros Potamianos, EliasIosif, Shrikanth Narayanan.
2013.
Distributionalsemantic models for affective text analysis.
IEEETrans.
Audio, Speech, and Language Processing,21(11): 2379-2392.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of Inter-national Conference on Learning Representations(ICLR-13): Workshop Track.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed repre-sentations of words and phrases and their composi-tionality.
In Advances in Neural Information Pro-cessing Systems 26, pages 3111-3119.229Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in in-formation retrieval, 2(1-2):1-135.Georgios Paltoglou, Mathias Theunis, Arvid Kappas,and Mike Thelwall.
2013.
Predicting emotional re-sponses to long informal text.
IEEE Trans.
Affec-tive Computing, 4(1):106-115.James A. Russell.
1980.
A circumplex model of affect.Journal of Personality and Social Psychology,39(6):1161.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ngand Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the 2013 EmpiricalMethods on Natural Language Processing(EMNLP-13), pages 1631-1642.Amy Beth Warriner, Victor Kuperman, and MarcBrysbaert.
2013.
Norms of valence, arousal, anddominance for 13,915 English lemmas.
Behaviorresearch methods, 45(4): 1191-1207.Xin Wang, Yuanchao Liu, Chengjie Sun, BaoxunWang and Xiaolong Wang.
2015.
Predicting polari-ties of tweets by composing word embeddings withlong short-term memory.
In Proceedings of the53th Annual Meeting of the Association for Com-putational Linguistics (ACL-15), pages 1343-1353.Wen-Li Wei, Chung-Hsien Wu, and Jen-Chun Lin.2011.
A regression approach to affective rating ofChinese words from ANEW.
In Proceedings of Af-fective Computing and Intelligent Interaction(ACII-11), pages 121-131.Liang-Chih Yu, Jin Wang, K. Robert Lai, and XuejieZhang.
2015.
Predicting valence-arousal ratings ofwords using a weighted graph method.
In Proceed-ings of the 53th Annual Meeting of the Associationfor Computational Linguistics (ACL-15), pages788-793.Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang,Yunchao He, Jun Hu, K. Robert Lai and XuejieZhang.
2016.
Building Chinese Affective Re-sources in Valence-Arousal Dimensions.
In Pro-ceedings of the 15th Annual Conference of theNorth American Chapter of the Association forComputational Linguistics: Human LanguageTechnologies (NAACL/HLT-16).230
