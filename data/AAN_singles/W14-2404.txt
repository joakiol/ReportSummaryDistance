Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 17?21,Baltimore, Maryland USA, June 26 2014. c?2014 Association for Computational LinguisticsSemantic Parsing for Text to 3D Scene GenerationAngel X. Chang, Manolis Savva and Christopher D. ManningComputer Science Department, Stanford Universityangelx,msavva,manning@cs.stanford.eduFigure 1: Generated scene for ?There is a roomwith a chair and a computer.?
Note that the systeminfers the presence of a desk and that the computershould be supported by the desk.1 IntroductionWe propose text-to-scene generation as an appli-cation for semantic parsing.
This is an applica-tion that grounds semantics in a virtual world thatrequires understanding of common, everyday lan-guage.
In text to scene generation, the user pro-vides a textual description and the system gener-ates a 3D scene.
For example, Figure 1 shows thegenerated scene for the input text ?there is a roomwith a chair and a computer?.
This is a challeng-ing, open-ended problem that prior work has onlyaddressed in a limited way.Most of the technical challenges in text toscene generation stem from the difficulty of map-ping language to formal representations of vi-sual scenes, as well as an overall absence of realworld spatial knowledge from current NLP sys-tems.
These issues are partly due to the omis-sion in natural language of many facts about theworld.
When people describe scenes in text, theytypically specify only important, relevant informa-tion.
Many common sense facts are unstated (e.g.,chairs and desks are typically on the floor).
There-fore, we focus on inferring implicit relations thatare likely to hold even if they are not explicitlystated by the input text.Text to scene generation offers a rich, interactiveenvironment for grounded language that is famil-iar to everyone.
The entities are common, every-day objects, and the knowledge necessary to ad-dress this problem is of general use across manydomains.
We present a system that leverages userinteractionwith 3D scenes to generate training datafor semantic parsing approaches.Previous semantic parsing work has dealt withgrounding text to physical attributes and rela-tions (Matuszek et al., 2012; Krishnamurthy andKollar, 2013), generating text for referring to ob-jects (FitzGerald et al., 2013) and with connect-ing language to spatial relationships (Golland etal., 2010; Artzi and Zettlemoyer, 2013).
Seman-tic parsing methods can also be applied to manyaspects of text to scene generation.
Furthermore,work on parsing instructions to robots (Matuszeket al., 2013; Tellex et al., 2014) has analogues inthe context of discourse about physical scenes.In this extended abstract, we formalize the textto scene generation problem and describe it as atask for semantic parsing methods.
To motivatethis problem, we present a prototype system thatincorporates simple spatial knowledge, and parsesnatural text to a semantic representation.
By learn-ing priors on spatial knowledge (e.g., typical posi-tions of objects, and common spatial relations) oursystem addresses inference of implicit spatial con-straints.
The user can interactively manipulate thegenerated scene with textual commands, enablingus to refine and expand learned priors.Our current system uses deterministic rules tomap text to a scene representation but we plan toexplore training a semantic parser from data.
Wecan leverage our system to collect user interactionsfor training data.
Crowdsourcing is a promisingavenue for obtaining a large scale dataset.17Objects:PLATE, FORKON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)?There is a piece of cake on a table.
?Scene Generation3D ModelsSpatial KBObjects:CAKE, TABLEON(CAKE, TABLE)SemanticParsingINTERACTIONScene InferenceObjectSelectionFigure 2: Illustration of our system architecture.2 Task DefinitionWe define text to scene generation as the task oftaking text describing a scene as input, and gen-erating a plausible 3D scene described by thattext as output.
More concretely, we parse theinput text into a scene template, which placesconstraints on what objects must be present andrelationships between them.
Next, using priorsfrom a spatial knowledge base, the system expandsthe scene template by inferring additional implicitconstraints.
Based on the scene template, we selectobjects from a dataset of 3D models and arrangethem to generate an output scene.After a scene is generated, the user can interactwith the scene using both textual commands andmouse interactions.
During interaction, semanticparsing can be used to parse the input text intoa sequence of scene interaction commands.
SeeFigure 2 for an illustration of the system archi-tecture.
Throughout the process, we need to ad-dress grounding of language to: 1) actions to beperformed, 2) objects to be instantiated or manip-ulated, and 3) constraints on the objects.2.1 Scene TemplateA scene template T = (O, C) consists of a setof object descriptions O = {o1, .
.
.
, on} and con-straints C = {c1, .
.
.
, ck} on the relationships be-tween the objects.
For each object oi, we identifyproperties associated with it such as category la-bel, basic attributes such as color and material, andnumber of occurrences in the scene.
Based on theobject category and attributes, and other words inthe noun phrase mentioning the object, we iden-tify a set of associated keywords to be used laterfor querying the 3D model database.
Spatial rela-tions between objects are extracted as predicates ofthe form on(oi, oj) or left(oi, oj) where oiand ojare recognized objects.As an example, given the input ?There is a roomwith a desk and a red chair.
The chair is to the leftof the desk.?
we extract the following objects andspatial relations:Objects category attributes keywordso0room roomo1desk desko2chair color:red chair, redRelations: left(o2, o1)2.2 Scene Interaction CommandsDuring interaction, we parse textual input providedby the user into a sequence of commands with rele-vant parts of the scene as arguments.
For example,given a scene S, we use the input text to identify asubset of relevant objects matchingX = {Os, Cs}where Osis the set of object descriptions and Csis the set of object constraints.
Commands canthen be resolved against this argument to manip-ulate the scene state: Select(X), Remove(X),Insert(X), Replace(X,Y ), Move(X,?X),Scale(X,?X), and Orient(X,?X).
X and Yare semantic representations of objects, while?Xis a change to be applied to X , expressed as eithera target condition (?put the lamp on the table?)
ora relative change (?move the lamp to the right?
).These basic operations demonstrate possiblescene manipulations through text.
This set of op-erations can be enlarged to cover manipulation ofparts of objects (?make the seat of the chair red?
),and of the viewpoint (?zoom in on the chair?
).2.3 Spatial KnowledgeOne of the richest sources of spatial knowledgeis 3D scene data.
Prior work by (Fisher et al.,2012) collected 133 small indoor scenes createdwith 1723 3D Warehouse models.
Based on theirapproach, we create a spatial knowledge base withpriors on the static support hierarchy of objects inscenes1, their relative positions and orientations.We also define a set of spatial relations such as left,right, above, below, front, back, on top of, next to,near, inside, and outside.
Table 1 gives examplesof the definitions of these spatial relations.We use a 3D model dataset collected fromGoogle 3DWarehouse by prior work in scene syn-1A static support hierarchy represents which objects arelikely to support which other objects on their surface (e.g.,the floor supports tables, tables support plates).18Relation P (relation)inside(A,B) V ol(A?B)V ol(A)right(A,B) V ol(A?
right (B))V ol(A)near(A,B) 1(dist(A,B) < tnear)Table 1: Definitions of spatial relation using objectbounding box computations.thesis and containing about 12490 mostly indoorobjects (Fisher et al., 2012).
These models havetext associated with them in the form of names andtags, and category labels.
In addition, we assumethe models have been scaled to physically plausi-ble sizes and oriented with consistent up and frontdirection (Savva et al., 2014).
All models are in-dexed in a database so they can be queried at run-time for retrieval.3 System DescriptionWe present how the parsed representations areused by our system to demonstrate the key issuesthat have to be addressed during text to scene gen-eration.
Our current implementation uses a sim-ple deterministic approach to map text to the scenetemplate and user actions on the scene.
We use theStanford CoreNLP pipeline2 to process the inputtext and use rules to match dependency patterns.3.1 Scene generationDuring scene generation, we want to construct themost likely scene given the input text.
We firstparse the text into a scene template and use it toselect appropriate models from the database.
Wethen perform object layout and arrangement giventhe priors on spatial knowledge.Scene Template Parsing We use the Stanfordcoreference system to determine when the sameobject is being referred to.
To identify objects,we look for noun phrases and use the head wordas the category, filtering with WordNet (Miller,1995) to determine which objects are visualizable(under the physical object synset, excluding loca-tions).
To identify properties of the objects, we ex-tract other adjectives and nouns in the noun phrase.We also match syntactic dependency patterns suchas ?X is made of Y?
to extract more attributes andkeywords.
Finally, we use dependency patterns toextract spatial relations between objects.2http://nlp.stanford.edu/software/corenlp.shtmlFigure 3: Select ?a blue office chair?
and ?awooden desk?
from the models databaseObject Selection Once we have the scene tem-plate, we use the keywords associated with eachobject to query the model database.
We select ran-domly from the top 10 results for variety and toallow the user to regenerate the scene with differ-ent models.
This step can be enhanced to take intoaccount correlations between objects (e.g., a lampon a table should not be a floor lamp model).
SeeFigure 3 for an example of object selection.Object Layout Given the selected models, thesource scene template, and priors on spatial rela-tions, we find an arrangement of the objects withinthe scene that maximizes the probability of the lay-out under the given scene template.3.2 Scene InteractionHere we address parsing of text after a scene hasbeen generated and during interaction sessions.Command Parsing We deterministically mapverbs to possible actions as shown in Table 2.Multiple actions are possible for some verbs (e.g.,?place?
and ?put?
can refer to either Move orInsert).
To differentiate between these, we as-sume new objects are introduced with the indefi-nite article ?a?
whereas old ones are modified withthe definite article ?the?.Object Resolution To allow interaction with thescene, wemust resolve references to objects withina scene.
Objects are disambiguated by categoryand view-centric spatial relations.
In addition tomatching objects by their categories, we use theWordNet hierarchy to handle hyponym or hyper-nym referents.
Depending on the current view,spatial relations such as ?left?
or ?right?
can referto different objects (see Figure 4).Scene Modification Based on the action weneed to appropriately modify the current scene.19verb Action Example Text Example Parsegenerate Generate generate a room with a desk and a lamp Generate( {room,desk,lamp} , {}) )select Select select the chair on the right of the table Select({lamp},{right(lamp,table)})add, insert Insert add a lamp to the table Insert({lamp},{on(lamp,table)})delete, remove Remove remove the lamp Remove({lamp})move Move move the chair to the left Move({chair},{left(chair)})place, put Move, Insert put the lamp on the table Move({lamp},{on(lamp,table)})replace Replace replace the lamp with a vase Replace({lamp},{vase})Table 2: Mapping of verbs to possible actions.Figure 4: Left: chair is selected by ?chair to theright of the table?
or ?object to the right of the ta-ble?, but not selected by ?cup to the right of thetable?.
Right: Different view results in a differentchair selection for ?chair to the right of the table?.Figure 5: Left: initial scene.
Right: after input?Put a lamp on the table?.We do this by maximizing the probability of a newscene template given the requested action and pre-vious scene template (see Figure 5 for an example).4 Future DirectionsWe described a system prototype to motivate ap-proaching text to scene generation as a semanticparsing application.
While this prototype illus-trates inference of implicit constraints using priorknowledge, it still relies on hand coded rules formapping text to the scene representation.
This issimilar to most previous work on text to scene gen-eration (Winograd, 1972; Coyne and Sproat, 2001)and limits handling of natural language.
More re-cently, (Zitnick et al., 2013) used data to learn howto ground sentences to a CRF representing 2D cli-part scenes.
Similarly, we plan to investigate usingdata to learn how to ground sentences to 3D scenes.Spatial knowledge can be helpful for resolvingambiguities during parsing.
For instance, fromspatial priors of object positions and reasoningwith physical constraints we can disambiguate theattachment of ?next to?
in ?there is a book on thetable next to the lamp?.
The book and lamp arelikely on the table and thus next_to(book, lamp)should be more likely.User interaction is a natural part of text to scenegeneration.
We can leverage such interaction toobtain data for training a semantic parser.
Everytime the user issues a command, the user can indi-cate whether the result of the interaction was cor-rect or not, and optionally provide a rating.
Bykeeping track of these scene interactions and theuser ratings we can construct a corpus of tuplescontaining: user action, parsed scene interaction,scene operation, scene state before and after theoperation, and rating by the user.
By building upsuch a corpus over multiple interactions and users,we obtain data for training semantic parsers.ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the Associ-ation for Computational Linguistics.Bob Coyne and Richard Sproat.
2001.
WordsEye: anautomatic text-to-scene conversion system.
In Pro-ceedings of the 28th annual conference on Computergraphics and interactive techniques.Matthew Fisher, Daniel Ritchie, Manolis Savva,Thomas Funkhouser, and Pat Hanrahan.
2012.Example-based synthesis of 3D object arrangements.ACM Transactions on Graphics.Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-moyer.
2013.
Learning distributions over logicalforms for referring expression generation.
In Pro-ceedings of the Conference on EMNLP.Dave Golland, Percy Liang, and Dan Klein.
2010.A game-theoretic approach to generating spatial de-scriptions.
In Proceedings of the 2010 conference onEMNLP.Jayant Krishnamurthy and Thomas Kollar.
2013.Jointly learning to parse and perceive: Connecting20natural language to the physical world.
Transactionsof the Association for Computational Linguistics.Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-moyer, Liefeng Bo, and Dieter Fox.
2012.
A jointmodel of language and perception for grounded at-tribute learning.
In International Conference onMa-chine Learning.Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,and Dieter Fox.
2013.
Learning to parse naturallanguage commands to a robot control system.
InExperimental Robotics.G.A.
Miller.
1995.
WordNet: a lexical database forenglish.
CACM.Manolis Savva, Angel X. Chang, Gilbert Bernstein,Christopher D. Manning, and Pat Hanrahan.
2014.On being the right scale: Sizing large collections of3D models.
Stanford University Technical ReportCSTR 2014-03.Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, andNicholas Roy.
2014.
Learning perceptuallygrounded word meanings from unaligned paralleldata.
Machine Learning.Terry Winograd.
1972.
Understanding natural lan-guage.
Cognitive psychology.C.
Lawrence Zitnick, Devi Parikh, and Lucy Vander-wende.
2013.
Learning the visual interpretationof sentences.
In IEEE Intenational Conference onComputer Vision (ICCV).21
