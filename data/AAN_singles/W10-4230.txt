The UMUS system for named entity generation at GREC 2010Benoit FavreLIUM, Universite?
du Maine72000 Le Mans, Francebenoit.favre@gmail.comBernd BohnetUniversita?t StuttgartStuttgart, Germanybohnet@informatik.uni-stuttgart.deAbstractWe present the UMUS (Universite?
duMaine/Universita?t Stuttgart) submissionfor the NEG task at GREC?10.
We re-fined and tuned our 2009 system but westill rely on predicting generic labels andthen choosing from the list of expressionsthat match those labels.
We handled recur-sive expressions with care by generatingspecific labels for all the possible embed-dings.
The resulting system performs at atype accuracy of 0.84 an a string accuracyof 0.81 on the development set.1 IntroductionThe Named Entity Generation (NEG) task con-sists in choosing a referential expression (com-plete name, last name, pronoun, possessive pro-noun, elision...) for all person entities in a text.Texts are biographies of chefs, composers and in-ventors from Wikipedia.
For each reference, a listof expressions is given from which the system hasto choose.
This task is challenging because of thefollowing aspects:1.
The data is imperfect as it is a patchwork ofmultiple authors?
writing.2.
The problem is hard to handle with a classi-fier because text is predicted, not classes.3.
The problem has a complex graph structure.4.
Some decisions are recursive for embeddedreferences, i.e.
?his father?.5.
Syntactic/semantic features cannot be ex-tracted with a classical parser because theword sequence is latent.We do not deal with all of these challenges butwe try to mitigate their impact.
Our system ex-tends our approach for GREC?09 (Favre and Bon-het, 2009).
We use a sequence classifier to predictgeneric labels for the possible expressions.2 Labels for classificationEach referential expression (REFEX) is given a la-bel consisting of sub-elements:?
The REG08 TYPE as given in the REFEX(name, common, pronoun, empty...)?
The CASE as given in the REFEX (plain,genitive, accusative...)?
If the expression is a pronoun, then one of?he, him, his, who, whom, whose, that?, aftergender and number normalization.?
?self?
if the expression contains ?self?.?
?short?
if the expression is a one-word longname or common name.?
?nesting?
if the expression is recursive.For recursive expressions, a special handling is ap-plied: All possible assignments of the embeddedentities are generated with labels correspondingto the concatenation of the involved entities?
la-bels.
If the embedding is on the right (left) sideof the expression, ?right?
(?left?)
is added to thelabel.
Non-sensical labels (i.e.
?he father?)
are notseen in the training data, and therefore not hypoth-esized.3 FeaturesEach reference is characterized with the followingfeatures:?
SYNFUNC, SEMCAT, SYNCAT: syntacticfunction, semantic category, syntactic cate-gory, as given in REF node.?
CHANGE, CHANGE+SYNFUNC: previousreference is for a different entity, possiblywith syntactic function.?
PREV GENDER NUMBER: if the refer-ence is from a different entity, can be ?same?or ?different?.
The attribute is being com-pared is ?male?, ?female?
or ?plural?, deter-mined by looking at the possible expressions.?
FIRST TIME: denotes if it?s the first timethat the entity is seen.
For plural entities, theentity is considered new if at least one of theinvolved entities is new.?
BEG PARAGRAPH: the first entity of aparagraph.?
{PREV,NEXT} PUNCT: the punctuationimmediately before (after) the entity.
Can be?sentence?
if the punctuation is one of ?.?!?,?comma?
for ?,;?, ?parenthesis?
for ?
()[]?and ?quote?.?
{PREV,NEXT} SENT: whether or not a sen-tence boundary occurs after (before) the pre-vious (next) reference.?
{PREV,NEXT} WORD {1,2}GRAM: cor-responding word n-gram.
Words are ex-tracted up to the previous/next reference orthe start/end of a sentence, with parenthe-sized content removed.
Words are lower-cased tokens made of letters and numbers.?
{PREV,NEXT} TAG: most likely part-of-speech tag for the previous/next word, skip-ping adverbs.?
{PREV,NEXT} BE: any form of the verb ?tobe?
is used after (before) the previous (next)reference.?
EMBEDS PREV: the entity being embeddedwas referred to just before.?
EMBEDS ALL KNOWN: all the entities be-ing embedded have been seen before.4 Sequence classifierWe rely on Conditional Random Fields1 (Laffertyet al, 2001) for predicting one label (as definedpreviously) per reference.
We lay the problem asone sequence of decisions per entity to prevent, forinstance, the use of the same name twice in a row.Last year, we generated one sequence per docu-ment with all entities, but it was less intuitive.
Tothe features extracted for each reference, we addthe features of the previous and next reference, ac-cording to label unigrams and label bigrams.
Thec hyperparameter and the frequency cutoff of theclassifier are optimized on the dev set.
Note that1CRF++, http://crfpp.sourceforge.netfor processing the test set, we added the develop-ment data to the training set.5 Text generationFor each reference, the given expressions areranked by classifier-estimated posterior probabil-ity and the best one is used for output.
In casemultiple expressions have the same labeling (andthe same score), we use the longest one and iter-ate through the list for each subsequent use (usefulfor repeated common names).
If an expression ismore than 4 words, it?s flagged for not being useda second time (only ad-hoc rule in the system).6 ResultsEvaluation scores for the output are presented inTable 1.
The source code of our systems is madeavailable to the community at http://code.google.com/p/icsicrf-grecneg.Sys.
T.acc Prec.
Rec.
S.acc Bleu NistOld 0.826 0.830 0.830 0.786 0.811 5.758New 0.844 0.829 0.816 0.813 0.817 6.021Table 1: Results on the dev set comparing our sys-tem from last year (old) to the refined one (new),according to REG08 TYPE accuracy (T.acc), pre-cision and recall, String accuracy (S.acc), BLEU1an NIST.About 50% of the errors are caused by the se-lection of pronouns instead of a name.
The selec-tion of the pronoun or name seems to depend onthe writing style since a few authors prefer nearlyalways the name.
The misuse of names insteadof pronouns is second most error with about 15%.The complex structured named entities are respon-sible for about 9% of the errors.
The selection ofthe right name such as given name, family name orboth seems to be more difficult.
The next frequenterrors are confusions between pronouns, elisions,common names, and names.ReferencesBenoit Favre and Bernd Bonhet.
2009.
ICSI-CRF: TheGeneration of References to the Main Subject andNamed Entities Using Conditional Random Fields.In ACL-IJCNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models forsegmenting and labeling sequence data.
MachineLearning, pages 282?289.
