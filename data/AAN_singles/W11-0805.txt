Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 20?24,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsDetecting Multi-Word Expressions improves Word Sense DisambiguationMark Alan Finlayson & Nidhi KulkarniComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA, 02139, USA{markaf,nidhik}@mit.eduAbstractMulti-Word Expressions (MWEs) are preva-lent in text and are also, on average, less poly-semous than mono-words.
This suggests thataccurate MWE detection should lead to a non-trivial improvement in Word Sense Disam-biguation (WSD).
We show that a straight-forward MWE detection strategy, due to Ar-ranz et al (2005), can increase a WSD al-gorithm?s baseline f-measure by 5 percentagepoints.
Our measurements are consistent withArranz?s, and our study goes further by us-ing a portion of the Semcor corpus containing12,449 MWEs - over 30 times more than theapproximately 400 used by Arranz.
We alsoshow that perfect MWE detection over Sem-cor only nets a total 6 percentage point in-crease in WSD f-measure; therefore there islittle room for improvement over the resultspresented here.
We provide our MWE detec-tion algorithms, along with a general detectionframework, in a free, open-source Java librarycalled jMWE.Multi-word expressions (MWEs) are prevalentin text.
This is important for the classic task ofWord Sense Disambiguation (WSD) (Agirre and Ed-monds, 2007), in which an algorithm attempts to as-sign to each word in a text the appropriate entry froma sense inventory.
A WSD algorithm that cannot cor-rectly detect the MWEs that are listed in its sense in-ventory will not only miss those sense assignments,it will also spuriously assign senses to MWE con-stituents that themselves have sense entries, dealinga double-blow to WSD performance.Beyond this penalty, MWEs listed in a sense in-ventory also present an opportunity to WSD algo-rithms - they are, on average, less polysemous thanmono-words.
In Wordnet 1.6, multi-words have anaverage polysemy of 1.07, versus 1.53 for mono-words.
As a concrete example, consider sentenceShe broke the world record.
In Wordnet 1.6 thelemma world has nine different senses and recordhas fourteen, while the MWE world record has onlyone.
If a WSD algorithm correctly detects MWEs,it can dramatically reduce the number of possiblesenses for such sentences.Measure Us ArranzNumber of MWEs 12,449 382Fraction of MWEs 7.4% 9.4%WSD impr.
(Best v. Baseline) 0.016F1 0.012F1WSD impr.
(Baseline v. None) 0.033F1 -WSD impr.
(Best v. None) 0.050F1 -WSD impr.
(Perfect v. None) 0.061F1 -Table 1: Improvement of WSD f-measures over anMWE-unaware WSD strategy for various MWE detec-tion strategies.
Baseline, Best, and Perfect refer to theMWE detection strategy used in the WSD preprocess.With this in mind, we expected that accurateMWE detection will lead to a small yet non-trivialimprovement in WSD performance, and this is in-deed the case.
Table 1 summarizes our results.
Inparticular, a relatively straightforward MWE detec-tion strategy, here called the ?best?
strategy and dueto Arranz et al (2005), yielded a 5 percentage pointimprovement1 in WSD f-measure.
We also mea-sured an improvement similar to that of Arranz when1For example, if the WSD algorithm has an f-measure of20moving from a Baseline MWE detection strategy tothe Best strategy, namely, 1.6 percentage points totheir 1.2.We performed our measurements over the brown1and brown2 concordances2 of the Semcor cor-pus (Fellbaum, 1998), which together contain12,449 MWEs, over 30 times as many as the approx-imately 400 contained in the portion of the XWNcorpus used by Arranz.
We also measured the im-provement for WSD f-measure for Baseline and Per-fect MWE detection strategies.
These strategiesimproved WSD f-measure by 3.3 and 6.1 percent-age points, respectively, showing that the relativelystraightforward Best MWE detection strategy, at 5.0percentage points, leaves little room for improve-ment.1 MWE Detection Algorithms by ArranzArranz et al describe their TALP Word Sense Dis-ambiguation system in (Castillo et al, 2004) and(Arranz et al, 2005).
The details of the WSD pro-cedure are not critical here; what is important isthat their preprocessing system attempted to detectMWEs that could later be disambiguated by theWSD algorithm.
This preprocessing occurred as apipeline that tokenized the text, assigned a part-of-speech tag, and finally determined a lemma for eachstemmable word.
This information was then passedto a MWE candidate identifier3 whose output wasthen filtered by an MWE selector.
The resulting listof MWEs, along with all remaining tokens, werethen passed into the WSD algorithm for disambigua-tion.The MWE identifier-selector pair determinedwhat combinations of tokens were marked asMWEs.
It considered only continuous (i.e., unbro-ken) sequences of tokens whose order matched theorder of the constituents of the associated MWE en-try in Wordnet.
Because of morphological variation,not all sequences of tokens are in base form; themain function of the candidate identifier, therefore,0.6, then a 5 percentage point increase yields an f-measure of0.65.2The third concordance, brownv, only has verbs marked, sowe did not test on it.3Arranz calls the candidate identification stage the MWE de-tector; we have renamed it because we take ?detection?
to be theend-to-end process of marking MWEs.was to determine what morphological variation wasallowed for a particular MWE entry.
They identifiedand tested four different strategies:1.
None - no morphological variation allowed, allMWEs must be in base form2.
Pattern - allows morphological variation ac-cording to a set of pre-defined patterns3.
Form - a morphological variant is allowed if itis observed in Semcor4.
All - all morphological variants allowedThe identification procedure produced a list ofcandidate MWEs.
These MWEs were then filteredby the MWE selection process, which used one oftwo strategies:1.
Longest Match, Left-to-Right - starting fromthe left to right, selects the longest multi-wordexpression found2.
Semcor - selects the multi-word expressionwhose tokens have the maximum probabilityof participating in an MWE, according to mea-surements over SemcorArranz identified the None/Longest-Match-Left-to-Right strategy as the Baseline, noting that this wasthe most common strategy for MWE-aware WSDalgorithms.
For this strategy the only MWE can-didates allowed were those already in base form(None), followed by resolution of conflicts by select-ing the MWEs that started farthest to the left, choos-ing the longest in case of ties (Longest-Match-Left-to-Right);Arranz?s Best strategy was Pattern/Semcor,namely, allowing candidate MWEs to vary mor-phologically according to a pre-defined set of syn-tactic patterns (Pattern), followed by selecting themost likely MWE based on examination of to-ken frequencies in the Semcor corpus (Semcor).They ran their detection strategies over a subset ofthe sense-disambiguated glosses of the eXtendedWordNet (XWN) corpus (Moldovan and Novischi,2004).
They selected all glosses whose sense-disambiguated words were all marked as ?gold?quality, namely, reviewed by a human annota-tor.
Over this set of words, their WSD sys-tem achieved 0.617F1 (0.622p/0.612r) when usingthe Baseline MWE detection strategy, and 0.629F1(0.638p/0.620r) when using the Best strategy.212 Extension of ResultsWe implemented both the Baseline and Best MWE-detection strategies, and used them as preprocessorsfor a simple WSD algorithm, namely, the Most-Frequent Sense algorithm.
This algorithm simplychooses, for each identified base form, the most fre-quent sense in the sense inventory.
We chose thisstrategy, instead of re-implementing Arranz?s strat-egy, for two reasons.
First, our purpose was to studythe improvement MWE-detection provides to WSDin general, not to a specific WSD algorithm.
Wewished to show that, to the first order, MWE detec-tion improves WSD irrespective of the WSD algo-rithm chosen.
Using a different algorithm than Ar-ranz?s supports this claim.
Second, for those wish-ing to further this work, or build upon it, the Most-Frequent-Sense strategy is easily implemented.We used JSemcor (Finlayson, 2008a) to inter-face with the Semcor data files.
We used Word-net version 1.6 with the original version of Sem-cor4.
Each token in each sentence in the brown1and brown2 concordances of Semcor was assigned apart of speech tag calculated using the Stanford JavaNLP library (Toutanova et al, 2003), as well as aset of lemmas calculated using the MIT Java Word-net Interface (Finlayson, 2008b).
This data was theinput to each MWE detection strategy.There was one major difference between our de-tector implementations and Arranz, stemming froma major difference between XWN and Semcor:Semcor contains a large number of proper nouns,whereas XWN glosses contain almost none.
There-fore our detector implementations included a simpleproper noun MWE detector, which marked all un-broken runs of tokens tagged as proper nouns as aproper noun MWE.
This proper noun detector wasrun first, before the Baseline and Best detectors, andthe proper noun MWEs detected took precedenceover the MWEs detected in later stages.Baseline MWE Detection This MWE detec-tion strategy was called None/Longest-Match-Left-4The latest version of Wordnet is 3.0, but Semcor has notbeen manually updated for Wordnet versions later than 1.6.
Au-tomatically updated versions of Semcor are available, but theycontain numerous errors resulting from deleted sense entries,and the sense assignments and multi-word identifications havenot been adjusted to take into account new entries.
Thereforewe decided to use versions 1.6 for both Wordnet and Semcor.to-Right by Arranz; we implemented it in fourstages.
First, we detected proper nouns, as de-scribed.
Second, for each sentence, the strategyused the part of speech tags and lemmas to iden-tify all possible consecutive MWEs, using a list ex-tracted from WordNet 1.6 and Semcor 1.6.
Theonly restriction was that at least one token identi-fied as part of the MWE must share the basic part ofspeech (e.g., noun, verb, adjective, or adverb) withthe part of speech of the MWE.
As noted, tokens thatwere identified as being part of a proper noun MWEwere not included in this stage.
In the third stage,we removed all non-proper-noun MWEs that wereinflected?this corresponds to Arranz?s None stage.In our final stage, any conflicts were resolved bychoosing the MWE with the leftmost token.
For twoconflicting MWEs that started at the same token, thelongest MWE was chosen.
This corresponds to Ar-ranz?s Longest-Match-Left-to-Right selection.Best MWE Detection This MWE detection strat-egy was called Pattern/Semcor by Arranz, and wealso implemented this strategy in four stages.
Thefirst and second stages were the same as the Baselinestrategy, namely, detection of proper nouns followedby identification of continuous MWEs.
The thirdstage kept only MWEs whose morphological inflec-tion matched one of the inflection rules describedby Arranz (Pattern).
The final stage resolved anyconflicts by choosing the MWE whose constituenttokens occur most frequently in Semcor as an MWErather than a sequence of monowords (Arranz?s Sem-cor selection).Word Sense Disambiguation No special tech-nique was required to chain the Most-FrequentSense WSD algorithm with the MWE detectionstrategies.
We measured the performance of theWSD algorithm using no MWE detection, the Base-line detection, the Best detection, and Perfect detec-tion5.
These results are shown in Table 2.Our improvement from Baseline to Best was ap-proximately the same as Arranz?s: 1.7 percentagepoints to their 1.2.
We attribute the difference to themuch worse performance of our Baseline detectionalgorithm: our Baseline MWE detection f-measurewas 0.552, compared their 0.740.
The reason for this5Perfect detection merely returned the MWEs identified inSemcor22Measure Arranz et al (2005) Finlayson & KulkarniCorpus eXtended WordNet (XWN) 2.0 Semcor 1.6 (brown1 & brown2)Number of Tokens (non-punctuation) 8,493 376,670Number of Open-Class Tokens 5,133 196,852Number of Open-Class Monowords 4,332 168,808Number of Open-Class MWEs 382 12,449Number of Tokens in Open-Class MWEs 801 28,044Number of Open-Class Words (mono & multi) 4,714 181,257Fraction MWEs 9.4% 7.4%MWE Detection, Baseline 0.740F1 (0.765p/0.715r) 0.552F1 (0.452p/0.708r)MWE Detection, Best 0.811F1 (0.806p/0.816r) 0.856F1 (0.874p/0.838r)WSD, MWE-unaware - 0.579F1 (0.572p/0.585r)WSD, Baseline MWE Detection 0.617F1 (0.622p/0.612r) 0.612F1 (0.614p/0.611r)WSD, Best MWE Detection 0.629F1 (0.638p/0.620r) 0.629F1 (0.630p/0.628r)WSD, Perfect MWE Detection - 0.640F1 (0.642p/0.638r)WSD Improvement, Baseline vs. Best 0.012F1 (0.016p/0.008r) 0.016F1 (0.016p/0.017r)WSD Improvement, Baseline vs. None - 0.033F1 (0.042p/0.025r)WSD Improvement, Best vs. None - 0.050F1 (0.058p/0.043r)WSD Improvement, Perfect vs. None - 0.061F1 (0.070p/0.053r)Table 2: All the relevant numbers for the study.
For purposes of comparison we recalculated the token counts for thegold-annotated portion of the XWN corpus, and found discrepancies with Arranz?s reported values.
They reported1300 fully-gold-annotated glosses containing 397 MWEs; we found 1307 glosses containing 382 MWEs.
The tablecontains our token counts, but Arranz?s actual MWE detection and WSD f-measures, precisions, and recalls.striking difference in Baseline performance seems tobe that, in the XWN glosses, a much higher fractionof the MWEs are already in base form (e.g., nounsin glosses are preferentially expressed as singular).To encourage other researchers to build upon ourresults, we provide our implementation of thesetwo MWE detection strategies, along with a gen-eral MWE detection framework and numerous otherMWE detectors, in the form of a free, open-sourceJava library called jMWE (Finlayson and Kulkarni,2011a).
Furthermore, to allow independent verifi-cation of our results, we have placed all the sourcecode and data required to run these experimentsin an online repository (Finlayson and Kulkarni,2011b).3 ContributionsWe have shown that accurately detecting multi-word expressions allows a non-trivial improvementin word sense disambiguation.
Our Baseline, Best,and Perfect MWE detection strategies show a 3.3,5.1, and 6.1 percentage point improvement in WSDf-measure.
Our Baseline-to-Best improvement iscomparable with that measured by Arranz, the dif-ference being due to more prevalent base-formMWEs between XWN glosses and Semcor.
Thevery small improvement of the Perfect strategy overthe Best shows that, at least for Wordnet over textswith an MWE distribution similar to Semcor, thereis little to be gained even from a highly-sophisticatedMWE detector.
We have provided these two MWEdetection algorithms in a free, open-source Java li-brary called jMWE.AcknowledgmentsThis work was supported in part by the Air ForceOffice of Scientific Research under grant num-ber A9550-05-1-0321, and the Defense AdvancedResearch Projects Agency under contract numberFA8750-10-1-0076.
Thanks to Michael Fay forhelpful comments.23ReferencesEneko Agirre and Philip Edmonds.
2007.
Word SenseDisambiguation.
Text, Speech, and Language Tech-nology.
Springer-Verlag, Dordrecht, The Netherlands.Victoria Arranz, Jordi Atserias, and Mauro Castillo.2005.
Multiwords and word sense disambiguation.In Alexander Gelbukh, editor, Proceedings of theSixth International Conference on Intelligent Text Pro-cessing and Computational Linguistics (CICLING),volume 3406 of Lecture Notes in Computer Sci-ence (LNCS), pages 250?262, Mexico City, Mexico.Springer-Verlag.Mauro Castillo, Francis Real, Jordi Asterias, and GermanRigau.
2004.
The TALP systems for disambiguat-ing WordNet glosses.
In Rada Mihalcea and Phil Ed-monds, editors, Proceedings of Senseval-3: Third In-ternational Workshop on the Evaluation of Systems forthe Semantic Analysis of Text, pages 93?96.
Associa-tion for Computational Linguistics.Christiane Fellbaum.
1998.
Wordnet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Mark Alan Finlayson and Nidhi Kulkarni.
2011a.jMWE, version 1.0.0.http://projects.csail.mit.edu/jmwehttp://hdl.handle.net/1721.1/62793.Mark Alan Finlayson and Nidhi Kulkarni.
2011b.
Sourcecode and data for MWE?2011 papers.http://hdl.handle.net/1721.1/62792.Mark Alan Finlayson.
2008a.
JSemcor, version 1.0.0.http://projects.csail.mit.edu/jsemcor.Mark Alan Finlayson.
2008b.
JWI: The MIT Java Word-net Interface, version 2.1.5.http://projects.csail.mit.edu/jwi.Dan Moldovan and Adrian Novischi.
2004.
Wordsense disambiguation of WordNet glosses.
ComputerSpeech and Language, 18:301?317.Kristina Toutanova, Daniel Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.pages 252?259.
Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL).24
