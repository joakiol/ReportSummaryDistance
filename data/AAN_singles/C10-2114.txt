Coling 2010: Poster Volume, pages 988?996,Beijing, August 2010Dynamic Parameters for Cross Document CorefereceOctavian Popescupapsi@racai.roRacai, Romanina AcademyAbstractIn this paper we present a new algorithm forthe Person Cross Document Coreference task.We show that accurate results require a way toadapt the parameters of the similarity function?
metrics and threshold ?
to the ontologicalconstraints obeyed by individuals.
The tech-nique we propose dynamically changes the ini-tial weights computed when the context is ana-lyzed.
The weight recomputation is necessaryin order to resolve clusters borders, which areinevitably blurred by a static approach.
The re-sults show a significant gain in accuracy.1 IntroductionThe Person Cross Document Coreference, CDC,task requires that all the personal name mentions,PNMs, in a corpus be clustered together accord-ing to the individuals they refer to (Grishman1994).
The coreference between two PNMs isdecided on the basis of the local contexts.
In thispaper we consider a news corpus, and the localcontext is the piece of news to which a particularPNM belongs.
We work on a seven year Italianlocal newspaper corpus, Adige 500K (Magniniet.
al.
2006).While there are certain similarities between adisambiguation task and the CDC task, we main-tain that there is a significant difference whichsets the CDC task apart.
Unlike in other disam-biguation tasks, in the CDC tasks the relevantcoreference context depends on the corpus itself.In word sense disambiguation, for instance, thedistribution of the relevant context is mainly re-gulated by strong syntactic and semantic rules.The existence of such rules allows for disambig-uation decisions which are made by consideringthe local context only.
On the other hand, thedistribution of the PNMs in a corpus is ratherrandom and the relevant coreference context is adynamic variable which depends on the diversityof the corpus, that is, on how many different per-sons with the same name share a similar context.Unlike the word senses which are subject tostrong linguistic constraints, the name distribu-tion is more or less random.
To exemplify, con-sider the name ?John Smith?
and an organiza-tion, say ?U.N.?.
The extent to which ?works forU.N.?
in ?John Smith works for U.N.?
is a rele-vant coreference context depends on the diversityof the corpus itself.
If in that corpus, among allthe ?John Smiths?
there is only one person whoworks for ?U.N.?
then ?works for U.N.?
is a re-levant coreference context, but if there are many?John Smiths?
working for U.N., then ?works forU.N.?
is not a relevant coreference system.In this paper we present a method to exactlydetermine the relevance of a piece of context forthe coreference.
As above, the exactness is un-derstood in relationship with the whole system ofclusters.
The relevance of a piece of context iscomputed by means of a weighting procedure.The classic weighting procedures are static, eachpiece of context receives an initial value that isalso a final one and the clustering proceeds onthe basis of these values.
We demonstrate thatthis approach has serious drawbacks and we ar-gue that in order to obtain accurate results, a dy-namic weighting procedure is necessary, whichoutputs new values depending on the cluster con-figuration.In Section 2 we review the relevant literature.In Section 3 we present the problems related tothe classical approach to the CDC task and wepresent evidence that the data distribution in anews corpus requires a proper treatment of these988problems.
In Section 4 we present the techniquethat permits to overcome the problems identifiedin Section 3.
In Section 5 we present the contextextraction technique that supports the methoddeveloped in Section 4.
In Section 6 we presentthe results of an evaluation experiment.
The pa-per ends with Conclusion and Further Work sec-tion.2 Related WorkIn a classical paper (Bagga and Baldwin 1998), aPCDC system based on the vector space model(VSM) is proposed.
While there are many advan-tages in representing the context as vectors onwhich a similarity function is applied, it has beenshown that there are inherent limitations asso-ciated with the vectorial model (Popescu 2008).These problems, related to the density in the vec-torial space (superposition) and to the discri-minative power of the similarity power (mask-ing), become visible as more cases are consi-dered.Testing the system on many names, (Gooi andAllan, 2004), it has been noted empirically thatthe accuracy of the results varies significantlyfrom name to name.
Indeed, by considering justthe sentence level context, which is a strong re-quirement for establishing coreference, a PCDCsystem obtains a good score for ?John Smith?.This happens because the prior probability ofcoreference of any two ?John Smiths?
mentionsis low, as this is a very common name and noneof the ?John Smiths?
has an overwhelming num-ber of mentions.
But for other types of names thesame system is not accurate.
If it considers, forinstance, ?Barack Obama?, the same system ob-tains a very low recall, as the probability of anytwo ?Barack Obama?
mentions to corefer is veryhigh and the relevant coreference context is veryoften found beyond the sentence level.
Withoutfurther adjustments, a vectorial model cannotresolve the problem of considering too much ortoo little contextual evidence in order to obtain agood precision for ?John Smith?
and simulta-neously a good recall for ?Barack Obama?.These types of name have different cluster sys-temsIn an experiment using bigrams (Pederson etal.
2005) on a news corpus, it has been observedthat the relationship between the amount of in-formation given to a CDC system and the per-formances is not linear.
If the system has re-ceived in input the correct number of personswith the same name, the accuracy of the systemhas dropped.
A typical case for this situation iswhen there is a person that is very often men-tioned, and few other persons that have few men-tions.
When the number of clusters is passed inthe input, the clusters representing the personswho are rarely mentioned are wrongly enriched.However, this situation can be avoided if there isa measure of how big the threshold should be.The system of clusters is not developed unrealis-tically if we are able to handle the fact that indi-viduals obey different constraints which are de-rived directly from the ontological properties.These constraints are determined directly fromthe context and adequate weights can be set.Recently, there has been a major interest in theCDC systems, and, in the last two years, two im-portant evaluation campaigns have been orga-nized: Web People Search-1 (Artiles et al 2007)and ACE 2008 (www.nist.gov/speech/tests/ace/).It has been noted that the data variance betweentraining and test is very high (Lefever 2007).
Ra-ther than being a particularity of those corpora,the problem is general.
The performances of abag of words VSM depends to a very high extenton the corpus diversity (see Section 3.2).
For re-liable results, a CDC system must have access toglobal information regarding the coreferencespace.Rich biographic facts have been shown to im-prove the accuracy of CDC (Mann and Ya-rowsky 2003).
Indeed, when available, the birthdate, the occupation etc.
represent a relevant co-reference context because the probability thattwo different persons have the same name, thesame birth date and the same occupation is neg-ligible.
However, it is equally unlikely to findthis information in a news corpus a sufficientnumber of times.
Even for a web corpus, wherethe amount of this kind of information is higherthan in a news corpus, the extended biographicfacts, including e-mail address, phones, etc., con-tribute only with approximately 3% to the totalnumber of coreferences (Elmacioglu et al 2007).In order to improve the performances of the CDCsystems based on VSM, the special importanceof pieces of context has been exploited by im-plementing a cascade clustering technique (Wei2006).
Other authors have relied on advancedclustering techniques (among others Han et al2005, Chen 2006).
However, these techniquesrely on the precise analysis of the context, whichis a time consuming process.
It has been alsonoted that, in spite of deep analysis, the relevantcoreference context is hard to find (Vu 2007).9893 Coreference Based on Association SetsThe coreference of two PNMs is realized on thebasis of the context.
In a news corpus, the con-text surrounding each PNM, which is relevant forcoreference, is extracted into a set, called associ-ation set.
In Table 1 we present an example ofassociation sets related to the same name.Name Associated SetsPaolo RossiTV, comedian, , satireresearch, conferencepolitics, meetingTable 1: Associated SetsA weighting schema, a global metrics andthreshold are set, and the distance between twoassociation sets is computed.
The decision ofcoreferencing two PNMs is made on comparingthe distance to the threshold and clustering thePNMs representing the same individual into aunique cluster.
The accuracy of a CDC systembased on association sets depends on two factors:(1) the ability to extract the relevant elements forthe association sets from the news context and(2) the adequacy of the similarity formula - me-trics and threshold.Regarding the first factor, the ability to extractthe relevant pieces of context, the right heuristicsmust be found, because the exact syntax-semantics analysis of text is unfortunately veryhard or impossible to implement.
A strong limi-tation comes from the fact that even a shallowparsing requires too much time in order to bepractical.
However, it has been shown that accu-rate parings of PNMs and co-occurring specialwords can be found by employing relaxed extrac-tion techniques (Buitelaar&Magnini 2005).
Theassociation sets built in this way are effective insolving the CDC task (Sekine 2008, Popescu2008).
We make use of these findings in order tobuild the association sets, which mainly includenamed entities and certain special words, whichare bound to an ontology.
The details of theseparticular association sets are given in Section 5.As straightforward as the classical approachbased on the distance between association setsmay seem, there are actually a series of problemsrelated to the second requirement, namely theadequacy of similarity formula.
We make theseproblems explicit below.3.1 Masking, Superposition and BorderProximityIn order to introduce the first problem we startwith an intuitive example.
Suppose that we wantto individuate the persons with the name MichaelJackson in a news corpus.
A simplistic solution isto cluster together all such PNMs and declarethat than there is just one person mentioned inthe whole corpus with this name.
This solutionhas the advantage of being very simple and ofobtaining a very high score in terms of precisionand recall.
This is because most of such PNMsrefer to only one person indeed ?
the pop star.However, the above method fails short when itcomes to presenting the evidence for its corefe-rence decision.
Actually, it turns out that this is avery hard task, because the number of PNMs,which do not refer to the pop star, is extremelysmall.
Thus, the prior chances of correctly find-ing two PNMs which do not refer to this personare quite small.
Unfortunately, the classical me-trics are too coarse to capture the difference insuch cases, even if the association sets are 100%correct.
To support this statement, let us considerthree classes under the same name, with eachclass corresponding to a different individual.
Letus further suppose that two classes contain thegreat majority of the PNMs, and the third classonly has a small number of PNMs.
A linear deci-sion is likely to confound the elements of thethird class to the ones of the first two1.
This hap-pens because the elements of the third class aretransparent to the hyper plane that separates thetwo well-represented classes.
This situation iscalled masking, and is a direct effect of applyingan inaccurate weighting schema and metrics(Hastie&Tibshirani 2001).
The effects of mask-ing on the CDC task have been empirically no-ticed in (Pederson 2005).
The main obstacle indealing with masking is the correct treatment ofthe border elements.
?ij, the discrimant functionbetween two classes, i and  j respectively, mustassign zero to all border elements.
In Section 4,we directly address this problem.The second problem that needs to be solved bythe CDC systems based on associated sets maybe regarded as the negative effect of counter ba-lancing the sparseness problem.
In general, theassociation sets  are too sparse to permit pair topair comparison.
Rather, the information must beinterpolated from a set of corefered associationsets.
For example, in Figure 1, any two associa-tion sets chosen from the three ones on the left,AS1, AS2 and AS3 respectively, are similar1In fact any decision functions that can be bijectivelytransformed into a linear function, like most exponen-tial kernel functions for example, are similarly proneto masking.990enough to one another to corefer.
However, noneof these association sets is similar enough to theone on the right ?
AS4.
But accepting the corefe-rence of any initial pair, in this particular case,we implicitly accept the coreference with thefourth one.Figure 1.
InterpolatingBy interpolating the information in the set ofthe initial three association sets, the coreferencebecomes possible between all four associationsets.
In general, by interpolating from a set of theassociation sets, one wants to find the right core-ferences and to avoid the false ones accurately.In a vector space, the interpolation is safe if theinitial vectors are orthogonal to each other, be-cause the sum of orthogonal vectors is also or-thogonal to any other vector that is not part of thesum.
Therefore the right coreferences have a bigdot product with the sum, while the false oneshave a dot product with the sum close to zero.This property of the sum of the orthogonal vec-tors is called superposition (Gallant 1993).
Byrepresenting the association sets as vectors,where each set of vectors is associated exclusive-ly with a certain individual, the sum of these vec-tors has the superposition property.However, if the vectors representing the asso-ciation sets are not orthogonal, then the interpo-lated vectors are prone to false coreferences.
Inthis case, the accidental coincidences ?
which areresponsible for the original vectors not being or-thogonal ?
biases the dot product and introducesfalse coreferences.
Consequently the superposi-tion affects negatively the overall accuracy.
Theaggravating effect of superposition in conjunc-tion with an agglomerative clustering procedurehas been empirically noted in Gooi&Allan.The third problem is directly related to the factthat in the most ambiguous cases the associationsets lead to high-dimensional, very sparse vec-tors.
The basic fact is that inside a cluster of cor-rectly corefered PNMs that refer to the same in-dividual, the distance from most of these PNMSto the center of the cluster is smaller than the dis-tance from these PNMs to the border.
Let us con-sider that all the m PNMs representing the sameindividual are points in an n dimensional vectorspace and their cluster is normalized to the unitsphere.
The distance from the center of thesphere to the closest point is an exponentiallygrowing formula both in 1/n and 1/m.
Even forsmall values, the distance from the center to theclosest point is larger than ?.
The pointsrepresenting the PNMs in the same cluster arecloser to the border, and not to the center of thesphere.
This is a secondary effect of the curse ofdimensionality problem in the vector space2.3.2 Data DistributionLet us consider the corpus, focusing on the dis-tribution of PNMs.
Many PNMs are the mentionsof the same name, considered as a string.
We areinterested in the frequency with which a certainname appears.
We have noticed that there is astrict relationship between the names, their fre-quencies and the number of mentions; see Table2.Freq  PNM # PNM1 317,245 317,2452 ?
5 166,029 467,5606 ?
20 61,570 634,30921 ?
100 25,651 1,090,836101 ?
1000 7,750 2,053,9941001 ?
2000 4,25 569,6272001 ?
4000 157 422,5854001 ?
5000 17 73,8605001 ?
31091 22 190,373Table 2 Frequency of Names and PNMs in Adige500kThe names have a very unbalanced distribu-tion.
A name which has a frequency over 20 andis ambiguous represents a difficult case.
Themeasure we use in order to evaluate the difficultyis the Gini?s mean difference.
Let X1, X2, ?, Xnbe the individuals that are named with the samename and let S be the set of the PNMs of thisname PNMS, S1, S2, ?
Sn.
The Gini?s mean dif-ference is a measure of the spread of the informa-tion in the set S: (1)The uniform distribution makes Gini?s factornull.
A value of this factor close to 1 shows askewed distribution.
In the first case, G ?
0, thesuperposition effect is likely to be responsible forfalse coreferences, while in the latter case, G ?
1,2The curse of dimensionality refers to the fact that thenumber of sample points required to state confidentvalues for a statistics grows exponentially with thedimension of vector space.991the masking effect is predominant.
However,there is a close relationship between all the threeproblems above.
As the most ambiguous casesare near the border, it is likely that the vectors arenot orthogonal and consequently the false corefe-rences are introduced in the system, which ulti-mately leads to masking.4 Resolving the Border ConditionWe are going to present a technique developed todeal with the problems identified in the previoussection.
The bottom line is that the weights andthe threshold required by the similarity functionof two association sets should be dynamicallycomputed.
In this way the border between anypair of clusters can be accurately set.We present the procedure of adjusting theweights and the threshold for a given group ofclusters in order to maximize the probability ofthe correct coreferences.
The first step is topresent the construction of the association sets,with initial weight values.
The second step is toshow how these initial weight values are recom-puted for a set of given clusters.InitializationAs mentioned in the first paragraph of Section3, the association sets are built out of the sur-rounding context by considering the named enti-ties, and special words.
The named entities areclearly marked in the input, the corpus havingbeing tagged by a Named Entities Recognitiontool.
The words considered special are identifiedusing an ontology and the procedure is given inSection 5.
The construction of the association setis a search procedure starting from the PNM.
Thefirst search space is the longest nominal groupwhich is headed  by a PNM:uno dei falchi dell' amministrazione di Stati Unitiguidata dal presidente George W.Bushone of the falcons of the U.S. administration leadby the president Georg W. BushAll the special words that are present in this no-minal group are included in the association set ofthis PNM.
In this example, these special wordsare ?president?
and ?administration?
respective-ly.
The named entity ?U.S.?
is also included.These elements receive the highest weights.
Thesearch space is extended to the sentence leveland new named entities/special words are in-cluded.
However, unlike in the first phase, theweight of these words is determined on the basisof a second parameter, namely the number ofdifferent names interfering between the PNMand these words.
We take into considerationthree values 0, 1 and 2 or more.
After the sen-tence, the next search domain is the whole news.Basically, the significance of an element de-creases linearly with the distance and the numberof other interfering PNMs.
In Table 3 we presentthe linear kernel weighting schema describedabove.
The series ?ij is decreasing linearly overboth indexes.Interfering PNMsDomain 0 1 ?2PNM Group ?11 ?12 ?13IN Sentence ?21 ?22 ?23Out Sentence ?31 ?32 ?33Table 3.
Linear Kernel for Initial WeightsRecomputationThe association set is basically a pair of twovectors: X = (x1, ?, xn) the set of words and W =(w1, ?, wn) the set of the initial weights.
TwoPNMs corefer or not depending on whether thesum of their common part is bigger, respectivelylesser than a threshold.  	  (2)  	 !
(3)Suppose now that we have an independentway to know the truth regarding the coreference.Then, we have to readjust the initial weights suchthat the real configuration of clusters is promotedalso by Equations (2) and (3).
For clarity, let usgive an example: suppose that we know that inour corpus there is only one person named ?Ro-berto Bizzo?
and only one person named ?Rober-to Cuillo?, and no other person is called ?Rober-to?.
Consequently the PNMs ?Roberto?
are clus-tered to the clusters ?Robert Bizzo?
xor ?RobertoCuillo?.
Suppose further that the named entity?Roma?
is associated with some of the PNMs?Roberto?.
If only ?Roberto Bizzo?
is associatedwith ?Roma?, then the coreference between those?Roberto?
associated with ?Roma?
and ?RobertoBizzo?
can be made.
However, it is often the992case that both ?Roberto Bizzo?
and ?RobertoCuillo?
are associated with ?Roma?, which hasits particular weight for each PNM.
In this casethis named entity, ?Roma?, may bear no relev-ance for the coreference of ?Roberto?
in either ofthe clusters.
Consequently, whatever the initialvalue for ?Roma?
in certain association sets, itmust be nullified.
In order to find out which ele-ments of the association sets are relevant, andwhat weights the relevant elements must have,we propose the following strategy: we replacethe ?Roberto Bizzo?
with ?Roberto X?, and ?Ro-berto Cuillo?
with ?Roberto X?.
We obtain a bigset of association sets corresponding to thePNMs ?Roberto X?.
We reweight the elementsof their association sets and the threshold, suchthat, from this set of association sets, we obtainexactly two clusters, one that is identical with?Roberto Bizzo?, and one that is identical with?Roberto Cuillo?.
Conceptually, this strategy issimilar to the pseudo words technique used inbuilding test corpora.
After the reweighting ofthe elements associated with ?Robert Bizzo?
and?Roberto Cuillo?
respectively, we can associatethe simple PNMs ?Roberto?
to one of these twoclusters.In the above example we make use of the factthat if two persons have different last names thenthey are different persons.
This is a prior onto-logical constraint.
In fact, whenever we know theset of ontological constraints that correctly clus-ter a set of PNMs in two or more clusters, we canintentionally confound the PNMs, recompute theweights and the thresholds of their associationsets, in order to obtain the initial cluster configu-ration.
Now we use the new computed values tocluster new PNMs whose relationship with theontological constraints could not have been de-termined from the corpus.We show that we can use the Simplex methodto recompute the initial weights.
Indeed, by in-tentionally confounding a system of clusters, wedetermine the coefficients which, when multip-lied with the initial weights, lead to the correctclustering.
These coefficients are the solution toa set of inequalities like those presented in Equa-tions (2), and (3).
The objective function inSimplex is a max or a min depending on whetherwe know that the PNMs corefer or not: if they donot corefer then there is a max Simplex system,and the threshold is just higher than the value ofthe objective function.
Let us give an example.Suppose we have the following configuration,where ASi represents the association set of thePNMi, where wi is the vector of the initialweights and T is the threshold:                 .AS1 ?
AS2 = {x1, x2, x3} wi = (1, 2, 2) T = 7  No Coreference x1 +2x2+2x3?
7AS1 ?
AS3 = {x1, x3}  wi = (5,0,4) T = 11  Coreference 5x1 +4x3?
11AS2 ?
AS4 = {x2, x3}  wi = (0,3,4) T = 9  Coreference 3x2 +4x3?
9AS5 ?
AS6 = {x1, x2}  w i= (2,1,0) T = ?
No Coreference max (2x1 +x2)The above cluster configuration leads to thefollowing Simplex system:max 2x1 +x2"#$ % # % #&'(#$ % )#&$$&# % )#&*+which has the solution wr = (1.55, 1.91, 0.82)with max = 5.
Therefore the initial weights forthe elements x1, x2, x3 must be multiplied with1.55, 1.91, 0.82 respectively and the appropriatethreshold for making a decision is 5.01.5 Ontological Constrained Association SetsIn the preceding section we presented a strategybased on Simplex Algorithm developed for theborder weight assignment.
The similarity formu-la is recomputed such that a set of ontologicalrestriction is satisfied.
In this section we presentthe way the set of ontological restrictions isfound.
The set of special words is identified onthe basis of an ontology.
We have used SUMO(Niles 2003) because it has the advantage that itshierarchies are connected to the WordNet, whichis a Multilanguage aligned resource.
Below wepresent the main categories of the SUMOattributes used.
Summing up, there are more than7 000 special words taken into account.CorporationOrganizationOccupational RoleOccupies PositionSocial InteractionSocial RoleUnemployedThere are mainly three different ways to createthe set of ontological restrictions: fixed, priorontological constraints, local restrictions and ex-clusive ontological relationships.The fixed, prior ontological constraints arethose that tend to be expressed in a fixed pattern,making it easy to identify them in the context.Usually they express the date and place of birth,993contact information, but also the gender, the fam-ily relationship, the ethnic group etc.The local restrictions are a very rich source ofinformation.
It has been argued that inside eachpiece of news the coreference of all the PNMs isa valid procedure, with more than 99% accuracy(Popescu et al 2008).
By comparing the structureof the largest nominal group headed by two lo-cally corefered PNMs we can found ontologicalcompatibilities.
Table 4 shows a sample of thecompatible pairs as extracted from corpus.
Thesepairs can be used successfully for coreferencingpurposes, but these do not form ontological hie-rarchies and cannot be used to build inferencechains.Pairs of compatible professionsalbergatore comercianteala giocatoreagronomo professoreallenatore misteralpinista guida alpinaarchitetto progettistaarcivescovo monsignoremonsignore teologomonsignore sacerdoteassessore consigliereTable 4.
Compatible Occupational RoleThe exclusive ontological relationships aregiven explicitly under the form of rules.
Theserules stipulate what is ontologically inacceptable.We have seen an example of such rules referringto the family names in Section 4.
The Occupa-tional Role and Social Role attributes are one ofthe most useful exclusive ontological ones, be-cause they are frequently mentioned in a newscorpus.
In average, local information at the newslevel produces a special word from the abovecategories in approximately in 30% of cases(Magnini et al2006.).
An example of the realiza-tion of the exclusive rules for a sample of multipairs of words as extracted from corpus is pre-sented below:Secretary?Priest?JudgeArchitect?AttorneyWaiter?ManagerActor?ResearcherThe system of clusters determined using thetechnique described in Section 4 obeys the set ofthese constraints.
The set C of ontological con-straints are used to generate active rules at theword level, which, by means of fixed text pat-terns, are compared against the association sets.This permits the realization of ontological moti-vated cluster systems, which in combination withthe technique of reweighting presented, leads toaccurate new coreferences outside the scope ofC, while avoiding the border problems presentedin Section 3                         ..Figure 2.
The dynamic reweighting schema flow6 EvaluationThe technique we propose is designed for an ac-curate border detection between clusters of am-biguous names.
We created a sample of the am-biguous names.
For each name we computed theGini?s mean difference using the formula intro-duced in Section 3, which gives an indication ofthe spread of information relevant for corefe-rence.
We have noticed that there is a strong cor-relation between the Gini?s mean difference andthe difficulty of a coreference system.
The nameschosen for this experiment are such that the Gi-ni?s factor uniformly distributed in (0,1).
How-ever, the number of PNMs for each name is big-ger than the number of individuals having thatname.
The choice is motivated by the fact thatthese are the most difficult cases for a CDC sys-tem, as they require strong and consistent evi-dence for accurate results.
The opposite cases,when the number of the individuals is close tothe number of PNMs or the Gini?s coefficient is994close to 0 or 1, can be approached with a purestatistical approach (Popescu 2009).The first column in Table 5 lists the names, thesecond column lists the number of the PNMsconsidered for each name, the third column liststhe number of individuals having the respectivename, the fourth column lists the number ofPMNs for each individual, the fifth column liststhe Gini?s factor and the sixth column lists howmany clusters have been found obeying ontolog-ical constraints/ and how many PNMs have beenclustered in these clusters.Name #PNMs #P Distribution Gini ConstraintsAngelo Elia 58 5 {20,24,7,2,2,3} .428 2 / 18Gifuni 89 3 {47,21,31} .175 3/ 12Giuseppe Rossi 185 12 {69,32,5,9,4,5,6,6,12,7,8,22} .503 5 / 38Paulo Rossi 137 9 {91,17,9,3,2,3,5,5,2} .673 3 / 74Schlesinger 62 4 {26,19,6,11} .274 4 / 19Tanzi 370 3 {315,49,16} .524 3/129Table 5.
Name Test SetWe compare the technique proposed in Section 4(DYN) against three different approaches: thefirst is a no weight coreference, requiring a fixnumber of similar elements in the association set(NOW), the second is Baga&Baldwin quadraticmetric formula at sentence level (BB), and thethird is an agglomerative vector space clusteringalgorithm as in Gooi&Allan(GA).
All these threeapproaches use fixed similarity parameters.The evaluation is done using the B-CUBED al-gorithm (Baga&Baldwin).
The results, computedwith F formula, are presented in Table 6.Name NW BB GA DYNAngelo Elia .426 .639 .684 .672Gifuni .53 .635 .661 .726Giuseppe Rossi .481 .619 .589 .673Paulo Rossi .446 .623 .598 .691Schlesinger .528 .588 .723 .829Tanzi .572 .539 .699 .815Average .417 .607 .659 .734Table 6.
F-formula on B-CUBEDThe BB and GA have been tested on the JohnSmith corpus, which contains the PNMs of justone name, John Smith.
As John Smith is a verycommon name and no famous person carries it,this corpus is rather biased as the Gini?s factor issmall; that is why BB performs better than GAon ?Giuseppe Rossi?
and ?Paulo Rossi?.
TheDYN scores the best , gaining in average 7 pointsin F formula.Conclusion and Further WorkIn this paper we present a new technique for theCDC task which allows us to dynamicallychange the weights in the association sets in or-der to accurately account for border cases.
As weshowed in Section 3, the border cases are actual-ly the most important ones due to the high di-mensionality of the vector space which modelsthe association sets.The results we have obtained are superior toother approaches.
We think that this is possiblebecause the technique we used directly addressesthe problem related to masking and superposi-tion.We plan to further study this technique by fol-lowing mainly three directions.
First, we want tostudy further the behavior of masking and super-position within a larger test corpus.
Second, wewant to extend the set of exclusive ontologicalrelationships which can be determined from thecontext with shallow text analysis.
Third, wewant to understand better the ways in which theset of ontological constraints interact with thevector space in order to increase the overall accu-racy of the coreference system.A secondary effect of the proposed techniqueis that a stronger control of the inferences result-ing from a cluster system can be obtained.
In thefuture this seems to be a promising method tolink the coreference tasks to the chain of infe-rences.995ReferencesJ.
Artiles, Gonzalo, J., S. Sekine.
2007.
Establishinga benchmark for WePS.
In Proceedings of SemEval.A.
Bagga, B. Baldwin.
1998.
Entity-based Cross-Document Co-referencing using the VectorSpace Model.
In Proceedings of ACL.J.
Chen, D. Ji, C. Tan, Z. Niu.
2006.
UnsupervisedRelation Disambiguation Using Spectral Cluster-ing.
In Proceedings of COLINGC.
Gooi, J. Allan.
2004.
Cross-Document Corefe-rence on a Large Scale Corpus.
In Proceedings ofACL.G.
Mann, D. Yarowsky.
2003.
Unsupervised NameDisambiguation, in Proceeding of HLT-NAACLI.
Niles, A. Pease, 2003.
Linking Lexicons and Ontol-ogies:  Mapping WordNet to the Suggested UpperMerged Ontology, in Proceeding IKER.
Grishman.
1994.
Whither Written LanguageEvaluation?
In Proceedings of Human LanguageTechnology Workshop, pp.
120-125.
San Mateor.E.
Elmacioglu, Y. M. F. M.Y.Khan, D. Lee.
2007.PSNUS: Web People Name Disambiguation bySimple Clustering with Rich Features, in Proceed-ings of SemEvalH.
Han, W. Xu.
2005.
A Hierarchical Bayes Mix-ture Model for Name Disambiguation in AuthorCitations, in Proceedings of SAC?05E.
Lefever, V. Hoste, F. Timur.
2007.
AUG: A Com-bined Classification and Clustering Approach forWeb People Disambiguation, In Proceedings ofSemEvalB.
Magnini, M. Speranza, M. Negri, L. Romano, R.Sprugnoli.
2006.
I-CAB ?
the Italian Content An-notation Bank.
LREC 2006V., Ng.
2007.
Shallow Semantics for CoreferenceResolution, In Proceedings of IJCAIT.
Pedersen, A. Purandare, A. Kulkarni.
2005.
NameDiscrimination by Clustering Similar Contexts, inProceeding of CICLINGO.
Popescu, C. Girardi.
2008.
Improving CrossDocument Coreference, in Proceedings of JADTO.
Popescu, B. Magnini.
2007.
Inferring Corefe-rence among Person Names in a Large Corpusof News Collection, in Proceedings of AIIAO.
Popescu 2009.
Name Perplexity.
In Proceed-ings of NAACL HLTP.
Buitelaar, B. Magnini (Eds.)
2005.
OntologyLearning from Text: Methods, Evaluation andapplications.
IOS PressQ.
Vu, T. Massada, A. Takasu, J. Adachi.
2007.
Us-ing Knowledge Base to Disambiguate Personalnames in Web Search Results, In Proceedings ofSACT.
Hastie, R. Tibshirani, J. Friedman, 2001.
The ele-ments of Statistical Learnig, Springer PressS.
Gallant, Neural Network Learning, MIT PressS.
Sekine, 2008 Extended Named Entity Ontologywith Attribute Information, in Proceeding of LRECY.
Wei, M. Lin, H. Chen.
2006.
Name Disambigua-tion in Person Information Mining, in Proceedingsof IEEE996
