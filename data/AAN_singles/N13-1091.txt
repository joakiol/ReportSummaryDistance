Proceedings of NAACL-HLT 2013, pages 752?757,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTruthTeller: Annotating Predicate TruthAmnon LotanDepartment of LinguisticsTel Aviv Universityamnonlot@post.tau.ac.ilAsher Stern and Ido DaganDepartment of Computer ScienceBar Ilan Universityastern7@gmail.com dagan@cs.biu.ac.ilAbstractWe propose a novel semantic anno-tation type of assigning truth valuesto predicate occurrences, and presentTruthTeller, a standalone publicly-available tool that produces such annota-tions.
TruthTeller integrates a rangeof semantic phenomena, such as nega-tion, modality, presupposition, implicativ-ity, and more, which were dealt only partlyin previous works.
Empirical evaluationsagainst human annotations show satisfac-tory results and suggest the usefulness ofthis new type of tool for NLP.1 IntroductionIn a text, the action or relation denoted by ev-ery predicate can be seen as being either pos-itively or negatively inferred from its sentence,or otherwise having an unknown truth status.Only in (3) below can we infer that Gal soldher shop, hence the positive truth value of thepredicate sell, while according to (2) and (4) Galdid not sell it, hence the negative truth values,and in (1) we do not know if she sold it or not(the notations pt+, pt- and pt?
denote truthstates, defined in Subsection 2.3).
Identifyingthese predicate truth values is an important sub-task within many semantic processing scenarios,including various applications such as QuestionAnswering (QA), Information Extraction (IE),paraphrasing and summarization.
The follow-ing examples illustrate the phenomenon:(1) Gal made an attempt pt+ to sell pt?
hershop.
(2) Gal did not try pt?
to sell pt?
her shop af-ter hearing pt+ the offers.
(3) Maybe Gal wasn?t smart pt?
to sell pt+ hershop.
(4) Gal wasn?t smart pt?
enough to sell pt?
theshop that she had bought pt+.Previous works addressed specific aspects ofthe truth detection problem: Nairn et al(2006), and later MacCartney & Manning (2007;2009), were the first to build paraphrasing andinference systems that combine negation (see tryin (2)), modality (smart in (3)) and ?naturallogic?, a recursive truth value calculus (sell in(1-3)); recently, Mausam et al(2012) built anopen IE system that identifies granulated vari-ants of modality and conditions on predicates(smart in (3)); and Kiparsky & Kiparsky (1970)and Karttunen (1971; 2012) laid the groundwork for factive and implicative entailment cal-culus (sell in (1-4)), as well as many genericconstructions of presupposition (hearing in (2)is presupposed because it heads an adverbialclause and bought in (4) heads a finite relativeclause), which, to our knowledge, have not yetbeen implemented computationally.
Notice inthe examples that presuppositions persist undernegation, in questions and if-clauses, while en-tailments do not.
In addition, there is a growingresearch line of negation and modality detection.See, for example, Morante & Daelemans (2012).752We present TruthTeller1, a novel algo-rithm and system that identifies the truth valueof each predicate in a given sentence.
It anno-tates nodes in the text?s dependency parse-treevia a combination of pattern-based annotationrules and a recursive algorithm based on natu-ral logic.
In the course of computing truth value,it also computes the implicativity/factivity sig-nature of predicates, and their negation andmodality to a basic degree, both of which aremade available in the system output.
It ad-dresses and combines the aforementioned phe-nomena (see Section 2), many of which weren?tdealt in previous systems.TruthTeller is an open source and pub-licly available annotation tool, offers a relativelysimple algebra for truth value computation, andis accompanied by a publicly available lexiconof over 1,700 implicative and factive predicates.Also, we provide an intuitive GUI for viewingand modifying the algorithm?s annotation rules.2 Annotation Types and AlgorithmThis section summarizes the annotation algo-rithm (a detailed report is available with the sys-tem release).
We perform the annotations overdependency parse trees, generated according tothe Stanford Dependencies standard (de Marn-effe and Manning, 2008).
For all verbs, nounsand adjectives in a sentence?s parse tree, we pro-duce the following 4 annotation types, given inthe order they are calculated, as described in thefollowing subsections:1.
Predicate Implication Signature (sig) - de-scribes the pattern by which the predi-cate entails or presupposes its complements,e.g., the verb refuse entails the negative ofits complements: Ed refused to pay entailsthat Ed didn?t pay.2.
Negation and Uncertainty (NU) - indicateswhether the predicate is modified by an un-certainty modifier like might, probably, etc.,or whether it?s negated by no, never etc.3.
Clause-Truth (CT) - indicates whether the1http://cs.biu.ac.il/~nlp/downloads/TruthTellerentire clause headed by the predicate is en-tailed by the complete sentence4.
Predicate Truth (PT) - indicates whetherthe predicate itself is entailed by the sen-tence, as defined belowBefore presenting the detailed definitions anddescriptions below, we give a high-level descrip-tion of TruthTeller?s algorithm, where eachstep relies on the results of its predecessor: a)every predicate in the parse tree is annotatedwith a predicate implication signature, identi-fied by lexicon lookup; b) NU annotations areadded, according to the presence of uncertaintymodifiers (maybe, might, etc.)
and negationmodifies (not, never, etc.
); c) predicates in cer-tain presupposition constructions (e.g., adver-bial clauses, WH arguments) are annotated withpositive CT values; d) the parse tree is depth-first scanned, in order to compute both CT andPT annotations by the recursive effects of fac-tives and implicatives; e) in conjunction withthe previous step, relative clause constructionsare identified and annotated with CT and PT.Except for steps a) and d), all of the pro-cedure is implemented as an ordered sequenceof annotation rule applications.
An annotationrule is a dependency parse tree template, pos-sibly including variables, which assigns certainannotations to any parse tree node that matchesagainst it.
Step a) is implemented with signa-ture lexicon lookups, and step d) is an algorithmimplemented in code.To illustrate this entire process, Figure 1presents the annotation process of a sim-ple sentence, step by step, resulting inTruthTeller?s complete output, fully speci-fied below.
Most other examples in this papershow only partial annotations for brevity.2.1 Predicate Implication SignatureOur system marks the signature of each predi-cate, as defined in Table 1.
There, each signa-ture has a left sign and a right sign.
The left signdetermines the clause truth value of the pred-icate?s complements, when the predicate is inpositive contexts (e.g., not negated), while theright sign applies in negative contexts (clause753# Sig Positive context example Negative context example1 +/- Ed managed to escape ?
Ed escaped Ed didn?t manage to escape ?
Ed didn?t escape2 +/?
Ed was forced to sell ?
Ed sold Ed wasn?t forced to sell ?
no entailments3 ?/- Ed was allowed to go ?
no entailments Ed wasn?t allowed to go ?
Ed didn?t go4 -/+ Ed forgot to pay ?
Ed didn?t pay Ed didn?t forget to pay ?
Ed paid5 -/?
Ed refused to fight ?
Ed didn?t fight Ed didn?t refuse to fight ?
no entailments6 ?/+ Ed hesitated to ask ?
no entailments Ed didn?t hesitate to ask ?
Ed asked7 +/+ Ed was glad to come ?
Ed came Ed wasn?t glad to come ?
Ed came8 -/- Ed pretended to pay ?
Ed didn?t pay Ed didn?t pretend to pay ?
Ed didn?t pay9 ?/?
Ed wanted to fly ?
no entailments Ed didn?t want to fly ?
no entailmentsTable 1: Implication signatures, based on MacCartney & Manning (2009) and Karttunen (2012).
The firstsix signatures are named implicatives, and the last three factive, counter factive and regular, respectively.a) Annotate signatures via lexicons lookupGal wasn?t allowed?/?
to come?/?b) Annotate NUGal wasn?t allowed?/?,nu?
to come?/?,nu+c) Annotate CT to presupposition constructionsGal wasn?t allowed?/?,nu?,ct+ to come?/?,nu+,ct+d) Recursive CT and PT annotationGal wasn?t allowed?/?,nu?,ct+,pt?
tocome?/?,nu+,ct?,pt?e) Annotate CT and PT of relative clauses(has no effect on this example)Gal wasn?t allowed?/?,nu?,ct+,pt?
tocome?/?,nu+,ct?,pt?Figure 1: An illustration of the annotation processtruth is defined in Subsection 2.3).
See exam-ples for both context types in the table.
Eachsign can be either + (positive), - (negative) or?
(unknown).
The unknown sign signifies thatthe predicate does not entail its complements inany way.Signatures are identified via lookup, using twolexicons, one for single-word predicates and theother for verb+noun phrasal verbs, e.g., take thetime to X.
Our single-word lexicon is similar tothose used in (Nairn et al 2006) and (Bar-Haimet al 2007), but is far greater, holding over1,700 entries, while each of the previous two has,to the best of our knowledge, less than 300 en-tries.
It was built semi automatically, out ofa kernel of 320 manually inspected predicates,which was then expanded with WordNet syn-onyms (Fellbaum, 1998).
The second lexiconis the implicative phrasal verb lexicon of Kart-tunen (2012), adapted into our framework.
The+/?
implicative serves as the default signaturefor all unlisted predicates.Signature is also sensitive to the type of thecomplement.
Consider:(6) Ed forgot?/+ to call pt?
Joe(7) Ed forgot+/+ that he called pt+ JoeTherefore, signatures are specified separately forfinite and non finite complements of each pred-icate.After the initial signature lookup, two anno-tation rules correct the signatures of +/+ fac-tives modified by enough and too, into +/- and-/+, correspondingly, see Kiparsky & Kiparsky(1970).
Compare:(8) Ed was mad+/+ to go ?
Ed went(9) Ed was too mad?/+ to go ?
Ed didn?t goIn addition, we observed, like Karttunen (2012),that most verbs that have passive voice and theinto preposition become +/?
implicatives, e.g.,(10) Workers were pushed / maddened /managed+/?
into signing ?
They signed(11) Workers weren?t pushed / maddened /managed+/?
into signing?
It is unknownwhether they signedso we captured this construction in another rule.7542.2 Negation and Uncertainty (NU)NU takes the values {nu+, nu-, nu?
}, stand-ing for non-negated certain actions, negated cer-tain actions, and uncertain actions.
The firstNU rules match against a closed set of negationmodifiers around the predicate, like not, never,neither etc.
(see (2)), while later rules detectuncertainty modifiers, like maybe, probably, etc.Therefore, nu?
takes precedence over nu-.Many constructions of subject-negation,object-negation and ?double negation?
areaccounted for in our rules, as in:(12) Nobody was seennu?
at the site(13) Almost nobody was seennu+ at the site2.3 Clause Truth and Predicate TruthClause Truth (CT, denoted as ct(p)) corre-sponds to polarity of Nairn et al(2006).
Itrepresents whether the clause headed by a pred-icate p is entailed by the sentence, contradictedor unknown, and thus takes three values {ct+,ct-, ct?
}.Predicate Truth (PT) (denoted as pt(p)) rep-resents whether we can infer from the sentencethat the action described by the predicate hap-pened (or that its relation holds).
It is definedas the binary product of NU and CT:Definition 1.
PT = NU ?
CTand takes analogous values: {pt+, pt-, pt?
}.Intuitively, the product of two identical posi-tive/negative values yields pt+, a positive and anegative yield pt-, and nu?
or ct?
always yieldpt?.
To illustrate these definitions, consider:(14) Meg may have sleptct+,pt?
aftereatingct+,pt+ the meal Ed cookedct+,pt+,while no one was therect+,pt?After signatures and NU are annotated, CTand PT are calculated.
At first, we applya set of rules that annotate generic presup-position constructions with ct+.
These in-clude adverbial clauses opening with {while, be-fore, after, where, how come, because, since,owing to, though, despite, yet, therefore...},WH arguments (who, which, whom, what), andct(p) =??????????????????
?ct+ :p was already annotatedby a presupposition rulect(gov(p)) :p heads a relativeclausecompCT (p) :otherwise, and p isa complementct?
: otherwise (default)Figure 2: Formula of ct(p), for any predicate p.ct(gov(p)) is the CT of p?s governing predicate.parataxis2.
See for example the effects of afterand while in (14).Then, we apply the following recursive se-quential procedure.
The tree root always getsct+ (see slept in (14)).
The tree is then scanneddownwards, predicate by predicate.
At each one,we compute CT by the formula in Figure 2, asfollows.
First, we check if one of the aforemen-tioned presupposition rules already matched thenode.
Second, if none matched, we apply to thenode?s entire subtree another set of rules thatannotate each relative clause with the CT of itsgoverning noun3, ct(gov(p)) (see failed in (15)).Third, if no previous rule matched, and p is acomplement of another predicate gov(p), thencompCT(p) is calculated, by the following logic:when pt(gov(p)) is pt+ or pt-, the correspond-ing left or right sign of sig(gov(p)) is copied.Otherwise, if pt(gov(p)) = pt?, ct?
is returned,except when the signature of gov(p) is +/+ (or-/-) factive, which always yields ct+ (or ct-).Third, if nothing applied to p, ct?
is returnedby default.
Finally, PT is set, according to Def-inition 1.To illustrate, consider these annotations:(15) Gal managed+/?,ct+,pt+ abuilding+/?,ct+,pt+, which Gingerfailed?/+,ct+,pt+ to sell+/?,ct?,pt?First, managed gets ct+ as the tree root.
Then,we get compCT (building) = ct+, as the com-plement of managed+/?,pt+.
Next, a relativeclause rule copies ct+ from building to failed.2The placing of clauses or phrases one after another,without words to indicate coordination, as in ?veni, vidi,vici?
in contrast to ?veni, vidi and vici?.3We also annotate nouns and adjectives as predicatesin copular constructions, and in instances where nounshave complements.755Finally, compCT (sell) = ct- is calculated, asthe complement of failed?/+,pt+.3 EvaluationTo evaluate TruthTeller?s accuracy, we sam-pled 25 sentences from each of the RTE5 andRTE6 Test datasets (Bentivogli et al 2009;Bentivogli et al 2010), widely used for textualinference benchmarks.
In these 50 sentences, wemanually annotated each predicate, 153 in to-tal, forming a gold standard.
As baseline, wereport the most frequent value for each annota-tion.
The results, in Table 2, show high accuracyfor all types, reducing the baseline CT and PTerrors by half.
Furthermore, most of the remain-ing errors were due to parser errors, accordingto a manual error analysis we conducted.The baseline for NU annotations shows thatnegations are scarce in these RTE datasets,which was also the case for ct- and pt- an-notations.
Thus, Table 2 mostly indicatesTruthTeller?s performance in distinguishing pos-itive CT and PT annotations from unknownones, the latter constituting v20% of the goldstandard.
To further assess ct- and pt- annota-tions we performed two targeted measurements.Precision for ct- and pt- was measured by man-ually judging the correctness of such annotationsby TruthTeller, on a sample from RTE6 Testincluding 50 ct- and 124 pt- annotations.
Thistest yielded 78% and 83% precision, respectively.pt- is more frequent as it is typically triggeredby ct-, as well as by other constructions involv-ing negation.
Recall was estimated by employ-ing a human annotator to go through the datasetand look for ct- and pt- gold standard anno-tations.
The annotator identified 40 ?ct-?s and50 ?pt-?s, out of which TruthTeller found47.5% of the ?ct-?s and 74% of the ?pt-?s.
Insummary, TruthTeller?s performance on ourtarget PT annotations is quite satisfactory with89% accuracy overall, having 83% precision and74% recall estimates specifically for pt-.4 Conclusions and Future WorkWe have presentedTruthTeller, a novel algo-rithm and system that identifies truth values ofAnnotation TruthTeller BaselineSignature 89.5% 81% (+/?
)NU 98% 97.3% (nu+)CT 90.8% 78.4% (ct+)PT 89% 77% (pt+)Table 2: The accuracy measures forTruthTeller?s 4 annotations.
The right col-umn gives the accuracy for the correspondingmost-frequent baseline: {+/?, nu+, ct+, pt+}.predicates, the first such system to a) address orcombine a wide variety of relevant grammaticalconstructions; b) be an open source annotationtool; c) address the truth value annotation taskas an independent tool, which makes it possiblefor client systems to use its output, while pre-vious works only embedded annotations in theirtask-specific systems; and d) annotate unknowntruth values extensively and explicitly.TruthTeller may be used for several pur-poses, such as inferring parts of a sentencefrom the whole and improving textual entail-ment (and contradiction) detection.
It includesa novel, large and accurate, lexicon of predicateimplication signatures.While in this paper we evaluated the correct-ness of TruthTeller as an individual com-ponent, in the future we propose integratingit in a state-of-the-art RTE system and reportits impact.
One challenge in this scenario ishaving other system components interact withTruthTeller?s decisions, possibly masking itseffects.
In addition, we plan to incorporatemonotonicity calculations in the annotation pro-cess, like in MacCartney and Manning (2009).5 AcknowledgementsThis work was partially supported by the IsraelScience Foundation grant 1112/08 and the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).We thank Roni Katzir and Fred Landman foruseful discussions.756ReferencesRoy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of AAAI, pages871?876.Luisa Bentivogli, Bernardo Magnini, Ido Dagan,Hoa Trang Dang, and Danilo Giampiccolo.
2009.The fifth pascal recognizing textual entailmentchallenge.
In Preproceedings of the Text AnalysisConference (TAC).Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.Dang, and Danilo Giampiccolo.
2010.
The sixthPASCAL recognizing textual entailment chal-lenge.
In The Text Analysis Conference (TAC2010).Marie-Catherine de Marneffe and Christopher D.Manning.
2008.
The stanford typed dependenciesrepresentation.
In COLING Workshop on Cross-framework and Cross-domain Parser Evaluation.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database (Language, Speech,and Communication).
MIT Press.Lauri Karttunen.
1971.
Implicative verbs.
Lan-guage, 47:340?358.Lauri Karttunen.
2012.
Simple and phrasal implica-tives.
In *SEM 2012, pages 124?131.P.
Kiparsky and C. Kiparsky.
1970.
Fact.In Progress in Linguistics, pages 143?173.
TheHague: Mouton de Gruyter.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof ACL workshop on textual entailment and para-phrasing.Bill MacCartney and Christopher D. Manning.
2009.An extended model of natural logic.
In Proceed-ings of the Eighth International Conference onComputational Semantics (IWCS-8).Mausam, Michael Schmitz, Stephen Soderland,Robert Bart, and Oren Etzioni.
2012.
Open lan-guage learning for information extraction.
In Pro-ceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages523?534.Roser Morante and Walter Daelemans.
2012.
An-notating modality and negation for a machinereading evaluation.
In CLEF (Online WorkingNotes/Labs/Workshop).Rowan Nairn, Cleo Condoravdi, and Lauri Kart-tunen.
2006.
Computing relative polarity for tex-tual inference.
In In Proceedings of ICoS-5 (Infer-ence in Computational Semantics).757
