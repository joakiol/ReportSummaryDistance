Proceedings of the Second Workshop on Statistical Machine Translation, pages 112?119,Prague, June 2007. c?2007 Association for Computational LinguisticsAn Iteratively-Trained Segmentation-Free Phrase Translation Modelfor Statistical Machine TranslationRobert C. Moore Chris QuirkMicrosoft ResearchRedmond, WA 98052, USA{bobmoore,chrisq}@microsoft.comAbstractAttempts to estimate phrase translationprobablities for statistical machine transla-tion using iteratively-trained models haverepeatedly failed to produce translations asgood as those obtained by estimating phrasetranslation probablities from surface statis-tics of bilingual word alignments as de-scribed by Koehn, et al (2003).
We pro-pose a new iteratively-trained phrase trans-lation model that produces translations ofquality equal to or better than those pro-duced by Koehn, et al?s model.
Moreover,with the new model, translation quality de-grades much more slowly as pruning is tigh-tend to reduce translation time.1 IntroductionEstimates of conditional phrase translation probabil-ities provide a major source of translation knowl-edge in phrase-based statistical machine translation(SMT) systems.
The most widely used method forestimating these probabilities is that of Koehn, etal.
(2003), in which phrase pairs are extracted fromword-aligned bilingual sentence pairs, and theirtranslation probabilities estimated heuristically fromsurface statistics of the extracted phrase pairs.
Wewill refer to this approach as ?the standard model?.There have been several attempts to estimatephrase translation probabilities directly, using gen-erative models trained iteratively on a parallel cor-pus using the Expectation Maximization (EM) algo-rithm.
The first of these models, that of Marcu andWong (2002), was found by Koehn, et al (2003),to produce translations not quite as good as theirmethod.
Recently, Birch et al (2006) tried theMarcu and Wong model constrained by a wordalignment and also found that Koehn, et al?s modelworked better, with the advantage of the standardmodel increasing as more features were added to theoverall translation model.
DeNero et al (2006) trieda different generative phrase translation model anal-ogous to IBM word-translation Model 3 (Brown etal., 1993), and again found that the standard modeloutperformed their generative model.DeNero et al (2006) attribute the inferiority oftheir model and the Marcu and Wong model to a hid-den segmentation variable, which enables the EMalgorithm to maximize the probability of the train-ing data without really improving the quality of themodel.
We propose an iteratively-trained phrasetranslation model that does not require different seg-mentations to compete against one another, and weshow that this produces translations of quality equalto or better than those produced by the standardmodel.
We find, moreover, that with the new model,translation quality degrades much more slowly aspruning is tightend to reduce translation time.Decoding efficiency is usually considered only inthe design and implementation of decoding algo-rithms, or the choice of model structures to supportfaster decoding algorithms.
We are not aware of anyattention previously having been paid to the effect ofdifferent methods of parameter estimation on trans-lation efficiency for a given model structure.The time required for decoding is of great im-portance in the practical application of SMT tech-112nology.
One of the criticisms of SMT often madeby adherents of rule-based machine translation isthat SMT is too slow for practical application.
Therapidly falling price of computer hardware has ame-liorated this problem to a great extent, but the fact re-mains that every factor of 2 improvement in transla-tion efficiency means a factor of 2 decrease in hard-ware cost for intensive applications of SMT, suchas a web-based translation service (?Translate thispage?).
SMT surely needs all the help in can get inthis regard.2 Previous ApproachesKoehn, et al?s (2003) method of estimating phrase-translation probabilities is very simple.
They startwith an automatically word-aligned corpus of bilin-gual sentence pairs, in which certain words arelinked, indicating that they are translations of eachother, or that they are parts of phrases that are trans-lations of each other.
They extract every possi-ble phrase pair (up to a given length limit) that (a)contains at least one pair of linked words, and (b)does not contain any words that have links to otherwords not included in the phrase pair.1 In otherwords, word alignment links cannot cross phrasepair boundaries.
Phrase translation probabilities areestimated simply by marginalizing the counts ofphrase instances:p(x|y) =C(x, y)?x?
C(x?, y)This method is used to estimate the conditionalprobabilities of both target phrases give sourcephrases and source phrases given target phrases.In contrast to the standard model, DeNero, et al(2006) estimate phrase translation probabilities ac-cording to the following generative model:1.
Begin with a source sentence a.2.
Stochastically segment a into some number ofphrases.3.
For each selected phrase in a, stochasticallychoose a phrase position in the target sentenceb that is being generated.1This method of phrase pair extraction was originally de-scribed by Och et al (1999).4.
For each selected phrase in a and the corre-sponding phrase position in b, stochasticallychoose a target phrase.5.
Read off the target sentence b from the se-quence of target phrases.DeNero et al?s analysis of why their model per-forms relatively poorly hinges on the fact that thesegmentation probabilities used in step 2 are, infact, not trained, but simply assumed to be uniform.Given complete freedom to select whatever segmen-tation maximizes the likelihood of any given sen-tence pair, EM tends to favor segmentations thatyield source phrases with as few occurrences as pos-sible, since more of the associated conditional prob-ability mass can be concentrated on the target phrasealignments that are possible in the sentence at hand.Thus EM tends to maximize the probability of thetraining data by concentrating probability mass onthe rarest source phrases it can construct to coverthe training data.
The resulting probability estimatesthus have less generalizability to unseen data thanif probability mass were concentrated on more fre-quently occurring source phrases.3 A Segmentation-Free ModelTo avoid the problem identified by DeNero et al,we propose an iteratively-trained model that doesnot assume a segmentation of the training data intonon-overlapping phrase pairs.
We refer to our modelas ?iteratively-trained?
rather than ?generative?
be-cause we have not proved any of the mathematicalproperties usually associated with generative mod-els; e.g., that the training procedure maximizes thelikelihood of the training data.
We will motivatethe model, however, with a generative story as tohow phrase alignments are produced, given a pair ofsource and target sentences.
Our model extends tophrase alignment the concept of a sentence pair gen-erating a word alignment developed by Cherry andLin (2003).Our model is defined in terms of two stochasticprocesses, selection and alignment, as follows:1.
For each word-aligned sentence pair, we iden-tify all the possible phrase pair instances ac-cording to the criteria used by Koehn et al1132.
Each source phrase instance that is included inany of the possible phrase pair instances inde-pendently selects one of the target phrase in-stances that it forms a possible phrase pair in-stance with.3.
Each target phrase instance that is included inany of the possible phrase pair instances inde-pendently selects one of the source phrase in-stances that it forms a possible phrase pair in-stance with.4.
A source phrase instance is aligned to a targetphrase instance, if and only if each selects theother.Given a set of selection probability distributionsand a word-aligned parallel corpus, we can eas-ily compute the expected number of alignment in-stances for a given phrase pair type.
The probabilityof a pair of phrase instances x and y being aligned issimply ps(x|y) ?
ps(y|x), where ps is the applica-ble selection probability distribution.
The expectednumber of instances of alignment, E(x, y), for thepair of phrases x and y, is just the sum of the align-ment probabilities of all the possible instances ofthat phrase pair type.From the expected number of alignments and thetotal number of occurrences of each source and tar-get phrase type in the corpus (whether or not theyparticpate in possible phrase pairs), we estimate theconditional phrase translation probabilities aspt(y|x) =E(x, y)C(x), pt(x|y) =E(x, y)C(y),where E denotes expected counts, and C denotesobserved counts.The use of the total observed counts of particu-lar source and target phrases (instead of marginal-ized expected joint counts) in estimating the condi-tional phrase translation probabilities, together withthe multiplication of selection probabilities in com-puting the alignment probability of particular phrasepair instances, causes the conditional phrase transla-tion probability distributions generally to sum to lessthan 1.0.
We interpret the missing probability massas the probability that a given word sequence doesnot translate as any contiguous word sequence in theother language.We have seen how to derive phrase translationprobabilities from the selection probabilities, butwhere do the latter come from?
We answer thisquestion by adding the following constraint to themodel:The probabilty of a phrase y selecting aphrase x is proportional to the probabilityof x translating as y, normalized over thepossible non-null choices for x presentedby the word-aligned sentence pair.Symbolically, we can express this asps(x|y) =pt(y|x)?x?
pt(y|x?
)where ps denotes selection probability, pt denotestranslation probability, and x?
ranges over the phraseinstances that could possibly align to y.
We are, ineffect, inverting and renormalizing translation prob-abilities to get selection probabilities.
The reasonfor the inversion may not be immediately apparent,but it in fact simply generalizes the e-step formulain the EM training for IBM Model 1 from words tophrases.This model immediately suggests (and, in fact,was designed to suggest) the following EM-liketraining procedure:1.
Initialize the translation probability distribu-tions to be uniform.
(It doesn?t matter at thispoint whether the possibility of no translationis included or not.)2.
E step: Compute the expected phrase alignmentcounts according to the model, deriving the se-lection probabilities from the current estimatesof the translation probabilities as described.3.
M step: Re-estimate the phrase translationprobabilities according to the expected phrasealignment counts as described.4.
Repeat the E and M steps, until the desired de-gree of convergence is obtained.We view this training procedure as iteratively try-ing to find a set of phrase translation probabilitiesthat satisfies all the constraints of the model, al-though we have not proved that this training proce-dure always converges.
We also have not proved that114the procedure maximizes the likelihood of anything,although we find empirically that each iteration de-creases the conditional entropy of the phrase trans-lation model.
In any case, the training procedureseems to work well in practice.
It is also very simi-lar to the joint training procedure for HMM word-alignment models in both directions described byLiang et al (2006), which was the original inspira-tion for our training procedure.4 Experimental Set-Up and DataWe evaluated our phrase translation model com-pared to the standard model of Koehn et al in thecontext of a fairly typical end-to-end phrase-basedSMT system.
The overall translation model scoreconsists of a weighted sum of the following eight ag-gregated feature values for each translation hypoth-esis:?
the sum of the log probabilities of each sourcephrase in the hypothesis given the correspond-ing target phrase, computed either by ourmodel or the standard model,?
the sum of the log probabilities of each tar-get phrase in the hypothesis given the corre-sponding source phrase, computed either byour model or the standard model,?
the sum of lexical scores for each source phrasegiven the corresponding target phrase,?
the sum of lexical scores for each target phrasegiven the corresponding source phrase,?
the log of the target language model probabilityfor the sequence of target phrases in the hypoth-esis,?
the total number of words in the target phrasesin the hypothesis,?
the total number of source/target phrase pairscomposing the hypothesis,?
the distortion penalty as implemented in thePharaoh decoder (Koehn, 2003).The lexical scores are computed as the (unnor-malized) log probability of the Viterbi alignment fora phrase pair under IBM word-translation Model 1(Brown et al, 1993).
The feature weights for theoverall translation models were trained using Och?s(2003) minimum-error-rate training procedure.
Theweights were optimized separately for our modeland for the standard phrase translation model.
Ourdecoder is a reimplementation in Perl of the algo-rithm used by the Pharaoh decoder as described byKoehn (2003).2The data we used comes from an English-Frenchbilingual corpus of Canadian Hansards parliamen-tary proceedings supplied for the bilingual wordalignment workshop held at HLT-NAACL 2003(Mihalcea and Pedersen, 2003).
Automatic sentencealignment of this data was provided by Ulrich Ger-mann.
We used 500,000 sentences pairs from thiscorpus for training both the phrase translation mod-els and IBM Model 1 lexical scores.
These 500,000sentence pairs were word-aligned using a state-of-the-art word-alignment method (Moore et al, 2006).A separate set of 500 sentence pairs was used to trainthe translation model weights, and two additionalheld-out sets of 2000 sentence pairs each were usedas test data.The two phrase translation models were trainedusing the same set of possible phrase pairs extractedfrom the word-aligned 500,000 sentence pair cor-pus, finding all possible phrase pairs permitted bythe criteria followed by Koehn et al, up to a phraselength of seven words.
This produced approximately69 million distinct phrase pair types.
No pruning ofthe set of possible phrase pairs was done during orbefore training the phrase translation models.
Ourphrase translation model and IBM Model 1 wereboth trained for five iterations.
The training pro-cedure for our phrase translation model trains mod-els in both directions simultaneously, but for IBMModel 1, models were trained separately in each di-rection.
The models were then pruned to includeonly phrase pairs that matched the source sides ofthe small training and test sets.5 Entropy MeasurementsTo verify that our iterative training procedure wasbehaving as expected, after each training iteration2Since Perl is a byte-code interpreted language, absolute de-coding times will be slower than with the standard machine-language-compiled implementation of Pharaoh, but relativetimes between models should be comparable.115we measured the conditional entropy of the modelin predicting English phrases given French phrases,according to the formulaH(E|F ) =?fp(f)?ept(e|f) log2 pt(e|f),where e and f range over the English and Frenchphrases that occur in the extracted phrase pairs, andp(f) was estimated according to the relative fre-quency of these French phrases in a 2000 sentencesample of the French sentences from the 500,000word-aligned sentence pairs.
Over the five train-ing iterations, we obtained a monotonically decreas-ing sequence of entropy measurements in bits perphrase: 1.329, 1.177, 1.146, 1.140, 1.136.We also compared the conditional entropy of thestandard model to the final iteration of our model,estimating p(f) using the first of our 2000 sentencepair test sets.
For this data, our model measured 1.38bits per phrase, and the standard model measured4.30 bits per phrase.
DeNero et al obtained corre-sponding measurements of 1.55 bits per phrase and3.76 bits per phrase, for their model and the stan-dard model, using a different data set and a slightlydifferent estimation method.6 Translation ExperimentsWe wanted to look at the trade-off between decod-ing time and translation quality for our new phrasetranslation model compared to the standard model.Since this trade-off is also affected by the settings ofvarious pruning parameters, we compared decodingtime and translation quality, as measured by BLEUscore (Papineni et al 2002), for the two models onour first test set over a broad range of settings for thedecoder pruning parameters.The Pharaoh decoding algorithm, has five pruningparameters that affect decoding time:?
Distortion limit?
Translation table limit?
Translation table threshold?
Beam limit?
Beam thresholdThe distortion limit is the maximum distance al-lowed between two source phrases that produce ad-jacent target phrases in the decoder output.
The dis-tortion limit can be viewed as a model parameter,as well as a pruning paramter, because setting it toan optimum value usually improves translation qual-ity over leaving it unrestricted.
We carried out ex-periments with the distortion limit set to 1, whichseemed to produce the highest BLEU scores on ourdata set with the standard model, and also set to 5,which is perhaps a more typical value for phrase-based SMT systems.
Translation model weightswere trained separately for these two settings, be-cause the greater the distortion limit, the higher thedistortion penalty weight needed for optimal trans-lation quality.The translation table limit and translation tablethreshold are applied statically to the phrase trans-lation table, which combines all components of theoverall translation model score that can be com-puted for each phrase pair in isolation.
This in-cludes all information except the distortion penaltyscore and the part of the language model score thatlooks at n-grams that cross target phrase boundaries.The translation table limit is the maximum numberof translations allowed in the table for any givensource phrase.
The translation table threshold isthe maximum difference in combined translation ta-ble score allowed between the highest scoring trans-lation and lowest scoring translation for any givensource phrase.
The beam limit and beam thresholdare defined similarly, but they apply dynamically tothe sets of competing partial hypotheses that coverthe same number of source words in the beam searchfor the highest scoring translation.For each of the two distortion limits we tried, wecarried out a systematic search for combinations ofsettings of the other four pruning parameters thatgave the best trade-offs between decoding time andBLEU score.
Starting at a setting of 0.5 for thethreshold parameters3 and 5 for the limit parameterswe performed a hill-climbing search over step-wiserelaxations of all combinations of the four parame-3We use difference in weighted linear scores directly forour pruning thresholds, whereas the standard implementation ofPharaoh expresses these as probability ratios.
Hence the specificvalues for these parameters are not comparable to published de-scriptions of experiments using Pharaoh, although the effects ofpruning are exactly the same.11630.230.330.430.50.1 1 10 100BLEU[%]milliseconds per wordFigure 1: BLEU vs Decoding Time (DL = 1)re-estimated phrase tablestandard phrase tablere-estimated phrase tableconvex hullstandard phrase table convexhull29.429.629.83030.230.430.61 10 100 1000BLEU[%]milliseconds per wordFigure 2: BLEU vs Decoding Time (DL = 5)re-estimated phrase tablestandard phrase tablere-estimated phrase tableconvex hullstandard phrase table convexhullters, incrementing the threshold parameters by 0.5and the limit parameters by 5 at each step.
For eachresulting point that provided the best BLEU score yetseen for the amount of decoding time used, we iter-ated the search.The resulting possible combinations of BLEUscore and decoding time for the two phrase trans-lation models are displayed in Figure 1, for a distor-tion limit of 1, and Figure 2, for a distortion limitof 5.
BLEU score is reported on a scale of 1?100(BLEU[%]), and decoding time is measured in mil-liseconds per word.
Note that the decoding time axisis presented on a log scale.The points that represent pruning parameter set-tings one might consider using in a practical systemare those on or near the upper convex hull of theset of points for each model.
These upper-convex-hull points are highlighted in the figures.
Points farfrom these boundaries represent settings of one ormore of the parameters that are too restrictive to ob-tain good translation quality, together with settingsof other parameters that are too permissive to obtaingood translation time.Examining the results for a distortion limit of1, we found that the BLEU score obtained withthe loosest pruning parameter settings (2.5 for boththreshold paramters, and 25 for both limit parame-ters) were essentially identical for the two mod-els: 30.42 BLEU[%].
As the pruning parametersare tightened to reduce decoding time, however,the new model performs much better.
At a decod-ing time almost 6 times faster than for the settingsthat produced the highest BLEU score, the changein score was only ?0.07 BLEU[%] with the newmodel.
To obtain a slightly worse4 BLEU score(?0.08 BLEU[%]) using the standard model took90% more decoding time.It does appear, however, that the best BLEU scorefor the standard model is slightly better than the bestBLEU score for the new model: 30.43 vs. 30.42.It is in fact currious that there seem to be numer-ous points where the standard model gets a slightly4Points on the convex hulls with exactly comparable BLEUscores do not often occur.117better BLEU score than it does with with the loos-est pruning settings, which should have the lowestsearch error.We conjectured that this might be an artifact ofour test procedure.
If a model is at all reasonable,most search errors will reduce the ultimate objec-tive function, in our case the BLEU score, but oc-casionally a search error will increase the objectivefunction just by chance.
The smaller the number ofsearch errors in a particular test, the greater the like-lihood that, by chance, more search errors will in-crease the objective function than decrease it.
Sincewe are sampling a fairly large number of combi-nations of pruning parameter settings (179 for thestandard model with a distortion limit of 1), it ispossible that a small number of these have more?good?
search errors than ?bad?
search errors sim-ply by chance, and that this accounts for the smallnumber of points (13) at which the BLEU score ex-ceeds that of the point which should have the fewestsearch errors.
This effect may be more pronouncedwith the standard model than with the new model,simply because there is more noise in the standardmodel.To test the hypothesis that the BLEU scoresgreater than the score for the loosest pruning set-tings simply represent noise in the data, we col-lected all the pruning settings that produced BLEUscores greater than or equal to the the one for theloosest pruning settings, and evaluated the standardmodel at those settings on our second held-out testset.
We then looked at the correlation between theBLEU scores for these settings on the two test sets,and found that it was very small and negative, withr = ?0.099.
The standard F-test for the significanceof a correlation yielded p = 0.74; in other words,completely insignificant.
This strongly suggests thatthe apparent improvement in BLEU score for certaintighter pruning settings is illusory.As a sanity check, we tested the BLEU score cor-relation between the two test sets for the points onthe upper convex hull of the plot for the standardmodel, between the point with the fastest decod-ing time and the point with the highest BLEU score.That correlation was very high, with r = 0.94,which was significant at the level p = 0.0004 ac-cording to the F-test.
Thus the BLEU score differ-ences along most of the upper convex hull seem toreflect reality, but not in the region where they equalor exceed the score for the loosest pruning settings.At a distortion limit of 5, there seems no questionthat the new model performs better than the standardmodel.
The difference BLEU scores for the upper-convex-hull points ranges from about 0.8 to 0.2BLEU[%] for comparable decoding times.
Again,the advantage of the new model is greater at shorterdecoding times.
Compared to the results with a dis-tortion limit of 1, the standard model loses transla-tion quality, with a change of about ?0.2 BLEU[%]for the loosest pruning settings, while the new modelgains very slightly (+0.04 BLEU[%]).7 ConclusionsThis study seems to confirm DeNero et al?s diagno-sis that the main reason for poor performance of pre-vious iteratively-trained phrase translation models,compared to Koehn et al?s model, is the effect of thehidden segmentation variable in these models.
Wehave developed an iteratively-trained phrase transla-tion model that is segmentation free, and shown that,at a minimum, it eliminates the shortfall in BLEUscore compared to the standard model.
With a largerdistortion limit, the new model produced transla-tions with a noticably better BLEU score.From a practical point of view, the main resultis probably that BLEU score degrades much moreslowly with our model than with the standard model,when the decoding search is tuned for speed.
Forsome settings that appear reasonable, this differenceis close to a factor of 2, even if there is no differ-ence in the translation quality obtainable when prun-ing is loosened.
For high-demand applications likeweb page translation, roughly half of the investmentin translation servers could be saved while provid-ing this level of translation quality with the same re-sponse time.AcknowledgementThe authors would like to thank Mark Johnson formany valuable discussions of how to analyze andpresent the results obtained in this study.ReferencesAlexandra Birch, Chris Callison-Burch, Miles Os-borne, and Philipp Koehn.
2006.
Constrain-118ing the Phrase-Based, Joint Probability StatisticalTranslation Model.
In Proceedings of the HLT-NAACL 06 Workshop, Statistical Machine Trans-lation, pp.
154?157, New York City, New York,USA.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguis-tics, 19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A Probabil-ity Model to Improve Word Alignment.
In Pro-ceedings of the 41st Annual Meeting of the ACL,pp.
88?95, Sapporo, Japan.John DeNero, Dan Gillick, James Zhang, and DanKlein.
2006.
Why Generative Phrase ModelsUnderperform Surface Heuristics.
In Proceed-ings of the HLT-NAACL 06 Workshop, StatisticalMachine Translation, pp.
31?38, New York City,New York, USA.Philipp Koehn.
2003.
Noun Phrase Translation.PhD Dissertation, Computer Science, Universityof Southern California, Los Angeles, California,USA.Philipp Koehn, Franz Joseph Och, and DanielMarcu.
2003.
Statistical Phrase-Based Trans-lation.
In Proceedings of the Human LanguageTechnology Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pp.
127?133, Edmonton, Alberta,Canada.Percy Liang, Ben Taskar, and Dan Klein.
2006.Alignment by Agreement.
In Proceedings ofthe Human Language Technology Conference ofthe North American Chapter of the Associationfor Computational Linguistics, pp.
104?111, NewYork City, New York, USA.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
In Proceedings of the 2002Conference on Empirical Methods in NaturalLanguage Processing, pp.
133?139, Philadelphia,Pennsylvania, USA.Rada Mihalcea and Ted Pedersen.
2003.
An Evalu-ation Exercise for Word Alignment.
In Proceed-ings of the HLT-NAACL 2003 Workshop, Buildingand Using Parallel Texts: Data Driven MachineTranslation and Beyond, pp.
1?6, Edmonton, Al-berta, Canada.Robert C. Moore, Wen-tau Yih, and Andreas Bode.2006.
Improved Discriminative Bilingual WordAlignment.
In Proceedings of the 21st Interna-tional Conference on Computational Linguisticsand 44th Annual Meeting of the Association forComputational Linguistics, pp.
513-520, Sydney,Australia.Franz Joseph Och, Christoff Tillmann, and HermannNey.
1999.
Improved Alignment Models for Sta-tistical Machine Translation.
In Proceedings ofthe 1999 Joint SIGDAT Conference on Empiri-cal Methods in Natural Language Processing andVery Large Corpora, pp.
20?28, College Park,Maryland, USA.Franz Joseph Och.
2003.
Minimum Error RateTraining in Statistical Machine Translation.
InProceedings of the 41st Annual Meeting of theACL, pp.
160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
311?318, Philadelphia, Pennsylvania, USA.119
