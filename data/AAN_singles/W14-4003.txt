Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22?33,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsBetter Semantic Frame Based MT Evaluationvia Inversion Transduction GrammarsDekai Wu Lo Chi-kiu Meriem Beloucif Markus SaersHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo|mbeloucif|masaers|dekai}@cs.ust.hkAbstractWe introduce an inversion transduc-tion grammar based restructuring ofthe MEANT automatic semantic framebased MT evaluation metric, which,by leveraging ITG language biases, isable to further improve upon MEANT?salready-high correlation with humanadequacy judgments.
The new metric,called IMEANT, uses bracketing ITGs tobiparse the reference and machine transla-tions, but subject to obeying the semanticframes in both.
Resulting improvementssupport the presumption that ITGs, whichconstrain the allowable permutationsbetween compositional segments acrossthe reference and MT output, score thephrasal similarity of the semantic rolefillers more accurately than the simpleword alignment heuristics (bag-of-wordalignment or maximum alignment) usedin previous version of MEANT.
Theapproach successfully integrates (1) thepreviously demonstrated extremely highcoverage of cross-lingual semantic framealternations by ITGs, with (2) the highaccuracy of evaluating MT via weightedf-scores on the degree of semantic framepreservation.1 IntroductionThere has been to date relatively little use of in-version transduction grammars (Wu, 1997) to im-prove the accuracy of MT evaluation metrics, de-spite long empirical evidence the vast majority oftranslation patterns between human languages canbe accommodated within ITG constraints (and theobservation that most current state-of-the-art SMTsystems employ ITG decoders).
We show thatITGs can be used to redesign the MEANT seman-tic frame based MT evaluation metric (Lo et al.,2012) to produce improvements in accuracy andreliability.
This work is driven by the motiva-tion that especially when considering semanticMTmetrics, ITGs would be seem to be a natural basisfor several reasons.To begin with, it is quite natural to think ofsentences as having been generated from an ab-stract concept using a rewriting system: a stochas-tic grammar predicts how frequently any particu-lar realization of the abstract concept will be gen-erated.
The bilingual analogy is a transductiongrammar generating a pair of possible realizationsof the same underlying concept.
Stochastic trans-duction grammars predict how frequently a partic-ular pair of realizations will be generated, and thusrepresent a good way to evaluate how well a pairof sentences correspond to each other.The particular class of transduction gram-mars known as ITGs tackle the problem thatthe (bi)parsing complexity for general syntax-directed transductions (Aho and Ullman, 1972)is exponential.
By constraining a syntax-directedtransduction grammar to allow only monotonicstraight and inverted reorderings, or equivalentlypermitting only binary or ternary rank rules, it ispossible to isolate the low end of that hierarchy intoa single equivalence class of inversion transduc-tions.
ITGs are guaranteed to have a two-normalform similar to context-free grammars, and canbe biparsed in polynomial time and space (O(n6)time andO(n4)space).
It is also possible to do ap-proximate biparsing in O(n3)time (Saers et al.,2009).
These polynomial complexities makes itfeasible to estimate the parameters of an ITG us-ing standard machine learning techniques such asexpectation maximization (Wu, 1995b) .At the same time, inversion transductions havealso been directly shown to be more than sufficientto account for the reordering that occur within se-mantic frame alternations (Addanki et al., 2012).This makes ITGs an appealing alternative for eval-22uating the possible links between both semanticrole fillers in different languages as well as thepredicates, and how these parts fit together to formentire semantic frames.
We believe that ITGs arenot only capable of generating the desired struc-tural correspondences between the semantic struc-tures of two languages, but also provide meaning-ful constraints to prevent alignments from wander-ing off in the wrong direction.In this paper we show that IMEANT, a newmet-ric drawing from the strengths of both MEANTand inversion transduction grammars, is able toexploit bracketing ITGs (also known as BITGsor BTGs) which are ITGs containing only a sin-gle non-differentiated non terminal category (Wu,1995a), so as to produce even higher correlationwith human adequacy judgments than any auto-matic MEANT variants, or other common auto-matic metrics.
We argue that the constraints pro-vided by BITGs over the semantic frames and ar-guments of the reference and MT output sentencesare essential for accurate evaluation of the phrasalsimilarity of the semantic role fillers.In common with the various MEANT semanticMT evaluation metrics (Lo and Wu, 2011a, 2012;Lo et al., 2012; Lo and Wu, 2013b), our proposedIMEANT metric measures the degree to whichthe basic semantic event structure is preservedby translation?the ?who did what to whom, forwhom, when, where, how and why?
(Pradhan etal., 2004)?emphasizing that a good translationis one that can successfully be understood by ahuman.
In the other versions of MEANT, sim-ilarity between the MT output and the referencetranslations is computed as a modified weighted f-score over the semantic predicates and role fillers.Across a variety of language pairs and genres, ithas been shown thatMEANT correlates better withhuman adequacy judgment than both n-gram basedMT evaluation metrics such as BLEU (Papineniet al., 2002), NIST (Doddington, 2002), and ME-TEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch etal., 2006), WER (Nie?en et al., 2000), and TER(Snover et al., 2006) when evaluating MT output(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo andWu, 2013b; Mach?
?ek and Bojar, 2013).
Further-more, tuning the parameters of MT systems withMEANT instead of BLEU or TER robustly im-proves translation adequacy across different gen-res and different languages (English and Chinese)(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,2013b).
This has motivated our choice of MEANTas the basis on which to experiment with deployingITGs into semantic MT evaluation.2 Related Work2.1 ITGs and MT evaluationRelatively little investigation into the potentialbenefits of ITGs is found in previous MT eval-uation work.
One exception is invWER, pro-posed by Leusch et al.
(2003) and Leusch and Ney(2008).
The invWER metric interprets weightedBITGs as a generalization of the Levenshtein editdistance, in which entire segments (blocks) can beinverted, as long as this is done strictly compo-sitionally so as not to violate legal ITG biparsetree structures.
The input and output languagesare considered to be those of the reference and ma-chine translations, and thus are over the same vo-cabulary (say,English).
At the sentence level, cor-relation of invWER with human adequacy judg-ments was found to be among the best.Our current approach differs in several keyrespects from invWER.
First,invWER operatespurely at the surface level of exact token match,IMEANT mediates between segments of refer-ence translation andMT output using lexical BITGprobabilities.Secondly, there is no explicit semantic model-ing in invWER.
Providing they meet the BITGconstraints, the biparse trees in invWER are com-pletely unconstrained.
In contrast, IMEANT em-ploys the same explicit, strong semantic framemodeling as MEANT, on both the reference andmachine translations.
In IMEANT, the semanticframes always take precedence over pure BITGbiases.
Compared to invWER, this strongly con-strains the space of biparses that IMEANT permitsto be considered.2.2 MT evaluation metricsLike invWER, other common surface-form ori-ented metrics like BLEU (Papineni et al., 2002),NIST (Doddington, 2002), METEOR (Banerjeeand Lavie, 2005; Denkowski and Lavie, 2014),CDER (Leusch et al., 2006), WER (Nie?en etal., 2000), and TER (Snover et al., 2006) donot correctly reflect the meaning similarities ofthe input sentence.
There are in fact severallarge scale meta-evaluations (Callison-Burch etal., 2006; Koehn and Monz, 2006) reporting cases23where BLEU strongly disagrees with human judg-ments of translation adequacy.Such observations have generated a recent surgeof work on developing MT evaluation metrics thatwould outperform BLEU in correlation with hu-man adequacy judgment (HAJ).
Like MEANT, theTINE automatic recall-oriented evaluation metric(Rios et al., 2011) aims to preserve basic eventstructure.
However, its correlation with human ad-equacy judgment is comparable to that of BLEUand not as high as that of METEOR.
Owczarzaket al.
(2007a,b) improved correlation with humanfluency judgments by using LFG to extend the ap-proach of evaluating syntactic dependency struc-ture similarity proposed by Liu and Gildea (2005),but did not achieve higher correlation with hu-man adequacy judgments than metrics like ME-TEOR.
Another automatic metric, ULC (Gim?nezand M?rquez, 2007, 2008), incorporates severalsemantic similarity features and shows improvedcorrelation with human judgement of translationquality (Callison-Burch et al., 2007; Gim?nezand M?rquez, 2007; Callison-Burch et al., 2008;Gim?nez and M?rquez, 2008) but no work hasbeen done towards tuning an SMT system usinga pure form of ULC perhaps due to its expensiverun time.
Likewise, SPEDE (Wang and Manning,2012) predicts the edit sequence needed to matchthe machine translation to the reference translationvia an integrated probabilistic FSM and probabilis-tic PDA model.
The semantic textual similaritymetric Sagan (Castillo and Estrella, 2012) is basedon a complex textual entailment pipeline.
Theseaggregated metrics require sophisticated featureextraction steps, contain many parameters thatneed to be tuned, and employ expensive linguis-tic resources such asWordNet or paraphrase tables.The expensive training, tuning and/or running timerenders these metrics difficult to use in the SMTtraining cycle.3 IMEANTIn this section we give a contrastive descriptionof IMEANT: we first summarize the MEANT ap-proach, and then explain how IMEANT differs.3.1 Variants of MEANTMEANT and its variants (Lo et al., 2012) measureweighted f-scores over corresponding semanticframes and role fillers in the reference andmachinetranslations.
The automatic versions of MEANTreplace humans with automatic SRL and align-ment algorithms.
MEANT typically outperformsBLEU, NIST, METEOR, WER, CDER and TERin correlation with human adequacy judgment, andis relatively easy to port to other languages, re-quiring only an automatic semantic parser and amonolingual corpus of the output language, whichis used to gauge lexical similarity between the se-mantic role fillers of the reference and translation.MEANT is computed as follows:1.
Apply an automatic shallow semantic parserto both the reference and machine transla-tions.
(Figure 1 shows examples of auto-matic shallow semantic parses on both refer-ence and MT.)2.
Apply the maximum weighted bipartitematching algorithm to align the semanticframes between the reference and machinetranslations according to the lexical similari-ties of the predicates.
(Lo and Wu (2013a)proposed a backoff algorithm that evaluatesthe entire sentence of theMT output using thelexical similarity based on the context vectormodel, if the automatic shallow semanticparser fails to parse the reference or machinetranslations.)3.
For each pair of the aligned frames, apply themaximum weighted bipartite matching algo-rithm to align the arguments between the ref-erence andMT output according to the lexicalsimilarity of role fillers.4.
Compute the weighted f-score over thematching role labels of these aligned predi-cates and role fillers according to the follow-ing definitions:q0i,j?
ARG j of aligned frame i in MTq1i,j?
ARG j of aligned frame i in REFw0i?#tokens filled in aligned frame i of MTtotal #tokens in MTw1i?#tokens filled in aligned frame i of REFtotal #tokens in REFwpred ?
weight of similarity of predicateswj?
weight of similarity of ARG jei,pred ?
the pred string of the aligned frame i of MTfi,pred ?
the pred string of the aligned frame i of REFei,j?
the role fillers of ARG j of the aligned frame i of MTfi,j?
the role fillers of ARG j of the aligned frame i of REFs(e, f) = lexical similarity of token e and f24[IN] ??
?
?
??
??
??
?
?
?
?
?
?
?????
??
??
??
??
?
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ?
II products have now been resumed .ARGM-TMP PRED ARGM-LOC PRED ARG1ARGM-LOC PRED ARG1 PRED ARG1ARG0 ARGM-TMP[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .PRED ARG0 ARG1[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .ARGM-TMP ARG1 PRED PRED ARG1[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .PREDPRED ARG0ARG1 ARGM-TMPARGM-ADVARG0ARGM-EXTFigure 1: Examples of automatic shallow semantic parses.
Both the reference and machine translationsare parsed using automatic English SRL.
There are no semantic frames forMT3 since there is no predicatein the MT output.prece,f =?e?e maxf?fs(e, f)| e |rece,f =?f?f maxe?es(e, f)| f |si,pred =2 ?
precei,pred,fi,pred ?
recei,pred,fi,predprecei,pred,fi,pred + recei,pred,fi,predsi,j=2 ?
precei,j,fi,j?
recei,j,fi,jprecei,j,fi,j+ recei,j,fi,jprecision =?iw0iwpredsi,pred+?jwjsi,jwpred+?jwj|q0i,j|?iw0irecall =?iw1iwpredsi,pred+?jwjsi,jwpred+?jwj|q1i,j|?iw1iMEANT = 2 ?
precision ?
recallprecision + recallwhere q0i,jand q1i,jare the argument of type j inframe i inMT andREF respectively.w0iandw1iarethe weights for frame i in MT/REF respectively.These weights estimate the degree of contributionof each frame to the overall meaning of the sen-tence.
wpred and wj are the weights of the lexicalsimilarities of the predicates and role fillers of thearguments of type j of all frame between the ref-erence translations and the MT output.There is atotal of 12 weights for the set of semantic role la-bels in MEANT as defined in Lo and Wu (2011b).For MEANT, they are determined using super-vised estimation via a simple grid search to opti-mize the correlation with human adequacy judg-ments (Lo andWu, 2011a).
For UMEANT (Lo andWu, 2012), they are estimated in an unsupervisedmanner using relative frequency of each semanticrole label in the references and thus UMEANT isuseful when human judgments on adequacy of thedevelopment set are unavailable.si,pred and si,j are the lexical similarities basedon a context vectormodel of the predicates and rolefillers of the arguments of type j between the ref-erence translations and the MT output.
Lo et al.
(2012) and Tumuluru et al.
(2012) described howthe lexical and phrasal similarities of the semanticrole fillers are computed.
A subsequent variant ofthe aggregation function inspired by Mihalcea etal.
(2006) that normalizes phrasal similarities ac-cording to the phrase length more accurately wasused in more recent work (Lo et al., 2013a; Lo andWu, 2013a; Lo et al., 2013b).
In this paper, wewill assess IMEANT against the latest version ofMEANT (Lo et al., 2014) which, as shown, usesf-score to aggregate individual token similaritiesinto the composite phrasal similarities of semanticrole fillers,since this has been shown to bemore ac-curate than the previously used aggregation func-tions.Recent studies (Lo et al., 2013a; Lo and Wu,2013a; Lo et al., 2013b) show that tuning MT sys-tems against MEANT produces more robustly ad-equate translations than the common practice oftuning against BLEU or TER across different datagenres, such as formal newswire text, informalweb forum text and informal public speech.25In an alternative quality-estimation oriented lineof research, Lo et al.
(2014) describe a cross-lingual variant called XMEANT capable of eval-uating translation quality without the need for ex-pensive human reference translations, by utiliz-ing semantic parses of the original foreign in-put sentence instead of a reference translation.Since XMEANT?s results could have been dueto either (1) more accurate evaluation of phrasalsimilarity via cross-lingual translation probabili-ties, or (2) better match of semantic frames with-out reference translations, there is no direct evi-dencewhether ITGs contribute to the improvementin MEANT?s correlation with human adequacyjudgment.
For the sake of better understandingwhether ITGs improve semantic MT evaluation,we will also assess IMEANT against cross-lingualXMEANT.3.2 The IMEANT metricAlthough MEANT was previously shown to pro-duce higher correlation with human adequacyjudgments compared to other automatic metrics,our error analyses suggest that it still suffers from acommon weakness among metrics employing lex-ical similarity, namely that word/token alignmentsbetween the reference and machine translationsare severely under constrained.
No bijectivity orpermutation restrictions are applied, even betweencompositional segments where this should be nat-ural.
This can cause role fillers to be aligned evenwhen they should not be.
IMEANT, in contrast,uses a bracketing inversion transduction grammarto constrain permissible token alignment patternsbetween aligned role filler phrases.
The semanticframes above the token level also fits ITG com-positional structure, consistent with the aforemen-tioned semantic frame alternation coverage studyof Addanki et al.
(2012).
Figure 2 illustrates howthe ITG constraints are consistent with the neededpermutations between semantic role fillers acrossthe reference and machine translations for a sam-ple sentence from our evaluation data, which aswe will see leads to higher HAJ correlations thanMEANT.Subject to the structural ITG constraints,IMEANT scores sentence translations in a spiritsimilar to the way MEANT scores them: it utilizesan aggregated score over the matched semanticrole labels of the automatically aligned semanticframes and their role fillers between the referenceand machine translations.
Despite the structuraldifferences, like MEANT, at the conceptual levelIMEANT still aims to evaluate MT output interms of the degree to which the translation haspreserved the essential ?who did what to whom,forwhom, when, where, how and why?
of the foreigninput sentence.Unlike MEANT, however, IMEANT aligns andscores under ITG assumptions.
MEANT uses amaximum alignment algorithm to align the tokensin the role fillers between the reference and ma-chine translations, and then scores by aggregatingthe lexical similarities into a phrasal similarity us-ing an f-measure.
In contrast, IMEANT aligns andscores by utilizing a length-normalized weightedBITG (Wu, 1997; Zens and Ney, 2003; Saers andWu, 2009; Addanki et al., 2012).
To be precise inthis regard, we can see IMEANT as differing fromthe foregoing description of MEANT in the defi-nition of si,pred and si,j , as follows.G ?
?
{A} ,W0,W1,R,A?R ?
{A ?
[AA] ,A ?
?AA?,A ?
e/f}p ([AA] |A) = p (?AA?|A) = 1p (e/f |A) = s(e, f)si,pred = lg?1?
?lg(P(A ??
ei,pred/fi,pred|G))max(| ei,pred |, | fi,pred |)?
?si,j= lg?1?
?lg(P(A ??
ei,j/fi,j|G))max(| ei,j|, | fi,j|)?
?where G is a bracketing ITG whose only non ter-minal is A, andR is a set of transduction rules withe ?
W0?{?}
denoting a token in theMToutput (orthe null token) and f ?
W1?{?}
denoting a tokenin the reference translation (or the null token).
Therule probability (or more accurately, rule weight)function p is set to be 1 for structural transductionrules, and for lexical transduction rules it is de-fined using MEANT?s context vector model basedlexical similarity measure.
To calculate the insideprobability (or more accurately, inside score) of apair of segments, P(A ??
e/f|G), we use the al-gorithm described in Saers et al.
(2009).
Giventhis, si,pred and si,j now represent the length nor-malized BITG parse scores of the predicates androle fillers of the arguments of type j between thereference and machine translations.4 ExperimentsIn this section we discuss experiments indicatingthat IMEANT further improves upon MEANT?s26[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .ARG0 ARG1 PREDARG0 PRED ARG1Thelevelofreductionisconducivetoraisingtheinspectionandsupervisionworkefficiency.Thereductioninhierarchyhelps raise theefficiencyofinspectionandsupervisory .workpredARG0ARG1predARG0ARG1Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using bothbiparse tree and alignment matrix depictions, for the Chinese input sentence ????????????????????.
Both the reference and machine translations are parsed using automatic English SRL.Compositional alignments between the semantic frames and the tokens within role filler phrases obeyinversion transduction grammars.already-high correlation with human adequacyjudgments.4.1 Experimental setupWe perform the meta-evaluation upon two differ-ent partitions of the DARPA GALE P2.5 Chinese-English translation test set.
The corpus includesthe Chinese input sentences, each accompanied byone English reference translation and three partic-ipating state-of-the-art MT systems?
output.For the sake of consistent comparison, the firstevaluation partition, GALE-A, is the same as theone used in Lo and Wu (2011a), and the secondevaluation partition, GALE-B, is the same as theone used in Lo and Wu (2011b).For both reference and machine translations, theASSERT (Pradhan et al., 2004) semantic role la-beler was used to automatically predict semanticparses.27Table 1: Sentence-level correlation with humanadequacy judgements on different partitions ofGALE P2.5 data.
IMEANT always yields topcorrelations, and is more consistent than eitherMEANT or its recent cross-lingual XMEANTquality estimation variant.
For reference, the hu-man HMEANT upper bound is 0.53 for GALE-Aand 0.37 for GALE-B?thus, the fully automatedIMEANT approximation is not far from closing thegap.metric GALE-A GALE-BIMEANT 0.51 0.33XMEANT 0.51 0.20MEANT 0.48 0.33METEOR 1.5 (2014) 0.43 0.10NIST 0.29 0.16METEOR 0.4.3 (2005) 0.20 0.29BLEU 0.20 0.27TER 0.20 0.19PER 0.20 0.18CDER 0.12 0.16WER 0.10 0.264.2 ResultsThe sentence-level correlations in Table 1 showthat IMEANT outperforms other automatic met-rics in correlation with human adequacy judgment.Note that this was achieved with no tuning what-soever of the default rule weights (suggesting thatthe performance of IMEANT could be further im-proved in the future by slightly optimizing the ITGweights).On the GALE-A partition, IMEANT shows 3points improvement over MEANT, and is tiedwith the cross-lingual XMEANT quality estimatordiscussed earlier.IMEANT produces much higherHAJ correlations than any of the other metrics.On the GALE-B partition, IMEANT is tied withMEANT, and is significantly better correlated withHAJ than the XMEANT quality estimator.
Again,IMEANT produces much higher HAJ correlationsthan any of the other metrics.We note that we have also observed this patternconsistently in smaller-scale experiments?whilethe monolingual MEANT metric and its cross-lingual XMEANT cousin vie with each other ondifferent data sets, IMEANT robustly and consis-tently produces top HAJ correlations.In both the GALE-A and GALE-B partitions,IMEANT comes within a few points of the humanupper bound benchmark HAJ correlations com-puted using the human labeled semantic framesand alignments used in the HMEANT.Data analysis reveals two reasons that IMEANTcorrelates with human adequacy judgement moreclosely than MEANT.
First, BITG constraints in-deed provide more accurate phrasal similarity ag-gregation, compared to the naive bag-of-wordsbased heuristics employed in MEANT.
Similar re-sults have been observed while trying to estimateword alignment probabilities where BITG con-straints outperformed alignments from GIZA++(Saers and Wu, 2009).Secondly, the permutation and bijectivity con-straints enforced by the ITG provide better lever-age to reject token alignments when they are notappropriate, compared with the maximal align-ment approach which tends to be rather promiscu-ous.
A case of this can be seen in Figure 3, whichshows the result on the same example sentence asin Figure 1.
Disregarding the semantic parsing er-rors arising from the current limitations of auto-matic SRL tools, the ITG tends to provide clean,sparse alignments for role fillers like the ARG1of the resumed PRED, preferring to leave tokenslike complete and range unaligned instead of aligningthem anyway as MEANT?s maximal alignment al-gorithm tends to do.
Note that it is not simply amatter of lowering thresholds for accepting tokenalignments: Tumuluru et al.
(2012) showed thatthe competitive linking approach (Melamed, 1996)which also generally produces sparser alignmentsdoes not work as well inMEANT, whereas the ITGappears to be selective about the token alignmentsin a manner that better fits the semantic structure.For contrast, Figure 4 shows a case whereIMEANT appropriately accepts dense alignments.5 ConclusionWe have presented IMEANT, an inversion trans-duction grammar based rethinking of the MEANTsemantic frame based MT evaluation approach,that achieves higher correlation with human ad-equacy judgments of MT output quality thanMEANT and its variants, as well as other com-mon evaluation metrics.
Our results improve uponprevious research showing that MEANT?s explicituse of semantic frames leads to state-of-the-art au-tomatic MT evaluation.
IMEANT achieves thisby aligning and scoring semantic frames under asimple, consistent ITG that provides empirically28[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ?
II products have now been resumed .ARG0 PRED ARGM-LOC PRED ARG1[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .ARGM-TMP ARG1 PRED PRED ARG1 PREDARGM-TMP ARGM-TMPSofar,inthemainlandofChinatostopsellingnearlytwomonthsofSK-2productssalesresumed.Untilaftertheirsale hadceasedinmainlandChinaforalmosttwo,sales of thecompletePREDPREDARG1ARGM-TMPARG1PREDPREDPREDARG1ARGM-LOCARG0range of SK- IIproductshavenowbeenresumed .monthsARGM-TMPARGM-TMPFigure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,complete, and range as MEANT tends to do.
(The semantic parse errors are due to limitations of automaticSRL.
)informative permutation and bijectivity biases, in-stead of the maximal alignment and bag-of-wordsassumptions used by MEANT.
At the same time,IMEANT retains the Occam?s Razor style simplic-ity and representational transparency characteris-tics of MEANT.Given the absence of any tuning of ITG weightsin this first version of IMEANT, we speculate that29[REF] Australian Prime Minister Howard said the government could cancel AWB 's monopoly in the wheat business next week .
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week .ARG0ARG0PREDARG0 PRED ARG1 PREDARG0ARGM-MOD ARGM-TMPARG1PRED ARGM-MOD ARG1 ARGM-TMPARG1AustralianPrimeMinisterJohnHowardsaidtheGovernmentmightcanceltheAWBcompanywheatmonopolyAustralianPrimeMinisterHoward said thegovernmentcouldcancelAWB 'smonopoly theinnextweekthat.wheatbusinessnextweek .predpredARG0ARG0ARGM-MODARG1ARGM-TMPARG1predpredARG0ARG0ARGM-MODARGM-TMPARG1ARG1Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence ?????????????????????
AWB??????????
(The semantic parse errors are due to limitationsof automatic SRL.
)IMEANT could perform even better than it alreadydoes here.We plan to investigate simple hyperpa-rameter optimizations in the near future.306 AcknowledgmentsThis material is based upon work supportedin part by the Defense Advanced ResearchProjects Agency (DARPA) under BOLT contractnos.
HR0011-12-C-0014 and HR0011-12-C-0016,and GALE contract nos.
HR0011-06-C-0022 andHR0011-06-C-0023; by the European Union un-der the FP7 grant agreement no.
287658; and bythe Hong Kong Research Grants Council (RGC)research grants GRF620811, GRF621008, andGRF612806.
Any opinions, findings and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessar-ily reflect the views of DARPA, the EU, or RGC.Thanks to Karteek Addanki for supporting work,and to Pascale Fung, Yongsheng Yang and Zhao-jun Wu for sharing the maximum entropy Chinesesegmenter and C-ASSERT, the Chinese semanticparser.ReferencesKarteek Addanki, Chi-kiu Lo, Markus Saers, andDekai Wu.
LTG vs. ITG coverage of cross-lingual verb frame alternations.
In 16th An-nual Conference of the European Associationfor Machine Translation (EAMT-2012), Trento,Italy, May 2012.Alfred V. Aho and Jeffrey D. Ullman.
The The-ory of Parsing, Translation, and Compiling.Prentice-Halll, Englewood Cliffs, New Jersey,1972.Satanjeev Banerjee and Alon Lavie.
METEOR:An automatic metric forMT evaluation with im-proved correlation with human judgments.
InWorkshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Sum-marization, Ann Arbor, Michigan, June 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in ma-chine translation research.
In 11th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL-2006), 2006.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation.
InSecond Workshop on Statistical Machine Trans-lation (WMT-07), 2007.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.Further meta-evaluation of machine transla-tion.
In Third Workshop on Statistical MachineTranslation (WMT-08), 2008.Julio Castillo and Paula Estrella.
Semantic tex-tual similarity for MT evaluation.
In 7th Work-shop on Statistical Machine Translation (WMT2012), 2012.Michael Denkowski and Alon Lavie.
METEORuniversal: Language specific translation eval-uation for any target language.
In 9th Work-shop on Statistical Machine Translation (WMT2014), 2014.George Doddington.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In The second interna-tional conference on Human Language Technol-ogy Research (HLT ?02), San Diego, California,2002.Jes?s Gim?nez and Llu?s M?rquez.
Linguistic fea-tures for automatic evaluation of heterogenousMT systems.
In Second Workshop on Statisti-cal Machine Translation (WMT-07), pages 256?264, Prague, Czech Republic, June 2007.Jes?s Gim?nez and Llu?s M?rquez.
A smorgas-bord of features for automaticMT evaluation.
InThird Workshop on Statistical Machine Transla-tion (WMT-08), Columbus, Ohio, June 2008.Philipp Koehn and Christof Monz.
Manual andautomatic evaluation of machine translation be-tween european languages.
InWorkshop on Sta-tistical Machine Translation (WMT-06), 2006.Gregor Leusch and Hermann Ney.
Bleusp, invwer,cder: Three improved mt evaluation measures.In NIST Metrics for Machine Translation Chal-lenge (MetricsMATR), at Eighth Conference ofthe Association for Machine Translation in theAmericas (AMTA 2008), Waikiki, Hawaii, Oct2008.Gregor Leusch, Nicola Ueffing, and HermannNey.
A novel string-to-string distance measurewith applications to machine translation evalu-ation.
In Machine Translation Summit IX (MTSummit IX), New Orleans, Sep 2003.Gregor Leusch, Nicola Ueffing, and HermannNey.
CDer: Efficient MT evaluation usingblock movements.
In 11th Conference of theEuropean Chapter of the Association for Com-putational Linguistics (EACL-2006), 2006.31Ding Liu and Daniel Gildea.
Syntactic features forevaluation of machine translation.
InWorkshopon Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization,Ann Arbor, Michigan, June 2005.Chi-kiu Lo and Dekai Wu.
MEANT: An inexpen-sive, high-accuracy, semi-automatic metric forevaluating translation utility based on seman-tic roles.
In 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies (ACL HLT 2011), 2011.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux:How semantic frames evaluate MT more ac-curately.
In Twenty-second International JointConference on Artificial Intelligence (IJCAI-11), 2011.Chi-kiu Lo andDekaiWu.
Unsupervised vs. super-vised weight estimation for semantic MT evalu-ation metrics.
In Sixth Workshop on Syntax, Se-mantics and Structure in Statistical Translation(SSST-6), 2012.Chi-kiu Lo and Dekai Wu.
Can informal genresbe better translated by tuning on automatic se-mantic metrics?
In 14th Machine TranslationSummit (MT Summit XIV), 2013.Chi-kiu Lo and Dekai Wu.
MEANT at WMT2013: A tunable, accurate yet inexpensive se-mantic frame based mt evaluation metric.
In8th Workshop on Statistical Machine Transla-tion (WMT 2013), 2013.Chi-kiu Lo, Anand Karthik Tumuluru, and DekaiWu.
Fully automatic semantic MT evaluation.In 7th Workshop on Statistical Machine Trans-lation (WMT 2012), 2012.Chi-kiu Lo, Karteek Addanki, Markus Saers, andDekai Wu.
Improving machine translation bytraining against an automatic semantic framebased evaluation metric.
In 51st Annual Meet-ing of the Association for Computational Lin-guistics (ACL 2013), 2013.Chi-kiu Lo, Meriem Beloucif, and Dekai Wu.
Im-proving machine translation into Chinese bytuning against Chinese MEANT.
In Interna-tional Workshop on Spoken Language Transla-tion (IWSLT 2013), 2013.Chi-kiu Lo, Meriem Beloucif, Markus Saers, andDekai Wu.
XMEANT: Better semantic MTevaluation without reference translations.
In52nd Annual Meeting of the Association forComputational Linguistics (ACL 2014), 2014.Matou?Mach?
?ek andOnd?ej Bojar.
Results of theWMT13 metrics shared task.
In Eighth Work-shop on Statistical Machine Translation (WMT2013), Sofia, Bulgaria, August 2013.I.
Dan Melamed.
Automatic construction ofclean broad-coverage translation lexicons.
In2nd Conference of the Association for Ma-chine Translation in the Americas (AMTA-1996), 1996.Rada Mihalcea, Courtney Corley, and Carlo Strap-parava.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In TheTwenty-first National Conference on ArtificialIntelligence (AAAI-06), volume 21, 2006.Sonja Nie?en, Franz Josef Och, Gregor Leusch,and Hermann Ney.
A evaluation tool for ma-chine translation: Fast evaluation for MT re-search.
In The Second International Conferenceon Language Resources and Evaluation (LREC2000), 2000.Karolina Owczarzak, Josef van Genabith, andAndy Way.
Dependency-based automatic eval-uation for machine translation.
In Syntaxand Structure in Statistical Translation (SSST),2007.Karolina Owczarzak, Josef van Genabith, andAndy Way.
Evaluating machine translationwith LFG dependencies.
Machine Translation,21:95?119, 2007.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automaticevaluation of machine translation.
In 40th An-nual Meeting of the Association for Compu-tational Linguistics (ACL-02), pages 311?318,Philadelphia, Pennsylvania, July 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
Shallow se-mantic parsing using support vector machines.In Human Language Technology Conferenceof the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL 2004), 2004.Miguel Rios, Wilker Aziz, and Lucia Specia.TINE: A metric to assess MT adequacy.
InSixth Workshop on Statistical Machine Transla-tion (WMT 2011), 2011.32Markus Saers and Dekai Wu.
Improving phrase-based translation via word alignments fromstochastic inversion transduction grammars.
InThird Workshop on Syntax and Structure inStatistical Translation (SSST-3), pages 28?36,Boulder, Colorado, June 2009.Markus Saers, Joakim Nivre, and Dekai Wu.Learning stochastic bracketing inversion trans-duction grammars with a cubic time biparsingalgorithm.
In 11th International Conference onParsing Technologies (IWPT?09), pages 29?32,Paris, France, October 2009.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
A studyof translation edit rate with targeted human an-notation.
In 7th Biennial Conference Asso-ciation for Machine Translation in the Ameri-cas (AMTA 2006), pages 223?231, Cambridge,Massachusetts, August 2006.Anand Karthik Tumuluru, Chi-kiu Lo, and DekaiWu.
Accuracy and robustness in measuring thelexical similarity of semantic role fillers for au-tomatic semantic MT evaluation.
In 26th Pa-cific Asia Conference on Language, Informa-tion, and Computation (PACLIC 26), 2012.Mengqiu Wang and Christopher D. Manning.SPEDE: Probabilistic edit distance metrics forMT evaluation.
In 7th Workshop on StatisticalMachine Translation (WMT 2012), 2012.DekaiWu.
An algorithm for simultaneously brack-eting parallel texts by aligning words.
In 33rdAnnual Meeting of the Association for Compu-tational Linguistics (ACL 95), pages 244?251,Cambridge, Massachusetts, June 1995.Dekai Wu.
Trainable coarse bilingual grammarsfor parallel text bracketing.
In Third AnnualWorkshop on Very Large Corpora (WVLC-3),pages 69?81, Cambridge, Massachusetts, June1995.Dekai Wu.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, 1997.Richard Zens and Hermann Ney.
A compara-tive study on reordering constraints in statisti-cal machine translation.
In 41st Annual Meetingof the Association for Computational Linguis-tics (ACL-2003), pages 144?151, Stroudsburg,Pennsylvania, 2003.33
