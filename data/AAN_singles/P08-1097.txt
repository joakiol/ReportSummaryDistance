Proceedings of ACL-08: HLT, pages 852?860,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGestural Cohesion for Topic SegmentationJacob Eisenstein, Regina Barzilay and Randall DavisComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology77 Massachusetts Ave., Cambridge MA 02139{jacobe, regina, davis}@csail.mit.eduAbstractThis paper explores the relationship betweendiscourse segmentation and coverbal gesture.Introducing the idea of gestural cohesion, weshow that coherent topic segments are char-acterized by homogeneous gestural forms andthat changes in the distribution of gesturalfeatures predict segment boundaries.
Gestu-ral features are extracted automatically fromvideo, and are combined with lexical featuresin a Bayesian generative model.
The resultingmultimodal system outperforms text-only seg-mentation on both manual and automatically-recognized speech transcripts.1 IntroductionWhen people communicate face-to-face, discoursecues are expressed simultaneously through multiplechannels.
Previous research has extensively studiedhow discourse cues correlate with lexico-syntacticand prosodic features (Hearst, 1994; Hirschberg andNakatani, 1998; Passonneau and Litman, 1997); thiswork informs various text and speech processingapplications, such as automatic summarization andsegmentation.
Gesture is another communicativemodality that frequently accompanies speech, yet ithas not been exploited for computational discourseanalysis.This paper empirically demonstrates that gesturecorrelates with discourse structure.
In particular,we show that automatically-extracted visual fea-tures can be combined with lexical cues in a sta-tistical model to predict topic segmentation, a fre-quently studied form of discourse structure.
Ourmethod builds on the idea that coherent discoursesegments are characterized by gestural cohesion; inother words, that such segments exhibit homoge-neous gestural patterns.
Lexical cohesion (Hallidayand Hasan, 1976) forms the backbone of many ver-bal segmentation algorithms, on the theory that seg-mentation boundaries should be placed where thedistribution of words changes (Hearst, 1994).
Withgestural cohesion, we explore whether the same ideaholds for gesture features.The motivation for this approach comes from aseries of psycholinguistic studies suggesting thatgesture supplements speech with meaningful andunique semantic content (McNeill, 1992; Kendon,2004).
We assume that repeated patterns in gestureare indicative of the semantic coherence that charac-terizes well-defined discourse segments.
An advan-tage of this view is that gestures can be brought tobear on discourse analysis without undertaking thedaunting task of recognizing and interpreting indi-vidual gestures.
This is crucial because coverbalgesture ?
unlike formal sign language ?
rarely fol-lows any predefined form or grammar, and may varydramatically by speaker.A key implementational challenge is automati-cally extracting gestural information from raw videoand representing it in a way that can applied to dis-course analysis.
We employ a representation of vi-sual codewords, which capture clusters of low-levelmotion patterns.
For example, one codeword maycorrespond to strong left-right motion in the up-per part of the frame.
These codewords are thentreated similarly to lexical items; our model iden-tifies changes in their distribution, and predicts topic852boundaries appropriately.
The overall framework isimplemented as a hierarchical Bayesian model, sup-porting flexible integration of multiple knowledgesources.Experimental results support the hypothesis thatgestural cohesion is indicative of discourse struc-ture.
Applying our algorithm to a dataset of face-to-face dialogues, we find that gesture commu-nicates unique information, improving segmenta-tion performance over lexical features alone.
Thepositive impact of gesture is most pronouncedwhen automatically-recognized speech transcriptsare used, but gestures improve performance by asignificant margin even in combination with manualtranscripts.2 Related WorkGesture and discourse Much of the work on ges-ture in natural language processing has focusedon multimodal dialogue systems in which the ges-tures and speech may be constrained, e.g.
(Johnston,1998).
In contrast, we focus on improving discourseprocessing on unconstrained natural language be-tween humans.
This effort follows basic psycho-logical and linguistic research on the communicativerole of gesture (McNeill, 1992; Kendon, 2004), in-cluding some efforts that made use of automaticallyacquired visual features (Quek, 2003).
We extendthese empirical studies with a statistical model of therelationship between gesture and discourse segmen-tation.Hand-coded descriptions of body posture shiftsand eye gaze behavior have been shown to correlatewith topic and turn boundaries in task-oriented dia-logue (Cassell et al, 2001).
These findings are ex-ploited to generate realistic conversational ?ground-ing?
behavior in an animated agent.
The seman-tic content of gesture was leveraged ?
again, forgesture generation ?
in (Kopp et al, 2007), whichpresents an animated agent that is capable of aug-menting navigation directions with gestures that de-scribe the physical properties of landmarks alongthe route.
Both systems generate plausible andhuman-like gestural behavior; we address the con-verse problem of interpreting such gestures.In this vein, hand-coded gesture features havebeen used to improve sentence segmentation, show-ing that sentence boundaries are unlikely to over-lap gestures that are in progress (Chen et al, 2006).Features that capture the start and end of gesturesare shown to improve sentence segmentation beyondlexical and prosodic features alone.
This idea of ges-tural features as a sort of visual punctuation has par-allels in the literature on prosody, which we discussin the next subsection.Finally, ambiguous noun phrases can be resolvedby examining the similarity of co-articulated ges-tures (Eisenstein and Davis, 2007).
While nounphrase coreference can be viewed as a discourse pro-cessing task, we address the higher-level discoursephenomenon of topic segmentation.
In addition, thisprior work focused primarily on pointing gesturesdirected at pre-printed visual aids.
The current pa-per presents a new domain, in which speakers do nothave access to visual aids.
Thus pointing gesturesare less frequent than ?iconic?
gestures, in which theform of motion is the principle communicative fea-ture (McNeill, 1992).Non-textual features for topic segmentation Re-search on non-textual features for topic segmenta-tion has primarily focused on prosody, under the as-sumption that a key prosodic function is to markstructure at the discourse level (Steedman, 1990;Grosz and Hirshberg, 1992; Swerts, 1997).
The ul-timate goal of this research is to find correlates ofhierarchical discourse structure in phonetic features.Today, research on prosody has converged onprosodic cues which correlate with discourse struc-ture.
Such markers include pause duration, fun-damental frequency, and pitch range manipula-tions (Grosz and Hirshberg, 1992; Hirschberg andNakatani, 1998).
These studies informed the devel-opment of applications such as segmentation toolsfor meeting analysis, e.g.
(Tur et al, 2001; Galley etal., 2003).In comparison, the connection between gestureand discourse structure is a relatively unexploredarea, at least with respect to computational ap-proaches.
One conclusion that emerges from ouranalysis is that gesture may signal discourse struc-ture in a different way than prosody does: while spe-cific prosodic markers characterize segment bound-aries, gesture predicts segmentation through intra-segmental cohesion.
The combination of these two853modalities is an exciting direction for future re-search.3 Visual Features for Discourse AnalysisThis section describes the process of building a rep-resentation that permits the assessment of gesturalcohesion.
The core signal-level features are basedon spatiotemporal interest points, which provide asparse representation of the motion in the video.
Ateach interest point, visual, spatial, and kinematiccharacteristics are extracted and then concatenatedinto vectors.
Principal component analysis (PCA)reduces the dimensionality to a feature vector ofmanageable size (Bishop, 2006).
These feature vec-tors are then clustered, yielding a codebook of visualforms.
This video processing pipeline is shown inFigure 1; the remainder of the section describes theindividual steps in greater detail.3.1 Spatiotemporal Interest PointsSpatiotemporal interest points (Laptev, 2005) pro-vide a sparse representation of motion in video.
Theidea is to select a few local regions that contain highinformation content in both the spatial and tempo-ral dimensions.
The image features at these regionsshould be relatively robust to lighting and perspec-tive changes, and they should capture the relevantmovement in the video.
The set of spatiotemporalinterest points thereby provides a highly compressedrepresentation of the key visual features.
Purely spa-tial interest points have been successful in a varietyof image processing tasks (Lowe, 1999), and spa-tiotemporal interest points are beginning to showsimilar advantages for video processing (Laptev,2005).The use of spatiotemporal interest points is specif-ically motivated by techniques from the computervision domain of activity recognition (Efros et al,2003; Niebles et al, 2006).
The goal of activityrecognition is to classify video sequences into se-mantic categories: e.g., walking, running, jumping.As a simple example, consider the task of distin-guishing videos of walking from videos of jump-ing.
In the walking videos, the motion at most ofthe interest points will be horizontal, while in thejumping videos it will be vertical.
Spurious verticalmotion in a walking video is unlikely to confuse theclassifier, as long as the majority of interest pointsmove horizontally.
The hypothesis of this paper isthat just as such low-level movement features can beapplied in a supervised fashion to distinguish activi-ties, they can be applied in an unsupervised fashionto group co-speech gestures into perceptually mean-ingful clusters.The Activity Recognition Toolbox (Dolla?r et al,2005)1 is used to detect spatiotemporal interestpoints for our dataset.
This toolbox ranks interestpoints using a difference-of-Gaussians filter in thespatial dimension, and a set of Gabor filters in thetemporal dimension.
The total number of interestpoints extracted per video is set to equal the numberof frames in the video.
This bounds the complexityof the representation to be linear in the length of thevideo; however, the system may extract many inter-est points in some frames and none in other frames.Figure 2 shows the interest points extracted froma representative video frame from our corpus.
Notethat the system has identified high contrast regionsof the gesturing hand.
From manual inspection,the large majority of interest points extracted in ourdataset capture motion created by hand gestures.Thus, for this dataset it is reasonable to assume thatan interest point-based representation expresses thevisual properties of the speakers?
hand gestures.
Invideos containing other sources of motion, prepro-cessing may be required to filter out interest pointsthat are extraneous to gestural communication.3.2 Visual DescriptorsAt each interest point, the temporal and spatialbrightness gradients are constructed across a smallspace-time volume of nearby pixels.
Brightness gra-dients have been used for a variety of problems incomputer vision (Forsyth and Ponce, 2003), and pro-vide a fairly general way to describe the visual ap-pearance of small image patches.
However, even fora small space-time volume, the resulting dimension-ality is still quite large: a 10-by-10 pixel box across 5video frames yields a 500-dimensional feature vec-tor for each of the three gradients.
For this reason,principal component analysis (Bishop, 2006) is usedto reduce the dimensionality.
The spatial location ofthe interest point is added to the final feature vector.1http://vision.ucsd.edu/?pdollar/research/cuboids doc/index.html854Figure 1: The visual processing pipeline for the extraction of gestural codewords from video.Figure 2: Circles indicate the interest points extractedfrom this frame of the corpus.This visual feature representation is substantiallylower-level than the descriptions of gesture formfound in both the psychology and computer scienceliteratures.
For example, when manually annotat-ing gesture, it is common to employ a taxonomyof hand shapes and trajectories, and to describe thelocation with respect to the body and head (Mc-Neill, 1992; Martell, 2005).
Working with automatichand tracking, Quek (2003) automatically computesperceptually-salient gesture features, such as sym-metric motion and oscillatory repetitions.In contrast, our feature representation takes theform of a vector of continuous values and is not eas-ily interpretable in terms of how the gesture actu-ally appears.
However, this low-level approach of-fers several important advantages.
Most critically,it requires no initialization and comparatively littletuning: it can be applied directly to any video with afixed camera position and static background.
Sec-ond, it is robust: while image noise may cause afew spurious interest points, the majority of inter-est points should still guide the system to an appro-priate characterization of the gesture.
In contrast,hand tracking can become irrevocably lost, requiringmanual resets (Gavrila, 1999).
Finally, the successof similar low-level interest point representations atthe activity-recognition task provides reason for op-timism that they may also be applicable to unsuper-vised gesture analysis.3.3 A Lexicon of Visual FormsAfter extracting a set of low-dimensional featurevectors to characterize the visual appearance at eachspatiotemporal interest point, it remains only toconvert this into a representation amenable to acohesion-based analysis.
Using k-means cluster-ing (Bishop, 2006), the feature vectors are groupedinto codewords: a compact, lexicon-like representa-tion of salient visual features in video.
The numberof clusters is a tunable parameter, though a system-atic investigation of the role of this parameter is leftfor future work.Codewords capture frequently-occurring patternsof motion and appearance at a local scale ?
interestpoints that are clustered together have a similar vi-sual appearance.
Because most of the motion in ourvideos is gestural, the codewords that appear duringa given sentence provide a succinct representation ofthe ongoing gestural activity.
Distributions of code-words over time can be analyzed in similar termsto the distribution of lexical features.
A change inthe distribution of codewords indicates new visualkinematic elements entering the discourse.
Thus, thecodeword representation allows gestural cohesion tobe assessed in much the same way as lexical cohe-sion.4 Bayesian Topic SegmentationTopic segmentation is performed in a Bayesianframework, with each sentence?s segment index en-coded in a hidden variable, written zt.
The hiddenvariables are assumed to be generated by a linearsegmentation, such that zt ?
{zt?1, zt?1 + 1}.
Ob-servations ?
the words and gesture codewords ?
are855generated by multinomial language models that areindexed according to the segment.
In this frame-work, a high-likelihood segmentation will includelanguage models that are tightly focused on a com-pact vocabulary.
Such a segmentation maximizesthe lexical cohesion of each segment.
This modelthus provides a principled, probabilistic frameworkfor cohesion-based segmentation, and we will seethat the Bayesian approach is particularly well-suited to the combination of multiple modalities.Formally, our goal is to identify the best possiblesegmentation S, where S is a tuple: S = ?z, ?, ?
?.The segment indices for each sentence are writtenzt; for segment i, ?i and ?i are multinomial lan-guage models over words and gesture codewords re-spectively.
For each sentence, xt and yt indicatethe words and gestures that appear.
We will seek toidentify the segmentation S?
= argmaxSp(S,x,y),conditioned on priors that will be defined below.p(S,x,y) = p(x,y|S)p(S)p(x,y|S) =?ip({xt : zt = i}|?i)p({yt : zt = i}|?i)(1)p(S) = p(z)?ip(?i)p(?i) (2)The language models ?i and ?i are multinomialdistributions, so the log-likelihood of the obser-vations xt is log p(xt|?i) =?Wj n(t, j) log ?i,j ,where n(t, j) is the count of word j in sentence t,and W is the size of the vocabulary.
An analogousequation is used for the gesture codewords.
Eachlanguage model is given a symmetric Dirichlet prior?.
As we will see shortly, the use of different pri-ors for the verbal and gestural language models al-lows us to weight these modalities in a Bayesianframework.
Finally, we model the probability ofthe segmentation z by considering the durations ofeach segment: p(z) =?i p(dur(i)|?).
A negative-binomial distribution with parameter ?
is applied todiscourage extremely short or long segments.Inference Crucially, both the likelihood (equa-tion 1) and the prior (equation 2) factor into a prod-uct across the segments.
This factorization en-ables the optimal segmentation to be found usinga dynamic program, similar to those demonstratedby Utiyama and Isahara (2001) and Malioutov andBarzilay (2006).
For each set of segmentation pointsz, the associated language models are set to theirposterior expectations, e.g., ?i = E[?|{xt : zt =i}, ?
].The Dirichlet prior is conjugate to the multino-mial, so this expectation can be computed in closedform:?i,j =n(i, j) + ?N(i) +W?, (3)where n(i, j) is the count of word j in segmenti and N(i) is the total number of words in seg-ment i (Bernardo and Smith, 2000).
The symmetricDirichlet prior ?
acts as a smoothing pseudo-count.In the multimodal context, the priors act to controlthe weight of each modality.
If the prior for the ver-bal language model ?
is high relative to the prior forthe gestural language model ?
then the verbal multi-nomial will be smoother, and will have a weaker im-pact on the final segmentation.
The impact of thepriors on the weights of each modality is exploredin Section 6.Estimation of priors The distribution over seg-ment durations is negative-binomial, with parame-ters ?.
In general, the maximum likelihood estimateof the parameters of a negative-binomial distribu-tion cannot be found in closed form (Balakrishnanand Nevzorov, 2003).
For any given segmentation,the maximum-likelihood setting for ?
is found viaa gradient-based search.
This setting is then usedto generate another segmentation, and the processis iterated until convergence, as in hard expectation-maximization.
The Dirichlet priors on the languagemodels are symmetric, and are chosen via cross-validation.
Sampling or gradient-based techniquesmay be used to estimate these parameters, but this isleft for future work.Relation to other segmentation models Othercohesion-based techniques have typically focusedon hand-crafted similarity metrics between sen-tences, such as cosine similarity (Galley et al, 2003;Malioutov and Barzilay, 2006).
In contrast, themodel described here is probabilistically motivated,maximizing the joint probability of the segmentationwith the observed words and gestures.
Our objec-tive criterion is similar in form to that of Utiyamaand Isahara (2001); however, in contrast to this prior856work, our criterion is justified by a Bayesian ap-proach.
Also, while the smoothing in our approacharises naturally from the symmetric Dirichlet prior,Utiyama and Isahara apply Laplace?s rule and addpseudo-counts of one in all cases.
Such an approachwould be incapable of flexibly balancing the contri-butions of each modality.5 Evaluation SetupDataset Our dataset is composed of fifteen audio-video recordings of dialogues limited to three min-utes in duration.
The dataset includes nine differ-ent pairs of participants.
In each video one of fivesubjects is discussed.
The potential subjects includea ?Tom and Jerry?
cartoon, a ?Star Wars?
toy, andthree mechanical devices: a latchbox, a piston, anda candy dispenser.
One participant ?
?participant A??
was familiarized with the topic, and is tasked withexplaining it to participant B, who is permitted toask questions.
Audio from both participants is used,but only video of participant A is used; we do not ex-amine whether B?s gestures are relevant to discoursesegmentation.Video was recorded using standard camcorders,with a resolution of 720 by 480 at 30 frames persecond.
The video was reduced to 360 by 240 gray-scale images before visual analysis is applied.
Audiowas recorded using headset microphones.
No man-ual postprocessing is applied to the video.Annotations and data processing All speech wastranscribed by hand, and time stamps were obtainedusing the SPHINX-II speech recognition system forforced alignment (Huang et al, 1993).
Sentenceboundaries are annotated according to (NIST, 2003),and additional sentence boundaries are automati-cally inserted at all turn boundaries.
Commonly-occurring terms unlikely to impact segmentation areautomatically removed by using a stoplist.For automatic speech recognition, the default Mi-crosoft speech recognizer was applied to each sen-tence, and the top-ranked recognition result was re-ported.
As is sometimes the case in real-world ap-plications, no speaker-specific training data is avail-able.
The resulting recognition quality is very poor,yielding a word error rate of 77%.Annotators were instructed to select segmentboundaries that divide the dialogue into coherenttopics.
Segmentation points are required to coincidewith sentence or turn boundaries.
A second annota-tor ?
who is not an author on any paper connectedwith this research ?
provided an additional set ofsegment annotations on six documents.
On this sub-set of documents, the Pk between annotators was.306, and the WindowDiff was .325 (these metricsare explained in the next subsection).
This is simi-lar to the interrater agreement reported byMalioutovand Barzilay (2006).Over the fifteen dialogues, a total of 7458 wordswere transcribed (497 per dialogue), spread over1440 sentences or interrupted turns (96 per dia-logue).
There were a total of 102 segments (6.8per dialogue), from a minimum of four to a maxi-mum of ten.
This rate of fourteen sentences or in-terrupted turns per segment indicates relatively fine-grained segmentation.
In the physics lecture corpusused by Malioutov and Barzilay (2006), there areroughly 100 sentences per segment.
On the ICSIcorpus of meeting transcripts, Galley et al (2003)report 7.5 segments per meeting, with 770 ?poten-tial boundaries,?
suggesting a similar rate of roughly100 sentences or interrupted turns per segment.The size of this multimodal dataset is orders ofmagnitude smaller than many other segmentationcorpora.
For example, the Broadcast News corpusused by Beeferman et al (1999) and others con-tains two million words.
The entire ICSI meetingcorpus contains roughly 600,000 words, althoughonly one third of this dataset was annotated for seg-mentation (Galley et al, 2003).
The physics lecturecorpus that was mentioned above contains 232,000words (Malioutov and Barzilay, 2006).
The taskconsidered in this section is thus more difficult thanmuch of the previous discourse segmentation workon two dimensions: there is less training data, and afiner-grained segmentation is required.Metrics All experiments are evaluated in termsof the commonly-used Pk (Beeferman et al, 1999)and WindowDiff (WD) (Pevzner and Hearst, 2002)scores.
These metrics are penalties, so lower val-ues indicate better segmentations.
The Pk metricexpresses the probability that any randomly chosenpair of sentences is incorrectly segmented, if theyare k sentences apart (Beeferman et al, 1999).
Fol-lowing tradition, k is set to half of the mean seg-857Method Pk WD1.
gesture only .486 .5022.
ASR only .462 .4763.
ASR + gesture .388 .4014. transcript only .382 .3975. transcript + gesture .332 .3496. random .473 .5267. equal-width .508 .515Table 1: For each method, the score of the best perform-ing configuration is shown.
Pk and WD are penalties, solower values indicate better performance.ment length.
The WindowDiff metric is a varia-tion of Pk (Pevzner and Hearst, 2002), applying apenalty whenever the number of segments within thek-sentence window differs for the reference and hy-pothesized segmentations.Baselines Two na?
?ve baselines are evaluated.Given that the annotator has divided the dialogueinto K segments, the random baseline arbitrarychooses K random segmentation points.
The re-sults of this baseline are averaged over 1000 itera-tions.
The equal-width baseline places boundariessuch that all segments contain an equal number ofsentences.
Both the experimental systems and thesena?
?ve baselines were given the correct number ofsegments, and also were provided with manually an-notated sentence boundaries ?
their task is to selectthe k sentence boundaries that most accurately seg-ment the text.6 ResultsTable 1 shows the segmentation performance for arange of feature sets, as well as the two baselines.Given only gesture features the segmentation resultsare poor (line 1), barely outperforming the baselines(lines 6 and 7).
However, gesture proves highly ef-fective as a supplementary modality.
The combina-tion of gesture with ASR transcripts (line 3) yieldsan absolute 7.4% improvement over ASR transcriptsalone (line 4).
Paired t-tests show that this resultis statistically significant (t(14) = 2.71, p < .01for both Pk and WindowDiff).
Even when man-ual speech transcripts are available, gesture featuresyield a substantial improvement, reducing Pk andWD by roughly 5%.
This result is statistically sig-nificant for both Pk (t(14) = 2.00, p < .05) andWD (t(14) = 1.94, p < .05).Interactions of verbal and gesture features Wenow consider the relative contribution of the verbaland gesture features.
In a discriminative setting, thecontribution of each modality would be explicitlyweighted.
In a Bayesian generative model, the sameeffect is achieved through the Dirichlet priors, whichact to smooth the verbal and gestural multinomials ?see equation 3.
For example, when the gesture prioris high and verbal prior is low, the gesture counts aresmoothed, and the verbal counts play a greater rolein segmentation.
When both priors are very high,the model will simply try to find equally-sized seg-ments, satisfying the distribution over durations.The effects of these parameters can be seen in Fig-ure 3.
The gesture model prior is held constant atits ideal value, and the segmentation performanceis plotted against the logarithm of the verbal prior.Low values of the verbal prior cause it to domi-nate the segmentation; this can be seen at the leftof both graphs, where the performance of the multi-modal and verbal-only systems are nearly identical.High values of the verbal prior cause it to be over-smoothed, and performance thus approaches that ofthe gesture-only segmenter.Comparison to other models While much ofthe research on topic segmentation focuses on writ-ten text, there are some comparable systems thatalso aim at unsupervised segmentation of sponta-neous spoken language.
For example, Malioutovand Barzilay (2006) segment a corpus of classroomlectures, using similar lexical cohesion-based fea-tures.
With manual transcriptions, they report a .383Pk and .417 WD on artificial intelligence (AI) lec-tures, and .298 Pk and .311 WD on physics lectures.Our results are in the range bracketed by these twoextremes; the wide range of results suggests that seg-mentation scores are difficult to compare across do-mains.
The segmentation of physics lectures was ata very course level of granularity, while the segmen-tation of AI lectures was more similar to our anno-tations.We applied the publicly-available executable forthis algorithm to our data, but performance waspoor, yielding a .417 Pk and .465 WD even whenboth verbal and gestural features were available.858?3 ?2.5 ?2 ?1.5 ?1 ?0.50.320.340.360.380.40.42log verbal priorPkverbal?onlymultimodal?3 ?2.5 ?2 ?1.5 ?1 ?0.50.320.340.360.380.40.42log verbal priorWDverbal?onlymultimodalFigure 3: The multimodal and verbal-only performance using the reference transcript.
The x-axis shows the logarithmof the verbal prior; the gestural prior is held fixed at the optimal value.This may be because the technique is not de-signed for the relatively fine-grained segmentationdemanded by our dataset (Malioutov, 2006).7 ConclusionsThis research shows a novel relationship betweengestural cohesion and discourse structure.
Automat-ically extracted gesture features are predictive of dis-course segmentation when used in isolation; whenlexical information is present, segmentation perfor-mance is further improved.
This suggests that ges-tures provide unique information not present in thelexical features alone, even when perfect transcriptsare available.There are at least two possibilities for how ges-ture might impact topic segmentation: ?visual punc-tuation,?
and cohesion.
The visual punctuation viewwould attempt to identify specific gestural patternsthat are characteristic of segment boundaries.
Thisis analogous to research that identifies prosodic sig-natures of topic boundaries, such as (Hirschberg andNakatani, 1998).
By design, our model is incapableof exploiting such phenomena, as our goal is to in-vestigate the notion of gestural cohesion.
Thus, theperformance gains demonstrated in this paper can-not be explained by such punctuation-like phenom-ena; we believe that they are due to the consistentgestural themes that characterize coherent topics.However, we are interested in pursuing the idea ofvisual punctuation in the future, so as to compare thepower of visual punctuation and gestural cohesionto predict segment boundaries.
In addition, the in-teraction of gesture and prosody suggests additionalpossibilities for future research.The videos in the dataset for this paper are fo-cused on the description of physical devices andevents, leading to a fairly concrete set of gestures.In other registers of conversation, gestural form maybe driven more by spatial metaphors, or may con-sist mainly of temporal ?beats.?
In such cases, theimportance of gestural cohesion for discourse seg-mentation may depend on the visual expressivity ofthe speaker.
We plan to examine the extensibility ofgesture cohesion to more naturalistic settings, suchas classroom lectures.Finally, topic segmentation provides only an out-line of the discourse structure.
Richer models of dis-course include hierarchical structure (Grosz and Sid-ner, 1986) and Rhetorical Structure Theory (Mannand Thompson, 1988).
The application of gesturalanalysis to such models may lead to fruitful areas offuture research.AcknowledgmentsWe thank Aaron Adler, C. Mario Christoudias,Michael Collins, Lisa Guttentag, Igor Malioutov,Brian Milch, Matthew Rasmussen, Candace Sidner,Luke Zettlemoyer, and the anonymous reviewers.This research was supported by Quanta Computer,the National Science Foundation (CAREER grantIIS-0448168 and grant IIS-0415865) and the Mi-crosoft Research Faculty Fellowship.859ReferencesNarayanaswamy Balakrishnan and Valery B. Nevzorov.2003.
A primer on statistical distributions.
John Wi-ley & Sons.Doug Beeferman, Adam Berger, and John D. Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1-3):177?210.Jose?
M. Bernardo and Adrian F. M. Smith.
2000.Bayesian Theory.
Wiley.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-more, Candace L. Sidner, and Charles Rich.
2001.Non-verbal cues for discourse structure.
In Proceed-ings of ACL, pages 106?115.Lei Chen, Mary Harper, and Zhongqiang Huang.
2006.Using maximum entropy (ME) model to incorporategesture cues for sentence segmentation.
In Proceed-ings of ICMI, pages 185?192.Piotr Dolla?r, Vincent Rabaud, Garrison Cottrell, andSerge Belongie.
2005.
Behavior recognition viasparse spatio-temporal features.
In ICCV VS-PETS.Alexei A. Efros, Alexander C. Berg, Greg Mori, and Ji-tendra Malik.
2003.
Recognizing action at a distance.In Proceedings of ICCV, pages 726?733.Jacob Eisenstein and Randall Davis.
2007.
Conditionalmodality fusion for coreference resolution.
In Pro-ceedings of ACL, pages 352?359.David A. Forsyth and Jean Ponce.
2003.
Computer Vi-sion: A Modern Approach.
Prentice Hall.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
Proceedings ofACL, pages 562?569.Dariu M. Gavrila.
1999.
Visual analysis of human move-ment: A survey.
Computer Vision and Image Under-standing, 73(1):82?98.Barbara Grosz and Julia Hirshberg.
1992.
Some into-national characteristics of discourse structure.
In Pro-ceedings of ICSLP, pages 429?432.Barbara Grosz and Candace Sidner.
1986.
Attention,intentions, and the structure of discourse.
Computa-tional Linguistics, 12(3):175?204.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman.Marti A. Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of ACL.Julia Hirschberg and Christine Nakatani.
1998.
Acousticindicators of topic segmentation.
In Proceedings ofICSLP.Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, andRonald Rosenfeld.
1993.
An overview of the Sphinx-II speech recognition system.
In Proceedings of ARPAHuman Language Technology Workshop, pages 81?86.Michael Johnston.
1998.
Unification-based multimodalparsing.
In Proceedings of COLING, pages 624?630.Adam Kendon.
2004.
Gesture: Visible Action as Utter-ance.
Cambridge University Press.Stefan Kopp, Paul Tepper, Kim Ferriman, and JustineCassell.
2007.
Trading spaces: How humans and hu-manoids use speech and gesture to give directions.
InToyoaki Nishida, editor, Conversational Informatics:An Engineering Approach.
Wiley.Ivan Laptev.
2005.
On space-time interest points.
In-ternational Journal of Computer Vision, 64(2-3):107?123.David G. Lowe.
1999.
Object recognition from localscale-invariant features.
In Proceedings of ICCV, vol-ume 2, pages 1150?1157.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL, pages 25?32.Igor Malioutov.
2006.
Minimum cut model for spokenlecture segmentation.
Master?s thesis, MassachusettsInstitute of Technology.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8:243?281.Craig Martell.
2005.
FORM: An experiment in the anno-tation of the kinematics of gesture.
Ph.D. thesis, Uni-versity of Pennsylvania.David McNeill.
1992.
Hand and Mind.
The Universityof Chicago Press.Juan Carlos Niebles, Hongcheng Wang, and Li Fei-Fei.2006.
Unsupervised Learning of Human Action Cate-gories Using Spatial-Temporal Words.
In Proceedingsof the British Machine Vision Conference.NIST.
2003.
The Rich Transcription Fall 2003 (RT-03F)Evaluation plan.Rebecca J. Passonneau and Diane J. Litman.
1997.
Dis-course segmentation by human and automated means.Computational Linguistics, 23(1):103?139.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.Francis Quek.
2003.
The catchment feature modelfor multimodal language analysis.
In Proceedings ofICCV.Mark Steedman.
1990.
Structure and intonation in spo-ken language understanding.
In Proceedings of ACL,pages 9?16.Marc Swerts.
1997.
Prosodic features at discourseboundaries of different strength.
The Journal of theAcoustical Society of America, 101:514.Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, andElizabeth Shriberg.
2001.
Integrating prosodic andlexical cues for automatic topic segmentation.
Com-putational Linguistics, 27(1):31?57.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of ACL, pages 491?498.860
