First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38?43,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSentence Clustering via Projection over Term ClustersLili Kotlerman, Ido DaganBar-Ilan UniversityIsraelLili.Kotlerman@biu.ac.ildagan@cs.biu.ac.ilMaya Gorodetsky, Ezra DayaNICE Systems Ltd.IsraelMaya.Gorodetsky@nice.comEzra.Daya@nice.comAbstractThis paper presents a novel sentence cluster-ing scheme based on projecting sentences overterm clusters.
The scheme incorporates exter-nal knowledge to overcome lexical variabilityand small corpus size, and outperforms com-mon sentence clustering methods on two real-life industrial datasets.1 IntroductionClustering is a popular technique for unsupervisedtext analysis, often used in industrial settings to ex-plore the content of large amounts of sentences.
Yet,as may be seen from the results of our research,widespread clustering techniques, which cluster sen-tences directly, result in rather moderate perfor-mance when applied to short sentences, which arecommon in informal media.In this paper we present and evaluate a novelsentence clustering scheme based on projectingsentences over term clusters.
Section 2 brieflyoverviews common sentence clustering approaches.Our suggested clustering scheme is presented inSection 3.
Section 4 describes an implementation ofthe scheme for a particular industrial task, followedby evaluation results in Section 5.
Section 6 listsdirections for future research.2 BackgroundSentence clustering aims at grouping sentences withsimilar meanings into clusters.
Commonly, vectorsimilarity measures, such as cosine, are used to de-fine the level of similarity over bag-of-words encod-ing of the sentences.
Then, standard clustering algo-rithms can be applied to group sentences into clus-ters (see Steinbach et al (2000) for an overview).The most common practice is representing thesentences as vectors in term space and applying theK-means clustering algorithm (Shen et al (2011);Pasquier (2010); Wang et al (2009); Nomoto andMatsumoto (2001); Boros et al (2001)).
An alterna-tive approach involves partitioning a sentence con-nectivity graph by means of a graph clustering algo-rithm (Erkan and Radev (2004); Zha (2002)).The main challenge for any sentence clusteringapproach is language variability, where the samemeaning can be phrased in various ways.
Theshorter the sentences are, the less effective becomesexact matching of their terms.
Compare the fol-lowing newspaper sentence ?The bank is phasing outthe EZ Checking package, with no monthly fee chargedfor balances over $1,500, and is instead offering cus-tomers its Basic Banking account, which carries a fee?with two tweets regarding the same event: ?Whatswrong.. charging $$ for checking a/c?
and ?Now theywant a monthly fee!?.
Though each of the tweets canbe found similar to the long sentence by exact termmatching, they do not share any single term.
Yet,knowing that the words fee and charge are semanti-cally related would allow discovering the similaritybetween the two tweets.External resources can be utilized to provide suchkind of knowledge, by which sentence representa-tion can be enriched.
Traditionally, WordNet (Fell-baum, 1998) has been used for this purpose (She-hata (2009); Chen et al (2003); Hotho et al (2003);Hatzivassiloglou et al (2001)).
Yet, other resources38of semantically-related terms can be beneficial, suchas WordNet::Similarity (Pedersen et al, 2004), sta-tistical resources like that of Lin (1998) or DIRECT(Kotlerman et al, 2010), thesauri, Wikipedia (Hu etal., 2009), ontologies (Suchanek et al, 2007) etc.3 Sentence Clustering via Term ClustersThis section presents a generic sentence clusteringscheme, which involves two consecutive steps: (1)generating relevant term clusters based on lexical se-mantic relatedness and (2) projecting the sentenceset over these term clusters.
Below we describe eachof the two steps.3.1 Step 1: Obtaining Term ClustersIn order to obtain term clusters, a term connectivitygraph is constructed for the given sentence set and isclustered as follows:1.
Create initially an undirected graph withsentence-set terms as nodes and use lexical re-sources to extract semantically-related termsfor each node.2.
Augment the graph nodes with the extractedterms and connect semantically-related nodeswith edges.
Then, partition the graph into termclusters through a graph clustering algorithm.Extracting and filtering related terms.
In Sec-tion 2 we listed a number of lexical resources pro-viding pairs of semantically-related terms.
Withinthe suggested scheme, any combination of resourcesmay be utilized.Often resources contain terms, which aresemantically-related only in certain contexts.
E.g.,the words visa and passport are semantically-relatedwhen talking about tourism, but cannot be consid-ered related in the banking domain, where visa usu-ally occurs in its credit card sense.
In order to dis-card irrelevant terms, filtering procedures can be em-ployed.
E.g., a simple filtering applicable in mostcases of sentence clustering in a specific domainwould discard candidate related terms, which do notoccur sufficiently frequently in a target-domain cor-pus.
In the example above, this procedure wouldallow avoiding the insertion of passport as related tovisa, when considering the banking domain.Clustering the graph nodes.
Once the termgraph is constructed, a graph clustering algorithmis applied resulting in a partition of the graph nodes(terms) into clusters.
The choice of a particular al-gorithm is a parameter of the scheme.
Many clus-tering algorithms consider the graph?s edge weights.To address this trait, different edge weights can beassigned, reflecting the level of confidence that thetwo terms are indeed validly related and the reliabil-ity of the resource, which suggested the correspond-ing edge (e.g.
WordNet synonyms are commonlyconsidered more reliable than statistical thesauri).3.2 Step 2: Projecting Sentences to TermClustersTo obtain sentence clusters, the given sentence sethas to be projected in some manner over the termclusters obtained in Step 1.
Our projection pro-cedure resembles unsupervised text categorization(Gliozzo et al, 2005), with categories representedby term clusters that are not predefined but ratheremerge from the analyzed data:1.
Represent term clusters and sentences as vec-tors in term space and calculate the similarityof each sentence with each of the term clusters.2.
Assign each sentence to the best-scoring termcluster.
(We focus on hard clustering, but theprocedure can be adapted for soft clustering).Various metrics for feature weighting and vectorcomparison may be chosen.
The top terms of term-cluster vectors can be regarded as labels for the cor-responding sentence clusters.Thus each sentence cluster corresponds to a sin-gle coherent cluster of related terms.
This is con-trasted with common clustering methods, where ifsentence A shares a term with B, and B shares an-other term with C, then A and C might appear in thesame cluster even if they have no related terms incommon.
This behavior turns out harmful for shortsentences, where each incidental term is influential.Our scheme ensures that each cluster contains onlysentences related to the underlying term cluster, re-sulting in more coherent clusters.4 Application: Clustering CustomerInteractionsIn industry there?s a prominent need to obtain busi-ness insights from customer interactions in a contactcenter or social media.
Though the number of key39sentences to analyze is often relatively small, suchas a couple hundred, manually analyzing just a hand-ful of clusters is much preferable.
This section de-scribes our implementation of the scheme describedin Section 3 for the task of clustering customer in-teractions, as well as the data used for evaluation.Results and analysis are presented in Section 5.4.1 DataWe apply our clustering approach over two real-lifedatasets.
The first one consists of 155 sentencescontaining reasons of account cancelation, retrievedfrom automatic transcripts of contact center interac-tions of an Internet Service Provider (ISP).
The sec-ond one contains 194 sentences crawled from Twit-ter, expressing reasons for customer dissatisfactionwith a certain banking company.
The sentences inboth datasets were gathered automatically by a rule-based extraction algorithm.
Each dataset is accom-panied by a small corpus of call transcripts or tweetsfrom the corresponding domain.1The goal of clustering these sentences is to iden-tify the prominent reasons of cancelation and dissat-isfaction.
To obtain the gold-standard (GS) anno-tation, sentences were manually grouped to clustersaccording to the reasons stated in them.Table 1 presents examples of sentences from theISP dataset.
The sentences are short, with only oneor two words expressing the actual reason stated inthem.
We see that exact term matching is not suffi-cient to group the related sentences.
Moreover, tra-ditional clustering algorithms are likely to mix re-lated and unrelated sentences, due to matching non-essential terms (e.g.
husband or summer).
We notethat such short and noisy sentences are commonin informal media, which became a most importantchannel of information in industry.4.2 Implementation of the Clustering SchemeOur proposed sentence clustering scheme presentedin Section 3 includes a number of choices.
Belowwe describe the choices we made in our current im-plementation.Input sentences were tokenized, lemmatized andcleaned from stopwords in order to extract content-word terms.
Candidate semantically-related terms1The bank dataset with the output of the tested methods willbe made publicly available.he hasn?t been using it all summer longit?s been sitting idle for about it almost a yearI?m getting married my husband has a computeryeah I bought a new laptop this summer sowhen I said faces my husband got laid off from workwell I?m them going through financial difficultiesTable 1: Example sentences expressing 3 reasons for can-celation: the customer (1) does not use the service, (2)acquired a computer, (3) cannot afford the service.were extracted for each of the terms, using Word-Net synonyms and derivations, as well as DIRECT2,a directional statistical resource learnt from a newscorpus.
Candidate terms that did not appear in theaccompanying domain corpus were filtered out asdescribed in Section 3.1.Edges in the term graph were weighted with thenumber of resources supporting the correspondingedge.
To cluster the graph we used the ChineseWhispers clustering tool3 (Biemann, 2006), whosealgorithm does not require to pre-set the desirednumber of clusters and is reported to outperformother algorithms for several NLP tasks.To generate the projection, sentences were rep-resented as vectors of terms weighted by their fre-quency in each sentence.
Terms of the term-clustervectors were weighted by the number of sentencesin which they occur.
Similarity scores were calcu-lated using the cosine measure.
Clusters were la-beled with the top terms appearing both in the un-derlying term cluster and in the cluster?s sentences.5 Results and AnalysisIn this section we present the results of evaluatingour projection approach, compared to the commonK-means clustering method4 applied to:(A) Standard bag-of-words representation of sen-tences;2Available for download at www.cs.biu.ac.il/?nlp/downloads/DIRECT.html.
For each term weextract from the resource the top-5 related terms.3Available at http://wortschatz.informatik.uni-leipzig.de/?cbiemann/software/CW.html4We use the Weka (Hall et al, 2009) implementation.
Dueto space limitations and for more meaningful comparison we re-port here one value of K, which is equal to the number of clus-ters returned by projection (60 for the ISP and 65 for the bankdataset).
For K = 20, 40 and 70 the performance was similar.40(B) Bag-of-words representation, where sentence?swords are augmented with semantically-relatedterms (following the common scheme of priorwork, see Section 2).
We use the same set ofrelated terms as is used by our method.
(C) Representation of sentences in term-clusterspace, using the term clusters generated by ourmethod as vector features.
A feature is acti-vated in a sentence vector if it contains a termfrom the corresponding term cluster.Table 2 shows the results in terms of Purity, Recall(R), Precision (P) and F1 (see ?Evaluation of clus-tering?, Manning et al (2008)).
Projection signifi-cantly5 outperforms all baselines for both datasets.Dataset Algorithm Purity R P F1ISPProjection .74 .40 .68 .50K-means A .65 .18 .22 .20K-means B .65 .13 .24 .17K-means C .65 .18 .26 .22BankProjection .79 .26 .53 .35K-means A .61 .14 .14 .14K-means B .64 .13 .19 .16K-means C .67 .17 .21 .19Table 2: Evaluation results.For completeness we experimented with applyingChinese Whispers clustering to sentence connectiv-ity graphs, but the results were inferior to K-means.Table 3 presents sample sentences from clustersproduced by projection and K-means for illustration.Our initial analysis showed that our approach indeedproduces more homogenous clusters than the base-line methods, as conjectured in Section 3.2.
We con-sider it advantageous, since it?s easier for a human tomerge clusters than to reveal sub-clusters.
E.g., a GScluster of 20 sentences referring to fees and chargesis covered by three projection clusters labeled fee,charge and interest rate, with 9, 8 and 2 sentencescorrespondingly.
On the other hand, K-means Cmethod places 11 out of the 20 sentences in a messycluster of 57 sentences (see Table 3), scattering theremaining 9 sentences over 7 other clusters.In our current implementation fee, charge and in-terest rate were not detected by the lexical resourceswe used as semantically similar and thus were not5p=0.001 according to McNemar test (Dietterich, 1998).grouped in one term cluster.
However, adding moreresources may introduce additional noise.
Such de-pendency on coverage and accuracy of resources isapparently a limitation of our approach.
Yet, asour experiments indicate, using only two generic re-sources already yielded valuable results.a.
Projectioncredit card, card, mastercard, visa (38 sentences)XXX has the worst credit cards everXXX MasterCard is the worst credit card I?ve ever hadntuc do not accept XXX visa now I have to redraw $150...XXX card declined again , $40 dinner in SF...b. K-means Cfee, charge (57 sentences)XXX playing games wit my interestarguing w incompetent pol at XXX damansara perdanaXXX?s upper management are a bunch of rude pricksXXX are ninjas at catching fraudulent charges.Table 3: Excerpt from resulting clusterings for the bankdataset.
Bank name is substituted with XXX.
Cluster la-bels are given in italics.
Two most frequent terms areassigned as cluster labels for K-means C.6 Conclusions and Future WorkWe presented a novel sentence clustering schemeand evaluated its implementation, showing signifi-cantly superior performance over common sentenceclustering techniques.
We plan to further explorethe suggested scheme by utilizing additional lexicalresources and clustering algorithms.
We also planto compare our approach with co-clustering meth-ods used in document clustering (Xu et al (2003),Dhillon (2001), Slonim and Tishby (2000)).AcknowledgmentsThis work was partially supported by the MAGNE-TON grant no.
43834 of the Israel Ministry of Indus-try, Trade and Labor, the Israel Ministry of Scienceand Technology, the Israel Science Foundation grant1112/08, the PASCAL-2 Network of Excellence ofthe European Community FP7-ICT-2007-1-216886and the European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).41ReferencesChris Biemann.
2006.
Chinese whispers - an efficientgraph clustering algorithm and its application to nat-ural language processing problems.
In Proceedingsof TextGraphs: the Second Workshop on Graph BasedMethods for Natural Language Processing, pages 73?80, New York City, USA.Endre Boros, Paul B. Kantor, and David J. Neu.
2001.
Aclustering based approach to creating multi-documentsummaries.Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su.2003.
Clustering and visualization in a multi-lingualmulti-document summarization system.
In Proceed-ings of the 25th European conference on IR re-search, ECIR?03, pages 266?280, Berlin, Heidelberg.Springer-Verlag.Inderjit S. Dhillon.
2001.
Co-clustering documents andwords using bipartite spectral graph partitioning.
InProceedings of the seventh ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, KDD ?01, pages 269?274, New York, NY,USA.
ACM.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learningalgorithms.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in text sum-marization.
J. Artif.
Int.
Res., 22(1):457?479, Decem-ber.C.
Fellbaum.
1998.
WordNet ?
An Electronic LexicalDatabase.
MIT Press.Alfio Massimiliano Gliozzo, Carlo Strapparava, and IdoDagan.
2005.
Investigating unsupervised learning fortext categorization bootstrapping.
In HLT/EMNLP.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.Holcombe, Regina Barzilay, Min yen Kan, and Kath-leen R. McKeown.
2001.
Simfinder: A flexible clus-tering tool for summarization.
In In Proceedings of theNAACL Workshop on Automatic Summarization, pages41?49.A.
Hotho, S. Staab, and G. Stumme.
2003.
Word-net improves text document clustering.
In YingDing, Keith van Rijsbergen, Iadh Ounis, and Joe-mon Jose, editors, Proceedings of the Semantic WebWorkshop of the 26th Annual International ACM SI-GIR Conference on Research and Development in In-formaion Retrieval (SIGIR 2003), August 1, 2003,Toronto Canada.
Published Online at http://de.scientificcommons.org/608322.Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, andXiaohua Zhou.
2009.
Exploiting wikipedia as exter-nal knowledge for document clustering.
In Proceed-ings of the 15th ACM SIGKDD international confer-ence on Knowledge discovery and data mining, KDD?09, pages 389?396, New York, NY, USA.
ACM.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distributionalsimilarity for lexical inference.
JNLE, 16:359?389.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th interna-tional conference on Computational linguistics - Vol-ume 2, COLING ?98, pages 768?774, Stroudsburg,PA, USA.
Association for Computational Linguistics.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, Cambridge, Juli.Tadashi Nomoto and Yuji Matsumoto.
2001.
A new ap-proach to unsupervised text summarization.
In Pro-ceedings of the 24th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, SIGIR ?01, pages 26?34, New York, NY,USA.
ACM.Claude Pasquier.
2010.
Task 5: Single documentkeyphrase extraction using sentence clustering and la-tent dirichlet alocation.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation, Se-mEval ?10, pages 154?157, Stroudsburg, PA, USA.Association for Computational Linguistics.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Shady Shehata.
2009.
A wordnet-based semantic modelfor enhancing text clustering.
Data Mining Work-shops, International Conference on, 0:477?482.Chao Shen, Tao Li, and Chris H. Q. Ding.
2011.
Integrat-ing clustering and multi-document summarization bybi-mixture probabilistic latent semantic analysis (plsa)with sentence bases.
In AAAI.Noam Slonim and Naftali Tishby.
2000.
Document clus-tering using word clusters via the information bottle-neck method.
In Proceedings of the 23rd annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, SIGIR ?00, pages208?215, New York, NY, USA.
ACM.M.
Steinbach, G. Karypis, and V. Kumar.
2000.
Acomparison of document clustering techniques.
KDDWorkshop on Text Mining.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A large ontology fromwikipedia and wordnet.42Dingding Wang, Shenghuo Zhu, Tao Li, and YihongGong.
2009.
Multi-document summarization us-ing sentence-based topic models.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,ACLShort ?09, pages 297?300, Stroudsburg, PA,USA.
Association for Computational Linguistics.Wei Xu, Xin Liu, and Yihong Gong.
2003.
Documentclustering based on non-negative matrix factorization.In Proceedings of the 26th annual international ACMSIGIR conference on Research and development in in-formaion retrieval, SIGIR ?03, pages 267?273, NewYork, NY, USA.
ACM.Hongyuan Zha.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcement prin-ciple and sentence clustering.
In SIGIR, pages 113?120.43
