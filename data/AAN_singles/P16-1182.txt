Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1935?1943,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMUTT: Metric Unit TesTingfor Language Generation TasksWillie Boag, Renan Campos, Kate Saenko, Anna RumshiskyDept.
of Computer ScienceUniversity of Massachusetts Lowell198 Riverside St, Lowell, MA 01854{wboag,rcampos,saenko,arum}@cs.uml.eduAbstractPrecise evaluation metrics are importantfor assessing progress in high-level lan-guage generation tasks such as machinetranslation or image captioning.
Histor-ically, these metrics have been evaluatedusing correlation with human judgment.However, human-derived scores are oftenalarmingly inconsistent and are also limitedin their ability to identify precise areas ofweakness.
In this paper, we perform a casestudy for metric evaluation by measuringthe effect that systematic sentence trans-formations (e.g.
active to passive voice)have on the automatic metric scores.
Thesesentence ?corruptions?
serve as unit testsfor precisely measuring the strengths andweaknesses of a given metric.
We find thatnot only are human annotations heavily in-consistent in this study, but that the Met-ric Unit TesT analysis is able to captureprecise shortcomings of particular metrics(e.g.
comparing passive and active sen-tences) better than a simple correlation withhuman judgment can.1 IntroductionThe success of high-level language generation taskssuch as machine translation (MT), paraphrasing andimage/video captioning depends on the existence ofreliable and precise automatic evaluation metrics.Figure 1: A few select entries from the SICK dataset.
Allof these entries follow the same ?Negated Subject?
transfor-mation between sentence 1 and sentence 2, yet humans anno-tated them with an inconsistently wide range of scores (from1 to 5).
Regardless of whether the gold labels for this partic-ular transformation should score this high or low, they shouldscore be scored consistently.Efforts have been made to create standard met-rics (Papineni et al, 2001; Lin, 2004; Denkowskiand Lavie, 2014; Vedantam et al, 2014) to helpadvance the state-of-the-art.
However, most suchpopular metrics, despite their wide use, have seri-ous deficiencies.
Many rely on ngram matching andassume that annotators generate all reasonable refer-ence sentences, which is infeasible for many tasks.Furthermore, metrics designed for one task, e.g.,MT, can be a poor fit for other tasks, e.g., video cap-tioning.To design better metrics, we need a principledapproach to evaluating their performance.
Histori-cally, MT metrics have been evaluated by how wellthey correlate with human annotations (Callison-Burch et al, 2010; Machacek and Bojar, 2014).However, as we demonstrate in Sec.
5, humanjudgment can result in inconsistent scoring.
Thispresents a serious problem for determining whether1935a metric is ?good?
based on correlation with incon-sistent human scores.
When ?gold?
target data isunreliable, even good metrics can appear to be inac-curate.Furthermore, correlation of system output withhuman-derived scores typically provides an overallscore but fails to isolate specific errors that met-rics tend to miss.
This makes it difficult to dis-cover system-specific weaknesses to improve theirperformance.
For instance, an ngram-based metricmight effectively detect non-fluent, syntactic errors,but could also be fooled by legitimate paraphraseswhose ngrams simply did not appear in the trainingset.
Although there has been some recent work onparaphrasing that provided detailed error analysis ofsystem outputs (Socher et al, 2011; Madnani et al,2012), more often than not such investigations areseen as above-and-beyond when assessing metrics.The goal of this paper is to propose a processfor consistent and informative automated analysisof evaluation metrics.
This method is demonstrablymore consistent and interpretable than correlationwith human annotations.
In addition, we extend theSICK dataset to include un-scored fluency-focusedsentence comparisons and we propose a toy metricfor evaluation.The rest of the paper is as follows: Section 2introduces the corruption-based metric unit testingprocess, Section 3 lists the existing metrics we usein our experiments as well as the toy metric wepropose, Section 4 describes the SICK dataset weused for our experiments, Section 5 motivates theneed for corruption-based evaluation instead of cor-relation with human judgment, Section 6 describesthe experimental procedure for analyzing the met-ric unit tests, Section 7 analyzes the results of ourexperiments, and in Section 8 we offer concludingremarks.2 Metric Unit TesTsWe introduce metric unit tests based on sentencecorruptions as a new method for automatically eval-uating metrics developed for language generationtasks.
Instead of obtaining human ranking forsystem output and comparing it with the metric-based ranking, the idea is to modify existing ref-erences with specific transformations, and exam-ine the scores assigned by various metrics to suchcorruptions.
In this paper, we analyze three broadcategories of transformations ?
meaning-altering,meaning-preserving, and fluency-disrupting sen-tence corruptions ?
and we evaluate how success-fully several common metrics can detect them.As an example, the original sentence ?A man isplaying a guitar.?
can be corrupted as follows:Meaning-Altering: A man is not playing guitar.Meaning-Preserving: A guitar is being playedby a man.Fluency-Disrupting: A man a guitar is playing.Examples for each corruption type we considerare shown in Tables 1 and 2.2.1 Meaning-altering corruptionsMeaning-altering corruptions modify the seman-tics of a sentence, resulting in a new sentencethat has a different meaning.
Corruptions (1?2) check whether a metric can detect small lex-ical changes that cause the sentence?s semanticsto entirely change.
Corruption (3) is designed tofool distributed and distributional representations ofwords, whose vectors often confuse synonyms andantonyms.2.2 Meaning-preserving corruptionsMeaning-preserving corruptions change the lexicalpresentation of a sentence while still preservingmeaning and fluency.
For such transformations, the?corruption?
is actually logically equivalent to theoriginal sentence, and we would expect that consis-tent annotators would assign roughly the same scoreto each.
These transformations include changessuch as rephrasing a sentence from active voice topassive voice (4) or paraphrasing within a sentence(5).2.3 Fluency disruptionsBeyond understanding semantics, metrics must alsorecognize when a sentence lacks fluency and gram-mar.
Corruptions (7?9) were created for this reason,and do so by generating ungrammatical sentences.1936Meaning Altering1 negated subject (337) ?A man is playing a harp?
?There is no man playing a harp?2 negated action (202) ?A jet is flying?
?A jet is not flying?3 antonym replacement (246) ?a dog with short hair?
?a dog with long hair?Meaning Preserving4 active-to-passive (238) ?A man is cutting a potato?
?A potato is being cut by a man?5 synonymous phrases (240) ?A dog is eating a doll?
?A dog is biting a doll?6 determiner substitution (65) ?A cat is eating food?
?The cat is eating food?Table 1: Corruptions from the SICK dataset.
The left column lists the number of instances for each corruption type.Fluency disruptions7 double PP (500) ?A boy walks at night?
?A boy walks at night at night?8 remove head from PP (500) ?A man danced in costume?
?A man danced costume?9 re-order chunked phrases (500) ?A woman is slicing garlics?
?Is slicing garlics a woman?Table 2: Generated corruptions.
The first column gives the total number of generated corruptions in parentheses.3 Metrics Overview3.1 Existing MetricsMany existing metrics work by identifying lexicalsimilarities, such as n-gram matches, between thecandidate and reference sentences.
Commonly-usedmetrics include BLEU, CIDEr, and TER:?
BLEU, an early MT metric, is a precision-based metric that rewards candidates whosewords can be found in the reference but pe-nalizes short sentences and ones which overusepopular n-grams (Papineni et al, 2001).?
CIDEr, an image captioning metric, usesa consensus-based voting of tf-idf weightedngrams to emphasize the most unique seg-ments of a sentence in comparison (Vedantamet al, 2014).?
TER (Translation Edit Rate) counts thechanges needed so the surface forms of the out-put and reference match (Snover et al, 2006).Other metrics have attempted to capture similar-ity beyond surface-level pattern matching:?
METEOR, rather than strictly measuringngram matches, accounts for soft similari-ties between sentences by computing synonymand paraphrase scores between sentence align-ments (Denkowski and Lavie, 2014).?
BADGER takes into account the contexts overthe entire set of reference sentences by usinga simple compression distance calculation af-ter performing a series of normalization steps(Parker, 2008).?
TERp (TER-plus) minimizes the edit dis-tance by stem matches, synonym matches,and phrase substitutions before calculating theTER score, similar to BADGER?s normaliza-tion step (Snover et al, 2009).We evaluate the strengths and weaknesses ofthese existing metrics in Section 7.3.2 Toy Metric: W2V-AVGTo demonstrate how this paper?s techniques can alsobe applied to measure a new evaluation metric, wecreate a toy metric, W2V-AVG, using the cosine ofthe centroid of a sentence?s word2vec embeddings(Mikolov et al, 2013).
The goal for this true bag-of-words metric is to serve as a sanity check for howcorruption unit tests can identify metrics that cap-ture soft word-level similarities, but cannot handledirected relationships between entities.4 Datasets4.1 SICKAll of our experiments are run on the Sentences In-volving Compositional Knowledge (SICK) dataset,1937which contains entries consisting of a pair of sen-tences and a human-estimated semantic relatednessscore to indicate the similarity of the sentence pair(Marelli et al, 2014).
The reason we use this data istwofold:1. it is a well-known and standard dataset withinsemantic textual similarity community.2.
it contains many common sentence transfor-mation patterns, such as those described in Ta-ble 1.The SICK dataset was built from the 8K Image-Flickr dataset1and the SemEval 2012 STS MSR-Video Description corpus2.
Each of these origi-nal datasets contain human-generated descriptionsof images/videos ?
a given video often has 20-50reference sentences describing it.
These referencesets prove very useful because they are more-or-lessparaphrases of one another; they all describe thesame thing.
The creators of SICK selected sentencepairs and instructed human annotators to ensure thatall sentences obeyed proper grammar.
The creatorsof SICK ensured that two of the corruption types?
meaning-altering and meaning-preserving ?
weregenerated in the annotated sentence pairs.
We thenfiltered through SICK using simple rule-based tem-plates3to identify each of the six corruption typeslisted in Table 1.
Finally, we matched the sentencesin the pair back to their original reference sets in theFlickr8 and MSR-Video Description corpora to ob-tain reference sentences for our evaluation metricsexperiments.4.2 SICK+Since all of the entries in the SICK dataset werecreated for compositional semantics, every sentencewas manually checked by annotators to ensure flu-ency.
For our study, we also wanted to measure1http://nlp.cs.illinois.edu/HockenmaierGroup/data.html2http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=data3For instance, the ?Antonym Replacement?
templatechecked to see if the two sentences were one wordapart, and if so whether they had a SICK-annotatedNOT ENTAILMENT relationsip.the effects of bad grammar between sentences, sowe automatically generated our own corruptionsto the SICK dataset to create SICK+, a set offluency-disrupting corruptions.
The rules to gener-ate these corruptions were simple operations involv-ing chunking and POS-tagging.
Fortunately, thesecorruptions were, by design, meant to be ungram-matical, so there was no need for (difficult) auto-matic correctness checking for fluency.5 Inconsistencies in Human JudgmentA major issue with comparing metrics against hu-man judgment is that human judgments are ofteninconsistent.
One reason for this is that high-levelsemantic tasks are difficult to pose to annotators.Consider SICK?s semantic relatedness annotation asa case study for human judgment.
Annotators wereshown two sentences, were asked ?To what extentare the two sentences expressing related meaning?
?,and were instructed to select an integer from 1 (com-pletely unrelated) to 5 (very related).
We can see thedifficulty annotators faced when estimating seman-tic relatedness, especially because the task descrip-tion was intentionally vague to avoid biasing anno-tator judgments with strict definitions.
In the end,?the instructions described the task only through [ahandful of] examples of relatedness?
(Marelli et al,2014).As a result of this hands-off annotation guideline,the SICK dataset contains glaring inconsistencies insemantic relatedness scores, even for sentence pairswhere the only difference between two sentences isdue to the same known transformation.
Figure 1demonstrates the wide range of human-given scoresfor the pairs from SICK that were created with theNegated Subject transformation.
Since the guide-lines did not establish how to handle the effect ofthis transformation, some annotators rated it highfor describing the same actions, while others ratedit low for having completely opposite subjects.To better appreciate the scope of these annota-tion discrepancies, Figure 2 displays the distributionof ?gold?
human scores for every instance of the?Negated Subject?
transformation.
We actually findthat the relatedness score approximately follows anormal distribution centered at 3.6 with a standard1938Figure 2: Human annotations for the Negated Subjectcorruption.Figure 3: Metric predictions for the Negated Subject cor-ruption.deviation of about .45.
The issue with this distri-bution is that regardless of whether the annotatorsrank this specific transformation as low or high, theyshould be ranking it consistently.
Instead, we seethat their annotations span all the way from 2.5 to4.5 with no reasonable justification as to why.Further, a natural question to ask is whether allsentence pairs within this common Negated Sub-ject transformation do, in fact, share a structure ofhow similar their relatedness scores ?should?
be.To answer this question, we computed the similar-ity between the sentences in an automated mannerusing three substantially different evaluation met-rics: METEOR, BADGER, and TERp.
These threemetrics were chosen because they present three verydifferent approaches for quantifying semantic sim-ilarity, namely: sentence alignments, compressionredundancies, and edit rates.
We felt that these dif-ferent approaches for processing the sentence pairswould allow for different views of their underlyingrelatedness.To better understand how similar an automaticmetric would rate these sentence pairs, Figure 3shows the distribution over scores predicted by theMETEOR metric.
The first observation is that themetric produces scores that are far more peaky thanthe gold scores in Figure 2, which indicates that theyhave a significantly more consistent structure aboutthem.In order to see how each metric?s scores com-pare, Table 3 lists all pairwise correlations betweenthe gold and the three metrics.
As a sanity check,we can see that the 1.0s along the diagonal indicateperfect correlation between a prediction and itself.More interestingly, we can see that the three met-rics have alarmingly low correlations with the goldscores: 0.09, 0.03, and 0.07.
However, we also seethat the three metrics all have significantly highercorrelations amongst one another: 0.80, 0.80, and0.91.
This is a very strong indication that the threemetrics all have approximate agreement about howthe various sentences should be scored, but this con-sensus is not at all reflected by the human judg-ments.6 MUTT ExperimentsIn our Metric Unit TesTing experiments, we wantedto measure the fraction of times that a given metricis able to appropriately handle a particular corrup-tion type.
Each (original,corruption) pair is consid-ered a trial, which the metric either gets correct orgold METEOR BADGER TERpgold 1.00 0.09 0.03 0.07METEOR 0.09 1.00 0.91 0.80BADGER 0.03 0.91 1.00 0.80TERp 0.07 0.80 0.80 1.00Table 3: Pairwise correlation between the predictions ofthree evaluation metrics and the gold standard.1939Figure 4: Results for the Determiner Substitution cor-ruption (using Difference formula scores).incorrect.
We report the percent of successful trialsfor each metric in Tables 4, 5, and 6.
Experimentswere run using 5, 10, and 20 reference sentencesto understand which metrics are able perform wellwithout much data and also which metrics are ableto effectively use more data to improve.
An accu-racy of 75% would indicate that the metric is able toassign appropriate scores 3 out of 4 times.4For Meaning-altering and Fleuncy-disruptingcorruptions, the corrupted sentence will be truly dif-ferent from the original and reference sentences.
Atrial would be successful when the score of the orig-inal sorigis rated higher than the score of the cor-ruption scorr:sorig> scorrAlternatively, Meaning-preserving transforma-tions create a ?corruption?
sentence which is just ascorrect as the original.
To reflect this, we consider atrial to be successful when the score of the corrup-tion scorris within 15% of the score of the originalsorig:?????sorig?
scorrsorig+ ??????
0.15where  is a small constant (10?9) to prevent divi-sion by zero.
We refer to this alternative trial formu-lation as the Difference formula.4Our code is made available at https://github.com/text-machine-lab/MUTTFigure 5: Results for the Active-to-Passive corruption(using Difference formula scores).7 Discussion7.1 Meaning-altering corruptionsAs shown by the middle figure in Table 4, it isCIDEr which performs the best for Antonym Re-placement.
Even with only a few reference sen-tences, it is already able to score significantly higherthan the other metrics.
We believe that a largecontributing factor for this is CIDEr?s use of tf-idfweights to emphasize the important aspects of eachsentence, thus highlighting the modified when com-pared against the reference sentences.The success of these metrics reiterates the earlierpoint about metrics being able to perform more con-sistently and reliably than human judgment.7.2 Meaning-preserving corruptionsThe graph in Figure 4 of the determiner substitu-tion corruption shows an interesting trend: as thenumber of references increase, all of the metrics in-crease in accuracy.
This corruption replaces ?a?
inthe candidate with a ?the?, or vice versa.
As the ref-erences increase, there we tend to see more exam-ples which use these determiners interchanagablywhile keeping the rest of the sentence?s meaning thesame.
Since a large number of references results infar more for the pair to agree on, the two scores arevery close.Conversely, the decrease in accuracy in the19401.
Negated Subjectnum refs 5 10 20CIDEr 99.4 99.4 99.4BLEU 99.1 99.7 99.7METEOR 97.0 98.5 98.2BADGER 97.9 97.6 98.2TERp 99.7 99.7 99.42.
Negated Actionnum refs 5 10 20CIDEr 98.5 98.5 98.5BLEU 97.5 97.5 98.0METEOR 96.0 96.0 97.0BADGER 93.6 95.5 96.5TERp 95.5 97.0 95.03.
Antonym Replacementnum refs 5 10 20CIDEr 86.2 92.7 93.5BLEU 76.4 85.4 88.6METEOR 80.9 86.6 91.5BADGER 76.0 85.8 88.6TERp 75.2 79.7 80.1Table 4: Meaning-altering corruptions.
These % accuracies represent the number of times that a givenmetric was able to correctly score the original sentence higher than the corrupted sentence.
Numbers refer-enced in the prose analysis are highlighted in bold.4.
Active-to-Passivenum refs 5 10 20CIDEr 5.5 0.8 2.5BLEU 7.6 4.6 3.8METEOR 23.9 16.0 13.0BADGER 13.4 11.3 12.2TERp 20.6 16.4 9.75.
Synonymous Phrasesnum refs 5 10 20CIDEr 32.1 26.2 30.0BLEU 45.0 36.7 34.2METEOR 62.1 62.1 62.1BADGER 80.8 80.4 86.7TERp 53.3 46.7 41.26.
DT Substitutionnum refs 5 10 20CIDEr 40.0 38.5 56.9BLEU 21.5 27.7 53.8METEOR 55.4 55.4 70.8BADGER 80.0 84.6 95.4TERp 6.2 10.8 27.7Table 5: Meaning-preserving corruptions.
These % accuracies represent the number of times that a givenmetric was able to correctly score the semantically-equaivalent ?corrupted?
sentence within 15% of theoriginal sentence.
Numbers referenced in the prose analysis are highlighted in bold.7.
Duplicate PPnum refs 5 10 20CIDEr 100 99.0 100BLEU 100 100 100METEOR 95.1 98.5 99.5BADGER 63.5 70.0 74.9TERp 96.6 99.0 99.08.
Remove Head From PPnum refs 5 10 20CIDEr 69.5 76.8 80.8BLEU 63.5 81.3 87.7METEOR 60.6 72.9 84.2BADGER 63.1 67.0 71.4TERp 52.7 66.5 70.49.
Re-order Chunksnum refs 5 10 20CIDEr 91.4 95.6 96.6BLEU 83.0 91.4 94.2METEOR 81.2 89.6 92.4BADGER 95.4 96.6 97.8TERp 91.0 93.4 93.4Table 6: Fluency-disrupting corruptions.
These % accuracies represent the number of times that a givenmetric was able to correctly score the original sentence higher than the corrupted sentence.
Numbers refer-enced in the prose analysis are highlighted in bold.Active-to-Passive table reflects how adding more(mostly active) references makes the system more(incorrectly) confident in choosing the active origi-nal.
As the graph in Figure 5 shows, METEOR per-formed the best, likely due to its sentence alignmentapproach to computing scores.7.3 Fluency disruptionsAll of the metrics perform well at identifying theduplicate prepositional phrase corruption, exceptfor BADGER which has noticeably lower accuracyscores than the rest.
These lower scores may be at-tributed to the compression algorithm that it uses tocompute similarity.
Because BADGER?s algorithmworks by compressing the candidate and referencesjointly, we can see why a repeated phrase would beof little effort to encode ?
it is a compression algo-rithm, after all.
The result of easy-to-compress re-dundancies is that the original sentence and its cor-ruption have very similar scores, and BADGER getsfooled.Unlike the other two fluency-disruptions, none of1941the accuracy scores of the ?Remove Head from PP?corruption reach 90%, so this corruption could beseen as one that metrics could use improvement on.BLEU performed the best on this task.
This is likelydue to its ngram-based approach, which is able toidentify that deleting a word breaks the fluency of asentence.All of the metrics perform well on the ?Re-orderChunks?
corruption.
METEOR, however, doesslightly worse than the other metrics.
We believethis to be due to its method of generating an align-ment between the words in the candidate and refer-ence sentences.
This alignment is computed whileminimizing the number of chunks of contiguousand identically ordered tokens in each sentence pair(Chen et al, 2015).
Both the original sentence andthe corruption contain the same chunks, so it makessense that METEOR would have more trouble dis-tinguishing between the two than the n-gram basedapproaches.7.4 W2V-AVGThe results for the W2V-AVG metric?s success oneach corruption are shown in Table 7.
?ShuffledChunks?
is one of the most interesting corruptionsfor this metric, because it achieves an accuracy of0% across the board.
The reason for this is thatW2V-AVG is a pure bag-of-words model, meaningthat word order is entirely ignored, and as a resultthe model cannot distinguish between the originalsentence and its corruption, and so it can never rankthe original greater than the corruption.Surprisingly, we find that W2V-AVG is far lessfooled by active-to-passive than most other metrics.Again, we believe that this can be attributed to itsbag-of-words approach, which ignores the word or-der imposed by active and passive voices.
Becauseeach version of the sentence will contain nearly allof the same tokens (with the exception of a few ?is?and ?being?
tokens), the two sentence representa-tions are very similar.
In a sense, W2V-AVG doeswell on passive sentences for the wrong reasons -rather than understanding that the semantics are un-changed, it simply observes that most of the wordsare the same.
However, we still see the trend thatperformance goes down as the number of referenceaverage word2vec metricnum references 5 10 201.
Negated Action 72.8 74.5 60.72.
Antonym Replacement 91.5 93.0 92.33.
Negated Subject 98.2 99.1 97.84.
Active-to-Passive* 84.9 83.6 80.35.
Synonymous Phrase* 98.3 99.1 98.36.
DT Substitution* 100.0 100.0 100.07.
Duplicate PP 87.6 87.6 87.68.
Remove Head From PP 78.4 82.5 82.59.
Shuffle Chunks 00.0 00.0 00.01.
Negated Action* 100.0 100.0 100.03.
Negated Subject* 82.8 87.3 87.1Table 7: Performance of the AVG-W2V metric.
These% accuracies represent the number of successful trials.Numbers referenced in the prose analysis are highlightedin bold.
* indicates the scores computed with the Differ-ence formula.sentences increases.Interestingly, we can see that although W2V-AVGachieved 98% accuracy on ?Negated Subject?, itscored only 75% on ?Negated Action?.
This ini-tially seems quite counter intuitive - either the modelshould be good at the insertion of a negation word,or it should be bad.
The explanation for this revealsa bias in the data itself: in every instance wherethe ?Negated Subject?
corruption was applied, thesentence was transformed from ?A/The [subject] is?to ?There is no [subject]?.
This is differs from thechange in ?Negated Action?, which is simply theinsertion of ?not?
into the sentence before an ac-tion.
Because one of these corruptions resulted in3x more word replacements, the model is able toidentify it fairly well.To confirm this, we added two final entries toTable 7 where we applied the Difference formulato the ?Negated Subject?
and ?Negated Action?corruptions to see the fraction of sentence pairswhose scores are within 15% of one another.
Wefound that, indeed, the ?Negated Action?
corruptionscored 100% (meaning that the corruption embed-dings were very similar to the original embeddings),while the ?Negated Subject?
corruption pairs wereonly similar about 85% of the time.
By analyz-ing these interpretable errors, we can see that stop1942words play a larger role than we?d want in our toymetric.
To develop a stronger metric, we mightchange W2V-AVG so that it considers only the con-tent words when computing the centroid embed-dings.8 ConclusionThe main contribution of this work is a novel ap-proach for analyzing evaluation metrics for lan-guage generation tasks using Metric Unit TesTs.Not only is this evaluation procedure able to high-light particular metric weaknesses, it also demon-strates results which are far more consistent thancorrelation with human judgment; a good metricwill be able to score well regardless of how noisy thehuman-derived scores are.
Finally, we demonstratethe process of how this analysis can guide the devel-opment and strengthening of newly created metricsthat are developed.ReferencesChris Callison-Burch, Philipp Koehn, ChristofMonz, Kay Peterson, Mark Przybocki, and OmarZaidan.
2010.
Findings of the 2010 joint work-shop on statistical machine translation and met-rics for machine translation.
In Proceedings ofthe Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 17?53, Up-psala, Sweden, July.
Association for Computa-tional Linguistics.X.
Chen, H. Fang, TY Lin, R. Vedantam, S. Gupta,P.
Dollr, and C. L. Zitnick.
2015.
Microsoft cococaptions: Data collection and evaluation server.arXiv preprint arXiv:1504.00325.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evalua-tion for any target language.
In Proceedings ofthe EACL 2014 Workshop on Statistical MachineTranslation.Chin-Yew Lin.
2004.
Rouge: a package for auto-matic evaluation of summaries.
pages 25?26.Matous Machacek and Ondrej Bojar.
2014.
Re-sults of the wmt14 metrics shared task.
In Pro-ceedings of the Ninth Workshop on StatisticalMachine Translation, pages 293?301, Baltimore,Maryland, USA, June.
Association for Computa-tional Linguistics.Nitin Madnani, Joel Tetreault, and MartinChodorow.
2012.
Re-examining machinetranslation metrics for paraphrase identification.In Proceedings of the 2012 Conference of theNorth American Chapter of the Association forComputational Linguistics: Human LanguageTechnologies, pages 182?190, Montr?eal, Canada,June.
Association for Computational Linguistics.M.
Marelli, S. Menini, M. Baroni, L. Ben-tivogli, R. Bernardi, R. Zamparelli, and Fon-dazione Bruno Kessler.
2014.
A sick cure forthe evaluation of compositional distributional se-mantic models.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregCorrado, and Jeffrey Dean.
2013.
Distributedrepresentations of words and phrases and theircompositionality.
In In Proceedings of NIPS.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of ma-chine translation.
Technical report, September.Steven Parker.
2008.
Badger: A new machinetranslation metric.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted humanannotation.
In In Proceedings of Association forMachine Translation in the Americas, pages 223?231.Matthew G. Snover, Nitin Madnani, Bonnie Dorr,and Richard Schwartz.
2009.
Ter-plus: Para-phrase, semantic, and alignment enhancements totranslation edit rate.Richard Socher, Eric H. Huang, Jeffrey Penning-ton, Andrew Y. Ng, and Christopher D. Manning.2011.
Dynamic Pooling and Unfolding Recur-sive Autoencoders for Paraphrase Detection.
InAdvances in Neural Information Processing Sys-tems 24.Ramakrishna Vedantam, C. Lawrence Zitnick, andDevi Parikh.
2014.
Cider: Consensus-based image description evaluation.
CoRR,abs/1411.5726.1943
