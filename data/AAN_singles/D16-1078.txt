Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815?825,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSpeculation and Negation Scope Detection via Convolutional NeuralNetworksZhong Qian1, Peifeng Li1, Qiaoming Zhu1, Guodong Zhou1, Zhunchen Luo2 and WeiLuo2School of Computer Science and Technology, Soochow University, Suzhou, 215006, China1China Defense Science and Technology Information Center, Beijing, 100142, China2qianzhongqz@163.com, {pfli, qmzhu, gdzhou}@suda.edu.cn,zhunchenluo@gmail.com, htqxjj@126.comAbstractSpeculation and negation are important infor-mation to identify text factuality.
In this paper,we propose a Convolutional Neural Network(CNN)-based model with probabilisticweighted average pooling to address specula-tion and negation scope detection.
In particular,our CNN-based model extracts those meaning-ful features from various syntactic paths be-tween the cues and the candidate tokens in bothconstituency and dependency parse trees.
Eval-uation on BioScope shows that our CNN-basedmodel significantly outperforms the state-of-the-art systems on Abstracts, a sub-corpus inBioScope, and achieves comparable perfor-mances on Clinical Records, another sub-corpus in BioScope.1 IntroductionFactual information is critical to understand a sen-tence or a document in most typical NLP applica-tions.
Speculation and negation extraction hasbeen drawing more and more attentions in recentyears due to its importance in distinguishing coun-terfactual or uncertain information from the facts.Generally speaking, speculation is a type of uncer-tain expression between certainty and negation,while negation is a grammatical category whichreverses the truth value of a proposition.Commonly, speculation and negation extractioninvolves two typical subtasks: cue identificationand scope detection.
Here, a cue is a word orphrase that has speculative or negative meaning(e.g., suspect, guess, deny, not), while a scope is atext fragment governed by the corresponding cuein a sentence.
Consider the following two sentenc-es for examples:(S1) The doctors warn that smoking [may harmour lungs].
(S2) He does [not like playing football] butlikes swimming.
1In sentence S1, the speculative cue ?may?
gov-erns the scope ?may harm our lungs?, while thenegative cue ?not?
governs the scope ?not likeplaying football?
in sentence S2.Previous work have achieved quite success oncue identification (e.g., with F1-score of 86.79 forspeculative cue detection in Tang et al (2010)).
Incomparison, speculation and negation scope detec-tion is still a challenge due to its inherent difficul-ties and those upstream errors.
In this paper, wefocus on scope detection.
Previous work on scopedetection can be classified into heuristic rulesbased methods (e.g., ?zg?r et al, 2009; ?vrelid etal., 2010), machine learning based methods (e.g.,Tang et al, 2010; Zou et al, 2013), and hybridapproaches which integrate empirical models withmanual rules (Velldal et al, 2012).Different from those previous studies, this paperpresents a Convolutional Neural Network (CNN)-based approach for scope detection.
CNN models,firstly invented to capture more abstract featuresfor computer vision (LeCun et al, 1989), haveachieved certain success on various NLP tasks in1 In this paper, cues are in bold face, and scopes are in[brackets] in the example sentences.815recent years, such as semantic role labeling (Col-lobert et al, 2011), machine translation (Meng etal., 2015; Hu et al, 2015), event extraction (Chenet al, 2015; Nguyen et al, 2015), etc.
These stud-ies have proved the ability of CNN models inlearning meaningful features.In particular, our CNN-based model extractsvarious kinds of meaningful features from the syn-tactic paths between the cue and the candidate to-ken in both constituency and dependency parsetrees.
The importance of syntactic information inscope detection has been justified in previouswork (Velldal et al, 2012; Lapponi et al, 2012;Zou et al, 2013, etc).
Our model can also benefitfrom the ability of neural networks in extractinguseful information from syntactic paths (Xu et al,2015a; Xu et al, 2015b) or more complex syntac-tic trees (Ma et al, 2015; Tai et al, 2015).
Moreo-ver, instead of traditional average pooling, ourCNN-based model utilizes probabilistic weightedaverage pooling to alleviate the overfitting prob-lem (Zeiler et al, 2013).
Experimental results onBioScope prove the effectiveness of our CNN-based model.The reminder of this paper is organized as fol-lows: Section 2 gives an overview of the relatedwork.
Section 3 describes our CNN-based modelwith probabilistic weighted average pooling forscope detection.
Section 4 illustrates the experi-mental settings, and reports the experimental re-sults and analysis.
Finally, Section 5 draws theconclusion.2 Related WorkIn this section, we give an overview of previouswork on both scope detection and utilization ofCNNs in NLP applications.2.1 Scope DetectionEarlier studies on speculation and negation scopedetection focused on developing various heuristicrules manually to detect scopes.Chapman et al (2001) developed various regu-lar expressions for negation scope detection.
Sub-sequently, various kinds of heuristic rules began toemerge.
?zg?r et al (2009) resorted to the part-of-speech of the speculative cues and the syntacticstructures of the current sentences for identifyingscopes, and developed heuristic rules according tothe syntactic trees.
?vrelid et al (2010) construct-ed a set of heuristic rules on dependency structuresand obtained the accuracy of 66.73% on theCoNLL evaluation data.
The approaches based onheuristic rules were effective because the sentencestructures in BioScope satisfy some grammaticalrules to a certain extent.With the release of the BioScope corpus (Szar-vas et al, 2008), machine learning based methodsbegan to dominate the research of speculation andnegation scope detection.Morante et al (2008) regarded negation scopedetection as a chunk classification task utilizinglexical and syntactic features.
Morante et al(2009a) further implemented a scope detectionsystem combining three classifiers, i.e., TiMBL,SVM and CRF, based on shallow syntactic fea-tures, and achieved the performance of 77.13%and 73.36% in Percentage of Correct Scopes (PCS)on speculation and negation scope detection onAbstracts, a sub-corpus of BioScope.
Velldal et al(2012) explored a hybrid method, adopting manu-ally crafted rules over dependency parse trees anda discriminative ranking function over nodes inconstituent parse trees.
Zou et al (2013) proposeda tree kernel based approach on the syntactic parsetrees to detect speculation and negation scopes.Alternative studies treated scope detection as asequential labeling task.
Tang et al (2010) pro-posed a CRF model with POS, chunks, NERs, de-pendency relations as features.
Similarly, Lapponiet al (2012) employed a CRF model with lexicaland dependency features for negation scope andevent resolution on the Conan Doyle corpus.
The-se machine learning methods manifest the effec-tiveness of syntactic features.2.2 CNN based NLP ApplicationsCurrently, CNNs have obtained certain success onvarious NLP tasks, e.g., part-of-speech tagging,chunking, named entity recognition (Collobert etal., 2011).
Specifically, CNNs have been proveneffective in extracting sentence-level features.
Forinstance, Zeng et al (2014) utilized a CNN-basedmodel to extract sentence-level features for rela-tion classification.
Zhang et al (2015) proposed ashallow CNN-based model for implicit discourserelation recognition.
Chen et al (2015) presented aCNN-based model with dynamic multi-pooling onevent extraction.More recently, researchers tend to learn featuresfrom complex syntactic trees.
Ma et al (2015) use816a CNN-based model for sentence embedding, uti-lizing dependency tree-based n-grams.
Xu et al(2015a) exploited a CNN-based model to learnfeatures from the shortest dependency path be-tween the subject and the object for semantic rela-tion classification.3 CNN-based Modeling with Probabilis-tic Weighted Average PoolingThis section describes our CNN-based model forspeculation and negation scope detection, which isrecast as a classification task to determine whethereach token in a sentence belongs to the scope ofthe corresponding cue or not.
Principally, ourCNN-based model first extracts path features fromsyntactic trees with a convolutional layer and con-catenates them with their relative positions intoone feature vector, which is then fed into a soft-max layer to compute the confidence scores of itslocation labels, described in subsection 3.1.3.1 Token LabelingWe employ following labeling scheme for eachcandidate token:?
A token is labeled as O if it is NOT an ele-ment of a speculation or negation scope;?
A token is labeled as B if it is inside a scopeand occurs before the cue, i.e., Ptoken?Pcue,where Ptoken and Pcue are the positions of thetoken and the cue in a sentence, respectively;?
A token is labeled as A if it is inside a scopeand occurs after the cue (inclusive), i.e., Ptoken?Pcue.Under this scheme, each token in a sentence isclassified into B, A or O.
For example, the labelsof all the tokens in sentence S3 are shown in sen-tence S4.
(S3) They think that [those bacteria may bekilled by white blood cells] , but other researchersdo not think so.
(S4) They/O think/O that/O [those/B bacteria/Bmay/A be/A killed/A by/A white/A blood/Acells/A] ,/O but/O other/O researchers/O do/Onot/O think/O so/O ./OThe advantage of our scheme is that it can de-scribe the location relationship among the tokens,cues and scopes more precisely than some previ-ous studies, which regarded scope detection as abinary classification task (?vrelid et al, 2010; Zouet al, 2013).
Compared to other schemes withmore than two labels (Morante et al, 2009a; Tanget al, 2010; Lapponi et al, 2012), our scheme canmuch alleviate the imbalance of labels, becausethe tokens occurring at the first or last positions ofthe scopes are much fewer than other tokens.3.2 Input RepresentationFigure 1 shows the framework of our model basedon neural network.
We concentrate on Path Fea-ture and Position Feature.
They are concatenatedinto one feature vector, which is finally fed intothe softmax layer to obtain the output vector.HiddenLayerWordembeddings ofa pathConvolutionalLayerW1?
+ b1Max                          Min                PavgPath Featurestanh(W2?
+b2)softmax(W3?
+b3)EmbeddingsPosition Features     &       Path FeaturesInputFeaturesPosition features Path featuresSoftmaxLayerconcatenateDropoutLayer?MmaskW0outputInputmay?MD?VP?VP?NP?PRPP?ournot?neg?does?prep?like?pcomp?playingNormalizationFigure 1: The framework of CNN for scope detection.
Figure 2: The architecture of CNN-based model to extractpath features.817Relative Position has been proven useful inprevious studies (Zeng et al, 2014; Chen et al,2015).
In this paper, relative position is defined asthe relative distance of the cue to the candidatetoken.
For instance, in sentence S1, the relativedistances of the cue ?may?
to the candidate tokens?warn?
and ?our?
are 3 and -2, respectively.
Thevalues of position features are mapped into a vec-tor P of dimension dp, with P initialized randomly.Instead of the word sequence (e.g., Zeng et al,2014; Zhang et al 2015; Chen et al, 2015), weargue that the Shortest Syntactic Path from thecue to the candidate token can offer effective fea-tures to determine whether a token belongs to thescope.
It is remarkable that the lowest commonancestor node of the cue and the token is the high-est tree node in the path.Figure 2 illustrates the architecture of our CNN-based model to extract path features.
Here, convo-lutional features are first extracted from the matrixof embeddings of the path, and then fed into thehidden layer to produce more complicated features.In this paper, the syntactic paths between thecues and the candidate tokens in constituency anddependency parse trees are both considered.
Fig-ure 3 presents the constituency parse tree of sen-tence S1 and the constituency path from the cue?may?
to the candidate token ?our?.
It shows thatthe tokens are at both the beginning and the end ofthe path with the arrows indicating the directions.Meanwhile, Figure 4 displays the dependencyparse tree of sentence S2 and the dependency pathfrom the cue ?not?
to the token ?playing?.As the input of our CNN-based model, both theconstituency path and the dependency path be-tween the cue and the token can be regarded as thespecial ?sentences?
S=(t1, t2,?, tn), whose ?words?can be tokens of sentences, syntactic categories,dependency relations, and arrows.Similar to other CNN-based models, we alsoconsider a fixed size window of tokens around thecurrent token to capture its local features in thepath.
Here, the window size is set as an odd num-ber w, indicating that there are (w-1)/2 tokens be-fore and after the candidate token, respectively.
Inthis case, path S is transferred into matrix0 nw d ?
?0X Raccording to embedding table0 0| |Td ??
R0W, where d0 is the dimension of the em-beddings and |T0| is the size of the table.3.3 Convolutional Neural NetworkingAfter fed into the convolutional layer, the matrixof the syntactic path X0 is processed with a linearoperation:?
?1 1 0 1Y W X b(1)where01n w d?
?1W Ris the parameter matrix, and1n?1b Ris the bias term.
To extract the most activeconvolutional features from1n n?
?1Y R, we consid-er two features Cmax and Cmin whose elementsare maximum, minimum values of rows in Y1, re-spectively:ROOTSNP VP PERIODDT NNThe doctorsVBZ SBARwarn IN Sthat NPNNsmoking.VPMDmayVPVB NPharm PRPP NNSour lungsCue: mayCurrent candidate token: ourConstituency path:may?MD?VP?VP?NP?PRPP?ourFigure 3:  An example for the constituency parse treeof sentence S1 and the path from the cue to the candi-date token.negROOTdoesrootHensubjnot likeprepplayingpcompfootballdobjbutccswimmingconjlikesamodCue: notCurrent candidate token: playingDependency path:not?neg?does?prep?like?pcomp?playingFigure 4: An example for the dependency parse tree ofsentence S2 and the path from the cue to the candidatetoken.818( ) [ ( , 0 ), ( ,1), ..., ( , 1)]r m a x r r r n?
?1 1 1C m a x Y Y Y(2)( ) [ ( , 0 ), ( ,1), ..., ( , 1)]r m in r r r n?
?1 1 1C m in Y Y Y(3)where10 1r n?
?
?.
Moreover, we extract a con-volutional feature Cpavg, whose elements areprobabilistic weighted average values of rows inY1.
Formally, Cpavg can be written as:10( ) ( , )niir p r i???
??
1C p a vg Y(4)In Equation (4), pi is the probability of the ele-ment ( , )r i1Y  in the vector ( , )r ?1Y :10( , )( , )i njr ipr j???
?11YY(5)Cpavg is a variant probabilistic weighted aver-age pooling used by Zeiler et al (2013).
Comparedto the standard average pooling, each element inCpavg has a weight depending on its value.
That is,during computing Cpavg, the most active elementswith the largest absolute values (i.e., the maximumand minimum values) play the leading roles, whilethose less active elements with smaller absolutevalues have less effect.
In this way, we can reducethe influence of less active elements, and can cap-ture more active information in ( , )r ?1Y .
From thisrespect, Cpavg can be regarded as a meaningfulconvolutional feature.The extracted convolutional features above arefirst concatenated into 13 n?
RC , as the output ofthe convolutional layer:[ ],?C C m ax C m in ,C pavg               (6)Then, C is fed into the hidden layer to learnmore complex and meaningful features.
Here, weprocess C with a linear operation just like in theconvolutional layer, and choose hyperbolic tanh asthe activation function to get22 n?
RY:ta n h ( )?
?2 2 2Y W C b(7)where2 13n n??
R2Wis the parameter matrix, and2n?
R2bis the bias term.
To produce the output ofthe hidden layer, a normalization operation is ap-plied to eliminate the manifold differences amongvarious features:/?
2 2H Y Y(8)In this way we obtain the path feature 2n?H Rfor each candidate token and then concatenate itwith the position feature P into one vector F0:[ , ]T T T?0F P H(9)wherefn?
R0Fis the feature vector of a candi-date token with the dimension equaling the sum ofn2 and the dimension of P. Besides, we also con-sider the dropout operation for regularization toprevent the co-adaptation of hidden units on thepenultimate layer:?1 0F F M(10)where  is an element-wise multiplication and Mis a mask vector whose elements follow the Ber-noulli distribution with the probability p of being 1.We determine whether the candidate token is inthe scope of the current cue according to its F1.3.4 OutputFinally, F1 is fed into the softmax layer:( )= so ftm a x ?3 1 3O W F b(11)where3 fn n??
R3Wis the parameter matrix, and3n?3b Ris the bias term.
The dimension of O isn3=3, which is equal to the number of labels repre-senting whether the token is an element of thescope, just as described in subsection 3.1, and theelements of O can be interpreted as the confidencescores of the three labels, i.e., B, A and O.To learn the parameters of the network, we su-pervise the predicted labels of O with the gold la-bels in the training set, and utilize the followingtraining objection function:211( ) lo g ( | , ) 2mi iiJ p y xm??
?
???
?
??
(12)where ( | , )i ip y x ?
is the confidence score of thegolden label yi (B, A, O) of the training instance xi,m is the number of the training instances, ?
is theregularization coefficient and ?={W0, W1, b1, W2,b2, W3, b3} is the set of parameters.
To train theCNN-based model, the Stochastic Gradient De-scent algorithm is applied to fine-tune ?.8194 ExperimentationIn this section, we first introduce the evaluationdata, and then describe the experimental settings.Finally, we report the experimental results andanalysis.4.1 CorpusWe evaluate our CNN-based model on BioScope(Szarvas et al, 2008; Vincze et al, 2008), a widelyused and freely available resource consisting ofsentences annotated with speculative and negativecues and their scopes in biomedical domain.Abs Papers CliTotal#Documents 1273 9 1954#Sentences 11871 2670 6383Ave.
Len Sentences 25.47 24.54 7.71Spe#Sentences 2101 519 855#Scopes 2659 672 1112Ave.
Len Sentences 29.77 30.76 11.96Ave.
Len Scopes 15.10 13.38 4.92Neg#Sentences 1597 339 865#Scopes 1719 376 870Ave.
Len Sentences 29.28 30.55 8.53Ave.
Len Scopes 7.60 7.35 3.87(Notes: ?Ave.
Len?
denotes average length; ?Abs?, ?Papers?and ?Cli?
denote Abstracts, Full Papers and Clinical Records,respectively; ?Spe?
and ?Neg?
denote speculation and nega-tion, respectively.
)Table 1: Statistics on the BioScope corpus.BioScope includes 3 different sub-corpora: Ab-stracts of biological papers from the GENIA cor-pus (Collier et al, 1999), Full scientific Papersfrom Flybase and BMC Bioinformatics website,and Clinical radiology Records corpus.
These textsin three sub-corpora ensure that BioScope can cap-ture the heterogeneity of language use in biomedi-cal domain.
While Abstracts and Full Papers sharethe same genre, Clinical Records consists of short-er sentences.
Previous studies regarded Abstractsas the main resource for text mining applicationsdue to its public accessibility (e.g.
through Pub-Med).Table 1 shows the statistics of the BioScopecorpus.
While in both Abstracts and Full Papers,the average lengths of speculation and negationsentences are comparable (Abstracts: 29.77 vs29.28; Full Papers: 30.76 vs 30.55).
However,their average lengths of the negation scopes areshorter than those of speculation ones (Abstracts:7.60 vs 15.10; Full Papers: 7.35 vs 13.38).
Moreo-ver, both the average lengths of sentences andscopes in Clinical Records are shorter than thoseof other two sub-corpora (Average length: 11.96(speculation sentence), 8.53 (negation sentence),4.92 (speculation scope) and 3.87 (negationscope)).4.2 Experimental SettingsFollowing the previous work (e.g., ?zg?r et al,2009; Morante et al, 2009a, 2009b; Zou et al,2013), we divide the Abstracts sub-corpus into 10folds to perform 10-fold cross-validation.
Moreo-ver, to examine the robustness of our CNN-basedmodel towards different text types within biomed-ical domain, all the models are trained on the sameAbstracts sub-corpus.
Therefore, the results onAbstracts can be regarded as in-domain evaluationwhile the results on Clinical Records and Full Pa-pers can be regarded as cross-domain evaluation.For the measurement, traditional Precision, Re-call, and F1-score are used to report the token-based performance in scope detection, while thePercentage of Correct Scopes (PCS) is adopted toreport the scope-based performance, which con-siders a scope correct if all the tokens in the sen-tence have been assigned the correct scope classesfor a specific cue.
Obviously, PCS can better de-scribe the overall performance in scope detection.Besides, Percentage of Correct Left Boundaries(PCLB) and Percentage of Correct Right Bounda-ries (PCRB) are reported as partial measurements.In all our experiments, both the constituencyand dependency parse trees are produced by Stan-ford Parser2.
Specially, we train the parser on theGENIA Treebank 1.03 (Tateisi et al, 2005), whichcontains Penn Treebank-style syntactic (phrasestructure) annotation for the GENIA corpus.
Theparser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation onGENIA TreeBank 1.0.For the hyper-parameters in our CNN-basedmodel, we set d0=100, dp=10, w=3, n1=200,n2=500, ?=10-4, p=0.8.
The embeddings of the to-kens in ordinary sentences (as word sequences) areinitialized by Word2Vec4 (Mikolov et al, 2013).For the baseline, we utilize the classifier-basedbaseline developed by Zou et al (2013).
Besidesthose typical features, constituency and dependen-2 http://nlp.stanford.edu/software/lex-parser.shtml3 http://www.geniaproject.org/genia-corpus/treebank4 https://code.google.com/archive/p/word2vec/820cy syntactic features are also included.
Further-more, Mallet5 is selected as the classifier.In addition, since our CNN-based model mayresult in discontinuous blocks, we utilize a post-processing algorithm (Morante et al, 2008) to en-sure the continuity of scopes.
Meanwhile, the cuemust be in its scope as defined in Bioscope.4.3 Experimental Results on AbstractsTable 2 summarizes the performances of scopedetection on Abstracts.
In Table 2, CNN_C andCNN_D refer the CNN-based model with constit-uency paths and dependency paths, respectively(the same below).
It shows that our CNN-basedmodels (both CNN_C and CNN_D) can achievebetter performances than the baseline in mostmeasurements.
This indicates that our CNN-basedmodels can better extract and model effective fea-tures.
Besides, compared to the baseline, ourCNN-based models consider fewer features andneed less human intervention.
It also manifeststhat our CNN-based models improve significantlymore on negation scope detection than on specula-tion scope detection.
Much of this is due to thebetter ability of our CNN-based models in identi-fying the right boundaries of scopes than the leftones on negation scope detection, with the hugegains of 29.44% and 25.25% on PCRB usingCNN_C and CNN_D, respectively.Table 2 illustrates that the performance of spec-ulation scope detection is higher than that of nega-tion (Best PCS: 85.75% vs 77.14%).
It is mainlyattributed to the shorter scopes of negation cues.Under the circumstances that the average length ofnegation sentences is almost as long as that ofspeculation ones (29.28 vs 29.77), shorter negationscopes mean that more tokens do not belong to thescopes, indicating more negative instances.
Theimbalance between positive and negative instanceshas negative effects on both the baseline and the5 http://mallet.cs.umass.edu/CNN-based models for negation scope detection.Table 2 also shows that our CNN_D outper-forms CNN_C in negation scope detection (PCS:77.14% vs 70.86%), while our CNN_C performsbetter than CNN_D in speculation scope detection(PCS: 85.75% vs 74.43%).
To explore the resultsof our CNN-based models in details, we presentthe analysis of top 10 speculative and negativecues below on CNN_C and CNN_D, respectively.Figure 5: PCSs of top 10 speculative cues for scope detectionin Abstracts sub-corpus.Figure 5 illustrates the PCSs of the most fre-quent 10 speculative cues using CNN_C.
The cuesin the horizontal axis are in the order of lowest tohighest in frequency.
Among those cues, ?sug-gest?, ?may?, ?indicate?, and ?appear?
arecommonly used to express opinions of certain in-dividuals.
The scopes of these cues are integratedsemantic fragments (probably clauses) governedby corresponding cues in grammatical sense, andthe tokens in the scope tend to share the samechunk with the cue in the constituency parse tree.Hence, constituency paths are more useful forspeculation scope detection.
Figure 5 also showsthat the PCSs of all the top 10 speculative cues arehigher than 70% except ?or?
(PCS: 60.44%),mainly due to the flexible usage of ?or?, whichSystems P(%) R(%) F1 PCLB(%) PCRB(%) PCS(%)SpeculationBaseline 94.71 90.54 92.56 84.81 85.11 72.47CNN_C 95.95 95.19 95.56 93.16 91.50 85.75CNN_D 92.25 94.98 93.55 86.39 84.50 74.43NegationBaseline 85.46 72.95 78.63 84.00 58.29 46.42CNN_C 85.10 92.74 89.64 81.04 87.73 70.86CNN_D 89.49 90.54 89.91 91.91 83.54 77.14Table 2: The performances on the Abstracts sub-corpus.821can connect two words, two professional terms, oreven two clauses.Figure 6: PCSs of top 10 negative cues for scope detection inAbstracts sub-corpus.Figure 6 illustrates the performances of the mostfrequent 10 negative cues using CNN_D.
In thosenegative cues, ?not?
is in the absolute majority,and ?not?
and ?no?
cover over 70%.
We have no-ticed that most negative cues (e.g., ?not?, ?no?,?without?, ?fail?)
are often applied to negatephrases, and the tokens in negation scope tend tohave the tight dependency relationship with them.Therefore, our model can achieve better resultsusing dependency paths for negation scope.In Figure 6, most negative cues have good PCSs(higher than 70%).
However, ?unable?
has poorPCS of 16.67%.
This is due to the fact that ?una-ble?
usually occurs in the phrase structure ?be un-able to?, which often follows a subject.
It is nota-ble that a cue is always in its scope and most cuesin BioScope are much closer to the left boundariesthan to the right ones.
Hence, the tokens labeled asB (i.e., inside the scope and before the cue) aremuch fewer than the ones labeled as A or O. Suchimbalance makes it hard to judge whether the to-kens before ?unable?
are in of its scope or not.4.4 Experimental Results on Clinical Recordsand Full PapersThe performances of our CNN-based models onthe other two sub-corpora, i.e., Clinical Recordsand Full Papers, are presented in Table 3.
Alt-hough Abstracts and Clinical Records have differ-ent genres, our CNN-based models can obtain sat-isfactory results on Clinical Records using bothconstituency paths and dependency paths, provingthe portability of our models.Table 3 also shows that the results of negationscope are better than those of speculation scope onClinical Records (PCS: 89.66% vs 73.92%).
Weargue the reason is that both the lengths of nega-tion sentences and scopes (8.53 and 3.87, respec-tively) in Clinical Records are much shorter, indi-cating that the structures of negation sentences aresimpler than those of speculation ones.
After erroranalysis of speculation scopes, we find that54.83% of our error scopes contain the annotatedscopes, just like sentence S5:(S5) This does not [appear to represent a stone]and is not mobile.The annotated scope of the cue ?appear?
is?appear to represent a stone?.
However, ourCNN-based model identifies the whole sentence asthe scope.
These errors indicate that some wordsmay be wrongly identified as the components ofscopes because the scopes in Clinical Records areshort and their structures are simple.Compared with Abstracts and Clinical Records,the results on Full Papers are much lower.
This ismainly due to the poor PCRBs, indicating a con-siderable quantity of right boundaries of scopescannot be identified correctly.
We should note thatthe average lengths of both speculation and nega-tion sentences (30.76 and 30.55, respectively) inFull Papers are longer than those in Abstracts andSystems P(%) R(%) F1 PCLB(%) PCRB(%) PCS(%)SpeculationClinicalRecordsCNN_C 86.85 93.84 90.21 84.35 86.87 73.92CNN_D 89.02 85.41 87.18 82.91 76.17 64.39FullPapersCNN_C 86.78 86.59 86.69 86.01 68.60 59.82CNN_D 86.13 85.09 85.61 80.95 64.14 52.98NegationClinicalRecordsCNN_C 88.29 97.00 92.44 95.98 93.45 89.66CNN_D 91.97 97.03 94.43 95.98 90.57 87.82FullPapersCNN_C 80.92 82.26 81.58 82.71 67.29 55.32CNN_D 82.08 84.90 83.46 84.04 64.89 53.99Table 3: The performances of our CNN-based models on Clinical Records and Full Papers.822Clinical Records.
Normally, longer sentencesmean more complicated syntactic structures.Besides the results trained on Abstracts, we alsoconsider the 10-fold cross-validation on ClinicalRecords and Full Papers.
The PCSs of speculationand negation scope detection are 74.73% (CNN_C)and 91.03% (CNN_C) on Clinical Records, whichare both higher than the ones trained on Abstracts.Remember that Abstracts and Clinical Recordscome from the different genres.
However, we getlower PCSs on Full Papers (49.54% for specula-tion scope detection using CNN_C, and 44.67%for negation scope detection using CNN_C).
Inaddition to the complex structures of long sentenc-es, another reason is that the smaller size of theFull Papers sub-corpus compared to the other twosub-corpora.
Fewer sentences and scopes (only672 speculation scopes in 519 sentences and 376negation scopes in 339 sentences) mean that wecannot get an excellent model.4.5 Comparison with the State-of-the-ArtTable 4 compares our CNN-based models with thestate-of-the-art systems.
It shows that our CNN-based models can achieve higher PCSs (+1.54%)than those of the state-of-the-art systems for spec-ulation scope detection and the second highestPCS for negation scope detection on Abstracts,and can get comparable PCSs on Clinical Records(73.92% vs 78.69% for speculation scopes,89.66% vs 90.74% for negation scopes).
It isworth noting that Abstracts and Clinical Recordscome from different genres.It also displays that our CNN-based models per-form worse than the state-of-the-art on Full Papersdue to the complex syntactic structures of the sen-tences and the cross-domain nature of our evalua-tion.
Although our evaluation on Clinical Recordsis cross-domain, the sentences in Clinical Recordsare much simpler and the results on Clinical Rec-ords are satisfactory.
Remind that our CNN-basedmodels are all trained on Abstracts.
Another rea-son is that those state-of-the-art systems on FullPapers (e.g., Li et al, 2010; Velldal et al, 2012)are tree-based, instead of token-based.
Li et al(2010) proposed a semantic parsing frameworkand focused on determining whether a constituent,rather than a word, is in the scope of a negativecue.
Velldal et al (2012) presented a hybridframework, combining a rule-based approach us-ing dependency structures and a data-driven ap-proach for selecting appropriate subtrees in con-stituent structures.
Normally, tree-based modelscan better capture long-distance syntactic depend-ency than token-based ones.
Compared to thosetree-based models, however, our CNN-basedmodel needs less manual intervention.
To improvethe performances of scope detection task, we willexplore this alternative in our future work.System Abs Cli PapersSpeMorante (2009a) 77.13 60.59 47.94?zg?r (2009) 79.89 N/A 61.13Velldal (2012) 79.56 78.69 75.15Zou (2013) 84.21 72.92 67.24Ours 85.75 73.92 59.82NegMorante (2008) 57.33 N/A N/AMorante (2009b) 73.36 87.27 50.26Li (2010) 81.84 89.79 64.02Velldal (2012) 74.35 90.74 70.21Zou (2013) 76.90 85.31 61.19Ours 77.14 89.66 55.32Table 4: Comparison of our CNN-based model with the state-of-the-art in PCS.5 ConclusionThis paper proposes a CNN-based model for spec-ulation and negation scope detection.
Comparedwith various lexical and syntactic features adoptedin previous studies (e.g., Lapponi et al, 2012; Zouet al, 2013), our CNN-based model only considersthe position feature and syntactic path feature.Experimental results on the BioScope corpusshow that our CNN-based model can get the bestperformances for speculation scopes and the se-cond highest performances for negation scopes onAbstracts in in-domain evaluation.
In cross-domain evaluations, we can achieve comparableresults on Clinical Records, but our CNN-basedmodel performs worse on Full Papers.
This sug-gests our future direction to extend the model fromtoken level to parse tree level in better capturinglong-distance syntactic dependency and to addressthe cross-domain adaptation issue.AcknowledgmentsThis research was supported by the National Natu-ral Science Foundation of China (Grant Nos.61472265, 61402314 and 61331011), and partiallysupported by Collaborative Innovation Center ofNovel Software Technology and Industrialization.In addition, thanks to the three anonymous re-viewers for their valuable comments.823ReferencesWendy W. Chapman, Will Bridewell, Paul Hanbury,Gregory F. Cooper, and Bruce G. Buchanan.
2001.Evaluation of Negation Phrases in Narrative ClinicalReports.
In Proceedings of American Medical Infor-matics Association Symposium, 2001, pages 105-109.Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, andJun Zhao.
2015.
Event Extraction via Dynamic Mul-ti-Pooling Convolutional Neural Networks.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics and the 7th Inter-national Joint Conference on Natural LanguageProcessing (ACL2015), Beijing, China, July 26-31,2015, pages 167-176.Nigel Collier, Hyun Seok Park, Norihiro Ogata, et al1999.
The GENIA Project: Corpus-basedKnowledge Acquisition and Information Extractionfrom Genome Research Papers.
In Proceedings ofthe European Chapter of the ACL 1999 (EACL1999).Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural Language Processing (almost) from Scratch.Journal of Machine Learning Research, 2011, 12(1):2493-2537.Baotian Hu, Zhaopeng Tu, Zhengdong Lu, Hang Li,and Qingcai Chen.
2015.
Context-Dependent Trans-lation Selection Using Convolutional Neural Net-work.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics andthe 7th International Joint Conference on NaturalLanguage Processing (ACL2015), Beijing, China,July 26-31, 2015, pages 536-541.Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, andJonathon Read.
2012.
UiO2: Sequence-Labeling Ne-gation Using Dependency Features.
In First JointConference on Lexical and Computational Semantics(*SEM), Montreal, Canada, June 7-8, 2012, pages319?327.Yann LeCun, Bernhard E. Boser, John S. Denker, Don-nie Henderson, R. E. Howard, Wayne E. Hubbard,and Lawrence D. Jackel.
1989.
BackpropagationApplied to Handwritten Zip Code Recognition.
Neu-ral Computation, 1989, 1(4): 541-551.Junhui Li, Guodong Zhou, Hongling Wang, andQiaoming Zhu.
2010.
Learning the Scope ofNegation via Shallow Semantic Parsing.
InProceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), Beijing,2010, pages 671-679.Mingbo Ma, Liang Huang, Bing Xiang, and BowenZhou.
2015.
Dependency-based Convolutional Neu-ral Networks for Sentence Embedding.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Short Papers) (ACL2015), Beijing, China,2015, pages 174-179.Fandong Meng, Zhengdong Lu, Mingxuan Wang, HangLi, Wenbin Jiang, and Qun Liu.
2015.
EncodingSource Language with Convolutional Neural Net-work for Machine Translation.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing(ACL2015), Beijing, China, July 26-31, 2015, pages20-30.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed Represen-tations of Words and Phrases and their Composition-ality.
Advances in Neural Information ProcessingSystems, 2013(26): 3111-3119.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008.
Learning the Scope of Negation in Bi-omedical Texts.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP 2008), Honolulu, 2008, pages715-724.Roser Morante and Walter Daelemans.
2009a.
Learningthe Scope of Hedge Cues in Biomedical Texts.
InProceedings of the Workshop on BioNLP.
Boulder,Colorado, 2009, pages 28-36.Roser Morante and Walter Daelemans.
2009b.
A Met-alearning Approach to Processing the Scope of Ne-gation.
In Proceedings of the Thirteenth Conferenceon Computational Natural Language Learning(CoNLL 2009), Boulder, Colorado, June 2009, pages21-29.Thien Huu Nguyen and Ralph Grishman.
2015.
EventDetection and Domain Adaptation with Convolu-tional Neural Networks.
In Proceedings of the 53rdAnnual Meeting of the Association for Computation-al Linguistics and the 7th International Joint Con-ference on Natural Language Processing (ACL2015),Beijing, China, July 26-31,2015, pages 365-371.Arzucan ?zg?r and Dragomir R. Radev.
2009.
Detect-ing Speculations and their Scopes in Scientific Text.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2009), Singapore, 2009, pages 1398-1407.Lilja ?vrelid, Erik Velldal, and Stephan Oepen.
2010.Syntactic Scope Resolution in Uncertainty Analysis.824In Proceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), Beijing,August 2010, pages 1379-1387.Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, andJ?nos Csirik.
2008.
The BioScope corpus: Annota-tion for Negation, Uncertainty and their Scope in Bi-omedical Texts.
In Proceedings of BioNLP 2008:Current Trends in Biomedical Natural LanguageProcessing, Columbus, Ohio, USA, 2008, pages 38-45.Kai Sheng Tai, Richard Socher and Christopher D.Manning.
2015.
Improved Semantic Representationsfrom Tree-Structured Long Short-Term MemoryNetworks.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (ACL-IJCNLP 2015), Bei-jing, China, July 26-31, 2015, pages 1556-1566.Buzhou Tang, Xiaolong Wang, XuanWang, Bo Yuan,and Shixi Fan.
2010.
A Cascade Method for Detect-ing Hedges and their Scope in Natural LanguageText.
In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning(CoNLL 2010): Shared Task, Uppsala, Sweden, 15-16 July 2010, pages 13-17.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun'ichi Tsujii.
2005.
Syntax annotation for theGENIA corpus.
In Proceedings of IJCNLP 2005, Je-ju Island, Korea, October 2005, pages 222-227.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and Negation: Rules,Rankers, and the Role of Syntax.
ComputationalLinguistics, 2012, 38(2): 369-410.Andreas Vlachos and Mark Craven.
2010.
DetectingSpeculative Language Using Syntactic Dependenciesand Logistic Regression.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2010): Shared Task, Upp-sala, Sweden, 15-16 July 2010, pages 18-25.Kun Xu, Yansong Feng, Songfang Huang, and Dong-yan Zhao.
2015a.
Semantic Relation Classificationvia Convolutional Neural Networks with SimpleNegative Sampling.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2015), Lisbon, Portugal, 2015,pages 536-540.Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,Zhi Jin.
2015b.
Classifying Relations via Long ShortTerm Memory Networks along Shortest DependencyPaths.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2015), Lisbon, Portugal, 2015, pages 1785-1794.Matthew D. Zeiler and Rob Fergus.
2013.
StochasticPooling for Regularization of Deep ConvolutionalNeural Networks.
arXiv preprint arXiv: 1301.3557v1.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation Classification viaConvolutional Deep Neural Network.
In Proceed-ings of the 25th International Conference on Compu-tational Linguistics: Technical Papers (COLING2014), Dublin, Ireland, August 23-29, 2014, pages2335-2344.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, HongDuan, and Junfeng Yao.
2015.
Shallow Convolu-tional Neural Network for Implicit Discourse Rela-tion Recognition.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2015), Lisbon, Portugal, 17-21September 2015, pages 2230-2235.Bowei Zou, Guodong Zhou, and Qiaoming Zhu.
2013.Tree Kernel-based Negation and Speculation ScopeDetection with Structured Syntactic Parse Features.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2013), Seattle, Washington, USA, 2013, pages 968-976.825
