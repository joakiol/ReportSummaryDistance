The Candide System for Machine TranslationAdam L. Berger, Peter F. Brown,* Stephen A. Della Pietra, Vincent J. Della Pietra,John R. GiUett, John D. Lafferty, Robert L. Mercer,* Harry Printz, Luboi UreiIBM Thomas J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598ABSTRACTWe present an overview of Candide, a system for automatictranslation of French text to English text.
Candide usesmethods of information theory and statistics to develop aprobability model of the translation process.
This model,which is made to accord as closely as possible with a largebody of French and English sentence pairs, is then used togenerate English translations of previously unseen Frenchsentences.
This paper provides a tutorial in these methods,discussions of the training and operation of the system, anda summary of test results.1.
Int roduct ionCandide is an experimental computer program, now in itsfifth year of development at IBM, for translation of Frenchtext to Enghsh text.
Our goal is to perform fuRy-automatic,high-quality text-to-text translation.
However, because weare still far from achieving this goal, the program can be usedin both fully-automatic and translator's-assistant modes.Our approach is founded upon the statistical analysis of lan-guage.
Our chief tools axe the source-channel model of com-munication, parametric probability models of language andtranslation, and an assortment of numerical algorithms fortraining such models from examples.
This paper presents el-ementary expositions of each of these ideas, and explains howthey have been assembled to produce Caadide.In Section 2 we introduce the necessary ideas from informa-tion theory and statistics.
The reader is assumed to know el-ementary probability theory at the level of \[1\].
In Sections 3and 4 we discuss our language and translation models.
InSection 5 we describe the operation of Candide as it trans-lates a French document.
In Section 6 we present results ofour internal evaluations and the AB.PA Machine TranslationProject evaluations.
Section 7 is a summary and conclusion.2.
Statistical Translat ionConsider the problem of translating French text to Englishtext.
Given a French sentence f, we imagine that it wasoriginally rendered as an equivalent Enghsh sentence .
Toobtain the French, the Enghsh was transmitted over a noisycommunication channel, which has the curious property thatEnglish sentences ent into it emerge as their French trans-lations.
The central assumption of Candide's design is thatthe characteristics of this channel can be determined experi-mentally, and expressed mathematically.
*Current address: Renaissance Technologies, Stony Brook, NY~ English-to-French I fe Channel "_\[ French-to-English-\] Decoder 6Figure 1: The Source-Channel Formalism of Translation.Here f is the French text to be translated, e is the putativeoriginal English rendering, and 6 is the English translation.This formalism can be exploited to yield French-to-Englishtranslations as follows.
Let us write Pr(e I f) for the probabil-ity that e was the original English rendering of the French f.Given a French sentence f, the problem of automatic transla-tion reduces to finding the English sentence that maximizesP.r(e I f).
That is, we seek 6 = argmsx e Pr(e I f).By virtue of Bayes' Theorem, we have= argmax Pr(e I f)  = argmax Pr(f I e)Pr(e) (1)e eThe term Pr ( f le  ) models the probability that f emergesfrom the channel when e is its input.
We call this functionthe translation model; its domain is all pairs (f, e) of Frenchand English word-strings.
The term Pr(e) models the a prioriprobability that e was suppled as the channel input.
We callthis function the language model.
Each of these factors--thetranslation model and the language model-- independentlyproduces a score for a candidate English translation e. Thetranslation model ensures that the words of e express theideas of f, and the language model ensures that e is a gram-matical sentence.
Candide sehcts as its translation the e thatmaximizes their product.This discussion begs two important questions.
First, wheredo the models P r ( f \ [  e) and Pr(e) come from?
Second, evenif we can get our hands on them, how can we search the set ofall English strings to find 6?
These questions are addressedin the next two sections.2.1.
Probabi l i ty ModelsWe begin with a brief detour into probability theory.
A prob-ability model is a mathematical formula that purports to ex-press the chance of some observation.
A parametric model isa probability model with adjustable parameters, which canbe changed to make the model better match some body ofdata.Let us write c for a body of data to be modeled, and 0 for avector of parameters.
The quantity Prs(c),  computed accord-ing to some formula involving c and 0, is called the hkelihood157of c. It is the model's assignment ofprobability to the obser-vation sequence c, according to the current parameter values0.
Typically the formula for the hkehhood includes ome con-attaints on the dements of 0 to ensure that Pr0(c) reaUy is aprobability distribution--that is, it is always a real vahe in\[0, 1\], and for fixed 0 the sum ~c Pr0(c) over all possible cvectors is 1.Consider the problem of training this parametric model to thedata c; that is, adjusting the 0 to maximize Pr0(c).
Findingthe maximizing 0 is an exercise in constrained optimization.If the expression for Pr0(c) is of a suitable (simple) form, themaximizing parameter vector 0 can be solved for directly.The key elements of this problem are?
a vector 0 of adjustable parameters,?
constraints on these parameters to ensure that we havea model,?
a vector c of observations, and?
the adjustment of0, subject o constraints, to maximizethe likelihood Pr0(c).We often seek more than a probability model of some ob-served data c. There may be some hidden statistics h, whichare related to c, but which are never directly revealed; in gen-eral h itself is restricted to some set 7f of admissible values.For instance, c may be a large corpus of grammatical text,and h an assignment of parts-of-speech to each of its words.model Pr(e).
Consider the translation model.
As any first-year language student knows, word-for-word translation ofEnglish to French does not work.
The dictionary equivalentsof the Enghsh words can move forward or backward in thesentence, they may disappear completely, and new Frenchwords may appear to arise spontaneously.Guided by this observation, our approach as been to writedown an enormous parametric expression, Pr0(f I e), for thetranslation model.
To give the reader some idea of the scale ofthe computation, there is a parameter, ~(/\[e), for the prob-ability that any given English word e will translate as anygiven French word f. There are parameters for the prob-ability that any f may arise spontaneously, and that any emay simply disappear.
There are parameters that words maymove forward or backward 1, 2, 3, .
.
.
positions.
And so on.We use a similar approach to write an expression for Pr0(e).In this case the parameters express things like the probabil-ity that a word e/may appear in a sentence after some wordsequence ta2.. ,  e~-t.
In general, the parameters are of theform Pr(e/Iv), where the vector v is a combination of observ-able statistics like the identities of nearby words, and hiddenstatistics like the grammatical structure of the sentence.
Werefer to v as a historyd, from which we predict e?.The parameter values of both models are determined by EMtraining.
For the translation model, the training data con-sists of English-French sentence pairs (e, f), where e and fare translations of one another.
For the language model, itconsists exclusively of Enghsh text.In such cases, we proceed as follows.
First we write down aparametric model Pr0(c, h).
Then we attempt o adjust theparameter vector 0 to maximize the likelihood Pr0(c), wherethis latter is obtained as the sum ~he~ Pr0(c, h).Unfortunately, when we attempt o solve this more compli-cated problem, we often discover that we cannot find a closed-form solution for 0.
Instead we obtain formulae that expresseach of the desired parameters in terms of all the others, andalso in terms of the observation vector c.Nevertheless, we can frequently apply an iterative techniquecalled the Ezpectation-Mazimization or EM Algorithm; thisis a recipe for computing a sequence 0z, 02, .. ?
of parametervectors.
It can be shown \[2\] that under suitable conditions,each iteration of the algorithm is guaranteed to produce abetter model of the training vector c; that is,Pr0,+l(c) > Pr0,(c), (2)with strict inequality everywhere except at stationary pointsof Pr0(c).
When we adjust the model's parameters this way,we say it has been EM-trained.Training a model with hidden statistics is just like trainingone that lacks them, except that it is not possible to finda maximizing t~ in just one go.
Training is now an itera-tire process, involving repeated passes over the observationvector.
Each pass yields an improved model of that data.Now we relate these methods to the problem at hand, whichis to develop a translation model Pr(f  \] e), and a language2.2.
DecodingWe do not actually search the infinite set of all English wordstrings to find the 6 that maximizes equation (1).
Even if werestricted ourselves to word strings of length h or less, for anyrealistic length and English vocabulary C, this is far too largea set to search exhaustively.
Instead we adapt the well-knownstack decoding algorithm \[5\] of speech recognition.
Thoughwe will say more about decoding in Section 6 below, mostof our research effort has been devoted to the two modelingproblems.This is not without reason.
The translation scheme we havejust described can fail in only two ways.
The first way is asearch error, which means that our decoding procedure didnot yield the fi that maximizes Pr(f  I e)Pr(e ).
The secondway is a modeling error, which means that the best Englishtranslation, as supplied by a competent human, did not max-imize this same product.
Our tests show that only 5% of oursystem's errors are search errors.3.
Language ModelingLet e be a string of English words el .. .
eL.
A language modelPr(e) gives the probability that e would appear in grammat-ical English text.By the laws of conditional probability we may writePr(e) = Pr (e t .
.
.eL )= Pr(et)  Pr(e21et) Pr(esle~e=)--.
Pr(eLle~ .
.
.
ez_t).158Given this decomposition the language modeler's job is toestimate ach of the f distributions on the right hand side.If IEI is the size of the English vocabulary, then the numberof different histories e t .
.
.
eh-t in the kth conditional growsas IEI h-t.
This presents problems both in practice and inprinciple--the former because we don't have enough storageto write down all the different histories, the latter becauseeven if we could, any one history would be exceedingly rare,making it impossible to estimate probabilities accurately.For these reasons, Candide has used the so-called trigrammodel as its workhorse.
In this model, we use the approxi-mationPr(ek I et .
.
.
e~_t) ~ Pr(e~ I eh-2ek-~)for each term on the right hand side above.
That is, we limitthe history to two words.
Each triple (ek-2ek- lek) is calleda trigram.It remains to estimate the Pr(e~leh_2eh_t ).
One solu-tion is to use maximum-likelihood trigram probabilities,T(eklek_2e~-t).
These are obtained by scanning the trainingcorpus c, counting the incidence of each trigram, and usingthese counts to form the appropriate conditional estimates.But even for this modest history size, we frequently encountertrigrams during translation that do not appear during train-ing.
This is not surprising, since there are IC\[ s = 1.773 x l0 tspossible different trigrams, yet we can encounter no morethan Icl of them during training.
There are 75,349,888 dis-tinct trigrams in our training corpus, of which 53,737,350occur exactly once.For this reason, we employ the technique of deleted interpola-tion \[6\]: we express Pr(ek\[ek-2e~-t) as a linear combinationof the trigram probability T(ek lek-2ek-t) ,  the bigram prob-ability B(ekleh_t), the unigram probability U(ek), and theuniform probability 1/IEI.
The distributions B and U areobtained by counting the incidence of bigrams and unigramsin the same training corpus c. But there are fewer distinctbigrams, so we have a higher chance of seeing any given onein our training data, and a still higher chance of seeing anygiven unigram.
The resulting formula for Pr(eklek_2ek_t) iscalled the smoothed trigrarn model.Even the smoothed trigram model eaves much to be desired,since it does not account for semantic and syntactic depen-dencies among words that do not lie within the same trigram.This has led us to use a link grammar model.
This is a train-able, probabilistic grammar that attempts to capture all theinformation present in the trigram model, and also to makethe long-range connections among words needed to advancebeyond it.
Link grammars are discussed in detail in \[7\].4.
Translat ion Model ingThis section describes the dements of our translation model,Pr( f  \[ e).
We have two distinct translation models, both de-scribed here: an EM-trained model, and a maximum-entropymodel.As we explain in Section 4.2 below, the EM-trained modelis developed through a succession of five provisional models.Before we describe them, we introduce the notion of align-ment.4.1 .
Al ignmentConsider a pair of French and English sentences (e, f)  that aretranslations of one another.
Although we argued above thatword-for-word translation will not work to develop f from e,it is clear that there is some relation between the individualwords of the two sentences.
A typical assignment of relationsis depicted in Figure 2.Thet dog2ILet chien2ares myt homework5as manger mess devoirseFigure 2: Alignment of a French-English Sentence Pair.
Thesubscripts give the position of each word in the sentence.We call such a set of connections between sentences an align-ment.
Formally we express it as a set a of pairs (j, i), whereeach pair stands for a connection between the j th  word of fand the ith word of e. Our intention is to connect f~ andei when ei was one of the words expressing in English theconcept that f j  (possibly along with other words of f) ex-presses in French.
In its most general form, an alignmentmay consist of any set a of (j, i) pairs.
But for shnplicity, werestrict ourselves to alignments in which each French word isconnected to a unique English word.We cannot hope to discover alignments with certainty.
Ourstrategy is to train a parametric model for the joint distrib-ution Pr(f, a \[ e), where the alignment a is hidden.
In prin-ciple, the desired conditional Pr( f  I e) may then be obtainedas ~aPr ( f ,  a le) ,  where the sum is taken over all possi-ble alignments of e and f. In practice this is possible onlyfor our first two models.
For the remaining models, we ap-proximate Pr( f  I e) as follows.
During training, we find thesingle most probable alignment &, and sum Pr(f,  a I e) overa small neighborhood of &.
During decoding, we simply usePr(f, ale).4.2 .
EM-Tra ined ModelsWe now sketch the structure of five models of increasing com-plexity, the last of which is our EM-trained translation model.For an in-depth treatment, he reader is referred to \[3\].1.
Word  Trans la t ion  This is our simplest model, intendedto discover probable individual-word translations.
The freeparameters of this model are word translation probabilitiest(fj I ei).
Each of these parameters i initialized to 1/I.FI,where Y is our French vocabulary.
Thus we make no initialassumptions about appropriate French-English word pair-ings.
The iterative training procedure automatically findsappropriate translations, and assigns them high probability.2.
Local  A l ignment  To make our model more realistic,we introduce an alignment variable aj for each position jof f; aj is the position in e to which the j th  word of f isaligned.
(French words that appear to arise spontaneously159are said to align to the null word, in position 0 ofe.)
Formally,we insert a parameter Pr(a~ I J, re, l) into our expression forPr(f,  a le  ).
This expresses the probability that positionin an arbitrary French sentence of length ra is aligned withposition aj in any English sentence of length l that is itstranslation.
The identities of the words in these positions donot influence the alignment probabilities.3.
Fer t i l i t ies  As we observed earlier, a single English wordmay yield 0, I or more French words, for instance as when nottranslates to ne...pus.
This idea is implicit in our notion ofalignment, but not explicitly related to word identities.
Tocapture this phenomenon explicitly, this model introducesthe notion of fertility.
The fertility ~(el) is the number ofFrench words in f that ei generates in translation.
Fertility isincorporated into this model through the parameters ~b(nlel),the probability that ~b(ei) equals n.4.
C lass -Based  A l ignment  In the preceding model,though the fertilities are conditioned upon word identities,the alignment parameters are not.
We have already pointedout how unrealistic this is, since it aligns positionsin the (e, f)pair with no regard for the words found there.
This modelremedies the problem by expressing alignments in terms ofparameters that depend upon the classes of words that lie atthe aligned positions.
Each word f in our French vocabulary.T is placed in one of approximately 50 classes; likewise foreach e in the English vocabulary S. The assignment of wordsto classes is made automatically through another statisticaltraining procedure \[3\].5.
Non-Def ic ient  A l ignment  The preceding two modelssuffer from a problem we call deficiency: they assign non-zeroprobability to "alignments" that do not correspond to stringsof French words at all.
For instance, two French words maybe assigned to lie at the same position in the sentence.
Wordsmay be placed before the start of the sentence, or after itsend.
This model eliminates uch spurious alignments.These five models are trained in succession on the same data,with the final parameter values of one model serving as thestarting point for the next.
For the current version of Can-dide, we used a corpus of 2,205,733 English-French sentencepairs, drawn mostly from the Hansards, which are the pro-ceedings of the Canadian Parliament.
The entire compu-tation took a total of approximately 3600 processor-hoursdistributed over fifteen IBM Model 530H POWERstations.The reader may be wondering why we have five translationmodels instead of one.
This is because the EM algorithm,though guaranteed to converge to a local maximum, neednot converge to a global one.
A weakness of the algorithm isthat it may yield a parameter vector 8 that is indeed a localmaximum, but which does not model the data well.It so happens though that model 1 has a special form that en-sures that EM training is guaranteed to converge to a globalmaximum.
By using model l 's  final parameter vector as theinitial vector for model 2, we are assured that we are at areasonably good starting point for training the latter.
Byextension of this argument, we proceed through the trainingof each model in succession, with some confidence that eachmodel's starting point is a good one.4.3.
Context Sensitive ModelsAll of the preceding translation models make one importantsimplification: each English word acts independently of allthe others in the sentence to generate the French words towhich it is aligned.
But it is easy to convince oneself that thisapproach is inadequate; clearly run will translate differentlyin Let's run the program!
and Let's run the race!.
Intuitively,we would like to make the translation of a word depend uponcontext in which it appears.For this reason, we have constructed translation models thattake context into account.
Our instinct is to make the trans-lation of a word depend upon its neighbors, say writingt( f j  \[ ei ei:~l .
.
. )
for the word-translation probabilities.
Butthis is impractical, because of the same difficulties that con-front language models with long histories.To overcome this, we employ a technique--maximum-entropymodeling--that deals with small chosen subsets of a poten-tially large number of conditioning variables.
We begin witha large set Q = {bl(f,e,e) b2(f,e,e) bs(f ,e,e) .
.
. )
of binary-valued functions.
Each such function asks some yes/no ques-tion about the French word f ,  the English word e, and thecontext e in which e appears.The training procedure works iteratively to find a small sub-set Q' = {bhl(fj,el,e) bh2(fj,ei,e)...b~,~(fj,el,e)) thatdisambiguates the senses of the English word in context.
For-mally, it develops a distribution t ( f j  I el Q')  that tells us iff j  is a good translation of e~ in the context e. Since this pro-cedure is costly in computer time, we develop such modelsonly for the 2,000 most common English words.
For moreinformation about maximum-entropy modeling, the reader isreferred to \[4\].5.
Analysis-Transfer-SynthesisAlthough we try to obtain accurate stimates of the parame-ters of our translation models by training on a large amountof text, this data is not used as effectively as it might be.
Forinstance, the word-translation probabilities ~(parle I speaks)and t(parlent I speak) must be learned separately, though theyexpress the underlying equivalence of the infinitives parlerand to speak.For this reason, we have adopted for Candide a variation ofthe analysis-transfer-synthesis paradigm.
In this paradigm,translation takes place not between raw French and Englishtexts, but between intermediate forms of the two languages.Note that because translation is effected between interme-diate French and intermediate English, all our models aretrained upon intermediate text as well.
For training, each(e, f) pair of our data is subjected to an analysis step: theFrench is rendered into an intermediate French f ' ,  the Eng-lish into intermediate English e'.
The English transformationis constructed to ensure that it is invertible; its inverse, fromintermediate English to standard English, is usually calledsynthesis.The aim of these transformations is three-fold: to suppresslexicai variations that conceal regularities between the twolanguages, to reduce the size of both vocabularies, and toreduce the burden on the alignment model by making coor-160dinating phrases resemble ach other as closely as possiblewith respect o length and word order.Both the English and the French analysis steps consist offive classes of operations: segmentation, ame and numberdetection, case and spelling correction, morphological naly-sis, and linguistic normalization.
During segmentation, theFrench is divided (if possible) into shorter phrases that rep-resent distinct concepts.
This does not modify the text, butthe translation model, used later, respects this division byignoring alignments that cross segment boundaries.During name and number detection, numbers and propernames--word strings such as Ethiopie, Grande Bretagne and$.85 era--are removed from the French text and replaced bygeneric name and number markers.
Removing names andnumbers greatly reduces the size of ?
and .T.
The excisedtexts are translated by rule and kept in a table, to be substi-tuted back into the English sentence during synthesis.During case  and spelling correction, we correct any obvi-ous spelling errors, and suppress the case variations in wordspellings that arise from the conventions of English andFrench typography.During morphological nalysis, we first use a hidden Maxkovmodel \[8\] to assign part-of-speech labels to the French, thenuse these labels to replace inflected verb forms with their in-fiuitives, preceded by an appropriate tense marker.
We alsoput nouns into singular form and precede them by numbermarkers, and perform a variety of other morphological trans-formations.Finally, during linguistic normalization we perform a series ofword reorderings, insertions and rewritings intended to reg-ularize each language, and to make the two languages moreclosely resemble ach other.
For example, the contractionsau and du are rewritten as d le and de le.
Constructions suchas il y a and he...pus are replaced with one-word tokens.
TheEnglish possessive construction is made to resemble Frenchby removing the 's  or 'sutfix, reordering noun phrases, andinserting an additional token.
Thus my aunt's pen becomesintermediate English dummy-article pen's  my aunt; note thesimilarity to the French le stylo de ma tante.6.
Operation of CandideIn previous sections we have indicated how the parametersof Candide's various models are determined via the EM algo-rithm and ma~c_imum-entropy methods.
We now outline thesteps involved in the execution of Candide as it translates aFrench passage into English.
The process of translation, di-vided into analysis, transfer, and synthesis tages, is depictedin Figure 3.In the analysis stage, the French input string f is convertedinto f~, as discussed above.
The output of this stage is de-noted in Figure 3 as Intermediate French.The transfer stage constitutes the decoding process ketchedin Section 2.2 above.
Decoding consists of two steps.
In thefirst step, Candide develops a set H* of candidate decodings,using coarse versions of our translation and language modelsto select its elements.
In the second step, the system expandsH* and rescores the enlarged set using more sophisticatedmodels.
We now describe both steps in greater detail.In the first step, Candide applies a variation of the stackdecoding algorithm to generate candidate decodings.
Decod-ing proceeds left-to-right, one intermediate English word at atime.
At each stage we maintain a ranked set H (~) of partialhypotheses for the intermediate English ~.In general, the elements of H (~) are partial decodings of f~;that is, only the leading i words of ~t have been filled in,and these account for only some of the words of f~.
To ad-vance the decoding, some elements of H (i) are selected to beextended by one word.
The translation and language mod-els work together to generate the i + 1st word; the result-ing partial decodings are ranked; this ranked set is H (~+l).An hypothesis is complete when all words of f~ have beenaccounted for.
Note that while the intermediate English isgenerated left-to-right, he treatment of intermediate Frenchwords does not necessarily proceed left-to-right, due to theword-reordering property of the channel.
This is one of thekey ways that translation differs from speech--a differencethat greatly complicates the decoding process.The ranking of hypotheses is according to the productPr(f~ I e~)Pr(e~).
In the interest of speed, and because wemust deal with partial rather than complete sentences, weemploy the EM-tralned translation model and the smoothedtrigram language model.
The output of this step is a rankedset H* of the 140 best intermediate English sentences.During the second step, called perturbation search, we enlargeH* by considering sequences of single-word eletions, inser-tions or replacements o its elements.
Then we rerank theenlarged set using the link grammar language model and themaximum-entropy translation model.
The highest-scoring in-termediate English sentence that we encounter during pertur-bation search is the output ~ of the transfer stage.The final stage, synthesis, converts the intermediate English~ into a plain English sentence ~.7.
Per fo rmanceWe evaluate our system in two ways: through participationin the ARPA evaluations, and through our own internal tests.The ARPA evaluation methodology, devised and executed byPRO, is detailed in \[9\]; we recount it here briefly.
AReA pro-vides us with a set of French passages, which we process intwo ways.
First, the passages are translated without any hu-man intervention.
This is the fully-automatic mode.
Second,each of the same passages is translated by two different hu-mans, once with and once without the aid of Transman, ourtranslation assistance tool.
Transman presents the user withan automated ictionary, a text editor, and parallel views ofthe French source and the English fully-automatic transla-tion.
The passages are ordered in such a way as to suppressthe influence of differing levels of translation skill, and thetimes of all the human translations are recorded.PRO scores all the resulting English texts for fluency and ad-161French Input f1Case and spelling correctionName and number detectionSegmentationMorphological analysisWord reordering1Intermediate French f ',LStack decoding with coarse modelsPerturbation search with refined models1Intermediate English ~ '1English synthesis1English OutputFigure 3: Steps in the Execution of Candideequacy, reporting these as numbers between 0 and 1.
Flu-ency is intended to measure the well-formedness of translatedsentences; adequacy is intended to measure to what extentthe meaning of each source text is present in the transla-tions.
The advantage afforded by Transman is determinedby computing the ratio tTrartsman/tmanual for each passage,where the numerator is the time to translate the passage withTransman's aid, and the denominator is the time for unaidedmanual translation.The means of all these statistics are presented in Table 1.As a benchmark, this table includes a line reporting :fluencyand adequacy results in these tests for Systran, a commercialfully-automatic French-English translation system, consid-ered by some to be the world's best.Our in-house evaluation methodology consists of fully-automatic translation of 100 sentences of 15 words or less;each translation is judged either correct or incorrect.
Thesesentences are drawn from the same domain as our trainingdata- - the  Hansard corpus--but hey are of course not sen-tences that we trained on.
Our 1992 system produced 45correct translations; our 1993 system produced 62 correcttranslations.Fluency Adequacy Time Ratio1992 1993 1992 1993 1992 1993Systran .466 .540 .686 .743Candide .511 .580 .575 .670Transman .819 .838 .837 .850 .688 .625Manual .833 .840Table 1: AB.PA Evaluation Results.
The Systran line reportsresults for Systran French-to-English fully-automatic trans-lations.
The Candide line reports results for our system'sfully-automatic translations; the Transman line reports re-suits for our system's machine-assisted translations.8.
SummaryWe began with a review of the source-channel formalism ofinformation theory, and how it may be applied to translation.Our approach reduces to formulating and training two para-metric probability models: the language model Pr(e), andthe translation model Pr( f  I e).
We described the structureof both models, and how they are trained.We explained the use of the analysis-transfer-synthesis par-adigm, and sketched the system's operation.
Finally, we gaveperformance results for Candide, in both its human-assistedand fully-automatic operating modes.In our opinion, the most promising avenues for explorationare: the continued elaboration of the link grammar languagemodel, more sophisticated translation models, the maximum-entropy modeling technique, and a more systematic approachto French and English morphological nd syntactic analysis.Re ferences1.
Allen, Arnold O. Probability, Statistics and QueueingTheory, Academic Press, New York, NY, 1978.2.
Baum, L. E. An inequality and associated maximizationtechnique in statistical estimation of probabilistic func-tions of a Matkov process.
Inequalities, 3, 1972, pp 1-8.3.
Brown, Peter F., Stephen A. Della Pietra, Vincent J.Della Pietra, Robert L. Mercer.
The mathematics ofstatistical machine translation: parameter estimation.Computational Linguistics 19(2), June 1993, pp 263-311.4.
Jaynes, E. T. Notes on present status and futureprospects.
Mazimum Entropy and Bayesian Methods,W.
T. Grandy and L. H. Schick, eds.
Kluwer AcademicPress, 1990, pp 1-13.5.
Jelinek, Frederick.
A fast sequential decoding algo#thmusing a stack.
IBM Journal of Research and Develop-ment, 13, November 1969, pp 675-685.6.
Jelinek, F., R. L. Mercer.
Interpolated estimation ofMarkov source parameters from sparse data.
In Proceed-ings, Workshop on Pattern Recognition i  Practice, Am-sterdam, The Netherlands, 1980.7.
Lafferty, John, Daniel Sleator, Davy Temperly.
Gram-matical trigrams: a probabilistic model of link gram-mar.
Proceedings of the 199~ AAAI  Fall Symposium onProbabilistic Approaches to Natural Language.8.
Meriaido, Bernard.
Tagging text with a probabilisticmodel.
Proceedings of the IBM Natural Language ITL,Paris, France, 1990, pp 161-172.9.
White, John S., Theresa A. O'Connell, Lynn M. Carl-son.
Evaluation of machine translation.
In Human Lan-guage Technology, Morgan Kaufman Publishers, 1993,pp 206-210.162
