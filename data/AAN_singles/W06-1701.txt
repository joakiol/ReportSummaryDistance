Web-based frequency dictionaries for medium density languagesAndra?s KornaiMetaCarta Inc.350 Massachusetts AvenueCambridge MA 02139andras@kornai.comPe?ter Hala?csyMedia Research and Education CenterStoczek u.
2H-1111 Budapesthalacsy@mokk.bme.huViktor NagyInstitute of LinguisticsBenczu?r u 33H-1399 Budapestnagyv@nytud.huCsaba OraveczInstitute of LinguisticsBenczu?r u 33H-1399 Budapestoravecz@nytud.huViktor Tro?nU of Edinburgh2 Buccleuch PlaceEH8 9LW Edinburghv.tron@ed.ac.ukDa?niel VargaMedia Research and Education CenterStoczek u.
2H-1111 Budapestdaniel@mokk.bme.huAbstractFrequency dictionaries play an importantrole both in psycholinguistic experimentdesign and in language technology.
Thepaper describes a new, freely available,web-based frequency dictionary of Hun-garian that is being used for both purposes,and the language-independent techniquesused for creating it.0 IntroductionIn theoretical linguistics introspective grammati-cality judgments are often seen as having method-ological primacy over conclusions based on whatis empirically found in corpora.
No doubt themain reason for this is that linguistics often studiesphenomena that are not well exemplified in data.For example, in the entire corpus of written En-glish there seems to be only one attested example,not coming from semantics papers, of Bach-Peterssentences, yet the grammaticality (and the pre-ferred reading) of these constructions seems be-yond reproach.
But from the point of view of thetheoretician who claims that quantifier meaningscan be computed by repeat substitution, even thisone example is one too many, since no such theorycan account for the clearly relevant (though barelyattested) facts.In this paper we argue that ordinary corpussize has grown to the point that in some areasof theoretical linguistics, in particular for is-sues of inflectional morphology, the dichotomybetween introspective judgments and empiricalobservations need no longer be maintained: inthis area at least, it is now nearly possible tomake the leap from zero observed frequency tozero theoretical probability i.e.
ungrammaticality.In many other areas, most notably syntax, thisis still untrue, and here we argue that facts ofderivational morphology are not yet entirelywithin the reach of empirical methods.
Bothfor inflectional and derivational morphologywe base our conclusions on recent work witha gigaword web-based corpus of Hungarian(Hala?csy et al2004) which goes some waytowards fulfilling the goals of the WaCky project(http://wacky.sslmit.unibo.it, seealso Lu?deling et al2005) inasmuch as the infras-tructure used in creating it is applicable to othermedium-density languages as well.
Section 1describes the creation of the WFDH Web-basedFrequency Dictionary of Hungarian from the rawcorpus.
The critical disambiguation step requiredfor lemmatization is discussed in Section 2,and the theoretical implications are presented inSection 3.
The rest of this Introduction is devotedto some terminological clarification and thepresentation of the elementary probabilistic modelused for psycholinguistic experiment design.0.1 The range of dataHere we will distinguish three kinds of corpora:small-, medium-, and large-range, based on the in-ternal coherence of the component parts.
A small-range corpus is one that is stylistically homoge-neous, generally the work of a single author.
Thelargest corpora that we could consider small-rangeare thus the oeuvres of the most prolific writers,rarely above 1m, and never above 10m words.
Amedium-range corpus is one that remains withinthe confines of a few text types, even if the au-thorship of individual documents can be discernede.g.
by detailed study of word usage.
The LDCgigaword corpora, composed almost entirely ofnews (journalistic prose), are from this perspec-1tive medium range.
Finally, a large-range corpusis one that displays a variety of text types, gen-res, and styles that approximates that of overalllanguage usage ?
the Brown corpus at 1m wordshas considerably larger range than e.g.
the Reuterscorpus at 100m words.The fact that psycholinguistic experiments needto control for word frequency has been known atleast since Thorndike (1941) and frequency ef-fects also play a key role in grammaticization (By-bee, 2003).
Since the principal source of variabil-ity in word (n-gram) frequencies is the choice oftopic, we can subsume overall considerations ofgenre under the selection of topics, especially asthe former typically dictates the latter ?
for ex-ample, we rarely see literary prose or poetry deal-ing with undersea sedimentation rates.
We assumea fixed inventory of topics T1, T2, .
.
.
, Tk, withk on the order 104, similar in granularity to theNorthern Light topic hierarchy (Kornai et al2003)and reserve T0 to topicless texts or ?General Lan-guage?.
Assuming that these topics appear in thelanguage with frequency q1, q2, .
.
.
, qk, summingto 1 ?
q0 ?
1, the ?average?
topic is expected tohave frequency about 1/k (and clearly, q0 is on thesame order, as it is very hard to find entirely topi-cless texts).As is well known, the salience of differentnouns and noun phrases appearing in the samestructural position is greatly impacted not just byfrequency (generally, less frequent words are morememorable) but also by stylistic value.
For ex-ample, taboo words are more salient than neutralwords of the same overall frequency.
But style isalso closely associated with topic, and if we matchfrequency profiles across topics we are thereforecontrolling for genre and style as well.
Present-ing psycholinguistical experiments is beyond thescope of this paper: here we put the emphasis oncreating the computational resource, the frequencydictionary, that allows for detail matching of fre-quency profiles.Defining the range r of a corpus C simplyas?j qj where the sum is taken over all topicstouched by documents in C, single-author cor-pora typically have r < 0.1 even for encyclope-dic writers, and web corpora have r > 0.9.
Notethat r just measures the range, it does not mea-sure how representative a corpus is for some lan-guage community.
Here we discuss results con-cerning all three ranges.
For small range, we usethe Hungarian translation of Orwell?s 1984 ?
98kwords including punctuation tokens, (Dimitrova etal., 1998).
For mid-range, we consider four topi-cally segregated subcorpora of the Hungarian sideof our Hungarian-English parallel corpus ?
34mwords, (Varga et al, 2005).
For large-range weuse our webcorpus ?
700m words, (Hala?csy et al,2004).1 Collecting and presenting the dataHungarian lags behind ?high density?
languageslike English and German but is hugely ahead ofminority languages that have no significant ma-chine readable material.
Varga et al(2005) es-timated there to be about 500 languages that fitin the same ?medium density?
category, togetheraccounting for over 55% of the world?s speakers.Halacsy et al(2004) described how a set of opensource tools can be exploited to rapidly clean theresults of web crawls to yield high quality mono-lingual corpora: the main steps are summarizedbelow.Raw data, preprocessing The raw datasetcomes from crawling the top-level domain, e.g..hu, .cz, .hr, .pl etc.
Pages that con-tain no usable text are filtered out, and all text isconverted to a uniform character encoding.
Iden-tical texts are dropped by checksum compari-son of page bodies (a method that can handlenear-identical pages, usually automatically gener-ated, which differ only in their headers, datelines,menus, etc.
)Stratification A spellchecker is used to stratifypages by recognition error rates.
For each page wemeasure the proportion of unrecognized (either in-correctly spelled or out of the vocabulary of thespellchecker) words.
To filter out non-Hungarian(non-Czech, non-Croatian, non-Polish, etc.)
docu-ments, the threshold is set at 40%.
If we lower thethreshold to 8%, we also filter out flat native textsthat employ Latin (7-bit) characters to denote theiraccented (8 bit) variants (these are still quite com-mon due to the ubiquity of US keyboards).
Finally,below the 4% threshold, webpages typically con-tain fewer typos than average printed documents,making the results comparable to older frequencycounts based on traditional (printed) materials.Lemmatization To turn a given stratum of thecorpus into a frequency dictionary, one needs tocollect the wordforms into lemmas based on the2same stem: we follow the usual lexicographicpractice of treating inflected, but not derived,forms of a stem as belonging to the same lemma.Inflectional stems are computed by a morphologi-cal analyzer (MA), the choice between alternativemorphological analyses is resolved using the out-put of a POS tagger (see Section 2 below).
Whenthere are several analyses that match the output ofthe tagger, we choose one with the least number ofidentified morphemes.
For now, words outside thevocabulary of the MA are not lemmatized at all ?this decision will be revisited once the planned ex-tension of the MA to a morphological guesser iscomplete.Topic classification Kornai et al(2003) pre-sented a fully automated system for the classifica-tion of webpages according to topic.
Combiningthis method with the methods described above en-ables the automatic creation of topic-specific fre-quency dictionaries and further, the creation of aper-topic frequency distribution for each lemma.This enables much finer control of word selectionin psycholinguistic experiments than was hithertopossible.1.1 How to present the data?For Hungarian, the highest quality (4% thresh-old) stratum of the corpus contains 1.22m uniquepages for a total of 699m tokens, already exceed-ing the 500m predicted in (Kilgarriff and Grefen-stette, 2003).
Since the web has grown consid-erably since the crawl (which took place in 2003),their estimate was clearly on the conservative side.Of the 699m tokens some 4.95m were outside thevocabulary of the MA (7% OOV in this mode,but less than 3% if numerals are excluded and theanalysis of compounds is turned on).
The remain-ing 649.7m tokens fall in 195k lemmas with anaverage 54 form types per lemma.
If all stems areconsidered, the ratio is considerably lower, 33.6,but the average entropy of the inflectional distri-butions goes down only from 1.70 to 1.58 bits.As far as the summary frequency list (which isless than a megabyte compressed) is concerned,this can be published trivially.
Clearly, the avail-ability of large-range gigaword corpora is in thebest interest of all workers in language technology,and equally clearly, only open (freely download-able) materials allow for replicability of experi-ments.
While it is possible to exploit search enginequeries for various NLP tasks (Lapata and Keller,2004), for applications which use corpora as unsu-pervised training material downloadable base datais essential.Therefore, a compiled webcorpus should con-tain actual texts.
We believe all ?cover your be-hind?
efforts such as publishing only URLs to befundamentally misguided.
First, URLs age veryrapidly: in any given year more than 10% be-come stale (Cho and Garcia-Molina, 2000), whichmakes any experiment conducted on such a ba-sis effectively irreproducible.
Second, by present-ing a quality-filtered and characterset-normalizedcorpus the collectors actually perform a service tothose who are less interested in such mundane is-sues.
If everybody has to start their work from theground up, many projects will exhaust their fund-ing resources and allotted time before anything in-teresting could be done with the data.
In contrast,the Free and Open Source Software (FOSS) modelactively encourages researchers to reuse data.In this regard, it is worth mentioning that dur-ing the crawls we always respected robots.txtand in the two years since the publication of the gi-gaword Hungarian web corpus, there has not beena single request by copyright holders to removematerial.
We do not advocate piracy: to the con-trary, it is our intended policy to comply with re-moval requests from copyright holders, analogousto Google cache removal requests.
Finally, evenwith copyright material, there are easy methodsfor preserving interesting linguistic data (say un-igram and bigram models) without violating theinterests of businesses involved in selling the run-ning texts.
12 The disambiguation of morphologicalanalysesIn any morphologically complex language, theMA component will often return more than onepossible analysis.
In order to create a lemma-tized frequency dictionary it is necessary to de-cide which MA alternative is the correct one, andin the vast majority of cases the context providessufficient information for this.
This morphologi-cal disambiguation task is closely related to, butnot identical with, part of speech (POS) tagging,a term we reserve here for finding the major parts1This year, we are publishing smaller pilot corpora forCzech (10m words), Croatian (4m words), and Polish (12mwords), and we feel confident in predicting that these willface as little actual opposition from copyright holders as theHungarian Webcorpus has.3of speech (N, V, A, etc).
A full tag contains bothPOS information and morphological annotation:in highly inflecting languages the latter can leadto tagsets of high cardinality (Tufis?
et al, 2000).Hungarian is particularly challenging in this re-gard, both because the number of ambiguous to-kens is high (reaching 50% in the Szeged Cor-pus according to (Csendes et al, 2004) who usea different MA), and because the ratio of tokensthat are not seen during training (unseen) can beas much as four times higher than in comparablesize English corpora.
But if larger training corporaare available, significant disambiguation is possi-ble: with a 1 m word training corpus (Csendes etal., 2004) the TnT (Brants, 2000) architecture canachieve 97.42% overall precision.The ratio of ambiguous tokens is usually cal-culated based on alternatives offered by a mor-phological lexicon (either built during the trainingprocess or furnished by an external application;see below).
If the lexicon offers alternative anal-yses, the token is taken as ambiguous irrespectiveof the probability of the alternatives.
If an exter-nal resource is used in the form of a morphologicalanalyzer (MA), this will almost always overgener-ate, yielding false ambiguity.
But even if the MAis tight, a considerable proportion of ambiguoustokens will come from legitimate but rare analysesof frequent types (Church, 1988).
For example theword nem, can mean both ?not?
and ?gender?, soboth ADV and NOUN are valid analyses, but the ad-verbial reading is about five orders of magnitudemore frequent than the noun reading, (12596 vs. 4tokens in the 1 m word manually annotated SzegedKorpusz (Csendes et al, 2004)).Thus the difficulty of the task is better mea-sured by the average information required for dis-ambiguating a token.
If word w is assignedthe label Ti with probability P (Ti|w) (estimatedas C(Ti, w)/C(w) from a labeled corpus) thenthe label entropy for a word can be calculatedas H(w) = ?
?i P (Ti|w) logP (Ti|w), and thedifficulty of the labeling task as a whole is theweighted average of these entropies with respectto the frequencies of words w:?w P (w)H(w).As we shall see in Section 3, according to thismeasure the disambiguation task is not as difficultas generally assumed.A more persistent problem is that the ratio ofunseen items has very significant influence on theperformance of the disambiguation system.
Theproblem is more significant with smaller corpora:in general, if the training corpus has N tokens andthe test corpus is a constant fraction of this, sayN/10, we expect the proportion of new words tobe cN q?1, where q is the reciprocal of the Zipfconstant (Kornai, 1999).
But if the test/train ra-tio is not kept constant because the training corpusis limited (manual tagging is expensive), the num-ber of tokens that are not seen during training cangrow very large.
Using the 1.2 m words of SzegedCorpus for training, in the 699 m word webcor-pus over 4% of the non-numeric tokens will be un-seen.
Given that TnT performs rather dismally onunseen items (Oravecz and Dienes, 2002) it wasclear from the outset that for lemmatizing the we-bcorpus we needed something more elaborate.The standard solution to constrain the prob-abilistic tagging model for some of the unseenitems is the application of MA (Hakkani-Tu?r et al,2000; Hajic?
et al, 2001; Smith et al, 2005).
Herea distinction must be made between those itemsthat are not found in the training corpus (these wehave called unseen tokens) and those that are notknown to the MA ?
we call these out of vocabulary(OOV).
As we shall see shortly, the key to the besttagging architecture we found was to follow dif-ferent strategies in the lemmatization and morpho-logical disambiguation of OOV and known (in-vocabulary) tokens.The first step in tagging is the annotation ofinflectional features, with lemmatization beingpostponed to later processing as in (Erjavec andDz?eroski, 2004).
This differs from the method of(Hakkani-Tu?r et al, 2000), where all syntacticallyrelevant features (including the stem or lemma) ofword forms are determined in one pass.
In our ex-perience, the choice of stem depends so heavilyon the type of linguistic information that later pro-cessing will need that it cannot be resolved in fullgenerality at the morphosyntactic level.Our first model (MA-ME) is based on disam-biguating the MA output in the maximum entropy(ME) framework (Ratnaparkhi, 1996).
In addi-tion to the MA output, we use ME features codingthe surface form of the preceding/following word,capitalization information, and different charac-ter length suffix strings of the current word.
TheMA used is the open-source hunmorph ana-lyzer (Tro?n et al, 2005) with the morphdb.huHungarian morphological resource, the ME is theOpenNLP package (Baldridge et al, 2001).
The4MA-ME model achieves 97.72% correct POS tag-ging and morphological analysis on the test corpus(not used in training).Maximum entropy or other discriminativeMarkov models (McCallum et al, 2000) sufferfrom the label bias problem (Lafferty et al, 2001),while generative models (most notably HMMs)need strict independence assumptions to make thetask of sequential data labeling tractable.
Con-sequently, long distance dependencies and non-independent features cannot be handled.
To copewith these problems we designed a hybrid archi-tecture, in which a trigramHMM is combined withthe MA in such a way that for tokens known to theMA only the set of possible analyses are allowedas states in the HMM whereas for OOVs all statesare possible.
Lexical probabilities P (wi|ti) forseen words are estimated from the training corpus,while for unseen tokens they are provided by thethe above MA-ME model.
This yields a trigramHMM where emission probabilities are estimatedby a weighted MA, hence the model is calledWMA-T3.
This improves the score to 97.93%.Finally, it is possible to define another archi-tecture, somewhat similar to Maximum EntropyMarkov Models, (McCallum et al, 2000), usingthe above components.
Here states are also theset of analyses the MA allows for known tokensand all analyses for OOVs, while emission prob-abilities are estimated by the MA-ME model.
Inthe first pass TnT is run with default settings overthe data sequence, and in the second pass the MEreceives as features the TnT label of the preced-ing/following token as well as the one to be ana-lyzed.
This combined system (TnT-MA-ME) in-corporates the benefits of all the submodules andreaches an accuracy of 98.17% on the Szeged Cor-pus.
The results are summarized in Table 1.model accuracyTnT 97.42MA+ME 97.72WMA+T3 97.93TnT+MA+ME 98.17Table 1: accuracy of morphologicaldisambiguationWe do not consider these results to be final:clearly, further enhancements are possible e.g.
bya Viterbi search on alternative sentence taggingsusing the T3 trigram tag model or by handlingOOVs on a par with known unseen words usingthe guesser function of our MA.
But, as we dis-cuss in more detail in Halacsy et al2005, we arealready ahead of the results published elsewhere,especially as these tend to rely on idealized MAsystems that have their morphological resourcesextended so as to have no OOV on the test set.3 ConclusionsOnce the disambiguation of morphological anal-yses is under control, lemmatization itself is amechanical task which we perform in a databaseframework.
This has the advantage that it sup-ports a rich set of query primitives, so that wecan easily find e.g.
nouns with back vowels thatshow stem vowel elision and have approximatelythe same frequency as the stem orvos ?doctor?.Such a database has obvious applications both inpsycholinguistic experiments (which was one ofthe design goals) and in settling questions of the-oretical morphology.
But there are always nag-ging doubts about the closed world assumption be-hind databases, famously exposed in linguistics byChomsky?s example colorless green ideas sleepfuriously: how do we distinguish this from *greensleep colorless furiously ideas if the observed fre-quency is zero for both?Clearly, a naive empirical model that assignszero probability to each unseen word form makesthe wrong predictions.
Better estimates can beachieved if unseen words which are known to bepossible morphologically complex forms of seenlemmas are assigned positive probability.
This canbe done if the probability of a complex form is insome way predictable from the probabilities of itscomponent parts.
A simple variant of this modelis the positional independence hypothesis whichtakes the probabilities of morphemes in separatepositional classes to be independent of each other.Here we follow Antal (1961) and Kornai (1992) inestablishing three positional classes in the inflec-tional paradigm of Hungarian nouns.# Position 1 parametersFAM 0.0001038986PLUR 0.1372398793PLUR_POSS 0.0210927964PLUR_POSS<1> 0.0011609442PLUR_POSS<1><PLUR> 0.0028751247PLUR_POSS<2> 0.0004958278PLUR_POSS<2><PLUR> 0.0000740203PLUR_POSS<PLUR> 0.0023850120POSS 0.14616359465POSS<1> 0.0073305415POSS<1><PLUR> 0.0073652648POSS<1>_FAM 0.0000092294POSS<2> 0.0027628071POSS<2><PLUR> 0.0003006440POSS<2>_FAM 0.0000030591POSS<PLUR> 0.0069613929POSS_FAM 0.0000000001ZERO1 0.6636759634# Position 2 parametersANP 0.0007780001ANP<PLUR> 0.0000248301ZERO2 0.9991971698# Position 3 parametersCAS<ABL> 0.0078638013CAS<ACC> 0.1346412632CAS<ADE> 0.0045132704CAS<ALL> 0.0138677701CAS<CAU> 0.0037332025CAS<DAT> 0.0301123636CAS<DEL> 0.0128222999CAS<ELA> 0.0118596792CAS<ESS> 0.0010230505CAS<FOR> 0.0031204983CAS<ILL> 0.0154186683CAS<INE> 0.0582887516CAS<INS> 0.0406197868CAS<SBL> 0.0386519707CAS<SUE> 0.0357416253CAS<TEM> 0.0013095685CAS<TER> 0.0034032438CAS<TRA> 0.0017860054ZERO3 0.5812231804Table 3: marginal probabilities in noun inflectionThe innermost class is used for number and pos-sessive, with a total of 18 choices including thezero morpheme (no possessor and singular).
Thesecond positional class is for anaphoric posses-sives with a total of three choices including thezero morpheme, and the third (outermost) classis for case endings with a total of 19 choicesincluding the zero morpheme (nominative) for atotal of 1026 paradigmatic forms.
The parame-ters were obtained by downhill simplex minimiza-tion of absolute errors.
The average absolute er-ror is of the values computed by the independecehypothesis from the observed values is 0.000099(mean squared error is 9.18 ?
10?7), including the209 paradigmatic slots for which no forms werefound in the webcorpus at all (but the indepen-dence model will assign positive probability to anyof them as the product of the component probabil-ities).
When checking the independence hypoth-esis with ?
statistics in the webcorpus for everynominal inflectional morpheme pair the membersof which are from different dimensions, the ?
co-efficient remained less than 0.1 for each pair but3.
For these 3 the coefficient is under 0.2 (whichmeans that the shared variance of these pairs is be-tween 1% and 2%) so we have no reason to discardthe independence hypothesis.
If we run the sametest on the 150 million words Hungarian NationalCorpus, which was analyzed and tagged by differ-ent tools, we also get the same result (Nagy, 2005).It is very easy to construct low probability com-binations using this model.
Taking a less frequentpossessive ending such as the 2nd singular poses-sor familiar plural -ode?k, the anaphoric plural -e?i,and a rarer case ending such as the formalis -ke?ntwe obtain combinations such as bara?tode?ke?ike?nt?as the objects owned by your friends?
company?.The model predicts we need a corpus with about4.2 ?
1012 noun tokens to see this suffix combina-tion (not necessarily with the stem bara?t ?friend?
)or about ten trillion tokens.
While the current cor-pus falls short by four orders of magnitude, thisis about the contribution of the anaphoric plural(which we expect to see only once in about 40knoun tokens) so for any two of the three positionclasses combined the prediction that valid inflec-tional combinations will actually be attested is al-ready testable.Using the fitted distribution of the positionclasses, the entropy of the nominal paradigm iscomputed simply as the sum of the class entropies,1.554 + 0.0096 + 2.325 or 3.888 bits.
Since thenominal paradigm is considerably more complexthan the verbal paradigm (which has a total of52 forms) or the infinitival paradigm (7 forms),this value can serve as an upper bound on the in-flectional entropy of Hungarian.
In Table 3 wepresent the actual values, computed on a varietyof frequency dictionaries.
The smallest of theseis based on a single text, the Hungarian transla-tion of Orwell?s 1984.
The mid-range corporaused in this comparison are segregated in broadtopics: law (EU laws and regulations), literature,movie subtitles, and software manuals: all werecollected from the web as part of building a bilin-gual English-Hungarian corpus.
Finally, the large-range is the full webcorpus at the best (4% reject)quality stratum.61984 law literature subtitles software webcorpustoken 98292 2310742 7971157 2667420 839339 69926550type 20343 110040 431615 188131 81729 2083023OOV token 3141 266368 335660 181292 140551 4951743OOV type 1132 39467 87574 50078 45799 994890lemma 10644 60602 165259 85491 58939 1189471lemma excl.
OOV 9513 21136 77686 35414 13141 194589lemma entropy 1.14282 1.04118 1.54922 1.41374 1.14516 1.57708lemma entropy excl.
OOV 1.18071 1.17687 1.61753 1.51718 1.37559 1.69743Table 3: inflectional entropy of Hungarian computed on a variety of frequency dictionariesOur overall conclusion is that for many pur-poses a web-based corpus has significant advan-tages over more traditional corpora.
First, it ischeap to collect.
Second, it is sufficiently hetero-geneous to ensure that language models based onit generalize better on new texts of arbitrary topicsthan models built on (balanced) manual corpora.As we have shown, automatically tagged and lem-matized webcorpora can be used to obtain largecoverage stem and wordform frequency dictionar-ies.
While there is a significant portion of OOVentries (about 3% for our current MA), in the de-sign of psycholinguistic experiments it is gener-ally sufficient to consider stems already known tothe MA, and the variety of these (over three timesthe stem lexicon of the standard Hungarian fre-quency dictionary) enables many controlled exper-iments hitherto impossible.ReferencesLa?szlo?
Antal.
1961.
A magyar esetrendszer.
Nyelvtu-doma?nyi E?rtekeze?sek, 29.Jason Baldridge, Thomas Morton, and Gann Bierner.2001.
The opennlp maximum entropy package.http://maxent.sourceforge.net.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference (ANLP-2000), Seattle, WA.Joan Bybee.
2003.
Mechanisms of change in gram-maticization: the role of frequency.
In Brian Josephand Richard Janda, editors, Handbook of HistoricalLinguistics, pages 602?623.
Blackwell.Junghoo Cho and Hector Garcia-Molina.
2000.
Theevolution of the web and implications for an incre-mental crawler.
In VLDB ?00: Proceedings of the26th International Conference on Very Large DataBases, pages 200?209, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Kenneth Ward Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text.In Proceedings of the second conference on Appliednatural language processing, pages 136?143, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Do?ra Csendes, Ja?no?s Csirik, and Tibor Gyimo?thy.2004.
The Szeged Corpus: A POS tagged and syn-tactically annotated Hungarian natural language cor-pus.
In Karel Pala Petr Sojka, Ivan Kopecek, editor,Text, Speech and Dialogue: 7th International Con-ference, TSD, pages 41?47.Ludmila Dimitrova, Tomaz Erjavec, Nancy Ide,Heiki Jaan Kaalep, Vladimir Petkevic, and DanTufis?.
1998.
Multext-east: Parallel and comparablecorpora and lexicons for six central and eastern euro-pean languages.
In Christian Boitet and Pete White-lock, editors, Proceedings of the Thirty-Sixth AnnualMeeting of the Association for Computational Lin-guistics and Seventeenth International Conferenceon Computational Linguistics, pages 315?319, SanFrancisco, California.
Morgan Kaufmann Publish-ers.Tomaz?
Erjavec and Sas?o Dz?eroski.
2004.
Machinelearning of morphosyntactic structure: Lemmatizingunknown Slovene words.
Applied Artificial Intelli-gence, 18(1):17?41.Jan Hajic?, Pavel Krbec, Karel Oliva, Pavel Kve?ton?,and Vladim?
?r Petkevic?.
2001.
Serial combinationof rules and statistics: A case study in Czech tag-ging.
In Proceedings of the 39th Association ofComputational Linguistics Conference, pages 260?267, Toulouse, France.Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khanTu?r.
2000.
Statistical morphological disambigua-tion for agglutinative languages.
In Proceedings ofthe 18th conference on Computational linguistics,pages 285?291, Morristown, NJ, USA.
Associationfor Computational Linguistics.Pe?ter Hala?csy, Andra?s Kornai, La?szlo?
Ne?meth, Andra?sRung, Istva?n Szakada?t, and Viktor Tro?n.
2004.
Cre-ating open language resources for Hungarian.
InProceedings of Language Resources and Evalua-tion Conference (LREC04).
European Language Re-sources Association.7Pe?ter Hala?csy, Andra?s Kornai, and Da?niel Varga.2005.
Morfolo?giai egye?rtelmu?s?
?te?s maximumentro?pia mo?dszerrel (morphological disambiguationwith the maxent method).
In Proc.
3rd Hungar-ian Computational Linguistics Conf.
Szegedi Tu-doma?nyegyetem.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the web as corpus.Computational Linguistics, 29(3):333?348.Andra?s Kornai, Marc Krellenstein, Michael Mulligan,David Twomey, Fruzsina Veress, and Alec Wysoker.2003.
Classifying the hungarian web.
In A. Copes-take and J. Hajic, editors, Proc.
EACL, pages 203?210.Andra?s Kornai.
1992.
Frequency in morphology.
InI.
Kenesei, editor, Approaches to Hungarian, vol-ume IV, pages 246?268.Andra?s Kornai.
1999.
Zipf?s law outside the middlerange.
In J. Rogers, editor, Proc.
Sixth Meeting onMathematics of Language, pages 347?356.
Univer-sity of Central Florida.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.Mirella Lapata and Frank Keller.
2004.
The webas a baseline: Evaluating the performance of un-supervised web-based models for a range of NLPtasks.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 121?128, Boston, Massachusetts, USA,May 2 - May 7.
Association for Computational Lin-guistics.Anke Luedeling, Stefan Evert, and Marco Baroni.2005.
Using web data for linguistic purposes.
InMarianne Hundt, Caroline Biewer, and Nadjia Nes-selhauf, editors, Corpus linguistics and the Web.Rodopi.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov mod-els for information extraction and segmentation.
InProceedings of the 17th International Conference onMachine Learning, pages 591?598.
Morgan Kauf-mann, San Francisco, CA.Viktor Nagy.
2005.
A magyar fo?ne?vi inflexio?statisztikai modellje (statistical model of nominalinflection in hungarian.
In Proc.
Kodola?nyi-ELTEConf.Csaba Oravecz and Pe?ter Dienes.
2002.
Effi-cient stochastic part-of-speech tagging for Hungar-ian.
In Proceedings of the Third InternationalConference on Language Resources and Evaluation(LREC2002), pages 710?717.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Karel PalaPetr Sojka, Ivan Kopecek, editor, Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 133?142, University ofPennsylvania.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proceedings of the Confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, Van-couver.Edward L. Thorndike.
1941.
The Teaching of EnglishSuffixes.
Teachers College, Columbia University.Viktor Tro?n, Gyo?rgy Gyepesi, Pe?ter Hala?csy, Andra?sKornai, La?szlo?
Ne?meth, and Da?niel Varga.
2005.Hunmorph: open source word analysis.
In Proceed-ing of the ACL 2005 Workshop on Software.Dan Tufis?, Pe?ter Dienes, Csaba Oravecz, and Tama?sVa?radi.
2000.
Principled hidden tagset design fortiered tagging of Hungarian.
In Proceedings of theSecond International Conference on Language Re-sources and Evaluation.Da?niel Varga, La?szlo?
Ne?meth, Pe?ter Hala?csy, Andra?sKornai, Viktor Tro?n, and Viktor Nagy.
2005.
Paral-lel corpora for medium density languages.
In Pro-ceedings of the Recent Advances in Natural Lan-guage Processing 2005 Conference, pages 590?596,Borovets.
Bulgaria.8
