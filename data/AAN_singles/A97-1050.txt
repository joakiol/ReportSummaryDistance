Semi-Automatic Acquisition ofDomain-Specific Translation LexiconsPh i l ip  Resn ikDept.
of Linguistics and UMIACSUnivers i ty of Mary landCollege Park,  MD 20742 USAresn ik~umiacs ,  umd.
eduI.
Dan  Me lamedDept.
of Computer  and Informat ion ScienceUnivers i ty of Pennsy lvaniaPhi ladelphia,  PA 19104 USAmelamed?unagi, cis.
upenn, eduAbst ractWe investigate the utility of an algo-rithm for translation lexicon acquisition(SABLE), used previously on a very largecorpus to acquire general translation lexi-cons, when that algorithm is applied to amuch smaller corpus to produce candidatesfor domain-specific translation lexicons.1 In t roduct ionReliable translation lexicons are useful in many ap-plications, such as cross-language t xt retrieval.
Al-though general purpose machine readable bilingualdictionaries are sometimes available, and althoughsome methods for acquiring translation lexicons au-tomatically from large corpora have been proposed,less attention has been paid to the problem of ac-quiring bilingual terminology specific to a domain,especially given domain-specific parallel corpora ofonly limited size.In this paper, we investigate the utility of an algo-rithm for translation lexicon acquisition (Melamed,1997), used previously on a very large corpus to ac-quire general translation lexicons, when that algo-rithm is applied to a much smaller corpus to producecandidates for domain-specific translation lexicons.The goal is to produce material suitable for post-processing in a lexicon acquisition process like thefollowing:1.
Run the automatic lexicon acquisition algo-rithm on a domain-specific parallel corpus.2.
Automatically filter out "general usage" entriesthat already appear in a machine readable dic-tionary (MRD) or other general usage lexicalresources.3.
Manually filter out incorrect or irrelevant en-tries from the remaining list.Our aim, therefore, is to achieve sufficient recall andprecision to make this process - -  in particular thetime and manual effort required in Step 3 - -  a viablealternative to manual creation of translation lexiconswithout automated assistance.The literature on cross-lingual text retrieval(CLTR) includes work that is closely related to thisresearch, in that recent approaches emphasize theuse of dictionary- and corpus-based techniques fortranslating queries from a source language into thelanguage of the document collection (Oard, 1997).Davis and Dunning (1995), for example, generatetarget-language queries using a corpus-based tech-nique that is similar in several respects to the workdescribed here.
However, the approach does not at-tempt to distinguish domain-specific from generalusage term pairs, and it involves no manual inter-vention.
The work reported here, focusing on semi-automating the process of acquiring translation lexi-cons specific to a domain, can be viewed as providingbilingual dictionary entries for CLTR methods likethat used by Davis in later work (Davis, 1996), inwhich dictionary-based generation of an ambiguoustarget language query is followed by corpus-baseddisambiguation f that query.Turning to the literature on bilingual terminologyidentification per se, although monolingual termi-nology extraction is a problem that has been previ-ously explored, often with respect o identifying rel-evant multi-word terms (e.g.
(Daille, 1996; Smadja,1993)), less prior work exists for bilingual acquisi-tion of domain-specific translations.
Termight (Da-gun and Church, 1994) is one method for analyzingparallel corpora to discover translations in techni-cal terminology; Dagan and Church report accuracyof 40% given an English/German technical manual,and observe that even this relatively low accuracypermits the successful application of the system in atranslation bureau, when used in conjunction withan appropriate user interface.The Champollion system (Smadja, McKeown, andHatzivassiloglou, 1996) moves toward higher accu-racy (around 73%) and considerably greater flex-ibility in the handling of multi-word translations,though the algorithm has been applied primarily tovery large corpora such as the Hansards (3-9 mil-lion words; Smadja et al observe that the methodhas difficulty handling low-frequency cases), and no340attempt is made to distinguish corpus-dependenttranslations from general ones.Daille et al (1994) report on a study in which asmall (200,000 word) corpus was used as the basis forextracting bilingual terminology, using a combina-tion of syntactic patterns for identifying simple two-word terms monolingually, and a statistical measurefor selecting related terms across languages.
Using amanually constructed reference list, they report 70%precision.The SABLE system (Melamed, 1996b) makes noattempt o handle collocations, but for single-wordto single-word translations it offers a very accuratemethod for acquiring high quality translation lexi-cons from very large parallel corpora: Melamed re-ports 90+% precision at 90+% recall, when evalu-ated on sets of Hansards data of 6-7 million words.Previous work with SABLE does not attempt o ad-dress the question of domain-specific vs. generaltranslations.This paper applies the SABLE system to a muchsmaller (approximately 400,000 word) corpus in atechnical domain, and assesses its potential con-tribution to the semi-automatic a quisition processoutlined above, very much in the spirit of Dagan andChurch (1994) and Daille et al (1994), but begin-ning with a higher accuracy starting point and fo-cusing on mono-word terms.
In the remainder of thepaper we briefly outline translation lexicon acquisi-tion in the SABLE system, describe its applicationto a corpus of technical documentation, and providea quantitative assessment of its performance.2 SABLESABLE (Scalable Architecture for Bilingual LExi-cography) is a turn-key system for producing cleanbroad-coverage translation lexicons from raw, un-aligned parallel texts (bitexts).
Its design is mod-ular and minimizes the need for language-specificcomponents, with no dependence on genre or wordorder similarity, nor sentence boundaries or other"anchors" in the input.SABLE was designed with the following featuresin mind:?
Independence from linguistic resources: SABLEdoes not rely on any language-specific resourcesother than tokenizers and a heuristic for iden-tifying word pairs that are mutual translations,though users can easily reconfigure the systemto take advantage of such resources as language-specific stemmers, part-of-speech taggers, andstop lists when they are available.?
Black box functionality: Automatic acquisitionof translation lexicons requires only that theuser provide the input bitexts and identify thetwo languages involved.Robustness: The system performs well even inthe face of omissions or inversions in transla-tions.
* Scalability: SABLE has been used successfullyon input bitexts larger than 130MB.
* Portability: SABLE was initially implementedfor French/English, then ported to Span-ish/English and to Korean/English.
The port-ing process has been standardized and docu-mented (Melamed, 1996c).The following is a brief description of SABLE'smain components.
A more detailed description ofthe entire system is available in (Melamed, 1997).2.1 Mapping Bitext CorrespondenceAfter both halves of the input bitext(s) have beentokenized, SABLE invokes the Smooth Injective MapRecognizer (SIMR) algorithm (Melamed, 1996a) andrelated components to produce a bitext map.
A bi-text map is an injective partial function between thecharacter positions in the two halves of the bitext.Each point of correspondence (x,y) in the bitextmap indicates that the word centered around char-acter position x in the first half of the bitext is atranslation of the word centered around characterposition y in the second half.
SIMR produces bitextmaps a few points at a time, by interleaving a pointgeneration phase and a point selection phase.SIMR is equipped with several "plug-in" match-ing heuristic modules which are based on cognates(Davis et al, 1995; Simard et al, 1992; Melamed,1995) and/or "seed" translation lexicons (Chen,1993).
Correspondence points are generated usinga subset of these matching heuristics; the particularsubset depends on the language pair and the avail-able resources.
The matching heuristics all work atthe word level, which is a happy medium betweenlarger text units like sentences and smaller text unitslike character n-grams.
Algorithms that map bitextcorrespondence at the phrase or sentences level arelimited in their applicability to bitexts that haveeasily recognizable phrase or sentence boundaries,and Church (1993) reports that such bitexts are farmore rare than one might expect.
Moreover, evenwhen these larger text units can be found, theirsize imposes an upper bound on the resolution ofthe bitext map.
On the other end of the spectrum,character-based bitext mapping algorithms (Church,1993; Davis et al, 1995) are limited to language pairswhere cognates are common; in addition, they mayeasily be misled by superficial differences in format-ting and page layout and must sacrifice precision tobe computationally tractable.SIMR filters candidate points of correspondenceusing a geometric pattern recognition algorithm.The recognized patterns may contain non-monotonicsequences of points of correspondence, to account for341( ?
point of correspondenceJ/ /}-~, - .- " ?
'~.-'"f Jcharacter position in bitoxt half AFigure 1: Word token pairs whose co-ordinateslie between the dashed boundaries count as co-occt l r ' rence8 .word order differences between languages.
The fil-tering algorithm can be efficiently interleaved withthe point generation algorithm so that SIMR runsin linear time and space with respect o the size ofthe input bitext.2.2 Translation Lexicon ExtractionSince bitext maps can represent crossing correspon-dences, they are more general than "alignments"(Melamed, 1996a).
For the same reason, bitextmaps allow a more general definition of token co-occurrence.
Early efforts at extracting translationlexicons from bitexts deemed two tokens to co-occurif they occurred in aligned sentence pairs (Gale andChurch, 1991).
SABLE counts two tokens as co-occurring if their point of correspondence lies withina short distance 8 of the interpolated bitext map inthe bitext space, as illustrated in Figure 1.
To en-sure that interpolation is well-defined, minimal setsof non-monotonic points of correspondence are re-placed by the lower left and upper right corners oftheir minimum enclosing rectangles (MERs).SABLE uses token co-occurrence statistics to in-duce an initial translation lexicon, using the methoddescribed in (Melamed, 1995).
The iterative filteringmodule then alternates between estimating the mostlikely translations among word tokens in the bitextand estimating the most likely translations betweenword types.
This re-estimation paradigm was pi-oneered by Brown et al (1993).
However, theirmodels were not designed for human inspection, andthough some have tried, it is not clear how to extracttranslation lexicons from their models (Wu and Xia,1995).
In contrast, SABLE automatically constructsan explicit ranslation lexicon, the lexicon consisting1oooooolOOOO:= \]000\[ ~ ,  3rd plateau:..3 ~ ~.
.
.
.
~ 2nd plateauloo ~ ......... 1st plateau10 ~ ?1|  i i i i0 2000 4000 6000 8000i1ooooEnt ry  Number12000Figure 2: Translation lexicon entries proposed bySABLE exhibit plateaus of likelihood.of word type pairs that are not filtered out duringthe re-estimation cycle.
Neither of the translationlexicon construction modules pay any attention toword order, so they work equally well for languagepairs with different word order.2.3 ThresholdingTranslation lexicon recall can be automatically com-puted with respect o the input bitext (Melamed,1996b), so SABLE users have the option of specify-ing the recall they desire in the output.
As always,there is a tradeoff between recall and precision; bydefault, SABLE will choose a likelihood thresholdthat is known to produce reasonably high precision.3 Eva luat ion  in a Techn ica l  Domain3.1 Materials EvaluatedThe SABLE system was run on a corpus compris-ing parallel versions of Sun Microsystems documen-tation ("Answerbooks") in French (219,158 words)and English (191,162 words).
As Melamed (1996b)observes, SABLE's output groups naturally accord-ing to "plateaus" of likelihood (see Figure 2).
Thetranslation lexicon obtained by running SABLEon the Answerbooks contained 6663 French-Englishcontent-word entries on the 2nd plateau or higher,including 5464 on the 3rd plateau or higher.
Table 1shows a sample of 20 entries elected at random fromthe Answerbook corpus output on the 3rd plateauand higher.
Exact matches, such as cpio/cpio orclock/clock, comprised roughly 18% of the system'soutput.In order to eliminate likely general usage entriesfrom the initial translation lexicon, we automat-ically filtered out all entries that appeared in aFrench-English machine-readable dictionary (MRD)(Cousin, Allain, and Love, 1991).
4071 entries re-mained on or above the 2nd likelihood plateau, in-cluding 3135 on the 3rd likelihood plateau or higher.342French Englishconstantesmulti-fen~tragerisqueextensionsexemplerelhch6rw- rrequspr6aiablecpiosontdefaultsfnalphab6tiqueactiv4emachinemettreconnect6sbernardsuperutilisateurconstantswindowsmayextensionssuchreleased17receivedfirstcpiowilldefaultsfnalphabeticallyactivatesworkstationturnsconnectedspunkyrootTable 1: Random sample of SABLE output on soft-ware manuals.In previous experiments on the Hansard corpus ofCanadian parl iamentary proceedings, SABLE haduncovered valid general usage entries that  were notpresent in the Collins MRD (e.g.
pointil lds/dotted).Since entries obtained from the Hansard corpus areunlikely to include relevant technical terms, we de-cided to test the efficacy of a second filtering step,deleting all entries that had also been obtained byrunning SABLE on the Hansards.
On the 2ndplateau or higher, 3030 entries passed both theCollins and the Hansard filters; 2224 remained onor above the 3rd plateau.Thus in total, we evaluated four lexicons de-rived from all combinations of two independent vari-ables: cutoff (after the 2nd plateau vs. after the 3rdplateau) and Hansards filter (with filter vs. without).Evaluations were performed on a random sample of100 entries from each lexicon variation, interleavingthe four samples to obscure any possible regularities.Thus from the evaluator's perspective the task ap-peared to involve a single sample of 400 translationlexicon entries.3.2 Evaluat ion ProcedureOur assessment of the system was designed to rea-sonably approximate the post-processing that wouldbe done in order to use this system for acquisitionof translation lexicons in a real-world setting, whichwould necessarily involve subjective judgments.
Wehired six fluent speakers of both French and Englishat the University of Maryland; they were briefed onthe general nature of the task, and given a data sheetcontaining the 400 candidate ntries (pairs contain-ing one French word and one English word) and a"multiple choice" style format for the annotations,along with the following instructions.1.
If the pair clearly cannot be of help inconstructing a glossary, circle "Invalid"and go on to the next pair.2.
If the pair can be of help in constructinga glossary, choose one of the following: 1V: The two words are of the "plainvanilla" type you might find in a bilin-gual dictionary.P: The pair is a case where a wordchanges its part of speech duringtranslation.
For example, "to haveprotection" in English is often trans-lated as %tre prot6g6" in Cana-dian parliamentary proceedings, sofor that domain the pair protec-tion/prot6g6 would be marked P.I: The pair is a case where a directtranslation is incomplete because thecomputer program only looked at sin-gle words.
For example, if French"imm6diatement" were paired withEnglish "right", you could select I be-cause the pair is almost certainly thecomputer's best but incomplete at-tempt to be pairing "imm4diatement"with "right away".3.
Then choose one or both of the following:?
Specific.
Leaving aside the relation-ship between the two words (yourchoice of P, V, or I), the word pairwould be of use in constructing a tech-nical glossary.?
General .
Leaving aside the rela-tionship between the two words (yourchoice of P, V, or I), the word pairwould be of use in constructing a gen-eral usage glossary.Notice that a word pair could makesense in both.
For example, "cor-beille/wastebasket" makes sense in thecomputer domain (in many populargraphical interfaces there is a wastebas-ket icon that is used for deleting files),but also in more general usage.
So in thiscase you could in fact decide to chooseboth "Specific" and "General".
If youcan't choose either "Specific" or "Gen-eral', chances are that you should recon-sider whether or not to mark this wordpair "Invalid".i Since part-of-speech tagging was used in the versionof SABLE that produced the candidates in this experi-ment, entries presented to the annotator also included aminimal form of part-of-speech information, e.g.
distin-guishing nouns from verbs.
The annotator was informedthat these annotations were the computer's best attemptto identify the part-of-speech for the words; it was sug-gested that they could be used as a hint as to why thatword pair had been proposed, if so desired, and otherwiseignored.3434.
If you're completely at a loss to decidewhether or not the word pair is valid, justput a slash through the number of theexample (the number at the beginning ofthe line) and go on to the next pair.Annotators also had the option of working electron-ically rather than on hardcopy.The assessment questionnaire was designed toelicit information primarily of two kinds.
First,we were concerned with the overall accuracy of themethod; that is, its ability to produce reasonablecandidate ntries whether they be general or domainspecific.
The "Invalid" category captures the sys-tem's mistakes on this dimension.
We also explic-itly annotated candidates that might be useful inconstructing a translation lexicon, but possibly re-quire further elaboration.
The V category capturescases that require minimal or no additional effort,and the P category covers cases where some addi-tional work might need to be done to accommodatethe part-of-speech divergence, depending on the ap-plication.
The I category captures cases where thecorrespondence that has been identified may not ap-ply directly at the single-world level, but nonethelessdoes capture potentially useful information.
Dailleet al (1994) also note the existence of "incomplete"cases in their results, but collapse them togetherwith "wrong" pairings.Second, we were concerned with domain speci-ficity.
Ultimately we intend to measure this in anobjective, quantitative way by comparing term us-age across corpora; however, for this study we reliedon human judgments.3.3 Use  o f  ContextMelamed (1996b) suggests that evaluation of trans-lation lexicons requires that judges have accessto bilingual concordances showing the contexts inwhich proposed word pairs appear; however, out-of-context judgments would be easier to obtain in bothexperimental nd real-world settings.
In a prelimi-nary evaluation, we had three annotators (one pro-fessional French/English translator and two gradu-ate students at the University of Pennsylvania) per-form a version of the annotation task just described:they annotated a set of entries containing the out-put of an earlier version of the SABLE system (onethat used aligned sub-sentence fragments to defineterm co-occurrence; cf.
Section 2.2).
No bilingualconcordances were made available to them.Analysis of the system's performance in this pi-lot study, however, as well as annotator commentsin a post-study questionnaire, confirmed that con-text is quite important.
In order to quantify its im:portance, we asked one of the pilot annotators torepeat the evaluation on the same items, this timegiving her access to context in the form of the bilin-gual concordances for each term pair.
These concor-dances contained up to the first ten instances of thatpair as used in context.
For example, given the paird@lacez/drag, one instance in that pair's bilingualconcordance would be:Maintenez SELECT enfoncd et d~placez  ledossier vers l' espace de travail .Press SELECT and drag  the folder onto theworkspace background .The instructions for the in-context evaluation spec-ify that the annotator should look at the contextfor every word pair, pointing out that "word pairsmay be used in unexpected ways in technical textand words you would not normally expect to be re-lated sometimes turn out to be related in a technicalcontext.
"Although we have data from only one annota-tor, Table 2 shows the clear differences between thetwo results.
2 In light of the results of the pilotstudy, therefore, our six annotators were given ac-cess to bilingual concordances for the entries theywere judging and instructed in their use as just de-scribed.4 Resu l t s4.1 Group Annotat ionsA "group annotation" was obtained for each candi-date translation lexicon entry based on agreementof at least three of the six annotators.
"Tie scores"or the absence of a 3-of-6 plurality were treated asthe absence of an annotation.
For example, if an en-try was annotated as "Invalid" by two annotators,marked as category V and Specific by two annota-tors, and marked as category P, Specific, and Gen-eral by the other two annotators, then the group an-notation would contain an "unclassified valid type"(since four annotators chose a valid type, but therewas no agreement by at least three on the specificsubclasification) and a "Specific" annotation (agreedon by four annotators).
All summary statistics arereported in terms of the group annotation.4.2 P rec i s ionSABLE's precision on the Answerbooks bitext issummarized in Figure 3.
3 Each of the percentagesbeing derived from a random sample of 100 observa-tions, we can compute confidence intervals under anormality assumption; if we assume that the obser-vations are independent, hen 95% confidence inter-vals are narrower than one twentieth of a percentagepoint for all the statistics computed.The results show that up to 89% of the translationlexicon entries produced by SABLE on or above the2Again, this sample of data was produced by an olderand less accurate version of SABLE, and therefore thepercentages should only be analyzed relative to eachother, not as absolute measures of performance.3The exact numbers gladly provided on request.344~1~1~1 ~'l ~' i~ Entries Domain-Specific OnlyOut-of-Context 39.519.2515.5\[ 57.75 29.75In-Context 46 .75 \ [ \ [  1 5  13 69.5 38General Only Usage I B?th23.5 123.25 3.5Table 2: Effect of in-context vs. out-of-context evaluation.
All numbers are in ~o.
n = 400.10075Eo~50 "5o~25V ~ P ~ I r-- - \]  Unclassified valid typeI::z:::: ::::::::iiiiiiiiwith no with noHansard Hansard Hansard Hansardfilter filter filter filter3rd plateau cutoff 2nd plateau cutoffFigure 3: Summary of filtered translation lexicon va-lidily statistics.3rd likelihood plateau "can be of help in constructinga glossary."
Up to 56% can be considered useful es-sentially as-is (the V category alone).
Including allentries on the 2nd plateau or higher provides bettercoverage, but reduces the fraction of useful entriesto 81%.
The fraction of entries that are useful as-isremains roughly the same, at 55%.
At both recalllevels, the extra Hansards-based filter had a detri-mental effect on precision.Note that these figures are based on translationlexicons from which many valid general usage en-tries have been filtered out (see Section 3).
We cancompute SABLE's precision on unfiltered transla-tion lexicons for this corpus by assuming that en-tries appearing in the Collins MRD are all correct.
4However, these are not the real figures of interesthere, because we are mainly concerned in this studywith the acquisition of domain-specific translationlexicons.4.3 Recal lFollowing Melamed (1996b), we adopt the followingapproach to measuring recall: the upper bound is de-fined by the number of different words in the bitext.Thus, perfect recall implies at least one entry con-taining each word in the corpus.
This is a much moreconservative metric than that used by Daille et al(1994), who report recall with respect o a relatively4Result: 88.4% precision at 37.0% recall or 93.7%precision at 30.4% recall.small, manually constructed reference set.
Althoughwe do not expect o achieve perfect recall on this cri-terion after general usage entries have been filteredout, the number is useful insofar as it provides asense of how recall for this corpus correlates withprecision.
We have no reason to expect this corre-lation to change across domain-specific and generallexicon entries.
For the unfiltered translation lexi-cons, recall on the 3rd likelihood plateau and abovewas 30.4%.
When all entries on and above the 2ndplateau were considered, recall improved to 37.0%.10075Q_E,o 50o25domain-specific only \ [~  both 1---\] general onlywith no with noHansard Hansard Hansard Hansardfilter filter filter filter3rd plateau cutoff 2nd plateau cutoffFigure 4: Summary of filtered translation lexicondomain-specificity statistics.HansardsFilter?%Plateau DomainCutoff Specific%GeneralUsageYes 3rd 82 37No 3rd 71 53Yes 2nd 66 27No 2nd 81 47%Both35352247Table 3: Domain-specificity of filtered translationlexicon entries.4.4 Domain  Specif icityFigure 4 demonstrates the effectiveness of the MRD-and corpus-based filters, with details in Table 3.
Ifwe assume that translation pairs in the Collins MRDare not specific to our chosen domain, then domain-specific translation lexicon entries constituted only345~-- \ ]A1  A2 A3 A4 A5 ~ 0.70 0.44 0.59 0.82 0.90 0.82 I 0.62 0.67 0.72 0.74 0.55 0.73 0.28 0.19 0.50 0.00 0.00 0.560.67 0.69 0.68 0.74 0.61 0.81Table 4: Infer-annotator agreement.49% of SABLE's unfiltered output on or above the2nd plateau and 41% on or above the 3rd plateau.The MRD filter increased this ratio to 81% and 71%,respectively.
As noted in Section 4.2, the second fil-ter, based on the Hansard bitext, reduced the overallaccuracy of the translation lexicons.
Its effects onthe proportion of domain-specific entries was mixed:an 11% increase for the entries more likely to be cor-rect, but a 15% decrease overall.
The corpus-basedfilter is certainly useful in the absence of an MRD.However, our results suggest hat combining filtersdoes not always help, and more research is neededto investigate optimal filter combination strategies.4.5 Cons istency of  Annotat ionsIn order to assess the consistency of annotation, wefollow Carletta (1996) in using Cohen's ~, a chance-corrected measure of inter-rater agreement.
Thestatistic was developed to distinguish among levelsof agreement such as "almost perfect, substantial,moderate, fair, slight, poor" (Agresti, 1992), andCarletta suggests that as a rule of thumb in the be-havioral sciences, values of g greater than .8 indicategood replicability, with values between .67 and .8 al-lowing tentative conclusions to be drawn.
For eachsuch comparison, four values of ~ were computed:~:1: agreement on the evaluation of whether or not apair should be immediately rejected or retained;~2: agreement, for the retained pairs, on the typeV, P, or I assigned to the pair;~a: agreement, for the retained pairs, on whether toclassify the pair as being useful for constructinga domain-specific glossary;g4: agreement, for the retained pairs, on whether toclassify the pair as being useful for constructinga general usage glossary.In each case, the computation of the agreementstatistic took into account hose cases, if any, wherethe annotator could not arrive at a decision for thiscase and opted simply to throw it out.
Resulting val-ues for inter-rater eliability are shown in Table 4;the six annotators are identified as A1, A2, .
.
.
A6;and each value of ~ reflects the comparison betweenthat annotator and the group annotation.With the exception of ~3, these values of n indi-cate that the reliability of the judgments i generallyreasonable, albeit not entirely beyond debate.
Theoutlandish values for ~3, despite high rates of abso-lute agreement on that dimension of annotation, areexplained by the fact that the ~ statistic is knownto be highly problematic as a measure of inter-raterreliability when one of the categories that can bechosen is overwhelmingly ikely (Grove et al, 1981;Spitznagel and Helzer, 1985).
Intuitively this is notsurprising: we designed the experiment to yield apredominance of domain-specific terms, by meansof the MRD and Hansards filters.
Our having suc-ceeded, there is a very high probability that the"Specific" annotation will be selected by any twoannotators, because it appears so very frequently;as a result the actual agreement rate for that anno-tation doesn't actually look all that different fromwhat one would get by chance, and so the ~ valuesare low.
The values of ~3 for annotators 4 and 5emphasize quite clearly that ~ is measuring not thelevel of absolute agreement, hut the distinguishabil-ity of that level of agreement from chance.5 Conc lus ionIn this paper, we have investigated the application ofSABLE, a turn-key translation lexicon constructionsystem for non-technical users, to the problem ofidentifying domain-specific word translations givendomain-specific corpora of limited size.
Evaluatedon a very small (400,000 word) corpus, the systemshows real promise as a method of processing smalldomain-specific corpora in order to propose candi-date single-word translations: once likely general us-age terms are automatically filtered out, the systemobtains precision up to 89% at levels of recall veryconservatively estimated in the range of 30-40% ondomain-specific terms.Of the proposed entries not immediately suitablefor inclusion in a translation lexicon, many representpart-of-speech divergences (of the protect/protdggvariety) and a smaller number incomplete ntries(of the immddiatement/right variety) that wouldnonetheless be helpful if used as the basis for a bilin-gual concordance search - -  for example, a search forFrench segments containing immddiatemeut in thevicinity of English segments containing right wouldmost likely yield up the obvious correspondence b -tween immgdiatement and right away.
Going be-yond single-word correspondences, however, is a pri-ority for future work.6 AcknowledgmentsThe authors wish to acknowledge the support of SunMicrosystems Laboratories, particularly the assis-tance of Gary Adams, Cookie Callahan, and BobKuhns, as well as useful input from Bonnie Dorr,Ralph Grishman, Marti Hearst, Doug Oard, andthree anonymous reviewers.
Melamed also acknowl-edges grants ARPA N00014-90-J-1863 and ARPAN6600194C 6043.346ReferencesAlan Agresti.
1992.
Modeling patterns of agreementand disagreement.
Statistical methods in medicalresearch, 1:201-218.P.
F. Brown, S. Della Pietra, V. Della Pietra, andR.
Mercer.
1993.
"The Mathematics of Statisti-cal Machine Translation: Parameter Estimation".Computational Linguistics 19:2.Jean Carletta.
1996.
Assessing agreement on clas-sification tasks: the Kappa statistic.
Computa-tional Linguistics, 22(2):249-254, June.S.
Chen.
1993.
"Aligning Sentences in BilingualCorpora Using Lexical Information".
Proceedingsof the 31st Annual Meeting of the Association forComputational Linguistics, Columbus, OH.K.
W. Church.
1993.
"Char_align: A Program forAligning Parallel Texts at the Character Level".Proceedings of the 31st Annual Meeting of the As-sociation for Computational Linguistics, Colum-bus, OH.P.
H. Cousin, L. Sinclair, J. F. Allain, and C. E.Love.
1991.
The Collins Paperback French Dic-tionary.
Harper Collins Publishers, Glasgow.Ido Dagan and Ken W. Church.
1994.
TERMIGHT:Identifying and translating technical terminology.In Proceedings of the Fourth ACL Conference onApplied Natural Language Processing (13-15 Oc-tober 1994, Stuttgart).
Association for Computa-tional Linguistics, October.I.
Dagan, K. Church, and W. Gale.
1993.
"RobustWord Alignment for Machine Aided Translation".Proceedings of the Workshop on Very Large Cor-pora: Academic and Industrial Perspectives, avail-able from the ACL.Bdatrice Daille.
1994.
Combined approach for termi-nology extraction: lexical statistics and linguisticfiltering.
Ph.D. thesis, University Paris 7.Bdatrice Daille.
1996.
Study and implementationof combined techniques for automatic extractionof terminology.
In Judith Klavans and PhilipResnik, editors, The Balancing Act: CombiningSymbolic and Statistical Approaches to Language.MIT Press.Mark Davis.
1996.
"New experiments in cross-language text retrieval at NMSU's Computing Re-search Lab".
Fifth Text Retrieval Conference(TREC-5).
NIST.Mark Davis and Ted Dunning.
1995.
"A TRECevaluation of query translation methods for multi-lingual text retrieval".
Fourth Text Retrieval Con-ference (TREC-4).
NIST.Mark Davis, Ted Dunning, and William Ogden.1995.
Text alignment in the real world: improv-ing alignments of noisy translation using commonlexical features, string matching strategies, and n-gram comparisons.
In EACL-95.W.
Gale and K. W. Church.
1991.
"IdentifyingWord Correspondences in Parallel Texts".
Pro-ceedings of the DARPA SNL Workshop, 1991.W.
Grove, N. Andreasen, P. McDonald-Scott,M.
Keller, and R. Shapiro.
1981.
Reliability stud-ies of psychiatric diagnosis.
Archives of GeneralPsychiatry, 38, April.I.
Dan Melamed, 1995.
Automatic evaluation anduniform filter cascades for inducing n-best trans-lation lexicons.
In Proceedings of the Third Work-shop on Very Large Corpora, Cambridge, Mas-sachusetts.I.
Dan Melamed.
1996a.
A geometric approach tomapping bitext correspondence.
In Conference onEmpirical Methods in Natural Language Process-ing, Philadelphia, Pennsylvania.I.
Dan Melamed.
1996b.
Automatic onstruction ofclean broad-coverage translation lexicons.
In Pro-ceedings of the 2nd Conference of the Associationfor Machine Translation in the Americas, Mon-treal, Canada.I.
Dan Melamed.
1996c.
Porting SIMR to new lan-guage pairs.
IRCS Technical Report 96-26.
Uni-versity of Pennsylvania.I.
Dan Melamed.
1997.
A scalable architecture forbilingual lexicography.
Dept.
of Computer andInformation Science Technical Report MS-CIS-97-01.
University of Pennsylvania.Douglas W. Oard.
1997.
"Cross-Language TextRetrieval Research in the USA".
Third DELOSWorkshop.
European Research Consortium for In-formatics and Mathematics.
March.M.
Simard, G. F. Foster and P. Isabelle.
1992.
"Us-ing Cognates to Align Sentences in Bilingual Cor-pora".
In Proceedings of the Fourth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, Montreal, Canada.Frank Smadja.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguistics,19(1):143-177.Frank Smadja, Kathleen McKeown, and VasileiosHatzivassiloglou.
1996.
Translating collocationsfor bilingual lexicons: A statistical approach.Computational Linguistics, 22(1), March.E.
Spitznagel and J. Helzer.
"A proposed solutionto the base rate problem in the kappa statistic".Archives of General Psychiatry, 42.
July, 1985.D.
Wu and X. Xia.
1994.
"Learning an English-Chinese Lexicon from a Parallel Corpus".
Pro-ceedings of the First Conference of the Associ-ation for Machine Translation in the Americas,Columbia, MD.347
