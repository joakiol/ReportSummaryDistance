MUC-4 EVALUATION METRICSNancy Chinchor, Ph .D.Science Applications International Corporation10260 Campus Point Drive, M/S A2-FSan Diego, CA 9212 1chinchor@esosun .css.gov(619) 458-2614INTRODUCTIONThe MUC-4 evaluation metrics measure the performance of the message understanding systems .
This paperdescribes the scoring algorithms used to arrive at the metrics as well as the improvements that were made to th eMUC-3 methods .
MUC-4 evaluation metrics were stricter than those used in MUC-3.
Given the differences in scor-ing between MUC-3 and MUC-4, the MUC-4 systems' scores represent a larger improvement over MUC-3 perfor-mance than the numbers themselves suggest .The major improvements in the scoring of MUC-4 were the automation of the scoring of set fill slots, partia lautomation of the scoring of string fill slots, content-based mapping enforced across the board, the focus on the AL LTEMPLATES score as opposed to the MATCHED/MISSING score in MUC-3, the exclusion of the template idscores from the score tallies, and the addition of the object level scores, string fills only scores, text filtering scores ,and F-measures .
These improvements and their effects on the scores are discussed in detail in this paper.SCORE REPORTThe MUC-4 Scoring System produces score reports in various formats .
These reports show the scores forthe templates and messages in the test set .
Varying amounts of detail can be reported .
The scores that are of the mostinterest are those that appear in the comprehensive summary report.
Figure 1 shows a sample summary score report.The rows and columns of this report are explained below.Scoring CategoriesThe basic scoring categories are located at the top of the score report These categories are defined in Tabl e1 .
The scoring program determines the scoring category for each system response .
Depending on the type of slotbeing scored, the program can either determine the category automatically or prompt the user to determine th eamount of credit the response should be assigned .?
If the response and the key are deemed to be equivalent, then the fill is assigned the cate-gory of correct (COR).?
If partial credit can be given, the category is partial (PAR).?
If the key and response simply do not match, the response is assigned an incorrect (INC) .?
If the key has a fill and the response has no corresponding fill, the response is missing(MIS).?
If the response has a fill which has no corresponding fill in the key, the response is spuri-ous (SPU) .?
If the key and response are both left intentionally blank, then the response is scored asnoncommittal (NON) .22POS SPU PRE1091 0653820664100000000010010083SO06988751008383SLOTtemplate-Id.
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.. .. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.. .. .
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.. .. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.Inc-be.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .. .
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.. .
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .inc-stage.
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.. .
.
.
.
.
.
.. .
.
.. .
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.. .
.. .
.
.
.. .
.
.
.
.
.
.
.
.. .
.
.
.
.. .
.
.
.
.
.
.. .
.
.inc-instr-type.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.. .
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
..perp-Ind-id.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.. .perp-org-conf.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.. .
.. .
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.phys-tgt-type.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.
.
.
.
.
.. .
.phys-tgt-natlonphys-tgt-total-num.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.. .
.
.
.
.
.
.
.
.. .
.
.
.
.. .
.. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.hum-tgt-deschum-tgt-num.
.
.. .
.... .... .. .
.
... .
.
.
.
.. .
.
.
.
.
.
.
.
.. .
.
.
.
.hum-tgt-effectACT COR PAR10 8 08 4 38 7 06 4 13 3 03 3 06 S 0INC ICR IPA0 0 01 0 01 0 00 0 00 0 01 0 00 0 00 0 01 0 01 0 01 0 0MIS NON REC20 70OVG FAL201 2000s000660SS00040002106228 SOs10SS 83inc-tota lphys-tgt-total.
.. .
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.
.
.. .
.
.
.
.5032 02419 S 0 0 828 590>< 0 `>'8479MATCHED/MISSINGMATCHED ONLYSET FILLS ONLYTEXT FILTERIN GF-MEASURES12 3>106k10 6637105:.
:?
::84 554.:: .
:.43287?105 84I S90000?1011904P&R69.5282 828282:: ..SO 70 8 1100 882P&R69.21112 20P&2R69.8Figure 1 : Sample Score Report2 3q Correctq Partialq Incorrec tq Non-committalq Spuriousq Missingresponse = keyresponse = keyresponse = keykey and response are both blan kkey is blank and response is notresponse is blank and key is notTable 1 : Scoring CategoriesIn Figure 1, the two columns titled ICR (interactive correct) and IPA (interactive partial) indicate the result sof interactive scoring .
Interactive scoring occurs when the scoring system finds a mismatch that it cannot automati -cally resolve .
It queries the user for the amount of credit to be assigned .
The number of fills that the user assigns to thecategory of correct appears in the ICR column; the number of fills assigned partial credit by the user appears in theIPA column .In Figure 1, the two columns labelled possible (POS) and actual (ACT) contain the tallies of the numbers o fslots filled in the key and response, respectively .
Possible and actual are used in the computation of the evaluationmetrics .
Possible is the sum of the correct, partial, incorrect, and missing .
Actual is the sum of the correct, partial ,incorrect, and spurious.Evaluation MetricsThe evaluation metrics were adapted from the field of Information Retrieval (IR) and extended for MUC .They measure four different aspects of performance and an overall combined view of performance .
The four evalua-tion metrics of recall, precision, overgeneration, and fallout are calculated for the slots and summary score rows (se eTable 2).
These are listed in the four rightmost columns of the score report in Figure 1 .
The fifth metric, the F-mea-sure, is a combined score for the entire system and is listed at the bottom of the score report .Table 2: Evaluation Metric sRecall (REC) is the percentage of possible answers which were correct .
Precision (PRE) is the percentage ofactual answers given which were correct A system has a high recall score if it does well relative to the number of slo tfills in the key.
A system has a high precision score if it does well relative to the number of slot fills it attempted .corr~eet partial z O.b)possibleerect + (partial x 0.81actualgpuriougactualincorrect + snuriouspossible incorrectrecallprecisionover-generationfallout24In IR, a common way of representing the characteristic performance of systems is in a precision-recal lgraph.
Normally as recall goes up, precision tends to go down and vice versa [1] .
One approach to improving recall isto increase the system's generation of slot fills .
To avoid overpopulation of the template database by the messageunderstanding systems, we introduced the measure of overgeneration .
Overgeneration (OVG) measures the percent-age of the actual attempted fills that were spurious .Fallout (FAL) is a measure of the false positive rate for slots with fills that come from a finite set.
Fallout isthe tendency for a system to choose incorrect responses as the number of possible responses increases .
Fallout is cal-culated for all of the set fill slots listed in the score report in Figure 1 and is shown in the last column on the right .Fallout can be calculated for the SET FILLS ONLY row because that row contains the summary tallies for the set fillslots .
The TEXT FILTERING row discussed later contains a score for fallout because the text filtering problem als ohas a finite set of responses possible .These four measures of recall, precision, overgeneration, and fallout characterize different aspects of syste mperformance.
The measures of recall and precision have been the central focus for analysis of the results .
Overgener-ation is a measure which should be kept under a certain value .
Fallout was rarely used in the analyses done of theresults.
It is difficult to rank the systems since the measures of recall and precision are often equally important ye tnegatively correlated.
In IR, a method was developed for combining the measures of recall and precision to get a sin -gle measure.
In MUC-4, we use van Rijsbergen's F-measure [1, 2] for this purpose .The F-measure provides a way of combining recall and precision to get a single measure which fall sbetween recall and precision .
Recall and precision can have relative weights in the calculation of the F-measure giv-ing it the flexibility to be used for different applications .
The formula for calculating the F-measure is(132 +1 .0)XPX R132 xP+ Rwhere P is precision, R is recall, and 13 is the relative importance given to recall over precision .
If recall and precisionare of equal weight, 13 = 1 .0.
For recall half as important as precision, 13 = 0.5 .
For recall twice as important as preci-sion,13 = 2 .0.The F-measure is higher if the values of recall and precision are more towards the center of the precision -recall graph than at the extremes and their sums are the same.
So, for 3 = 1 .0, a system which has recall of 50% an dprecision of 50% has a higher F-measure than a system which has recall of 20% and precision of 80%.
This behavioris exactly what we want from a single measure .The F-measures are reported in the bottom row of the summary score report in Figure 1 .
The F-measure withrecall and precision weighted equally is listed as "P&R ."
The F-measure with precision twice as important as recall islisted as "2P&R ."
The F-measure with precision half as important as recall is listed as "P&2R ."
The F-measure is cal -culated from the recall and precision values in the ALL TEMPLATES row.
Note that the recall and precision value sin the ALL TEMPLATES row are rounded integers and that this causes a slight inaccuracy in the F-measures .
Thevalues used for calculating statistical significance of results are floating point values all the way through the calcula-tions.
Those more accurate values appear in the paper "The Statistical Significance of the MUC-4 Results " and inAppendix G of these proceedings .Summary RowsThe four rows labeled "inc-total," "perp-total," "phys-tgt-total," and "hum-tgt-total" in the summary scor ereport in Figure 1 show the subtotals for associated groups of slots referred to as "objects . "
These are object levelscores for the incident, perpetrator, physical target, and human target .
They are the sums of the scores shown for th eF=25individual slots associated with the object as designated by the first part of the individual slot labels .
The template forMUC-4 was designed as a transition from a flat template to an object-oriented template .
Although referred to asobject-oriented, the template is not strictly object-oriented, but rather serves as a data representation upon which a nobject-oriented system could be built[3] .
However, no object-oriented database system was developed using this tem-plate as a basis.The four summary rows in the score report labelled "MATCHED/MISSING," "MATCHED/SPURIOUS, ""MATCHED ONLY," and "ALL TEMPLATES" show the accumulated tallies obtained by scoring spurious and miss -ing templates in different manners .
Each message can cause multiple templates to be generated depending on thenumber of terrorist incidents it reports.
The keys and responses may not agree in the number of templates generatedor the content-based mapping restrictions may not allow generated key and response templates to be mapped to eac hother.
These cases lead to spurious and/or missing templates.
There are differing views as to how much systemsshould be penalized for spurious or missing templates depending upon the requirements of the application .
These dif-fering views have lead us to provide the four ways of scoring spurious and missing information as outlined in Table 3 .qMatched OnlyMissing and spurious templates scored in template-id slot onl yqMatched/MissingMissing template slots scored as missingSpurious templates scored only in template-id slotqMatched/SpuriousSpurious template slots scored as spuriousMissing templates scored only in template-id slotqAll TemplatesMissing template slots scored as missingSpurious template slots scored as spuriousTable 3 : Manners of ScoringThe MATCHED ONLY manner of scoring penalizes the least for missing and spurious templates by scorin gthem only in the template id slot.
This template id score does not impact the overall score because the template id slo tis not included in the summary tallies; the tallies only include the other individual slots .
The MATCHED/MISSINGmethod scores the individual slot fills that should have been in the missing template as missing and scores the tem-plate as missing in the template id slot ; it does not penalize for slot fills in spurious templates except to score the spu-rious template in the template id slot .
MATCHED/SPURIOUS, on the other hand, penalizes for the individual slo tfills in the spurious templates, but does not penalize for the missing slot fills in the missing templates .
ALL TEM-PLATES is the strictest manner of scoring because it penalizes for both the slot fills missing in the missing template sand the slots filled in the spurious templates .
The metrics calculated based on the scores in the ALL TEMPLATESrow are the official MUC-4 scores .These four manners of scoring provide four points defining a rectangle on a precision-recall graph which werefer to as the "region of performance" for a system (see Figure 2) .
At one time, we thought that it would be useful to26compare the position of the center of this rectangle across systems, but later realized that two systems could have th esame centers but very different size rectangles .
Plotting the entire region of performance for each system does provid ea useful comparison of systems .Figure 2: Region of PerformanceIn Figure 1, the score report contains two summary rows (SET FILLS ONLY and STRING FILLS ONLY )which give tallies for a subset of the slots based on the type of fill the slot can take.
These rows give tallies that showthe system's performance on these two types of slots: set fill slots and string fill lots .
Set fill slots take a fill from afinite set specified in a configuration file .
String fill slots take a fill that is a string from a potentially infinite set .Text FilteringThe purpose of the text filtering row is to report how well systems distinguish relevant and irrelevant mes-sages.
The scoring program keeps track of how many times each of the situations in the contingency table arises for asystem (see Table 4).
It then uses those values to calculate the entries in the TEXT FILTERING row.
The evaluationmetrics are calculated for the row as indicated by the formulas at the bottom of Table 4 .
An analysis of the text filter-ing results appears elsewhere in these proceedings .IMPROVEMENTS OVER MUC- 3The major improvements in the scoring of MUC-4 included:?
automating the scoring as effectively as possible?
restricting the mapping of templates to cases where particular slots matched in content asopposed to mapping only according to an optimized scor e?
a focus on the ALL TEMPLATES score as opposed to the MATCHED/MISSING score i nMUC-3REGION OF PERFORMANCEMATCHED/MISSINGMATCHED ONLYI?
nALL TEMPLATESMATCHED/SPURIOUSRECAL L3MVWa.27?
the exclusion of template id scores from the summary score tallie s?
the inclusion of more summary information including object level scores, string fills onl yscores, text filtering scores, and F-measures.These changes are interdependent ; they interact in ways that affect the overall scores of systems and serve to makeMUC-4 a more demanding evaluation than MUC-3 .Table 4 : Text FilteringThe complete automation of the scoring of set fill slots was possible due to the information in a slot configu-ration file which told the program the hierarchical structure of the set fills .
If a response exactly matches the key, it isscored as correct.
If a response is a more general set fill element than the key according to the pre-specified hierarchy ,it is scored as partially correct.
If the response cannot be scored as correct or partially correct by these criteria then th eset fill is scored as incorrect .
All set fills can thus be automatically scored .
Often, however, the set fill is cross-refer-enced to another slot which is a string fill .
The scoring of string fills cannot be totally automated .
Instead the scoringprogram refers to the history of the interactive scoring of the cross-referenced slot, and with that information, it the ndetermines the score for the set fill slot which cross-references the string fill slot .The scoring of the string fill slots was partially automated by using two methods .
In the first method, usedfor mapping purposes, strings were considered correct if there was a one-word overlap and the word was not from ashort list of premodifiers.
In the second method, used for scoring purposes, some mismatching string fills could b ematched automatically by stripping these premodifiers from the key and response and seeing if the remaining materia lmatched.
Other mismatching string fills caused the user to be queried for the score .
The automation of the set fill andstring fill scoring was critical to the functioning of the content-based mapping.The content-based mapping restrictions were added to MUC-4 to prevent fortuitous mappings whic hoccurred in MUC-3.
In MUC-3, templates were mapped to each other based on a simple optimization of scores .Sometimes the optimal score was the result of a lucky mapping which was not really the most appropriate mapping .Relevant IsCorrectIrrelevant isCorrectDecides Relevant acbdSa+bcoldDecides Irrelevanta+c b+d a+b+c+d = nPOS ACT COR PAR INC ICR IPA SPU MIS NONTextFiltering a+c a+b a - - - - b c dRecall = a/(a+c)Precision = a/(a+b)Overgeneration = b/(a+b )Fallout = b/(b+d )2 8Certain slots such as incident type were considered essential for the mapping to occur in MUC-4 .
The mappingrestrictions can be specified in the scorer's configuration file using a primitive logic .
For the MUC-4 testing, the tem-plates must have at least a partial match on the incident type and at least one of the following slots :?
physical target identifier?
physical target typ e?
human target name?
human target description?
human target type?
perpetrator individual identifier?
perpetrator organization identifie rThe content-based mapping restrictions could result in systems with sparse templates having few or no tem -plates mapped.
When a template does not map, the result is one missing and one spurious template.
This kind of pen-alty is severe when the ALL TEMPLATES row is the official score, because the slots in the unmapped templates allcount against the system as either missing or spurious.
This aspect of the scoring was one of the main reasons thatMUC-4 was more demanding than MUC-3.The focus on the ALL TEMPLATES score as opposed to the MATCHED/MISSING score in MUC-3 mean tthat the strictest scores for a system were its official scores .
So even if a system's official scores were the same forMUC-3 and MUC-4, the system had improved in MUC-4 .
Additionally, the scores for the template id row were notincluded in the summary row tallies in MUC-4 as they had been in MUC-3 .
Previously, systems were getting extracredit for the optimal mapping.
This bonus was taken away by the exclusion of the template id scores from the scoretallies in MUC-4.In addition to the more demanding scoring, MUC-4 also measured more information about system perfor -mance .
Object level scores were added to see how well the systems did on different groupings of slots concerning theincident, perpetrator, physical target, and human target.
Also, the score for the string fill slots was tallied as a compar -ison with the score for set fill slots that was already there in MUC-3.
The text filtering scores gave additional informa-tion on the capabilities of systems to determine relevancy .
The F-measures combined recall and precision to give asingle measure of performance for the systems .SUMMARYThe evaluation metrics used in MUC-4 gave a stricter and more complete view of the performance of th esystems than the metrics used in MUC-3 .
The improved overall numerical scores of the systems under these more dif -ficult scoring conditions indicate that the state of the art has moved forward with MUC-4 .REFERENCES[1] Frakes, W.B .and R. Baeza-Yates (eds .)
(1992) Information Retrieval : Data Structures & Algorithms .Englewood Cliffs: Prentice Hall .
[2] Van Rijsbergen, CJ.
(1979) Information Retrieval.
London: Butterworths.
[3] Nierstrasz, Oscar (1989) "A Survey of Object-Oriented Concepts" in W .
Kim and F. H. Lochovsky (Eds .
)Obiect-Oriented Concepts.
Databases.
and Applications.
New York: Addison-Wesley.29
