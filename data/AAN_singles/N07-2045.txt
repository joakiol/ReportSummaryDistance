Proceedings of NAACL HLT 2007, Companion Volume, pages 177?180,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsLanguage Modeling for Determiner SelectionJenine Turner and Eugene CharniakDepartment of Computer ScienceBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{jenine|ec}@cs.brown.eduAbstractWe present a method for automatic deter-miner selection, based on an existing lan-guage model.
We train on the Penn Tree-bank and also use additional data from theNorth American News Text Corpus.
Ourresults are a significant improvement overprevious best.1 IntroductionDeterminer placement (choosing if a noun phraseneeds a determiner, and if so, which one) is anon-trivial problem in several language processingtasks.
While context beyond that of the current sen-tence can sometimes be necessary, native speakersof languages with determiners can select determin-ers quite well for most NPs.
Native speakers of lan-guages without determiners have a much more diffi-cult time.Automating determiner selection is helpful in sev-eral applications.
A determiner selection programcan aid in Machine Translation of determiner-freelanguages (by adding determiners after the text hasbeen translated), correct English text written by non-native speakers (Lee, 2004), and choose determinersfor text generation programs.Early work on determiner selection focuses onrule-based systems (Gawronska, 1990; Murata andNagao, 1993; Bond and Ogura, 1994; Heine, 1998).Knight and Chander (1994) use decision trees tochoose between the and a/an, ignoring NPs with nodeterminer, and achieve 78% accuracy on their WallStreet Journal corpus.
(Deciding between a and anis a trivial postprocessing step.
)Minnen et al (2000) use a memory-based learner(Daelemans et al, 2000) to choose determiners ofbase noun phrases.
They choose between no deter-miner (hencefore null), the, and a/an.
They use syn-tactic features (head of the NP, part-of-speech tag ofthe head of the NP, functional tag of the head of theNP, category of the constituent embedding the NP,and functional tag of the constituent embedding theNP), whether the head is a mass or count noun andsemantic classes of the head of the NP (Ikehara etal., 1991).
They report 83.58% accuracy.In this paper, we use the Charniak language model(Charniak, 2001) for determiner selection.
Our ap-proach significantly improves upon the work of Min-nen et al (2000).
We also use additional automat-ically parsed data from the North American NewsText Corpus (Graff, 1995), further improving our re-sults.2 The Immediate-Head Parsing ModelThe language model we use is described in (Char-niak, 2001).
It is based upon a parser that, for asentence s, tries to find the parse pi defined as:argmaxpip(pi | s) = argmaxpip(pi, s) (1)The parser can be turned into a language model p(s)describing the probability distribution over all pos-sible strings s in the language, by considering allparses pi of s:p(s) =?pip(pi, s) (2)177Here p(pi, s) is zero if the yield of pi 6= s.The parsing model assigns a probability to a parsepi by a top-down process.
For each constituent c inpi it first guesses the pre-terminal of c, t(c) (t for?tag?
), then the lexical head of c, h(c), and then theexpansion of c into further constituents e(c).
Thusthe probability of a parse is given by the equationp(pi) =?c?pip(t(c) | l(c),H(c))?
p(h(c) | t(c), l(c),H(c))?
p(e(c) | l(c), t(c), h(c),H(c))where l(c) is the label of c (e.g., whether it is a nounphrase NP, verb phrase, etc.)
and H(c) is the rel-evant history of c ?
information outside c deemedimportant in determining the probability in question.H(c) approximately consists of the label, head, andhead-part-of-speech for the parent of c: m(c), i(c),and u(c) respectively and also a secondary head(e.g., in ?Monday Night Football?
Monday wouldbe conditioned on both the head of the noun-phrase?Football?
and the secondary head ?Night?
).It is usually clear to which constituent we are re-ferring and we omit the (c) in, e.g., h(c).
In this no-tation the above equation takes the following form:p(pi) =?c?pip(t | l,m, u, i) ?
p(h | t, l,m, u, i)?
p(e | l, t, h,m, u).
(3)Next we describe how we assign a probability tothe expansion e of a constituent.
We break up a tra-ditional probabilistic context-free grammar (PCFG)rule into a left-hand side with a label l(c) drawnfrom the non-terminal symbols of our grammar, anda right-hand side that is a sequence of one or moresuch symbols.
For each expansion we distinguishone of the right-hand side labels as the ?middle?
or?head?
symbol M(c).
M(c) is the constituent fromwhich the head lexical item h is obtained accordingto deterministic rules that pick the head of a con-stituent from among the heads of its children.
To theleft of M is a sequence of one or more left labelsLi(c) including the special termination symbol ?,which indicates that there are no more symbols tothe left.
We do the same for the labels to the right,Ri(c).
Thus, an expansion e(c) looks like:l ?
?Lm...L1MR1...Rn?.
(4)The expansion is generated first by guessing M ,then in order L1 through Lm+1 (= ?
), and then, R1through Rn+1.Let us turn to how this works in the case of de-terminer recovery.
Consider a noun-phrase, which,missing a possible determiner, is simply ?FBI.?
Thelanguage model is interested in the probability of thestrings ?the FBI,?
?a/an FBI?
and ?FBI.?
The ver-sion with the highest probability will dictate the de-terminer, or lack thereof.
So, consider (most of) theprobability calculation for the answer ?the FBI:?p(NNP | H) ?
p(FBI | NNP,H)?
p(det | FBI,NNP,H)?
p(?
| det,FBI,NNP,H)?
p(the | det,FBI,NNP,H) (5)Of these, the first two terms, the probability thatthe head will be an NNP (a singular proper noun)and the probability that it will be ?FBI?, are sharedby all three competitors, null, the, and a/an.
Theseterms can therefore be ignored when we only wish toidentify the competitor with the highest probability.The next two probabilities state that the noun-phrasecontains a determiner to the left of ?FBI?
and thatthe determiner is the last constituent of the left-handside.
The last of the probabilities states that the de-terminer in question is the.
Ignoring the first twoprobabilities, the critical probabilities for ?the FBI?are:p(det | FBI,NNP,H)?
p(?
| det,FBI,NNP,H)?
p(the | det,FBI,NNP,H) (6)Conversely, to evaluate the probability of the noun-phrase ?FBI?
?
i.e., no determiner, we evaluate:p(?
| FBI,NNP,H) (7)We ask the probability of the NP stopping immedi-ately to the left of ?FBI.?
For ?a/an FBI?
we evalu-ate:p(det | FBI,NNP,H)?
p(?
| det,FBI,NNP,H) (8)?
(p(a | det,FBI,NNP,H) +p(an | det,FBI,NNP,H))178Test Data Method Accuracyleave-one-out Minnen et al 83.58%Language Model (LM) 86.74%tenfold on development LM 84.72%LM trained on WSJ + 3 million words of NANC 85.83%LM trained on WSJ + 10 million words of NANC 86.36%LM trained on WSJ + 20 million words of NANC 86.64%tenfold on test LM trained on WSJ + 20 million words of NANC 86.63%Table 1: Results of classificationThis equation is very similar to Equation 6 (theequation for ?the FBI?, except the term for the prob-ability of the is replaced by the sum of the probabil-ities for a and an.To choose between null, the, or a/an, the languagemodel in effect constructs Equations 6, 7 and 8 andwe pick the one that has the highest probability.2.1 Training the modelAs with (Minnen et al, 2000), we train the lan-guage model on the Penn Treebank (Marcus et al,1993).
As far as we know, language modelingalways improves with additional training data, sowe add data from the North American News TextCorpus (NANC) (Graff, 1995) automatically parsedwith the Charniak parser (McClosky et al, 2006) totrain our language model on up to 20 million addi-tional words.3 Results and DiscussionThe best results of Minnen et al (2000) are usingleave-one-out cross-validation.
We also test our lan-guage model using leave-one-out cross-validationon the Penn Treebank (Marcus et al, 1993) (WSJ),giving us 86.74% accuracy (see Table 1).Leave-one-out cross-validation does not makesense in this case.
When choosing determiners, wecan train a language model on similar data, but noton other NPs in the article.
Therefore, for the restof our tests, we use tenfold cross-validation.
Thedifference between leave-one-out and tenfold cross-validation is due to the co-occurrence of NPs withinan article.
Church (2000) shows that a word appearswith much higher probability when seen elsewherein an article.
Thus, a rare NP might be unseen intenfold cross-validation, but seen in leave-one-out.For each of our sets in tenfold cross validation,we use 80% of the Penn Treebank for training, 10%for development, and 10% for testing.
The divisionsoccur at article boundaries.
On our development setwith tenfold cross-validation, we get 84.72% accu-racy using the language model (Table 1).As expected, we achieve significant improvementwhen adding NANC data over training on data fromthe Penn Treebank alone (Table 1).
With 20 mil-lion additional words, we seem to be approachingan upper bound on the language model features.
Weobtain improvement despite the fact that the parseswere automatic, but there may have been errors indeterminer selection due to parsing error.Table 2 gives ?error?
examples.
Some errors arewrong (either grammatically or yielding a signifi-cantly different interpretation), but some ?incorrect?answers are reasonable possibilities.
Furthermore,even all the text of the article is not enough for clas-sification at times.
In particular note Example 5,where unless you know whether IBM was the worldleader or simply one of the world leaders at the timeof the article, no additional context would help.4 Conclusions and Future WorkWith the Charniak (Charniak, 2001) languagemodel, our results exceed those of the previous best(Minnen et al, 2000) on the determiner selectiontask.
This shows the benefits of the language modelfeatures in determining the most grammatical deter-miner to use in a noun phrase.
Such a languagemodel looks at much of the structure in individualsentences, but there may be additional features thatcould improve performance.
There is a high rate ofambiguity for many of the misclassified sentences.The success of using a state-of-the-art language179Guess Correct Sentencethe null (1) The computers were crude by today?s standards.null the (2) In addition, the Apple II was an affordable $1,298.
(3) Highway officials insist the ornamental railings on older bridges aren?t strong enoughto prevent vehicles from crashing through.a/an the (4) The new carrier can tote as many as four cups at once.
(5) IBM, the world leader in computers, didn?t offer its first PCuntil August 1981 as many other companies entered the market.the a/an (6) In addition, the Apple II was an affordable $1,298.
(7) ?The primary purpose of a railing is to contain a vehicle and not to providea scenic view,?
says Jack White, a planner with the Indiana Highway Department.a/an null (8) Crude as they were, these early PCs triggered explosive product development indesktop models for the home and office.Table 2: Examples of ?errors?model in determiner selection also suggests that onewould be helpful in making other decisions in thesurface realization stage of text generation.
This isan avenue worth exploring.AcknowledgementsThis work was supported by NSF PIRE grant OISE-0530118.We would also like to thank the BLLIP team for their comments.ReferencesFrancis Bond and Kentaro Ogura.
1994.
Countabilityand number in Japanese-to-English machine transla-tion.
In 15th International Conference on Computa-tional Linguistics, pages 32?38.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics.
The Association for Computational Linguistics.Kenneth Church.
2000.
Empirical estimates of adap-tation: The chance of Two Noriegas is closer to p/2than p2.
In Proceedings of COLING-2000.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2000.
TiMBL: Tilburg memorybased learner, version 3.0, reference guide.
ILK Tech-nical Report ILK-0001, ILK, Tilburg University, TheNetherlands.Barbara Gawronska.
1990.
Translation great problem.In Proceedings of the 13th International Conferenceon Computational Linguistics.David Graff.
1995.
North American News Text Corpus.Linguistic Data Consortium.
LDC95T21.Julia E. Heine.
1998.
Definiteness predictions forJapanese noun phrases.
In Proceedings of the 36thAnnual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics, pages 519?525.Satoru Ikehara, Satoship Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editing - effects of new methods in ALT-J/E.
In ThirdMachine Translation Summit, pages 101?106.Kevin Knight and Ishwar Chander.
1994.
Automatedpostediting of documents.
In Proceedings of theTwelfth National Conference on Artificial Intelligence,pages 779?784.John Lee.
2004.
Automatic article restoration.
In Pro-ceedings of the 2004 NAACL Conference Student Re-search Workshop, pages 195?200.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of HLT-NAACL 2006.Guido Minnen, Francis Bond, and Ann Copestake.
2000.Memory-based learning for article generation.
In Pro-ceedings of the Fourth Conference on ComputationalNatural Language Learning and of the Second Learn-ing Language in Logic Workshop, pages 43?48.Masaki Murata and Makoto Nagao.
1993.
Determina-tion of referential property and number of nouns inJapanese sentences for machine translation into En-glish.
In Fifth International Conference on Theoret-ical and Methodological Issues in Machine Transla-tion, pages 218?225.180
