f.'?
:??
"i,..i" 5Statistical Acquisition of Terminology Dictionary*Huang Xuan-jing, Wu Li-de, Wang Wen-xinDept.
of Computer Science, Fudan University, 200433 Shanghai960048~ms.fudan.sh.cn, ldwu~fudan.ihep.ac.cnAbstract: Terminologies are specialized words and compound words used in a particular domain, such ascomputer science.
Since they are very common in scientific articles, the ability to automatic identification fterminology could greatly assist any domain related natural language processing applications.
Unfortunately,the collection of terminology information is very difficult and requires much tedious and time consumingmanual work.
In this paper, a semi-automatic approach isdeveloped to extract technical words and phrasesfrom on-line corpora.
This approach can significantly reduce the manual effort in the generation ofterminology dictionary.
First, those domain specific words which have no entries in the universal dictionaryare identified.
Second, terminology words are extracted from these new words as well as the universaldictionary.
Then compound words are extracted from the combination of terminology words and otherwords.
The final computer terminology dictionary contains 1,034 words and 3,471 compound words.Experiment shows that 89.5 percent of all the occurrences ofcomputer terminology can be identified withthis terminology dictionary.keyword: Chi-square Test, Automatic Indexing, Mutual Information1.
IntroductionTerminologies are specialized words and compound words used in a particular domain, suchas computer science.
They are extensively used in scientific articles.
Previous research adshown that about 25% of the words in science abstract were technical words \[ 6 \ ] .
Therefore, theability to automatic identification of terminology could greatly aid any domain related naturallanguage processing applications, such as automatic indexing, information retrieval anddocument categorization.
For example, automatic indexing is the foundation of many otherrelevant tasks.
It needs to automatically identify those words which most appropriately reflect atext's theme.
Since terminologies are highly relevant to the text's domain, they are proved to bemuch valuable index words.
Even in more universal applications such as semantic analysis andtranslation, terminologies also play important roles, and therefore require special treatment.Unfortunately, the identification of terminology is a hard work.
Most terminologies don'thave entries in universal dictionaries.
In addition, terminology dictionaries are h i l l y  variable inthe coverage.
For example, computer science dictionaries' coverage of computer scienceterminology ranged from 24% to 66% \[ 6\] .
* This paper is supported by Chinese Natural Science Foundation and high technology 863 project.142!IiIi.
,IIWith regard to Chinese, the identification procedure is even more difficult.
First, there arescarcely any available machine readable Chinese dictionaries for specialized omains.
Therefore,the generation of terminology dictionary would inevitably require a great deal of tedious andtime consuming manual work.
Second, in most Indo-European languages, even a word couldn'tbe found in the dictionary, it still could be separated by the spaces between it and neighboringwords; however, Chinese is written in character sequences, with no delimiters betweensuccessive words.
Hence the first step of Chinese information processing is necessarily tosegment the character sequences into word sequences.
The main knowledge base of segmentationis the dictionary.
However, most of the terminologies couldn't be found in the dictionary.Therefore, before further processing, those domain specific words which are unavailable in thedictionary should be extracted and added to it.
This procedure is called new word extraction.Due to the availability of large scale on-line real text, corpus based natural anguageresearch as become one of the focuses of computational linguistics.
Among all the corpus l~asedresearches, ome of them are quite similar to the work reported here, including sublanguagevocabulary identification \[6 \] , automatic suggestion of significant erminology \[15 \] ,identification and translation of technical terminology\[ 3 ], automatic extraction of terminology\ [4\]  .
For example, Haas introduced a method for automatic identification of snblanguagevocabulary words.
First, words that could be easily identified as belonging to the vocabulary ofthe given domain were extracted, then the rest of the vocabulary were extracted using these seedwords.Another elevant research is statistical collocation extraction.
In fact, terminology phrasebelongs to one certain kind of collocation m fixed collocation, whether two or more words cancompose a collocation is measured by the correlation coefficient of these words \[ 11 \] .
If thesewords' correlation coefficient is large enough, they may probably make up a collocation.
Thereare many statistical methods to calculate words' correlation coefficient, including co-occtarencefrequency \[ 10\], mutual information \[ 1 \] ,generalized likelihood estimation \ [5 \ ] ,  chi-square test\ [2\ ]  \ [7\ ]  , Dice coefficient \[11 \] , etc.There are also many valuable works in China, especially about he distinctive new wordextraction of Chinese text.
Wang Kai-zhu presented a statistical method to extract possible wordsfrom texts.
Weights of possible words were calculated using their frequency and lengthinformation \[13\] .
Zhang Shu-wu also presented a strategy which made use of co-occurrencefrequencies to collect new words \[ 14\] .
Pascale Fung extended a tool originally designed forextracting English compounds - CXtract to collect new words in order to improve thesegmentation precision \[ 9 \] .Due to the distinct characteristic of Chinese, there is still no systematic approach to generatepractical and relatively complete Chinese terminology dictionaries from on-line corpora.
In thispaper, a semi-automatic approach is developed to extract echnical words and phrases fromcorpora.
This approach integrates such methods as new word collecting, terminology wordextraction and terminology phrase generation.
It can significantly reduce the manual effort in thegeneration of terminology dictionary.
First, those domain specific words which can't be found inthe universal dictionary are identified.
Second, terminology words are extracted from these new143words as well as the universal dictionary.
Then compound words which are combined byterminology words and other words are generated.The following sections are organized as such: Section 2 introduces the identification ofdomain specific words; Section 3 describes how to extract terminology words from the universaldictionary; Section 4 presents the method for terminology phrase extraction; Section 5 providesdetailed experimental results; The final section is the concluding remarks.2.
New Word ExtractionA Chinese word is usually composed of no more than 4 Chinese characters.
Most of thewords are uni-grams, hi-grams, tri-grams and 4-grams.
Uni-grams only consist of one character,and most of them are common words and then can be found in universal dictionaries.
Thenumber of n-grams with n>4 is very small, and the occurrence of most of them is rare.
Amongthe 9000 most frequently used words, far below 1% of them are longer than 4 characters \[9 \] .
Inaddition, most of these words are idioms or terminologies, then can be extracted in the phrasegeneration phase.
Therefore, in this section, only bi-grams, tri-grams and 4-grams are taken imoconsideration.Now consider two neighboring characters A and B.
We call these two characters as a bi-gram candidate.
They belong to either the same word, or two neighboring words.
We canintuitively suppose that the two characters are more correlate to each other when they belong tothe same word.
Therefore, we may choose a statistic to measure the correlation coefficient ofneighboring characters, then use this statistic to judge the probability that they belong to thesame word.The correlation coefficient could be measured by several methods, such as co-occurrencefrequency, mutual information, generalized likelihood estimation, chi-square test, Dicecoefficient.
Among them, chi-square test needs special attention.
First, it is closely related to thebinomial distribution model of text.
Second, the computation is quite simple.
Experiment insection 5 also showed that it could lead to better performance.
Following is the detaileddescription of this method.Compare ach bi-gram (4, B) candidate to every two neighboring characters ( C,, C,-1) inthe text sequence C-- ( CIC:'"C,C,-z ""Cn ), where n is the size of the text, and record thecomparison results.
Thus there are four types of results altogether:Result 1: C~A and C-\]=B, which is noted as (.4, B);Result 2: CFA and C-/-~B, which is noted as (.4, B );?
Result 3: C,~A and C,_tfB, which is noted as ( A, B);Result 4: C,~-A and C- laB ,  which is noted as ( A, B).Let n be the count of (C,C,-~), nlpn:2, n2p n22 be the count of (.4, B),(A, B), (A,B) ,( A, B ) respectively.
Obviously, n = nit + n n + n21 + %rLetn r fn l+n2 , n =n O+n2j , ( i=1, 2 ; j= l ,  2).Then a contingency table is established as such:144Table 1: Contingency Table o f  Characters A and BB B EA nil hi2 n 1A n21 n22 n2-n. 1 vl.
2 Viii l lIf the characters A and B occur independently, then we would expect P(AB)=P(A) XP(B),where P(ABJ is the probability of A and B occurring next to each other; P(A) is the probability ofA, P(B) is the probability of B.
To test the null hypothesis P(ABJ=P(A) XP(B), we compute thechi-square statistic:22 2 (n?
x \]'/I ?
xn .
j lz: -- XX":=1 j=l nt - X~2 - 3 'The above equation can be simplified as: Z 2 = n(n. x n= - n,2 x n22) 2nt X/q2 .Xn .
t  X n .2We define the correlation coefficient of characters A and B to be the value of chi-square test.Those bi-gram candidates with correlation coefficient smaller than a pre-defined threshold areconsidered to occur randomly and should be discarded.
Others are sorted according to theircorrelation coefficient in descending order.Tri-gram and 4-gram candidates are processed in the same way.
To compute the correlationcoefficient of all tri-grams, we shouldn't set the null hypothesis to P(ABC)=P(A) XP(B) XP(C),otherwise we would be faced with the critical problem of data sparseness and then get unreliableand vulnerable results.
In alternate, we just look a tri-gram as the combination o fa  bi-gram and acharacter, then calculate their correlation coefficient.
Similarly, a 4-gram can be looked either asthe combination ofa tri-gram and a character, or two bi-grams.The rest of bi-gram, tri-gram, 4-gram candidates constitute 3 separate tables.
In these tables,many candidates are available in the universal dictionary, others are potential words.
Thesepotential words are carefully examined by skillful computer professionals, and many of them areaccepted and then appended to the dictionary in order to improve segmentation precision.
Thesewords are called new words.
Human intervention is still inevitable, since statistical methods notonly generate useful, but also noisy words.
Thresholds can be applied to limit this effect, but"an't eliminate it.Terminology Word Extractionrminology words are divided into two subsets and treated respectively.
Most of them haves in the universal dictionary.
These words should be extracted from the new word tables.number of new words is limited, and most of new words are domain specific wordsqnologies and proper names, this work is also done manually.
"minologies are available in the universal dictionary.
They are either frequently used145words, such as " i=t '~  ( computer )" and "~.~ ( network )", or have meanings outside ofscience areas, such as "f'~tL~ ( agent )" and " ~.~.~ ( procedure )".
These words are alsoextracted in statistical method.If a word is a terminology, then it probably occurs more often in related omain corpus thannormal.
Let Pc(W) be the frequency of word W in domain corpus, P,(W) be the normal frequencyof W. If Pc(W)>>P,(W), W is extracted and further examined by professionals, otherwise it isdiscarded.
In the following experiment, this formula is replaced with Pc(W) > T2 ?
P,(W), whereT2 is a threshold.
Similar method could be found in Zhou95 \[ 15\] .To gather all word frequency information i  a specific domain, the domain corpus hould befirst segmented with the augmented dictionary.
The normal frequency could be obtained eitherfrom a balanced on-line frequency dictionary or a universal corpus.
Since on-line frequencydictionary is not available for us, another universal corpus is used.
For those words which appearin the domain corpus, but don't appear in the universal corpus, P, is approximately replacedwith the average frequency of all words.4.
Terminology Phrase GenerationTerminology phrases are word pairs composed of terminology words and other words.Current research only concerns word pairs.
Terminology phrases are generated in three steps.At the first step, all the candidate phrases are extracted.
The whole corpus is segmented withthe augmented ictionary in advance.
A small window is put over each terminology wordappearing in the text sequence.
Candidate terminology phrases are those word pairs which arecomposed of one terminology word and another word inside this terminology's border window.Those word pair's with too low frequencies are filtered out.Whether a word pair is a phrase is measured by its weight.
At the next step, most ofcandidates are also filtered out if their weights are too small.
A word pair's weight is mainlydecided by its correlation coefficient.
In addition, two heuristic rules are adopted to modify theweights:Rule 1: If a word pair is composed of two terminology words, its weight is strengthened.Rule 2: If a word pair contains function words, it is also filtered out.
A stop word table isintroduced for this reason.
This table contains more than 1000 Chinese function words, such as"~ (of)"and"~ (be)".At the last step, all the remaining word pairs are manually examined.
Those acceptedphrases as well as terminologies words compose the final terminology dictionary.5.
Implementation and ResultsTwo corpora were chosen for this research.
One is a Computer World corpus (CW).
It iscomposed of all articles of the newspaper "Computer World ( ~t'~'LIJ~L~- )" from 1990 to1994.
The 100M bytes corpus contains more than 40M Chinese characters.
The other is auniversal corpus - -  XinHua news ( ~.~.~?
~.,kK~ ) corpus (XN).
It contains more than 8,000 I/I1IIiI146news articles with 10M bytes of text.CW corpus contains many computer terminologies, most of which just appeared in last twodecades.
Therefore, only a small number of them have entries in universal dictionaries.
XNcorpus also contains many new words, but the number is much smaller.To collect new words, each article was scanned and all the bi-gram, tri-gram and 4-gramcandidates with frequency greater than threshold T\] were extracted ( for CW corpus, Tr=4, forXN corpus, T~=2 ).
In addition, some shorter candidates were actually parts of longer ones, andcouldn't exist independently.
For example, every time "~31~rL" was seen in the text, it followed"i~'; every time " l~"  was seen, it was followed by "~:".
So "~g~L" and " \ [~g"  are only partsof longer candidates " ~  ( computer )" and "1~:  (Afghanistan)".
Thus they should beremoved from candidate tables.The remaining candidates were sorted by their correlation coefficient in descending order.Those candidates on the top of the table have higher probability to be real words.
To evaluate thecomputing methods, we may consider the distribution in the candidate table of those wordsavailable in the dictionary.
These words are called as available words.
Let D be the sortedcandidate table, DS be a sub.table of D starting from the beginning of D. Two evaluationstandards precision and recall were defined as follows:Precision ofDS = Number of available words in DS /Number of candidates inDS;Recall ofDS = Number of available words in DS/Number of all available words in D.Obviously, since many new words have no entries in the dictionary, the real precision andrecall should be somewhat higher.
Figure 1 is the Recall-Precision curves of the bi-gramcandidate able of CW corpus.
Figure 2 is those of XN corpus.I '  Figure l: Computer World Corpus Figure 2: XinHua News CorpusI ~  ~ I w I"'" ' '| , .
.
,  o,0.8  ?
0 .8  "~.-, ~.0.7 " ,  0.7 t""--'~-'~.~ .
~ .
O.6 O.6I , .... o 4 -...o.~?'~?
'~ .
, ,  i~: I ""~ | o.~ oo o~ o.~ o.~ o.~ o o.~ o.~ o.~ o.~Recall RecallI .
.
.
.
.
CHI Method .
.
.
.
.
CHI MethodMI Method ~ MI MethodGL Method GL MethodThree computing method were used: mutual information (MI) \[ 1 \] , generalized likelihoodestimation (GL) \ [5 \ ]  and chi-square test (CHI).
From these figures we can see that theperformance of GL method is the worst.
When recaU is not much high ( less than 40-50%),which means Bat only those top candidates are considered, CHI method is the best.
When recall147becomes higher, MI is better than others.
Since only top of the table should be further examinedmanually, CHI method was chosen.Figure 3 demonstrates the Recall-Precision curves of two corpora using CHI method.Although XN corpus is only one tenth of CW in size, it gains better esults.
This result can beattributed to the fact that XN corpus contains less new words.There are more than 400,000 bi-gram candidates in CW corpus.
Among them, 17,779 areavailable words.
Only 61,584 candidates have frequencies greater than Ti(Ti=4), including7,089 available words.
These candidates compose the bi-gram candidate table.
New words areextracted from the top 16% of this table.
Among these 9,856 high-rank candidates, 4,041 areavailable in the dictionary, which amount o 57% of all the available words in the whole table.The remaining 5,815 were potential new words and then further examined by computerprofessionals.
Finally, 1,699 were accepted.
Similar results were obtained from tri-gram and 4-gram candidates.
A little more differently, the proportion of available words in tri-grarn and 4-gram candidate tables is much smaller than in hi-gram table.
Therefore, new words were onlyextracted from the top 4% tri-grams and the top 2% 4-grams.
The quantities of accepted tri-gramsand 4-grams is also smaller than that of bi-grarns.
Table 2 presents the vocabulary distribution ofCW corpus.
Among the whole vocabulary, more than 10% are extracted new words.
Later therecall and precision were recalculated using the augmented dictionary.
Figure 4 demonstrates theRecall-Precision curves of Computer World corpus using original dictionary and augmenteddictionary respectively.
We can find that the precision is significantly improved aRer new wordswere appended.Figure 3: Comparison betweenXN and CW CorpusI0.90.80.70.6 Prec.
0.50.40.30.20.10 t l , i  I ,,0 0.2 0.4 0.6 0.8Re:callXinHua News....... Computer WorldFigure 4: Comparison between Originaland Augemented Dictionary10.90.80.70.6 Prec.
0.50.40.30.20.10 1 !
I I , , ,0 0.2 0.4 0.6 0.8 IRecall,Augmented Dictionary.
.
.
.
.
.
.
Original DictionaryTable 2: the Vocabula.r~, Distribution of Computer World CorpusUni-~ram bi-gram tri-grarn 4-~'arn TotalAvailable Words 3298 17779 1830 2370 25277i New Words 1699 1122 49 2870Sum 3298 19478 2952 2419 28147148IIiIIIi,To extract terminology words from new words, all new words were manually examined andput to any of three categories: terminology words, proper names and other domain specific words,or to say, those words which are related to this domain to some degree, but cannot be consideredas terminology ofthis domain, for example: ~eg:~:~ ( cable 'IV ) and computer domain.
Table3 shows the distribution of new words.
Table 4 presents ome example words with highestcorrelation coefficient.
From table 3 and table 4 we can see, about one fourth of new words areterminology words; another one fourth are proper names; the rest are other domain specificwords.
Those words with highest correlation coefficient are almost terminology words andproper names.
In addition, many tri-grams are proper names, because most of Chinese names arecomposed of 3 characters.
Since Chinese name recognition is also an complex problem inChinese real text processing, this method can also be utilized to recognize names.bi-grarntri-~rarn?
4-gramallTable 3: the Distribution ofNew Wordsterminolo~:389proper names215othe~1095toml1699302 503 317 112272620 211433 711492870Table 4 : Examples ofNew Words, Exampleshi-gramtri-gram~j:l~ (virus) ~t.l~ (honeycomb) ~ (bottleneck) .~:l~j~ (share) ~.~.
(Toshiba) ~-~.q,~( media ) 'l~\[~J ( portable ) ~ ~ (sector) ~l~ D ( interface )~ I~.
(place) J~llll (name) ~ (name) 'l~-~J~ (Oregon) ;liEl~'fl:~(chemicalcompound ) :E~,~ ( work station ) ~ j~ ( database )4-gram ~ l l  ( Barcelona ) ,~,/~,~:j~ ( Honeywell ) .~ ' : :~  ( Markov )~J ' i~  (Vt) ~F~..E (bottomup) ~ (cableTV)To extract terminologies from the original universal dictionary, the frequency of each of the25,277 words in CW corpus was compared to the frequency in XN corpus.
The threshold of T2was set to 3. only 1,938 words' frequencies in CW corpus were three times higher than in XNand then satisfied this threshold limitation.
These words were further categorized manually.
Thecategorization results are demonstrated in table 5.Table 5: Manual Examination results of the Universal Dictionaryterminolo~7 others totalbi-gramtri-grarn4.-gramall287 1427 171433 155 1884 32 36323 1615 1938149We can find terminologies extracted from the universal dictionary are much fewer thanthose extracted from new words: of the 1,938 words, only 323 were accepted finally.
In addition,to make sure only a small portion of terminology words had been missed, 1,000 words wererandomly selected from the rest 23,329 words and only 4 were found to be terminologies.
Thishelped to explained that most of the terminology words in the universal dictionary had beenextracted.Terminology phrases were later extracted from the combination of 1,034 terminology wordsand their neighboring words within a distance of ?3.
There are altogether 35,178 phrasecandidates with frequency greater than a threshold T3 (here T3ffi3).
Random sampling showedthat 30% of them are acceptable t rminology phrases.
These candidates' weights were computedin the method introduced in section 4.
Then they were sorted in descending weight order.
Figure5 shows the approximate r call-precision curve of terminology phrase xtraction.
The reason forapproximate evaluation was that it was impossible to manually examine all 35,178 terminologyphrases, therefore only randomly selected 3,000 candidates were examined.
From figure 5, wecan find that he performance of phrase xtraction wasn't as good as that of word extraction.
Thisphenomenon can be explained by the fact that some highly associated candidates still couldn'tcompose terminology phrases.
Most of these pseudo phrases can be divided into two classes:Class 1: The two words compose a Verb.Object, Subject-Verb, or other phrases.
For example,"~ (left mouse key) ~ig~ (drag)".Class 2: The two words are two highly associate words, but have no direct syntactic relations.For example, "~\]~-~ -)~" (two Chinese character input methods).
In fact, similar phenomenacan also be found in English \ [8 \ ]  .Therefore, the precision will surely be improved whensyntactic information is used to further filter candidates.Figure 5: Recall-Precision Curve of Phrase Table10.90.80.70.6Precision 0.50.40.30.20.10 I I I l0.2 0.4 0.6 0.8RecallTerminology phrases were extracted from the top 20% ( with precision of about 50%)terminology phrase candidates, these candidates were examined manually and 3,471 wereaccepted.
These 3,471 phrases as well as the 1,034 words compose our computer terminologydictionary.
Table 6 presents some example terminologies with high rank.100 pieces of article of 72K bytes were randomly selected to test the coverage of thisterminology dictionary.
A simple automatic pattern matching program was used to identifyterminologies and 1,174 occurrences of terminologies were spotted.
This identification procedurewas also done by several graduate students major in computer science.
The automatic recognition150IIiII, ' ,results were compared to the union set of three experimenters.
89.5% of all terminologies foundby experimenters were als0 found by the program.
And 73.9% of all the program output wasjudged to be correct.
The relatively lower precision can be attributed to the fact that someterminologies, especially those available in the original dictionary, have meaning outsidecomputer domain.
In large scale natural langnage processing applications where contextinformation and local parsing are available, the precision would be increased certainly.Table 6: the Distribution of Terminology, l=Number , Exampleavailable 323 ~ ( software ) ~.~ ( concurrent ) ~-~ ( program ) ~g0L ( computer)=~$1Jwords ( binar)~ ) /b~M'i~ (machine translation )hi-gram 389 ~ ( virus ) J~\[~ (bottleneck) /l:~zg (share) ~-'~(media) ~( portable ) ~,,~ (seccm\[) ~ l~l ( interface ), ~ ( video ) ,.,tri-gram 302 I~  ( work station ) ~ (database) ~ (multimedia) ~:~l~( LAN ) ~ j~ ( driver ) ~; t~ ( distributed ) ~/~.
( scanner )C-gram 20 --~;J-~ (Markov) I~iF~..I: (bottomup) ~ (cableTV) ~.... ( Robot science ) ~;~g.gq'f-~S ( tinformat )phrase 3471 ~n-\[-~:-, (Bayesbelief) :l:,~i~=J~ (Thesaurus) ~fl~,~(decorapression) "~~ ( Predicate calculation ) i~ \ ]~\ ]~ ( MODEM )6.
ConclusionThis research presents a chi-square method based approach to semi-automatically generateterminology dictionaries.
This approach integrates uch methods as new word collecting,terminology word extraction and terminology phrase generation.
It significantly reduces most ofthe hard work which should be done manually, and reduce the effort and time which are neededto transport a natural language processing work from one domain to another.
Using thisterminology dictionary, encouraging results has been achieved about the coverage ofterminologies.This research as practical importance inmany domain related natural language applications.It can improve indexing results.
It can help to decide texts' category.
It also can help to rankdocuments with user queries.
In fact, this approach will soon be embedded into an integratedChinese information processing system - FDASCT \[ 12\] .Our future work mainly includes the utilization of deeper text processing techniques such aspart of speech tagging and partial syntactic analysis in phrase generation.
Word pairs would bediscarded if there are no consistent syntactic relations between constituent words.
And those non-noun phrases would also be discarded since terminologies are always nouns.
Thus manual effortcan be further educed.151Reference\[ 1 \] Church ICW, Hanks P., Word Association Norms, Mutual Information, and Lexicography, ComputationalLinguistic, 16:1, 1990,22 - -  29\[ 2 \] Church K.W, Gale W.A.
et.
al, Using Statistics in Lexical Analysis, Lexical Aeqnisilion: Using On-lineResources to Build a Lexicon, edited by Uri Zemik, Lawrence Erlbaum, Hillsdale, New Jersey, 115-165\[ 3 \] Dagan I. and Church K.W, Termight: ldentO~,ing and translating technical terminology, ANLP, 34.-..40, 1994\[ 4 \] Daille B., Study and implementation fcombined techniques for automatic extraction of terminology, 29-36,The Balancing Act, Combining Symbolic and Statistical Approaches to Language - Proceedings of theWorkshop, 1994\[ 5 \] Dunning T., Accurate Methods for the Statistics of Surprise and Coincidence, Computational Linguistic 19:1,1993, 61 - -  74\[ 6 \] Haas, St~hanie, He Shaoyi.
Toward the automatic Identification of Sublanguage Vocabulary, InformationProcessing & Management, 29:6, 1993, 721-732\[ 7 \] Huang Xuan-jing, Wu Li-d?, Wang Wen-xin, Ye Dan-jin, ~#/ /~ '~7~'3E~X~/ /~7~/*%F.
.~f f( A Machine Learning Based System Without Maual Dictionary ) , ~ '~t~J~Jk .
I~  ( PatternRecognition and Artificial Intelligence ), 1996.
12, 9:4,297 -- 303\[ 8 \] Justeson J. and Katz S., Technical terminology: some linguistic properties and an algorithm for id~ifw.ationin text, Natural Language Engineering, 1995, 1:I, 9 -- 28\[ 9 \] Pascale Ftmg.
Dekal Wu, Statistical Augmentation of a Chinese machine-readable Dictionary, TechnicalReport HKUST-CS94-31, November 1994\[ 10 \] Srnadja, Frank, Retrieve collocations~om text: Xtract, Computational Linguistic 19:1.1993, 143~ 177\[ il \] $madja, Flank, ?t al., Translating collocations for Bilingual Lexicons: A Statistical Approach,Computational Linguistic 22:1, 1996, 1 ~ 38\[ 12 \] Wu Li=de ,Wei Xiong-guan.
Huang Xuan-jing, ?t al, Fudan Abstract System of Chinese Text, 1996.
6,Communications of COLIFS\[ 13 \] Wang Kai-zhu, et al ~f f1~ ~'~7h~r~ (Study of Nondictionay Chinese Segmentation), ~t'~f~-~~ ~  ( Advances and Applications on Computational Linguistics), Tsinghua University Press,1995, 359\[ 14 \] Zhang Shu-wu, et al ~i~i~-~'~qf~/l f l f fzd=~s37~l~..~'~'~'~r~ (An Automat*c Buildng Method ofElectronic Dictionary Used for Chinese Speech Recognition), "b t '~  ~ ~-~I~ ( Advances andApplications on Computational Linguistics), T$inghua University Press, 1995, 219 -- 224\[ 15 \] Zhou J. and Dapkus P., Automatic suggestion ofszgnificant terms for a predefined topic, Proceedings of Thirdthe Workshop on Very Large Corpora, I31-147, 1995152
