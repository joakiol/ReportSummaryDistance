Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84?91,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsUncertainty Detection as Approximate Max-Margin Sequence LabellingOscar Ta?ckstro?mSICS / Uppsala UniversityKista / Uppsala, Swedenoscar@sics.seGunnar ErikssonSICSKista, Swedenguer@sics.seSumithra VelupillaiDSV, Stockholm UniversityKista, Swedensumithra@dsv.su.seHercules DalianisDSV, Stockholm UniversityKista, Swedenhercules@dsv.su.seMartin HasselDSV, Stockholm UniversityKista, Swedenxmartin@dsv.su.seJussi KarlgrenSICSKista, Swedenjussi@sics.seAbstractThis paper reports experiments for theCoNLL-2010 shared task on learning todetect hedges and their scope in natu-ral language text.
We have addressedthe experimental tasks as supervised lin-ear maximum margin prediction prob-lems.
For sentence level hedge detectionin the biological domain we use an L1-regularised binary support vector machine,while for sentence level weasel detectionin the Wikipedia domain, we use an L2-regularised approach.
We model the in-sentence uncertainty cue and scope de-tection task as an L2-regularised approxi-mate maximum margin sequence labellingproblem, using the BIO-encoding.
In ad-dition to surface level features, we use avariety of linguistic features based on afunctional dependency analysis.
A greedyforward selection strategy is used in ex-ploring the large set of potential features.Our official results for Task 1 for the bio-logical domain are 85.2 F1-score, for theWikipedia set 55.4 F1-score.
For Task 2,our official results are 2.1 for the entiretask with a score of 62.5 for cue detec-tion.
After resolving errors and final bugs,our final results are for Task 1, biologi-cal: 86.0, Wikipedia: 58.2; Task 2, scopes:39.6 and cues: 78.5.1 IntroductionThis paper reports experiments to detect uncer-tainty in text.
The experiments are part of the twoshared tasks given by CoNLL-2010 (Farkas et al,2010).
The first task is to identify uncertain sen-tences; the second task is to detect the cue phrasewhich makes the sentence uncertain and to markits scope or span in the sentence.Uncertainty as a target category needs to be ad-dressed with some care.
Sentences, utterances,statements are not uncertain ?
their producer, thespeaker or author, is.
Statements may explicitlyindicate this uncertainty, employing several differ-ent linguistic and textual mechanisms to encodethe speaker?s attitude with respect to the verac-ity of an utterance.
The absence of such markersdoes not necessarily indicate certainty ?
the oppo-sition between certain and uncertain is not clearlydemarkable, but more of a dimensional measure.Uncertainty on the part of the speaker may be dif-ficult to differentiate from a certain assessment ofan uncertain situation, It is unclear whether thisspecimen is an X or a Y vs.
The difference betweenX and Y is unclear.In this task, the basis for identifying uncertaintyin utterances is almost entirely lexical.
Hedges,the main target of this experiment, are an estab-lished category in lexical grammar analyses - seee.g.
Quirk et al (1985), for examples of Englishlanguage constructions.
Most languages use vari-ous verbal markers or modifiers for indicating thespeaker?s beliefs in what is being said, most proto-typically using conditional or optative verb forms,Six Parisiens seraient morts, or auxiliaries, Thismushroom may be edible, but aspectual markersmay also be recruited for this purpose, more indi-rectly, I?m hoping you will help vs.
I hope you willhelp; Do you want to see me now vs. Did you wantto see me now.
Besides verbs, there are classesof terms that through their presence, typically inan adverbial role, in an utterance make explicitits tentativeness: possibly, perhaps... and morecomplex constructions with some reservation, es-pecially such that explicitly mention the speakerand the speaker?s beliefs or doubts, I suspect thatX.Weasels, the other target of this experiment,on the other hand, do not indicate uncertainty.84Weasels are employed when speakers attempt toconvince the listener of something they most likelyare certain of themselves, by anchoring the truth-fulness of the utterance to some outside fact or au-thority (Most linguists believe in the existence ofan autonomous linguistic processing component),but where the authority in question is so unspecificas not to be verifiable when scrutinised.We address both CoNLL-2010 shared tasks(Farkas et al, 2010).
The first, detecting uncer-tain information on a sentence level, we solve byusing an L1-regularised support vector machinewith hinge loss for the biological domain, andan L2-regularised maximum margin model for theWikipedia domain.
The second task, resolution ofin-sentence scopes of hedge cues, we approach asan approximate L2-regularized maximum marginstructured prediction problem.
Our official resultsfor Task 1 for the biological domain are 85.2 F1-score, for the Wikipedia set 55.4 F1-score.
ForTask 2, our official results were 2.1 for the entiretask with a score of 62.5 for cue detection.
Afterresolving errors and unfortunate bugs, our final re-sults are for Task 1, biological: 86.0, Wikipedia:58.2; Task 2: 39.6 and 78.5 for cues.2 Detecting Sentence Level UncertaintyOn the sentence level, word- and lemma-basedfeatures have been shown to be useful for uncer-tainty detection (see e.g.
Light et al (2004), Med-lock and Briscoe (2007), Medlock (2008), andSzarvas (2008)).
Medlock (2008) and Szarvas(2008) employ probabilistic, weakly supervisedmethods, where in the former, a stemmed singleterm and bigram representation achieved best re-sults (0.82 BEP), and in the latter, a more complexn-gram feature selection procedure was appliedusing a Maximum Entropy classifier, achievingbest results when adding reliable keywords froman external hedge keyword dictionary (0.85 BEP,85.08 F1-score on biomedical articles).
More lin-guistically motivated features are used by Kil-icoglu and Bergler (2008), such as negated ?un-hedging?
verbs and nouns and that preceded byepistemic verbs and nouns.
On the fruit-fly dataset(Medlock and Briscoe, 2007) they achieve 0.85BEP, and on the BMC dataset (Szarvas, 2008) theyachieve 0.82 BEP.
Light et al (2004) also foundthat most of the uncertain sentences appeared to-wards the end of the abstract, indicating that theposition of an uncertain sentence might be a use-ful feature.Ganter and Strube (2009) consider weasel tagsin Wikipedia articles as hedge cues, and achieveresults of 0.70 BEP using word- and distancebased features on a test set automatically derivedfrom Wikipedia, and 0.69 BEP on a manually an-notated test set using syntactic patterns as fea-tures.
These results suggest that syntactic featuresare useful for identifying weasels that ought to betagged.
However, evaluation is performed on bal-anced test sets, which gives a higher baseline.2.1 Learning and Optimization FrameworkA guiding principle in our approach to this sharedtask has been to focus on highly computationallyefficient models, both in terms of training and pre-diction times.
Although kernel based non-linearseparators may sometimes obtain better predic-tion performance, compared to linear models, thespeed penalty at prediction time is often substan-tial, since the number of support patterns oftengrows linearly with the size of the training set.
Wetherefore restrict ourselves to linear models, butallow for a restricted family of explicit non-linearmappings by feature combinations.For sentence level hedge detection in the bio-logical domain, we employ an L1-regularised sup-port vector machine with hinge loss, as providedby the library implemented by Fan et al (2008),while for weasel detection in the Wikipedia do-main, we instead use the L2-regularised maximummargin model described in more detail in section3.1.
In both cases, we approximately optimise theF1-measure by weighting each class by the inverseof its proportion in the training data.The reason for using L1-regularisation in the bi-ological domain is that the annotation is heavilybiased towards a rather small number of lexicalcues, making most of the potential surface featuresirrelevant.
The Wikipedia weasel annotation, onthe other hand, is much more noisy and less de-termined by specific lexical markers.
Regularisingwith respect to the L1-norm is known to give pref-erence to sparse models and for the special caseof logistic regression, Ng (2004) proved that thesample complexity grows only logarithmically inthe number of irrelevant features, instead of lin-early as when regularising with respect to the L2-norm.
Our preliminary experiments indicated thatL1-regularisation is superior to L2-regularisationin the biological domain, while slightly inferior in85the Wikipedia domain.2.2 Feature DefinitionsThe asymmetric relationship between certain anduncertain sentences becomes evident when onetries to learn this distinction based on surface levelcues.
While the UNCERTAIN category is to a largeextent explicitly anchored in lexical markers, theCERTAIN category is more or less defined implic-itly as the complement of the UNCERTAIN cate-gory.
To handle this situation, we use a bias fea-ture to model the weight of the CERTAIN category,while explicit features are used to model the UN-CERTAIN category.The following list describes the feature tem-plates explored for sentence level uncertainty de-tection.
Some features are based on a linguisticanalysis by the Connexor Functional Dependency(FDG) parser (Tapanainen and Ja?rvinen, 1997).SENLEN Preliminary experiments indicated that taking sen-tence length into account is beneficial.
We incorporatethis by using three different bias terms, according to thelength (in tokens) of the sentences.
This feature takesthe following values: S < 18 ?
M ?
32 < L.DOCPT Document part, e.g., TITLE, ABSTRACT and BODYTEXT, allowing for different models for different docu-ment parts.TOKEN, LEMMA Tokens in most cases equals words, butmay in some special cases also be multiword units, e.g.of course, as defined by the FDG tokenisation.
Lemmasare base forms of words, with some special featuresintroduced for numeric tokens, e.g., year, short number,and long number.QUANT Syntactic function of a noun phrase with a quanti-fier head (at least some of the isoforms are conservedbetween mouse and humans), or a modifying quantifier(Recently, many investigators have been interested inthe study on eosinophil biology).HEAD, DEPREL Functional dependency head of the token,and the type of dependency relation between the headand the token, respectively.SYN Phrase-level and clause-level syntactic functions of aword.MORPH Part-of-speech and morphological traits of a word.Each feature template defines a set of featureswhen applied to data.
The TOKEN, LEMMA,QUANT, HEAD, DEPREL templates yield single-ton sets of features for each token, while the SYNand MORPH templates extends to sets consistingof several features for each token.
A sentence isrepresented as the union of all active token levelfeatures and the SENLEN and DOCPT, if active.In addition to the linear combination of concretefeatures, we allow combined features by the Carte-sian product of the feature set extensions of two ormore feature templates.2.3 Feature Template SelectionAlthough regularised maximum margin modelsoften cope well even in the presence of irrelevantfeatures, it is a good idea to search the large set ofpotential features for an optimal subset.In order to make this search feasible we maketwo simplifications.
First, we do not explore thefull set of individual features, but instead the set offeature templates, as defined above.
Second, weperform a greedy search in which we iterativelyadd the feature template that gives the largest per-formance improvement, when added to the cur-rent optimal set of templates.
The performance ofa feature set for sentence level detection is mea-sured as the mean F1-score, with respect to theUNCERTAIN class, minus one standard deviation?
the mean and standard deviation are computedby three fold cross-validation on the training set.We subtract one standard deviation from the meanin order to promote stable solutions over unstableones.Of course, these simplifications do not come forfree.
The solution of the optimisation problemmight be quite unstable with respect to the optimalhyper-parameters of the learning algorithm, whichin turn may depend on the feature set used.
Thisrisk could be reduced by conducting a more thor-ough parameter search for each candidate featureset, however, this was simply too time consumingfor the present work.
A further risk of using for-ward selection is that feature interactions are ig-nored.
This issue is handled better with backwardelimination, but that is also more time consuming.The full set of explored feature templates is toolarge to be listed here; instead we list the featuresselected in each iteration of the search, togetherwith their corresponding scores, in Table 1.3 Detecting In-sentence UncertaintyWhen it comes to the automatic identification ofhedge cues and their linguistic scopes, Moranteand Daelemans (2009) and O?zgu?r and Radev(2009) report experiments on the BioScope cor-pus (Vincze et al, 2008), achieving best results(10-fold cross evaluation) on the identification ofhedge cues of 71.59 F-score (using IGTree withcurrent, preceding and subsequent word and cur-86Task Template set Dev F1 Test F1BioSENLEN - -?
LEMMA 88.9 (.25) 78.79?
LEMMABI 90.3 (.19) 85.86?
LEMMA?QUANT 90.3 (.07) 85.97WikiSENLEN - -?
TOKEN?DOCPT 59.0 (.76) 60.12?
TOKENBI?SENLEN 59.9 (.09) 58.26Table 1: Top feature templates for sentence levelhedge and weasel detection.rent lemma as features) and 82.82 F-score (using aSupport Vector Machine classifier and a complexfeature set including keyword and dependency re-lation information), respectively.
On the task ofautomatic scope resolution, best results are re-ported as 59.66 (F-score) and 61.13 (accuracy),respectively, on the full paper subset.
O?zgu?r andRadev (2009) use a rule-based method for this sub-task, while Morante and Daelemans (2009) usethree different classifiers as input to a CRF-basedmeta-learner, with a complex set of features, in-cluding hedge cue information, current and sur-rounding token information, distance informationand location information.3.1 Learning and Optimisation FrameworkIn recent years, a wide range of different ap-proaches to general structured prediction prob-lems, of which sequence labelling is a specialcase, have been suggested.
Among others, Con-ditional Random Fields (Lafferty et al, 2001),Max-Margin Markov Networks (Taskar et al,2003), and Structured Support Vector Machines(Tsochantaridis et al, 2005).
A drawback ofthese approaches is that they are all quite com-putationally demanding.
As an alternative, wepropose a much more computationally lenient ap-proach based on the regularised margin-rescalingformulation of Taskar et al (2003), which we in-stead optimise by stochastic subgradient descentas suggested by Ratliff et al (2007).
In addi-tion we only perform approximate decoding, us-ing beam search, which allows arbitrary complexjoint feature maps to be employed, without sacri-ficing speed.3.1.1 Technical DetailsLet X denote the pattern set and let Y denote theset of structured labels.
Let A denote the set ofatomic labels and let each label y ?
Y consist ofan indexed sequence of atomic labels yi ?
A. De-note by Yx ?
Y the set of possible label assign-ments to pattern x ?
X and by yx ?
Yx its cor-rect label.
In the specific case of BIO-sequencelabelling, A = {BEGIN, INSIDE, OUTSIDE} andYx = A|x|, where |x| is the length of the sequencex ?
X .A structured classification problem amountsto learning a mapping from patterns to labels,f : X 7?
Y , such that the expected lossEX?Y [?
(yx, f(x))] is minimised.
The predictionloss, ?
: Y ?
Y 7?
<+, measures the loss ofpredicting label y = f(x) when the correct la-bel is yx, with ?
(yx, yx) = 0.
Here we assumethe Hamming loss, ?H(y, y?)
= ?|y|i=1 ?
(yi, y?i),where ?
(yi, y?i) = 1 if yi 6= y?i and 0 otherwise.The idea of the margin-rescaling approach is tolet the structured margin between the correct labelyx and a hypothesis y ?
Yx scale linearly with theprediction loss ?
(yx, y) (Taskar et al, 2003).
Thestructured margin is defined in terms of a scorefunction S : X ?
Y 7?
<, in our case the linearscore function S(x, y) = wT?
(x, y), where w ?<m is a vector of parameters and?
: X?Y 7?
<mis a joint feature function.
The learning problemthen amounts to finding parameters w such thatS(x, yx) ?
S(x, y) + ?
(yx, y) for all y ?
Yx \{yx} over the training data D. In other words, wewant the score of the correct label to be higher thanthe score plus the loss, of all other labels, for eachinstance.
In order to balance margin maximisationand margin violation, we add theL2-regularisationterm ?w?2.By making use of the loss augmented decodingfunctionf?
(x, yx) = argmaxy?Yx[S(x, y) + ?
(yx, y)] , (1)we get the following regularised risk functional:Q?,D(w) =|D|?i=1S?
(x(i), yx(i)) + ?2 ?w?2, (2)whereS?
(x, yx) = maxy?Yx [S(x, y) + ?
(yx, y)]?S(x, yx)(3)We optimise (2) by stochastic approximate subgra-dient descent with step size sequence [?0/?t]?t=1(Ratliff et al, 2007).
The initial step size ?0and the regularisation factor ?
are data depen-dent hyper-parameters, which we tune by cross-validation.87This framework is highly efficient both at learn-ing and prediction time.
Training cues and scopeson the biological data, takes about a minute, whileprediction times are in the order of seconds, usinga Java based implementation on a standard laptop;the absolute majority of that time is spent on read-ing and extracting features from an inefficient in-ternal JSON-based format.3.1.2 Hashed Feature FunctionsJoint feature functions enable encoding of depen-dencies between labels and relations between pat-tern and label.
Most feature templates are de-fined based on input only, while some are de-fined with respect to output features as well.
Let?
(x, y1:i?1, i) ?
<m denote the joint feature func-tion corresponding to the application of all activefeature templates to pattern x ?
X and partiallydecoded label y1:i?1 ?
Ai?1 when decoding atposition i.
The feature mapping used in scoringcandidate label yi ?
A is then computed as theCartesian product ?
(x, y, i) = ?
(x, y1:i?1, i) ??
(yi), where ?
(yi) ?
<m is a unique unitary fea-ture vector representation of label yi.
The featurerepresentation for a complete sequence x and itsassociated label y is then computed as?
(x, y) =|x|?i=1?
(x, y, i)When employing joint feature functions and com-bined features, the number of unique features maygrow very large.
This is a problem when theamount of internal memory is limited.
Featurehashing, as described by Weinberger et al (2009),is a simple trick to circumvent this problem.
As-sume that we have an original feature function?
: X ?
Y 7?
<m, where m might be arbitrar-ily large.
Let h : N+ 7?
[1, n] be a hash functionand let h?1(i) ?
[1,m] be the set of integers suchthat j ?
h?1(i) iff h(j) = i.
We now use thishash function to map the index of each feature in?
(x, y) to its corresponding index in ?
(x, y), as?i(x, y) =?j?h?1(i) ?j(x, y).
The features in ?are thus unions of multisets of features in ?.
Givena hash function with good collision properties, wecan expect that the subset of features mapped toany index in?
(x, y) is small and composed of ele-ments drawn at random from ?
(x, y).
Weinbergeret al (2009) contains proofs of bounds on thesedistributions.
Furthermore, by using a k-valuedhash function h : Nk 7?
[1, n], the Cartesian prod-uct of k feature sets can be computed much moreefficiently, compared to using a dictionary.3.2 Position Based Feature DefinitionsFor in-sentence cue and scope prediction we makeuse of the same token level feature templates asfor sentence level detection.
An additional levelof expressivity is added in that each token leveltemplate is associated with a token position.
Atemplate is addressed either relative to the tokencurrently being decoded, or by the dependency arcof a token, which in turn is addressed by a relativeposition.
The addressing can be either to a singleposition, or a range of positions.
Feature templatesmay further be defined with respect to features ofthe input pattern, the token level labels predictedso far, or with respect to combinations of inputand label features.
Joint features, just as complexfeature combinations, are created by forming theCartesian product of an input feature set and a la-bel feature set.The feature templates are instantiated by pre-fixing the template name to each member of thefeature set.
To exemplify, the single position tem-plate TOKENi, given that the token currently be-ing decoded at position i is suggests, is instanti-ated as the singleton set {TOKENi = suggests}.The range template TOKENi,i+1, given that thecurrent token is suggests and the next token isthat, is instantiated as the set {TOKENi,i+1 =suggests, TOKENi,i+1 = that}; i.e.
each memberof the set is prefixed by the range template name.In addition to the token level templates used forsentence level prediction, the following templateswere explored:LABEL Label predicted so far at the addressed position(s).HEAD.X An arbitrary feature, X, addressed by follow-ing the dependency arc(s) from the addressed posi-tion(s).
For example, HEAD.LEMMAi corresponds tothe lemma found by looking at the dependency head ofthe current token.CUE, CUESCOPE Whether the token(s) addressed is re-spectively, a cue marker, or within the syntactic scopeof the current cue, following the definition of scopeprovided by Vincze et al (2008).3.3 Feature Template SelectionJust as with sentence level detection, we used agreedy forward selection strategy when searchingfor the optimal subset of feature templates.
Thecue and scope detection subtasks were optimisedseparately.88The scoring measures used in the search forcue and scope detection features differ.
In orderto match the official scoring measure for cue de-tection, we optimise the F1-score of labels cor-responding to cue tags, i.e.
we treat the BEGINand INSIDE cue tags as an equivalence class.
Theofficial scoring measure for scope prediction, onthe other hand, corresponds to the exact matchof scope boundaries.
Unfortunately using exactmatch performance turned out to be not very wellsuited for use in greedy forward selection.
Thisis because before a sufficient per token accuracyhas been reached, and even when it has, the ex-act match score may fluctuate wildly.
Therefore,as a substitute, we instead guide the search by to-ken level accuracy.
This discrepancy between thesearch criterion and the official scoring metric isunfortunate.Again, when taking into account position ad-dressing, joint features and combined features, thecomplete set of explored templates is too large tofit in the current experiment.
The selected featurestogether with their corresponding scores are foundin Table 2.Task Template set Dev F1 Test F1CueTOKENi 74.0 (1.5) -?
TOKENi?1 81.0 (.30) 68.78?
MORPHi 83.6 (.10) 74.06?
LEMMAi ?
LEMMAi+1 85.6 (.20) 78.41?
SYNi 86.5 (.41) 78.28?
LEMMAi?1 ?
LEMMAi 86.7 (.42) 78.52ScopeCueScopei 66.9 (.92) -?
LABELi?2,i?1 79.5 (.67) 34.80?
LEMMAi 82.4 (1.1) 33.18?
MORPHi 83.1 (.35) 35.70?
CUEi?2,i?1 83.4 (.13) 40.14?
CUEi,i+1,i+2 83.6 (.11) 41.15?
LEMMAi?1 84.1 (.16) 40.04?
MORPHi 84.4 (.33) 40.04?
TOKENi+1 84.5 (.09) 39.64Table 2: Top feature templates for in-sentence de-tection of hedge cues and scopes.4 DiscussionOur final F1-score results for the corrected systemare, in Task 1 for the biological domain 85.97, forthe Wikipedia domain 58.25; for Task 2, our re-sults are 39.64 for the entire task with a score of78.52 for cue detection.Any gold standard-based shared experiment un-avoidably invites discussion on the reliability ofthe gold standard.
It is easy to find borderline ex-amples in the evaluation corpus, e.g.
sentencesthat may just as well be labeled ?certain?
ratherthan ?uncertain?.
This gives an indication of thetrue complexity of assessing the hidden variable ofuncertainty and coercing it to a binary judgmentrather than a dimensional one.
It is unlikely thateveryone will agree on a binary judgment everytime.To improve experimental results and the gen-eralisability of the results for the task of detect-ing uncertain information on a sentence level, wewould need to break reliance on the purely lexicalcues.
For instance, we now have identified possi-ble and putative as markers for uncertainty, but inmany instances they are not (Finally, we wish toensure that others can use and evaluate the GRECas simply as possible).
This would be avoidablethrough either a deeper analysis of the sentenceto note that possible in this case does not modifyanything of substance in the sentence, or alterna-tively through a multi-word term preprocessor toidentify as simply as possible as an analysis unit.In the Wikipedia experiment, where the objec-tive is to identify weasel phrases, the judicious en-coding of quantifiers such as ?some of the mostwell-known researchers say that X?
would belikely to identify the sought-for sentences whenthe quantified NP is in subject position.
In ourexperiment we find that our dependency analysisdid not distinguish between the various syntacticroles of quantified NPs.
As a result, we markedseveral sentences with a quantifier as a ?weasel?sentence, even where the quantified NP was in anon-subject role ?
leading to overly many weaselsentences.
An example is given in Table 3.If certainty can be identified separately, not asabsence of overt uncertainty, identifying uncer-tainty can potentially be aided through the iden-tification of explicit certainty together with nega-tion, as found by Kilicoglu and Bergler (2008).
Inkeeping with their results, we found negations in asizeable proportion of the annotated training mate-rial.
Currently we capture negation as a lexical cuein immediate bigrams, but with longer range nega-tions, we will miss some clear cases: Table 3 givestwo examples.
To avoid these misses, we will bothneed to identify overt expressions of certainty andto identify and track the scope of negation ?
thefirst challenge is unexplored but would not seemto be overly complex; the second is a well-known89and established challenge for NLP systems in gen-eral.In the task of detecting in-sentence uncertainty?
identification of hedge cues and their scopes ?we find that an evaluation method based on ex-act match of a token sequence is overly unforgiv-ing.
There are many cases where the marginal to-kens of a sequence are less than central or irrele-vant for the understanding of the hedge cue and itsscope: moving the boundary by one position overan uninteresting token may completely invalidatean otherwise arguably correct analysis.
A token-by-token scoring would be a more functional eval-uation criterion, or perhaps a fuzzy match, allow-ing for a certain amount of erroneous characters.For our experiments, this has posed some chal-lenges.
While we model the in-sentence un-certainty detection as a sequence labelling prob-lem in the BIO-representation (BEGIN, INSIDE,OUTSIDE), the provided corpus uses an XML-representation.
Moreover, the official scoring toolrequires that the predictions are well formed XML,necessitating a conversion from XML to BIO priorto training and from BIO to XML after prediction.Consistent tokenisation is important, but the syn-tactic analysis components used by us distorted theoriginal tokenisation and restoring the exact sametoken sequence proved problematic.Conversion from BIO to XML is straightforwardfor cues, while some care must be taken when an-notating scopes, since erroneous scope predictionsmay result in malformed XML.
When adding thescope annotation, we use a stack based algorithm.For each sentence, we simultaneously traverse thescope-sequence corresponding to each cue, left toright, token by token.
The stack is used to en-sure that scopes are either separated or nested andan additional restriction ensures that scopes maynever start or end inside a cue.
In case the al-gorithm fails to place a scope according to theserestrictions, we fall back and let the scope coverthe whole sentence.
Several of the more frequenterrors in our analyses are scoping errors, manylikely to do with the fallback solution.
Our analy-sis quite frequently fails also to assign the subjectof a sentence to the scope of a hedging verb.
Ta-ble 3 shows one example each of these errors ?overextended scope and missing subject.Unfortunately, the tokenisation output by ouranalysis components is not always consistent withthe tokenisation assumed by the BioScope annota-tion.
A post-processing step was therefore addedin which each, possibly complex, token in the pre-dicted BIO-sequence is heuristically mapped to itscorresponding position in the XML structure.
Thispost-processing is not perfect and scopes and cuesat non-word token boundaries, such as parenthe-ses, are quite often misplaced with respect to theBioScope annotation.
Table 3 gives one examplewhich is scored ?erroneous?
since the token ?
(63)?is in scope, where the ?correct?
solution has it out-side the scope.
These errors are not important toaddress, but are quite frequent in our results ?
ap-proximately 80 errors are of this type.To achieve more general and effective methodsto detect uncertainty in an argument, we shouldnote that uncertainty is signalled in a text throughmany mechanisms, and that the purely lexical andexplicit signal found through the present experi-ments in hedge identification is effective and use-ful, but will not catch everything we might want tofind.
Lexical approaches are also domain depen-dent.
For instance, Szarvas (2008) and Moranteand Daelemans (2009) report loss in performance,when applying the same methods developed on bi-ological data, on clinical text.
Using the systemsdeveloped for scientific text elsewhere poses a mi-gration challenge.
It would be desirable both toautomatically learn a hedging lexicon from a gen-eral seed set and to have features on a higher levelof abstraction.Our main result is that casting this task as a se-quence labelling problem affords us the possibilityto combine linguistic analyses with a highly effi-cient implementation of a max-margin predictionalgorithm.
Our framework processes the data setsin minutes for training and seconds for predictionon a standard personal computer.5 AcknowledgementsThe authors would like to thank Joakim Nivrefor feedback in earlier stages of this work.
Thiswork was funded by The Swedish National Grad-uate School of Language Technology and by theSwedish Research Council.ReferencesRong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A libraryfor large linear classification.
Journal of Machine Learn-ing Research, 9:1871?1874.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos90Neg + certain However, how IFN-?
and IL-4 inhibit IL-17 production is not yet known.Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.?some?
: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj .?some?
: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.Prediction dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?
becausedRas85DV12 can activate endogenous PI3K signaling [16]</xcope>.Gold standard dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?</xcope> becausedRas85DV12 can activate endogenous PI3K signaling [16].Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR?t<xcope .1>are still <cue .1>unclear</cue></xcope>.Gold standard However, <xcope .1>the precise molecular mechanisms of Stat3-mediated expression of ROR?tare still <cue .1>unclear</cue></xcope>.Prediction Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit ROR?tactivity on its target genes, at least in par,t through direct interaction with ROR?t (63)</xcope>.Gold standard Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit RORtactivity on its target genes, at least in par,t through direct interaction with RORt</xcope> (63).Table 3: Examples of erroneous analyses.Csirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010Shared Task: Learning to Detect Hedges and their Scopein Natural Language Text.
In Proceedings of the 14thConference on Computational Natural Language Learn-ing (CoNLL-2010): Shared Task, pages 1?12, Uppsala,Sweden, July.
Association for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Finding hedgesby chasing weasels: hedge detection using Wikipedia tagsand shallow linguistic features.
In ACL-IJCNLP ?09: Pro-ceedings of the ACL-IJCNLP 2009 Conference Short Pa-pers, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Halil Kilicoglu and Sabine Bergler.
2008.
Recognizing spec-ulative language in biomedical research articles: a linguis-tically motivated perspective.
BMC Bioinformatics, 9.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.
InProc.
18th Int.
Conf.
on Machine Learning.
Morgan Kauf-mann Publishers.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004.The language of bioscience: Facts, speculations, and state-ments in between.
In Lynette Hirschman and JamesPustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-oLINK 2004, Linking Biological Literature, Ontologiesand Databases, Boston, USA.
ACL.Ben Medlock and Ted Briscoe.
2007.
Weakly supervisedlearning for hedge classification in scientific literature.
InProceedings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, Prague, Czech Repub-lic.
Association for Computational Linguistics.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Informatics,41(4):636?654.Roser Morante and Walter Daelemans.
2009.
Learning thescope of hedge cues in biomedical texts.
In BioNLP ?09:Proceedings of Workshop on BioNLP, Morristown, NJ,USA.
ACL.Andrew Y. Ng.
2004.
Feature selection, l1 vs. l2 regulariza-tion, and rotational invariance.
In ICML ?04: Proceedingsof the 21st International Conference on Machine learning,page 78, New York, NY, USA.
ACM.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detectingspeculations and their scopes in scientific text.
In Pro-ceedings of 2009 Conference on Empirical Methods inNatural Language Processing, Singapore.
ACL.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, andJan Svartvik.
1985.
A comprehensive grammar of theEnglish language.
Longman.Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-vich.
2007.
(Online) subgradient methods for structuredprediction.
In Eleventh International Conference on Arti-ficial Intelligence and Statistics (AIStats).Gyo?rgy Szarvas.
2008.
Hedge classification in biomedicaltexts with a weakly supervised selection of keywords.
InProceedings of ACL-08: HLT, Columbus, Ohio.
ACL.Pasi Tapanainen and Timo Ja?rvinen.
1997.
A non-projectivedependency parser.
In Proceedings of the 5th Conferenceon Applied Natural Language Processing.Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003.Max-margin Markov networks.
In Sebastian Thrun,Lawrence K. Saul, and Bernhard Scho?lkopf, editors,NIPS.
MIT Press.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large margin methodsfor structured and interdependent output variables.
Jour-nal of Machine Learning Research, 6:1453?1484.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas, Gyo?rgyMo?ra, and Ja?nos Csirik.
2008.
The BioScope corpus:biomedical texts annotated for uncertainty, negation andtheir scopes.
BMC Bioinformatics, 9(S-11).Kilian Weinberger, Anirban Dasgupta, John Langford, AlexSmola, and Josh Attenberg.
2009.
Feature hashing forlarge scale multitask learning.
In ICML ?09: Proceedingsof the 26th Annual International Conference on MachineLearning, New York, NY, USA.
ACM.91
