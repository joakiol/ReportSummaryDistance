Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 148?150,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Baseline Approach for Detecting Sentences Containing UncertaintyErik Tjong Kim SangUniversity of Groningenerikt@xs4all.nlAbstractWe apply a baseline approach to theCoNLL-2010 shared task data sets onhedge detection.
Weights have been as-signed to cue words marked in the train-ing data based on their occurrences incertain and uncertain sentences.
Newsentences received scores that correspondwith those of their best scoring cue word,if present.
The best acceptance scores foruncertain sentences were determined us-ing 10-fold cross validation on the trainingdata.
This approach performed reasonablyon the shared task?s biological (F=82.0)and Wikipedia (F=62.8) data sets.1 IntroductionCoNLL-2010 offered two shared tasks which in-volve finding text parts which express uncertaintyor unreliability (Farkas et al, 2010).
We focuson Task 1, identifying sentences which containstatements which can be considered uncertain orunreliable.
We train a basic statistical model onthe training data supplied for the task, apply thetrained model to the test data and discuss the re-sults.
The next section describes the format ofthe data and introduces the model that was used.Section three discusses the experiments with themodel and their results.
Section four concludesthe paper.2 Data and modelThe CoNLL-2010 shared task training data setscontain sentences which are classified as eithercertain or uncertain.
Sentences of the uncertainclass contain one or more words which have beenmarked as indicator of uncertainty, the so-calledhedge cues.
Here is an example of such a sentencewith the hedge cues written in bold font:These results indicate that in mono-cytic cell lineage, HIV-1 could mimicsome differentiation/activation stimuliallowing nuclear NF-KB expression.CoNLL-2010 offers two shared tasks: classify-ing sentences in running text as either certain oruncertain (Task 1) and finding hedge cues in sen-tences classified as uncertain together with theirscopes (Task 2).
We have only participated inTask 1.We built a basic model for the training data, tak-ing advantage of the fact that the hedge cues weremarked explicitly.
We estimated the probability ofeach training data word appearing in a hedge cuewith unigram statistics:P (w in cue) = f(w in cue)f(w)where P (w in cue) is the probability that word wappears in a hedge cue, f(w) is frequency of theword w in the data and f(w in c) is the frequencyof the word inside hedge cues.
We performed onlylittle text preprocessing, converting all words tolower case and separating six common punctua-tion signs from the words.In the classification stage, we assigned to eachword the estimated hedge cue probability accord-ing to the training data.
Next, we assigned a scoreto each sentence that was equal to one minus thehighest individual score of its words:P (s is certain) = 1?
argmaxw in sP (w in cue)P (s is certain) is the estimated probability thatthe sentence s is certain, and it is equal to one mi-nus the highest probability of any of its words be-ing part of a hedge cue.
So a sentence contain-ing only words that never appeared as a hedge cuewould receive score 1.0.
Meanwhile a sentence148with a single word that had appeared in a hedgecue in the training data would receive one minusthe probability associated with that word.
Thismodel ignores any relations between the wordsof the sentence.
We experimented with combin-ing the scores of the different words but found theminimum word score to perform best.3 ExperimentsApart from the word probabilities, we needed toobtain a good threshold score for deciding whetherto classify a sentence as certain or uncertain.For this purpose, we performed a 10-fold cross-validation experiment on each of the two trainingdata files (biological andWikipedia) and measuredthe effect of different threshold values.
The resultscan be found in Figure 1.The model performed well on the biologicaltraining data, with F scores above 80 for a largerange of threshold values (0.15?0.85).
It per-formed less well on the Wikipedia training data,with a maximum F score of less than 60 and 50+scores being limited to the threshold range 0.45?0.85.
The maximum F scores were reached forthreshold values 0.55 and 0.65 for biological data(F=88.8) and Wikipedia data (F=59.4), respec-tively.
We selected the threshold value 0.55 forour further work because the associated precisionand recall values were closer to each other than forvalue 0.65.We build domain-specific models with the bio-logical data (14,541 sentences) and the Wikipediadata (11,111 sentences) and applied the models tothe related training data.
We obtained an F scoreof 80.2 on the biological data (13th of 20 partici-pants) and a score of 54.4 on the Wikipedia data(9th of 15 participants).
The balance between pre-cision and recall scores that we strived for whenprocessing the training data, was not visible in thetest results.
On the biological test data the sys-tem?s recall score was 13 points higher than theprecision score while on the Wikipedia test dataprecision outperformed recall by 31 points (seeTable 1).Next, we tested the effect of increasing the datasets with data from another domain.
We repeatedthe cross-validation experiments with the trainingdata, this time adding the available data of theother domain to each of the sets of nine folds usedas training data.
Unfortunately, this did not re-sult in a performance improvement.
The best per-train-test thre.
Precis.
Recall F?=1bio-bio .55 74.3% 87.1% 80.2?1.0wik-wik .55 74.0% 43.0% 54.4?0.9all-bio .55 69.3% 74.6% 71.8?1.2all-wik .55 69.0% 44.6% 54.2?1.0Table 1: Performances of the models for differentcombinations of training and test data sets with theassociated acceptance threshold values.
Trainingand testing with data from the same domain pro-duces the best scores.
Higher recall scores wereobtained for biological data than for Wikipediadata.
Standard deviations for F scores were esti-mated with bootstrap resampling (Yeh, 2000).formance for the biological data dropped to F =84.2 (threshold 0.60) while the top score for theWikipedia data dropped to F = 56.5 (0.70).We kept the threshold value of 0.55, built amodel from all available training data and testedits performance on the two test sets.
In both casesthe performances were lower than the ones ob-tained with domain dependent training data: F =71.8 for biological data and F = 54.2 for Wikipediadata (see Table 1).As post-deadline work, we added statistics forword bigrams to the model, following up workby Medlock (2008), who showed that consideringword bigrams had a positive effect on hedge detec-tion.
We changed the probability estimation scoreof words appearing in a hedge cue toP (wi?1wi in cue) = f(wi?1wi in cue)f(wi?1wi)where wi?1wi is a bigram of successive words in asentence.
Bigrams were considered to be part of ahedge cue when either or both words were insidethe hedge cue.
Unigram probabilities were usedas backoff for known words that appeared outsideknown bigrams while unknown words received themost common score for known words (0).
Sen-tences received a score which is equal to one mi-nus the highest score of their word bigrams:P (s is certain) = 1?
argmaxwi?1wi in sP (wi?1wi in cue)We repeated the threshold estimation experimentsand found that new bigram scores enabled themodels to perform slightly better on the training149Figure 1: Precision-recall plot (left) and F plot (right) for different values of the certainty acceptancethresholds measured by 10-fold cross-validation experiments on the two shared task training data sets(biological and Wikipedia).
The best attained F scores were 88.8 for biological data (threshold 0.55) and59.4 for Wikipedia data (0.65).data.
The maximum F score for biological trainingdata improved from 88.8 to 90.1 (threshold value0.35) while the best F score for the Wikipediatraining data moved up slightly to 59.8 (0.65).We applied the bigram models with the two op-timal threshold values for the training data to thetest data sets.
For the biological data, we obtainedan F score of 82.0, a borderline significant im-provement over the unigram model score.
Theperformance on the Wikipedia data improved sig-nificantly, by eight points, to F = 62.8 (see Table2).
This is also an improvement of the officialbest score for this data set (60.2).
We believe thatthe improvement originates from using the bigrammodel as well as applying a threshold value thatis better suitable for the Wikipedia data set (notethat in our unigram experiments we used the samethreshold value for all data sets).4 Concluding remarksWe applied a baseline model to the sentence clas-sification part of the CoNLL-2010 shared task onhedge detection.
The model performed reason-ably on biological data (F=82.0) but less well onWikipedia data (F=62.8).
The model performedbest when trained and tested on data of the samedomain.
Including additional training data fromanother domain had a negative effect.
Adding bi-gram statistics to the model, improved its perfor-mance on Wikipedia data, especially for recall.Although the model presented in this paper per-forms reasonably on the hedge detection tasks, itis probably too simple to outperform more com-plex models.
However, we hope to have shown itstrain-test thre.
Precis.
Recall F?=1bio-bio .35 79.8% 84.4% 82.0?1.1wik-wik .65 62.2% 63.5% 62.8?0.8all-bio .50 73.2% 77.7% 75.4?1.2all-wik .60 63.5% 57.9% 60.6?0.9Table 2: Performances of bigram models for dif-ferent combinations of training and test data sets.The bigram models performed better than the uni-gram models (compare with Table 1).usefulness as baseline and as possible feature formore advanced models.
We were surprised aboutthe large difference in performance of the modelon the two data sets.
However, similar perfor-mance differences were reported by other partic-ipants in the shared task, so they seem data-relatedrather than being an effect of the chosen model.Finding the origin of the performance differenceswould be an interesting goal for future work.ReferencesRicha?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proc.of the Computational Natural Language Learning(CoNLL-2010): Shared Task, pages 1?12.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Infor-matics, 41:636?654.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th International Conference onComputational Linguistics, pages 947?953.150
