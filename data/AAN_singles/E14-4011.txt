Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54?58,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsLightly-Supervised Word Sense Translation Error Detection for anInteractive Conversational Spoken Language Translation SystemDennis N. Mehay, Sankaranarayanan Ananthakrishnan and Sanjika HewavitharanaSpeech, Language and Multimedia Processing UnitRaytheon BBN TechnologiesCambridge, MA, 02138, USA{dmehay,sanantha,shewavit}@bbn.comAbstractLexical ambiguity can lead to concepttransfer failure in conversational spo-ken language translation (CSLT) systems.This paper presents a novel, classification-based approach to accurately detectingword sense translation errors (WSTEs) ofambiguous source words.
The approachrequires minimal human annotation effort,and can be easily scaled to new languagepairs and domains, with only a word-aligned parallel corpus and a small set ofmanual translation judgments.
We showthat this approach is highly precise in de-tecting WSTEs, even in highly skeweddata, making it practical for use in an in-teractive CSLT system.1 IntroductionLexical ambiguity arises when a single word formcan refer to different concepts.
Selecting a con-textually incorrect translation of such a word ?here referred to as a word sense translation error(WSTE) ?
can lead to a critical failure in a con-versational spoken language translation (CSLT)system, where accuracy of concept transfer isparamount.
Interactive CSLT systems are espe-cially prone to mis-translating less frequent wordsenses, when they use phrase-based statistical ma-chine translation (SMT), due to its limited use ofsource context (source phrases) when constructingtranslation hypotheses.
Figure 1 illustrates a typi-cal WSTE in a phrase-based English-to-Iraqi Ara-bic CSLT system, where the English word boardDisclaimer: This paper is based upon work supported bythe DARPA BOLT program.
The views expressed here arethose of the authors and do not reflect the official policy orposition of the Department of Defense or the U.S. Govern-ment.Distribution Statement A (Approved for Public Release,Distribution Unlimited)Figure 1: Example WSTE in English-to-Iraqi SMT.is mis-translated as mjls (?council?
), completelydistorting the intended message.Interactive CSLT systems can mitigate thisproblem by automatically detecting WSTEs inSMT hypotheses, and engaging the operator in aclarification dialogue (e.g.
requesting an unam-biguous rephrasing).
We propose a novel, two-level classification approach to accurately detectWSTEs.
In the first level, a bank of word-specificclassifiers predicts, given a rich set of contextualand syntactic features, a distribution over possi-ble target translations for each ambiguous sourceword in our inventory.
A single, second-level clas-sifier then compares the predicted target words tothose chosen by the decoder and determines thelikelihood that an error was made.A significant novelty of our approach is that thefirst-level classifiers are fully unsupervised withrespect to manual annotation and can easily beexpanded to accommodate new ambiguous wordsand additional parallel data.
The other innovativeaspect of our solution is the use of a small set ofmanual translation judgments to train the second-level classifier.
This classifier uses high-level fea-tures derived from the output of the first-level clas-sifiers to produce a binary WSTE prediction, andcan be re-used unchanged even when the first levelof classifiers is expanded.Our goal departs from the large body of workdevoted to lightly-supervised word sense disam-biguation (WSD) using monolingual and bilingualcorpora (Yarowsky, 1995; Schutze, 1998; Diaband Resnik, 2002; Ng et al., 2003; Li and Li, 2002;Purandare and Pedersen, 2004), which seeks to la-54bel and group unlabeled sense instances.
Instead,our approach detects mis-translations of a knownset of ambiguous words.The proposed method also deviates from ex-isting work on global lexical selection models(Mauser et al., 2009) and on integration of WSDfeatures within SMT systems with the goal of im-proving offline translation performance (Chan etal., 2007).
Rather, we detect translation errors dueto ambiguous source words with the goal of pro-viding feedback to and soliciting clarification fromthe system operator in real time.
Our approachis partly inspired by Carpuat and Wu?s (2007b;2007a) unsupervised sense disambiguation mod-els for offline SMT.
More recently, Carpuat et al.
(2013) identify unseen target senses in new do-mains, but their approach requires the full test cor-pus upfront, which is unavailable in spontaneousCSLT.
Our approach can, in principle, identifynovel senses when unfamiliar source contexts areencountered, but this is not our current focus.2 Baseline SMT SystemIn this paper, we focus on WSTE detection inthe context of phrase-based English-to-Iraqi Ara-bic SMT, an integral component of our interac-tive, two-way CSLT system that mediates con-versation between monolingual speakers of En-glish and Iraqi Arabic.
The parallel training cor-pus of approximately 773K sentence pairs (7.3MEnglish words) was derived from the DARPATransTac English-Iraqi two-way spoken dialoguecollection and spans a variety of domains includ-ing force protection, medical diagnosis and aid,etc.
Phrase pairs were extracted from bidirectionalIBM Model 4 word alignment after applying amerging heuristic similar to that of Koehn et al.(2003).
A 4-gram target LM was trained on IraqiArabic transcriptions.
Our phrase-based decoder,similar to Moses (Koehn et al., 2007), performsbeam search stack decoding based on a standardlog-linear model, whose parameters were tunedwith MERT (Och, 2003) on a held-out develop-ment set (3,534 sentence pairs, 45K words).
TheBLEU and METEOR scores of this system on aseparate test set (3,138 sentence pairs, 38K words)were 16.1 and 42.5, respectively.3 WSTE DetectionThe core of the WSTE detector is a novel, two-level classification pipeline.
Our approach avoidsFigure 2: An English?Iraqi training pair.the need for expensive, sense-labeled training databased on the observation that knowing the sense ofan ambiguous source word is distinct from know-ing whether a sense translation error has occurred.Instead, the target (Iraqi Arabic) words typicallyassociated with a given sense of an ambiguoussource (English) word serve as implicit sense la-bels, as the following describes.3.1 A First Level of Unsupervised ClassifiersThe main intuition behind our approach is thatstrong disagreement between the expanded con-text of an ambiguous source word and the corre-sponding SMT hypothesis indicates an increasedlikelihood that a WSTE has occurred.
To identifysuch disagreement, we train a bank of maximum-entropy classifiers (Berger et al., 1996), one foreach ambiguous word.
The classifiers are trainedon the same word-aligned parallel data used fortraining the baseline SMT system, as follows.For each instance of an ambiguous source wordin the training set, and for each target word it isaligned to, we emit a training instance associatingthat target word and the wider source context ofthe ambiguous word.
Figure 2 illustrates a typicaltraining instance for the ambiguous English wordboard, which emits a tuple of contextual featuresand the aligned Iraqi Arabic word lwHp (?plac-ard?)
as a target label.
We use the following con-textual features similar to those of Carpuat andWu (2005), which are in turn based on the clas-sic WSD features of Yarowsky (1995).Neighboring Words/Lemmas/POSs.
The to-kens, t, to the left and right of the current ambigu-ous token, as well as all trigrams of tokens thatspan the current token.
Separate features for word,lemma and parts of speech tokens, t.Lemma/POS Dependencies.
The lemma-lemma and POS-POS labeled and unlabeleddirected syntactic dependencies of the currentambiguous token.55Figure 3: An unsupervised first-level classifier.Bag-of-words/lemmas.
Distance decayed bag-of-words-style features for each word and lemmain a seven-word window around the current token.Figure 3 schematically illustrates how this classi-fier operates on a sample test sentence.
The ex-ample assumes that the ambiguous English wordboard is only ever associated with the Iraqi Arabicwords lwHp (?placard?)
and mjls (?council?)
inthe training word alignment.
We emphasize thateven though the first-level maximum entropy clas-sifiers are intrinsically supervised, their trainingdata is derived via unsupervised word alignment.3.2 A Second-Level Meta-ClassifierThe first-level classifiers do not directly predictthe presence of a WSTE, but induce a distribu-tion over possible target words that could be gen-erated by the ambiguous source word in that con-text.
In order to make a binary decision, this distri-bution must be contrasted with the correspondingtarget phrase hypothesized by the SMT decoder.One straightforward approach, which we use asa baseline, is to threshold the posterior probabil-ity of the word in the SMT target phrase which isranked highest in the classifier-predicted distribu-tion.
However, this approach is not ideal becauseeach classifier has a different target label set and istrained on a different number of instances.To address this issue, we introduce a secondmeta-classifier, which is trained on a small numberof hand-annotated translation judgments of SMThypotheses of source sentences containing am-biguous words.
The bilingual annotator was sim-ply asked to label the phrasal translation of sourcephrases containing ambiguous words as correct orincorrect.
We obtained translation judgments for511 instances from the baseline SMT developmentand test sets, encompassing 147 pre-defined am-biguous words obtained heuristically from Word-Net, public domain homograph lists, etc.The second-level classifier is trained on a smallFigure 4: The two-level WSTE architecture.set of meta-features drawn from the predictions ofthe first-level classifiers and from simple statisticsof the training corpus.
For an ambiguous wordwain source sentence S, with contextual featuresf1(S), and aligned to target words t ?
T (the setof words in the target phrase) in the SMT hypoth-esis, we extract the following features:1.
The first-level classifier?s maximumlikelihood of any decoded target word:maxt?Tpwa(t|f1(S))2.
The entropy of the predicted distribution:?tpwa(t|f1(S)) ?
ln(pwa(t|f1(S)))3.
The number of training instances for wa4.
The inverse of the number of distinct targetlabels for wa.5.
The product of meta-features (1) and (4)A high value for feature 1 indicates that the first-level model and the SMT decoder agree.
By con-trast, a high value for feature 2 indicates uncer-tainty in the classifier?s prediction, due either to anovel source context, or inadequate training data.Feature 3 indicates whether the second scenario ofmeta-feature 2 might be at play, and feature 4 canbe thought of as a simple, uniform prior for eachclassifier.
Finally, feature 5 attenuates feature 1by this simple, uniform prior.
We feed these fea-tures to a random forest (Breiman, 2001), whichis a committee of decision trees, trained using ran-domly selected features and data points, using theimplementation in Weka (Hall et al., 2009).
Thetarget labels for training the second-level classifierare obtained from the binary translation judgmentson the small annotated corpus.
Figure 4 illustratesthe interaction of the two levels of classification.563.3 Scalability and PortabilityScalability was an important consideration in de-signing the proposed WSTE approach.
For in-stance, we may wish to augment the inventorywith new ambiguous words if the vocabularygrows due to addition of new parallel data or dueto a change in the domain.
The primary advan-tage of the two-level approach is that new ambigu-ous words can be accommodated by augmentingthe unsupervised first-level classifier set with addi-tional word-specific classifiers, which can be doneby simply extending the pre-defined list of am-biguous words.
Further, the current classificationstack requires only ?1.5GB of RAM and performsper-word WSTE inference in only a few millisec-onds on a commodity, quad-core laptop, which iscritical for real-time, interactive CSLT.The minimal annotation requirements also al-low a high level of portability to new languagepairs.
Moreover, as our results indicate (below), agood quality WSTE detector can be bootstrappedfor a new language pair without any annotation ef-fort by simply leveraging the first-level classifiers.4 Experimental ResultsThe 511 WSTE-annotated instances used for train-ing the second-level classifier doubled as an eval-uation set using the leave-one-out cross-validationmethod.
Of these, 115 were labeled as errors bythe bilingual judge, while the remaining 396 weretranslated correctly by the baseline SMT system.The error prediction score from the second-levelclassifier was thresholded to obtain the receiveroperating characteristic (ROC) curve shown in thetop (black) curve of Figure 5.
We obtain a 43%error detection rate with only 10% false alarmsand 71% detection with 20% false alarms, in spiteof the highly skewed label distribution.
In abso-lute terms, true positives outnumber false alarmsat both the 10% (49 to 39) and 20% (81 to 79) falsealarm rates.
This is important for deployment, aswe do not want to disrupt the flow of conversationwith more false alarms than true positives.For comparison, the bottom (red) ROC curveshows the performance of a baseline WSTE pre-dictor comprised of just meta-feature (1), obtain-able directly from the first-level classifiers.
Thisperforms slightly worse than the two-level modelat 10% false alarms (40% detection, 46 true pos-itives, 39 false alarms), and considerably worseat 20% false alarms (57% detection, 66 true pos-Figure 5: WST error detection ROC curve.itives, 78 false alarms).
Nevertheless, this resultindicates the possibility of bootstrapping a goodquality baseline WSTE detector in a new languageor domain without any annotation effort.5 ConclusionWe proposed a novel, lightly-supervised, two-level classification architecture that identifies pos-sible mis-translations of pre-defined ambiguoussource words.
The WSTE detector pre-emptscommunication failure in an interactive CSLT sys-tem by serving as a trigger for initiating feed-back and clarification.
The first level of our de-tector comprises of a bank of word-specific classi-fiers trained on automatic word alignment over theSMT parallel training corpus.
Their predicted dis-tributions over target words feed into the second-level meta-classifier, which is trained on a smallset of manual translation judgments.
On a 511-instance test set, the two-level approach exhibitsWSTE detection rates of 43% and 71% at 10%and 20% false alarm rates, respectively, in spite ofa nearly 1:4 skew against actual WSTE instances.Because adding new ambiguous words to the in-ventory only requires augmenting the set of first-level unsupervised classifiers, our WSTE detec-tion approach is scalable to new domains andtraining data.
It is also easily portable to new lan-guage pairs due to the minimal annotation effortrequired for training the second-level classifier.
Fi-nally, we show that it is possible to bootstrap agood quality WSTE detector in a new languagepair without any annotation effort using only un-supervised classifiers and a parallel corpus.57ReferencesAdam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A Maximum Entropy Ap-proach to Natural Language Processing.
Computa-tional Linguistics, 22(1):39?71.Leo Breiman.
2001.
Random Forests.
Technical re-port, Statistics Department, University of California,Berkeley, Berkeley, CA, USA, January.Marine Carpuat and Dekai Wu.
2005.
Word sense dis-ambiguation vs. statistical machine translation.
InProceedings of the 43rd Annual Meeting on Associa-tion for Computational Linguistics, pages 387?394.Marine Carpuat and Dekai Wu.
2007a.
How phrasesense disambiguation outperforms word sense dis-ambiguation for statistical machine translation.
InProceedings of the 11th International Conference onTheoretical and Methodological Issues in MachineTranslation (TMI 2007), Skovde, Sweden, Septem-ber.Marine Carpuat and Dekai Wu.
2007b.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72, Prague,Czech Republic, June.Marine Carpuat, Hal Daume?
III, Katharine Henry,Ann Irvine, Jagadeesh Jagarlamudi, and RachelRudinger.
2013.
Sensespotting: Never let your par-allel data tie you to an old domain.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1435?1445, Sofia, Bulgaria, August.Association for Computational Linguistics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statisti-cal machine translation.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 33?40, Prague, Czech Republic,June.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel cor-pora.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages255?262, July.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11(1).Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54, Morristown, NJ, USA.Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Hang Li and Cong Li.
2002.
Word translation dis-ambiguation using bilingual bootstrapping.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 343?351, July.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.Extending statistical machine translation with dis-criminative and trigger-based lexicon models.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages210?218, Singapore, August.
Association for Com-putational Linguistics.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.Exploiting parallel texts for word sense disambigua-tion: An empirical study.
In Proceedings of 41stAnnual Meeting on Association for ComputationalLinguistics, pages 455?462, July.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Amruta Purandare and Ted Pedersen.
2004.
Wordsense discrimination by clustering contexts in vectorand similarity spaces.
In Proceedings of the Confer-ence on Computational Natural Language Learning,pages 41?48.Hinrich Schutze.
1998.
Automatic word sense dis-crimination.
Journal of Computational Linguistics,24:97?123.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd annual meeting on Associationfor Computational Linguistics, ACL ?95, pages 189?196, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.58
