Proceedings of the 10th Conference on Parsing Technologies, pages 1?10,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsUsing Self-Trained Bilexical Preferences to Improve DisambiguationAccuracyGertjan van NoordUniversity of Groningenvannoord@let.rug.nlAbstractA method is described to incorporate bilex-ical preferences between phrase heads, suchas selection restrictions, in a Maximum-Entropy parser for Dutch.
The bilexicalpreferences are modelled as association rateswhich are determined on the basis of a verylarge parsed corpus (about 500M words).We show that the incorporation of such self-trained preferences improves parsing accu-racy significantly.1 MotivationIn parse selection, the task is to select the correctsyntactic analysis of a given sentence from a setof parses generated by some other mechanism.
Onthe basis of correctly labelled examples, supervisedparse selection techniques can be employed to ob-tain reasonable accuracy.
Although parsing has im-proved enormously over the last few years, even themost successful parsers make very silly, sometimesembarassing, mistakes.
In our experiments with alarge wide-coverage stochastic attribute-value gram-mar of Dutch, we noted that the system sometimesis insensitive to the naturalness of the various lexicalcombinations it has to consider.
Although parsersoften employ lexical features which are in principleable to represent preferences with respect to wordcombinations, the size of the training data will betoo small to be able to learn the relevance of suchfeatures successfully.In maximum-entropy parsing, the supervisedparsing technique that we use in our experiments, ar-bitrary features can be defined which are employedto characterize different parses.
So it is possible toconstruct features for any property that is thoughtto be important for disambiguation.
However, suchfeatures can be useful for disambiguation only incase the training set contains a sufficient number ofoccurrences of these features.
This is problematic,in practice, for features that encode bilexical prefer-ences such as selection restrictions, because typicaltraining sets are much too small to estimate the rele-vance of features representing cooccurrences of twowords.
As a simple example consider the ambiguousDutch sentence(1) Melk drinkt de baby nietMilk drinks the baby notThe standard model of the parser we experimentedwith employs a wide variety of features includingsyntactic features and lexical features.
In particu-lar, the model also includes features which encodewhether or not the subject or the object is fronted ina parse.
Since subjects, in general, are fronted muchmore frequently than objects, the model has learntto prefer readings in which the fronted constituentis analysed as the subject.
Although the model alsocontains features to distinguish whether e.g.
milkoccurs as the subject or the object of drink, themodel has not learnt a preference for either of thesefeatures, since there were no sentences in the train-ing data that involved both these two words.To make this point more explicit, we found that inabout 200 sentences of our parsed corpus of 27 mil-lion sentences milk is the head of the direct objectof the verb drink.
Suppose that we would need atleast perhaps 5 to 10 sentences in our training corpus1in order to be able to learn the specific preferencebetween milk and drink.
The implication is thatwe would need a (manually labeled!)
training cor-pus of approximately 1 million sentences (20 mil-lion words).
In contrast, the disambiguation modelof the Dutch parser we are reporting on in this paperis trained on a manually labeled corpus of slightlyover 7,000 sentences (145,000 words).
It appearsthat semi-supervised or un-supervised methods arerequired here.Note that the problem not only occurs for artifi-cial examples such as (1); here are a few mis-parsedexamples actually encountered in a large parsed cor-pus:(2) a. Campari moet u gedronken hebbenCampari must have drunk youYou must have drunk Camparib.
De wijn die Elvis zou hebben gedronken alshij wijn zou hebben gedronkenThe wine Elvis would have drunk if he haddrunk wineThe wine that would have drunk Elvis if hehad drunk winec.
De paus heeft tweehonderd daklozen te etengehadThe pope had twohunderd homeless peoplefor dinnerIn this paper, we describe an alternative approachin which we employ pointwise mutual informa-tion association score in the maximum entropy dis-ambiguation model.
Pointwise mutual information(Fano, 1961) was used to measure strength of selec-tion restrictions for instance by Church and Hanks(1990).
The association scores used here are esti-mated using a very large parsed corpus of 500 mil-lion words (27 million sentences).
We show that theincorporation of this additional knowledge sourceimproves parsing accuracy.
Because the associationscores are estimated on the basis of a large corpusthat is parsed by the parser that we aim to improveupon, this technique can be described as a somewhatparticular instance of self-training.
Self-training hasbeen investigated for statistical parsing before.
Al-though naively adding self-labeled material to ex-tend training data is normally not succesfull, therehave been successful variants of self-learning forparsing as well.
For instance, in McClosky et al(2006) self-learning is used to improve a two-phaseparser reranker, with very good results for the clas-sical Wall Street Journal parsing task.Clearly, the idea that selection restrictions oughtto be useful for parsing accuracy is not new.
How-ever, as far as we know this is the first time that au-tomatically acquired selection restrictions have beenshown to improve parsing accuracy results.
Relatedresearch includes Abekawa and Okumura (2006)and Kawahara and Kurohashi (2006) where statis-tical information between verbs and case elementsis collected on the basis of large automatically anal-ysed corpora.2 Background: Alpino parserThe experiments are performed using the Alpinoparser for Dutch.
In this section we briefly describethe parser, as well as the corpora that we have usedin the experiments described later.2.1 Grammar and LexiconThe Alpino system is a linguistically motivated,wide-coverage grammar and parser for Dutch in thetradition of HPSG.
It consists of over 600 gram-mar rules and a large lexicon of over 100,000 lex-emes and various rules to recognize special con-structs such as named entities, temporal expressions,etc.
The grammar takes a ?constructional?
approach,with rich lexical representations and a large numberof detailed, construction specific rules.
Both the lex-icon and the rule component are organized in a mul-tiple inheritance hierarchy.
Heuristics have been im-plemented to deal with unknown words and word se-quences, and ungrammatical or out-of-coverage sen-tences (which may nevertheless contain fragmentsthat are analysable).
The Alpino system includes aPOS-tagger which greatly reduces lexical ambiguity,without an observable decrease in parsing accuracy(Prins, 2005).2.2 ParserBased on the categories assigned to words, andthe set of grammar rules compiled from the HPSGgrammar, a left-corner parser finds the set of allparses, and stores this set compactly in a packedparse forest.
All parses are rooted by an instance2of the top category, which is a category that general-izes over all maximal projections (S, NP, VP, ADVP,AP, PP and some others).
If there is no parse cover-ing the complete input, the parser finds all parses foreach substring.
In such cases, the robustness com-ponent will then select the best sequence of non-overlapping parses (i.e., maximal projections) fromthis set.In order to select the best parse from the com-pact parse forest, a best-first search algorithm is ap-plied.
The algorithm consults a Maximum Entropydisambiguation model to judge the quality of (par-tial) parses.
Since the disambiguation model in-cludes inherently non-local features, efficient dy-namic programming solutions are not directly appli-cable.
Instead, a best-first beam-search algorithm isemployed (van Noord andMalouf, 2005; van Noord,2006).2.3 Maximum Entropy disambiguation modelThe maximum entropy model is a conditional modelwhich assigns a probability to a parse t for a givensentence s. Furthermore, fi(t) are the feature func-tions which count the occurrence of each feature i ina parse t. Each feature i has an associated weight ?i.The score ?
of a parse t is defined as the sum of theweighted feature counts:?
(t) =?i?ifi(t)If t is a parse of s, the actual conditional proba-bility is given by the following, where T (s) are allparses of s:P (t|s) =exp(?
(t))?u?T (s) exp(?
(u))However, note that if we only want to select thebest parse we can ignore the actual probability, and itsuffices to use the score ?
to rank competing parses.The Maximum Entropy model employs a large setof features.
The standard model uses about 42,000different features.
Features describe various prop-erties of parses.
For instance, the model includesfeatures which signal the application of particulargrammar rules, as well as local configurations ofgrammar rules.
There are features signalling spe-cific POS-tags and subcategorization frames.
Otherfeatures signal local or non-local occurrences of ex-traction (WH-movement, relative clauses etc.
), thegrammatical role of the extracted element (subjectvs.
non-subject etc.
), features to represent the dis-tance of a relative clause and the noun it modifies,features describing the amount of parallelism be-tween conjuncts in a coordination, etc.
In addition,there are lexical features which represent the co-occurrence of two specific words in a specific de-pendency, and the occurrence of a specific word as aspecific dependent for a given POS-tag.
Each parseis characterized by its feature vector (the counts foreach of the 42,000 features).
Once the model istrained, each feature is associated with its weight ?
(a positive or negative number, typically close to 0).To find out which parse is the best parse accordingto the model, it suffices to multiply the frequencyof each feature with its corresponding weight, andsum these weighted frequencies.
The parse with thehighest sum is the best parse.
Formal details of thedisambiguation model are presented in van Noordand Malouf (2005).2.4 Dependency structuresAlthough Alpino is not a dependency grammar inthe traditional sense, dependency structures are gen-erated by the lexicon and grammar rules as the valueof a dedicated feature dt.
The dependency struc-tures are based on CGN (Corpus Gesproken Ned-erlands, Corpus of Spoken Dutch) (Hoekstra et al,2003), D-Coi and LASSY (van Noord et al, 2006).Such dependency structures are somewhat idiosyn-cratic, as can be observed in the example in figure 1for the sentence:(3) waar en wanneer dronk Elvis wijn?where and when did Elvis drink wine?2.5 EvaluationThe output of the parser is evaluated by comparingthe generated dependency structure for a corpus sen-tence to the gold standard dependency structure in atreebank.
For this comparison, we represent the de-pendency structure (a directed acyclic graph) as aset of named dependency relations.
The dependencygraph in figure 1 is represented with the followingset of dependencies:3?whqwhd1conjcnjadvwaar0crdvgen1cnjadvwanneer2bodysv1mod1hdverbdrink3sunameElvis4obj1nounwijn5Figure 1: Dependency graph example.
Reentrantnodes are visualized using a bold-face index.
Rootforms of head words are explicitly included in sepa-rate nodes, and different types of head receive a dif-ferent relation label such as hd, crd (for coordina-tion), whd (for WH-phrases) etc.
In this case, theWH-phrase is both the whd element of the top-node,as well as a mod dependent of drink.crd/cnj(en,waar) crd/cnj(en,wanneer)whd/body(en,drink) hd/mod(drink,en)hd/obj1(drink,wijn) hd/su(drink,Elvis)Comparing these sets, we count the number of de-pendencies that are identical in the generated parseand the stored structure, which is expressed tradi-tionally using f-score (Briscoe et al, 2002).
We pre-fer to express similarity between dependency struc-tures by concept accuracy:CA = 1?
?i Difmax(?i Dig,?i Dip)where Dip is the number of dependencies producedby the parser for sentence i, Dg is the number ofdependencies in the treebank parse, and Df is thenumber of incorrect and missing dependencies pro-duced by the parser.The standard version of Alpino that we use hereas baseline system is trained on the 145,000 wordAlpino treebank, which contains dependency struc-tures for the cdbl (newspaper) part of the Eind-hoven corpus.
The parameters for training the modelare the same for the baseline model, as well as themodel that includes the self-trained bilexical prefer-ences (introduced below).
These parameters include#sentences 100% 30,000,000#words 500,000,000#sentences without parse 0.2% 100,000#sentences with fragments 8% 2,500,000#single full parse 92% 27,500,000Table 1: Approximate counts of the number of sen-tences and words in the parsed corpus.
About 0,2%of the sentences did not get a parse, for computa-tional reasons (out of memory, or maximum parsetime exceeded).the Gaussian penalty, thresholds for feature selec-tion, etc.
Details of the training procedure are de-scribed in van Noord and Malouf (2005).2.6 Parsed CorporaOver the course of about a year, Alpino has beenused to parse most of the TwNC-02 (Twente News-paper Corpus), Dutch Wikipedia, and the Duch partof Europarl.
TwNC consists of Dutch newspapertexts from 1994 - 2004.
We did not use the ma-terial from Trouw 2001, since part of that mate-rial is used in the test set used below.
We usedthe 200 node Beowulf Linux cluster of the High-Performance Computing center of the University ofGroningen.
The dependency structures are stored inXML.
The XML files can be processed and searchedin various ways, for instance, using XPATH, XSLTand Xquery (Bouma and Kloosterman, 2002).
Somequantitative information of this parsed corpus islisted in table 1.
In the experiments described be-low, we do not distinguish between full and frag-ment parses; sentences without a parse are obviouslyignored.3 Bilexical preferences3.1 Association ScoreThe parsed corpora described in the previous sec-tion have been used in order to compute associationscores between lexical dependencies.
The parsesconstructed by Alpino are dependency structures.
Insuch dependency structures, the basic dependenciesare of the form r(w1, w2) where r is a relation suchas subject, object, modifier, prepositional comple-ment, .
.
.
, and wi are root forms of words.Bilexical preference between two root forms w14tokens 480,000,000types 100,000,000types with frequency ?
20 2,000,000Table 2: Number of lexical dependencies in parsedcorpora (approximate counts)bijltje gooi neer 13duimschroef draai aan 13peentje zweet 13traantje pink weg 13boontje dop 12centje verdien bij 12champagne fles ontkurk 12dorst les 12Table 3: Pairs involving a direct object relationshipwith the highest pointwise mutual information score.andw2 is computed using an association score basedon pointwise mutual information, as defined by Fano(1961) and used for a similar purpose in Church andHanks (1990), as well as in many other studies incorpus linguistics.
The association score is definedhere as follows:I(r(w1, w2) = logf(r(w1, w2))f(r(w1, ))f( ( , w2))where f(X) is the relative frequency of X .
In theabove formula, the underscore is a place holder foran arbitrary relation or an arbitrary word.
The as-sociation score I compares the actual relative fre-quency of w1 and w2 with dependency r, withthe relative frequency we would expect if thewords were independent.
For instance, to computeI(hd/obj1(drink,melk)) we lookup the numberof times drink occurs with a direct object out of all462,250,644 dependencies (15,713) and the numberof times melk occurs as a dependent (10,172).
If wemultiply the two corresponding relative frequencies,we get the expected relative frequency (0.35) forhd/obj1(drink,melk), which is about 560 timesas big as the actual frequence, 195.
Taking the logof this gives us the association score (6.33) for thisbi-lexical dependency.
Note that pairs that we haveseen fewer than 20 times are ignored.
Mutual in-formation scores are unreliable for low frequencies.An additional benefit of a frequency threshold is amanageable size of the resulting data-structures.The pairs involving a direct object relationshipwith the highest scores are listed in table 3.
Thebiertje small glass of beer 8borreltje strong alcoholic drink 8glaasje small glass 8pilsje small glass of beer 8pintje small glass of beer 8pint glass of beer 8wijntje small glass of wine 8alcohol alcohol 7bier beer 7Table 4: Pairs involving a direct object relationshipwith the highest pointwise mutual information scorefor the verb drink.overlangs snijd door 12welig tier 12dunnetjes doe over 11stief moederlijk bedeel 11on zedelijk betast 11stierlijk verveel 11cum laude studeer af 10hermetisch grendel af 10ingespannen tuur 10instemmend knik 10kostelijk amuseer 10Table 5: Pairs involving a modifier relationship be-tween a verb and an adverbial with the highest asso-ciation score.highest scoring nouns that occur as the direct objectof drink are listed in table 4.Selection restrictions are often associated onlywith direct objects.
We include bilexical associationscores for all types of dependencies.
We found thatassociation scores for other types of dependenciesalso captures both collocational preferences as wellas weaker cooccurrence preferences.
Some exam-ples including modifiers are listed in table 5.
Suchpreferences are useful for disambiguation as well.Consider the ambiguous Dutch sentence(4) omdat we lauw bier dronkenbecause we drank warm beerbecause we drank beer warmlyThe adjective lauw (cold, lukewarm, warm) can beused to modify both nouns and verbs; this latter pos-sibility is exemplified in:(5) We hebben lauw gereageerdWe reacted indifferently5?smainobj1conjcnjnounbier0crdvgof1cnjnounwijn2hdverbdrink3sunameElvis4modadvniet5Figure 2: Dependency structure produced for coor-dination3.2 Extending pairsThe CGN dependencies that we work with fail to re-late pairs of words in certain syntactic constructionsfor which it can be reasonably assumed that bilexi-cal preferences should be useful.
We have identifiedtwo such constructions, namely relative clauses andcoordination, and for these constructions we gener-alize our method, to take such dependencies into ac-count too.Consider coordinations such as:(6) Bier of wijn drinkt Elvis nietBeer or wine, Elvis does not drinkThe dependency structure of the intended analysisis given in figure 2.
The resulting set of dependen-cies for this example treats the coordinator as thehead of the conjunction:hd/obj1(drink,of) crd/cnj(of,bier)crd/cnj(of,wijn) hd/su(drink,elvis)hd/mod(drink,niet)So there are no direct dependencies between the verband the individual conjuncts.
For this reason, weadd additional dependencies r(A,C) for every pairof dependency r(A,B), crd/cnj(B,C).Relative clauses are another syntactic phe-nomenon where we extend the set of dependencies.For a noun phrase such as:(7) Wijn die Elvis niet dronkWine which Elvis did not drinkthere is no direct dependency between wijn anddrink, as can be seen in the dependency structure?nphdnounwijn0modrelrhd1prondie1bodyssubobj11sunameElvis2modadvniet3hdverbdrink4Figure 3: Dependency structure produced for rela-tive clausegiven in figure 3.
Sets of dependencies are extendedin such cases, to make the relation between the nounand the role it plays in the relative clause explicit.3.3 Using association scores as featuresThe association scores for all dependencies are usedin our maximum entropy disambiguation model asfollows.
The technique is reminiscent of the inclu-sion of auxiliary distributions in stochastic attribute-value grammar (Johnson and Riezler, 2000).Recall that a maximum entropy disambiguationmodel exploits features.
Features are properties ofparses, and we can use such features to describe anyproperty of parses that we believe is of importancefor disambiguation.
For the disambiguation model,a parse is fully characterized by a vector of featurecounts.We introduce features z(t, r) for each of the ma-jor POS labels t (verb, noun, adjective, adverb, .
.
.
)and each of the dependency relations r. The ?count?of such a feature is determined by the associationscores for actually occuring dependency pairs.
Forexample, if in a given parse a given verb v has adirect object dependent n, then we compute the as-sociation of this particular pair, and use the resultingnumber as the count of that feature.
Of course, ifthere are multiple dependencies of this type in a sin-gle parse, the corresponding association scores areall summed.To illustrate this technique, consider the depen-dency structure given earlier in figure 2.
For this6example, there are four of these new features with anon-zero count.
The counts are given by the corre-sponding association scores as follows:z(verb, hd/su) = I(hd/su(drink,elvis))z(verb, hd/mod) = I(hd/mod(drink,niet))z(verb, hd/obj1) = I(hd/obj1(drink,of))+ I(hd/obj1(drink,bier))+ I(hd/obj1(drink,wijn))z(conj, crd/cnj) = I(crd/cnj(of,bier))+ I(crd/cnj(of,wijn))It is crucial to observe that the new features do notinclude any direct reference to actual words.
Thismeans that there will be only a fairly limited numberof new features (depending on the number of tags tand relations r), and we can expect that these fea-tures are frequent enough to be able to estimate theirweights in training material of limited size.Association scores can be negative if two words ina lexical dependency occur less frequently than onewould expect if the words were independent.
How-ever, since association scores are unreliable for lowfrequencies (including, often, frequencies of zero),and since such negative associations involve low fre-quencies by their nature, we only take into accountpositive association scores.4 ExperimentsWe report on two experiments.
In the first exper-iment, we report on the results of tenfold cross-validation on the Alpino treebank.
This is the ma-terial that is standardly used for training and test-ing.
For each of the sentences of this corpus, thesystem produces atmost the first 1000 parses.
Forevery parse we compute the quality by comparingits dependency structure with the gold standard de-pendency structure in the treebank.
For training, at-most 100 parses are selected randomly for each sen-tence.
For (tenfold cross-validated) testing, we useall available parses for a given sentence.
In order totest the quality of the model, we check for each givensentence which of its atmost 1000 parses is selectedby the disambiguation model.
The quality of thatparse is used in the computation of the accuracy, aslisted in table 6.
The column labeled exact measuresthe proportion of sentences for which the model se-lected the best possible parse (there can be multiplefscore err.red.
exact CA% % % %baseline 74.02 0.00 16.0 73.48oracle 91.97 100.00 100.0 91.67standard 87.41 74.60 52.0 87.02+self-training 87.91 77.38 54.8 87.51Table 6: Results with ten-fold cross-validation onthe Eindhoven-cdbl part of the Alpino treebank.
Inthese experiments, the models are used to select aparse from a given set of atmost 1000 parses per sen-tence.best possible parses).
The baseline row reports onthe quality of a disambiguation model which simplyselects the first parse for each sentence.
The oraclerow reports on the quality of the best-possible dis-ambiguation model, which would (by magic) alwaysselect the best possible parse (some parses are out-side the coverage of the system, and some parses aregenerated only after more than 1000 inferior parses).The error reduction column measures which part ofthe disambiguation problem (difference between thebaseline and oracle scores) is solved by the model.1The results show a small but clear increase inerror reduction, if the standard model (without theassociation score features) is compared with a (re-trained) model that includes the association scorefeatures.
The relatively large improvement of the ex-act score suggests that the bilexical preference fea-tures are particularly good at choosing between verygood parses.For the second experiment, we evaluate how wellthe resulting model performs in the full system.
Firstof all, this is the only really convincing evalua-tion which measures progress for the system as awhole by virtue of including bilexical preferences.The second motivation for this experiment is formethodological reasons: we now test on a trulyunseen test-set.
The first experiment can be criti-1Note that the error reduction numbers presented in the ta-ble are lower than those presented in van Noord and Malouf(2005).
The reason is, that we report here on experiments inwhich parses are generated with a version of Alpino with thePOS-tagger switched on.
The POS-tagger already reduces thenumber of ambiguities, and in particular solves many of the?easy?
cases.
The resulting models, however, are more effec-tive in practice (where the model also is applied after the POS-tagger).7prec rec fscore CA% % % %standard 90.77 90.49 90.63 90.32+self-training 91.19 90.89 91.01 90.73Table 7: Results on the WR-P-P-H part of the D-Coicorpus (2267 sentences from the newspaper Trouw,from 2001).
In these experiments, we report on thefull system.
In the full system, the disambiguationmodel is used to guide a best-first beam-search pro-cedure which extracts a parse from the parse forest.Difference in CA was found to be significant (usingpaired T-test on the per sentence CA scores).cized on methodological grounds as follows.
TheAlpino Treebank was used to train the disambigua-tion model which was used to construct the largeparsed treebank from which we extracted the countsfor the association scores.
Those scores might some-how therefore indirectly reflect certain aspects of theAlpino Treebank training data.
Testing on that datalater (with the inclusion of the association scores) istherefore not sound.For this second experiment we used the WR-P-P-H (newspaper) part of the D-Coi corpus.
This partcontains 2256 sentences from the newspaper Trouw(2001).
In table 7 we show the resulting f-score andCA for a system with and without the inclusion ofthe z(t, r) features.
The improvement found in theprevious experiment is confirmed.5 Conclusion and OutlookOne might wonder why self-training works in thecase of selection restrictions, at least in the set-updescribed above.
One may argue that, in order tolearn that milk is a good object for drink, the parserhas to analyse examples of drink milk in the raw datacorrectly.
But if the parser is capable of analysingthese examples, why does it need selection restric-tions?
The answer appears to be that the parser(without selection restrictions) is able to analyse thelarge majority of cases correctly.
These cases in-clude the many easy occurrences where no (diffi-cult) ambiguities arise (case marking, number agree-ment and other syntactic characteristics often force asingle reading).
The easy cases outnumber the mis-parsed difficult cases, and therefore the selection re-strictions can be learned.
Using these selection re-strictions as additional features, the parser is thenable to also get the difficult, ambiguous, cases right.There are various aspects of our method thatneed further investigation.
First of all, existingtechniques that involve selection restrictions (e.g.,Resnik (1993)) typically assume classes of nouns,rather than individual nouns.
In future work wehope to generalize our method to take classes intoaccount, where the aim is to learn class membershipalso on the basis of large parsed corpora.Another aspect of the technique that needs fur-ther research involves the use of a threshold in estab-lishing the association score, and perhaps related tothis issue, the incorporation of negative associationscores (for instance for cases where a large numberof cooccurrences of a pair would be expected butwhere in fact none or very few were found).There are also some more practical issues thatperhaps had a negative impact on our results.
First,the large parsed corpus was collected over a periodof about a year, but during that period, the actualsystem was not stable.
In particular, due to variousimprovements of the dictionary, the root form ofwords that was used by the system changed overtime.
Since we used root forms in the computationof the association scores, this could be harmful insome specific cases.
A further practical issue con-cerns repeated sentences or even full paragraphs.This happens in typical newspaper material forinstance in the case of short descriptions of moviesthat may be repeated weekly for as long as thatmovie is playing.
Pairs of words that occur insuch repeated sentences receive association scoresthat are much too high.
The method should beadapted to take this into account, perhaps simply byremoving duplicated sentences.Clearly, the idea that selection restrictions oughtto be useful for parsing is not new.
However, as faras we know this is the first time that automaticallyacquired selection restrictions have been shown toimprove parsing accuracy results.AcknowledgementsThis research was carried out in part in thecontext of the D-Coi and Lassy projects.The D-Coi and Lassy projects are carried8out within the STEVIN programme which isfunded by the Dutch and Flemish governments(http://taalunieversum.org/taal/technologie/stevin/).ReferencesTakeshi Abekawa and Manabu Okumura.
2006.Japanese dependency parsing using co-occurrence in-formation and a combination of case elements.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 833?840, Sydney, Australia, July.
Associationfor Computational Linguistics.Gosse Bouma and Geert Kloosterman.
2002.
Query-ing dependency treebanks in XML.
In Proceedings ofthe Third international conference on Language Re-sources and Evaluation (LREC), pages 1686?1691,Gran Canaria, Spain.Ted Briscoe, John Carroll, Jonathan Graham, and AnnCopestake.
2002.
Relational evaluation schemes.In Proceedings of the Beyond PARSEVAL Workshopat the 3rd International Conference on Language Re-sources and Evaluation, pages 4?8, Las Palmas, GranCanaria.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information and lexicogra-phy.
Computational Linguistics, 16(1):22?29.Robert Mario Fano.
1961.
Transmission of Information:A Statistical Theory of Communications.
MIT Press,Cambridge, MA.Heleen Hoekstra, Michael Moortgat, Bram Renmans,Machteld Schouppe, Ineke Schuurman, and Tonvan der Wouden, 2003.
CGN Syntactische Annotatie,December.Mark Johnson and Stefan Riezler.
2000.
Exploitingauxiliary distributions in stochastic unification-basedgrammars.
In Proceedings of the first conference onNorth American chapter of the Association for Com-putational Linguistics, pages 154?161, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Daisuke Kawahara and Sadao Kurohashi.
2006.
A fully-lexicalized probabilistic model for japanese syntacticand case structure analysis.
In Proceedings of the mainconference on Human Language Technology Confer-ence of the North American Chapter of the Associationof Computational Linguistics, pages 176?183, Morris-town, NJ, USA.
Association for Computational Lin-guistics.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159, NewYork City, USA, June.
Association for ComputationalLinguistics.Robbert Prins.
2005.
Finite-State Pre-Processing forNatural Language Analysis.
Ph.D. thesis, Universityof Groningen.Philip Stuart Resnik.
1993.
Selection and information:a class-based approach to lexical relationships.
Ph.D.thesis, University of Pennsylvania, Philadelphia, PA,USA.Gertjan van Noord and Robert Malouf.
2005.Wide coverage parsing with stochastic at-tribute value grammars.
Draft available fromhttp://www.let.rug.nl/?vannoord.
A preliminary ver-sion of this paper was published in the Proceedingsof the IJCNLP workshop Beyond Shallow Analyses,Hainan China, 2004.Gertjan van Noord, Ineke Schuurman, and Vincent Van-deghinste.
2006.
Syntactic annotation of large cor-pora in STEVIN.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC), Genoa, Italy.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le Traitement Au-tomatique des Langues naturelles, pages 20?42, Leu-ven.ExamplesHere we list a number of examples, which suggestthat selection restrictions can also be important fordependencies, other than direct objects.High scoring pairs involving a subject relation-ship with a verb:alarmbel rinkelchampagnekurk knalgij echtbreekhaan kraaikikker kwaakrups verpopvonk overspringzweet parelbelletje rinkelbrievenbus klepperHigh scoring pairs involving a modifier relation-ship with a noun:9in vitro fertilisatieHubble ruimtetelescoopzelfrijzend bakmeelbezittelijk voornaamwoordingegroeid teennagelknapperend haardvuurlevendbarend hagedisonbevlekt ontvangenisongeblust kalkHigh scoring pairs involving a predicative com-plement relationship with a verb:beetgaar kookbeuk murwschuimig klopsuf peinssuf piekerdoormidden scheurragfijn hakstuk bijtau serieux neemin duigen vallam legHigh scoring pairs involving an apposition rela-tionship with a noun:jongensgroep Boyzonecommunicatiesysteem C2000blindeninstituut De Steffenberghaptonoom Ted Troostgebedsgenezeres Greet Hofmansrally Parijs-Dakartovenaar Gandalfaartsengel Gabrielkeeperstrainer Joep Hielebasketbalcoach Ton Bootpartizaan TitoHigh scoring pairs involving a measure phrase re-lationship with an adjective:graadje ergerlichtjaar verwijderdmijlenver verwijderdniets lievereindje verderopgraad warmerillusie armerkilogram wegendonsje mindermaatje te grootknip waard10
