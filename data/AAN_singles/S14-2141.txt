Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796?801,Dublin, Ireland, August 23-24, 2014.UTexas: Natural Language Semantics using Distributional Semantics andProbabilistic LogicIslam Beltagy?, Stephen Roller?, Gemma Boleda?, Katrin Erk?, Raymond J.
Mooney?
?Department of Computer Science?Department of LinguisticsThe University of Texas at Austin{beltagy, roller, mooney}@cs.utexas.edugemma.boleda@upf.edu, katrin.erk@mail.utexas.eduAbstractWe represent natural language semanticsby combining logical and distributional in-formation in probabilistic logic.
We useMarkov Logic Networks (MLN) for theRTE task, and Probabilistic Soft Logic(PSL) for the STS task.
The system isevaluated on the SICK dataset.
Our bestsystem achieves 73% accuracy on the RTEtask, and a Pearson?s correlation of 0.71 onthe STS task.1 IntroductionTextual Entailment systems based on logical infer-ence excel in correct reasoning, but are often brit-tle due to their inability to handle soft logical in-ferences.
Systems based on distributional seman-tics excel in lexical and soft reasoning, but are un-able to handle phenomena like negation and quan-tifiers.
We present a system which takes the bestof both approaches by combining distributional se-mantics with probabilistic logical inference.Our system builds on our prior work (Belt-agy et al., 2013; Beltagy et al., 2014a; Beltagyand Mooney, 2014; Beltagy et al., 2014b).
Weuse Boxer (Bos, 2008), a wide-coverage semanticanalysis tool to map natural sentences to logicalform.
Then, distributional information is encodedin the form of inference rules.
We generate lexicaland phrasal rules, and experiment with symmetricand asymmetric similarity measures.
Finally, weuse probabilistic logic frameworks to perform in-ference, Markov Logic Networks (MLN) for RTE,and Probabilistic Soft Logic (PSL) for STS.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/2 Background2.1 Logical SemanticsLogic-based representations of meaning have along tradition (Montague, 1970; Kamp and Reyle,1993).
They handle many complex semantic phe-nomena such as relational propositions, logicaloperators, and quantifiers; however, they can nothandle ?graded?
aspects of meaning in languagebecause they are binary by nature.2.2 Distributional SemanticsDistributional models use statistics of word co-occurrences to predict semantic similarity ofwords and phrases (Turney and Pantel, 2010;Mitchell and Lapata, 2010), based on the obser-vation that semantically similar words occur insimilar contexts.
Words are represented as vec-tors in high dimensional spaces generated fromtheir contexts.
Also, it is possible to compute vec-tor representations for larger phrases composition-ally from their parts (Mitchell and Lapata, 2008;Mitchell and Lapata, 2010; Baroni and Zampar-elli, 2010).
Distributional similarity is usually amixture of semantic relations, but particular asym-metric similarity measures can, to a certain ex-tent, predict hypernymy and lexical entailmentdistributionally (Kotlerman et al., 2010; Lenci andBenotto, 2012; Roller et al., 2014).
Distribu-tional models capture the graded nature of mean-ing, but do not adequately capture logical struc-ture (Grefenstette, 2013).2.3 Markov Logic NetworkMarkov Logic Networks (MLN) (Richardson andDomingos, 2006) are a framework for probabilis-tic logic that employ weighted formulas in first-order logic to compactly encode complex undi-rected probabilistic graphical models (i.e., Markovnetworks).
Weighting the rules is a way of soft-ening them compared to hard logical constraints.796MLNs define a probability distribution over pos-sible worlds, where the probability of a world in-creases exponentially with the total weight of thelogical clauses that it satisfies.
A variety of in-ference methods for MLNs have been developed,however, computational overhead is still an issue.2.4 Probabilistic Soft LogicProbabilistic Soft Logic (PSL) is another recentlyproposed framework for probabilistic logic (Kim-mig et al., 2012).
It uses logical representations tocompactly define large graphical models with con-tinuous variables, and includes methods for per-forming efficient probabilistic inference for the re-sulting models.
A key distinguishing feature ofPSL is that ground atoms (i.e., atoms without vari-ables) have soft, continuous truth values on theinterval [0, 1] rather than binary truth values asused in MLNs and most other probabilistic logics.Given a set of weighted inference rules, and withthe help of Lukasiewicz?s relaxation of the logicaloperators, PSL builds a graphical model defining aprobability distribution over the continuous spaceof values of the random variables in the model(Kimmig et al., 2012).
Then, PSL?s MPE infer-ence (Most Probable Explanation) finds the overallinterpretation with the maximum probability givena set of evidence.
This optimization problem is asecond-order cone program (SOCP) (Kimmig etal., 2012) and can be solved in polynomial time.2.5 Recognizing Textual EntailmentRecognizing Textual Entailment (RTE) is the taskof determining whether one natural language text,the premise, Entails, Contradicts, or is not related(Neutral) to another, the hypothesis.2.6 Semantic Textual SimilaritySemantic Textual Similarity (STS) is the task ofjudging the similarity of a pair of sentences ona scale from 1 to 5 (Agirre et al., 2012).
Goldstandard scores are averaged over multiple humanannotations and systems are evaluated using thePearson correlation between a system?s output andgold standard scores.3 Approach3.1 Logical RepresentationThe first component in the system is Boxer (Bos,2008), which maps the input sentences into logicalform, in which the predicates are words in the sen-tence.
For example, the sentence ?A man is drivinga car?
in logical form is:?x, y, z. man(x) ?
agent(y, x) ?
drive(y) ?patient(y, z) ?
car(z)3.2 Distributional RepresentationNext, distributional information is encoded inthe form of weighted inference rules connectingwords and phrases of the input sentences T and H .For example, for sentences T : ?A man is drivinga car?, and H: ?A guy is driving a vehicle?, wewould like to generate rules like ?x.
man(x) ?guy(x) |w1, ?x.car(x)?
vehicle(x) |w2, wherew1and w2are weights indicating the similarity ofthe antecedent and consequent of each rule.Inferences rules are generated as in Beltagy etal.
(2013).
Given two input sentences T and H ,for all pairs (a, b), where a and b are words orphrases of T and H respectively, generate an infer-ence rule: a ?
b | w, where the rule weight w isa function of sim(?
?a ,?
?b ), and sim is a similaritymeasure of the distributional vectors?
?a ,?
?b .
Weexperimented with the symmetric similarity mea-sure cosine, and asym, the supervised, asymmet-ric similarity measure of Roller et al.
(2014).The asym measure uses the vector difference(?
?a ??
?b ) as features in a logistic regression clas-sifier for distinguishing between four differentword relations: hypernymy, cohyponymy, meron-omy, and no relation.
The model is trained us-ing the noun-noun subset of the BLESS data set(Baroni and Lenci, 2011).
The final similarityweight is given by the model?s estimated probabil-ity that the word relationship is either hypernymyor meronomy: asym(?
?a ,?
?b ) = P (hyper(a, b))+P (mero(a, b)).Distributional representations for words are de-rived by counting co-occurrences in the ukWaC,WaCkypedia, BNC and Gigaword corpora.
Weuse the 2000 most frequent content words as ba-sis dimensions, and count co-occurrences withina two word context window.
The vector space isweighted using Positive Pointwise Mutual Infor-mation.Phrases are defined in terms of Boxer?s outputto be more than one unary atom sharing the samevariable like ?a little kid?
(little(k) ?
kid(k)),or two unary atoms connected by a relation like?a man is driving?
(man(m) ?
agent(d,m) ?drive(d)).
We compute vector representations of797phrases using vector addition across the compo-nent predicates.
We also tried computing phrasevectors using component-wise vector multiplica-tion (Mitchell and Lapata, 2010), but found it per-formed marginally worse than addition.3.3 Probabilistic Logical InferenceThe last component is probabilistic logical infer-ence.
Given the logical form of the input sen-tences, and the weighted inference rules, we usethem to build a probabilistic logic program whosesolution is the answer to the target task.
A proba-bilistic logic program consists of the evidence setE, the set of weighted first order logical expres-sions (rule base RB), and a query Q.
Inference isthe process of calculating Pr(Q|E,RB).3.4 Task 1: RTE using MLNsMLNs are the probabilistic logic framework weuse for the RTE task (we do not use PSL here asit shares the problems of fuzzy logic with proba-bilistic reasoning).
The RTE classification prob-lem for the relation between T and H can besplit into two inference tasks.
The first is test-ing if T entails H , Pr(H|T,RB).
The secondis testing if the negation of the text ?T entails H ,Pr(H|?T,RB).
In case Pr(H|T,RB) is high,while Pr(H|?T,RB) is low, this indicates En-tails.
In case it is the other way around, this in-dicates Contradicts.
If both values are close, thismeans T does not affect the probability of H andindicative of Neutral.
We train an SVM classifierwith LibSVM?s default parameters to map the twoprobabilities to the final decision.The MLN implementation we use isAlchemy (Kok et al., 2005).
Queries in Alchemycan only be ground atoms.
However, in ourcase the query is a complex formula (H).
Weextended Alchemy to calculate probabilities ofqueries (Beltagy and Mooney, 2014).
Probabilityof a formula Q given an MLN K equals the ratiobetween the partition function Z of the groundnetwork of K with and without Q added as a hardrule (Gogate and Domingos, 2011)P (Q | K) =Z(K ?
{(Q,?
)})Z(K)(1)We estimate Z of the ground networks using Sam-pleSearch (Gogate and Dechter, 2011), an ad-vanced importance sampling algorithm that is suit-able for ground networks generated by MLNs.A general problem with MLN inference isits computational overhead, especially for thecomplex logical formulae generated by our ap-proach.
To make inference faster, we reduce thesize of the ground network through an automatictype-checking technique proposed in Beltagy andMooney (2014).
For example, consider the ev-idence ground atom man(M) denoting that theconstant M is of type man.
Then, consider an-other predicate like car(x).
In case there are no in-ference rule connecting man(x) and car(x), thenwe know that M which we know is a man cannotbe a car, so we remove the ground atom car(M)from the ground network.
This technique reducesthe size of the ground network dramatically andmakes inference tractable.Another problem with MLN inference is thatquantifiers sometimes behave in an undesir-able way, due to the Domain Closure Assump-tion (Richardson and Domingos, 2006) that MLNsmake.
For example, consider the text-hypothesispair: ?There is a black bird?
and ?All birds areblack?, which in logic are T : bird(B)?black(B)and H : ?x.
bird(x) ?
black(x).
Because ofthe Domain Closure Assumption, MLNs concludethat T entails H because H is true for all constantsin the domain (in this example, the single constantB).
We solve this problem by introducing extraconstants and evidence in the domain.
In the ex-ample above, we introduce evidence of a new birdbird(D), which prevents the hypothesis from be-ing true.
The full details of the technique of deal-ing with the domain closure is beyond the scope ofthis paper.3.5 Task 2: STS using PSLPSL is the probabilistic logic we use for the STStask since it has been shown to be an effectiveapproach for computing similarity between struc-tured objects.
We showed in Beltagy et al.
(2014a)how to perform the STS task using PSL.
PSLdoes not work ?out of the box?
for STS, be-cause Lukasiewicz?s equation for the conjunctionis very restrictive.
We address this by replacingLukasiewicz?s equation for conjunction with anaveraging equation, then change the optimizationproblem and grounding technique accordingly.For each STS pair of sentences S1, S2, we runPSL twice, once where E = S1, Q = S2and an-other where E = S2, Q = S1, and output the twoscores.
The final similarity score is produced from798an Additive Regression model with WEKA?s de-fault parameters trained to map the two PSL scoresto the overall similarity score (Friedman, 1999;Hall et al., 2009).3.6 Task 3: RTE and STS using VectorSpaces and Keyword CountsAs a baseline, we also attempt both the RTE andSTS tasks using only vector representations andunigram counts.
This baseline model uses a super-vised regressor with features based on vector sim-ilarity and keyword counts.
The same input fea-tures are used for performing RTE and STS, but aSVM classifier and Additive Regression model istrained separately for each task.
This baseline ismeant to establish whether the task truly requiresthe sophisticated logical inference of MLNs andPSL, or if merely checking for logical keywordsand textual similarity is sufficient.The first two features are simply the cosine andasym similarities between the text and hypothesis,using vector addition of the unigrams to computea single vector for the entire sentence.We also compute vectors for both the text andhypothesis using vector addition of the mutuallyexclusive unigrams (MEUs).
The MEUs are de-fined as the unigrams of the premise and hypoth-esis with common unigrams removed.
For exam-ple, if the premise is ?A dog chased a cat?
and thehypothesis is ?A dog watched a mouse?, the MEUsare ?chased cat?
and ?watched mouse.?
We com-pute vector addition of the MEUs, and computesimilarity using both the cosine and asym mea-sures.
These form two features for the regressor.The last feature of the model is a keywordcount.
We count how many times 13 differentkeywords appear in either the text or the hypoth-esis.
These keywords include negation (no, not,nobody, etc.)
and quantifiers (a, the, some, etc.
)The counts of each keyword form the last 13 fea-tures as input to the regressor.
In total, there are17 features used in this baseline system.4 EvaluationThe dataset used for evaluation is SICK:Sentences Involving Compositional Knowledgedataset, a task for SemEval 2014 (Marelli et al.,2014a; Marelli et al., 2014b).
The dataset is10,000 pairs of sentences, 5000 training and 5000for testing.
Sentences are annotated for both tasks.SICK-RTE SICK-STSBaseline 70.0 71.1MLN/PSL + Cosine 72.8 68.6MLN/PSL + Asym 73.2 68.9Ensemble 73.2 71.5Table 1: Test RTE accuracy and STS Correlation.4.1 Systems ComparedWe compare multiple configurations of our proba-bilistic logic system.?
Baseline: Vector- and keyword-only baselinedescribed in Section 3.6;?
MLN/PSL + Cosine: MLN and PSL basedmethods described in Sections 3.4 and 3.5,using cosine as a similarity measure;?
MLN/PSL + Asym: MLN and PSL basedmethods described in Sections 3.4 and 3.5,using asym as a similarity measure;?
Ensemble: An ensemble method which usesall of the features in the above methods as in-puts for the RTE and STS classifiers.4.2 Results and DiscussionTable 1 shows our results on the held-out test setfor SemEval 2014 Task 1.On the RTE task, we see that both the MLN +Cosine and MLN + Asym models outperformedthe Baseline, indicating that textual entailment re-quires real inference to handle negation and quan-tifiers.
The MLN + Asym and Ensemble sys-tems perform identically on RTE, further suggest-ing that the logical inference subsumes keyworddetection.The MLN + Asym system outperforms theMLN + Cosine system, emphasizing the impor-tance of asymmetric measures for predicting lex-ical entailment.
Intuitively, this makes perfectsense: dog entails animal, but not vice versa.In an error analysis performed on a developmentset, we found our RTE system was extremely con-servative: we rarely confused the Entails and Con-tradicts classes, indicating we correctly predict thedirection of entailment, but frequently misclassifyexamples as Neutral.
An examination of these ex-amples showed the errors were mostly due to miss-ing or weakly-weighted distributional rules.On STS, our vector space baseline outperformsboth PSL-based systems, but the ensemble outper-forms any of its components.
This is a testament to799the power of distributional models in their abilityto predict word and sentence similarity.
Surpris-ingly, we see that the PSL + Asym system slightlyoutperforms the PSL + Cosine system.
This mayindicate that even in STS, some notion of asymme-try plays a role, or that annotators may have beenbiased by simultaneously annotating both tasks.As with RTE, the major bottleneck of our systemappears to be the knowledge base, which is builtsolely using distributional inference rules.Results also show that our system?s perfor-mance is close to the baseline system.
One ofthe reasons behind that could be that sentences arenot exploiting the full power of logical represen-tations.
On RTE for example, most of the con-tradicting pairs are two similar sentences with oneof them being negated.
This way, the existenceof any negation cue in one of the two sentences isa strong signal for contradiction, which what thebaseline system does without deeply representingthe semantics of the negation.5 Conclusion & Future WorkWe showed how to combine logical and distribu-tional semantics using probabilistic logic, and howto perform the RTE and STS tasks using it.
Thesystem is tested on the SICK dataset.The distributional side can be extended in manydirections.
We would like to use longer phrases,more sophisticated compositionality techniques,and contextualized vectors of word meaning.
Wealso believe inference rules could be dramaticallyimproved by integrating from paraphrases collec-tions like PPDB (Ganitkevitch et al., 2013).Finally, MLN inference could be made more ef-ficient by exploiting the similarities between thetwo ground networks (the one with Q and the onewithout).
PLS inference could be enhanced by us-ing a learned, weighted average of rules, ratherthan the simple mean.AcknowledgementsThis research was supported by the DARPA DEFTprogram under AFRL grant FA8750-13-2-0026.Some experiments were run on the MastodonCluster supported by NSF Grant EIA-0303609.The authors acknowledge the Texas AdvancedComputing Center (TACC)1for providing grid re-sources that have contributed to these results.
Wethank the anonymous reviewers and the UTexas1http://www.tacc.utexas.eduNatural Language and Learning group for theirhelpful comments and suggestions.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof Semantic Evaluation (SemEval-12).Marco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,pages 1?10, Edinburgh, UK, July.
Association forComputational Linguistics.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of Conference on Empirical Methods inNatural Language Processing (EMNLP-10).Islam Beltagy and Raymond J. Mooney.
2014.
Ef-ficient Markov logic inference for natural languagesemantics.
In Proceedings of AAAI 2014 Workshopon Statistical Relational AI (StarAI-14).Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.Montague meets Markov: Deep semantics withprobabilistic logical form.
In Proceedings of theSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM-13).Islam Beltagy, Katrin Erk, and Raymond Mooney.2014a.
Probabilistic soft logic for semantic textualsimilarity.
In Proceedings of Association for Com-putational Linguistics (ACL-14).Islam Beltagy, Katrin Erk, and Raymond Mooney.2014b.
Semantic parsing using distributional se-mantics and probabilistic logic.
In Proceedingsof ACL 2014 Workshop on Semantic Parsing (SP-2014).Johan Bos.
2008.
Wide-coverage semantic analysiswith Boxer.
In Proceedings of Semantics in TextProcessing (STEP-08).J.H.
Friedman.
1999.
Stochastic gradient boosting.Technical report, Stanford University.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT-13).Vibhav Gogate and Rina Dechter.
2011.
Sample-search: Importance sampling in presence of deter-minism.
Artificial Intelligence, 175(2):694?729.Vibhav Gogate and Pedro Domingos.
2011.
Proba-bilistic theorem proving.
In 27th Conference on Un-certainty in Artificial Intelligence (UAI-11).800Edward Grefenstette.
2013.
Towards a formal distri-butional semantics: Simulating logical calculi withtensors.
In Proceedings of Second Joint Conferenceon Lexical and Computational Semantics (*SEM2013).Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer.Angelika Kimmig, Stephen H. Bach, MatthiasBroecheler, Bert Huang, and Lise Getoor.
2012.A short introduction to Probabilistic Soft Logic.In Proceedings of NIPS Workshop on ProbabilisticProgramming: Foundations and Applications (NIPSWorkshop-12).Stanley Kok, Parag Singla, Matthew Richardson, andPedro Domingos.
2005.
The Alchemy systemfor statistical relational AI.
Technical report, De-partment of Computer Science and Engineering,University of Washington.
http://www.cs.washington.edu/ai/alchemy.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Alessandro Lenci and Giulia Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of the first Joint Conference on Lexicaland Computational Semantics (*SEM-12).Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014a.
SemEval-2014 task 1: Evaluation ofcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014b.
A sick cure for the evaluationof compositional distributional semantic models.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Thierry Declerck, Hrafn Loftsson, BenteMaegaard, Joseph Mariani, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation (LREC?14), Reykjavik, Ice-land, may.
European Language Resources Associa-tion (ELRA).Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof Association for Computational Linguistics (ACL-08).Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(3):1388?1429.Richard Montague.
1970.
Universal grammar.
Theo-ria, 36:373?398.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning,62:107?136.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
In Proceedings of the TwentyFifth International Conference on ComputationalLinguistics (COLING-14), Dublin, Ireland.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.801
