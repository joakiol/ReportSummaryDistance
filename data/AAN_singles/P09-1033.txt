Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 288?296,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPAbstraction and Generalisation in Semantic Role Labels:PropBank, VerbNet or both?Paola MerloLinguistics DepartmentUniversity of Geneva5 Rue de Candolle, 1204 GenevaSwitzerlandPaola.Merlo@unige.chLonneke Van Der PlasLinguistics DepartmentUniversity of Geneva5 Rue de Candolle, 1204 GenevaSwitzerlandLonneke.VanDerPlas@unige.chAbstractSemantic role labels are the representa-tion of the grammatically relevant aspectsof a sentence meaning.
Capturing thenature and the number of semantic rolesin a sentence is therefore fundamental tocorrectly describing the interface betweengrammar and meaning.
In this paper, wecompare two annotation schemes, Prop-Bank and VerbNet, in a task-independent,general way, analysing how well they farein capturing the linguistic generalisationsthat are known to hold for semantic rolelabels, and consequently how well theygrammaticalise aspects of meaning.
Weshow that VerbNet is more verb-specificand better able to generalise to new seman-tic role instances, while PropBank bettercaptures some of the structural constraintsamong roles.
We conclude that these tworesources should be used together, as theyare complementary.1 IntroductionMost current approaches to language analysis as-sume that the structure of a sentence depends onthe lexical semantics of the verb and of other pred-icates in the sentence.
It is also assumed that onlycertain aspects of a sentence meaning are gram-maticalised.
Semantic role labels are the represen-tation of the grammatically relevant aspects of asentence meaning.Capturing the nature and the number of seman-tic roles in a sentence is therefore fundamentalto correctly describe the interface between gram-mar and meaning, and it is of paramount impor-tance for all natural language processing (NLP)applications that attempt to extract meaning rep-resentations from analysed text, such as question-answering systems or even machine translation.The role of theories of semantic role lists is toobtain a set of semantic roles that can apply toany argument of any verb, to provide an unam-biguous identifier of the grammatical roles of theparticipants in the event described by the sentence(Dowty, 1991).
Starting from the first proposals(Gruber, 1965; Fillmore, 1968; Jackendoff, 1972),several approaches have been put forth, rangingfrom a combination of very few roles to lists ofvery fine-grained specificity.
(See Levin and Rap-paport Hovav (2005) for an exhaustive review).In NLP, several proposals have been put forth inrecent years and adopted in the annotation of largesamples of text (Baker et al, 1998; Palmer et al,2005; Kipper, 2005; Loper et al, 2007).
The an-notated PropBank corpus, and therefore implicitlyits role labels inventory, has been largely adoptedin NLP because of its exhaustiveness and becauseit is coupled with syntactic annotation, propertiesthat make it very attractive for the automatic learn-ing of these roles and their further applications toNLP tasks.
However, the labelling choices madeby PropBank have recently come under scrutiny(Zapirain et al, 2008; Loper et al, 2007; Yi et al,2007).The annotation of PropBank labels has beenconceived in a two-tiered fashion.
A first tierassigns abstract labels such as ARG0 or ARG1,while a separate annotation records the second-tier, verb-sense specific meaning of these labels.Labels ARG0 or ARG1 are assigned to the mostprominent argument in the sentence (ARG1 forunaccusative verbs and ARG0 for all other verbs).The other labels are assigned in the order of promi-nence.
So, while the same high-level labels areused across verbs, they could have different mean-ings for different verb senses.
Researchers haveusually concentrated on the high-level annotation,but as indicated in Yi et al (2007), there is rea-son to think that these labels do not generaliseacross verbs, nor to unseen verbs or to novel verb288senses.
Because the meaning of the role annota-tion is verb-specific, there is also reason to thinkthat it fragments the data and creates data sparse-ness, making automatic learning from examplesmore difficult.
These short-comings are more ap-parent in the annotation of less prominent and lessfrequent roles, marked by the ARG2 to ARG5 la-bels.Zapirain et al (2008), Loper et al (2007) andYi et al (2007) investigated the ability of the Prop-Bank role inventory to generalise compared to theannotation in another semantic role list, proposedin the electronic dictionary VerbNet.
VerbNet la-bels are assigned in a verb-class specific way andhave been devised to be more similar to the inven-tories of thematic role lists usually proposed bylinguists.
The results in these papers are conflict-ing.While Loper et al (2007) and Yi et al (2007)show that augmenting PropBank labels with Verb-Net labels increases generalisation of the less fre-quent labels, such as ARG2, to new verbs and newdomains, they also show that PropBank labels per-form better overall, in a semantic role labellingtask.
Confirming this latter result, Zapirain et al(2008) find that PropBank role labels are more ro-bust than VerbNet labels in predicting new verbusages, unseen verbs, and they port better to newdomains.The apparent contradiction of these results canbe due to several confounding factors in the exper-iments.
First, the argument labels for which theVerbNet improvement was found are infrequent,and might therefore not have influenced the over-all results enough to counterbalance new errors in-troduced by the finer-grained annotation scheme;second, the learning methods in both these exper-imental settings are largely based on syntactic in-formation, thereby confounding learning and gen-eralisation due to syntax ?
which would favourthe more syntactically-driven PropBank annota-tion ?
with learning due to greater generality ofthe semantic role annotation; finally, task-specificlearning-based experiments do not guarantee thatthe learners be sufficiently powerful to make useof the full generality of the semantic role labels.In this paper, we compare the two annotationschemes, analysing how well they fare in captur-ing the linguistic generalisations that are knownto hold for semantic role labels, and consequentlyhow well they grammaticalise aspects of mean-ing.
Because the well-attested strong correlationbetween syntactic structure and semantic role la-bels (Levin and Rappaport Hovav, 2005; Merloand Stevenson, 2001) could intervene as a con-founding factor in this analysis, we expressly limitour investigation to data analyses and statisticalmeasures that do not exploit syntactic properties orparsing techniques.
The conclusions reached thisway are not task-specific and are therefore widelyapplicable.To preview, based on results in section 3, weconclude that PropBank is easier to learn, butVerbNet is more informative in general, it gener-alises better to new role instances and its labels aremore strongly correlated to specific verbs.
In sec-tion 4, we show that VerbNet labels provide finer-grained specificity.
PropBank labels are more con-centrated on a few VerbNet labels at higher fre-quency.
This is not true at low frequency, whereVerbNet provides disambiguations to overloadedPropBank variables.
Practically, these two setsof results indicate that both annotation schemescould be useful in different circumstances, and atdifferent frequency bands.
In section 5, we reportresults indicating that PropBank role sets are high-level abstractions of VerbNet role sets and thatVerbNet role sets are more verb and class-specific.In section 6, we show that PropBank more closelycaptures the thematic hierarchy and is more corre-lated to grammatical functions, hence potentiallymore useful for semantic role labelling, for learn-ers whose features are based on the syntactic tree.Finally, in section 7, we summarise some previ-ous results, and we provide new statistical evi-dence to argue that VerbNet labels are more gen-eral across verbs.
These conclusions are reachedby task-independent statistical analyses.
The dataand the measures used to reach these conclusionsare discussed in the next section.2 Materials and MethodIn data analysis and inferential statistics, carefulpreparation of the data and choice of the appropri-ate statistical measures are key.
We illustrate thedata and the measures used here.2.1 Data and Semantic Role AnnotationProposition Bank (Palmer et al, 2005) addsLevin?s style predicate-argument annotation andindication of verbs?
alternations to the syntacticstructures of the Penn Treebank (Marcus et al,2891993).It defines a limited role typology.
Roles arespecified for each verb individually.
Verbal pred-icates in the Penn Treebank (PTB) receive a labelREL and their arguments are annotated with ab-stract semantic role labels A0-A5 or AA for thosecomplements of the predicative verb that are con-sidered arguments, while those complements ofthe verb labelled with a semantic functional labelin the original PTB receive the composite seman-tic role label AM-X , where X stands for labelssuch as LOC, TMP or ADV, for locative, tem-poral and adverbial modifiers respectively.
Prop-Bank uses two levels of granularity in its annota-tion, at least conceptually.
Arguments receivinglabels A0-A5 or AA do not express consistent se-mantic roles and are specific to a verb, while argu-ments receiving an AM-X label are supposed tobe adjuncts and the respective roles they expressare consistent across all verbs.
However, amongargument labels, A0 and A1 are assigned attempt-ing to capture Proto-Agent and Proto-Patient prop-erties (Dowty, 1991).
They are, therefore, morevalid across verbs and verb instances than the A2-A5 labels.
Numerical results in Yi et al (2007)show that 85% of A0 occurrences translate intoAgent roles and more than 45% instances of A1map into Patient and Patient-like roles, using aVerbNet labelling scheme.
This is also confirmedby our counts, as illustrated in Tables 3 and 4 anddiscussed in Section 4 below.VerbNet is a lexical resource for English verbs,yielding argumental and thematic information(Kipper, 2005).
VerbNet resembles WordNet inspirit, it provides a verbal lexicon tying verbal se-mantics (theta-roles and selectional restrictions) toverbal distributional syntax.
VerbNet defines 23thematic roles that are valid across verbs.
The listof thematic roles can be seen in the first column ofTable 4.For some of our comparisons below to be valid,we will need to reduce the inventory of labels ofVerbNet to the same number of labels in Prop-Bank.
Following previous work (Loper et al,2007), we define equivalence classes of VerbNetlabels.
We will refer to these classes as VerbNetgroups.
The groups we define are illustrated inFigure 1.
Notice also that all our comparisons,like previous work, will be limited to the obliga-tory arguments in PropBank, the A0 to A5, AAarguments, to be comparable to VerbNet.
VerbNetis a lexicon and by definition it does not list op-tional modifiers (the arguments labelled AM-X inPropBank).In order to support the joint use of both these re-sources and their comparison, SemLink has beendeveloped (Loper et al, 2007).
SemLink1 pro-vides mappings from PropBank to VerbNet for theWSJ portion of the Penn Treebank.
The mappinghave been annotated automatically by a two-stageprocess: a lexical mapping and an instance classi-fier (Loper et al, 2007).
The results were hand-corrected.
In addition to semantic roles for bothPropBank and VerbNet, SemLink contains infor-mation about verbs, their senses and their VerbNetclasses which are extensions of Levin?s classes.The annotations in SemLink 1.1. are not com-plete.
In the analyses presented here, we haveonly considered occurrences of semantic roles forwhich both a PropBank and a VerbNet label isavailable in the data (roughly 45% of the Prop-Bank semantic roles have a VerbNet semanticrole).2 Furthermore, we perform our analyses ontraining and development data only.
This meansthat we left section 23 of the Wall Street Journalout.
The analyses are done on the basis of 106,459semantic role pairs.For the analysis concerning the correlation be-tween semantic roles and syntactic dependenciesin Section 6, we merged the SemLink data with thenon-projectivised gold data of the CoNNL 2008shared task on syntactic and semantic dependencyparsing (Surdeanu et al, 2008).
Only those depen-dencies that bear both a syntactic and a semanticlabel have been counted for test and developmentset.
We have discarded discontinous arguments.Analyses are based on 68,268 dependencies in to-tal.2.2 MeasuresIn the following sections, we will use simple pro-portions, entropy, joint entropy, conditional en-tropy, mutual information, and a normalised formof mutual information which measures correlationbetween nominal attributes called symmetric un-certainty (Witten and Frank, 2005, 291).
These areall widely used measures (Manning and Schuetze,1999), excepted perhaps the last one.
We brieflydescribe it here.1(http://verbs.colorado.edu/semlink/)2In some cases SemLink allows for multiple annotations.In those cases we selected the first annotation.290AGENT: Agent, Agent1PATIENT: PatientGOAL: Recipient, Destination, Location, Source,Material, Beneficiary, GoalEXTENT: Extent, Asset, ValuePREDATTR: Predicate, Attribute, Theme,Theme1, Theme2, Topic, Stimulus, PropositionPRODUCT: Patient2, Product, Patient1INSTRCAUSE: Instrument, Cause, Experiencer,Actor2, Actor, Actor1Figure 1: VerbNet GroupsGiven a random variable X, the entropy H(X)describes our uncertainty about the value of X, andhence it quantifies the information contained in amessage trasmitted by this variable.
Given tworandom variables X,Y, the joint entropy H(X,Y)describes our uncertainty about the value of thepair (X,Y).
Symmetric uncertainty is a normalisedmeasure of the information redundancy betweenthe distributions of two random variables.
It cal-culates the ratio between the joint entropy of thetwo random variables if they are not independentand the joint entropy if the two random variableswere independent (which is the sum of their indi-vidual entropies).
This measure is calculated asfollows.U(A,B) = 2H(A) + H(B)?H(A,B)H(A) + H(B)where H(X) = ?
?x?X p(x)logp(x) andH(X,Y ) = ?
?x?X,y?Y p(x, y)logp(x, y).Symmetric uncertainty lies between 0 and 1.
Ahigher value for symmetric uncertainty indicatesthat the two random variables are more highly as-sociated (more redundant), while lower values in-dicate that the two random variables approach in-dependence.We use these measures to evaluate how well twosemantic role inventories capture well-known dis-tributional generalisations.
We discuss several ofthese generalisations in the following sections.3 Amount of Information in SemanticRoles InventoryMost proposals of semantic role inventories agreeon the fact that the number of roles should be smallto be valid generally.
33With the notable exception of FrameNet, which is devel-oping a large number of labels organised hierarchically andTask PropBank ERR VerbNet ERRRole generalisation 62 (82?52/48) 66 (77?33/67)No verbal features 48 (76?52/48) 43 (58?33/67)Unseen predicates 50 (75?52/48) 37 (62?33/67)Table 2: Percent Error rate reduction (ERR) acrossrole labelling sets in three tasks in Zapirain et al(2008).
ERR= (result ?
baseline / 100% ?
base-line )PropBank and VerbNet clearly differ in the levelof granularity of the semantic roles that have beenassigned to the arguments.
PropBank makes fewerdistinctions than VerbNet, with 7 core argumentlabels compared to VerbNet?s 23.
More importantthan the size of the inventory, however, is the factthat PropBank has a much more skewed distribu-tion than VerbNet, illustrated in Table 1.
Conse-quently, the distribution of PropBank labels hasan entropy of 1.37 bits, and even when the Verb-Net labels are reduced to 7 equivalence classesthe distribution has an entropy of 2.06 bits.
Verb-Net therefore conveys more information, but it isalso more difficult to learn, as it is more uncertain.An uninformed PropBank learner that simply as-signed the most frequent label would be correct52% of the times by always assigning an A1 label,while for VerbNet would be correct only 33% ofthe times assigning Agent.This simple fact might cast new light on someof the comparative conclusions of previous work.In some interesting experiments, Zapirain et al(2008) test generalising abilities of VerbNet andPropBank comparatively to new role instances ingeneral (their Table 1, line CoNLL setting, col-umn F1 core), and also on unknown verbs and inthe absence of verbal features.
They find that alearner based on VerbNet has worse learning per-formance.
They interpret this result as indicatingthat VerbNet labels are less general and more de-pendent on knowledge of specific verbs.
However,a comparison that takes into consideration the dif-ferential baseline is able to factor the difficulty ofthe task out of the results for the overall perfor-mance.
A simple baseline for a classifier is basedon a majority class assignment (see our Table 1).We use the performance results reported in Zapi-rain et al (2008) and calculate the reduction in er-ror rate based on this differential baseline for thetwo annotation schemes.
We compare only theresults for the core labels in PropBank as thoseinterpreted frame-specifically (Ruppenhofer et al, 2006).291PropBank VerbNetA0 38.8 Agent 32.8 Cause 1.9 Source 0.9 Asset 0.3 Goal 0.00A1 51.7 Theme 26.3 Product 1.6 Actor1 0.8 Material 0.2 Agent1 0.00A2 9.0 Topic 11.5 Extent 1.3 Theme2 0.8 Beneficiary 0.2A3 0.5 Patient 5.8 Destination 1.2 Theme1 0.8 Proposition 0.1A4 0.0 Experiencer 4.2 Patient1 1.2 Attribute 0.7 Value 0.1A5 0.0 Predicate 2.3 Location 1.0 Patient2 0.5 Instrument 0.1AA 0.0 Recipient 2.2 Stimulus 0.9 Actor2 0.3 Actor 0.0Table 1: Distribution of PropBank core labels and VerbNet labels.are the ones that correspond to VerbNet.4 Wefind more mixed results than previously reported.VerbNet has better role generalising ability overallas its reduction in error rate is greater than Prop-Bank (first line of Table 2), but it is more degradedby lack of verb information (second and third linesof Table 2).
The importance of verb informationfor VerbNet is confirmed by information-theoreticmeasures.
While the entropy of VerbNet labelsis higher than that of PropBank labels (2.06 bitsvs.
1.37 bits), as seen before, the conditional en-tropy of respective PropBank and VerbNet distri-butions given the verb is very similar, but higherfor PropBank (1.11 vs 1.03 bits), thereby indicat-ing that the verb provides much more informationin association with VerbNet labels.
The mutual in-formation of the PropBank labels and the verbsis only 0.26 bits, while it is 1.03 bits for Verb-Net.
These results are expected if we recall thetwo-tiered logic that inspired PropBank annota-tion, where the abstract labels are less related toverbs than labels in VerbNet.These results lead us to our first conclusion:while PropBank is easier to learn, VerbNet is moreinformative in general, it generalises better to newrole instances, and its labels are more strongly cor-related to specific verbs.
It is therefore advisableto use both annotations: VerbNet labels if the verbis available, reverting to PropBank labels if no lex-4We assume that our majority class can roughly corre-spond to Zapirain et al (2008)?s data.
Notice however thatboth sampling methods used to collect the counts are likelyto slightly overestimate frequent labels.
Zapirain et al (2008)sample only complete propositions.
It is reasonable to as-sume that higher numbered PropBank roles (A3, A4, A5) aremore difficult to define.
It would therefore more often happenthat these labels are not annotated than it happens that A0,A1, A2, the frequent labels, are not annotated.
This reason-ing is confirmed by counts on our corpus, which indicate thatincomplete propositions include a higher proportion of lowfrequency labels and a lower proportion of high frequencylabels that the overall distribution.
However, our method isalso likely to overestimate frequent labels, since we count alllabels, even those in incomplete propositions.
By the samereasoning, we will find more frequent labels than the under-lying real distribution of a complete annotation.ical information is known.4 Equivalence Classes of Semantic RolesAn observation that holds for all semantic role la-belling schemes is that certain labels seem to bemore similar than others, based on their ability tooccur in the same syntactic environment and tobe expressed by the same function words.
Forexample, Agent and Instrumental Cause are of-ten subjects (of verbs selecting animate and inan-imate subjects respectively); Patients/Themes canbe direct objects of transitive verbs and subjectsof change of state verbs; Goal and Beneficiary canbe passivised and undergo the dative alternation;Instrument and Comitative are expressed by thesame preposition in many languages (see Levinand Rappaport Hovav (2005).)
However, most an-notation schemes in NLP and linguistics assumethat semantic role labels are atomic.
It is there-fore hard to explain why labels do not appear to beequidistant in meaning, but rather to form equiva-lence classes in certain contexts.
5While both role inventories under scrutiny hereuse atomic labels, their joint distribution showsinteresting relations.
The proportion counts areshown in Table 3 and 4.If we read these tables column-wise, therebytaking the more linguistically-inspired labels inVerbNet to be the reference labels, we observethat the labels in PropBank are especially con-centrated on those labels that linguistically wouldbe considered similar.
Specifically, in Table 3A0 mostly groups together Agents and Instrumen-tal Causes; A1 mostly refers to Themes and Pa-tients; while A2 refers to Goals and Themes.
If we5Clearly, VerbNet annotators recognise the need to ex-press these similarities since they use variants of the samelabel in many cases.
Because the labels are atomic however,the distance between Agent and Patient is the same as Patientand Patient1 and the intended greater similarity of certain la-bels is lost to a learning device.
As discussed at length in thelinguistic literature, features bundles instead of atomic labelswould be the mechanism to capture the differential distanceof labels in the inventory (Levin and Rappaport Hovav, 2005).292A0 A1 A2 A3 A4 A5 AAAgent 32.6 0.2 - - - - -Patient 0.0 5.8 - - - - -Goal 0.0 1.5 4.0 0.2 0.0 0.0 -Extent - 0.2 1.3 0.2 - - -PredAttr 1.2 39.3 2.9 0.0 - - 0.0Product 0.1 2.7 0.6 - 0.0 - -InstrCause 4.8 2.2 0.3 0.1 - - -Table 3: Distribution of PropBank by VerbNetgroup labels according to SemLink.
Counts indi-cated as 0.0 approximate zero by rounding, whilea - sign indicates that no occurrences were found.read these tables row-wise, thereby concentratingon the grouping of PropBank labels provided byVerbNet labels, we see that low frequency Prop-Bank labels are more evenly spread across Verb-Net labels than the frequent labels, and it is moredifficult to identify a dominant label than for high-frequency labels.
Because PropBank groups to-gether VerbNet labels at high frequency, whileVerbNet labels make different distinctions at lowerfrequencies, the distribution of PropBank is muchmore skewed than VerbNet, yielding the differ-ences in distributions and entropy discussed in theprevious section.We can draw, then, a second conclusion: whileVerbNet is finer-grained than PropBank, the twoclassifications are not in contradiction with eachother.
VerbNet greater specificity can be used indifferent ways depending on the frequency of thelabel.
Practically, PropBank labels could providea strong generalisation to a VerbNet annotation athigh-frequency.
VerbNet labels, on the other hand,can act as disambiguators of overloaded variablesin PropBank.
This conclusion was also reachedby Loper et al (2007).
Thus, both annotationschemes could be useful in different circumstancesand at different frequency bands.5 The Combinatorics of Semantic RolesSemantic roles exhibit paradigmatic generalisa-tions ?
generalisations across similar semanticroles in the inventory ?
(which we saw in section4.)
They also show syntagmatic generalisations,generalisations that concern the context.
One kindof context is provided by what other roles they canoccur with.
It has often been observed that cer-tain semantic roles sets are possible, while oth-ers are not; among the possible sets, certain aremuch more frequent than others (Levin and Rap-paport Hovav, 2005).
Some linguistically-inspiredA0 A1 A2 A3 A4 A5 AAActor 0.0 - - - - - -Actor1 0.8 - - - - - -Actor2 - 0.3 0.1 - - - -Agent1 0.0 - - - - - -Agent 32.6 0.2 - - - - -Asset - 0.1 0.0 0.2 - - -Attribute - 0.1 0.7 - - - -Beneficiary - 0.0 0.1 0.1 0.0 - -Cause 0.7 1.1 0.1 0.1 - - -Destination - 0.4 0.8 0.0 - - -Experiencer 3.3 0.9 0.1 - - - -Extent - - 1.3 - - - -Goal - - - - 0.0 - -Instrument - - 0.1 0.0 - - -Location 0.0 0.4 0.6 0.0 - 0.0 -Material - 0.1 0.1 0.0 - - -Patient 0.0 5.8 - - - - -Patient1 0.1 1.1 - - - - -Patient2 - 0.1 0.5 - - - -Predicate - 1.2 1.1 0.0 - - -Product 0.0 1.5 0.1 - 0.0 - -Proposition - 0.0 0.1 - - - -Recipient - 0.3 2.0 - 0.0 - -Source - 0.3 0.5 0.1 - - -Stimulus - 1.0 - - - - -Theme 0.8 25.1 0.5 0.0 - - 0.0Theme1 0.4 0.4 0.0 0.0 - - -Theme2 0.1 0.4 0.3 - - - -Topic - 11.2 0.3 - - - -Value - 0.1 - - - - -Table 4: Distribution of PropBank by originalVerbNet labels according to SemLink.
Countsindicated as 0.0 approximate zero by rounding,while a - sign indicates that no occurrences werefound.semantic role labelling techniques do attempt tomodel these dependencies directly (Toutanova etal., 2008; Merlo and Musillo, 2008).Both annotation schemes impose tight con-straints on co-occurrence of roles, independentlyof any verb information, with 62 role sets forPropBank and 116 role combinations for VerbNet,fewer than possible.
Among the observed rolesets, some are more frequent than expected un-der an assumption of independence between roles.For example, in PropBank, propositions compris-ing A0, A1 roles are observed 85% of the time,while they would be expected to occur only in 20%of the cases.
In VerbNet the difference is also greatbetween the 62% observed Agent, PredAttr propo-sitions and the 14% expected.Constraints on possible role sets are the expres-sion of structural constraints among roles inheritedfrom syntax, which we discuss in the next section,but also of the underlying event structure of theverb.
Because of this relation, we expect a strongcorrelation between role sets and their associated293A0,A1 A0,A2 A1,A2Agent, Theme 11650 109 4Agent, Topic 8572 14 0Agent, Patient 1873 0 0Experiencer, Theme 1591 0 15Agent, Product 993 1 0Agent, Predicate 960 64 0Experiencer, Stimulus 843 0 0Experiencer, Cause 756 0 2Table 5: Sample of role sets correspondencesverb, as well as role sets and verb classes for bothannotation schemes.
However, PropBank roles areassociated based on the meaning of the verb, butalso based on their positional prominence in thetree, and so we can expect their relation to the ac-tual verb entry to be weaker.We measure here simply the correlation as in-dicated by the symmetric uncertainty of the jointdistribution of role sets by verbs and of role setsby verb classes, for each of the two annotationschemes.
We find that the correlation betweenPropBank role sets and verb classes is weakerthan the correlation between VerbNet role sets andverb classes, as expected (PropBank: U=0.21 vsVerbNet: U=0.46).
We also find that correlationbetween PropBank role sets and verbs is weakerthan the correlation between VerbNet role sets andverbs (PropBank: U=0.23 vs VerbNet U=0.43).Notice that this result holds for VerbNet role labelgroups, and is therefore not a side-effect of a dif-ferent size in role inventory.
This result confirmsour findings reported in Table 2, which showeda larger degradation of VerbNet labels in the ab-sence of verb information.If we analyse the data, we see that many rolesets that form one single set in PropBank are splitinto several sets in VerbNet, with those roles thatare different being roles that in PropBank form agroup.
So, for example, a role list (A0, A1) inPropBank will corresponds to 14 different lists inVerbNet (when using the groups).
The three mostfrequent VerbNet role sets describe 86% of thecases: (Agent, Predattr) 71%, (InstrCause, Pre-dAttr) 9%, and (Agent, Patient) 6% .
Using theoriginal VerbNet labels ?
a very small sample ofthe most frequent ones is reported in Table 5 ?we find 39 different sets.
Conversely, we see thatVerbNet sets corresponds to few PropBank sets,even for high frequency.The third conclusion we can draw then is two-fold.
First, while VerbNet labels have been as-signed to be valid across verbs, as confirmed bytheir ability to enter in many combinations, thesecombinations are more verb and class-specificthan combinations in PropBank.
Second, the fine-grained, coarse-grained correspondence of anno-tations between VerbNet and PropBank that wasillustrated by the results in Section 4 is also borneout when we look at role sets: PropBank role setsappear to be high-level abstractions of VerbNetrole sets.6 Semantic Roles and GrammaticalFunctions: the Thematic HierarchyA different kind of context-dependence is pro-vided by thematic hierarchies.
It is a well-attestedfact that lexical semantic properties described bysemantic roles and grammatical functions appearto be distributed according to prominence scales(Levin and Rappaport Hovav, 2005).
Seman-tic roles are organized according to the thematichierarchy (one proposal among many is Agent> Experiencer> Goal/Source/Location> Patient(Grimshaw, 1990)).
This hierarchy captures thefact that the options for the structural realisationof a particular argument do not depend only onits role, but also on the roles of other arguments.For example in psychological verbs, the positionof the Experiencer as a syntactic subject or ob-ject depends on whether the other role in the sen-tence is a Stimulus, hence lower in the hierar-chy, as in the psychological verbs of the fear classor an Agent/Cause as in the frighten class.
Twoprominence scales can combine by matching ele-ments harmonically, higher elements with higherelements and lower with lower (Aissen, 2003).Grammatical functions are also distributed accord-ing to a prominence scale.
Thus, we find that mostsubjects are Agents, most objects are Patients orThemes, and most indirect objects are Goals, forexample.The semantic role inventory, thus, should showa certain correlation with the inventory of gram-matical functions.
However, perfect correlation isclearly not expected as in this case the two levelsof representation would be linguistically and com-putationally redundant.
Because PropBank wasannotated according to argument prominence, weexpect to see that PropBank reflects relationshipsbetween syntax and semantic role labels morestrongly than VerbNet.
Comparing syntactic de-pendency labels to their corresponding PropBankor VerbNet groups labels (groups are used to elim-294inate the confound of different inventory sizes), wefind that the joint entropy of PropBank and depen-dency labels is 2.61 bits while the joint entropy ofVerbNet and dependency labels is 3.32 bits.
Thesymmetric uncertainty of PropBank and depen-dency labels is 0.49, while the symmetric uncer-tainty of VerbNet and dependency labels is 0.39.On the basis of these correlations, we can con-firm previous findings: PropBank more closelycaptures the thematic hierarchy and is more corre-lated to grammatical functions, hence potentiallymore useful for semantic role labelling, for learn-ers whose features are based on the syntactic tree.VerbNet, however, provides a level of annotationthat is more independent of syntactic information,a property that might be useful in several applica-tions, such as machine translation, where syntacticinformation might be too language-specific.7 Generality of Semantic RolesSemantic roles are not meant to be domain-specific, but rather to encode aspects of our con-ceptualisation of the world.
A semantic role in-ventory that wants to be linguistically perspicuousand also practically useful in several tasks needs toreflect our grammatical representation of events.VerbNet is believed to be superior in this respectto PropBank, as it attempts to be less verb-specificand to be portable across classes.
Previous results(Loper et al, 2007; Zapirain et al, 2008) appear toindicate that this is not the case because a labellerhas better performance with PropBank labels thanwith VerbNet labels.
But these results are task-specific, and they were obtained in the context ofparsing.
Since we know that PropBank is moreclosely related to grammatical function and syn-tactic annotation than VerbNet, as indicated abovein Section 6, then these results could simply indi-cate that parsing predicts PropBank labels betterbecause they are more closely related to syntacticlabels, and not because the semantic roles inven-tory is more general.Several of the findings in the previous sectionsshed light on the generality of the semantic roles inthe two inventories.
Results in Section 3 show thatprevious results can be reinterpreted as indicatingthat VerbNet labels generalise better to new roles.We attempt here to determine the generality ofthe ?meaning?
of a role label without recourseto a task-specific experiment.
It is often claimedin the literature that semantic roles are better de-scribed by feature bundles.
In particular, the fea-tures sentience and volition have been shown to beuseful in distinguishing Proto-Agents from Proto-Patients (Dowty, 1991).
These features can be as-sumed to be correlated to animacy.
Animacy hasindeed been shown to be a reliable indicator ofsemantic role differences (Merlo and Stevenson,2001).
Personal pronouns in English grammati-calise animacy.
We extract all the occurrences ofthe unambiguously animate pronouns (I, you, he,she, us, we, me, us, him) and the unambiguouslyinanimate pronoun it, for each semantic role label,in PropBank and VerbNet.
We find occurrencesfor three semantic role labels in PropBank and sixin VerbNet.
We reduce the VerbNet groups to fiveby merging Patient roles with PredAttr roles toavoid artificial variation among very similar roles.An analysis of variance of the distributions of thepronous yields a significant effect of animacy forVerbNet (F(4)=5.62, p< 0.05), but no significanteffect for PropBank (F(2)=4.94, p=0.11).
This re-sult is a preliminary indication that VerbNet labelsmight capture basic components of meaning moreclearly than PropBank labels, and that they mighttherefore be more general.8 ConclusionsIn this paper, we have proposed a task-independent, general method to analyse anno-tation schemes.
The method is based oninformation-theoretic measures and comparisonwith attested linguistic generalisations, to evalu-ate how well semantic role inventories and anno-tations capture grammaticalised aspects of mean-ing.
We show that VerbNet is more verb-specificand better able to generalise to new semantic roles,while PropBank, because of its relation to syntax,better captures some of the structural constraintsamong roles.
Future work will investigate anotherbasic property of semantic role labelling schemes:cross-linguistic validity.AcknowledgementsWe thank James Henderson and Ivan Titov foruseful comments.
The research leading to theseresults has received partial funding from the EUFP7 programme (FP7/2007-2013) under grantagreement number 216594 (CLASSIC project:www.classic-project.org).295ReferencesJudith Aissen.
2003.
Differential object marking:Iconicity vs. economy.
Natural Language and Lin-guistic Theory, 21:435?483.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the Thirty-Sixth Annual Meeting of the As-sociation for Computational Linguistics and Seven-teenth International Conference on ComputationalLinguistics (ACL-COLING?98), pages 86?90, Mon-treal, Canada.David Dowty.
1991.
Thematic proto-roles and argu-ment selection.
Language, 67(3):547?619.Charles Fillmore.
1968.
The case for case.
In EmmonBach and Harms, editors, Universals in LinguisticTheory, pages 1?88.
Holt, Rinehart, and Winston.Jane Grimshaw.
1990.
Argument Structure.
MITPress.Jeffrey Gruber.
1965.
Studies in Lexical Relation.MIT Press, Cambridge, MA.Ray Jackendoff.
1972.
Semantic Interpretation inGenerative Grammar.
MIT Press, Cambridge, MA.Karin Kipper.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Ph.D. thesis, University ofPennsylvania.Beth Levin and Malka Rappaport Hovav.
2005.
Ar-gument Realization.
Cambridge University Press,Cambridge, UK.Edward Loper, Szu ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping betweenPropBank and VerbNet.
In Proceedings of theIWCS.Christopher Manning and Hinrich Schuetze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Mitch Marcus, Beatrice Santorini, and M.A.Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.Computational Linguistics, 19:313?330.Paola Merlo and Gabriele Musillo.
2008.
Semanticparsing for high-precision semantic role labelling.In Proceedings of the Twelfth Conference on Com-putational Natural Language Learning (CONLL-08), pages 1?8, Manchester, UK.Paola Merlo and Suzanne Stevenson.
2001.
Automaticverb classification based on statistical distributionsof argument structure.
Computational Linguistics,27(3):373?408.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31:71?105.Josef Ruppenhofer, Michael Ellsworth, MiriamPetruck, Christopher Johnson, and Jan Scheffczyk.2006.
Framenet ii: Theory and practice.
Technicalreport, Berkeley,CA.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning (CoNLL-2008), pages 159?177.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2).Ian Witten and Eibe Frank.
2005.
Data Mining.
Else-vier.Szu-ting Yi, Edward Loper, and Martha Palmer.
2007.Can semantic roles generalize across genres?
InProceedings of the Human Language Technologies2007 (NAACL-HLT?07), pages 548?555, Rochester,New York, April.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.2008.
Robustness and generalization of role sets:PropBank vs. VerbNet.
In Proceedings of ACL-08:HLT, pages 550?558, Columbus, Ohio, June.296
