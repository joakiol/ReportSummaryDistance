2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsDetecting Visual TextJesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6Kota Yamaguchi3, Yejin Choi3, Hal Daume?
III2, Alexander C. Berg3 and Tamara L. Berg31University of Washington, 2University of Maryland, 3Stony Brook University4MIT, 5Oregon Health & Science University, 6Columbia Universitydodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.eduacmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edukyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edume@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.eduAbstractWhen people describe a scene, they often in-clude information that is not visually apparent;sometimes based on background knowledge,sometimes to tell a story.
We aim to sepa-rate visual text?descriptions of what is beingseen?from non-visual text in natural imagesand their descriptions.
To do so, we first con-cretely define what it means to be visual, an-notate visual text and then develop algorithmsto automatically classify noun phrases as vi-sual or non-visual.
We find that using textalone, we are able to achieve high accuraciesat this task, and that incorporating featuresderived from computer vision algorithms im-proves performance.
Finally, we show that wecan reliably mine visual nouns and adjectivesfrom large corpora and that we can use theseeffectively in the classification task.1 IntroductionPeople use language to describe the visual world.Our goal is to: formalize what ?visual text?
is (Sec-tion 2.2); analyze naturally occurring written lan-guage for occurrences of visual text (Section 2); andbuild models that can detect visual descriptions fromraw text or from image/text pairs (Section 3).
Thisis a challenging problem.
One challenge is demon-strated in Figure 1, which contains two images thatcontain the noun ?car?
in their human-written cap-tions.
In one case (the top image), there actually is acar in the image; in the other case, there is not: thecar refers to the state of the speaker.The ability to automatically identify visual text ispractically useful in a number of scenarios.
One canAnother dream car toadd to the list, this onespotted in Hanbury St.Shot out my car win-dow while stuck in traf-fic because people inCincinnati can?t drive inthe rain.Figure 1: Two image/caption pairs, both containing thenoun ?car?
but only the top one in a visual context.imagine automatically mining image/caption data(like that in Figure 1) to train object recognition sys-tems.
However, in order to do so reliably, one mustknow whether the ?car?
actually appears or not.When building image search engines, it is commonto use text near an image as features; this is moreuseful when this text is actually visual.
Or whentraining systems to automatically generate captionsof images (e.g., for visually impaired users), weneed good language models for visual text.One of our goals is to define what it means for abit of text to be visual.
As inspiration, we considerimage/description pairs automatically crawled fromFlickr (Ordonez et al, 2011).
A first pass attemptmight be to say ?a phrase in the description of animage is visual if you can see it in the correspondingimage.?
Unfortunately, this is too vague to be useful;the biggest issues are discussed in Section 2.2.762Based on our analysis, we settled on the follow-ing definition: A piece of text is visual (with re-spect to a corresponding image) if you can cut outa part of that image, paste it into any other image,and a third party could describe that cut-out part inthe same way.
In the car example, the claim is that Icould cut out the car, put it in the middle of any otherimage, and someone else might still refer to that caras ?dream car.?
The car in the bottom image in Fig-ure 1 is not visual because there?s nothing you couldcut out that would retain car-ness.2 Data AnalysisBefore embarking on the road to building models ofvisual text, it is useful to obtain a better understand-ing of what visual text is like, and how it compares tothe more standard corpora that we are used to work-ing with.
We describe the two large data sets that weuse (one visual, one non-visual), then describe thequantitative differences between them, and finallydiscuss our annotation effort for labeling visual text.2.1 Data setsWe use the SBU Captioned Photo Dataset (Ordonezet al, 2011) as our primary source of image/captiondata.
This dataset contains 1 million images withuser associated captions, collected in the wild by in-telligent filtering of a huge number of Flickr pho-tos.
Past work has made use of this dataset to re-trieve whole captions for association with a queryimage (Ordonez et al, 2011).
Their method firstused global image descriptors to retrieve an initialmatched set, and then applied more local estimatesof content to re-rank this (relatively small) set (Or-donez et al, 2011).
This means that content basedmatching was relatively constrained by the bottle-neck of global descriptors, and local content (e.g.,objects) had relatively small effect on accuracy.As an auxiliary source of information for (largely)non-visual text, we consider a large corpus of textobtained by concatenating ukWaC1 and the NewYork Times Newswire Service (NYT) section of theGigaword (Graff, 2003) Corpus.
The Web-derivedukWaC is already tokenized and POS-tagged withthe TreeTagger (Schmid, 1995).
NYT is tokenized,1ukWaC is a freely available Wikipedia-derived corpus from2009; see http://wacky.sslmit.unibo.it/doku.php.and POS-tagged using TagChunk (Daume?
III andMarcu, 2005).
This consists of 171 million sen-tences (4 billion words).
We refer to this generictext corpus as Large-Data.2.2 Formalizing visual textWe begin our analysis by revisiting the definitionof visual text from the introduction, and justifyingthis particular definition.
In order to arrive at a suf-ficiently specific definition of ?visual text,?
we fo-cused on the applications of visual text that we careabout.
As discussed in the introduction, these are:training object detectors, building image search en-gines and automatically generating captions for im-ages.
Our definition is based on access to image/textpairs, but later we discuss how to talk about it purelybased on text.
To make things concrete, consider animage/text pair like that in the top of Figure 1.
Andthen consider a phrase in the text, like ?dream car.
?The question is: is ?dream car?
visual or not?One of the challenges in arriving at such a defi-nition is that the description of an image in Flickris almost always written by the photographer of thatimage.
This means the descriptions often contain in-formation that is not actually pictured in the image,or contain references that are only relevant to thephotographer (referring to a person/pet by name).One might think that this is an artifact of this par-ticular dataset, but it appears to be generic to all cap-tions, even those written by a viewer (rather than thephotographer).
Figure 2 shows an image from thePascal dataset (Everingham et al, 2010), togetherwith captions written by random people collectedvia crowd-sourcing (Rashtchian et al, 2010).
Thereis much in this caption that is clearly made-up by theauthor, presumably to make the caption more inter-esting (e.g., meta-references like ?the camera?
or ?Aphoto?
as well as ?guesses?
about the image, such as?garage?
and ?venison?
).Second, there is a question of how much inferenceyou are allowed to do when you say that you ?see?something.
For example, in the top image in Fig-ure 1, the street is pictured, but does that mean that?Hanbury St.?
is visual?
What if there were a streetsign that clearly read ?Hanbury St.?
in the image?This problem comes up all the time, when peoplesay things like ?in London?
or ?in France?
in theircaptions.
If it?s just a portrait of people ?in France,?7631.
A distorted photo of a man cutting up a large cut of meat in a garage.2.
A man smiling at the camera while carving up meat.3.
A man smiling while he cuts up a piece of meat.4.
A smiling man is standing next to a table dressing a piece of venison.5.
The man is smiling into the camera as he cuts meat.Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing.
Measurements on theSMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest arenon-visual (underlined).
See Section 2.4 for details.it?s hard to say that this is visual.
If you see the Eif-fel tower in the background, this is perhaps better(though it could be Las Vegas!
), but how does thiscompare to a photo taken out of an airplane windowin which you actually do see France-the-country?This problem becomes even more challengingwhen you consider things other than nouns.
For in-stance, when is a verb visual?
For instance, the mostcommon non-copula verb in our data is ?sitting,?which appears in roughly two usages: (1) ?Took thisshot, sitting in a bar and enjoying a Portugese beer.
?and (2) ?Lexy sitting in a basket on top of her cattree.?
The first one is clearly not visual; the secondprobably is.
A more nuanced case is for ?playing,?as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-ing in Nannie?s yard.?
The corresponding image forthe latter description shows a sleeping cat.Our final definition, based on cutting out the po-tentially visual part of the image, allows us to saythat: (1) ?venison?
is not visual (because you cannotactually tell); (2) ?Hanbury St.?
and ?Lexy?
are notvisual (you can infer them, in the first case becausethere is only one street and in the second case be-cause there is only one cat); (3) that seeing the realEiffel tower in the background does not mean that?France?
is visual (but again, may be inferred); etc.2.3 Most Pronounced DifferencesTo get an intuitive sense of how Flickr captions (ex-pected to be predominantly visual) and generic text(expected not to be so) differ, we computed somesimple statistics on sentences from these.
In gen-eral, the generic text had twice as many main verbsas the Flickr data, four times as many auxiliaries orlight verbs, and about 50% more prepositions.Flickr captions tended to have far more referencesto physical objects (versus abstract objects) than thegeneric text, according to the WordNet hierarchy.Approximately 64% of the objects in Flickr werephysical (about 22% abstract and 14% unknown).Whereas in the generic text, only 30% of the objectswere physical, 53% were abstract (17% unknown).A third major difference between the corpora isin terms of noun modifiers.
In both corpora, nounstend not to have any modifiers, but modifiers are stillmore prevalent in Flickr than in generic text.
In par-ticular, 60% of nouns in Flickr have zero modifiers,but 70% of nouns in generic text have zero modi-fiers.
In Flickr, 30% of nouns have exactly one mod-ifier, as compared to only 22% for generic text.The breakdown of what those modifiers look likeis even more pronounced, even when restricted justto physical objects (modifier types are obtainedthrough the bootstrapping process discussed in Sec-tion 3.1).
Almost 50% of nominal modifiers in theFlickr data are color modifiers, whereas color ac-counts for less than 5% of nominal modifiers ingeneric text.
In Flickr, 10% of modifiers talk aboutbeauty, in comparison to less than 5% in generictext.
On the other hand, less than 3% of modifiersin Flickr reference ethnicity, as compared to almost20% in generic text; and 20% of Flickr modifiersreference size, versus 50% in generic text.2.4 Annotating Visual TextIn order to obtain ground truth data, we rely oncrowdsourcing (via Amazon?s Mechanical Turk).Each instance is an image, a paired caption, and ahighlighted noun phrase in that caption.
The anno-tation for this instance is a label of ?visual,?
?non-visual?
or ?error,?
where the error category is re-764served for cases where the noun phrase segmenta-tion was erroneous.
Each worker is given five in-stances to label and paid one cent per annotation.2For a small amount of data (803 images contain-ing 2339 instances), we obtained annotations fromthree separate workers per instance to obtain higherquality data.
For a large amount of data (48k im-ages), we obtained annotations from only a sin-gle worker.
Subsequently, we will refer to thesetwo data sets as the SMALL and LARGE data sets.In both data sets, approximately 70% of the nounphrases were visual, 28% were non-visual and 2%were erroneous.
For simplicity, we group erroneousand non-visual for all learning and evaluation.In the SMALL data set, the rate of disagreementbetween annotators was relatively low.
In 74% of theannotations, there was no disagreement at all.
Wereconciled the annotations using the quality manage-ment technique of Ipeirotis et al (2010); only 14%of the annotations need to be changed in order to ob-tain a gold standard.One immediate question raised in this process iswhether one needs to actually see the image to per-form the annotation.
In particular, if we expect anNLP system to be able to classify noun phrases asvisual or non-visual, we need to know whether peo-ple can do this task sans image.
We therefore per-formed the same annotation on the SMALL data set,but where the workers were not shown the image.Their task was to imagine an image for this captionand then annotate the noun phrase based on whetherthey thought it would be pictured or not.
We ob-tained three annotations as before and reconciledthem (Ipeirotis et al, 2010).
The accuracy of thisreconciled version against the gold standard (pro-duced by people who did see the image) was 91%.This suggests that while people are able to do thistask with some reliability, seeing the image is veryimportant (recall that always guessing ?visual?
leadsto an accuracy of 70%).3 Visual Features from Raw TextOur first goal is to attempt to obtain relatively largeknowledge bases of terms that are (predominantly)visual.
This is potentially useful in its own right2Data available at http://hal3.name/dvt/, with direct linksback to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determinewhich query terms are likely to be pictured).
Wehave explored two techniques for performing thistask, the first based on bootstrapping (Section 3.1)and the second based on label propagation (Sec-tion 3.2).
We then use these lists to generate featuresfor a classifier that predicts whether a noun phrase?in context?is visual or not (Section 4).In addition, we consider the task of separating ad-jectives into different visual categories (Section 3.3).We have already used the results of this in Sec-tion 2.3 to understand the differences between ourtwo corpora.
It is also potentially useful for thepurpose of building new object detection systems oreven attribute detection systems, to get a vocabularyof target detections.3.1 Bootstrapping for Visual TextIn this section, we learn visual and non-visual nounsand adjectives automatically based on bootstrappingtechniques.
First, we construct a graph between ad-jectives by computing distributional similarity (Tur-ney and Pantel, 2010) between them.
For comput-ing distributional similarity between adjectives, eachtarget adjective is defined as a vector of nouns whichare modified by the target adjective.
To be exact, weuse only those adjectives as modifiers which appearadjacent to a noun (that is, in a JJ NN construction).For example, in ?small red apple,?
we consider onlyred as a modifier for noun.
We use Pointwise Mu-tual Information (PMI) (Church and Hanks, 1989)to weight the contexts, and select the top 1000 PMIcontexts for each adjective.3Next, we apply cosine similarity to find the top10 distributionally similar adjectives with respect toeach target adjective based on our large generic cor-pus (Large-Data from Section 2.1).
This creates agraph with adjectives as nodes and cosine similarityas weight on the edges.
Analogously, we construct agraph with nouns as nodes (here, adjectives are usedas contexts for nouns).We then apply bootstrapping (Kozareva et al,2008) on the noun and adjective graphs by select-ing 10 seeds for visual and non-visual nouns andadjectives (see Table 1).
We use in-degree (sum ofweights of incoming edges) to compute the score for3We are interested in descriptive adjectives, which ?typi-cally ascribe to a noun a value of an attribute?
(Miller, 1998).765Visual car house tree horse animalnouns man table bottleseeds woman computerNon-visual idea bravery deceit trustnouns dedication anger humour luckseeds inflation honestyVisual brown green wooden stripedadjectives orange rectangular furryseeds shiny rusty featheredNon-visual public original whole righteousadjectives political personal intrinsicseeds individual initial totalTable 1: Example seeds for bootstrapping.each node that has connections with known (seeds)or automatically labeled nodes, previously exploitedto learn hyponymy relations from the web (Kozarevaet al, 2008).
Intuitively, in-degree captures the pop-ularity of new instances among instances that havealready been identified as good instances.
We learnvisual and non-visual words together (known as themutual exclusion principle in bootstrapping (The-len and Riloff, 2002; McIntosh and Curran, 2008)):each word (node) is assigned to only one class.Moreover, after each iteration, we harmonically de-crease the weight of the in-degree associated withinstances learned in later iterations.
We added 25new instances at each iteration and ran 500 iterationsof bootstrapping, yielding 11955 visual and 11978non-visual nouns, and 7746 visual and 7464 non-visual adjectives.Based on manual inspection, the learned visualand non-visual lists look great.
In the future, wewould like to do a Mechanical Turk evaluation todirectly evaluate the visual and non-visual nounsand adjectives.
For now, we show the coverage ofthese classes in the Flickr data-set: Visual nouns:53.71%; Non-visual nouns: 14.25%; Visual ad-jectives: 51.79%; Non-visual adjectives: 14.40%.Overall, we find more visual nouns and adjectivesare covered in the Flickr data-set, which makessense, since the Flickr data-set is largely visual.Second, we show the coverage of these classeson the large text corpora (Large-Data from Sec-tion 2.1): Visual nouns: 26.05%; Non-visual nouns:41.16%; Visual adjectives: 20.02%; Non-visual ad-Visual: attend, buy, clean, comb, cook, drink, eat,fry, pack, paint, photograph, smash, spill, steal,taste, tie, touch, watch, wear, wipeNon-visual: achieve, admire, admit, advocate, al-leviate, appreciate, arrange, criticize, eradicate,induce, investigate, minimize, overcome, pro-mote, protest, relieve, resolve, review, support,tolerateTable 2: Predicates that are visual and non-visual.Visual: water, cotton, food, pumpkin, chicken,ring, hair, mouth, meeting, kind, filter, game, oil,show, tear, online, face, class, carNon-visual: problem, poverty, pain, issue, use,symptom, goal, effect, thought, government,share, stress, work, risk, impact, concern, obsta-cle, change, disease, disputeTable 3: Learned visual/non-visual nouns.jectives: 40.00%.
Overall, more non-visual nounsand adjectives cover text data, since Large-Data isa non-visual data-set.3.2 Label Propagation for Visual TextTo propagate visual labels, we construct a bipartitegraph between visually descriptive predicates andtheir arguments.
Let VP be the set of nodes that cor-responds to predicates, and let VA be the set of nodesthat corresponds to arguments.
To learn the visuallydescriptive words, we set VP to 20 visually descrip-tive predicates shown in the top of Table 2, and VAto all nouns that appear in the object argument posi-tion with respect to the seed predicates.
We approx-imate this by taking nouns on the right hand sideof the predicates within a window of 4 words usingthe Web 1T Google N-gram data (Brants and Franz.,2006).
For edge weights, we use conditional prob-abilities between predicates and arguments so thatw(p?
a) := pr(a|p) and w(a?
p) := pr(p|a).In order to collectively induce the visually de-scriptive words from this graph, we apply the graphpropagation algorithm of Velikovich et al (2010),a variant of label propagation algorithms (Zhu andGhahramani, 2002) that has been shown to be ef-fective for inducing a web-scale polarity lexiconbased on word co-occurrence statistics.
This algo-766Color purple blue maroon beige greenMaterial plastic cotton wooden metallic silverShape circular square round rectangular triangularSize small big tiny tall hugeSurface coarse smooth furry fluffy roughDirection sideways north upward left downPattern striped dotted checked plaid quiltedQuality shiny rusty dirty burned glitteryBeauty beautiful cute pretty gorgeous lovelyAge young mature immature older seniorEthnicity french asian american greek hispanicTable 4: Attribute Classes with their seed valuesrithm iteratively updates the semantic distance be-tween each pair of nodes in the graph, then producesa score for each node that represents how visuallydescriptive each word is.
To learn the words thatare not visually descriptive, we use the predicatesshown in the bottom of Table 2 as VP instead.
Ta-ble 3 shows the top ranked nouns that are visuallydescriptive and not visually descriptive.3.3 Bootstrapping Visual AdjectivesOur goal in this section is to automatically gener-ate comprehensive lists of adjectives for different at-tributes, such as color, material, shape, etc.
To ourknowledge, this is the first significant effort of thistype for adjectives: most bootstrapping techniquesfocus exclusively on nouns, although Almuhareband Poesio (2005) populated lists of attributes us-ing web-based similarity measures.
We found thatin some ways adjectives are easier than nouns, butrequire slightly different representations.One might conjecture that listing attributes byhand is difficult.
Colors names are well known tobe quite varied.
For instance, our bootstrappingapproach is able to discover colors like ?grayish,??chestnut,?
?emerald,?
and ?rufous?
that would behard to list manually (the last is a reddish-browncolor, somewhat like rust).
Although perhaps noteasy to create, the Wikipedia list of colors (http://en.wikipedia.org/wiki/List of colors) includes all of theseexcept ?grayish?.
On the other hand, it includescolor terms that might be difficult to make use of ascolors, such as ?bisque,?
?bone?
and ?bubbles?
(thelast is a very light cyan), which might over-generatehits.
For shape, we find ?oblong,?
?hemispherical,??quadrangular?
and, our favorite, ?convex?.We use essentially the same bootstrapping processas described earlier in Section 3.1, but on a slightlydifferent data representation.
The only difference isthat instead of linking adjectives to their 10 mostsimilar neighbors, we link them only to 25 neigh-bors to attempt to improve recall.We begin with seeds for each attribute class fromTable 4.
We conduct a manual evaluation to di-rectly measure the quality of attribute classes.
Werecruited 3 annotators and developed annotationguidelines that instructed each recruiter to judgewhether a learned value belongs to an attribute classor not.
The annotators assigned ?1?
if a learnedvalue belongs to a class, otherwise ?0?.We conduct an Information Retrieval (IR) Stylehuman evaluation.
Analogous to an IR evaluation,here the total number of relevant values for attributeclasses can not be computed.
Therefore, we assumethe correct output of several systems as the total re-call which can be produced by any system.
Now,with the help of our 3 manual annotators, we obtainthe correct output of several systems from the totaloutput produced by these systems.First, we measured the agreement on whethereach learned value belongs to a semantic class ornot.
We computed ?
to measure inter-annotatoragreement for each pair of annotators.
We focusour evaluation on 4 classes: age, beauty, color, anddirection; between Human 2 and Human 3 and be-tween Human 1 and Human 3, the ?
value was 0.48;between Human 1 and Human 2 it was 0.45.
Thesenumbers are somewhat lower than we would like,but not terrible.
If we evaluate the classes individu-ally, we find that age has the lowest ?.
If we remove?age,?
the pairwise ?s rise to 0.59, 0.57 and 0.55.Second, we compute Precision (Pr), Recall (Rec)and F-measure (F1) for different bootstrapping sys-tems (based on the number of iterations and thenumber of new words added in each iteration).Two parameter settings performed consistently bet-ter than others (10 iterations with 25 items, and 5 it-erations with 50 items).
The former system achievesa precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-man 2; the latter achieves scores of 0.54, 0.72, 0.62.4 Recognizing Visual TextWe train a logistic regression (aka maximum en-tropy) model (Daume?
III, 2004) to classify text asvisual or non-visual.
The features we use fall into767the following categories: WORDS (the actual lexi-cal items and stems); BIGRAMS (lexical bigrams);SPELL (lexical features such as capitalization pat-tern, and word prefixes and suffixes); WORDNET(set of hypernyms according to WordNet); andBOOTSTRAP (features derived from bootstrappingor label propagation).For each of these feature categories, we computefeatures inside the phrase being categorized (e.g.,?the car?
), before the phrase (two words to the left)and after the phrase (two words to the right).
Weadditionally add a feature that computes the num-ber of words in a phrase, and a feature that com-putes the position of the phrase in the caption (firstfifth through last fifth of the description).
This leadsto seventeen feature templates that are computed foreach example.
In the SMALL data set, there are 25kfeatures (10k non-singletons); in the LARGE dataset, there are 191k features (79k non-singletons).To train models on the SMALL data set, we use1500 instances as training, 200 as development andthe remaining 639 as test data.
To train models onthe LARGE data set, we use 45000 instances as train-ing and the remaining 4401 as development.
Wealways test on the 639 instances from the SMALLdata, since it has been redundantly annotated.
Thedevelopment data is used only to choose the regular-ization parameter for a Gaussian prior on the logis-tic regression model; this parameter is chosen in therange {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.Because of the imbalanced data problem, evalu-ating according to accuracy is not appropriate forthis task.
Even evaluating by precision/recall is notappropriate, because a baseline system that guessesthat everything is visual obtains 100% recall and70% precision.
Due to these issues, we insteadevaluate according to the area under the ROC curve(AUC).
To check statistical significance, we com-pute standard deviations using bootstrap resampling,and consider there to be a significant difference if aresult falls outside of two standard deviations of thebaseline (95% confidence).Figure 3 shows learning curves for the two datasets.
The SMALL data achieves an AUC score of71.3 in the full data setting (1700 examples); theLARGE data needs 12k examples to achieve similaraccuracy due to noise.
However, with 49k examples,we are able to achieve a AUC score of 75.3 using the101 102 103 104 1050.550.60.650.70.750.8Figure 3: Learning curves for training on SMALL data(blue solid) and LARGE data (black dashed).
X-axis (inlog-scale) is number of training examples; Y-axis is AUC.large data set.
By pooling the data (and weightingthe small data), this boosts results to 76.1.
The con-fidence range on these data is approximately ?1.9,meaning that this boost is likely not significant.4.1 Using Image FeaturesAs discussed previously, humans are only able toachieve 90% accuracy on the visual/non-visual taskwhen they are not allowed to view the image.This potentially upper-bounds the performance of alearned system that can only look at text.
In order toattempt to overcome this, we augment our basic sys-tem with a number of features computed from thecorresponding images.
These features are derivedfrom the output of state of the art vision algorithmsto detect 121 different objects, stuff and scenes.As our object detectors, we use standard stateof the art deformable part-based models (Felzen-szwalb et al, 2010) for 89 common object cate-gories, including: the original 20 objects from Pas-cal, 49 objects from Object Bank (Li-Jia Li and Fei-Fei, 2010), and 20 from Im2Text (Ordonez et al,2011).
We additionally use coarse image parsingto estimate background elements in each databaseimage.
Six possible background (stuff) categoriesare considered: sky, water, grass, road, tree, andbuilding.
For this we use detectors (Ordonez etal., 2011) which compute color, texton, HoG (Dalaland Triggs, 2005) and Geometric Context (Hoiemet al, 2005) as input features to a sliding win-dow based SVM classifier.
These detectors are runon all database images, creating a large pool ofbackground elements for retrieval.
Finally, we ob-768Figure 4: (Left) Highest confidence flower detected in animage; (Right) All detections in the same image.tain scene descriptors for each image by comput-ing scene classification scores for 26 common scenecategories, using the features, methods and trainingdata from the SUN dataset (Xiao et al, 2010).Figure 4 shows an example image on which sev-eral detectors have been run.
From each image, weextract the following features: which object detec-tors fired; how many times they fired; the confidenceof the most-likely firing; the percentage of the image(in pixels) that the bounding box corresponding tothis object occupies; and the percentage of the width(and height) of the image that it occupies.Unfortunately, object detection is a highly noisyprocess.
The right image in Figure 4 shows all de-tections for that image, which includes, for instance,a chair detection that spans nearly the entire image,and a person detection in the bottom-right corner.For an average image, if a single detector (e.g., theflower detector) fires once, it actually fires 40 times(??
= 1.8).
Moreover, of the 120 detectors, onan average image over 22 (??
= 5.6) of them fireat least once (though certainly in an average imageonly a few objects are actually present).
Exacerbat-ing this problem, although the confidence scores fora single detector can be compared, the scores be-tween different detectors are not at all comparable.In order to attenuate this problem, we include dupli-cate copies of all the above features restricted to themost confident object for each object type.On the SMALL data set, this adds 400 new fea-CATEGORY POSITION AUCBootstrap Phrase 65.2+ Spell Phrase 68.6+ Image - 69.2+ Words Phrase 70.0+ Length - 69.8+ Wordnet Phrase 70.4+ Wordnet Before 70.6+ Spell Before 71.8+ Words Before 72.2+ Bootstrap Before 72.4+ Spell After 71.5Table 5: Results of feature ablation on SMALL data set.Best result is in bold; results that are not statistically sig-nificantly worse are italicized.tures (300 of which are non-singletons4); on theLARGE data set, this adds 500 new features (480non-singletons).
Overall, the AUC scores trained onthe small data set increase from 71.3 to 73.9 (a sig-nificant improvement).
On the large data set, the in-crease is only from 76.1 to 76.8, which is not likelyto be significant.
In general, the improvement ob-tained by adding image features is most pronouncedin the setting of small training data, perhaps becausethese features are more generic than the highly lexi-calized features used in the textual model.
But oncethere is a substantial amount of text data, the noisyimage features become less useful.4.2 Feature AblationsIn order to ascertain the degree to which each featuretemplate is useful, we perform an ablation study.
Wefirst perform feature selection at the template levelusing the information gain criteria, and then trainmodels using the corresponding subset of features.The results on the SMALL data set are shown inTable 5.
Here, the bootstrapping features computedon words within the phrase to be classified werejudged as the most useful, followed by spelling fea-tures.
Image features were judged third most use-ful.
In general, features in the phrase were most use-ful (not surprisingly), and then features before thephrase (presumably to give context, for instance asin ?out of the window?).
Features from after thephrase were not useful.4Non-singleton features appear more than once in the data.769CATEGORY POSITION AUCWords Phrase 74.7+ Image - 74.4+ Bootstrap Phrase 74.3+ Spell Phrase 75.3+ Length - 74.7+ Words Before 76.2+ Wordnet Phrase 76.1+ Spell After 76.0+ Spell Before 76.8+ Wordnet Before 77.0+ Wordnet After 75.6Table 6: Results of feature ablation on LARGE data set.Corresponding results on the LARGE data set areshown in Table 6.
Note that the order of featuresselected is different because the training data is dif-ferent.
Here, the most useful features are simply thewords in the phrase to be classified, which alone al-ready gives an AUC score of 74.7, only a few pointsoff from the best performance of 77.0 once imagefeatures, bootstrap features and spelling features areadded.
As before, these features are rated as veryuseful for classification performance.Finally, we consider the effect of using Bootstrap-based features or label-propagation-based features.In all the above experiments, the features usedare based on the union of word lists created bythese two techniques.
We perform three experi-ments.
Beginning with the system that contains allfeatures (SMALL=73.9, LARGE=76.8), we first re-move the bootstrap-based features (SMALL?71.8,LARGE?75.5) or remove the label-propagation-based features (SMALL?71.2, LARGE?74.9) orremove both (SMALL?70.7, LARGE?74.2).
Fromthese results, we can see that these techniques areuseful, but somewhat redundant: if you had tochoose one, you should choose label-propagation.5 DiscussionAs connections between language and vision be-come stronger, for instance in the contexts of ob-ject detection (Hou and Zhang, 2007; Kim and Tor-ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;Gu et al, 2009), attribute detection (Ferrari and Zis-serman, 2007; Farhadi et al, 2009; Kumar et al,2009; Berg et al, 2010), visual phrases (Farhadi andSadeghi, 2011), and automatic caption generation(Farhadi et al, 2010; Feng and Lapata, 2010; Or-donez et al, 2011; Kulkarni et al, 2011; Yang etal., 2011; Li et al, 2011; Mitchell et al, 2012), itbecomes increasingly important to understand, andto be able to detect, text that actually refers to ob-served phenomena.
Our results suggest that whilethis is a hard problem, it is possible to leverage largetext resources and state-of-the-art computer visionalgorithms to address it with high accuracy.AcknowledgmentsT.L.
Berg and K. Yamaguchi were supported in partby NSF Faculty Early Career Development (CA-REER) Award #1054133; A.C. Berg and Y. Choiwere partially supported by the Stony Brook Uni-versity Office of the Vice President for Research; H.Daume?
III and A. Goyal were partially supported byNSF Award IIS-1139909; all authors were partiallysupported by a 2011 JHU Summer Workshop.ReferencesB.
Alexe, T. Deselaers, and V. Ferrari.
2010.
What is anobject?
In Computer Vision and Pattern Recognition(CVPR), 2010 IEEE Conference on, pages 73 ?80.A.
Almuhareb and M. Poesio.
2005.
Finding concept at-tributes in the web.
In Corpus Linguistics Conference.Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.2010.
Automatic attribute discovery and characteriza-tion from noisy web data.
In European Conference onComputer Vision (ECCV).Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
In Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.K.
Church and P. Hanks.
1989.
Word Associa-tion Norms, Mutual Information and Lexicography.In Proceedings of ACL, pages 76?83, Vancouver,Canada, June.N.
Dalal and B. Triggs.
2005.
Histograms of orientedgradients for human detection.
In CVPR.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin meth-ods for structured prediction.
In Proceedings of the In-ternational Conference on Machine Learning (ICML).Hal Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper availableat http://pub.hal3.name/#daume04cg-bfgs, implementationavailable at http://hal3.name/megam/, August.770M.
Everingham, L. Van Gool, C. K. I. Williams,J.
Winn, and A. Zisserman.
2010.
The PASCALVisual Object Classes Challenge 2010 (VOC2010)Results.
http://www.pascal-network.org/challenges/VOC/voc2010/workshop/index.html.Ali Farhadi and Amin Sadeghi.
2011.
Recognition us-ing visual phrases.
In Computer Vision and PatternRecognition (CVPR).A.
Farhadi, I. Endres, D. Hoiem, and D.A.
Forsyth.
2009.Describing objects by their attributes.
In ComputerVision and Pattern Recognition (CVPR).A.
Farhadi, M. Hejrati, M.A.
Sadeghi, P. Young,C.
Rashtchian1, J. Hockenmaier, and D.A.
Forsyth.2010.
Every picture tells a story: Generating sentencesfrom images.
In ECCV.P.
F. Felzenszwalb, R. B. Girshick, and D. McAllester.2010.
Discriminatively trained deformable partmodels, release 4. http://people.cs.uchicago.edu/?pff/latent-release4/.Y.
Feng and M. Lapata.
2010.
How many words is apicture worth?
automatic caption generation for newsimages.
In ACL.V.
Ferrari and A. Zisserman.
2007.
Learning visual at-tributes.
In Advances in Neural Information Process-ing Systems (NIPS).D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik.
2009.Recognition using regions.
In Computer Vision andPattern Recognition, 2009.
CVPR 2009.
IEEE Confer-ence on, pages 1030 ?1037.Derek Hoiem, Alexei A. Efros, and Martial Hebert.2005.
Geometric context from a single image.
InICCV.Xiaodi Hou and Liqing Zhang.
2007.
Saliency detection:A spectral residual approach.
In Computer Vision andPattern Recognition, 2007.
CVPR ?07.
IEEE Confer-ence on, pages 1 ?8.P.
Ipeirotis, F. Provost, and J. Wang.
2010.
Quality man-agement on amazon mechanical turk.
In Proceedingsof the Second Human Computation Workshop (KDD-HCOMP).Gunhee Kim and Antonio Torralba.
2009.
UnsupervisedDetection of Regions of Interest using Iterative LinkAnalysis.
In Annual Conference on Neural Informa-tion Processing Systems (NIPS 2009).Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008.Semantic class learning from the web with hyponympattern linkage graphs.
In Proceedings of ACL-08:HLT, pages 1048?1056, Columbus, Ohio, June.
As-sociation for Computational Linguistics.G.
Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. CBerg, and T. L Berg.
2011.
Babytalk: Understandingand generating simple image descriptions.
In CVPR.N.
Kumar, A.C. Berg, P. Belhumeur, and S.K.
Nayar.2009.
Attribute and simile classifiers for face verifi-cation.
In ICCV.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composing sim-ple image descriptions using web-scale n-grams.
InCONLL.Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei.
2010.
Ob-ject bank: A high-level image representation for sceneclassification and semantic feature sparsification.
InNIPS.Tara McIntosh and James R Curran.
2008.
Weightedmutual exclusion bootstrapping for domain indepen-dent lexicon and template acquisition.
In Proceedingsof the Australasian Language Technology AssociationWorkshop 2008, pages 97?105, December.K.J.
Miller.
1998.
Modifiers in WordNet.
In C. Fell-baum, editor, WordNet, chapter 2.
MIT Press.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,Alex Berg, Tamara Berg, and Hal Daume?
III.
2012.Midge: Generating image descriptions from computervision detections.
Proceedings of EACL 2012.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2Text: Describing Images Using 1 MillionCaptioned Photographs.
In NIPS.Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-lia Hockenmaier.
2010.
Collecting image annotationsusing amazon?s mechanical turk.
In Proceedings ofthe NAACL HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk.Association for Computational Linguistics.H.
Schmid.
1995.
Improvements in part?of?speech tag-ging with an application to german.
In Proceedings ofthe EACL SIGDAT Workshop.J.
Sivic, B.C.
Russell, A. Zisserman, W.T.
Freeman, andA.A.
Efros.
2008.
Unsupervised discovery of visualobject class hierarchies.
In Computer Vision and Pat-tern Recognition, 2008.
CVPR 2008.
IEEE Conferenceon, pages 1 ?8.M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pat-tern Contexts.
In Proceedings of the Empirical Meth-ods in Natural Language Processing, pages 214?221.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
Journal of Artificial Intelligence Research (JAIR),37:141.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the North771American Chapter of the Association for Computa-tional Linguistics.
Association for Computational Lin-guistics.J.
Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.2010.
Sun database: Large-scale scene recognitionfrom abbey to zoo.
In CVPR.Yezhou Yang, Ching Lik Teo, Hal Daume?
III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gener-ation of natural images.
In EMNLP.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learn-ing from labeled and unlabeled data with label prop-agation.
In Technical Report CMU-CALD-02-107.CarnegieMellon University.772
