Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514?523,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsJoint Satisfaction of Syntactic and Pragmatic ConstraintsImproves Incremental Spoken Language UnderstandingAndreas PeldszusUniversity of PotsdamDepartment for Linguisticspeldszus@uni-potsdam.deOkko Bu?University of PotsdamDepartment for Linguisticsokko@ling.uni-potsdam.deTimo BaumannUniversity of HamburgDepartment for Informaticsbaumann@informatik.uni-hamburg.deDavid SchlangenUniversity of BielefeldDepartment for Linguisticsdavid.schlangen@uni-bielefeld.deAbstractWe present a model of semantic processingof spoken language that (a) is robust againstill-formed input, such as can be expectedfrom automatic speech recognisers, (b) re-spects both syntactic and pragmatic con-straints in the computation of most likelyinterpretations, (c) uses a principled, ex-pressive semantic representation formalism(RMRS) with a well-defined model the-ory, and (d) works continuously (produc-ing meaning representations on a word-by-word basis, rather than only for fullutterances) and incrementally (computingonly the additional contribution by the newword, rather than re-computing for thewhole utterance-so-far).We show that the joint satisfaction of syn-tactic and pragmatic constraints improvesthe performance of the NLU component(around 10% absolute, over a syntax-onlybaseline).1 IntroductionIncremental processing for spoken dialogue sys-tems (i. e., the processing of user input even whileit still may be extended) has received renewed at-tention recently (Aist et al 2007; Baumann etal., 2009; Bu?
and Schlangen, 2010; Skantze andHjalmarsson, 2010; DeVault et al 2011; Purveret al 2011).
Most of the practical work, how-ever, has so far focussed on realising the poten-tial for generating more responsive system be-haviour through making available processing re-sults earlier (e. g. (Skantze and Schlangen, 2009)),but has otherwise followed a typical pipeline ar-chitecture where processing results are passedonly in one direction towards the next module.In this paper, we investigate whether the otherpotential advantage of incremental processing?providing ?higher-level?-feedback to lower-levelmodules, in order to improve subsequent process-ing of the lower-level module?can be realised aswell.
Specifically, we experimented with giving asyntactic parser feedback about whether semanticreadings of nominal phrases it is in the process ofconstructing have a denotation in the given con-text or not.
Based on the assumption that speak-ers do plan their referring expressions so that theycan successfully refer, we use this information tore-rank derivations; this in turn has an influenceon how the derivations are expanded, given con-tinued input.
As we show in our experiments, fora corpus of realistic dialogue utterances collectedin a Wizard-of-Oz setting, this strategy led to anabsolute improvement in computing the intendeddenotation of around 10% over a baseline (evenmore using a more permissive metric), both formanually transcribed test data as well as for theoutput of automatic speech recognition.The remainder of this paper is structured as fol-lows: We discuss related work in the next section,and then describe in general terms our model andits components.
In Section 4 we then describe thedata resources we used for the experiments andthe actual implementation of the model, the base-lines for comparison, and the results of our exper-iments.
We close with a discussion and an outlookon future work.2 Related WorkThe idea of using real-world reference to informsyntactic structure building has been previouslyexplored by a number of authors.
Stoness et al(2004, 2005) describe a proof-of-concept imple-514mentation of a ?continuous understanding?
mod-ule that uses reference information in guiding abottom-up chart-parser, which is evaluated on asingle dialogue transcript.
In contrast, our modeluses a probabilistic top-down parser with beamsearch (following Roark (2001)) and is evalu-ated on a large number of real-world utterancesas processed by an automatic speech recogniser.Similarly, DeVault and Stone (2003) describe asystem that implements interaction between aparser and higher-level modules (in this case, evenmore principled, trying to prove presuppositions),which however is also only tested on a small, con-structed data-set.Schuler (2003) and Schuler et al(2009) presenta model where information about reference isused directly within the speech recogniser, andhence informs not only syntactic processing butalso word recognition.
To this end, the processingis folded into the decoding step of the ASR, andis realised as a hierarchical HMM.
While techni-cally interesting, this approach is by design non-modular and restricted in its syntactic expressiv-ity.The work presented here also has connectionsto work in psycholinguistics.
Pado?
et al(2009)present a model that combines syntactic and se-mantic models into one plausibility judgementthat is computed incrementally.
However, thatwork is evaluated for its ability to predict readingtime data and not for its accuracy in computingmeaning.3 The Model3.1 OverviewDescribed abstractly, the model computes theprobability of a syntactic derivation (and its ac-companying logical form) as a combination of asyntactic probability (as in a typical PCFG) anda semantic or pragmatic plausibility.1 The prag-matic plausibility here comes from the presuppo-sition that the speaker intended her utterance tosuccessfully refer, i. e. to have a denotation in thecurrent situation (a unique one, in the case of def-inite reference).
Hence, readings that do have adenotation are preferred over those that do not.1Note that, as described below, in the actual implemen-tation the weights given to particular derivations are not realprobabilities anymore, as derivations fall out of the beam andnormalisation is not performed after re-weighting.The components of our model are described inthe following sections: first the parser which com-putes the syntactic probability in an incremental,top-down manner; the semantic construction al-gorithm which associates (underspecified) logi-cal forms to derivations; the reference resolutioncomponent that computes the pragmatic plausi-bility; and the combination that incorporates thefeedback from this pragmatic signal.3.2 ParserRoark (2001) introduces a strategy for incremen-tal probabilistic top-down parsing and shows thatit can compete with high-coverage bottom-upparsers.
One of the reasons he gives for choosinga top-down approach is that it enables fully left-connected derivations, where at every process-ing step new increments directly find their placein the existing structure.
This monotonically en-riched structure can then serve as a context for in-cremental language understanding, as the authorclaims, although this part is not further developedby Roark (2001).
He discusses a battery of dif-ferent techniques for refining his results, mostlybased on grammar transformations and on con-ditioning functions that manipulate a derivationprobability on the basis of local linguistic and lex-ical information.We implemented a basic version of his parserwithout considering additional conditioning orlexicalizations.
However, we applied left-facto-rization to parts of the grammar to delay cer-tain structural decisions as long as possible.
Thesearch-space is reduced by using beam search.
Tomatch the next token, the parser tries to expandthe existing derivations.
These derivations arestored in a priorized queue, which means that themost probable derivation will always be servedfirst.
Derivations resulting from rule expansionsare kept in the current queue, derivations result-ing from a successful lexical match are pushed ina new queue.
The parser proceeds with the nextmost probable derivation until the current queueis empty or until a threshhold is reached at whichremaining analyses are pruned.
This threshholdis determined dynamically: If the probability ofthe current derivation is lower than the product ofthe best derivation?s probability on the new queue,the number of derivations in the new queue, and abase beam factor (an initial parameter for the sizeof the search beam), then all further old deriva-515FormulaIUCandidateAnalysisIUTagIUTextualWordIUFormulaIU[ [l0:a1:i2]{ [l0:a1:i2] } ] FormulaIU[ [l0:a1:e2]{ [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2)]CandidateAnalysisIULD=[s*/s, s/vp, vp/vvimp-v1, m(vvimp)]P=0.49S=[V1, S!]CandidateAnalysisIULD=[]P=1.00S=[S*,S!
]TagIUvvimpFormulaIU...CandidateAnalysisIULD=[s*/s,kon,s*, s/vp, vp/vvimp-v1, m(vvimp)]P=0.14S=[V1, kon, S*, S!
]FormulaIU[ [l0:a1:e2]{ [l18:a19:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),qeq(h21,l18)]CandidateAnalysisIULD=[v1/np-vz, np/det-n1, m(det)]P=0.2205S=[N1, VZ, S!
]TagIUdetFormulaIU...CandidateAnalysisIULD=[v1/np-vz, np/pper, i(det)]P=0.00441S=[pper, VZ, S!
]FormulaIU[ [l0:a1:e2]{ [l29:a30:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),l18:a19:_winkel(x14),qeq(h21,l18)]CandidateAnalysisIULD=[n1/nn-nz, m(nn)]P=0.06615S=[NZ, VZ, S!
]TagIUnnFormulaIU...CandidateAnalysisIULD=[n1/adjp-n1, adjp/adja, i(nn)]P=0.002646S=[adja, N1, VZ, S!
]FormulaIU...CandidateAnalysisIULD=[n1/nadj-nz, nadj/adja, i(nn)]P=0.000441S=[adja, NZ, VZ, S!
]FormulaIU[ [l0:a1:e2]{ [l42:a43:x44] [l29:a30:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),l18:a19:_winkel(x14),ARG1(a40,x14),ARG2(a40,x44),l39:a40:_in(e41),qeq(h21,l18)]CandidateAnalysisIULD=[nz/pp-nz, pp/appr-np, m(appr)]P=0.0178605S=[NP, NZ, VZ, S!
]TagIUapprFormulaIU...CandidateAnalysisIULD=[nz/advp-nz, advp/adv, i(appr)]P=0.0003969S=[adv, NZ, VZ, S!
]FormulaIU...CandidateAnalysisIULD=[nz/eps, vz/advp-vz, advp/adv, i(appr)]P=0.00007938S=[adv, VZ, S!
]TagIU$TopOfTagsTextualWordIUnimm TextualWordIUden TextualWordIUwinkel TextualWordIUinTextualWordIU$TopOfWordsFigure 1: An example network of incremental units, including the levels of words, POS-tags, syntactic derivationsand logical forms.
See section 3 for a more detailed description.tions are pruned.
Due to probabilistic weighingand the left factorization of the rules, left recur-sion poses no direct threat in such an approach.Additionally, we implemented three robust lex-ical operations: insertions consume the currenttoken without matching it to the top stack item;deletions can ?consume?
a requested but actu-ally non-existent token; repairs adjust unknowntokens to the requested token.
These robust op-erations have strong penalties on the probabilityto make sure they will survive in the derivationonly in critical situations.
Additionally, only asingle one of them is allowed to occur betweenthe recognition of two adjacent input tokens.Figure 1 illustrates this process for the first fewwords of the example sentence ?nimm den winkelin der dritten reihe?
(take the bracket in the thirdrow), using the incremental unit (IU) model torepresent increments and how they are linked; see(Schlangen and Skantze, 2009).2 Here, syntactic2Very briefly: rounded boxes in the Figures representIUs, and dashed arrows link an IU to its predecessor on thesame level, where the levels correspond to processing stages.The Figure shows the levels of input words, POS-tags, syn-tactic derivations and logical forms.
Multiple IUs sharingderivations (?CandidateAnalysisIUs?)
are repre-sented by three features: a list of the last parser ac-tions of the derivation (LD), with rule expansionsor (robust) lexical matches; the derivation proba-bility (P); and the remaining stack (S), where S*is the grammar?s start symbol and S!
an explicitend-of-input marker.
(To keep the Figure small,we artificially reduced the beam size and cut offalternatives paths, shown in grey.
)3.3 Semantic Construction Using RMRSAs a novel feature, we use for the representationof meaning increments (that is, the contributionsof new words and syntactic constructions) as wellas for the resulting logical forms the formalismRobust Minimal Recursion Semantics (Copestake,2006).
This is a representation formalism that wasoriginally constructed for semantic underspecifi-cation (of scope and other phenomena) and thenadapted to serve the purposes of semantics repre-the same predecessor can be regarded as alternatives.
Solidarrows indicate which information from a previous level anIU is grounded in (based on); here, every semantic IU isgrounded in a syntactic IU, every syntactic IU in a POS-tag-IU, and so on.516sentations in heterogeneous situations where in-formation from deep and shallow parsers must becombined.
In RMRS, meaning representations ofa first order logic are underspecified in two ways:First, the scope relationships can be underspeci-fied by splitting the formula into a list of elemen-tary predications (EP) which receive a label ` andare explicitly related by stating scope constraintsto hold between them (e.g.
qeq-constraints).
Thisway, all scope readings can be compactly repre-sented.
Second, RMRS allows underspecificationof the predicate-argument-structure of EPs.
Ar-guments are bound to a predicate by anchor vari-ables a, expressed in the form of an argument re-lation ARGREL(a,x).
This way, predicates canbe introduced without fixed arity and argumentscan be introduced without knowing which predi-cates they are arguments of.
We will make use ofthis second form of underspecification and enrichlexical predicates with arguments incrementally.Combining two RMRS structures involves atleast joining their list of EPs and ARGRELs andof scope constraints.
Additionally, equations be-tween the variables can connect two structures,which is an essential requirement for semanticconstruction.
A semantic algebra for the combi-nation of RMRSs in a non-lexicalist setting is de-fined in (Copestake, 2007).
Unsaturated semanticincrements have open slots that need to be filledby what is called the hook of another structure.Hook and slot are triples [`:a:x] consisting of alabel, an anchor and an index variable.
Every vari-able of the hook is equated with the correspondingone in the slot.
This way the semantic representa-tion can grow monotonically at each combinatorystep by simply adding predicates, constraints andequations.Our approach differs from (Copestake, 2007)only in the organisation of the slots: In an incre-mental setting, a proper semantic representationis desired for every single state of growth of thesyntactic tree.
Typically, RMRS composition as-sumes that the order of semantic combination isparallel to a bottom-up traversal of the syntactictree.
Yet, this would require for every incrementalstep first to calculate an adequate underspecifiedsemantic representation for the projected nodeson the lower right border of the tree and then toproceed with the combination not only of the newsemantic increments but of the complete tree.
Forour purposes, it is more elegant to proceed withsemantic combination in synchronisation with thesyntactic expansion of the tree, i.e.
in a top-downleft-to-right fashion.
This way, no underspecifica-tion of projected nodes and no re-interpretation ofalready existing parts of the tree is required.
This,however, requires adjustments to the slot structureof RMRS.
Left-recursive rules can introduce mul-tiple slots of the same sort before they are filled,which is not allowed in the classic (R)MRS se-mantic algebra, where only one named slot ofeach sort can be open at a time.
We thus organizethe slots as a stack of unnamed slots, where mul-tiple slots of the same sort can be stored, but onlythe one on top can be accessed.
We then definea basic combination operation equivalent to for-ward function composition (as in standard lambdacalculus, or in CCG (Steedman, 2000)) and com-bine substructures in a principled way across mul-tiple syntactic rules without the need to representslot names.Each lexical items receives a generic represen-tation derived from its lemma and the basic se-mantic type (individual, event, or underspecifieddenotations), determined by its POS tag.
Thismakes the grammar independent of knowledgeabout what later (semantic) components will ac-tually be able to process (?understand?
).3 Parallelto the production of syntactic derivations, as thetree is expanded top-down left-to-right, seman-tic macros are activated for each syntactic rule,composing the contribution of the new increment.This allows for a monotonic semantics construc-tion process that proceeds in lockstep with thesyntactic analysis.Figure 1 (in the ?FormulaIU?
box) illustratesthe results of this process for our example deriva-tion.
Again, alternatives paths have been cut tokeep the size of the illustration small.
Notice that,apart from the end-of-input marker, the stack ofsemantic slots (in curly brackets) is always syn-chronized with the parser?s stack.3.4 Computing Noun Phrase DenotationsFormally, the task of this module is, given a modelM of the current context, to compute the set ofall variable assignments such that M satisfies ?
:G = {g | M |=g ?}.
If |G| > 1, we say that ?refers ambiguously; if |G| = 1, it refers uniquely;3This feature is not used in the work presented here, butit could be used for enabling the system to learn the meaningof unknown words.517and if |G| = 0, it fails to refer.
This process doesnot work directly on RMRS formulae, but on ex-tracted and unscoped first-order representations oftheir nominal content.3.5 Parse Pruning Using ReferenceInformationAfter all possible syntactic hypotheses at an in-crement have been derived by the parser andthe corresponding semantic representations havebeen constructed, reference resolution informa-tion can be used to re-rank the derivations.
Ifpragmatic feedback is enabled, the probability ofevery reprentation that does not resolve in the cur-rent context is degraded by a constant factor (weused 0.001 in our experiments described below,determined by experimentation).
The degradationthus changes the derivation order in the parsingqueue for the next input item and increases thechances of degraded derivations to be pruned inthe following parsing step.4 Experiments and Results4.1 DataWe use data from the Pentomino puzzle piece do-main (which has been used before for exampleby (Ferna?ndez and Schlangen, 2007; Schlangen etal., 2009)), collected in a Wizard-of-Oz study.
Inthis specific setting, users gave instructions to thesystem (the wizard) in order to manipulate (select,rotate, mirror, delete) puzzle pieces on an upperboard and to put them onto a lower board, reach-ing a pre-specified goal state.
Figure 2 shows anexample configuration.
Each participant took partin several rounds in which the distinguishing char-acteristics for puzzle pieces (color, shape, pro-posed name, position on the board) varied widely.In total, 20 participants played 284 games.We extracted the semantics of an utterancefrom the wizard?s response action.
In some cases,such a mapping was not possible to do (e. g. be-cause the wizard did not perform a next action,mimicking a non-understanding by the system),or potentially unreliable (if the wizard performedseveral actions at or around the end of the utter-ance).
We discarded utterances without a clear se-mantics alignment, leaving 1687 semantically an-notated user utterances.
The wizard of course wasable to use her model of the previous discourse forresolving references, including anaphoric ones; asFigure 2: The game board used in the study, as pre-sented to the player: (a) the current state of the gameon the left, (b) the goal state to be reached on the right.our study does not focus on these, we have dis-regarded another 661 utterances in which piecesare referred to by pronouns, leaving us with 1026utterances for evaluation.
These utterances con-tained on average 5.2 words (median 5 words;std dev 2 words).In order to test the robustness of our method,we generated speech recognition output using anacoustic model trained for spontaneous (German)speech.
We used leave-one-out language modeltraining, i. e. we trained a language model for ev-ery utterance to be recognized which was basedon all the other utterances in the corpus.
Unfor-tunately, the audio recordings of the first record-ing day were too quiet for successful recognition(with a deletion rate of 14%).
We thus decidedto limit the analysis for speech recognition out-put to the remaining 633 utterances from the otherrecording days.
On this part of the corpus worderror rate (WER) was at 18%.The subset of the full corpus that we used forevaluation, with the utterances selected accordingto the criteria described above, nevertheless stillonly consists of natural, spontaneous utterances(with all the syntactic complexity that brings) thatare representative for interactions in this type ofdomain.4.2 Grammar and Resolution ModelThe grammar used in our experiments was hand-constructed, inspired by a cursory inspection ofthe corpus and aiming to reach good coverage518Words Predicates Statusnimm nimm(e) -1nimm den nimm(e,x) def(x) 0nimm den Winkel nimm(e,x) def(x) winkel(x) 0nimm den Winkel in nimm(e,x) def(x) winkel(x) in(x,y) 0nimm den Winkel in der nimm(e,x) def(x) winkel(x) in(x,y) def(y) 0nimm den Winkel in der dritten nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) 1nimm den Winkel in der dritten Reihe nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) row(y) 1Table 1: Example of logical forms (flattened into first-order base-language formulae) and reference resolutionresults for incrementally parsing and resolving ?nimm den winkel in der dritten reihe?for a core fragment.
We created 30 rules, whoseweights were also set by hand (as discussed be-low, this is an obvious area for future improve-ment), sparingly and according to standard intu-itions.
When parsing, the first step is the assign-ment of a POS tag to each word.
This is done bya simple lookup tagger that stores the most fre-quent tag for each word (as determined on a smallsubset of our corpus).4The situation model used in reference resolu-tion is automatically derived from the internalrepresentation of the current game state.
(Thiswas recorded in an XML-format for each utter-ance in our corpus.)
Variable assignments werethen derived from the relevant nominal predicatestructures,5 consisting of extracted simple pred-ications, e. g. red(x) and cross(x) for the NP ina phrase such as ?take the red cross?.
For eachunique predicate argument X in these EP struc-tures (such as as x above), the set of domain ob-jects that satisfied all predicates of which X wasan argument were determined.
For example forthe phrase above, X mapped to all elements thatwere red and crosses.Finally, the size of these sets was determined:no elements, one element, or multiple elements,as described above.
Emptiness of at least one setdenoted that no resolution was possible (for in-stance, if no red crosses were available, x?s setwas empty), uniqueness of all sets denoted thatan exact resolution was possible while multipleelements in at least some sets denoted ambiguity.This status was then leveraged for parse pruning,as per Section 3.5.A more complex example using the scene de-picted in Figure 2 and the sentence ?nimm den4A more sophisticated approach has recently been pro-posed by Beuck et al(2011); this could be used in our setup.5The domain model did not allow making a plausibilityjudgement based on verbal resolution.winkel in der dritten reihe?
(take the bracket in thethird row) is shown in Table 1.
The first columnshows the incremental word hypothesis string, thesecond the set of predicates derived from the mostrecent RMRS representation and the third the res-olution status (-1 for no resolution, 0 for some res-olution and 1 for a unique resolution).4.3 Baselines and Evaluation Metric4.3.1 Variants / BaselinesTo be able to accurately quantify and assess theeffect of our reference-feedback strategy, we im-plemented different variants / baselines.
These alldiffer in how, at each step, the reading is deter-mined that is evaluated against the gold standard,and are described in the following:In the Just Syntax (JS) variant, we simply takesingle-best derivation, as determined by syntaxalone and evaluate this.The External Filtering (EF) variant adds in-formation from reference resolution, but keepsit separate from the parsing process.
Here, welook at the 5 highest ranking derivations (as de-termined by syntax alone), and go through thembeginning at the highest ranked, picking the firstderivation where reference resolution can be per-formed uniquely; this reading is then put up forevaluation.
If there is no such reading, the highestranking one will be put forward for evaluation (asin JS).Syntax/Pragmatics Interaction (SPI) is thevariant described in the previous section.
Here,all active derivations are sent to the reference res-olution module, and are re-weighted as describedabove; after this has been done, the highest-ranking reading is evaluated.Finally, the Combined Interaction and Fil-tering (CIF) variant combines the previous twostrategies, by using reference-feedback in com-puting the ranking for the derivations, and then519again using reference-information to identify themost promising reading within the set of 5 highestranking ones.4.3.2 MetricWhen a reading has been identified accordingto one of these methods, a score s is computed asfollows: s = 1, if the correct referent (accordingto the gold standard) is computed as the denota-tion for this reading; s = 0 if no unique referentcan be computed, but the correct one is part of theset of possible referents; s = ?1 if no referentcan be computed at all, or the correct one is notpart of the set of those that are computed.As this is done incrementally for each word(adding the new word to the parser chart), for anutterance of length m we get a sequence of msuch numbers.
(In our experiments we treat the?end of utterance?
signal as a pseudo-word, sinceknowing that an utterance has concluded allowsthe parser to close off derivations and removethose that are still requiring elements.
Hence, wein fact have sequences ofm+1 numbers.)
A com-bined score for the whole utterance is computedaccording to the following formula:su =m?n=1(sn ?
n/m)(where sn is the score at position n).
The fac-tor n/m causes ?later?
decisions to count moretowards the final score, reflecting the idea thatit is more to be expected (and less harmful) tobe wrong early on in the utterance, whereas thelonger the utterance goes on, the more pressingit becomes to get a correct result (and the moredamaging if mistakes are made).6Note that this score is not normalised by utter-ance length m; the maximally achievable scorebeing (m + 1)/2.
This has the additional ef-fect of increasing the weight of long utteranceswhen averaging over the score of all utterances;we see this as desirable, as the analysis task be-comes harder the longer the utterance is.We use success in resolving reference to eval-uate the performance of our parsing and semanticconstruction component, where more tradition-ally, metrics like parse bracketing accuracy might6This metric compresses into a single number some ofthe concerns of the incremental metrics developed in (Bau-mann et al 2011), which can express more fine-grainedlythe temporal development of hypotheses.be used.
But as we are building this module for aninteractive system, ultimately, accuracy in recov-ering meaning is what we are interested in, and sowe see this not just as a proxy, but actually as amore valuable metric.
Moreover, this metric canbe applied at each incremental step, which is notclear how to do with more traditional metrics.4.4 ExperimentsOur parser, semantic construction and referenceresolution modules are implemented within theInproTK toolkit for incremental spoken dialoguesystems development (Schlangen et al 2010).
Inthis toolkit, incremental hypotheses are modifiedas more information becomes available over time.Our modules support all such modifications (i. e.also allow to revert their states and output if wordinput is revoked).As explained in Section 4.1, we used offlinerecognition results in our evaluation.
However,the results would be identical if we were to usethe incremental speech recognition output of In-proTK directly.The system performs several times faster thanreal-time on a standard workstation computer.
Wethus consider it ready to improve practical end-to-end incremental systems which perform within-turn actions such as those outlined in (Bu?
andSchlangen, 2010).The parser was run with a base-beam factor of0.01; this parameter may need to be adjusted if alarger grammar was used.4.5 ResultsTable 2 shows an overview of the experiment re-sults.
The table lists, separately for the manualtranscriptions and the ASR transcripts, first thenumber of times that the final reading did not re-solve at all, or to a wrong entitiy; did not uniquelyresolve, but included the correct entity in its de-notiation; or did uniquely resolve to the correctentity (-1, 0, and 1, respectively).
The next linesshow ?strict accuracy?
(proportion of ?1?
amongall results) at the end of utterance, and ?relaxedaccuracy?
(which allows ambiguity, i.e., is the set{0, 1}).
incr.scr is the incremental score as de-scribed above, which includes in the evaluationthe development of references and not just the fi-nal state.
(And in that sense, is the most appro-priate metric here, as it captures the incrementalbehaviour.)
This score is shown both as absolute520JS EF SPI CIFtranscript?1 563 518 364 3630 197 198 267 2681 264 308 392 392str.acc.
25.7% 30.0% 38.2% 38.2%rel.acc.
44.9% 49.3% 64.2% 64.3%incr.scr ?1568 ?1248 ?536 ?504avg.incr.scr ?1.52 ?1.22 ?0.52 ?0.49recogntion?1 362 348 254 2550 122 121 173 1731 143 158 196 195str.acc.
22.6% 25.0% 31.0% 30.8%rel.acc.
41.2% 44.1% 58.3% 58.1%incr.scr ?1906 ?1730 ?1105 ?1076avg.incr.scr ?1.86 ?1.69 ?1.01 ?1.05Table 2: Results of the Experiments.
See text for explanation of metrics.number as well as averaged for each utterance.As these results show, the strategy of provid-ing the parser with feedback about the real-worldutility of constructed phrases (in the form of refer-ence decisions) improves the parser, in the sensethat it helps the parser to successfully retrieve theintended meaning more often compared to an ap-proach that only uses syntactic information (JS)or that uses pragmatic information only outsideof the main programme: 38.2% strict or 64.2%relaxed for SPI over 25.7% / 44.9% for JS, anabsolute improvement of 12.5% for strict or evenmore, 19.3%, for the relaxed metric; the incre-mental metric shows that this advantage holds notonly at the final word, but also consistently withinthe utterance, the average incremental score foran utterance being ?0.49 for SPI and ?1.52for JS.
The improvement is somewhat smalleragainst the variant that uses some reference infor-mation, but does not integrate this into the parsingprocess (EF), but it is still consistently present.Adding such n-best-list processing to the outputof the parser+reference-combination (as variantCIF does) finally does not further improve theperformance noticeably.
When processing par-tially defective material (the output of the speechrecogniser), the difference between the variantsis maintained, showing a clear advantage of SPI,although performance of all variants is degradedsomewhat.Clearly, accuracy is rather low for the base-line condition (JS); this is due to the large num-ber of non-standard constructions in our sponta-neous material (e.g., utterances like ?lo?schen, un-ten?
(delete, bottom) which we did not try to coverwith syntactic rules, and which may not even con-tain NPs.
The SPI condition can promote deriva-tions resulting from robust rules (here, deletion)which then can refer.
In general though state-of-the art grammar engineering may narrow the gapbetween JS and SPI ?
this remains to be tested ?but we see as an advantage of our approach thatit can improve over the (easy-to-engineer) set ofcore grammar rules.5 ConclusionsWe have described a model of semantic process-ing of natural, spontaneous speech that strivesto jointly satisfy syntactic and pragmatic con-straints (the latter being approximated by the as-sumption that referring expressions are intendedto indeed successfully refer in the given context).The model is robust, accepting also input of thekind that can be expected from automatic speechrecognisers, and incremental, that is, can be fedinput on a word-by-word basis, computing at eachincrement only exactly the contribution of the newword.
Lastly, as another novel contribution, themodel makes use of a principled formalism for se-mantic representation, RMRS (Copestake, 2006).While the results show that our approach ofcombining syntactic and pragmatic informationcan work in a real-world setting on realisticdata?previous work in this direction has so far521only been at the proof-of-concept stage?there ismuch room for improvement.
First, we are nowexploring ways of bootstrapping a grammar andderivation weights from hand-corrected parses.Secondly, we are looking at making the variableassignment / model checking function probabilis-tic, assigning probabilities (degree of strength ofbelief) to candidate resolutions (as for examplethe model of Schlangen et al(2009) does).
An-other next step?which will be very easy to take,given the modular nature of the implementationframework that we have used?will be to integratethis component into an interactive end-to-end sys-tem, and testing other domains in the process.Acknowledgements We thank the anonymousreviewers for their helpful comments.
The workreported here was supported by a DFG grant inthe Emmy Noether programme to the last authorand a stipend from DFG-CRC (SFB) 632 to thefirst author.ReferencesGregory Aist, James Allen, Ellen Campana, Car-los Gomez Gallo, Scott Stoness, Mary Swift, andMichael K. Tanenhaus.
2007.
Incremental under-standing in human-computer dialogue and experi-mental evidence for advantages over nonincremen-tal methods.
In Proceedings of Decalog 2007, the11th International Workshop on the Semantics andPragmatics of Dialogue, Trento, Italy.Timo Baumann, Michaela Atterer, and DavidSchlangen.
2009.
Assessing and improving the per-formance of speech recognition for incremental sys-tems.
In Proceedings of the North American Chap-ter of the Association for Computational Linguis-tics - Human Language Technologies (NAACL HLT)2009 Conference, Boulder, Colorado, USA, May.Timo Baumann, Okko Bu?, and David Schlangen.2011.
Evaluation and optimization of incremen-tal processors.
Dialogue and Discourse, 2(1):113?141.Niels Beuck, Arne Ko?hn, and Wolfgang Menzel.2011.
Decision strategies for incremental pos tag-ging.
In Proceedings of the 18th Nordic Con-ference of Computational Linguistics, NODALIDA-2011, Riga, Latvia.Okko Bu?
and David Schlangen.
2010.
Modellingsub-utterance phenomena in spoken dialogue sys-tems.
In Proceedings of the 14th InternationalWorkshop on the Semantics and Pragmatics of Dia-logue (Pozdial 2010), pages 33?41, Poznan, Poland,June.Ann Copestake.
2006.
Robust minimal recursion se-mantics.
Technical report, Cambridge ComputerLab.
Unpublished draft.Ann Copestake.
2007.
Semantic composition with(robust) minimal recursion semantics.
In Proceed-ings of the Workshop on Deep Linguistic Process-ing, DeepLP ?07, pages 73?80, Stroudsburg, PA,USA.
Association for Computational Linguistics.David DeVault and Matthew Stone.
2003.
Domaininference in incremental interpretation.
In Proceed-ings of ICOS 4: Workshop on Inference in Compu-tational Semantics, Nancy, France, September.
IN-RIA Lorraine.David DeVault, Kenji Sagae, and David Traum.
2011.Incremental Interpretation and Prediction of Utter-ance Meaning for Interactive Dialogue.
Dialogueand Discourse, 2(1):143?170.Raquel Ferna?ndez and David Schlangen.
2007.
Re-ferring under restricted interactivity conditions.
InSimon Keizer, Harry Bunt, and Tim Paek, editors,Proceedings of the 8th SIGdial Workshop on Dis-course and Dialogue, pages 136?139, Antwerp,Belgium, September.Ulrike Pado?, Matthew W Crocker, and Frank Keller.2009.
A probabilistic model of semantic plausi-bility in sentence processing.
Cognitive Science,33(5):794?838.Matthew Purver, Arash Eshghi, and Julian Hough.2011.
Incremental semantic construction in a di-alogue system.
In J. Bos and S. Pulman, editors,Proceedings of the 9th International Conference onComputational Semantics (IWCS), pages 365?369,Oxford, UK, January.Brian Roark.
2001.
Robust Probabilistic PredictiveSyntactic Processing: Motivations, Models, andApplications.
Ph.D. thesis, Department of Cogni-tive and Linguistic Sciences, Brown University.David Schlangen and Gabriel Skantze.
2009.
A gen-eral, abstract model of incremental dialogue pro-cessing.
In EACL ?09: Proceedings of the 12thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 710?718.Association for Computational Linguistics, mar.David Schlangen, Timo Baumann, and Michaela At-terer.
2009.
Incremental reference resolution: Thetask, metrics for evaluation, and a bayesian filteringmodel that is sensitive to disfluencies.
In Proceed-ings of SIGdial 2009, the 10th Annual SIGDIALMeeting on Discourse and Dialogue, London, UK,September.David Schlangen, Timo Baumann, HendrikBuschmeier, Okko Bu?, Stefan Kopp, GabrielSkantze, and Ramin Yaghoubzadeh.
2010.
Middle-ware for Incremental Processing in ConversationalAgents.
In Proceedings of SigDial 2010, Tokyo,Japan, September.522William Schuler, Stephen Wu, and Lane Schwartz.2009.
A framework for fast incremental interpre-tation during speech decoding.
Computational Lin-guistics, 35(3).William Schuler.
2003.
Using model-theoretic se-mantic interpretation to guide statistical parsing andword recognition in a spoken language interface.
InProceedings of the 41st Meeting of the Associationfor Computational Linguistics (ACL 2003), Sap-poro, Japan.
Association for Computational Lin-guistics.Gabriel Skantze and Anna Hjalmarsson.
2010.
To-wards incremental speech generation in dialoguesystems.
In Proceedings of the SIGdial 2010 Con-ference, pages 1?8, Tokyo, Japan, September.Gabriel Skantze and David Schlangen.
2009.
Incre-mental dialogue processing in a micro-domain.
InProceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL 2009), pages 745?753, Athens,Greece, March.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, Massachusetts.Scott C. Stoness, Joel Tetreault, and James Allen.2004.
Incremental parsing with reference inter-action.
In Proceedings of the Workshop on In-cremental Parsing at the ACL 2004, pages 18?25,Barcelona, Spain, July.Scott C. Stoness, James Allen, Greg Aist, and MarySwift.
2005.
Using real-world reference to improvespoken language understanding.
In AAAI Workshopon Spoken Language Understanding, pages 38?45.523
