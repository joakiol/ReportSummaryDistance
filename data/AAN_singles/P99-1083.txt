Modeling Filled Pauses in Medical DictationsSerge)' V.. PakhomovUniversity of Minnesota190 Klaeber Court320-16 th Ave. S.E Minneapolis, MN 55455pakh0002@tc.umn.eduAbstractFilled pauses are characteristic ofspontaneous speech and can presentconsiderable problems for speechrecognition by being often recognized asshort words.
An um can be recognized asthumb or arm if the recognizer's languagemodel does not adequately represent FP's.Recognition of quasi-spontaneous speech(medical dictation) is subject o this problemas well.
Results from medical dictations by21 family practice physicians show thatusing an FP model trained on the corpuspopulated with FP's produces overall betterresults than a model trained on a corpus thatexcluded FP's or a corpus that had randomFP's.IntroductionFilled pauses (FP's), false starts, repetitions,fragments, etc.
are characteristic ofspontaneous speech and can presentconsiderable problems for speechrecognition.
FP's are often recognized asshort words of similar phonetic quality.
Forexample, an um can be recognized as thumbor arm if the recognizer's language modeldoes not adequately represent FP's.Recognition of quasi-spontaneous speech(medical dictation) is subject to this problemas well.
The FP problem becomesespecially pertinent where the corpora usedto build language models are compiled fromtext with no FP's.
Shriberg (1996) hasshown that representing FP's in a languagemodel helps decrease the model' sperplexity.
She finds that when a FP occursat a major phrase or discourse boundary, theFP itself is the best predictor of thefollowing lexical material; conversely, in anon-boundary context, FP's are predictablefrom the preceding words.
Shriberg (1994)shows that the rate of disfluencies growsexponentially with the length of thesentence, and that FP's occur more often inthe initial position (see also Swerts (1996)).This paper presents a method of usingbigram probabilities for extracting FPdistribution from a corpus of hand-transcribed am.
The resulting bigrammodel is used to populate another Irainingcorpus that originally had no FP's.
Resultsfrom medical dictations by 21 familypractice physicians how that using an FPmodel trained on the corpus populated withFP's produces overall better esults than amodel trained on a corpus that excludedFP's or a corpus that had random FP's.Recognition accuracy improvesproportionately to the frequency of FP's inthe speech.1.
Filled PausesFP's are not random events, but have asystematic distribution and well-definedfunctions in discourse.
(Shriberg andStolcke 1996, Shriberg 1994, Swerts 1996,Macalay and Osgood 1959, Cook 1970,Cook and Lalljee 1970, Christenfeld, et al1991) Cook and Lalljee (1970) make aninteresting proposal that FP's may havesomething to do with the listener'sperception of disfluent speech.
Theysuggest that speech may be more619comprehensible when it contains fillermaterial during hesitations by preservingcontinuity and that a FP may serve as asignal to draw the listeners attention to thenext utterance in order for the listener not tolose the onset of the following utterance.Perhaps, from the point of view ofperception, FP's are not disfluent events atall.
This proposal bears directly on thedomain of medical dictations, since manydoctors who use old voice operatedequipment rain themselves to use FP'sinstead of silent pauses, so that the recorderwouldn't cut off the beginning of the postpause utterance.2.
Quasi-spontaneous speechFamily practice medical dictations tend to bepre-planned and follow an establishedSOAP format: (Subjective (informalobservations), Objective (examination),Assessment (diagnosis) and Plan (treatmentplan)).
Despite that, doctors vary greatly inhow frequently they use FP's, which agreeswith Cook and Lalljee's (1970) findings ofno correlation between FP use and the modeof discourse.
Audience awareness may alsoplay a role in variability.
My observationsprovide multiple examples where thedoctors address the transcriptionists directlyby making editing comments and thankingthem.3.
Training Corpora and FPModelThis study used three base and two derivedcorpora Base corpora represent hreedifferent sets of dictations described insection 3.1.
Derived corpora are variationson the base corpora conditioned in severaldifferent ways described in section 3.2.3.1 BaseBalanced FP training corpus (BFP-CORPUS) that has 75, 887 words ofword-by-word transcription data evenlydistributed between 16 talkers.
This3.2corpus was used to build a BIGRAM-FP-LM which controls the process ofpopulating a no-FP corpus with artificialFP's.Unbalanced FP training corpus (UFP-CORPUS) of approximately 500,000words of all available word-by-wordtranscription data from approximately20 talkers.
This corpus was used only tocalculate average frequency of FP useamong all available talkers.Finished transcriptions corpus (FT-CORPUS) of 12,978,707 wordscontains all available dictations and noFP's.
It represents over 200 talkers ofmixed gender and professional status.The corpus contains no FP's or anyother types of disfluencies such asrepetitions, repairs and false starts.
Thelanguage in this corpus is also edited forgrammar.DerivedCONTROLLED-FP-CORPUS is aversion of the finished transcriptionscorpus populated stochastically with2,665,000 FP's based on the BIGRAM-FP-LM.RANDOM-FP-CORPUS- 1 (normaldensity) is another version of thefinished transcriptions corpus populatedwith 916,114 FP's where the insertionpoint was selected at random in therange between 0 and 29.
The randomfunction is based on the averagefrequency of FPs in the unbalancedUFP-CORPUS where an FP occurs onthe average after every 15 th word.Another RANDOM-FP-CORPUS-2(high density) was used to approximatethe frequency of FP's in theCONTROLLED-FP-CORPUS.6204.
ModelsThe language modeling process in this studywas conducted in two stages.
First, a bigrammodel containing bigram probabilities ofFP's in the balanced BFP-COPRUS wasbuilt followed by four different trigramlanguage models, some of which usedcorpora generated with the BIGRAM-FP-LM built during the first stage.4.1 Bigram FP modelThis model contains the distribution of  FP'sobtained by using the following formulas:P(FPIwi-O = Cw-i Fp/Cw-iP(FPIwH) = CFp w+l/Cw+lThus, each word in a corpus to be populatedwith FP's becomes a potential landing sitefor a FP and does or does not receive onebased on the probability found in theBIGRAM-FP-LM.4.2 Trigram modelsThe following trigram models were builtusing ECRL's Transcriber languagemodeling tools (Valtchev, et al 1998).
Bothbigram and trigram cutoffs were set to 3.?
NOFP-LM was built using the FT-CORPUS with no FP's.?
ALLFP-LM was built entirely onCONTROLLED-FP-CORPUS.?
ADAPTFP-LM was built byinterpolating ALLFP-LM and NOFP-LM at 90/10 ratio.
Here 90 % of theresulting ADAPTFP-LM represents theCONTROLLED-FP-CORPUS and 10%represents FT-CORPUS.?
RANDOMFP-LM-1 (normal density)was built entirely on the RANDOM-FP-CORPUS-1.= RANDOMFP-LM-2 (high density) wasbuilt entirely on the RANDOM-FP-CORPUS-25.
Testing DataTesting data comes from 21 talkers selectedat random and represents 3 (1-3 min)dictations for each talker.
The talkers are arandom mix of male and female medicaldoctors and practitioners who vary greatly intheir use of FP's.
Some use literally no FP's(but long silences instead), others use FP'salmost every other word.
Based on thefrequency of FP use, the talkers wereroughly split into a high FP user and low FPuser groups.
The relevance of such divisionwill become apparent during the discussionof test results.6.
AdaptationTest results for ALLFP-LM (63.01% avg.word accuracy) suggest that the model overrepresents FP's.
The recognition accuracyfor this model is 4.21 points higher than thatof the NOFP-LM (58.8% avg.
wordaccuracy) but lower than that of both theRANDOMFP-LM-1 (67.99% avg.
wordaccuracy) by about 5% and RANDOMFP-LM-2 (65.87% avg.
word accuracy) byabout 7%.
One way of decreasing the FPrepresentation is to correct the BIGRAM-FP-LM, which proves to be computationallyexpensive because of having to rebuild thelarge training corpus with each change inBIGRAM-FP-LM.
Another method is tobuild a NOFP-LM and an ALLFP-LM onceand experiment with their relative weightsthrough adaptation.
I chose the secondmethod because ECRL Transcriber toolkitprovides an adaptation tool that achieves thegoals of the first method much faster.
Theresults show that introducing a NOFP-LMinto the equation improves recognition.
Thedifference in recognition accuracy betweenthe ALLFP-LM and ADAPTFP-LM is onaverage 4.9% across all talkers inADAPTFP-LM's favor.
Separating thetalkers into high FP user group and low FPuser group raises ADAPTFP-LM's gain to6.2% for high FP users and lowers it to 3.3%621for low FP users.
This shows thatadaptation to no-FP data is, counter-intuitively more beneficial for high FP users.7.
Results and discussionAlthough a perplexity test provides a goodtheoretical measure of  a language model, itis not always accurate in predicting themodel's performance in a recognizer (Chen1998); therefore, both perplexity andrecognition accuracy were used in thisstudy.
Both were calculated using ECRL'sLM Transcriber tools.7.1 PerplexityPerplexity tests were conducted withECRL's LPlex tool based on the same textcorpus (BFP-CORPUS) that was used tobuild the BIGRAM-FP-LM.
Threeconditions were used.
Condition A used thewhole corpus.
Condition B used a subset ofthe corpus that contained high frequency FPusers (FPs/Words ratio above 1.0).Condition C used the remaining subsetcontaining data from lower frequency FPusers (FPs/Words ratio below 1.0).
Table 1summarizes the results of  perplexity tests at3-gram level for the models under the threeconditions..... , : Lp~ Lplex.
: :: i OOV: ~.
: Lp l~:NOFP~LIV, ::, = ,,: ,: 617.59 6.35 1618.35 6.08 287.46ADAVT~.
M ........ i .
;'L = 132.74 6.35 ::: ..... 6.08 ' ~:13L70 : ....: ~DOMFP~LM~.
: 138.02 6.3_5 ~ 6.08 125,79i ,R.ANDOMFP~2 156.09 6.35 152.16 6.08 145.47 6.06980.67 6.35 964.48 6.08 916.53 6.06Table 1.
Perplexity measurementsOOV r~::(%),:,, ,,,,6.066.066.06The perplexity measures in Condition A showover 400 point difference between ADAPTFP-LM and NOFP-LM language models.
The363,08 increase in perplexity for ALLFP-LMmodel corroborates the results discussed inSection 6.
Another interesting result iscontained in the highlighted fields of  Table 1.ADAPTFP-LM based on CONTROLLED-FP-CORPUS has lower perplexity in general.When tested on conditions B and C, ADAPTFP-LM does better on frequent FP users, whereasRANDOMFP-LM-?
does better on infrequentFP users, which is consistent with therecognition accuracy results for the two models(see Table 2).7.2 Recognition accuracyRecognition accuracy was obtained withECRL's HResults tool and is summarized inTable 2.::~.
~,::,~: 1 5140 %\[ .
.
.
.
.
~ I ~ ~ / )  ~ :::l 66.57 %\ [ ~  ii: ~ii~!
iiiiiii!
!iiiiiii!i ii\]67.14%Table 2.
Recognition accuracy tests for LM's.
!A~ !
i ~ ~ )  i:~i~::.~:i.
~i!~i I67.76%71.46 %69.23 %71.24%The results in Table 2 demonstrate twothings.
First, a FP model performs betterthan a clean model that has no FPrepresentation~ Second, a FP model based onpopulating a no-FP training corpus withFP's whose distribution was derived from a622small sample of  speech data performs betterthan the one populated with FP's at randombased solely on the frequency of FP's.
Theresults also show that ADAPTFP-LMperforms lightly better than RANDOMFP-LM-1 on high FP users.
The gain becomesmore pronounced towards the higher end ofthe FP use continuum.
For example, thescores for the top four high FP users are62.07% with RANDOMFP-LM-1 and63.51% with ADAPTFP-LM.
Thisdifference cannot be attributed to the factthat RANDOMFP-LM-1 contains fewerFP's than ADAPTFP-LM.
The wordaccuracy rates for RANDOMFP-LM-2indicate that frequency of FP's in thetraining corpus is not responsible for thedifference in performance between theRANDOM-FP-LM-1 and the ADAPTFP-LM.
The frequency is roughly the same forboth RANDOMFP-CORPUS-2 andCONTROLLED-FP-CORPUS, butRANDOMFP-LM-2 scores are lower thanthose of RANDOMFP-LM-1, which allowsin absence of further evidence to attributethe difference in scores to the pattern of FPdistribution, ot their frequency.ConclusionBased on the results so far, severalconclusions about FP modeling can bemade:1.
Representing FP's in the training dataimproves both the language model'sperplexity and recognition accuracy.2.
It is not absolutely necessary to have acorpus that contains naturally occurringFP's for successful recognition.
FPdistribution can be extrapolated from arelatively small corpus containingnaturally occurring FP's to a largerclean corpus.
This becomes vital insituations where the language model hasto be built from "clean" text such asfinished transcriptions, newspaperarticles, web documents, etc.3.
If one is hard-pressed for handtranscribed ata with natural FP's, a.random population can be used withrelatively good results.FP's are quite common to both quasi-spontaneous monologue andspontaneous dialogue (medicaldictation).Research in progressThe present study leaves a number of issuesto be investigated further:1.
The results for RANDOMFP-LM-1are very close to those ofADAPTFP-LM.
A statistical test isneeded in order to determine if thedifference is significant.2.
A systematic study of the syntactic aswell as discursive contexts in whichFP's are used in medical dictations.This will involve tagging a corpus ofliteral transcriptions for various kinds ofsyntactic and discourse boundaries suchas clause, phrase and theme/rhemeboundaries.
The results of the analysisof the tagged corpus may lead toinvestigating which lexical items may behelpful in identifying syntactic anddiscourse boundaries.
Although FP'smay not always be lexicallyconditioned, lexical information may beuseful in modeling FP's that occur atdiscourse boundaries due to co-occurrence of such boundaries andcertain lexical items.3.
The present study roughly categorizestalkers according to the frequency ofFP's in their speech into high FP usersand low FP users.
A more finely tunedcategorization f talkers in respect to FPuse as well as its usefulness remain to beinvestigated.4.
Another area of investigation will focuson the SOAP structure of medicaldictations.
I plan to look at relativefrequency of FP use in the four parts ofa medical dictation.
Informalobservation of data collected so fa rindicates that FP use is more frequentand different from other parts during the623Subjective part of  a dictation.
This iswhen the doctor uses fewer frozenexpressions and the discourse is closestto a natural conversation.AcknowledgementsI would like to thank Joan Bachenko andMichael Shonwetter, at LinguisticTechnologies, Inc. and Bruce Downing atthe University of  Minnesota for helpfuldiscussions and comments.ReferencesChen, S., Beeferman, Rosenfeld, R.
(1998).
"Evaluation metrics for language models," InDARPA Broadcast News Transcription andUnderstanding Workshop.Christenfeld, N, Schachter, S and Bilous, F.(1991).
"Filled Pauses and Gestures: It's notcoincidence," Journal of PsycholinguisticResearch, Vol.
20(1).Cook, M. (1977).
"The incidence of filled pausesin relation to part of speech," Language andSpeech, Vol.
14, pp.
135-139.Cook, M. and Lalljee, M. (1970).
"Theinterpretation f pauses by the listener," Brit.J.
Soc.
Clin.
Psy.
Vol.
9, pp.
375-376.Cook, M., Smith, J, and Lalljee, M (1977).
"Filled pauses and syntactic omplexity,"Language and Speech, Vol.
17, pp.11-16.Valtchev, V. Kershaw, D. and Odell, J.
1998.The truetalk transcriber book.
EntropicCambridge Research Laboratory, Cambridge,England.Heeman, P.A.
and Loken-Kim, K. and Allen,J.F.
(1996).
"Combining the detection andcorrelation of speech repairs," In Proc.,ICSLP.Lalljee, M and Cook, M. (1974).
"Filled pausesand floor holding: The final test?
"Semiotica, Vol.
12, pp.219-225.Maclay, H, and Osgood, C. (1959).
"Hesitationphenomena in spontaneous speech," Word,Vol.15, pp.
19-44.Shriberg, E. E. (1994).
Preliminaries toa theoryof speech disfluencies.
Ph.D. thesis,University of California t Berkely.Shriberg, E.E and Stolcke, A.
(1996).
"Wordpredictability after hesitations: A corpus-based study,, In Proc.
ICSLP.Shriberg, E.E.
(1996).
"Disfluencies inSwitchboard," InProc.
ICSLP.Shriberg, EE.
Bates, R. and Stolcke, A.
(1997).
"A prosody-only decision-tree model fordisfluency detection" In Proc.EUROSPEECH.Siu, M. and Ostendorf, M. (1996).
"Modelingdisfluencies inconversational speech," Proc.ICSLP.Stolcke, A and Shriberg, E. (1996).
"Statisticallanguage modeling for speech disfluencies,"In Proc.
ICASSP.Swerts, M, Wichmann, A and Beun, R.
(1996).
"Filled pauses as markers of discoursestructure," Proc.
ICSLP.624
