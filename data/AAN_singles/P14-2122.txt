Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752?758,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsEmpirical Study of Unsupervised Chinese Word Segmentation Methodsfor SMT on Large-scale CorporaXiaolin Wang Masao Utiyama Andrew Finch Eiichiro SumitaNational Institute of Information and Communications Technology{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jpAbstractUnsupervised word segmentation (UWS)can provide domain-adaptive segmenta-tion for statistical machine translation(SMT) without annotated data, and bilin-gual UWS can even optimize segmenta-tion for alignment.
Monolingual UWS ap-proaches of explicitly modeling the proba-bilities of words through Dirichlet process(DP) models or Pitman-Yor process (PYP)models have achieved high accuracy, buttheir bilingual counterparts have only beencarried out on small corpora such as ba-sic travel expression corpus (BTEC) due tothe computational complexity.
This paperproposes an efficient unified PYP-basedmonolingual and bilingual UWS method.Experimental results show that the pro-posed method is comparable to super-vised segmenters on the in-domain NISTOpenMT corpus, and yields a 0.96 BLEUrelative increase on NTCIR PatentMT cor-pus which is out-of-domain.1 IntroductionMany languages, especially Asian languages suchas Chinese, Japanese and Myanmar, have no ex-plicit word boundaries, thus word segmentation(WS), that is, segmenting the continuous texts ofthese languages into isolated words, is a prerequi-site for many natural language processing applica-tions including SMT.Though supervised-learning approaches whichinvolve training segmenters on manually seg-mented corpora are widely used (Chang et al,2008), yet the criteria for manually annotat-ing words are arbitrary, and the available anno-tated corpora are limited in both quantity andgenre variety.
For example, in machine transla-tion, there are various parallel corpora such asBTEC for tourism-related dialogues (Paul, 2008)and PatentMT in the patent domain (Goto etal., 2011)1, but researchers working on Chinese-related tasks often use the Stanford Chinese seg-menter (Tseng et al, 2005) which is trained on asmall amount of annotated news text.In contrast, UWS, spurred by the findings thatinfants are able to use statistical cues to determineword boundaries (Saffran et al, 1996), relies onstatistical criteria instead of manually crafted stan-dards.
UWS learns from unsegmented raw text,which are available in large quantities, and thusit has the potential to provide more accurate andadaptive segmentation than supervised approacheswith less development effort being required.The approaches of explicitly modeling theprobability of words(Brent, 1999; Venkataraman,2001; Goldwater et al, 2006; Goldwater et al,2009; Mochihashi et al, 2009) significantly out-performed a heuristic approach (Zhao and Kit,2008) on the monolingual Chinese SIGHAN-MSRcorpus (Emerson, 2005), which inspired the workof this paper.However, bilingual approaches that model wordprobabilities suffer from computational complex-ity.
Xu et al (2008) proposed a bilingual methodby adding alignment into the generative model, butwas only able to test it on small-scale BTEC data.Nguyen et al (2010) used the local best alignmentto increase the speed of the Gibbs sampling intraining but the impact on accuracy was not ex-plored.This paper is dedicated to bilingual UWS onlarge-scale corpora to support SMT.
To this end,we model bilingual UWS under a similar frame-work with monolingual UWS in order to improveefficiency, and replace Gibbs sampling with ex-pectation maximization (EM) in training.We aware that variational bayes (VB) may beused for speeding up the training of DP-based1http://ntcir.nii.ac.jp/PatentMT752or PYP-based bilingual UWS.
However, VB re-quires formulating the m expectations of (m?1)-dimensional marginal distributions, where m isthe number of hidden variables.
For UWS, thehidden variables are indicators that identify sub-strings of sentences in the corpus as words.
Thesevariables are large in number and it is not clearhow to apply VB to UWS, and as far the authorsaware there is no previous work related to the ap-plication of VB to monolingual UWS.
Therefore,we have not explored VB methods in this paper,but we do show that our method is superior to theexisting methods.The contributions of this paper include,?
state-of-the-art accuracy in monolingualUWS;?
the first bilingual UWS method practical forlarge corpora;?
improvement of BLEU scores comparedto supervised Stanford Chinese word seg-menter.2 MethodsThis section describes our unified monolingualand bilingual UWS scheme.
Table 1 lists the mainnotation.
The set F is chosen to represent an un-segmented foreign language sentence (a sequenceof characters), because an unsegmented sentencecan be seen as the set of all possible segmentationsof the sentence denoted F , i.e.
F ?
F .Notation MeaningF an unsegmented foreign sentenceFk?kunsegmented substring of the un-derlying string of F from k to k?F a segmented foreign sentencefjthe j-th foreign wordM monolingual segmentation modelPM(x) probability of x being a word ac-cording to ME a tokenized English sentenceeithe i-th English word(F ,E) a bilingual sentence pairB bilingual segmentation modelPB(x|ei) probability of x being a word ac-cording to B given eiTable 1: Main Notation.Monolingual and bilingual WS can be formu-lated as follows, respectively,?F (F) = argmaxF?FP (F |F ,M), (1)?F (F , E) = argmaxF?F?aP (F, a|F , E,B), (2)where a is an alignment between F and E. TheEnglish sentence E is used in the generation of asegmented sentence F .UWS learns models by maximizing the likeli-hood of the unsegmented corpus, formulated as,?M = argmaxM?F?F(?F?FP (F |M)), (3)?B = argmaxB?
(F ,E)?B(?F?F?aP (F, a|F , E,B)).
(4)Our method of learning M and B proceeds in asimilar manner to the EM algorithm.
The follow-ing two operations are performed iteratively foreach sentence (pair).?
Exclude the previous expected counts of thecurrent sentence (pair) from the model, andthen derive the current sentence in all pos-sible ways, calculating the new expectedcounts for the words (see Section 2.1), thatis, we calculate the expected probabilities ofthe Fk?kbeing words given the data excludingF , i.e.
EF/{F}(P (Fk?k|F)) = P (Fk?k|F ,M)in a similar manner to the marginalization inthe Gibbs sampling process which we are re-placing;?
Update the respective model M or B accord-ing to these expectations (see Section2.2).2.1 Expectation2.1.1 Monolingual ExpectationP (Fk?k|F ,M) is the marginal probability of allthe possible F ?
F that contain Fk?kas a word,which can be calculated efficiently through dy-namic programming (the process is similar to theforeward-backward algorithm in training a hiddenMarkov model (HMM) (Rabiner, 1989)):Pa(k) =U?u=1Pa(k ?
u)PM(Fkk?u)Pb(k?)
=U?u=1Pb(k?+ u)PM(Fk?+uk?
)P (Fk?k|F ,M) = Pa(k)PM(Fk?k)Pb(k?
), (5)753where U is the predefined maximum length of for-eign language words, Pa(k) and Pb(k?)
are theforward and backward probabilities, respectively.This section uses a unigram model for descriptionconvenience, but the method can be extended ton-gram models.2.1.2 Bilingual ExpectationP (Fk?k|F , E,B) is the marginal probability of allthe possible F ?
F that contain Fk?kas a word andare aligned with E, formulated as:P (Fk?k|F , E,B) =?F?FFk?k?F?aP (F, a|E,B)?
?F?FFjk=Fk?k?aJ?j=1P (aj|j, I, J)PB(fj|eaj)=?F?Ffjk=Fk?kJ?j=1?aP (aj|j, I, J)PB(fj|eaj),(6)where J and I are the number of foreign and En-glish words, respectively, and ajis the position ofthe English word that is aligned to fjin the align-ment a.
For the alignment we employ an approx-imation to IBM model 2 (Brown et al, 1993; Ochand Ney, 2003) described below.We define the conditional probability of fjgiven the corresponding English sentence E andthe model B as:PB(fj|E) =?aP (aj|j, I, J)PB(fj|eaj) (7)Then, the previous dynamic programmingmethod can be extended to the bilingual expecta-tionPa(k|E) =U?u=1Pa(k ?
u|E)PB(Fkk?u|E)Pb(k?|E) =U?u=1Pb(k?+ u|E)PB(Fk?+uk?|E)P (Fk?k|F , E,B) = Pa(k|E)PB(Fk?k|E)Pb(k?|E).(8)Eq.
7 can be rewritten (as in IBM model 2):PB(fj|E) =I?i=1P?
(i|j, I, J)PB(fj|ei) (9)P?
(i|j, I, J) =?a:aj=iP (aj|, j, I, J)In order to maintain both speed and accuracy, thefollowing window function is adoptedP?
(i|j, I, J) ?
P?
(i|k, I,K) =???e?|i?kI/K|/?
|i?
kI/K| 6 ?b/2?
?eiis empty word0 otherwise(10)where K is the number of characters in F , andthe k-th character is the start of the word fj, sincej and J are unknown during the computation ofdynamic programming.
?bis the window size, ?
?is the prior probability of an empty English word,and ?
ensures all the items sum to 1.2.2 MaximizationInspired by (Teh, 2006; Mochihashi et al, 2009;Neubig et al, 2010; Teh and Jordan, 2010), weemploy a Pitman-Yor process model to build thesegmentation model M or B.
The monolingualmodel M isPM(fj) =max(n(fj)?
d, 0)+ (?
+ d ?
nM)G0(fj)?f?jn(f?j) + ?nM=??
{fj|n(fj) > d}?
?, (11)where fjis a foreign language word, and n(fj) isthe observed counts of fj, ?
is named the strengthparameter, G0(fj) is named the base distributionof fj, and d is the discount.The bilingual model isPB(fj|ei) =max(n(fj, ei)?
d, 0)+ (?
+ d ?
nei)G0(fj|ei)?f?jn(f?j, ei) + ?nei=??
{x |n(x, ei) > d}??.
(12)In Eqs.
11 and 12,n(fj) =?F?FP (fj|F ,M) (13)n(fj, ei) =?
(F ,E)?BP (fj|F , E,B)P?
(i|j, I, J)PB(fj|ei)?Ii?=1P?
(i?|j, I, J)PB(fj|ei?).
(14)7543 Complexity AnalysisThe computational complexity of our method islinear in the number of iterations, the size of thecorpus, and the complexity of calculating the ex-pectations on each sentence or sentence pair.
Inpractical applications, the size of the corpus isfixed, and we found empirically that the numberof iterations required by the proposed method forconvergence is usually small (less than five itera-tions).
We now look in more detail at the complex-ity of the expectation calculation in monolingualand bilingual models.The monolingual expectation is calculated ac-cording to Eq.
5; the complexity is linear in thelength of sentences and the square of the prede-fined maximum length of words.
Thus its overallcomplexity isOunigrammonoling = O(Ni|F|KU2), (15)where Ni is the number of iterations, K is the av-erage number of characters per sentence, and U isthe predefined maximum length of words.For the monolingual bigram model, the numberof states in the HMM is U times more than thatof the monolingual unigram model, as the states atspecific position of F are not only related to thelength of the current word, but also related to thelength of the word before it.
Thus its complexityis U2 times the unigram model?s complexity:Obigrammonoling = O(Ni|F|KU4).
(16)The bilingual expectation is given by Eq.
8,whose complexity is the same as the monolingualcase.
However, the complexity of calculating thetransition probability, in Eqs.
9 and 10, is O(?b).Thus its overall complexity is:Ounigrambiling = O(Ni|F|KU2?b).
(17)4 ExperimentsIn this section, the proposed method is first val-idated on monolingual segmentation tasks, andthen evaluated in the context of SMT to studywhether the translation quality, measured byBLEU, can be improved.4.1 Experimental Settings4.1.1 Experimental CorporaTwo monolingual corpora and two bilingual cor-pora are used (Table 2).
CHILDES (MacWhin-ney and Snow, 1985) is the most common testCorpus Type # Sentences # CharactersCHILDES Mono.
9,790 95,809SIGHAN-MSR Mono.
90,903 4,234,824OpenMT06 Biling.
437,004 19,692,605PatentMT9 Biling.
1,004,000 63,130,757Table 2: Experimental Corporacorpus for UWS methods.
The SIGHAN-MSRcorpus (Emerson, 2005) consists of manually seg-mented simplified Chinese news text, released inthe SIGHAN bakeoff 2005 shared tasks.The first bilingual corpus: OpenMT06 was usedin the NIST open machine translation 2006 Eval-uation 2.
We removed the United Nations cor-pus and the traditional Chinese data sets from theconstraint training resources.
The data sets ofNIST Eval 2002 to 2005 were used as the develop-ment for MERT tuning (Och, 2003).
This data setmainly consists of news text 3.
PatentMT9 is fromthe shared task of NTCIR-9 patent machine trans-lation .
The training set consists of 1 million par-allel sentences extracted from patent documents,and the development set and test set both consistof 2000 sentences.4.1.2 Performance Measurement andBaseline MethodsFor the monolingual tasks, the F1score againstthe gold annotation is adopted to measure the ac-curacy.
The results reported in related papers arelisted for comparison.For the bilingual tasks, the publicly availablesystem of Moses (Koehn et al, 2007) with defaultsettings is employed to perform machine transla-tion, and BLEU (Papineni et al, 2002) was usedto evaluate the quality.
Character-based segmen-tation, LDC segmenter and Stanford Chinese seg-menters were used as the baseline methods.4.1.3 Parameter settingsThe parameters are tuned on held-out data sets.The maximum length of foreign language wordsis set to 4.
For the PYP model, the base distri-bution adopts the formula in (Chung and Gildea,2009), and the strength parameter is set to 1.0, andthe discount is set to 1.0?
10?6.For bilingual segmentation,the size of the align-ment window is set to 6; the probability ?
?of for-eign language words being generated by an empty2http://www.itl.nist.gov/iad/mig//tests/mt/2006/3It also contains a small number of web blogs755Method Accuracy TimeCHILD.
MSR CHILD.
MSRNPY(bigram)a 0.750 0.802 17 m ?NPY(trigram)a 0.757 0.807 ?
?HDP(bigram)b 0.723 ?
10 h ?Fitnessc ?
0.667 ?
?Prop.
(unigram) 0.729 0.804 3 s 50 sProp.
(bigram) 0.774 0.806 15 s 2530 sa by (Mochihashi et al,2009);b by (Goldwater et al,2009);c by (Zhao and Kit, 2008).Table 3: Results on Monolingual Corpora.English word, was set to 0.3.The training was started from assuming thatthere was no previous segmentations on each sen-tence (pair), and the number of iterations wasfixed.
It was set to 3 for the monolingual unigrammodel, and 2 for the bilingual unigram model,which provided slightly higher BLEU scores onthe development set than the other settings.
Themonolingual bigram model, however, was slowerto converge, so we started it from the segmenta-tions of the unigram model, and using 10 itera-tions.4.2 Monolingual Segmentation ResultsIn monolingual segmentation, the proposed meth-ods with both unigram and bigram models weretested.
Experimental results show that they arecompetitive to state-of-the-art baselines in both ac-curacy and speed (Table 3).
Note that the com-parison of speed is only for reference because thetimes are obtained from their respective papers.4.3 Bilingual Segmentation ResultsTable 4 presents the BLEU scores for Moses usingdifferent segmentation methods.
Each experimentwas performed three times.
The proposed methodwith monolingual bigram model performed poorlyon the Chinese monolingual segmentation task;thus, it was not tested.
We intended to test (Mochi-hashi et al, 2009), but found it impracticable onlarge-scale corpora.The experimental results show that the proposedUWS methods are comparable to the Stanford seg-menters on the OpenMT06 corpus, while achievesa 0.96 BLEU increase on the PatentMT9 corpus.This is because this corpus is out-of-domain forthe supervised segmenters.
The CTB and PKUStanford segmenter were both trained on anno-tated news text, which was the major domain ofOpenMT06.Method BLEUOpenMT06 PatentMT9Character 29.50 ?
0.03 28.36 ?
0.09LDC 31.33 ?
0.10 30.22 ?
0.14Stanford(CTB) 31.68 ?
0.25 30.77 ?
0.13Stanford(PKU) 31.54 ?
0.13 30.86 ?
0.04Prop.(mono.)
31.47 ?
0.18 31.62 ?
0.06Prop.(biling.)
31.61 ?
0.14 31.73 ?
0.05Table 4: Results on Bilingual Corpora.Method TimeOpenMT06 PatentMT9Prop.(mono.)
28 m 1 h 01 mProp.(biling.)
2 h 25 m 5 h 02 mTable 5: Time Costs on Bilingual Corpora.Table 5 presents the run times of the proposedmethods on the bilingual corpora.
The programis single threaded and implemented in C++.
Thetime cost of the bilingual models is about 5 timesthat of the monolingual model, which is consistentwith the complexity analysis in Section 3.5 ConclusionThis paper is devoted to large-scale Chinese UWSfor SMT.
An efficient unified monolingual andbilingual UWS method is proposed and applied tolarge-scale bilingual corpora.Complexity analysis shows that our method iscapable of scaling to large-scale corpora.
This wasverified by experiments on a corpus of 1-millionsentence pairs on which traditional MCMC ap-proaches would struggle (Xu et al, 2008).The proposed method does not require anyannotated data, but the SMT system with itcan achieve comparable performance comparedto state-of-the-art supervised word segmenterstrained on precious annotated data.
Moreover,the proposed method yields 0.96 BLEU improve-ment relative to supervised word segmenters onan out-of-domain corpus.
Thus, we believe thatthe proposed method would benefit SMT related tolow-resource languages where annotated data arescare, and would also find application in domainsthat differ too greatly from the domains on whichsupervised word segmenters were trained.In future research, we plan to improve the bilin-gual UWS through applying VB and integratingmore accurate alignment models such as HMMmodels and IBM model 4.756ReferencesMichael R Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discov-ery.
Machine Learning, 34(1-3):71?105.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational linguistics, 19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher DManning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the 3rd Workshop on Statistical MachineTranslation, pages 224?232.
Association for Com-putational Linguistics.Tagyoung Chung and Daniel Gildea.
2009.
Unsu-pervised tokenization for machine translation.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume2-Volume 2, pages 718?726.
Association for Com-putational Linguistics.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedingsof the 4th SIGHAN Workshop on Chinese LanguageProcessing, volume 133.Sharon Goldwater, Thomas L Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsu-pervised word segmentation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 673?680.
Association for Computational Linguistics.Sharon Goldwater, Thomas L Griffiths, and Mark John-son.
2009.
A Bayesian framework for word seg-mentation: exploring the effects of context.
Cogni-tion, 112(1):21?54.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K Tsou.
2011.
Overview of the patentmachine translation task at the NTCIR-9 workshop.In Proceedings of NTCIR, volume 9, pages 559?578.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Brian MacWhinney and Catherine Snow.
1985.
Thechild language data exchange system.
Journal ofchild language, 12(2):271?296.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested Pitman-Yor language modeling.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1-Volume 1, pages 100?108.Association for Computational Linguistics.Graham Neubig, Masato Mimura, Shinsuke Mori, andTatsuya Kawahara.
2010.
Learning a languagemodel from continuous speech.
In InterSpeech,pages 1053?1056.ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.2010.
Nonparametric word segmentation for ma-chine translation.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 815?823.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics-Volume 1, pages 160?167.
As-sociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318.
Associationfor Computational Linguistics.Michael Paul.
2008.
Overview of the IWSLT 2008evaluation campaign.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation,pages 1?17.Lawrence R Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Jenny R Saffran, Richard N Aslin, and Elissa L New-port.
1996.
Statistical learning by 8-month-old in-fants.
Science, 274(5294):1926?1928.Yee Whye Teh and Michael I Jordan.
2010.
Hierar-chical Bayesian nonparametric models with appli-cations.
Bayesian Nonparametrics: Principles andPractice, pages 158?207.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting on Association for Computational Linguis-tics, pages 985?992.
Association for ComputationalLinguistics.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter for SIGHANBakeoff 2005.
In Proceedings of the 4th SIGHANWorkshop on Chinese Language Processing, volume171.
Jeju Island, Korea.757Anand Venkataraman.
2001.
A statistical model forword discovery in transcribed speech.
Computa-tional Linguistics, 27(3):351?372.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervisedChinese word segmentation for statistical machinetranslation.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics-Volume 1, pages 1017?1024.
Association for Com-putational Linguistics.Hai Zhao and Chunyu Kit.
2008.
An empirical com-parison of goodness measures for unsupervised chi-nese word segmentation with a unified framework.In Proceedings of the 3rd International Joint Con-ference on Natural Language Processing, pages 9?16.758
