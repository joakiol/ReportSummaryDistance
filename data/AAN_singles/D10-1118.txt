Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1214?1222,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsIt Depends on the Translation:Unsupervised Dependency Parsing via Word AlignmentSamuel BrodyDept.
of Biomedical InformaticsColumbia Universitysamuel.brody@dbmi.columbia.eduAbstractWe reveal a previously unnoticed connectionbetween dependency parsing and statisticalmachine translation (SMT), by formulatingthe dependency parsing task as a problem ofword alignment.
Furthermore, we show thattwo well known models for these respectivetasks (DMV and the IBM models) share com-mon modeling assumptions.
This motivates usto develop an alignment-based framework forunsupervised dependency parsing.
The frame-work (which will be made publicly available)is flexible, modular and easy to extend.
Us-ing this framework, we implement several al-gorithms based on the IBM alignment mod-els, which prove surprisingly effective on thedependency parsing task, and demonstrate thepotential of the alignment-based approach.1 IntroductionBoth statistical machine translation (SMT) and un-supervised dependency parsing have seen a surge ofinterest in recent years, as the need for large scaledata processing has increased.
The problems ad-dressed by each of the fields seem quite differentat first glance.
However, in this paper, we reveal astrong connection between them and show that theproblem of dependency parsing can be formulatedas one of word alignment within the sentence.
Fur-thermore, we show that the two models that are ar-guably the most influential in their respective fields,the IBM models 1-3 (Brown et al, 1993) and Kleinand Manning?s (2004) Dependency Model with Va-lence (DMV), share a common set of modeling as-sumptions.Based on this connection, we develop a frame-work which uses an alignment-based approach forunsupervised dependency parsing.
The frameworkis flexible and modular, and allows us to explore dif-ferent modeling assumptions.
We demonstrate theseproperties and the merit of the alignment-based pars-ing approach by implementing several dependencyparsing algorithms based on the IBM alignmentmodels and evaluating their performance on the task.Although the algorithms are not competitive withstate-of-the-art systems, they outperform the right-branching baseline and approach the performance ofDMV.
This is especially surprising when we con-sider that the IBM models were not originally de-signed for the task.
These results are encourag-ing and indicate that the alignment-based approachcould serve as the basis for competitive dependencyparsing systems, much as DMV did.This paper offers two main contributions.
First,by revealing the connection between the two tasks,we introduce a new approach to dependency pars-ing, and open the way for use of SMT alignment re-sources and tools for parsing.
Our experiments withthe IBMmodels demonstrate the potential of this ap-proach and provide a strong motivation for furtherdevelopment.
The second contribution is a publicly-available framework for exploring new alignmentmodels.
The framework uses Gibbs sampling tech-niques and includes our sampling-based implemen-tations of the IBM models (see Section 3.4).
Thesampling approach makes it easy to modify the ex-isting models and add new ones.
The frameworkcan be used both for dependency parsing and for bi-lingual word alignment.The rest of the paper is structured as follows.
InSection 2 we present a brief overview of those worksin the fields of dependency parsing and alignmentfor statistical machine translation which are directly1214relevant to this paper.
Section 3 describes the con-nection between the two problems, examines theshared assumptions of the DMV and IBM models,and describes our framework and algorithms.
InSection 4 we present our experiments and discussthe results.
We conclude in Section 5.2 Background and Related Work2.1 Unsupervised Dependency ParsingIn recent years, the field of supervised parsing hasadvanced tremendously, to the point where highlyaccurate parsers are available for many languages.However, supervised methods require the manualannotation of training data with parse trees, a pro-cess which is expensive and time consuming.
There-fore, for domains and languages with minimal re-sources, unsupervised parsing is of great impor-tance.Early work in the field focused on models thatmade use primarily of the co-occurrence informa-tion of the head and its argument (Yuret, 1998;Paskin, 2001).
The introduction of DMV by Kleinand Manning (2004) represented a shift in the di-rection of research in the field.
DMV is based ona linguistically motivated generative model, whichfollows common practice in supervised parsing andtakes into consideration the distance between headand argument, as well as the valence (the capac-ity of a head word to attach arguments).
Klein andManning (2004) also shifted from a lexical repre-sentation of the sentences to representing them aspart-of-speech sequences.
DMV strongly outper-formed previous models and was the first unsuper-vised dependency induction system to achieve accu-racy above the right-branching baseline.
Much sub-sequent work in the field has focused on modifica-tions and extensions of DMV, and it is the basis fortoday?s state-of-the-art systems (Cohen and Smith,2009; Headden III et al, 2009).2.2 Alignment for SMTSMT treats translation as a machine learning prob-lem.
It attempts to learn a translation model froma parallel corpus composed of sentences and theirtranslations.
The IBM models (Brown et al, 1993)represent the first generation of word-based SMTmodels, and serve as a starting point for most cur-Figure 1: An example of an alignment between an En-glish sentence (top) and its French translation (bottom).rent SMT systems (e.g., Moses, Koehn et al 2007;Hiero, Chiang 2005).
The models employ the notionof alignment between individual words in the sourceand translation.
An example of such an alignment isgiven in Figure 1.The IBM models all seek to maximize Pr( f |e),the probability of a French translation f of an En-glish sentence e. This probability is broken downby taking into account all possible alignments a be-tween e and f , and their probabilities:Pr( f |e) =?aPr( f ,a|e) (1)Each of the IBMmodels is based on the previous onein the series, and adds another level of latent parame-ters which take into account a specific characteristicof the data.3 Alignment-based Dependency Parsing3.1 The ConnectionThe task of dependency parsing requires finding aparse tree for a sentence, where two words are con-nected by an edge if they participate in a syntacticdependency relation.
When dealing with unlabeleddependencies, the exact nature of the relationship isnot determined.
An example of a dependency parseof a sentence is given in Figure 2 (left).Another possible formulation of the problem is asfollows.
Find a set of pairwise relations (si,s j) con-necting a dependent word s j with its head word si inthe sentence.
This alternate formulation allows us toview the problem as one of alignment of a sentenceto itself, as shown in Figure 2 (right).Given this perspective on the problem, it makessense to examine existing alignment models, com-pare them to dependency parsing models, and seeif they can be successfully employed for the depen-dency parsing task.1215Figure 2: Left: An example of an unlabeled dependency parse of a sentence.
Right: The same parse, in the form ofan alignment between a head words (top) and their dependents (bottom).3.2 Comparing IBM & DMV AssumptionsLexical Association The core assumption of IBMModel 1 is that the lexical identities of the En-glish and French words help determine whether theyshould be aligned.
The same assumption is made inall the dependency models mentioned in Section 2regarding a head and its dependent (although DMVuses word classes instead of the actual words).Location IBM Model 2 adds the considerationof difference in location between the English andFrench words when considering the likelihood ofalignment.
One of the improvements contributingto the success of DMV was the notion of distance,which was absent from previous models (see Sec-tion 3 in Klein and Manning 2004).Fertility IBM Model 3 adds the notion of fertil-ity, or the idea that different words in the source lan-guage tend to generate different numbers of words inthe target language.
This corresponds to the notionof valence, used by Klein and Manning (2004), andthe other major contributor to the success of DMV(ibid.
).Null Source The IBM models all make use ofan additional ?null?
word in every sentence, whichhas special status.
It is attached to words in thetranslation that do not correspond to a word in thesource.
It is treated separately when calculatingdistance (since it has no location) and fertility.
Inthese characteristics, it is very similar to the ?root?node, which is artificially added to parse trees andused to represent the head of words which are notdependents of any other word in the sentence.In examining the core assumptions of the IBMmodels, we note that there is a strong resemblanceto those of DMV.
The similarity is at an abstractlevel since the nature of the relationship that eachmodel attempts to detect is quite different.
TheIBMmodels look for an equivalence relationship be-tween lexical items in two languages, whereas DMVaddresses functional relationships between two el-ements with distinct meanings.
However, both at-tempt to model a similar set of factors, which theyposit will be important to their respective tasks1.This similarity motivates the work presented in therest of the paper, i.e, exploring the use of the IBMalignment models for dependency parsing.
It is im-portant to note that the IBM models do not addressmany important factors relevant to the parsing task.For instance, they have no notion of a parse tree, adeficit which may lead to degenerate solutions andmalformed parses.
However, they serve as a goodstarting point for exploring the alignment approachto parsing, as well as discovering additional factorsthat need to be addressed under this approach.3.3 Experimental FrameworkWe developed a Gibbs sampling framework foralignment-based dependency parsing2.
The tradi-tional approach to alignment uses Expectation Max-imization (EM) to find the optimal values for thelatent variables.
In each iteration, it considers allpossible alignments for each pair of sentences, and1These abstract notions (lexical association, proximity, ten-dencies towards few or many relations, and allowing for unasso-ciated items) play an important role in many relation-detectiontasks (e.g., co-reference resolution, Haghighi and Klein 2010).2Available for download at:http://people.dbmi.columbia.edu/?sab70121216chooses the optimal one based on the current pa-rameter estimates.
The sampling method, on theother hand, only considers a small change in eachstep - that of re-aligning a previously aligned targetword to a new source.
The reason for our choice isthe ease of modification of such sampling models.They allow for easy introduction of further param-eters and more complex probabilistic functions, aswell as Bayesian priors, all of which are likely to behelpful in development3.Under the sampling framework, the model pro-vides the probability of changing the alignment A[i]of a target word i from a previously aligned sourceword j to a new one j?.
In all the models we consider,this probability is proportional to the ratio betweenthe scores of the old sentence alignment A and thenew one A?, which differs from the old only in therealignment of i to j?.P(A[i] = j ?
A[i] = j? )
?Pmodel(A?
)Pmodel(A)(2)As a starting point for our dependency parsingmodel, we re-implemented the first three IBM mod-els 4 in the sampling framework.3.4 Reformulating the IBM modelsIBM Model 1 According to this model, the prob-ability of an alignment between target word i andsource word j?
depends only on the lexical identitiesof the two words wi and w j?
respectively.
This givesus equation 3.P(A[i] ?
j? )
?Pmodel(A?)Pmodel(A)=?kP(wk,wA[k])?k?
P(wk?
,wA?[k?
])P(A[i] ?
j? )
?P(wi,w j?
)P(wi,w j)(3)In our implementation we assume the alignmentfollows a Chinese Restaurant Process (CRP), where3Preliminary experiments using the EM approach via theGIZA++ toolkit (Och and Ney, 2003) resulted in similar per-formance to that of the sampling method for IBM Models 1 and2.
However, we were unable to explore the use of Model 3under that framework, since the implementation of the modelwas strongly coupled to other, SMT-specific, optimizations andheuristics.4Our implementation, as well as some core components inour framework, are based on code kindly provided by ChrisDyer.the probability of wi aligning to w j is proportionalto the number of times they have been aligned in thepast (the rest of the data), as follows:P(wi,w j?)
=#(wi,w j)+?1/V#(?,w j)+?1(4)Here, #(wi,w j) represents the number of times thetarget word wi was observed to be aligned to w j inthe rest of the data, and ?
stands for any word, Vis the size of the vocabulary, and ?1 is a hyperpa-rameter of the CRP, which can also be viewed as asmoothing factor.IBM Model 2 The original IBM model 2 is adistortion model that assumes that the probabilityof an alignment between target word i and sourceword j?
depends only on the locations of the words,i.e., the values i and j?, taking into account the dif-ferent lengths l and m of the source and target sen-tences, respectively.
For dependency parsing, wherewe align sentences to themselves, l = m. This givesus equation 5.P(A[i] ?
j? )
?Pmodel(A?
)Pmodel(A)=P(i, j?, l)P(i, j, l)P(i, j?, l) =#(i, j?, l)+?2/D#(i,?, l)+?2(5)Again, we assume a CRP when choosing a dis-tortion value, where D is the expected number ofdistance values (set to 10 in our experiments), ?2is the CRP hyperparameter, #(i, j, l) is the numberof times a target word in position i was aligned toa source word in position j in sentences of length l,and #(i,?, l) is the number of times word in positioni was aligned (to any source position) in sentencesof length l.Even without the need for handling differentlengths for source and target sentences, this modelis complex and requires estimating a separate prob-ability for each triplet (i, j, l).
In addition, the as-sumption that the distance distribution depends onlyon the sentence length and is similar for all to-kens seems unreasonable, especially when dealingwith part-of-speech tokens and dependency rela-tions.
Such concerns have been mentioned in theSMT literature and were shown to be justified inour experiments (see Sec.
4).
For this reason, we1217also implemented an alternate distance model, basedloosely on Liang et al (2006).
Under the alternatemodel, the probability of an alignment between tar-get word i and source word j?
depends on the distancebetween them, their order, the sentence length, andthe word type of the head, according to equation 6.P(i, j?, l) =#[wi,(i- j?
), l]+?3/D#(wi,?, l)+?3(6)IBM Model 3 This model handles the notion offertility (or valence).
Under this model, the proba-bility of an alignment depends on how many targetwords are aligned to each of the source words.
Eachsource word type w j?, has a distribution specifyingthe probability of having n aligned target words.
Theprobability of an alignment is proportional to theproduct of the probabilities of the fertilities in thealignment and takes into account the special status ofthe null word (represented by the index j = 0).
Thisprobability is given in Equation 7, which is based onEquation 32 in Brown et al (1993)5.P(A) ?(l?
?0?0)pl?2?00 p?01l?j=1?
j!#(w j,?
j)+?4/F#(w j,?
)+?4(7)Here, ?
j denotes the number of target words alignedto the j-th source word in alignment A. p1 and p0sum to 1 and are used to derive the probability thatthere will be ?0 null-aligned words in a sentencecontaining l words6.
#(w j,?
j) represents the num-ber of times source word w j was observed to have?
j dependent target words, #(w j,?)
is the numberof times w j appeared in the data, F is the expectednumber of fertility values (5 in our experiments),and ?4 is the CRP hyperparameter.Combining the Models The original IBM mod-els work in an incremental fashion, with each modelusing the output of the previous one as a startingpoint and adding a new component to the probabil-ity distribution.
The dependency parsing frameworkemploys a similar approach.
It uses the alignments5The transitional version of this equation depends onwhether either the old source word ( j) or the new one ( j?)
arenull, and is omitted for brevity.
Further details can be found inBrown et al (1993) Section 4.4 and Equation 43.6For details, see Brown et al (1993) Equation 31.learned by the previous model as the starting point ofthe next and combines the probability distributionsof each component via a product model.
This al-lows for the easy introduction of new models whichconsider different aspects of the alignment and com-plement each other.Preventing Self-Alignment When adapting thealignment approach to dependency parsing, we viewthe task as that of aligning a sentence to itself.
Oneissue we must address is preventing the degeneratesolution of aligning each word to itself.
For this pur-pose we introduce a simple model into the productwhich gives zero probability to alignments whichcontain a word aligned to itself, as in equation 8.P(A[i] = j? )
={0 if i = j?1l?1 otherwise(8)4 Experiments4.1 DataWe evaluated our model on several corpora.
The firstof these was the Penn.
Treebank portion of the WallStreet Journal (WSJ).
We used the Constituent-to-Dependency Conversion Tool7 to convert the tree-bank format into CoNLL format.We also made use of the Danish and Dutchdatasets from the CoNLL 2006 shared task8.
Sincewe do not make use of annotation, we can induce adependency structure on the entire dataset provided(disregarding the division into training and testing).Following Klein and Manning (2004), we usedthe gold-standard part-of-speech sequences ratherthan the lexical forms and evaluated on sentencescontaining 10 or fewer tokens after removal of punc-tuation.4.2 ResultsTable 1 shows the results of the IBM Models onthe task of directed (unlabeled) dependency parsing.We compare to the right-branching baseline used byKlein and Manning (2004).
For the WSJ10 corpus,the authors reported 43.2% accuracy for DMV and33.6% for the baseline.
Although there are small7nlp.cs.lth.se/software/treebank converter/8http://nextens.uvt.nl/?conll/1218Corpus M 1 M2 M3 R-brWSJ10 25.42 35.73 39.32 32.85Dutch10 25.17 32.46 35.28 28.42Danish10 23.12 25.96 41.94 16.05 *Table 1: Percent accuracy of IBMModels 1-3 (M1-3) andthe right-branching baseline (R-br) on several corpora.PoS attachmentNN DETIN NNNNP NNPDET NNJJ NNPoS attachmentNNS JJRB VBZVBD NNVB TOCC NNSTable 2: Most likely dependency attachment for the topten most common parts-of-speech, according to Model 1.differences in evaluation, as evidenced by the dif-ference between our baseline scores, IBM Models2 and 3 outperform the baseline by a large marginand Model 3 approaches the performance of DMV.On the Dutch and Danish datasets, the trends aresimilar.
On the latter dataset, even Model 1 out-performs the right-branching baseline.
However, theDanish dataset is unusual (see Buchholz and Marsi2006) in that the alternate adjacency baseline of left-branching (also mentioned by Klein and Manning2004) is extremely strong and achieves 48.8% di-rected accuracy.4.3 AnalysisIn order to better understand what our alignmentmodel was learning, we looked at each componentelement individually.Lexical Association To explore what Model 1 waslearning, we analyzed the resulting probability ta-bles for association between tokens.
Table 2 showsthe most likely dependency attachment for the topten most common parts-of-speech.
The model isclearly learning meaningful connections betweenparts of speech (determiners and adjectives to nouns,adverbs to verbs, etc.
), but there is little notion ofdirectionality, and cycles can exist.
For instance,the model learns the connection between determinerand noun, but is unsure which is the head and whichthe dependent.
A similar connection is learned be-tween to and verbs in the base form (VB).
This in-consistency is, to a large extent, the result of thedeficiencies of the model, stemming from the factthat the IBM models were designed for a differenttask and are not trying to learn a well-formed tree.However, there is a strong linguistic basis to con-sider the directionality of these relations difficult.There is some debate among linguists as to whetherthe head of a noun phrase is the noun or the deter-miner9 (see Abney 1987).
Each can be seen as a dif-ferent kind of head element, performing a differentfunction, similarly to the multiple types of depen-dency relations identified in Hudson?s (1990) WordGrammar.
A similar case can be made regarding thehead of an infinitive phrase.
The infinitive form ofthe verb may be considered the lexical head, deter-mining the predicate, while to can be seen as thefunctional head, encoding inflectional features, as inChomsky?s (1981) Government & Binding model10.Distance Models The original IBM distortionmodel (Model 2), which does not differentiate be-tween words types and looks only at positions, hasan accuracy of 33.43% on the WSJ10 corpus.
Inaddition, it tends to strongly favor left-branching at-tachment (57.2% of target words were attached tothe word immediately to their right, 22.6% to theirleft, as opposed to 31% and 25.8% in the gold stan-dard).
The alternative distance model we proposed,which takes into account the identity of the headword, achieves better accuracy and is closer to thegold standard balance (43.5% right and 35.3% left).Figure 3 shows the distribution of the location ofthe dependent relative to the head word (at position0) for several common parts-of-speech.
It is inter-esting to see that singular and plural nouns (NN,NNS) behave similarly.
They both have a strongpreference for local attachment and a tendency to-wards a left-dependent (presumably the determiner,see above Table 2).
Pronouns (NNP), on the otherhand, are more likely to attach to the right sincethey are not modified by determiners.
Verbs in past(VBZ) and present (VBD, VBP) forms have simi-lar behavior, with a flatter distribution of dependentlocations, whereas the base form (VB) attaches al-most exclusively to the preceding token, presumably9In fact, the original DMV chose the determiner as the head(see discussion in Klein and Manning 2004, Section 3).10We thank an anonymous reviewer for elucidating this point.1219Figure 3: Distribution of head-to-dependent distance for several types of verbs (left) and nouns (right), as learned byour alternate distance model.to (see Table 2).Fertility Figure 4 shows the distribution of fertil-ity values for several common parts of speech.
Verbshave a relatively flat distribution with a longer tailas compared to nouns, which means they are likelyto have a larger number of arguments.
Once again,the base form (VB) exhibits different behavior fromthe other verbs forms, taking almost exclusively oneargument.
This is likely an effect of the strong con-nection between base form verbs and the precedingword to.Hyper-Parameters Each of our models requires avalue for its CRP hyperparameter (see Section 3.4).In this work, since parameter estimation was not ourfocus, we set the hyperparameters to be approxi-mately 1K , where K is the number of possible val-ues, according to the rule of thumb common in theliterature.
Specifically, we chose ?1 = 0.01,?3 =0.05,?4 = 0.1.
We investigated the effect of thesechoices on performance in a separate set of exper-iments, which showed that small variations (up toan order of magnitude) in these parameters had littleeffect on the results.In addition to the CRP parameters, Model 3 re-quires a value for p1, the null fertility hyperparame-ter.
In our experiments, we found that this hyper-parameter had a very strong effect on results if itwas above 0.1, creating many spurious null align-ments.
However, below that threshold, the effectswere small.
In the experiments reported here, we setp1 = 0.01.Initialization One issue with DMV, which is of-ten mentioned, is its sensitivity to initialization.
Wetested our model with random initialization (uniformalignment probabilities) and with an approximationof the ad-hoc ?harmonic?
initialization described inKlein and Manning (2004) and found no noticeabledifference in accuracy.4.4 DiscussionThe accuracy achieved by the IBM models (Table 1)is surprisingly high, given the fact that the IBMmodels were not designed with dependency parsingin mind.
It is likely that customizing the models tothe task will result in even better performance.
Ourfindings in Section 4.3 support this hypothesis.
Theanalysis showed that the lack of tree structure in themodel impacted the learning, and therefore it is ex-pected that a component which enforces tree struc-ture (prevents cycles) will be beneficial.Although it lacks an inherent notion of tree struc-ture, the alignment-based approach has several ad-vantages over the head-outward approach of DMVand related models.
It can consider the alignment asa whole and take into account global sentence con-straints, not just head-dependent relations.
Thesemay also include tree-structure constraints commonto the head-outward approaches, but can be moreflexible in how they are addressed.
For instance,1220Figure 4: Distribution of fertility values for several types of verbs (left) and nouns (right), as learned by IBM Model 3.DMV?s method of modeling tree structure doesnot allow non-projective dependencies, whereas analignment-based model may choose to allow or con-strain non-projectivity, as learned from the data.
An-other advantage of our alignment-based models isthe fact that they are not strongly sensitive to ini-tialization and can be started from a set of randomalignments.5 Conclusions and Future WorkWe have described an alternative formulation of de-pendency parsing as a problem of word alignment.This connection motivated us to explore the possi-bility of using alignment tools for the task of un-supervised dependency parsing.
We chose to ex-periment with the well-known IBM alignment mod-els which share a set of similar modeling assump-tions with Klein and Manning?s (2004) DependencyModel with Valence.
Our experiments showed thatthe IBM models are surprisingly effective at thedependency parsing task, outperforming the right-branching baseline and approaching the accuracy ofDMV.
Our results demonstrate that the alignmentapproach can be used as a foundation for depen-dency parsing algorithms and motivates further re-search in this area.There are many interesting avenues for further re-search.
These include improving and extending theexisting IBM models, as well as introducing newmodels that are specifically designed for the parsingtask and represent relevant linguistic considerations(e.g., enforcing tree structure, handling crossing de-pendencies, learning left- or right-branching tenden-cies).In Spitkovsky et al (2010), the authors show thata gradual increase in the complexity of the data canaid the learning process.
The IBM approach demon-strated the benefit of a gradual increase of modelcomplexity.
It would be interesting to see if the twoapproaches could be successfully combined.Finally, although we use our framework for de-pendency parsing, the sampling approach and theframework we developed can be used to explore newmodels for bilingual word alignment.
Furthermore,an alignment-based parsing method is expected tointegrate well with SMT bi-lingual alignment mod-els and may, therefore, be suitable for combinedmodels which use parse trees to improve word align-ment (e.g., Burkett et al 2010).AcknowledgmentsI would like to thank Chris Dyer for providing thebasis for the sampling implementation.
I wouldalso like to thank Chris, Adam Lopez, Trevor Cohn,Adam Faulkner and the anonymous reviewers fortheir time and effort and their helpful comments andsuggestions.ReferencesAbney, Steven.
1987.
The English Noun Phrase in itsSentential Aspect.
Ph.D. thesis, Massachusetts Insti-1221tute of Technology.Brown, Peter F., Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
The math-ematics of statistical machine translation: parameterestimation.
Comput.
Linguist.
19(2):263?311.Buchholz, Sabine and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In In Proc.of CoNLL.
pages 149?164.Burkett, David, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In North American Association for Com-putational Linguistics.
Los Angeles.Chiang, David.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics.
Association for Com-putational Linguistics, Morristown, NJ, USA, pages263?270.Chomsky, Noam.
1981.
Lectures on government andbinding : the Pisa lectures / Noam Chomsky.Cohen, Shay B. and Noah A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsu-pervised grammar induction.
In NAACL ?09: Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics.
Associa-tion for Computational Linguistics, Morristown, NJ,USA, pages 74?82.Haghighi, Aria and Dan Klein.
2010.
Coreference res-olution in a modular, entity-centered model.
In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics.
Association forComputational Linguistics, Los Angeles, California,pages 385?393.Headden III, William P., Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics.
Asso-ciation for Computational Linguistics, Boulder, Col-orado, pages 101?109.Hudson, R. 1990.
English Word Grammar.
Basil Black-well, Oxford.Klein, Dan and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In ACL ?04: Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics.
Association for ComputationalLinguistics, Morristown, NJ, USA, page 478.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In ACL ?07:Proceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions.
As-sociation for Computational Linguistics, Morristown,NJ, USA, pages 177?180.Liang, Percy, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference.
Association for Computational Linguis-tics, New York City, USA, pages 104?111.Och, Franz Josef and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics 29(1):19?51.Paskin, Mark A.
2001.
Grammatical bigrams.
InThomas G. Dietterich, Suzanna Becker, and ZoubinGhahramani, editors, NIPS.
MIT Press, pages 91?97.Spitkovsky, Valentin I., Hiyan Alshawi, and Daniel Juraf-sky.
2010.
From Baby Steps to Leapfrog: How ?Lessis More?
in unsupervised dependency parsing.
In Proc.of NAACL-HLT .Yuret, D. 1998.
Discovery of linguistic relations usinglexical attraction.
Ph.D. thesis, Department of Com-puter Science and Electrical Engineering, MIT.1222
