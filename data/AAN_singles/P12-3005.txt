Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25?30,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguisticslangid.py: An Off-the-shelf Language Identification ToolMarco Lui and Timothy BaldwinNICTA VRLDepartment of Computing and Information SystemsUniversity of Melbourne, VIC 3010, Australiamhlui@unimelb.edu.au, tb@ldwin.netAbstractWe present langid.py, an off-the-shelf lan-guage identification tool.
We discuss the de-sign and implementation of langid.py, andprovide an empirical comparison on 5 long-document datasets, and 2 datasets from the mi-croblog domain.
We find that langid.pymaintains consistently high accuracy acrossall domains, making it ideal for end-users thatrequire language identification without want-ing to invest in preparation of in-domain train-ing data.1 IntroductionLanguage identification (LangID) is the task of de-termining the natural language that a document iswritten in.
It is a key step in automatic processingof real-world data, where a multitude of languagesmay be present.
Natural language processing tech-niques typically pre-suppose that all documents be-ing processed are written in a given language (e.g.English), but as focus shifts onto processing docu-ments from internet sources such as microbloggingservices, this becomes increasingly difficult to guar-antee.
Language identification is also a key compo-nent of many web services.
For example, the lan-guage that a web page is written in is an importantconsideration in determining whether it is likely tobe of interest to a particular user of a search engine,and automatic identification is an essential step inbuilding language corpora from the web.
It has prac-tical implications for social networking and socialmedia, where it may be desirable to organize com-ments and other user-generated content by language.It also has implications for accessibility, since it en-ables automatic determination of the target languagefor automatic machine translation purposes.Many applications could potentially benefit fromautomatic language identification, but building acustomized solution per-application is prohibitivelyexpensive, especially if human annotation is re-quired to produce a corpus of language-labelledtraining documents from the application domain.What is required is thus a generic language identi-fication tool that is usable off-the-shelf, i.e.
with noend-user training and minimal configuration.In this paper, we present langid.py, a LangIDtool with the following characteristics: (1) fast,(2) usable off-the-shelf, (3) unaffected by domain-specific features (e.g.
HTML, XML, markdown),(4) single file with minimal dependencies, and (5)flexible interface2 Methodologylangid.py is trained over a naive Bayes clas-sifier with a multinomial event model (McCallumand Nigam, 1998), over a mixture of byte n-grams(1?n?4).
One key difference from conventionaltext categorization solutions is that langid.pywas designed to be used off-the-shelf.
Sincelangid.py implements a supervised classifier,this presents two primary challenges: (1) a pre-trained model must be distributed with the classi-fier, and (2) the model must generalize to data fromdifferent domains, meaning that in its default con-figuration, it must have good accuracy over inputsas diverse as web pages, newspaper articles and mi-croblog messages.
(1) is mostly a practical consid-eration, and so we will address it in Section 3.
Inorder to address (2), we integrate information aboutthe language identification task from a variety of do-mains by using LD feature selection (Lui and Bald-win, 2011).Lui and Baldwin (2011) showed that it is rela-tively easy to attain high accuracy for language iden-25Dataset Documents Langs Doc Length (bytes)EUROGOV 1500 10 1.7?104 ?3.9?104TCL 3174 60 2.6?103 ?3.8?103WIKIPEDIA 4963 67 1.5?103 ?4.1?103EMEA 19988 22 2.9?105 ?7.9?105EUROPARL 20828 22 1.7?102 ?1.6?102T-BE 9659 6 1.0?102 ?3.2?101T-SC 5000 5 8.8?101 ?3.9?101Table 1: Summary of the LangID datasetstification in a traditional text categorization setting,where we have in-domain training data.
The task be-comes much harder when trying to perform domainadaptation, that is, trying to use model parameterslearned in one domain to classify data from a dif-ferent domain.
LD feature selection addresses thisproblem by focusing on key features that are relevantto the language identification task.
It is based on In-formation Gain (IG), originally introduced as a split-ting criteria for decision trees (Quinlan, 1986), andlater shown to be effective for feature selection intext categorization (Yang and Pedersen, 1997; For-man, 2003).
LD represents the difference in IG withrespect to language and domain.
Features with ahigh LD score are informative about language with-out being informative about domain.
For practi-cal reasons, before the IG calculation the candidatefeature set is pruned by means of a term-frequencybased feature selection.Lui and Baldwin (2011) presented empirical evi-dence that LD feature selection was effective for do-main adaptation in language identification.
This re-sult is further supported by our evaluation, presentedin Section 5.3 System ArchitectureThe full langid.py package consists of thelanguage identifier langid.py, as well as twosupport modules LDfeatureselect.py andtrain.py.langid.py is the single file which packages thelanguage identification tool, and the only file neededto use langid.py for off-the-shelf language iden-tification.
It comes with an embedded model whichcovers 97 languages using training data drawn from5 domains.
Tokenization and feature selection arecarried out in a single pass over the input documentvia Aho-Corasick string matching (Aho and Cora-sick, 1975).
The Aho-Corasick string matching al-gorithm processes an input by means of a determin-istic finite automaton (DFA).
Some states of the au-tomaton are associated with the completion of oneof the n-grams selected through LD feature selec-tion.
Thus, we can obtain our document represen-tation by simply counting the number of times theDFA enters particular states while processing our in-put.
The DFA and the associated mapping from stateto n-gram are constructed during the training phase,and embedded as part of the pre-trained model.The naive Bayes classifier is implemented usingnumpy,1 the de-facto numerical computation pack-age for Python.
numpy is free and open source, andavailable for all major platforms.
Using numpy in-troduces a dependency on a library that is not in thePython standard library.
This is a reasonable trade-off, as numpy provides us with an optimized im-plementation of matrix operations, which allows usto implement fast naive Bayes classification whilemaintaining the single-file concept of langid.py.langid.py can be used in the three ways:Command-line tool: langid.py supports aninteractive mode with a text prompt and line-by-lineclassification.
This mode is suitable for quick in-teractive queries, as well as for demonstration pur-poses.
langid.py also supports language identi-fication of entire files via redirection.
This allows auser to interactively explore data, as well as to inte-grate language identification into a pipeline of otherunix-style tools.
However, use via redirection isnot recommended for large quantities of documentsas each invocation requires the trained model to beunpacked into memory.
Where large quantities ofdocuments are being processed, use as a library orweb service is preferred as the model will only beunpacked once upon initialization.Python library: langid.py can be imported asa Python module, and provides a function that ac-cepts text and returns the identified language of thetext.
This use of langid.py is the fastest in asingle-processor setting as it incurs the least over-head.Web service: langid.py can be started as aweb service with a command-line switch.
This1http://numpy.scipy.org26allows language identitication by means of HTTPPUT and HTTP POST requests, which return JSON-encoded responses.
This is the preferred method ofusing langid.py from other programming envi-ronments, as most languages include libraries for in-teracting with web services over HTTP.
It also al-lows the language identification service to be run asa network/internet service.
Finally, langid.py isWSGI-compliant,2 so it can be deployed in a WSGI-compliant web server.
This provides an easy way toachieve parallelism by leveraging existing technolo-gies to manage load balancing and utilize multipleprocessors in the handling of multiple concurrent re-quests for a service.LDfeatureselect.py implements the LDfeature selection.
The calculation of term frequencyis done in constant memory by index inversionthrough a MapReduce-style sharding approach.
Thecalculation of information gain is also chunked tolimit peak memory use, and furthermore it is paral-lelized to make full use of modern multiprocessorsystems.
LDfeatureselect.py produces a listof byte n-grams ranked by their LD score.train.py implements estimation of parametersfor the multinomial naive Bayes model, as well asthe construction of the DFA for the Aho-Corasickstring matching algorithm.
Its input is a list of bytepatterns representing a feature set (such as that se-lected via LDfeatureselect.py), and a corpusof training documents.
It produces the final model asa single compressed, encoded string, which can besaved to an external file and used by langid.pyvia a command-line option.4 Training Datalangid.py is distributed with an embeddedmodel trained using the multi-domain languageidentification corpus of Lui and Baldwin (2011).This corpus contains documents in a total of 97 lan-guages.
The data is drawn from 5 different do-mains: government documents, software documen-tation, newswire, online encyclopedia and an inter-net crawl, though no domain covers the full set oflanguages by itself, and some languages are presentonly in a single domain.
More details about this cor-pus are given in Lui and Baldwin (2011).2http://www.wsgi.orgWe do not perform explicit encoding detection,but we do not assume that all the data is in the sameencoding.
Previous research has shown that explicitencoding detection is not needed for language iden-tification (Baldwin and Lui, 2010).
Our training dataconsists mostly of UTF8-encoded documents, butsome of our evaluation datasets contain a mixtureof encodings.5 EvaluationIn order to benchmark langid.py, we carried outan empirical evaluation using a number of language-labelled datasets.
We compare the empirical resultsobtained from langid.py to those obtained fromother language identification toolkits which incor-porate a pre-trained model, and are thus usable off-the-shelf for language identification.
These tools arelisted in Table 3.5.1 Off-the-shelf LangID toolsTextCat is an implementation of the method ofCavnar and Trenkle (1994) by Gertjan van Noord.It has traditionally been the de facto LangID tool ofchoice in research, and is the basis of language iden-tification/filtering in the ClueWeb09 Dataset (Callanand Hoy, 2009) and CorpusBuilder (Ghani et al,2004).
It includes support for training with user-supplied data.LangDetect implements a Naive Bayes classi-fier, using a character n-gram based representationwithout feature selection, with a set of normaliza-tion heuristics to improve accuracy.
It is trained ondata from Wikipedia,3 and can be trained with user-supplied data.CLD is a port of the embedded language identi-fier in Google?s Chromium browser, maintained byMike McCandless.
Not much is known about theinternal design of the tool, and there is no supportprovided for re-training it.The datasets come from a variety of domains,such as newswire (TCL), biomedical corpora(EMEA), government documents (EUROGOV, EU-ROPARL) and microblog services (T-BE, T-SC).
Anumber of these datasets have been previously usedin language identification research.
We provide a3http://www.wikipedia.org27Test Dataset langid.py LangDetect TextCat CLDAccuracy docs/s ?Acc Slowdown ?Acc Slowdown ?Acc SlowdownEUROGOV 0.987 70.5 +0.005 1.1?
?0.046 31.1?
?0.004 0.5?TCL 0.904 185.4 ?0.086 2.1?
?0.299 24.2?
?0.172 0.5?WIKIPEDIA 0.913 227.6 ?0.046 2.5?
?0.207 99.9?
?0.082 0.9?EMEA 0.934 7.7 ?0.820 0.2?
?0.572 6.3?
+0.044 0.3?EUROPARL 0.992 294.3 +0.001 3.6?
?0.186 115.4?
?0.010 0.2?T-BE 0.941 367.9 ?0.016 4.4?
?0.210 144.1?
?0.081 0.7?T-SC 0.886 298.2 ?0.038 2.9?
?0.235 34.2?
?0.120 0.2?Table 2: Comparison of standalone classification tools, in terms of accuracy and speed (documents/second), relativeto langid.pyTool Languages URLlangid.py 97 http://www.csse.unimelb.edu.au/research/lt/resources/langid/LangDetect 53 http://code.google.com/p/language-detection/TextCat 75 http://odur.let.rug.nl/vannoord/TextCat/CLD 64+ http://code.google.com/p/chromium-compact-language-detector/Table 3: Summary of the LangID tools comparedbrief summary of the characteristics of each datasetin Table 1.The datasets we use for evaluation are differ-ent from and independent of the datasets fromwhich the embedded model of langid.py wasproduced.
In Table 2, we report the accuracy ofeach tool, measured as the proportion of documentsfrom each dataset that are correctly classified.
Wepresent the absolute accuracy and performance forlangid.py, and relative accuracy and slowdownfor the other systems.
For this experiment, we useda machine with 2 Intel Xeon E5540 processors and24GB of RAM.
We only utilized a single core, asnone of the language identification tools tested areinherently multicore.5.2 Comparison on standard datasetsWe compared the four systems on datasets used inprevious language identification research (Baldwinand Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), aswell as an extract from a biomedical parallel cor-pus (Tiedemann, 2009) (EMEA) and a corpus ofsamples from the Europarl Parallel Corpus (Koehn,2005) (EUROPARL).
The sample of EUROPARLwe use was originally prepared by Shuyo Nakatani(author of LangDetect) as a validation set.langid.py compares very favorably with otherlanguage identification tools.
It outperformsTextCat in terms of speed and accuracy on all ofthe datasets considered.
langid.py is generallyorders of magnitude faster than TextCat, but thisadvantage is reduced on larger documents.
This isprimarily due to the design of TextCat, which re-quires that the supplied models be read from file foreach document classified.langid.py generally outperformsLangDetect, except in datasets derived fromgovernment documents (EUROGOV, EUROPARL).However, the difference in accuracy betweenlangid.py and LangDetect on such datasetsis very small, and langid.py is generally faster.An abnormal result was obtained when testingLangDetect on the EMEA corpus.
Here,LangDetect is much faster, but has extremelypoor accuracy (0.114).
Analysis of the results re-veals that the majority of documents were classifiedas Polish.
We suspect that this is due to the earlytermination criteria employed by LangDetect,together with specific characteristics of the corpus.TextCat also performed very poorly on thiscorpus (accuracy 0.362).
However, it is importantto note that langid.py and CLD both performedvery well, providing evidence that it is possible tobuild a generic language identifier that is insensitiveto domain-specific characteristics.langid.py also compares well with CLD.
It isgenerally more accurate, although CLD does bet-ter on the EMEA corpus.
This may reveal someinsight into the design of CLD, which is likely tohave been tuned for language identification of web28pages.
The EMEA corpus is heavy in XML markup,which CLD and langid.py both successfully ig-nore.
One area where CLD outperforms all other sys-tems is in its speed.
However, this increase in speedcomes at the cost of decreased accuracy in other do-mains, as we will see in Section 5.3.5.3 Comparison on microblog messagesThe size of the input text is known to play a sig-nificant role in the accuracy of automatic languageidentification, with accuracy decreasing on shorterinput documents (Cavnar and Trenkle, 1994; Sibunand Reynar, 1996; Baldwin and Lui, 2010).Recently, language identification of short stringshas generated interest in the research community.Hammarstrom (2007) described a method that aug-mented a dictionary with an affix table, and tested itover synthetic data derived from a parallel bible cor-pus.
Ceylan and Kim (2009) compared a number ofmethods for identifying the language of search en-gine queries of 2 to 3 words.
They develop a methodwhich uses a decision tree to integrate outputs fromseveral different language identification approaches.Vatanen et al (2010) focus on messages of 5?21characters, using n-gram language models over datadrawn from UDHR in a naive Bayes classifier.A recent application where language identifica-tion is an open issue is over the rapidly-increasingvolume of data being generated by social media.Microblog services such as Twitter4 allow users topost short text messages.
Twitter has a worldwideuser base, evidenced by the large array of languagespresent on Twitter (Carter et al, to appear).
It is es-timated that half the messages on Twitter are not inEnglish.
5This new domain presents a significant challengefor automatic language identification, due to themuch shorter ?documents?
to be classified, and iscompounded by the lack of language-labelled in-domain data for training and validation.
This has ledto recent research focused specifically on the task oflanguage identification of Twitter messages.
Carteret al (to appear) improve language identification inTwitter messages by augmenting standard methods4http://www.twitter.com5http://semiocast.com/downloads/Semiocast_Half_of_messages_on_Twitter_are_not_in_English_20100224.pdfwith language identification priors based on a user?sprevious messages and by the content of links em-bedded in messages.
Tromp and Pechenizkiy (2011)present a method for language identification of shorttext messages by means of a graph structure.Despite the recently published results on languageidentification of microblog messages, there is nodedicated off-the-shelf system to perform the task.We thus examine the accuracy and performance ofusing generic language identification tools to iden-tify the language of microblog messages.
It is im-portant to note that none of the systems we test havebeen specifically tuned for the microblog domain.Furthermore, they do not make use of any non-textual information such as author and link-basedpriors (Carter et al, to appear).We make use of two datasets of Twitter messageskindly provided to us by other researchers.
The firstis T-BE (Tromp and Pechenizkiy, 2011), which con-tains 9659 messages in 6 European languages.
Thesecond is T-SC (Carter et al, to appear), which con-tains 5000 messages in 5 European languages.We find that over both datasets, langid.py hasbetter accuracy than any of the other systems tested.On T-BE, Tromp and Pechenizkiy (2011) reportaccuracy between 0.92 and 0.98 depending on theparametrization of their system, which was tunedspecifically for classifying short text messages.
Inits off-the-shelf configuration, langid.py attainsan accuracy of 0.94, making it competitive withthe customized solution of Tromp and Pechenizkiy(2011).On T-SC, Carter et al (to appear) report over-all accuracy of 0.90 for TextCat in the off-the-shelf configuration, and up to 0.92 after the inclusionof priors based on (domain-specific) extra-textualinformation.
In our experiments, the accuracy ofTextCat is much lower (0.654).
This is becauseCarter et al (to appear) constrained TextCat tooutput only the set of 5 languages they considered.Our results show that it is possible for a generic lan-guage identification tool to attain reasonably highaccuracy (0.89) without artificially constraining theset of languages to be considered, which corre-sponds more closely to the demands of automaticlanguage identification to real-world data sources,where there is generally no prior knowledge of thelanguages present.29We also observe that while CLD is still the fastestclassifier, this has come at the cost of accuracy in analternative domain such as Twitter messages, whereboth langid.py and LangDetect attain betteraccuracy than CLD.An interesting point of comparison between theTwitter datasets is how the accuracy of all systemsis generally higher on T-BE than on T-SC, despitethem covering essentially the same languages (T-BEincludes Italian, whereas T-SC does not).
This islikely to be because the T-BE dataset was producedusing a semi-automatic method which involved alanguage identification step using the method ofCavnar and Trenkle (1994) (E Tromp, personal com-munication, July 6 2011).
This may also explainwhy TextCat, which is also based on Cavnar andTrenkle?s work, has unusually high accuracy on thisdataset.6 ConclusionIn this paper, we presented langid.py, an off-the-shelf language identification solution.
We demon-strated the robustness of the tool over a range of testcorpora of both long and short documents (includingmicro-blogs).AcknowledgmentsNICTA is funded by the Australian Government as rep-resented by the Department of Broadband, Communica-tions and the Digital Economy and the Australian Re-search Council through the ICT Centre of Excellence pro-gram.ReferencesAlfred V. Aho and Margaret J. Corasick.
1975.
Efficientstring matching: an aid to bibliographic search.
Com-munications of the ACM, 18(6):333?340, June.Timothy Baldwin and Marco Lui.
2010.
Language iden-tification: The long and the short of the matter.
In Pro-ceedings of NAACL HLT 2010, pages 229?237, LosAngeles, USA.Jamie Callan and Mark Hoy, 2009.
ClueWeb09Dataset.
Available at http://boston.lti.cs.cmu.edu/Data/clueweb09/.Simon Carter, Wouter Weerkamp, and Manos Tsagkias.to appear.
Microblog language identification: Over-coming the limitations of short, unedited and idiomatictext.
Language Resources and Evaluation Journal.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of theThird Symposium on Document Analysis and Informa-tion Retrieval, Las Vegas, USA.Hakan Ceylan and Yookyung Kim.
2009.
Languageidentification of search engine queries.
In Proceedingsof ACL2009, pages 1066?1074, Singapore.George Forman.
2003.
An Extensive Empirical Studyof Feature Selection Metrics for Text Classification.Journal of Machine Learning Research, 3(7-8):1289?1305, October.Rayid Ghani, Rosie Jones, and Dunja Mladenic.
2004.Building Minority Language Corpora by Learning toGenerate Web Search Queries.
Knowledge and Infor-mation Systems, 7(1):56?83, February.Harald Hammarstrom.
2007.
A Fine-Grained Model forLanguage Identication.
In Proceedings of iNEWS07,pages 14?20.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
MT summit, 11.Marco Lui and Timothy Baldwin.
2011.
Cross-domainfeature selection for language identification.
In Pro-ceedings of 5th International Joint Conference on Nat-ural Language Processing, pages 553?561, ChiangMai, Thailand.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for Naive Bayes text classifi-cation.
In Proceedings of the AAAI-98 Workshop onLearning for Text Categorization, Madison, USA.J.R.
Quinlan.
1986.
Induction of Decision Trees.
Ma-chine Learning, 1(1):81?106, October.Penelope Sibun and Jeffrey C. Reynar.
1996.
Languagedetermination: Examining the issues.
In Proceedingsof the 5th Annual Symposium on Document Analysisand Information Retrieval, pages 125?135, Las Vegas,USA.Jo?rg Tiedemann.
2009.
News from OPUS - A Collectionof Multilingual Parallel Corpora with Tools and Inter-faces.
Recent Advances in Natural Language Process-ing, V:237?248.Erik Tromp and Mykola Pechenizkiy.
2011.
Graph-Based N-gram Language Identification on Short Texts.In Proceedings of Benelearn 2011, pages 27?35, TheHague, Netherlands.Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.2010.
Language identification of short text segmentswith n-gram models.
In Proceedings of LREC 2010,pages 3423?3430.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
InProceedings of ICML 97.30
