Extension of Zipf?s Law to Words and PhrasesLe Quan Ha, E. I. Sicilia-Garcia, Ji Ming, F. J. SmithSchool Computer ScienceQueen?s University of BelfastBelfast BT7 1NN, Northern Irelandq.le@qub.ac.ukAbstractZipf?s law states that the frequency ofword tokens in a large corpus of naturallanguage is inversely proportional to therank.
The law is investigated for twolanguages English and Mandarin and for n-gram word phrases as well as for singlewords.
The law for single words is shownto be valid only for high frequency words.However, when single word and n-gramphrases are combined together in one listand put in order of frequency the combinedlist follows Zipf?s law accurately for allwords and phrases, down to the lowestfrequencies in both languages.
The Zipfcurves for the two languages are thenalmost identical.1.
IntroductionThe law discovered empirically by Zipf(1949) for word tokens in a corpus states that iff is the frequency of a word in the corpus and ris the rank, then:rkf =  (1)where k is a constant for the corpus.
Whenlog(f) is drawn against log(r) in a graph (whichis often called a Zipf curve), a straight line isobtained with a slope of ?1.
An example with asmall corpus of 250,000 tokens is given inFigure 1.
Zipf?s discovery was followed by alarge body of literature reviewed in a series ofpapers edited by Guiter and Arapov (1982).
Itcontinues to stimulate interest today(Samuelson, 1996; Montermurro, 2002; Ferrerand Sol?, 2002) and, for example, it has beenapplied to citations Silagadze (1997) and toDNA sequences (Yonezawa & Motohasi, 1999;Li, 2001).1101001000100001000001 10 100 1000 10000 100000log rankFigure 1 Zipf curve for the unigrams extracted from a250,000 word tokens corpusZipf discovered the law by analysingmanually the frequencies of words in the novel?Ulysses?
by James Joyce.
It contains avocabulary of 29,899 different word typesassociated with 260,430 word tokens.Following its discovery in 1949,several experiments aided by the appearance ofthe computer in the 1960?s, confirmed that thelaw was correct for the small corpora whichcould be processed at that time.
The slope ofthe curve was found to vary slightly from ?1for some corpora; also the frequencies for thehighest ranked words sometimes deviatedslightly from the straight line, which suggestedseveral modifications of the law, and inparticular one due to Mandelbrot (1953):??
)( += rkf  (2)where ?
and ?
are constants for the corpusbeing analysed.
However, generally theconstants ?
and ?
were found to be only smallstatistical deviations from the original law byZipf (exceptions are legal texts which havesmaller ?
values (?0.9) showing that lawyersuse more words than other people!
)(Smith &Devine, 1985).A number of theoretical developmentsof Zipf?s law had been derived in the 50?s and60?s and have been reviewed by Fedorowicz(1982), notably those due to Mandelbrot (1954,1957) and Booth (1967).
A well-knownderivation, due to Simon (1955), is based onempirically derived distribution functions.However, Simon?s derivation was controversialand a correspondence in the scientific pressdeveloped between Mandelbrot and Simon onthe validity of this derivation (1959-1961); thedispute was not resolved by the time Zipfcurves for larger corpora were beginning to becomputed.The processing of larger corpora with 1million words or more was facilitated by thedevelopment of PC?s in the 1980?s.
When Zipfcurves for these corpora were drawn they werefound to drop below the Zipf straight line withslope of ?1 at the bottom of the curve, startingfor rank greater than about 5000.
This isillustrated in Figure 2 which shows the Zipfcurve for the Brown corpus of 1 million wordsof American English (Francis & Kucera, 1964).1101001000100001000001 10 100 1000 10000 100000log rankFigure 2 Zipf curve for the unigrams extracted fromthe 1 million words of the Brown corpusThis appeared to confirm the opinion ofthe opponents of Simon?s derivation: the lawclearly did not hold for r>5000; so it appearedthat the derivation must be invalid.2.
Zipf Curves for Large CorporaThis paper is principally concernedwith exploring the above invalidity of Zipf?slaw for large corpora in two languages, Englishand Mandarin.
We begin with English.English corporaThe English corpora used in ourexperiments are taken from the Wall Streetjournal (Paul & Baker, 1992) for 1987, 1988,1989, with sizes approximately 19 million, 16million and 6 million tokens respectively.
TheZipf curves for the 3 corpora are shown inFigure 3.11010010001000010000010000001 10 100 1000 10000 100000 1000000log rankWSJ87 1-gramWSJ88 1-gramWSJ89 1-gramFigure 3 Zipf curves for the unigrams extracted fromthe 3 training corpora of the WSJThe curves are parallel, showingsimilar structures and all 3 deviating fromZipf?s law for larger r. Their separation is dueto their different sizes.Language is not made of individualwords but also consists of phrases of 2, 3 andmore words, usually called n-grams for n=2, 3,etc.
For each value of n between 2 and 5, wecomputed the frequencies of all n-gram in eachcorpus and put them in rank order as we haddone for the words.
This enabled us to draw theZipf curves for 2-grams to 5-grams which areshown along with the single word curves inFigures 4, 5 and 6 for the three corpora.
Thesecurves are similar to the first Zipf curves drawnfor n-grams by Smith and Devine (1985); butthese earlier curves were for a much smallercorpus.1101001000100001000001000000log rank1-gram2-gram3-gram4-gram5-gramFigure 4 Zipf curves for the WSJ87 corpus1101001000100001000001000000log rank1-gram2-gram3-gram4-gram5-gramFigure 5 Zipf curves for the WSJ88 corpus1101001000100001000001000000log rank1-gram2-gram3-gram4-gram5-gramFigure 6 Zipf curves for the WSJ89 corpusThe n-gram Zipf curves approximatelyfollow straight lines and can be represented bya single Mandelbrot form:?rkf =  (3)where ?
is the magnitude of the negative slopeof each line.
We found the values of ?
for theWSJ in Table 1.Table 1 Slopes for best-fit straight lineapproximations to the Zipf curvesWSJ87 WSJ88 WSJ89 Mandarin2-gram 0.67 0.66 0.65 0.753-gram 0.51 0.50 0.46 0.594-gram 0.42 0.42 0.39 0.535-gram 0.42 0.41 0.34 0.48Note that the unigram curves crosses thebigram curves when the rank ?
3000 in all threecases.The ten most common words, bigramsand trigrams in the combined WSJ corpus of 40million words are listed in Table 2.Mandarin corporaThe Mandarin corpus used in our experimentsis the TREC Corpus.
It was obtained from thePeople?s Daily Newspaper from 01/1991 to12/1993 and from the Xinhua News Agency for04/1994 to 09/1995 from the Linguistic DataConsortium (http://www.ldc.upenn.edu).
TREChas  19,546,872  tokens  similar in size to thelargest of the English corpora.
The Mandarinlanguage is a syllable-class language, in whicheach syllable is at the same time a word and aChinese character.
Other words, compoundwords, are built up by combining syllablestogether, similar to word n-grams in English.The most common unigrams, bigrams andtrigrams are listed in Table 3.The number of syllable-types (i.e.unigrams) in the TREC corpus is  only  6,300,very different from English (the WSJ87 corpushas 114,718 word types); so it is not surprisingthat the Zipf curve for unigrams in Mandarin inFigure 7 is very different from the Zipf curvefor unigrams in English.
It is similar to aprevious curve for a smaller Mandarin corpusof 2,022,604 tokens by Clark, Lua andMcCallum (1986).
The Zipf curves for n-gramsTable 2 The 10-highest frequency unigrams, bigrams and trigrams in the WSJ corpusUnigrams Bigrams TrigramsFrequency Token Frequency Token Frequency Token2057968973650940525853342825489711462368012362771298646281190THEOFTOAANDINTHATFORONEIS21742717379711029189184837997618772312655656383855014OF THEIN THEMILLION DOLLARSU.
S.NINETEEN EIGHTYFOR THETO THEON THEONE HUNDREDTHAT THE42030272602416518233167861531614943145171232711981THE U. S.IN NINETEEN EIGHTYCENTS A SHARENINETEEN EIGHTY SIXNINETEEN EIGHTY SEVENFIVE MILLION DOLLARSMILLION DOLLARS ORMILLION DOLLARS ININ NEW YORKA YEAR EARLIERTable 3 The 10-highest frequency unigrams, bigrams and trigrams in the Mandarin TREC corpus.for the Mandarin corpus are also shown inFigure 7.1101001000100001000001000000log rank1-gram2-gram3-gram4-gram5-gramFigure 7 Zipf curves for the TREC Mandarin corpusExcept for the unigrams, the shapes ofthe other TREC n-gram Zipfian curves aresimilar to but not quite the same as those forthe  English corpora.
In particular  the   bigramcurve  for  Mandarin  is  more  curved  than  theEnglish curve because there are morecompound words in Mandarin than English.The Mandarin ?-values in Table 1 arealso higher than for English, on average byabout 0.1, which is due to the differentdistribution of unigrams.
For TREC, thecrossing point between the unigram curve andthe bigram curve is at rank: 1224, frequency:1750, unigram: " ", bigram: " ".
Theunigram curve and the trigram curve cross eachother at rank: 1920, frequency: 491, unigram:" ", trigram: " ".
This is verydifferent from English.Comparisons between the n-gramscurves (n = 1 to 4) for English and Mandarinare made in Figure 8, 9, 10 and 11.
The Englishcurves are for the 3 WSJ corpora joinedtogether making a 40 million word corpus.110100100010000100000100000010000000log rankWSJ-UnigramsTREC-UnigramsFigure 8 Zipf curve for the unigrams for the WSJEnglish and TREC Mandarin corpora1101001000100001000001000000log rankWSJ-2-gramTREC-2-gramFigure 9 Zipf curve for the bigrams for the WSJEnglish and TREC Mandarin corpora110100100010000100000log rankWSJ-3-gramTREC-3-gramFigure 10 Zipf curve for the trigrams for WSJ Englishand TREC Mandarin corpora110100100010000100000log rankWSJ-4gramsTREC-4gramsFigure 11 Zipf curve for the 4-grams for the WSJEnglish and TREC Mandarin corpora3.
Combined n-gramsThe derivation of Zipf?s law by Simonwas based solely on single words and it failedfor English when the number of word typeswas greater than about 5000 words.
InMandarin it failed almost immediately forunigrams because of the limited number ofcharacters.
However it might not have failed ifthe Mandarin compound words in the bigram,trigram and higher n-gram statistics had beenincluded; this suggested that the n-gram andunigram curves should be combined.
Perhapsthe same may be true for English.
So we shouldcombine the English curves also.This can be justified in another way.
Ina critical part of his derivation Simon gives aninitial probability to a new word found in thecorpus as it introduces some new meaning notexpressed by previous words.
However, as thenumber of words increases new ideas arefrequently expressed not in single words, but inmulti-word phrases or compound words.
Thiswas left out of Simon?s derivation.
If he hadincluded it, the law he derived would haveincluded phrases as well as words.
So perhapsZipf?s law should include words and phrases.We therefore put all unigram and n-gram together with their frequencies into onelarge file, sorted on frequency and put in rankorder as previously.
The resulting Zipf curvefor the combined curves for both English andMandarin are shown in Figure 12.110100100010000100000100000010000000log rankWSJTRECFigure 12 Combined Zipf curves for the English WSJand the TREC Mandarin corporaThis shows that the n-grams (n > 2)exactly make up for the deviation of the twovery different unigram curves from Zipf?s lawand the combined curves for both languages arestraight lines with slopes close to ?1 for allranks>100.
This result appears to vindicateSimon?s derivation.
However, whether Simon?sderivation is entirely valid or not, the results inFigure 12 are a new confirmation of Zipf?soriginal law in an extended form.
Thisremarkable result has been found to be valid for3 other natural languages: Irish, Latin andVietnamese, in preliminary experiments.ReferencesBooth, A. D. (1967) ?A Law of Occurrences for Wordsof Low Frequency?.
Inform.
& Control Vol.
10, No.4, pp 386-393.
April.Clark, J. L., Lua, K. T. & McCallum, J.
(1986).
?UsingZipf's Law to Analyse the Rank FrequencyDistribution of Elements in Chinese Text?.
In Proc.Int.
Conf.
on Chinese Computing, pp.
321-324.August, Singapore.Fedorowicz, J.
(1982) ?A Zipfian Model of anAutomatic Bibliographic System: an Application toMEDLINE", Journal of American Society ofInformation Science, Vol.
33, pp 223-232.Ferrer Cancho, R. & Sol?, R. V. (2002)  ?Two Regimesin the Frequency of Words and the Origin of ComplexLexicons?
To appear in Journal of QuantitativeLinguistics.Francis, W. N. & Kucera, H. (1964).
?Manual ofInformation to Accompany A Standard Corpus ofPresent-Day Edited American English, for use withDigital Computers?
Department of Linguistics,Brown University, Providence, Rhode IslandGuiter H. & Arapov M., editors.
(1982) "Studies onZipf 's Law".
Brochmeyer, Bochum.Li, W. (2001) ?Zipf's Law in Importance of Genes forCancer Classification Using Microarray Data?
Labof Statistical Genetics, Rockefeller University, NY.Mandelbrot, B.
(1953).
?An Information Theory of theStatistical Structure of Language?.
CommunicationTheory, ed.
By Willis Jackson, pp 486-502.
NewYork: Academic Press.Mandelbrot, B.
(1954) ?Simple Games of StrategyOccurring in Communication through NaturalLanguages?.
Transactions of the IRE ProfessionalGroup on Information Theory, 3, 124-137Mandelbrot, B.
(1957) ?A probabilistic Union Modelfor Partial and temporal Corruption of Speech?.Automatic Speech Recognition and UnderstandingWorkshop.
Keystone, Colorado, December.Mandelbrot, B.
(1959) "A note on a class of skewdistribution function analysis and critique of a paperby H.A.
Simon", Inform.
& Control, Vol.
2, pp 90-99.Mandelbrot, B.
(1961) "Final note on a class of skewdistribution functions: analysis and critique of amodel due to H.A.
Simon", Inform.
& Control, Vol.
4,pp 198-216.Mandelbrot, B.
B.
(1961) "Post Scriptum to 'finalnote'", Inform.
& Control, Vol.
4, pp 300-304.Montemurro, M. (2002) ?Beyond the Zipf-MandelbrotLaw in Quantitative Linguistics?.
To appear inPhysica A.Paul, D. B.
& Baker, J.M.
(1992) ?The Design for theWall Street Journal-based CSR Corpus?, Proc.ICSLP 92, pp 899-902, November.Samuelson, C. (1996).
?Relating Turing's Formula andZipf's Law?.
Proceedings of the 4th Workshop onVery Large Corpora, Copenhagen, Denmark.Silagadze, Z. K. (1997) ?Citations and the Zipf-Mandelbrot Law?.
Complex Systems, Vol.
11, No.6, pp 487-499.Simon, H. A.
(1955) "On a Class of Skew DistributionFunctions", Biometrika, Vol.
42, pp 425-440.Simon, H. A.
(1960) "Some Further Notes on a Classof Skew Distribution Functions", Inform.
& Control,Vol.
3, pp 80-88.Simon, H. A.
(1961) "Reply to Dr. Mandelbrot's postScriptum" Inform.
& Control, Vol.
4, pp 305-308.Simon, H. A.
(1961) "Reply to 'final note' by BenoitMandelbrot", Inform.
& Control, Vol.
4, pp 217-223.Smith, F. J.
& Devine, K. (1985) ?Storing andRetrieving Word Phrases?
Information Processing &Management, Vol.
21, No.
3, pp 215-224.Yonezawa, Y.
& Motohasi, H. (1999) ?Zipf-ScalingDescription in the DNA Sequence?
10th Workshop onGenome Informatics.
Japan.Zipf, G. K. (1949) ?Human Behaviour and thePrinciple of Least Effort?
Reading, MA: Addison-Wesley Publishing co.
