PARSING AND SYNTACTIC THEORYSummaryIt is argued that many constraints on syntacticrules are a consequence of simple assumptionsabout parsing mechanisms.
If generally true,this suggests an interesting new line of re-search for syntactic theory.Recent syntactic theory has aimed for atheory of grammar rich enough to be descrip-tively adequate, but restricted enough to makeplausible claims about the acquisition of 3suchsystems.
Thus in Chomsky's trace theory,sentences, their constituent structures andgrammatical relations are determined by a fewsimple rules which in themselves would generatemany non-sentences, but which are constrainedby putatively universal principles marking thenon-sentences as such.
These principles areattributed to the human language faculty, butwith no direct implications for models of per-ception or production,involving as they do manyabstract concepts (empty nodes, traces etc.)
forwhich there is no obvious psychological inter-pretation.Bresnan I presents an interesting alter-native: that proposals about the nature ofgrammar should be responsible to psycholinguis-tic evidence--to put it crudely, grammarsshould be 'psychologically real'.
By thiscriterion, many constructions such as passivesor infinitive phrases previously regarded astransformational are to be seen as generateddirectly by phrase-structure rules, withassociated conditions on lexical insertion andrelationships between lexical entries.
Trans-formations for which there appears to be psy-cholinguistic evidence, like Wh-movement ll,remain, though under a different formulationthan they receive under trace theory.Taken literally, the requirement of strongpsychological reality for grammars makes nosense: I shall take it to mean that such agrammar is one which can be incorporated withlittle or no adjustment into a model of per-ception or production, and that the varioussubcomponents and divisions of labour in thatmodel are both consistent with those implied bythe grammar and not disconfirmed by experimentalevidence.
What sort of parsing model is impliedby Bresnan's theory?
Bresnan herself envisagesrealistic grammar being implemented directly asan Augmented Transition Network 1,11 with aHOLD analysis accounting for Wh-phrases.
PSrules correspond directly to sections of net-work, and HOLD is Wh-movement in reverse, so tospeak.
However, there are at least two de-ficiencies in such a proposal: firstly, in allATN models yet proposed, there are many pointsS.G.
PulmanUniversity of East Anglia,Norwich NR4 7TJ, England.where exits from a state are tried in a fixed,often arbitrary, order.
As a claim about howhumans parse this is desperately implausible,for it suggests that at such points we expectan A, look at the next word, find that it isnot a possibility for A, revise our expectationto B, find that it is not ... until we have anexpectation confirmed or arrive at a defaultvalue.
There may be some points at which sucha situation occurs but in general it does notseem to: more plausible is that we look aheadto the next word without any particular expect-ation and use the resulting information todecide which exit to take--we go 'bottom-up'in other words.
Secondly, in the currentgeneration of ATN proposals there is little orno facility for revising syntactic expectationson the basis of lexical (much less other typesof semantic or pragmatic) information, thoughthere is ample intuitive and experimentalevidence that humans do this.
However, thisis probably a criticism of practice rather thanof principle.A 'realistic' ~parserA better picture of a parser is one whichworks at two levels simultaneously: the lowestlevel mostly bottom up, assembling phrase levelconstituents NP VP PP etc., possibly incorporat-ing the 'l~te closure' and 'minimal attachment'principles o of Fodor and Frazier.
The in-formation given by PS rules and the lexiconwould be available to it and guide its opera-tion but would not necessarily corresponddirectly to portions of a network.
The second,more 'top-down' routine would organise thesephrases into functionally complete units--roughly, a verb with its obligatory argumentsand any optional arguments or modifers whichmight be present.
This second stage useswhatever grammatical information is present inthe input (presence of Wh-phrases, complement-isers, passive morphology etc.)
along withlexical information (subcategorisation res-trictions, possibly semantic restrictions onarguments) to determine construction type andassign arguments to their grammatical (andpossibly semantic) functions.
It is alsoplausible, and in some cases necessary, toassume that it can guide the operation of thephrase level stage at various points.
(A twostage model along somewhat different lines hasalready been proposed: U in support of the claimthat functional completeness rather than sur-face clause segmentation or 'sentoids'~guidesassembly into units see Carroll et al2).This is not the place to go into detailsof implementation; though a preliminary version54of a parser embodying these principles has runwith some success, there is still much work todo.
The present aim is rather to show how sucha model could account for some of the range ofphenomena which linguists are currently interes-ted in and of which any theoretical approach mmust present a credible account.
In particu-lar we want to know whether anything over andabove the basic grammatical information re-presented in an extended lexicalist grammar ofthe type described by Bresnan will need to bemade available to the parser.To simplify matters, let us ignore mosttypes of complex sentence, and phenomena likeellipsis, anaphora, and conjunction.
Most ofthe simple sentence types of English can thenbe summarised as follows, ignoring details:(1) NP Ibe  Adj ~ I  IV (Prep) (NP)( ) (PP)Sentences which do not have any of thesestructures must have undergone grammaticaloperations of various types,which, using auseful earlier frameworkW we can describe as:(2) i Root transformations: Subject verbinversion, Topicalisation etc.ii Wh-movements: Questions, relatives etc.iii Equi-NP-deletion: infinitive verbphrases etc.iv Structure-preserving NP movements:Passive, Raising, Tough-movement etc.or some combination of these.As well as assigning (phrase level) con-stituent structure, our parser must of coursedetermine grammatical relations, both overt andcovert, in the course of building functionallycomplete units.
On sentences having any ofthe forms in (1) this is quite straightforward,as surface grammatical relations provide allthe information needed.
The following definit-ions will suffice:N~/u~es V (V in- (3) Subject (S) = tensec Auxiliaries)Direct Object (DO) = NP/V(Prep)Prepositional Object (~0)= NP~P-~--Notoriously, though, in sentences resultingfrom (2)i-iv overt grammatical relations areoften different from covert grammatical re-lations.
How would we deal with these?
Forconcreteness, and without any great warrant,let us assume that root transformations are'unwound', possibly by an extension of theHOLD mechanism for some cases, resulting instructures like (1).
(3) can then be usedon the result.
Similarly, let us assumethat something like the existing HOLD treatmentof Wh-movements ll is adequate: a Wh-phrase isput in HOLD until a gap is found, either anobligatory one--a tensed V with no subject, anunfilled subcategorisation slot or strandedpreposition; or an optional one--an optionalsubcategorisation slot or a modifier position.The Wh-phrase is then treated as if it were inthe gap and (3) can apply directly.
For thesentences in (iii) there is a simple, well-known generalisation:(4) The subject of an infinitive is the DO ofthe matrix verb if it has one, otherwiseits subject.The equally well known exceptions to this( ro~,  a ~ ,  the indirect question senseof ask etc.)
can be marked as such in thelexicon, as in all other treatments.
For thestructures in (iv), surface grammatical re-lations can be determined directly by (3)-Lexical information ~ la Bresnan is required todetermine their underlying grammatical relations,and in the case of passive, grammatical infor-mation can be used:(5) In a structure ... be-V-en/ed ... the(surface) S of be (by rule (3)) is the(underlying) ~f  V. The underlying S ofV is the PO of b~, if present (and a'possible agent'), or Indefinite (to bepragmatically interpreted) otherwise.This is essentially Bresnan's lexical redund-ancy rule.
(4) will account directly for subject-raised sentences:(6) John seems to like Billand (3) will account directly for theirputative source:(7) It seems that John likes Bill.Presumably some semantic statement about thei_~_of such extraposed structures will be needed,as in any account.
Other constructions arestraightforwardly dealt with by (3) and (4) withthe exception of Tough movements:(8) John is difficult to persuadeThe parser will go doubly wrong here, assigningJohn as S of persuade (by (4)) and marking thestring as ungrammatical because of an unfilledgap after the verb.
The obvious solution isto mark Tough movement adjectives lexically sothat when they appear in these contexts theycause the parser to put the subject (John)in HOLD, and assign (in this case) Indefiniteas the subject of persuade.
John will thenbe correctly inserted in the gap.
This 3treatment captures the many similarities tobe found between Tough and Wh-movement.55Some examples will make the operation of(3) - (5) clearer.
(9) The bo gave the book to MaryS 0?
po(i0) John wanted to try to leave(by 3)(by 3& 4)(ll) John wanted Bill to t r\[ to leave(by 3 & 4).ELL__J ) ~ s j .
I  sjT(12) John wanted to be ignored (by 3,4 & 5)h?
~ .
.
.
.
(13)W did John want Bill to try to be kissed by(by 3,4,5 and HOLD)We can define a 'functionally deviant' sentenceas one in which obligatory arguments to verbshave not been found by either stage of theparser (using lexical information, (3) - (5) andHOLD) or where phrases are 'left over', unableto be consistently assigned as arguments toverbs.
Such a definition follows naturallyfrom the operation of the parser.Trace theor2In Chomsky's trace theory 3 all major struct-ures are generated by base rules.
Rules in-serting lexical material are optional, and somerather simple transformational rules (Move NP,Move Wh-phrase) allow categories to be moved toany other (empty) slot of the same type, leav-ing behind a 'trace' which is interpreted asidentical in reference to the moved category.Traces are used to indicate underlying grammati-cal relations (of the sort dealt with in 4, 5and HOLD), to assign aspects of interpretation,and they also function to block the applicationof syntactic and (some) phonological rules as ifthey were terminal elements.
A sample deriva-tion is the following, where ~ = empty NP node:<14) i seemS Es B in  to like John7ii Bill seems t to like JohnBill has been moved by Move NP to fill the emptynode leaving behind a trace, ~ which is correct-ly interpreted as the subject of like, andidentical in reference to Bill.
If Move NPhad not applied (i) would have been markedungrammatical by a general convertion that nouninterpreted empty node can remain at the endof a derivation.
If Move NP had moved Johnto replace the empty node, the result:(15) John seems Bill to likewould have been declared ungrammatical sincesuch an application violates an allegedlyuniversal constraint, either at the stage atwhich John is moved or when an interpretationrule seeks to relate John to its trace.
Theconstraint in question is the Specified SubjectConstraint (SSC), and in the simplest cases itsays that no rule can relate or involve positionswhich are on either side of a full subject:(16) ... X ... \[sNP ... Y "''lA parallel mechanism, the Tensed SentenceConstraint (TSC), says that no rule can relatea position inside a tensed sentence to one out-side that sentence, i.e.
in (16), nothing canrelate Y or NP to X if S contains Tense.
Thiswould correctly rule out the final stage of thederivation:(17) i we believe Is the dog is hungry by~-\]ii t I is believed \[ the dog is hungry by usiii the dog is believed L t is hungry by usIf the embedded sentence had not contained aTense, i.e.
been ... to be ... the derivationwould have been permitted.Obviously this is a very simplified andincomplete account.
In particular it shouldbe noted that Wh-movement, which looks like anexception to SSC and TSC, is not, on Chomsky'saccount of it.
3 For our purposes, what it isimportant to focus on is that it is not therules of grammar themselves which characterisegrammaticality, for they will over-generatemassively, but the interaction of rules ofgrammar with constraints like TSC and SSC.In our framework, however, such constraintsare unnecessary.
Uninterpreted empty nodescorrespond to unfilled gaps, and will be reject-ed via the functional completeness requirement.Cases like (15) are ungrammatical on twogrounds: (3) automatically analyses Bill asDO of seems but this does not match any of itssubcategorisation entries and so the sentenceis rejected.
Furthermore, like is subcategor-ised for either a DO or an infinitive: neitheris present in (15) and so the sentence would56also be rejected on these grounds.
All thecases in (17) would be excluded because theycontain unfilled gaps (obviously traces and~are not present in the input string): in (i)there is a stranded preposition demanding a PO,while in (ii) and (iii) is (believed) and is(hungry) respectively cannot be assigned s~-jects by (3) since there is no overt NP, norby (4) since they are not infinitives.
Thusthey too are marked as functionally incomplete.As for as I can ascertain, all the syn-tactic effects of TSC and SSC as regards NPmovement can be accounted for in a similarfashion, using only that information alreadyrequired for the recognition of grammaticalsentences.
If this is true, it suggests astrong and interesting hypothesis: namelythat all the syntactic effects of all the con-straints proposed at various times would alsobe automatic consequences of the operation ofthis type of parser.
This has already beensuggested for particular instances, 7, 5 if itshould prove generally true this would be afascinating discovery.More constraintsAnother major constraint in Chomsky'sframework is known as Subjacency--the require-ment that no rules can relate positions separa-ted by two or more cyclic (i.e.
NP or S)boundaries.
The effects of Subjacency on NPmovement result in nothing not accounted forby the earlier discussion but whereas Wh-movement is not affected by SSC and TSC it isnot exempt from Subjacency.
Subjacency sub-sumes, as well as other principles, Ross's lOComplex Noun Phrase Constraint (CNPC), whichprohibits movements out o f~-NpNPS ~ structures,and some cases of the Co-ordi5~te StructureConstraint (CSC), which prohibits movementsout of conjoined structures.
These principlesaccount for the unacceptability of:(18) (i) *Who did John see the man that Bill hit-(ii) *Who did John believe the claim thatBill hit -(iii) *Who did John see - and BillUnlike TSC and SSC there is no chance of rulingout the sentences via subcategorisation res-trictions or 'functional deviance'--in otherwords we will have to rely on assumptionsabout the implementation of the parser.
Never-theless, proposals have been made which willcapture the effects of CNPC and CSC and whichare in accordance with the logic of our app-roach,namely, that the most straightforwardway of handling grammatical cases will auto-matically, without further stipulation, ruleout the ungrammatical cases.
As regards CSC,Winograd 12 has proposed that the simplest syn-tactic treatment of conjunctions is to inter-pret them as an instruction to look for asyntactic unit to the right of the conjunctionof the same type as one which has been recog-nised ending immediately to the left of it.This is equivalent to the simple and pervasivegeneralisation about conjunctions that theyconjoin like with like.
Thus (18)iii wouldbe ruled out because there is no NP immediatelypreceding and to match with Bill.
Thistreatment allows for 'across the board' move-ments:(19) What does Bill like - and John detest - ?without qualification.
Someelaboration hasto be made to allow for anaphorically inter-preted deleted elements when dealing with con-joined sentences but otherwise the principleseems essentially correct and is presumably a'parsing universal' of a substantive kind.The only workable suggestion concerning 9the CNPC that I am aware of comes from Ritchie.He proposed an analysis of relative clauses andother Wh-movements similar in spirit to theHOLD analysis.
In his programme, the Wh-phrase(presumably a copy of the head NP if no Wh-phrase is present) is stored in a registercalled WHSLOT which is a local variable.
Sinceit is a local variable, if the relative clauseprocedure is called recursively, the contents ofthe register on any earlier call are unavail-able to the current call.
As he points out,this is a simple and natural way of treatingany construction capable of unlimited embedding.The results of this treatment (exactly the sameas a HOLD list which is a push-down stack) ofrelative clauses is that in a sentence like(18)i, the analysis of the relative ... the manthat Bill hit ... will invoke a use of thecurrent incarnation of WHSLOT and will there-fore put that (or the head NP) into the gap itfinds after hit, assigning it DO of hit(assimilating Ritchie's analysis now to ours).The WHSLOT in which the first who was placedis no longer accessible.
(Alternatively, thewho on the HOLD list is not 'first out').Thus who could never be placed in the gap andthe sentence will be deemed functionallydeviant, since at the end of it there is stillan unassigned Wh-phrase: all similar caseslikewise fall out of this simple and natural(though refutable) principle concerning thetreatment of embedded structures involving Wh-movement.Cases like (18)ii are somewhat more com-plex.
Ritchie points out that at the pointat which the parser is awarethat it is parsinga complex NP structure, sentences like (18)iiare always potentially ambiguous, between atrue relative and a true complex NP reading:(19) i ... the claim that he madeii ... the claim that he made a mistakeIt seems plausible to assume that the parserpostpones a decision until the ambiguity is57resolved.
Again, the natural way to do thisis to store that (or a copy of the head NP)in WHSLOT or in HOLD, and insert it into a gapif one is found, confirming the expectation thatit is a relative clause, or revising the dec-ision ahmt construction type if no gap isfound.
Either way, no previously encounteredWh-phrase can be inserted into the gap, and sothe parser rules out (18)ii as functionallydeviant, on two counts in this particular in-stance: the claim will be marked as DO of hit,and presumably lexical information will showthat such an assignment of argument typeviolates semantic restrictions (you can't hitclaims).
Secondly, since the gap at the endof the sentence has been filled there is anunassigned Wh-phrase left over.A further syntactic constraint yet to beaccounted for is Ross's Left Branch Constraint(LBC).
This prohibits movement of a NP whichis the leftmost constituent of a longer NP.Most cases ruled out by this constraint fallunder the kind of treatment discussed earlierfor TSC and SSC but one class of cases, inwhich a determiner constituent is fronted,remain:(20) "Whose did you drink - beerThe acceptability of sentences like:(21) Whose - did you drink?where the object of the possessive determineris understood from context, suggests that whena possessive wh-phrase is encountered, if thereis no NP immediately following, a dummy isinserted, to be pragmatically interpreted.
In(20) no gap will be found, since beer can be DOof drink.
There will thus be an unassignedWh-phrase whose NP left over and the sentencewill be marked functionally deviant.Clearly, there is much work left to do inexploring the hypothesis that all such con-straints can be accounted for by no extramechanisms than are needed for grammaticalsentences.
Nevertheless, there seems ampleevidence that this is a worthwhile avenue topursue, and if it is true interesting questionscan be raised.
What, for example, is thestatus of TSC and SSC with respect to semanticrules?
In Chomsky's framework, rules inter-preting reciprocals (each other), reflexives,and pronouns are constrained by TSC and SSC:~ourselves(22) i We expeC~each other% to winha ~ourselves ~wi 1 ii ~We expect t t&each other~ 1 win(by TSC)~each other)iii "We expected Bill to warn~ourselves(by SSC)In our framework, specified subjects ofinfinitival sentences are regarded by (3) asdirect objects.
For sentences like (22),therefore, something similar to a clause-mateaccount will give the right results: reflex-ives and reciprocals must refer to a pre-ceding (agreeing) NP to which they are grammati-cally related via (3), (4) or (5).
For pro-nouns, the reverse is the case: they cannotrefer to a preceding NP to which they aregrammatically related.A preliminary attempt to extend such ananalysis has been made, ~ though it facesobvious problems with verbs like arrange,which can take a non-finite sentence as com-plement, and with non-finite relative clauses.If it can be carried through though, then thefact that the functional structures deliveredby the parser's second stage provide the rightkind of information to state such restrictionson would constitute further evidence that suchan approach is on the right lines.
Alter-natively, it might prove more accurate toretain TSC and SSC as purely semantic con-straints.Another question to be raised concerns therelationship between constraints, grammars andparsers.
For Chomsky, constraints are part ofthe theory of grammar and do not have to bestated for individual grammars (though somelanguage particular parameters might have tobe specified).
But there is a good sense inwhich the constraints proposed are arbitrary:it would be just as easy, for instance, tohave a Non-Tensed S Constraint, or a SpecifiedNon-Subject Constraint.
This fact is what~ives plausibility to Chomsky's claim that theconstraints are the result of as yet unknowninnate mechanisms.
But in our framework, ifit is anything like correct, a good deal ofthis arbitrariness is removed.
Our grammaruses only the same kind of distributionalinformation as is used in trace theory, and yetdoes not over-generate to quite the same extent.If we now implement this grammar inside aparser with only such additional information asis required to accept and functionally encodegrammatical sentences ((3)-(5), conjunctions,HOLD as a push down stack) it seems to auto-matically rule out ungrammatical sentences whichrequire constraints in the alternative frame-work.
If this proves to be generally true,the obvious next step is to devise psycho-logical tests to discover whether what seem tobe simple and natural assumptions about parsingare in fact an accurate description of thehuman parsing system.
The status of constraintschanges radically: instead of being abstractproperties of the language faculty to be in-vestigated by some future neurophysiologistthey constitute valuable evidence concerningthe operation of the human parsing system, andthe kind of hypothesis that can be advancedk58 rshould be accessible to more direct experimentaltesting.
This, it seems to me, would con-stitute considerable progress in our under-standing of these phenomena.FootnoteThis is a condensed version of a longer papercurrently under revision.
Some of the ideas 8are sk etched out in an earlier working paperand I would like to thank R.A. Hudson and G.Ritchie for their comments on that.
The usualdisclaimers apply.References(1) J.Bresnan 1978 'A realistic transformationalgrammar' in M.Halle et al, 'LinguisticTheory and Psychological Reality' M.I.T.Press, Cambridge and London pp.l-59.
(2) J. Carroll, M. Tanenhaus and T. Bever 1978'The perception of functional relations' inW.
Levelt, F. d'Arcais eds., 'Studies inthe perception of language' John Wiley andSons, Chichester and New York, pp.187-218.
(3) N. Chomsky 1977 'On Wh-movement' in P.Culicover et al 'Formal Syntax' AcademicPress, New York and London.
(4) J. Emonds 1976 'A transformational approachto Syntax' Academic Press, New York andLondon.
(5) J.D.Fodor 1978 'Parsing strategies andconstraints on transformations' LinguisticInquiry Vol.
8 1978 pp.427-473.
(6) J.D.Fodor and L. Frazier 1978 'The sausagemachine' cognition Vol.
6 1978, pp.291-325.
(7) M.Marcus 1977 'A theory of syntacticrecognition for natural language' mimeo,M.I.T.A.I.Lab.
(8) S.Pulman 1979 'Grammatical description andsentence processing' UEA Papers inLinguistics Vol.
lO, University of FEastAnglia, Norwich.
(9) G.Ritchie 1977 Edinburgh University Ph.D.Thesis.
(10) J.Ross 1967 'Constraints on variables insyntax' excerpted in 'On Noam Chomsky' ed.G.
Harman, Anchor Books 1974.
(ll) E. Wanner and M. Maratsos 1978 'Thecomprehension of relative clauses' in M.Halle et al, op cit.
pp.ll9-161.
(12) T. Winograd 1972 'Understanding naturallanguage' Edinburgh University Press.1 59- -
