Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204?215,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsAugmenting String-to-Tree Translation Models with Fuzzy Use ofSource-side SyntaxJiajun Zhang, Feifei Zhai and Chengqing ZongInstitute of Automation, Chinese Academy of SciencesBeijing, China{jjzhang, ffzhai, cqzong}@nlpr.ia.ac.cnAbstractDue to its explicit modeling of thegrammaticality of the output via target-sidesyntax, the string-to-tree model has beenshown to be one of the most successfulsyntax-based translation models.
However,a major limitation of this model is that itdoes not utilize any useful syntacticinformation on the source side.
In thispaper, we analyze the difficulties ofincorporating source syntax in a string-to-tree model.
We then propose a new way touse the source syntax in a fuzzy manner,both in source syntactic annotation and inrule matching.
We further explore threealgorithms in rule matching: 0-1 matching,likelihood matching, and deep similaritymatching.
Our method not only guaranteesgrammatical output with an explicit targettree, but also enables the system to choosethe proper translation rules via fuzzy use ofthe source syntax.
Our extensiveexperiments have shown significantimprovements over the state-of-the-artstring-to-tree system.1 IntroductionIn recent years, statistical translation models basedupon linguistic syntax have shown promisingprogress in improving translation quality.
Itappears that encoding syntactic annotations oneither side or both sides in translation rules canincrease the expressiveness of rules and canproduce more accurate translations with improvedreordering.One of the most successful syntax-based modelsis the string-to-tree model (Galley et al, 2006;Marcu et al, 2006; Shen et al, 2008; Chiang et al,2009).
Since it explicitly models thegrammaticality of the output via target-side syntax,the string-to-tree model (Xiao et al, 2010)significantly outperforms both the state-of-the-artphrase-based system Moses (Koehn et al, 2007)and the formal syntax-based system Hiero (Chiang,2007).
However, there is a major limitation in thestring-to-tree model: it does not utilize any usefulsource-side syntactic information, and thus to someextent lacks the ability to distinguish goodtranslation rules from bad ones.The source syntax is well-known to be helpful inimproving translation accuracy, as shownespecially by tree-to-string systems (Quirk et al,2005; Liu et al, 2006; Huang et al, 2006; Mi et al,2008; Zhang et al, 2009).
The tree-to-stringsystems are simple and efficient, but they also havea major limitation: they cannot guarantee thegrammaticality of the translation output becausethey lack target-side syntactic constraints.Thus a promising solution is to combine theadvantages of the tree-to-string and string-to-treeapproaches.
A natural idea is the tree-to-tree model(Ding and Palmer, 2005; Cowan et al, 2006; Liu etal., 2009).
However, as discussed by Chiang(2010), while tree-to-tree translation is indeedpromising in theory, in practice it usually ends upover-constrained.
Alternatively, Mi and Liu (2010)proposed to enhance the tree-to-string model withtarget dependency structures (as a language model).In this paper, we explore in the other direction:based on the strong string-to-tree model whichbuilds an explicit target syntactic tree duringdecoding rather than apply only a syntacticlanguage model, we aim to find a useful way toincorporate the source-side syntax.204First, we give a motivating example to show theimportance of the source syntax for a string-to-treemodel.
Then we discuss the difficulties ofintegrating the source syntax into the string-to-treemodel.
Finally, we propose our solutions.Figure 1 depicts a standard process thattransforms a Chinese string into an English treeusing several string-to-tree translation rules.
Thetree with solid lines is produced by the baselinestring-to-tree system.
Although the yield isgrammatical, the translation is not correct since thesystem mistakenly applies rule r2, thus translatingthe Chinese preposition ?
(h? )
in the examplesentence into the English conjunction and.
As aresult, the Chinese prepositional phrase ??
????
??
(?with terrorist networks?)
is wronglytranslated as a part of the relevant noun phrase(?
[Hussein] and terrorists networks?).
Why doesthis happen?
We find that r2 occurs 103316 timesin our training data, while r3 occurs only 1021times.
Thus, without source syntactic clues, theChinese word ?
(h ? )
is converted into theconjunction and in most cases.
In general, thisconversion is correct when the word?(h?)
is usedas a conjunction.
But?(h?)
is a preposition in thesource sentence.
If we are given this sourcesyntactic clue, rule r3 will be preferred.
Thisexample motivates us to provide a moderateamount of source-side syntactic information so asto obtain the correct English tree with dotted lines(as our proposed system does).A natural question may arise that is it easy toincorporate source syntax in the string-to-treemodel?
To the best of our knowledge, no one hasstudied this approach before.
In fact, it is not atrivial question if we look into the string-to-treemodel.
We find that the difficulties lie in at leastthree problems: 1) For a string-to-tree rule such asr6 in figure 1, how should we syntactically annotateits source string?
2) Given the source-annotatedstring-to-tree rules, how should we match theserules according to the test source tree duringdecoding?
3) How should we binarize the source-annotated string-to-tree rules for efficient decoding?For the first problem, one may require thesource side of a string-to-tree rule to be aconstituent.
However, such excessive constraintswill exclude many good string-to-tree rules whosesource strings are not constituents.
Inspired byChiang (2010), we adopt a fuzzy way to labelevery source string with the complex syntacticcategories of SAMT (Zollmann and Venugopal,2006).
This method leads to a one-to-onecorrespondence between the new rules and thestring-to-tree rules.
We will detail our fuzzylabeling method in Section 2.For the second problem, it appears simple andintuitive to match rules by requiring a rule?s sourcesyntactic category to be the same as the category ofthe test string.
However, this hard constraint willgreatly narrow the search space during decoding.Continuing to pursue the fuzzy methodology, weadopt a fuzzy matching procedure to enablematching of all the rules whose source stringsmatch the test string, and then determine thedegree of matching between the test source treeand each rule.
We will discuss three fuzzymatching algorithms, from simple to complex, inSection 3.The third question is a technical problem, andwe will give our solution in Section 4.Our method not only guarantees thegrammaticality of the output via the target treestructure, but also enables the system to chooseappropriate translation rules during decodingthrough source syntactic fuzzy labeling and fuzzymatching.The main contributions of this paper are asfollows:1) We propose a fuzzy method for both sourcesyntax annotation and rule matching foraugmenting string-to-tree models.2) We design and investigate three fuzzy rulematching algorithms: 0-1 matching,likelihood matching, and deep similaritymatching.We hope that this paper will demonstrate how toeffectively incorporate both source and targetsyntax into a translation model with promisingresults.2 Rule ExtractionSince we annotate the source side of each string-to-tree rule with source parse tree information in afuzzy way, we will henceforward denote thesource-syntax-decorated string-to-tree rule as afuzzy-tree to exact-tree rule.
We first brieflyreview issues of string-to-tree rule extraction; thenwe discuss how to augment the string-to-tree rulesto yield fuzzy-tree to exact-tree rules.205Figure 1:  Two alternative derivations for a sample string-to-tree translation.
The rules used are listed on the right.The target yield of the tree with solid lines is hussein and terrorist networks established relations.
The target yieldof the tree with dotted lines is hussein established relations with terrorist networks.2.1 String-to-Tree Rule ExtractionGalley et al (2004) proposed the GHKM algorithmfor extracting (minimal) string-to-tree translationrules from a triple (f, et, a), where f is the source-language sentence, et is a target-language parse treewhose yield e is the translation of f, and a is the setof word alignments between e and f. The basic ideaof GHKM is to obtain the set of minimally-sizedtranslation rules which can explain the mappingsbetween source string and target parse tree.
Theminimal string-to-tree rules are extracted in threesteps: (1) frontier set computation; (2)fragmentation; and (3) extraction.The frontier set (FS) is the set of potential pointsat which to cut the graph G constructed by thetriple (f, et, a) into fragments.
A node satisfying theword alignment is a frontier.
Bold italic nodes inthe English parse tree in Figure 2 are all frontiers.Given the frontier set, a well-formedfragmentation of G is generated by restricting eachfragment to take only nodes in FS as the root andleaf nodes.With fragmentation completed, the rules areextracted through a depth-first traversal of te : foreach frontier being visited, a rule is extracted.These extracted rules are called minimal rules(Galley et al, 2004).
For example, rules r ra i?
inFigure 2 are part of the total of 13 minimal rules.To improve the rule coverage, SPMT modelscan be employed to obtain phrasal rules (Marcu etat., 2006).
In addition, the minimal rules whichshare the adjacent tree fragments can be connectedtogether to form composed rules (Galley et al,2006).
In Figure 2, jr  is a rule composed bycombining cr and gr .2.2 Fuzzy-tree to Exact-tree Rule ExtractionOur fuzzy-tree to exact-tree rule extraction workson word-aligned tree-to-tree data (Figure 2illustrates a Chinese-English tree pair).
Basically,the extraction algorithm includes two parts:(1) String-to-tree rule extraction (withoutconsidering the source parse tree);(2) Decoration of the source side of the string-to-tree rules with syntactic annotations.We use the same algorithm introduced in theprevious section for extracting the base string-to-tree rules.
The source-side syntactic decoration ismuch more complicated.The simplest way to decorate, as mentioned inthe Introduction, is to annotate the source-side of astring-to-tree rule with the syntactic tag thatexactly covers the source string.
This is what theexact tree-to-tree procedure does (Liu et al, 2009).However, many useful string-to-tree rules willbecome invalid if we impose such a tightrestriction.
For example, in Figure 2, the Englishphrase discuss ?
them is a VP, but its Chinesecounterpart is not a constituent.
Thus we will missthe rule rh although it is a useful reordering rule.According to the analysis of our training data, therules with rigid source-side syntactic constraintsaccount for only about 74.5% of the base string-to-tree rules.
In this paper, we desire more generalapplicability.206IPNP VPADJP PP VPAD P NP VP NPPN VV NNPN???
???
??
?
?ithehappytodiscussmatteramwith themNPINPPNNDTNPVBVPTOVPJJADJPVBPVPSNPFWrb: ??
JJ(happy)String-to-Tree rules:ra: ?
FW(i)rm: ?
{P} IN(with)rd: ??
NP(them)re: ??
VB(discuss)rf: ??
NP(DT(the) NN(matter))rg: x0 x1 PP(x0:IN x1:NP)rh: x2 x0 x1 VP(x0:VB x1:NP x2:PP)ri: x0 VP(TO(to) x0:VP)rj: ?
x0 PP(IN(with) x0:VP)Fuzzy-tree to exact-tree rules:rk: ?
{PN} FW(i)rl: ??
{AD} JJ(happy)rc: ?
IN(with)rn: x2 x0 x1{PP*VP} VP(x0:VB x1:NP x2:PP)ro: x0{PP*VP} VP(TO(to) x0:VP)......Figure 2:  A sample Chinese-English tree pair for rule extraction.
The bold italic nodes in the target English tree arefrontiers.
Note that string-to-tree rules are extracted without considering source-side syntax (upper-right).
The newfuzzy-tree to exact-tree rules are extracted with both-side parse trees (bottom-right).Inspired by (Zollmann and Venugopal, 2006;Chiang, 2010), we resort to SAMT-style syntacticcategories in the style of categorial grammar (Bar-Hillel, 1953).
The annotation of the source side ofstring-to-tree rules is processed in three steps: (1)If the source-side string corresponds to a syntacticcategory C in the source parse tree, we label thesource string with C. (2) Otherwise, we check ifthere exists an extended category of the formsC1*C2, C1/C2 or C2\C11, indicating respectively thatthe source string spans two adjacent syntacticcategories, a partial syntactic category C1 missing aC2 on the right, or a partial C1 missing a C2 on theleft.
(3) If the second step fails, we check if there isan extended category of the forms C1*C2*C3 orC1..C2, showing that the source string spans threeadjacent syntactic categories or a partial categorywith C1 and C2 on each side.
In the worst case,C1..C2 can denote every source string, thus all ofthe decorations in our training data can beexplained within the above three steps.
Using theSAMT-style grammar, each source string can beassociated with a syntactic category.
Thus ourfuzzy-tree to exact-tree extraction does not lose1 The kinds of categories are checked in order.
This means thatif C1*C2, C1/C2 can both describe the same source string, we will choose C1*C2.any rules as compared with string-to-treeextraction.
For example, rule ro in Figure 2 uses theproduct category *PP VP  on the source side.A problem may arise: How should we handle thesituation where several rules are observed whichonly differ in their source-side syntactic categories?For example, besides the rule rm in Figure 2, weencountered rules like ?
?
?
?CC IN with??
in thetraining data.
Which source tag should we retain?We do not make a partial choice in the ruleextraction phase.
Instead, we simply make a unionof the relevant rules and retain the respective tagcounts.
Applying this strategy, the rule takes theform of ?
?
?
?
: 6, : 4P CC IN with??
2, indicating thatthe source-side preposition tag appears six timeswhile the conjunction occurs four times.
Note thatthe final rule format used in translation depends onthe specific fuzzy rule matching algorithm adopted.3 Fuzzy Rule Matching AlgorithmsThe extracted rules will ultimately be applied toderive translations during decoding.
One way toapply the fuzzy-tree to exact-tree rules is to narrowthe rule search space.
Given a test source sentence2 6 and 4 are not real counts.
They are used for illustrationonly.207with its parse tree, we can according to thisstrategy choose only the rules whose source syntaxmatches the test source tree.
However, thisrestriction will rule out many potentially correctrules.
In this study, we keep the rule search spaceidentical to that of the string-to-tree setting, andpostpone the use of source-side syntax until thederivation stage.
During derivation, a fuzzymatching algorithm will be adopted to compute ascore to measure the compatibility between therule and the test source syntax.
The translationmodel will learn to distinguish good rules from badones via the compatibility scores.In this section, three fuzzy matching algorithms,from simple to complex, are investigated in order.3.1 0-1 Matching0-1 matching is a straightforward approach thatrewards rules whose source syntactic categoryexactly matches the syntactic category of the teststring and punishes mismatches.
It has mainly beenemployed in hierarchical phrase-based models forintegrating source or both-side syntax (Marton andResnik, 2008; Chiang et al, 2009; Chiang, 2010).Since it is verified to be very effective inhierarchical models, we borrow this idea in oursource-syntax-augmented string-to-tree translation.In 0-1 matching, the rule?s source side mustcontain only one syntactic category, but a rule mayhave been decorated with more than one syntacticcategory on the source side.
Thus we have tochoose the most reliable category and discard theothers.
Here, we select the one with the highestfrequency.
For example, the tag P in the rule?
?
?
?
: 6, : 4P CC IN with??
appears more frequently,so the final rule used in 0-1 matching will be?
?
?
?P IN with??
.
Accordingly, we design twofeatures:1. match_count calculates in a derivation thenumber of rules whose source-side syntacticcategory matches the syntactic category of thetest string.2.
unmatch_count counts the number ofmismatches.For example, in the derivations of Figure 1, weknow the Chinese word?(h?)
is a preposition inthis sentence (and thus can be written as P(?
)),therefore, match_count += 1 if the above rule?
?
?
?P IN with??
is employed.These two features are integrated into the log-linear translation model and the correspondingfeature weights will be tuned along with othermodel features to learn which rules are preferred.3.2 Likelihood MatchingIt appears intuitively that the 0-1 matchingalgorithm does not make full use of the source-sidesyntax because it keeps only the most-frequentsyntactic label and discards some potentially usefulinformation.
Therefore, it runs the risk of treatingall the discarded source syntactic categories of therule as equally likely.
For example, there is anextracted rule as follows:?
?
?
?
:11233, :11073, : 65DEC DEG DEV IN of?
?0-1 matching converts it into ?
?
?
?DEC IN of??
.The use of this rule will be penalized if thesyntactic category of the test string ?(d?)
is parsedas DEG or DEV.
On one hand, the frequency of thetag DEG is just slightly less than that of DEC, butthe 0-1 matching punishes the former whilerewarding the latter.
On the other hand, thefrequency of DEG is much more than that of DEV,but they are penalized equally.
It is obvious thatthe syntactic categories are not finely distinguished.Considering this situation, we propose thelikelihood matching algorithm.
First, we computethe likelihood of the rule?s source syntacticcategories.
Since we need to deal with the potentialproblem that the rule is hit by the test string but thesyntactic category of the test string is not in thecategory set of the rule?s source side, we apply them-estimate of probability (Mitchell, 1997) tocalculate a smoothed likelihoodccn mplikelihood n m??
?
(1)in which nc is the count of each syntactic categoryc in a specific rule, n denotes the total count of therule, m is a constant called the equivalent samplesize, and p is the prior probability of the category c.In our work, we set the constant m=1 and the priorp to 1/12599 where 12599 is the total number ofsource-side syntactic categories in our training data.For example, the rule ?
?
?
?
: 6, : 4P CC IN with?
?becomes ?
?
?
?
: 0.545, : 0.364, 7.2 -6P CC e IN with?
?after likelihood computation.
Then, if we applylikelihood matching in the derivations in Figure 1where the test string is?
and its syntax is P(?
),208the matching score with the above rule will be0.545.
When the test Chinese word ?
is parsed asa category other than P or CC, the matching scorewith the above rule will be 7.2e-6.Similar to 0-1 matching, likelihood matching willserve as an additional model feature representingthe compatibility between categories and rules.3.3 Deep Similarity MatchingConsidering the two algorithms above, we can seethat the purpose of fuzzy matching is in fact tocalculate a similarity.
0-1 matching assignssimilarity 1 for exact matches and 0 for mismatch,while likelihood matching directly utilizes thelikelihood to measure the similarity.
Going onestep further, we adopt a measure of deep similarity,computed using latent distributions of syntacticcategories.
Huang et al (2010) proposed thismethod to compute the similarity between twosyntactic tag sequences, used to impose softsyntactic constraints in hierarchical phrase-basedmodels.
Analogously, we borrow this idea tocalculate the similarity between two SAMT-stylesyntactic categories, and then apply it to calculatethe degree of matching between a translation ruleand the syntactic category of a test source stringfor purposes of fuzzy matching.
We call thisprocedure deep similarity matching.Instead of directly using the SAMT-stylesyntactic categories, we represent each category bya real-valued feature vector.
Suppose there is a setof n latent syntactic categories ?
?1, , nV v v?
?
(n=16in our experiments).
For each SAMT-stylesyntactic category, we compute its distribution oflatent syntactic categories ?
?
?
?
?
??
?1 , ,c c c nP V P v P v??
?
.For example, ?
?
?
?
* 0.4, 0.2, 0.3, 0.1VP NPP V ??
means thatthe latent syntactic categories v1, v2, v3, v4 aredistributed as p(v1)=0.4, p(v2)=0.2, p(v3)=0.3 andp(v4)=0.1 for the SAMT-style syntactic categoryVP*NP.
Then we further transform the distributionto a normalized feature vector?
?
?
?
?
?c cF c P V P V??
?
?
to represent the SAMT-stylesyntactic category c.With the real-valued vector representation foreach SAMT-style syntactic category, the degree ofsimilarity between two syntactic categories can besimply computed as a dot-product of their featurevectors:?
?
?
?
?
?
?
?1' 'i ii nF c F c f c f c?
??
?
???
??
(2)This computation yields a similarity score rangingfrom 0 (totally different syntactically) to 1 (totallyidentical syntactically).Since we can now compute the similarity of anysyntactic category pair, we are currently ready tocompute the matching degree between thesyntactic category of a test source string and afuzzy-tree to exact-tree rule.
To do this, we firstconvert the original fuzzy-tree to exact-tree rule tothe rule of likelihood format without anysmoothing.
For example, the rule?
?
?
?
: 6, : 4P CC IN with?
?
becomes?
?
?
?
: 0.6, : 0.4P CC IN with??
after conversion.
Wethen denote the syntax of a rule?s source-side RSby weighting all the SAMT-style categories in RS?
?
?
?
?
?RSc RSF RS P c F c??
??
?
(3)where ?
?RSP c  is the likelihood of the category c.Finally, the deep similarity between a SAMT-stylesyntactic category tc of a test source string and afuzzy-tree to exact-tree rule is computed as follows:?
?
?
?
?
?,DeepSim tc RS F tc F RS?
??
?
(4)This deep similarity score will serve as a usefulfeature in the string-to-tree model which willenable the model to learn how to take account ofthe source-side syntax during translation.We have ignored the details of latent syntacticcategory induction in this paper.
In brief, the set oflatent syntactic categories is automatically inducedfrom a source-side parsed, word-aligned parallelcorpus.
The EM algorithm is employed to inducethe parameters.
We simply follow the algorithm of(Huang et al, 2010), except that we replace the tagsequence with SAMT-style syntactic categories.4 Rule BinarizationIn the baseline string-to-tree model, the rules arenot in Chomsky Normal Form.
There are severalways to ensure cubic-time decoding.
One way is toprune the extracted rules using a scope-3 grammarand do SCFG decoding without binarization(Hopkins and Lengmead, 2010).
The other, andmost popular way is to binarize the translationrules (Zhang et al, 2006).
We adopt the latterapproach for efficient decoding with integrated n-gram language models since this binarizationtechnique has been well studied in string-to-tree209translation.
However, when the rules?
source stringis decorated with syntax (fuzzy-tree to exact-treerules), how should we binarize these rules?We use the rule rn in Figure 2 for illustration: ?
?
?
?2 0 1 0 1 2: * : : :nr x x x PP VP VP x VB x NP x PP?
.Without regarding the source-side syntax, weobtain the following two binarized rules: ?
??
?0 10 12 0*1 0*1 * 20 1 * 0 11: : :2 : : :x xx xB x x VP x V x PPB x x V x VB x NP?
?Since the source-side syntax PP*VP in rule rnonly accounts for the entire source side, it isunclear how to annotate the source side of a partialrule such as the second binary rule B2.Analyzing the derivation process, we observethat a partial rule such as binary rule B2 neverappears in the final derivation unless the rootedbinary rule B1 also appears in the derivation.Based on this observation, we design a heuristic3strategy: we simply attach the syntax PP*VP in therooted binary rule B1, and do not decorate otherbinary rules with source syntax.
Thus rule rn willbe binarized as:?
?
?
?
?
??
?
?
?0 10 12 0*1 0*1 * 20 1 * 0 11 * : :2 : :x xx xx x PP VP VP x V x PPx x V x VB x NP?
?5 Translation Model and DecodingThe proposed translation system is anaugmentation of the string-to-tree model.
In thebaseline string-to-tree model, the decoder searchesfor the optimal derivation *d  that parses a sourcestring f into a target tree et from all possiblederivations D:?
??
?
?
??
?
*1 23arg max log|LMd Dd p d dd R d f?
?
?
????
??
?
(5)where the first element is a language model scorein which ?
?d?
is the target yield of derivation d ;the second element is the translation length penalty;the third element is used to control the derivationlength; and the last element is a translation scorethat includes six features:3 We call it heuristic because there may be other syntacticannotation strategies for the binarized rules.
It should be notedthat our strategy makes the annotated binarized rulesequivalent to the original rule.?
?
?
?
?
??
?
?
??
?
?
?4 56 78 9| log | ( ) log | ( )log | ( ) log ( ) | ( )log ( ) | ( ) _r dlexlexR d f p r root r p r lhs rp r rhs r p lhs r rhs rp rhs r lhs r is comp?
??
??
?
???
??
??
??
(6)In equation (6), the first three elements denote theconditional probability of the rule given the root,the source-hand side, and the target-hand side.
Thenext two elements are bidirectional lexicaltranslation probabilities.
The last element is thepreferred binary feature for learning: either thecomposed rule or the minimal rule.In our source-syntax-augmented model, thedecoder also searches for the best derivation.
Withthe help of the source syntactic information, thederivation rules in our new model are much moredistinguishable than that in the string-to-tree model:?
??
?
?
??
?
*1 23arg max log|LMd Dd p d dd R d f?
?
?
????
??
?
(7)Here, all elements except the last one are the sameas in the string-to-tree model.
The last item is:?
?
?
??
?
?
??
??
?
?
??
??
?
?
?
?
??
?101112 13| |log ,log ,01r dR d f R d fDeepSim DeepSim tag rlikelihood likelihood tag rmatch unmatch?
??
??
?
?
?
??????
??
(8)The 0-1 matching4 is triggered only when we set?
?01 1?
?
.
The other two fuzzy matching algorithmsare triggered in a similar way.During decoding, we use a CKY-style parserwith beam search and cube-pruning (Huang andChiang, 2007) to decode the new source sentences.6 Experiments6.1 Experimental SetupThe experiments are conducted on Chinese-to-English translation, with training data consisting ofabout 19 million English words and 17 millionChinese words5.
We performed bidirectional wordalignment using GIZA++, and employed the grow-diag-final balancing strategy to generate the final4  In theory, the features unmatch_count, match_count andderivation_length are linearly dependent, so theunmatch_count is redundant.
In practice, since the derivationmay include glue rules which are not scored by fuzzymatching.
Thus, "unmatch_count + match_count +glue_rule_number = derivation_length".5  LDC catalog number: LDC2002E18, LDC2003E14,LDC2003E07, LDC2004T07 and LDC2005T06.210symmetric word alignment.
We parsed both sidesof the parallel text with the Berkeley parser (Petrovet al, 2006) and trained a 5-gram language modelwith the target part of the bilingual data and theXinhua portion of the English Gigaword corpus.For tuning and testing, we use NIST MTevaluation data for Chinese-to-English from 2003to 2006 (MT03 to MT06).
The development dataset comes from MT06 in which sentences withmore than 20 words are removed to speed upMERT6 (Och, 2003).
The test set includes MT03to MT05.We implemented the baseline string-to-treesystem ourselves according to (Galley et al, 2006;Marcu et al, 2006).
We extracted minimal GHKMrules and the rules of SPMT Model 1 with sourcelanguage phrases up to length L=4.
We furtherextracted composed rules by composing two orthree minimal GHKM rules.
We also ran the state-of-the-art hierarchical phrase-based system Joshua(Li et al, 2009) for comparison.
In all systems, weset the beam size to 200.
The final translationquality is evaluated in terms of case-insensitiveBLEU-4 with shortest length penalty.
Thestatistical significance test is performed using there-sampling approach (Koehn, 2004).6.2 ResultsTable 1 shows the translation results ondevelopment and test sets.
First, we investigate theperformance of the strong baseline string-to-treemodel (s2t for short).
As the table shows, s2toutperforms the hierarchical phrase-based systemJoshua by more than 1.0 BLEU point in alltranslation tasks.
This result verifies the superiorityof the baseline string-to-tree model.With the s2t system providing a baseline, wefurther study the effectiveness of our source-syntax-augmented string-to-tree system withfuzzy-tree to exact-tree rules (we use FT2ET todenote our proposed system).
The last three linesin Table 1 show that, for each fuzzy matchingalgorithm, our new system TF2ET performssignificantly better than the baseline s2t system,with an improvement of more than 0.5 absoluteBLEU points in all tasks.
This result demonstratesthe success of our new method of incorporatingsource-side syntax in a string-to-tree model.6 The average decoding speed is about 50 words per minute inthe baseline string-to-tree system and our proposed systems.System MT06(dev)MT03 MT04 MT05Joshua 29.42 28.62 31.52 31.39s2t 30.84 29.75 32.68 32.410-1 31.61** 30.60** 33.45** 33.37**LH 31.35* 30.34* 33.21* 33.05*FT2ETDeepSim 31.77** 30.82** 33.69** 33.50**Table 1: Results (in BLEU scores) of differenttranslation models in multiple tasks.
LH=likelihood.
*or**=significantly better than s2t system (p<0.05 or0.01 respectively).Very similar?
?
?
?
'F c F c??
?
>0.9Very dissimilar?
?
?
?
'F c F c??
?
<0.1ADJP JJ;  AD\ADJP VP;  ADVP\NPNP DT*NN;  LCP*P*NP CP;  BA*CPTable 2: Example of similar and dissimilar categories.Specifically, the FT2ET system with deepsimilarity matching obtains the best translationquality in all tasks and surpasses the baseline s2tsystem by 0.93 BLEU points in development dataand by more than 1.0 BLEU point in test sets.
The0-1 matching algorithm is simple but effective, andit yields quite good performance (line 3).
Thecontribution of 0-1 matching as reflected in ourexperiments is consistent with the conclusions of(Marton and Resnik, 2008; Chiang, 2010).
Bycontrast, the system with likelihood matching doesnot perform as well as the other two algorithms,although it also significantly improves the baselines2t in all tasks.6.3 Analysis and DiscussionWe are a bit surprised at the large improvementgained by the 0-1 matching algorithm.
Thisalgorithm has several advantages: it is simple andeasy to implement, and enhances the translationmodel by enabling its rules to take account of thesource-side syntax to some degree.
However, amajor deficiency of this algorithm is that it doesnot make full use of the source side syntax, since itretains only the most frequent SAMT-stylesyntactic category to describe the rule?s sourcesyntax.
Thus this algorithm penalizes all the othercategories equally, although some may be morefrequent than others, as in the case of DEG andDEV in the rule?
?
?
?
:11233, :11073, : 65DEC DEG DEV IN of??
.To some extent, the likelihood matchingalgorithm solves the main problem of 0-1 matching.211Instead of rewarding or penalizing, this algorithmuses the likelihood of the syntactic category toapproximate the degree of matching between thetest source syntactic category and the rule.
For acategory not in the rule?s source syntactic categoryset, the likelihood algorithm computes a smoothedlikelihood.
However, the likelihood algorithm doesnot in fact lead to very promising improvement.We conjecture that this disappointing performanceis due to the simple smoothing method weemployed.
Future work will investigate more fully.Compared with the above two matchingalgorithms, the deep similarity matching algorithmbased on latent syntactic distribution is much morebeautiful in theory.
This algorithm can successfullymeasure the similarity between any two SAMT-style syntactic categories (Table 2 gives someexamples of similar and dissimilar category pairs).Then it can accurately compute the degree ofmatching between a test source syntactic categoryand a fuzzy-tree to exact-tree rule.
Thus thisalgorithm obtains the best translation quality.However, the deep similarity matching algorithmhas two practical shortcomings.
First, it is not easyto determine the number of latent categories.
Wehave to conduct multiple experiments to arrive at anumber which can yield a tradeoff betweentranslation quality and model complexity.
In ourwork, we have tried the numbers n=4, 8, 16, 32,and have found n=16 to give the best tradeoff.
Thesecond shortcoming is that the induction of latentsyntactic categories has been very time consuming,since we have applied the EM algorithm to theentire source-parsed parallel corpus.
Even withn=8, it took more than a week to induce the latentsyntactic categories on our middle-scale trainingdata when using a Xeon four-core computer( 2.5 2 16GHz CPU GB?
?
memory).
When the trainingdata contains tens of millions of sentence pairs, thecomputation time may no longer be tolerable.Table 3 shows some translation examples forcomparison.
In the first example, the Chinesepreposition word ?
is mistakenly translated intoEnglish conjunction word and in Joshua andbaseline string-to-tree system s2t, however, oursource-syntax-augmented system FT2ET-DeepSimcorrectly converts the Chinese word ?
intoEnglish preposition with and finally yield the righttranslation.
In the second example, our proposedsystem moves the prepositional phrase at an earlydate after the sibling verb phrase.
It is morereasonable compared with the baseline system s2t.In the third example, the proposed system FT2ET-DeepSim successfully recognizes the Chinese longprepositional phrase ?
?
??
??
???
??
??
?
??
???
??
?
and short verb phrase ?,and obtains the correct phrase reordering at last.7 Related WorkSeveral studies have tried to incorporate source ortarget syntax into translation models in a fuzzymanner.Zollmann and Venugopal (2006) augment thehierarchical string-to-string rules (Chiang, 2005)with target-side syntax.
They annotate the targetside of each string-to-string rule using SAMT-stylesyntactic categories and aim to generate the outputmore syntactically.
Zhang et al (2010) base theirapproach on tree-to-string models, and generategrammatical output more reliably with the help oftree-to-tree sequence rules.
Neither of them buildstarget syntactic trees using target syntax, however.Thus they can be viewed as integrating targetsyntax in a fuzzy manner.
By contrast, we base ourapproach on a string-to-tree model which doesconstruct target syntactic trees during decoding.
(Marton and Resnik, 2008; Chiang et al, 2009and Huang et al, 2010) apply fuzzy techniques forintegrating source syntax into hierarchical phrase-based systems (Chiang, 2005, 2007).
The first twostudies employ 0-1 matching and the last tries deepsimilarity matching between two tag sequences.
Bycontrast, we incorporate source syntax into astring-to-tree model.
Furthermore, we apply fuzzysyntactic annotation on each rule?s source stringand design three fuzzy rule matching algorithms.Chiang (2010) proposes a method for learning totranslate with both source and target syntax in theframework of a hierarchical phrase-based system.He not only executes 0-1 matching on both sides ofrules, but also designs numerous features such as.
'X Xroot  which counts the number of rules whosesource-side root label is X  and target-side rootlabel is 'X .
This fuzzy use of source and targetsyntax enables the translation system to learnwhich tree labels are similar enough to becompatible, which ones are harmful to combine,and which ones can be ignored.
The differencesbetween us are twofold: 1) his work applies fuzzysyntax in both sides, while ours bases on the string-212Source sentence ??
?
[?
??
???]
??
?
?
?Reference hussein also established ties with terrorist networksJoshua hussein also and terrorist networks established relationss2t hussein also and terrorist networks established relations1FT2ET- DeepSim hussein also established relations with terrorist networksSource sentence ?
[?
?]
[??]
[??]
[?
?
??
??
?
??
??
]Reference .. to end years of bloody conflict between israel and palestine as soon as possible.. to end at an early date years of bloody conflict between israel and palestineJoshua ?
in the early period to end years of blood conflict between israel and palestines2t ?
at an early date to end years of blood conflict between israel and palestine2FT2ET- DeepSim ?
to end years of blood conflict between israel and palestine at an early dateSource sentence ??
[?
?
??
??
???
??
??
?
??
???
??
?]
[?]
?Referencethe europen union said in a joint statement issued after its summit meeting with china ?spremier wen jiabao ?in a joint statement released after the summit with chinese premier wen jiabao , theeuropen union said ?Joshua the europen union with chinese premier wen jiabao in a joint statement issued after thesummit meeting said ?s2t the europen union in a joint statement issued after the summit meeting with chinesepremier wen jiabao said ?3FT2ET- DeepSim the europen union said in a joint statement issued after the summit meeting with chinesepremier wen jiabao ?Table 3: Some translation examples produced by Joshua, string-to-tree system s2t and source-syntax-augmentedstring-to-tree system FT2ET with deep similarity matching algorithmto-tree model and applies fuzzy syntax on sourceside; and 2) we not only adopt the 0-1 fuzzy rulematching algorithm, but also investigate likelihoodmatching and deep similarity matching algorithms.8 Conclusion and Future WorkIn this paper, we have proposed a new method foraugmenting string-to-tree translation models withfuzzy use of the source syntax.
We first applied afuzzy annotation method which labels the sourceside of each string-to-tree rule with SAMT-stylesyntactic categories.
Then we designed andexplored three fuzzy rule matching algorithms: 0-1matching, likelihood matching, and deep similaritymatching.
The experiments show that our newsystem significantly outperforms the strongbaseline string-to-tree system.
This substantialimprovement verifies that our fuzzy use of sourcesyntax is effective and can enhance the ability tochoose proper translation rules during decodingwhile guaranteeing grammatical output withexplicit target trees.
We believe that our work maydemonstrate effective ways of incorporating both-side syntax in a translation model to yieldpromising results.Next, we plan to further study the likelihoodfuzzy matching and deep similarity matchingalgorithms in order to fully exploit their potential.For example, we will combine the merits of 0-1matching and likelihood matching so as to avoidthe setting of parameter m in likelihood matching.We also plan to explore another direction: we willannotate the source side of each string-to-tree rulewith subtrees or subtree sequences.
We can thenapply tree-kernel methods to compute a degree ofmatching between a rule and a test source subtreeor subtree sequence.AcknowledgmentsThe research work has been funded by the NaturalScience Foundation of China under Grant No.60975053, 61003160 and 60736014 and supportedby the External Cooperation Program of theChinese Academy of Sciences.
We would also liketo thank Mark Seligman and Yu Zhou for revisingthe early draft, and anonymous reviewers for theirvaluable suggestions.213ReferencesYehoshua Bar-Hillel, 1953.
A quasi-arithmeticalnotation for syntactic description.
Language, 29 (1).pages 47-58.David Chiang, 2005.
A hiearchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL2005, pages 263-270.David Chiang, 2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33 (2).pages 201-228.David Chiang, 2010.
Learning to translate with sourceand target syntax.
In Proc.
of ACL 2010, pages1443-1452.David Chiang, Kevin Knight and Wei Wang, 2009.11,001 new features for statistical machinetranslation.
In Proc.
of NAACL 2009, pages 218-226.Brooke Cowan, Ivona Kucerova and Michael Collins,2006.
A discriminative model for tree-to-treetranslation.
In Proc.
of EMNLP, pages 232-241.Yuan Ding and Martha Palmer, 2005.
Machinetranslation using probabilistic synchronousdependency insertion grammars.
In Proc.
of ACL2005, pages 541-548.Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu, 2004.
What?s in a translation rule.
In Proc.of HLT-NAACL 2004, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and IgnacioThayer, 2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL-COLING 2006.Mark Hopkins and Greg Langmead, 2010.
SCFGdecoding without binarization.
In Proc.
of EMNLP2010, pages 646-655.Liang Huang and David Chiang, 2007.
Forest rescoring:Faster decoding with integrated language models.In Proc.
of ACL 2007, pages 144-151.Liang Huang, Kevin Knight and Aravind Joshi, 2006.
Asyntax-directed translator with extended domain oflocality.
In Proc.
of AMTA 2006, pages 65-73.Zhongqiang Huang, Martin Cmejrek and Bowen Zhou,2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntacticdistributions.
In Proc.
of EMNLP 2010, pages 138-147.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin and Evan Herbst, 2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL 2007, pages 177-180.Philipp Koehn, 2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP2004, pages 388?395.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese and Omar F.Zaidan, 2009.
Joshua: An open source toolkit forparsing-based machine translation.
In Proc.
of ACL2009, pages 135-139.Yang Liu, Qun Liu and Shouxun Lin, 2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL-COLING 2006, pages609-616.Yang Liu, Yajuan Lv and Qun Liu, 2009.
Improvingtree-to-tree translation with packed forests.
In Proc.of ACL-IJCNLP 2009, pages 558-566.Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight, 2006.
SPMT: Statistical machinetranslation with syntactified target languagephrases.
In Proc.
of EMNLP 2006, pages 44-52.Yuval Marton and Philip Resnik, 2008.
Soft syntacticconstraints for hierarchical phrased-basedtranslation.
In Proc.
of ACL-08: HLT.
pages 1003?1011.Haitao Mi, Liang Huang and Qun Liu, 2008.
Forest-based translation.
In Proc.
of ACL-08: HLT.
pages192?199.Haitao Mi and Qun Liu, 2010.
Constituency todependency translation with forests.
In Proc.
ofACL 2010, pages 1433-1442.Tom M. Mitchell, 1997.
Machine learning.
Mac GrawHill.Franz Josef Och, 2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL2003, pages 160-167.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein, 2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proc.
of COLING-ACL 2006, pages 433-440.Chris Quirk, Arul Menezes and Colin Cherry, 2005.Dependency treelet translation: Syntacticallyinformed phrasal SMT.
In Proc.
of ACL 2005,pages 271-279.Libin Shen, Jinxi Xu and Ralph Weischedel, 2008.
Anew string-to-dependency machine translationalgorithm with a target dependency languagemodel.
In Proc.
of ACL-08: HLT, pages 577-585.Tong Xiao, Jingbo Zhu, Muhua Zhu and and HuizhenWang, 2010.
Boosting-based System Combinationfor Machine Translation.
In Proc.
of ACL 2010,pages 739-748.Hao Zhang, Liang Huang, Daniel Gildea and KevinKnight, 2006.
Synchronous binarization formachine translation.
In Proc.
of HLT-NAACL 2006,pages 256-263.214Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, ChewLim Tan, 2009.
Forest-based tree sequence tostring translation model.
In Proc.
of ACL-IJCNLP2009, pages 172-180.Hui Zhang, Min Zhang, Haizhou Li and Chng EngSiong, 2010.
Non-isomorphic forest pairtranslation.
In Proc.
of EMNLP 2010, pages 440-450.Andreas Zollmann and Ashish Venugopal, 2006.
Syntaxaugmented machine translation via chart parsing.In Proc.
of Workshop on Statistical MachineTranslation 2006, pages 138-141.215
