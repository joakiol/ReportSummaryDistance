Strategies for Advanced Question AnsweringSanda Harabagiu and Finley LacatusuLanguage Computer Corporation1701 N. Collins Ste.
2000Richardson, TX 75080{sanda, finley}@languagecomputer.comAbstractProgress in Question Answering can beachieved by (1) combining multiple strategiesthat optimally resolve different questionclasses of various degrees of complexity; (2)enhancing the precision of question interpreta-tion and answer extraction; and (3) questiondecomposition and answer fusion.
In this pa-per we also present the impact of modelingthe user background on Q/A and discuss thepragmatics pf processing negation in Q/A.1 IntroductionOur fundamental premise is that progress in Q/A cannotbe achieved only by enhancing the processing compo-nents, but it also requires generating the best strategiesfor processing each individual question.
Thus we be-lieve that Q/A systems capable of successfully process-ing complex questions should employ multiplestrategies instead of the current pipeline approach, con-sisting of (1) question processing, (2) passage retrievaland (3) answer selection.
The pipeline architecture wasreported in (Prager et al, 2000; Moldovan et al, 2000;Hovy et al, 2001).
Recently, a novel approach based oncombinations of multiple independent agents imple-menting different answer finding strategies (multi-strategy) and multiple search spaces (multiple-source)was developed by the IBM QA group (Chu-Carroll etal., 2003).
In (Echihabi and Marcu, 2003) another formof combining strategies for advanced QA is proposed:(1) a knowledge-based Q/A implementation based onsyntactic/semantic processing is combined using amaximum-entropy framework with (2) a statisticalnoisy-channel algorithm for Q/A and (3) a pattern-basedapproach that learn from Web data.
In this project wepropose a different form of finding optimal strategies ofadvanced QA which is based on (a) Question Decompo-sition, (b) Answer Fusion and feedback from (c) Inter-active Q&A and (d) User Background Recognition.We argue that all this new architectures operate un-der the assumption that there is a concept-based or pat-tern-based method for identifying the correct answer forany question that will be processed.
However, we be-lieve that there are complex questions that need first tobe decomposed into simple questions, for which con-cept-based or pattern-based resolving techniques eitherexists or may be developed.
For instance, when askingQ1: ?How have thefts impacted on the safety of Russia?snuclear navy, and has the theft problem been increasedor decreased over time??
we may have series of simplerquestions that decompose the question focus.
One suchexample of simple question is Q1a: ?What specific in-stances of theft do we know about??
?
which is a list-question similar to those evaluated in the recent TRECtracks (Harabagiu et al, 2003).
Related, simpler ques-tion is Q1b:  ?What sort of items have been stolen?
?.Question Q1a asks about instantiations of the theftevents, whereas question Q1b inquires about the objectsof the events.
The decompositions may follow otherarguments of the event predicates, e.g.
?
the agents inQ1c: ?Who are the perpetrators of these thefts??
as wellas specializations of the events, e.g.
?economical im-pact?
specializing one of the possible impacts of thethefts in the question Q1d: ?Do thefts have an economi-cal impact on the naval bases??.
Furthermore, the con-cepts from the complex question need to be clearlyunderstood, and often definition questions will be con-sidered as decompositions that enable the processing ofcomplex questions.
The definition may involve entitiesfrom the complex question, e.g.
Q1e: ?What is meant bynuclear navy??
or events from the complex question,e.g.
Q1f: ?What does ?impact?
mean?
?There are several criteria that guide question de-composition, which also determine the answer resolu-tion strategies.
The criteria are:1.
There are coordinations in the question format,suggesting decompositions along the constituentsthey coordinate.
Coordinations may exist at: (a)question stem level, e.g.
?When and where did thethefts occur??
; (b) at predicate level, e.g.
?Howdoes one define an increase or a decrease in thetheft problem??
; (c) at argument level, e.g.
?Towhat degree do different thefts put nuclear or ra-dioactive materials at risk??
; (d) at question level,e.g.
?What specific instances of theft do we knowabout, and what are the sources of this informa-tion??.
Question decomposition by identifying co-ordinations involves: (a) disambiguation ofconjunctives for identifying when they indicateseparate questions as opposed to when they just co-ordinate constituents; (b) reference and ellipsisresolution of anaphoric expressions in the originalquestion; (c) recognition of the relations betweenthe resulting, decomposed questions, e.g.
contrast,reinforcement, mutual exclusion.2.
The question asks about (a) a complex relation, e.g.cause-effect, resultative, trend, likelihood, (b) com-parison with similar situations, or (c) elaboration ofa state of affairs.
Therefore the expected answertype is of complex nature and it requires definitionsin the context of the complex scenario.
The ex-pected answer, recognized in a predicate from thequestion, determines the decomposition into (a) adefinition question, (b) specializations of the predi-cate-concept, and (c) examples.3.
In order to search for the complex answer, elabora-tions of its arguments are needed.
Such elabora-tions, called argument-answer decompositions, mayinvolve (a) nested predicate-argument structures,(b) quantifications, or (c) instantiations.When a complex question is processed, and is de-composed into a set of simpler questions which are ana-lyzed independently.
Each decomposed question maybelong to a different class, for which certain strategiesmay be optimal.
Such strategies implement the prag-matic processes that interact with the syntactic and se-mantic information that results from the derivation of:(1) expected answer types or structures, (2) name enti-ties which are recognized, as well as (3) syntactic andsemantic dependencies derived from the parsing of thequestion into predicate-argument structures.
To be ableto process the question precisely we are developingtechniques that leverage a database of one million ques-tions that have answers in a controlled corpus.
Thislarge database provides wide coverage of answer typesand answer instances.
It also enhances the retrieval,navigation and fusion of partial answers.The challenge of creating a set of approximately onemillion question and answer pairs are twofold.
First, thepairs need to be diverse in terms of difficulty, wheredifficulty can be defined in terms of answer type com-plexity (common, uncommon, requiring decomposi-tion), answer granularity (concentrated within a smallfragment or spread across several passages and docu-ments), ease of matching (requiring both surface-textand deep semantic understanding).
Second, the pairsshould be reliable, i.e.
each question must be associatedwith a  correct answer.
Our solution is a combination ofcollection and generation from semi-structured re-sources, followed by expansion and validation.
We willgenerate the collection of QA pairs from FrequentlyAsked Questions (FAQ) files on various topics.
We willdevelop a dedicated harvesting algorithm to identifyFAQ's on the Web and extract the QA pairs.The large database of questions also allows us tocreate a benchmark that will support the development ofstatistical techniques for Q/A.
The architecture of thebenchmark system is illustrated in Figure 1.
Our systemselects answers based on (1) question processing strate-gies; (2) passage retrieval strategies made possible by(3) question decomposition and (4) answer fusion.When a question is posed to the system, it is either de-composed on a set of simpler questions or it is proc-essed in parallel with similar questions provided by theInteractive Question Answering component.
Based onthe user background, a set of similar questions may beselected and analyzed in parallel.
Multiple strategies areavailable for retrieving relevant passages.
The possibleselections are once again dictated by feedback fromQuestion AnalysisQuestionDecompositionQuestion AnalysisQuestion AnalysisQuestion Analysis?EnglishQuestionPassage Retrieval 1Passage Retrieval 2Passage Retrieval n?Answer Selection 1Answer Selection 2Answer Selection nInteractive Question Answering User BackgroundQuestion Processing StrategiesAnswerFusionEnglishAnswer1 million question/answer pairsPassage Retrieval StrategiesCounter-Training for Answer ExtractionFigure 1: Answer resolution strategiesinteractions with the user.
The relevant passages mayalso be combined on the basis of the same interactiveand background information.
We propose to study anddevelop several kernel methods that can operate in Sup-port Vector Machines for determining the optimalstrategies and compare the results with the MaximumEntropy combinations reported in (Echihabi and Marcu,2003).
The answer is produced by an answer fusionmodule that uses fusion operators.
Since such operatorsare template-like, pattern acquisition methods may beemployed for acquiring them.The rest of the paper is organized as follows.
Theanswer fusion strategies are presented in Section 2.
Sec-tion 3details the methods for bootstrapping QuestionAnswering.
Section 4 describes the impact of the userbackground on the pragmatics of Q/A.
Section 5 pre-sents the problems engendered by processing negationsin Question Answering.
Section 6 summarizes the con-clusions.2 Answer Fusion, Ranking and ReliabilityGiven the size of today?s very large document re-positories, one can expect that any complex topic willbe covered from multiple points of view.
This feature isexploited by the question decomposition techniques,which generate a set of multiple questions in order tocover all of the possible interpretations of a complextopic.
However, a set of decomposed questions mayend up producing a disparate (and potentially contradic-tory) set of answers.
In order for Q/A systems to usethese collections of answers to their advantage, answerfusion must be performed in order to identify a single,unique, and coherent answer.We view answer fusion as a three-step process.
First,an open-domain, template-based answer formalization isconstructed based on predicate-argument frames.
Sec-ond, a probabilistic model is trained to detect relationsbetween the extracted templates.
Finally, a set of tem-plate merging operators are introduced to construct themerged answer.
The block architecture for answer fu-sion is illustrated in Figure 2.
The system functionalityis demonstrated with the example illustrated in Figure 3.Our method first converts the extracted answers intoa series of open-domain templates, which are based onpredicate-argument frames (Surdeanu et al 2003).
Thenext component detects generic inter-template relations.Typical ?greedy?
approaches in Information Extraction(Hobbs et al 1997; Surdeanu and Harabagiu, 2002) useheuristics that favor proximity for template merging.The example in Figure 3 proves that this is not alwaysthe best decision, even for templates that share the samepredicate and have compatible slots.2.1 Open-domain template representationA key issue to the proposed approach is the open-domain template representation.
While template-basedrepresentations have been proposed for informationmerging in the past (Radev and McKeown, 1998), theyconsidered only domain-specific scenarios.
Based onour recent successes with the extraction of predicate-argument frames (Surdeanu et al 2003), we propose atemplate representation that is a direct mapping ofpredicate-argument frames.
For example, the first tem-plate in Figure 3 is generated from the frame detectedfor the predicate ?assassinate?
: the first slot ?
ARG0 ?typically stands for subject or agent; the second slot ?ARG1 ?
stands for the predicate object, and the modi-fier arguments ARGM-LOC and ARGM-TMP indicatethe location and date of the event.2.2 Detection of template relationsIn this section, we introduce a probabilistic modelfor the detection of template relations that has beenproven to infer better connectivity.If the templates that are candidates for merging areselected entirely based on heuristics (Radev and McKe-own, 1998; Surdeanu and Harabagiu, 2002), the applica-tion of fusion operators for QA is unreliable, due totheir relatively weak semantic understanding of the...Pred/ArgFrameDetectionanswersTemplateGenerationframes templatesTemplateRelationDetectiontemplaterelationscombinedanswerFusionOperatorsFigure 2: Answer fusion block architectureIsraeli agents have assassinated a mastermind Palestinian terrorist and his bodyguard Fridayin the Gaza Strip.
Yehya Ayyash, nicknamed ?The Engineer?, is considered responsible for killingtwelve Israelis.
An Israeli source said the terrorists were killed with a phone bomb.T1: assassinateARG0: Israeli agentsARG1: a mastermind Palestinianterrorist and his bodyguardARGM-LOC: Gaza StripARGM-TMP: FridayT2: killARG0: Yehya Ayyash,nicknamed ?The Engineer?ARG1: twelve IsraelisT3: killARG1: the terroristsARG2: a phone bomb?greedy?
mergecorrect mergeFigure 3: Examples of templates and template relationstemplates.
The novelty in our approach is to precedetemplate merging by the discovery of relations amongtemplates.We propose a novel matching approach based ontemplate attributes that support relation detection formerging.
The approach combines phrasal parsing,lemma normalization and semantic approximation (viaWordNet lexical chains).
For example, this approachdetects that the attributes ARG1 of the first template(?assassinate?)
and ARG1 of the third template (?kill?
)from Figure 3 refer to the same entity, by matching ?ter-rorist?
with ?terrorists?.
Moreover, the names of thetemplates (?assassinate?
and ?kill?)
are connectedthrough a WordNet lexical chain.A "greedy" detection procedure would incorrectlymerge templates with the same name and a similarstructure.
The second and third templates from Figure 3,both named ?kill?, illustrate this case.
Instead, we pro-pose a novel probabilistic algorithm for relation detec-tion.
The algorithm computes a probability distributionof possible relations among entity templates, and retainsthose relations whose probabilities exceed a confidencethreshold.Operator DescriptionCONTRADICTION Two templates contain contradicting informa-tion, e.g.
the same terrorist event is reportedto have a different number of victims.ADDITION The second template introduces additionalfacts, e.g.
one template indicates the loca-tion/date of a terrorist event while the secondindicates number of victims.REFINEMENT The second template provides more refinedinformation about the same event, e.g.
thetown instead of the country of location.AGREEMENT The templates contain redundant information.This operator is useful to heighten the answerstrength.GENERALIZATION The two templates contain only incompletefacts that form an event only when combined.TREND The templates indicate similar patterns overtime.NO INFORMATION The templates contain no useful information,e.g.
unconfirmed event.Table 1: Fusion Operators2.3 Fusion OperatorsThe probabilistic model detects relations.
A set of 7template fusion operators is applied on the detected rela-tions to generate the final set of templates.
The opera-tors are described in Table 1.
The purpose of the fusionoperators is to label the generic relations with the re-quired merge operation, e.g.
ADDITION,CONTRADICTION, TREND.
For example, the tem-plates T1 and T3 can be merged with the ADDITIONoperator.
Optionally, the resulting template can bemerged with template T2 with the weaker operatorTREND, because they mark a similar type of event thattakes place in the same location and date.The generic template relations are labeled with oneof the operators described in Table 1 with a machinelearning approach based on Support Vector Machines(SVM) and a dedicated SVM kernel.
SVMs are ideal forthis task because they do not require an explicit set offeatures (a very complex endeavor in the planned open-domain environment), but localized kernels that providea measure of template similarity.
The labeled templaterelations direct the actual merging operation, whichyields the final list of templates.
Actual text can be gen-erated from these templates, but this is beyond the goalof this paper.3 Bootstrapping Question AnsweringTwo key components of modern QA systems are theidentification of the answer type, and the extraction ofcandidate answers that are classified in the correspond-ing answer type category.
For example, the question?What weapons of mass destruction (WMD) does Iraqhave??
has the answer type ?WMD?
and accepts con-cepts such as ?anthrax?
as valid (but not necessarilycorrect) candidate answers.
This approach provides anefficient implementation for the ?exact answer?
ques-tion answering spirit, but it is plagued by limited scal-ability.We address the above scalability problem throughseveral innovations: we develop a novel bootstrappingtechnique that increases significantly the coverage ofthe existing answer type categories.
Furthermore, newanswer type categories are created for concepts that cannot be classified according to the currently availableknowledge.
In addition to the immediate application foranswer extraction, the induced answer type knowledgeis used to bootstrap the passage retrieval component,through intelligent query expansion.Like most of the successful AQUAINT QA systems,LCC?s system uses an answer type (AT) ontology forthe classification of AT categories.
The AT ontology isbased on WordNet but can be extended with other open-domain or domain-specific categories.
Instances ofgiven categories are identified in answer passages usinga modified version of the CiceroLite Named-EntityRecognizer (NER).The first innovation in bootstrapping focuses on thecapability of LCC?s QA system to identify AT in-stances.
The algorithm is summarized in Figure 4.
Thealgorithm uses as input a very large set of ques-tion/answer pairs, and the existing AT ontology cur-rently used by LCC?s QA system.
For each ATcategory, the algorithm adds the exact answers from thetraining question/answer pairs that share the same ATcategory to the BootstrappedLexicon, which is the lexi-con generated as one outcome of this algorithm.
Besidesthe lexicon, the algorithm induces a set of answer ex-traction patterns, BootstrappedPatterns, which guaran-tees the scalability of the proposed approach.
Boot-strappedPatterns is initialized to the empty set and isiteratively increased during the bootstrap loop.
Duringthe loop, the system scores all possible extraction pat-terns, and selects the best pattern to be added to Boot-strappedPatterns.
Concepts discovered with the newlyextracted pattern are appended to BootstrappedLexicon,and the process repeats.If a question/answer pair exists in the training setwith ?anthrax?
as the exact answer, step 1.2 of the boot-strapping algorithm adds ?anthrax?
to Boot-strappedLexicon.
The bootstrap loop (step 1.4) minesthe training documents for all possible patterns that con-tain anthrax.
The best pattern selected is ?deploy an-thrax?, which is generalized to ?deploy ANY-WMD?.This pattern is then used to extract other candidates forthe WMD category, such as ?smallpox?, ?botulinum?etc.The algorithm illustrated in Figure 4 addresses thediscovery of new instances for existing AT categories.A direct extension of this algorithm handles the situa-tion when the discovered entities and patterns do notbelong to a known category.
The detection of new ATcategories will be performed based on the AT word,which is the question concept that drives the selection ofthe AT.
For example, the AT concept in the question:?What viral agent was used in Iraq??
is ?viral agent?,which does not exist in the current WordNet ontology.If the answer type concept does not exist in WordNet,the bootstrapping algorithm will create a distinct cate-gory for this concept.
If the answer type concept existsin WordNet, the algorithm attaches the bootstrappedentities and patterns to the concept hypernym that pro-vides the largest coverage without overlapping anyother known categories.
This approach is robust enoughto function without word sense disambiguation: the al-gorithm explores all relevant synsets and selects the onethat maximizes the above condition.
For example, theanswer type concept ?fighter aircraft?
from the ques-tion: ?What fighter aircrafts are in use in the Iraqiarmy??
is mapped to the hypernym synset airplane(Sense #1), instead of vehicle (Sense #1), which over-laps with other vehicle categories such as cars.3.1 Enhancing retrieval, navigation, and fusionAnswer accuracy is conditioned by the ability of theQA system to generate effective queries for the retrievalsubsystem (Moldovan et al, 2003).
Queries that are toorestrictive will incorrectly narrow the search space, andfail to retrieve the relevant answer passages.
An exam-ple of such a query is (biological AND agents ANDQaeda), which is generated for the question ?What bio-logical agents does al Qaeda possess??.
This query willmiss most of the relevant text passages since they do notinclude any explicit reference to biological agents.The extensions to the AT ontology, described above,enable an intelligent query expansion based on two ex-pansion resources: AT instances, and extraction pat-terns.
More precisely, each question concept mappedunder any of the AT categories is expanded with theinstances and keywords from the extraction patternsassociated with that category.
In this case, the expandedquery for the above question is:  ((biological ANDagents) OR (bacterial AND agent) OR (viral ANDagent) OR (fungal AND agent) OR (toxic AND agent)OR botulism OR botulinum OR smallpox OR encephali-tis OR (deploy)) AND (Qaeda).
This query illustratestwo important requirements: the conversion of extrac-tion patterns into keywords (e.g., ?deploy?
for "deployANY-WMD"); and the controlled expansion throughselective keyword selection (e.g., for ?biologicalagents?
).3.2 Continuous updating of scenario knowledgeThe bootstrapping algorithm described in the previ-ous section is based on the large question/answer dataset, which is largely open-domain.
We consider a directextension of this algorithm that automatically learnsscenario knowledge by monitoring the user?s browsinghabits.Question/answer pairs are extracted based on theuser?s feedback.
These pairs form the seeds for a meta-bootstrapping loop, as illustrated in Figure 5.
Similardocuments ?
i.e.
documents where the identical exactanswer and the question keywords are identified ?
areproduced from the relevant Q/A pairs.
This process can1.
For each AT category:1.1.
Extract all question/answer pairs from the training data set, where the question has this AT category.1.2.
Add all exact answers to BootstrappedLexicon.1.3.
BootstrappedPatterns = ?1.4.
Bootstrap loop:1.4.1.
Generate all possible extraction patterns for BootstrappedLexicon.1.4.2.
Score all extracted patterns.1.4.3.
Find the best pattern Pbest not in BootstrappedPatterns.1.4.4.
Add Pbest to BootstrappedPatterns.1.4.5.
Add instances discovered with Pbest to BootstrappedLexicon.If termination condition not met go to 1.5.1.Figure 4: Answer instance bootstrapping algorithmbe equally applied on the Web or on a static documentcollection.
The bootstrapping algorithm described inFigure 5 is applied on the extracted documents.
Theinferred AT instances are further used to enrich thecollection of considered documents, which forms themeta-bootstrapping loop.4 User BackgroundResearch in question answering, and in the moregeneral field of information retrieval, has traditionallyfocused on building generic representations of thedocument content, largely independent of any subjectivefactors.
It is important to note, however, that all usersare different: not only do they have different back-grounds and expertise, but they also vary in their goalsand reasons for using a Q/A system.
This variety hasmade it difficult for systems to represent user intentionsautomatically or to make use of them in Q/A systems.Figure 6 illustrates the inherent differences betweensystem end-users.
Since (by definition) a novice lacksthe domain-specific knowledge available to an expert,we should expect a novice user to choose a path com-pletely different than an expert user, leading to ex-tremely different results for the same top level question.4.1 Assessing User BackgroundWe evaluate users via a discrete evaluation scale,which ranks users as novice, casual, or expert usersbased on how much background knowledge they haveon the given topic.
The approach classifies users basedon the path chosen in the generated question decomposi-tion tree.This kind of characterization of user expertise can beused to reduce the exploration space generated throughquestion decomposition.
The most significant drawbackof question decomposition is the exponential increase inthe number of questions to be answered, which, to ourknowledge, is not addressed by current QA research.We filter the generated question decomposition treeusing the detected user expertise: for example, if theuser is known to be an ?expert?, only the paths gener-ated through ?expert?
decomposition - i.e.
generatedusing significant world and topic knowledge ?
will befollowed.To be able to use the question decomposition treefor user classification, we must first classify the decom-position tree itself, i.e.
the branches must be markedwith one of the three discrete classification values.
Byshifting the classification problem from the (yet) ab-stract user background to the decomposition tree, weargue that the problem is simplified because we knowhow much background and world knowledge was nec-essary for the question decomposition.
For example, togenerate the ?expert user?
path in Figure 6, the systemmust have access to world knowledge that indicates thatan ?impact?
can be economic, social etcetera and thatQ/ApairsUserFeedbackSimilarDocumentsBootstrapATFigure 6:Scenario-specific meta-bootstrapping loopHow have thefts impacted the safety of Russia?s nuclear navy,and has the the theft problem been increased or reduced over time?How have thefts impacted the safety of Russia?s nuclear navy,and has the the theft problem been increased or reduced over time?What sort of itemshave been stolen?What sort of itemshave been stolen?To what degree dodifferent thefts putnuclear or radioactivematerials at risk?To what degree dodifferent thefts putnuclear or radioactivematerials at risk?What is meant bynuclear navy?What is meant bynuclear navy?Bases withactive vesselscarying nuclearweapons only?Bases withactive vesselscarying nuclearweapons only?Bases withnuclear-poweredvessels (andconventionalweapons)?Bases withnuclear-poweredvessels (andconventionalweapons)?What does?impact?
mean?What does?impact?
mean?Is this onlythefts of sensitiveequipment?Is this onlythefts of sensitiveequipment?Does it includehaving aneconomic impacton the naval base?Does it includehaving aneconomic impacton the naval base?How does onedefine an increasein the problem?How does onedefine an increasein the problem?By dollaramounts?By dollaramounts?By degreeof accessto sensitivesites?By degreeof accessto sensitivesites?...............Novice UserExpert UserFigure 5: Example of different user selections from the generated question decomposition treenuclear materials are sensitive equipment.
We willquantify the amount of knowledge used for decomposi-tion and label the generated branches accordingly.Once a labeled decomposition tree is available, theuser?s background can be classified based on the se-lected path.
The relevant answers (where ?relevancy?can be explicitly requested from the user, or implicitlydetected based on the documents visited) are mappedback to the corresponding questions, which provides adynamic trace in the question decomposition tree.
Usingthe tree structure and the classification labels previouslyassigned, we will train machine learning algorithms thatwill infer the final user expertise classification.4.2 Representing User BackgroundWe propose a new multi-modal approach for therepresentation of the user profile that includes conceptsand relations in addition to terms.Traditionally, the user profile (or background) hasbeen represented as a term vector, derived from the pre-viously relevant document (be it online or offline infor-mation).
Under this approach, each profile P, isrepresented as: P = ((t1, w1), (t2, w2), ?, (tn, wn)), whereti are terms from relevant documents and wi are termweights, typically computed with the tf * idf metric.Our approach is novel in two regards.
The first in-novation stems from the observation that it is commonfor one user to explore multiple topics even during thesame session.
For example, an analyst interested in thecurrent Iraq situation, must explore topics related tomilitary action, peace keeping, and terrorism.
Hence theone vector representation for the profile P is clearly in-sufficient.
In the proposed representation, the profile Pis represented as a set of vectors p1, p2, ?, pn, where:  pi= ((ti1, wi1), (ti2, wi2), ?
(tim, wim)), i = 1, 2, ?, n, and mis the size of vector pi.
We expect the number and sizeof the profile vectors to change dynamically.
When anew document is marked as relevant, the document vec-tor is either: (a) merged with an existing profile, if theirsimilarities are higher than a given threshold, or (b) usedto generate a new profile.
Profile vectors are removedbased on negative feedback: if a document vector simi-lar to an existing profile receives continuous negativefeedback the corresponding profile is deleted in order tokeep the profile synchronized with the user?s currentinterest patterns.
We believe this profile representationto be flexible enough to accommodate all expertise lev-els, from novice to expert.
For example, the expertuser?s background will consist of multiple vectors; eachspecializes on a clear, domain-specific direction, whilethe novice user?s profile will most likely contain fewervectors with more generic terms.The second innovation includes concepts and rela-tions in addition to lexical terms in the user profile.
Apreliminary analysis of the CNS documents indicatesthat ?al?
is among the most frequent terms, but, by it-self, ?al?
is considered a stop word by most informationretrieval systems.
However, the significance of the termbecomes evident when the complete concept, ?alQaeda?
is considered.
This observation indicates thatsemantic information is significant for the representa-tion of the user profile.
In addition to indexing entities,we index generic of entity-to-entity relations that aresignificant, and often the goal, of the intelligence ana-lyst?s work.5 Processing Negation in Question An-sweringAlthough all human systems of communication rep-resent negation in some format, the issue of how best toaddress negation in open-domain Q/A still remains anopen research question.
Previous Q/A systems havedealt with negation by filtering the retrieved answer andeliminating answers that share key terms with the querybut are irrelevant for the reasons of negation (Martino-vic, 2002; Attardi et al 2001) or by constructing rela-tional databases to query the answers can handlenegation in the question since the scope is clearly de-fined in the relational database (Jung and Lee, 2001).However, neither of these systems has dealt with thecentral problem that negation poses for Q/A:  determin-ing the scope of the negation context.
Consider the fol-lowing examples:a.
Which countries did not vote for the Iraq warresolution in the Security Council?b.
Which countries did not provide help to the coali-tion during the Gulf War in 1991?c.
What planets have no moon?In question (a), the scope of negation only includesthe countries that were members of the Security Councilduring the Iraq war resolution that were able to vote butdid not.
However, examples (b) and (c) are ambiguouswith respect to the scope of negation.
In question (b),the scope could encompass the whole world, or all thecountries in the Middle East that should have providedhelp but did not.
In question (c), even more entities canbe included under the scope of negation:  all of theplanets in the solar system or even all of the planets inthe entire universe (including planets that are not yetdiscovered).In order for a Q/A system to answer questions like(b) or (c), the scope of negation must first be deter-mined.
We initially propose to develop empirical stud-ies for recognizing the most frequent cases of negation:e.g.
the ?no?
negation ?
?with no terrorists, the worldwould be safer?, the ?nothing?
negation ?
?the inspec-tors found nothing?, and other core cases of local nega-tion ?
e.g.
?thefts did not occur at the beginning?.
Weshall complement our methods of recognizing negationin textual sentences by analyzing various syntactic andsemantic contexts of negation, e.g.
adverbial negation ?
?the president never leaves the White House without theSecret Service approval?.In addition, we assume that when a speaker is for-mulating a question to find out whether a proposition istrue or false, s/he formulates the question with the formof the proposition which would be the most informativeif it turned out to be true.
We expect that if a questionhas the form of negation, the speaker believes that thenegative answer is the most informative.
Using suchhypotheses, we argue that in a negation question, if thescope is ambiguous, like in (b) or (c), then we can solvethe ambiguity by choosing the scope that will be moreinformative for the user.Given these assumptions, we propose that negationcan be addressed in Q/A in three ways:By using the user background.
In questions like(b) above, if the user background is terrorism, then wecan limit the scope of the countries to those who havebeen linked to terrorism.By interacting with the user.
If no user back-ground can be established, as in question (c), we expectto use dialogue techniques to enable the user can specifythe relevant scope.By finding cues from the answers to the positivequestion.
Finally, we expect to be able to use a combi-nation of heuristics and information extraction methodsto deduce the answer to a negative question from theanswers to the corresponding positive question.
Forexample, when searching for the answer to the positiveanalog of question (c), we can limit the scope of thenegation to the solar systems where there are planetswith moons.6 ConclusionsIn this position paper we discuss several strategiesthat enhance the pragmatic process involved in the in-terpretation and resolution of complex questions.
Insupport of our claims we present a Q/A architectureused to benchmark the impact of (1) question decompo-sitions following several criteria; (2) answer fusionwhich composes a unique, coherent answer from thepartial answers extracted for each decomposed question;(3) modeling of user background and (4) processing ofnegation in questions and/or answers.
Additionally, wepresent a bootstrapping algorithm that enhances the pre-cision of factual Q/A.
We argue that each of these en-hancements would allow us to advance the state-of-the-art in Q/A and will enable us to correctly process com-plex questions.ReferencesA.
L. Berger, S. A. Della Pietra and V. J. Della Pietra.
Amaximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1):39-72,March 1996.Jennifer Chu-Carroll and Sandra Carberry.
GeneratingInformation-Sharing Subdialogues in Expert-UserConsultation.
In Proceedings of the 14th InternationalJoint Conference on Artificial Intelligence (IJCAI-95), pages 1243-1250, Montreal, Canada, 1995.Michael Collins.
A New Statistical Parser Based onBigram Lexical Dependencies.
In Proceedings of the34th Annual Meeting of the Association for Computa-tional Linguistics, ACL-96, pages 184-191, 1996.Abdessamad Echihabi and Daniel Marcu.
A Noisy-Channel Approach to Question Answering.
In Pro-ceedings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2003),Sapporo, Japan, 2003.Christiane Fellbaum (Editor).
WordNet ?
An ElectronicLexical Database.
MIT Press, 1998.C.J.
Fillmore and C.F.
Baker and H. Sato.
The  Frame-Net Database  and  Software Tools.
In M. Gonz?lez-Rodr?guez and C.P.
Su?rez-Araujo, Eds.
In Proceed-ings of the Third International Conference on Lan-guage Resources and Evaluation.
Las Palmas, Spain,2002.Michael Fleischman, Eduard Hovy and AbdessamadEchihabi.
Offline Strategies for Online Question An-swering: Answering Questions Before They AreAsked.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics (ACL-2003), pages 1-7, Sapporo, Japan, 2003.Sanda Harabagiu, Marius Pa?ca and Steven Maiorano.Experiments with Open-Domain Textual QuestionAnswering.
In Proceedings of the 18th InternationalConference on Computational Linguistics (COLING-2000), pages 292-298, Saarbrucken, Germany, 2000.Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, John Williams and JeremyBensley.
Answer Mining by Combining ExtractionTechniques with Abductive Reasoning.
In Proceed-ings of the 12th Text Retrieval Conference (TREC2003).Jerry R. Hobbs, Doug E. Appelt, John Bear, David Is-rael, Megumi Kameyama, Mark Stickel and MabryTyson.
FASTUS: A Cascaded Finite-State Trans-ducer for Extracting Information Natural-LanguageText.
In Finite State Language Processing, Edited byEmmanuel Roche and Yves Schabes, MIT Press,1997.E.H.
Hovy, U. Hermjakob, C.-Y.
Lin, ?The Use of Ex-ternal Knowledge in Factoid QA?, TREC-10 confer-ence, November 2001.Dan I. Moldovan, Sanda M. Harabagiu, Marius Pa?ca,Rada Mihalcea, Roxana G?rju, Richard Goodrum andVasile Rus.
The Structure and Performance of anOpen-Domain Question Answering System.
In Pro-ceedings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2000),2000.John Prager, Eric Brown, Anni Coden and DragomirRadev.
Question-answering by predictive annotation.In Proceedings of the 23rd annual international ACMSIGIR conference on Research and development ininformation retrieval, pages: 184-191, Athens,Greece, 2000.Dragomir R. Radev and Kathleen McKeown.
Generat-ing natural languages summaries from multiple on-line sources.
Computational Linguistics, 24(3):469-500, 1998.M.
Surdeanu and S. Harabagiu, ?Infrastructure forOpen-Domain Information Extraction?, in Proceed-ings of the Conference for Human Language Tech-nology (HLT-2002), pages 325-330, March 2002.Mihai Surdeanu, Sanda Harabagiu, John Williams andPaul Aarseth, ?Using Predicate-Argument Structuresfor Information Extraction?, In Proceedings of ACL2003.
