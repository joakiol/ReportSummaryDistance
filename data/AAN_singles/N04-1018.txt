Detecting Structural Metadata with Decision Trees andTransformation-Based LearningJoungbum Kim?
and Sarah E. Schwarm?
and Mari Ostendorf??Dept.
of Electrical Engineering ?Dept.
of Computer ScienceUniversity of WashingtonSeattle, WA 98195.
USA{bummie,sarahs,mo}@ssli.ee.washington.eduAbstractThe regular occurrence of disfluencies is adistinguishing characteristic of spontaneousspeech.
Detecting and removing such disflu-encies can substantially improve the usefulnessof spontaneous speech transcripts.
This pa-per presents a system that detects various typesof disfluencies and other structural informationwith cues obtained from lexical and prosodicinformation sources.
Specifically, combina-tions of decision trees and language models areused to predict sentence ends and interruptionpoints and, given these events, transformation-based learning is used to detect edit disfluen-cies and conversational fillers.
Results are re-ported on human and automatic transcripts ofconversational telephone speech.1 IntroductionAutomatic speech-to-text (STT) transcripts of sponta-neous speech are often difficult to comprehend even with-out the challenges arising from word recognition errorsintroduced by imperfect STT systems (Jones et al, 2003).Such transcripts lack punctuation that indicates clausal orsentential boundaries, and they contain a number of dis-fluencies that would not normally occur in written lan-guage.
Repeated words, hesitations such as ?um?
and?uh?, and corrections to a sentence in mid-stream area normal part of conversational speech.
These disflu-encies are handled easily by human listeners (Shriberg,1994), but their existence makes transcripts of sponta-neous speech ill-suited for most natural language pro-cessing (NLP) systems developed for text, such as parsersor information extraction systems.
Similarly, the lackof meaningful segmentation in automatically generatedspeech transcripts makes them problematic to use in NLPsystems, most of which are designed to work at the sen-tence level.
Detecting and removing disfluencies and lo-cating sentential unit boundaries in spontaneous speechtranscripts can improve their readability and make themmore suitable for NLP.
Automatically annotating dis-course markers and other conversational fillers is alsolikely to be useful, since proper handling is needed to fol-low the flow of conversation.
Hence, the overall goal ofour work is to detect such structural information in con-versational speech using features generated by currentlyavailable speech processing systems and statistical ma-chine learning tools.This paper is organized as follows.
In Section 2, wedescribe the types of metadata that this work addresses,followed by a discussion of related prior work in Sec-tion 3.
Section 4 describes the system architecture anddetails the algorithms and features used by our system.Section 5 discusses the experimental paradigm and re-sults.
Finally we provide a summary and directions forfuture work in Section 6.2 Structural MetadataWe consider three main types of structural metadata:sentence-like units, conversational fillers and edit disflu-encies.
These structures were chosen primarily becauseof the availability of annotated conversational speech datafrom the Linguistic Data Consortium (Strassel, 2003) andstandard scoring tools (NIST, 2003).2.1 Sentence UnitsConversational speech lacks the clear sentence bound-aries of written text.
Instead, we detect SUs (variouslyreferred to as sentence, semantic, and slash units), whichare linguistic units maximally equivalent to sentencesthat are used to mark segmentation boundaries in con-versational speech where utterances often end withoutforming ?grammatical?
sentences in the sense expectedin written text.
SUs can be sub-categorized accordingto their discourse role.
In our data, annotations distin-guish statement, question, backchannel, incomplete SUand SU-internal clause boundaries.
Here, we ignore theSU-internal boundaries, and merge all but the incompleteSU categories in characterizing SU events.Table 1: Filled pauses and discourse markers to be de-tected by our system.Filled Pauses ah, eh, er, uh, umDiscourse Markers actually, anyway, basically, Imean, let?s see, like, now, see,so, well, you know, you seeTable 2: Examples of edit disfluencies.Disfluency ExampleRepetition (I was) + I was very interested...(I was) + { uh} I was very interested...Repair (I was) + she was very interested...(I was) + { I mean } she was very...Restart (I was very) + Did you hear the news?2.2 Conversational FillersConversational fillers include filled pauses (hesitationsounds such as ?uh?, ?um?
and ?er?
), discourse mark-ers (e.g.
?well?, ?you know?
), and explicit editing terms.Defining an all-inclusive set of English filled pauses anddiscourse markers is a problematic task.
Our system de-tects only a limited set of filled pauses and discoursemarkers, listed in Table 1, which cover a large majority ofcases (Strassel, 2003).
An explicit editing term is a filleroccurring within an edit disfluency, described further be-low.
For example, the discourse marker I mean serves asan explicit editing term in the following edit disfluency:?I didn?t tell her that, I mean, I couldn?t tell her that hewas already gone.
?2.3 Edit DisfluenciesEdit disfluencies largely encompass three separate phe-nomena: repetition, repair and restart (Shriberg, 1994).A repetition occurs when a speaker repeats the most re-cently spoken portion of an utterance to hold off the flowof speech.
A repair happens when the speaker attemptsto correct a mistake that he or she just made.
Finally, ina restart, the speaker abandons a current utterance com-pletely and starts a new one.Previous studies characterize edit disfluencies usinga structure with different segments (Shriberg, 1994;Nakatani and Hirschberg, 1994).
The first part of thisstructure is called the reparandum, a string of words thatgets repeated or corrected.
The reparandum is immedi-ately followed by a non-lexical boundary event termedthe interruption point (IP).
The IP marks the point wherethe speaker interrupts a fluent utterance.
Optionally, theremay be a filled pause or explicit editing term.
The finalpart of the edit disfluency structure is called the alter-ation, which is a repetition or revised copy of the reparan-dum.
In the case of a restart, the alteration is empty.
InTable 2, reparanda are enclosed in parentheses, IPs arerepresented by ?+?, optional fillers are in braces, and al-terations are in boldface.Annotation of complex edit disfluencies, where a dis-fluency occurs within an alteration, can be difficult.
Thedata used here is annotated with a flattened structurethat treats these cases as simple disfluencies with mul-tiple IPs (Strassel, 2003).
IPs within a complex disflu-ency are detected separately, and contiguous sequencesof edit words associated with these IPs are referred to asa deletable region.3 Previous WorkIn an early study on automatic disfluency detection adeterministic parser and correction rules were used toclean up edit disfluencies (Hindle, 1983).
However theirswas not a truly automatic system as it relied on hand-annotated ?edit signals?
to locate IPs.
Bear et al (1992)explored pattern matching, parsing and acoustic cues andconcluded that multiple sources of information would beneeded to detect edit disfluencies.
A decision-tree-basedsystem that took advantage of various acoustic and lexi-cal features to detect IPs was developed in (Nakatani andHirschberg, 1994).Shriberg et al (1997) applied machine prediction ofIPs with decision trees to the broader Switchboard corpusby generating decision trees with a variety of prosodicfeatures.
Stolcke et al (1998) then expanded the prosodictree model with a hidden event language model (LM)to identify sentence boundaries, filled pauses and IPs indifferent types of edit disfluencies.
The hidden eventLM used in their work adapted Hidden Markov Model(HMM) algorithms to an n-gram LM paradigm to repre-sent non-lexical events such as IPs and sentence bound-aries as hidden states.
Liu et al (2003) built on thisframework and extended prosodic features and the hiddenevent LM to predict edit IPs on both human transcriptsand STT system output.
Their system also detected theonset of the reparandum by employing rule-based patternmatching once edit IPs have been detected.Edit disfluency detection systems that rely exclusivelyon word-based information have been presented by Hee-man et al (Heeman et al, 1996) and Charniak and John-son (Charniak and Johnson, 2001).
Common to both ofthese approaches is a focus on repeated or similar se-quences of words and information about the words them-selves and the length and similarity of the sequences.Our approach is most similar to (Liu et al, 2003), sincewe also detect boundary events such as IPs first and usethem as ?signals?
when identifying the reparandum ina later stage.
The motivation to detect IPs first is thatSpeech IP/SUPredictionProsodic and LexicalFeature ExtractionWord BoundaryEvent Prediction(DT/HE-LM)Filler/EditWord Detection(TBL)OutputFigure 1: System Diagramspeech before an IP is fluent and is likely to be free ofany prosodic or lexical irregularities that can indicate theoccurrence of an edit disfluency.
Like Liu et al, we use adecision tree trained with prosodic features and a hiddenevent language model for the IP detection task.
However,we incorporate SU detection in those models as well.
Weuse part-of-speech (POS) tags and pattern match featuresin decision tree training whereas Liu et al (2003) devel-oped language models for them.
We explore three dif-ferent methods of combining the hidden event languagemodel and the decision tree model, namely linear inter-polation, joint tree-based modeling and an HMM-basedapproach.
Moreover, our system uses the transformation-based learning algorithm rather than hand-crafted rulesfor the second stage of edit region detection.Another key difference between our system and mostprevious work is the prediction target.
Our system incor-porates detecting word boundary events such as SUs andIPs, locating onsets of edit regions, and identifying filledpauses, discourse markers and explicit editing terms.
Webelieve that such a comprehensive detection scheme al-lows our system to better model dependencies betweenthese events, which will lead to an improvement in theoverall detection performance.4 System Description4.1 Overall ArchitectureAs shown in Figure 1, our system detects disfluenciesin a two-step process.
First, for each word boundary inthe given transcription, a decision tree predicts one of thefour boundary events IP, SU, ISU (incomplete SU), andthe null event.
Then in the second stage, rules learnedvia the transformation-based learning (TBL) algorithmare applied to the data containing predicted boundaryevents and other lexical information to identify edits andfillers.
Following edit region and filler prediction, the sys-tem output was post-processed to eliminate edit regionpredictions not associated with IP predictions as well asIP predictions for which no edit region or filler was de-tected.
An analysis of post-processing alternatives con-firmed that this strategy reduced insertion errors.4.2 Detecting Boundary EventsIn order to detect boundary events, we trained a CART-style decision tree (Breiman et al, 1984) with variousprosodic and lexical features.
Decision trees are well-suited for this task because they provide a convenient wayto integrate both symbolic and numerical features in pre-diction.
Furthermore, a trained decision tree is highly ex-plainable by its nature, which allows us to gain additionalinsight into the utilities of and the interactions betweenmultiple information sources.Prosodic features generated for decision tree trainingincluded the following:?
Word and rhyme1 durations.?
Rhyme duration differences between two neighbor-ing words.?
F0 statistics (minimum, mean, maximum, slope)over a word.?
Differences in F0 statistics between two neighboringwords.?
Energy statistics over a word and its rhyme.?
Silence duration following a word.?
A flag indicating start and end of a speaker turn andspeaker overlap.?
Ordinal position of a word in a turn.Energy and F0 features were generated with the EntropicSystem ESPS/Waves package and the F0 stylization tooldeveloped in (So?nmez et al, 1998).
Word and rhymeduration were normalized by phone duration statistics(mean and variance) calculated over all available trainingdata.
F0 and energy features were normalized for eachindividual speaker?s baseline.
A turn boundary was hy-pothesized for word boundaries with silences longer thanfour seconds.Since inclusion of features that do not contribute tothe classification of data can degrade the performance ofa decision tree, we selected only the prosodic featureswhose exclusion from the training process led to a de-crease in boundary event detection accuracy on the de-velopment data by utilizing the leave-one-out method.Lexical features consisted of POS tag groups, word andPOS tag pattern matches, and a flag indicating existence1In our work, a rhyme was defined to contain the final vowelof a word and any consonants following the final vowel.of filler words to the right of the current word bound-ary.
The POS tag features were produced by first predict-ing the tags with Ratnaparkhi?s Maximum Entropy Tag-ger (Ratnaparkhi, 1996) and then clustered by hand intoa smaller number of groups based on their syntactic role.The clustering was performed to speed up decision treetraining as well as to reduce the impact of tagger errors.Word pattern match features were generated by com-paring words over the range of up to four words across theword boundary in consideration.
Grouped POS tags werecompared in a similar way, but the range was limited toat most two tags across the boundary since a wider com-parison range would have resulted in far more matchesthan would be useful due to the low number of availablePOS tag groups.
When words known to be identified fre-quently as fillers existed after the boundary, they wereskipped and the range of pattern matching was extendedaccordingly.Another useful cue for boundary event detection is theexistence of word fragments.
Since word fragments occurwhen the speaker cuts short the word being spoken, theyare highly indicative of IPs.
However currently availableSTT systems do not recognize word fragments.
As ourgoal is to build an automatic detection system, our sys-tem was not designed to use any features related to wordfragments.
However, for a control case, we conductedan experiment with reference transcripts using a single?frag?
word token to show the potential for improved per-formance of a system capable of recognizing fragments.In addition to the decision tree model, we also em-ployed a hidden event language model to predict bound-ary events.
A hidden event LM is the same as a typicaln-gram LM except that it models non-lexical events inthe n-gram context by counting special non-word tokensrepresenting such events.
The hidden event LM estimatesthe joint distribution P (W,E) of words W and events E.Once the model has been trained, a forward-backward al-gorithm can be used to calculate P (E|W ), or the poste-rior probability of an event given the preceding word se-quence (Stolcke et al, 1998; Stolcke and Shriberg, 1996).The SRI Language Modeling Toolkit (SRILM) (Stolcke,2002) was used to train a trigram open-vocabulary lan-guage model with Kneser-Ney discounting (Kneser andNey, 1995) on data that had boundary events (SU, ISU,and IP) inserted in the word stream.
Posterior probabil-ities of boundary events for every word boundary werethen estimated with SRILM?s capability for computinghidden event posteriors.While the hidden event LM alone can be used to de-tect boundary events, prior work has shown that it ben-efits from also using prosodic cues, so we combined thelanguage model and the decision tree model in three dif-ferent ways.
In the first approach, which we call the jointtree model, the boundary event posterior probability fromthe hidden event LM is jointly modeled with other fea-tures in the decision tree to make predictions about theboundary events.
In the second approach, referred to asthe linearly interpolated model, a decision is made basedon the combined posterior probability?Ptree(E|A,W ) + (1 ?
?
)PLM (E|W ),where A corresponds to the acoustic-prosodic featuresand the weighting factor ?
can be chosen empirically tomaximize target performance, i.e.
bias the prediction to-ward the more accurate model.
In the third approach,the decision tree features, words and boundary eventsare jointly modeled via an integrated HMM (Shriberget al, 2000).
This approach augments the hidden eventLM by modeling decision tree features as emissions fromthe HMM states represented by the word and boundaryevent.
Under this framework, the forward-backward al-gorithm can again be used to determine posterior prob-abilities of boundary events.
Similar to the linearly in-terpolated model, a weighting factor can be used to intro-duce the desired bias to the combination model.
The jointtree model has the advantage that the (possibly) complexinteraction between lexical and prosodic cues can be cap-tured.
However, since the tree is trained on reference tran-scriptions, it favors lexical cues, which are less reliable inSTT output.
In the linearly interpolated and joint HMMapproaches, the relative weighting of the two knowledgesources is estimated on the development test set for STToutput, so it is possible for prosodic cues to be given ahigher weight.4.3 Edit and Filler DetectionAfter SUs and IPs have been marked, we usetransformation-based learning (TBL) to learn rules todetect edit disfluencies and conversational fillers.
TBLis an automatic rule learning technique that has beensuccessfully applied to a variety of problems in natu-ral language processing, including part-of-speech tag-ging (Brill, 1995), spelling correction (Mangu and Brill,1997), error correction in automatic speech recogni-tion (Mangu and Padmanabhan, 2001), and named entitydetection (Kim and Woodland, 2000).
We selected TBLfor our tagging-like metadata detection task since it hasbeen used successfully for these other tagging tasks.TBL is an iterative technique for inducing rules fromtraining data.
A TBL system consists of a baseline pre-dictor, a set of rule templates, and an objective functionfor scoring potential rules.
After tagging the training datausing the baseline predictor, the system learns a list ofrules to correct errors in these predictions.
At each iter-ation, the system uses the rule templates to generate allpossible rules that correct at least one error in the trainingdata and selects the best rule according to the objectivefunction, commonly token error rate.
The best rule isTable 3: Example word and POS matches for TBL.Word Match that IP thatPOS Match the dog IP the catrecorded and applied to the training data in preparationfor the next iteration.
The standard stopping criterion forrule learning is to stop when the score of the best rule fallsbelow a threshold value; statistical significance measureshave also been used (Mangu and Padmanabhan, 2001).To tag new data, the rules are applied in the order in whichthey were learned.
This allows rules which are learnedlater in the process to fine tune the effects of the earlierrules.
TBL produces concise, comprehensible rules, anduses the entire corpus to train all of the rules.
We usedFlorian and Ngai?s Fast TBL system (fnTBL) (Ngai andFlorian, 2001) to train rules using disfluency annotatedconversational speech data.The input to our TBL system consists of text dividedinto utterances, with IPs and SUs inserted as if they wereextra words.
(For simplicity, these special words are alsoassigned ?IP?
and ?SU?
as part of speech tags.
)Our TBL system used the following types of features:?
Identity of the word.?
Part of speech (POS) and grouped part of speech(GPOS) of the word (same as the decision tree).?
Is the word commonly used as: filled pause (FP),backchannel (BC), explicit editing term (EET), dis-course marker (DM)??
Does this word/ POS/ GPOS match the word/ POS/GPOS that is 1/2/3 positions to its right??
Is this word at the beginning of a turn or utterance??
Tag to be learned.The ?tag?
feature is the one we want the system tolearn.
It is also used in templates that consider featuresof neighboring words.
The baseline predictor sets the tagto its most common value, ?no disfluency,?
for all words.Other values of the tag are the three types of fillers (FP,EET, DM) and edit.
The objective function for our learneris token error rate, and rule learning is stopped at a thresh-old score of 5.We generated a set of rule templates using these fea-tures.
The rule templates account for individual featuresof the current word and/or its neighbors, the proximityof potential FP/EET/DM terms, and matches between thecurrent word and nearby words, especially when in closeproximity to a boundary event or potential filler.
Exampleword and POS matches are shown in Table 3.5 Experiments5.1 Experimental SetupFor training our system and its components, we used twodifferent subsets of Switchboard, a corpus of conversa-tional telephone speech (CTS) (Godfrey et al, 1992).One of the data sets included 417 conversations (LDC1.3)that were hand-annotated by the Linguistic Data Consor-tium for disfluencies and SUs according to the V5 guide-lines detailed in (Strassel, 2003).
Another set of 1086conversations from the Switchboard corpus was anno-tated according to (Meteer et al, 1995) and is available aspart of the Treebank3 corpus (TB3).
We used a versionof this set that contained annotations machine-mapped toapproximate the V5 annotation specification.For development and testing of our system, we usedhand transcripts and STT system output for 72 conversa-tions from Switchboard and the Fisher corpus, a recentCTS data collection.
Half of these conversations wereheld out and used as development data (dev set), and theother 36 conversations were used as test data (eval set).The STT output, used only in testing, was from a state-of-the-art large vocabulary conversational speech recognizerdeveloped by BBN.
The word error rates for the STT out-put were 27% on the dev set and 25% on the eval set.To assess the performance of our overall system, dis-fluencies and boundary events were predicted and thenevaluated by the scoring tools developed for the NISTRich Transcript evaluation task.5.2 Boundary Event PredictionDecision trees to predict boundary events were trainedand tested using the IND system developed byNASA (Buntine and Caruan, 1991).
All decision treeswere pruned by ten-fold cross validation.
The LDC1.3set2 with reference transcriptions was used to train thetrees3 and the dev set was used to evaluate their perfor-mances.Several decision trees with different combinations offeature groups were trained to assess the usefulness ofdifferent knowledge sources for boundary event detec-tion.
The tree was then used to predict the boundaryevents on the reference transcription of the dev set.
Theresults are presented in Table 4.
The inclusion of a spe-cial token for fragments resulted in improved precisionand recall for SUs and IPs but, surprisingly, degraded per-formance for ISUs.
These results show that prosodic fea-tures by themselves failed to detect ISUs and IPs, though2Experiments combining the LDC1.3 set with the mappedTB3 set were not as successful as LDC1.3 set alne for decisiontree training.3While it might be better to train from automatic transcripts,it is difficult to define target class labels in cases where there areinsertion errors or a sequence of several word errors.Table 4: Impact of different features on boundary event prediction using the joint tree model on reference transcripts.Features SU ISU IPRecall Precision Recall Precision Recall PrecisionProsody Only 46.5 74.6 0 - 8.8 47.2POS, Pattern, LM 77.3 79.6 30.0 53.3 64.4 77.4Prosody, POS, Pattern, LM 81.5 80.4 36.5 69.7 66.1 78.7All Above + Fragments 81.1 81.6 20.1 60.7 80.7 80.4they lead to performance gains when combined with lex-ical cues.
Examination of the decision tree trained withonly the prosodic features revealed that pause durationand turn information features were placed near the top ofthe tree.Use of lexical features brought substantial perfor-mance improvement in all aspects, and classification ac-curacy increased when features extracted from differentknowledge sources were combined.
However, we ob-served that a smaller number of prosodic features endedup being used in the tree and they were placed at or nearleaf nodes as more lexical features were made availablefor training.
The importance of prosodic features is likelyto be much more apparent for STT data.
The word errorsprevalent in the STT transcriptions will affect lexical fea-tures far more severely than prosodic features, and there-fore the prosodic features contribute to the robustness ofthe overall system when lexical features become less re-liable.5.3 Edit and Filler DetectionAfter the prediction of boundary events, the rules learnedby the TBL system described in section 4.3 were appliedto detect fillers and edit regions.
As with the decisiontrees, we trained rules using the LDC1.3 data alone, andcombined with the mapped TB3 data, finding that thecombined dataset gave better results for TBL training.Again we used only reference word transcripts but dis-covered that training with SUs and IPs predicted by thefirst stage of our system was more effective than usingreference boundary events.It is difficult to formally assess the effectiveness of theTBL module independently, and results for the entire sys-tem are discussed in detail in the next section.
Informalinspection of the rules learned by the TBL system indi-cates that, not surprisingly, word match features and thepresence of IPs are very important for the detection ofedit regions.
The most commonly used features for iden-tifying discourse markers are the identity or POS of thecurrent and/or neighboring words and the tag already as-signed to neighboring words.Table 5: Detection of boundary events and disfluencieson STT output as scored by rt-eval.Task % Corr % Del % Ins % SERFiller 63.9 36.1 14.0 50.1Edit 25.5 74.5 13.7 88.2IP 49.6 50.5 16.3 66.8SU 73.1 26.9 19.7 46.65.4 Overall System ResultsThe performance of our system was evaluated on the fall2003 NIST Rich Transcription Evaluation test set (RT-03F) using the rt-eval scoring tool (NIST, 2003), whichcombines ISUs and SUs in a single category, and reportsresults for detection of SUs, IPs, fillers, and edits with-out differentiating subcategories of fillers and edits.
Thistool produces a collection of results, including percentagecorrect, deletions, insertions, and Slot Error Rate (SER),similar to the word error rate measure used in speechrecognition.
SER is defined as the number of insertionsand deletions divided by the number of reference items.Note that scores are somewhat different from those inTable 4, because of differences in scoring and metadataalignment methods.Figure 2: Detection of boundary events and disfluencieson reference and STT transcripts (joint tree model).Results of our system on the RT-03F task are shown inTable 6: Percentage of missed IPs on the dev set.Transcription % IPs afterfragments% Other editIPsReference 81.7 37.6STT 74.0 51.2Table 5 for the joint tree version of the system as appliedto the STT transcription of the test data.
SU detectionby our system is relatively good.
IP detection is not assuccessful, which also impacts edit detection.Figure 2 contrasts the results of the joint tree model forSTT output with those obtained on reference data withand without fragments.
As expected, all error rates arehigher on STT output; IPs and fillers take the biggest hit.Filler performance in particular seems to be affected byrecognition errors, which is not surprising, since misrec-ognized words would likely not be on the target lists offilled pauses and discourse markers.
In particular, nearlyall missed and incorrectly inserted filled pauses are dueto recognition errors.
Detection of discourse markers ismore challenging; fewer than half the errors on discoursemarkers are due to recognition errors.
Most non-STT-related filler errors involved the words ?so?
and ?like?used as DMs, which are hard problems since the vast ma-jority of the occurrences of these two words are not DMs.It is also not surprising that improved IP detection on ref-erence data contributes to a lower error rate for edits.As expected, the inclusion of fragments improves per-formance on IP and edit detection, where fragments fre-quently occur.
In LDC1.3, 17.2% of edit IPs have wordfragments occurring before them; 9.9% of edits consistof just a single fragment.
In the dev set, 35.5% of editIPs are associated with fragments.
However, fragmentsare rarely output by the STT system, so for most of ourwork we chose to use the identical system for processingreference and STT transcripts and did not include frag-ments.
IP detection performance was significantly worsefor those IPs associated with fragments, as shown in Ta-ble 6.
However, since fragments are often deleted or rec-ognized as a full word, STT output actually ?helps?
withdetection of IPs after fragments, apparently because thePOS tagger and hidden event LM tend to give unreliableresults on the reference transcripts near fragments.Figure 3 compares the eval test set performances of thedifferent alternatives for incorporating the hidden eventLM posterior, i.e.
inclusion in the decision tree, linearinterpolation and the joint HMM.
For this experiment,the interpolation weighting factor was selected empiri-cally to maximize boundary event prediction accuracy onthe STT transcription of the dev set.
The results of thiscomparison are mixed: SU detection is better with thejoint tree model, but IP detection and consequently editFigure 3: Results for joint tree (JTM), linearly interpo-lated (LIM) and joint HMM models on STT transcripts.detection are better with the interpolation and HMM ap-proaches.
The degradation of SU detection performancewith the HMM is counter to findings in previous work(Stolcke et al, 1998; Shriberg et al, 2000).
This maybe due to differences in evaluation criteria, given thatthe HMM approach typically had higher precision whichmight benefit earlier word-based measures more.
In addi-tion, the difference in conclusions may be due to the factthat the decision trees used here include lexical patternmatch features in addition to hidden event posteriors.A problem in our system is the inability to predict morethan one label for a given word or boundary.
Words la-beled as both filler and edit account for only 0.5% of allfillers and edits in the LDC1.3 training data, so it is prob-ably not a significant problem.
We also do not predictboundaries as both SU and IP.
In LDC1.3, these accountfor 12.8% of SU boundaries, and are treated as simply SUin training.
This does not affect IPs for edits, but impacts38.6% of IPs before fillers.
By predicting a combinedSU-IP boundary in addition to isolated SUs and IPs, weobtain a small reduction in SER for IPs but at the expenseof an increase in SU SER.
However, separating predictionof IPs after edit regions vs. before fillers also yields smallimprovements in edit region precision and filler recall, re-sulting in 3.3% and 0.8% relative reduction in filler andedit SERs respectively for the joint HMM.6 ConclusionsWe have demonstrated a two-tiered system that detectsvarious types of disfluencies in spontaneous speech.
Inthe first tier, a decision tree model utilizes multipleknowledge sources to predict interword boundary events.Then the system employs a transformation-based learn-ing algorithm to identify the extent and type of disflu-encies.
Experimental results show that the large vari-ance and noise inherent in prosodic features makes themmuch less effective than lexical features for referencedata; however, in the presence of word recognition errorsprevalent in automatic transcripts of spontaneous speech,prosodic features have more value.
Performance differ-ences for the various score combination methods weresmall, but combining decision tree and HE-LM scoreswith a weight optimized on dev data is slightly better foredit disfluencies.
Transformation-based learning is an ef-fective way to tag fillers and edit regions after boundaryevents are tagged, but the best performance is obtainedwhen training with automatically predicted SU and IPboundary events.As this is a new task, error rates are relatively high(though significantly better than chance), but this ap-proach achieved competitive results on the Fall 2003NIST Rich Transcription Evaluation, and there are manydirections for future improvements.AcknowledgmentsThis work was supported by DARPA, no.
MDA904-02-C-0437,in a project led by BBN.
The authors thank their colleagues atBBN for providing recognizer output for the training and testdata, and colleagues at SRI for providing F0 conditioning toolsand mapped TB3 data.
Any opinions, conclusions or recom-mendations expressed in this material are those of the authorsand do not necessarily reflect the views of the sponsor or ourcollaborators.ReferencesJ.
Bear et al, ?Integrating multiple knowledge sources for de-tection and correction of repairs in human-computer dialog?,Meeting of the ACL, pp.
56?63, 1992.L.
Breiman et al, Classification and Regression Trees Chap-man and Hall, 1984.E.
Brill, ?Transformation-based error-driven learning and natu-ral language: a case study in part of speech tagging?, Com-putational Linguistics, 21(4), pp.
543?565, 1995.W.
Buntine and R. Caruan ?Introduction to IND and recursivepartitioning?, NASA Ames Research Center, TR.
FIA-91-28, 1991.E.
Charniak and M. Johnson, ?Edit detection and parsing fortranscribed speech?, Proc.
NAACL, pp.
118?126, 2001.J.
J. Godfrey et al, ?SWITCHBOARD: Telephone speech cor-pus for research and development?, Proc.
ICASSP, v. I, pp.517?520, 1992.P.
A. Heeman et al, ?Combining the detection and correctionof speech repairs?, Proc.
ICSLP, v. 1, pp.
362?365, 1996.C.
Hemphill et al, ?The ATIS spoken language systems pi-lot corpus?, Proc.
of DARPA Speech and Natural LanguageWorkshop, pp.
96?101, 1990.D.
Hindle ?Deterministic parsing of syntactic nonfluencies?,Meeting of the ACL, pp.
123?128, 1983.D.
Jones et al, ?Measuring the readability of automatic speech-to-text transcripts?, Proc.
Eurospeech, pp.
1585?1588, 2003.J.-H. Kim and P. Woodland ?A rule-based named entity recog-nition system for speech input?, Proc.
ICSLP, pp.2757?2760, 2001.R.
Kneser and H. Ney ?Improved backing-off for mgram lan-guage modeling?, Proc.
ICASSP, pp.
181?184, 1995.Y.
Liu, ?Automatic disfluency identification in conversa-tional speech using multiple knowledge sources?, Proc.
Eu-rospeech, pp.
957?960, 2003.L.
Mangu and E. Brill, ?Automatic rule acquisition for spellingcorrection?, Proc.
Intl.
Conf on Machine Learning, pp.
187?194, 1997.L.
Mangu and M. Padmanabhan, ?Error corrective mechanismsfor speech recognition?, Proc.
ICASSP, pp.
29?32, 2001.M.
Meteer et al, ?Dysfluency annotation stylebook for theSwitchboard corpus?, Distributed by the LDC, 1995.C.
Nakatani and J. Hirschberg ?A corpus-based study of re-pair cues in spontaneous speech?, Journal of the AcousticalSociety of America, pp.
1603?1616, 1994.G.
Ngai and R. Florian ?Transformation-based learning in thefast lane?, Proc.
NAACL, pp.
40?47, 2001.NIST, ?The Rich Transcription Fall 2003 (RT-03F) evaluation plan,?
http://www.nist.gov/speech/tests/rt/rt2003/fall/docs/rt03-fall-eval-plan-v9.pdf, 2003.A.
Ratnaparkhi, ?A maximum entropy part-of-speech tagger?,Proc.
Empirical Methods in Natural Language ProcessingConf., pp.
133?141, 1996.E.
Shriberg, Preliminaries to a theory of speech disfluencies,PhD thesis, Department of Psychology, University of Cali-fornia, Berkeley, 1994.E.
Shriberg et al, ?A prosody-only decision-tree model for dis-fluency detection?, Proc.
Eurospeech, pp.
2383?2386, 1997.E.
Shriberg et al, ?Prosody-based automatic segmentation ofspeech into sentences and topics?
Speech Communication,32(1-2), pp.
127?154, 2000.K.
So?nmez et al, ?Modeling dynamic prosodic variation forspeaker verification,?
Proc.
Intl.
Conf.
on Spoken LanguageProcessing, v. 7, pp.
3189?3192, 1998.A.
Srivastava and F. Kubala ?Sentence boundary detection inArabic speech?, Proc.
Eurospeech, pp.
949?952, 2003.A.
Stolcke and E. Shriberg ?Automatic linguistic segmenta-tion of conversational speech?, Proc.
ICSLP, v. 2, pp.
1005?1008, 1996.A.
Stolcke et al, ?Automatic detection of sentence boundariesand disfluencies based on recognized words,?
Proc.
ICSLP,1998, v. 5, pp.
2247?2250.A.
Stolcke, ?SRILM - an extensible language modelingtoolkit?, Proc.
ICSLP, v. 2, pp.
901-904, 2002.S.
Strassel, ?Simple metadata annotation specification version5.0?, http://www.ldc.upenn.edu/Projects/MDE/Guidelines/SimpleMDE\_V5.0.pdf, 2003.
