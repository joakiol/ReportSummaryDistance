Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 300?303,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCLR: Linking Events and Their Participants in Discourse Using aComprehensive FrameNet DictionaryKen LitkowskiCL ResearchDamascus, MD USA.ken@clres.comAbstractThe CL Research system for SemEval-2 Task10 for linking events and their participants indiscourse is an exploration of the use of a spe-cially created FrameNet dictionary that cap-tures all FrameNet information about frames,lexical units, and frame-to-frame relations.This system is embedded in a specially de-signed interface, the Linguistic Task Analyzer.The implementation of this system was quiteminimal at the time of submission, allowingonly an initial completion of the role recogni-tion and labeling task, with recall of 0.112,precision of 0.670, and F-score of 0.192.
Wedescribe the design of the system and the con-tinuing efforts to determine how much of thistask can be performed with the available lexi-cal resources.
Changes since the official sub-mission have improved the F-score to 0.266.1 IntroductionThe semantic role labeling (SRL) task has re-ceived considerable attention in recent years,with previous tasks in Senseval-2 (Litkowski,2004), Semeval-1 (Baker et al, 2007), andCoNLL (Carreras & Marquez, 2004; Carreras &Marquez, 2005).
The current task, LinkingEvents and their Participants in Discourse, con-tinues the evolution of SRL tasks with the intentof identifying Null Instantiations, i.e., frameelements that are absent from the local context,but potentially recoverable from the wider dis-course context.CL Research participated in one subtask, rolerecognition and labeling, unable to implementtechniques for the null instantiation subtask.
Thispaper describes our efforts thus far (clearly awork in progress), specifically the implementa-tion of a development interface (section 2), theuse of a specially constructed FrameNet dictio-nary (section 3), techniques for performing therole recognition and labeling task (section 4), ourresults (section 5), and future developments (sec-tion 6).2 The Linguistic Task AnalyzerCL Research participated in the linking task byextending its Linguistic Task Analyzer (LTA),an interface also used for such tasks as word-sense disambiguation and recognizing textualentailment.
LTA includes a wide array of mod-ules, including a full-scale parser, post-parsingsemantic analysis routines, the use of XML func-tionality for creating and analyzing input andoutput, and access to several integrated dictiona-ries (used for semantic analysis).
Modification ofLTA for the linking task involves using existingfunctionality and implementing new functionali-ty specific to the task.
We describe LTA in somedetail to illustrate steps that might be relevant toa symbolic approach to the linking task.Each task in LTA consists of a set of items tobe analyzed, in this case, an identifier for eachsentence in the document being analyzed.
LTAloads the appropriate XML files (usually the an-notation file and the gold file) and provides vari-ous data for each sentence, including the numberof terminals, non-terminals, frames, frame ele-ments that have been recognized, true positives,false positives, false negatives, and a characteri-zation of problems that have been encountered.Summary statistics are given, showing suchthings as the total number of frames and the scor-ing for the current annotation (when a gold file isavailable).Whenever a sentence is selected in the LTA,the text is shown (accomplished by querying theXML for the selected sentence and retrieving allits terminals).
LTA provides a capability for se-300lecting all sentences matching particular criteria,e.g., all sentences containing a Color frame or allsentences having targets that have problematicentries in the FrameNet dictionary.LTA contains a basic command to run andevaluate the system against the selected sen-tences.
This can be used during development totest the effect of changes to the underlying codefor performing any of the tasks.
During the testphase, all sentences are selected, the Run andEvaluate command is executed, the XML testfile is modified with the insertion of frame ele-ments constituting the system?s answers, and theXML file is saved for the official submission.For the official submission, this took less than aminute for each of the two chapters.A single sentence can be selected in the LTAfor detailed examination.
This Sentence Detailshows (1) the sentence itself (as in the mainform), (2) a tree of the frames in the sentence,along with each of the frame elements that havebeen identified, minimally showing the target,and the text that has been identified for the frameelement, and (3) from the training data, the frameelement differences from the gold file, alongwith their terminal or non-terminal id references.The Sentence Detail also has buttons to (1)score the annotation against the gold file for thesentence, (2) identify the missing core frameelements, (3) examine the FrameNet entries forthe targets, and (4) perform the task.
The func-tionality underlying the scoring and the task per-formance are called from the main form when allor selected sentences are to be processed (e.g., inthe Run and Evaluate command).Implementation of the scoring functionalityfor the Sentence Detail form attempts to followthe implementation in the official scorer.
Wehave not yet captured every nuance of the scorer;however, we seem to have 99.9 percent agree-ment.The Sentence Detail functionality is at theheart of the investigation and implementation oftechniques for performing the tasks.
At this time,we must view the implementation as only in itsinitial stages, minimally capable of performingthe role recognition and labeling task.
Furtherdetails about the implementation, including itsshortcomings, will be described below.3 The FrameNet DictionaryCentral to the performance of the linking task isthe use of a dictionary constructed from the Fra-meNet data.
This dictionary is in a format usedby the CL Research DIMAP dictionary mainten-ance program.
1  The FrameNet dictionary at-tempts to capture all the information in Frame-Net, in a form that can be easily accessed andused for tasks such as the linking task.
This dic-tionary is also used in general word-sense dis-ambiguation tasks, when all words in a text aresimultaneously disambiguated with several dic-tionaries.
The FrameNet dictionary has almost11,000 entries 2  of four main types: frames,frame-to-frame relations, normal entries, andframe elements 3 .
This dictionary was initiallydescribed in Litkowski (2007), but is describedin more detail in the following subsections inorder to show how the information in these en-tries is used in the linking task.3.1 Frame EntriesA FrameNet frame is entered in the dictionary bypreceding its name with a ?#?
sign to distinguishit from other types of entries.
A frame entry,such as #Abandonment, consists of one sensewith no part of speech.
This sense contains a listof its frame elements and the coreness of eachframe element.
The sense also lists all the lexicalunits associated with the frame, along with theidentifying number for each so that a link can bemade if necessary to the appropriate lexical unitand lexical entry XML files.
The sense identifiesany frame-to-frame relations in which the frameparticipates, such as ?IS_INHERITED_BY?
witha link to the inheriting frame.
Thus, whenever aspecific frame is signaled in the linking task, itsproperties can be accessed and we can investi-gate which of the frame elements might bepresent in the context.3.2 Frame-to-Frame RelationsWhile the entries for the individual frames iden-tify the frame-to-frame relations in which aframe participates, separate entries are created to1 These dictionaries are stored in a Btree file format forrapid access.
A free demonstration version of DIMAP isavailable at CL Research (http://www.clres.com).
This ver-sion can be used to manipulate any of several dictionariesthat are also available.
These include WordNet and the basicFrameNet.
CL Research also makes available a publiclyavailable FrameNet Explorer and a DIMAP Frame ElementHierarchy dictionary.2 By contrast, the DIMAP dictionary for WordNet contains147,000 entries.3 When a new version of FrameNet is made available, a newversion of the DIMAP dictionary is created.
This was thecase with the preliminary FrameNet version 1.4a madeavailable by the task organizers.
This creation takes abouttwo hours.301hold the mappings between the frame elementsof the two frames.
These entries are prefixedwith an ?@?
sign, followed by the name of aframe, the frame relation, and the name of thesecond frame, as in the name?
@Abounding_with INHERITS Loca-tive_relation?.
The single sense for such an entryshows the mapping, e.g., of the Location frameelement of Abounding_with to the Figure frameelement of Locative_relation.
The informationin these entries has not yet been used in the link-ing task.3.3 Frame ElementsFrame element entries are preceded with a ?%?,as in %Toxic_substance.
We have a taxonomyof the 1131 uniquely-named frame elements inall the FrameNet frames.
4  Each frame elemententry identifies its superordinate frame element(or none for the 12 roots) and the frame elementsin which it is used.
The information in these en-tries has not yet been used in the linking task.3.4 Main EntriesThe bulk of the entries in the FrameNet dictio-nary are for the lexical units.
An entry wascreated for each unique form, with senses foreach lexical unit of the base form.
Thus, beat hasfour senses, two verb, one noun, and one adjec-tive.
Minimally, each sense contains its part ofspeech, its frame, and its id number.
A sense mayalso contain a definition and its source, if presentn the FrameNet lexical unit files.If available, the information available in thelexical entry (LE) files is encapsulated in thesense, from the FERealization elements.
Thiscaptures the phrase type, the grammatical func-tion, the frame element, and the frequency in theFrameNet annotation files.
An example of whatinformation is available for one verb sense ofbeat is shown in Table 1.Table 1.
Lexical Entry Syntactic Patterns for ?beat?Feature Name Feature ValueNP(Ext) Loser (12)Loser (28)Winner (5)Winner (5)Winner (2)Winner (31)NP(Obj)PP[by](Dep)CNI()PP[against](Dep)NP(Ext)4 This taxonomy can be viewed athttp://www.clres.com/db/feindex.html, which provides linksdescribing how it was constructed and which can be down-loaded in DIMAP or MySQL format.At the present time, this type of information isthe primary information used in the linking task.4 Role Recognition and LabelingTo perform the role recognition and labelingtask, the system first retrieves all the frames forthe sentence and then iterates over each.
Theframe name and the target are retrieved.
Fromthe target XML, the id reference is used to re-trieve the part of speech and lemma from the tar-gets terminal node.
With this information, anattempt is made to add child nodes to the framenode in the XML, thus supplying the system?sperformance of the task.
After any nodes havethus been added, it is only necessary to save themodified XML as the output file.The first step in adding child nodes is to obtainthe lexical entries from the FrameNet dictionaryfor the frame and the lemma.
Since the lemmamay have multiple senses, we obtain the specificsense that corresponds to the frame.
We iteratethrough the features for the sense, focusing onthose providing syntactic patterns, such as thosein Table 1.
We deconstruct the feature value intoits frame element name and its frequency.
Wethen call a function with the feature name and thetarget?s id reference to see if we can find amatching constituent; if successful, we create achild node of the frame with the frame elementname and the id reference (for the child <fe-node> of frame element <fe> node).The matching constituent function operates onthe syntactic pattern, calling specific functions tosearch the XML terminals and non-terminals forconstituent that fit the syntactic criterion.
Atpresent, this only operates on four patterns:DEN(), Poss(Gen), NP(Ext), and N(Head).
5 Asan example, for Poss(Gen), we select the non-terminals with the target as the ?head?
and searchthese for a terminal node marked as PRP$.
Aspecial constituent matching function was alsowritten to look for the Supported frame elementin the Support frame.5 System ResultsCL Research?s results for the role recognitionand labeling task are shown in Table 2.
Theseresults are generally consistent across the twochapters in the test and with results obtained withthe training data during development.
Combining5 The DEN pattern identifies incorporated frame elements.Since the official submission, two patterns (NP(OBJ) andPP(Dep)  have been added.302the two chapters, the recall was 0.112, the preci-sion was 0.670, and the F-score was 0.192.
6Table 2.
Scores for Chapters 13 and 14Measure Ch.
13 Ch.
14True Positives 191 246False Positives 82 133False Negatives 1587 1874Correct Labels 189 237Precision 0.700 0.649Recall 0.107 0.116F-Score 0.186 0.197Label Accuracy 0.106 0.112As can be seen, for entries with patterns (albeita low recall), a substantial number of frame ele-ments could be recognized with high precisionfrom a very small number of constituent match-ing functions.
A detailed analysis of the results,identifying the contribution of each pattern rec-ognition and the problem of false positives, hasnot yet been completed.
One such observation isthat when the same syntactic pattern is presentfor more than one frame element, such asNP(Ext) for both Loser and Winner in the caseof beat as shown in Table 1, the same constituentwill be identified for both.A significant shortcoming in the system oc-curs when there are no syntactic patterns availa-ble for a particular sense (27 percent of the tar-gets).
For example, the lemma hour frequentlyappears in the training set as the target of eitherthe Measure_duration or Calendric_unitframes, but it has no syntactic patterns (i.e., theFrameNet data contain no annotations for thislexical unit), while decade, also used in the sameframes, does have syntactic patterns.
This is afrequent occurrence with the FrameNet dictio-nary.6 Future DevelopmentsAs should be clear from the preceding descrip-tion, there are many opportunities for improve-ment.
First, several improvements can be madein the LTA to improve the ability to facilitatedevelopment.
The LTA has only barely begunexploitation of the many integrated modules thatare available.
Additional functionality needs tobe developed so that it will be possible to deter-mine the effect of any changes in constituentmatching, i.e., what is the effect on recall and6The additional patterns described in the previous footnotehave improved recall to 0.166 and F-score to 0.266, whilemaintaining a high precision (0.676).precision.
The sentence detail form can be im-proved to provide better insights into the relationbetween syntactic patterns and their matchingconstituents.Secondly, major improvements appear likelyfrom greater exploitation of the FrameNet dictio-nary.
At present, no use is made of the frequencyinformation or the weighting of choices formatching constituents.
When a given lemma hasno syntactic patterns, it is likely that some use ofthe patterns for other lexical units in the framecan be made.
It is also possible that some generalpatterns can be discerned using the frame ele-ment taxonomy.It is important to see how far the FrameNet da-ta can be further exploited and where other lexi-cal data, such as available in WordNet or in moretraditional lexical databases, can be used.
Thedata developed for this linking task providemany opportunities for further exploration.ReferencesCollin Baker, Michael Ellsworth, and Katrin Erk.2007.
Semeval-2007 Task 19: Frame SemanticStructure Extraction.
Proceedings of the Fourth In-ternational Workshop on Semantic Evaluations(SemEval-2007).
Prague, Czech Republic, Associa-tion for Computational Linguistics, pp.
99-104.Xavier Carreras and Luis Marquez.
2004.
Introduc-tion to the CoNLL-2004 Shared Task SemanticRole Labeling.
Proceedings of the Eighth Confe-rence on Computational Natural Language Learn-ing (CoNLL-2004) International Workshop on Se-mantic Evaluations (SemEval-2007).
Boston, MAAssociation for Computational Linguistics, pp.
89-97.Xavier Carreras and Luis Marquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task SemanticRole Labeling.
Proceedings of the Eighth Confe-rence on Computational Natural Language Learn-ing (CoNLL-2004) International Workshop on Se-mantic Evaluations (SemEval-2007).
Ann Arbor,MI Association for Computational Linguistics, pp.152-164.Kenneth C. Litkowski.
2004.
Senseval-3 Task: Auto-matic Labeling of Semantic Roles.
Proceedings ofSenseval-3: The Third International Workshop onthe Evaluation of Systems for the Semantic Analy-sis of Text.
Barcelona, Spain, Association forComputational Linguistics, pp.
9-12.Kenneth C. Litkowski.
2007.
CLR: Integration ofFrameNet in a Text Representation System.
Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007).
Prague,Czech Republic, Association for ComputationalLinguistics, pp.
113-6.303
