Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002?1010,Honolulu, October 2008. c?2008 Association for Computational LinguisticsWord Sense Disambiguation Using OntoNotes:An Empirical StudyZhi Zhong and Hwee Tou Ng and Yee Seng ChanDepartment of Computer ScienceNational University of SingaporeLaw Link, Singapore 117590{zhongzhi, nght, chanys}@comp.nus.edu.sgAbstractThe accuracy of current word sense disam-biguation (WSD) systems is affected by thefine-grained sense inventory of WordNet aswell as a lack of training examples.
Using theWSD examples provided through OntoNotes,we conduct the first large-scale WSD evalua-tion involving hundreds of word types and tensof thousands of sense-tagged examples, whileadopting a coarse-grained sense inventory.
Weshow that though WSD systems trained with alarge number of examples can obtain a highlevel of accuracy, they nevertheless suffer asubstantial drop in accuracy when applied toa different domain.
To address this issue, wepropose combining a domain adaptation tech-nique using feature augmentation with activelearning.
Our results show that this approachis effective in reducing the annotation effortrequired to adapt a WSD system to a new do-main.
Finally, we propose that one can maxi-mize the dual benefits of reducing the annota-tion effort while ensuring an increase in WSDaccuracy, by only performing active learningon the set of most frequently occurring wordtypes.1 IntroductionIn language, many words have multiple meanings.The process of identifying the correct meaning, orsense of a word in context, is known as word sensedisambiguation (WSD).
WSD is one of the funda-mental problems in natural language processing andis important for applications such as machine trans-lation (MT) (Chan et al, 2007a; Carpuat and Wu,2007), information retrieval (IR), etc.WSD is typically viewed as a classification prob-lem where each ambiguous word is assigned a senselabel (from a pre-defined sense inventory) during thedisambiguation process.
In current WSD research,WordNet (Miller, 1990) is usually used as the senseinventory.
WordNet, however, adopts a very finelevel of sense granularity, thus restricting the accu-racy of WSD systems.
Also, current state-of-the-artWSD systems are based on supervised learning andface a general lack of training data.To provide a standardized test-bed for evalua-tion of WSD systems, a series of evaluation exer-cises called SENSEVAL were held.
In the Englishall-words task of SENSEVAL-2 and SENSEVAL-3 (Palmer et al, 2001; Snyder and Palmer, 2004),no training data was provided and systems must tagall the content words (noun, verb, adjective, andadverb) in running English texts with their correctWordNet senses.
In SENSEVAL-2, the best per-forming system (Mihalcea and Moldovan, 2001) inthe English all-words task achieved an accuracy of69.0%, while in SENSEVAL-3, the best perform-ing system (Decadt et al, 2004) achieved an accu-racy of 65.2%.
In SemEval-2007, which was themost recent SENSEVAL evaluation, a similar En-glish all-words task was held, where systems had toprovide the correct WordNet sense tag for all theverbs and head words of their arguments in run-ning English texts.
For this task, the best perform-ing system (Tratz et al, 2007) achieved an accuracyof 59.1%.
Results of these evaluations showed thatstate-of-the-art English all-words WSD systems per-formed with an accuracy of 60%?70%, using thefine-grained sense inventory of WordNet.The low level of performance by these state-of-the-art WSD systems is a cause for concern, sinceWSD is supposed to be an enabling technologyto be incorporated as a module into applications1002such as MT and IR.
As mentioned earlier, one ofthe major reasons for the low performance is thatthese evaluation exercises adopted WordNet as thereference sense inventory, which is often too fine-grained.
As an indication of this, inter-annotatoragreement (ITA) reported for manual sense-taggingon these SENSEVAL English all-words datasets istypically in the mid-70s.
To address this issue, acoarse-grained English all-words task (Navigli et al,2007) was conducted during SemEval-2007.
Thistask used a coarse-grained version of WordNet andreported an ITA of around 90%.
We note that thebest performing system (Chan et al, 2007b) of thistask achieved a relatively high accuracy of 82.5%,highlighting the importance of having an appropri-ate level of sense granularity.Another issue faced by current WSD systems isthe lack of training data.
We note that the top per-forming systems mentioned in the previous para-graphs are all based on supervised learning.
Withthis approach, however, one would need to obtaina corpus where each ambiguous word occurrence ismanually annotated with the correct sense, to serveas training data.
Since it is time consuming to per-form sense annotation of word occurrences, only ahandful of sense-tagged corpora are publicly avail-able.
Among the existing sense-tagged corpora, theSEMCOR corpus (Miller et al, 1994) is one of themost widely used.
In SEMCOR, content words havebeen manually tagged with WordNet senses.
Cur-rent supervised WSD systems (which include allthe top-performing systems in the English all-wordstask) usually rely on this relatively small manuallyannotated corpus for training examples, and this hasinevitably affected the accuracy and scalability ofcurrent WSD systems.Related to the problem of a lack of training datafor WSD, there is also a lack of test data.
Havinga large amount of test data for evaluation is impor-tant to ensure the robustness and scalability of WSDsystems.
Due to the expensive process of manualsense-tagging, the SENSEVAL English all-wordstask evaluations were conducted on relatively smallsets of evaluation data.
For instance, the evaluationdata of SENSEVAL-2 and SENSEVAL-3 Englishall-words task consists of 2,473 and 2,041 test exam-ples respectively.
In SemEval-2007, the fine-grainedEnglish all-words task consists of only 465 test ex-amples, while the SemEval-2007 coarse-grained En-glish all-words task consists of 2,269 test examples.Hence, it is necessary to address the issues ofsense granularity, and the lack of both training andtest data.
To this end, a recent large-scale anno-tation effort called the OntoNotes project (Hovy etal., 2006) was started.
Building on the annotationsfrom the Wall Street Journal (WSJ) portion of thePenn Treebank (Marcus et al, 1993), the projectadded several new layers of semantic annotations,such as coreference information, word senses, etc.In its first release (LDC2007T21) through the Lin-guistic Data Consortium (LDC), the project man-ually sense-tagged more than 40,000 examples be-longing to hundreds of noun and verb types with anITA of 90%, based on a coarse-grained sense inven-tory, where each word has an average of only 3.2senses.
Thus, besides providing WSD examples thatwere sense-tagged with a high ITA, the project alsoaddressed the previously discussed issues of a lackof training and test data.In this paper, we use the sense-tagged data pro-vided by the OntoNotes project to investigate theaccuracy achievable by current WSD systems whenadopting a coarse-grained sense inventory.
Throughour experiments, we then highlight that domainadaptation for WSD is an important issue as it sub-stantially affects the performance of a state-of-the-art WSD system which is trained on SEMCOR butevaluated on sense-tagged examples in OntoNotes.To address this issue, we then show that by com-bining a domain adaptation technique using featureaugmentation with active learning, one only needsto annotate a small amount of in-domain examplesto obtain a substantial improvement in the accuracyof the WSD system which is previously trained onout-of-domain examples.The contributions of this paper are as follows.To our knowledge, this is the first large-scale WSDevaluation conducted that involves hundreds of wordtypes and tens of thousands of sense-tagged exam-ples, and that is based on a coarse-grained sense in-ventory.
The present study also highlights the practi-cal significance of domain adaptation in word sensedisambiguation in the context of a large-scale empir-ical evaluation, and proposes an effective method toaddress the domain adaptation problem.In the next section, we give a brief description of1003our WSD system.
In Section 3, we describe exper-iments where we conduct both training and evalu-ation using data from OntoNotes.
In Section 4, weinvestigate the WSD performance when we train oursystem on examples that are gathered from a differ-ent domain as compared to the OntoNotes evalua-tion data.
In Section 5, we perform domain adapta-tion experiments using a recently introduced featureaugmentation technique.
In Section 6, we investi-gate the use of active learning to reduce the annota-tion effort required to adapt our WSD system to thedomain of the OntoNotes data, before concluding inSection 7.2 The WSD SystemFor the experiments reported in this paper, we fol-low the supervised learning approach of (Lee andNg, 2002), by training an individual classifier foreach word using the knowledge sources of local col-locations, parts-of-speech (POS), and surroundingwords.For local collocations, we use 11 features:C?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,C?3,?1, C?2,1, C?1,2, and C1,3, where Ci,j refers tothe ordered sequence of tokens in the local contextof an ambiguous word w. Offsets i and j denote thestarting and ending position (relative to w) of the se-quence, where a negative (positive) offset refers to atoken to its left (right).
For parts-of-speech, we use7 features: P?3, P?2, P?1, P0, P1, P2, P3, whereP0 is the POS of w, and P?i (Pi) is the POS of theith token to the left (right) of w. For surroundingwords, we consider all unigrams (single words) inthe surrounding context of w. These words can be ina different sentence from w. For our experiments re-ported in this paper, we use support vector machines(SVM) as our learning algorithm, which was shownto achieve good WSD performance in (Lee and Ng,2002; Chan et al, 2007b).3 Training and Evaluating on OntoNotesThe annotated data of OntoNotes is drawn from theWall Street Journal (WSJ) portion of the Penn Tree-bank corpus, divided into sections 00-24.
TheseWSJ documents have been widely used in variousNLP tasks such as syntactic parsing (Collins, 1999)and semantic role labeling (SRL) (Carreras and Mar-Section No.
of No.
of word tokensword types Individual Cumulative02 248 425 42503 79 107 53204 186 389 92105 287 625 154606 224 446 199207 270 549 254108 177 301 284209 308 677 351910 648 3048 656711 724 4071 1063812 740 4296 1493413 749 4577 1951114 710 3900 2341115 748 4768 2817916 306 576 2875517 219 398 2915318 266 566 2971919 219 389 3010820 288 536 3064421 262 470 3111423 685 3755 -Table 1: Size of the sense-tagged data in the various WSJsections.quez, 2005).
In these tasks, the practice is to usedocuments from WSJ sections 02-21 as training dataand WSJ section 23 as test data.
Hence for our ex-periments reported in this paper, we follow this con-vention and use the annotated instances from WSJsections 02-21 as our training data, and instances inWSJ section 23 as our test data.As mentioned in Section 1, the OntoNotes dataprovided WSD examples for a large number ofnouns and verbs, which are sense-tagged accord-ing to a coarse-grained sense inventory.
In Table 1,we show the amount of sense-tagged data availablefrom OntoNotes, across the various WSJ sections.1In the table, for each WSJ section, we list the num-ber of word types, the number of sense-tagged ex-amples, and the cumulative count on the number of1We removed erroneous examples which were simplytagged with ?XXX?
as sense-tag, or tagged with senses that werenot found in the sense-inventory provided.
Also, since we willbe comparing against training on SEMCOR later (which wastagged using WordNet senses), we removed examples taggedwith OntoNotes senses which were not mapped to WordNetsenses.
On the whole, about 7% of the original OntoNotes ex-amples were removed as a result.1004sense-tagged examples.
From the table, we see thatsections 02-21, which will be used as training datain our experiments, contain a total of slightly over31,000 sense-tagged examples.Using examples from sections 02-21 as trainingdata, we trained our WSD system and evaluated onthe examples from section 23.
In our experiments,if a word type in section 23 has no training exam-ples from sections 02-21, we randomly select anOntoNotes sense as the answer.
Using these ex-perimental settings, our WSD system achieved anaccuracy of 89.1%.
We note that this accuracy ismuch higher than the 60%?70% accuracies achievedby state-of-the-art English all-words WSD systemswhich are trained using the fine-grained sense inven-tory of WordNet.
Hence, this highlights the impor-tance of having an appropriate level of sense granu-larity.Besides training on the entire set of examplesfrom sections 02-21, we also investigated the per-formance achievable from training on various sub-sections of the data and show these results as ?ON?in Figure 1.
From the figure, we see that WSD accu-racy increases as we add more training examples.The fact that current state-of-the-art WSD sys-tems are able to achieve a high level of perfor-mance is important, as this means that WSD systemswill potentially be more usable for inclusion in end-applications.
For instance, the high level of perfor-mance by syntactic parsers allows it to be used as anenabling technology in various NLP tasks.
Here, wenote that the 89.1% WSD accuracy we obtained iscomparable to state-of-the-art syntactic parsing ac-curacies, such as the 91.0% performance by the sta-tistical parser of Charniak and Johnson (2005).4 Building WSD Systems withOut-of-Domain DataAlthough our WSD system had achieved a highaccuracy of 89.1%, this was achieved by train-ing on a large amount (about 31,000) of manuallysense annotated examples from sections 02-21 of theOntoNotes data.
Further, all these training data andtest data are gathered from the same domain of WSJ.In reality, however, since manual sense annotation istime consuming, it is not feasible to collect such alarge amount of manually sense-tagged data for ev-ery domain of interest.
Hence, in this section, we in-vestigate the performance of our WSD system whenit is trained on out-of-domain data.In the English all-words task of the previous SEN-SEVAL evaluations (SENSEVAL-2, SENSEVAL-3, SemEval-2007), the best performing Englishall-words task systems with the highest WSD ac-curacy were trained on SEMCOR (Mihalcea andMoldovan, 2001; Decadt et al, 2004; Chan et al,2007b).
Hence, we similarly trained our WSD sys-tem on SEMCOR and evaluated on section 23 of theOntoNotes corpus.
For those word types in section23 which do not have training examples from SEM-COR, we randomly chose an OntoNotes sense asthe answer.
In training on SEMCOR, we have alsoensured that there is a domain difference betweenour training and test data.
This is because whilethe OntoNotes data was gathered from WSJ, whichcontains mainly business related news, the SEMCORcorpus is the sense-tagged portion of the Brown Cor-pus (BC), which is a mixture of several genres suchas scientific texts, fictions, etc.Evaluating on the section 23 test data, our WSDsystem achieved only 76.2% accuracy.
Compared tothe 89.1% accuracy achievable when we had trainedon examples from sections 02-21, this is a substan-tially lower and disappointing drop of performanceand motivates the need for domain adaptation.The need for domain adaptation is a general andimportant issue for many NLP tasks (Daume III andMarcu, 2006).
For instance, SRL systems are usu-ally trained and evaluated on data drawn from theWSJ.
In the CoNLL-2005 shared task on SRL (Car-reras and Marquez, 2005), however, a task of train-ing and evaluating systems on different domains wasincluded.
For that task, systems that were trained onthe PropBank corpus (Palmer et al, 2005) (whichwas gathered from the WSJ), suffered a 10% dropin accuracy when evaluated on test data drawn fromBC, as compared to the performance achievablewhen evaluated on data drawn from WSJ.
More re-cently, CoNLL-2007 included a shared task on de-pendency parsing (Nivre et al, 2007).
In this task,systems that were trained on Penn Treebank (drawnfrom WSJ), but evaluated on data drawn from adifferent domain (such as chemical abstracts andparent-child dialogues) showed a similar drop in per-formance.
For research involving training and eval-100555606570758085909510002 02-0302-0402-0502-0602-0702-0802-0902-1002-1202-1402-21WSDaccuracy(%)Section numberWSD Accuracies on Section 2359.276.877.560.577.177.564.477.177.673.378.9 80.376.879.3 80.980.279.9 82.180.580.5 82.681.680.8 83.1 85.883.3 85.6 87.586.1 87.6 88.387.2 88.789.187.9 88.9ONSC+ONSC+ON AugmentFigure 1: WSD accuracies evaluated on section 23, using SEMCOR and different OntoNotes sections as trainingdata.
ON: only OntoNotes as training data.
SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment:Combining SEMCOR and OntoNotes via the Augment domain adaptation technique.uating WSD systems on data drawn from differentdomains, several prior research efforts (Escudero etal., 2000; Martinez and Agirre, 2000) observed asimilar drop in performance of about 10% when aWSD system that was trained on the BC part of theDSO corpus was evaluated on the WSJ part of thecorpus, and vice versa.In the rest of this paper, we perform domain adap-tation experiments for WSD, focusing on domainadaptation methods that use in-domain annotateddata.
In particular, we use a feature augmentationtechnique recently introduced by Daume III (2007),and active learning (Lewis and Gale, 1994) to per-form domain adaptation of WSD systems.5 Combining In-Domain andOut-of-Domain Data for TrainingIn this section, we will first introduce the AUGMENTtechnique of Daume III (2007), before showing theperformance of our WSD system with and withoutusing this technique.5.1 The AUGMENT technique for DomainAdaptationThe AUGMENT technique introduced by Daume III(2007) is a simple yet very effective approach to per-forming domain adaptation.
This technique is appli-cable when one has access to training data from thesource domain and a small amount of training datafrom the target domain.The technique essentially augments the featurespace of an instance.
Assuming x is an instance andits original feature vector is ?
(x), the augmentedfeature vector for instance x is??
(x) ={< ?(x),?
(x),0 > if x ?
Ds< ?(x),0,?
(x) > if x ?
Dt,where 0 is a zero vector of size |?
(x)|, Ds andDt are the sets of instances from the source andtarget domains respectively.
We see that the tech-nique essentially treats the first part of the aug-mented feature space as holding general features thatare not meant to be differentiated between different1006domains.
Then, different parts of the augmented fea-ture space are reserved for holding source domainspecific, or target domain specific features.
Despiteits relative simplicity, this AUGMENT technique hasbeen shown to outperform other domain adaptationtechniques on various tasks such as named entityrecognition, part-of-speech tagging, etc.5.2 Experimental ResultsAs mentioned in Section 4, training our WSD sys-tem on SEMCOR examples gave a relatively low ac-curacy of 76.2%, as compared to the 89.1% accuracyobtained from training on the OntoNotes section 02-21 examples.
Assuming we have access to some in-domain training data, then a simple method to poten-tially obtain better accuracies is to train on both theout-of-domain and in-domain examples.
To investi-gate this, we combined the SEMCOR examples withvarious amounts of OntoNotes examples to train ourWSD system and show the resulting ?SC+ON?
ac-curacies obtained in Figure 1.
We also performedanother set of experiments, where instead of simplycombining the SEMCOR and OntoNotes examples,we applied the AUGMENT technique when combin-ing these examples, treating SEMCOR examples asout-of-domain (source domain) data and OntoNotesexamples as in-domain (target domain) data.
Wesimilarly show the resulting accuracies as ?SC+ONAugment?
in Figure 1.Comparing the ?SC+ON?
and ?SC+ON Aug-ment?
accuracies in Figure 1, we see that the AUG-MENT technique always helps to improve the ac-curacy of our WSD system.
Further, notice fromthe first few sets of results in the figure that whenwe have access to limited in-domain training exam-ples from OntoNotes, incorporating additional out-of-domain training data from SEMCOR (either usingthe strategies ?SC+ON?
or ?SC+ON Augment?
)achieves better accuracies than ?ON?.
Significancetests using one-tailed paired t-test reveal that theseaccuracy improvements are statistically significantat the level of significance 0.01 (all significance testsin the rest of this paper use the same level of signif-icance 0.01).
These results validate the contributionof the SemCor examples.
This trend continues tillthe result for sections 02-06.The right half of Figure 1 shows the accuracytrend of the various strategies, in the unlikely eventDS ?
the set of SEMCOR training examplesDA?
the set of OntoNotes sections 02-21 examplesDT ?
emptywhile DA 6= ?pmin ???
?WSD system trained on DS and DT using AUGMENTtechniquefor each d ?
DA dobs?
word sense prediction for d using ?p?
confidence of prediction bsif p < pmin thenpmin?
p, dmin ?
dendendDA?
DA ?
{dmin}provide correct sense s for dmin and add dmin to DTendFigure 2: The active learning algorithm.that we have access to a large amount of in-domaintraining examples.
Although we observe that inthis scenario, ?ON?
performs better than ?SC+ON?,?SC+ON Augment?
continues to perform betterthan ?ON?
(where the improvement is statisticallysignificant) till the result for sections 02-09.
Beyondthat, as we add more OntoNotes examples, signif-icance testing reveals that the ?SC+ON Augment?and ?ON?
strategies give comparable performance.This means that the ?SC+ON Augment?
strategy,besides giving good performance when one has fewin-domain examples, does continue to perform welleven when one has a large number of in-domain ex-amples.6 Active Learning with AUGMENTTechniqueSo far in this paper, we have seen that when we haveaccess to some in-domain examples, a good strategyis to combine the out-of-domain and in-domain ex-amples via the AUGMENT technique.
This suggeststhat when one wishes to apply a WSD system to anew domain of interest, it is worth the effort to an-notate a small number of examples gathered fromthe new domain.
However, instead of randomly se-lecting in-domain examples to annotate, we coulduse active learning (Lewis and Gale, 1994) to helpselect in-domain examples to annotate.
By doingso, we could minimize the manual annotation effortneeded.1007WSD Accuracies on Section 237678808284868890SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34Iteration NumberWSDAccuracy(%)50 100 150 200300 400 500 allFigure 3: Results of applying active learning with the AUGMENT technique on different number of word types.
Eachcurve represents the adaptation process of applying active learning on a certain number of most frequently occurringword types.In WSD, several prior research efforts have suc-cessfully used active learning to reduce the annota-tion effort required (Zhu and Hovy, 2007; Chan andNg, 2007; Chen et al, 2006; Fujii et al, 1998).
Withthe exception of (Chan and Ng, 2007) which triedto adapt a WSD system trained on the BC part ofthe DSO corpus to the WSJ part of the DSO corpus,the other researchers simply applied active learningto reduce the annotation effort required and did notdeal with the issue of adapting a WSD system to anew domain.
Also, these prior research efforts onlyexperimented with a few word types.
In contrast, weperform active learning experiments on the hundredsof word types in the OntoNotes data, with the aim ofadapting our WSD system trained on SEMCOR tothe WSJ domain represented by the OntoNotes data.For our active learning experiments, we use theuncertainty sampling strategy (Lewis and Gale,1994), as shown in Figure 2.
For our experiments,the SEMCOR examples will be our initial set oftraining examples, while the OntoNotes examplesfrom sections 02-21 will be used as our pool ofadaptation examples, from which we will select ex-amples to annotate via active learning.
Also, sincewe have found that the AUGMENT technique is use-ful in increasing WSD accuracy, we will apply theAUGMENT technique during each iteration of activelearning to combine the SEMCOR examples and theselected adaptation examples.As shown in Figure 2, we train an initial WSDsystem using only the set DS of SEMCOR exam-ples.
We then apply our WSD system on the set DAof OntoNotes adaptation examples.
The example inDA which is predicted with the lowest confidencewill be removed from DA and added to the set DTof in-domain examples that have been selected viaactive learning thus far.
We then use the AUGMENTtechnique to combine the set of examples in DS andDT to train a new WSD system, which is then ap-plied again on the set DA of remaining adaptationexamples, and this active learning process continuesuntil we have used up all the adaptation examples.Note that because we are using OntoNotes sections02-21 (which have already been sense-tagged be-forehand) as our adaptation data, the annotation ofthe selected example during each active learning it-eration is simply simulated by referring to its taggedsense.6.1 Experimental ResultsAs mentioned earlier, we use the examples inOntoNotes sections 02-21 as our adaptation exam-1008ples during active learning.
Hence, we performactive learning experiments on all the word typesthat have sense-tagged examples from OntoNotessections 02-21, and show the evaluation results onOntoNotes section 23 as the topmost ?all?
curve inFigure 3.
Since our aim is to reduce the human an-notation effort required in adapting a WSD systemto a new domain, we may not want to perform activelearning on all the word types in practice.
Instead,we can maximize the benefits by performing activelearning only on the more frequently occurring wordtypes.
Hence, in Figure 3, we also show via var-ious curves the results of applying active learningonly to various sets of word types, according to theirfrequency, or number of sense-tagged examples inOntoNotes sections 02-21.
Note that the various ac-curacy curves in Figure 3 are plotted in terms ofevaluation accuracies over all the test examples inOntoNotes section 23, hence they are directly com-parable to the results reported thus far in this pa-per.
Also, since the accuracies for the various curvesstabilize after 35 active learning iterations, we onlyshow the results of the first 35 iterations.From Figure 3, we note that by performing ac-tive learning on the set of 150 most frequently oc-curring word types, we are able to achieve a WSDaccuracy of 82.6% after 10 active learning iterations.Note that in Section 4, we mentioned that trainingonly on the out-of-domain SEMCOR examples gavean accuracy of 76.2%.
Hence, we have gained anaccuracy improvement of 6.4% (82.6% ?
76.2%)by just using 1,500 in-domain OntoNotes examples.Compared with the 12.9% (89.1% ?
76.2%) im-provement in accuracy achieved by using all 31,114OntoNotes sections 02-21 examples, we have ob-tained half of this maximum increase in accuracy, byrequiring only about 5% (1,500/31,114) of the totalnumber of sense-tagged examples.
Based on theseresults, we propose that when there is a need to applya previously trained WSD system to a different do-main, one can apply the AUGMENT technique withactive learning on the most frequent word types, togreatly reduce the annotation effort required whileobtaining a substantial improvement in accuracy.7 ConclusionUsing the WSD examples made available throughOntoNotes, which are sense-tagged according to acoarse-grained sense inventory, we show that ourWSD system is able to achieve a high accuracyof 89.1% when we train and evaluate on these ex-amples.
However, when we apply a WSD systemthat is trained on SEMCOR, we suffer a substan-tial drop in accuracy, highlighting the need to per-form domain adaptation.
We show that by com-bining the AUGMENT domain adaptation techniquewith active learning, we are able to effectively re-duce the amount of annotation effort required for do-main adaptation.ReferencesM.
Carpuat and D. Wu.
2007.
Improving Statistical Ma-chine Translation Using Word Sense Disambiguation.In Proc.
of EMNLP-CoNLL07, pages 61?72.X.
Carreras and L. Marquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.In Proc.
of CoNLL-2005, pages 152?164.Y.
S. Chan and H. T. Ng.
2007.
Domain Adaptation withActive Learning for Word Sense Disambiguation.
InProc.
of ACL07, pages 49?56.Y.
S. Chan, H. T. Ng, and D. Chiang.
2007a.
Word SenseDisambiguation Improves Statistical Machine Transla-tion.
In Proc.
of ACL07, pages 33?40.Y.
S. Chan, H. T. Ng, and Z. Zhong.
2007b.
NUS-PT: Ex-ploiting Parallel Texts for Word Sense Disambiguationin the English All-Words Tasks.
In Proc.
of SemEval-2007, pages 253?256.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proc.
of ACL05, pages 173?180.J.
Y. Chen, A. Schein, L. Ungar, and M. Palmer.
2006.An Empirical Study of the Behavior of Active Learn-ing for Word Sense Disambiguation.
In Proc.
ofHLT/NAACL06, pages 120?127.M.
Collins.
1999.
Head-Driven Statistical Model forNatural Language Parsing.
PhD dissertation, Univer-sity of Pennsylvania.H.
Daume III and D. Marcu.
2006.
Domain Adaptationfor Statistical Classifiers.
Journal of Artificial Intelli-gence Research, 26:101?126.H.
Daume III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proc.
of ACL07, pages 256?263.B.
Decadt, V. Hoste, and W. Daelemans.
2004.
GAMBL,Genetic Algorithm Optimization of Memory-BasedWSD.
In Proc.
of SENSEVAL-3, pages 108?112.1009G.
Escudero, L. Marquez, and G. Riagu.
2000.
AnEmpirical Study of the Domain Dependence of Super-vised Word Sense Disambiguation Systems.
In Proc.of EMNLP/VLC00, pages 172?180.A.
Fujii, K. Inui, T. Tokunaga, and H. Tanaka.
1998.
Se-lective Sampling for Example-based Word Sense Dis-ambiguation.
Computational Linguistics, 24(4).E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% solution.In Proc.
of HLT-NAACL06, pages 57?60.Y.
K. Lee and H. T. Ng.
2002.
An Empirical Evaluationof Knowledge Sources and Learning Algorithms forWord Sense Disambiguation.
In Proc.
of EMNLP02,pages 41?48.D.
D. Lewis and W. A. Gale.
1994.
A Sequential Al-gorithm for Training Text Classifiers.
In Proc.
of SI-GIR94.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.D.
Martinez and E. Agirre.
2000.
One Sense perCollocation and Genre/Topic Variations.
In Proc.
ofEMNLP/VLC00, pages 207?215.R.
Mihalcea and D. Moldovan.
2001.
Pattern Learningand Active Feature Selection for Word Sense Disam-biguation.
In Proc.
of SENSEVAL-2, pages 127?130.G.
A. Miller, M. Chodorow, S. Landes, C. Leacock, andR.
G. Thomas.
1994.
Using a Semantic Concordancefor Sense Identification.
In Proc.
of ARPA HumanLanguage Technology Workshop, pages 240?243.G.
A. Miller.
1990.
WordNet: An On-line Lexi-cal Database.
International Journal of Lexicography,3(4):235?312.R.
Navigli, K. C. Litkowski, and O. Hargraves.
2007.SemEval-2007 Task 07: Coarse-Grained English All-Words Task.
In Proc.
of SemEval-2007, pages 30?35.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proc.
ofEMNLP-CoNLL07, pages 915?932.M.
Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T.Dang.
2001.
English Tasks: All-Words and Verb Lex-ical Sample.
In Proc.
of SENSEVAL-2, pages 21?24.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?105.B.
Snyder and M. Palmer.
2004.
The English All-WordsTask.
In Proc.
of SENSEVAL-3, pages 41?43.S.
Tratz, A. Sanfilippo, M. Gregory, A. Chappell,C.
Posse, and P. Whitney.
2007.
PNNL: A SupervisedMaximum Entropy Approach to Word Sense Disam-biguation.
In Proc.
of SemEval-2007, pages 264?267.J.
B. Zhu and E. Hovy.
2007.
Active Learning for WordSense Disambiguation with Methods for Addressingthe Class Imbalance Problem.
In Proc.
of EMNLP-CoNLL07, pages 783?790.1010
