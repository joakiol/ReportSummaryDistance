Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 765?774,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsKneser-Ney Smoothing on Expected CountsHui ZhangDepartment of Computer ScienceUniversity of Southern Californiahzhang@isi.eduDavid ChiangInformation Sciences InstituteUniversity of Southern Californiachiang@isi.eduAbstractWidely used in speech and language pro-cessing, Kneser-Ney (KN) smoothing hasconsistently been shown to be one ofthe best-performing smoothing methods.However, KN smoothing assumes integercounts, limiting its potential uses?for ex-ample, inside Expectation-Maximization.In this paper, we propose a generaliza-tion of KN smoothing that operates onfractional counts, or, more precisely, ondistributions over counts.
We rederive allthe steps of KN smoothing to operateon count distributions instead of integralcounts, and apply it to two tasks whereKN smoothing was not applicable before:one in language model adaptation, and theother in word alignment.
In both cases,our method improves performance signifi-cantly.1 IntroductionIn speech and language processing, smoothing isessential to reduce overfitting, and Kneser-Ney(KN) smoothing (Kneser and Ney, 1995; Chenand Goodman, 1999) has consistently proven to beamong the best-performing and most widely usedmethods.
However, KN smoothing assumes inte-ger counts, whereas in many NLP tasks, traininginstances appear with possibly fractional weights.Such cases have been noted for language model-ing (Goodman, 2001; Goodman, 2004), domainadaptation (Tam and Schultz, 2008), grapheme-to-phoneme conversion (Bisani and Ney, 2008), andphrase-based translation (Andr?es-Ferrer, 2010;Wuebker et al, 2012).For example, in Expectation-Maximization(Dempster et al, 1977), the Expectation (E) stepcomputes the posterior distribution over possi-ble completions of the data, and the Maximiza-tion (M) step reestimates the model parameters asif that distribution had actually been observed.
Inmost cases, the M step is identical to estimatingthe model from complete data, except that countsof observations from the E step are fractional.
Itis common to apply add-one smoothing to theM step, but we cannot apply KN smoothing.Another example is instance weighting.
If weassign a weight to each training instance to indi-cate how important it is (say, its relevance to a par-ticular domain), and the counts are not integral,then we again cannot train the model using KNsmoothing.In this paper, we propose a generalization of KNsmoothing (called expected KN smoothing) thatoperates on fractional counts, or, more precisely,on distributions over counts.
We rederive all thesteps of KN smoothing to operate on count distri-butions instead of integral counts.
We demonstratehow to apply expected KN to two tasks where KNsmoothing was not applicable before.
One is lan-guage model domain adaptation, and the other isword alignment using the IBM models (Brown etal., 1993).
In both tasks, expected KN smoothingimproves performance significantly.2 Smoothing on integral countsBefore presenting our method, we review KNsmoothing on integer counts as applied to lan-guage models, although, as we will demonstratein Section 7, KN smoothing is applicable to othertasks as well.2.1 Maximum likelihood estimationLet uw stand for an n-gram, where u stands forthe (n ?
1) context words and w, the predictedword.
Let c(uw) be the number of occurrencesof uw.
We use a bullet (?)
to indicate summa-tion over words, that is, c(u?)
=?wc(uw).
Undermaximum-likelihood estimation (MLE), we max-765imizeL =?uwc(uw) log p(w | u),obtaining the solutionpmle(w | u) =c(uw)c(u?).
(1)2.2 Absolute discountingAbsolute discounting (Ney et al, 1994) ?
on whichKN smoothing is based ?
tries to generalize bet-ter to unseen data by subtracting a discount fromeach seen n-gram?s count and distributing the sub-tracted discounts to unseen n-grams.
For now, weassume that the discount is a constant D, so thatthe smoothed counts arec?
(uw) =??????
?c(uw) ?
D if c(uw) > 0n1+(u?
)Dqu(w) otherwisewhere n1+(u?)
= |{w | c(uw) > 0}| is the numberof word types observed after context u, and qu(w)specifies how to distribute the subtracted discountsamong unseen n-gram types.
Maximizing the like-lihood of the smoothed counts c?, we getp(w | u) =??????????????????
?c(uw) ?
Dc(u?
)if c(uw) > 0n1+(u?)Dqu(w)c(u?)otherwise.
(2)How to choose D and qu(w) are described in thenext two sections.2.3 Estimating D by leaving-one-outThe discount D can be chosen by various means;in absolute discounting, it is chosen by the methodof leaving one out.
Given N training instances, weform the probability of each instance under theMLE using the other (N ?
1) instances as train-ing data; then we maximize the log-likelihood ofall those instances.
The probability of an n-gramtoken uw using the other tokens as training data isploo(w | u) =??????????????????
?c(uw) ?
1 ?
Dc(u?)
?
1c(uw) > 1(n1+(u?)
?
1)Dqu(w)c(u?)
?
1c(uw) = 1.We want to find the D that maximizes theleaving-one-out log-likelihoodLloo=?uwc(uw) log ploo(w | u)=?uw|c(uw)>1c(uw) logc(uw) ?
1 ?
Dc(u?)
?
1+?uw|c(uw)=1log(n1+(u?)
?
1)Dqu(w)c(u?)
?
1=?r>1rnrlog(r ?
1 ?
D) + n1logD +C, (3)where nr= |{uw | c(uw) = r}| is the number of n-gram types appearing r times, and C is a constantnot depending on D. Setting the partial derivativewith respect to D to zero, we have?Lloo?D= ?
?r>1rnrr ?
1 ?
D+n1Dn1D=?r>1rnrr ?
1 ?
D?2n21 ?
D.Solving for D, we haveD ?n1n1+ 2n2.
(4)Theoretically, we can use iterative methods to op-timize D. But in practice, setting D to this upperbound is effective and simple (Ney et al, 1994;Chen and Goodman, 1999).2.4 Estimating the lower-order distributionFinally, qu(w) is defined to be proportional to an(n ?
1)-gram model p?
(w | u?
), where u?is the(n ?
2)-gram suffix of u.
That is,qu(w) = ?(u)p?
(w | u?
),where ?
(u) is an auxiliary function chosen to makethe distribution p(w | u) in (2) sum to one.Absolute discounting chooses p?
(w | u?)
to bethe maximum-likelihood unigram distribution; un-der KN smoothing (Kneser and Ney, 1995), it ischosen to make p in (2) satisfy the following con-straint for all (n ?
1)-grams u?w:pmle(u?w) =?vp(w | vu?)pmle(vu?).
(5)Substituting in the definition of pmlefrom (1) andp from (2) and canceling terms, we getc(u?w) =?v|c(vu?w)>0(c(vu?w) ?
D)+?v|c(vu?w)=0n1+(vu??)D?(vu?)p?
(w | u?
).766Solving for p?
(w | u?
), we havep?
(w | u?)
=?v|c(vu?w)>01?v|c(vu?w)=0n1+(vu??)?(vu?
).Kneser and Ney assume the denominator is con-stant in w and renormalize to get an approximationp?
(w | u?)
?n1+(?u?w)n1+(?u??
), (6)wheren1+(?u?w) = |{v | c(vu?w) > 0}|n1+(?u??)
=?wn1+(?u?w).3 Count distributionsThe computation of D and p?above made use ofnrand nr+, which presupposes integer counts.
Butin many applications, the counts are not integral,but fractional.
How do we apply KN smoothing insuch cases?
In this section, we introduce count dis-tributions as a way of circumventing this problem.3.1 DefinitionIn the E step of EM, we compute a probability dis-tribution (according to the current model) over allpossible completions of the observed data, and theexpected counts of all types, which may be frac-tional.
However, note that in each completion ofthe data, the counts are integral.
Although it doesnot make sense to compute nror nr+on fractionalcounts, it does make sense to compute them onpossible completions.In other situations where fractional counts arise,we can still think of the counts as expectations un-der some distribution over possible ?realizations?of the data.
For example, if we assign a weightbetween zero and one to every instance in a cor-pus, we can interpret each instance?s weight as theprobability of that instance occurring or not, yield-ing a distribution over possible subsets of the data.Let X be a random variable ranging over pos-sible realizations of the data, and let cX(uw) bethe count of uw in realization X.
The expecta-tion E[cX(uw)] is the familiar fractional expectedcount of uw, but we can also compute the proba-bilities p(cX(uw) = r) for any r. From now on, forbrevity, we drop the subscript X and understandc(uw) to be a random variable depending on X.The nr(u?)
and nr+(u?)
and related quantities alsobecome random variables depending on X.For example, suppose that our data consists ofthe following bigrams, with their weights:(a) fat cat 0.3(b) fat cat 0.8(c) big dog 0.9We can interpret this as a distribution over eightsubsets (not all distinct), with probabilities:?
0.7 ?
0.2 ?
0.1 = 0.014{a} 0.3 ?
0.2 ?
0.1 = 0.006{b} 0.7 ?
0.8 ?
0.1 = 0.056{a, b} 0.3 ?
0.8 ?
0.1 = 0.024{c} 0.7 ?
0.2 ?
0.9 = 0.126{a, c} 0.3 ?
0.2 ?
0.9 = 0.054{b, c} 0.7 ?
0.8 ?
0.9 = 0.504{a, b, c} 0.3 ?
0.8 ?
0.9 = 0.216Then the count distributions and the E[nr] are:r = 1 r = 2 r > 0p(c(fat cat) = r) 0.62 0.24 0.86p(c(big dog) = r) 0.9 0 0.9E[nr] 1.52 0.243.2 Efficient computationHow to compute these probabilities and expecta-tions depends in general on the structure of themodel.
If we assume that all occurrences of uware independent (although in fact they are not al-ways), the computation is very easy.
If there arek occurrences of uw, each occurring with proba-bility pi, the count c(uw) is distributed accordingto the Poisson-binomial distribution (Hong, 2013).The expected count E[c(uw)] is just?ipi, and thedistribution of c(uw) can be computed as follows:p(c(uw) = r) = s(k, r)where s(k, r) is defined by the recurrences(k, r) =??????????????????
?s(k ?
1, r)(1 ?
pk)+ s(k ?
1, r ?
1)pkif 0 ?
r ?
k1 if k = r = 00 otherwise.We can also computep(c(uw) ?
r) = max{s(m, r), 1 ?
?r?<rs(m, r?
)},the floor operation being needed to protect againstrounding errors, and we can computeE[nr(u?)]
=?wp(c(uw) = r)E[nr+(u?)]
=?wp(c(uw) ?
r).Since, as we shall see, we only need to computethese quantities up to a small value of r (2 or 4),this takes time linear in k.7674 Smoothing on count distributionsWe are now ready to describe how to apply KNsmoothing to count distributions.
Below, we reca-pitulate the derivation of KN smoothing presentedin Section 2, using the expected log-likelihoodin place of the log-likelihood and applying KNsmoothing to each possible realization of the data.4.1 Maximum likelihood estimationThe MLE objective function is the expected log-likelihood,E[L] = E???????
?uwc(uw) log p(w | u)??????
?=?uwE[c(uw)] log p(w | u)whose maximum ispmle(w | u) =E[c(uw)]E[c(u?)].
(7)4.2 Absolute discountingIf we apply absolute discounting to every realiza-tion of the data, the expected smoothed counts areE[c?
(uw)] =?r>0p(c(uw) = r)(r ?
D)+ p(c(uw) = 0)E[n1+(u?
)]Dqu(w)= E[c(uw)] ?
p(c(uw) > 0)D+ p(c(uw) = 0)E[n1+(u?
)]Dqu(w) (8)where, to be precise, the expectation E[n1+(u?
)]should be conditioned on c(uw) = 0; in practice, itseems safe to ignore this.
The MLE is thenp(w | u) =E[c?(uw)]E[c?(u?)].
(9)4.3 Estimating D by leaving-one-outIt would not be clear how to perform leaving-one-out estimation on fractional counts, but herewe have a distribution over realizations of thedata, each with integral counts, and we canperform leaving-one-out estimation on each ofthese.
In other words, our goal is to find the Dthat maximizes the expected leaving-one-out log-likelihood, which is just the expected value of (3):E[Lloo] = E[n1logD +?r>1rnrlog(r ?
1 ?
D) +C]= E[n1] logD+?r>1rE[nr] log(r ?
1 ?
D) +C,where C is a constant not depending on D. Wehave made the assumption that the nrare indepen-dent.By exactly the same reasoning as before, we ob-tain an upper bound for D:D ?E[n1]E[n1] + 2E[n2].
(10)In our example above, D =1.521.52+2?0.24= 0.76.4.4 Estimating the lower-order distributionWe again require p?to satisfy the marginal con-straint (5).
Substituting in (7) and solving for p?asin Section 2.4, we obtain the solutionp?
(w | u?)
=E[n1+(?u?w)]E[n1+(?u??)].
(11)For the example above, the estimates for the un-igram model p?
(w) arep?
(cat) =0.860.86+0.9?
0.489p?
(dog) =0.90.86+0.9?
0.511.4.5 ExtensionsChen and Goodman (1999) introduce three exten-sions to Kneser-Ney smoothing which are nowstandard.
For our experiments, we used all three,for both integral counts and count distributions.4.5.1 InterpolationIn interpolated KN smoothing, the subtracted dis-counts are redistributed not only among unseenevents but also seen events.
That is,c?
(uw) = max{0, c(uw) ?
D} + n1+(u?)Dp?
(w | u?
).In this case, ?
(u) is always equal to one, so thatqu(w) = p?
(w | u?).
(Also note that (6) becomesan exact solution to the marginal constraint.)
The-oretically, this requires us to derive a new estimatefor D. However, as this is not trivial, nearly all im-plementations simply use the original estimate (4).On count distributions, the smoothed counts be-comeE[c?
(uw)] = E[c(uw)] ?
p(c(uw) > 0)D+ E[n1+(u?)]Dp?
(w | u?).
(12)In our example, the smoothed counts are:uw E[c?
]fat cat 1.1 ?
0.86 ?
0.76 + 0.86 ?
0.76 ?
0.489 ?
0.766fat dog 0 ?
0 ?
0.76 + 0.86 ?
0.76 ?
0.511 ?
0.334big cat 0 ?
0 ?
0.76 + 0.9 ?
0.76 ?
0.489 ?
0.334big dog 0.9 ?
0.9 ?
0.76 + 0.9 ?
0.76 ?
0.511 ?
0.566768which give the smoothed probability estimates:p(cat | fat) =0.7660.766+0.334= 0.696p(dog | fat) =0.3340.766+0.334= 0.304p(dog | big) =0.3340.334+0.556= 0.371p(cat | big) =0.5560.334+0.556= 0.629.4.5.2 Modified discountsModified KN smoothing uses a different discountDrfor each count r < 3, and a discount D3+forcounts r ?
3.
On count distributions, a similar ar-gument to the above leads to the estimates:D1?
1 ?
2YE[n2]E[n1]D2?
2 ?
3YE[n3]E[n2]D3+?
3 ?
4YE[n4]E[n3]Y =E[n1]E[n1] + 2E[n2].
(13)One side-effect of this change is that (6) is nolonger the correct solution to the marginal con-straint (Teh, 2006; Sundermeyer et al, 2011).
Al-though this problem can be fixed, standard imple-mentations simply use (6).4.5.3 Recursive smoothingIn the original KN method, the lower-ordermodel p?was estimated using (6); recursive KNsmoothing applies KN smoothing to p?.
To do this,we need to reconstruct counts whose MLE is (6).On integral counts, this is simple: we generate, foreach n-gram type vu?w, an (n?1)-gram token u?w,for a total of n1+(?u?w) tokens.
We then apply KNsmoothing to these counts.Analogously, on count distributions, for each n-gram type vu?w, we generate an (n ?
1)-gram to-ken u?w with probability p(c(vu?w) > 0).
SinceE[c(u?w)] =?vp(c(vu?w) > 0) = E[n1+(?u?w)],this has (11) as its MLE and therefore satisfies themarginal constraint.
We then apply expected KNsmoothing to these count distributions.For the example above, the count distributionsused for the unigram distribution would be:r = 0 r = 1p(c(cat) = r) 0.14 0.86p(c(dog) = r) 0.1 0.94.6 SummaryIn summary, to perform expected KN smoothing(either the original version or Chen and Good-man?s modified version), we perform the stepslisted below:orig.
mod.compute count distributions ?3.2estimate discount D (10) (13)estimate lower-order model p?
(11) ?4.5.3compute smoothed counts c?
(8) (12)compute probabilities p (9)The computational complexity of expected KNis almost identical to KN on integral counts.
Themain addition is computing and storing the countdistributions.
Using the dynamic program in Sec-tion 3.2, computing the distributions for each r islinear in the number of n-gram types, and we onlyneed to compute the distributions up to r = 2 (orr = 4 for modified KN), and store them for r = 0(or up to r = 2 for modified KN).5 Related WorkWitten-Bell (WB) smoothing is somewhat easierthan KN to adapt to fractional counts.
The SRI-LM toolkit (Stolcke, 2002) implements a methodwhich we call fractional WB:p(w | u) = ?
(u)pmle(w | u) + (1 ?
?(u))p?
(w | u?)?
(u) =E[c(u)]E[c(u)] + n1+(u?
),where n1+(u?)
is the number of word types ob-served after context u, computed by ignoring allweights.
This method, although simple, inconsis-tently uses weights for counting tokens but nottypes.
Moreover, as we will see below, it does notperform as well as expected KN.The only previous adaptation of KN smoothingto fractional counts that we are aware of is thatof Tam and Schultz (2008) and Bisani and Ney(2008), called fractional KN.
This method sub-tracts D directly from the fractional counts, zero-ing out counts that are smaller than D. The dis-count D must be set by minimizing an error metricon held-out data using a line search (Tam, p. c.) orPowell?s method (Bisani and Ney, 2008), requiringrepeated estimation and evaluation of the languagemodel.
By contrast, we choose D by leaving-one-out.
Like KN on integral counts, our method hasa closed-form approximation and requires neitherheld-out data nor trial and error.7696 Language model adaptationN-gram language models are widely used in appli-cations like machine translation and speech recog-nition to select fluent output sentences.
Althoughthey can easily be trained on large amounts of data,in order to perform well, they should be trained ondata containing the right kind of language.
For ex-ample, if we want to model spoken language, thenwe should train on spoken language data.
If wetrain on newswire, then a spoken sentence mightbe regarded as ill-formed, because the distributionof sentences in these two domains are very differ-ent.
In practice, we often have limited-size trainingdata from a specific domain, and large amountsof data consisting of language from a variety ofdomains (we call this general-domain data).
Howcan we utilize the large general-domain dataset tohelp us train a model on a specific domain?Many methods (Lin et al, 1997; Gao et al,2002; Klakow, 2000; Moore and Lewis, 2010; Ax-elrod et al, 2011) rank sentences in the general-domain data according to their similarity to thein-domain data and select only those with scorehigher than some threshold.
Such methods are ef-fective and widely used.
However, sometimes it ishard to say whether a sentence is totally in-domainor out-of-domain; for example, quoted speech in anews report might be partly in-domain if the do-main of interest is broadcast conversation.
Here,we propose to assign each sentence a probabilityto indicate how likely it is to belong to the domainof interest, and train a language model using ex-pected KN smoothing.
We show that this approachyields models with much better perplexity than theoriginal sentence-selection approach.6.1 MethodOne of the most widely used sentence-selectionapproaches is that of Moore and Lewis (2010).They first train two language models, pinon a setof in-domain data, and pouton a set of general-domain data.
Then each sentence w is assigned ascoreH(w) =log(pin(w)) ?
log(pout(w))|w|.They set a threshold on the score to select a subset.We adapt this approach as follows.
After selec-tion, for each sentence in the subset, we use a sig-moid function to map the scores into probabilities:p(w is in-domain) =11 + exp(?H(w))...0 0.2 0.40.60.8 1 1.2 1.4140160180200220240260sentences selected (?107)perplexity.
.fractional KN.
.fractional WB.
.integral KN.
.expected KNFigure 1: On the language model adaptation task,expected KN outperforms all other methods acrossall sizes of selected subsets.
Integral KN is ap-plied to unweighted instances, while fractionalWB, fractional KN and expected KN are appliedto weighted instances.Then we use the weighted subset to train a lan-guage model with expected KN smoothing.6.2 ExperimentsMoore and Lewis (2010) test their method bypartitioning the in-domain data into training dataand test data, both of which are disjoint fromthe general-domain data.
They use the in-domaintraining data to select a subset of the general-domain data, build a language model on the se-lected subset, and evaluate its perplexity on the in-domain test data.
Here, we follow this experimen-tal framework and compare Moore and Lewis?sunweighted method to our weighted method.For our experiments, we used all the Englishdata allowed for the BOLT Phase 1 Chinese-English evaluation.
We took 60k sentences (1.7Mwords) of web forum data as in-domain data,further subdividing it into 54k sentences (1.5Mwords) for training, 3k sentences (100k words)for testing, and 3k sentences (100k words) for fu-ture use.
The remaining 12.7M sentences (268Mwords) we treated as general-domain data.We trained trigram language models and com-pared expected KN smoothing against integral KNsmoothing, fractional WB smoothing, and frac-tional KN smoothing, measuring perplexity acrossvarious subset sizes (Figure 1).
For fractional KN,for each subset size, we optimized D to mini-770mize perplexity on the test set to give it the great-est possible advantage; nevertheless, it is clearlythe worst performer.
Expected KN consistentlygives the best perplexity, and, at the optimal sub-set size, obtains better perplexity (148) than theother methods (156 for integral KN, 162 for frac-tional WB and 197 for fractional KN).
Finally, wenote that integral KN is very sensitive to the subsetsize, whereas expected KN and the other methodsare more robust.7 Word AlignmentIn this section, we show how to apply expected KNto the IBM word alignment models (Brown et al,1993).
This illustrates both how to use expectedKN inside EM and how to use it beyond languagemodeling.
Of course, expected KN can be appliedto other instances of EM besides word alignment.7.1 ProblemGiven a French sentence f = f1f2?
?
?
fmand itsEnglish translation e = e1e2?
?
?
en, an alignment ais a sequence a1, a2, .
.
.
, am, where aiis the indexof the English word which generates the Frenchword fi, or NULL.
As is common, we assume thateach French word can only be generated from oneEnglish word or from NULL (Brown et al, 1993;Och and Ney, 2003; Vogel et al, 1996).The IBM models and related models defineprobability distributions p(a, f | e, ?
), which modelhow likely a French sentence f is to be generatedfrom an English sentence ewith word alignment a.Different models parameterize this probability dis-tribution in different ways.
For example, Model 1only models the lexical translation probabilities:p(a, f | e, ?)
?m?j=1p( fj| eaj).Models 2?5 and the HMM model introduce addi-tional components to model word order and fer-tility.
All, however, have the lexical translationmodel p( fj| ei) in common.
It also contains mostof the model?s parameters and is where overfit-ting occurs most.
Thus, here we only apply KNsmoothing to the lexical translation probabilities,leaving the other model components for futurework.7.2 MethodThe f and e are observed, while a is a latent vari-able.
Normally, in the E step, we collect expectedcounts E[c(e, f )] for each e and f .
Then, in the Mstep, we find the parameter values that maximizetheir likelihood.
However, MLE is prone to over-fitting, one symptom of which is the ?garbage col-lection?
phenomenon where a rare English word iswrongly aligned to many French words.To reduce overfitting, we use expected KNsmoothing during the M step.
That is, during theE step, we calculate the distribution of c(e, f ) foreach e and f , and during the M step, we train alanguage model on bigrams e f using expected KNsmoothing (that is, with u = e and w = f ).
Thisgives a smoothed probability estimate for p( f | e).One question that arises is: what distribution touse as the lower-order distribution p??
Followingcommon practice in language modeling, we usethe unigram distribution p( f ) as the lower-orderdistribution.
We could also use the uniform distri-bution over word types, or a distribution that as-signs zero probability to all known word types.
(The latter case is equivalent to a backoff languagemodel, where, since all bigrams are known, thelower-order model is never used.)
Below, we com-pare the performance of all three choices.7.3 Alignment experimentsWe modified GIZA++ (Och and Ney, 2003) toperform expected KN smoothing as describedabove.
Smoothing is enabled or disabled with acommand-line switch, making direct comparisonssimple.
Our implementation is publicly availableas open-source software.1We carried out experiments on two languagepairs: Arabic to English and Czech to English.For Arabic-English, we used 5.4+4.3 millionwords of parallel text from the NIST 2009 con-strained task,2and 346 word-aligned sentencepairs (LDC2006E86) for evaluation.
For Czech-English, we used all 2.0+2.2 million words oftraining data from the WMT 2009 shared task,and 515 word-aligned sentence pairs (Bojar andProkopov?a, 2006) for evaluation.For all methods, we used five iterations of IBMModels 1, 2, and HMM, followed by three iter-ations of IBM Models 3 and 4.
We applied ex-pected KN smoothing to all iterations of all mod-els.
We aligned in both the foreign-to-English1https://github.com/hznlp/giza-kn2All data was used except for: United Nations pro-ceedings (LDC2004E13), ISI Automatically Extracted Par-allel Text (LDC2007E08), and Ummah newswire text(LDC2004T18).771Alignment F1 BleuSmoothing p?Ara-Eng Cze-Eng Ara-Eng Cze-Engnone (baseline) ?
66.5 67.2 37.0 16.6variational Bayes uniform 65.7 65.5 36.5 16.6fractional WBunigram 60.1 63.7 ?
?uniform 60.8 66.5 37.8 16.9zero 60.8 65.2 ?
?fractional KN unigram 67.7 70.2 37.2 16.5expected KNunigram 69.7 71.9 38.2 17.0uniform 69.4 71.3 ?
?zero 69.2 71.9 ?
?Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all othermethods.
For variational Bayes, we followed Riley and Gildea (2012) in setting ?
to zero (so that thechoice of p?is irrelevant).
For fractional KN, we chose D to maximize F1 (see Figure 2).and English-to-foreign directions and then usedthe grow-diag-final method to symmetrize them(Koehn et al, 2003), and evaluated the alignmentsusing F-measure against gold word alignments.As shown in Table 1, for KN smoothing, in-terpolation with the unigram distribution performsthe best, while for WB smoothing, interestingly,interpolation with the uniform distribution per-forms the best.
The difference can be explained bythe way the two smoothing methods estimate p?.Consider again a training example with a word ethat occurs nowhere else in the training data.
InWB smoothing, p?
( f ) is the empirical unigramdistribution.
If f contains a word that is muchmore frequent than the correct translation of e,then smoothing may actually encourage the modelto wrongly align e with the frequent word.
Thisis much less of a problem in KN smoothing,where p?is estimated from bigram types ratherthan bigram tokens.We also compared with variational Bayes (Ri-ley and Gildea, 2012) and fractional KN.
Overall,expected KN performs the best.
Variational Bayesis not consistent across different language pairs.While fractional KN does beat the baseline forboth language pairs, the value of D, which we op-timized D to maximize F1, is not consistent acrosslanguage pairs: as shown in Figure 2, on Arabic-English, a smaller D is better, while for Czech-English, a larger D is better.
By contrast, expectedKN uses a closed-form expression for D that out-performs the best performance of fractional KN.Table 2 shows that, if we apply expected KNsmoothing to only selected stages of training,adding smoothing always brings an improvement,.0 0.2 0.40.60.8 16466687072DalignmentF1.
.Cze-Eng.
.Ara-EngFigure 2: Alignment F1 vs. D of fractional KNsmoothing for word alignment.Smoothed models Alignment F11 2 H 3 4 Ara-Eng Cze-Eng?
?
?
?
?
66.5 67.2?
?
?
?
?
67.3 67.9?
?
?
?
?
68.0 68.7?
?
?
?
?
68.6 70.0?
?
?
?
?
66.9 68.4?
?
?
?
?
67.0 68.6?
?
?
?
?
69.7 71.9Table 2: Smoothing more stages of training makesalignment accuracy go up.
For each row, wesmoothed all iterations of the models indicated.Key: H = HMM model; ?
= smoothing enabled;?
= smoothing disabled.772with the best setting being to smooth all stages.This shows that expected KN smoothing is consis-tently effective.
It is also interesting to note thatsmoothing is less helpful for the fertility-basedModels 3 and 4.
Whether this is because modelingfertility makes them less susceptible to ?garbagecollection,?
or the way they approximate the E stepmakes them less amenable to smoothing, or an-other reason, would require further investigation.7.4 Translation experimentsFinally, we ran MT experiments to see whether theimproved alignments also lead to improved trans-lations.
We used the same training data as before.For the Arabic-English tasks, we used the NIST2008 test set as development data and the NIST2009 test set as test data; for the Czech-Englishtasks, we used the WMT 2008 test set as develop-ment data and the WMT 2009 test set as test data.We used the Moses toolkit (Koehn et al, 2007)to build MT systems using various alignments(for expected KN, we used the one interpolatedwith the unigram distribution, and for fractionalWB, we used the one interpolated with the uni-form distribution).
We used a trigram languagemodel trained on Gigaword (AFP, AP World-stream, CNA, and Xinhua portions), and minimumerror-rate training (Och, 2003) to tune the featureweights.Table 1 shows that, although the relationshipbetween alignment F1 and Bleu is not very con-sistent, expected KN smoothing achieves the bestBleu among all these methods and is significantlybetter than the baseline (p < 0.01).8 ConclusionFor a long time, and as noted by many authors,the usage of KN smoothing has been limited by itsrestriction to integer counts.
In this paper, we ad-dressed this issue by treating fractional counts asdistributions over integer counts and generalizingKN smoothing to operate on these distributions.This generalization makes KN smoothing, widelyconsidered to be the best-performing smoothingmethod, applicable to many new areas.
We havedemonstrated the effectiveness of our method intwo such areas and showed significant improve-ments in both.AcknowledgementsWe thank Qing Dou, Ashish Vaswani, Wilson Yik-Cheung Tam, and the anonymous reviewers fortheir input to this work.
This research was sup-ported in part by DOI IBC grant D12AP00225.ReferencesJes?us Andr?es-Ferrer.
2010.
Statistical approaches fornatural language modelling and monotone statisti-cal machine translation.
Ph.D. thesis, UniversidadPolit?ecnica de Valencia.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proc.
EMNLP, pages 355?362.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Ondr?ej Bojar and Magdalena Prokopov?a.
2006.Czech-English word alignment.
In Proc.
LREC,pages 1236?1239.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19:263?311.Stanley F. Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques for lan-guage modeling.
Computer Speech and Language,13:359?394.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety, Series B, 39:1?38.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to statisti-cal language modeling for Chinese.
ACM Transac-tions on Asian Language Information, 1:3?33.Joshua T. Goodman.
2001.
A bit of progress in lan-guage modeling: Extended version.
Technical Re-port MSR-TR-2001-72, Microsoft Research.Joshua Goodman.
2004.
Exponential priors for maxi-mum entropy models.
In Proc.
HLT-NAACL, pages305?312.Yili Hong.
2013.
On computing the distribution func-tion for the Poisson binomial distribution.
Compu-tational Statistics and Data Analysis, 59:41?51.Dietrich Klakow.
2000.
Selecting articles from thelanguage model training corpus.
In Proc.
ICASSP,pages 1695?1698.773Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
InProc.
ICASSP 1995, pages 181?184.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
ACL, Companion Volume, pages 177?180.Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-Jiann Chen, and Lin-Shan Lee.
1997.
Chinese lan-guage model adaptation based on document classifi-cation and multiple domain-specific language mod-els.
In Proc.
Eurospeech, pages 1463?1466.Robert Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Proc.ACL, pages 220?224.Hermann Ney, Ute Essen, and Reinhard Kneser.1994.
On structuring probabilistic dependencies instochastic language modelling.
Computer Speechand Language, 8:1?38, 1.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL, pages160?167.Darcey Riley and Daniel Gildea.
2012.
Improvingthe IBM alignment models using variational Bayes.In Proc.
ACL (Volume 2: Short Papers), pages 306?310.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proc.
International Con-ference on Spoken Language Processing, volume 2,pages 901?904.Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.2011.
On the estimation of discount parameters forlanguage model smoothing.
In Proc.
Interspeech,pages 1433?1436.Yik-Cheung Tam and Tanja Schultz.
2008.
Correlatedbigram LSA for unsupervised language model adap-tation.
In Proc.
NIPS, pages 1633?1640.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProc.
COLING-ACL, pages 985?992.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proc.
COLING, pages 836?841.Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.2012.
Leave-one-out phrase model training forlarge-scale deployment.
In Proc.
WMT, pages 460?467.774
