A New Unsupervised Approach toWord SegmentationHanshi Wang?Beijing Institute of TechnologyJian Zhu?
?Beijing Institute of TechnologyShiping Tang?Beijing Institute of TechnologyXiaozhong Fan?Beijing Institute of TechnologyThis article proposes ESA, a new unsupervised approach to word segmentation.
ESA is aniterative process consisting of three phases: Evaluation, Selection, and Adjustment.
In Eval-uation, both the certainty and uncertainty of character sequence co-occurrence in corpora areconsidered as statistical evidence supporting goodness measurement.
Additionally, the statisticaldata of character sequences with various lengths become comparable with each other by usinga simple process called Balancing.
In Selection, a local maximum strategy is adopted withoutthresholds, and the strategy can be implemented with dynamic programming.
In Adjustment,a part of the statistical data is updated to improve successive results.
In our experiment, ESAwas evaluated on the SIGHAN Bakeoff-2 data set.
The results suggest that ESA is effectiveon Chinese corpora.
It is noteworthy that the F-measures of the results are basically monotoneincreasing and can rapidly converge to relatively high values.
Furthermore, empirical formulaebased on the results can be used to predict the parameter in ESA to avoid parameter estimationthat is usually time-consuming.1.
IntroductionWord segmentation is an important task in natural language processing (NLP) forlanguages without word delimiters (e.g., Chinese).
To date, most existing approachesto Chinese word segmentation (CWS) are supervised.
Although supervised approaches?
School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China.E-mail: necrostone@gmail.com.??
School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China.?
School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China.?
School of Computer Science and Technology, Beijing Institute of Technology, Beijing, 100081 China.E-mail: fxz@bit.edu.cn.Submission received: 8 December 2009; revised submission received: 14 October 2010; accepted forpublication: 18 November 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 3reach higher accuracy than unsupervised ones in many cases, they involve much morehuman effort.
Furthermore, unsupervised approaches are more adaptive to relativelyunfamiliar languages for which we do not have enough linguistic knowledge.
In ad-dition, unsupervised approaches can cooperate with supervised ones to overcomedrawbacks of both.Since Sproat and Shih (1990) introduced mutual information (MI) to word seg-mentation, some researchers have conducted research on unsupervised approaches toword segmentation (Chang and Su 1997).
Peng and Schuurmans (2001) proposed anunsupervised approach based on an improved expectation maximum (EM) learningalgorithm and a pruning algorithm based on MI.
Their approach outperforms soft-counting (Ge, Pratt, and Smyth 1999) that is also based on EM and MI.
Non-parametricBayesian techniques?for example, the Pitman-Yor process (PYP, a generalization of theDirichlet process, DP) (Pitman and Yor 1997), hierarchical DP (HDP) (Teh et al 2006;Goldwater, Griffiths, and Johnson 2006), hierarchical PYP (HPYP) (Teh 2006a, 2006b),and hierarchical HPYP (HHPYP) (Wood and Teh 2008)?have been introduced to wordsegmentation.
Mochihashi, Yamada, and Ueda (2009) proposed a novel unsupervisedapproach based on HPYP, and evaluated it on a part of SIGHAN Bakeoff-2 data set(Emerson 2005).
Their evaluation results suggested that their approach outperformedthe previous ones.Some approaches, such as TONGO (Ando and Lee 2000, 2003) and Voting Experts(Cohen, Heeringa, and Adams 2002; Cohen, Adams, and Heeringa 2007), are based onrelatively simple ideas.
In most cases, an unsupervised approach can be viewed asa kind of goodness measurement to find boundaries between words or filter wordsfrom candidates or both.
There are four goodness algorithms reviewed by Zhao andKit (2008a).
The algorithms, including Description Length Gain (DLG) (Kit and Wilks1999), Accessor Variety (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii2005; Jin and Tanaka-Ishii 2006), were evaluated on SIGHAN Bakeoff-3 data set (Levow2006).In this article, we propose ESA, a new unsupervised approach to word segmen-tation, and demonstrate its effectiveness on Chinese corpora.
The approach was moti-vated by the following considerations:1.
In contrast to the semi-supervised or supervised approaches, we wantto find an approach which produces acceptable results under harshconditions.
The harsh conditions are lack of prior knowledge, namely, nolexicons, annotated corpora, or linguistic rules.
The acceptability involvescomparison with the gold standards, which usually means the manuallysegmented results.2.
In contrast to existing unsupervised approaches, we want to explore thepotential of completely unsupervised approaches.
Therefore, we tryto avoid any manual interference.To avoid manual interference, we need to consider the following issues:1.
Unsupervised approaches usually rely on a maximization strategy orthresholds or both.
Approaches adopting the maximization strategy alonecan be easily adapted to various contexts with few manual adjustments,whereas approaches using thresholds may have a higher accuracy insome cases.422Wang et al A New Unsupervised Approach to Word Segmentation2.
Many approaches have constraints on maximum word length.Furthermore, some approaches adopt different strategies for processingwords of different lengths.
The constraints and the different strategiesmay improve results on specific languages, however they reduce thegenerality of the approaches.3.
Many approaches process characters with different strategies accordingto different character types.
The character types are usually identifiedby encoding information.
Because encoding information is priorknowledge of specific languages, a completely unsupervised approachshould avoid it as much as possible in order to be applicable underany conditions.ESA is based on a new goodness algorithm that adopts a local maximum strategyand avoids thresholds.
ESA has no constraints on maximum word length.
In practice,this kind of constraint can have a negative impact on ESA?s segmentation.
A simpleprocess called Balancing is introduced to uniformly process words of different lengths.Moreover, ESA uses a self-revision mechanism to improve segmentation accuracy andguarantees convergence after a small number of iterations.
In practice, ESA has only oneparameter that needs to be configured, and the parameter can be predicted by empiricalformulae proposed in this article.In Section 2, we describe ESA in detail.
The SIGHAN Bakeoff-2 archives are avail-able for research on the official Web site and therefore we can easily test ESA on thatdata.
We provide our experimental results and discuss them in Section 3.
In Section 4,we compare ESA with other approaches.
Finally, we draw our conclusions in Section 5.2.
ESAESA consists of Evaluation, Selection, and Adjustment as shown in Figure 1, and it isbased on two simple ideas:1.
A better result can be produced by combining certainty and uncertainty.The key is how to combine them.2.
A better result can be produced by adopting the self-revised pattern basedon an iterative process.Figure 1ESA and input/output data.423Computational Linguistics Volume 37, Number 3The input text can be viewed as a character sequence.
A character sequence can bedivided into two adjacent subsequences.
The certainty mentioned previously meanscertainty of co-occurrence of adjacent subsequences.
And the uncertainty means un-certainty of co-occurrence of adjacent subsequences.
For example, suppose there aretwo character sequences, AB and AC.
The occurrence of AB represents the certainty ofco-occurrence of A and B, whereas the occurrence of AC represents the uncertainty ofco-occurrence of A and B, and vice versa.
The two kinds of information are combined toevaluate the segmentation.
In other words, the decision of whether to segment a char-acter sequence into two adjacent subsequences or not depends on both certainty anduncertainty.
An iterative process can produce better results than a non-iterative scheme(Chang and Su 1997).
In fact, the current result can be viewed as prior knowledge toadjust the next one.Devising an unsupervised approach is similar to clarifying how infants segmentwords without explicit instructions.
In particular, infants are able to learn words fromvarious kinds of information such as familiar names (Bortfeld et al 2005), edges ofutterances (Seidl and Johnson 2006), meaning maps (Estes et al 2007), and auditoryforms of words (Swingley 2008).
There are a few notable issues:1.
Familiarity (Bortfeld et al 2005) can be represented by high frequency.Frequent character sequences provide more credibility than infrequentones.
The appearance frequencies are the most important information forword segmentation.2.
The edges of utterances (Seidl and Johnson 2006) can be viewed as naturalboundaries.
In practice, the boundaries given by punctuation can improvethe accuracy of segmentation.
However, we think that punctuation shouldbe ignored by completely unsupervised approaches in order to avoidrelying on encoding information.3.
Both word lists and statistical data as prior knowledge enable humaninfants to segment words (Estes et al 2007).
For a completelyunsupervised approach, the prior knowledge can be the approachitself and the previous results produced by the approach.4.
The early vocabularies of human infants are based on the sounds of words(Swingley 2008).
Some research (Goldwater, Griffiths, and Johnson 2006)is based on phonemes, but the input data are still text.Before completely clarifying the mechanism of human learning, we tend to believe thatmachines can understand symbol sequences with simple logic.2.1 EvaluationEvaluation is the phase that gives a character sequence or a pair of adjacent subse-quences a goodness value according to statistical information.
There are three issuesto be settled:1.
What are the character sequence and the pair of adjacent subsequencesthat can be evaluated?2.
What is the necessary statistical information and how do we get it?3.
How do we calculate the goodness?424Wang et al A New Unsupervised Approach to Word Segmentation2.1.1 The Target of Evaluation.
A character sequence contains (N+1)?N2 subsequences,where N is the number of characters in the character sequence.
These subsequencesare the targets to be evaluated.A character sequence can be divided into various pairs of adjacent subsequences asshown in Figure 2.
A pair of adjacent subsequences contains a gap that is the potentialboundary between the two subsequences.
Every character sequence has an individualgoodness value (IV).
Every pair of adjacent subsequences has a combined goodnessvalue (CV) based on the IV of each subsequence and the goodness value of the gap(LRV).
IV and LRV indicate certainty and uncertainty of co-occurrence, respectively.Therefore, CV is the combination of certainty and uncertainty.
IV is the base of CV, andLRV serves as the modifier of the base.2.1.2 The Information Needed.
The information mentioned here is the statistical informa-tion that can be extracted from corpora.
There are two basic quantities to be directlymeasured:1.
The frequency of a character sequence.
For example, if the charactersequence is ABAB: the frequencies of A, B, and AB are all 2; and thefrequencies of ABAB, ABA, BAB, and BA are all 1.2.
The number of character sequences of the same length.
For example, if thecharacter sequence is ABC: the sequences of length 1 are A, B, and C; thesequences of length 2 are AB and BC; and the sequence of length 3 is ABCitself.
Therefore, the number of the sequences of length 1, 2, and 3 are 3, 2,and 1, respectively.Furthermore, there are three other quantities that can be calculated according to thosejust mentioned:1.
The average frequency of character sequences of the same length.
Forexample, there are only two sequences of length 1: A and B. Thefrequencies of A and B are 2 and 8, respectively.
Therefore, the arithmeticmean of frequencies of A and B is 5.
In other words, the average frequencyof character sequences of length 1 is 5.2.
The entropy of Sequence Plus One (SP1) of a character sequence.
Forexample, consider the character sequence X.
The SP1 of X is a set, and eachmember of the SP1 contains X and one character.
In detail, the entropyFigure 2A character sequence and its subsequence pairs.425Computational Linguistics Volume 37, Number 3mentioned here is denoted by H(SP1L(X)) and H(SP1R(X)), where SP1L(SP1 Left) and SP1R (SP1 Right) are two subsets of SP1.
In other words,SP1L(X) and SP1R(X) mean that the left side of X is attached with onecharacter and the right side of X is attached with one character,respectively.
For example, there are several character sequences: BC, ABC,BBC, BCD, and BCB.
ABC and BBC are the members of SP1L(BC), whereasBCD and BCB are the members of SP1R(BC).
Therefore, H(SP1L(BC)) iscalculated according to the frequencies of ABC, BBC, and other membersof SP1L(BC), whereas H(SP1R(BC)) is calculated according to thefrequencies of BCD, BCB, and other members of SP1R(BC).
The formaldescriptions of SP1L and SP1R areSP1L(x) = {s|s = c ?
x, s ?
S, x ?
S, c ?
?}
(1)andSP1R(x) = {s|s = x ?
c, s ?
S, x ?
S, c ?
?}
(2)respectively.
The symbol ?
denotes the attachment operator; s denotes asubsequence of the character sequence S; c denotes a character in thealphabet ?.
In fact, x is the largest proper subsequence of s.3.
The average entropies of SP1s of character sequences of the same length.The SP1s refer to both SP1Ls and SP1Rs.
For example, there are only threecharacter sequences of length 2: AB, BC, and CD.
Therefore, the sum ofH(SP1L(AB)), H(SP1L(BC)), and H(SP1L(CD)) is the numerator of thearithmetic mean, and the denominator is 3.
The arithmetic mean is theaverage entropies of SP1Ls of AB, BC, and CD.We directly use the prefix tree (trie) (Fredkin 1960) to record the information.
Some otherdata structures can also be used (Morrison 1968; McCreight 1976; Manber and Myers1990).2.1.3 The Calculation of Goodness.
The IV of a character sequence is formulated asIV(x) = (FxFML)L (3)The superscript L is the exponent; x is the character sequence to be evaluated; L is thelength of x. F denotes the frequency of a character sequence, and therefore Fx is thatof x; FM denotes the average frequency of character sequences of the same length,and therefore FML is that of length L. F can be viewed as a local variable, and FMbrings global effects to the formula.
By the division in IV, the character sequences ofdifferent lengths become comparable with each other.
The division is based on a patterncalled Balancing, which means keeping balance between the local and global effects.Furthermore, FM is formulated asFML =1N?NFL (4)426Wang et al A New Unsupervised Approach to Word SegmentationThe entropies of SP1L and SP1R are formulated asH(SP1L(x)) = ?n?i=1p(FLxi ) ln p(FLxi ) (5)andH(SP1R(x)) = ?n?i=1p(FRxi ) ln p(FRxi ) (6)respectively.
Lxi and Rxi denote the ith members in SP1L(x) and SP1R(x), respectively;FLx and FRx denote the frequencies of members in SP1L(x) and SP1R(x), respectively.The average entropies of SP1Ls and SP1Rs of character sequences of the same lengthare formulated asHLML =1N?NHLL (7)andHRML =1N?NHRL (8)respectively.
HLM and HRM denote the average entropies of SP1Ls and SP1Rs ofcharacter sequences of certain length, respectively; HL and HR denote the entropies ofSP1L and SP1R of a character sequence of a certain length, respectively; N is the numberof HL or HR; L is the length of character sequences.The CV of a pair of adjacent subsequences is formulated asCV(Sleft ?
Sright) = IV(Sleft) ?
IV(Sright) ?
LRV(Sleft ?
Sright) (9)where the LRV is formulated asLRV(Sleft ?
Sright) = (H(SP1R(Sleft)) ?H(SP1L(Sright))HRML1 ?HLML2)x (10)The superscript x is the exponent; The symbol ?
denotes that Sleft and Sright are twoadjacent sequences; L1 and L2 denote the lengths of character sequences in SP1R(Sleft)and SP1L(Sright), respectively.
Actually, L1 and L2 are equal to the lengths of Sleft + 1 andSright + 1, respectively.
The division in LRV represents a Balancing process.
As before,the entropies of SP1s of character sequences of different lengths become comparablewith each other by using Balancing.IV and LRV represent certainty and uncertainty of co-occurrence of two adjacentcharacter sequences, respectively.
Their combination is CV.
The exponent in LRV repre-sents the weight of LRV in CV.427Computational Linguistics Volume 37, Number 32.2 SelectionSelection is the phase in which a maximization strategy is used to determine the bestsegmentation of a character sequence.For example, the character sequence ABC has six subsequences: A, B, C, AB, BC,and ABC.
Therefore, ABC can be divided into various pairs of adjacent subsequences,as shown in Figure 3.The process of Selection can be viewed as the sets of comparisons.
Each set consistsof all comparisons between a character sequence itself and all pairs of adjacent subse-quences of the character sequence.
In Figure 3, the character sequence BC is comparedwith the pair of adjacent subsequences B ?
C. If the CV of B ?
C is higher than the IV ofBC, BC will be segmented into B and C.Processing the longer sequence is done in a similar manner.
For example, ABChas two adjacent subsequence pairs: A ?
BC and AB ?
C. Both BC and AB have theirown adjacent subsequence pairs B ?
C and A ?
B, respectively.
Therefore, Selection mustprocess BC and AB before processing ABC.
The IV of ABC, the CV of A ?
BC, and theCV of AB ?
C are compared with each other after Selection finishes processing both BCand AB.
The one with the highest goodness value is the final choice.We adopt a dynamic programming technique to limit the computational complexityof the algorithm in Selection.
Therefore, the algorithm takes polynomial time over thelength of the character sequence being processed.2.2.1 The Primary Criterion and Secondary Criteria.
The task of Selection is to maximize thegoodness value, which can be defined asseg?
= argmaxsegE(seg) (11)where seg denotes all possible segmentations of a character sequence, seg* denotes thefinally selected segmentation, and E denotes an evaluation.Figure 3The pairs of adjacent subsequences.428Wang et al A New Unsupervised Approach to Word SegmentationFor a character sequence and its pairs of adjacent subsequences, Selection is furtherformulated asS?
= argmax0?
i?NE(Si) (12)whereE(Si) ={IV(S0) i = 0,CV(Si) 1 ?
i ?
N ?
1.
(13)S0 is the character sequence and N is its length; Si(1?i?N ?
1) is the pair of adjacentsubsequences in S0, where the second subsequence starts at i.
The evaluations for acharacter sequence and a pair of adjacent subsequences are IV and CV, respectively.The combination of IV and CV is the primary criterion in Selection.Besides the primary criterion, it is necessary to provide a mechanism for making thedecision when the primary criterion cannot resolve it.
For instance, when the IVs andCVs being compared are equal to each other, other criteria are needed.
LRV is used as asecondary criterion in Selection:E(Si) ={LRV(S0) = 1 i = 0,LRV(Si) = LRV(Si.left ?
Si.right) 1 ?
i ?
N ?
1.
(14)Although the secondary criteria exist in Selection, their effects are not significant inpractice.2.2.2 The Path of Selection.
ESA records the paths of processes of Selection as shown inFigure 4.
The result of Selection can be viewed as a binary tree as shown in Figure 5.2.3 AdjustmentAdjustment is the phase that updates the data.
Specifically, it uses the result producedby the previous Selection to update the data that will be used by the next Evaluation.There are two issues to be settled:1.
What data can be updated after the previous Selection?2.
How can we use the updated data for the next Evaluation?Figure 4The path of Selection.429Computational Linguistics Volume 37, Number 3Figure 5The binary tree of Selection.2.3.1 Updatable Data.
Our idea is that the result of the previous Selection is based onthe overestimation of frequencies of some character sequences.
For example, when thecharacter sequence X is selected as a word after Selection, the original frequencies ofX?s proper subsequences are considered as being overestimated because all of themwere regarded as potential words before the Selection.
In other words, if X is selectedas a word, its proper subsequences will not continue to be regarded as potential words.Therefore, the frequencies of these subsequences are reduced, and Adjustment can beviewed as the corrections to the overestimated frequencies.In our example, the character sequence ABC occurs twice in an input, as shownin Figure 6.
After Selection, one of the ABCs is selected as a word but not the other.Therefore, the frequencies of all proper subsequences of the selected ABC are reducedby 1 and those of the other ABC are not changed, as shown in Figure 7.This process reminds us of Statistical Substring Reduction (SSR) (Zhang et al2003; Lu?, Zhang, and Hu 2004), although the idea of SSR is not similar to ours.
SSRimplies that the existence of a character sequence usually has a negative impact onthe independent existence of subsequences of this character sequence.
If the frequencyof a subsequence is near to that of its supersequence, the subsequence will be re-moved.
Frequency of Substring with Reduction (FSR) (Zhao and Kit 2008a) is derivedfrom SSR.2.3.2 Using Updated Data.
The frequency of a character sequence (the F in IV) is the onlyquantity to be changed.
FM in IV, LRV, and others are not changed.Figure 6The initial frequencies of character sequences.430Wang et al A New Unsupervised Approach to Word SegmentationFigure 7The adjusted frequencies of character sequences.For example, there are two character sequences: ABC and BBC.
The initial recordsare A(1), B(3), C(2), AB(1), BB(1), BC(2), ABC(1), and BBC(1), where the number inparentheses is the F in IV.
After Selection, ABC and BBC are segmented into AB C andBB C, respectively.
The subsequences of AB are A and B, and the subsequences of BBare two Bs.
Because of the idea of Adjustment, the Fs of these subsequences need to bereduced.
Therefore, the records become A(1 ?
1 = 0), B(3 ?
1 ?
2 = 0), C(2), AB(1),BB(1), BC(1), ABC(1), and BBC(1) after Adjustment.2.4 Preprocessing Input DataSentences are the input data directly accepted by many approaches.
The differencebetween a sentence and a character sequence as discussed in this article is whether ornot punctuation is used as prior knowledge to segment text.
Some approaches furtherutilize encoding information.
For example, non-Chinese characters such as digits andLatin letters can be separated from Chinese characters so that they can be separatelyprocessed.In ESA, the length of a character sequence is limited for computational complexity.Limiting the length of a character sequence is different from limiting that of a word.
Inother words, ESA limits the length of the input but not that of the output.
In practice,the limitation has a negative impact on the segmentation accuracy of ESA.2.4.1 Maximum Sequence Length.
The time complexity of ESA is polynomial over thelength of the character sequence being processed.
The preprocessing segments thewhole character sequence into multiple sequences within a length limit, and line breaksare regarded as the only natural delimiters.
If a character sequence is still longer thanthe limited length, it will be further divided into two shorter sequences.
The location ofsegmentation (LoS) is determined by the formulaLoS(S) = argmax0<i<LLRV(s0i ?
siL?i) (15)where S denotes the character sequence to be divided, L is the length of S, i is the indexof the character in S, and s denotes the sequence after division.
The superscript and thesubscript of s denote the start index and the length of s, respectively.
If the length of sexceeds the limit, s will be further divided.431Computational Linguistics Volume 37, Number 3The algorithm of LoS uses LRV alone to segment character sequences before theexecution of the main algorithm of ESA.
LRV alone is inferior to the main algorithm.Therefore, limiting the input character sequence to a short length reduces the effective-ness of the main algorithm.One-character and two-character words are the most common for Chinese, butwords of more than five characters are very rare (Teahan et al 2000).
Therefore, Zhaoand Kit (2008a) limited the word length to two or seven in their test.
In practice, limitingthe maximum word length usually has a positive impact on many approaches.2.4.2 Encoding Information.
There are two levels of encoding information that can be usedto improve the results:1.
Punctuation can be used to divide a character sequence into naturalsentences.2.
Different types of characters can be separately processed.2.5 The Result FormESA can output the segmentation results in a hierarchical format.
Figure 8 shows anexample.The result in the hierarchical format cannot directly be evaluated by the Bakeoffscore script.
Therefore, the hierarchical format will be changed to a format that lookslike A B C, where the symbol denotes a space character.2.6 SummaryIn this section, we briefly describe the whole algorithm of ESA and provide somethoughts about it.2.6.1 The Whole Algorithm in Brief.
The implementation of ESA consists of four steps:Step 1: Preprocessing.Step 2: Evaluation and Selection.Figure 8The hierarchical form of a result.432Wang et al A New Unsupervised Approach to Word SegmentationStep 3: Adjustment.Step 4: Completion.
When the current result of segmentation is the same as the previousresult, or ESA reaches a given number of iterations, ESA stops.
Otherwise, ESA repeatsthe process from Step 1.Preprocessing has two steps:Step 1: Segment a corpus into multiple character sequences.
No character sequenceexceeds the limit of maximum length.Step 2: The frequencies (F in IV) of all subsequences in every character sequence arerecorded.
According to the frequencies, FM in IV and the other quantities in LRV arecalculated.ESA integrates Evaluation and Selection into a recursive algorithm Segment:Algorithm: Segment.Input: An entry of data structure.
The entry represents a character sequence S.Output: None.Comment: L denotes the length of S; FV denotes the final goodness value; FS denotesthe final segmentation; s[i][j] denotes the subsequence of S, where i and j denote thestart and end indices of s in S, respectively; and ?
denote the delimiter and linkage ofcharacter sequences, respectively.01: If S?s FV = 0 then02: If L = 1 then03: S?s FV ?
IV(S)04: S?s FS ?
s[1][L]05: Else06: Max ?
IV(S)07: Seg ?
s[1][L]08: For i ?
1 to L09: Segment (s[1][i])10: Segment (s[i][L])11: CV ?
s[1][i]?s FV ?
s[i][L]?s FV ?
LRV(s[1][i]?
s[i][L])12: If Max < CV then13: Max ?
CV14: Seg ?
s[1][i]?s FS?
?
s[i][L]?s FS15: End if16: End for17: S?s FV ?
Max18: S?s FS ?
Seg19: End if20: End if2.6.2 Discussion.
IV is an exponential formula.
The base is a ratio, which represents theinfluence of a segment on the goodness of segmentation.
The exponent is the length ofthe segment, which means that each character in the segment has an equal influence onthe goodness.433Computational Linguistics Volume 37, Number 3LRV is also an exponential formula.
The base is the product of two ratios, whichrepresents the influence of the gap between two adjacent segments on the goodness ofsegmentation.
The exponent is the weight needed to harmonize the influences of IV andLRV in CV.CV is the combination of IV and LRV.
The whole goodness of segmentation canbe viewed as nested CVs.
For example, the character sequence ABC is segmented intoA B C. The whole goodness is equivalent toCV(A ?
B ?
C) = IV(A) ?
CV(B ?
C) ?
LRV(A ?
BC) (16)orCV(A ?
B ?
C) = CV(A ?
B) ?
IV(C) ?
LRV(AB ?
C) (17)Although the character sequence can possibly be segmented in a different order, thewhole goodness is equivalent toCV(A ?
B ?
C) = IV(A) ?
IV(B) ?
IV(C) ?
LRV(A ?
BC) ?
LRV(AB ?
C) (18)Therefore, the design of CV ensures the consistency of the goodness measurementacross different orders of segmentation.By using the length as the exponent of IV, CV has a feature.
For example, thecharacter sequence ABC can be segmented into A B C, AB C, A BC, and ABC.
Theproducts of IVs in the nested CVs areFAFM1?
FBFM1?FCFM1(19)FABFM2?
FABFM2?FCFM1(20)FAFM1?FBCFM2?FBCFM2(21)FABCFM3?FABCFM3?FABCFM3(22)No matter how it is segmented, the influence of each character is equally considered.The idea of ESA is to introduce few manually assigned parameters when using theunannotated corpora alone.
The only parameter in ESA is the exponent in LRV, whichcan be predicted with the empirical formulae.We think that a completely unsupervised approach to word segmentation shouldbe tested with the closed criterion in Bakeoff.
Furthermore, there are three reasons whythe approach should not depend on punctuation to segment sentences:1.
Although punctuation is easily identified with encoding information, itcannot be identified when lacking such information.434Wang et al A New Unsupervised Approach to Word Segmentation2.
Segmenting sentences with punctuation is based on the assumption that alanguage must have punctuation.
In fact, some languages, such as ancientChinese, have no such symbols.
This phenomenon even partly exists inmodern Chinese, which implies a lack of delimiters between words.3.
The completely unsupervised approach should have more abilities toprocess unfamiliar languages, because this kind of approach is similar tothe infant language learner without prior knowledge such as lexicons,annotated corpora, and character information.
The completelyunsupervised approach should discover new knowledge instead of justusing it.3.
ExperimentThe experiment uses the SIGHAN Bakeoff-2 data set that is publicly available on theSIGHAN Web site (www.sighan.org).
We tested ESA with various settings.In this section, we describe the test settings, report experimental results, and discussthose results.
According to the experiment, we establish the empirical formulae topredict the exponent in LRV.3.1 SettingsWe used four different settings to test ESA:1.
Punctuation and other encoding information are not used, and themaximum length of character sequences is 30.
The result with this settingcan be viewed as a baseline.2.
Punctuation and other encoding information are not used, and themaximum length of character sequences is 10.
The result with this settingdemonstrates that the limitation to the maximum length of charactersequences has a negative impact on ESA.3.
Punctuation is used to segment character sequences into sentences, andthe maximum length of character sequences is 30.
The result with thissetting demonstrates that punctuation can significantly improve thesegmentation accuracy of ESA.4.
Both punctuation and other encoding information are used, and themaximum length of character sequences is 30.
The result with this settingdemonstrates that discriminating non-Chinese characters from Chineseones can further improve the accuracy.A simple algorithm called character-as-word (CAW) (Palmer 1997) was used forthe baseline in the paper of Zhao and Kit (2008a).
We believe there are two rea-sons why CAW might be viewed as evidence of the effectiveness of unsupervisedapproaches:1.
The unsupervised approaches are not comparable with supervisedones in general, because the conventional criteria are the manualsegmentations known as gold standards.
Gold standards, however,435Computational Linguistics Volume 37, Number 3cannot be unified into a single standard (Fung and Wu 1994; Sproat et al1996).2.
The unsupervised approaches are not comparable with each other tosome extent.
This is not only because the researchers carried out theirexperiments on different corpora and with test settings, but also becausethe different approaches may be adapted to different applications(Sproat and Shih 2001; Sproat and Emerson 2003; Wu 2003; Gao et al2005).Zhao (2009) even suggested that character-level analysis could replace word-levelanalysis for Chinese.
However, we think that making comparisons based on similarcorpora and settings with other approaches is necessary when regarding word segmen-tation as an independent task.3.2 TargetsThere are eight corpora consisting of four training and four test corpora in the Bakeoff-2data set.
Because of the different sizes of the corpora and the different settings in theexperiment, the number of character sequences (N1) and nodes in trie (N2) producedby ESA are also different, as shown in Table 1.3.3 ResultsThe experimental results produced by ESA with the four different settings are shownin Tables 2, 3, 4, and 5, respectively.
X denotes the exponent in LRV; the numbers inthe column headings of the tables are the numbers of iterations; the bold F-measure isalmost the best; the asterisked number is the proper exponent.Table 1The scales of corpora.Corpus Character Type Setting 1 Setting 2 Setting 3 Setting 4CITYU test 67,689 N1 1,428,763 609,921 463,756 406,312N2 1,308,964 491,328 363,120 312,151PKU test 172,733 N1 4,431,621 1,640,688 1,248,317 1,093,046N2 3,953,008 1,214,480 896,741 769,674MSR test 184,355 N1 3,910,003 1,665,511 1,313,351 1,269,858N2 3,462,762 1,227,640 946,220 907,226AS test 197,681 N1 1,840,266 1,353,924 1,305,937 1,210,572N2 1,477,908 993,669 992,100 912,221PKU train 1,826,448 N1 47,565,891 17,430,107 13,227,039 12,721,709N2 41,906,011 12,011,275 8,837,184 8,414,698CITYU train 2,403,354 N1 50,633,595 21,654,982 17,152,484 15,065,762N2 43,831,426 15,074,334 11,714,172 9,987,122MSR train 4,050,469 N1 84,599,783 36,594,096 30,639,574 29,616,260N2 71,293,182 23,936,279 20,164,516 19,183,186AS train 8,368,050 N1 69,454,846 53,877,473 51,986,335 49,455,126N2 46,782,170 31,499,015 32,641,248 30,788,602436Wang et al A New Unsupervised Approach to Word SegmentationThe results are monotone increasing and rapidly converging in most cases, un-less the exponent considerably diverges from the proper value.
The larger exponentleads to more insertion errors, whereas the smaller one leads to more deletion er-rors.
On one hand, ESA segments a character sequence into more parts when weincrease the exponent in LRV (the weight of LRV in CV), which can produce fewerdeletion errors.
On the other hand, the character sequence is segmented into fewer partsTable 2The results of setting 1 (Punctuation and other encoding information are not used; the maximumlength is 30).X 1 2 3 4 5 6 7 8 9 10CITYU test 0.5* .613 .679 .702 .713 .718 .720 .721 .722 .723 .7231.0 .586 .647 .669 .683 .688 .693 .695 .696 .697 .6971.5 .557 .613 .629 .637 .643 .645 .647 .649 .649 .6502.0 .532 .572 .585 .590 .592 .594 .594 .595 .595 .595PKU test 0.5 .666 .722 .737 .743 .745 .744 .744 .744 .743 .7421.0* .648 .713 .732 .740 .745 .747 .748 .749 .750 .7501.5 .625 .687 .713 .724 .725 .727 .728 .729 .730 .7302.0 .607 .665 .678 .687 .691 .693 .693 .694 .694 .695MSR test 0.5 .672 .739 .755 .759 .759 .758 .758 .758 .758 .7570.9* .658 .725 .749 .757 .762 .764 .763 .764 .764 .7641.0 .654 .721 .743 .754 .758 .760 .761 .761 .762 .7621.5 .629 .697 .716 .723 .726 .729 .731 .732 .732 .7322.0 .606 .668 .686 .694 .697 .699 .699 .700 .700 .700AS test 0.5 .664 .725 .747 .757 .758 .760 .761 .762 .761 .7620.6* .661 .725 .743 .754 .759 .761 .762 .763 .764 .7641.0 .640 .704 .729 .739 .744 .747 .748 .749 .749 .7491.5 .604 .663 .690 .699 .705 .709 .710 .710 .711 .7112.0 .579 .623 .643 .650 .653 .655 .656 .656 .656 .656PKU train 0.5 .686 .714 .712 .710 .706 .705 .703 .703 .702 .7021.0 .690 .729 .733 .734 .734 .733 .733 .732 .732 .7321.5 .688 .736 .747 .748 .750 .751 .752 .752 .753 .7531.8* .685 .740 .755 .758 .760 .760 .760 .761 .761 .7612.0 .682 .734 .750 .755 .758 .759 .759 .759 .759 .760CITYU train 0.5 .681 .718 .719 .716 .713 .713 .711 .712 .711 .7111.0 .683 .728 .737 .739 .739 .739 .738 .738 .738 .7381.5 .678 .732 .746 .751 .754 .754 .755 .756 .757 .7571.7* .676 .732 .747 .756 .759 .761 .761 .763 .763 .7642.0 .666 .727 .745 .753 .756 .758 .759 .761 .761 .762MSR Train 0.5 .687 .701 .690 .684 .681 .680 .678 .678 .677 .6781.0 .696 .722 .720 .717 .714 .713 .712 .712 .711 .7111.5 .695 .735 .740 .742 .742 .742 .742 .742 .742 .7422.0 .689 .740 .753 .757 .758 .759 .760 .760 .760 .7612.5* .683 .736 .752 .758 .760 .761 .762 .763 .763 .763AS Train 0.5 .670 .659 .647 .641 .639 .638 .637 .637 .637 .6371.0 .685 .688 .680 .676 .673 .673 .672 .672 .672 .6721.5 .698 .713 .709 .706 .704 .704 .704 .703 .703 .7032.0 .701 .726 .731 .733 .734 .734 .734 .735 .734 .7342.8* .699 .739 .750 .753 .755 .754 .755 .755 .755 .756Column headings represent the number of iterations.
Boldfaced results represent almost the best F-measure.
* = the proper exponent.437Computational Linguistics Volume 37, Number 3Table 3The results of setting 2 (Punctuation and other encoding information are not used; the maximumlength is 10).X 1 2 3 4 5 6 7 8 9 10CITYU test 0.35* .614 .676 .694 .701 .705 .704 .705 .706 .706 .7060.5 .607 .667 .690 .697 .699 .701 .702 .703 .702 .7031.0 .577 .636 .656 .669 .674 .678 .679 .680 .680 .6811.5 .552 .601 .617 .624 .629 .630 .632 .632 .633 .6332.0 .525 .564 .574 .577 .578 .579 .580 .580 .580 .581PKU test 0.5 .662 .715 .731 .737 .738 .739 .739 .738 .738 .7380.65* .655 .714 .730 .734 .737 .737 .739 .738 .739 .7391.0 .642 .702 .721 .728 .732 .734 .735 .736 .737 .7371.5 .619 .674 .696 .707 .711 .713 .714 .715 .715 .7152.0 .598 .648 .663 .667 .671 .673 .674 .675 .676 .676MSR test 0.5 .668 .733 .748 .753 .753 .754 .754 .753 .753 .7530.65* .662 .729 .745 .751 .753 .754 .754 .754 .754 .7551.0 .649 .712 .731 .739 .744 .746 .747 .748 .749 .7491.5 .622 .686 .706 .712 .716 .718 .719 .719 .719 .7202.0 .598 .656 .674 .682 .684 .687 .687 .688 .688 .688AS test 0.45* .659 .718 .736 .744 .747 .749 .750 .751 .751 .7520.5 .656 .716 .734 .742 .745 .747 .748 .749 .749 .7501.0 .633 .696 .717 .725 .729 .731 .731 .732 .732 .7321.5 .598 .652 .676 .686 .690 .693 .694 .695 .695 .6952.0 .574 .615 .631 .639 .642 .642 .643 .643 .643 .643PKU train 0.5 .700 .734 .737 .737 .734 .735 .733 .734 .733 .7341.0 .696 .737 .747 .749 .749 .750 .750 .750 .750 .7501.5* .686 .736 .748 .75 .754 .753 .754 .754 .754 .7542.0 .677 .728 .743 .747 .750 .749 .751 .750 .752 .750CITYU train 0.5 .695 .741 .744 .746 .743 .745 .742 .744 .742 .7441.0 .689 .739 .750 .755 .755 .756 .755 .757 .755 .7571.5* .679 .733 .746 .755 .756 .760 .759 .761 .760 .7612.0 .663 .723 .740 .747 .748 .752 .751 .753 .752 .753MSR Train 0.5 .708 .735 .731 .729 .726 .726 .724 .725 .724 .7251.0 .706 .743 .747 .748 .746 .747 .746 .747 .746 .7471.5 .698 .744 .753 .756 .756 .757 .756 .758 .757 .7581.9* .690 .740 .751 .757 .757 .760 .759 .760 .759 .7602.0 .687 .738 .750 .756 .756 .759 .758 .759 .758 .759AS Train 0.5 .694 .697 .691 .688 .686 .686 .685 .685 .685 .6851.0 .702 .716 .714 .713 .712 .712 .712 .712 .712 .7121.5 .706 .730 .732 .733 .733 .733 .733 .733 .733 .7332.0 .704 .732 .739 .742 .743 .744 .744 .745 .745 .7452.8* .694 .733 .745 .748 .751 .752 .752 .752 .752 .752Column headings represent the number of iterations.
Boldfaced results represent almost the best F-measure.
* = the proper exponent.after a number of iterations, which can produce more deletion errors.
ESA producedtoo many deletion errors with an excessively low weight of LRV (such as 0.5 in thetest of the AS training corpus), which is why the iteration finally produced a worseresult.Increasing the maximum length of input character sequences magnifies the effectof the main algorithm (CV) and reduces that of preprocessing (LRV alone).
In prac-tice, the results are improved when increasing the maximum length from 10 to 30438Wang et al A New Unsupervised Approach to Word SegmentationTable 4The results of setting 3 (Punctuation is used; the maximum length is 30).X 1 2 3 4 5 6 7 8 9 10CITYU test 0.3* .627 .701 .726 .739 .744 .747 .748 .749 .748 .7490.5 .615 .688 .714 .723 .729 .734 .737 .738 .738 .7391.0 .587 .649 .673 .683 .689 .692 .696 .697 .698 .6991.5 .553 .606 .629 .639 .642 .644 .645 .645 .645 .6462.0 .528 .570 .583 .589 .590 .591 .591 .591 .591 .591PKU test 0.5* .673 .739 .759 .767 .771 .774 .774 .774 .774 .7741.0 .651 .719 .741 .751 .756 .759 .760 .761 .761 .7621.5 .623 .687 .713 .724 .731 .735 .736 .737 .737 .7372.0 .602 .657 .672 .678 .680 .681 .682 .683 .683 .683MSR test 0.5* .680 .753 .773 .779 .781 .782 .783 .783 .783 .7841.0 .656 .726 .749 .759 .766 .769 .771 .772 .772 .7721.5 .627 .693 .716 .724 .728 .730 .731 .732 .732 .7322.0 .600 .658 .681 .686 .690 .692 .694 .695 .695 .695AS test 0.3* .673 .739 .762 .773 .776 .778 .778 .779 .779 .7790.5 .664 .732 .753 .764 .770 .773 .775 .776 .776 .7771.0 .632 .699 .725 .737 .742 .745 .746 .747 .748 .7481.5 .598 .658 .678 .690 .695 .696 .697 .697 .697 .6982.0 .575 .619 .633 .641 .644 .645 .646 .646 .646 .646PKU train 0.5 .736 .777 .783 .783 .782 .782 .781 .781 .781 .7811.0 .725 .776 .788 .791 .793 .794 .794 .794 .794 .7941.1* .721 .775 .787 .792 .794 .794 .794 .795 .795 .7951.5 .710 .767 .782 .788 .791 .792 .792 .792 .793 .7932.0 .694 .754 .772 .779 .781 .783 .783 .784 .784 .784CITYU train 0.5 .740 .790 .797 .797 .796 .796 .795 .795 .795 .7951.0 .732 .788 .802 .806 .808 .808 .808 .808 .808 .8081.4* .719 .782 .800 .808 .812 .814 .815 .815 .816 .8161.5 .714 .778 .797 .806 .810 .812 .813 .814 .815 .8152.0 .693 .759 .781 .790 .794 .796 .797 .798 .798 .798MSR Train 0.5 .744 .775 .774 .771 .770 .769 .768 .768 .768 .7681.0 .741 .782 .788 .788 .788 .788 .788 .787 .787 .7871.5 .728 .781 .793 .796 .797 .798 .798 .799 .799 .7991.6* .728 .782 .795 .799 .800 .801 .801 .802 .802 .8022.0 .709 .770 .786 .792 .794 .796 .796 .797 .797 .797AS Train 0.5 .728 .738 .735 .732 .730 .730 .729 .729 .729 .7291.0 .732 .752 .753 .752 .752 .751 .751 .751 .751 .7511.5 .732 .762 .766 .768 .769 .769 .769 .769 .769 .7692.0 .729 .764 .774 .776 .778 .779 .779 .780 .780 .7802.2* .726 .763 .772 .777 .779 .781 .782 .782 .782 .782Column headings represent the number of iterations.
Boldfaced results represent almost the best F-measure.
* = the proper exponent.as shown in Figure 9.
Therefore, the limitation on the maximum length really makesa negative impact on ESA.
However, when the maximum length of input charactersequences is further increased to 50 or 100, the results are not very different, as shown inTable 6.Although we insist that a completely unsupervised approach should not rely onencoding information, punctuation and other encoding information are effective inimproving segmentation results, as shown in Figure 9.439Computational Linguistics Volume 37, Number 3Table 5The results of setting 4 (Punctuation and other encoding information are used; the maximumlength is 30).X 1 2 3 4 5 6 7 8 9 10CITYU test 0.3* .635 .709 .735 .748 .754 .757 .758 .759 .760 .7600.5 .623 .695 .722 .732 .738 .743 .744 .745 .746 .7471.0 .596 .656 .682 .693 .699 .702 .705 .706 .707 .7071.5 .561 .615 .637 .644 .649 .650 .650 .650 .650 .6502.0 .538 .579 .591 .597 .600 .600 .601 .601 .601 .601PKU test 0.5* .682 .746 .766 .773 .776 .778 .778 .778 .778 .7781.0 .659 .728 .749 .759 .764 .766 .768 .769 .770 .7701.5 .632 .696 .720 .732 .737 .740 .741 .742 .742 .7422.0 .610 .666 .683 .688 .691 .693 .693 .694 .694 .694MSR test 0.5* .693 .770 .790 .797 .800 .800 .800 .800 .801 .8011.0 .670 .741 .767 .777 .783 .786 .788 .789 .789 .7891.5 .640 .708 .731 .740 .745 .748 .750 .750 .751 .7512.0 .614 .675 .699 .704 .708 .710 .712 .712 .712 .713AS test 0.3* .682 .747 .769 .779 .783 .783 .784 .785 .785 .7850.5 .673 .740 .759 .771 .777 .780 .782 .784 .784 .7841.0 .641 .708 .733 .744 .749 .751 .753 .754 .754 .7541.5 .606 .666 .686 .698 .703 .704 .705 .705 .705 .7052.0 .584 .627 .642 .649 .652 .653 .654 .654 .654 .655PKU train 0.5 .743 .782 .788 .787 .786 .785 .785 .785 .784 .7841.0 .732 .781 .792 .795 .797 .798 .798 .797 .798 .7981.2* .726 .777 .791 .795 .797 .798 .799 .799 .800 .8001.5 .718 .771 .786 .792 .795 .796 .797 .797 .798 .7982.0 .702 .760 .777 .783 .785 .786 .787 .787 .787 .787CITYU train 0.5 .750 .803 .812 .813 .812 .811 .811 .811 .811 .8111.0 .740 .799 .814 .819 .821 .822 .822 .822 .822 .8221.4* .728 .792 .811 .820 .824 .826 .827 .828 .829 .8291.5 .723 .787 .807 .816 .821 .823 .824 .825 .825 .8262.0 .701 .768 .791 .800 .804 .807 .808 .809 .809 .809MSR Train 0.5 .760 .792 .790 .787 .785 .784 .784 .784 .784 .7841.0 .758 .799 .805 .805 .805 .805 .804 .804 .804 .8041.5 .745 .799 .811 .814 .815 .815 .816 .816 .816 .8161.6* .742 .797 .810 .814 .816 .817 .817 .818 .818 .8182.0 .725 .787 .803 .809 .812 .813 .814 .814 .814 .814AS Train 0.5 .732 .742 .738 .735 .733 .733 .732 .732 .732 .7321.0 .736 .755 .756 .755 .755 .755 .755 .754 .754 .7541.5 .736 .765 .770 .771 .772 .772 .772 .772 .772 .7722.0 .734 .767 .777 .780 .781 .782 .783 .783 .784 .7842.2* .730 .767 .776 .781 .783 .785 .785 .785 .785 .786Column headings represent the number of iterations.
Boldfaced results represent almost the best F-measure.
* = the proper exponent.It is noteworthy that there is a strong correlation between the proper exponent inLRV and the scale of the corpus (N1 and N2), as shown in Figure 10.
According tothe simple regression analysis, the proper exponents can be approximately predicted.The empirical formulae for the prediction are shown in Table 7.
The predictions can-not perfectly fit the proper exponents, but they can be used to produce acceptableresults.440Wang et al A New Unsupervised Approach to Word SegmentationFigure 9The difference between the results of four settings.3.4 DiscussionIn this section, we discuss the convergence of segmentation results and computationalcomplexity of ESA.3.4.1 Convergence.
In this section, we discuss the convergence of ESA.Definition 1For a given character sequence A of length N, there is a set I of all possible segmen-tations.
The seg denotes the member of I (i.e., seg ?
I).
There is a sequence S of whicheach item si is a subset of I: I =?N?
10 si, si ?
sj = ?, i ?
[0,N ?
1], j ?
[0,N ?
1], and i =j.Therefore,si = {segk|The amount of delimitors of segk is i;segk ?
I, k ?
[1,(N ?
1i)]}, i ?
[0,N ?
1] (23)Table 6The results brought by different maximum lengths.Corpus 10 30 50 100CITYU test .706 .723 .726 .726PKU test .739 .750 .750 .749MSR test .755 .764 .765 .764AS test .752 .764 .765 .765441Computational Linguistics Volume 37, Number 3Figure 10The correlation between the scales and the proper exponents.For example, the set I consists of all possible segmentations of the character sequenceABC, namely I = {A B C, AB C, A BC, ABC}, where is a delimiter.
There are threeitems s0, s1, and s2 in the sequence S. Specifically, s0 = {ABC}, s1 = {AB C, A BC},and s2 = {A B C}, where the subscript is the number of delimiters contained by theTable 7The empirical formulae for the prediction (linear model).Setting Equation df R21 P = 0.0699?N1?N2?0.8?0.7766 6 .9732 P = 1.2056?N1?N2?0.94?3.132 6 .9823 P = 0.0005?N1?N2?0.55+0.0696 6 .9864 P = 0.0023?N1?N2?0.63?0.0586 6 .985In all cases, significance = .000 (i.e., p < .001)P denotes the proper exponent in LRV.R2 is the coefficient of determination.df is the degree of freedom.442Wang et al A New Unsupervised Approach to Word Segmentationsegmentation in the item.
The subscripts are non-negative integers and they are neverequal to each other.
Therefore, S can also be viewed as a finite sequence of non-negativeintegers (i.e., the subscripts 0, 1, 2).Definition 2According to Definition 1, there are four types of changes from the current segmentationto the next one, as shown in Figure 11:Change 1 denotes that the number of delimiters in the next segmentationis smaller than that in the current one.Change 2 denotes that the number of delimiters in the next segmentationis larger than that in the current one.Change 3a denotes that the number of delimiters in the next segmentationis equal to that in the current one, and the locations of delimiters in thenext segmentation are identical to those in the current one.Change 3b denotes that the number of delimiters in the next segmentationis equal to that in the current one, but the locations of delimiters in thenext segmentation are different from those in the current one.Theorem 1The main algorithm of ESA is a monotone increasing function of F (the F in IV).ProofIn ESA, the goodness evaluation E of segmentations of a character sequence X isE =?CV =?M1IV?
?M2LRV (24)Figure 11The four types of changes.443Computational Linguistics Volume 37, Number 3where?M1IV =(FFM)N(25)M1 and M2 are the numbers of segments and delimiters in the segmentation, respec-tively.
N is the length of X. LRV, FM, and N are constant for a given X. M1 and M2 arealso constant for a given segmentation of X.
Therefore, F is the only variable and E canbe viewed as a function of F, that is,E = C?
FN (26)Both C and N are constant; meanwhile, C, N, and F are positive integers.
Consequently,E is a monotone increasing function of F. Theorem 2For an input character sequence of finite length, the segmentation results produced byESA converge.ProofWhen si and si+1 are the selected result and the discarded result in the current segmen-tation of X respectively, the goodness value of si is larger than that of si+1, that is, E(si) >E(si+1).
Additionally, the number of delimiters in si+1 is larger than that in si.
Adjustmentin ESA ensures that the frequencies of segments in the selected result are not changed,and the frequencies of all proper subsequences of the segments are reduced.
Accordingto Theorem 1, E(si) > E(si+1) is true in the next segmentation.
Therefore, change 2 inDefinition 2 cannot exist.For example, the selected result in the current segmentation of a character sequenceABC is AB C. Therefore, E(AB C) > E(A B C) is true in the current segmentation.
Thefrequencies of subsequences A and B are reduced by 1, whereas those of segmentsAB and C are not changed.
According to Theorem 1, E(AB C) > E(A B C) is true inthe next segmentation.
Therefore, the selected result in the next segmentation cannotbe A B C.The selected results that have the same number of delimiters are regarded as thesame segmentation, which means that the results with changes 3a or 3b are viewedas unchanged results by ESA.
Therefore, the results produced by ESA are monotonedecreasing on the sequence S defined by Definition 1, which means that there are onlychanges 1 and 3 in ESA.
Additionally, S can be viewed as a finite sequence of non-negative integers, which means that S can also be viewed as a finite sequence of realnumbers with a lower bound.
According to the monotone convergence theorem, thesuccessive results produced by ESA converge.
The experimental results support the conclusion of convergence.
ESA approxi-mately converged after five iterations in all cases as shown in Figure 12.
Although wecannot prove that the results always converge to the optimum F-measure, ESA ensuresthat the F-measure is monotone increasing in most cases.Otherwise, we could head in another direction to explain the convergence: If theiterative process of ESA can be viewed as an EM type, the property of EM will be borne444Wang et al A New Unsupervised Approach to Word SegmentationFigure 12Convergence of results.by ESA, which means that ESA theoretically can converge (Dempster, Laird, and Rubin1977; Wu 1983).3.4.2 Complexity.
The core algorithm, Segment, is implemented with dynamic pro-gramming.
Each character sequence is only processed once.
The total number ofprocesses isN?
(N+1)2 , where N is the number of characters in the character sequence.In detail, each character sequence is compared N ?
1 times and is calculated N timesincluding 1 IV and N ?
1 CVs, which means that the growth rate is O(N2).
Furtheranalyzing the algorithm, the total number of comparisons isN?1?k=1(N?
k) ?
k = NN?1?k=1k?N?1?k=1k2=N2(N?
1)2?N(N?
1)(2N?
1)6=N3 ?N6(27)445Computational Linguistics Volume 37, Number 3and the total number of calculations isN?k=1(N?
k+ 1) ?
k = (N+ 1)N?k=1k?N?k=1k2=N(N+ 1)22?N(N+ 1)(2N+ 1)6=N3 + 3N2 + 2N6(28)Therefore, the time complexity is O(N3) in the worst case.For an iterative process, the total complexity is the product of the number of itera-tions and the complexity per iteration.
For example, the complexity of Nested Pitman-Yor Language Model (NPYLM) (Mochihashi, Yamada, and Ueda 2009) is O(N?L2) forbigrams, where N is the length of a character sequence and L is the maximum wordlength accepted.
Therefore, the time complexity of NPYLM is O(N3) when the lengthof the character sequence and the maximum word length are equal to each other.
Inaddition, NPYLM approximately converges around 50 iterations in the experimentreported, whereas ESA converges around five iterations in our experiment.
Therefore,ESA seems to be faster.In practice, the time complexity of ESA is not greater than O(N2), as shown inFigure 13, where N is the maximum length of input character sequences.
The ESAimplementation is a single thread program, and we run it on an AMD Athlon64 systemat 2.61GHz.4.
ComparisonIn this section, we briefly describe some approaches to word segmentation and comparethem with ESA.
We mainly concentrate on unsupervised approaches because of themotivation for our study.4.1 Descriptive ComparisonA statistical approach was proposed by Teahan et al (2000) (TH), which is based onthe partial matching (PPM) symbol-wise compression scheme.
The approach consists ofmultiple order models and an escape strategy that is used to transition from the higherorder model to the lower one.
The approach calculates the escape probabilities and theprobabilities of successive characters according to the training corpus that is manuallysegmented.
Therefore, TH is a supervised approach.Iterative Word Segmentation and Likelihood Ratio Ranking (IWSLRR) (Chang andSu 1997) is an iterative process that uses both certainty and uncertainty information(MI and entropy, respectively).
The approach segments words according to an aug-mented dictionary and adds potential words to the dictionary.
The program consistsof a segmentation module and a filtering module.
The augmented dictionary consistsof a system dictionary and the potential words.
The system dictionary stores the knownwords given as prior knowledge.
The potential words are produced by merging andfiltering.
MI and entropy are combined by Gaussian mixture in the filtering algorithm.The filtering algorithm uses a threshold to make the choice for the potential words.
InChang and Su (1997), the system dictionary was the combination of two dictionaries.446Wang et al A New Unsupervised Approach to Word SegmentationFigure 13The time complexity in practice (4 samples: 10, 30, 50, and 100).In addition, their approach extracted the potential words from unannotated corpora.Therefore, IWSLRR is semi-supervised.
Whereas ESA is completely unsupervised, bothIWSLRR and ESA use the combination of certainty and uncertainty.
MI and entropyare the measures of two kinds of information in IWSLRR, whereas ESA uses IV andLRV.
ESA directly multiplies IV and LRV to combine them, unlike IWSLRR, which usesGaussian mixture.Description Length Gain (DLG) (Kit and Wilks 1999) can be viewed as a compres-sion algorithm.
The algorithm finds the best substitutes for certain sequences to reducethe description length calculated by the encoding algorithms.
The DLG value is negativewhen replacing the one-character sequence.
Therefore DLG cannot uniformly processwords of different lengths.
That is to say, DLG needs an additional strategy to processone-character words, whereas ESA uniformly processes words of different lengths.TONGO (Threshold and maximum for n-grams that overlap) (Ando and Lee 2000,2003) counts non-straddling strings on two sides of potential boundaries and straddlingstrings containing the potential boundaries.
To process words of different lengths, the al-gorithm compares the two types of strings of the same length with each other.
The sumof goodness values produced from the comparisons is called the ?total vote.?
If the?total vote?
is the local maximum or exceeds a threshold, the location of the boundary447Computational Linguistics Volume 37, Number 3will be determined.
The algorithm uses held-out data sets to estimate the maximumorder of n-grams and the threshold.
The held-out data sets are manually annotated.Therefore, TONGO is semi-supervised.
The non-straddling and straddling strings canbe viewed as uncertainty and certainty, respectively.
In addition, the ?total vote?
is thestrategy used to combine the two kinds of information.
Therefore, the similarities anddifferences between ESA and TONGO are similar to those between ESA and IWSLRR.The Self-Supervised (SS) (Peng and Schuurmans 2001) approach has two parts:1.
Use EM to establish a core lexicon and a candidate lexicon.
The selectedword candidates move from the candidate lexicon to the core lexicon,which is termed forward selection.
The selected word candidates movefrom the core lexicon to the candidate lexicon, which is termed backwardselection.
The two kinds of selection are made by a self-improvingalgorithm.
The algorithm is based on the changes of F-measures evaluatedby validation corpora.2.
Use MI to split long words in the lexicon.The pruning algorithm uses two thresholds that are manually assigned, and the vali-dation corpus is manually annotated.
Therefore, SS is semi-supervised.
The word can-didates represent certainty and MI is used to measure uncertainty.
These two kinds ofinformation are independently considered by SS, whereas ESA uses a different methodto combine them.Voting Experts (VE) (Cohen, Heeringa, and Adams 2002; Cohen, Adams, andHeeringa 2007) uses logarithm frequency and boundary entropy.
Two independentvoting experts are based on the two kinds of information, respectively.
VE uses a localmaximum strategy in a window.
The maximum window size limits the word length.To process words of different lengths, VE standardizes frequencies and boundary en-tropies.
A threshold is used to make the final decision on segmentation.
VE indepen-dently processes the two kinds of information, whereas ESA combines them in CV.Accessor Variety (AV) (Feng et al 2004a, 2004b) uses uncertainty between a stringand its adjacent characters to assess the string?s independence of its context.
The countsof right and left adjacent characters are called right and left AV, respectively.
The AVof the string is the minimum between the right and left AV.
The algorithm uses a localmaximum strategy.
It uses a threshold to process short words, especially one-characterwords.
Several functions are provided to balance words of different lengths.
The AVvalue is similar to H(SP1) in ESA.Branching Entropy (BE) (Tanaka-Ishii 2005; Jin and Tanaka-Ishii 2006) is based ona law, namely: The uncertainty of tokens coming after a long character sequence mustbe lower than that coming after a short character sequence when the long sequence isa supersequence of the short sequence.
If the law is broken, there must be boundaries.Specifically, the algorithm has three rules to determine the location of boundaries:1.
The entropy of the location is the local maxima.2.
The entropy of the location is greater than that of the previous location inthe same sequence and the difference of the two entropies is greater than agiven threshold.3.
The entropy of the location is larger than a given threshold.BE only uses uncertainty information.448Wang et al A New Unsupervised Approach to Word SegmentationNested Pitman-Yor Language Model (NPYLM) (Mochihashi, Yamada, and Ueda2009) is a Hierarchical Pitman-Yor Language Model (HPYLM) (Teh 2006a, 2006b).
Thebase measure of the HPYLM is also a HPYLM.
Specifically, NPYLM uses a characterHPYLM as the base measure of a word HPYLM.
To process words of different lengths,NPYLM uses Poisson distribution to correct the base measure of the character HPYLM.The parameter ?
of the Poisson distribution is a variable determined by specific lan-guage and word types.
In detail, the ?
is estimated by a Gamma distribution withtwo hyperparameters assigned manually.
It is noteworthy that the F-measures of thisapproach were higher than 0.8 on two corpora of Bakeoff-2.Most approaches use certainty and uncertainty information.
Some use only oneof them, such as AV and BE.
Others use both of them, such as IWSLRR, TONGO, SS,VE, and ESA.
The combination of certainty and uncertainty is necessary for the latterapproaches, though the specific strategies of combination are different in each approach.Seeking the maxima and using thresholds are two strategies adopted to make a finaldecision on segmentation.
All of the approaches use either the local maximum strategy,the global strategy, or both.
Their difference lies in whether or not they use thresholds.Using thresholds involves more human effort and introduces random factors becauseof differences in each corpus.
ESA avoids using thresholds so that it can be applied todifferent corpora without much adjustment.Many approaches have their own strategies to process words of different lengths:1.
The Poisson distribution in NPYLM.2.
Pruning in SS.3.
The restriction on the order of n-gram in TONGO.4.
Standardizing in VE.5.
Several functions and thresholds in AV.6.
Ignoring one-character words in DLG.7.
Merging and filtering in IWSLRR.8.
Balancing in ESA.Some approaches adopt an iterative process, such as NPYLM, SS, IWSLRR, andESA.
The iterative process can often improve the accuracy of an unsupervised approach(Chang and Su 1997).
In practice, ESA greatly improves its segmentation results byusing an iterative process.4.2 Quantitative ComparisonIn this section, we compare the performance of ESA with that of other approaches,which we can divide into two categories:1.
Unsupervised approaches.
NPYLM, AV, BE, and DLG were tested onBakeoff data sets, and therefore ESA can be directly compared with them.VE was not evaluated on a Bakeoff data set, and therefore we compareESA with it on a similar scale and setting.2.
Semi-supervised and supervised approaches.
We compare ESA withIWSLRR, SS, TONGO, and TH on similar scales and settings.449Computational Linguistics Volume 37, Number 3Mochihashi, Yamada, and Ueda (2009) trained NPYLM on Bakeoff-2 training dataand tested it on Bakeoff-2 test data, which means that the statistical information of thetraining and test data was used together by NPYLM.
Therefore, we evaluate ESA on themerged corpora to compare with NPYLM.
Specifically, we merge the test corpora withthe corresponding training ones.
For example, the CITYU test corpus is added to the endof the CITYU training corpus as a single corpus.
NPYLM considered specific languageand word types.
Therefore, we use setting 4 to test ESA, as shown in Table 8.
In addition,we cite the best and worst F-measures of supervised approaches in the Bakeoff-2 closedtest (Emerson 2005) for comparison in Table 8.Zhao and Kit (2008a) tested AV, BE, and DLG with both the training and test data ofBakeoff-3 to extract word candidates.
Therefore, we evaluate ESA on merged corpora.Zhao and Kit claimed that the approaches were tested without any prior knowledge.Therefore, we use setting 1 to test ESA, as shown in Table 9.
In addition, we cite the bestand worst F-measures of supervised approaches in the Bakeoff-3 closed test (Levow2006) for comparison in Table 9.In Tables 8 and 9, there are two different evaluations: E1 and E2.
ESA segments themerged corpora in both of them.
The part of a segmentation result belonging to theoriginal test corpus is evaluated alone in E1, whereas the whole of the segmentationresult is evaluated in E2.VE was evaluated on Guo Jin?s Mandarin Chinese PH corpus (Cohen, Heeringa,and Adams 2002; Cohen, Adams, and Heeringa 2007).
When the average length ofTable 8The comparison between NPYLM and ESA.CITYU MSRNPYLM bigram .824 .802NPYLM trigram .817 .807ESA E1 .828 .819ESA E2 .804 .831Worst Closed .759 .896Best Closed .943 .966The exponent in LRV 1.4 1.7Table 9The comparison between DLG, AV, BE, and ESA.CKIP(AS) CITYU UPUC(CTB) MSRADLG .655 .659 .632 .655AV .630 .650 .618 .638BE .629 .649 .618 .638DLG&AV .663 .692 .658 .667DLG&BE .650 .689 .650 .656ESA E1 .752 .757 .770 .760ESA E2 .748 .764 .783 .779Worst Closed .710 .589 .818 .819Best Closed .958 .972 .933 .963The exponent in LRV 2.8 1.8 1.4 1.9450Wang et al A New Unsupervised Approach to Word Segmentationwords (VE1) was given, the F-measure of VE was 0.77.
However, the F-measure was0.57 without a given length (VE2).
Because the scale of the PH corpus is relatively small(19,163 characters), we use the arithmetic mean of F-measures of four Bakeoff-2 testcorpora, whose scales are also relatively small, to compare with VE.
The window sizeof VE was six and the punctuation of the corpus was removed in their experiment.Therefore, we use the result of setting 3 to compare with VE, as shown in Table 10.We use the result of setting 4 to compare both semi-supervised and supervised ap-proaches: IWSLRR (Chang and Su 1997), SS (Peng and Schuurmans 2001), and TONGO(Ando and Lee 2000, 2003) are semi-supervised; TH (Teahan et al 2000) is supervised.These four approaches used relatively large corpora to train and test, and therefore weuse the arithmetic mean of F-measures of four Bakeoff-2 training corpora to comparewith them, as shown in Table 11.The system dictionary in the tests of IWSLRR was the combination of the AcademiaSinica dictionary (CKIP 90) and the BDC electronic dictionary (BDC 93).
The unanno-tated Chinese corpus used by IWSLRR contained 311,591 sentences (about 1,670,000words, a relatively large scale), which came from the China Times Daily News.
B, T, andQ denote bigrams, trigrams, and quadragrams (i.e., words of 2, 3, and 4 characters),respectively.
The result of IWSLRR was achieved after 21 iterations.SS used segmented text as validation data.
The training corpus had 90M characters,which contained one year of the People?s Daily news service stories.
The test corpus wasthe Chinese Tree bank from LDC (1M characters), which contained 325 articles from theXinhua newswire.
The maximum length of words in the test of SS was four.TONGO used segmented text as held-out data.
The corpus had 79,326,406 char-acters, which came from the 1993 Nikkei Japanese newswire.
The maximum order ofn-grams in the test of TONGO was six.
However, Ando and Lee (2000, 2003) did notdirectly present specific F-measures.
The F-measure of TONGO was approximately0.816 in their charts.TH is typically supervised, and therefore we estimate the accuracy of TH on cross-corpora to compare with ESA.
According to the results presented in Teahan et al (2000),we establish the correlation between error rate and F-measure by using simple regres-sion analysis.
The corpora used by TH were Guo Jin?s Mandarin Chinese PH corpuscontaining about 1M words and the Rocling Standard Segmentation Corpus containingabout 2M words.
L and P in the table denote the estimations of a linear regressionmodel (R2 = 0.905) and a second order polynomial model (R2 = 0.95), respectively.T1 denotes training with the PH corpus and testing with the Rocling corpus, and T2denotes training with the Rocling corpus and testing with the PH corpus.Table 10The comparison between VE and ESA.VE1 VE2 ESAF-measure .770 .570 .781Table 11The comparison between IWSLRR(I), SS(S), TONGO(O), TH(T), and ESA.I-B I-T I-Q S O T1-L T1-P T2-L T2-P ESAF-measure .761 .536 .703 .742 .816 .805 .637 .750 .419 .808451Computational Linguistics Volume 37, Number 35.
ConclusionThis article proposes ESA, an unsupervised approach to word segmentation anddemonstrates its effectiveness on Chinese corpora.
ESA has no thresholds or parametersestimated.
The only parameter (the exponent in LRV) can be predicted via empiricalformulae.
ESA can produce acceptable results without any encoding information ex-cept line breaks.
When ESA utilizes prior knowledge such as punctuation and otherencoding information, it performs much better.In practice, unsupervised approaches can take a few steps to improve usability,including:1.
Combine with supervised approaches (Zhao and Kit 2008b) or becomea supervised approach (Mochihashi, Yamada, and Ueda 2009).2.
Find more suitable applications for unsupervised approaches (Bod 2006).AcknowledgmentsWe thank Dr. Gina-Anne Levow very muchfor providing the Bakeoff-3 corpora.ReferencesAndo, Rie Kubota and Lillian Lee.
2000.Mostly-unsupervised statisticalsegmentation of Japanese: Applications toKanji.
In Proceedings of the 1st Meeting of theNorth American Chapter of the Association forComputational Linguistics (NAACL?2000),pages 241?248, Seattle, WA.Ando, Rie Kubota and Lillian Lee.
2003.Mostly-unsupervised statisticalsegmentation of Japanese kanji sequences.Natural Language Engineering, 9(2):127?149.Bod, Rens.
2006.
An all-subtrees approachto unsupervised parsing.
In Proceedingsof the 21st International Conference onComputational Linguistics and 44th AnnualMeeting of the Association for ComputationalLinguistics (COLING/ACL?2006),pages 865?872, Sydney.Bortfeld, Heather, James L. Morgan,Roberta Michnick Golinkoff, andKaren Rathbun.
2005.
Mommy and me:Familiar names help launch babies intospeech-stream segmentation.
PsychologicalScience, 16(4):298?304.Chang, Jing-Shin and Keh-Yih Su.
1997.
Anunsupervised iterative method for Chinesenew lexicon extraction.
International Journalof Computational Linguistics & ChineseLanguage Processing, 1(1):101?157.Cohen, Paul, Niall Adams, and BrentHeeringa.
2007.
Voting experts: Anunsupervised algorithm for segmentingsequences.
Intelligent Data Analysis,11(6):607?625.Cohen, Paul, Brent Heeringa, and NiallAdams.
2002.
Unsupervised segmentationof categorical time series into episodes.
InProceedings of the 2nd IEEE InternationalConference on Data Mining (ICDM?2002),pages 99?106, Maebashi.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society.Series B (Methodological), 39(1):1?38.Emerson, Thomas.
2005.
The secondinternational Chinese word segmentationBakeoff.
In Proceedings of the FourthSIGHAN Workshop on Chinese LanguageProcessing, pages 123?133, Jeju Island.Estes, Katharine Graf, Julia L. Evans,Martha W. Alibali, and Jenny R. Saffran.2007.
Can infants map meaning to newlysegmented words?
Statistical segmentationand word learning.
Psychological Science,18(3):254?260.Feng, Haodi, Kang Chen, Xiaotie Deng, andWeimin Zheng.
2004a.
Accessor varietycriteria for Chinese word extraction.Computational Linguistics, 30(1):75?93.Feng, Haodi, Kang Chen, Chunyu Kit, andXiaotie Deng.
2004b.
Unsupervisedsegmentation of Chinese corpus usingaccessor variety.
In Proceedings of the 1stInternational Joint Conference on NaturalLanguage Processing (IJCNLP?2004),pages 282?288, Sanya, Hainan Island.Fredkin, Edward.
1960.
Trie memory.Communications of the ACM, 3(9):490?499.Fung, Pascale and Dekai Wu.
1994.Statistical augmentation of a Chinesemachine-readable dictionary.
InProceedings of the 2nd Workshop onVery Large Corpora (WVLC-2) at the452Wang et al A New Unsupervised Approach to Word Segmentation15th International Conference onComputational Linguistics (COLING?1994),pages 69?85, Kyoto.Gao, Jianfeng, Mu Li, Andi Wu, andChang-Ning Huang.
2005.
Chineseword segmentation and named entityrecognition: A pragmatic approach.Computational Linguistics, 31(4):531?574.Ge, Xianping, Wanda Pratt, and PadhraicSmyth.
1999.
Discovering Chinese wordsfrom unsegmented text.
In Proceedings ofthe 22nd Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR?1999),pages 271?272, Berkeley, CA.Goldwater, Sharon, Thomas L. Griffiths,and Mark Johnson.
2006.
Contextualdependencies in unsupervised wordsegmentation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics(COLING/ACL?2006), pages 673?680,Sydney.Jin, Zhihui and Kumiko Tanaka-Ishii.2006.
Unsupervised segmentation ofChinese text by use of branching entropy.In Proceedings of the 21st InternationalConference on Computational Linguisticsand 44th Annual Meeting of the Associationfor Computational Linguistics(COLING/ACL?2006) Main ConferencePoster Sessions, pages 428?435, Sydney.Kit, Chunyu and Yorick Wilks.
1999.Unsupervised learning of word boundarywith description length gain.
In Proceedingsof Ninth Conference of the European Chapterof the Association for ComputationalLinguistics (EACL?1999): ComputationalNatural Language Learning (CoNLL?1999),pages 1?6, Bergen.Levow, Gina-Anne.
2006.
The thirdinternational Chinese language processingbakeoff: Word segmentation and namedentity recognition.
In Proceedings of the FifthSIGHAN Workshop on Chinese LanguageProcessing, pages 108?117, Sydney.Lu?, Xueqiang, Le Zhang, and Junfeng Hu.2004.
Statistical substring reduction inlinear time.
In Proceedings of the 1stInternational Joint Conference on NaturalLanguage Processing (IJCNLP?2004),pages 320?327, Sanya, Hainan Island.Manber, Udi and Gene Myers.
1990.
Suffixarrays: A new method for on-line stringsearches.
In Proceedings of the 1st AnnualACM-SIAM Symposium on DiscreteAlgorithms (SODA?1990), pages 319?327,San Francisco, CA.McCreight, Edward M. 1976.
A space-economical suffix tree constructionalgorithm.
Journal of the ACM, 23(2):262?272.Mochihashi, Daichi, Takeshi Yamada,and Naonori Ueda.
2009.
Bayesianunsupervised word segmentation withnested Pitman-Yor language modeling.In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the4th International Joint Conference on NaturalLanguage Processing of the AFNLP(ACL-IJCNLP?2009), pages 100?108, Suntec.Morrison, Donald R. 1968.
PATRICIA?practical algorithm to retrieve informationcoded in alphanumeric.
Journal of the ACM,15(4):514?534.Palmer, David D. 1997.
A trainablerule-based algorithm for wordsegmentation.
In Proceedings of the 35thAnnual Meeting of the Association forComputational Linguistics (ACL?1997),pages 321?328, Madrid.Peng, Fuchun and Dale Schuurmans.2001.
Self-supervised Chinese wordsegmentation.
In Proceedings of theFourth International Symposium onIntelligent Data Analysis (IDA?2001),pages 238?247, Lisbon.Pitman, Jim and Marc Yor.
1997.
Thetwo-parameter Poisson-Dirichletdistribution derived from a stablesubordinator.
The Annals of Probability,25(2):855?900.Seidl, Amanda and Elizabeth K. Johnson.2006.
Infant word segmentation revisited:Edge alignment facilitates targetextraction.
Developmental Science,9(6):565?573.Sproat, Richard and Thomas Emerson.
2003.The first international Chinese wordsegmentation bakeoff.
In Proceedings of the2nd SIGHAN Workshop on Chinese LanguageProcessing, pages 133?143, Sapporo.Sproat, Richard, William Gales, Chilin Shih,and Nancy Chang.
1996.
A stochasticfinite-state word-segmentation algorithmfor Chinese.
Computational Linguistics,22(3):377?404.Sproat, Richard and Chilin Shih.
1990.A statistical method for finding wordboundaries in Chinese text.
ComputerProcessing of Chinese and Oriental Languages,4(4):336?351.Sproat, Richard and Chilin Shih.
2001.Corpus-based methods in Chinesemorphology and phonology.
Unpublishedcourse notes, 2001 Summer Institute ofthe Linguistic Society of America, in the453Computational Linguistics Volume 37, Number 3Subinstitute on Chinese CorpusLinguistics at the University ofCalifornia, Santa Barbara, CA.Swingley, Daniel.
2008.
The roots of theearly vocabulary in infants?
learning fromspeech.
Current Directions in PsychologicalScience, 17(5):308?312.Tanaka-Ishii, Kumiko.
2005.
Entropy as anindicator of context boundaries: Anexperiment using a Web search engine.In Proceedings of the 2nd InternationalJoint Conference on Natural LanguageProcessing (IJCNLP?2005), pages 93?105,Jeju Island.Teahan, W. J., Rodger McNab, Yingying Wen,and Ian H. Witten.
2000.
A compression-based algorithm for Chinese wordsegmentation.
Computational Linguistics,26(3):375?393.Teh, Yee Whye.
2006a.
A Bayesianinterpretation of interpolated Kneser-Ney.Technical report TRA2/06, NationalUniversity of Singapore, School ofComputing.Teh, Yee Whye.
2006b.
A hierarchicalBayesian language model based onPitman-Yor processes.
In Proceedingsof the 21st International Conference onComputational Linguistics and 44th AnnualMeeting of the Association for ComputationalLinguistics (COLING/ACL?2006),pages 985?992, Sydney.Teh, Yee Whye, Michael I. Jordan,Matthew J. Beal, and David M. Blei.2006.
Hierarchical Dirichlet processes.Journal of the American StatisticalAssociation, 101(476):1566?1581.Wood, Frank and Yee Whye Teh.
2008.A hierarchical, hierarchical Pitman-Yor processlanguage model.
In The 25th InternationalConference on Machine Learning(ICML?2008) Workshop on NonparametricBayes, Helsinki.Wu, Andi.
2003.
Customizable segmentationof morphologically derived wordsin Chinese.
International Journal ofComputational Linguistics and ChineseLanguage Processing, 8(1):1?27.Wu, C. F. Jeff.
1983.
On the convergenceproperties of the EM algorithm.
TheAnnals of Statistics, 11(1):95?103.Zhang, Le, Xueqiang Lu?, Yanna Shen,and Tianshun Yao.
2003.
A statisticalapproach to extract Chinese chunkcandidates from large corpora.In Proceedings of 20th InternationalConference on Computer Processing ofOriental Languages (ICCPOL?2003),pages 109?117, ShengYang.Zhao, Hai.
2009.
Character-leveldependencies in Chinese: Usefulnessand learning.
In Proceedings of the 12thConference of the European Chapter ofthe ACL (EACL?2009), pages 879?887,Athens.Zhao, Hai and Chunyu Kit.
2008a.
Anempirical comparison of goodnessmeasures for unsupervised Chineseword segmentation with a unifiedframework.
In Proceedings of the 3rdInternational Joint Conference on NaturalLanguage Processing (IJCNLP?2008)Volume-I, pages 9?16, Hyderabad.Zhao, Hai and Chunyu Kit.
2008b.
Exploitingunlabeled text with different unsupervisedsegmentation criteria for Chinese wordsegmentation.
Research in ComputingScience, 33:93?104.454
