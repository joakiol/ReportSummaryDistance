Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 366?375,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Transition-based Algorithm for AMR ParsingChuan WangBrandeis Universitycwang24@brandeis.eduNianwen XueBrandeis Universityxuen@brandeis.eduSameer PradhanHarvard Medical SchoolSameer.Pradhan@childrens.harvard.eduAbstractWe present a two-stage framework to parsea sentence into its Abstract Meaning Repre-sentation (AMR).
We first use a dependencyparser to generate a dependency tree for thesentence.
In the second stage, we designa novel transition-based algorithm that trans-forms the dependency tree to an AMR graph.There are several advantages with this ap-proach.
First, the dependency parser can betrained on a training set much larger than thetraining set for the tree-to-graph algorithm, re-sulting in a more accurate AMR parser over-all.
Our parser yields an improvement of 5%absolute in F-measure over the best previousresult.
Second, the actions that we design arelinguistically intuitive and capture the regular-ities in the mapping between the dependencystructure and the AMR of a sentence.
Third,our parser runs in nearly linear time in practicein spite of a worst-case complexity of O(n2).1 IntroductionAbstract Meaning Representation (AMR) is arooted, directed, edge-labeled and leaf-labeledgraph that is used to represent the meaning of a sen-tence.
The AMR formalism has been used to anno-tate the AMR Annotation Corpus (Banarescu et al,2013), a corpus of over 10 thousand sentences thatis still undergoing expansion.
The building blocksfor an AMR representation are concepts and rela-tions between them.
Understanding these conceptsand their relations is crucial to understanding themeaning of a sentence and could potentially bene-fit a number of natural language applications suchas Information Extraction, Question Answering andMachine Translation.The property that makes AMR a graph instead ofa tree is that AMR allows reentrancy, meaning thatthe same concept can participate in multiple rela-tions.
Parsing a sentence into an AMR would seemto require graph-based algorithms, but moving tograph-based algorithms from the typical tree-basedalgorithms that we are familiar with is a big step interms of computational complexity.
Indeed, quite abit of effort has gone into developing grammars andefficient graph-based algorithms that can be used toparse AMRs (Chiang et al, 2013).wantpoliceThearresttoKarrasinMichaelSingaporensubjxcompdetauxdobjprepnnpobj(a) Dependency treewant-01policearrest-01personSingaporename?Michael?
?Karras?ARG0ARG1ARG0ARG1locationnameop1 op2(b) AMR graphFigure 1: Dependency tree and AMR graph for thesentence, ?The police want to arrest Micheal Karrasin Singapore.
?Linguistically, however, there are many similari-ties between an AMR and the dependency structureof a sentence.
Both describe relations as holding be-tween a head and its dependent, or between a parentand its child.
AMR concepts and relations abstractaway from actual word tokens, but there are regular-ities in their mappings.
Content words generally be-366come concepts while function words either becomerelations or get omitted if they do not contribute tothe meaning of a sentence.
This is illustrated in Fig-ure 1, where ?the?
and ?to?
in the dependency treeare omitted from the AMR and the preposition ?in?becomes a relation of type location.
In AMR, reen-trancy is also used to represent co-reference, but thisonly happens in some limited contexts.
In Figure 1,?police?
is both an argument of ?arrest?
and ?want?
asthe result of a control structure.
This suggests that itis possible to transform a dependency tree into anAMR with a limited number of actions and learn amodel to determine which action to take given pairsof aligned dependency trees and AMRs as trainingdata.This is the approach we adopt in the presentwork, and we present a transition-based frameworkin which we parse a sentence into an AMR by tak-ing the dependency tree of that sentence as input andtransforming it to an AMR representation via a se-ries of actions.
This means that a sentence is parsedinto an AMR in two steps.
In the first step the sen-tence is parsed into a dependency tree with a depen-dency parser, and in the second step the dependencytree is transformed into an AMR graph.
One advan-tage of this approach is that the dependency parserdoes not have to be trained on the same data set asthe dependency to AMR transducer.
This allows usto use more accurate dependency parsers trained ondata sets much larger than the AMR Annotation Cor-pus and have a more advantageous starting point.Our experiments show that this approach is very ef-fective and yields an improvement of 5% absoluteover the previously reported best result (Flanigan etal., 2014) in F-score, as measure by the Smatch met-ric (Cai and Knight, 2013).The rest of the paper is as follows.
In ?2, wedescribe how we align the word tokens in a sen-tence with its AMR to create a span graph basedon which we extract contextual information as fea-tures and perform actions.
In ?3, we present ourtransition-based parsing algorithm and describe theactions used to transform the dependency tree of asentence into an AMR.
In ?4, we present the learn-ing algorithm and the features we extract to train thetransition model.
In ?5, we present experimental re-sults.
?6 describes related work, and we conclude in?7.2 Graph RepresentationUnlike the dependency structure of a sentence whereeach word token is a node in the dependency treeand there is an inherent alignment between the wordtokens in the sentence and the nodes in the depen-dency tree, AMR is an abstract representation wherethe word order of the corresponding sentence is notmaintained.
In addition, some words become ab-stract concepts or relations while other words aresimply deleted because they do not contribute tomeaning.
The alignment between the word tokensand the concepts is non-trivial, but in order to learnthe transition from a dependency tree to an AMRgraph, we have to first establish the alignment be-tween the word tokens in the sentence and the con-cepts in the AMR.
We use the aligner that comeswith JAMR (Flanigan et al, 2014) to produce thisalignment.
The JAMR aligner attempts to greedilyalign every concept or graph fragment in the AMRgraph with a contiguous word token sequence in thesentence.want-01policearrest-01personname?Micheal?
?Karras?ARG0ARG1ARG0ARG1nameop1 op2(a) AMR graphs0,1:ROOTs3,4:want-01s2,3:polices5,6:arrest-01s6,8:person+nameARG0ARG1ARG0ARG1(b) Span graphFigure 2: AMR graph and its span graph for the sen-tence, ?The police want to arrest Micheal Karras.
?We use a data structure called span graph torepresent an AMR graph that is aligned with theword tokens in a sentence.
For each sentence w =w0, w1, .
.
.
, wn, where token w0is a special rootsymbol, a span graph is a directed, labeled graphG = (V,A), where V = {si,j|i, j ?
(0, n) and j >i} is a set of nodes, and A ?
V ?
V is a set of arcs.Each node si,jof G corresponds to a continuousspan (wi, .
.
.
, wj?1) in sentence w and is indexedby the starting position i.
Each node is assigned aconcept label from a set LVof concept labels andeach arc is assigned a relation label from a set LA367of relation labels, respectively.For example, given an AMR graph GAMRin Fig-ure 2a, its span graph G can be represented as Fig-ure 2b.
In span graph G, node s3,4?s sentence spanis (want) and its concept label is want-01, whichrepresents a single node want-01 in AMR.
To sim-plify the alignment, when creating a span graph outof an AMR, we also collapse some AMR subgraphsin such a way that they can be deterministically re-stored to their original state for evaluation.
For ex-ample, the four nodes in the AMR subgraph that cor-respond to span (Micheal, Karras) is collapsed intoa single node s6,8in the span graph and assigned theconcept label person+name, as shown in Figure 3.So the concept label set that our model predicts con-sists of both those from the concepts in the originalAMR graph and those as a result of collapsing theAMR subgraphs.personname?Micheal?
?Karras?s6,8:person+namenameop1 op2Figure 3: Collapsed nodesRepresenting AMR graph this way allows us toformulate the AMR parsing problem as a joint learn-ing problem where we can design a set of actions tosimultaneously predict the concepts (nodes) and re-lations (arcs) in the AMR graph as well as the labelson them.3 Transition-based AMR Parsing3.1 Transition SystemSimilar to transition-based dependency pars-ing (Nivre, 2008), we define a transition system forAMR parsing as a quadruple S = (S, T, s0, St),where?
S is a set of parsing states (configurations).?
T is a set of parsing actions (transitions), eachof which is a function t : S ?
S.?
s0is an initialization function, mapping eachinput sentence w and its dependency tree D toan initial state.?
St?
S is a set of terminal states.Each state (configuration) of our transition-basedparser is a triple (?, ?,G).
?
is a buffer that storesindices of the nodes which have not been processedand we write ?
= ?0|?
?to indicate that ?0is the top-most element of ?.
?
is also a buffer [?0, ?1, .
.
.
, ?j]and each element ?iof ?
indicates the edge (?0, ?i)which has not been processed in the partial graph.We also write ?
= ?0|?
?to indicate the topmost el-ement of ?
is ?0.
We use span graph G to store thepartial parses for the input sentence w. Note thatunlike traditional transition-based syntactic parserswhich store partial parses in the stack structure andbuild a tree or graph incrementally, here we use?
and ?
buffers only to guide the parsing process(which node or edge to be processed next) and theactual tree-to-graph transformations are applied toG.When the parsing procedure starts, ?
is initializedwith a post-order traversal of the input dependencytreeD with topmost element ?0, ?
is initialized withnode ?0?s children or set to null if ?0is a leaf node.G is initialized with all the nodes and edges of D.Initially, all the nodes of G have a span length ofone and all the labels for nodes and edges are setto null.
As the parsing procedure goes on, the parserwill process all the nodes and their outgoing edges independency treeD in a bottom-up left-right manner,and at each state certain action will be applied tothe current node or edge.
The parsing process willterminate when both ?
and ?
are empty.The most important part of the transition-basedparser is the set of actions (transitions).
As statedin (Sartorio et al, 2013), the design space of possi-ble actions is actually infinite since the set of pars-ing states is infinite.
However, if the problem isamenable to transition-based parsing, we can designa finite set of actions by categorizing all the possi-ble situations we run into in the parsing process.
In?5.2 we show this is the case here and our action setcan account for almost all the transformations fromdependency trees to AMR graphs.We define 8 types of actions for the actions setT , which is summarized in Table 1.
The action setcould be divided into two categories based on con-ditions of buffer ?.
When ?
is not empty, parsingdecisions are made based on the edge (?0, ?0); oth-368Action Current state?
Result state Assign labels PreconditionNEXT EDGE-lr(?0|?
?, ?0|?
?, G)?
(?0|?
?, ?
?, G?)
?
[(?0, ?0)?
lr]?
is not emptySWAP-lr(?0|?
?, ?0|?
?, G)?
(?0|?0|?
?, ?
?, G?)
?
[(?0, ?0)?
lr]REATTACHk-lr(?0|?
?, ?0|?
?, G)?
(?0|?
?, ?
?, G?)
?
[(k, ?0)?
lr]REPLACE HEAD (?0|?
?, ?0|?
?, G)?
(?0|?
?, ?
= CH(?0, G?
), G?)
NONEREENTRANCEk-lr(?0|?
?, ?0|?
?, G)?
(?0|?
?, ?0|?
?, G?)
?
[(k, ?0)?
lr]MERGE (?0|?
?, ?0|?
?, G)?
(??|?
?, ?
?, G?)
NONENEXT NODE-lc(?0|?1|?
?, [], G)?
(?1|?
?, ?
= CH(?1, G?
), G?)
?[?0?
lc]?
is emptyDELETE NODE (?0|?1|?
?, [], G)?
(?1|?
?, ?
= CH(?1, G?
), G?)
NONETable 1: Transitions designed in our parser.
CH(x, y) means getting all node x?s children in graph y.erwise, only the current node ?0is examined.
Also,to simultaneously make decisions on the assignmentof concept/relation label, we augment some of theactions with an extra parameter lror lc.
We define?
: V ?
LVas the concept labeling function fornodes and ?
: A?
LAas the relation labeling func-tion for arcs.
So ?
[(?0, ?0) ?
lr] means assign-ing relation label lrto arc (?0, ?0).
All the actionsupdate buffer ?, ?
and apply some transformationG ?
G?to the partial graph.
The 8 actions are de-scribed below.?
NEXT-EDGE-lr(ned).
This action assigns arelation label lrto the current edge (?0, ?0)and makes no further modification to the par-tial graph.
Then it pops out the top element ofbuffer ?
so that the parser moves one step for-ward to examine the next edge if it exists.opposeKoreaSouthandIsraelopposeandKoreaSouth Israelop1Figure 4: SWAP action?
SWAP-lr(sw).
This action reverses the de-pendency relation between node ?0and ?0andthen makes node ?0as new head of the sub-graph.
Also it assigns relation label lrto thearc (?0, ?0).
Then it pops out ?0and inserts itinto ?
right after ?0for future revisiting.
Thisaction is to resolve the difference in the choiceof head between the dependency tree and theAMR graph.
Figure 4 gives an example of ap-plying SWAP-op1 action for arc (Korea, and)in the dependency tree of sentence ?South Ko-rea and Israel oppose ...?.?
REATTACHk-lr(reat).
This action removesthe current arc (?0, ?0) and reattaches node ?0to some node k in the partial graph.
It alsoassigns a relation label lrto the newly cre-ated arc (k, ?0) and advances one step by pop-ping out ?0.
Theoretically, the choice of nodek could be any node in the partial graph un-der the constraint that arc (k, ?0) doesn?t pro-duce a self-looping cycle.
The intuition behindthis action is that after swapping a head andits dependent, some of the dependents of theold head should be reattached to the new head.Figure 5 shows an example where node Israelneeds to be reattached to node and after a head-dependent swap.opposeandKoreaSouth Israelop1reattachopposeandKoreaSouthIsraelop1 op2Figure 5: REATTACH action?
REPLACE-HEAD (rph).
This action removesnode ?0, replaces it with node ?0.
Node ?0alsoinherits all the incoming and outgoing arcs of?0.
Then it pops out ?0and inserts it into thetop position of buffer ?.
?
is re-initialized withall the children of ?0in the transformed graphG?.
This action targets nodes in the dependencytree that do not correspond to concepts in AMR369graph and become a relation instead.
An exam-ple is provided in Figure 6, where node in, apreposition, is replaced with node Singapore,and in a subsequent NEXT-EDGE action thatexamines arc (live, Singapore), the arc is la-beled location.liveinSingaporeliveSingaporeFigure 6: REPLACE-HEAD action?
REENTRANCEk-lr(reen).
This is the actionthat transforms a tree into a graph.
It keeps thecurrent arc unchanged, and links node ?0to ev-ery possible node k in the partial graph that canalso be its parent.
Similar to the REATTACHaction, the newly created arc (k, ?0) should notproduce a self-looping cycle and parameter k isbounded by the sentence length.
In practice, weseek to constrain this action as we will explainin ?3.2.
Intuitively, this action can be used tomodel co-reference and an example is given inFigure 7.wantpolicearrestreentrancewantpolicearrestARG0Figure 7: REENTRANCE action?
MERGE (mrg).
This action merges nodes ?0and ?0into one node ??
which covers multiplewords in the sentence.
The new node inher-its all the incoming and outgoing arcs of bothnodes ?0and ?0.
The MERGE action is in-tended to produce nodes that cover a continu-ous span in the sentence that corresponds to asingle name entity in AMR graph.
see Figure 8for an example.arrestMichaelKarrasarrestMichael,KarrasFigure 8: MERGE actionWhen ?
is empty, which means all the outgoing arcsof node ?0have been processed or ?0has no outgo-ing arcs, the following two actions can be applied:?
NEXT-NODE-lc(nnd).
This action first as-signs a concept label lcto node ?0.
Then itadvances the parsing procedure by popping outthe top element ?0of buffer ?
and re-initializesbuffer ?
with all the children of node ?1whichis the current top element of ?.
Since this actionwill be applied to every node which is kept inthe final parsed graph, concept labeling couldbe done simultaneously through this action.?
DELETE-NODE (dnd).
This action simplydeletes the node ?0and removes all the arcs as-sociated with it.
This action models the factthat most function words are stripped off in theAMR of a sentence.
Note that this action onlytargets function words that are leaves in the de-pendency tree, and we constrain this action byonly deleting nodes which do not have outgo-ing arcs.When parsing a sentence of length n (excludingthe special root symbol w0), its corresponding de-pendency tree will have n nodes and n ?
1 arcs.For projective transition-based dependency parsing,the parser needs to take exactly 2n ?
1 steps or ac-tions.
So the complexity is O(n).
However, forour tree-to-graph parser defined above, the actionsneeded are no longer linearly bounded by the sen-tence length.
Suppose there are no REATTACH,REENTRANCE and SWAP actions during the pars-ing process, the algorithm will traverse every nodeand edge in the dependency tree, which results in2n actions.
However, REATTACH and REEN-TRANCE actions would add extra edges that needto be re-processed and the SWAP action adds bothnodes and edges that need to be re-visited.
Since the370space of all possible extra edges is (n ?
2)2and re-visiting them only adds more actions linearly, the to-tal asymptotic runtime complexity of our algorithmis O(n2).In practice, however, the number of applicationsof the REATTACH action is much less than theworst case scenario due to the similarities betweenthe dependency tree and the AMR graph of a sen-tence.
Also, nodes with reentrancies in AMR onlyaccount for a small fraction of all the nodes, thusmaking the REENTRANCE action occur at constanttimes.
These allow the tree-to-graph parser to parsea sentence in nearly linear time in practice.3.2 Greedy Parsing AlgorithmAlgorithm 1 Parsing algorithmInput: sentence w = w0.
.
.
wnand its dependencytree DwOutput: parsed graph Gp1: s?
s0(Dw, w)2: while s /?
Stdo3: T ?
all possible actions according to s4: bestT ?
arg maxt?Tscore(t, c)5: s?
apply bestT to s6: end while7: return GpOur parsing algorithm is similar to the parser in(Sartorio et al, 2013).
At each parsing state s ?
S,the algorithm greedily chooses the parsing actiont ?
T that maximizes the score function score().The score function is a linear model defined overparsing action t and parsing state s.score(t, s) = ~?
?
?
(t, s) (1)where ~?
is the weight vector and ?
is a functionthat extracts the feature vector representation for onepossible state-action pair ?t, s?.First, the algorithm initializes the state s with thesentence w and its dependency tree Dw.
At eachiteration, it gets all the possible actions for currentstate s (line 3).
Then, it chooses the action with thehighest score given by function score() and appliesit to s (line 4-5).
When the current state reaches aterminal state, the parser stops and returns the parsedgraph.As pointed out in (Bohnet and Nivre, 2012), con-straints can be added to limit the number of possibleactions to be evaluated at line 3.
There could be for-mal constraints on states such as the constraint thatthe SWAP action should not be applied twice to thesame pair of nodes.
We could also apply soft con-straints to filter out unlikely concept labels, relationlabels and candidate nodes k for REATTACH andREENTRANCE.
In our parser, we enforce the con-straint that NEXT-NODE-lccan only choose fromconcept labels that co-occur with the current node?slemma in the training data.
We also empirically setthe constraint that REATTACHkcould only choosek among ?0?s grandparents and great grandparents.Additionally, REENTRANCEkcould only choose kamong its siblings.
These constraints greatly reducethe search space, thus speeding up the parser.4 Learning4.1 Learning AlgorithmAs stated in section 3.2, the parameter of our modelis weight vector ~?
in the score function.
To train theweight vector, we employ the averaged perceptronlearning algorithm (Collins, 2002).Algorithm 2 Learning algorithmInput: sentence w = w0.
.
.
wn, Dw, GwOutput: ~?1: s?
s0(Dw, w)2: while s /?
Stdo3: T ?
all possible actions according to s4: bestT ?
arg maxt?Tscore(t, s)5: goldT ?
oracle(s,Gw)6: if bestT 6= goldT then7: ~?
?
~?
?
?
(bestT, s) + ?
(goldT, s)8: end if9: s?
apply goldT to s10: end whileFor each sentence w and its corresponding AMRannotation GAMRin the training corpus, we couldget the dependency tree Dwof w with a dependencyparser.
Then we represent GAMRas span graphGw, which serves as our learning target.
The learn-ing algorithm takes the training instances (w, Dw,Gw), parses Dwaccording to Algorithm 1, and getthe best action using current weight vector ~?.
The371gold action for current state s is given by consultingspan graph Gw, which we formulate as a functionoracle() (line 5).
If the gold action is equal to thebest action we get from the parser, then the best ac-tion is applied to current state; otherwise, we updatethe weight vector (line 6-7) and continue the parsingprocedure by applying the gold action.4.2 Feature ExtractionSingle node features?
?0.w, ?
?0.lem, ?
?0.ne, ?
?0.t, ?
?0.dl, ??0.len??0.w,??0.lem,??0.ne,??0.t,??0.dl,??0.len?k.w,?k.lem,?k.ne,?k.t,?k.dl,?k.len?
?0p.w, ?
?0p.lem, ?
?0p.ne, ?
?0p.t, ?
?0p.dlNode pair features??0.lem+?
?0.t, ??0.lem+??0.dl??0.t+?
?0.lem, ?
?0.dl +??0.lem??0.ne+??0.ne,?k.ne+??0.ne?k.t+?
?0.lem,?k.dl +?
?0.lemPath features??0.lem+?
?0.lem+ path?0,?0?k.lem+?
?0.lem+ pathk,?0Distance featuresdist?0,?0distk,?0dist?0,?0+ path?0,?0dist?0,?0+ pathk,?0Action specific features??0.lem+??0.nswp?
?0.rephTable 2: Features used in our parser.
??0,?
?0,?k, ?
?0prepresents elements in feature context of nodes?0, ?0, k, ?0p, separately.
Each atomic feature isrepresented as follows: w - word; lem - lemma; ne -name entity; t - POS-tag; dl - dependency label; len- length of the node?s span.For transition-based dependency parsers, the fea-ture context for a parsing state is represented by theneighboring elements of a word token in the stackcontaining the partial parse or the buffer containingunprocessed word tokens.
In contrast, in our tree-to graph parser, as already stated, buffers ?
and ?only specify which arc or node is to be examinednext.
The feature context associated with current arcor node is mainly extracted from the partial graphG.
As a result, the feature context is different forthe different types of actions, a property that makesour parser very different from a standard transition-based dependency parser.
For example, when evalu-ating action SWAP we may be interested in featuresabout individual nodes ?0and ?0as well as featuresinvolving the arc (?0, ?0).
In contrast, when evaluat-ing action REATTACHk, we want to extract not onlyfeatures involving ?0and ?0, but also informationabout the reattached node k. To address this prob-lem, we define the feature context as ???0,?
?0,?k, ?
?0p?,where each element x?
consists of its atomic featuresof node x and ?0pdenotes the immediate parent ofnode ?0.
For elements in feature context that are notapplicable to the candidate action, we just set the el-ement to NONE and only extract features which arevalid for the candidate action.
The list of features weuse is shown in Table 2.Single node features are atomic features concern-ing all the possible nodes involved in each candi-date state-action pair.
We also include path featuresand distance features as described in (Flanigan et al,2014).
A path feature pathx,yis represented as thedependency labels and parts of speech on the pathbetween nodes x and y in the partial graph.
Herewe combine it with the lemma of the starting andending nodes.
Distance feature distx,yis the num-ber of tokens between two node x, y?s spans in thesentence.
Action-specific features record the his-tory of actions applied to a given node.
For exam-ple,?
?0.nswp records how many times node ?0hasbeen swapped up.
We combine this feature with thelemma of node ?0to prevent the parser from swap-ping a node too many times.?
?0.reph records theword feature of nodes that have been replaced withnode ?0.
This feature is helpful in predicting rela-tion labels.
As we have discussed above, in an AMRgraph, some function words are deleted as nodes butthey are crucial in determining the relation label be-tween its child and parent.5 Experiments5.1 Experiment SettingOur experiments are conducted on thenewswire section of AMR Annotation Cor-pus (LDC2013E117) (Banarescu et al, 2013).372We follow Flanigan et al (2014) in setting up thetrain/development/test splits1for easy comparison:4.0k sentences with document years 1995-2006as the training set; 2.1k sentences with documentyear 2007 as the development set; 2.1k sentenceswith document year 2008 as the test set, and onlyusing AMRs that are tagged ::preferred.Each sentence w is preprocessed with the StanfordCoreNLP toolkit (Manning et al, 2014) to get part-of-speech tags, name entity information, and basicdependencies.
We have verified that there is nooverlap between the training data for the StanfordCoreNLP toolkit2and the AMR Annotation Corpus.We evaluate our parser with the Smatch tool (Caiand Knight, 2013), which seeks to maximize thesemantic overlap between two AMR annotations.5.2 Action Set ValidationOne question about the transition system we pre-sented above is whether the action set defined herecan cover all the situations involving a dependency-to-AMR transformation.
Although a formal theo-retical proof is beyond the scope of this paper, wecan empirically verify that the action set works wellin practice.
To validate the actions, we first run theoracle() function for each sentencew and its depen-dency tree Dwto get the ?pseudo-gold?
G?w.
Thenwe compare G?wwith the gold-standard AMR graphrepresented as span graph Gwto see how similarthey are.
On the training data we got an overall 99%F-score for all ?G?w, Gw?
pairs, which indicates thatour action set is capable of transforming each sen-tence w and its dependency tree Dwinto its gold-standard AMR graph through a sequence of actions.5.3 ResultsTable 3 gives the precision, recall and F-score of ourparser given by Smatch on the test set.
Our parserachieves an F-score of 63% (Row 3) and the resultis 5% better than the first published result reportedin (Flanigan et al, 2014) with the same training andtest set (Row 2).
We also conducted experiments onthe test set by replacing the parsed graph with gold1A script to create the train/dev/test partitions is available atthe following URL: http://goo.gl/vA32iI2Specifically we used CoreNLP toolkit v3.3.1 and parsermodel wsjPCFG.ser.gz trained on the WSJ treebank sections02-21.relation labels or/and gold concept labels.
We cansee in Table 3 that when provided with gold conceptand relation labels as input, the parsing accuracy im-proves around 8% F-score (Row 6).
Rows 4 and 5present results when the parser is provided with justthe gold relation labels (Row 4) or gold concept la-bels (Row 5), and the results are expectedly lowerthan if both gold concept and relation labels are pro-vided as input.Precision Recall F-scoreJAMR .52 .66 .58Our parser .64 .62 .63Our parser +lgr.68 .65 .67Our parser +lgc.69 .67 .68Our parser +lgrc.72 .70 .71Table 3: Results on the test set.
Here, lgc- goldconcept label; lgr- gold relation label; lgrc- goldconcept label and gold relation label.5.4 Error AnalysisFigure 9: Confusion Matrix for actions ?tg, t?.
Ver-tical direction goes over the correct action type, andhorizontal direction goes over the parsed action type.Wrong alignments between the word tokens in thesentence and the concepts in the AMR graph ac-count for a significant proportion of our AMR pars-ing errors, but here we focus on errors in the tran-sition from the dependency tree to the AMR graph.Since in our parsing model, the parsing process hasbeen decomposed into a sequence of actions ap-plied to the input dependency tree, we can use theoracle() function during parsing to give us the cor-373rect action tgto take for a given state s. A compar-ison between tgand the best action t actually takenby our parser will give us a sense about how accu-rately each type of action is applied.
When we com-pare the actions, we focus on the structural aspect ofAMR parsing and only take into account the eightaction types, ignoring the concept and edge labels at-tached to them.
For example, NEXT-EDGE-ARG0and NEXT-EDGE-ARG1 would be considered to bethe same action and counted as a match when wecompute the errors even though the labels attachedto them are different.Figure 9 shows the confusion matrix that presentsa comparison between the parser-predicted actionsand the correct actions given by oracle() func-tion.
It shows that the NEXT-EDGE (ned), NEXT-NODE (nnd), and DELETENODE (dnd) actions ac-count for a large proportion of the actions.
Theseactions are also more accurately applied.
As ex-pected, the parser makes more mistakes involvingthe REATTACH (reat), REENTRANCE (reen) andSWAP (sw) actions.
The REATTACH action is of-ten used to correct PP-attachment errors made by thedependency parser or readjust the structure result-ing from the SWAP action, and it is hard to learngiven the relatively small AMR training set.
TheSWAP action is often tied to coordination structuresin which the head in the dependency structure andthe AMR graph diverges.
In the Stanford depen-dency representation which is the input to our parser,the head of a coordination structure is one of theconjuncts.
For AMR, the head is an abstract con-cept signaled by one of the coordinating conjunc-tions.
This also turns out to be one of the more dif-ficult actions to learn.
We expect, however, as theAMR Annotation Corpus grows bigger, the parsingmodel trained on a larger training set will learn theseactions better.6 Related WorkOur work is directly comparable to JAMR (Flaniganet al, 2014), the first published AMR parser.
JAMRperforms AMR parsing in two stages: concept iden-tification and relation identification.
They treat con-cept identification as a sequence labeling task andutilize a semi-Markov model to map spans of wordsin a sentence to concept graph fragments.
For rela-tion identification, they adopt the graph-based tech-niques for non-projective dependency parsing.
In-stead of finding maximum-scoring trees over words,they propose an algorithm to find the maximumspanning connected subgraph (MSCG) over conceptfragments obtained from the first stage.
In con-trast, we adopt a transition-based approach that findsits root in transition-based dependency parsing (Ya-mada and Matsumoto, 2003; Nivre, 2003; Sagaeand Tsujii, 2008), where a series of actions are per-formed to transform a sentence to a dependency tree.As should be clear from our description, however,the actions in our parser are very different in na-ture from the actions used in transition-based depen-dency parsing.There is also another line of research that attemptsto design graph grammars such as hyperedge re-placement grammar (HRG) (Chiang et al, 2013) andefficient graph-based algorithms for AMR parsing.Existing work along this line is still theoretical innature and no empirical results have been reportedyet.7 Conclusion and Future WorkWe presented a novel transition-based parsing algo-rithm that takes the dependency tree of a sentenceas input and transforms it into an Abstract Mean-ing Representation graph through a sequence of ac-tions.
We show that our approach is linguisticallyintuitive and our experimental results also show thatour parser outperformed the previous best reportedresults by a significant margin.
In future work weplan to continue to perfect our parser via improvedlearning and decoding techniques.AcknowledgmentsWe want to thank the anonymous reviewers for theirsuggestions.
We also want to thank Jeffrey Flanigan,Xiaochang Peng, Adam Lopez and Giorgio Sattafor discussion about ideas related to this work dur-ing the Fred Jelinek Memorial Workshop in Praguein 2014.
This work was partially supported by theNational Science Foundation via Grant No.0910532entitled Richer Representations for Machine Trans-lation.
All views expressed in this paper are thoseof the authors and do not necessarily represent theview of the National Science Foundation.374ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider, 2013.
Abstract Meaning Representationfor Sembanking.
In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 178?186.
Association for Computa-tional Linguistics.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proceed-ings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 1455?1465.Association for Computational Linguistics.Shu Cai and Kevin Knight.
2013.
Smatch: an evaluationmetric for semantic feature structures.
In Proceedingsof the 51st Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers), pages748?752.
Association for Computational Linguistics.David Chiang, Jacob Andreas, Daniel Bauer, Karl MoritzHermann, Bevan Jones, and Kevin Knight.
2013.Parsing graphs with hyperedge replacement gram-mars.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 924?932, Sofia, Bulgaria,August.
Association for Computational Linguistics.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 111.
Association forComputational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe ACL-02 conference on Empirical methods in natu-ral language processing-Volume 10, pages 1?8.
Asso-ciation for Computational Linguistics.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, ChrisDyer, and Noah A. Smith.
2014.
A discriminativegraph-based parser for the abstract meaning represen-tation.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1426?1436, Baltimore,Maryland, June.
Association for Computational Lin-guistics.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT).
Citeseer.Joakim Nivre.
2007.
Incremental non-projective de-pendency parsing.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 396?403.Association for Computational Linguistics.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Joakim Nivre.
2009.
Non-projective dependency parsingin expected linear time.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1-Volume1, pages 351?359.
Association for Computational Lin-guistics.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reduce de-pendency dag parsing.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics-Volume 1, pages 753?760.
Association for Computa-tional Linguistics.Francesco Sartorio, Giorgio Satta, and Joakim Nivre.2013.
A transition-based dependency parser using adynamic parsing strategy.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 135?144.Association for Computational Linguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, volume 3.375
