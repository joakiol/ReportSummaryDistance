Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181?1191,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCompositional Sequence Labeling Models for Error Detectionin Learner WritingMarek ReiThe ALTA InstituteComputer LaboratoryUniversity of CambridgeUnited Kingdommarek.rei@cl.cam.ac.ukHelen YannakoudakisThe ALTA InstituteComputer LaboratoryUniversity of CambridgeUnited Kingdomhelen.yannakoudakis@cl.cam.ac.ukAbstractIn this paper, we present the first exper-iments using neural network models forthe task of error detection in learner writ-ing.
We perform a systematic comparisonof alternative compositional architecturesand propose a framework for error detec-tion based on bidirectional LSTMs.
Ex-periments on the CoNLL-14 shared taskdataset show the model is able to outper-form other participants on detecting er-rors in learner writing.
Finally, the modelis integrated with a publicly deployedself-assessment system, leading to perfor-mance comparable to human annotators.1 IntroductionAutomated systems for detecting errors in learnerwriting are valuable tools for second languagelearning and assessment.
Most work in recentyears has focussed on error correction, with er-ror detection performance measured as a byprod-uct of the correction output (Ng et al, 2013; Nget al, 2014).
However, this assumes that systemsare able to propose a correction for every detectederror, and accurate systems for correction mightnot be optimal for detection.
While closed-classerrors such as incorrect prepositions and determin-ers can be modeled with a supervised classificationapproach, content-content word errors are the 3rdmost frequent error type and pose a serious chal-lenge to error correction frameworks (Leacock etal., 2014; Kochmar and Briscoe, 2014).
Eval-uation of error correction is also highly subjec-tive and human annotators have rather low agree-ment on gold-standard corrections (Bryant andNg, 2015).
Therefore, we treat error detection inlearner writing as an independent task and proposea system for labeling each token as being corrector incorrect in context.Common approaches to similar sequence label-ing tasks involve learning weights or probabilitiesfor context n-grams of varying sizes, or relying onpreviously extracted high-confidence context pat-terns.
Both of these methods can suffer from datasparsity, as they treat words as independent unitsand miss out on potentially related patterns.
In ad-dition, they need to specify a fixed context size andare therefore often limited to using a small windownear the target.Neural network models aim to address theseweaknesses and have achieved success in variousNLP tasks such as language modeling (Bengioet al, 2003) and speech recognition (Dahl et al,2012).
Recent developments in machine transla-tion have also shown that text of varying lengthcan be represented as a fixed-size vector usingconvolutional networks (Kalchbrenner and Blun-som, 2013; Cho et al, 2014a) or recurrent neu-ral networks (Cho et al, 2014b; Bahdanau et al,2015).In this paper, we present the first experimentsusing neural network models for the task of er-ror detection in learner writing.
We performa systematic comparison of alternative composi-tional structures for constructing informative con-text representations.
Based on the findings, wepropose a novel framework for performing er-ror detection in learner writing, which achievesstate-of-the-art results on two datasets of error-annotated learner essays.
The sequence labelingmodel creates a single variable-size network overthe whole sentence, conditions each label on allthe words, and predicts all labels together.
Theeffects of different datasets on the overall perfor-mance are investigated by incrementally provid-ing additional training data to the model.
Finally,we integrate the error detection framework with apublicly deployed self-assessment system, leading1181to performance comparable to human annotators.2 Background and Related WorkThe field of automatically detecting errors inlearner text has a long and rich history.
Most workhas focussed on tackling specific types of errors,such as usage of incorrect prepositions (Tetreaultand Chodorow, 2008; Chodorow et al, 2007), ar-ticles (Han et al, 2004; Han et al, 2006), verbforms (Lee and Seneff, 2008), and adjective-nounpairs (Kochmar and Briscoe, 2014).However, there has been limited work on moregeneral error detection systems that could handleall types of errors in learner text.
Chodorow andLeacock (2000) proposed a method based on mu-tual information and the chi-square statistic to de-tect sequences of part-of-speech tags and func-tion words that are likely to be ungrammaticalin English.
Gamon (2011) used Maximum En-tropy Markov Models with a range of features,such as POS tags, string features, and outputsfrom a constituency parser.
The pilot Helping OurOwn shared task (Dale and Kilgarriff, 2011) alsoevaluated grammatical error detection of a num-ber of different error types, though most systemswere error-type specific and the best approach washeavily skewed towards article and preposition er-rors (Rozovskaya et al, 2011).
We extend thisline of research, working towards general error de-tection systems, and investigate the use of neuralcompositional models on this task.The related area of grammatical error correctionhas also gained considerable momentum in thepast years, with four recent shared tasks highlight-ing several emerging directions (Dale and Kilgar-riff, 2011; Dale et al, 2012; Ng et al, 2013; Ng etal., 2014).
The current state-of-the-art approachescan broadly be separated into two categories:1.
Phrase-based statistical machine translationtechniques, essentially translating the incor-rect source text into the corrected version(Felice et al, 2014; Junczys-Dowmunt andGrundkiewicz, 2014)2.
Averaged Perceptrons and Naive Bayes clas-sifiers making use of native-language errorcorrection priors (Rozovskaya et al, 2014;Rozovskaya et al, 2013).Error correction systems require very specialisedmodels, as they need to generate an improved ver-sion of the input text, whereas a wider range oftagging and classification models can be deployedon error detection.
In addition, automated writingfeedback systems that indicate the presence andlocation of errors may be better from a pedagogicpoint of view, rather than providing a panacea andcorrecting all errors in learner text.
In Section 7we evaluate a neural sequence tagging model onthe latest shared task test data, and compare it tothe top participating systems on the task of errordetection.3 Sequence Labeling ArchitecturesWe construct a neural network sequence labelingframework for the task of error detection in learnerwriting.
The model receives only a series of tokensas input, and outputs the probability of each tokenin the sentence being correct or incorrect in a givencontext.
The architectures start with the vectorrepresentations of individual words, [x1, ..., xT],where T is the length of the sentence.
Differentcomposition functions are then used to calculate ahidden vector representation of each token in con-text, [h1, ..., hT].
These representations are passedthrough a softmax layer, producing a probabilitydistribution over the possible labels for every to-ken in context:pt= softmax(Woht) (1)whereWois the weight matrix between the hiddenvector htand the output layer.We investigate six alternative neural network ar-chitectures for the task of error detection: con-volutional, bidirectional recurrent, bidirectionalLSTM, and multi-layer variants of each of them.In the convolutional neural network (CNN, Fig-ure 1a) for token labeling, the hidden vector htis calculated based on a fixed-size context win-dow.
The convolution acts as a feedforward net-work, using surrounding context words as input,and therefore it will learn to detect the presence ofdifferent types of n-grams.
The assumption behindthe convolutional architecture is that memorisingerroneous token sequences from the training datais sufficient for performing error detection.The convolution uses dwtokens on either sideof the target token, and the vectors for these tokensare concatenated, preserving the ordering:ct= xt?dw: ... : xt+dw(2)where x1: x2is used as notation for vector con-catenation of x1and x2.
The combined vector is1182Figure 1: Alternative neural composition architectures for error detection.
a) Convolutional networkb) Deep convolutional network c) Recurrent bidirectional network d) Deep recurrent bidirectional net-work.
The bottom layers are embeddings for individual tokens.
The middle layers are context-dependentrepresentations, built using different composition functions.
The top layers are softmax output layers,predicting a label distribution for every input token.then passed through a non-linear layer to producethe hidden representation:ht= tanh(Wcct) (3)The deep convolutional network (Figure 1b)adds an extra convolutional layer to the architec-ture, using the first layer as input.
It creates con-volutions of convolutions, thereby capturing morecomplex higher-order features from the dataset.In a recurrent neural network (RNN), each hid-den representation is calculated based on the cur-rent token embedding and the hidden vector at theprevious time step:ht= f(Wxt+ V ht?1) (4)where f(z) is a nonlinear function, such as thesigmoid function.
Instead of a fixed context win-dow, information is passed through the sentenceusing a recursive function and the network is ableto learn which patterns to disregard or pass for-ward.
This recurrent network structure is referredto as an Elman-type network, after Elman (1990).The bidirectional RNN (Figure 1c) consists oftwo recurrent components, moving in opposite di-rections through the sentence.
While the unidirec-tional version takes into account only context onthe left of the target token, the bidirectional ver-sion recursively builds separate context represen-tations from either side of the target token.
The leftand right context are then concatenated and usedas the hidden representation:h?t= f(Wrxt+ Vrh?t?1) (5)h?t= f(Wlxt+ Vlh?t+1) (6)ht= h?t: h?t(7)Recurrent networks have been shown to per-form well on the task of language modeling(Mikolov et al, 2011; Chelba et al, 2013), wherethey learn an incremental composition functionfor predicting the next token in the sequence.However, while language models can estimatethe probability of each token, they are unable todifferentiate between infrequent and incorrect to-ken sequences.
For error detection, the compo-sition function needs to learn to identify seman-tic anomalies or ungrammatical combinations, in-dependent of their frequency.
The bidirectionalmodel provides extra information, as it allows thenetwork to use context on both sides of the targettoken.Irsoy and Cardie (2014) created an extensionof this architecture by connecting together mul-tiple layers of bidirectional Elman-type recurrentnetwork modules.
This deep bidirectional RNN(Figure 1d) calculates a context-dependent rep-resentation for each token using a bidirectionalRNN, and then uses this as input to another bidi-rectional RNN.
The multi-layer structure allowsthe model to learn more complex higher-level fea-tures and effectively perform multiple recurrentpasses through the sentence.The long-short term memory (LSTM) (Hochre-iter and Schmidhuber, 1997) is an advanced al-ternative to the Elman-type networks that hasrecently become increasingly popular.
It uses1183two separate hidden vectors to pass informationbetween different time steps, and includes gat-ing mechanisms for modulating its own output.LSTMs have been successfully applied to var-ious tasks, such as speech recognition (Graveset al, 2013), machine translation (Luong et al,2015), and natural language generation (Wen etal., 2015).Two sets of gating values (referred to as the in-put and forget gates) are first calculated based onthe previous states of the network:it= ?
(Wixt+ Uiht?1+ Vfct?1+ bi) (8)ft= ?
(Wfxt+ Ufht?1+ Vfct?1+ bf) (9)where xtis the current input, ht?1is the previoushidden state, biand bfare biases, ct?1is the pre-vious internal state (referred to as the cell), and ?is the logistic function.
The new internal state iscalculated based on the current input and the pre-vious hidden state, and then interpolated with theprevious internal state using ftand itas weights:?ct= tanh(Wcxt+ Ucht?1+ bc) (10)ct= ftct?1+ it?ct(11)where  is element-wise multiplication.
Finally,the hidden state is calculated by passing the inter-nal state through a tanh nonlinearity, and weight-ing it with ot.
The values of otare conditioned onthe new internal state (ct), as opposed to the previ-ous one (ct?1):ot= ?
(Woxt+ Uoht?1+ Voct+ bo) (12)ht= ottanh(ct) (13)Because of the linear combination in equation(11), the LSTM is less susceptible to vanishinggradients over time, thereby being able to makeuse of longer context when making predictions.
Inaddition, the network learns to modulate itself, ef-fectively using the gates to predict which operationis required at each time step, thereby incorporatinghigher-level features.In order to use this architecture for error de-tection, we create a bidirectional LSTM, mak-ing use of the advanced features of LSTM and in-corporating context on both sides of the target to-ken.
In addition, we experiment with a deep bidi-rectional LSTM, which includes two consecu-tive layers of bidirectional LSTMs, modeling evenmore complex features and performing multiplepasses through the sentence.For comparison with non-neural models, wealso report results using CRFs (Lafferty et al,2001), which are a popular choice for sequencelabeling tasks.
We trained the CRF++1imple-mentation on the same dataset, using as featuresunigrams, bigrams and trigrams in a 7-word win-dow surrouding the target word (3 words beforeand after).
The predicted label is also conditionedon the previous label in the sequence.4 ExperimentsWe evaluate the alternative network structureson the publicly released First Certificate in En-glish dataset (FCE-public, Yannakoudakis et al(2011)).
The dataset contains short texts, writ-ten by learners of English as an additional lan-guage in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level.
The texts havebeen manually error-annotated using a taxonomyof 77 error types.
We use the released test setfor evaluation, containing 2,720 sentences, leaving30,953 sentences for training.
We further separate2,222 sentences from the training set for develop-ment and hyper-parameter tuning.The dataset contains manually annotated errorspans of various types of errors, together with theirsuggested corrections.
We convert this to a token-level error detection task by labeling each tokeninside the error span as being incorrect.
In order tocapture errors involving missing words, the errorlabel is assigned to the token immediately after theincorrect gap ?
this is motivated by the intuitionthat while this token is correct when considered inisolation, it is incorrect in the current context, asanother token should have preceeded it.As the main evaluation measure for error de-tection we use F0.5, which was also the measureadopted in the CoNLL-14 shared task on error cor-rection (Ng et al, 2014).
It combines both pre-cision and recall, while assigning twice as muchweight to precision, since accurate feedback isoften more important than coverage in error de-tection applications (Nagata and Nakatani, 2010).Following Chodorow et al (2012), we also reportraw counts for predicted and correct tokens.
Re-lated evaluation measures, such as the M2-scorer(Ng et al, 2014) and the I-measure (Felice and1https://taku910.github.io/crfpp/1184Development TestP R F0.5predicted correct P R F0.5CRF 62.2 13.6 36.3 914 516 56.5 8.2 25.9CNN 52.4 24.9 42.9 3518 1620 46.0 25.7 39.8Deep CNN 48.4 26.2 41.4 3992 1651 41.4 26.2 37.1Bi-RNN 63.9 18.0 42.3 2333 1196 51.3 19.0 38.2Deep Bi-RNN 60.3 17.6 40.6 2543 1255 49.4 19.9 38.1Bi-LSTM 54.5 28.2 46.0 3898 1798 46.1 28.5 41.1Deep Bi-LSTM 56.7 21.3 42.5 2822 1359 48.2 21.6 38.6Table 1: Performance of the CRF and alternative neural network structures on the public FCE dataset fortoken-level error detection in learner writing.Briscoe, 2015), require the system to propose acorrection and are therefore not directly applica-ble on the task of error detection.During the experiments, the input text was low-ercased and all tokens that occurred less than twicein the training data were represented as a singleunk token.
Word embeddings were set to size300 and initialised using the publicly released pre-trained Word2Vec vectors (Mikolov et al, 2013).The convolutional networks use window size 3on either side of the target token and produce a300-dimensional context-dependent vector.
Therecurrent networks use hidden layers of size 200in either direction.
We also added an extra hid-den layer of size 50 between each of the compo-sition functions and the output layer ?
this allowsthe network to learn a separate non-linear trans-formation and reduces the dimensionality of thecompositional vectors.
The parameters were opti-mised using gradient descent with initial learningrate 0.001, the ADAM algorithm (Kingma and Ba,2015) for dynamically adapting the learning rate,and batch size of 64 sentences.
F0.5on the devel-opment set was evaluated at each epoch, and thebest model was used for final evaluations.5 ResultsTable 1 contains results for experiments compar-ing different composition architectures on the taskof error detection.
The CRF has the lowest F0.5score compared to any of the neural models.
Itmemorises frequent error sequences with high pre-cision, but does not generalise sufficiently, result-ing in low recall.
The ability to condition on theprevious label also does not provide much help onthis task ?
there are only two possible labels andthe errors are relatively sparse.The architecture using convolutional networksperforms well and achieves the second-highest re-sult on the test set.
It is designed to detect errorpatterns from a fixed window of 7 words, whichis large enough to not require the use of moreadvanced composition functions.
In contrast, theperformance of the bidirectional recurrent network(Bi-RNN) is somewhat lower, especially on thetest set.
In Elman-type recurrent networks, thecontext signal from distant words decreases fairlyrapidly due to the sigmoid activation function anddiminishing gradients.
This is likely why the Bi-RNN achieves the highest precision of all sys-tems ?
the predicted label is mostly influencedby the target token and its immediate neighbours,allowing the network to only detect short high-confidence error patterns.
The convolutional net-work, which uses 7 context words with equal at-tention, is able to outperform the Bi-RNN despitethe fixed-size context window.The best overall result and highest F0.5isachieved by the bidirectional LSTM compositionmodel (Bi-LSTM).
This architecture makes use ofthe full sentence for building context vectors onboth sides of the target token, but improves on Bi-RNN by utilising a more advanced compositionfunction.
Through the application of a linear up-date for the internal cell representation, the LSTMis able to capture dependencies over longer dis-tances.
In addition, the gating functions allow it toadaptively decide which information to include inthe hidden representations or output for error de-tection.We found that using multiple layers of compo-sitional functions in a deeper network gave com-parable or slightly lower results for all the com-position architectures.
This is in contrast to Ir-1185Training data Dev F0.5Test F0.5FCE-public 46.0 41.1+NUCLE 39.0 41.0+IELTS 45.6 50.7+FCE 57.2 61.1+CPE 59.0 62.1+CAE 60.7 64.3Table 2: Results on the public FCE test set whenincrementally providing more training data to theerror detection model.soy and Cardie (2014), who experimented withElman-type networks and found some improve-ments using multiple layers of Bi-RNNs.
The dif-ferences can be explained by their task benefitingfrom alternative features: the evaluation was per-formed on opinion mining where most target se-quences are longer phrases that need to be identi-fied based on their semantics, whereas many errorsin learner writing are short and can only be iden-tified by a contextual mismatch.
In addition, ournetworks contain an extra hidden layer before theoutput, which allows the models to learn higher-level representations without adding complexitythrough an extra compositional layer.6 Additional Training DataThere are essentially infinitely many ways of com-mitting errors in text and introducing additionaltraining data should alleviate some of the prob-lems with data sparsity.
We experimented with in-crementally adding different error-tagged corporainto the training set and measured the resultingperformance.
This allows us to provide some con-text to the results obtained by using each of thedatasets, and gives us an estimate of how muchannotated data is required for optimal performanceon error detection.
The datasets we consider are asfollows:?
FCE-public ?
the publicly released subset ofFCE (Yannakoudakis et al, 2011), as de-scribed in Section 4.?
NUCLE ?
the NUS Corpus of Learner En-glish (Dahlmeier et al, 2013), used as themain training set for CoNLL shared tasks onerror correction.?
IELTS ?
a subset of the IELTS examina-tion dataset extracted from the CambridgeLearner Corpus (CLC, Nicholls (2003)), con-taining 68,505 sentences from all proficiencylevels, also used by Felice et al (2014).?
FCE ?
a larger selection of FCE texts fromthe CLC, containing 323,192 sentences.?
CPE ?
essays from the proficient examinationlevel in the CLC, containing 210,678 sen-tences.?
CAE ?
essays from the advanced examina-tion level in the CLC, containing 219,953sentences.Table 2 contains results obtained by incremen-tally adding training data to the Bi-LSTM model.We found that incorporating the NUCLE datasetdoes not improve performance over using only theFCE-public dataset, which is likely due to the twocorpora containing texts with different domainsand writing styles.
The texts in FCE are writ-ten by young intermediate students, in responseto prompts eliciting letters, emails and reviews,whereas NUCLE contains mostly argumentativeessays written by advanced adult learners.
Thedifferences in the datasets offset the benefits fromadditional training data, and the performance re-mains roughly the same.Figure 2: F0.5measure on the public FCE test set,as a function of the total number of tokens in thetraining set.In contrast, substantial improvements are ob-tained when introducing the IELTS and FCEdatasets, with each of them increasing the F0.5score by roughly 10%.
The IELTS dataset con-tains essays from all proficiency levels, and FCEfrom mid-level English learners, which providesthe model with a distribution of ?average?
errors tolearn from.
Adding even more training data from1186Annotation 1 Annotation 2predicted correct P R F0.5correct P R F0.5Annotator 1 2992 - - - - 1800 60.2 42.9 55.7Annotator 2 4199 1800 42.9 60.2 45.5 - - - -CAMB 2170 731 33.7 24.4 31.3 1052 48.5 25.1 40.8CUUI 1582 550 34.8 18.4 29.5 755 47.7 18.0 35.9AMU 1260 479 38.0 16.0 29.8 643 51.0 15.3 34.8P1+P2+S1+S2 887 388 43.7 13.0 29.7 535 60.3 12.7 34.5Bi-LSTM (FCE-public) 4449 683 15.4 22.8 16.4 1052 23.6 25.1 23.9Bi-LSTM (full) 1540 627 40.7 21.0 34.3 911 59.2 21.7 44.0Table 3: Error detection results on the two official annotations for the CoNLL-14 shared task test dataset.high-proficiency essays in CPE and CAE only pro-vides minor further improvements.Figure 2 also shows F0.5on the FCE-public testset as a function of the total number of tokens inthe training data.
The optimal trade-off betweenperformance and data size is obtained at around 8million tokens, after introducing the FCE dataset.7 CoNLL-14 Shared TaskThe CoNLL-14 shared task (Ng et al, 2014)focussed on automatically correcting errors inlearner writing.
The NUCLE dataset was pro-vided as the main training dataset, but participantswere allowed to include other annotated corporaand external resources.
For evaluation, 25 stu-dents were recruited to each write two new essays,which were then annotated by two experts.We used the same methods from Section 4 forconverting the shared task annotation to a token-level labeling task in order to evaluate the mod-els on error detection.
In addition, the correctionoutputs of all the participating systems were madeavailable online, therefore we are able to reporttheir performance on this task.
In order to con-vert their output to error detection labels, the cor-rected sentences were aligned with the original in-put using Levenshtein distance, and any changesproposed by the system resulted in the correspond-ing source words being labeled as errors.The results on the two annotations of the sharedtask test data can be seen in Table 3.
We first eval-uated each of the human annotators with respect tothe other, in order to estimate the upper bound onthis task.
The average F0.5of roughly 50% showsthat the task is difficult and even human expertshave a rather low agreement.
It has been shownbefore that correcting grammatical errors is highlysubjective (Bryant and Ng, 2015), but these resultsindicate that trained annotators can disagree evenon the number and location of errors.In the same table, we provide error detection re-sults for the top 3 participants in the shared task:CAMB (Felice et al, 2014), CUUI (Rozovskayaet al, 2014), and AMU (Junczys-Dowmunt andGrundkiewicz, 2014).
They each preserve theirrelative ranking also in the error detection evalu-ation.
The CAMB system has a lower precisionbut the highest recall, also resulting in the highestF0.5.
CUUI and AMU are close in performance,with AMU having slightly higher precision.After the official shared task, Susanto et al(2014) published a system which combines severalalternative models and outperforms the shared taskparticipants when evaluated on error correction.However, on error detection it receives lower re-sults, ranking 3rd and 4th when evaluated on F0.5(P1+P2+S1+S2 in Table 3).
The system has de-tected a small number of errors with high preci-sion, and does not reach the highest F0.5.Finally, we present results for the Bi-LSTM se-quence labeling system for error detection.
Usingonly FCE-public for training, the overall perfor-mance is rather low as the training set is very smalland contains texts from a different domain.
How-ever, these results show that the model behaves asexpected ?
since it has not seen similar languageduring training, it labels a very large portion of to-kens as errors.
This indicates that the network istrying to learn correct language constructions fromthe limited data and classifies unseen structures aserrors, as opposed to simply memorising error se-quences from the training data.1187When trained on all the datasets from Section6, the model achieves the highest F0.5of all sys-tems on both of the CoNLL-14 shared task testannotations, with an absolute improvement of 3%over the previous best result.
It is worth notingthat the full Bi-LSTM has been trained on moredata than the other CoNLL contestants.
However,as the shared task systems were not restricted tothe NUCLE training set, all the submissions alsoused differing amounts of training data from vari-ous sources.
In addition, the CoNLL systems aremostly combinations of many alternative models:the CAMB system is a hybrid of machine transla-tion, a rule-based system, and a language modelre-ranker; CUUI consists of different classifiersfor each individual error type; and P1+P2+S1+S2is a combination of four different error correctionsystems.
In contrast, the Bi-LSTM is a singlemodel for detecting all error types, and thereforerepresents a more scalable data-driven approach.8 Essay ScoringIn this section, we perform an extrinsic evalua-tion of the efficacy of the error detection systemand examine the extent to which it generalises athigher levels of granularity on the task of auto-mated essay scoring.
More specifically, we repli-cate experiments using the text-level model de-scribed by Andersen et al (2013), which is cur-rently deployed in a self-assessment and tutoringsystem (SAT), an online automated writing feed-back tool actively used by language learners.2The SAT system predicts an overall score fora given text, which provides a holistic assessmentof linguistic competence and language proficiency.The authors trained a supervised ranking percep-tron model on the FCE-public dataset, using fea-tures such as error-rate estimates from a languagemodel and various lexical and grammatical prop-erties of text (e.g., word n-grams, part-of-speechn-grams and phrase-structure rules).
We replicatethis experiment and add the average probabilityof each token in the essay being correct, accord-ing to the error detection model, as an additionalfeature for the scoring framework.
The systemwas then retrained on FCE-public and evaluatedon correctly predicting the assigned essay score.Table 4 presents the experimental results.The human performance on the test set is cal-2http://www.cambridgeenglish.org/learning-english/free-resources/write-and-improve/r ?Human annotators 79.6 79.2SAT 75.1 76.0SAT + Bi-LSTM (FCE-public) 76.0 77.0SAT + Bi-LSTM (full) 78.0 79.9Table 4: Pearson?s correlation r and Spearman?scorrelation ?
on the public FCE test set on the taskof automated essay scoring.culated as the average inter-annotator correlationon the same data, and the existing SAT system hasdemonstrated levels of performance that are veryclose to that of human assessors.
Nevertheless,the Bi-LSTM model trained only on FCE-publiccomplements the existing features, and the com-bined model achieves an absolute improvement ofaround 1% percent, corresponding to 20-31% rela-tive error reduction with respect to the human per-formance.
Even though the Bi-LSTM is trainedon the same dataset and the SAT system alreadyincludes various linguistic features for capturingerrors, our error detection model manages to fur-ther improve its performance.When the Bi-LSTM is trained on all the avail-able data from Section 6, the combination achievesfurther substantial improvements.
The relative er-ror reduction on Pearson?s correlation is 64%, andthe system actually outperforms human annotatorson Spearman?s correlation.9 ConclusionsIn this paper, we presented the first experimentsusing neural network models for the task of er-ror detection in learner writing.
Six alternativecompositional network architectures for modelingcontext were evaluated.
Based on the findings,we propose a novel error detection framework us-ing token-level embeddings, bidirectional LSTMsfor context representation, and a multi-layer archi-tecture for learning more complex features.
Thisstructure allows the model to classify each tokenas being correct or incorrect, using the full sen-tence as context.
The self-modulation architectureof LSTMs was also shown to be beneficial, as it al-lows the network to learn more advanced composi-tion rules and remember dependencies over longerdistances.Substantial performance improvements wereachieved by training the best model on additional1188datasets.
We found that the largest benefit was ob-tained from training on 8 million tokens of textfrom learners with varying levels of language pro-ficiency.
In contrast, including even more datafrom higher-proficiency learners gave marginalfurther improvements.
As part of future work, itwould be beneficial to investigate the effect of au-tomatically generated training data for error detec-tion (e.g., Rozovskaya and Roth (2010)).We evaluated the performance of existing errorcorrection systems from CoNLL-14 on the task oferror detection.
The experiments showed that suc-cess on error correction does not necessarily meansuccess on error detection, as the current best cor-rection system (P1+P2+S1+S2) is not the same asthe best shared task detection system (CAMB).
Inaddition, the neural sequence tagging model, spe-cialised for error detection, was able to outperformall other participating systems.Finally, we performed an extrinsic evaluation byincorporating probabilities from the error detec-tion system as features in an essay scoring model.Even without any additional data, the combina-tion further improved performance which is al-ready close to the results from human annotators.In addition, when the error detection model wastrained on a larger training set, the essay scorerwas able to exceed human-level performance.AcknowledgmentsWe would like to thank Prof Ted Briscoe and thereviewers for providing useful feedback.References?istein E. Andersen, Helen Yannakoudakis, FionaBarker, and Tim Parish.
2013.
Developing and test-ing a self-assessment and tutoring system.
Proceed-ings of the Eighth Workshop on Innovative Use ofNLP for Building Educational Applications.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural Machine Translation by JointlyLearning to Align and Translate.
In InternationalConference on Learning Representations.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A Neural Probabilistic Lan-guage Model Yoshua.
Journal of Machine LearningResearch, 3.Christopher Bryant and Hwee Tou Ng.
2015.
How Farare We from Fully Automatic High Quality Gram-matical Error Correction?
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing.Ciprian Chelba, Tom?a?s Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One Billion Word Benchmark for Mea-suring Progress in Statistical Language Modeling.In arXiv preprint.Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the Prop-erties of Neural Machine Translation: Encoder-Decoder Approaches.
In Eighth Workshop on Syn-tax, Semantics and Structure in Statistical Transla-tion.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014b.
Learn-ing Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.
InConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2014).Martin Chodorow and Claudia Leacock.
2000.
Anunsupervised method for detecting grammatical er-rors.
In Proceedings of the first conference on NorthAmerican chapter of the Association for Computa-tional Linguistics.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel Tetreault.
2012.
Problems in evaluating gram-matical error detection systems.
In COLING 2012.George.
E. Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-Dependent Pre-Trained Deep Neu-ral Networks for Large-Vocabulary Speech Recog-nition.
IEEE Transactions on Audio, Speech, andLanguage Processing, 20.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS corpus of learner English.
Pro-ceedings of the Eighth Workshop on Innovative Useof NLP for Building Educational Applications.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 Pilot Shared Task.
In Pro-ceedings of the 13th European Workshop on NaturalLanguage Generation.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the Preposition andDeterminer Error Correction Shared Task.
In TheSeventh Workshop on Building Educational Appli-cations Using NLP.Jeffrey L. Elman.
1990.
Finding structure in time.Cognitive science, 14(2).1189Mariano Felice and Ted Briscoe.
2015.
Towards astandard evaluation method for grammatical errordetection and correction.
In The 2015 Annual Con-ference of the North American Chapter of the ACL.Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
Conference on ComputationalNatural Language Learning: Shared Task (CoNLL-2014).Michael Gamon.
2011.
High-Order Sequence Model-ing for Language Learner Error Detection.
Proceed-ings of the Sixth Workshop on Innovative Use of NLPfor Building Educational Applications.Alex Graves, Navdeep Jaitly, and Abdel Rahman Mo-hamed.
2013.
Hybrid speech recognition withDeep Bidirectional LSTM.
In IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU 2013).Na-Rae Han, Martin Chodorow, and Claudia Leacock.2004.
Detecting Errors in English Article Usagewith a Maximum Entropy Classifier Trained on aLarge, Diverse Corpus.
Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
LongShort-term Memory.
Neural Computation, 9.Ozan Irsoy and Claire Cardie.
2014.
Opinion Miningwith Deep Recurrent Neural Networks.
In EMNLP-2014.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2014.
The AMU System in the CoNLL-2014Shared Task: Grammatical Error Correction byData-Intensive and Feature-Rich Statistical MachineTranslation.
Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task (CoNLL-2014).Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentContinuous Translation Models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2013).Diederik P. Kingma and Jimmy Lei Ba.
2015.
Adam:a Method for Stochastic Optimization.
In Interna-tional Conference on Learning Representations.Ekaterina Kochmar and Ted Briscoe.
2014.
De-tecting Learner Errors in the Choice of ContentWords Using Compositional Distributional Seman-tics.
In Proceedings of COLING 2014, the 25th In-ternational Conference on Computational Linguis-tics: Technical Papers.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning.
Citeseer.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel R. Tetreault.
2014.
Automated Grammati-cal Error Detection for Language Learners: SecondEdition.John Lee and Stephanie Seneff.
2008.
Correcting mis-use of verb forms.
In Proceedings of the 46th An-nual Meeting of the ACL.Mnih-Thang Luong, Hieu Pham, and Christopher D.Manning.
2015.
Effective Approaches to Attention-based Neural Machine Translation.
In Proceedingsof the 2015 Conference on Empirical Methods inNatural Language Processing.Tom?a?s Mikolov, Stefan Kombrink, Anoop Deo-ras, Luk?a?s Burget, and Jan?Cernock?y.
2011.RNNLM-Recurrent neural network language mod-eling toolkit.
In ASRU 2011 Demo Session.Tom?a?s Mikolov, Greg Corrado, Kai Chen, and JeffreyDean.
2013.
Efficient Estimation of Word Rep-resentations in Vector Space.
Proceedings of theInternational Conference on Learning Representa-tions (ICLR 2013).Ryo Nagata and Kazuhide Nakatani.
2010.
Evaluatingperformance of grammatical error detection to max-imize learning effect.
Coling 2010: Poster Volume.Hwee Tou Ng, Yuanbin Wu, and Christian Hadiwinoto.2013.
The CoNLL-2013 Shared Task on Grammat-ical Error Correction.
Computational Natural Lan-guage Learning (CoNLL), Shared Task.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon Grammatical Error Correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task.Diane Nicholls.
2003.
The Cambridge Learner Cor-pus - error coding and analysis for lexicography andELT.
Proceedings of the Corpus Linguistics 2003Conference.Alla Rozovskaya and Dan Roth.
2010.
TrainingParadigms for Correcting Errors in Grammar andUsage.
Human Language Technologies: The 2010Annual Conference of the North American Chapterof the Association for Computational Linguistics.Alla Rozovskaya, Mark Sammons, Joshua Gioja, andDan Roth.
2011.
University of Illinois System inHOO Text Correction Shared Task.
Proceedings ofthe 13th European Workshop on Natural LanguageGeneration (ENLG).1190Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,and Dan Roth.
2013.
The University of IllinoisSystem in the CoNLL-2013 Shared Task.
In Pro-ceedings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared Task.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,Dan Roth, and Nizar Habash.
2014.
The Illinois-Columbia System in the CoNLL-2014 Shared Task.Proceedings of the Eighteenth Conference on Com-putational Natural Language Learning: SharedTask (CoNLL-2014).Raymond Hendy Susanto, Peter Phandi, and Hwee TouNg.
2014.
System Combination for GrammaticalError Correction.
Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2014).Joel R. Tetreault and Martin Chodorow.
2008.
TheUps and Downs of Preposition Error Detection inESL Writing.
Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008).Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,Pei-Hao Su, David Vandyke, and Steve Young.2015.
Semantically Conditioned LSTM-based Nat-ural Language Generation for Spoken Dialogue Sys-tems.
EMNLP 2015.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for Automati-cally Grading ESOL Texts.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies.1191
