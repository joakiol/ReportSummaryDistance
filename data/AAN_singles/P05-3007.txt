Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 25?28, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsHigh Throughput Modularized NLP System for Clinical TextSerguei Pakhomov James Buntrock Patrick DuffyMayo College of Medicine Division of Biomedical Infor-maticsDivision of BiomedicalInformaticsMayo Clinic Mayo Clinic Mayo ClinicRochester, MN, 55905 Rochester, MN, 55905 Rochester, MN, 55905pakhomov@mayo.edu Buntrock@mayo.edu duffp@mayo.eduAbstractThis paper presents the results of the de-velopment of a high throughput, real timemodularized text analysis and informationretrieval system that identifies clinicallyrelevant entities in clinical notes, mapsthe entities to several standardized no-menclatures and makes them available forsubsequent information retrieval and datamining.
The performance of the systemwas validated on a small collection of 351documents partitioned into 4 query topicsand manually examined by 3 physiciansand 3 nurse abstractors for relevance tothe query topics.
We find that simple keyphrase searching results in 73% recall and77% precision.
A combination of NLPapproaches to indexing improve the recallto 92%, while lowering the precision to67%.1 IntroductionUntil recently the NLP systems developed forprocessing clinical texts have been narrowly fo-cused on a specific type of document such as radi-ology reports [1], discharge summaries [2],medline abstracts [3], pathology reports [4].
In ad-dition to being developed for a specific task, thesesystems tend to fairly monolithic in that their com-ponents have fairly strict dependencies on eachother, which make plug-and-play functionality dif-ficult.
NLP researchers and systems developers inthe field realize that modularized approaches arebeneficial for component reuse and more rapid de-velopment and advancement of NLP technology.In addition to the issue of modularity, the NLP sys-tems development efforts are starting to take scal-ability into account.
The Mayo Clinic?s repositoryof clinical notes contains over 16 million docu-ments growing at the rate of 50K documents perweek.
The time and space required for processingthese large amounts of data impose constraints onthe complexity of NLP systems.Another engineering challenge is to make theNLP systems work in real time.
This is particularlyimportant in a clinical environment for patient re-cruitment or patient identification for clinical re-search use cases.
In order to satisfy thisrequirement, a text processing system has to inter-face with the Electronic Health Record (EHR) sys-tem in real time and process documentsimmediately after they become available electroni-cally.
All of these are non-trivial issues and arecurrently being addressed in the community.
In thisposter we present the design and architecture of alarge-scale, highly modularized, real-time enabledtext analysis system as well as experimental vali-dation results.2 System DescriptionMayo Clinic and IBM have collaborated on aText Analytics project as part of a strategic LifeSciences and Computational Biology partnership.The goal of the Text Analytics collaboration was toprovide a text analysis system that would indexand retrieve clinical documents at the Mayo Clinic.The Text Analytics architecture leveraged ex-isting interface feeds for clinical documents byrouting them to the warehouse.
A work managerwas written using messaging queues to distributework for text analysis for real-time and bulk proc-essing (see Figure 1).
Additional text analysisengines can be configured and added with appro-priate hardware to increase document throughputof the system.25Figure 1- Text Analysis Process FlowFor deployment of text analysis engines we testedtwo configurations.
During the development phasewe used synchronous messaging using ApacheWeb Server with Tomcat/Axis.
The Apache Webserver provided a round robin mechanism to dis-tributed SOAP requests for text analysis.
This test-ing was deployed on a 20 CPU Beowulf clusterusing AMD Athlon?
processors running Linuxoperating system.
For production deployment weused Message Driven Beans (MDBs)using IBMWebsphere Application Server?
(WAS) and IBMWebsphere Message Queue?.
The text engineswere deployed on 2-CPU blade servers with 4GbRAM.
Each WAS instance had two MDBs withtext analysis engines.Work was distributed using message queues.
Eachtext analysis engine was deployed to function in-dependent of other engines.
A total of 20 bladeservers were configured for text processing.
Theaverage document throughput for each blade was20 documents per minute.The text analysis engine was designed by concep-tually breaking up the task into granular functionsthat could be implemented as components to beassembled into a text processing system.To implement the components we used anIBM AlphaWorks package called UnstructuredInformation Management Architecture (UIMA).UIMA is a software architecture that defines roles,interface, and communications of components fornatural language processing.
The four main UIMAservices include: acquisition, unstructured informa-tion analysis, structured information access, andcomponent discovery.
For the Mayo project weused the first three services.
The ability to custom-ize annotator sequences was advantageous duringthe design process.
Also, the ability to add annota-tors for specific dictionaries amounted only in mi-nor work.
Once annotators are written toconformance, UIMA provides pipeline develop-ment and permits the developer to quickly custom-ize processing to a specific task.
The final annota-tor layout is depicted in Figure 2.The context free tokenizer is a finite statetransducer that parses the document text into thesmallest meaningful spans of text.
A token is a setof characters that can be classified into one ofthese categories: word, punctuation, number, con-traction, possessive, symbol without taking intoaccount any additional context.The context sensitive spell corrector annotatoris used for automatic spell correction on word to-kens.
This annotator uses a combination of iso-lated-word and context-sensitive statisticalapproaches to rank the possible suggestions [5].The suggestion with the highest ranking is storedas a feature of a token.Figure 2 ?
Text Analysis PipelineThe lexical normalizer annotator is appliedonly to words, possessives, and contractions.
Itgenerates a canonical form by using the NationalLibrary of Medicine UMLS Lexical Variant Gen-erator (LVG) tool1.
Apart from generating lexicalvariants and stemming optimized for the biomedi-cal domain, it also generates a list of lemma entrieswith Penn Treebank tags as input for the POS tag-ger.The sentence detector annotator parses thedocument text into sentences.
The sentence detec-tor is based on a Maximum Entropy classifiertechnology2 and is trained to recognize sentenceboundaries from hand annotated data.1 http://umlslex.nlm.nih.gov2 http://maxent.sourceforge.net/26The context dependent tokenizer uses contextto detect complex tokens such as dates, times, andproblem lists3.The part of speech (POS) pre-tagger annotatoris intended to execute prior to the POS tagger an-notator.
The pre-tagger loads a list of words thatare unambiguous with respect to POS and havepredetermined Penn Treebank tags.
Words in thedocument text are tagged with these predeterminedtags.
The POS tagger can ignore these words andfocus on the remaining syntactically ambiguouswords.The POS tagger annotator attaches a part ofspeech tag to each token.
The current version ofthe POS tagger is from IBM based on HiddenMarkov models technology.
This tagger has beentrained on a combination of the Penn Treebankcorpus of general English and a corpus of manuallytagged clinical data developed at the Mayo Clinic[6], [7].The shallow parser annotator makes higherlevel constructs at the phrase level.
The ShallowParser is from IBM.
The shallow parser uses a setof rules operating on tokens and their part-of-speech category to identify linguistic phrases in thetext such as noun phrases, verb phrases, and adjec-tival phrases.The dictionary named entity annotator uses aset of enriched dictionaries (SNOMED-CT, MeSH,RxNorm and Mayo Synonym Clusters (MSC) tolookup named entities in the document text.
Thesenamed entities include drugs, diagnoses, signs, andsymptoms.
The MSC database contains a set ofclusters each consisting of diagnostic statementsthat are considered to be synonymous.
Synonymyhere is defined as two or more terms that have beenmanually classified to the same category in theMayo Master Sheet repository, which containsover 20 million manually coded diagnostic state-ments.
These diagnostic statements are used asentry terms for dictionary lookup.
A set of Mayocompiled dictionaries are also used to detect ab-breviations and hyphenated terms.The abbreviation disambiguation annotator at-tempts to detect and expand abbreviations and ac-ronyms based on Maximum Entropy classifierstrained on automatically generated data [8].3 Problem lists typically consist of numbered items in the Im-pression/Report/Plan section of the clinical notesThe negation annotator assigns a certainty at-tribute to each named entity with the exception ofdrugs.
This annotator is based on a generalizedversion of Chapman?s NegEx algorithm [9].The ML (Machine Learning) Named Entityannotator is based on a Na?ve Bayes classifiertrained on a combination of the UMLS entry termsand the MCS where each diagnostic statement isrepresented as a bag-of-words and used as a train-ing sample for generating a Naive Bayes classifierwhich assigns MCS id?s to noun phrases identifiedin the text of clinical notes.
The architecture of thiscomponent is given in Figure 3.TextDictionary LookupFoundNoun Phrase Head identifierNa?ve Bayes classifierBest guess clusterMayo Synonym ClustersM001|cholangeocarcinomaM001|bile duct cancerM001|?
Y NFigure 3.
ML Named Entity ClassifierThe text of a clinical note is first looked up in theMSC database using the dictionary named entityannotator.
If a span of text matched something inthe database, then the span is marked as a namedentity annotation and the appropriate cluster ID isassigned to it.
The portions of text where no matchwas found continue to be processed with a namedentity identification algorithm that relies on theoutput of the shallow parser annotator to findnoun phrases whose heads are on a list of nounsthat exist in the MSC database as individual manu-ally coded entries.
For example, a noun phrasesuch as ?metastasized cholangiocarcinoma?
will beidentified as a named entity and subsequentlyautomatically classified, but a noun phrase such as?patient?s father?
will not.3 EvaluationThe system performance was evaluated using acollection of 351 documents partitioned into 4 top-ics: pulmonary fibrosis, cholangiocarcinoma, dia-betes mellitus and congestive heart failure.
Each of27the topics contained approximately 90 documentsthat were manually examined by three nurse ab-stractors and three physicians.
Each note wasmarked as either relevant or not relevant to a giventopic.
In order to establish the reliability of this testcorpus, we used a standard weighted Kappa statis-tic [10].
The overall Kappa for the four topics were0.59 for pulmonary fibrosis, 0.79 for cholangiocar-cinoma, 0.79 for diabetes mellitus and 0.59 forcongestive heart failure.
We ran a set of queries foreach of the 4 topics on the partition generated forthat topic.
Each query used the primary term thatrepresented the topic.
For example, for pulmonaryfibrosis, only the term ?pulmonary fibrosis?
wasused while other closely related terms such as ?in-terstitial pneumonitis?
were excluded.
The baselinequery was executed using the term as a key phraseon the original text of the documents.
The rest ofthe queries were executed using the concept id?sautomatically generated for each primary term.
Onthe back end, the text of the clinical notes was an-notated with the Metamap program [3] for theUMLS concepts and the ML Named Entity annota-tor for MSC cluster id?s.
On the front end, theUMLS concept id?s were generated via the UMLSKnowledge Server online and the MSC id?s weregenerated using a combination of the same Na?veBayes classifier and the same dictionary lookupmechanism as were used to annotate the clinicalnotes.
We also tested a query that combinedMetamap and MSC annotations and query parame-ters.
Recall, precision and f-score (?=0.5) werecalculated for each query.
The results are summa-rized in Table 1.Precision Recall F-scoreKey Phrase 0.77 0.73 0.749467MSC cluster 0.67 0.89 0.764487Metamap 0.71 0.84 0.769548Metamap+MSC 0.67 0.92 0.775346Table 1.
Performance of different annotation methods.The f-score results are fairly close for all methods;however, the recall is highest for the method thatcombines Metamap and the MSC methodology.This is particularly important for using this systemin recruiting patients for epidemiological researchfor disease  incidence or disease prevalence studiesand clinical trials where recall is valued more thanprecision.
A combination of Metamap and MSCannotations and queries produced the highest recallwhich shows that these systems are complemen-tary.
The modular design of our system makes iteasy to incorporate complementary annotation sys-tems like Metamap into the annotation process.AcknowledgementsThe authors wish to thank the Mayo ClinicEmeritus Staff Physicians and Nurse Abstractorswho served as experts for this study.
The authorsalso wish to thank Patrick Duffy for programmingsupport and David Hodge for statistical analysisand interpretation.References1.
Friedman, C., et al, A general natural-language textprocessor for clinical radiology.
Journal of Ameri-can Medical Informatics Association, 1994.
1(2): p.161-174.2.
Friedman, C. Towards a Comprehensive MedicalLanguage Processing System: Methods and Issues.in American Medical Informatics Association(AMIA).
1997.3.
Aronson, A.
Effective mapping of biomedical text tothe UMLS Metathesaurus: the MetaMap program.
inProceedings of the 2001 AMIA Annual Symposium.2001.
Washington, DC.4.
Mitchell, K. and R. Crowley.
GNegEx ?
Implemen-tation and Evaluation of a Negation Tagger for theShared Pathology Iinformatics Network.
in Advanc-ing Practice, Instruction and Innovation through In-formatics (APIII).
2003.5.
Thompson-McInness, B., S. Pakhomov, and T.Pedersen.
Automating Spelling Correction Tools Us-ing Bigram Statistics.
in Medinfo Symposium.
2004.San Francisco, CA, USA.6.
Coden, A., et al, Domain-specific language modelsand lexicons for tagging.
In print in Journal of Bio-medical Informatics, 2005.7.
Pakhomov, S., A. Coden, and C. Chute, Developinga Corpus of Clinical Notes Manually Annotated forPart-of-Speech.
To appear in International Journal ofMedical Informatics, 2005(Special Issue on NaturalLanguage Processing in Biomedical Applications).8.
Pakhomov, S. Semi-Supervised Maximum EntropyBased Approach to Acronym and Abbreviation Nor-malization in Medical Texts.
in 40th Meeting of theAssociation for Computational Linguistics (ACL2002).
2002.
Philadelohia, PA.9.
Chapman, W.W., et al Evaluation of NegationPhrases in Narrative Clinical Reports.
in AmericanMedical Informatics Association.
2001.
Washington,DC, USA.10.
Landis, J.R. and G.G.
Koch, The Measurement ofObserver Agreement for Categorical Data.
Biomet-rics, 1977.
33: p. 159-174.28
