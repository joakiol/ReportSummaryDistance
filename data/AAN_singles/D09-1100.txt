Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958?967,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPReading to Learn: Constructing Features from Semantic AbstractsJacob Eisenstein?James Clarke?Dan Goldwasser?Dan Roth??
?Beckman Institute for Advanced Science and Technology,?Department of Computer ScienceUniversity of IllinoisUrbana, IL 61801{jacobe,clarkeje,goldwas1,danr}@illinois.eduAbstractMachine learning offers a range of toolsfor training systems from data, but thesemethods are only as good as the underly-ing representation.
This paper proposes toacquire representations for machine learn-ing by reading text written to accommo-date human learning.
We propose a novelform of semantic analysis called read-ing to learn, where the goal is to obtaina high-level semantic abstract of multi-ple documents in a representation that fa-cilitates learning.
We obtain this abstractthrough a generative model that requiresno labeled data, instead leveraging repe-tition across multiple documents.
The se-mantic abstract is converted into a trans-formed feature space for learning, result-ing in improved generalization on a rela-tional learning task.1 IntroductionMachine learning offers a range of powerful toolsfor training systems to act in complex environ-ments, but these methods depend on a well-chosenrepresentation for features.
For learning to suc-ceed the representation often must be crafted withknowledge about the application domain.
Thisposes a bottleneck, requiring expertise in both ma-chine learning and the application domain.
How-ever, domain experts often express their knowl-edge through text; one direct expression is throughtext designed to aid human learning.
In this paperwe exploit text written by domain experts in or-der to build a more expressive representation forlearning.
We term this approach reading to learn.The following scenario demonstrates the moti-vation for reading to learn.
Imagine an agent givena task within its world/environment.
The agent hasno prior knowledge of the task but can perceive theworld through low-level sensors.
Learning directlyfrom the sensors may be difficult, as interestingtasks typically require a complex combination ofsensors.
Our goal is to acquire domain knowledgethrough the semantic analysis of text, so as to pro-duce higher-level relations through combinationsof sensors.As a concrete example consider the problem oflearning how to make legal moves in Freecell soli-taire.
Relevant sensors may indicate if an objectis a card or a freecell, whether a card is a certainvalue, and whether two values are in sequence.Although it is possible to express the rules witha combination of sensors, learning this combina-tion is difficult.
Text can facilitate learning by pro-viding relations at the appropriate level of gen-eralization.
For example, the sentence: ?You canplace a card on an empty freecell,?
suggests notonly which sensors are useful together but alsohow these sensors should be linked.
Assuming thesensors are represented as predicates, one possi-ble relation this sentence suggests is: r(x, y) =card(x) ?
freecell(y) ?
empty(y).
Armedwith this new relation the agent?s learning taskmay be simpler.
Throughout the paper we refer tolow-level sensory input as sensor or predicate, andto a higher level concept as a logical formula or re-lation.Our approach to semantic analysis does not re-quire a complete semantic representation of thetext.
We merely wish to acquire a semantic ab-stract of a document or document collection, anduse the discovered relations to facilitate data-driven learning.
This will allow us to directly eval-uate the contribution of the extracted relations forlearning.We develop an approach to recover semantic ab-stracts that uses minimal supervision: we assumeonly a very small set of lexical glosses, which mapfrom words to sensors.
This marks a substantialdeparture from previous work on semantic pars-ing, which requires either annotations of the mean-ings of each individual sentence (Zettlemoyer andCollins, 2005; Liang et al, 2009), or alignmentsof sentences to grounded representations of the958world (Chen and Mooney, 2008).
For the purposeof learning, this approach may be inapplicable, assuch text is often written at a high level of abstrac-tion that permits no grounded representation.There are two properties of our setting thatmake unsupervised learning feasible.
First, it isnot necessary to extract a semantic representationof each individual sentence, but rather a summaryof the semantics of the document collection.
Er-rors in the semantic abstract are not fatal, as longit guides the learning component towards a moreuseful representation.
Second, we can exploit rep-etition across documents, which should generallyexpress the same underlying meaning.
Logical for-mulae that are well-supported by multiple docu-ments are especially likely to be useful.The rest of this paper describes our approachfor recovering semantic abstracts and outlines howwe apply and evaluate this approach on the Free-cell domain.
The paper contributes the followingkey ideas: (1) Interpreting abstract ?instructional?text, written at a level that does not correspondto concrete sensory inputs in the world, so thatno grounded representation is possible, (2) read-ing to learn, a new setting in which extracted se-mantic representations are evaluated by whetherthey facilitate learning; (3) abstractive semanticsummarization, aimed at capturing broad seman-tic properties of a multi-document dataset, ratherthan a semantic parse of individual sentences; (4) anovel, minimally-supervised generative model forsemantic analysis which leverages both lexical andsyntactic properties of text.2 Approach OverviewWe describe our approach to text analysis as mul-tidocument semantic abstraction, with the goal ofdiscovering a compact set of logical formulae toexplain the text in a document collection.
To thisend, we develop a novel generative model in whichnatural language sentences (e.g., ?You can alwaysplace cards in empty freecells?)
are stochasticallygenerated from logical formulae (e.g., card(x)?freecell(y) ?
empty(y)).
We formally definea generative process that reflects our intuitionsabout the relationship between formulae and sen-tences (Section 3), and perform sampling-basedinference to recover the formulae most likely tohave generated the observed data (Section 4).
Thetop N such formulae can then be added as addi-tional predicates for relational learning.Our semantic representation consists of con-junctions of literals, each of which includes a sin-gle predicate (e.g., empty) and one or more vari-ables (e.g., x).
Predicates describe atomic seman-tic concepts, while variables construct networksof relationships between them.
While the impor-tance of the predicates is obvious, the variableassignments also exert a crucial influence on thesemantics of the conjunction: modifying a sin-gle variable in the formula above from empty(y)to empty(x) yields a formula that is triviallyfalse for all groundings (since cards can never beempty).Thus, our generative model must account for theinfluence of both predicates and variables on thesentences in the documents.
A natural choice is touse the predicates to influence the lexical items,while letting the variables determine the syntac-tic structure.
For example, the formula card(x)?freecell(y) ?
empty(y) contains three pred-icates and two variables.
The predicates influencethe lexical items in a direct way: we expect thatsentences generated from this formula will includea member of the gloss set for each predicate ?the sentence ?Put the cards on the empty free-cells?
should be more likely than ?Columns areconstructed by playing cards in alternating colors.
?The impact of the variables on the generativeprocess is more subtle.
The sharing of the variabley suggests a relationship between the predicatesfreecell and empty.
This should be realizedin the syntactic structure of the sentence.
Model-ing syntax using a dependency tree, we expect thatthe glosses for predicates that share terms will ap-pear in compact sub-trees, while predicates that donot share terms should be more distant.
One pos-sible surface realization of this logical formula isthe sentence, ?Put the card on the empty freecell,?whose dependency parse is shown in the left treeof Figure 1.
The glosses empty and freecell are im-mediately adjacent, while card is more remote.We develop two metrics that quantify the com-pactness of a set of variable assignments withrespect to a dependency tree: excess terms, andshared terms.
The number of excess terms in asubtree is the number of unique terms assignedto words in the subtree, minus the maximum arityof any predicate in the subtree.
Shared terms arisewhenever a node has multiple subtrees which eachcontain the same variable.
We will use the alterna-tive alignments in Figure 1 to provide a more de-tailed explanation.
In each tree, the variables arewritten in the nodes belonging to the associatedlexical items; variables are written over arrows toindicate membership in some node in the subtree.Excess Terms Alignment A of Fig-ure 1, corresponding to the formula959Putcard onfreecellthe emptytheXXXXXXX XXXXXXXYXXYX XYXYYXYYXYYX YYYYYXYZXYZX YZYZZDependency tree Alignment A Alignment B Alignment C Alignment DFigure 1: A dependency parse and four different variable assignments.
Each literal is aligned to a word (anode in the graph), and the associated variables are written in the box.
Variables belonging to descendantnodes are written over the arrows.card(x)?freecell(x)?empty(x), has zeroexcess terms in every subtree; there is a total of onevariable, and all the predicates are unary.
In Align-ment B, card(x) ?
freecell(x) ?
empty(y),there are excess terms at the root, and in the toptwo subtrees on the right-hand side.
Alignment Ccontains an excess term at only the root node.Even though it contains the same number ofunique variables as Alignment B, it is not penal-ized as harshly because the alignment of variablesbetter corresponds to the syntactic structure.Alignment D contains the greatest number ofexcess terms: two at the root of the tree, and onein each of the top two subtrees on the right side.Shared Terms According to the excess termmetric, the best choice is simply to introduce asfew variables as possible.
For this reason, we alsopenalize shared terms which occur when a nodehas subtree children that share a variable.
In Fig-ure 1, Alignments A and B each contain a sharedterm at the top node; Alignments C and D containno shared terms.Overall, we note that Alignment B is penalizedon both metrics, as it contains both excess termsand shared terms; the syntactic structure of thesentence makes such a variable assignment rela-tively improbable.card(x) & freecell(y) & empty(y)f(y)e(y)c(x)f(y)e(y)c(x)Put the card on the empty freecell(a)(b)(c)(d)(e)Figure 2: A graphical depiction of the generativeprocess by which sentences are produced from for-mulae3 Generative ModelThese intuitions are formalized in a generativeaccount of how sentences are stochastically pro-duced from a set of logical formulae.
This gener-ative story guides an inference procedure for re-covering logical formulae that are likely to havegenerated any observed set of texts, which is de-scribed in Section 4.The outline of the generative process is depictedin Figure 2.
For each sentence, we begin in step (a)by drawing a formula f from a Dirichlet pro-cess (Ferguson, 1973).
The Dirichlet process de-960fines a non-parametric mixture model, and has theeffect of adaptively selecting the appropriate num-ber of formulae to explain the observed sentencesin the corpus.1We then draw the sentence lengthfrom some distribution over positive integers; asthe sentence length is always observed, we neednot define the distribution (step (b)).
In step (c), adependency tree is drawn from a uniform distribu-tion over spanning trees with a number of nodesequal to the length of the sentence.
In step (d) wedraw an alignment of the literals in f to nodes inthe dependency tree, written at(f).
The distribu-tion over alignments is described in Section 3.1.Finally, the aligned literals are used to generate thewords at each slot in the dependency tree.
A moreformal definition of this process is as follows:?
Draw ?, the expected number of literals performula, from a Gamma distribution G(u, v).?
Draw an infinite set of formulae f .
For eachformula fi,?
Draw the formula length #|fi| from aPoisson distribution, ni?
Poisson(?).?
Draw niliterals from a uniform distri-bution.?
Draw pi, an infinite multinomial distributionover formulae: pi ?
GEM(pi0), where GEMrefers to the stick-breaking prior (Sethura-man, 1994) and pi0= 1 is the concentra-tion parameter.
By attaching the multinomialpi to the infinite set of formulae f , we cre-ate a Dirichlet process.
This is conventionallywrittenDP (pi0, G0), where the base distribu-tionG0encodes only the distribution over thenumber of literals, Poisson(?).?
For each of D documents, draw the numberof sentences T ?
Poisson.
For each of the Tsentences in the document,?
Draw a formula f ?
DP (pi0, G0) fromthe Dirichlet Process described above.?
Draw a sentence length #|s| ?
Poisson.?
Draw a dependency graph t (a spanningtree of size #|s|) from a uniform distri-bution.?
Draw an alignment at(f), an injectivemapping from literals in f to nodes inthe dependency structure t. The distribu-tion over alignments is described in Sec-tion 3.1.1There are many recent applications of Dirichlet pro-cesses in natural language processing, e.g.
Goldwater et al(2006).?
Draw the sentence s from the formulaf and the alignment a(f).
For eachword token wi?
s is drawn fromp(wi|at(f, i)), where at(f, i) indicatesthe (possibly empty) literal assignedto slot i in the alignment at(f) (Sec-tion 3.2).3.1 Distribution over AlignmentsThe distribution over alignments reflects our intu-ition that when literals share variables, they willbe aligned to word slots that are nearby in the de-pendency structure; literals that do not share vari-ables should be more distant.
This is formalized byapplying the concepts of excess terms and sharedterms defined in Section 2.
After computing thenumber of excess and shared terms in each sub-tree ti, we can compute a local score (LS ) for thatsubtree:LS (at(f); ti) = ?
?NShared(at(f), ti)+ ?
?NExcess(at(f), ti) ?
height(ti).This scoring function can be applied recursively toeach subtree in t; the overall score of the tree is therecursive sum,score(at(f); t) = LS (at(f); t)+n?iscore(at(f); ti),(1)where tiindicates the ithsubtree of t. We hypoth-esize a generative process that produces all possi-ble alignments, scores them using score(at(f); t),and selects an alignment with probability,p(at(f)) ?
exp{?score(at(f); t)}.
(2)In our experiments, we define the parameters ?
=1, ?
= 1.3.2 Generation of Lexical ItemsOnce the logical formula is aligned to the parsestructure, the generation of the lexical items inthe sentence is straightforward.
For word slots towhich no literals are aligned, the lexical item isdrawn from a language model ?, estimated fromthe entire document collection.
For slots to whichat least one literal is aligned, we construct a lan-guage model ?
in which the probability mass isdivided equally among all glosses of aligned pred-icates.
The language model ?
is used as a backoff,so that there is a strong bias in favor of generatingglosses, but some probability mass is reserved forthe other lexical items.9614 InferenceThis section describes a sampling-based inferenceprocedure for obtaining a set of formulae f thatexplain the observed text s and dependency struc-tures t. We perform Gibbs sampling over theformulae assigned to each sentence.
Using theChinese Restaurant Process interpretation of theDirichlet Process (Aldous, 1985), we marginalizepi, the infinite multinomial over all possible for-mulae: at each sampling step we select either anexisting formula, or stochastically generate a newformula.
After each full round of Gibbs sampling,a set of Metropolis-Hastings moves are applied toexplore modifications of the formulae.
This proce-dure converges on a stationary Markov chain cen-tered on a set of formulae that cohere well with thelexical and syntactic properties of the text.4.1 Assigning Sentences to FormulaeFor each sentence siand dependency tree ti, a hid-den variable yiindicates the index of the formulathat generates the text.
We can resample yiusingGibbs sampling.
In the non-parametric setting, yiranges over all non-negative integers; the ChineseRestaurant Process formulation marginalizes theinfinite-dimensional parameter pi, yielding a priorbased on the counts for each ?active?
formula (towhich at least one other sentence is assigned), anda pseudo-count representing all non-active formu-lae.
Given K formulae, the prior on selecting for-mula j is:p(yi= j|y?i, pi0) ?
{n?i(j) j < Kpi0j = K,(3)where y?irefers to the assignments of all y otherthan yiand n?irefers to the counts over these as-signments.
Each j < K identifies an existing for-mula in f , to which at least one other sentence isassigned.
When j = K, this means a new formulaf?must be generated.To perform Gibbs sampling, we draw from theposterior distribution over yi,p(yi|si, tif , f?,y?i, pi0) ?p(yi|y?i, pi0)p(si, ti|yi, f , f?
),where the first term is the prior defined in Equa-tion 3 and the latter term is the likelihood of gener-ating the parsed sentence ?si, ti?
from the formulaindexed by yi.To compute the probability of a parsed sentencegiven a formula, we sum over alignments,p(s, t|f) =?at(f)p(s, t,at(f)|f)=?at(f)p(s|at(f))p(t,at(f)|f)=?at(f)p(s|at(f))p(at(f)|t, f)p(t|f),(4)applying the chain rule and independence assump-tions from the generative model.
The result is aproduct of three terms: the likelihood of the lexi-cal items given the aligned predicates (defined inSection 3.2; the likelihood of the alignment giventhe dependency tree and formula (defined in equa-tion 2), and the probability of the dependency treegiven the formula, which is uniform.Equation 4 takes a sum across alignments, butmost of the probability mass of p(s|at(f)) willbe concentrated on alignments in which predicatescover words that gloss them.
Thus, we can applyan approximation,p(s, t|f) ?N?at(f)p(s|at(f))p(at(f)|t, f)p(t|f),(5)in which we draw N samples in which predicatesare aligned to their glosses whenever possible.Similarly, Equation 2 quantifies the likelihoodof an alignment only to a constant of proportional-ity; again, a sum over possible alignments is nec-essary.
We do not expect the prior on alignmentsto be strongly peaked like the sentence likelihood,so we approximate the normalization term by sam-pling M alignments at random and extrapolating:p(at(f)|t, f) ?
q(at(f); t)=q(at(f); t)?a?t(f)q(a?t(f); t)?#|a?t(f)|Mq(at(f); t)?Ma?t(f)q(a?t(f); t),where q(at(f); t) = exp{?score(at(f); t)}, de-fined in Equation 2.
In our experiments, we set Nto at most 10, and M = 20.
Drawing larger num-bers of samples had no discernible effect on sys-tem output.9624.1.1 Generating new formulaeChinese Restaurant Process sampling requires thegeneration of new candidate formulae at each re-sampling stage.
To generate a new formula, wefirst sample the number of literals.
As describedin the generative story (Section 3), the numberof literals is drawn from a Poisson distributionwith parameter ?.
We treat ?
as unknown andmarginalize, using the Gamma hyperprior G(u, v).Due to Poisson-Gamma conjugacy, this marginal-ization can be performed analytically, yieldinga Negative-Binomial distribution with parameters?u+?i#|fi|, (1+K+v)?1?, where?i#|fi| isthe sum of the number of literals in each formula,and K is the number of formulae which generateat least one sentence.
In this sense, the hyperpriorsu and v act as pseudo counts.
We set u = 3, v = 1,reflecting a weak prior expectation of three literalsper predicate.After drawing the size of the formula, the predi-cates are selected from a uniform random distribu-tion.
Finally, the terms are assigned: at each slot,we reuse a previous term with probability 0.5, un-less none is available; otherwise a new term is gen-erated.4.2 Proposing changes to formulaeThe assignment resampling procedure has theability to generate new formulae, thus exploringthe space of relational features.
However, to ex-plore this space more rapidly, we introduce fourMetropolis-Hastings moves that modify existingformulae (Gilks, 1995): adding a literal, deletinga literal, substituting a literal, and rearranging theterms of the formula.
For each proposed move, werecompute the joint likelihood of the formula andall aligned sentences.
The move is stochasticallyaccepted based on the ratio of the joint likelihoodsof the new and old configurations, multiplied by aHastings correction.The joint likelihood with respect to formula fis computed as p(s, t, f) = p(f)?ip(si, ti|f).The prior on f considers only the number of liter-als, using a Negative-Binomial distribution as de-scribed in section 4.1.1.
The likelihood p(si, ti|f)is given in equation 4.
The Hastings correction isp?(f??
f)/p?
(f ?
f?
), with p?
(f ?
f?)
indicat-ing the probability of proposing a move from fto f?,and p?(f??
f) indicating the probability ofproposing the reverse move.
The Hastings correc-tions depend on the arity of the predicates beingadded and removed; the derivation is straightfor-ward but tedious.
We plan to release a technicalreport with complete details.4.3 Summary of inferenceThe final inference procedure iterates betweenGibbs sampling of assignments of formulae tosentences, and manipulating the formulae throughMetropolis-Hastings moves.
A full iteration com-prises proposing a move to each formula, and thenusing Gibbs sampling to reconsider all assign-ments.
If a formula no longer has any sentencesassigned to it, then it is dropped from the activeset, and can no longer be selected in Gibbs sam-pling ?
this is standard in the Chinese RestaurantProcess.Five separate Markov chains are maintained inparallel.
To allow the sampling procedure to con-verge to a stationary distribution, each chain be-gins with 100 iterations of ?burn-in?
sampling,without storing the output.
At this point, we per-form another 100 iterations, storing the state at theend of each iteration.2All formulae are ranked ac-cording to the cumulative number of sentences towhich they are assigned (across all five Markovchains), aggregating the counts for multiple in-stances of identical formulae.
This yields a rankedlist of formulae which will be used in our frame-work as features for relational learning.5 EvaluationOur experimental setup is designed to evaluate thequality of the semantic abstraction performed byour model.
The logical formulae obtained by oursystem are applied as features for relational learn-ing of the rules of the game of Freecell solitaire.We investigate whether these features enable bet-ter generalization given varying number of train-ing examples of Freecell game states.
We alsoquantify the specific role of syntax, lexical choice,and feature expressivity in learning performance.This section describes the details of this evalua-tion.5.1 Relational LearningWe perform relational learning using InductiveLogic Programming (ILP), which constructs gen-eralized rules by assembling smaller logical for-mulae to explain observed propositional exam-ples (Muggleton, 1995).
The lowest level formu-lae consist of basic sensors that describe the en-vironment.
ILP?s expressivity enables it to buildcomplex conjunctions of these building blocks,but at the cost of tractability.
Our evaluation askswhether the logical formulae abstracted from text2Sampling for more iterations was not found to affect per-formance on development data, and the model likelihood ap-peared stationary after 100 iterations.963Predicate Glossescard(x) cardtableau(x) column, tableaufreecell(x) freecell, cellhomecell(x) foundation, cell, homecellvalue(x,y) ace, king, rank, 8, 3, 7, lowest,highestsuccessor(x,y) higher, sequence, sequentialcolor(x,y) black, red, colorsuit(x,y) suit, club, diamond, spade,hearton(x,y) ontotop(x,y) bottom, available, topempty(x) emptyTable 1: Predicates in the Freecell world model,with natural language glosses obtained from thedevelopment set text.can transform the representation to facilitate learn-ing.
We compare against both the sensor-level rep-resentation as well as richer representations that donot benefit from the full power of our model?s se-mantic analysis.The ALEPH3ILP system, which is primarilybased on PROGOL (Muggleton, 1995), was usedto induce the rules of game.
The search parame-ters remained constant for all experiments.5.2 ResourcesThere are four types of resources required to workin the reading-to-learn setting: a world model, in-structional text, a small set of glosses that mapfrom text to elements of the world model, and la-beled examples of correct and incorrect actionsin the world.
In our experiments, we considerthe domain of Freecell solitaire, a popular cardgame (Morehead and Mott-Smith, 1983) in whichcards are moved between various types of loca-tions, depending on their suit and rank.
We nowdescribe the resources for the Freecell domain inmore detail.WorldModel Freecell solitaire can be describedformally using first order logic; we consider aslightly modified version of the representationfrom the Planning Domain Definition Language(PDDL), which is used in automatic game-playingcompetitions.
Specifically, there are 87 constants:52 cards, 16 locations, 13 values, four suits, andtwo colors.
These constants are combined with afixed set of 11 predicates, listed in Table 1.Instructional Text Our approach relies on textthat describes how to operate in the Freecell soli-taire domain.
A total of five instruction sets were3Freely available from http://www.comlab.ox.ac.uk/activities/machinelearning/Aleph/obtained from the Internet.
Due to the popular-ity of the Microsoft implementation of Freecell,instructions often contain information specific toplaying Freecell on a computer.
We manually re-moved sentences which did not focus on the cardaspects of Freecell (e.g., how to set up the boardand information regarding where to click to movecards).
In order to use our semantic abstractionmodel, the instructions were part-of-speech taggedwith the Stanford POS Tagger (Toutanova andManning, 2000) and dependency parses were ob-tained using Malt (Nivre, 2006).Glosses Our reading to learn setting requires asmall set of glosses, which are surface forms com-monly used to represent predicates from the worldmodel.
We envision an application scenario inwhich a designer manually specifies a few glossesfor each predicate.
However, for the purposes ofevaluation, it would be unprincipled for the exper-imenters to handcraft the ideal set of glosses.
In-stead, we gathered a development set of text andannotated the lexical mentions of the world modelpredicates in text.
This annotation is used to ob-tain glosses to apply to the evaluation text.
Thisapproximates a scenario in which the designer hasa reasonable idea of how the domain will be de-scribed in text, but no prior knowledge of the spe-cific details of the text instructions.
Our exper-iments used glosses that occurred two or moretimes in the instructions: this yields a total of 32glosses for 11 predicates, as shown in Table 1.Evaluation game data Ultimately, the seman-tic abstraction obtained from the text is appliedto learning on labeled examples of correct andincorrect actions in the world model.
For evalu-ation, we automatically generated a set of movescenarios: game states with one positive example(a legal move) and one negative example (an ille-gal move).
To avoid bias in the data we generatean equal number of move scenarios from each ofthree types: moves to the freecells, homecells, andtableaux.
For our experiments we vary the numberof move scenarios in the training set; the develop-ment and test sets consist of 900 and 1500 movescenarios respectively.5.3 Evaluation SettingsWe compare four different feature sets, whichwill be provided to the ALEPH ILP learner.
Allfeature sets include the sensor-level predicatesshown in Table 1.
The FULL-MODEL featureset alo includes the top logical formulae ob-tained in our model?s semantic abstract (see Sec-964tion 4.3).
The NO-SYNTAX feature set is obtainedfrom a variant of our model in which the in-fluence of syntax is removed by setting parame-ters ?, ?
= 0.
The SENSORS-ONLY feature setuses only the sensor-level predicates.
Finally, theRELATIONAL-RANDOM feature set is constructedby replacing each feature in the FULL-MODEL setwith a randomly generated relational feature ofidentical expressivity (each predicate is replacedby a randomly chosen alternative with identicalarity; terms are also assigned randomly).
This en-sures that any performance gains obtained by ourmodel were not due merely to the greater expres-sivity of its relational features.
The number of fea-tures included in each scenario is tuned on a de-velopment set of test examples.The performance metric assesses the abilityof the ILP learner to classify proposed Freecellmoves as legal or illegal.
As the evaluation setcontains an equal number of positive and negativeexamples, accuracy is the appropriate metric.
Thetraining scenarios are randomly generated; we re-peat each run 50 times and average our results.
Forthe RELATIONAL-RANDOM feature set ?
in whichpredicates and terms are chosen randomly ?
wealso regenerate the formulae per run.6 ResultsTable 2 shows a comparison of the resultsusing the setup described above.
Our FULL-MODEL achieves the best performance at ev-ery training set size, consistently outperformingthe SENSORS-ONLY representation by an abso-lute difference of three to four percent.
Thisdemonstrates the semantic abstract obtained byour model does indeed facilitate machine learningin this domain.RELATIONAL-RANDOM provides a baseline ofrelational features with equal expressivity to thosechosen by our model, but with the predicates andterms selected randomly.
We consistently outper-form this baseline, demonstrate that the improve-ment obtained over the sensors only representationis not due merely to the added expressivity of ourfeatures.The third row compares against NO-SYNTAX,a crippled version of our model that incorpo-rates lexical features but not the syntactic struc-ture.
The results are stronger than the SENSORS-ONLY and RELATIONAL-RANDOM baselines, butstill weaker than our full system.
This demon-strates the syntactic features incorporated by ourmodel result in better semantic representations ofthe underlying text.Features Number of training scenarios15 30 60 120SENSORS-ONLY 79.12 88.07 92.77 93.73RELATIONAL-RANDOM 82.72 89.14 93.08 94.17NO-SYNTAX 80.98 89.79 94.11 97.04FULL-MODEL 82.89 91.00 95.23 97.45Table 2: Results as number of training examplesvaried.
Each value represents the accuracy of theinduced rules obtained with the given feature set.card(x1) ?
tableau(x2)card(x1) ?
freecell(x2)homecell(x1) ?
value(x2,x3)empty(x1) ?
freecell(x1)card(x1) ?
top(x1,x2)card(x1) ?
homecell(x2)freecell(x1) ?
homecell(x2)card(x1) ?
tableau(x1)card(x1) ?
top(x2,x1)homecell(x1)card(x1) ?
homecell(x1)color(x1,x2) ?
value(x3,x4)suit(x1,x2) ?
value(x3,x4)value(x1,x2) ?
value(x3,x4)homecell(x1) ?
successor(x2,x3)Figure 3: The top 15 features recovered by the se-mantic abstraction of our full model.Figure 3 shows the top 15 formulae recoveredby the full model running on the evaluation text.Features such as empty(x1) ?
freecell(x1)are useful because they reuse variables to ensurethat objects have key properties ?
in this case, en-suring that a freecell is empty.
Other features, suchas homecell(x1) ?
value(x2, x3), help to fo-cus the search on useful conjunctions of predicates(in Freecell, the legality of playing a card on ahomecell depends on the value of the card).
Notethat three of these 15 formulae are trivially use-less, in that they are always false: e.g., card(x1)?
tableau(x1).
This illustrates the importanceof term assignment in obtaining useful featuresfor learning.
In the NO-SYNTAX system, whichignores the relationship between term assignmentand syntactic structure, eight of the top 15 formu-lae were trivially useless due to term incompatibil-ity.7 Related WorkThis paper draws on recent literature on extract-ing logical forms from surface text (Zettlemoyerand Collins, 2005; Ge and Mooney, 2005; Downeyet al, 2005; Liang et al, 2009), interpreting lan-guage in the context of a domain (Chen andMooney, 2008), and using an actionable domainto guide text interpretation (Branavan et al, 2009).We differentiate our research in several dimen-sions:965Language Interpretation Instructional text de-scribes generalized statements about entities inthe domain and the way they interact, thus thetext does not correspond directly to concrete sen-sory inputs in the world (i.e., a specific worldstate).
Our interpretation captures these general-izations as first-order logic statements that can beevaluated given a specific state.
This contrasts toprevious work which interprets ?directions?
andthus assumes a direct correspondence between textand world state (Branavan et al, 2009; Chen andMooney, 2008).Supervision Our work avoids supervision in theform of labeled examples, using only a minimalset of natural language glosses per predicate.
Pre-vious work also considered the supervision signalobtained by interpreting natural language in thecontext of a formal domain.
Branavan et al (2009)use feedback from a world model as a supervi-sion signal.
Chen and Mooney (2008) use tempo-ral alignment of text and grounded descriptions ofthe world state.
In these approaches, concrete do-main entities are grounded in language interpreta-tion, and therefore require only a propositional se-mantic representation.
Previous approaches for in-terpreting generalized natural language statementsare trained from labeled examples (Zettlemoyerand Collins, 2005; Lu et al, 2008).Level of analysis We aim for an abstractivesemantic summary across multiple documents,whereas other approaches attempt to produce log-ical forms for individual sentences (Zettlemoyerand Collins, 2005; Ge and Mooney, 2005).
Weavoid the requirement that each sentence have ameaningful interpretation within the domain, al-lowing us to handle relatively unstructured text.Evaluation We do not evaluate the representa-tions obtained by our model; rather we assesswhether these representations improve learningperformance.
This is similar to work on Geo-Query (Wong and Mooney, 2007; Ge and Mooney,2005), and also to recent work on following step-by-step directions (Branavan et al, 2009).
Whilethese evaluations are performed on the basis of in-dividual sentences, actions, or system responses,we evaluate the holistic semantic analysis obtainedby our system.Model We treat surface text as generated from alatent semantic description.
Lu et al (2008) ap-ply a generative model, but require a completederivation from semantics to the lexical represen-tation, while we favor a more flexible semanticanalysis that can be learned without annotationand applied to noisy text.
More similar is the workof Liang et al (2009), which models the gener-ation of semantically-relevant fields using lexicaland discourse features.
Our approach differs byaccounting for syntax, which enables a more ex-pressive semantic representation that includes un-grounded variables.Relational learning The output of our semanticanalysis is applied to learning in a structured rela-tional space, using ILP.
A key difficulty with ILPis that the increased expressivity dramatically ex-pands the hypothesis space, and it is widely agreedthat some learning bias is required for ILP to betractable (N?edellec et al, 1996; Cumby and Roth,2003).
Our work can be viewed as a new methodfor acquiring such bias from text; moreover, ourapproach is not specialized for ILP and may beused to transform the feature space in other formsof relational learning as well (Roth and Yih, 2001;Cumby and Roth, 2003; Richardson and Domin-gos, 2006).8 ConclusionThis paper demonstrates a new setting for seman-tic analysis, which we term reading to learn.
Wehandle text which describes the world in gen-eral terms rather than refereing to concrete enti-ties in the domain.
We obtain a semantic abstractof multiple documents, using a novel, minimally-supervised generative model that accounts for bothsyntax and lexical choice.
The semantic abstractis represented as a set of predicate logic formu-lae, which are applied as higher-order features forlearning.
We demonstrate that these features im-prove learning performance, and that both the lex-ical and syntactic aspects of our model yield sub-stantial contributions.In the current setup, we produce an ?overgener-ated?
semantic representation comprised of usefulfeatures for learning but also some false positives.Learning in our system can be seen as the processof pruning this representation by selecting usefulformulae based on interaction with the trainingdata.
In the future we hope to explore ways to in-terleave semantic analysis with exploration of thelearning domain, by using the environment as asupervision signal for linguistic analysis.Acknowledgments We thank Gerald DeJong,Julia Hockenmaier, Alex Klementiev and theanonymous reviewers for their helpful feedback.This work is supported by DARPA funding underthe Bootstrap Learning Program and the BeckmanInstitute Postdoctoral Fellowship.966ReferencesAldous, David J.
1985.
Exchangeability and re-lated topics.
Lecture Notes in Math 1117:1?198.Branavan, S. R. K., Harr Chen, Luke Zettle-moyer, and Regina Barzilay.
2009.
Reinforce-ment learning for mapping instructions to ac-tions.
In Proceedings of the Joint Conferenceof the Association for Computational Linguis-tics and International Joint Conference on Nat-ural Language Processing Processing (ACL-IJCNLP 2009).
Singapore.Chen, David L. and Raymond J. Mooney.
2008.Learning to sportscast: A test of grounded lan-guage acquisition.
In Proceedings of 25th In-ternational Conference on Machine Learning(ICML 2008).
Helsinki, Finland, pages 128?135.Cumby, Chad and Dan Roth.
2003.
On kernelmethods for relational learning.
In Proceed-ings of the Twentieth International Conference(ICML 2003).
Washington, DC, pages 107?114.Downey, Doug, Oren Etzioni, and Stephen Soder-land.
2005.
A probabilistic model of redun-dancy in information extraction.
In Proceedingsof the International Joint Conference on Arti-ficial Intelligence (IJCAI 2005).
pages 1034?1041.Ferguson, Thomas S. 1973.
A bayesian analysisof some nonparametric problems.
The Annalsof Statistics 1(2):209?230.Ge, Ruifang and Raymond J. Mooney.
2005.
Astatistical semantic parser that integrates syn-tax and semantics.
In Proceedings of theNinth Conference on Computational NaturalLanguage Learning (CoNLL-2005).
Ann Arbor,MI, pages 128?135.Gilks, Walter R. 1995.
Markov Chain MonteCarlo in Practice.
Chapman & Hall/CRC.Goldwater, Sharon, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual dependencies in un-supervised word segmentation.
In Proceedingsof the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics(COLING-ACL 2006).
Sydney, Australia, pages673?680.Liang, Percy, Michael Jordan, and Dan Klein.2009.
Learning semantic correspondences withless supervision.
In Proceedings of the JointConference of the Association for Computa-tional Linguistics and International Joint Con-ference on Natural Language Processing Pro-cessing (ACL-IJCNLP 2009).
Singapore.Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model forparsing natural language to meaning representa-tions.
In Proceedings of Empirical Methods inNatural Language Processing (EMNLP 2008).Honolulu, Hawaii, pages 783?792.Morehead, Albert H. and Geoffrey Mott-Smith.1983.
The Complete Book of Solitaire and Pa-tience Games.
Bantam.Muggleton, Stephen.
1995.
Inverse entailment andprogol.
New Generation Computing Journal13:245?286.N?edellec, C., C. Rouveirol, H. Ad?e, F. Bergadano,and B. Tausend.
1996.
Declarative bias in ILP.In L. De Raedt, editor, Advances in InductiveLogic Programming, IOS Press, pages 82?103.Nivre, Joakim.
2006.
Inductive dependency pars-ing.
Springer.Richardson, Matthew and Pedro Domingos.
2006.Markov logic networks.
Machine Learning62:107?136.Roth, Dan and Wen-tau Yih.
2001.
Relationallearning via propositional algorithms: An infor-mation extraction case study.
In Proceedings ofthe International Joint Conference on ArtificialIntelligence (IJCAI 2001).
pages 1257?1263.Sethuraman, Jayaram.
1994.
A constructive def-inition of dirichlet priors.
Statistica Sinica4(2):639?650.Toutanova, Kristina and Christopher D. Manning.2000.
Enriching the knowledge sources usedin a maximum entropy part-of-speech tagger.In Proceedings of the Joint SIGDAT Confer-ence on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora(EMNLP/VLC-2000).
pages 63?70.Wong, Yuk Wah and Raymond J. Mooney.
2007.Learning synchronous grammars for semanticparsing with lambda calculus.
In Proceedings ofthe 45th Annual Meeting of the Association forComputational Linguistics (ACL 2007).
Prague,Czech Republic, pages 128?135.Zettlemoyer, Luke S. and Michael Collins.
2005.Learning to map sentences to logical form:Structured classification with probabilistic cat-egorial grammars.
In Proceedings of the 21stConference on Uncertainty in Artificial Intelli-gence (UAI 2005).
pages 658?666.967
