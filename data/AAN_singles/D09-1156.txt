Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1503?1512,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPCharacter-level Analysis of Semi-Structured Documents for Set ExpansionRichard C. WangLanguage Technologies InstituteCarnegie Mellon Universityrcwang@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon Universitywcohen@cs.cmu.eduAbstractSet expansion refers to expanding a par-tial set of ?seed?
objects into a more com-plete set.
One system that does set ex-pansion is SEAL (Set Expander for AnyLanguage), which expands entities auto-matically by utilizing resources from theWeb in a language-independent fashion.In this paper, we illustrated in detail theconstruction of character-level wrappersfor set expansion implemented in SEAL.We also evaluated several kinds of wrap-pers for set expansion and showed thatcharacter-based wrappers perform betterthan HTML-based wrappers.
In addition,we demonstrated a technique that extendsSEAL to learn binary relational concepts(e.g., ?x is the mayor of the city y?)
fromonly two seeds.
We also show that theextended SEAL has good performance onour evaluation datasets, which includesEnglish and Chinese, thus demonstratinglanguage-independence.1 IntroductionSEAL1(Set Expander for Any Language) is aset expansions system that accepts input ele-ments (seeds) of some target set S and automat-ically finds other probable elements of S in semi-structured documents such as web pages.
SEALis a research system that has shown good perfor-mance in previously published results (Wang andCohen, 2007).
By using only three seeds andtop one hundred documents returned by Google,SEAL achieved 90% in mean average precision(MAP), averaged over 36 datasets from three lan-guages: English, Chinese, and Japanese.
Un-like other published research work (Etzioni et al,2005), SEAL focuses on finding small closed sets1http://rcwang.com/sealof items (e.g., Disney movies) rather than largeand more open sets (e.g., scientists).In this paper, we explore the impact on perfor-mance of one of the innovations in SEAL, specif-ically, the use of character-level techniques to de-tect candidate regular structures, or wrappers, inweb pages.
Although some early systems forweb-page analysis induce rules at character-level(e.g., such as WIEN (Kushmerick et al, 1997) andDIPRE (Brin, 1998)), most recent approaches forset expansion have used either tokenized and/orparsed free-text (Carlson et al, 2009; Talukdar etal., 2006; Snow et al, 2006; Pantel and Pennac-chiotti, 2006), or have incorporated heuristics forexploiting HTML structures that are likely to en-code lists and tables (Nadeau et al, 2006; Etzioniet al, 2005).In this paper, we experimentally evaluateSEAL?s performance under two settings: 1) us-ing the character-level page analysis techniquesof the original SEAL, and 2) using page analy-sis techniques constrained to identify only HTML-related wrappers.
Our conjecture is that the lessconstrained character-level methods will producemore candidate wrappers than HTML-based tech-niques.
We also conjecture that a larger number ofcandidate wrappers will lead to better performanceoverall, due to SEAL?s robust methods for rankingcandidate wrappers.The experiments in this paper largely vali-date this conjecture.
We show that the HTML-restricted version of SEAL performs less well,losing 13 points in MAP on a dozen Chinese-language benchmark problems, 8 points in MAPon a dozen English-language problems, and 2points in MAP on a dozen Japanese-languageproblems.SEAL currently only handles unary relation-ships (e.g., ?x?
is a mayor).
In this paper, weshow that SEAL?s character-level analysis tech-niques can, like HTML-based methods, be read-1503ily extended to handle binary relationships.
Wethen demonstrate that this extension of SEAL canlearn binary concepts (e.g., ?x is the mayor ofthe city y?)
from a small number of seeds, andshow that, as with unary relationships, MAP per-formance is 26 points lower when wrappers arerestricted to be HTML-related.
Furthermore, wealso illustrate that the learning of binary conceptscan be bootstrapped to improve its performance.Section 2.1 explains how SEAL constructswrappers and rank candidate items for unary re-lations.
Section 3 describes the experiments andresults for unary relations.
Section 4 presents themethod for extending SEAL to handle binary re-lationships, as well as their experimental results.Related work is discussed in Section 5, and thepaper concludes in Section 6.2 SEAL2.1 Identifying Wrappers for UnaryRelationsWhen SEAL performs set expansion, it accepts asmall number of seeds from the user (e.g., ?Ford?,?Nissan?, and ?Toyota?).
It then uses a websearch engine to retrieve some documents thatcontain these instances, and then analyzes thesedocuments to find candidate wrappers (i.e., regu-lar structures on a page that contain the seed in-stances).
Strings that are extracted by a candidatewrapper (but are not equivalent to any seed) arecalled candidate instances.
SEAL then statisti-cally ranks the candidate instances (and wrappers),using the techniques outlined below, and outputs aranked list of instances to the user.One key step in this process is identifying can-didate wrappers.
In SEAL, a candidate wrapper isdefined by a pair of left and right character strings,` and r. A wrapper ?extracts?
items from a partic-ular document by locating all strings in the docu-ment that are bracketed by the wrapper?s left andright strings, but do not contain either of the twostrings.
In SEAL, wrappers are always learnedfrom, and applied to, a single document.Table 1 illustrates some candidate wrapperslearned by SEAL.
(Here, a wrapper is written as`[...]r, with the [...] to be filled by an extractedstring.)
Notice that the instances extracted bywrappers can and do appear in surprising places,such as embedded in URLs or in HTML tag at-tributes.
Our experience with these character-based wrappers lead us to conjecture that exist-ing heuristics for identifying structure in HTMLare fundamentally limited, in that many potentiallyuseful structures will not be identified by analyz-ing HTML structure only.SEAL uses these rules to find wrappers.
Eachcandidate wrapper `, r is a maximally long pair ofstrings that bracket at least one occurrence of ev-ery seed in a document: in other words, for eachpair `, r, the set of strings C extracted by `, r hasthe properties that:1.
For every seed s, there exists some c ?
C thatis equivalent to s; and2.
There are no strings `?, r?that satisfy property(1) above such that ` is a proper suffix of `?and r is a proper prefix of r?.SEAL?s wrappers can be found quite efficiently.The algorithm we use has been described previ-ously (Wang and Cohen, 2007), but will be ex-plained again here for completeness.
As an ex-ample, below shows a mock document, written inan unknown mark-up language, that has the seeds:Ford, Nissan, and Toyota located (and boldfaced).There are two other car makers hidden inside thisdocument (can you spot them?).
In this section,we will show you how to automatically constructwrappers that reveal them.GtpKxHnIsSaNxjHJglekuDialcLBxKHforDxkrpWNaCMwAAHOFoRduohdEXocUvaGKxHaCuRAxjHjnOxoTOyOTazxKHAUdIxkrOyQKxHToYotAxjHCRdmLxapuRAPprtqOVKxHfoRdxjHaJAScRFrlaFoRDofwNLWxKHtOYotaxkrHxQKlacXlGEKtxKHNisSanxkrEqGiven a set of seeds and a semi-structured doc-ument, the wrapper construction algorithm startsby locating all strings equivalent to a seed in thedocument; these strings are called seed instancesbelow.
(In SEAL, we always use case-insensitivestring matching, so a string is ?equivalent to?
anycase variant of itself.)
The algorithm then insertsall the instances into a list and assigns a unique idto each of them by its index in the list (i.e., the idof an instance is its position in the list.
)For every seed instance in the document, itsimmediate left character string (starting from thefirst character of the document) and right charac-ter string (ending at the last character of the docu-ment) are extracted and inserted into a left-contexttrie and a right-context trie respectively, where theleft context is inserted in reversed character or-der.
(Here, we implemented a compact trie called1504URL: http://www.shopcarparts.com/Wrapper: .html" CLASS="shopcp">[...] Parts</A> <br>Content: acura, audi, bmw, buick, cadillac, chevrolet, chevy, chrysler, daewoo, daihatsu, dodge, eagle, ford, ...URL: http://www.allautoreviews.com/Wrapper: </a><br> <a href="auto reviews/[...]/Content: acura, audi, bmw, buick, cadillac, chevrolet, chrysler, dodge, ford, gmc, honda, hyundai, infiniti, isuzu, ...URL: http://www.hertrichs.com/Wrapper: <li class="franchise [...]"> <h4><a href="#">Content: buick, chevrolet, chrysler, dodge, ford, gmc, isuzu, jeep, lincoln, mazda, mercury, nissan, pontiac, scion, ...URL: http://www.metacafe.com/watch/1872759/2009 nissan maxima performance/Wrapper: videos">[...]</a> <a href="/tags/Content: avalon, cars, carscom, driving, ford, maxima, nissan, performance, speed, toyotaURL: http://www.worldstyling.com/Wrapper: ?>[...] Accessories</option><option value=?Content: chevy, ford, isuzu, mitsubishi, nissan, pickup, stainless steel, suv, toyotaTable 1: Examples of wrappers constructed from web pages given the seeds: Ford, Nissan, Toyota.Patricia trie where every node stores a substring.
)Every node in the left-context trie maintains a listof ids for keeping track of the seed instances thatfollow the string associated with that node.
Samething applies to the right-context trie symmetri-cally.
Figure 1 shows the two context tries andthe list of seed instances when provided the mockdocument with the seeds: Ford, Nissan, and Toy-ota.Provided that the left and right context tries arepopulated with all the contextual strings of ev-ery seed instance, the algorithm then finds maxi-mally long contextual strings that bracket at leastone seed instance of every seed.
The pseudo-codefor finding these strings for building wrappers isillustrated in Table 2, where Seeds is the set ofinput seeds and ` is the minimum length of thestrings.
We observed that longer strings producehigher precision but lower recall.
This is an in-teresting parameter that is worth exploring, butfor this paper, we consider and use only a min-imum length of one throughout the experiments.The basic idea behind the pseudo-code is to firstfind all the longest possible strings from one triegiven some constraints, then for every such strings, find the longest possible string s?from anothertrie such that s and s?bracket at least one occur-rence of every given seed in a document.The wrappers constructed as well as the itemsextracted given the mock document and the exam-ple seeds are shown below.
Notice that Audi andAcura are uncovered (did you spot them?
).Wrapper: xKH[...]xkrContent: audi, ford, nissan, toyotaWrapper: KxH[...]xjHContent: acura, ford, nissan, toyotaWrappers MakeWrappers(Trie `, Trie r)Return Wraps(l, r) ?Wraps(r, l)Wrappers Wraps(Trie t1, Trie t2)For each n1?
TopNodes(t1, `)For each n2?
BottomNodes(t2, n1)For each n1?
BottomNodes(t1, n2)Construct a new Wrapper(Text(n1), Text(n2))Return a union of all wrappers constructedNodes BottomNodes(Trie t1, Node n?
)Find node n ?
t1such that:(1) NumCommonSeeds(n, n?)
== |Seeds|, and(2) All children nodes of n (if exist) fail on (1)Return a union of all nodes foundNodes TopNodes(Trie t, int `)Find node n ?
t such that:(1) Text(n).length ?
`, and(2) Parent node of n (if exist) fails on (1)Return a union of all nodes foundString Text(Node n)Return the textual string represented by thepath from root to n in the trie containing nInteger NumCommonSeeds(Node n1, Node n2)For each index i ?
Intersect(n1, n2):Find the seed at index i of seed instance listReturn the size of the union of all seeds foundIntegers Intersect(Node n1, Node n2)Return n1.indexes ?
n2.indexesTable 2: Pseudo-code for constructing wrappers.Table 1 shows examples of wrappers con-structed from real web documents.
We have alsoobserved items extracted from plain text (.txt),comma/tab-separated text (.csv/.tsv), latex (.tex),and even Word documents (.doc) of which thewrappers have binary character strings.
These ob-servations support our claim that the algorithm isindependent of mark-up language.
In our experi-mental results, we will show that it is independentof human language as well.1505Figure 1: The context tries and the seed instance list constructed given the mock document presented inSection 2.1 and the seeds: Ford, Nissan and Toyota.2.2 Ranking Wrappers and CandidateInstancesIn previous work (Wang and Cohen, 2007), wepresented a graph-walk based technique that iseffective for ranking sets and wrappers.
Thismodel encapsulates the relations between docu-ments, wrappers, and extracted instances (entitymentions).
Similarly, our graph also consists ofa set of nodes and a set of labeled directed edges.Figure 2 shows an example graph where each nodedirepresents a document, wia wrapper, and mian extracted entity mention.
A directed edge con-nects a node dito a wiif dicontains wi, a wito amiifwiextractsmi, and a dito amiif dicontainsmi.
Although not shown in the figure, every edgefrom node x to y actually has an inverse relationedge from node y to x (e.g., miis extracted by wi)to ensure that the graph is cyclic.We will use letters such as x, y, and z to denotenodes, and xr??
y to denote an edge from x toy with labeled relation r. Each node represents anobject (document, wrapper, or mention), and eachedge xr??
y asserts that a binary relation r(x, y)holds.
We want to find entity mention nodes thatare similar to the seed nodes.
We define the sim-ilarity between two nodes by random walk withrestart (Tong et al, 2006).
In this algorithm, towalk away from a source node x, one first choosesan edge relation r; then given r, one picks a targetnode y such that xr??
y.
When given a sourcenode x, we assume that the probability of pickingan edge relation r is uniformly distributed amongthe set of all r, where there exist a target node ysuch that xr??
y.
More specifically,Figure 2: Example graph built by Random Walk.P (r|x) =1|r : ?y xr??
y|(1)We also assume that once an edge relation r ischosen, a target node y is picked uniformly fromthe set of all y such that xr??
y.
More specifi-cally,P (y|r, x) =1|y : xr??
y|(2)In order to perform random walk, we will builda transition matrix M where each entry at (x, y)represents the probability of traveling one stepfrom a source node x to a target node y, or morespecifically,Mxy=?rP (r|x)P (y|r, x) (3)We will also define a state vector ~vtwhich rep-resents the probability at each node after iteratingthrough the entire graph t times, where one itera-tion means to walk one step away from every node.The state vector at t+ 1 iteration is defined as:~vt+1= ?~v0+ (1?
?
)M~vt(4)1506Since we want to start our walk from the seeds,we initialize v0to have probabilities uniformlydistributed over the seed nodes.
In each step ofour walk, there is a small probability ?
of tele-porting back to the seed nodes, which prevents usfrom walking too far away from the seeds.
Weiterate our graph until the state vector converges,and rank the extracted mentions by their probabil-ities in the final state vector.
We use a constant ?of 0.01 in the experiments below.2.3 Bootstrapping Candidate InstancesBootstrapping refers to iterative unsupervised setexpansion.
This process requires minimal super-vision, but is very sensitive to the system?s perfor-mance because errors can easily propagate fromone iteration to another.
As shown in previouswork (Wang and Cohen, 2008), carefully designedseeding strategies can minimize the propagated er-rors.
Below, we show the pseudo-code for ourbootstrapping strategy.stats?
?, used?
inputsfor i = 1 to M dom = min(3, |used|)seeds?
selectm(used) ?
top(list)stats?
expand(seeds, stats)list?
rank(stats)used?
used ?
seedsend forwhere M is the total number of iterations, inputsare the two initial input seeds, selectm(S) ran-domly selects m different seeds from the set S,used is a set that contains previously expandedseeds, top(list) returns an item that has the high-est rank in list, expand(seeds, stats) expandsthe selected seeds using stats and outputs accu-mulated statistics, and rank(stats) applies Ran-dom Walk described in Section 2.2 on the accu-mulated stats to produce a list of items.
Thisstrategy dumps the highest-ranked item into theused bucket after every iteration.
It starts by ex-panding two input seeds.
For the second iteration,it expands three seeds: two used plus one fromlast iteration.
For every successive iteration, it ex-pands four seeds: three randomly selected usedones plus one from last iteration.3 Experiments with Unary RelationsWe would like to determine whether character-based or HTML-based wrappers are more suitedfor the task of set expansion.
In order to do that,# L. Context [...]R. Context Eng Jap Chi Avg1 .+[...].+ 87.6 96.9 95.4 93.32 .*[<>].*[...].*[<>].
*85.7 96.8 90.7 91.13 .*>[...]<.
*85.7 96.7 90.7 91.04 .*<.+?>.*[...].*<.+?>.
*80.1 95.8 83.7 86.55 .*<.+?>[...]<.+?>.
*79.6 94.9 82.4 85.6Table 3: The performance (MAP) of various typesof wrappers on semi-structured web pages.we introduce five types of wrappers, as illustratedin Table 3.
The first type is the character-basedwrapper that does not have any restriction on thealphabets of its characters.
Starting from the sec-ond type, the allowable alphabets in a wrapper be-come more restrictive.
The fifth type requires thatan item must be tightly bracketed by two completeHTML tags in order to be extracted.All pure HTML-based wrappers are type 5, pos-sibly with additional restrictions imposed (Nadeauet al, 2006; Etzioni et al, 2005).
SEAL cur-rently does not use an HTML parser (or any otherkinds of parser), so restrictions cannot be easilyimposed.
As far as we know, there isn?t an agree-ment on what restrictions make the most senseor work the best.
Therefore, we evaluate perfor-mance for varying wrapper constraints from type1 (most general) to type 5 (most strict) in our ex-periments.For set expansion, we use the same evaluationset as in (Wang and Cohen, 2007) which contains36 manually constructed lists across three differ-ent languages: English, Chinese, and Japanese (12lists per language).
Each list contains all instancesof a particular semantic class in a certain language,and each instance contains a set of synonyms (e.g.,USA, America).Since the output of our system is a ranked listof extracted instances, we choose mean averageprecision (MAP) as our evaluation metric.
MAPis commonly used in the field of Information Re-trieval for evaluating ranked lists because it is sen-sitive to the entire ranking and it contains both re-call and precision-oriented aspects.
The MAP formultiple ranked lists is simply the mean value ofaverage precisions calculated separately for eachranked list.
We define the average precision of asingle ranked list as:AvgPrec(L) =|L|?r=1Prec(r)?
isFresh(r)Total # of Correct Instances1507where L is a ranked list of extracted instances, ris the rank ranging from 1 to |L|, Prec(r) is theprecision at rank r, or the percentage of correctsynonyms above rank r (inclusively).
isFresh(r)is a binary function for ensuring that, if a list con-tains multiple synonyms of the same instance (orinstance pair), we do not evaluate that instance (orinstance pair) more than once.
More specifically,the function returns 1 if a) the synonym at r is cor-rect, and b) it is the highest-ranked synonym of itsinstance in the list; it returns 0 otherwise.We evaluate the performance of each type ofwrapper by conducting set expansion on the 36datasets across three languages.
For each dataset,we randomly select two seeds, expand them bybootstrapping ten iterations (where each iterationretrieves at most 200 web pages only), and evalu-ate the final result.
We repeat this process threetimes for every dataset and report the averageMAP for English, Japanese, and Chinese in Ta-ble 3.
As illustrated, the more restrictive a wrapperis, the worse it performs.
As a result, this indicatesthat further restrictions on wrappers of type 5 willnot improve performance.4 Set Expansion for Binary Relations4.1 Identifying Wrappers for BinaryRelationsWe extend the wrapper construction algorithm de-scribed in Section 2.1 to support relational set ex-pansion.
The major difference is that we introducea third type of context called the middle contextthat occurs between the left and right contexts ofa wrapper for separating any two items.
We ex-ecute the same algorithm as before, except that aseed instance in the algorithm is now a seed in-stance pair bracketing some middle context (i.e.,?s1?middle?
s2?
).Given some seed pairs (e.g., Ford and USA),the algorithm first locates the seeds in some givendocuments.
For every pair of seeds located, it ex-tracts their left, middle, and right contexts.
Theleft and right contexts are inserted into their corre-sponding tries, while the middle context is insertedinto a list.
Every middle context is assigned a flagindicating whether the two instances bracketing itwere found in the same or reversed order as theinput seed pairs.
Every entry in the seed instancelist described previously now stores a pair of in-stances as one single string (e.g.
?Ford/USA?).
Anid stored in a node now matches the index of a pairof instances as well as a middle context.Shown below is a mock example document ofwhich the seed pairs: Ford and USA, Nissan andJapan, Toyota and Japan are located (and bold-faced).GtpKxHnIsSaNoKpjaPaNxjHJgleTuoLpBlcLBxKHforDEFcuSAxkrpWNapnIkAAHOFoRdawHDaUSauohdeQsKxHaCuRAoKpJapANxjHdIjWnOxoTOyOTaVaqjApaNzxKHAUdIEFcgErmANyxkrOyQKxHToYotAoKpJApaNxjHCRdmtqOVKxHfoRdoKpusAxjHaJASzEinSfrlaFoRDLMmpuSaofwNLWxKHtOYotaEFcjAPanxkrHxQKzrHpoKdGEKtxKHNisSanEFcJApAnxkrEqAfter performing the abovementioned proce-dures on this mock document, we now have con-text tries that are much more complicated thanthose illustrated in Figure 1, as well as a list ofmiddle contexts similar to the one shown below:id Seed Pairs r Middle Context0 Nissan/Japan No oKp1 Nissan/Japan No EFc2 Nissan/Japan Yes xkrHxQKzrHpoKd...4 Toyota/Japan No oKp6 Toyota/Japan Yes xjHdIjWnOxo9 Ford/USA No EFc13 Ford/USA Yes xkrpWNapnIkAAHOwhere r indicates if the two instances bracketingthe middle context were found in the reversed or-der as the input seed pairs.
In order to find themaximally long contextual strings, the ?Intersect?function in the set expansion pseudo-code pre-sented in Table 2 needs to be replaced with thefollowing:Integers Intersect(Node n1, Node n2)Define S = n1.indexes ?
n2.indexesReturn the largest subset s of S such that:Every index ?
s corresponds to same middle contextwhich returns those seed pairs that are bracketedby the strings associated with the two input nodeswith the same middle context.
A wrapper for re-lational set expansion, or relational wrapper, isdefined by the left, middle, and right contextualstrings.
The relational wrappers constructed fromthe mock document given the example seed pairsare shown below.
Notice that Audi/Germany andAcura/Japan are discovered.Wrapper: xKH[.1.]EFc[.2.
]xkrContent: audi/germany, ford/usa, nissan/japan,toyota/japanWrapper: KxH[.1.]oKp[.2.
]xjHContent: acura/japan, ford/usa, nissan/japan,toyota/japan1508Dataset ID Item #1 vs.
Item #2 Lang.
#1 Lang.
#2 Size Complete?US Governor US State/Territory vs.
Governor English English 56 YesTaiwan Mayor Taiwanese City vs. Mayor Chinese Chinese 26 YesNBA Team NBA Team vs. NBA Team Chinese English 30 YesFed.
Agency US Federal Agency Acronym vs. Full Name English English 387 NoCar Maker Car Manufacturer vs. Headquartered Country English English 122 NoTable 4: The five relational datasets for evaluating relational set expansion.Mean Avg.
Precision Precision@100Datasets 1 2 3 4 5 1 2 3 4 5US Governor 97.4 89.3 89.2 89.3 89.2 55 50 51 50 50Taiwan Mayor 99.8 95.6 94.3 91.3 90.8 25 25 24 23 23NBA Team 100.0 99.9 99.9 99.9 99.2 30 30 30 30 30Fed.
Agency 43.7 14.5 5.2 11.1 5.2 96 55 20 40 20Car Maker 61.7 0.0 0.0 0.0 0.0 74 0 0 0 0Average 80.5 59.9 57.7 58.3 56.9 56 32 25 29 25Table 5: Performance of various types of wrappers on the five relational datasets after first iteration.Mean Avg.
Precision Precision@100Datasets 1 2 3 4 5 1 2 3 4 5US Governor 98.9 97.0 95.3 94.1 93.9 55 55 54 53 53Taiwan Mayor 99.8 98.3 96.9 93.8 94.3 25 25 25 24 24NBA Team 100.0 100.0 99.2 98.4 98.6 30 30 30 30 30Fed.
Agency 65.5 54.5 27.9 55.3 30.0 97 97 61 95 69Car Maker 81.6 0.0 0.0 0.0 0.0 90 0 0 0 0Average 89.2 70.0 63.9 68.3 63.4 59 41 34 40 35Table 6: Performance of various types of wrappers on the five relational datasets after 10thiteration.4.2 Experiments with Binary RelationsFor binary relations, we performed the same ex-periment as with unary relations described in Sec-tion 3.
A relational wrapper is of type t if thewrapper?s left and right context match t?s con-straint for left and right respectively, and alsothat the wrapper?s middle context match both con-straints.For choosing the evaluation datasets for rela-tional set expansion, we surveyed and obtained adozen relationships, from which we randomly se-lected five of them and present in Table 4.
Eachdataset was then manually constructed.
For thelast two datasets, since there are too many items,we tried our best to make the lists as exhaustive aspossible.To evaluate relational wrappers, we performedrelational set expansion on randomly selectedseeds from the five relational datasets.
For everydataset, we select two seeds randomly and boot-strap the relational set expansion ten times.
Theresults after the first iteration are shown in Table 5and after the tenth iteration in Table 6.
When com-puting precision at 100 for each resulting list, wekept only the top-most-ranked synonym of everyinstance and remove all other synonyms from thelist; this ensures that every instance is unique.
No-tice that for the ?Car Maker?
dataset, there existsno wrappers of types 2 to 5; thus resulting in zeroperformance for those wrapper types.
In each ta-ble, the results indicate that character-based wrap-pers perform the best, while those HTML-basedwrappers that require tight HTML bracketing ofitems (type 3 and 5) perform the worse.In addition, the results illustrate that bootstrap-ping is effective for expanding relational pairs ofitems.
As illustrated in Table 6, the result of find-ing translation pairs of NBA team names is per-fect, and it is almost perfect for finding pairs ofU.S.
states/territories and governors, as well asTaiwanese cities and mayors.
In finding pairs ofacronyms and full names of federal agencies, theprecision at top 100 is nearly perfect (97%).
Theresults for finding pairs of car makers and coun-tries is good as well, with a high precision of90%.
For the last two datasets, we believe thatMAP could be improved by increasing the numberof bootstrapping iterations.
Table 7 shows someexample wrappers constructed and instances ex-tracted for wrappers of type 1.1509Seeds: kentucky / steve beshear, north dakota / john hoevenURL: http://wikifoia.pbworks.com/Alaska-Governor-Sarah-PalinWrapper: Governor [.2.]">[.1.]
GovernorURL: http://blogs.suntimes.com/sweet/2008/02/sweet state dinner for governo.htmlWrapper: <br /> <br /> The Honorable [.2.
], Governor of [.1.]
<br /> <br />URL: http://en.wikipedia.org/wiki/United States Senate elections, 2010Wrapper: " title="Governor of [.1.
]">Governor</a> <a href="/wiki/[.2.]"
title="URL: http://ballotbox.governing.com/2008/07/index.htmlWrapper: , [.1.
]?s [.2.
],Content: alabama / bob riley, alaska / sarah palin, arizona / janet napolitano, arkansas / mike huckabee, california /arnold schwarzenegger, colorado / bill ritter, connecticut / mary jodi rell, delaware / ruth ann minner, florida/ charlie crist, georgia / sonny perdue, hawaii / linda lingle, idaho / butch otter, illinois / rod blagojevich.
.
.Seeds: cia / central intelligence agency, usps / united states postal serviceURL: http://www1.american.edu/dccampus/links/whitehouse.htmlWrapper: <a href="http://www.[.1.
].gov" class="Links2nd">[.2.
]</a><span class="Links2nd">URL: http://www.usembassy.at/en/us/gov.htmWrapper: /" target=" blank">[.2.]
([.1.
])</a> -URL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agenciesWrapper: The [.2.]
([.1.])
isURL: http://www.nationmaster.com/encyclopedia/List-of-United-States-federal-agenciesWrapper: </li> <li>[.1.
]- <a href="/encyclopedia/[.2.]"
onmouseover="pv(event, 2Content: achp / advisory council on historic preservation, arc / appalachian regional commission, cftc / commod-ity futures trading commission, cia / central intelligence agency, cms / centers for medicare and medicaidservices, exim bank / export import bank of the united states, ntrc / national transportation research center.
.
.Seeds: mazda / japan, venturi / franceURL: http://www.jrfilters.com/filtres/index.php?lng=enWrapper: &page=filtres&lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.
])</option><option value="index.php?URL: http://www.jrfilters.com/suspensions/index.php?famille=1&lng=enWrapper: &lng=en">[.1.]&nbsp;&nbsp;&nbsp;([.2.
])</option><option value="index.php?famille=1&rubrique1URL: http://www.street-car.net/forums/forumdisplay.php?f=10Wrapper: "><strong>[.1.
]</strong></a> </div> <div class="smallfont">Country of origin:[.2.
].URL: http://www.allcarcentral.com/Wrapper: file.html">[.1.],[.2.
]</a><br />Content: abarth / italy, acura / japan, alfa romeo / italy, aston martin / england, auburn / usa, audi / germany, austinhealey / england, austin / england, auto union / germany, balwin / usa, bandini / italy, bentley / england, bmw/ germany, brabham / england, bricklin / usa, bristol / england, brm / england, bucciali / france.
.
.Table 7: Examples of (type 1) wrappers constructed and instances (contents) extracted.15105 Related WorkIn recent years, many research has been doneon extracting relations from free text (e.g., (Pan-tel and Pennacchiotti, 2006; Agichtein and Gra-vano, 2000; Snow et al, 2006)); however, al-most all of them require some language-dependentparsers or taggers for English, which restrictthe language of their extractions to English only(or languages that have these parsers).
Therehas also been work done on extracting relationsfrom HTML-structured tables (e.g., (Etzioni et al,2005; Nadeau et al, 2006; Cafarella et al, 2008));however, they all incorporated heuristics for ex-ploiting HTML structures; thus, they cannot han-dle documents written in other mark-up languages.Extracting relations at character-level fromsemi-structured documents has been proposed(e.g., (Kushmerick et al, 1997),(Brin, 1998)).In particular, Brin?s approach (DIPRE) is themost similar to ours in terms of expanding rela-tional items.
One difference is that it requiresmaximally-long contextual strings to bracket alseed occurrences.
This technique has been experi-mentally illustrated to perform worse than SEAL?sapproach on unary relations (Wang and Cohen,2007).
Brin presented five seed pairs of authornames and book titles that he used in the exper-iment (unfortunately, he did not provide detailedresults).
We input the top two seed pairs listed inhis paper into the relational SEAL, performed tenbootstrapping iterations (took about 3 minutes),and obtained 26,000 author name/book title pairsof which the precision at 100 is perfect (100%).6 ConclusionsIn this paper, we have described in detail an al-gorithm for constructing document-specific wrap-pers automatically for set expansion.
In the exper-imental results, we have illustrated that character-based wrappers are better suited than HTML-based wrappers for the task of set expansion.
Wealso presented a method that utilizes an additionalmiddle context for constructing relational wrap-pers.
We also showed that our relational set ex-pansion approach is language-independent; it canbe applied to non-English and even cross-lingualseeds and documents.
Furthermore, we have il-lustrated that bootstrapping improves the perfor-mance of relational set expansion.
In the future,we will explore automatic mining of binary con-cepts given only the relation (e.g., ?mayor of?
).7 AcknowledgmentsThis work was supported by the Google ResearchAwards program.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In In Proceedings of the 5th ACM Inter-national Conference on Digital Libraries, pages 85?94.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In In WebDB Work-shop at 6th International Conference on ExtendingDatabase Technology, EDBT98, pages 172?183.Michael J. Cafarella, Alon Y. Halevy, Daisy Z. Wang,Eugene W. 0002, and Yang Zhang.
2008.
Webta-bles: exploring the power of tables on the web.PVLDB, 1(1):538?549.A.
Carlson, J. Betteridge, E.R.
Hruschka Junior, andT.M.
Mitchell.
2009.
Coupling semi-supervisedlearning of categories and relations.
In NAACL HLTWorkshop on Semi-supervised Learning for NaturalLanguage Processing, pages 1?9.
Association forComputational Linguistics.Oren Etzioni, Michael J. Cafarella, Doug Downey,Ana-Maria Popescu, Tal Shaked, Stephen Soder-land, Daniel S. Weld, and Alexander Yates.
2005.Unsupervised named-entity extraction from theweb: An experimental study.
Artif.
Intell.,165(1):91?134.N.
Kushmerick, D. Weld, and B. Doorenbos.
1997.Wrapper induction for information extraction.
InProc.
Int.
Joint Conf.
Artificial Intelligence.David Nadeau, Peter D. Turney, and Stan Matwin.2006.
Unsupervised named-entity recognition:Generating gazetteers and resolving ambiguity.
InLuc Lamontagne and Mario Marchand, editors,Canadian Conference on AI, volume 4013 of Lec-ture Notes in Computer Science, pages 266?277.Springer.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for automat-ically harvesting semantic relations.
In ACL-44:Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 113?120, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In ACL ?06: Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the ACL, pages 801?808, Morristown, NJ, USA.
Association for Compu-tational Linguistics.1511Partha P. Talukdar, Thorsten Brants, Mark Liberman,and Fernando Pereira.
2006.
A context patterninduction method for named entity extraction.
InTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X).Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.2006.
Fast random walk with restart and its appli-cations.
In ICDM, pages 613?622.
IEEE ComputerSociety.Richard C. Wang and William W. Cohen.
2007.Language-independent set expansion of named enti-ties using the web.
In ICDM, pages 342?350.
IEEEComputer Society.Richard C. Wang and William W. Cohen.
2008.
Iter-ative set expansion of named entities using the web.In ICDM, pages 1091?1096.
IEEE Computer Soci-ety.1512
