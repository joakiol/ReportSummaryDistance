First Joint Conference on Lexical and Computational Semantics (*SEM), pages 11?19,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsAdaptive Clustering for Coreference Resolution with Deterministic Rulesand Web-Based Language ModelsRazvan C. BunescuSchool of EECSOhio UniversityAthens, OH 45701, USAbunescu@ohio.eduAbstractWe present a novel adaptive clustering modelfor coreference resolution in which the expertrules of a state of the art deterministic sys-tem are used as features over pairs of clus-ters.
A significant advantage of the new ap-proach is that the expert rules can be eas-ily augmented with new semantic features.We demonstrate this advantage by incorporat-ing semantic compatibility features for neutralpronouns computed from web n-gram statis-tics.
Experimental results show that the com-bination of the new features with the expertrules in the adaptive clustering approach re-sults in an overall performance improvement,and over 5% improvement in F1 measure forthe target pronouns when evaluated on theACE 2004 newswire corpus.1 IntroductionCoreference resolution is the task of clustering asequence of textual entity mentions into a set ofmaximal non-overlapping clusters, such that men-tions in a cluster refer to the same discourse entity.Coreference resolution is an important subtask ina wide array of natural language processing prob-lems, among them information extraction, questionanswering, and machine translation.
The availabil-ity of corpora annotated with coreference relationshas led to the development of a diverse set of super-vised learning approaches for coreference.
Whilelearning models enjoy a largely undisputed role inmany NLP applications, deterministic models basedon rich sets of expert rules for coreference have beenshown recently to achieve performance rivaling, ifnot exceeding, the performance of state of the artmachine learning approaches (Haghighi and Klein,2009; Raghunathan et al, 2010).
In particular, thetop performing system in the CoNLL 2011 sharedtask (Pradhan et al, 2011) is a multi-pass system thatapplies tiers of deterministic coreference sieves fromhighest to lowest precision (Lee et al, 2011).
ThePRECISECONSTRUCTS sieve, for example, createscoreference links between mentions that are foundto match patterns of apposition, predicate nomina-tives, acronyms, demonyms, or relative pronouns.This is a high precision sieve, correspondingly it isamong the first sieves to be applied.
The PRONOUN-MATCH sieve links an anaphoric pronoun with thefirst antecedent mention that agrees in number andgender with the pronoun, based on an ordering of theantecedents that uses syntactic rules to model dis-course salience.
This is the last sieve to be applied,due to its lower overall precision, as estimated ondevelopment data.
While very successful, this de-terministic multi-pass sieve approach to coreferencecan nevertheless be quite unwieldy when one seeksto integrate new sources of knowledge in order toimprove the resolution performance.
Pronoun reso-lution, for example, was shown by Yang et al (2005)to benefit from semantic compatibility informationextracted from search engine statistics.
The seman-tic compatibility between candidate antecedents andthe pronoun context induces a new ordering betweenthe antecedents.
One possibility for using compat-ibility scores in the deterministic system is to ig-nore the salience-based ordering and replace it withthe new compatibility-based ordering.
The draw-11back of this simple approach is that now discoursesalience, an important signal in pronoun resolution,is completely ignored.
Ideally, we would want touse both discourse salience and semantic compat-ibility when ranking the candidate antecedents ofthe pronoun, something that can be achieved natu-rally in a discriminative learning approach that usesthe two rankings as different, but overlapping, fea-tures.
Consequently, we propose an adaptive cluster-ing model for coreference in which the expert rulesare successfully supplemented by semantic compat-ibility features obtained from limited history web n-gram statistics.2 A Coreference Resolution AlgorithmFrom a machine learning perspective, the determin-istic system of Lee et al (2011) represents a troveof coreference resolution features.
Since the de-terministic sieves use not only information about apair of mentions, but also the clusters to which theyhave been assigned so far, a learning model that uti-lized the sieves as features would need to be ableto work with features defined on pairs of clusters.We therefore chose to model coreference resolu-tion as the greedy clustering process shown in Algo-rithm 1.
The algorithm starts by initializing the clus-tering C with a set of singleton clusters.
Then, aslong as the clustering contains more than one clus-ter, it repeatedly finds the highest scoring pair ofclusters ?Ci, Cj?.
If the score passes the threshold?
= f(?, ?
), the clusters Ci, Cj are joined into onecluster and the process continues with another high-est scoring pair of clusters.Algorithm 1 CLUSTER(X ,f )Input: A set of mentions X = {x1, x2, ..., xn};A measure f(Ci, Cj) = wT?
(Ci, Cj).Output: A greedy agglomerative clustering of X .1: for i = 1 to n do2: Ci ?
{xi}3: C ?
{Ci}1?i?n4: ?Ci, Cj?
?
argmaxp?P(C)f(p)5: while |C| > 1 and f(Ci, Cj) > ?
do6: replace Ci, Cj in C with Ci ?
Cj7: ?Ci, Cj?
?
argmaxp?P(C)f(p)8: return CThe scoring function f(Ci, Cj) is a linearlyweighted combination of features ?
(Ci, Cj) ex-tracted from the cluster pair, parametrized by aweight vector w. The function P takes a cluster-ing C as argument and returns a set of cluster pairs?Ci, Cj?
as follows:P(C)={?Ci, Cj?
| Ci, Cj?C, Ci 6=Cj}?{?
?, ??
}P(C) contains a special cluster pair ?
?, ?
?, where?
(?, ?)
is defined to contain a binary featureuniquely associated with this empty pair.
Its cor-responding weight is learned together with all otherweights and will effectively function as a clusteringthreshold ?
= f(?, ?
).Algorithm 2 TRAIN(C,T )Input: A dataset of training clusterings C;The number of training epochs T .Output: The averaged parameters w.1: w?
02: for t = 1 to T do3: for all C ?
C do4: w?
UPDATE(C,w)5: return wAlgorithm 3 UPDATE(C,w)Input: A gold clustering C = {C1, C2, ..., Cm};The current parameters w.Output: The updated parameters w.1: X ?
C1 ?
C2 ?
... ?
Cm = {x1, x2, ..., xn}2: for i = 1 to n do3: C?i ?
{xi}4: C?
?
{C?i}1?i?n5: while |C?| > 1 do6: ?C?i, C?j?
= argmaxp?P (C?)wT?
(p)7: B ?
{?C?k, C?l?
?
P(C?)
| g(C?k, C?l|C) >g(C?i, C?j |C)}8: if B 6= ?
then9: ?C?k, C?l?
= argmaxp?BwT?
(p)10: w?
w + ?
(C?k, C?l)?
?
(Ci, Cj)11: if ?C?i, C?j?
= ?
?, ??
then12: return w13: replace C?i, C?j in C?
with C?i ?
C?j14: return w12Algorithms 2 and 3 show an incremental learningmodel for the weight vector w that is parametrizedwith the number of training epochs T and a set oftraining clusterings C in which each clustering con-tains the true coreference clusters from one docu-ment.
Algorithm 2 repeatedly uses all true cluster-ings to update the current weight vector and insteadof the last computed weights it returns an averagedweight vector to control for overfitting, as originallyproposed by Freund and Schapire (1999).
The coreof the learning model is in the update procedureshown in Algorithm 3.
Like the greedy clustering ofAlgorithm 1, it starts with an initial system cluster-ing C?
that contains all singleton clusters.
At everystep in the iteration (lines 5?13), it joins the high-est scoring pair of clusters ?C?i, C?j?, computed ac-cording to the current parameters.
The iteration endswhen either the empty pair obtains the highest scoreor everything has been joined into only one cluster.The weight update logic is implemented in lines 7?10: if a more accurate pair ?C?k, C?l?
can be found,the highest scoring such pair is used in the percep-tron update in line 10.
If multiple cluster pairs obtainthe maximum score in lines 6 and 9, the algorithmselects one of them at random.
This is useful es-pecially in the beginning, when the weight vector iszero and consequently all cluster pairs have the samescore of 0.
We define the goodness g(C?k, C?l|C) of aproposed pair ?C?k, C?l?
with respect to the true clus-teringC as the accuracy of the coreference pairs thatwould be created if C?k and C?l were joined:g(?)
=???
{(x, y)?
C?k?C?l | ?Ci?C : x, y?Ci}??
?|C?k| ?
|C?l|(1)It can be shown that this definition of the goodnessfunction selects a cluster pair (lines 7?9) that, whenjoined, results in a clustering with a better pairwiseaccuracy.
Therefore, the algorithm can be seen astrying to fit the training data by searching for param-eters that greedily maximize the clustering accuracy,while overfitting is kept under control by comput-ing an averaged version of the parameters.
We havechosen to use a perceptron update for simplicity, butthe algorithm can be easily instantiated to accommo-date other types of incremental updates, e.g.
MIRA(Crammer and Singer, 2003).3 Expert Rules as FeaturesWith the exception of mention detection which isrun separately, all the remaining 12 sieves men-tioned in (Lee et al, 2011) are used as Boolean fea-tures defined on cluster pairs, i.e.
if any of the men-tion pairs in the cluster pair ?C?i, C?j?
were linkedby sieve k, then the corresponding sieve feature?k(C?i, C?j) = 1.
We used the implementation fromthe Stanford CoreNLP package1 for all sieves, with amodification for the PRONOUNMATCH sieve whichwas split into 3 different sieves as follows:?
ITPRONOUNMATCH: this sieve finds an-tecedents only for neutral pronouns it.?
ITSPRONOUNMATCH: this sieve finds an-tecedents only for neutral possessive pronounsits.?
OTHERPRONOUNMATCH: this is a catch-allsieve for the remaining pronouns.This 3-way split was performed in order to enablethe combination of the discourse salience featurescaptured by the pronoun sieves with the semanticcompatibility features for neutral pronouns that willbe introduced in the next section.
The OTHER-PRONOUNMATCH sieve works exactly as the orig-inal PRONOUNMATCH: for a given non-neutral pro-noun, it searches in the current sentence and the pre-vious 3 sentences for the first mention that agrees ingender and number with the pronoun.
The candi-date antecedents for the pronoun are ordered basedon a notion of discourse salience that favors syntac-tic salience and document proximity (Raghunathanet al, 2010).4 Discourse Salience FeaturesThe IT/SPRONOUNMATCH sieves use the same im-plementation for finding the first matching candi-date antecedent as the original PRONOUNMATCH.However, unlike OTHERPRONOUNMATCH and theother sieves that generate Boolean features, the neu-tral pronoun sieves are used to generate real valuedfeatures.
If the neutral pronoun is the leftmost men-tion in the cluster C?j from a cluster pair ?C?i, C?j?,the corresponding normalized feature is computedas follows:1http://nlp.stanford.edu/software/corenlp.shtml131.
Let Sj = ?S1j , S2j , ..., Snj ?
be the sequenceof candidate mentions that precede the neutralpronoun and agree in gender and number withit, ordered from most salient to least salient.2.
Let Ai ?
C?i be the set of mentions in the clus-ter C?i that appear before the pronoun and agreewith it.3.
For each mention m ?
Ai, find its rank in thesequence Sj :rank(m,Sj) = k ?
m = Skj (2)4.
Find the minimum rank across all the mentionsin Ai and compute the feature as follows:?it/s(C?i, C?j) =(minm?Airank(m,Sj))?1(3)If Ai is empty, set ?it/s(C?i, C?j) = 0.The discourse salience feature described above is bydefinition normalized in the interval [0, 1].
It takesthe maximum value of 1 when the most salient men-tion in the discourse at the current position agreeswith the pronoun and also belongs to the candidatecluster.
The feature is 0 when the candidate clusterdoes not contain any mention that agrees in genderand number with the pronoun.5 Semantic Compatibility FeaturesEach of the two types of neutral pronouns is associ-ated with a new feature that computes the semanticcompatibility between the syntactic head of a candi-date antecedent and the context of the neutral pro-noun.
If the neutral pronoun is the leftmost mentionin the cluster C?j from a cluster pair ?C?i, C?j?
and cjis the pronoun context, then the new normalized fea-tures ?it/s(C?i, C?j) are computed as follows:1.
Compute the maximum semantic similarity be-tween the pronoun context and any mention inC?i that precedes the pronoun and is in agree-ment with it:Mj = maxm?Aicomp(m, cj)2.
Compute the maximum and minimum seman-tic similarity between the pronoun context andany mention that precedes the pronoun and isin agreement with it:Mall = maxm?Sjcomp(m, cj)mall = minm?Sjcomp(m, cj)3.
Compute the semantic compatibility feature asfollows:?it/s(C?i, C?j) =Mj ?mallMall ?mall(4)To avoid numerical instability, if the over-all maximum and minimum similarities arevery close (Mall ?
mall < 1e?4) we set?it/s(C?i, C?j) = 1.Like the salience feature ?it/s, the semantic com-patibility feature ?it/s is normalized in the interval[0, 1].
Its definition assumes that we can computecomp(m, cj), the semantic compatibility between acandidate antecedent mention m and the pronouncontext cj .
For the possessive pronoun its, we ex-tract the syntactic head h of the mention m and re-place the pronoun with the mention head h in thepossessive context.
We use the resulting possessivepronoun context pcj(h) to define the semantic com-patibility as the following conditional probability:comp(m, cj) = logP (pcj(h)|h) (5)= logP (pcj(h))?
logP (h)To compute the n-gram probabilities P (pcj(h)) andP (h) in Equation 6, we use the language mod-els provided by the Microsoft Web N-Gram Cor-pus (Wang et al, 2010), as described in the next sec-tion.Figure 1 shows an example of a possessive neu-tral pronoun context, together with the set of can-didate antecedents that agree in number and genderwith the pronoun, from the current and previous 3sentences.
Each candidate antecedent is given an in-dex that reflects its ranking in the discourse saliencebased ordering.
We see that discourse salience doesnot help here, as the most salient mention is notthe correct antecedent.
The figure also shows the14In 1946, the nine justices dismissed a case[7] involvingthe apportionment[8] of congressional districts.
Thatview[6] would slowly change.
In 1962, the court[3]abandoned its[5] caution[4].
Finding remedies to theunequal distribution[1] of political power[2] was indeedwithin its constitutional authority.
[3] P (court?s constitutional authority | court)?
exp(?5.91)[5] P (court?s constitutional authority | court) (*)?
exp(?5.91)[7] P (case?s constitutional authority | case)?
exp(?8.32)[2] P (power?s constitutional authority | power)?
exp(?9.30)[8] P (app-nt?s constitutional authority | app-nt)?
exp(?9.32)[4] P (caution?s constitutional authority | caution)?
exp(?9.39)[1] P (dist-ion?s constitutional authority | dist-ion)?
exp(?9.40)[6] P (view?s constitutional authority | view)?
exp(?9.69)Figure 1: Possessive neutral pronoun example.compatibility score computed for each candidate an-tecedent, using the formula described above.
In thisexample, when ranking the candidate antecedentsbased on their compatibility scores, the top rankedmention is the correct antecedent, whereas the mostsalient mention is down in the list.When the set of candidate mentions contains pro-nouns, we require that they are resolved to a nominalor named mention, and use the head of this mentionto instantiate the possessive context.
This is the caseof the pronominal mention [5] in Figure 1, whichwe assumed was already resolved to the noun court(even if the pronoun [5] were resolved to an incor-rect mention, the noun court would still be rankedfirst due to mention [3]).
This partial ordering be-tween coreference decisions is satisfied automati-cally by setting the semantic compatibility feature?it/s(C?i, C?j) = 0 whenever the antecedent clusterC?i contains only pronouns.A similar feature is introduced for all neutralpronouns it appearing in subject-verb-object triples.The letter[5] appears to be an attempt[6] to calm theconcerns of the current American administration[7].
?Iconfirm my commitment[1] to the points made therein,?Aristide said in the letter[2], ?confident that they willhelp strengthen the ties between our two nations wheredemocracy[3] and peace[4] will flourish.?
Since 1994,when it sent 20,000 troops to restore Aristide to power,the administration ...[7] P (administration sent troops | administration)?
exp(?6.00)[2] P (letter sent troops | letter)?
exp(?6.57)[5] P (letter sent troops | letter)?
exp(?6.57)[4] P (peace sent troops | peace)?
exp(?7.92)[6] P (attempt sent troops | attempt)?
exp(?8.26)[3] P (democracy sent troops | democracy)?
exp(?8.30)[1] P (commitment sent troops | commitment)?
exp(?8.62)Figure 2: Neutral pronoun example.The new pronoun context pcj(h) is obtained byreplacing the pronoun it in the subject-verb-objectcontext cj with the head h of the candidate an-tecedent mention.
Figure 2 shows a neutral pro-noun context, together with the set of candidate an-tecedents that agree in number and gender with thepronoun, from an abridged version of the originalcurrent and previous 3 sentences.
Each candidateantecedent is given an index that reflects its rankingin the discourse salience based ordering.
Discoursesalience does not help here, as the most salient men-tion is not the correct antecedent.
The figure showsthe compatibility score computed for each candidateantecedent, using Equation 6.
In this example, thetop ranked mention in the compatibility based order-ing is the correct antecedent, whereas the most mostsalient mention is at the bottom of the list.To summarize, in the last two sections we de-scribed two special features for neutral pronouns:the discourse salience feature ?it/s and the seman-tic compatibility feature ?it/s.
The two real-valued15Candidate mentions Original context N-gram contextcapital, store, GE, side, offer with its corporate tentacles reaching GE?s corporate tentaclesAOL, Microsoft, Yahoo, product its substantial customer base AOL?s customer baseregime, Serbia, state, EU, embargo meets its international obligations Serbia?s international obligationscompany, secret, internet, FBI it was investigating the incident FBI was investigating the incidentgoal, team, realm, NHL, victory something it has not experienced since NHL has experiencedOnvia, line, Nasdaq, rating said Tuesday it will cut jobs Onvia will cut jobscoalition, government, Italy but it has had more direct exposure Italy has had direct exposurePinochet, arrest, Chile, court while it studied a judge ?s explanation court studied the explanationTable 1: N-gram generation examples.features are computed at the level of cluster pairs asdescribed in Equations 3 and 4.
Their computationrelies on the mention level rank (Equation 2) and se-mantic compatibility (Equation 6) respectively.6 Web-based Language ModelsWe used the Microsoft Web N-Gram Corpus2 tocompute the pronoun context probability P (pcj(h))and the candidate head probability P (h).
Thiscorpus provides smoothed back-off language mod-els that are computed dynamically from N-gramstatistics using the CALM algorithm (Wang and Li,2009).
The N-grams are collected from the tok-enized versions of the billions of web pages indexedby the Bing search engine.
Separate models havebeen created for the document body, the documenttitle and the anchor text.
In our experiments, weused the April 2010 version of the document bodylanguage models.
The number of words in the pro-noun context and the antecedent head determine theorder of the language models used for estimating theconditional probabilities.
For example, to estimateP (administration sent troops | administration), weused a trigram model for the context probabilityP (administration sent troops) and a unigram modelfor the head probability P (administration).
Sincethe maximum order of the N-grams available in theMicrosoft corpus is 5, we designed the context andhead extraction rules to return N-grams with sizeat most 5.
Table 1 shows a number of examplesof N-grams generated from the original contexts, inwhich the pronoun was replaced with the correct an-tecedent.
To get a sense of the utility of each con-text in matching the right antecedent, the table also2http://web-ngram.research.microsoft.comshows a sample of candidate antecedents.For possessive contexts, the N-gram extractionrules use the head of the NP context and its clos-est premodifier whenever available.
Using the pre-modifier was meant to increase the discriminativepower of the context.
For the subject-verb-objectN-grams, we used the verb at the same tense as inthe original context, which made it necessary to alsoinclude the auxiliary verbs, as shown in lines 4?7 inthe table.
Furthermore, in order to keep the gener-ated N-grams within the maximum size of 5, we didnot include modifiers for the subject or object nouns,as illustrated in the last line of the table.
Some ofthe examples in the table also illustrate the limits ofthe context-based semantic compatibility feature.
Inthe second example, all three company names areequally good matches for the possessive context.
Inthese situations, we expect the discourse saliencefeature to provide the additional information neces-sary for extracting the correct antecedent.
This com-bination of discourse salience with semantic com-patibility features is done in the adaptive clusteringalgorithm introduced in Section 2.7 Experimental ResultsWe compare our adaptive clustering (AC) approachwith the state of the art deterministic sieves (DT)system of Lee et al (2011) on the newswire portionof the ACE-2004 dataset.
The newswire section ofthe corpus contains 128 documents annotated withgold mentions and coreference information, wherecoreference is marked only between mentions thatbelong to one of seven semantic classes: person, or-ganization, location, geo-political entity, facility, ve-hicle, and weapon.
This set of documents has beenused before to evaluate coreference resolution sys-16System Mentions P R F1DT Gold, all 88.1 73.3 80.0AC Gold, all 88.7 73.5 80.4DT Gold, neutral 82.5 51.5 63.4AC Gold, neutral 83.0 52.1 64.0DT Auto, neutral 84.4 34.9 49.3AC Auto, neutral 86.1 40.0 54.6Table 2: B3 comparative results on ACE 2004.tems in (Poon and Domingos, 2008; Haghighi andKlein, 2009; Raghunathan et al, 2010), with the bestresults so far obtained by the deterministic sieve sys-tem of Lee at al.
(2011).
There are 11,398 annotatedgold mentions, out of which 135 are possessive neu-tral pronouns its and 88 are neutral pronouns it ina subject-verb-object triple.
Given the very smallnumber of neutral pronouns, in order to obtain re-liable estimates for the model parameters we testedthe adaptive clustering algorithm in a 16 fold cross-validation scenario.
Thus, the set of 128 documentswas split into 16 folds, where each fold contains 120documents for training and 8 documents for testing.The final results were pooled together from the 16disjoint test sets.
During training, the AC?s updateprocedure was run for 10 epochs.
Since the AC al-gorithm does not need to tune any hyper parameters,there was no need for development data.Table 2 shows the results obtained by the two sys-tems on the newswire corpus under three evaluationscenarios.
We use the B3 version of the precision(P), recall (R), and F1 measure, computed either onall mention pairs (all) or only on links that contain atleast one neutral pronoun (neutral) marked as a men-tion in ACE.
Furthermore, we report results on goldmentions (Gold) as well as on mentions extractedautomatically (Auto).
Since the number of neutralpronouns marked as gold mentions is small com-pared to the total number of mentions, the impacton the overall performance shown in the first tworows is small.
However, when looking at corefer-ence links that contain at least one neutral pronoun,the improvement becomes substantial.
AC increasesF1 with 5.3% when the mentions are extracted auto-matically during testing, a setting that reflects a morerealistic use of the system.
We have also evaluatedthe AC approach in the Gold setting using only theoriginal DT sieves as features, obtaining an F1 of80.3% for all mentions and 63.4% ?
same as DT ?for neutral pronouns.By matching the performance of the DT system inthe first two rows of the table, the AC system provesthat it can successfully learn the relative importanceof the deterministic sieves, which in (Raghunathanet al, 2010) and (Lee et al, 2011) have been manu-ally ordered using a separate development dataset.Furthermore, in the DT system the sieves are ap-plied on mentions in their textual order, whereas theadaptive clustering algorithm AC does not assumea predefined ordering among coreference resolutiondecisions.
Thus, the algorithm has the capability tomake the first clustering decisions in any section ofthe document in which the coreference decisions arepotentially easier to make.
We have run experimentsin which the AC system was augmented with a fea-ture that computed the normalized distance betweena cluster and the beginning of the document, but thisdid not lead to an improvement in the results, lend-ing further credence to the hypothesis that a strictlyleft to right ordering of the coreference decisions isnot necessary, at least with the current features.The same behavior, albeit with smaller increasesin performance, was observed when the DT and ACapproaches were compared on the newswire sectionof the development dataset used in the CoNLL 2011shared task (Pradhan et al, 2011).
For these exper-iments, the AC system was trained on all 128 docu-ments from the newswire portion of ACE 2004.
Ongold mentions, the DT and AC systems obtained avery similar performance.
When evaluated only onlinks that contain at least one neutral pronoun, in asetting where the mentions were automatically de-tected, the AC approach improved the F1 measureover the DT system from 58.6% to 59.1%.
One rea-son for the smaller increase in performance in theCoNLL experiments could be given by the differentannotation schemes used in the two datasets.
Com-pared to ACE, the CoNLL dataset does not includecoreference links for appositives, predicate nomi-nals or relative pronouns.
The different annotationschemes may have led to mismatches in the trainingand test data for the AC system, which was trainedon ACE and tested on CoNLL.
While we tried tocontrol for these conditions during the evaluationof the AC system, it is conceivable that the differ-17System Mentions P R F1DT Auto, its 86.0 46.9 60.7AC Auto, its 91.7 47.5 62.6Table 3: B3 comparative results on CoNLL 2011.ences in annotation still had some effect on the per-formance of the AC approach.
Another cause forthe smaller increase in performance was that thepronominal contexts were less discriminative in theCoNLL data, especially for the neutral pronoun it.When evaluated only on links that contained at leastone possessive neutral pronoun its, the improvementin F1 increased at 1.9%, as shown in Table 3.8 Related WorkClosest to our clustering approach from Section 2is the error-driven first-order probabilistic model ofCulotta et al (2007).
Among significant differenceswe mention that our model is non-probabilistic, sim-pler and easier to understand and implement.
Fur-thermore, the update step does not stop after thefirst clustering error, instead the algorithm learns anduses a clustering threshold ?
to determine when tostop during training and testing.
This required thedesign of a method to order cluster pairs in which theclusters may not be consistent with the true coref-erence chains, which led to the introduction of thegoodness function in Equation 1 as a new scoringmeasure for cluster pairs.
The strategy of contin-uing the clustering during training as long as a anadaptive threshold is met better matches the trainingwith the testing, and was observed to lead to betterperformance.
The cluster ranking model of Rahmanand Ng (2009) proceeds in a left-to-right fashion andadds the current discourse old mention to the highestscoring preceding cluster.
Compared to it, our adap-tive clustering approach is less constrained: it usesonly a weak, partial ordering between coreferencedecisions, and does not require a singleton cluster atevery clustering step.
This allows clustering to startin any section of the document where coreferencedecisions are easier to make, and thus create accu-rate clusters earlier in the process.The use of semantic knowledge for coreferenceresolution has been studied before in a number ofworks, among them (Ponzetto and Strube, 2006),(Bengtson and Roth, 2008), (Lee et al, 2011), and(Rahman and Ng, 2011).
The focus in these studieshas been on the semantic similarity between a men-tion and a candidate antecedent, or the parallelismbetween the semantic role structures in which thetwo appear.
One of the earliest methods for usingpredicate-argument frequencies in pronoun resolu-tion is that of Dagan and Itai (1990).
Closer to ouruse of semantic compatibility features for pronounsare the approaches of Kehler et al (2004) and Yanget al (2005).
The last work showed that pronounresolution can be improved by incorporating seman-tic compatibility features derived from search enginestatistics in the twin-candidate model.
In our ap-proach, we use web-based language models to com-pute semantic compatibility features for neutral pro-nouns and show that they can improve performanceover a state-of-the-art coreference resolution system.The use of language models instead of search enginestatistics is more practical, as they eliminate the la-tency involved in using search engine queries.
Web-based language models can be built on readily avail-able web N-gram corpora, such as Google?s Web 1T5-gram Corpus (Brants and Franz, 2006).9 ConclusionWe described a novel adaptive clustering methodfor coreference resolution and showed that it cannot only learn the relative importance of the origi-nal expert rules of Lee et al (2011), but also ex-tend them effectively with new semantic compati-bility features.
Experimental results show that thenew method improves the performance of the stateof the art deterministic system and obtains a sub-stantial improvement for neutral pronouns when thementions are extracted automatically.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful suggestions.
This work was supportedby grant IIS-1018590 from the NSF.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the author anddo not necessarily reflect the views of the NSF.18ReferencesEric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 294?303, Hon-olulu, Hawaii, October.
Association for ComputationalLinguistics.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991.Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for coreferenceresolution.
In Human Language Technologies 2007:The Conference of the North American Chapter of theAssociation for Computational Linguistics; Proceed-ings of the Main Conference, pages 81?88, Rochester,New York, April.
Association for Computational Lin-guistics.Ido Dagan and Alon Itai.
1990.
Automatic processingof large corpora for the resolution of anaphora refer-ences.
In Proceedings of the 13th conference on Com-putational linguistics - Volume 3, COLING?90, pages330?332.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1152?1161, Singapore, August.Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-sandr Simma.
2004.
The (non)utility of predicate-argument frequencies for pronoun interpretation.
InHLT-NAACL 2004: Main Proceedings, pages 289?296, Boston, Massachusetts, USA.
Association forComputational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the conll-2011 shared task.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28?34.Simone Paolo Ponzetto and Michael Strube.
2006.
Ex-ploiting semantic role labeling, wordnet and wikipediafor coreference resolution.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 192?199.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with markov logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Honolulu,Hawaii, October.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-sky, and Christopher D. Manning.
2010.
A multi-passsieve for coreference resolution.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?10), pages 492?501.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977, Singapore, Au-gust.
Association for Computational Linguistics.Altaf Rahman and Vincent Ng.
2011.
Coreference res-olution with world knowledge.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 814?824, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kuansan Wang and Xiaolong Li.
2009.
Efficacy of a con-stantly adaptive language modeling technique for web-scale applications.
In Proceedings of the 2009 IEEEInternational Conference on Acoustics, Speech andSignal Processing, ICASSP ?09, pages 4733?4736,Washington, DC, USA.
IEEE Computer Society.Kuansan Wang, Christopher Thrasher, Evelyne Viegas,Xiaolong Li, and Bo-june (Paul) Hsu.
2010.
Anoverview of microsoft web n-gram corpus and appli-cations.
In Proceedings of the NAACL HLT 2010Demonstration Session, HLT-DEMO ?10, pages 45?48, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2005.
Im-proving pronoun resolution using statistics-based se-mantic compatibility information.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 165?172.19
