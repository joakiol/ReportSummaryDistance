Redundancy in Collaborative DialogueMar i lyn  A.  Wa lkerUn ivers i ty  of  Pe imsy lvan ia ,  Computer  Sc ience Dept .
*Ph i lade lph ia ,  PA  19104lyn@l inc .c i s .upenn.edu1 In t roduct ionIt seems a perfectly valid rule of conversation ot totell people what they already know.
Indeed, Grice'sQUANTITY lllaxim has often been interpreted this way:Do not make your contribution more informative thanis required\[f\].
Stalnaker, as well, suggests that to assertsomething that is already presupposed is to attempt odo something that is already done\[14\].
Thus, the notionof what is informative is judged against a backgroundof what is presupposed, i.e.
propositions that all conver-sants assume are mutually known or believed.
Thesepropositions are known as the COMMON GROUND\[10, 5\].The various formulations of this 'no redundancy' rulepermeate many computational nalyses of natural lan-guage and notions of eooperativity.
However considerthe following excerpt from the middle of an advisorydialogue between IIarry (h), a talk show host, and Ray(r) his caller 1.Example 1 :(6 )  r .
uh 2 tax  qunst ionu .onu: s ince  April 81 we have had an85 year  old lother l i v ing  ~ith us.her only income has been social securityplus approximately $3000 from acertificate O~ deposit and i wonderwhatJs the situation as far asclaiming her as a dependent or doesthag income from the certificate ofdeposit rule her out as a dependent?
(7 )  h. yes  i t  does .
(8 )  r. IT DOES.
(9)  h. ?UP THAT KHflCKS HER OUT.In standard information theoretic terms, both (8) and(9) are REDUNDANT.
Harry's assertion in (9) simplyparaphrases what was said in (7) and (8) and so it*This researcll was partially funded by AltO grmat DAAL03-89-00031PRI and DARPA grant N00014-90~J-1863 at the Uni-vernity of Pem~ylvania, by Hewlett Packard, U.K., and by artNSF award for 1991 Summer \[rmtitute in Jalmn1 Thee examples come ft~m the talk show for financial advice,6'peaking o/ Your Money, on WCAU in Philadelphia.
This col~pus w~s collected and transcribed by Marth~t Pollack anti JuliaHirschberg\[12\].cannot be adding beliefs to the cmnmon ground 2.
Fur-thermore, the truth of (9) cannot be in question, for in-stead of 19), \[ larry could not say Yup, but lhat doesn'tknock her out.
So why does Ray (r) in (8) RF~PEATHarry's (h) assertion of it does, and why does lfarryPARAPHRASE himself and Kay in (9)?My claim is that mformationally redundant utterances(IRU's) have two main discourse functions: (1) to pro-vide EVIUENCI~ to support he assumptions underlyingthe inference of mutual beliefs, (2) to CENTER a propo-sition, is.
make or keep a proposition salient\[6\].
Thispaper will focus on (1) leaving (2) for future work.First consider the notion of evidence.
One reason whyagents need EVIDENCE for beliefs is that they only havepartial information about: (1) the state of world; (2)the effects of actions; (3) other agent's beliefs, prefer-ences and goals.
This is especially true when it comesto modelling the effects of linguistic actions.
Linguisticactions are different han physical actions.
An agent'sprior beliefs, preferences and goals cannot be ascer-tained by direct inspection.
This means that it isdifficult for the speaker to verify when an action hasachieved its expected result, and so giving and receiv-ing evidence is critical and the process of establishingmutual beliefs is carefully monitored by the conver-sants.The characterization f IRU's ms informationally re-dundant follows from an axiomatization of action indialogue that I will call the DETERMIniSTIC MODEL.This model consists of a number of simplifying assump-tions such as: (I) Propositions are are either believedor not believed, (2) Propositions representing beliefsand intentions get added to tim context by the unilat-eral action of one conversant, (3) Agents are logicallyormfiscient.
(4) The context of a disconrse is an undif-ferentiated set of propositions with no specific relationsbetween them.
I claim that these assumptions nmst bedropped in order to explain the function of IRU's indialogue.Section 2 discusses assumption (1); section 3 shows howassmnption (2) can be dropped; section 4 discusses (3);section 4.1 shows that some IRU's facilitate the infer-ence of relations between adjacent propositions.2\[8) is not realized with a rising question intonation.
Thiswill be discussed in sectiott 6.1.A~\]~s DE COLING-92, NAINrFES, 23-28 AO~" 1992 3 4 5 PROC.
OF COLING-92, NANTES, AUG. 23-28, 19922 Mutua l  Be l ie fs  in a SharedEnv i ronmentThe account proposed here of how the COMMONGROUND is augmented, is based is Lewis's SHARED EN-VIRONMENT model for common knowledge\[10, 2\].
Inthis model, mutual beliefs depend on evidence, openlyavailable to the conversants, plus a number of under-lying assumptions.Shared Environment Mutua l  Bel ief  In-duct ion  SchemaIt is mutually believed in a population P thatql if and only if some situation ~q holds suchthat:1.
Everyone in P has reason to believe that,q holds.2.
3 indicates to everyone in P that every-one in P has reason to believe that 8holds.3.
S indicates to everyone in P that @.The situation ~q, used above in the mutual belief in-duction schema, is the context of what has been said.This schema supports a weak model of mutual be-liefs, that is more akin to mutual assumptions or mu-tual suppositions\[13\].
Mutual beliefs can be inferredbased on some evidence, but these beliefs may dependon underlying assumptions that are easily defensible.This model can be implemented using Gallier's theoryof autonomous belief revision and the correspondingsystem\[4\].A key part  of this model is that some types ofevidence provide better support for beliefs thanother types.
The types of evidence considered arecategorized and ordered based on the source ofthe evidence: hypothes is  < defau l t  < in ference< l ingu is t i c  < phys ica l (See \[2, 4\]).
This orderingreflects the re lat ive  defeasibility of different assump-tions.
Augmenting the strength of an assumption thusdecreases its relative defensibility.A claim of this paper i8 that one role of IRU's is to en-sure that these assumptions are supported by evidence,thus decreasing the defensibility of the mutual beliefsthat depend on them\[4\].Thus mutual beliefs depend on a defensible inferenceprocess.
All inferences depend on the evidence to sup-port them, and stronger evidence can defeat weaker ev-idence.
So a mutual belief supported as an inferencecan get defeated by l ingu is t i c  information.
In addi-tion, I adopt an an assumption that a chain of reason-ing is only as strong as its weakest link:Weakest  L ink Assumpt ion :  The strengthof a belief P depending on a set of under-lying assumptions al,...an is MIN(Strength(a,, ...4,))This seems intuitively plausible and means that thestrength of belief depends on the strength of underly-ing assumptions, and that for all inference rules thatdepend on multiple premises, the strength of an in-ferred belief is the weakest of the supporting beliefs.This representation of mutual belief differs fromthe common representation i terms of an iteratedeonjunction\[ll\] in that: (1) it relocates informationfrom mental states to the environment in which utter-anees occur; (2) it allows one to represent the differentkinds of evidence for mutual  belief; (3) it controls rea-soning when discrepancies in mutual beliefs are discov-ered since evidence and assumptions can be inspected;(4) it does not consist of an infinite list of statements.3 Inference of UnderstandingThis section examines the assumption from the DETER-MINISTIC MODEL that: (2) Propositions representingbeliefs and intentions get added to the context by theunilateral action of one conversant 3.
This assumptionwill also be examined in section 5.The key claim of this section is that agents monitor theeffects of their utterance actions and that the next ac-tion by the addressee is taken as evidence of the effectof the speaker's utterance 4.
That  the utterance willhave the intended effect is only a hypothes is  at thepoint where the utterance has just been made, irrespec-tive of the intentions of the speaker.
This distinguishesthis account from others that assume ither that utter-ance actions always ucceed or that they succeed unlessthe addressee previously believed othecwise\[ll, 8\].I adopt the assumption that the participants in a dia-logue are trying to achieve some purpose\[7\].
Some as-pects of the structure of dialogue arises from the struc-ture of these purposes and their relation to one another.The minimal purpose of any dialogue is that an utter-ance be understood, and this goal is a prerequisite toachieving other goals in dialogue, such as commitmentto future action.
Thus achieving mutual  belief of un-derstanding is an instance of the type of activity thatagents must perform as they collaborate to achieve thepurposes of the dialogue.
I claim that a model of theachievement of mutual belief of understanding can heextended to the achievement of other goals in dialogue.Achieving understanding is not unproblematic, it isa process that must be managed, just as other goalachieving processes are\[3\].
Inference of mutual under-standing relies upon some evidence, e.g.
the utterancethat is made, and a number of underlying assumptions.The assumptions are given with the inference rule be-low.say(it, B, u, p) - -A->aThis is an utterance action version of the STRIPSassumption.4Except for circumstances where it is clear that the flow ofthe conversation has been interrupted.AcrEs DE COLING-92, NANTES, 23-28 AOI~T 1992 3 4 6 Pgoc.
OF COLING-920 NANTES, AUG. 23-28, 1992Next AssumptionUtterance addressedPROMPT attentionREPEAT hearing., attentionPARAPHRASE realize, hearing, attentionINFERENCE license, realize, hearing, attentionIMPLICATURE license, realize, hearing, attentionANY Next copresence linguisticUtterance license, realize, hearing , attention defau l t~Evidenceqype~l ingu~linguistic Ilinguistic \]linguist)c~linguistic \]Figure 1: How tbe Addressee's Following utterance upgrades the evidence underlying assumptionsunderstand(B, u, p) \[evidence-type\]Assumptions =eopresent(h, B, u) \[evidence-type\]attend(B, U) \[evidence-type\]hear(B, u) \[evidence-type\]bel(B, realize(u, p)) \[evidence-type\]This schema means that when A says u to B intendingto convey p, that this leads to the mutual belief thatB understands u as p under certain assumptions.
Theassumptions are that A and B were cnpresent, hat Bwas attending to the utterance vent, that B heard theutterance, and that B believes that the utterance urealizes tim intended meaning p.The \[evidence-typeJ annotation indicates thestrength of evidence supporting the assumption.
All ofthe assumptions start out supported by no evidence;their evidence type is therefore hypothesis.
It isn'tuntil af ter  the addressee's next action that an assump-tion can have its strength modified.The claim here is that one class of IRU's addressesthese assumptions underlying the inference of mutualunderstanding.
Each type of IRU, the assumption ad-dressed and the evidence type provided is given in Fig-ure 1.
Examples are provided in sections 3.1 and 3.2.It is also possible that A inteuds that BY saying u,which realizes p, B should make a certain inference q.Then B's understanding of u should include B makingthis inference.
This adds an additional assumption:bel(B, license (p, q)) \[evidence-typeJThus assuming that q was inferred relies on the as-sumption that B believes that p licenses q in the con-text.Figure 1 says that prompts, repetitions, paraphrasesand making inferences explicit all provide linguistic ev-idence of attention.
All that prompts uch as sh huhdo is provide evidence of attention.
However repeti-tions, paraphrases and making inferences explicit alsodemonstrate complete hearing.
In addition, a para~phrase and making an inference xplicit provides lin-guistic evidence of what proposition the paraphraserbelieves the previous utterance realizes.
Explicit infer-ences additionally provide evidence of what inferencestile inferrer believes the realized proposition licenses inthis context.Ill each case, the IRU addresses one or moresumptions that have to be made in order to inferthat mutual understanding has actually been achieved.The assumption, rather than being a hypothesi~or a defau l t ,  get upgraded to a support type ofl ingu is t i c  as a result of the IRU.
The fact thatdifferent II~U's address different assumptions leads tothe perception that some 1KU's are better evidencefor understanding than others, e.g.
a PARAPHRASE i8stronger evidence of understanding than a REPEAT\[3\].In addition, any next utterance by the addressee canupgrade the strength of the underlying assumptions todefau l t  (See Figure 1).
Of course de fau l t  evidence isweaker than l ingu is t i c  evidence.
The basis for thesedefault inthrences will be discussed in section 5.3.1 Example  o f  a Repet i t ionConsider example 1 ill section 1.
Ray, in (8), repeatsIIarry's assertion from (7).
This upgrades the evidencefor tile assumptions ofhearing and attention associatedwith utterance (7) from hypothes is  to X inguist ic .The assumption about what proposition p7 is realizedby u7 remains a defau l t .
This instantiates the infer-ACRES DE COL1NG-92, NANTI~S, 23-28 Aotrr 1992 3 4 7 Paoc.
OF COLING-92, NAIVrr~s, Auc;, 23-28.
1992ence rule for understanding asfollows:say(harry ,  ray,  uT, pT) - - I ->understand(Ray, u7, p7) \ [defau l t \ ]Assunptiona :{ eoprseant(harry, ray, u7) \[linguistic\]s t rand( ray ,  u7) \[linguistic\]hear(ray, uT) \[linguistic\]bel(ray, realize(uT, pT)) \[default\]}Because of the WEAKEST LINK assumption, the beliefabout understanding is still a default.3.2 Example of a ParaphraseThis assumption is challenged by a number of casesin naturally occurring dialogues where inferences thatfollow from what has been said are made explicit.
Irestrict he inferences that I discuss to those that are(a) based on information explicitly provided in the di-alogue or, (b) licensed by applications of Gricean Max-ims such as scalar implicature inferences\[9\].For example the logical omniscience assumption wouldmean that if l(a) and (b) below are in the context, then(c) will be as well since it is entailed from (a) and (b).
(1) a.
You can buy an I It A if and only if you doNOT have an existing pension plan,b.
You have an existing pension plan.c.
You cannot buy an I It A,Consider the following excerpt:Exawple 2:(18) h. i see .
are there  any uther  ch i ld renbes ide  your g i la?
(19) d. no(20) h. YOUR WIFE IS AN OILY CHILD(21) d. right, and uh wants to giveher some secur i ty  .
.
.
.
.
.
.
.
.
.Harry's utterance of (20) is said with a falling intona-tional contour and hence is unlikely to be a question.This utterance results in an instantiation of the infer-ence rule as follows:say(harry ,  ray,  u20, p20) --l->understand(Ray, u20, p20) \[linguistic\]Assumptions ={ copretent (haxry ,  ray,  uT) \ [ l ingu is t i c \ ]ate end(ray, u7) \[linguistic\]hear(ray, u7) \[linguistic\]bel(ray, realize(.7, pT)) \[linguistic\]}In this ease, the belief about understanding is sup-ported by l ingu is t i c  evidence since all of the sup-porting assumptions are supported by linguistic evi-dence.
Thus a paraphrase provides excellent evidencethat an agent actually understood what another agentmeant.In addition, these IItU's leave a proposition salient,where otherwise the discourse might have moved on toother topics.
This is part of the CENTERING functionof IKU's and is left to future work.4 Making Inferences ExplicitThis section discusses assumption (3) of the determisticmodel, namely that: Agents are logically omniscient.The following excerpt demonstrates this structure.
Ut-terance (15) realizes la, utterance (16) realizes lb, andutterance (17) makes the inference xplicit hat is givenin lc for the particular tax year of 1981.Example 3:(18) h. oh no.I E A'e were availableas long as you are not a participantin an existing pension(as)  j .
oh i see.well i did uork i do uork for acompany that has a pension(17) h. ahh.
THEN YOU'RE NOT ELIGIBLEFOR EIGHTY ONE(18) j .
i see, but i am for  82After (16), since the propositional content of (17) isinferrable, the assumption that Harry has made thisinference is supported by the inf  arance evidence type:bel(H, lieense(p16, p17)) \[inference\]According to the model of achieving mutual under-standing that was outlined in section 3, utterance (17)provides l ingu is t i c  evidence that I larry (h) believesthat the proposition realized by utterance (16) licensesthe inference of (17) in this context.hal(H, license(pl6, p17)) \[linguistic\]Furthermore, the context here consists of a discussionof two tax years 1981 and 1982.
Utterance (17) selects*ighLy one, with a narrow focus pitch accent.
Thisimplicates that there is some other tax year for whichJoe is eligible, namely 198219\].
Joe's next utterance,but I am for 82, reinforces the implicature that Harrymakes in (17), and upgrades the evidence underlyingthe assumption that (17) licenses (18) to l ingu is t i c .ACIES DE COLING-92, NANTES, 23-28 AO~r 1992 3 4 8 PROC.
OF COLING-92, NANTES, AUG. 23-28, 19924.1 Supporting InferencesA subcase of ensuring that certain inferences get madeinvolves the juxtaposition of two propositions.
Thesecases challenge the assumption that: (4) The contextof a discourse is an undifferentiated set of propositionswith no specific relations between them.
While thisassumption is certainly not made in most discoursemodels, it is often made in semantic models of thecontext\[14\].
In the following segment, Jane (j) de-scribes her financial situation to I iarry (h) and a choicebetween a setthment and an annuity.Example %:( l )  j .
hello harry ,  my name is jane(2: )  h. welcome jane(3)  j. i just retired december first,and in addition to my pension andsocial secur i ty ,  I have asupplemental annuity(4)  h. yes(5)  j .
which i cont r ibuted  towhile i was employed(8 )  h. right(7 )  j .
from the s ta te  of IJ mutual fund.and ISm ent i t led  to a lump sumset t lement  which would be betueeni6,800 and 17,800, or a lesser lifeannu i ty ,  and the cho ices  of the annui tyum would be $128.45 per month.That would be the maximumwith no beneficiaries(8)  h. You can stop right there:take your money.
(9 )  j .
take the  money.
(10) h. absolutely.YOU'RE ONLY GETTING 1500 A YEAR.at 17,000, no trouble at all ~oget  10 percent  on 17,000 bucks.I iarry interrupts her at (8) since he believes he hasenough information to suggest a course of action, andtells her lake ~lonr money.
To provide SUPPORT for thiscourse of action he produces an inference that followsfrom what she has told him in (7), namely You're onlygelling 1500 (dollars) a year.
SUPPORT is a generalrelation that holds between beliefs and intentions inthis model.Presumably Jane would have no trouble calculatingthat $125.45 a month for 12 months amounts to a littleover $1500 a year, and thus can easily accept his state-ment that is intended to provide the necessary SUP-PORT relation, ie.
the juxtapt~ition of this fact againstthe advice to lake the money conveys that the fact thatshe is only getting 1500 dollars a year is a reason forher to adopt the goal of taking the money, althoughthis is not explicitly stated.5 Ev idence  o f  AcceptanceIn section 3, I examine the assumption that: (2) Propo-sitions representing beliefs and intentions get added tothe context by the unilateral action of one conversant.I suggested that this assumption can be replaced byadopting a model in which agents' behavior providesevidence for whether or not mutual understanding hasbeen achieved.
I also discussed some of the effects ofresource bounds, is.
eases of ensuring that or providingevidence that certain inferences dependent on what issaid are made.Achieving understanding and compensating for re-source bounds are issues for a model of dialoguewhether or not agents are autonomous.
But agents' au-tonomy means there are a number of other reasons whyA's utterance to B conveying a proposition p might notachieve its intended effect: (1) p may not cohere withB's beliefs, (2) B may not think that p is relevant, (3) Bmay believe that p does not contribute to the commongoal, (4) B may prefer doing or believing some q wherep is mutually exclusive with q, (5) If p is about an ac-tion, B may want to partially modify p with additionalconstraints about how, or when p,Therefore it is important o distinguish an agent actu-ally ACCEPTING the belief that p or intending to per-form an action described by p from merely understand-ing that p was conveyed.
Other accounts legislate thathelpful agents should adopt other's beliefs and inten-tions or that acceptance depends on whether or not theagent previously believed ~ Pi l l ,  8\].
But agents candecide whether as well as how to revise their beliefs\[4\].Evidence of acceptance may be given explicitly, butacceptance can be inferred in sonm dialogue situationsvia the operation of a simple principle of cooperativedialogueS:COLLABORATIVE PRINCIPLE: Conversantsmust provide evidence of a detected iscrep-ancy in belief as soon as possible.This principle claims that evidence of conflict shouldbe made apparent in order to keep defau l t  infer-ences about acceptance or understanding from go-ing through.
1RU's such as PROMPTSp REPETITIONS~PARAP|IRASES, and making an INFERENCE explicit can-not function as evidence for conflicts in beliefs orintentions via their propositional content since theyare informationally redundant.
If they are realizedwith question intonation, the inference of acceptanceis blocked.In the dialogue below between tiarry (b) and Ruth (r),Ruth in (39), first ensures that she understood Harrycorrectly, and then provides explicit evidence of non-acceptance in (41), based on her autonomous prefer-ences about how her money is invested..STiffs is a simplification of the COLLABOnATIVE PLANNINGPRINC~PLE~ described in \[15\].Ac'rEs DE COLING-92, NANTES, 23-28 Ao~-r 1992 3 4 9 PROC.
OF COLING-92, NANTES, AUG. 23-28.
1992Exa~le  5 :(38)  h. and I 'd  l i ke  1K thouwand in  a2 and a ha l f  year  cer t i f i ca te(39) r. the full  18 in a 2 and a half?
(40) h. that's correct(41) r. GEE.
NOT AT MY AGEIn the following example, Joe in (14) makes a statementthat provides propositional content hat conflicts withHarry's statement in (13) and thus provides evidenceof non-acceptance.Exmaple 6(13)  h. and - -  there 's  no reason  why youshou ldn '~ have an I It h fo r  las t  year(14)  j .
WELL I THOUGHT TSEY JUST ST?RTEDTHIS YEaRJoe's statement is based on his prior beliefs.
In bothof these cases this evidence for conflict is given im-mediately.
However when there is no evidence to thecontrary s, and goals of the discourse require achieve-ment of acceptance, inferences about acceptance arelicensed as de fau l t .
They can he defeated later bystronger evidence.Without this principle, a conversant might not bringup an objection until much later in the conversation,at which point the relevant belief and some inferencesfollowing from that belief will have been added to thecommon ground as dtafanlts .
The result of this is thatthe retraction of that belief results in many beliefs be-ing revised.
The operation of this principle helps con-versants avoid replanning resulting from inconsistencyin beliefs, and thus provides a way to manage the aug-mentation of the common ground efficiently.6 Other  hypothesesThe first point to note is that the examples here areonly a subset of the types of IRU's that occur in dia-logues.
I use the term antecedent to refer to the mostrecent utterance which should have added the proposi-tion to the context.
This paper has mainly focused oncases where the IRU: (1) is adjacent o its antecedent,rather than remote; (2) realizes a proposition whose an-tecedent was said by another conversant, (3) has onlyone antecedent.
It is with respect o this subset of thedata that the alternate hypotheses are examined.A distributional nalysis of a subBet of the corpus (171IKU's from 24 dialogues consisting of 976 turns), on therelation of an IRU to its antecedent and the context,shows that  35% of the tokens occur remotely from theirantecedents, that 32% have more than one antecedent,that 480?
consist of the speaker epeating somethingthat he said before and 52% consist of the speaker e-peating something that the other conversant said.
SosThls displaying of evidence to the contrary was called satinterruption i \[15\].the data that this paper focuses on accounts for about30% of the data.6.1 Indirect Question HypothesisIn example (1) of section 1, an alternative account ofRay's repetition in (8) is that it is a question of somekind.
This raises a number of issues: (i) Why doesn't ithave the form of a question?, (2) What is it a questionabout?, and (3) Why is it never denied?.Of 171 IRU's, only 28 are realized with rising ques-tion intonation.
Of these 28, 6 are actually redundantquestions with question syntax, and 14 are followed byaffirmations.If these are generally questions, then one possible an-swer to what the question is about is that Ray is ques-tioning whether he actually heard properly.
But thenwhy doesn't he use an intonational contour that con-veys this fact as Ruth does in example 5?
On an ef-ficiency argument, it is hard to imagine that it wouldhave cost Ray any more effort to have done so.Finally, if it were a question it would seem that itshould have more than one answer.
While 50 of theseIRU's are followed by an affirmation such as that's cot.reef, right, yup, none of them are ever followed by adenial of their content.
It seems an odd question thatonly has one answer.6.2 Dead Air HypothesisAnother hypothesis i that  IRU's result from the radiotalk show environment in which silence is not tolerated.So agents produce IRg 's  because they cannot think ofanything else to say but feel as though they must saysomething.The first point to note is that IRU's actually occurin dialogues timt aren't on the radio\[l\].
The secondquestion is why an agent would produce an IRU, ratherthan some other trivial statement such as I didn't knowthai.
Third, why don't these utterance correlate withtypical stalling behavior such as false starts, pauses,and filled pauses uch as uhhh.The dead air hypothesis would seem to rely on an as-sumption that at unpredictable intervals, agents justcan't think very well.
My claim is that IRU's are re-lated to goals, that  they support inferencing and ad-dress assumptions underlying mutual beliefs, is.
theyare not random.
In order to prove this it must be pos-sible to test the hypothesis that it is only impor tantpropositions that get repeated, paraphrased or madeexplicit.
This can be based on analyzing when theinformation that is repeated has been specifically re-quested, such as in the caller's opening question or bya request for information from Harry.
It should also bepossible to test whether the IRU realizes a propositionthat plays a role in the final plan that Harry and thecaller negotiate.
However this type of strong evidenceAcIxs DE COLING-92, NANTES, 23-28 Aour 1992 3 5 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992against he dead air hypothesis i left to future work.7 Discuss ionIt should be apparent from the account hat the typesof utterances examined here are not really redundant.The reason that many models of belief transfer in di-alogue would characterize them as redundant followsfrom a combination of facts: (1) The representation fbelief in these models has been binary; (2) The effectsof utterance actions are either assumed to always hold,or to hold as defaults unlcss the listener already be-lieved otherwise.
This means that these accounts can-not represent the fact that a belief must be supportedby some kind of evidence and that the evidence may bestronger or weaker.
It also follows from (2) that thesemodels assume that agents are not autonomous, or atleast do not have control over their own mental states.But belief revision is surely an autonomous process;agents can choose whether to accept a new belief orrevise old beliefs\[4, 8\].The occurrence of IRU's in dialogue bas many ramifi-cations for a model of dialogue.
Accounting for IRU'shas two direct effects on a dialogue model.
First it re-quires a model of nmtual beliefs that specifies how mu-tual beliefs are inferred and how some mutual beliefscan be as weak as mutual suppositions.
One functionof IRU's is to address the assumptions on which mutualbeliefs are based.
Second the assumption that propo-sitions representing beliefs and intentions get added tothe context by the unilateral action of one conversantmust be dropped.
This account replaces that assump-tion with a model in which the evidence of the hearermust be considered to establish mutual beliefs.
Theclaim here is that both understanding and acceptanceare monitored.
The model outlined here can be usedfor different ypes of dialogue, including dialogues inwhich agents are constructing mutual beliefs to sup-port future action by them jointly or alone.ltow and when agents decide to augment the strengthof evidence for a belief has not been addressed in thiswork as yet.
Future work includes analyzing the corpuswith respect o whether the IRU plays a role in the finalplan that is negotiated between the conversants.8 AcknowledgementsDiscussions with Aravind Joshi, Ellen Prince and Bon-nie Webber have been extremely helpful in the devel-oprnent of these ideas.
In addition I would like tothank Herb Clark, Sharon Cote, Julia Galliers, EllenGermain, Beth Ann Hockey, Megan Moser, HideyukiNakashima, Owen Rainbow, Craige Roberts, Phil Sten-ton, and Steve Whittaker for the influence of their ideasand for useful discussions.References\[1\] Jean C. Carletta.
Risk Taking and Recovery inTask-Oriented Dialogue.
PhD thesis, EdinburghUniversity, 1991.\[2\] Herbert H. Clark and Catherine R. Marshall.
Def-inite reference and nmtual knowledge.
In Joshi,Webber, and Sag, editors, Elements of DiscourseUnderstanding, pages 10-63.
CUP, 1981.\[3\] H H. Clark and Edward F. Schaefer.
Contributingto discourse.
Cognitive Science, 13, 1989.\[4\] Julia R. Galliers.
Cooperative interaction asstrategic belief revision.
In M.S.
Deen, editor, Co-operating Knowledge Based Systems, 1991.\[5\] H. P. Grice.
William James Lectures.
1967.\[6\] Barbara J. Grosz, Aravind K. Josbi, and ScottWeinstein.
Towards a computational theory of dis-course interpretation.
Unpublished Manuscript,1986.\[7\] Barbara J. Grosz and Caudaee L. Sidner.
Atten-tions, intentions and tile structure of discourse.Computational Linguistics, 12:pp.
175-204, 1986.\[8\] Barbara J. Grosz and Candace L. Sidner.
Plansfor discourse.
In Cohen, Morgan and Pollack, eds.Intentions in Communication, MIT Press, 1990.\[9\] Julia Iiirschberg.
A Theory of Scalar lmplicature.PhD thesis, University of Pennsylvania, Computerand Information Science, 1985.\[10\] David Lewis.
Convention.
l larvard UniversityPress, 1969.\[11\] Diane Litman and James Allen.
Recognizingand relating discourse intentions and task-orientedplans.
In Cohen, Molyan and Pollack, eds.
Inten-tions in Communication, MIT Press, 1990.\[12\] Martha Pollack, Julia Hirschberg, and BonnieWebber.
User participation i  the reasoning pro-cess of expert systems.
In AAAI, 1982.\[13\] Ellen F. Prince.
On the function of existentialpresupposition i discourse.
In Papers from l~thRegional Meeting.
CLS, Chicago, IL, 1978.\[14\] Robert C. Stalnaker.
Assertion.
In Peter Cole,editor, Syntax and Semantics, Volume 9: Prag-mattes, pages 315-:\]32.
Academic Press, 1978.\[15\] Marilyn A. Walker and Steve Whittaker.
Mixedinitiative in dialogue: An investigation i to dis-course segmentation.
In ACL, pages 70-79, 1990.ACRES DE COLING-92, NARI~S.
23-28 nOt\]T 1992 3 5 1 PROC.
oF COLING-92, NAntES, AUG. 23-28, 1992
