In: Proceedings of CoNLL-2000 and LLL-2000, pages 103-106, Lisbon, Portugal, 2000.Genetic Algorithms for Feature Relevance Assignment inMemory-Based Language ProcessingAnne Koo l  and Wal ter  Dae lemans  and Jakub  Zavrel*CNTS - Language Technology GroupUniversity of Antwerp, UIA, Universiteitsplein 1, 2610 Antwerpen, Belgium{kool, daelem, zavrel}@uia.ua.ac.beAbst rac tWe investigate the usefulness of evolutionary al-gorithms in three incarnations of the problem offeature relevance assignment in memory-basedlanguage processing (MBLP): feature weight-ing, feature ordering and feature selection.
Weuse a simple genetic algorithm (GA) for thisproblem on two typical tasks in natural lan-guage processing: morphological synthesis andunknown word tagging.
We find that GA fea-ture selection always significantly outperformsthe MBLP variant without selection and thatfeature ordering and weighting with CA signifi-cantly outperforms a situation where no weight-ing is used.
However, GA selection does not sig-nificantly do better than simple iterative featureselection methods, and GA weighting and order-ing reach only similar performance as currentinformation-theoretic feature weighting meth-ods.1 Memory-Based  LanguageProcess ingMemory-Based Language Processing (Daele-mans, van den Bosch, and Zavrel, 1999) isbased on the idea that language acquisitionshould be seen as the incremental storage ofexemplars of specific tasks, and language pro-cessing as analogical reasoning on the basis ofthese stored exemplars.
These exemplars takethe form of a vector of, typically, nominal fea-tures, describing a linguistic problem and itscontext, and an associated class symbol repre-senting the solution to the problem.
A new in-stance is categorized on the basis of its similar-ity with a memory instance and its associated* Research funded by CELE, S.AI.L Trust V.Z.W.,Ieper, Belgium.class.The basic algorithm we use to calculate the dis-tance between two items is a variant of IB1(Aha, Kibler, and Albert, 1991).
IB1 doesnot solve the problem of modeling the differ-ence in relevance between the various sourcesof information.
In an MBLP approach, thiscan be overcome by means of feature weighting.The IBi-IG algorithm uses information gain toweight the cost of a feature value mismatch dur-ing comparison.
IGTREE is a variant in which anoblivious decision tree is created with featuresas tests, and in which tests are ordered accord-ing to information gain of the associated fea-tures.
In this case, the accuracy of the trainedsystem is very much dependent on a good fea-ture ordering.
For all variants of MBLP dis-cussed here, feature selection can also improveboth accuracy and efficiency by discarding somefeatures altogether because of their irrelevanceor even counter-productivity in learning to solvethe task.
In our experiments we will use a rel-evance assignment method that radically dif-fers from information-theoretic measures: ge-netic algorithms.2 Genet ic  A lgor i thms for  Ass ign ingRe levanceIn the experiments, we linked our memory-based learner TIMBL 1 to  PGAPACK 2.
During theweighting experiments a gene corresponds to aspecific real-valued feature-weight (we will indi-cate this by including GA in the algorithm name,i.e.
IB1-GA and GATREE, cf.
IBi - IG and IGTREE).1TIMBL is available from http://ilk.kub.nl/ and thealgorithms are described in more detail in (Daelemanset al, 1999).2A software environment for evolutionary computa-tion developed by D. Levine, Argonne National Labora-tory, available from ftp://ftp.mcs.anl.gov/pub/pgapack/103In the case of selection the string is composedof binary values, indicating presence or absenceof a feature (we will call this GASEL).
The fit-ness of the strings is determined by running thememory-based learner with each string on a val-idation set, and returning the resulting accuracyas a fitness value for that string.
Hence, bothweighting and selection with the GA is an in-stance of a wrapper approach as opposed to afilter approach such as information gain (Ko-havi and John, 1995).For comparison, we include two popularclassical wrapper methods: backward elimina-tion selection (BASEL) and forward selection(FOSEL).
Forward selection starts from anempty set of features and backward selectionbegins with a full set of features.
At each fur-ther addition (or deletion, for BASEL) the fea-ture with the highest accuracy increase (resp.lowest accuracy decrease) is selected, until im-provement stalls (resp.
performance drops).During the morphology experiment the pop-ulation size was 50, but for prediction of un-known words it was set to 16 because the largerdataset was computationally more demanding.The populations were evolved for a maximumof 200 generations or stopped when no changehad occurred for over 50 generations.
Parame-ter settings for the genetic algorithm were keptconstant: a two-point crossover probability of0.85, a mutation rate of 0.006, an elitist replace-ment strategy, and tournament selection.2.1 DataThe first task 3 we consider is prediction of whatdiminutive suffix a Dutch noun should take onthe basis of its form.
There are five differentpossible suffix forms (the classes).
There are 12features which contain information (stress andsegmental information) about the structure ofthe last three syllables of a noun.
The data setcontains 3949 such instances.The second data set 4 is larger and contains65275 instances, the task we consider here ispart-of-speech (morpho-syntactic category) tag-ging of unknown words.
The features used hereare the coded POS-tags of two words before andtwo words after the focus word to be tagged, the3Data from the CELEX lexical data base, availableon CD-ROM from the LDC, http://ldc.upenn, edu.4This dataset is based on the TOSCA tagged LOBcorpus of English.last three letters of the focus word, and informa-tion on hyphenation and capitalisation.
Thereare 111 possible classes (part of speech tags) topredict.2.2 MethodWe have used 10-fold-cross-validation n all ex-periments.
Because the wrapper methods gettheir evaluation feedback directly from accuracymeasurements on the data, we further split thetrainfile for each fold into 2/3 sub-trainset anda 1/3 validation set.
The settings obtained bythis are then tested on the test set of that fold.2,3 Resu l tsIn Table 1 we show the results of our exper-iments (average accuracy and standard devia-tion over ten folds).
We can see that applyingany feature selection scheme when no weightsare used (IB1) significantly improves classifi-cation performance (p<0.01) 5.
Selection alsoimproves accuracy when using the IBI-IG orIGTREE algorithm.
These differences are sig-nificant on the morphology dataset (p<0.05),but for the unknown words dataset only the dif-ference between (IB1) and (IBi-~-GASEL) is sig-nificant (p<0.01).
In both cases, however, theresults in Table 1 do not reveal significant dif-ferences between evolutionary, backward or for-ward selection.With respect o feature weighting by meansof a GA the results are much less clear: for themorphology data, the GA-weights significantlyimprove upon IB1, refered to as IB1-GA in thetable, (p<0.01) but not IGTREE (GATREE in thetable).
For the other dataset OA-weights do noteven improve upon IB1.
But in general, thoseweights found by the genetic algorithm lead tocomparable classification accuracy as with gainratio based weighting.
The same applies to thecombination of aA-weights with further selec-tion of irrelevant features (GATREE-bGASEL).2.4 The  Ef fect  o f  GA ParametersWe also wanted to test whether the GA wouldbenefit from optimisation in the crossover andmutation probabilities.
To this end, we usedthe morphology dataset, which was split into an80% trainfile, a 10% validationfile and a held-out 10% testfile.
The mutation rate was var-5All significance tests in this paper are one-tailedpaired t-tests.104Classifier Morphology Unknown WordsIB1 87.2 (?
1.6) 81.7 (?
0.5)IBi+GASEL 96.5 (?
1.0) 82.8 (?
0.6)IB1WFOSEL 96.6 (?
1.1) 82.9 (?
0.2)IB1WBASEL 96.6 (?
1.1) 82.9 (?
0.2)IBi-IG 96.2 (?
0.8) 82.8 (?
0.3)IBi-IG+GASEL 97.3 (?
0.9) 83.0 (?
0.3)IBI-IG+FOSEL 97.1 (?
0.9) 82.8 (?
0.3)IBI-IGWBASEL 97.3 (?
1.0) 82.9 (?
0.3)IGTREE 96.2 (?0.8) 81.4 (?
0.4)IGTREE-t-GASEL 97.1 (?
0.9) 81.4 (?
0.4)IGTREE--~-FOSEL 97.0 (?
0.9) 81.3 (?
0.4)IGTREE--~-BASEL 97.0 (?
1.1) 81.3 (?
0.4)ml-GA 95.6 (?
1.0) 81.6 (?
0.8)IB1-GA+GASEL 97.0 (?
1.1) 82.0 (?
1.2)GATREE 96.0 (?
1.0) 80.4 (?
1.2)GATREE--t-GASEL 9'7.1 (?
1.0) 81.0 (?
0.6)Table 1: Accuracy (:i: standard deviation) results ofthe experiments.
Boldface marks the best results foreach basic algorithm per data set.ied stepwise adding a value of 0.001 at each ex-periment, starting at a 0.004 value up to 0.01.The different values for crossover anged from0.65 to 0.95, in steps of 0.05.
The effect ofchanging crossover and mutation probabilitieswas tested for IBl-IG+GA-selection, for IB1 withCA weighting, for IGTREE+GA-selection, and forIGTREE with GA-weight settings.These experiments show considerable fluctua-tion in accuracy within the tested range, but dif-ferent parameter settings could also yield sameresults although they were far apart in value.Some settings achieved a particularly high accu-racy in this training regime (e.g.
crossover: 0.75,mutation: 0.009).
However, when we used thesein the ten-fold cv setup of our main experi-ments, this gave a mean score of 97.4 (?
0.9)for IBi-IG with CA-selection and a mean scoreof 97.1 (?
1.1) for IGTREE with GA-selection.These accuracies are similar to those achievedwith our default parameter settings.2.5 DiscussionFeature selection on the morphology task showsa significant increase in performance accuracy,whereas on the unknown words task the differ-ences are less outspoken.
To get some insightinto this phenomenon, we looked at the averageprobabilities of the features that were left outby the evolutionary algorithm and their aver-age weights.On the morphology task this reveals that nu-cleus and coda of the last syllable are highlyrelevant, they are always included.
The onsetof all three syllables is always left out.
Further,in all partitions the nucleus and coda of the sec-ond syllable are left out.
6 For part-of-speechtagging of unknown words all features appearto be more or less equally relevant.
Over theten partitions, either no omission is suggestedat all, or the features that carry the pos-tag ofn-2 word before and the n+2 word after the fo-cus word are deleted.
This is comparable to re-ducing the context window of this classificationtask to one word before and one after the focus.The fact that all features seem to contributeto the classification when doing POS-tagging(making selection irrelevant) could also explainwhy the IGTREE algorithm seems to benefit lessfrom the feature orders suggested and why thenon-weighted approach IB1 already has a highscore on the tagging task.
The IGTREE algo-rithm is more suited for problems where the fea-tures can be ordered in a straightforward waybecause they have significantly different rele-vance.3 Conc lus ions  and  Re la ted  ResearchThe issue of feature-relevance assignment iswell-documented in the machine learning lit-erature.
Excellent comparative surveys are(Wettschereck, Aha, and Mohri, 1997) and(Wettschereck and Aha, 1995) or (Blum andLangley, 1997).
Feature subset selectionby means of evolutionary algorithms was in-vestigated by Skalak (1994), Vafaie and deJong (1992), and Yang and Honavar (1997).Other work deals with evolutionary approachesfor continuous feature weight assignment suchas Wilson and Martinez (1996), or Punch andGoodman (1993).The conclusions from these papers are inagreement with our findings on the natural an-guage data, suggesting that feature selectionand weighting with GA's significantly outper-form non-weighted approaches.
Feature selec-tion generally improves accuracy with a reduc-6This fits in with current heory about this morpho-logical process (e.g.
Trommelen (1983), Daelemans etal.
(1997)).105tion in the number of features used.
However,we have found no results (on these particulardata) that indicate an advantage of evolutionaryfeature selection approach over the more classi-cal iterative methods.
Our experiments furthershow that there is no evidence that GA weight-ing is in general competitive with simple filtermethods uch as gain ratio.
Possibly, a parame-ter setting for the GA could be found that givesbetter results, but searching for such an optimalparameter setting is at present computationallyunfeasible for typical natural language process-ing problems.ReferencesAha, D., D. Kibler, and M. Albert.
1991.
Instance-based learning algorithms.
In Machine LearningVol.
6, pp 37-66.Blum, A. and P. Langley.
1997.
Selection of rele-vant features and examples in machine learning.In Machine Learning: Artificial Intelligence,97,pp 245-271.Daelemans, W., P. Berck, and S. Gillis.
1997.
Datamining as a method for linguistic analysis: Dutchdiminutives.
In Folia Linguistica , XXXI/1-2, pp57-75.Daelemans, W., A. van den Bosch, and J. Zavrel.1999.
Forgetting exceptions is harmful in lan-guage learning.
In Machine Learning, special is-sue on natural anguage learning, 34 , pp 11-43.Daelemans, W., J. Zavrel, K. van der Sloot, andA.
van den Bosch.
1999.
Timbl: Tilburg mem-ory based learner, version 2.0, reference guide.
Ilktechnical report 99-01, ILK.John, G.H., R. Kohavi, and K. Pfleger.
1994.
Irrel-evant features and the subset selection problem.In Machine Learning: Proceedings of the EleventhInternational Conference, pp 121-129.Kohavi, R. and G.H.
John.
1995.
Wrappers forfeature subset selection.
In Artificial IntelligenceJournal, Special Issue on Relevance Vol.97, pp273-324.Punch, W. F., E.D.
Goodman, Lai Chia-ShunMin Pei, P. Hovland, and R. Enbody.
1993.
Fur-ther research on feature selection and classifica-tion using genetic algorithms.
In Proceedings ofthe Fifth International Conference on Genetic Al-gorithms, pp 557.Quinlan, J.R. 1993.
C4.5: Programs for MachineLearning.
San Mateo: Morgan Kaufmann.Skalak, D. 1993.
Using a genetic algorithm to learnprototypes for case retrieval and classification.In Case-Based Reasoning: Papers from the 1993Workshop, Tech.
Report WS-93-01, pp 211-215.AAAI  Press.Skalak, D. B.
1994.
Prototype and feature selec-tion by sampling and random mutation hill climb-ing algorithms.
In Proceedings of the eleventh In-ternational Conference on Machine Learning, pp293-301.Trommelen, M.T.G.
1983.
The Syllable in Dutch,with special Reference to Diminutive Formation.Foris: Dordrecht.Vafaie, H. and K. de Jong.
1992.
Genetic algorithmsas a tool for feature selection in machine learn-ing.
In Machine Learning, Proceeding of the 4thInternational Conference on Tools with ArtificialIntelligence, pp 200-204.Wettschereck, D. and D. Aha.
1995.
Weighting fea-tures.
In Proceedings of the First InternationalConference on Case-Based Reasoning, ICCBR-95, pp 347-358.Wettschereck, D., D. Aha, and T. Mohri.
1997.
Areview and empirical evaluation of feature weight-ing methods for a class of lazy learning algorithms.In Artificial Intelligence Review Vol.11, pp 273-314.Wilson, D. and T. Martinez.
1996.
Instance-based learning with genetically derived attributeweights.
In Proceedings of the International Con-ference on Artificial Intelligence, Expert Systems,and Neural Networks, pp 11-14.Yang, J. and V. Honavar.
1997.
Feature subset se-lection using a genetic algorithm.
In Genetic Pro-gramming 1997: Proceedings of the Second An-nual Conference, pp 380.106
