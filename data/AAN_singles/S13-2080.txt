Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsFBM: Combining lexicon-based ML and heuristicsfor Social Media PolaritiesCarlos Rodr?
?guez-Penagos, Jordi Atserias, Joan Codina-Filba`,David Garc?
?a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur?
?Barcelona MediaAv.
Diagonal 177, Barcelona 08018Corresponding author: carlos.rodriguez@barcelonamedia.orgAbstractThis paper describes the system implementedby Fundacio?
Barcelona Media (FBM) for clas-sifying the polarity of opinion expressions intweets and SMSs, and which is supported bya UIMA pipeline for rich linguistic and sen-timent annotations.
FBM participated in theSEMEVAL 2013 Task 2 on polarity classifi-cation.
It ranked 5th in Task A (constrainedtrack) using an ensemble system combiningML algorithms with dictionary-based heuris-tics, and 7th (Task B, constrained) using anSVM classifier with features derived from thelinguistic annotations and some heuristics.1 IntroductionWe introduce the FBM system for classifying thepolarity of short user-generated text (tweets andSMSs), which participated in the two subtasks ofSEMEVAL 2013 Task 2 on Sentiment Analysis inTwitter.
These are: Task A. Contextual Polarity Dis-ambiguation, and Task B.
Message Polarity Classifi-cation.
The former aimed at classifying the polarityof already identified opinion expressions (or cues),whereas the latter consisted in classifying the polar-ity of the whole text (Wilson et al 2013).The literature agrees on two main approaches forclassifying opinion expressions: using supervisedlearning methods and applying dictionary/rule-based knowledge (see (Liu, 2012) for an overview).Each of them on its own has been used in work-able systems, and a principled combination of bothof them can yield good results on noisy data, sincegenerally one (dictionaries/rules) offers good preci-sion while the other (ML) is able to discover unseenexamples and thus enhances recall.FBM combined both approaches in order to bene-fit from their respective strengths and compensatingas much as possible their weaknesses.
For Task Awe used linguistic (lexical and syntactic) annotationsto implement both types of approaches.
On the onehand, we built machine learning classifiers based onSupport Vector Machines (SVMs) and ConditionalRandom Fields (CRFs).
On the other, we imple-mented a basic classification system mainly basedon polarity dictionaries and negation information, aswell as simple decision tree-like heuristics extractedfrom the training data.
For task B we trained anSVM classifier using some of the annotations fromTask A.The paper first presents the process of data com-pilation and preprocessing (section 2), and then de-scribes the systems for Tasks A (section 3) and B(section 4).
Results and conclusions are discussedin the last section.2 Data Compilation and Processing2.1 Making data availableThe corpus of SMSs was provided to the partici-pants by the organizers of the task.
As for the corpusof tweets, legal restrictions on twitter data distribu-tion required the participants to download the tex-tual contents of the corpus from a list of tweet ids.We retrieved the tweet text using the official twit-ter API instead of script provided by the organizers,but not all the tweets were available for download483due to restrictions of different types (e.g.
geograph-ical), or because the twitter account was temporarilysuspended.
In total, we managed to retrieve 10,764tweets out of 11,777 ids provided by the organizers(91.4%).
It is worth pointing out that the restric-tions on tweets distribution can become an issue forfuture users of the dataset, as the amount of avail-able tweets will diminish over time.
By contrast, thetwitter test corpus was distributed with the full textto avoid those problems.2.2 Leveraging the data with rich linguisticinformationWe applied the same linguistic processing to bothcorpora (SMSs and tweets), even though the SMStest data presents very different characteristics fromthe twitter data, not only because of what can be ap-preciated as genre differences, but also due to thefact that is apparently written in Singaporean En-glish, which differs significantly from American orBritish English.
No efforts were made to adaptour linguistic processing modules and dictionariesto this data.Tweets and SMSs were processed with a UIMA1-based pipeline consisting of a set of linguistic andopinion-oriented modules, which includes:Basic linguistic processing: Sentence segmen-tation, tokenization, POS-tagging, lemmatiza-tion.Syntax: Dependency parsing.Lexicon-based annotations:?
Basic polarity, distinguishing among: positive,negative, and neutral, as encoded in Wilson etal.
(2010).?
Polarity strength, using the score for pos-itive and negative polarity in SentiWordnet3.0 (Baccianella et al 2010).
Each Sen-tiWordNet synset has an associated triplet ofnumerical scores (positive, negative,and objective) expressing the intensity ofpositive, negative and objective polarity of theterms it contains.
They range from 0.0 to 1.0,and their sum is 1.0 for each synset (Esuli andSebastiani, 2007).
We selected only the synset1http://uima.apache.org/uima-specification.htmlwith positive or negative scores higher than 0.5,containing a total of 16,791 words.?
Subjectiviy clues, from Wilson et al(2010),which are classified as weak or strong depend-ing on their degree of subjectivity.?
Sentiment expressions, from the Linguistic In-quiry and Word Count (LIWC) 2001 Dictio-nary (Pennebaker et al 2001).?
In-house compiled lexicons of negation mark-ers (such as ?no?, ?never?, ?none?)
and quanti-fiers (?all?, ?many?, etc.
), the latter further clas-sified into low, medium and high according totheir quantification degree.The different classifiers employed by FBM con-structed their vectors from this output to learn globaland contextual polarities.3 Task A: Ensemble SystemOur system combined Machine Learning and rule-based approaches.
The aim was to combine thestrengths of each individual component while avoid-ing as much as possible their weaknesses.
In whatfollows we describe each system component as wellas the way the ensemble system worked out the col-lective decisions.3.1 Conditional Random FieldsOne of the classifiers uses the Conditional RandomFields implementation of a biomedical Named En-tity Recognition system (JNET from JulieLab) 2, ex-ploiting the classification capabilities of the system(rather than its span detection) by strongly associat-ing already defined ?marked instances?
with a polar-ity, and exploring a 5-word window.
It uses depen-dency labels, POS tags, polar words, sentiwordnetand LWIC sentiment annotations, as well as indica-tions for quantifiers and negation markers.3.2 Support Vector MachinesThis classifier was implemented using an SVM algo-rithm with a linear kernel and the C parameter set to0.2 (determined using a 5 fold cross-validation).
Thefeatures set includes those that we used in RepLab2http://www.julielab.de4842012 (Chenlo et al 2012) (including number of:characters, words, links, hashtags, positive and neg-ative emoticons, question-exclamation marks, ad-jectives, nouns, verbs, adverbs, uppercased words,words with duplicated vowels), plus a set of newfeatures at tweet level obtained from the linguisticannotations: number of high/medium/low polarityquantifiers, number of positive and negative polarwords, sentiwordnet applied to both the cue and thewhole tweet.Moreover, the RepLab polarity calculation basedon different dictionaries was modified to take intoaccount negation (in a 3-word window) potentiallyinverting the polarity (negPol).
This polarity mea-sure was applied to the cue and to the whole tweet,thus generating two additional features.3.3 Heuristic ApproachIn task A, in parallel to the supervised learning sys-tem, we developed a method (named Heur) basedon polarity dictionary lookup and simple heuristics(see Figure 1) taking into account opinion wordsas well as negation markers and quantifiers.
Theseheuristics were implemented so as to maximize thenumber of correct positive and negative labels in thetraining data.
To this end, we calculated the aggre-gate polarity of a cue segment as the sum of wordpolarities found in the polarity lexicon.
The aggre-gate values in the training set ranged from -3 to +3,taking respectively 1, 0 and -1 as the polarity of pos-itive, neutral and negative words.
The label distri-bution of cue segments with an aggregate polarityvalue of -1 is shown in Table 1.Aggregate polarity -1Negation no yesnegative 1,032 30neutral 37 4positive 178 71Table 1: Cue segment polarity statistics in training datafor an aggregate polarity value of -1.In this case, if no negation is present in the cuesegment, a majority (1,032) of examples had thenegative label.
In case there was at least a negation, amajority (71) of examples had a positive label.
Thisbehaviour was observed with all negative aggregate1: if has polar word(CUE) then2: polarity= lex(P)-0.5*lex(QP)3: -lex(N)+0.5*lex(QN)4: if polarity>0 then5: if has negation(CUE) then negative6: else positive7: end if8: else if polarity<0 then9: if has negation(CUE) then positive10: else negative11: end if12: else13: if has negation(CUE) then positive14: else negative15: end if16: end if17: else if has negation(CUE) then negative18: else19: polarity= tlex(P)-0.5*tlex(QP)20: -tlex(N)+0.5*tlex(QN)21: if polarity<0 then negative22: else if tlex(NEU)>0 then neutral23: else if polarity>0 then positive24: else if has negemo(CUE) then negative25: else if has posemo(CUE) then positive26: else unknwn27: end if28: end ifFigure 1: Heuristics used by the lexicon-based system toclassify the polarity of a segment marked up as opinioncue (Task A).polarity values in training data, yielding the rule inlines 8 to 11 of Figure 1.
Similar rules were ex-tracted for the other aggregate polarity values (lines4 to 16 of Figure 1).Figure 1 details the complete classification algo-rithm.
Note (lines 1 to 17) that we first rely on thebasic polarity lexicon annotations (described in sec-tion 2).
The final aggregate polarity formula (lines2-3) was refined to distinguish sentiment wordswhich act as quantifiers, such as pretty in pretty mad.The word pretty is both a positive polar word and aquantifier.
We want its polarity to be positive in caseit occurs in isolation, but less than one so that thesum with a following negative polar word (such asmad) be negative.
We thus give this kind of wordsa polarity of 0.5 by substracting 0.5 for each polarword which is also a quantifier.
In the polarity for-mula of lines 2-3, lex(X) refers to the number ofwords annotated as X, P and N refer respectivelyto positive and negative polar words, and QP and485QN refer to positive and negative polar words whichare also quantifiers.
Quantifiers which are not polarwords are not taken into account because they arenot likely to change the opinion polarity.In case that no annotations from the basic polar-ity, quantifiers, and negative markers lexicons arefound (lines 18 to 28), we look up in dictionariesbuilt from the training data (tlex in lines 19-20).To build these dictionaries, we counted how manytimes each word was labeled positive, negative andneutral.
We considered that a word has a given po-larity if the number of times it was assigned to thisclass is greater than the number of times it was as-signed to any other class by a given threshold.
Wecalculated the polarity in the same way as before,but now with the counts from the lexicon automati-cally compiled from the training data.
To improvethe recall of the dictionary lookup, we performedsome text normalization: lowercasing, deletion ofrepeated characters (such as gooood) and deletion ofthe hashtag ?#?
character.
Finally, if no polar wordis found in the automatically compiled lexicon, welook at the sentiment annotations (extracted from theLIWC dictionary).3.4 Ensemble Voting AlgorithmAs already mentioned, we combined the results fromthe described polarity methods to build a collectivedecision.
Table 2 shows the performance (in termsof F1 measure) of the different single methods overthe tweet test data.SVM Heur Heur+ CRFTest 80.74 83.47 84.62 62.85Table 2: Twitter Task A results for different methodsAlthough the heuristic method outperforms theML methods, they are not only different in nature(ML vs. heuristic) but also use different information(see Table 5).
This suggests that the ensemble solu-tion will be complementary and capable of obtainingbetter results than any of the individual methods byitself.The development set was used to calculate the en-semble response given the individual votes of thedifferent systems in a way similar to the behaviorknowledge space method (Huang and Suen, 1993).Table 3 shows an example of how the assemblevoting is built.
For each method vote combina-tion (SVM-Heuristics-CRF) the number of positives/ negatives / neutral is calculated in the developmentdata.
The ensemble (EV) selects the vote that max-imizes the number of correct votes in the develop-ment data (in bold).SVM Heur CRF EV# Instancespos neg neu?
+ ?
?
0 6 0?
?
+ ?
1 23 2?
?
?
?
3 125 2?
u + + 1 0 0+ u n ?
0 1 0+ ?
+ + 17 13 2+ + + + 314 18 17+ ?
n + 3 1 0Table 3: Oracle building example (EV: Ensemble Vote,+:positive, ?
:negative, n:neutral, u:unknown)The test data contains some combination of votesthat were not seen in the development data.
Thus,in order to deal with these unseen combinations ofvotes in the test set we use the following backupheuristics based on the preformance figures of theindividual methods: Use the vote of the heuristicmethod.
If this method does not vote (u), then se-lect the SVM vote.Table 4 shows the results of the proposed ensem-ble method, the well-known majority voting and theupper bound of this ensemble method (calculatedwith the same strategy over the test data), over thedevelopment and test tweet dataEnsemble Majority UpperVoting Voting BoundDev 85.48 81.31 85.48Test 85.50 82.70 89.37Table 4: Results for different ensemble strategiesIn the development corpus, the upper bound andensemble results are the same, given that they ap-ply the same knowledge.
The difference is in thetest dataset, where the ensemble voting is calculatedbased on the knowledge obtained from the develop-ment corpus, while the upper bound uses the knowl-edge that can be derived from the test corpus.486Table 5 illustrates the features used by each com-ponent.SVM SVM CRF Heur(task A) (task B)word ?
?
?lemmapos ?
?deps ?pol ?
?
?
?polW ?sent ?
?
?sentiwn ?
?
?quant ?
?
?
?neg ?
?
?
?links ?hashTags ?Table 5: Information used (pos: part-of-speech; deps: de-pendencies; pol: basic polarity classification; polW: basicpolarity word; sent: LIWC sentiments; sentwn: Senti-Wordnet; quant/neg: quantifiers and negation markers.
)4 Task B: A Support VectorMachine-based SystemThe system presented for task B is based on ML us-ing a SVM model.
The feature vector used as inputfor the SVM component is composed of the annota-tions provided by the linguistic annotation pipeline,extended with a feature obtained by applying nega-tion to the next polar words (window of size 3).The features used do not include the words (ortheir lemmas) because the number of tweets avail-able for training is small (104) compared to the num-ber of different words (4 ?
104).
A model based onbag-of-words would suffer from overfitting and thusbe very domain and time-dependent.
If the train andtest sets were randomly selected from a bigger set,the use of words could increase the model?s accu-racy, but the model would also be too narrowly ap-plied to this specific dataset.From the annotation pipeline we extracted as fea-tures: the polar words (PolW) and their basic po-larity (Pol); the sentiment annotations from LIWC(Sent); the negation markers (Neg) and quantifiers(Quant).
The model was trained using Weka (Hallet al 2009).The model used is SVM with the C parameter setto 1.0 and applying a 10 fold cross-validation.
Theoption of doing first a model to discriminate polarand neutral tweets was discarded because Weka al-ready does that when training classifiers for morethan two training classes, and the combination of thetwo classifiers (a first one between polar and opin-ionated and a second one between positive and neg-ative) would produce the same results.5 Results and DiscussionThe results of our system in each subcorpus and taskare presented in Table 5 (average of the F1-measureover the classes positive and negative, constrainedtrack), with the ranking achieved in the competitionin parentheses.Tweet Corpus SMS CorpusTask A 0.86 (5th) 0.73 (11th)Task B 0.61 (7th) 0.47 (28th)Table 6: FBM system performance (F1 average over pos-itive and negative classes, constrained track) and rankingsGiven the differences in style and vocabularies be-tween the SMS and tweet corpora, and the fact thatwe made not effort whatsoever to adapt our systemor models to them, the drop in performance fromone to the other is considerable, but to be expectedsince domain customization is an important aspectof opinion mining.Task A: The confusion matrix in Table 7 showsan acceptable performance for the most frequentclasses in the corpus (with an error of 7.75% and19.5% for postive and negative cues, respectively)and a very poor job for neutral cues (98.1% of er-ror), clearly a minority class in the training corpus(5% of the data).GOLD: Pos Neg NeuSYSTEM: Pos 2,522 296 126Neg 206 1,240 31Neu 6 5 3Table 7: Task A confusion matrixGiven the skewed distribution of polarity cate-gories in the test corpus, however, neutral mistakesamount to only 23% of our system error, and so we487focus our analysis on the problems in positive andnegative cues, respectively amounting to 31.7% and44.8% of the total error.
There are 2 main sources oferror:?
Limitations of the dictionaries employed,which were short in covering somewhat fre-quent slang words (e.g., wacky, baddest, shit-loads), expressions (e.g., ouch, yukk, C?MON),or phrases (e.g., over the top), some of whichexpress a particular polarity but contain a wordexpressing just the opposite (have a blast, towant something bad/ly).?
Problems in UGC processing, mainly related tonormalization (e.g., fooooool) and tokenization(Perfect...not sure), which put at risk the cor-rect identification of lexical elements that arecrucial for polarity classification.Task B: The average F-score of positive and neg-ative classes was 0.62 in the development set (thatwas included in the training set) and the averaged F-score for the test set was 0.61 (so they are very simi-lar).
If focusing on precision and recall, the positiveand negative classes have higher precision but lowerrecall in the test set.
We think that this low degrada-tion of perfomance indicates the model?s potentialfor generalization.6 ConclusionsFrom our results, we can conclude that the use ofensemble combination of orthogonal methods pro-vides good performance for Task A.
Similar resultscould be expected for Task B (judging from mix-ing dictionaries and ML in similar tasks at RepLab2012 (Chenlo et al 2012)).
The ML methods thatwe applied for Task B are essentially additive, andhence have difficulties in applying features such aspolarity shifters.
To overcome this, one of the fea-tures includes negation of polar words when a polar-ity shifter is near.Overall, the SemEval Tasks have make evident theusual challenges when mining opinions from SocialMedia channels: noisy text, irregular grammar andorthography, highly specific lingo, etc.
Moreover,temporal dependencies can affect the performance ifthe training and test data have been gathered at dif-ferent times, as is the case with text of such a volatilenature as tweets and SMSs.0.00%5.00%10.00%15.00%20.00%25.00%30.00%35.00%40.00%45.00%50.00%traindevtestFigure 2: Distribution of tweets over timeThe histogram in Figure 2 shows that this also ap-plies to the Semeval tweets dataset.
It illustrates thedistribution of tweets over time (extrapolated fromthe sequential ids) in the 3 subcorpora (train, devel-opment and test), showing some divergence betweenthe test corpus on the one hand, and the develop-ment and training corpora on the other.
Neverthe-less, our system shows little performance degrada-tion between development and testing results, as at-tested in Table 4 (ensemble voting column).Our work here and at other competitions alreadycited validate a system that combines stochastic andsymbolic methodologies in a principled, data-drivenapproach.
Time and domain dependencies of SocialMedia data make system and model generalizationhighly desirable, and our system hybrid nature alsocontribute to this objective.AcknowledgmentsThis work has been partially funded by the SpanishGovernment project Holopedia, TIN2010-21128-C02-02, the CENIT program project Social Media,CEN-20101037, and the Marie Curie ReintegrationGrant PIRG04-GA-2008-239414.488ReferencesBaccianella, Stefano, Andrea Esuli and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th conference on InternationalLanguage Resources and Evaluation, Valletta, Malta.Chenlo, Jose M., Jordi Atserias, Carlos Rodr?
?guez-Penagos and Roi Blanco.
2012.
FBM-Yahoo!at RepLab 2012.
In: P. Forner, J. Karlgren,C.
Womser-Hacker (eds.)
CLEF 2012 Evalua-tion Labs and Workshop, Online Working Notes.http://clef2012.org/index.php?page=Pages/procee-dings.php.Esuli, Andrea and Fabrizio Sebastiani.
2007.
SEN-TIWORDNET: a high-coverage lexical resource foropinion mining.
Technical Report ISTI-PP-002/2007,Institute of Information Science and Technologies(ISTI) of the Italian National Research Council(CNR).Hall, Mark, Frank Eibe, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann and Ian H. Witten.
2009.The WEKA data mining software: an update.
In:ACM SIGKDD Explorations Newsletter, 1: 10?18.Huang, Y. S. and C. Y. Suen.
1993.
Behavior-knowledgespace method for combination of multiple classifiers.In Proceedings of IEEE Computer Vision and PatternRecognition, 347?352.Liu, Bing.
2012.
Sentiment analysis and opinion mining.Synthesis Lectures on Human Language Technologies,(5-1), 1?167.Pennebaker, James W., Martha E. Francis and RogerJ.
Booth.
2001.
Linguistic inquiry and word count:LIWC 2001.
Mahway: Lawrence Erlbaum Asso-ciates.Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov and Alan.
Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13.Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.2010.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational Linguistics, 35(3), 399?433.489
