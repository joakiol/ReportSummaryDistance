Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806?814,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning to Follow Navigational DirectionsAdam Vogel and Dan JurafskyDepartment of Computer ScienceStanford University{acvogel,jurafsky}@stanford.eduAbstractWe present a system that learns to fol-low navigational natural language direc-tions.
Where traditional models learnfrom linguistic annotation or word distri-butions, our approach is grounded in theworld, learning by apprenticeship fromroutes through a map paired with Englishdescriptions.
Lacking an explicit align-ment between the text and the referencepath makes it difficult to determine whatportions of the language describe whichaspects of the route.
We learn this corre-spondence with a reinforcement learningalgorithm, using the deviation of the routewe follow from the intended path as a re-ward signal.
We demonstrate that our sys-tem successfully grounds the meaning ofspatial terms like above and south into ge-ometric properties of paths.1 IntroductionSpatial language usage is a vital component forphysically grounded language understanding sys-tems.
Spoken language interfaces to robotic assis-tants (Wei et al, 2009) and Geographic Informa-tion Systems (Wang et al, 2004) must cope withthe inherent ambiguity in spatial descriptions.The semantics of imperative and spatial lan-guage is heavily dependent on the physical set-ting it is situated in, motivating automated learn-ing approaches to acquiring meaning.
Tradi-tional accounts of learning typically rely on lin-guistic annotation (Zettlemoyer and Collins, 2009)or word distributions (Curran, 2003).
In con-trast, we present an apprenticeship learning sys-tem which learns to imitate human instruction fol-lowing, without linguistic annotation.
Solved us-ing a reinforcement learning algorithm, our sys-tem acquires the meaning of spatial words through1.
go vertically down until you?re underneath ehdiamond mine2.
then eh go right until you?re3.
you?re between springbok and highest view-pointFigure 1: A path appears on the instruction giver?smap, who describes it to the instruction follower.grounded interaction with the world.
This drawson the intuition that children learn to use spatiallanguage through a mixture of observing adult lan-guage usage and situated interaction in the world,usually without explicit definitions (Tanz, 1980).Our system learns to follow navigational direc-tions in a route following task.
We evaluate ourapproach on the HCRC Map Task corpus (Ander-son et al, 1991), a collection of spoken dialogsdescribing paths to take through a map.
In thissetting, two participants, the instruction giver andinstruction follower, each have a map composedof named landmarks.
Furthermore, the instruc-tion giver has a route drawn on her map, and itis her task to describe the path to the instructionfollower, who cannot see the reference path.
Oursystem learns to interpret these navigational direc-tions, without access to explicit linguistic annota-tion.We frame direction following as an apprentice-ship learning problem and solve it with a rein-forcement learning algorithm, extending previouswork on interpreting instructions by Branavan etal.
(2009).
Our task is to learn a policy, or mapping806from world state to action, which most closely fol-lows the reference route.
Our state space com-bines world and linguistic features, representingboth our current position on the map and the com-municative content of the utterances we are inter-preting.
During training we have access to the ref-erence path, which allows us to measure the util-ity, or reward, for each step of interpretation.
Us-ing this reward signal as a form of supervision, welearn a policy to maximize the expected reward onunseen examples.2 Related WorkLevit and Roy (2007) developed a spatial seman-tics for the Map Task corpus.
They representinstructions as Navigational Information Units,which decompose the meaning of an instructioninto orthogonal constituents such as the referenceobject, the type of movement, and quantitative as-pect.
For example, they represent the meaning of?move two inches toward the house?
as a referenceobject (the house), a path descriptor (towards), anda quantitative aspect (two inches).
These represen-tations are then combined to form a path throughthe map.
However, they do not learn these rep-resentations from text, leaving natural languageprocessing as an open problem.
The semanticsin our paper is simpler, eschewing quantitative as-pects and path descriptors, and instead focusingon reference objects and frames of reference.
Thissimplifies the learning task, without sacrificing thecore of their representation.Learning to follow instructions by interactingwith the world was recently introduced by Brana-van et al (2009), who developed a system whichlearns to follow Windows Help guides.
Our re-inforcement learning formulation follows closelyfrom their work.
Their approach can incorpo-rate expert supervision into the reward functionin a similar manner to this paper, but is also ableto learn effectively from environment feedbackalone.
The Map Task corpus is free form conversa-tional English, whereas the Windows instructionsare written by a professional.
In the Map Task cor-pus we only observe expert route following behav-ior, but are not told how portions of the text cor-respond to parts of the path, leading to a difficultlearning problem.The semantics of spatial language has beenstudied for some time in the linguistics literature.Talmy (1983) classifies the way spatial meaning isFigure 2: The instruction giver and instruction fol-lower face each other, and cannot see each othersmaps.encoded syntactically, and Fillmore (1997) studiesspatial terms as a subset of deictic language, whichdepends heavily on non-linguistic context.
Levin-son (2003) conducted a cross-linguistic semantictypology of spatial systems.
Levinson categorizesthe frames of reference, or spatial coordinate sys-tems1, into1.
Egocentric: Speaker/hearer centered frameof reference.
Ex: ?the ball to your left?.2.
Allocentric: Speaker independent.
Ex: ?theroad to the north of the house?Levinson further classifies allocentric frames ofreference into absolute, which includes the cardi-nal directions, and intrinsic, which refers to a fea-tured side of an object, such as ?the front of thecar?.
Our spatial feature representation followsthis egocentric/allocentric distinction.
The intrin-sic frame of reference occurs rarely in the MapTask corpus and is ignored, as speakers tend notto mention features of the landmarks beyond theirnames.Regier (1996) studied the learning of spatiallanguage from static 2-D diagrams, learning todistinguish between terms with a connectionistmodel.
He focused on the meaning of individualterms, pairing a diagram with a given word.
Incontrast, we learn from whole texts paired with a1Not all languages exhibit all frames of reference.
Termsfor ?up?
and ?down?
are exhibited in most all languages, while?left?
and ?right?
are absent in some.
Gravity breaks the sym-metry between ?up?
and ?down?
but no such physical distinc-tion exists for ?left?
and ?right?, which contributes to the dif-ficulty children have learning them.807path, which requires learning the correspondencebetween text and world.
We use similar geometricfeatures as Regier, capturing the allocentric frameof reference.Spatial semantics have also been explored inphysically grounded systems.
Kuipers (2000) de-veloped the Spatial Semantic Hierarchy, a knowl-edge representation formalism for representingdifferent levels of granularity in spatial knowl-edge.
It combines sensory, metrical, and topolog-ical information in a single framework.
Kuiperset al demonstrate its effectiveness on a physicalrobot, but did not address the learning problem.More generally, apprenticeship learning is wellstudied in the reinforcement learning literature,where the goal is to mimic the behavior of an ex-pert in some decision making domain.
Notable ex-amples include (Abbeel and Ng, 2004), who traina helicopter controller from pilot demonstration.3 The Map Task CorpusThe HCRC Map Task Corpus (Anderson et al,1991) is a set of dialogs between an instructiongiver and an instruction follower.
Each participanthas a map with small named landmarks.
Addition-ally, the instruction giver has a path drawn on hermap, and must communicate this path to the in-struction follower in natural language.
Figure 1shows a portion of the instruction giver?s map anda sample of the instruction giver language whichdescribes part of the path.The Map Task Corpus consists of 128 dialogs,together with 16 different maps.
The speech hasbeen transcribed and segmented into utterances,based on the length of pauses.
We restrict ourattention to just the utterances of the instructiongiver, ignoring the instruction follower.
This is toreduce redundancy and noise in the data - the in-struction follower rarely introduces new informa-tion, instead asking for clarification or giving con-firmation.
The landmarks on the instruction fol-lower map sometimes differ in location from theinstruction giver?s.
We ignore this caveat, givingthe system access to the instruction giver?s land-marks, without the reference path.Our task is to build an automated instructionfollower.
Whereas the original participants couldspeak freely, our system does not have the abilityto query the instruction giver and must instead relyonly on the previously recorded dialogs.Figure 3: Sample state transition.
Both actions getcredit for visiting the great rock after the indiancountry.
Action a1 also gets credit for passing thegreat rock on the correct side.4 Reinforcement Learning FormulationWe frame the direction following task as a sequen-tial decision making problem.
We interpret ut-terances in order, where our interpretation is ex-pressed by moving on the map.
Our goal is toconstruct a series of moves in the map which mostclosely matches the expert path.We define intermediate steps in our interpreta-tion as states in a set S, and interpretive steps asactions drawn from a set A.
To measure the fi-delity of our path with respect to the expert, wedefine a reward function R : S ?
A?
R+ whichmeasures the utility of choosing a particular actionin a particular state.
Executing action a in state scarries us to a new state s?, and we denote this tran-sition function by s?
= T (s, a).
All transitions aredeterministic in this paper.2For training we are given a set of dialogs D.Each dialog d ?
D is segmented into utter-ances (u1, .
.
.
, um) and is paired with a map,which is composed of a set of named landmarks(l1, .
.
.
, ln).4.1 StateThe states of our decision making problem com-bine both our position in the dialog d and the pathwe have taken so far on the map.
A state s ?
S iscomposed of s = (ui, l, c), where l is the namedlandmark we are located next to and c is a cardinaldirection drawn from {North,South,East,West}which determines which side of l we are on.Lastly, ui is the utterance in d we are currentlyinterpreting.2Our learning algorithm is not dependent on a determin-istic transition function and can be applied to domains withstochastic transitions, such as robot locomotion.8084.2 ActionAn action a ?
A is composed of a named land-mark l, the target of the action, together with acardinal direction c which determines which sideto pass l on.
Additionally, a can be the null action,with l = l?
and c = c?.
In this case, we interpretan utterance without moving on the map.
A targetl together with a cardinal direction c determine apoint on the map, which is a fixed distance from lin the direction of c.We make the assumption that at most one in-struction occurs in a given utterance.
This does notalways hold true - the instruction giver sometimeschains commands together in a single utterance.4.3 TransitionExecuting action a = (l?, c?)
in state s = (ui, l, c)leads us to a new state s?
= T (s, a).
This tran-sition moves us to the next utterance to interpret,and moves our location to the target of the action.If a is the null action, s = (ui+1, l, c), otherwises?
= (ui+1, l?, c?).
Figure 3 displays the state tran-sitions two different actions.To form a path through the map, we connectthese state waypoints with a path planner3 basedon A?, where the landmarks are obstacles.
In aphysical system, this would be replaced with arobot motion planner.4.4 RewardWe define a reward function R(s, a) which mea-sures the utility of executing action a in state s.We wish to construct a route which follows theexpert path as closely as possible.
We consider aproposed route P close to the expert path Pe if Pvisits landmarks in the same order as Pe, and alsopasses them on the correct side.For a given transition s = (ui, l, c), a = (l?, c?
),we have a binary feature indicating if the expertpath moves from l to l?.
In Figure 3, both a1 anda2 visit the next landmark in the correct order.To measure if an action is to the correct side ofa landmark, we have another binary feature indi-cating if Pe passes l?
on side c. In Figure 3, onlya1 passes l?
on the correct side.In addition, we have a feature which counts thenumber of words in ui which also occur in thename of l?.
This encourages us to choose poli-cies which interpret language relevant to a given3We used the Java Path Planning Library, available athttp://www.cs.cmu.edu/?ggordon/PathPlan/.landmark.Our reward function is a linear combination ofthese features.4.5 PolicyWe formally define an interpretive strategy as apolicy pi : S ?
A, a mapping from states to ac-tions.
Our goal is to find a policy pi which max-imizes the expected reward Epi[R(s, pi(s))].
Theexpected reward of following policy pi from states is referred to as the value of s, expressed asV pi(s) = Epi[R(s, pi(s))] (1)When comparing the utilities of executing an ac-tion a in a state s, it is useful to define a functionQpi(s, a) = R(s, a) + V pi(T (s, a))= R(s, a) +Qpi(T (s, a), pi(s)) (2)which measures the utility of executing a, and fol-lowing the policy pi for the remainder.
A given Qfunction implicitly defines a policy pi bypi(s) = maxaQ(s, a).
(3)Basic reinforcement learning methods treatstates as atomic entities, in essence estimating V pias a table.
However, at test time we are followingnew directions for a map we haven?t previouslyseen.
Thus, we represent state/action pairs with afeature vector ?
(s, a) ?
RK .
We then representthe Q function as a linear combination of the fea-tures,Q(s, a) = ?T?
(s, a) (4)and learn weights ?
which most closely approxi-mate the true expected reward.4.6 FeaturesOur features ?
(s, a) are a mixture of world andlinguistic information.
The linguistic informationin our feature representation includes the instruc-tion giver utterance and the names of landmarkson the map.
Additionally, we furnish our algo-rithm with a list of English spatial terms, shownin Table 1.
Our feature set includes approximately200 features.
Learning exactly which words in-fluence decision making is difficult; reinforcementlearning algorithms have problems with the large,sparse feature vectors common in natural languageprocessing.For a given state s = (u, l, c) and action a =(l?, c?
), our feature vector ?
(s, a) is composed ofthe following:809above, below, under, underneath, over, bottom,top, up, down, left, right, north, south, east, west,onTable 1: The list of given spatial terms.?
Coherence: The number of wordsw ?
u thatoccur in the name of l??
Landmark Locality: Binary feature indicat-ing if l?
is the closest landmark to l?
Direction Locality: Binary feature indicat-ing if cardinal direction c?
is the side of l?closest to (l, c)?
Null Action: Binary feature indicating if l?
=NULL?
Allocentric Spatial: Binary feature whichconjoins the side c we pass the landmark onwith each spatial term w ?
u.
This allows usto capture that the word above tends to indi-cate passing to the north of the landmark.?
Egocentric Spatial: Binary feature whichconjoins the cardinal direction we move inwith each spatial term w ?
u.
For instance, if(l, c) is above (l?, c?
), the direction from ourcurrent position is south.
We conjoin this di-rection with each spatial term, giving binaryfeatures such as ?the word down appears inthe utterance and we move to the south?.5 Approximate Dynamic ProgrammingGiven this feature representation, our problem isto find a parameter vector ?
?
RK for whichQ(s, a) = ?T?
(s, a) most closely approximatesE[R(s, a)].
To learn these weights ?
we useSARSA (Sutton and Barto, 1998), an online learn-ing algorithm similar to Q-learning (Watkins andDayan, 1992).Algorithm 1 details the learning algorithm,which we follow here.
We iterate over trainingdocuments d ?
D. In a given state st, we act ac-cording to a probabilistic policy defined in termsof the Q function.
After every transition we up-date ?, which changes how we act in subsequentsteps.Exploration is a key issue in any RL algorithm.If we act greedily with respect to our current Qfunction, we might never visit states which are ac-Input: Dialog set DReward function RFeature function ?Transition function TLearning rate ?tOutput: Feature weights ?1 Initialize ?
to small random values2 until ?
converges do3 foreach Dialog d ?
D do4 Initialize s0 = (l1, u1, ?
),a0 ?
Pr(a0|s0; ?
)5 for t = 0; st non-terminal; t++ do6 Act: st+1 = T (st, at)7 Decide: at+1 ?
Pr(at+1|st+1; ?
)8 Update:9 ??
R(st, at) + ?T?
(st+1, at+1)10 ?
?T?
(st, at)11 ?
?
?
+ ?t?
(st, at)?12 end13 end14 end15 return ?Algorithm 1: The SARSA learning algorithm.tually higher in value.
We utilize Boltzmann ex-ploration, for whichPr(at|st; ?)
=exp( 1?
?T?
(st, at))?a?
exp(1?
?T?
(st, a?
))(5)The parameter ?
is referred to as the tempera-ture, with a higher temperature causing more ex-ploration, and a lower temperature causing moreexploitation.
In our experiments ?
= 2.Acting with this exploration policy, we iteratethrough the training dialogs, updating our fea-ture weights ?
as we go.
The update step looksat two successive state transitions.
Suppose weare in state st, execute action at, receive rewardrt = R(st, at), transition to state st+1, and therechoose action at+1.
The variables of interest are(st, at, rt, st+1, at+1), which motivates the nameSARSA.Our current estimate of the Q function isQ(s, a) = ?T?
(s, a).
By the Bellman equation,for the true Q functionQ(st, at) = R(st, at) + maxa?Q(st+1, a?)
(6)After each action, we want to move ?
to minimizethe temporal difference,R(st, at) +Q(st+1, at+1)?Q(st, at) (7)810Map 4g Map 10gFigure 4: Sample output from the SARSA policy.
The dashed black line is the reference path and thesolid red line is the path the system follows.For each feature ?i(st, at), we change ?i propor-tional to this temporal difference, tempered by alearning rate ?t.
We update ?
according to?
= ?+?t?
(st, at)(R(st, at)+ ?T?
(st+1, at+1)?
?T?
(st, at)) (8)Here ?t is the learning rate, which decays overtime4.
In our case, ?t = 1010+t , which was tuned onthe training set.
We determine convergence of thealgorithm by examining the magnitude of updatesto ?.
We stop the algorithm when||?t+1 ?
?t||?
<  (9)6 Experimental DesignWe evaluate our system on the Map Task corpus,splitting the corpus into 96 training dialogs and 32test dialogs.
The whole corpus consists of approx-imately 105,000 word tokens.
The maps seen attest time do not occur in the training set, but someof the human participants are present in both.4To guarantee convergence, we requirePt ?t = ?
andPt ?2t < ?.
Intuitively, the sum diverging guarantees wecan still learn arbitrarily far into the future, and the sum ofsquares converging guarantees that our updates will convergeat some point.6.1 EvaluationWe evaluate how closely the path P generated byour system follows the expert path Pe.
We mea-sure this with respect to two metrics: the orderin which we visit landmarks and the side we passthem on.To determine the order Pe visits landmarks wecompute the minimum distance from Pe to eachlandmark, and threshold it at a fixed value.To score path P , we compare the order it visitslandmarks to the expert path.
A transition l ?
l?which occurs in P counts as correct if the sametransition occurs in Pe.
Let |P | be the numberof landmark transitions in a path P , and N thenumber of correct transitions in P .
We define theorder precision as N/|P |, and the order recall asN/|Pe|.We also evaluate how well we are at passinglandmarks on the correct side.
We calculate thedistance of Pe to each side of the landmark, con-sidering the path to visit a side of the landmarkif the distance is below a threshold.
This meansthat a path might be considered to visit multiplesides of a landmark, although in practice it is usu-811Figure 5: This figure shows the relative weights of spatial features organized by spatial word.
The toprow shows the weights of allocentric (landmark-centered) features.
For example, the top left figure showsthat when the word above occurs, our policy prefers to go to the north of the target landmark.
The bottomrow shows the weights of egocentric (absolute) spatial features.
The bottom left figure shows that giventhe word above, our policy prefers to move in a southerly cardinal direction.ally one.
If C is the number of landmarks we passon the correct side, define the side precision asC/|P |, and the side recall as C/|Pe|.6.2 Comparison SystemsThe baseline policy simply visits the closest land-mark at each step, taking the side of the landmarkwhich is closest.
It pays no attention to the direc-tion language.We also compare against the policy gradientlearning algorithm of Branavan et al (2009).
Theyparametrize a probabilistic policy Pr(s|a; ?)
as alog-linear model, in a similar fashion to our explo-ration policy.
During training, the learning algo-rithm adjusts the weights ?
according to the gradi-ent of the value function defined by this distribu-tion.Reinforcement learning algorithms can be clas-sified into value based and policy based.
Valuemethods estimate a value function V for eachstate, then act greedily with respect to it.
Pol-icy learning algorithms directly search throughthe space of policies.
SARSA is a value basedmethod, and the policy gradient algorithm is pol-icy based.Visit Order SideP R F1 P R F1Baseline 28.4 37.2 32.2 46.1 60.3 52.2PG 31.1 43.9 36.4 49.5 69.9 57.9SARSA 45.7 51.0 48.2 58.0 64.7 61.2Table 2: Experimental results.
Visit order showshow well we follow the order in which the answerpath visits landmarks.
?Side?
shows how success-fully we pass on the correct side of landmarks.7 ResultsTable 2 details the quantitative performance of thedifferent algorithms.
Both SARSA and the policygradient method outperform the baseline, but stillfall significantly short of expert performance.
Thebaseline policy performs surprisingly well, espe-cially at selecting the correct side to visit a land-mark.The disparity between learning approaches andgold standard performance can be attributed toseveral factors.
The language in this corpus is con-versational, frequently ungrammatical, and con-tains troublesome aspects of dialog such as con-versational repairs and repetition.
Secondly, ouraction and feature space are relatively primitive,and don?t capture the full range of spatial expres-sion.
Path descriptors, such as the difference be-tween around and past are absent, and our feature812representation is relatively simple.The SARSA learning algorithm accrues morereward than the policy gradient algorithm.
Likemost gradient based optimization methods, policygradient algorithms oftentimes get stuck in localmaxima, and are sensitive to the initial conditions.Furthermore, as the size of the feature vectorK in-creases, the space becomes even more difficult tosearch.
There are no guarantees that SARSA hasreached the best policy under our feature space,and this is difficult to determine empirically.
Thus,some accuracy might be gained by consideringdifferent RL algorithms.8 DiscussionExamining the feature weights ?
sheds some lighton our performance.
Figure 5 shows the relativestrength of weights for several spatial terms.
Re-call that the two main classes of spatial features in?
are egocentric (what direction we move in) andallocentric (on which side we pass a landmark),combined with each spatial word.Allocentric terms such as above and below tendto be interpreted as going to the north and southof landmarks, respectively.
Interestingly, our sys-tem tends to move in the opposite cardinal direc-tion, i.e.
the agent moves south in the egocen-tric frame of reference.
This suggests that peopleuse above when we are already above a landmark.South slightly favors passing on the south side oflandmarks, and has a heavy tendency to move ina southerly direction.
This suggests that south isused more frequently in an egocentric referenceframe.Our system has difficulty learning the meaningof right.
Right is often used as a conversationalfiller, and also for dialog alignment, such as?right okay right go vertically up thenbetween the springboks and the highestviewpoint.
?Furthermore, right can be used in both an egocen-tric or allocentric reference frame.
Compare?go to the uh right of the mine?which utilizes an allocentric frame, with?right then go eh uh to your right hori-zontally?which uses an egocentric frame of reference.
Itis difficult to distinguish between these meaningswithout syntactic features.9 ConclusionWe presented a reinforcement learning systemwhich learns to interpret natural language direc-tions.
Critically, our approach uses no semanticannotation, instead learning directly from humandemonstration.
It successfully acquires a subsetof spatial semantics, using reinforcement learningto derive the correspondence between instructionlanguage and features of paths.
While our resultsare still preliminary, we believe our model repre-sents a significant advance in learning natural lan-guage meaning, drawing its supervision from hu-man demonstration rather than word distributionsor hand-labeled semantic tags.
Framing languageacquisition as apprenticeship learning is a fruitfulresearch direction which has the potential to con-nect the symbolic, linguistic domain to the non-symbolic, sensory aspects of cognition.AcknowledgmentsThis research was partially supported by the Na-tional Science Foundation via a Graduate Re-search Fellowship to the first author and awardIIS-0811974 to the second author and by the AirForce Research Laboratory (AFRL), under primecontract no.
FA8750-09-C-0181.
Thanks toMichael Levit and Deb Roy for providing digitalrepresentations of the maps and a subset of the cor-pus annotated with their spatial representation.ReferencesPieter Abbeel and Andrew Y. Ng.
2004.
Apprentice-ship learning via inverse reinforcement learning.
InProceedings of the Twenty-first International Con-ference on Machine Learning.
ACM Press.A.
Anderson, M. Bader, E. Bard, E. Boyle, G. Do-herty, S. Garrod, S. Isard, J. Kowtko, J. Mcallister,J.
Miller, C. Sotillo, H. Thompson, and R. Weinert.1991.
The HCRC map task corpus.
Language andSpeech, 34, pages 351?366.S.R.K.
Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In ACL-IJCNLP?09.James Richard Curran.
2003.
From Distributional toSemantic Similarity.
Ph.D. thesis, University of Ed-inburgh.Charles Fillmore.
1997.
Lectures on Deixis.
Stanford:CSLI Publications.Benjamin Kuipers.
2000.
The spatial semantic hierar-chy.
Artificial Intelligence, 119(1-2):191?233.813Stephen Levinson.
2003.
Space In Language AndCognition: Explorations In Cognitive Diversity.Cambridge University Press.Michael Levit and Deb Roy.
2007.
Interpretationof spatial language in a map navigation task.
InIEEE Transactions on Systems, Man, and Cybernet-ics, Part B, 37(3), pages 667?679.Terry Regier.
1996.
The Human Semantic Potential:Spatial Language and Constrained Connectionism.The MIT Press.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: An Introduction.
MIT Press.Leonard Talmy.
1983.
How language structures space.In Spatial Orientation: Theory, Research, and Ap-plication.Christine Tanz.
1980.
Studies in the acquisition of de-ictic terms.
Cambridge University Press.Hongmei Wang, Alan M. Maceachren, and GuorayCai.
2004.
Design of human-GIS dialogue for com-munication of vague spatial concepts.
In GIScience.C.
J. C. H. Watkins and P. Dayan.
1992.
Q-learning.Machine Learning, pages 8:279?292.Yuan Wei, Emma Brunskill, Thomas Kollar, andNicholas Roy.
2009.
Where to go: interpreting nat-ural directions using global inference.
In ICRA?09:Proceedings of the 2009 IEEE international con-ference on Robotics and Automation, pages 3761?3767, Piscataway, NJ, USA.
IEEE Press.Luke S. Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In ACL-IJCNLP ?09, pages976?984.814
