Proceedings of the ACL 2010 Student Research Workshop, pages 97?102,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsWeakly Supervised Learning of Presupposition Relations between VerbsGalina TremperDepartment of Computational LinguisticsHeidelberg University, Germanytremper@cl.uni-heidelberg.deAbstractPresupposition relations between verbs arenot very well covered in existing lexicalsemantic resources.
We propose a weaklysupervised algorithm for learning presup-position relations between verbs that dis-tinguishes five semantic relations: presup-position, entailment, temporal inclusion,antonymy and other/no relation.
We startwith a number of seed verb pairs selectedmanually for each semantic relation andclassify unseen verb pairs.
Our algorithmachieves an overall accuracy of 36% fortype-based classification.1 IntroductionA main characteristics of natural language is thatsignificant portions of content conveyed in a mes-sage may not be overtly realized.
This is the casefor presuppositions: e.g, the utterance Columbusdidn?t manage to reach India.
presupposes thatColumbus had tried to reach India.
This presup-position does not need to be stated, but is im-plicitly understood.
Determining the presupposi-tions of events reported in texts can be exploitedto improve the quality of many natural languageprocessing applications, such as information ex-traction, text understanding, text summarization,question-answering or machine translation.The phenomenon of presupposition has beenthroughly investigated by philosophers and lin-guists (i.a.
Stalnaker, 1974; van der Sandt, 1992).There are only few attempts for practical imple-mentations of presupposition in computational lin-guistics (e.g.
Bos, 2003).
Especially, presupposi-tion is understudied in the field of corpus-basedlearning of semantic relations.
Machine learningmethods have been previously applied to deter-mine semantic relations such as is-a and part-of,also succession, reaction and production (Panteland Pennacchiotti, 2006).
Chklovski and Pantel(2004) explored classification of fine-grained verbsemantic relations, such as similarity, strength,antonymy, enablement and happens-before.
Forthe task of entailment recognition, learning of en-tailment relations was attempted (Pekar, 2008).None of the previous work investigated subclassi-fying semantic relations including presuppositionand entailment, two relations that are closely re-lated, but behave differently in context.In particular, the inferential behaviour of pre-suppositions and entailments crucially differs inspecial semantic contexts.
E.g., while presup-positions are preserved under negation (as inColumbus managed/didn?t manage to reach In-dia the presupposition tried to), entailments donot survive under negation (John F. Kennedyhas been/has not been killed).
Here the entail-ment died only survives in the positive sentence.Such differences are crucial for both analysis andgeneration-oriented NLP tasks.This paper presents a weakly supervised al-gorithm for learning presupposition relations be-tween verbs cast as a discriminative classificationproblem.
The structure of the paper is as follows:Section 2 reviews state of the art.
Section 3 intro-duces our task and the learning algorithm.
Section4 reports on experiment organization; the resultsare presented in Section 5.
Finally, we summariseand present objectives for future work.2 Related WorkOne of the existing semantic resources related toour paper is WordNet (Fellbaum, 1998).
It com-prises lexical semantic information about Englishnouns, verbs, adjectives and adverbs.
Among thesemantic relations defined specifically for verbsare entailment, hyponymy, troponymy, antonymyand cause.
However, not all of them are well cov-ered, for example, there are only few entries forpresupposition and entailment in WordNet.97One attempt to acquire fine-grained semanticrelations from corpora is VerbOcean (Chklovskiand Pantel, 2004).
Chklovski and Pantel used asemi-automatic approach for extracting semanticrelations between verbs using a list of patterns.The selection of the semantic relations was in-spired by WordNet.
VerbOcean showed good ac-curacy values for the antonymy (50%), similar-ity (63%) and strength (75%) relations.
How-ever, VerbOcean doesn?t distinguish between en-tailment and presupposition; they are conflated inthe classes enablement and happens-before.A distributional method for extracting highlyassociated verbs was proposed by Lin and Pantel(2001).
This method extracts semantically relatedwords with good precision, but it does not deter-mine the type and symmetry of the relation.
How-ever, the method is able to recognize the existenceof semantic relations holding between verbs andhence can be used as a basis for finding and furtherdiscriminating more detailed semantic relations.3 A Weakly Supervised Approach toLearning Presupposition RelationsWe describe a weakly supervised approach forlearning semantic relations between verbs includ-ing implicit relations such as presupposition.
Ouraim is to perform a type-based classification ofverb pairs.
I.e., we determine the class of a verb-pair relation by observing co-occurrences of theseverbs in contexts that are indicative for their in-trinsic meaning relation.
This task differs from atoken-based classification, which aims at classify-ing each verb pair instance as it occurs in context.Classified relations.
We distinguish betweenthe five classes of semantic relations presented inTable 1.
We chose entailment, temporal inclu-sion and antonymy, because these relations maybe confounded with the presupposition relation.A special class other/no comprises semantic rela-tions not discussed in this paper (e.g.
synonymy)and verb pairs that are not related by a semantic re-lation.
The relations can be subdivided into sym-metric and asymmetric relations, and relations thatinvolve temporal sequence, or those that do not in-volve a temporal order, as displayed in Table 1.A Weakly Supervised Learning Approach.Our algorithm starts with a small number of seedverb pairs selected manually for each relation anditeratively classifies a large set of unseen and un-Semantic Example Symmetry TemporalRelation SequencePresuppo- find - seek, asymmetric yessition answer - askEntailment look - see, asymmetric yesbuy - ownTemporal walk - step, symmetric noInclusion talk - whisperAntonymy win - lose, symmetric nolove - hateOther/no have - own, undefined undefinedsing - jumpTable 1: Selected Semantic Relationslabeled verb pairs.
Each iteration has two phases:1.
Training the Classifiers We independentlytrain binary classifiers for each semantic re-lation using both shallow and deep features.2.
Ensemble Learning and Ranking Each ofthe five classifiers is applied to each sentencefrom an unlabeled corpus.
The predictionsof the classifiers are combined using ensem-ble learning techniques to determine the mostconfident classification.
The obtained list ofthe classified instances is ranked using pat-tern scores, in order to select the most reliablecandidates for extension of the training set.Features.
Both shallow lexical-syntactic anddeep syntactic features are used for the classifica-tion of semantic relations.
They include:1. the distance between two analyzed verbs andthe order of their appearance2.
verb form (tense, aspect, modality, voice),presence of negation and polarity verbs13.
coordinating/subordinating conjunctions4.
adverbial adjuncts5.
PoS-tag-contexts (two words preceding andtwo words following each verb)6. the length of the path of grammatical func-tions relating the two verbs7.
co-reference relation holding between thesubjects and objects of the verbs (both verbshave the same subject/object, subject of oneverb corresponds to the object of the secondor there is no relation between them).In order to extract these features the trainingcorpus is parsed using a deep parser.1Polarity verbs are taken from the polarity lexicon ofNairn et al (2006).
It encodes whether the complement ofproposition embedding verbs is true or false.
We used theverbs themselves as a feature without their polarity-tags.984 Experimental SettingInitial Subset of Verb Pair Candidates.
Unlikeother semi-supervised approaches, we don?t usepatterns for acquiring new candidates for classi-fication.
Candidate verb pairs are obtained froma previously compiled list of highly associatedverbs.
We use the DIRT Collection (Lin and Pan-tel, 2001) from which we further extract pairs ofhighly associated verbs as candidates for classifi-cation.
The advantage of this resource is that itconsists of pairs of verbs which stand in a semanticrelation (cf.
Section 2).
This considerably reducesthe number of verb pairs that need to be processedas candidates in our classification task.DIRT contains 5,604 verb types and 808,764verb pair types.
This still represents a huge num-ber of verb pairs to be processed.
We thereforefiltered the extracted set by checking verb pair fre-quency in the first three parts of the ukWAC cor-pus (Baroni et al, 2009) (UKWAC 1. .
.
3) and byapplying the PMI test with threshold 2.0.
This re-duces the number of verb pairs to 199,393.For each semantic relation we select three verbpairs as seeds.
The only exception is temporal in-clusion for which we selected six verb pairs, dueto the low frequency of such verb pairs within asingle sentence.
These verb pairs were used forbuilding an initial training corpus of verb pairs incontext.
The remaining verb pairs are used to buildthe corpus of unlabeled verb pairs in context in theiterative classification process.Preprocessing.
Given these verb pairs, we ex-tracted sentences for training and for unlabeleddata set from the first three parts of the UKWACcorpus (Baroni et al, 2009).
We compiled a set ofCQP queries (Evert, 2005) to find sentences thatcontain both verbs of a verb pair and applied themon UKWAC 1. .
.
3 to build the training and un-labeled subcorpora.
We filter out sentences withmore than 60 words and sentences with a dis-tance between verbs exceeding 20 words.
To avoidgrowing complexity, only sentences with exactlyone occurrence of each verb pair are retained.
Wealso remove sentences that trigger wrong candi-dates, in which the auxiliaries have or do appearin a candidate verb pair.The corpus is parsed using the XLE parser(Crouch et al, 2008).
Its output contains both thestructural and functional information we need toextract the shallow and deep features used in theclassification, and to generate patterns.Training Corpus.
From this preprocessed cor-pus, we created a training corpus that containsthree different components:1.
Manually annotated training set.
All sen-tences containing seed verb pairs extractedfrom UKWAC 1 are annotated manually withtwo values true/false in order to separate thenegative training data.2.
Automatically annotated training set.
Webuild an extended, heuristically annotatedtraining set for the seed verb pairs, by ex-tracting further instances from the remainingcorpora (UKWAC 2 and UKWAC 3).
Usingthe manual annotations of step 1., we manu-ally compiled a small stoplist of patterns thatare used to filter out wrong instances.
Theconstructed stoplist serves as an elementarydisambiguation step.
For example, the verbslook and see can stand in an entailment rela-tion if look is followed by the prepositions at,on, in, but not in case of prepositions after orforward (e.g.
looking forward to).3.
Synonymous verb pairs.
To further enrichthe training set of data, synonyms of theverb pairs are manually selected from Word-Net.
The corresponding verb pairs were ex-tracted from UKWAC 1. .
.
3.
In order toavoid adding noise, we used only synonymsof unambiguous verbs.
The problem of am-biguity of the target verbs wasn?t consideredat this step.The overall size of the training set for the firstclassification step is 15,717 sentences from which5,032 are manually labeled, 9,918 sentences areautomatically labeled and 757 sentences containsynonymous verb pairs.
The distribution is unbal-anced: temporal inclusion e.g.
covers only 2%,while entailment covers 39% of sentences.
Webalanced the training set by undersampling entail-ment and other/no by 20% and correspondinglyoversampling the temporal inclusion class.Patterns.
Similar to other pattern-based ap-proaches we use a set of seed verb pairs to induceindicative patterns for each semantic relation.
Weuse the induced patterns to restrict the number ofthe verb pair candidates and to rank the labelledinstances in the iterative classification step.The patterns use information about the verbforms of analyzed verb pairs, modal verbs and the99polarity verbs (only if they are related to the ana-lyzed verbs) and coordinating/subordinating con-junctions connecting two verbs.
The analyzedverbs in the sentence are substituted with V1 andV2 placeholders in the pattern.
For example, forthe sentence: Here we should be careful for thereare those who seek and do not find.
and the verbpair (find,seek) we induce the following pattern:V2 and do [not|n?t] V1.
The patterns are extractedautomatically from deep parses of the training cor-pus.
Examples of the best patterns we determinedfor semantic relations are presented in Table 2.Semantic Relation PatternsPresupposition V2-ed * though * was * V1-ed,V2-ed * but was [not|n?t] V1-ed,V2-ing * might V1Entailment if * V1 * V2,V1-ing * [shall|will|?ll] V2,V2 * by V1-ingTemporal V2 * V1-ing,Inclusion V1-ing and V2-ing,when V2 * V1Antonymy V1 or * V2,either * V1 or * V2,V1-ed * but V2-edOther/no V1 * V2,V1-ing * V2-ing,V2-ed * and * V1-edTable 2: Patterns for Selected Semantic RelationsPattern ranks are used to compute the reliabil-ity score for instances, as proposed by Pantel andPennacchiotti (2006).
The pattern reliability is cal-culated as follows:rpi(p) =1|I|?i?Ipmi(i,p)maxpmi?
ri(i) (1)where:pmi(i, p) - pointwise mutual information (PMI)between the instance i and the pattern p;maxpmi- maximum PMI between all patterns andall instances;ri(i) - reliability of an instance i.
For seedsri(i) = 1 (they are selected manually), for the nextiterations the instance reliability is:ri(i) =1|P |?p?Ppmi(i,p)maxpmi?
rpi(p) (2)We also consider using the patterns as a featurefor classification, in case they turn out to be suffi-ciently discriminative.Training Binary Classifiers.
We independentlytrain 5 binary classifiers, one for each semantic re-lation, using the J48 decision tree algorithm (Wit-ten and Frank, 2005).Data Sets.
As the primary goal of this paper isto classify semantic relations on the type level, weelaborated a first gold standard dataset for type-based classification.
We used a small sample of100 verb pairs randomly selected from the auto-matically labeled corpus.
This sample was man-ually annotated by two judges after we had elim-inated the system annotations in order not to in-fluence the judges?
decisions.
The judges had thepossibility to select more than one annotation, ifnecessary.
We measured inter-annotator agree-ment was 61% (k ?
0.21).
The low agreementshows the difficulty of decision in the annotationof fine-grained semantic relations.2While the first gold standard dataset of verbpairs was annotated out of context, we constructeda second gold standard of verb pairs annotated atthe token level, i.e.
in context.
This second dataset can be used to evaluate a token-based classi-fier (a task not attempted in the present paper).
Italso offers a ground truth for type-based classifi-cation, in that it controls for contextual ambiguityeffects.
I.e., we can extract a type-based gold stan-dard on the basis of the token-annotated data.3Weproposed to one judge to annotate the same 100verb pair types as in the previous annotation task,this time in context.
For this purpose we randomlyselected 10 instances for each verb pair type (forrare verb pair types only 5).
We compared the goldstandards elaborated by the same judge for type-based and token-based classification:?
62% of verb pair types were annotated withthe same labels on both levels, indicating cor-rect annotation?
10% of verb pair types were assigned con-flicting labels, indicating wrong annotation?
28% of verb pair types were assigned labelsnot present on the type level, or the type levellabel was not assigned in contextThe figures show that for the most part the type-based annotation conforms with the ground truthobtained from token-based annotation.
Only 10%of verb pair types were established as conflictingwith the ground truth.
The remaining 28% can beconsidered as potentially correct: either the anno-tated data does not contain the appropriate con-text for a given type label or the type-level anno-2Data inspection revealed that one annotator was more ex-perienced in semantic annotation tasks.
We evaluate our sys-tem using the annotations of only one judge.3This option was not pursued in the present paper.100tation, performed without context, does not fore-see an existing relation.
This points to a generaldifficulty, namely to acquire representative datasets for token-level annotation, and also to per-form type-level annotations without context forthe present task.Combining Classifiers in Ensemble Learning.Both token-based and type-based classificationstarts with determining of the most confident clas-sification for instances.
Each instance of the cor-pus of unlabeled verb pairs is classified by the in-dividual binary classifiers.
In order to select themost confident classification we compare the votesof the individual classifiers as follows:1.
If an instance is classified by one of the clas-sifiers as true with confidence less than 0.75,we discard this classification.2.
If an instance is classified as true by morethan one classifier, we consider only the clas-sification with the highest confidence.4In contrast to token-based classification that ac-cepts only one semantic relation, for type-basedclassification we allow the existence of more thanone semantic relation for a verb pair.
To avoid theunreliable classifications, we apply several filters:1.
If less than 10% of the instances for a verbpair are classified with some specific seman-tic relation, this classification is considered tobe unconfident and is discarded.2.
If a verb pair is classified as positive for morethan three semantic relations, this verb pairremains unclassified.3.
If a verb pair is classified with up to three se-mantic relations and if more than 10% of theexamples are classified with any of these rela-tions, the verb pair is labeled with all of them.Iteration and Stopping Criterion.
After deter-mining the most confident classification we rankthe instances, following the ranking procedure ofPantel and Pennacchiotti (2006).
Instances thatexceed a reliability threshold (0.3 for our exper-iment) are selected for the extended training set.The remainining instances are returned to the un-labeled set.
The algorithm stops if the average re-liability score is smaller than a threshold value.
Inour paper we concentrate on the first iteration.
Ex-tension of the training set and re-ranking of pat-terns will be reported in future work.4We assume that within a given context a verb pair canexhibit only one relation.Semantic relation Majority Without Baseline(Count1/Count2) NONEPresupposition (12/22) 67% 36% 18%Entailment (9/20) 67% 35% 8%Temp.
Inclusion (7/11) 71% 36% 19%Antonymy (11/24) 72% 42% 12%NONE (61/29) 49% 31% 43%Macro-Average 56% 36%Micro-Average 65% 36%Table 3: Accuracy for type-based classification5 Evaluation ResultsResults for type-based classification.
We eval-uate the accuracy of classification based on twoalternative measures:1.
Majority - the semantic relation with whichthe majority of the sentences containing averb pair have been annotated.2.
Without NONE - as in 1., but after removingthe label NONE from all relation assignmentsexcept for those cases where NONE is theonly label assigned to a verb pair.5We computed accuracy as the number of verbpairs which were correctly labeled by the systemdivided by the total number of system labels.
Wecompare our results against a baseline of randomassignment, taking the distribution found in themanually labeled gold standard as the underlyingverb relation distribution.
Table 3 shows the accu-racy results for each semantic relation6.Results for token-based classification.
We alsoevaluate the accuracy of classification for token-based classification as the number of instanceswhich were correctly labeled by the system di-vided by the total number of system labels.
Asthe baseline we took the relation distribution onthe token level.
Table 4 shows the accuracy resultsfor each semantic relation.Discussion.
The results obtained for type-basedclassification are well above the baseline with oneexception.
The best performance is achieved byantonymy (72% and 42% respectively for both5The second measure was used because in many cases therelation NONE has been determined to be the majority class.6Count1 is the total number of system labels for the Ma-jority measure and Count2 is the total number of system la-bels for the Without NONE measure.101Semantic relation Count Accuracy BaselinePresupposition 43 21% 8%Entailment 39 15% 5%Temp.
Inclusion 15 13% 3%Antonymy 34 29% 5%NONE 511 81% 79%Macro-Average 61%Micro-Average 31%Table 4: Accuracy for token-based classificationmeasures), followed by temporal inclusion, pre-supposition and entailment.
Accuracy scores fortoken-based classification (excluding NONE) arelower at 29% to 13%.
Error analysis of randomlyselected false positives shows that the main reasonfor lower accuracy on the token level is that thecontext is not always significant enough to deter-mine the correct relation.Comparison to Related Work.
Other projectssuch as VerbOcean (Chklovski and Pantel, 2004)report higher accuracy: the average accuracy is65.5% if at least one tag is correct and 53% forthe correct preferred tag.
However, we cannot ob-jectively compare the results of VerbOcean to oursystem because of the difference in the set of re-lation classes and evaluation procedures.
Simi-lar to us, Chklovski and Pantel (2004) evaluatedVerbOcean using a small sample of data whichwas presented to two judges for manual evalua-tion.
In contrast to our setup, they didn?t removethe system annotations from the evaluation dataset.
Given the difficulty of the classification wesuspect that correction of system output relationsfor establishing a gold standard bears a strong riskin favouring system classifications.6 Conclusion and Future WorkThe results achieved in our experiment show thatweakly supervised methods can be applied forlearning presupposition relations between verbs.Our work also shows that they are more difficultto classify than other typical lexical semantic rela-tions, such as antonymy.
Error analysis suggeststhat many errors can be avoided if verbs are dis-ambiguated in context.
It would be interesting totest our algorithm with different amounts of man-ually annotated training sets and different combi-nations of manually and automatically annotatedtraining sets to determine the minimal amount ofdata needed to assure good accuracy.In future work we will integrate word sensedisambiguation as well as information aboutpredicate-argument structure.
Also, we are go-ing to analyze the influence of single features onthe classification and determining optimal featuresets, as well as the question of including patternsin the feature set.
In this paper we used the samecombination of features for all classifiers.7 AcknowledgementsI would like to thank Anette Frank for supervisionof this work, Dekang Lin and Patrick Pantel forsharing the DIRT resource and Carina Silberer andChristine Neupert creation of the gold standard.ReferencesBaroni, M., Bernardini, S., Ferraresi, A., Zanchetta, E.:The WaCky Wide Web: a collection of very largelinguistically processed web-crawled corpora.
Jour-nal of Language Resources and Evaluation, Vol.43(3), 209?226 (2009)Bos, J.: Implementing the Binding and Accommoda-tion Theory for Anaphora Resolution and Presuppo-sition Projection.
Computational Linguistics, Vol.29(2), 179?210 (2003)Chklovski, T., Pantel, P.: Verbocean: Mining the webfor fine-grained semantic verb relations.
Proceed-ings of EMNLP 2004, 33?40, Barcelona (2004)Crouch, D., Dalrymple, M., Kaplan, R., King, T.,Maxwell, J., Newman, P.: XLE Documentation.Palo Alto Research Center (2008)Evert, S.: The CQP Query Language Tutorial (CWBVersion 2.2.b90).
IMS, Stuttgart (2005)Fellbaum, C.: WordNet: An Electronic LexicalDatabase.
1st edition, MIT Press (1998)Lin, D., Pantel, P.: Discovery of Inference Rules forQuestion Answering.
Natural Language Engineer-ing, Vol.7, 343?360 (2001)Nairn, R., Condoravdi, C., Karttunen, L.: Comput-ing Relative Polarity for Textual Inference.
Proc.
ofICoS-5, Buxton, UK (2006)Pantel, P., Pennacchiotti, M.: Espresso: LeveragingGeneric Patterns for Automatically Harvesting Se-mantic Relations.
COLING 2006, 113-120 (2006)Pekar, V.: Discovery of event entailment knowledgefrom text corpora.
Computer Speech & Language,Vol.22 (1), 1?16 (2008)Stalnaker, R.C.
: Pragmatic Presuppositions.
Semanticsand Philosophy, New York: Univ.
Press (1974)van der Sandt, R.: Presupposition Projection asAnaphora Resolution.
Journal of Semantics, Vol.9,333?377 (1992)Witten, I., Frank, E.: Data Mining: Practical MachineLearning Tools and Techniques.
(2005)102
