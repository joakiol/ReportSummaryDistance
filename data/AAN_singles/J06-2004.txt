The PARADISE Evaluation Framework:Issues and FindingsMelita Hajdinjak?University of LjubljanaFrance Mihelic?
?University of LjubljanaThere has been a great deal of interest over the past 20 years in developing metrics andframeworks for evaluating and comparing the performance of spoken-language dialogue sys-tems.
One of the results of this interest is a potential general methodology, known as thePARADISE framework.
This squib highlights some important issues concerning the applicationof PARADISE that have, up to now, not been sufficiently emphasized or have even been neglectedby the dialogue-system community.
These include considerations regarding the selection ofappropriate regression parameters, normalization effects on the accuracy of the prediction, theinfluence of speech-recognition errors on the performance function, and the selection of anappropriate user-satisfaction measure.
In addition, it gives the results of an evaluation of datafrom two Wizard-of-Oz experiments.
These evaluations include different dependent variablesand examination of individual user-satisfaction measures.1.
IntroductionA long list of objective dialogue metrics (Danieli and Gerbino 1995; Smith and Gordon1997) for dialogue evaluation, which can be calculated without recourse to human judg-ment, and subjective dialogue metrics (Shriberg, Wade, and Price 1992; Danieli andGerbino 1995), which are based on human judgments, have been proposed.
Their well-known limitations led Walker, Litman, Kamm, and Abella (1997) to propose their para-digm for dialogue system evaluation (PARADISE), a potentially general methodologyfor evaluating spoken-language dialogue systems, the goal of which was to compareand optimize different dialogue managers and task domains independently.The PARADISE framework maintains that the system?s primary objective is tomaximize user satisfaction (Shriberg, Wade, and Price 1992), and it derives a combinedperformance metric for a dialogue system as a weighted linear combination of task-success measures and dialogue costs.
The dialogue costs are of two types: dialogue-efficiency costs (e.g., number of utterances, dialogue time), which are measures ofthe system?s efficiency in helping the user to complete the task, and dialogue-qualitycosts (e.g., system-response delay, mean recognition score), which are intended to cap-ture other aspects that can have large effects on the user?s perception of the system?sperformance.Applying PARADISE to dialogue data requires dialogue corpora to be collectedvia controlled experiments during which users subjectively rate their satisfaction.
Here,user satisfaction is calculated with a survey (Walker et al 1998) that asks users tospecify the degree to which they agree with several statements about the performance?
Faculty of Electrical Engineering, Trz?as?ka 25, 1000 Ljubljana, Slovenia?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 2of the system.
In addition, the other parameters of the model of performance, i.e., thetask-success measures and the dialogue costs, must be either automatically logged bythe system or be hand-labeled.
The PARADISE model of performance posits that aperformance function can then be derived by applying multivariate linear regression(MLR) with user satisfaction as the dependent variable and task-success measures anddialogue costs as the independent variables.The squib addresses some PARADISE issues (with most of them arising from theapplication of MLR) that have, up to now, not been sufficiently emphasized or haveeven been neglected by the dialogue-system community.
Moreover, most of the con-siderations about these issues are supported by the results of applying PARADISE tothe data from two Wizard-of-Oz (WOZ) experiments (Hajdinjak and Mihelic?
2004a)carried out during the development of a weather-information-providing, natural-language spoken dialogue system (Z?ibert et al 2003).
In contrast to previous PARA-DISE applications, we evaluated data that were collected in the early stages of adialogue system?s design where speech understanding was simulated by a human.2.
PARADISE-Framework IssuesThe PARADISE model of performance (Walker et al 1998) is defined as follows:Performance = (?
?
N (?))
?n?i=1wi ?
N (ci) (1)Here, ?
is the weight on the kappa coefficient ?, which is calculated from a confusionmatrix that summarizes how well the dialogue system achieves the information require-ments of particular tasks within the dialogue and measures task success, wi are weightson the costs ci, and N is a Z-score normalization function:N (x) = x ?
x0?x0 (2)where x0 and ?x0 are the mean value and the standard deviation for x, respectively,computed from the sample set of observations.
Normalization guarantees that theweights directly indicate the relative contributions to the performance function, whichcan be used to predict user satisfaction, i.e., the value of the dependent variable.2.1 Relationships between MLR VariablesNevertheless, MLR sets some constraints on the relationships between the parameters(Seber 1977).
Most obviously, there must exist at least an approximately linear relation-ship between the dependent and the independent variables.
On the other hand, it canbe shown (Patel and Bruce 1995) that dropping the predictors, i.e., the independentvariables, that are not statistically significant to the dependent variable (i.e., the p valueis greater than 0.05) can reduce the average error of the predictions.Furthermore, the predictors must not correlate highly (i.e., the absolute values ofthe correlation coefficients must not be greater than 0.7).
If they do, small errors orvariations in the values of sample observations can have a large effect on the weightsin the performance function (1).
Therefore, the redundant predictors must be removedbefore applying MLR.
Obviously, it is reasonable to remove those predictors that are264Hajdinjak and Mihelic?
The PARADISE Evaluation Frameworkless statistically significant (i.e., with greater p values) to the dependent variable.
Note,high correlations may exist even between variables that seem unrelated (Section 3.2).2.2 Normalization EffectsMLR is based on the least squares method, where the model minimizes the sum of thesquares of the differences between the observed and the predicted values.
Hence,N (US) = N?
(US) +  (3)where N (US) is the normalized observed user-satisfaction value, N?
(US) is the predictednormalized user-satisfaction value, and  is the error of the prediction.
Further,US = N?
(US)?US0 + US0 + ?US0 = ?US + ?US0 (4)is the MLR model for the unnormalized user-satisfaction values, where US0 and ?US0are the mean value and the standard deviation of the sample set of observed user-satisfaction values, respectively.
Note, the initial noise variable  is multiplied by ?US0 .There are several different measures of goodness of fit for a regression model, butthe most widely used is the coefficient of determination R2.
In a regression model withnormalized variables, R2 turns out to be the variance of the predicted variable:R2 =?ni=1(N?
(USi) ?N (US))2?ni=1(N (USi) ?N (US))2=?ni=1 N?
(USi)2n = var(N?
(US)) (5)where USi and N?
(USi) are the ith component of the vector US of the observed valuesand the ith component of the vector N?
(US) of the predicted normalized values, respec-tively.
One noteworthy consequence of the equality (5) is that the absolute values of theweights in the performance function are not greater than 1.Nevertheless, the accuracy of the prediction ?US for US is indicated by the q ratio:q(US, ?US) =|US ?
?US||US| (6)The definitions (2) and (6) and the equality (4) lead to the equalityq(N (US), N?
(US))q(US, ?US)=|N (US)?N?
(US)||N (US)||US??US||US|=|US?US0?N?
(US)?US0 ||US?US0||US?N?
(US)?US0?US0||US|=|US||US ?
US0|(7)which shows that as soon as US > US02 the prediction for the normalized values is(usually by a large margin) not as good as the prediction for the unnormalized values.Therefore, after predicting the normalized user-satisfaction values, these values shouldbe transformed back to the original scale to guarantee more accurate predictions.In previous work (Walker et al 1997, 1998; Walker, Kamm, and Litman 2000; Litmanand Shimei 2002), not only was there no attention paid to these details, it was not265Computational Linguistics Volume 32, Number 2mentioned that the observed user-satisfaction values need to be normalized as well ifone wants an acceptable error in the prediction.2.3 Choosing the Best Set of PredictorsA major problem in regression analysis is that of deciding which predictors shouldbe in the model.
There are two conflicting criteria.
First, the model chosen shouldinclude as many predictors as possible if reliable predictions are to be obtained (also,R2 increases with the number of predictors).
Second, because of the costs involvedin determining a large number of predictors and the benefit of focusing only on themost significant predictors, we would like the equation to include as few predictorsas possible.An appropriate method of choosing the best set of predictors is backward elimination(Seber 1977).
Here, at each step, a single predictor is eliminated from the current regres-sion model if its removal would increase the sum-of-squares differences between theobserved and the predicted values,RSS =n?i=1(N (USi) ?
N?
(USi))2 =n?i=1i2 (8)by not more than Fout (some properly chosen constant, typically between 2 and 4)times the residual mean square RSSn?p , where n is the number of observations and p isthe number of predictors.
That is, the predictor giving the smallest increase in RSSis chosen.2.4 Why Model the Sum of User-Satisfaction ScoresHone and Graham (2000) argued that the items chosen for the user-satisfaction survey(Table 1) introduced within the PARADISE framework were based neither on theorynor on well-conducted empirical research.
Moreover, they said that the way that thecollected data was used would be inappropriate, i.e., the approach of summing all thescores could only be justified on the basis of evidence that all of the items are measuringthe same construct, otherwise the overall score would be meaningless.Table 1The user-satisfaction survey used within the PARADISE framework.1.
Was the system easy to understand?
(TTS Performance)2.
Did the system understand what you said?
(ASR Performance)3.
Was it easy to find the message you wanted?
(Task Ease)4.
Was the pace of interaction with the system appropriate?
(Interaction Pace)5.
Did you know what you could say at each point of the dialogue?
(User Expertise)6.
How often was the system sluggish and slow to reply to you?
(System Response)7.
Did the system work the way you expected it?
(Expected Behavior)8.
From your current experience with using the system, do you think you?d use the system regularlywhen you are away from your desk?
(Future Use)266Hajdinjak and Mihelic?
The PARADISE Evaluation FrameworkIf, in spite of Hone and Graham?s remarks, one wants to evaluate user satisfac-tion with the survey given in Table 1, the question that arises is whether the targetto be predicted should really be the sum of all the user-satisfaction scores.
How-ever, our experiments (Section 2.3) showed a remarkable, but expected, difference inthe significance of the predictors when taking different satisfaction-measure sums oreven individual scores as the target to be predicted.
Moreover, some individual user-satisfaction measures could not be well modeled.
Consequently, we think that it wouldbe more appropriate to take the sum of the user-satisfaction scores that are likelyto measure the selected aspect (e.g., dialogue-manager performance) of the system?sperformance.2.5 Speech-recognition EffectsIt has often been reported (Walker et al 1998; Kamm, Walker, and Litman 1999; Walker,Kamm, and Litman 2000; Litman and Shimei 2002) that the mean concept accuracy,often referred to as the mean recognition score, is the exceptional predictor of a dialoguesystem?s performance.
Moreover, it has been shown that as recognizer performanceimproves the significance of the predictors can change (Walker, Borland, and Kamm1999).Thus, we would go so far as to claim that the influence of automatic speechrecognition hinders the other predictors from showing significance when evaluatingthe performance of the dialogue-manager component, excluding the efficiency of itsclarification strategies.
Only if the users are disencumbered from speech-recognitionerrors are they able to reliably assess the mutual influence of the observable less sig-nificant contributors to their satisfaction with the dialogue manager?s performance.Therefore, in our WOZ experiments (Section 2), which were carried out in order toevaluate the performance of the dialogue manager, speech understanding (i.e., speechrecognition and natural-language understanding) was performed by a human.
Asexpected, ?
with mean values near 1 (0.94 and 0.98, respectively), which was theonly predictor reflecting speech-recognition performance (Section 2.1), did not showthe usual degree of significance in predicting the performance of our WOZ systems(Section 2.3).3.
PARADISE Framework ApplicationWith the intention of involving the user in all the stages of the design of the spokennatural language, weather-information-providing dialogue system (Z?ibert et al 2003),WOZ data were collected and evaluated even before the completion of all thesystem?s components.
The aim of the first two WOZ experiments (Hajdinjak andMihelic?
2004a) was to evaluate the performance of the dialogue-manager component(Hajdinjak and Mihelic?
2004b).
Therefore, while the task of the wizard in the firstWOZ experiment was to perform speech understanding and dialogue management,the task in the second WOZ experiment was to perform only speech understand-ing, and the dialogue-management task was assigned to the newly implementeddialogue manager.There were 76 and 68 users involved in the first and the second WOZ experiment,respectively.
The users were given two tasks: The first task was to obtain a particularpiece of weather-forecast information, and the second task was a given situation, the267Computational Linguistics Volume 32, Number 2aim of which was to stimulate them to ask context-specific questions.
In addition,the users were given the freedom to ask extra questions.
User satisfaction was thenevaluated with the user-satisfaction survey given in Table 1, and a comprehensive usersatisfaction (US) was computed by summing each question?s score and thus ranged invalue from a low of 8 to a high of 40.3.1 Selection of Regression ParametersThe selection of regression parameters is crucial for the quality of the performanceequation, and it is usually the result of thorough considerations made during severalsuccessive regression analyses.
However, following the recommended Cohen?s method(Di Eugenio and Glass 2004), we first computed the task-success measureKappa coefficient (?
), reflecting the wizard?s typing errors andunauthorized corrections,and the dialogue-efficiency costsMean elapsed time (MET), i.e., the mean elapsed time for the completionof the tasks that occurred within the interaction, andMean user moves (MUM), i.e., the mean number of conversational movesthat the user needed to either fulfil or abandon the initiatedinformation-providing games.Second, the following dialogue-quality costs were selected:Task completion (Comp), i.e., the user?s perception of completing thefirst task;Number of user initiatives (NUI), i.e., the number of user?s movesinitiating information-providing games;Mean words per turn (MWT), i.e., the mean number of words in eachof the user?s turns;Mean response time (MRT), i.e., the mean system-response time;Number of missing responses (NMR), i.e., the difference between thenumber of turns by the system and the number of turns by the user;Number of unsuitable requests (NUR) and unsuitable-request ratio(URR), i.e., the number and the ratio of user?s initiating moves that wereout of context;Number of inappropriate responses (NIR) and inappropriate-responseratio (IRR), i.e., the number and the ratio of unexpected responses fromthe system, including pardon moves;Number of errors (Error), i.e., the number of system errors, e.g.,interruptions of the telephone connection and unsuitable natural-language sentences;268Hajdinjak and Mihelic?
The PARADISE Evaluation FrameworkNumber of help messages (NHM) and help-message ratio (HMR), i.e.,the number and the ratio of system?s help messages;Number of check moves (NCM) and check-move ratio (CMR), i.e.,the number and the ratio of system?s moves checking some informationregarding past dialogue events;Number of given data (NGD) and given-data ratio (GDR), i.e., thenumber and the ratio of system?s information-providing moves;Number of relevant data (NRD) and relevant-data ratio (RDR), i.e.,the number and the ratio of system?s moves directing the user to selectrelevant, available data;Number of no data (NND) and no-data ratio (NDR), i.e., the number andthe ratio of system?s moves stating that the requested information is notavailable;Number of abandoned requests (NAR) and abandoned-request ratio(ARR), i.e., the number and the ratio of the information-providing gamesabandoned by the user.Note that special attention was given to the parameters NGD, GDR, NRD, RDR,NND, and NDR, which have not so far been reported in the literature as costs foruser satisfaction.
We will refer to them as database parameters.
It has, however, beenargued (Walker et al 1998) that the database size might be a relevant predictor ofperformance.
However, the decision to introduce the database parameters as dialoguecosts was based on the extremely sparse and dynamical weather-information source(Hajdinjak and Mihelic?
2004b) with a time-dependent data structure that we had atour disposal.3.2 Correlations between MLR ParametersIn the data from both WOZ experiments, some high correlations between the regressionparameters were observed.
Note that the high correlations were not found onlybetween the newly introduced or the obviously related parameters such as NUT andNGD.
In the first experiment, not entirely evident high correlations were observedbetween MET and MRT (0.7), NMR and NHM (0.7), GDR and NDR (?0.8), but inthe second experiment between MET and MRT (0.8), NMR and NHM (0.7), GDRand RDR (?0.7).
The regression parameters GDR and NDR as well as GDR andRDR were highly correlated only in one of the WOZ experiments, and moderately inthe other.3.3 Performance-function ResultsWe applied PARADISE to the data from both WOZ experiments (Hajdinjak and Mihelic?2004a).
As the target to be predicted we first took user satisfaction (US) and afterwardsthe sum of those user-satisfaction values that (in our opinion) measured the dialoguemanager?s performance (DM) and could, in addition, be well modeled, i.e., the sumof the user-satisfaction-survey scores assigned to ASR Performance, Task Ease, SystemResponse, and Expected Behavior (Table 1).269Computational Linguistics Volume 32, Number 2After removing about 10% of the outliers,1 backward elimination for Fout = 2was performed.
Thus, the data from the first WOZ experiment gave the followingperformance equations:N?
(US) = ?0.69N (NND) ?
0.16N (NRD)N?
(DM) = ?0.61N (NND) ?
0.16N (NRD) + 0.21N (Comp)with 58% (R2 = 0.58) and 59% of the variance explained, respectively.
To be able tosee the difference between these two equations, note that Comp was significant for US(p < 0.02), but removed by backward elimination.
In contrast, the data from the secondWOZ experiment gave the following performance equations:N?
(US) = ?0.30N (CMR) + 0.18N (?)
?
0.23N (MET)N?
(DM) = ?0.35N (CMR) + 0.35N (?)
+ 0.35N (GDR) ?
0.17N (ARR)with 26% and 46% of the variance explained, respectively.
Note, MET was significantfor DM (p < 0.02) and that GDR and ARR were significant for US (p < 0.04), but theywere all removed by backward elimination.
Moreover, MET was trivially correlatedwith GDR and ARR (i.e., the correlation coefficients were lower than 0.1).Let us compare both performance equations predicting DM.
The first observationwe make is that none of the predictors is common to both performance equations.All the predictors from the first performance equation (i.e., NND, NRD, Comp) wereinsignificant (p > 0.1) for DM in the second experiment.
On the other hand, the onlypredictor from the second performance equation that was significant for DM (p < 0.004)in the first experiment, but removed by backward elimination, was GDR.
Unlike thefirst performance equation with the database parameters NND and NRD as crucial(negative) predictors, the second performance equation clearly shows their insignif-icance to users?
satisfaction.
Hence it follows that the developed dialogue manager(Hajdinjak and Mihelic?
2004b) with its rather consistent flexibility in directing the userto select relevant, available data does not (negatively) influence users?
satisfaction.Moreover, we thought that it would be very interesting to see which parametersare significant for individual user-satisfaction measures (Table 1).
However, the situ-ation in which the Task Ease question was the only measure of user satisfaction, theaim of which was to maximize the relationship between elapsed time and user satis-faction, was considered before (Walker, Borland, and Kamm 1999).
First, we discoveredthat Future Use could not be well modeled in the first WOZ experiment and that UserExpertise and Interaction Pace could not be well modeled in the second WOZ experi-ment, i.e., the corresponding MLR models explained less than 10% of the variance.Second, the parameters that most significantly contributed to the remaining, in-dividual user-satisfaction measures are given in Table 2.
Surprisingly, the parametersthat were most significant to an individual user-satisfaction measure in the first WOZ1 Eliminating outliers, i.e., observations that lie at an abnormal distance from other values, is a commonpractice in multivariate linear regression (Tabachnick and Fidell 1996).
However, before the elimination ofthe outliers in the data from the first WOZ experiment, the MLR models explained 44% and 39% of thevariance, respectively.
In contrast, before the elimination of the outliers in the data from the second WOZexperiment, the MLR models explained 18% and 34% of the variance, respectively.270Hajdinjak and Mihelic?
The PARADISE Evaluation FrameworkTable 2Most significant predictors of the individual user-satisfaction measures in the first (WOZ1) andthe second (WOZ2) WOZ experiment.WOZ1 WOZ2TTS Performance NND (p < 0.00005) UMN (p < 0.004)ASR Performance NND (p < 0.00005) CMR (p < 0.012)Task Ease NND (p < 0.002) GDR (p < 0.02)System Response NND (p < 0.0003) CMR (p < 0.0002)Expected Behavior NND (p < 0.00005) Comp, RDR, CMR (p < 0.04)experiment were insignificant to the same measure in the second WOZ experimentand vice versa.
On the one hand, this could indicate that the selected individualuser-satisfaction measures really measure the performance of the dialogue managerand consequently illustrate the obvious difference between both dialogue-managementmanners.
On the other hand, one could argue that this simply means that the individualuser-satisfaction measures are not appropriate measures of attitude because peopleare likely to vary in the way they interpret the item wording (Hone and Graham2000).
However, due to the huge difference in significance the latter seems an unlikelyexplanation.4.
ConclusionThe application of PARADISE to the data from two WOZ experiments led us to thefollowing conclusions.
First, the identified high correlations between some dialoguecosts and the explained normalization effects on the accuracy of the prediction reinforcethe need for careful regression analysis.
Second, if speech recognition is performed by ahuman, the PARADISE evaluation will lead to the identification of the significant pre-dictors of the dialogue-manager?s performance.
Third, inaccurate MLR models of someindividual user-satisfaction measures and the observed differences between pairs ofperformance equations predicting the same dependent variable require further empiri-cal research.
Not only does a reliable user-satisfaction measure that would capture theperformance measures of different dialogue-system components need to be established,but the reasons for the possible differences between several performance equations alsoneed to be understood and properly assessed.ReferencesDanieli, Morena and Elisabetta Gerbino.1995.
Metrics for evaluating dialoguestrategies in a spoken language system.In Proceedings of the 1995 AAAI SpringSymposium on Empirical Methods inDiscourse Interpretation and Generation,pages 34?39, Stanford.Di Eugenio, Barbara and Michael Glass.2004.
The Kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Hajdinjak, Melita and France Mihelic?.2004a.
Conducting the Wizard-of-Ozexperiment.
Informatica, 28(4):425?430.Hajdinjak, Melita and France Mihelic?.2004b.
Information-providing dialoguemanagement.
In Proceedings of TSD,pages 595?602, Brno.Hone, Kate S. and Robert Graham.2000.
Towards a tool for the SubjectiveAssessment of Speech SystemInterfaces (SASSI).
Natural LanguageEngineering: Special Issue on BestPractice in Spoken Dialogue Systems,6(3?4):287?303.Kamm, Candace A., Marilyn A. Walker, andDiane J. Litman.
1999.
Evaluating spokenlanguage systems.
In Proceedings of AmericanVoice Input/Output Society, San Jose.271Computational Linguistics Volume 32, Number 2Litman, Diane J. and Pan Shimei.
2002.Designing and evaluating an adaptivespoken dialogue system.
User Modelingand User-Adapted Interaction, 12:111?137.Patel, Nitin R. and Peter C. Bruce.
1995.Resampling Stats?Data Mining in Excel:Lecture Notes and Cases.
Trafford HoldingsLtd., Victoria, Canada.Seber, George A. F. 1977.
Linear RegressionAnalysis.
John Wiley & Sons, New York.Shriberg, Elizabeth, Elizabeth Wade,and Patti Price.
1992.
Human-machineproblem solving using SpokenLanguage Systems (SLS): Factorsaffecting performance and usersatisfaction.
In Proceedings of theDARPA Speech and NL Workshop,pages 49?54, New York.Smith, Ronnie W. and Steven A. Gordon.1997.
Effects of variable initiativeon linguistic behavior in human-computer spoken natural languagedialog.
Computational Linguistics,23(1):141?168.Tabachnick, Barbara G. and Linda S. Fidell.1996.
Using Multivariate Statistics, 3rd ed.Harper Collins, New York.Walker, Marilyn A., Julie Borland, andCandace A. Kamm.
1999.
The utilityof elapsed time as a usability metricfor spoken dialogue systems.
InProceedings of ASRU, pages 317?320,Keystone.Walker, Marilyn A., Candace A. Kamm,and Diane J. Litman.
2000.
Towardsdeveloping general models of usabilitywith PARADISE.
Natural LanguageEngineering: Special Issue on BestPractice in Spoken Dialogue Systems,6(3?4):363?377.Walker, Marilyn A., Diane J. Litman,Candace A. Kamm, and Alicia Abella.1997.
PARADISE: A general frameworkfor evaluating spoken dialogue agents.In Proceedings of the 35th Annual Meetingof ACL, pages 271?280, Madrid.Walker, Marilyn A., Diane J. Litman,Candace A. Kamm, and Alicia Abella.1998.
Evaluating spoken dialogue agentswith PARADISE: Two case studies.Computer Speech and Language,12(3):317?347.Z?ibert, Janez, Sanda Martinc?ic?-Ips?ic?, MelitaHajdinjak, Ivo Ips?ic?, and France Mihelic?.2003.
Development of a bilingual spokendialog system for weather informationretrieval.
In Proceedings of EUROSPEECH,pages 1917?1920, Geneva.272
