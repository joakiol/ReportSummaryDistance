Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200?209,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsOn Hierarchical Re-ordering and Permutation Parsingfor Phrase-based DecodingColin CherryNational Research Councilcolin.cherry@nrc-cnrc.gc.caRobert C. MooreGooglebobmoore@google.comChris QuirkMicrosoft Researchchrisq@microsoft.comAbstractThe addition of a deterministic permutationparser can provide valuable hierarchical in-formation to a phrase-based statistical ma-chine translation (PBSMT) system.
Permuta-tion parsers have been used to implement hier-archical re-ordering models (Galley and Man-ning, 2008) and to enforce inversion trans-duction grammar (ITG) constraints (Feng etal., 2010).
We present a number of theoret-ical results regarding the use of permutationparsers in PBSMT.
In particular, we show thatan existing ITG constraint (Zens et al, 2004)does not prevent all non-ITG permutations,and we demonstrate that the hierarchical re-ordering model can produce analyses duringdecoding that are inconsistent with analysesmade during training.
Experimentally, we ver-ify the utility of hierarchical re-ordering, andcompare several theoretically-motivated vari-ants in terms of both translation quality andthe syntactic complexity of their output.1 IntroductionDespite the emergence of a number of syntax-basedtechniques, phrase-based statistical machine transla-tion remains a competitive and very efficient trans-lation paradigm (Galley and Manning, 2010).
How-ever, it lacks the syntactically-informed movementmodels and constraints that are provided implicitlyby working with synchronous grammars.
There-fore, re-ordering must be modeled and constrainedexplicitly.
Movement can be modeled with a dis-tortion penalty or lexicalized re-ordering probabili-ties (Koehn et al, 2003; Koehn et al, 2007), whiledecoding can be constrained by distortion limits orby mimicking the restrictions of inversion transduc-tion grammars (Wu, 1997; Zens et al, 2004).Recently, we have begun to see deterministic per-mutation parsers incorporated into phrase-based de-coders.
These efficient parsers analyze the sequenceof phrases used to produce the target, and assem-ble them into a hierarchical translation history thatcan be used to inform re-ordering decisions.
Thusfar, they have been used to enable a hierarchicalre-ordering model, or HRM (Galley and Manning,2008), as well as an ITG constraint (Feng et al,2010).
We discuss each of these techniques in turn,and then explore the implications of ITG violationson hierarchical re-ordering.We present one experimental and four theoreti-cal contributions.
Examining the HRM alone, wepresent an improved algorithm for extracting HRMstatistics, reducing the complexity of Galley andManning?s solution from O(n4) to O(n2).
Examin-ing ITG constraints alone, we demonstrate that thethree-stack constraint of Feng et al can be reducedto one augmented stack, and we show that anotherphrase-based ITG constraint (Zens et al, 2004) ac-tually allows some ITG violations to pass.
Finally,we show that in the presence of ITG violations, theoriginal HRM can fail to produce orientations thatare consistent with the orientations collected duringtraining.
We propose three HRM variants to addressthis situation, including an approximate HRM thatrequires no permutation parser, and compare themexperimentally.
The variants perform similarly tothe original in terms of BLEU score, but differentlyin terms of how they permute the source sentence.200We begin by establishing some notation.
We viewthe phrase-based translation process as producing asequence of source/target blocks in their target or-der.
For the purposes of this paper, we disregardthe lexical content of these blocks, treating blocksspanning the same source segment as equivalent.The block [si, ti] indicates that the source segmentwsi+1, .
.
.
, wti was translated as a unit to producethe ith target phrase.
We index between words;therefore, a block?s length in tokens is t ?
s, andfor a sentence of length n, 0 ?
s ?
t ?
n. Emptyblocks have s = t, and are used only in special cases.Two blocks [si?1, ti?1] and [si, ti] are adjacent iffti?1 = si or ti = si?1.
Note that we concern our-selves only with adjacency in the source.
Adjacencyin the target is assumed, as the blocks are in targetorder.
Figure 1 shows an example block sequence,where adjacency corresponds to cases where blockcorners touch.
In the shift-reduce permutation parserwe describe below, the parsing state is encoded as astack of these same blocks.2 Hierarchical Re-orderingHierarchical re-ordering models (HRMs) for phrase-based SMT are an extension of lexicalized re-ordering models (LRMs), so we begin by brieflyreviewing the LRM (Tillmann, 2004; Koehn et al,2007).
The goal of an LRM is to characterize howa phrase-pair tends to be placed with respect to theblock that immediately precedes it.
Both the LRMand the HRM track orientations traveling throughthe target from left-to-right as well as right-to-left.For the sake of brevity and clarity, we discuss onlythe left-to-right direction except when stated oth-erwise.
Re-ordering is typically categorized intothree orientations, which are determined by exam-ining two sequential blocks [si?1, ti?1] and [si, ti]:?
Monotone Adjacent (M): ti?1 = si?
Swap Adjacent (S): ti = si?1?
Disjoint (D): otherwiseFigure 1 shows a simple example, where the firsttwo blocks are placed in monotone orientation, fol-lowed by a disjoint ?red?, a swapped ?dog?
and adisjoint period.
The probability of an orientationOi ?
{M,S,D} is determined by a conditionaldistribution: Pr(Oi|source phrasei, target phrasei).Emily??aime??son??gros??chien??rouge????.?[0,?2]?Emily??loves??[2,?4]?her??big??[5,6]?red??[4,5]?dog?[6,7]?.
?Figure 1: A French-to-English translation with 5 blocks.To build this model, orientation counts can be ex-tracted from aligned parallel text using a simpleheuristic (Koehn et al, 2007).The HRM (Galley and Manning, 2008) maintainssimilar re-ordering statistics, but determines orienta-tion differently.
It is designed to address the LRM?sdependence on the previous block [si?1, ti?1].
Con-sider the period [6,7] in Figure 1.
If a different seg-mentation of the source had preceded it, such as onethat translates ?chien rouge?
as a single [4,6] block,the period would have been in monotone orienta-tion.
Galley and Manning (2008) introduce a de-terministic shift-reduce parser into decoding, so thatthe decoder always has access to the largest possibleprevious block, given the current translation history.The parser has two operations: shift places a newlytranslated block on the top of the stack.
If the toptwo blocks are adjacent, then a reduce is immedi-ately performed, replacing them with a single blockspanning both.
Table 1 shows the parser states cor-responding to our running example.
Whether ?chienrouge?
is translated using [5,6],[4,5] or [4,6] alone,the shift-reduce parser provides a consolidated pre-vious block of [0,6] at the top of the stack (shownwith dotted lines).
Therefore, [6,7] is placed inmonotone orientation in both cases.The parser can be easily integrated into a phrase-based decoder?s translation state, so each partial hy-pothesis carries its own shift-reduce stack.
Time andmemory costs for copying and storing stacks canbe kept small by sharing tails across decoder states.The stack subsumes the coverage vector in that itcontains strictly more information: every covered201Op StackS [0,2]S [0,2],[2,4]R [0,4]S [0,4],[5,6]S [0,4],[5,6],[4,5]R [0,4],[4,6]R [0,6]S [0,6],[6,7]R [0,7]Table 1: Shift-reduce states corresponding to Figure 1.word will be present in one of the stack?s blocks.However, it can be useful to maintain both.The top item of a parser?s stack can be approxi-mated using only the coverage vector.
The approx-imate top is the largest block of covered words thatcontains the last translated block.
This approxima-tion will always be as large or larger than the true topof the stack, and it will often match the true top ex-actly.
For example, in Figure 1, after we have trans-lated [2,4], we can see that the coverage vector con-tains all of [0,4], making the approximate top [0,4],which is also the true top.
In fact, this approxima-tion is correct at every time step shown in Figure 1.Keep this approximation in mind, as we return to itin Sections 3.2 and 4.3.We do not use a shift-reduce parser that consumessource words from right-to-left;1 therefore, we ap-ply the above approximation to handle the right-to-left HRM.
Before doing so, we re-interpret the de-coder state to simulate a right-to-left decoder.
Thelast block becomes [si, ti] and the next block be-comes [si?1, ti?1], and the coverage vector is in-verted so that covered words become uncovered andvice versa.
Taken all together, the approximate testfor right-to-left adjacency checks that any gap be-tween [si?1, ti?1] and [si, ti] is uncovered in theoriginal coverage vector.2 Figure 2 illustrates how amonotone right-to-left orientation can be (correctly)determined for [2, 4] after placing [5, 6] in Figure 1.Statistics for the HRM can be extracted fromword-aligned training data.
Galley and Manning(2008) propose an algorithm that begins by run-1This would require a second, right-to-left decoding pass.2Galley and Manning (2008) present an under-specified ap-proximation that is consistent with what we present here.Prev?2?
4?
5?
7?6?0?Next?Coverage?/?Approx?Top?Next?2?
4?
5?
7?6?0?Prev?Cov?/?Approx?Top?Le?-??to-??Right?(Disjoint?[5,6])?Implied?Right-??to-??Le??(Monotone?
[2,4])?Figure 2: Illustration of the coverage-vector stack ap-proximation, as applied to right-to-left HRM orientation.Phrase?Source?Target???M???S?
??M??
?S?Figure 3: Relevant corners in HRM extraction.
?
indi-cates left-to-right orientation, and?
right-to-left.ning standard phrase extraction (Och and Ney, 2004)without a phrase-length limit, noting the corners ofeach phrase found.
Next, the left-to-right and right-to-left orientation for each phrase of interest (thosewithin the phrase-length limit) can be determined bychecking to see if any corners noted in the previousstep are adjacent, as shown in Figure 3.2.1 Efficient Extraction of HRM statisticsThe time complexity of phrase extraction is boundedby the number of phrases to be extracted, which isdetermined by the sparsity of the input word align-ment.
Without a limit on phrase length, a sentencepair with nwords in each language can have as manyas O(n4) phrase-pairs.3 Because it relies on unre-stricted phrase extraction, the corner collection stepfor determining HRM orientation is also O(n4).By leveraging the fact that the first step col-lects corners, not phrase-pairs, we can show thatHRM extraction can actually be done inO(n2) time,through a process we call corner propagation.
In-stead of running unrestricted phrase-extraction, cor-ner propagation begins by extracting all minimal3Consider a word-alignment with only one link in the centerof the grid.202Source?Target???M???S?
??M???S???S???M???M??
?S?Figure 4: Corner Propagation: Each of the four passespropagates two types of corners along a single dimension.phrase-pairs; that is, those that do not include un-aligned words at their boundaries.
The complex-ity of this step is O(n2), as the number of mini-mal phrases is bounded by the minimum of the num-ber of monolingual phrases in either language.
Wenote corners for each minimal pair, as in the orig-inal HRM extractor.
We then carry out four non-nested propagation steps to handle unaligned words,traversing the source (target) in forward and reverseorder, with each unaligned row (column) copyingcorners from the previous row (column).
Each passtakes O(n2) time, for a total complexity of O(n2).This process is analogous to the growing step inphrase extraction, but computational complexity isminimized because each corner is considered inde-pendently.
Pseudo-code is provided in Algorithm 1,and the propagation step is diagrammed in Fig-ure 4.
In our implementation, corner propagation isroughly two-times faster than running unrestrictedphrase-extraction to collect corners.Note that the trickiest corners to catch are thosethat are diagonally separated from their minimalblock (they result from unaligned growth in boththe source and target).
These cases are handled cor-rectly because each corner type is touched by twopropagators, one for the source and one for the tar-get (see Figure 4).
For example, the top-right-cornerarray Aq is populated by both propagate-right andpropagate-up.
Thus, one propagator can copy a cor-ner along one dimension, while the next propagatorcopies the copies along the other dimension, movingthe original corner diagonally.Algorithm 1 Corner PropagationInitialize target-source indexed binary arraysAq[m][n], Ay[m][n], Ap[m][n] and Ax[m][n] torecord corners found in minimal phrase-pairs.
{Propagate Right}for i from 2 to m s.t.
target [i] is unaligned dofor j from 1 to n doAq[i][j] = True if Aq[i?
1][j] is TrueAy[i][j] = True if Ay[i?
1][j] is True{Propagate Up}for j from 2 to n s.t.
source[j] is unaligned dofor i from 1 to m doAp[i][j] = True if Ap[i][j ?
1] is TrueAq[i][j] = True if Aq[i][j ?
1] is True{Propagate Left and Down are similar}return Aq, Ay, Ap and Ax3 ITG-Constrained DecodingPhrase-based decoding places no implicit limits onre-ordering; all n!
permutations are theoreticallypossible.
This is undesirable, as it leads to in-tractability (Knight, 1999).
Therefore, re-ordering islimited explicitly, typically using a distortion limit.One particularly well-studied re-ordering constraintis the ITG constraint, which limits source permu-tations to those achievable by a binary bracketingsynchronous context-free grammar (Wu, 1997).
ITGconstraints are known to stop permutations that gen-eralize 3142 and 2413,4 and can drastically limit there-ordering space for long strings (Zens and Ney,2003).
There are two methods to incorporate ITGconstraints into a phrase-based decoder, one usingthe coverage vector (Zens et al, 2004), and theother using a shift-reduce parser (Feng et al, 2010).We begin with the latter, returning to the coverage-vector constraint later in this section.Feng et al (2010) describe an ITG constraint thatis implemented using the same permutation parserused in the HRM.
To understand their method, it isimportant to note that the set of ITG-compliant per-mutations is exactly the same as those that can bereduced to a single-item stack using the shift-reducepermutation parser (Zhang and Gildea, 2007).
Infact, this manner of parsing was introduced to SMT42413 is shorthand notation that denotes the block sequence[1,2],[3,4],[0,1],[2,3] as diagrammed in Figure 5a.203Source?Target?0[1,2]4?[2,3]?[0,1]?2[3,4]4?0[1,2]5?2[2,3]4?[0,1]?[3,4]?Source?2[4,5]5?Target?(a)?
(b)?Figure 5: Two non-ITG permutations.
Violations of po-tential adjacency are indicated with dotted spans.
Boundsfor the one-stack constraint are shown as subscripts.in order to binarize synchronous grammar produc-tions (Zhang et al, 2006).
Therefore, enforcingan ITG constraint in the presence of a shift-reduceparser amounts to ensuring that every shifted itemcan eventually be reduced.
To discuss this con-straint, we introduce a notion of potential adjacency,where two blocks are potentially adjacent if anywords separating them have not yet been covered.Formally, blocks [s, t] and [s?, t?]
are potentially ad-jacent iff one of the following conditions holds:?
they are adjacent (t?
= s or t = s?)?
t?
< s and [t?, s] is uncovered?
t < s?
and [t, s?]
is uncoveredRecall that a reduction occurs when the top twoitems of the stack are adjacent.
To ensure that re-ductions remain possible, we only shift items ontothe stack that are potentially adjacent to the cur-rent top.
Figure 5 diagrams two non-ITG permu-tations and highlights where potential adjacency isviolated.
Note that no reductions occur in eitherof these examples; therefore, each block [si, ti] isalso the top of the stack at time i.
Potential ad-jacency can be confirmed with some overhead us-ing the stack and coverage vector together, but Fenget al (2010) present an elegant three-stack solutionthat provides potentially adjacent regions in constanttime, without a coverage vector.
We improve upontheir method later this section.
From this point on,we abbreviate potential adjacency as PA.We briefly sketch a proof that maintaining po-tential adjacency maintains reducibility, by showingthat non-PA shifts produce irreducible stacks, andthat PA shifts are reducible.
It is easy to see that ev-ery non-PA shift leads to an irreducible stack.
Let[s?, t?]
be an item to be shifted onto the stack, and[s, t] be the current top.
Assume that t?
< s and thetwo items are not PA (the case where t < s?
is simi-lar).
Because they are not PA, there is some index kin [t?, s] that has been previously covered.
Since it iscovered, k exists somewhere in the stack, buried be-neath [s, t].
Because k cannot be re-used, no seriesof additional shift and reduce operations can extend[s?, t?]
so that it becomes adjacent to [s, t].
Therefore,[s, t] will never participate in a reduction, and pars-ing will close with at least two items on the stack.Similarly, one can easily show that every PA shift isreducible, because the uncovered space [t?, s] can befilled by extending the new top toward the previoustop using strictly adjacent shifts.3.1 A One-stack ITG ConstraintAs mentioned earlier, Feng et al (2010) provide amethod to track potential adjacency that does not re-quire a coverage vector.
Instead, they maintain threestacks, the original stack and two others to track po-tentially adjacent regions to the left and right respec-tively.
These regions become available to the de-coder only when the top of the original stack is ad-jacent to one of the adjacency stacks.We show that the same goal can be achieved witheven less book-keeping by augmenting the items onthe original stack to track the regions of potentialadjacency around them.
The intuition behind thistechnique is that on a shift, the new top inherits allof the constraints on the old top, and the old top be-comes a constraint itself.
Each stack item now hasfour fields, the original block [s, t], plus a left andright adjacency bound, denoted together as `[s, t]r,where ` and r are indices for the maximal span con-taining [s, t] that is uncovered except for [s, t].
If thetop of the stack is `[s, t]r, then shifted items must fallinside one of the two PA regions, [`, s] or [t, r].
Theregion shifted into determines new item?s bounds.The stack is initialized with a special 0[0, 0]n item,and we then shift unannotated blocks onto the stack.As we shift [s?, t?]
onto the stack, rules derive bounds`?
and r?
for the new top based on the old top `[s, t]r:?
Shift-left (t?
?
s): `?
= `, r?
= s?
Shift-right (t ?
s?
): `?
= t, r?
= r204[2,4]?[5,7]?0?
9?Shi??[5,7]?4?
9?[2,4]?
9?0?[2,7]?[4,7]?0?
9?Reduce?4?
9?[2,4]?
9?0?(a)?
(b)?Figure 6: Two examples of boundaries for the one-stack solution for potential adjacency.
Stacks are built from bottomto top, blocks indicate [s,t] blocks, while tails are left and right adjacency boundaries.Meanwhile, when reducing a stack with `?
[s?, t?
]r?at the top and `[s, t]r below it, the new top simplycopies ` and r. The merged item is larger than [s, t],but it is PA to the same regions.
Figure 6 diagramsa shift-right and a reduce, while Figure 5 annotatesbounds for blocks during its ITG violations.3.2 The Coverage-Vector ITG Constraint isIncompleteThe stack-based solution for ITG constraints is el-egant, but there is also a proposed constraint thatuses only the coverage vector (Zens et al, 2004).This constraint can be stated with one simple rule:if the previously translated block is [si?1, ti?1] andthe next block to be translated is [si, ti], one mustbe able to travel along the coverage vector from[si?1, ti?1] to [si, ti] without transitioning from anuncovered word to a covered word.
Feng et al(2010) compare the two ITG constraints, and showthat they perform similarly, but not identically.
Theyattribute the discrepancy to differences in when theconstraints are applied, which is strange, as the twoconstraints need not be timed differently.Let us examine the coverage-vector constraintmore carefully, assuming that ti < si?1 (the casewhere ti?1 < si is similar).
The constraint consistsof two phases: first, starting from si?1, we travel tothe left toward ti, consuming covered words until wereach the first uncovered word.
We then enter intothe second phase, and the path must remain uncov-ered until we reach ti.
The first step over coveredpositions corresponds to finding the left boundaryof the largest covered block containing [si?1, ti?1],which is an approximation to the top of the stack(Section 2).
The second step over uncovered posi-tions corresponds to determining whether [si, ti] isPA to the approximate top.
That is, the coverage-vector ITG constraint checks for potential adjacencyusing the same top-of-stack approximation as theright-to-left HRM.This implicit approximation implies that theremay well be cases where the coverage-vector con-straint makes the wrong decision.
Indeed this isthe case, which we prove by example.
Considerthe irreducible sequence 25314, illustrated in Fig-ure 5b.
This non-ITG permutation is allowed bythe coverage-vector approximation, but not by thestack-based constraint.
Both constraints allow theplacement of the first three blocks [1, 2], [4, 5] and[2, 3].
After adding [0, 1], the stack-based solutiondetects a PA-violation.
Meanwhile, the vector-basedsolution checks the path from 2 to 1 for a transitionfrom uncovered to covered.
This short path touchesonly covered words.
Similarly, as we add [3, 4], thepath from 1 to 3 is also completely covered.
Theentire permutation is accepted without complaint.The proof provided by Zens et al (2004) missesthis case, as it accounts for phrasal generalizationsof the 2413 ITG-forbidden substructure, but it doesnot account for generalizations where the substruc-ture is interrupted by a discontiguous item, such asin 25{3}14, where 2413 is revealed not by mergingitems but by deleting 3.4 Inconsistencies in HRM parsingWe have shown that the HRM and the ITG con-straints for phrase-based decoding use the same de-terministic shift-reduce parser.
The entirety of theITG discussion was devoted to preventing the parserfrom reaching an irreducible state.
However, upuntil now, work on the HRM has not addressedthe question of irreducibility (Galley and Manning,2008; Nguyen et al, 2009).Irreducible derivations do occur during HRM de-coding, and when they do, they can create inconsis-tencies with respect to HRM extraction from word-205????????????????[4,6]?How?can?[0,1]?you?[6,7]?achieve?[1,2]?the?[3,4]?economic?and?[2,3]?tourism?[7,9]?benefits??
?Figure 7: An example irreducible derivation, drawn fromour Chinese-to-English decoder?s k-best output.Last translated block 2-red *-red approxHow can [4, 6] [4,6] [4,6] [4,6]you [0, 1] [0,1] [0,1] [0,1]achieve [6, 7] [6,7] [6,7] [4,7]the [1, 2] [1,2] [1,2] [0,2]economic and [3, 4] [3,4] [3,4] [3,7]tourism [2, 3] [1,4] [0,7] [0,7]benefits?
[7, 9] [7,9] [0,9] [0,9]Table 2: Top of stack at each time step in Figure 7, under2-reduction (as in the original HRM), *-reduction, andthe coverage-vector approximation.aligned training data.
In Figure 7, we show an ir-reducible block sequence, extracted from a Chinese-English decoder.
The parser can perform a few smallreductions, creating a [1,4] block indicated with adashed box, but translation closes with 5 items onthe stack.
One can see that [7,9] is assigned a dis-joint orientation by the HRM.
However, if the sametranslation and alignment were seen during train-ing, the unrestricted phrase extractor would find aphrase at [0,7], indicated with a dotted box, and [7,9]would be assigned monotone orientation.
This in-consistency penalizes this derivation, as ?benefits ?
?is forced into an unlikely disjoint orientation.
Onepotential implication is that the decoder will tendto avoid irreducible states, as those states will tendto force unlikely orientations, resulting in a hidden,soft ITG-constraint.
Indeed, our decoder does notselect this hypothesis, but instead a (worse) transla-tion that is fully reducible.
The impact of these in-consistencies on translation quality can only be de-termined empirically.
However, to do so, we requirealternatives that address these inconsistencies.
Wedescribe three such variants below.4.1 ITG-constrained decodingPerhaps the most obvious way to address irreduciblestates is to activate ITG constraints whenever decod-ing with an HRM.
Irreducible derivations will disap-pear from the decoder, along with the correspondinginconsistencies in orientation.
Since both techniquesrequire the same parser, there is very little overhead.However, we will have also limited our decoder?s re-ordering capabilities.4.2 Unrestricted shift-reduce parsingThe deterministic shift-reduce parser used through-out this paper is actually a special case of a generalclass of permutation parsers, much in the same waythat a binary ITG is a special case of synchronouscontext-free grammar.
Zhang and Gildea (2007) de-scribe a family of k-reducing permutation parsers,which can reduce the top k items of the stack in-stead of the top 2.
For k ?
2 we can generalize theadjacency requirement for reduction to a permuta-tion requirement.
Let {[si, ti]|i=1.
.
.
k} be the top kitems of a stack; they are a permutation iff:maxi(ti)?mini(si) =?i[ti ?
si]That is, every number between the max and min ispresent somewhere in the set.
Since two adjacentitems always fulfill this property, we know the orig-inal parser is 2-reducing.
k-reducing parsers reduceby moving progressively deeper in the stack, lookingfor the smallest 2 ?
i ?
k that satisfies the permu-tation property (see Algorithm 2).
As in the originalparser, a k-reduction is performed every time the topof the stack changes; that is, after each shift and eachsuccessful reduction.If we set k = ?, the parser will find the small-est possible reduction without restriction; we referto this as a *-reducing parser.
This parser will neverreach an irreducible state.
In the worst case, it re-duces the entire permutation as a single n-reductionafter the last shift.
This means it will exactly mimicunrestricted phrase-extraction when predicting ori-entations, eliminating inconsistencies without re-stricting our re-ordering space.
The disadvantage is206Algorithm 2 k-reduce a stackinput stack {[si, ti]|i = 1 .
.
.
l}; i = 1 is the topinput max reduction size k, k ?
2set s?
= s1; t?
= t1; size = t1 ?
s1for i from 2 to min(k, l) doset s?
= min(s?, si); t?
= max(t?, ti)set size = size + (ti ?
si)if t?
?
s?
== size thenpop {[sj , tj ]|j = 1 .
.
.
i} from the stackpush [s?, t?]
onto the stack;return true // successful reductionreturn false // failed to reducethat reduction is no longer a constant-time operation,but is insteadO(n) in the worst case (consider Algo-rithm 2 with k =?
and l = n items on the stack).5As a result, we will carefully track the impact of thisparser on decoding speed.4.3 Coverage vector approximationOne final option is to adopt the top-of-stack approxi-mation for left-to-right orientations, in addition to itscurrent use for right-to-left orientations, eliminatingthe need for any permutation parser.
The next block[si, ti] is adjacent to the approximate top of the stackonly if any space between [si, ti] and the previousblock [si?1, ti?1] is covered.
But before committingfully to this approximation, we should better under-stand it.
Thus far, we have implied that this approx-imation can fail to predict correct orientations, butwe have not specified when these failures occur.
Wenow show that incorrect orientations can only occurwhile producing a non-ITG permutation.Let [si?1, ti?1] be the last translated block, and[si, ti] be the next block.
Recall that the approxima-tion determines the top of the stack using the largestblock of covered words that contains [si?1, ti?1].The approximate top always contains the true top,because they both contain [si?1, ti?1] and the ap-proximate top is the largest block that does so.Therefore, the approximation errs on the side of ad-jacency, meaning it can only make mistakes when5Zhang and Gildea (2007) provide an efficient algorithm for*-reduction that uses additional book-keeping so that the num-ber of permutation checks as one traverses the entire sequenceis linear in aggregate; however, we implement the simpler, lessefficient version here to simplify decoder integration.Prev?
Next?si-??1?
ti-??1?
si?
ti?t?
?True?top?Approximate?top?Breaks?PA?Figure 8: Indices for when the coverage approximationpredicts a false M.assigning an M or S orientation; if it assigns a D, itis always correct.
Let us consider the false M case(the false S case is similar).
If we assign a false M,then ti?1 < si and si is adjacent to the approximatetop; therefore, all positions between ti?1 and si arecovered.
However, since the M is false, the true topof the stack must end at some t?
: ti?1 ?
t?
< si.Since we know that every position between t?
and siis covered, [si, ti] cannot be PA to the true top of thestack, and we must be in the midst of making a non-ITG permutation.
See Figure 8 for an illustration ofthe various indices involved.
As it turns out, both theapproximation and the 2-reducing parser assign in-correct orientations only in the presence of ITG vio-lations.
However, the approximation may be prefer-able, as it requires only a coverage vector.4.4 Qualitative comparisonEach solution manages its stack differently, and weillustrate the differences in terms of the top of thestack at time i in Table 2.
The *-reducing parser isthe gold standard, so we highlight deviations fromits decisions in bold.
As one can see, the original 2-reducing parser does fine before and during an ITGviolation, but can create false disjoint orientationsafter the violation is complete, as the top of its stackbecomes too small due to missing reductions.
Con-versely, the coverage-vector approximation makeserrors inside the violation: the approximate top be-comes too large, potentially creating false monotoneor swap orientations.
Once the violation is complete,it recovers nicely.5 ExperimentsWe compare the LRM, the HRM and the three HRMvariants suggested in Section 4 on a Chinese-to-English translation task.
We measure the impact ontranslation quality in terms of BLEU score (Papineniet al, 2002), as well as the impact on permutation207BLEU NIST 08 Complexity Counts SpeedMethod nist04 nist06 nist08 > 2 4 5 6 7 ?
8 sec/sentLRM 38.00 33.79 27.12 241 146 40 32 12 11 3.187HRM 2-red 38.53 34.20 27.57 176 113 31 20 8 4 3.353HRM apprx 38.58 34.09 27.60 280 198 41 26 13 2 3.231HRM *-red 38.39 34.22 27.41 328 189 71 34 20 14 3.585HRM itg 38.70 34.26 27.33 0 0 0 0 0 0 3.274Table 3: Chinese-to-English translation results, comparing the LRM and 4 HRM variants: the original 2-reducingparser, the coverage vector approximation, the *-reducing parser, and an ITG-constrained decoder.complexity, as measured by the largest k required tok-reduce the translations.5.1 DataThe system was trained on data from the NIST 2009Chinese MT evaluation, consisting of more than10M sentence pairs.
The training corpora were splitinto two phrase tables, one for Hong Kong and UNdata, and one for all other data.
The dev set wastaken from the NIST 05 evaluation set, augmentedwith some material reserved from other NIST cor-pora; it consists of 1.5K sentence pairs.
The NIST04, 06, and 08 evaluation sets were used for testing.5.2 SystemWe use a phrase-based translation system similar toMoses (Koehn et al, 2007).
In addition to our 8translation model features (4 for each phrase table),we have a distortion penalty incorporating the min-imum possible completion cost described by Mooreand Quirk (2007), a length penalty, a 5-gram lan-guage model trained on the NIST09 Gigaword cor-pus, and a 4-gram language model trained on the tar-get half of the parallel corpus.
The LRM and HRMare represented with six features, with separateweights for M, S and D in both directions (Koehn etal., 2007).
We employ a gap constraint as our onlydistortion limit (Chang and Collins, 2011).
This re-stricts the maximum distance between the start of aphrase and the earliest uncovered word, and is set to7 words.
Parameters are tuned using a batch-latticeversion of hope-fear MIRA (Chiang et al, 2008;Cherry and Foster, 2012).
We re-tune parametersfor each variant.5.3 ResultsOur results are summarized in Table 3.
Speed andcomplexity are measured on the NIST08 test set,which has 1357 sentences.
We measure permutationcomplexity by parsing the one-best derivations fromeach system with an external *-reducing parser, andnoting the largest k-reduction for each derivation.Therefore, the>2 column counts the number of non-ITG derivations produced by each system.Regarding quality, we have verified the effective-ness of the HRM: each HRM variant outperformsthe LRM, with the 2-reducing HRM doing so by 0.4BLEU points on average.
Unlike Feng et al (2010),we see no consistent benefit from adding hard ITGconstraints, perhaps because we are building on anHRM-enabled system.
In fact, all HRM variantsperform more or less the same, with no clear win-ner emerging.
Interestingly, the approximate HRMis included in this pack, which implies that groupswishing to augment their phrase-based decoder withan HRM need not incorporate a shift-reduce parser.Regarding complexity, the 2-reducing HRM pro-duces about half as many non-ITG derivations as the*-reducing system, confirming our hypothesis thata 2-reducing HRM acts as a sort of soft ITG con-straint.
Both the approximate and *-reducing de-coders produce more violating derivations than theLRM.
This is likely due to their encouragement ofmore movement overall.
The largest reduction weobserved was k = 11.Our speed tests show that all of the systems trans-late at roughly the same speed, with the LRM beingfastest and the *-reducing HRM being slowest.
The*-reducing system is less than 7% slower than the 2-reducing system, alleviating our concerns regardingthe cost of *-reduction.2086 DiscussionWe have presented a number of theoretical contribu-tions on the topic of phrase-based decoding with anon-board permutation parser.
In particular, we haveshown that the coverage-vector ITG constraint is ac-tually incomplete, and that the original HRM canproduce inconsistent orientations in the presence ofITG violations.
We have presented three HRM vari-ants that address these inconsistencies, and we havecompared them in terms of both translation qualityand permutation complexity.
Though our results in-dicate that a permutation parser is actually unneces-sary to reap the benefits of hierarchical re-ordering,we are excited about the prospects of further ex-ploring the information provided by these on-boardparsers.
In particular, we are interested in using fea-tures borrowed from transition-based parsing whiledecoding.ReferencesYin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models through la-grangian relaxation.
In EMNLP, pages 26?37, Edin-burgh, Scotland, UK., July.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In HLT-NAACL, Montreal, Canada, June.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In EMNLP, pages 224?233.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
Anefficient shift-reduce decoding algorithm for phrase-based machine translation.
In COLING, pages 285?293, Beijing, China, August.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP, pages 848?856, Honolulu, Hawaii,October.Michel Galley and Christopher D. Manning.
2010.
Ac-curate non-hierarchical phrase-based translation.
InHLT-NAACL, pages 966?974, Los Angeles, Califor-nia, June.Kevin Knight.
1999.
Squibs and discussions: Decod-ing complexity in word-replacement translation mod-els.
Computational Linguistics, 25(4):607?615, De-cember.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL,pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,pages 177?180, Prague, Czech Republic, June.Robert C. Moore and Chris Quirk.
2007.
Faster beam-search decoding for phrasal statistical machine trans-lation.
In MT Summit XI, September.Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,and Thai Phuong Nguyen.
2009.
Improving a lexi-calized hierarchical reordering model using maximumentropy.
In MT Summit XII, Ottawa, Canada, August.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4), December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL, pages 311?318.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In HLT-NAACL,pages 101?104, Boston, USA, May.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In ACL, pages 144?151.Richard Zens, Hermann Ney, Taro Watanabe, and Ei-ichiro Sumita.
2004.
Reordering constraints forphrase-based statistical machine translation.
In COL-ING, pages 205?211, Geneva, Switzerland, August.Hao Zhang and Daniel Gildea.
2007.
Factorizationof synchronous context-free grammars in linear time.In Proceedings of SSST, NAACL-HLT 2007 / AMTAWorkshop on Syntax and Structure in Statistical Trans-lation, pages 25?32, Rochester, New York, April.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In HLT-NAACL, pages 256?263, NewYork City, USA, June.209
