PART ~E - LELIEOSTATIST\]DSThe Swadesh theory of lexicostatistics (1950, 1952, 1955)provided the first quantitative comparison of related languagesbased on a well-defined model of language change.
The stochasticnature of this model was poorly understood by linguists, in themain, and many have rejected the theory in the course of a protractedand confused controversy.
Meanwhile field linguists, especiallythose working with language groups of unknown history, have acceptedlexicestatisties and have found it to he an efficient, valid andreliable tec huique.The Swadesh thee .r~There are serious oversimplifications of reality implicitin lexicostatistics, and it is these, rather than the stochasticaspects, which are limitations of the theory.
Swadesh hypothesized,in effect, that(i) it is possible to discover a set of basic, universal andnon-cultural meanings, and he constructed a list of about 200 suchmeanings;(ii) in every natural language, at a given tl ~e, there is aunique lexical representation (word)corresponding to each of thesemeanings; but(iii) over short time intervals, the word representing anymeaning runs a small but constant risk of being replaced by adifferent (non-cognate) word; and(iv) the replacement, o r  non-replacement, of the lexicalrepresentation of a meaning occurs independently of that of any othermeaning, and independently over different periods of time.To formalize (i) and (ii), we must postulate the existence,for each natural language, at all points t in time, of a lexico_..___~n,represented by a finite abstract set Lt. A well-defined equivalencerelation corresponding to cognation parti-tions the elements of ~TLt  (T a real interval) into equivalenceclasses.
If k6Ls ,  I~L  t (t ,s6T) are cognate, we write ~(k,l) = i.Otherwise ~(k,1) = O.Further, we must postulate the existence of a finiteabstract set M (corresponding to the universal set of meanings), anda procedure for defining, for any t, and any Lt, a unique map fromM into Lt.
This map, written M ~Lt ,  specifies that for eacht m~M there is a i~ L t such that m ~ l  (i mean., ss m).Hypotheses (iii) and (iv) imply that the changes overtime in the image of the map M---~*-t Lt have a certain stochasticaspect.
This can be modelled by the probability statementP\ [~(k , l )  = 1\] = i - X ( t -  s) ?
h ,a universal constant, t>s ,  and ~t -s  -~ 0 as t-~s; and twoindependence conditions ; letSm i ----~ k ifor i = 1,2,...,IM~ (~Mi the number of elements in M),tmi------'-~ i i~'oCkl,ll; , ~ (k2 ,12) ,  ?
?
?
, ~o(k~Ml , l~m ) , , ' ,  ~do~ndent  ~henrandom var iab les;  let3 Js im ~ h  ifor _ \[si,ti) , i = 1,2,...,N,tim ~  j?then ~ (h~,j~, I(~,j2~,...  ,variables.This model has a number of immediate properties which formthe central thesis of the Swadesh theory.
These are presented hereas Theorems I, 2 and 3.
For simplicity we will assume that at anytime t, at most one word in L t can belong to a cognation equivalenceclass.
This simplifies notation and proofs, although the assumptionmay be relaxed without substantively affecting this development.One further type of assumption is required to ensure adegree of randomness in the choice of replacing word during lexicalreplacement.
To prove Theorem I as stated below, we requireBC > I such thatfor all m, t>s ,  ~(k,l) = O.a finite number ofdisjoint intervals,(~  ' JN ) are  independent randomILtl= O(ILtl ) ,Theorem i\]MCI ~S(~, Im)  = e "~t's> ?
O(\[Ltl )ProofLet N(t-s) be the number of changes (with respect to thecognation relation) of the mapping m~t  i in the interval (s.t\].Then N(Z-s) = 0 is just the event tha% a Poisson process remains atzero onthe interval (s,t\] (see, e.g.
Parzen, 1960, p.252), andHowever,P \[ ~(~s): 0\] : e "~(~s)?
PIN(t-s)> o\] x p \[ last c~ge Is tek (o~ co~a~)  I N(t-~) > o\]= e "~ ' ( t ' s )  ?
(i - e "~(~'s ) )  O( IL t l )= e-A(~-s) .
O( IT .
t l )ThenIMi -1~.
~(k,1) = e "A(t's) ?
0(|Lt!
)EZ~(~,~> ~ i~ 1 ~E~(k~,~)m(M mEM= tMj -1 IM ILe -~s)  ?
O( ILtl )~= e + O(ILt l )DefinitionB I|Lt, Lt, t> s represent the lexicons of two languageswhich are indenendent ~ languages of the same parent language(which are said to solit at time s) if' = L"  Ls s ' and ifs m---*-k (in beth languages)m--t-~t l' in the first languaget m~l"  in the second language,then ~(k,l') and ~(k)l") are independent random variables.Theorem 2ThenLet Lt, .Lt, t>s  be as above.el P\ [ l (~ ' , l " )  = 1\] = .
-2X i~s) .
O(~.~L,ti ,iLtl)m6.M )ProofAss'~l~g m~-~k in beth languages,By t rans i t iv i ty  of the equivalence relation represented by S ,  thefirst term on the right isP\ [~(k , l " )  = 1~ $(k , l ' )  = 1\]~w~oh, by independence.. \[s-~,C~s>.
o~<, ~1 \[ e .a~+'s> * o~(,.~, >\]-z~ (t-s) e-A(t -s)  ' = e ?
(O( IL t l )  * 0(1' .
i ) )  ?
O( IL t l l L t | '  " )tl = e -zA( t ' s )  * O(~Iz , \ [ l  , ILtl )Now the second probability on the right hand side above is, similarly,P\ [~( l ' , f ' )  = 1~ I (k , l ' )  = o~ ~(k, f ' )  = o\]= ~P\[m-~e-t l ' ( lst  language)~ m--~t l"(2nd language)\] J ( l ' , l " )I |I'~ L t11 It1 ~L  tS(~', ~o~(~", ~o= ~P\ [m t--~l '\] P\[m t--~p-lU ~(l ' , l " ) ,  by independence.Since we have fixed m- -~ kThe summation contains at mostI I!mx (I L t \ [ ,  { Ltl )terms which are not annihilated by "-  " ~)(i',i")and so the total iso c max ( I L ' I , I L " I ) t  t"< IL\[I {L~It H = O(.~n f Lt| ,  ILt/)This completes the proof of the first statement of the theorem.The proof of the second parallels the analogous result in theprevious theorem.In natural languages, \[Ltl is several thousands and\[LJis ne~ligible compared to the exponential te~u, except forvery high values of t (where the theory has little applicability).In the next  theorem, the results of Theorems 1 and 2 are utilized,neglecting the emr  terms of the form O(lLt\[ ).Under certain, more spec i f i c  restrictions onP\[o-*~ l lm-~ ~\] , B ~  ~n.~ ~Ived ~or ~ e~ot ~o~ ofthe error term at tached to the expommtial laws (here formulated asTheorems 1 mad 2).Theorem 3Insofar as we may approximate the results of Theorems I and 2byl.1-1 E~ $(k~) = j':~e,..s)andl .
l  "1 E ~., Jo . '
, l " )  = o ' '2~e '~)respect ive~ i f  i t  i s  known that  t - s  = T, then: - l o~ lx l - l~ck .1 )Tis the maximum likelihood estimator (~)  of ~ in the firstformula above, and if ~ is known,- log  |M l .
l~Sd(k~l  ) - v .
./%%~-s -- .... kis the }~E of t-s.In the case of two independent daughter languages (Thin.
R),A~ - io~ IMI'ZZ ~( i '~ i " )  t - s  =is the MLE of t-s.ProofIt suffices to find the MLE of ~ , the other cases beinganalogous.-AT Consider binomial trials with parameter p = e~(k,l) = 1 is the equivalent of a success in one such trial.~ .
~(km,l m) = r is the equivalent of r successes in \[M I trials.m~MThe likelihood function of ~ in such a case isl og  L (~)  = constant -~Tr  ?
( |M| - r )  log  (1 -e 'A~.d io~ L( ~ ) = -Tr -Te -~T~_ML~dk l-e-""At the MLE, ~ , this derivative should be zero,~ = - l og~ ITand the same process yieldst-sALet r= ~me M as in Theorem 3.
Swadesh (1950) deriveda method~logy to utilize the three results~), = - log ( r /~)t.-sA~.$ = - loz  Cr l IM l )2Aas follows.
He first selected his list of meanings which heconsidered basic to all languages.
He then ?
compared Old Englishwith Modern English (t-s ~.I000 years), i.e.
he compared thewords in each language corresponding to the basic meanings.
Theetymology of words in those languages being fairly well known,he was able to decide when a pair of words corresponding tothe same meaning were cognate (i.e.
one was historically derivedfrom the other, or both were derived from a co~n root, by aseries of phonological alterations, each of which affected onlya part of the word in question).
This Immedi&tely led to~-~ 2 ~ I0"4?
Using the estimate which he obtained as a constant,he dated the relative times of separation or "split" of10various Salish (western NorthAmerican Indian) languages from acommon parent with the estimator t-s .
After the work of Lees (1953),was considered to be a universal constant~ t-s could estimateabsolute dates of split, and t-s could date a collection of textsfrom a dead language.Criticisms of the theor 7Criticisms of lexicostatlstics fall into two classes.
Inthe first class are protests based on or resulting from the stochas-tic nature of the model an~or  the stochastic nature of the pheno-mena of lexical loss and replacement.
The second class of criticismsrefer to particular assumptions in the model , and I will discussthese in the next section.Bergsland and Vogt (1962) presented four cases wheret-s (or ~s  are not accurate (thre~ too low and one too high),and rejected the Swadesh theory on this basis.
In statistical terms,the authors constructed a sample consisting entirely of outliers andrejected an hypothesis without even considering the distribution ofthe test statistic.
Fodor (1962) took the same approach to "disprove"lexicostatistlcs.
Chretien (1962) calculated and published pagesof ordinary binomial functions to prove, in essence, that t-s is arandom variable and hence not "an acceptable mathematical formula.tion" of the Swadesh theory.
This basic misunderstanding of thenature of statistical estimation is characteristic not only ofcritics of lexicostatistics, hut also of many of its practitioners.11A more important criticism has been expounded, at greatlengthp by Fodor (1965) and, more clearly, by Teeter (1963).Quoting from the latter:"Lexical similarities and dissimilarities do netcome about in any one simple way, and any mechanicalmethod of counting lexical similarities cannotseparate those due to chance, universals, diffusion,and common origin.
Lexical change is the result ofmany factors, and all are scrambled together in thefinal result.
"(p.~l)This diversity of causes of lexical and semantic change has receiveddetailed study by linguists and semanticists; see, for example,Bloomfield (1933) p.392 ff., Ullman (1957) p.183 ff.
Quoting fromLees (1953):" The reasons for morpheme decay, i.e.
for changesin vocabulary, have been classified by many authors;they include such processes as word tabu, phonemicconfusion of etymologically distinct items close inmeaning, change in material culture with loss of ob-solete terms, rise of witty terms or slang, adoptionof prestige forms from a superstratum language, andvarious gradual semantic shifts, such as specializa-tion, generalization, and peroration."(p.
114)And it is Just this diversity and the difficulty of "unscrambling"which, contrary to Teeter and to Fodor, justifies a stochasticmodel incorporating retention parameters.
Consider, for comparison,the problem of constructing a model for the behaviour of gases.
Wehave an enclosed volume containing a large number of particles offinite dimension, undergoing rapid motion.
We can assume everythingis perfectly deterministic, all the particles obeying Eewton's threelaws of motion, and all collisions perfectly elastic.
The positionof any particle at any time can, theoretically, be calculated pre-cisely if we know the initial state of the system and the timeelapsed.
Practically speaking, of course, this would be impossiblytedious, boring and pointless, there being so many particles, anytwo of which may collide, plus the walls, plus gravitational orelectrical charge attractions and repulsions to consider.
What ispossible, interesting, and of great value (witness the fields ofkinetic theory and statistical mechanics, dating from the work ofmen such as Maxwell, Bolt~man% and Einstein) is to consider thenature of each particle as a random process involving appropriateparameters and to consider the statistical bohaviour of the modelthus constructed.
It is complexity and great difficulty of predie.tion which make a statistical model workable.
In the same way, Fodorand others have inadvertently Justified the preposition that somesort of stochastic process might be an appropriate model for lexicalchange phenomena.
The question remains, what process?
The Swadeshtheory provides at least a first approximation to the correct answer.Problems with Swadesh's mode ~Before discussing details of the model, it is appropriateto present the results of an early (1953) lexicostatistic investiga.tion of R. Lees.
He chose thirteen language pairs, each pair con-sistlng of an historio language and a modern descendant.
The1213par t i cu la r  choice of pairs presumably s tewed from ava i lab i l i ty  andnot from any sampling technique.
He t rans la ted  each word inSwadesh's 215-word list (1950) into the 26 languages.
After count-ing the number, r ,  of cognates between each language pairp he used(in effect),-~Swhere {MJ ~ 215 according to the number Of indeterminate cognationsand uncertainties of translation.
To get an estimate of a "universal", he combined the individual estimates ini=l( ~ =~A~gives approximately the same result.
)Using p = e "At as the parameter in the binomial experimenthe calculated, for each language pair,( JM Io  - r)2IMI p(1-p)which should be approximately the square of a standard normal randomvariable, if the assumptions of the theory are true.
Since anest~imate of ~ is used in calculating p, the sum of the squaredva~bles s~d ~ ~,~-~strlbuted.
~ut Z~<9.5, ei~iflcant ,tthe I% level, suggesting rejection of the theory.Lees, however, suggested four reasons for not rejecting onthe basis of the ~2 test; the large values for ~M~ and r, uncertaintyin t, possible inappropriateness of the ~2 test, and the error inestimating ~ .
The first and third of those are not valid14statistically, and the fourth is a source of very little of theexcess ~2.
The variability in the time parameter can be incor-porated into the ~2 calculation.
This only reduces~2 to 25.9 - 27.5 depending on the variation assumed in t. Lees'results, then, indicate strongly that the theory Is an inadequatemodel for the phenomena.We turn now to the second class of criticisms of theSwadesh model, those that involve objections, evaluations or im-provements related to the generalizations and simplification ofreality inherent in lexicostatistlo theory.
The listing of assump-tions earlier in this chapter will serve as a framework for classify-ing this latter class of criticisms.
(i) There are no universal sets of meanings, it being difficultto specify most meanings without recourse to particular naturallanguages.
~o llst of meanings yet devised is completely satisfactoryfor sufficiently diverse languages; Holier (1956), O'Grady (1960),Cohen (196~), Levin (1964), Trager (1966).
(ii) The existence of synonymy proves the non-uniqueness ofthe meaning map MT-~L; and no known methods of eliclting words forgiven meanings are completely and reliably reproducible, fromspeaker to speaker or even from occasion to occasion for a singlespeaker; Gudschinsky (1960).
The existence of general and specificterms for a single entity provides a further complication.i(iii) If the parameter ~ can be sald to exist at all, It Isconstant neither from language to'language; Bergsland and Vogt (1962),15Fodor (1962), from meaning to meaning; Swadesh (1955), Androyev(1962), Ellegard (1962), and especially Dyen (1964), van der Merwe(1966), Dyen, James and Cole (1967), nor even from time intervalto time interval for the same meaning; Swadesh (1962).Judgements about cognation are unreliable, especiallywith respect to languages which are separated by large t-s andwhose history is mostly unknown; Fairbanks (1955), Teeter (1963),Lunt (1964).
An analysis of this latter problem is beyond the scopeof this study.
(iv) Lexioal loss and replacement do not occur independentlyfor different meanings, neither are current and future trends entire-ly independent of what has happened in the past, especially in lang-uages which have possessed an orthography for some time.
This hasbeen noted especially in connection with the independence assumptionof Theorem 2, as in the interval immediately after a split we mightexpect parallel (to some extent, at least) evolution of the twodaughter langumges; Lees (1953), Hymes (1960), Teeter (1963).
Alsoin this connection~ independence of evolution does not strictly holdwhere borrowings, loan-translations and imitations of other types arefrequent occurrences.Towards a new thgor ~A number of authors have attempted to deal with one or moreof these problems.
Swadesh (1952) discarded more than half of themeanings in his original list.
For choosing among synonyms, Gudschin-sky (19~) proposed a random selection, ~vmes (1960) suggested aprocedure which would seleot cognate forms whenever they were16available, Satterthwaite (1960) and D,Jen (1960) pointed out thatit would be more reasonable to choose the word which is most fre-quently used for the meaning in question.Little could be done about the central postulate or resultof the theory; that ~ is a constant, until the work of Dyen becamewell known.
Dyen, on the basis of comparisons of a large number ofMalayopolynesian languages was able to segregate meanings intogroups on the basis of their individual ~ 's.
A discussion of themathematical implications of this ( p=e~i (t's) for meaning m i leadsto E(r/I M I) = "i~=e "~i(t's) ) was published by van der Merwe (1966).Meanwhile, Dyen (1964) had statistically demonstrated that meaningswith high A in the Malayopolynesian languages tend to have highin the Indoeuropean languages and vice ve~.
This was the firstnew type of lexicostatistic result since the work of Lees.
Later(1967) this work was refined so that Dyen et alwere able toestimate a separate ~ for each meaning on a 196-word list of theSwadesh type.On the problem of independence, Swadesh pointed out thatinteraction between languages because of contact would bias estimatesof t-s downward.
Hattori (1953) suggested and Hymes (1960) discussedthe formula~-(r/t Z~I) = e " l '~( t ' s )as a way of taking into account parallel evolution and the effect ofthose meanings with lower ~ than the rest of the llst.
The lattereffect is, however, properly described by using a sum of exponen-tials and, for the former, it is unreasonable to expect a constantmultiplier (1.4) to express the dependence of two languages overall time.
It is Clear that the multiplier of -~(t~s) should benear zero when t is close to s and to approach 2 as t gets verylarge.
This was noted by Gleason (1960) who rightly suggested thatfor all sufficiently large t, estimates of t-s could be correctedby adding a small positive constant.One further suggestion that has been made by many authorsand implemented by some, e.g.
Hirseh (19~)~ Hattori (1957), is toattempt to construct a larger set M to provide a better (i.e.lower variance) estimate of time intervals.The primary purpose of this paper will be to develop aformal theory of word-meaning relationship, applicable to lexicaland semantic change, which incorporates most of the criticismslevelled against the Swadesh theor~17Relationship j to linguistic theoriesThis theory is unique in that ~ t provides a link betweentwo previously unrelated linguistic theories, that of generativegrammar, and the conventional descriptive semantics.
Elsewhere (1969)we show how stochastic models, like our theory of word meaningbehaviour, and Labov's (1967,1968) frequency approach to optionalgrammatical rules, can be derived by imposing probabilistic struc-ture on formal grammars.
On the other hand, the major phenomenaand problems of descriptive and historical semantics can be elegant-ly formalized in terms of this same model.18PART TWO - WORD-MEAN\]I~G PROCESSESThe problems of the Swadesh theory stem from its assump-tions about the nature of meaning, and its oversimplified mechanismof lexical replacement.
I propose a model of word-meaning relation-ship in which lexical replacement is a consequence of a more basicstochastic phenomenon - fluctuations in probabilities of word usage.The only aspect of a "meaning" which is relevant to this model isits representability by one or more words.
I make no assumptionas to the psychological or cultural nature of meaning.
In fact, Thm.
4below shows that the set of meanings as defined here can beconsidered a purely analytical construct.
This set is completelydetermined by comparing word usage probabilities in certain con-texts.
For a natural language there is the possibility of construct-ing the set of meanings by empirical means (from word usage frequencydata).Whether the entities I refer to as meanings correspond wellto aspects of the intuitive (or the semanticists') concept ofmeaning depends on whether they have important properties in commonand whether they behave similarly over time.
It is ~ thesis thatthese entities model the processes of historical semantics at leastas closely as, say, the "meanings" of Osgood e_~t a_~l (1957) modelpsychological aspects of meaning or the "m?anings" of Katz andPostal (1964) model the grammatical function of meaning.19The word-meanin~ relationshipThe mapping type of relationship in the Swadesh theorycan be represented by a bipartite graph as in Fig.
I .M Lm 1, , ira1~' ~ ' lm 2miMl ~imiM iFig.
I. Map relationship (many-to-one possible but notone-to-many).The first generalization to be made is to allow a many-to-one(in both directions) relation= as in Fig.
2.Fig.
2 ?
Unrestricted word-meanlngrelationship.The next important refinement of the model is the introduc-tion of probability distributions on words and meanings.
Thefrequency with which a word takes on a meaning in M has, as citedin PART 1, been recognized as important to lexicostatistics.DFen's (1960) essay contains a clear description of how fluctuationsin these frequencies underlie the phenomena of lexical replacement.In what follows, L can be understood as in PART 1, hut M iscompletely reinterpreted.,~finitio.nLet L and M be finite sets.Let p(.
," ) be a bivariate probability distribution on MX L..et S m =  l qp(m,l  > 01.If S m ~ ~ for all m~M,  and if for distinct m,n~M, S m ~ S n ,then M is a set of meanings on L, with respect to the distributionp, and eaoh non-zero p(m,l) represents a word-mean in ~ relationshi pbetween 1 and m.p(m,l) should be understood as the probability that theword i will be used, and that meaning m will be intended (when noinformation is given about the context).
The definition incorporatestwo restrictions on abstract meaning, s, neither of which is overlyrestrictive when considered as properties of meanings in the in-tuitive sense.
First, if a meaning is expressible by some word orother in the lexicon, that word must have a non-zero probabilityof expressing it (in some context which has a non-zero probabilityof occurring).
Second, if two meanings are to be distinct, on ourlevel of analysis, at least one of them must be expressible by atleast one word which the other is not, Fig.
3 illustrates theseconditions.
The latter principle, lexica..._~l distinguishability ofmeanings, might seem to place too much emphasis on marginal orthreshold word-meaning relationships (those with very low p(.
,.
)).20- .
~ calmmj ~ i  t = happym k ~ l  u overjoyed~ I v exube rantFig.
3 .
Part of word-meaning system.iff p(m,l)> 0.A line Joins m and iSuch objections will be seen to have little importance, however,after Theorem 9 below, where M is embedded in a metric space.Hero all meanings which do not differ greatly in their usage proba-bilities will cluster together in the metric space, and any com-parisons between meanings will be in terms of the metric.
Assuminglexical distinguishability facilitates the particular line ofdevelopment followed here, but relaxing it (e.g.
in favour of a morequantitative distinction between meanings, or in favour of a defini-tion of meaning grouping closely related lexically distinguishedentities) is not likely to radically affect the behaviour ofmeanings in the metric space.
An important consequence of thedefinition of a set of meanings is.T..hooremLet ~(L) be the set  of subsets of L, and le t  M be a setof meanings on L with respect to p. I f  S m ~l~L Ip (m,1)  > 0~,thenm ~ S  mis a one-one map from M onto a subset of ~(L).21Proo~It need only be shown that ifm -----*- S m , n ~ S  nthenS n = S mor equivalently,m~nm ~ n =:$ s m ~ s n ,but this is just the condition of lexical distinguishability in thedefinition.Theorem @ tells us t~at, for analytical or computationalpurposes, we can treat meanings as sets of words.
Two meaningsare distinguished by the words they do no_~t share and are related bythese they have in common.
Note that the case p(m,l) = 0 can arisein two ways.
Either p(m,l) = O, for all i, in which case S m =and m is not a meaning, or m i_~s a meaning but I ~S  m. From nowon, no distinction will be drawn between the meaning m and the setSm, and the latter notation will be discarded.
Sometimes, anentity whose status as a meaning or not is under study, will belabelled m. If m is not a meaning, p(m,l) -- O, for all i; m ~ M;S m = ~, etc., and every attempt will be made to keep this usageunambiguous._Interpretation of the marglnal, distrlbu~ionsWith the usage probability interp~tation of p(.
,.
),g(1) = ~p(m, l )m2223is the overall probability that i is used.
The probabilityfunction g(1) underlies word-frequency distributions, e.g.
thoseof Zipf (1945), Josselson (1953), and Juilland (196~a, 1965b).1is the overall probability that m is used.
This is related (atleast conceptually) to the "semantic frequency lists" of Eaton (1940).Since these are probability distribution functions,m i m,land~ of course,p(m,l) ~ 0 .Recapitulating, a word-meaning relationship exists betweenm and i, or a line is drawn between m and i on a word-meaning graphlike Fig.
2 or 3 , iff i can take on meaning m, which occurs iffp(m,l) ~ O.
(I.e., we require that if a word ca..~n take on a meaning,there is a non-zero probability that it wil_.~l do so. )
The statementf(m) = 0 is equivalent to saying that m is not lexically represent-able by elements of L, and m~ M.Precision of speechIn constructing a model involving the grouping of wordsand the distinction between meanings, provision should be madefor some degree o f  variation to correspond to the variation whichoccurs in reality, from person to person and, more especially, fromsituation to situation.
This variation is a complex effect, but24a good deal of it may.
be interpreted as alternation between preciseand loose speech.
In certain situations, and for certain topics,effective communication requires unambiguous usages, specific ratherthan generic terms, and other manifestations of precision whichare, on the other hand, inefficient, uneconomical or just toodifficult to sustain in everyday speech.
This alternation may occurindependently in different parts of the lexicon in a naturallanguage, but for our model we will use a single precision parameter06.
Each value of o6 will specify a set M~ of meanings on L. Inthe next few sections, the probability distributions and otherentities dependent on OC will be so subscripted (e.g.
pK(m,l), M~).In what manner should the system depend on ~6 ?
In naturallanguages, as a speaker becomes more precise he draws more distinc-tions between words and he groups two words of similar meaning lessfrequently (i,e.
with smaller probabilities).
One measurementwhich is sensitive to this process in the model is the average sizeof the meaningswhere Iml = \[Sol, the number of words connected to, representing,or simply in, a meaning.
This measurement would be too crude, byitself, to serve as a precision parameter, since it does notdistinguish between overall precision in the system and extremeprecision in one part of  the system but li%tle precision in therest.
Instead, a condition should be placed on the system so thatif ~ increases, then in any Dart o~'the ~ I, this increase25would co inc ide  w i than  increase  ~n the  probab i l i ty  weight  on smal lmeanings (i.e.
~m~ is small) and a decrease in ~ would coincidewith an increase on largo meanings.
Such a restriction may beformalized as follows.Let 0(6 \[0,I\].
Let DC ~(L) be any set of subsets of L,meanings or not, such thatm ~ D, nCm ~ n~ D .Then it is required thatm~D mGD \]~mis monotonic and non-decreaslng with ~.
Another way of looking atthis is in terms of the lattice of subsets of L. If we choose anypoints in the lattice or even draw a llne right across it~ the prob-ability assigned to all sets below these points 9 or below the line~must increase (or at least not decrease) as ~9 the precision~ in-A simple example ~ illustrate this.
Let L = (11,~,13} creases.
@Fig.
4 depicts the lattice of subsets of L.: \[ i ,12,13IFig.
4 .
Possible meanings when L = \[11,12,13~ .26For three values of ~ , values of ~(m,l) might he as inTable 1, and it is easy to verify that the precision conditionhel~ ~o car = one o~ ~I~, \[~, ~ ,  ~,~,  ~'=3\]  'f~3 '~ ' ~=i'~'~\] ' {~'~'~}' f~'~-'~3~ ' ~ '~3 '~J  '{=~.,~,,b,=~,=~,=~,,~ >.~=1 If1213~/8  - o ~ - o ~,0~ =~.~.
1/8 - ~ 0 0 high precisionI ll ~=0.5  i z t / to  - .- o - z / lo  z/zo ~.5  = {"4-'"~'mS'm6'm?}
- Z/tO - o Z/5 - l /Zo %~\]  =z 'z  - - 0 - 1/10 1/10 1/10 medium prec is ion~=0 fl13o - - o - 1 /8  ,~ ~={'~.
"~}- o .
o o - .~ ~- .O=, \ ]=z .7~.
- -  0 l O 1/8 ~ low precisionTable 1.
A word-meaning system at  3 levels  of precision.The example suggests the next theprem, which confirmsthat  the precis ion requirement is  strong enough to implymonotonlcity of the averQge meaning s ize.2?ProofE,~\[|ml) i s  a decreas ing  funct ion  of' ~ .~t ~(i )  =~\] ~,~(~) "~ i = 1,2,. , I~ I  ,Iml=ilml=i.Then a(.)
and b(.)
are probability distributions on the integers,where a(i) is the probability that an unspecified meaning willcontain i words.
ConsiderClearlyrequiresD i= \[m~L( ImISJ~ .re(D, n~m ~ n &D .
Then the precision conditionThereforeImI.~J lm|eJJi=l " i=1Since a(.)
and b(') are probability distributions,i l l  ?
ILti=l i=ii=J i--i i=1 i=JJ28Then~"  ja(J) ~" Jb(J)J=l J=lsince a(. )
and b(,) are the probability distributions of thevalues of |m|.Regularity conditionsWe have imposed a condition on the p~(m,l) so that theprobability weight must flow down the lattice of subsets of L asC( increases.
It would be desirable, from the viewpoints of modelrealism and analytical convenience, to have this "flo~' behave inas continuous a manner as possible.
It would be most convenient ifthe pJm,l) were required to be continuous functions of ~ , butthere are good reasons to relax this somewhat.Again trying to model natural fan,ages, it would berealistic to require that the following process may occur in thesystem.
Suppose a meaning m'~ is connected to k,ll,12,...,ir~ L.our earlier notation, Sme= ~k,ll,12,...,I~, in our current (innotation m' -- {k,11,12,...,lr~ , po<~',k),O, Po(m',li>> O, ~ li'~).As ~ increases, the values of all the p~, l  i) fluctuate butremain greater than some positive value, except for p~(m~k) whichgradually drops to zero at ~o" In terms of speech behaviour,the words k, iI,12,...,i r are used interohangeably (in certainZ9contexts) to mean m t, when precision is low.
As precision increases,11,12,...,i r continue to be interchangeable but k is seldom usablein this sense and~ at ~(o, never.
It is most important in whatensues to understandthat the set m I = {k,11,12,...,I ~ ceases tobe a meaning when the precision is ~o 'i.e.
em e M~, GC<O( om' ~ M~..It is, however, most natural that m = me-~I,~1,12,... Sir ~b~e a meaning at O( o' since the interchangeability of these words isnot necessarily dependent on the behaviour of k. Hence, if anypsychological interpretation is to be attached to the set of abstractmeanings in our model, it must be realized that as precision changes,the abstract label attached to a psychological or cognitive entitymay suddenly change as lexical representability of that entitychanges.
If this seems strange behaviour for a symbolic system,it should seem less so later, when the M~are embedded in a metricspace and the relative position of meanings in this space becomesmore important than the letters that identify them.e Returning to quantitative considerations, since m ceasesto be a meaning at ~o and m suddenly takes over its role, it isnecessary that p~(m' ~l~,..., p~m', l  r) drop discontinuously tozero at % and poc(m,ll), ..., p~(m,l r) Jump to compensate.We must, therefore, accept certain discontinuities ofthis sort in the model.
For simplicity?s sake~ we restrict30occurrences such as this so that only on_~e p~(m,l) may drop' k continuously to zero at any particular value of a6 o (p~(m , ) inthe example above).
This is in fact a weak restriction, in thatwe can approximate situations where N of the pm(m,l) go to zeroat ~o  by having them do this one at a time, at a(o, ~o ~ E ,~o  * 2G , .
.
.
, ~o  ~ (N-I)E for arbitrarily smallAn appropriate continuity-discontinuity condition may bemost economically phrased as in condition (iii) in the next seetlon.Summary of development thus farWe assume that there exists a finite set L (the set ofwords) and for each ~ ?
\[0,I\] a finite set M~ (a set of meaningson L) and a hivariate probability distribution pm on MmM L such thatm~M~ IGLThe elements of M~ are in one-one correspondence with certainnon-empty subsets of L.mw-- - *s  m ?~ ~(m, l )> O, ~ l~S m .This correspondence enables us %o unambiguously identify S m withm~ and we may rewrite the above conditionAs(i) p~(m,l) > 0 ~ le m and mcM~varies between zero and I, the followin E conditions must hold:(ii) If DC @(L) such that m~D,  n~m ~ n&D,  then~ p~(m,1) is monotone non-decreasing with ~.maD l~m31( i i i )  The pc(m,1) a re  cont inuous  funct ions  o f  ~ on ly  whereM~ i s  f i xed .
M K changes a t  @~o on ly  as a resu l t  o f  d i scont inu i -t ies  occur r ing ,  fo r  unique m, and unique k~ m, to a l l  o fbut " "p, Jm,1) ?
p=(m * {k} , l )  is  oontinuous, for ~n 1~ L.Before.
enunciating the continuity and discontinuitycondition (lii), we described the desired behaviour of some of thefunctions p(.
,. )
at a point where the condition is relevant.
We canprove that this condition implies this behaviour.In the system as described above, if ~o  is a point whereM l changes, then p~(m @ {k},k) (as in condition (ill) above) iscontinuous at ~op and if it goes to zero at ~o  it is the onlysuch function.ProofBy condition (iii),There forep~(~,k) ?
p~(~ ?
\[k},k) is aontinuous ,t =o.Butpw(m,k) ~ o since k ~ m ;he~.
the contin~ty of ~(.
* @},kL32~ow if any other p (n,l')?
goes to zero at go'  n ceases to be ameaning and MoQ changes as a result.
This contradicts condition(iii) unless n = m or m ?
~ k~, in which case discontinuities areprescribed by the same condition.Existence and local behaviourThe next theorem gives assurance that the conditionson the components of a word-meaning system, as developed so far,are not contradictory.
The proof consists of a construction of aparticular system (which is otherwise uninteresting) and is presentedas Appendix 3 in Sankoff (1969).,Theorem 7Word-meanings systems exist.Specifically, it is possible to construct a word-meaningsystem using any finite set, LThe regu lar i ty  condit ions are strong enough, however, sothat  as ide f romeont inuous var ia t ion  in the p ,~( .
, . )
,  only cer ta intypes of change in 1~,~ are possible.Theorem 8Suppose M K changes at ~o" Let M" , M ~ be the state ofMm in small enough intervals to the left and right of ~o ,Irespectively.
Then one of A, B or C must hold.A.
For a unique m, and unique k~m, as in condition (ili),represented  byBeC.Proof(~ ,~:  ~,~) ,(~, #': ?,~),?
p~.Cm * {k~,k) - 0 ,.P~o(m + (k},k) ~ 0 ,p{.
(m ?
Ik),k) = 0 .33There are 16 ways of filling four places ~rAth e or 4 ?
( ?
,c ,  ~ ,e  ), (?
,4  ,4 ,~ ), ( ~ ,~ : ?,#) and (4 ,4 ,  e,~)i nvo lve  no change in  H,~ ,.imply either p~(m,l)--'--- 0 or p (m ?
{k\]tl)~ 0 near ~ot and henoehaveno d iscont inu i ty .zn (4 ,e,4,~ ) and (~,?
:6,4), p~(m,Z) and p=(m ?
{k},Z) "jump.i n  the  same d i rec~ion ,  henoe the i r  sum cou ld  not  be cont inuous .(?
,E ;~,~) ,  (4-,~ ,e ,~)  and (~,~;~,4)  v4_oZ,,te con~tJ.on (44).
_There remain only' the three possibilities,A.
m *~k) disappears, llm p~(m *{k~,k) = p~m "l" (k~,k) = 0 .B.
reappears ,  me (k} in H" andH +,  p~ (m +{k},k)>0.DC.
m appears, m ?
{k\]dlsappears, p~o(m ?
{kl,k) = o .These three situations are illustrated in Fig.
5A,~ B and 5C.tF~.g..~A~ .
(,-,Z) .
f , ' ;~Possibility A, ( ~ ' J "~The.
8(m.?0 @C@ -- ~--~N.B.
right oontinulty instead ofleft oontinuity would be" ~'p\[ equally ~sslble here., L .
.
k '~  ~ , .
~  ..., '1 "Fi~.
5,B, / "~-R : .
: \ ] .
.
(,,,,Z)Possibility B, G\] t " " '  " - :  " "?
(a +(m+Fig.
5~;Possibility C,Thm.
8( "',..,.--"1Lk\] L .
/ ' "~-_ .
. )
:k'~,k) .
- .,%35NeaninMs as points in a metric spaceThe idea of distances between meanings is not new, andthere have been a number of attempts to operetionalize this concept.We shall examine a very natural way of defining such a distance forthe meanings in a word-meaning system in terms of the functionsp,~(m,l)._ LDefinitionLet agM~,  nGM BThe?rein 9d~,a defines a metric on Mo~.The norm Z~o \[ defines a metric on probabi l i ty  d i s t r ibut ionsdefines a probabi l i ty  d i s t r ibut ion  on L.z~(.
)It remains to prove that two such mK M~ do not define thesame distribution.
But this follows from the fact that each m ~ M~defines a unique subset of L such that pg(m,l)> O.RemarkIf as/~ increases beyond ~< , p~(m,l) changes, d~Afm,m)will have a minimum value at F = ~ and will increase for ~ oneither side of @~ .
In a neighbourhood of ~ , ~ (re,m) for fixed36m, then, measures distance from ~ .
This relevance of d to theparameter as well as to the meanings will become important in latersections.Theorem iOIf ~ =M I ,M  i =Mj~orand ifProo___ffm~I,  #e  J, two intervals,mG M I , n~Mjd~a~(m,n) is continuous on IXJ.This follows from the continuity of the p~ on such intervalsand from the fact that d is a continuous function of such p~ .As ~ ehanges, the points in M~ move continuously.When M ~ changes, two (at most) points experience a sudden shift inposition with respect to the rest of the points.
This may involvethe creation or annihilation of these points.
When ~ is close to1, there will be few ~rds  in common between two meanings, on theaverage~ and hence the distance between them will be close to I.When c~ is close to zero, on the other hand, the reverse is true,and distances will tend toward zero.
This rather succinct comparisonof precise versus loose usage accords well with more intuitive notionsof precision of spe~h.
Fig.
6 and Table ~ present, as illustrations tthe distances in the metric spa@es defined by the 3-word systemdescribed earlier in this chapter.3?~'=1c,' =0..5,% 1,~ 2/31?~2"b121 2/32/3 ~/3 ~/~m6= o ~!I1/3Table 2.
d~,o((o,.)
for systemof Table 1.m6 ----_m 7Fig.
6.
2-dimensionalvisualization ofdistances inTable 2.Di~c~ronic word.meanin~ systemsWe have developed, in some detail, a synchronic (i.e.
at afixed point in time) theory of words and meanings.
It remains to showwhat relevance this has to historical linguistics and lexicostatlstics.As Ullman (1957) remarks:"The two C semantic relationship, simple or multiple,and semantic change~ are interdependentt one beingthe projection of the other on a different plane.The functional analysis of meaning will entail there-fore a definition of semantic change along similarlines.
If a meaning is conceived as a reciprocalrelation obtaining between name and sense ~word andmeaning 3 , then a semantic change will occur whenevera new name becomes attached to a sense an~or  a newsense to a name."
(p.171)and, as he points out, word-meaning phenomena at a fixed time haveparallels in processes of change over time.In our particular model, changes in the system as theprecision parameter changes will provide the prototype for changewith time.DefinitionA word-meaning system history is a word-meaning system withW(\[0,1\] replaced by tE\[O,T\] (time parameter) and with condition (il)relaxed entirely.
Condition (iii) is changed so that if k and m aregiven as before, and ~ is a ne_~w meaning or if m disappearsstarting at to, there are discontinuities in Pt(m ?
\[kJ ,k) andPt(m @ (k~,l) ~" Pt(m,1) for one lgm, butPt(m @ \[k},k) 4', Pt(m .
{- {~k} ,1) '?"
pt(m,1)is continuous.Although an adjustment to the construct ion ecessary for T~.8could adapt the existence proof of word.meaning systems to thatof word-meaning system histor ies~ i t  w i l l  be simpler to leaveexistence to be impl i c i t  in  the construct ions carr ied out la ter .Theo rein , 11Suppose M t changes at t o .
Let M', ~ be as in Thm.lThen one of A, B, C, A', B', C' holds.A.
(~,~,~,~), ~o(m* {k},k) :0 ,A e .8 .
( ~,~ ~,  4), ~(~ * {kJ,k) > o. pg(= ?
{k},k) = 0,B.
(~ ,E ;  ?,~) ,  Pto(m *{k},k)>O,B'.
(?
,G ;~,e) ,  Pto(m * {k},k)~O,C.
... (~ ,~;~, 'e ) ,  p~(m?
~k},k)~O,C'.
(?,G'; ~, #), p~(m ?
~k},k)> 0,A, B and C were the three possibilities admitted in Thin.
8.The only new restriction applies when m ?
~k~ appears or m dis-appears at to, and therefore applies to none of the three.
A',B' and C' were discarded in Thin.
8 because they violated condition(ii).
The new condition (iii) applies to all of these cases.
InA, ~d c ' ,  ~ .
~k~ app~are so pt(~ * ~k},k)  ~ust J~p r~ se~ at  t o.The cases A, B and C are still represented by Fig.
5A~ 5BandSC , with e~ replaced by t. Cases A', B' and C' would he rep-resented by mirror images of these three figures, except thatPt(m ?
{k~,k) must exhibit a d iscont inu i ty  a t  to ,  and one of  thePt(m * (kt  ,1) must compensate fo r  th i s .The asynnuetry with respect  to time of  the condit ions forchanges in ~ may be in terpreted  as foll.ows, The probab i l i tythat  a word may he used fo r  a meaning may drop to zero cont inuously,but it may not increase from zero continuously.
Instead, it mustat some time jump to some finite value.
This distinction is not tooimportant to the overall characteristics of word-meaning system3940h is to r ies ,  but we note i t  because the par t i cu la r  type of  h i s to r ieswe sha l l  study have th is  property.The development of  the metric d in the previous sect ioncar r ies  over completely when the time parameter replaces the pre-c i s ion  parameter, except,  of  course, that  there i s  no longer anynecessary trend in the average distance between meanings as t increases.Anticipating some of our later discussion, consider thecase where all meanings consist of exactly one word, as in theSwadesh model.
In this case, letting s and t be time as in Thin.
I,ds,t(m,n) = I - ~(k,1)where m = {k%~Ms, n = {l~'~.
d then, is in a certain sense aPgeneralization of the cognation indicator ~ .Wo rd-meanln~ DrogessesSo far, changes in M~ or M t have been deterministic as thevalue of the parameter changes.
(Even though the p~ or Pt areabillty functions, we have not studied further properties of therandom variables which are distributed according to these functions,and we will not do so.
In linguistic terms, we are still dealingwith lan~ue and not parole. )
To generalize the Swadesh theory, andto provide a realistic model, we must take into account unpredict-ability of lexical and semantic change.
In probability theoreticalterms, we must impose a probability measure, on the set of allpossible histories.
We shall not do this explicitly.
Rather weshall assume it is possible, and assume that the examples we constructby specify-ing loca l  behaviour are well-behaved in terms o fanunderlying probab i l i ty  measure space.DefinitionA word-meaning process is a set of word-meaning systemhistories indexed by 60 ~ ~ where (~ ,~  , P) is a probabilitymeasure space.
This means that any event or combination of eventsin which we may be interested is represented by a set, A, ofhistories (WE ~ ) where A is a member of the ~-algebra ~ ,and where P(A) is well-deflned for all A6~.A wo,rd-meaning process based on Brownian , motionTo construct the word-meaning process which is the bestmodel for natural languages would require the operationalizing ofdefinitions, collection of much data and its statistical analysis.At present, we shall attempt only an heuristic investigation.In PART i, we emphasized the basic unpredictabilityof change in the word-meaning relationship.
In terms of our model,(and considering only small intervals of time) this means that fort>  s,X \[Pt(m,l) - Ps(m,l)3 = 0Furthermore, it should not be possible to.
predict the futurebehavlouF of Individtml Pt(mDl) from trends established in thepast: for any t>s l>s2~ .
.
.>s  rp\[pt(m,1) ~ps (re,l), ps z(m,1), .
.
.
,psr(m,1) \].
the  oo d tionBut these two conditions and the continuity conditions onPt indicate that the local behaviour of Pt(m,l) should resemble a41diffusion process, with zero drift.
The simplest such process isthe well-known Brownian motion, whose behaviour characteristicschange ne i ther  wi th  t ime,  t ,  nor  wi th  pos i t ion ,  x.We proceed to construct a word-meaning process satisfying :these properties.
Let (L i M~, p~,,  ,)) be a word-meaning systemfo r  a f i xed  c~ .
For  t = O, le t  P t (m, l )  = p~(m, l ) ,  M t = M=.Letn o is the number of word-meaning relationships in the system.Let xl(t) , ~(t),  .
.
.
)~( t )  t t ~ 0 be n o sample paths of an OBrownian motion process, chosen independently, and x(t) =~ ?~--j1xl (t).Let Yi(t) = xi(t) .
x(t).
The Yi are also 8rownlan sample paths,but are no longer completely independent in thatyi(t) = xi(t) - ~(t)i=1 i=l i=l= no~(t) - no~(t)Let= O.Pt(m,l) = po(m,l) * yi(t) ,where i = l(m,l) is determined beforehand.
Then Pt is continuousin ~ O,T~ with probability I.
We must en~ure that pt(.
,.)
is aprobab i l i ty  d i s t r ibut ion .i~3n o~.
Z pt(m, l) = I ~ Pc(m, 1) ~ ~ Yi (t)m~l~ lem m~M o l~m i=l= 1 ?
0= 1It is not necessarily true, however, that pt(m,l)~ O,since Yi(t) may be negative.
To adjust for this, letIn other words, all the Pt(m,l) are positive before 1 ~ .
Thentwith probability 1, there is a unique m'~ ~,  k~ m , such thatzim ~(~' ,k )  = p~(m',k) = 0 .But this is reminiscent of case A or C in Thin.
11 (see Fig.5),where one word in a meaning loses its ability to be grouped with theothers.
Then all the pT(m' ,1) should drop %o zero and all thep~(m'-\[k~,l) Jump to compensate.
According to whether m' - {k} ~ %or not, we have ease A or case C respectively.
Then it is a simplematter to determine ~.
Now, change the definition of all thePt(m,l) for t >~,  by calculatingand starting over as for t = O.Continuing this way until t = T, we ensure that no Pt(m,l)ever drops below zero.We now have a word.meaning process, but not a very healthyOne, in that IMtJ decreases monotonically with t.To counteract th i s ,  we superimpose another process on ourconstruct ion.
We se lect  points in  \[O,T\] at  random as follows:The probab i l i ty  of no points being selected in  an in terva l  I t , t?
~isI - /~At  + hwhere ~ t ~  0 as ArgO).
At each point ~ selected,  randomlychoose m~ M~and keL ,  k/re.
I f  f%~(m + {k} )>0,  the systemundergoes a change as in  case B w of Thin.
11 .
I f  f~ .
(m + ~k}) = Ojthe system undergoes an A' - type change with probabi l i ty  1Y , anda C'-type change with probability I -~  .
In each of these casesan element i~ m must be selected at random so that Pt(m + ~k},k)Pt(m +{k),l) and Pt(m,l) are discontinuous but their sum iscontinuous.
The size of the discontinuity is uniformly distributedbetween 0 and Pt(m,l).
In case A', we assume that after this latterstep is done, each element in m loses a random (but fixed) propor-tion of its probability weight to the corresponding element inThis ends the construct ion.
Note that  case B of Thm.lldoes not occur in  th is  example.Had we not insisted on the extra discontinuities (inPt(m + ~k~,k)) in the definition of a word-meaning system history,we would not have been able to use the Brownian motion.
Ifp~ (m + {k~,k) = O, and if we add a Brownian motion Yi(t),B~.~t(m + ~k),k)  w i l l  be zero again for  a rb i t ra r i l y  small t .
Hencewe must s ta r t  p (m + (k~,k) at  a f in i te  value, i .e .
discont inuously.4.45Stability"The first thing we would like to know about our system iswhether or not it is degenerate.
Does it tend to degenerate into asingle word-meaning relationship with p(~l~ ,1) = I ?
Does thenumber of meanings I~l or word-meaning relationships n t tend togrow without bounds as T and ILl increase?By increasing ~A to a high enough value, we can increasethe rate at which new word-meaning relationships are created,and hence reduce the time during which n t is at low values.
Atthe same time, n t cannot increase without bound, since as thenumber of word-meaning relationships increases, the probabilityweight attached to each must decrease, on the average.
Hence ahigher proportion of relationships tends to be annihilated perunit time, as in cases A and C of Theorem11 .
A rigorous proofthat n t is neither too large nor too small most of the time doesnot seem easy to achieve, simply because of the complication ofthe model and the importance of the initial conditions.
In anycase, such a result would be rather weak.
It seems likely, andwe will present evidence from sampling experiments to supportthis, that as t ~ ~,  n t tends to vary about an equilibriummean value acoording to an equilibrium distribution, depending onlyon the system parameters ~A and ~ .~6Re~ar i ty  of 'c~nge ~ (~,  dt, t)For each ~ , a word-meaning system ( re la t ive ly  complicated)was associated with a relatively simple metric space (M~,d,~.
Themeanings corresponded to points in the metric space and the distance?
between meanings varied continuously almost everywhere with respectto ~.The same remarks hold true, of course, for the analogousmetric spaces (~,dt,  ~.
As t increases each meaning moves continu-ously except at certain points where it can split into two or mergewith another meaning.
At such times there are discontinuities in~,t, but these are not usually very large.
This regularity of motionensures that we have some sort of correspondence between the sets ofmeanings at two distinct times.
In the Swadesh model, a well definedcorrespondence is assumed, in terms of the universal set of meanings.If we do not postulate anything of this nature, since it mustnecessarily refer to cultural universals, not linguistic universals,it becomes more difficult to make word-meaning comparisons at twopoints in time.
Indeed, if after a point in time, s, a meaningloses a lexical representation (as in case C in Thin.
11 ), it ceasesto exist, in our technical sense, and ethers close to it take upits semantic load - and we must, at the very least, assume some rulefor choosing a related or close meaning, for all later points intime, if we are to make lexical comparisons.
The intuitive use ofthe term "close" gives a clue as to the appropriate choice - the%meaning n which minimizesds,t(m, n) ?This has one important desirable property for such a rule.
For tvery close to s, in most cases n will, of course, be m itself.ds,t(m,m) = ds,t(m,n) will then be the sum of the absolute valuesof quantities approximately proportional to Brownian motion (seedefinition of d~, 6) and hence will, on the average (or in expectation)increase monotonically.
1 - ds,t(m,m) will decrease monotonically.After a discontinuity 1 - rain do +(re,n) will continue to decrease.Since it is the processes of lexlcal less and lexical replacementwhich are responsible for this decrease, 1 ~ (1 - mln ds,t(m,n))is a likely candidate to replace Swadesh's ~Mm~Zlexicostatlstlc indicator.
We will so use it, keeping in mind thatit does not involve any pan-cultural or pan-llnguistic method ofselecting universal meanings to compare.
If such a method existed(and it does, approximately speaking, e.g.
the Swadesh llst) ourindicator must necessarily provide an upper bound for any indicatorof the form i - ds,t.Simulating word-meaning processesA complete, purely mathematical treatment of the Browuian-based word-meaning system would be difficult, and no results analo-gous to Theorems I - 3 are yet available.
On the other hand, byChOOSing a set of Po(m,l) = p~o(m,l) from a word-meaning system,48and fixing ~ and ~ it is possible to simulate the behaviour ofthe bivariate functions pt(m,l).
A sample from a number of simulatedhistories might produce some hint of what the Corresponding theoremsmight be.
The remainder of this chapter consists of an account ofsuch an experiment.Asimulat ion nro ~ramA computer program (see Fig.
7) was written to provideword.meaning histories sampled from the Brownian-based process(actually an approximation of this process).The program accepts as initial data T (the length of thesimulation), parameters I (from which /~ can be calculated), 7/and ~ ; and two matrices N(i,J) and P(i,J) with ~M} rows and 20columns.
The row index i identifies the meaning being consideredpand the non-zero N(i,J) identify the words connected to that meaning(up to 20).
P(i,J) then, represents Po(mi,lk) of the system where~(i,j) = i k. (It is more economical to store two IMI X 20 matricesthan one ~M i X |LJ matrix if ~L~> 40.
)To approximate the Brownian motion from time t=O to t=I,one part of the program adds a normal random variable to each of thenon-zero P(i,j).
These variables have mean zero and variance I andtheir sum is zero, as specified in the model.
Each of these P(i,J)is then examined to see whether it has dropped to zero or below.
Ifit has, the rest of the non-zero P(i,k) are set to zero as in casesA and C of Thin.
11 and P(h,g) are increased by compensating amountswhere h and g are the appropriate meanings and words for the cases.49Another part of the program picks an integer according toa Poisson random variable, with mean lOp and this variable representsthe number of cases A', B' and C' which have occurred during the timeincrement I.
Hence J4  ~ 10/I.
For each of these occurrences theprogram then allows a choice of whether the word (see Thm~l ) is tobe a new word (borrowing) or a word that is already used for anothermeaning (this choice is made at random with probabilities 0 ,I-~ ).The meaning m and the word 16 m (again as in ThinS1 ) are chosen atrandom.
If necessary (not in case B') a random choice is madebetween A' and C' according to parameter ~ , and if necessary (caseA') the allocation of probabilities between m and m ?
~ is decidedby choosing a random number (uniformly distributed between 0 andp(~,z)).The program then provides for the examination of the systemto oalculats the resulting values of ~M~, IL|, ~(., .)
, P(.,-) andn t and it prints these out.
From this point it returns to the Brownlanmotion section and sets t = 21 and adds another batch of normal varia-bles with variance I, eto.The above is only a summary of  the program.
Other routinesrelabal words or meanings so that they may be stored and examinedeconomically, and others allocate any "negative probability" fromBrownian paths going ~ zero during a time increment (when intheory they are only allowed to go as far as zero) among the otherword-meaning relationships of the meaning involved.
Finally, in theversionrsprosentsd~nFl~Tthere is a routine which compares the word-meaning system at time t with the initial word-meaning system (at50N,P,T ~ ?I,/, ,~ , ~|if t--T,store /neCalculate tIMt I, ILtl, ~t  I~tus status1 0,~t |, nt,F(t)Choose Poiinteger J,mean i0Calculate F(t) II Add normal r.v.
1to each P(i,J)~O~I Adjust according .to Thin, 11 1 e~I Allocate "negative 0probabilities" ~e~.4~ess .
.
.
.
.
.
than J J cycles /cycles completedcompleted1 Choose meaning 1Borrowed wordor old word(choice)IFig.
?.
Flow ohart fo r  s imu la t ion  program.5:time t--O) according to our lexicostatistic indicator~---~I ~0(1  ~ do,t(m,n)) F(t) = .rain .Results of a simulation exper~mep%To illustrate the properties of a Brownian-based process,we will present the results on 12 sample histories of a simulatedprocess with the parameters fixed.These histories were obtained as follows.
For the first, theinitial system was represented as in Fig.
8 .20 118 ~ 313 ~ 811 10Fig.
8.
Ini%ial word-meaning system.where each line between an m and an i represents Po(m,l) = .01 .~ere I~2o, l~o,  no=lO0.
CO,T\]was ~vlded inte I00 inore.ments, and details of the system were extracted at time Tand these were used to provide the initial system for the secondhistory.
This general procedure was followed thereafter with thefinal status o f  some of the systems serving as the initial systemsfor others.Stability and equilibrium distributionsAs we conjectured earlier, the system moves rather quicklyto equilibrium and we can trace this in the first history.
Fig.
9shows how \ [~{,  ~Ltl and n t tend to approach and then osc i l l a te  aroundan equilibrium value,The "equilibrium" distributions in Fig.
I0 are calculatedfrom all the values of the system characteristics, at all points intime, of the last 11 histories (since the first history started witha non-equilibrium state).Zi~f' s LawIt is a property  .
of natural languages that, asidefrom the few most frequent words, the frequency of occurrence of aword G(1) and the rank order of this frequency, H(ll are relatedapproximately asG(l~ = Ce "KH~I~where C and K are constants.Our word-meaning systems do not have as many words as naturallanguages.
Nevertheless, it is possible to calculate the probabili-ties (not frequencies) g(1) fromg(1) = m ~,  Pt(m,l) .l&mO"0O?
4: -O00 0I0t+/!
"d0~r"gi..u0IN0~ oOqf :l,I+I +I,O".
I00 0\I.,.,IQqi%+ib ~00l I+liI0Is \ .
0 .0ii !I: J~.
c',0 ~b0!~?0I \] J Aj0J0(D I J.IJ.~I~ I ~"00O 0!g~oO"o0 -~ .mThis was car r ied  out fo r  e iEht  of  the termina l  word-meaningsystems of  our s imulat ion and the g (1)were  then ordered to give H(1).Plotting these (Fig.
11), it is clear that a Zipf's law can be statedwhich holds for the majority of the words in the system, exceptingthe first few and the last few.
The "tailing off" effect can perhapsbe ascribed to the homogeneity of the Brownlan process - any word,whose total probability fluctuates close to zero, is very likely tohit zero and be absorbed.
By introducing an inhomogeneous diffusion,where the variance of the displacement of p(m,l) after time ~t  isan increasing function of Pt(m,l), this effect could be removed, andthe total number of words end meanings could increase as well.One interesting comparison can be made between the g(1)vs. H(1) curves for the initial and the terminal states of the firsthistory (see Fig.
8 ).
In the initial, non-equilibrlum state allwords have equal probability g(1) =.05 .
The terminal state hasshifted to a typical Zipf's law.Lexico statistic sFinally, we present the results of the lexicostatisticsurvey of the 11 equilibrium system histories.
These are displayedin Fig.
12 end the mean behaviour is extracted and is displayed inFig.
13 ?
These diagrams speak for themselves - after an initialsharp drop, the index~--  ~ (I - min do,t(m,n))undergoes an unmistakeably exponent ia l  decline.0.1lJo ee0.1?oee oe?leee?oe0.
Ieee?
e?o ?
e-"..O.
01t g{z~O.
001Fig.
11.. Zipf's law for 8 examples (note semi- -logarithmic plot).
Successiveexamples shi f ted downward byfactors of  0.1 (,Con%Lnued on nex% page)?
ee?eOeeeeee ?eaeeee ?
?
eeol i ' ?eeeeee  ?eeeo e?eea eo ?0 ?e .
e ?
e ee?oeo  ?oe?
eoe??
?
o  ?
!?
ea ee  e ?8e  ??
~ ?
o eete%eeo- ?eo  ?eOee o?
"e e%eeee ??
0e ~ '~ .
.% ~eeo" ?
I ee?
.
I e e?e om??
e ?e~ ee eeeeeeeee eOe?eeee eee eeeeeeoeeeee eeoeeee?eeoe Ieeeee eeeoeeH(1) - -~O0 eQO0e?%ee eeoe oQeee0lIIlIllllII|l0.1b.0,' I  ~??e,0.10.01O.
001be o?
i eeeoC,oo ?oe e tee  eoe"e 'eee me eeeo?
a?
QoNQ~eg~ot g(1)Fig.
11eoe  eeee ?
eeee o ,  ee e?eeoo j?e  oe~o ?e .
?
?
ee ee?oe ?eeoee"?es  ?Qeo5doge@eeeoe?O oee~eoe?eeqb ?eoeoeee~ eeoe ee ??
?eeeo  eo~~oeee.
(i) ---~(Continued)ee egee.e?e  ee ?
oe ?oee%e eeeee?e~ e?
e ?
?
%,,@0.~0.1)ee ?oee IO.
(X;.001g(1)Be eeee,e ,eetH(15--,-aee ??
eoe?
eee  ?
eeeeo ?e~,eeeee?
e ,"ee ~)ee) .000~ - -me me eeee  eeee eeee,eo  eee ?
eeee eFig.
11 (Continued)Q o?
Q ?Q o QoQIQ ooo I@@?i ?
?QiOooQI O?e) ewe" ee,,) e ,~"ee .q)e~e e%i )e ,gQt 0Emc~m0o ~N ID~...g,ele"l ~ 2~dJ0!?
0O :te0OQOQ59fII ~ .
.~"0UPIQ~, i l l6OTo what extent  the  in i~a l  drop i s  a proper ty  o f  thepa l~ icu la r  metr i c  he~u~ used and to  what extent  i t  i s  an inev i tab leconsequence o f  the  Brounian mo~ton,  lus t  awa i t  fu r ther  s tudy.
Inany case, it does not seem to be a s id le ccmmequence of a Zipf'slaw distribution of wrd  probubilltlem or t~e analogous effect formeaning, since i t  a l so  occurs fo r  very  sy~etr i ca l  initial systemssuch as  the  one in  F ig .
8 .Without cc~ to any speclf~ cc~cluslons, it isappropriate to end this chapter ~ ~Int~n E out that both $wadesh'srelatively simple model of lexical loss, using a universal meaningset to compare language stages i and our  more  complicated model, inwhich compar isons between s t~Ees  o f  languages  are  made in  terms o fin terna l  p roper t ies  o f  the  lex icon ;  co ,cur  in  the  very  s imi la rbehav iour  o f  the i r  iex icos ta t i s t~ indexes .61Bibliography62Andreyev, N.D.1962 "Comment" on Bergsland and Vogt, Cur..~nt Anthro~ole .~.7 3:130.Bergsland, Knut & Hans Vogt1962 "On the validity of glottochronology", Current &nthro-3: 115-153.Bloomfield, Leonard1933 Language.
New York.Brainerd, B.n.d.
"A stochastic process related to language change"Chretien, C.D.1962 "The mathematical models of glottochronology",38: 11-37.Cohen, David1964 "Probl~mes de lexicostatistique sud-sdmitique", Proceedingsof the Ninth International Congress of Linguists, H.
Lunt,ed., The Hague, pp.490-496.Dyen, Isidore1960 '~omment" on Bymes, Current Anthropology 1: 34-39.1964 "On the validity of comparative lexicostatistics", Proceed-ings of the Ninth International Congress of Linguists,H.
Lunt, ed., The Hague, pp.238-252.Djen, I., James, A.T., & J.W.L.
Cole1967 "Language divergence and estimated word retention rate",LanP.uaF.e 43| 150-171.63Eaton, Helen S.1940 Semantic frequency list for English, French, German andSpanish.
Chicago, University of Chicago Press.Ellegard, A.1962 "Comment" on Bergsland & Vogt, Current Anthronolozv 3:130-131 .Fairbanks, G.H.1955 "A note on glottochronolos~", International Journal ofAmerican Linguistics 21s 116-120.Fodor, Istvan1962 "Comment" on Bergsland & Vogt, Curren_.._._~t Anthronolo~v 3:132-134.1965 The rate of linguistic change: limits of the applicationof mathemtical meth~dsiln linguistics.
University ofBudapest.Gleason, H.A., Jr.1960 "Comment" on Hymes, Current Anthropology 1=20.Gudsc hinsky, S.C.1956 "The ABC's of lexicostatistics", Word 12: 175-210.1960 "Comment" on Hymes, Current Anthropology I: 39-40.Hatteri, S.1953 "On the method of  glottochronology and the time-depth ofproto.Japanese", Journa_.____~l of the Linguistic ~ ofJa_~, no's.22,23, pp.29-7-~ g l ish su~mry pp.7~-~).1957 Kiso goi chosabyo (A test list of basic vocabulary).Hirsch, David I.1954 "Glottochronology and Eskimo and Eskimo-Aleut prehistory",American Anthropologist 56: 825-838.6~Hockett, C.F.1958 A course in modern linguistics.
Bew York, Macmillan?Hoijer, Harry1956 "Lexicostatistiosl a critique", Lanzuaze 32: ~9-60.Hymes, D.H.1960 eLexicostatistios so far", Current Anthronolo~v I: 3-@3.Josselson, H.1953 The Russian word count.Juilland, Alphonse, & E. Chang-Rodriguez1965 a Frequency dictionary of Spanish words.
Mouton, The Hague.Juilland, A., F.M.H.
Edwards & I. Juilland1965b Frequency dictionary of Rumanian words.
Mouton, The Hague.Katz, J.J. & P.M. Postal1964 An integrated theory of linguistic descriptions.
MIT, Cambridge.Labov, W.1967 "Contraction, deletion and inherent variability of theEnglish copula", paper given before the Linguistic Societyof America , Chicago, December,1967.1968 "Consonant cluster simplification and the reading of the'-ed' suffix", unpublished manuscript, Columbia University.Lees, Robert B.1953 "The basis of glottochrenology", ~ 29: 113-127.65Levln, Saul1964 "The fallacy of a universal list of basic vocabulary",Proceedings of the Ninth International Congress ofLinguiStics, H. Lunt, ed., pp.232-236.
Mouton, The Hague.Lunt, H.196# "Comment" on Dyen, Proceedings of the Ninth InternationalCongress of Lingulsties, pp.247-2~2.O' Grad~, G.~.1960 "Comm-nt" on Hymes, Curren..._._~t Anthro~olo~ 1: 338-339.Osgood, C.E., G.J.
Suci & P.H.
Tannenbaum1957 The measurement of meaning.
Urbana.Parzen, Emanuel1960 Modern probability theory and its applications.
Wiley~New York.Sankoff, D.1969 Historical linguistics as stochastic process.
UnpublishedPh.D.
thesis.
McGill University.Satterthwaite, A.C.1960 "Rate of morphemic decay in Meccan Arabic", Internationalof American Linn~uistics 26.Swadesh, Morris1950 "Salish internal relationships", Internationla\] ~ Jo2o_o_o_o_o_o_o~.
~ o_!fAmerican Linguistics 16: 157-167.192 "Lexioo-statistio dating of prehistoric ethnic contacts",Proceedings of the American Philosophical ~ 96, @~2-463.66Swmdesh, Morris1955 "Towards greater accuracy in lexicostatistic datin~"pInternational Journal of American Lin~ulst~cs 21:121-137 .1962 "Conunent" on Bergsland & Vogt, ~ Anthre~olo~/ 3:143-145.Teeter, Karl V.1963 "Lexicostatistios and genetic relationship", Lactate39: 638-648.Trager, G.L.1966 "Comment" on van dsr Merwe, Current AnthroDolo~ 7s 497-498.Ullman, Stephen1957 The principles of semantics.van der Merwe, N.J.1966Barnes & Noble, New York.
"New mathematics for glottochronolog~, Curren..._._~t Anthro~lo~v7: ,85-5o0.Zlpf, G.K.1945 "The meaning-frequency relationship of words", Journal of~svcholo~v 33: 251-256.
