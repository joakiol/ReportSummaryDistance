Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
151?160, Prague, June 2007. c?2007 Association for Computational LinguisticsUsing Foreign Inclusion Detection to Improve Parsing PerformanceBeatrice Alex, Amit Dubey and Frank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LW, UK{balex,adubey,keller}@inf.ed.ac.ukAbstractInclusions from other languages can be asignificant source of errors for monolin-gual parsers.
We show this for English in-clusions, which are sufficiently frequent topresent a problem when parsing German.We describe an annotation-free approach foraccurately detecting such inclusions, and de-velop two methods for interfacing this ap-proach with a state-of-the-art parser for Ger-man.
An evaluation on the TIGER cor-pus shows that our inclusion entity modelachieves a performance gain of 4.3 points inF-score over a baseline of no inclusion de-tection, and even outperforms a parser withaccess to gold standard part-of-speech tags.1 IntroductionThe status of English as a global language meansthat English words and phrases are frequently bor-rowed by other languages, especially in domainssuch as science and technology, commerce, adver-tising, and current affairs.
This is an instance of lan-guage mixing, whereby inclusions from other lan-guages appear in an otherwise monolingual text.While the processing of foreign inclusions has re-ceived some attention in the text-to-speech (TTS) lit-erature (see Section 2), the natural language process-ing (NLP) community has paid little attention bothto the problem of inclusion detection, and to poten-tial applications thereof.
Also the extent to whichinclusions pose a problem to existing NLP methodshas not been investigated.In this paper, we address this challenge.
We focuson English inclusions in German text.
Anglicismsand other borrowings from English form by far themost frequent foreign inclusions in German.
In spe-cific domains, up to 6.4% of the tokens of a Ger-man text can be English inclusions.
Even in regularnewspaper text as used for many NLP applications,English inclusions can be found in up to 7.4% of allsentences (see Section 3 for both figures).Virtually all existing NLP algorithms assume thatthe input is monolingual, and does not contain for-eign inclusions.
It is possible that this is a safeassumption, and inclusions can be dealt with ac-curately by existing methods, without resorting tospecialized mechanisms.
The alternative hypothe-sis, however, seems more plausible: foreign inclu-sions pose a problem for existing approaches, andsentences containing them are processed less ac-curately.
A parser, for example, is likely to haveproblems with inclusions ?
most of the time, theyare unknown words, and as they originate fromanother language, standard methods for unknownwords guessing (suffix stripping, etc.)
are unlikely tobe successful.
Furthermore, the fact that inclusionsare often multiword expressions (e.g., named enti-ties) means that simply part-of-speech (POS) tag-ging them accurately is not sufficient: if the parserposits a phrase boundary within an inclusion this islikely to severely decrease parsing accuracy.In this paper, we focus on the impact of En-glish inclusions on the parsing of German text.
Wedescribe an annotation-free method that accuratelyrecognizes English inclusions, and demonstrate thatinclusion detection improves the performance of astate-of-the-art parser for German.
We show that theway of interfacing the inclusion detection and theparser is crucial, and propose a method for modify-ing the underlying probabilistic grammar in order to151enable the parser to process inclusions accurately.This paper is organized as follows.
We review re-lated work in Section 2, and present the English in-clusion classifier in Section 3.
Section 4 describesour results on interfacing inclusion detection withparsing, and Section 5 presents an error analysis.Discussion and conclusion follow in Section 6.2 Related WorkPrevious work on inclusion detection exists in theTTS literature.
Here, the aim is to design a sys-tem that recognizes foreign inclusions on the wordand sentence level and functions at the front-end toa polyglot TTS synthesizer.
Pfister and Romsdor-fer (2003) propose morpho-syntactic analysis com-bined with lexicon lookup to identify foreign wordsin mixed-lingual text.
While they state that their sys-tem is precise at detecting the language of tokensand determining the sentence structure, it is not eval-uated on real mixed-lingual text.
A further approachto inclusion detection is that of Marcadet et.
al(2005).
They present experiments with a dictionary-driven transformation-based learning method and acorpus-based n-gram approach and show that a com-bination of both methods yields the best results.Evaluated on three mixed-lingual test sets in differ-ent languages, the combined approach yields word-based language identification error rates (i.e.
the per-centage of tokens for which the language is identi-fied incorrectly) of 0.78% on the French data, 1.33%on the German data and 0.84% on the Spanish data.Consisting of 50 sentences or less for each language,their test sets are very small and appear to be se-lected specifically for evaluation purposes.
It wouldtherefore be interesting to determine the system?sperformance on random and unseen data and exam-ine how it scales up to larger data sets.Andersen (2005), noting the importance of rec-ognizing anglicisms to lexicographers, tests algo-rithms based on lexicon lookup, character n-gramsand regular expressions and a combination thereof toautomatically extract anglicisms in Norwegian text.On a 10,000 word subset of the neologism archive(Wangensteen, 2002), the best method of combin-ing character n-grams and regular expression match-ing yields an accuracy of 96.32% and an F-score of59.4 (P = 75.8%, R = 48.8%).
This result is unsur-prisingly low as no differentiation is made betweenfull-word anglicisms and tokens with mixed-lingualmorphemes in the gold standard.In the context of parsing, Forst and Kaplan (2006)have observed that the failure to properly deal withforeign inclusions is detrimental to a parser?s accu-racy.
However, they do not substantiate this claimusing numeric results.3 English Inclusion DetectionPrevious work reported by Alex (2006; 2005) hasfocused on devising a classifier that detects angli-cisms and other English inclusions in text written inother languages, namely German and French.
Thisinclusion classifier is based on a lexicon and searchengine lookup as well as a post-processing step.The lexicon lookup is performed for tokenstagged as noun (NN ), named entity (NE ), foreignmaterial (FM ) or adjective (ADJA/ADJD ) using theGerman and English CELEX lexicons.
Tokens onlyfound in the English lexicon are classified as En-glish.
Tokens found in neither lexicon are passedto the search engine module.
Tokens found inboth databases are classified by the post-processingmodule.
The search engine module performs lan-guage classification based on the maximum nor-malised score of the number of hits returned for twosearches per token, one for each language (Alex,2005).
This score is determined by weighting thenumber of hits, i.e.
the ?absolute frequency?
by theestimated size of the accessible Web corpus for thatlanguage (Alex, 2006).
Finally, the rule-based post-processing module classifies single-character tokensand resolves language classification ambiguities forinterlingual homographs, English function words,names of currencies and units of measurement.
Afurther post-processing step relates language infor-mation between abbreviations or acronyms and theirdefinitions in combination with an abbreviation ex-traction algorithm (Schwartz and Hearst, 2003).
Fi-nally, a set of rules disambiguates English inclusionsfrom person names (Alex, 2006).For German, the classifier has been evaluatedon test sets in three different domains: newspaperarticles, selected from the Frankfurter AllgemeineZeitung, on internet and telecoms, space travel andEuropean Union related topics.
Table 1 presents an152Domain EI tokens EI types EI TTR Accuracy Precision Recall FInternet 6.4% 5.9% 0.25 98.13% 91.58% 78.92% 84.78Space 2.8% 3.5% 0.33 98.97% 84.02% 85.31% 84.66EU 1.1% 2.1% 0.50 99.65% 82.16% 87.36% 84.68Table 1: English inclusion (EI) token and type statistics, EI type-token-ratios (TTR) as well as accuracy,precision, recall and F-scores for the unseen German test sets.overview of the percentages of English inclusion to-kens and types within the gold standard annotationof each test set, and illustrates how well the Englishinclusion classifier is able to detect them in termsof F-score.
The figures show that the frequency ofEnglish inclusions varies considerably depending onthe domain but that the classifier is able to detectthem equally well with an F-score approaching 85for each domain.The recognition of English inclusions bears sim-ilarity to classification tasks such as named en-tity recognition, for which various machine learning(ML) techniques have proved successful.
In order tocompare the performance of the English inclusionclassifier against a trained ML classifier, we pooledthe annotated English inclusion evaluation data forall three domains.
As the English inclusion classifierdoes not rely on annotated data, it can be tested andevaluated once for the entire corpus.
The ML classi-fier used for this experiment is a conditional Markovmodel tagger which is designed for, and proved suc-cessful in, named entity recognition in newspaperand biomedical text (Klein et al, 2003; Finkel et al,2005).
It can be trained to perform similar informa-tion extraction tasks such as English inclusion detec-tion.
To determine the tagger?s performance over theentire set and to investigate the effect of the amountof annotated training data available, a 10-fold cross-validation test was conducted whereby increasingsub-parts of the training data are provided when test-ing on each fold.
The resulting learning curves inFigure 1 show that the English inclusion classifierhas an advantage over the supervised ML approach,despite the fact the latter requires expensive hand-annotated data.
A large training set of 80,000 tokensis required to yield a performance that approximatesthat of our annotation-free inclusion classifier.
Thissystem has been shown to perform similarly well onunseen texts in different domains, plus it is easily203040506070809010000  20000  30000  40000  50000  60000  70000  80000F-scoreAmount of training data (in tokens)Statistical TaggerEnglish Inclusion ClassifierFigure 1: Learning curve of a ML classifier versusthe English inclusion classifier?s performance.extendable to a new language (Alex, 2006).4 ExperimentsThe primary focus of this paper is to apply the En-glish inclusion classifier to the German TIGER tree-bank (Brants et al, 2002) and to evaluate the clas-sifier on a standard NLP task, namely parsing.
Theaim is to investigate the occurrence of English in-clusions in more general newspaper text, and to ex-amine if the detection of English inclusions can im-prove parsing performance.The TIGER treebank is a bracketed corpus con-sisting of 40,020 sentences of newspaper text.
TheEnglish inclusion classifier was run once over theentire TIGER corpus.
In total, the system detectedEnglish inclusions in 2,948 of 40,020 sentences(7.4%), 596 of which contained at least one multi-word inclusion.
This subset of 596 sentences is thefocus of the work reported in the remainder of thispaper, and will be referred to as the inclusion set.A gold standard parse tree for a sentence contain-ing a typical multi-word English inclusion is illus-trated in Figure 2.
The tree is relatively flat, which153is a trait trait of TIGER treebank annotation (Brantset al, 2002).
The non-terminal nodes of the tree rep-resent the phrase categories, and the edge labels thegrammatical functions.
In the example sentence, theEnglish inclusion is contained in a proper noun (PN )phrase with a grammatical function of type nounkernel element (NK ).
Each terminal node is POS-tagged as a named entity (NE ) with the grammaticalfunction ot type proper noun component (PNC ).4.1 DataTwo different data sets are used in the experiments:(1) the inclusion set, i.e., the sentences containingmulti-word English inclusions recognized by the in-clusion classifier, and (2) a stratified sample of sen-tences randomly extracted from the TIGER corpus,with strata for different sentence lengths.
The stratawere chosen so that the sentence length distributionof the random set matches that of the inclusion set.The average sentence length of this random set andthe inclusion set is therefore the same at 28.4 tokens.This type of sampling is necessary as the inclusionset has a higher average sentence length than a ran-dom sample of sentences from TIGER, and becauseparsing accuracy is correlated with sentence length.Both the inclusion set and the random set consist of596 sentences and do not overlap.4.2 ParserThe parsing experiments were performed with astate-of-the-art parser trained on the TIGER corpuswhich returns both phrase categories and grammati-cal functions (Dubey, 2005b).
Following Klein andManning (2003), the parser uses an unlexicalizedprobabilistic context-free grammar (PCFG) and re-lies on treebank transformations to increase parsingaccuracy.
Crucially, these transformations make useof TIGER?s grammatical functions to relay pertinentlexical information from lexical elements up into thetree.The parser also makes use of suffix analysis.However, beam search or smoothing are not em-ployed.
Based upon an evaluation on the NEGRAtreebank (Skut et al, 1998), using a 90%-5%-5%training-development-test split, the parser performswith an accuracy of 73.1 F-score on labelled brack-ets with a coverage of 99.1% (Dubey, 2005b).
Thesefigures were derived on a test set limited to sentencescontaining 40 tokens or less.
In the data set usedin this paper, however, sentence length is not lim-ited.
Moreover, the average sentence length of ourtest sets is considerably higher than that of the NE-GRA test set.
Consequently, a slightly lower perfor-mance and/or coverage is anticipated, albeit the typeand domain as well as the annotation of both the NE-GRA and the TIGER treebanks are very similar.
Theminor annotation differences that do exist betweenNEGRA and TIGER are explained in Brants et.
al(2002).4.3 Parser ModificationsWe test several variations of the parser.
The baselineparser does not treat foreign inclusions in any spe-cial way: the parser attempts to guess the POS tagand grammatical function labels of the word usingthe same suffix analysis as for rare or unseen Ger-man words.
The additional versions of the parserare inspired by the hypothesis that inclusions makeparsing difficult, and this difficulty arises primarilybecause the parser cannot detect inclusions prop-erly.
Therefore, a suitable upper bound is to givethe parser perfect tagging information.
Two furtherversions interface with our inclusion classifier andtreat words marked as inclusions differently fromnative words.
The first version does so on a word-by-word basis.
In contrast, the inclusion entity ap-proach attempts to group inclusions, even if a group-ing is not posited by phrase structure rules.
We nowdescribe each version in more detail.In the TIGER annotation, preterminals includeboth POS tags and grammatical function labels.For example, rather than a preterminal node hav-ing the category PRELS (personal pronoun), it isgiven the category PRELS-OA (accusative personalpronoun).
Due to these grammatical function tags,the perfect tagging parser may disambiguate moresyntactic information than provided with POS tagsalone.
Therefore, to make this model more realistic,the parser is required to guess grammatical functions(allowing it to, for example, mistakenly tag an ac-cusative pronoun as nominative, dative or genitive).This gives the parser information about the POS tagsof English inclusions (along with other words), butdoes not give any additional hints about the syntaxof the sentence.The two remaining models both take advantage154SNP-SBART-NKDasADJA-NKscho?nstePN-NKNE-PNCRoadNE-PNCMovieVVFIN-HDkamPP-MOAPPR-ACausART-NKderNE-NKSchweizFigure 2: Example parse tree of a German TIGER sentence containing an English inclusion.
Translation:The nicest road movie came from Switzerland.NE FM NN KON CARD ADJD APPR1185 512 44 8 8 1 1Table 2: POS tags of foreign inclusions.PNFOM.
.
.FOM.
.
.
(a) Whenever a FOM is encoun-tered...PNFPFOM.
.
.FOM.
.
.
(b) ...a new FP category is cre-atedFigure 3: Tree transformation employed in the in-clusion entity parser.of information from the inclusion detector.
To inter-face the detector with the parser, we simply markany inclusion with a special FOM (foreign mate-rial) tag.
The word-by-word parser attempts to guessPOS tags itself, much like the baseline.
However,whenever it encounters a FOM tag, it restricts itselfto the set of POS tags observed in inclusions duringtraining (the tags listed in Table 2).
When a FOM isdetected, these and only these POS tags are guessed;all other aspects of the parser remain the same.The word-by-word parser fails to take advantageof one important trend in the data: that foreign in-clusion tokens tend to be adjacent, and these adja-cent words usually refer to the same entity.
Thereis nothing stopping the word-by-word parser frompositing a constituent boundary between two adja-cent foreign inclusions.
The inclusion entity modelwas developed to restrict such spurious bracketing.It does so by way of another tree transformation.The new category FP (foreign phrase) is added be-low any node dominating at least one token markedFOM during training.
For example, when encoun-tering a FOM sequence dominated by PN as in Fig-ure 3(a), the tree is modified so that it is the FP rulewhich generates the FOM tokens.
Figure 3(b) showsthe modified tree.
In all cases, a unary rule PN?FPis introduced.
As this extra rule decreases the proba-bility of the entire tree, the parser has a bias to intro-duce as few of these rules as possible ?
thus limitingthe number of categories which expand to FOMs.Once a candidate parse is created during testing, theinverse operation is applied, removing the FP node.4.4 MethodFor all experiments reported in this paper, the parseris trained on the TIGER treebank.
As the inclusionand random sets are drawn from the whole TIGERtreebank, it is necessary to ensure that the data usedto train the parser does not overlap with these testsentences.
The experiments are therefore designedas multifold cross-validation tests.
Using 5 folds,each model is trained on 80% of the data while theremaining 20% are held out.
The held out set is then155Data P R F Dep.
Cov.
AvgCB 0CB ?2CBBaseline modelInclusion set 56.1 62.6 59.2 74.9 99.2 2.1 34.0 69.0Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1Perfect tagging modelInclusion set 61.3 63.0 62.2 75.1 92.7 1.7 41.5 72.6Random set 65.8 68.9 67.3 82.4 97.7 1.4 45.9 77.1Word-by-word modelInclusion set 55.6 62.8 59.0 73.1 99.2 2.1 34.2 70.2Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1Inclusion entity modelInclusion set 61.3 65.9 63.5 78.3 99.0 1.7 42.4 77.1Random set 63.4 67.5 65.4 80.8 99.2 1.6 40.1 75.7Table 3: Baseline and perfect tagging for inclusion and random sets and results for the word-by-word andthe inclusion entity models.intersected with the inclusion set (or, respectively,the random set).
The evaluation metrics are calcu-lated on this subset of the inclusion set (or randomset), using the parser trained on the correspondingtraining data.
This process ensures that the test sen-tences are not contained in the training data.The overall performance metrics of the parser arecalculated on the aggregated totals of the five heldout test sets.
For each experiment, we report pars-ing performance in terms of the standard PARSE-VAL scores (Abney et al, 1991), including cov-erage (Cov), labeled precision (P) and recall (R),F-score, the average number of crossing brackets(AvgCB), and the percentage of sentences parsedwith zero and with two or fewer crossing brack-ets (0CB and ?2CB).
In addition, we also reportdependency accuracy (Dep), calculated using theapproach described in Lin (1995), using the head-picking method used by Dubey (2005a).
The la-beled bracketing figures (P, R and F), and the de-pendency score are calculated on all sentences, withthose which are out-of-coverage getting zero nodes.The crossing bracket scores are calculated only onthose sentences which are successfully parsed.4.5 Baseline and Perfect TaggingThe baseline, for which the unmodified parser isused, achieves a high coverage at over 99% for boththe inclusion and the random sets (see Table 3).However, scores differ for the bracketing measures.Using stratified shuffling1, we performed a t-test onprecision and recall, and found both to be signif-icantly worse in the inclusion condition.
Overall,the harmonic mean (F) of precision and recall was65.2 on the random set, 6 points better than 59.2F observed on the inclusion set.
Similarly, depen-dency and cross-bracketing scores are higher on therandom test set.
This result strongly indicates thatsentences containing English inclusions present dif-ficulty for the parser, compared to length-matchedsentences without inclusions.When providing the parser with perfect tagginginformation, scores improve both for the inclusionand the random TIGER samples, resulting in F-scores of 62.2 and 67.3, respectively.
However, thecoverage for the inclusion set decreases to 92.7%whereas the coverage for the random set is 97.7%.In both cases, the lower coverage is caused by theparser being forced to use infrequent tag sequences,with the much lower coverage of the inclusion setlikely due to infrequent tags (notable FM ), solelyassociated with inclusions.
While perfect taggingincreases overall accuracy, a difference of 5.1 in F-score remains between the random and inclusion testsets.
Although smaller than that of the baseline runs,this difference shows that even with perfect tagging,1This approach to statistical testing is described in: http://www.cis.upenn.edu/?dbikel/software.html156parsing English inclusions is harder than parsingmonolingual data.So far, we have shown that the English inclusionclassifier is able to detect sentences that are difficultto parse.
We have also shown that perfect tagginghelps to improve parsing performance but is insuffi-cient when it comes to parsing sentences containingEnglish inclusions.
In the next section, we will ex-amine how the knowledge provided by the Englishinclusion classifier can be exploited to improve pars-ing performance for such sentences.4.6 Word-by-word ModelThe word-by-word model achieves the same cover-age on the inclusion set as the baseline but with aslightly lower F of 59.0.
All other scores, includ-ing dependency accuracy and cross bracketing re-sults are similar to those of the baseline (see Ta-ble 3).
This shows that limiting the parser?s choiceof POS tags to those encountered for English inclu-sions is not sufficient to deal with such constructionscorrectly.
In the error analysis presented in Sec-tion 5, we report that the difficulty in parsing multi-word English inclusions is recognizing them as con-stituents, rather than recognizing their POS tags.
Weattempt to overcome this problem with the inclusionentity model.4.7 Inclusion Entity ModelThe inclusion entity parser attains a coverage of99.0% on the inclusion set, similiar to the cover-age of 99.2% obtained by the baseline model onthe same data.
On all other measures, the inclu-sion entity model exceeds the performance of thebaseline, with a precision of 61.3% (5.2% higherthan the baseline), a recall of 65.9% (3.3% higher),an F of 63.5 (4.3 higher) and a dependency accu-racy of 78.3% (3.4% higher).
The average numberof crossing brackets is 1.7 (0.4 lower), with 42.4%of the parsed sentences having no crossing brack-ets (8.2% higher), and 77.1% having two or fewercrossing brackets (8.1% higher).
When testing theinclusion entity model on the random set, the per-formance is very similar to the baseline model onthis data.
While coverage is the same, F and cross-brackting scores are marginally improved, and thedependency score is marginally deteriorated.
Thisshows that the inclusion entity model does not harm00.0020.0040.0060.0080.010.0120.01410  20  30  40  50  60  70  80AverageTokenFrequencySentence Length in TokensInclusion sampleStratified random sampleFigure 4: Average relative token frequencies for sen-tences of equal length.the parsing accuracy of sentences that do not actu-ally contain foreign inclusions.Not only did the inclusion entity parser performabove the baseline on every metric for the inclusionset, its performance also exceeds that of the perfecttagging model on all measures except precision andaverage crossing brackets, where both models aretied.
These results clearly indicate that the inclusionentity model is able to leverage the additional infor-mation about English inclusions provided by our in-clusion classifier.
However, it is also important tonote that the performance of this model on the in-clusion set is still consistently lower than that of allmodels on the random set.
This demonstrates thatsentences with inclusions are more difficult to parsethan monolingual sentences, even in the presence ofinformation about the inclusions that the parser canexploit.Comparing the inclusion set to the length-matched random set is arguably not entirely fair asthe latter may not contain as many infrequent tokensas the inclusion set.
Figure 4 shows the average rel-ative token frequencies for sentences of equal lengthfor both sets.
The frequency profiles of the two datasets are broadly similar (the difference in means ofboth groups is only 0.000676), albeit significantlydifferent according to a paired t-test (p?
0.05).
Thisis one reason why the inclusion entity model?s per-formance on the inclusion set does not reach the up-per limit set by the random sample.157Phrase cat.
Frequency ExamplePN 91 The IndependentCH 10 Made in GermanyNP 4 Peace EnforcementCNP 2 Botts and Company?
2 Chief ExecutivesTable 4: Gold phrase categories of inclusions.5 Error AnalysisThe error analysis is limited to 100 sentences se-lected from the inclusion set parsed with both thebaseline and the inclusion entity model.
This sam-ple contains 109 English inclusions, five of whichare false positives, i.e., the output of the English in-clusion classifier is incorrect.
The precision of theclassifier in recognizing multi-word English inclu-sions is therefore 95.4% for this TIGER sample.Table 4 illustrates that the majority of multi-wordEnglish inclusions are contained in a proper noun(PN ) phrase, including names of companies, politi-cal parties, organizations, films, newspapers, etc.
Aless frequent phrasal category is chunk (CH ) whichtends to be used for slogans, quotes or expressionslike Made in Germany.
Even in this small sam-ple, annotations of inclusions as either PN or CH ,and not the other, can be misleading.
For example,the organization Friends of the Earth is annotatedas a PN , whereas another organization InternationalUnion for the Conservation of Nature is marked asa CH in the gold standard.
This suggests that theannotation guidelines on foreign inclusions could beimproved when differentiating between phrase cate-gories containing foreign material.For the majority of sentences (62%), the baselinemodel predicts more brackets than are present in thegold standard parse tree (see Table 5).
This numberdecreases by 11% to 51% when parsing with the in-clusion entity model.
This suggests that the baselineparser does not recognize English inclusions as con-stituents, and instead parses their individual tokensas separate phrases.
Provided with additional infor-mation of multi-word English inclusions in the train-ing data, the parser is able to overcome this problem.We now turn our attention to how accurately thevarious parsers are at predicting both phrase brack-eting and phrase categories (see Table 6).
For 46Phrase bracket (PB) frequency BL IEPBPRED > PBGOLD 62% 51%PBPRED < PBGOLD 11% 13%PBPRED = PBGOLD 27% 36%Table 5: Bracket frequency of the predicted baseline(BL) and inclusion entity (IE) model output com-pared to the gold standard.
(42.2%) of inclusions, the baseline model makes anerror with a negative effect on performance.
In 39cases (35.8%), the phrase bracketing and phrase cat-egory are incorrect, and constituent boundaries oc-cur within the inclusion, as illustrated in Figure 5(a).Such errors also have a detrimental effect on theparsing of the remainder of the sentence.
Overall,the baseline model predicts the correct phrase brack-eting and phrase category for 63 inclusions (57.8%).Conversely, the inclusion entity model, which isgiven information on tag consistency within inclu-sions via the FOM tags, is able to determine thecorrect phrase bracketing and phrase category for67.9% inclusions (10.1% more), e.g.
see Figure 5(b).Both the phrase bracketing and phrase category arepredicted incorrectly in only 6 cases (5.5%).
Theinclusion entity model?s improved phrase boundaryprediction for 31 inclusions (28.4% more correct) islikely to have an overall positive effect on the pars-ing decisions made for the context which they ap-pear in.
Nevertheless, the inclusion entity parser stillhas difficulty determining the correct phrase cate-gory in 25 cases (22.9%).
The main confusion liesbetween assigning the categories PN , CH and NP ,the most frequent phrase categories of multi-wordEnglish inclusions.
This is also partially due to theambiguity between these phrases in the gold stan-dard.
Finally, few parsing errors (4) are caused bythe inclusion entity parser due to the markup of falsepositive inclusions (mainly boundary errors).6 Discussion and ConclusionThis paper has argued that English inclusions inGerman text is an increasingly pervasive instanceof language mixing.
Starting with the hypothesisthat such inclusions can be a significant source oferrors for monolingual parsers, we found evidencethat an unmodified state-of-the-art parser for Ger-158...PN-NKNP-PNCNE-NKMadePP-MNRAPPR-ADInNE-NKHeaven(a) Partial parsing output of the baseline model with a con-stiuent boundary in the English inclusion....PN-NKFOMMadeFOMInFOMHeaven(b) Partial parsing output of the inclusion en-tity model with the English inclusion parsed cor-rectly.Figure 5: Comparing baseline model output to inclusion entity model output.Errors No.
of inclusions (in %)Parser: baseline model, data: inclusion setIncorrect PB and PC 39 (35.8%)Incorrect PC 5 (4.6%)Incorrect PB 2 (1.8%)Correct PB and PC 63 (57.8%)Parser: inclusion entity model, data: inclusion setIncorrect PB and PC 6 (5.5%)Incorrect PC 25 (22.9%)Incorrect PB 4 (3.7%)Correct PB and PC 74 (67.9%)Table 6: Baseline and inclusion entity model errorsfor inclusions with respect to their phrase bracketing(PB) and phrase category (PC).man performs substantially worse on a set of sen-tences with English inclusions compared to a set oflength-matched sentences randomly sampled fromthe same corpus.
The lower performance on theinclusion set persisted even when the parser whengiven gold standard POS tags in the input.To overcome the poor accuracy of parsing inclu-sions, we developed two methods for interfacing theparser with an existing annotation-free inclusion de-tection system.
The first method restricts the POStags for inclusions that the parser can assign to thosefound in the data.
The second method applies treetransformations to ensure that inclusions are treatedas phrases.
An evaluation on the TIGER corpusshows that the second method yields a performancegain of 4.3 in F-score over a baseline of no inclusiondetection, and even outperforms a model involvingperfect POS tagging of inclusions.To summarize, we have shown that foreign inclu-sions present a problem for a monolingual parser.We also demonstrated that it is insufficient to knowwhere inclusions are or even what their parts ofspeech are.
Parsing performance only improves ifthe parser also has knowledge about the structure ofthe inclusions.
It is particularly important to knowwhen adjacent foreign words are likely to be part ofthe same phrase.
As our error analysis showed, thisprevents cascading errors further up in the parse tree.Finally, our results indicate that future work couldimprove parsing performance for inclusions further:we found that parsing the inclusion set is still harderthan parsing a randomly sampled test set, even forour best-performing model.
This provides an up-per bound on the performance we can expect froma parser that uses inclusion detection.
Future workwill also involve determining the English inclusionclassifier?s merit when applied to rule-based parsing.AcknowledgementsThis research is supported by grants from the Scot-tish Enterprise Edinburgh-Stanford Link (R36759),ESRC, and the University of Edinburgh.
We wouldalso like to thank Claire Grover for her commentsand feedback.159ReferencesSteven Abney, Dan Flickenger, Claudia Gdaniec, RalphGrishman, Philip Harrison, Donald Hindle, Robert In-gria, Frederick Jelinek, Judith Klavans, Mark Liber-man, Mitchell P. Marcus, Salim Roukos, Beatrice San-torini, and Tomek Strzalkowski.
1991.
Procedure forquantitatively comparing the syntactic coverage of En-glish grammars.
In Ezra Black, editor, HLT?91: Pro-ceedings of the workshop on Speech and Natural Lan-guage, pages 306?311, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Beatrice Alex.
2005.
An unsupervised system for identi-fying English inclusions in German text.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL 2005), Student Re-search Workshop, pages 133?138, Ann Arbor, Michi-gan, USA.Beatrice Alex.
2006.
Integrating language knowledgeresources to extend the English inclusion classifier toa new language.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC 2006), Genoa, Italy.Gisle Andersen.
2005.
Assessing algorithms for auto-matic extraction of Anglicisms in Norwegian texts.
InCorpus Linguistics 2005, Birmingham, UK.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER Tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories (TLT02), pages 24?41, So-zopol, Bulgaria.Amit Dubey.
2005a.
Statistical Parsing for German:Modeling syntactic properties and annotation differ-ences.
Ph.D. thesis, Saarland University, Germany.Amit Dubey.
2005b.
What to do when lexicalizationfails: parsing German with suffix analysis and smooth-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL2005), pages 314?321, Ann Arbor, Michigan, USA.Jenny Finkel, Shipra Dingare, Christopher D. Manning,Malvina Nissim, Beatrice Alex, and Claire Grover.2005.
Exploring the boundaries: Gene and proteinidentification in biomedical text.
BMC Bioinformat-ics, 6(Suppl 1):S5.Martin Forst and Ronald M. Kaplan.
2006.
The impor-tance of precise tokenizing for deep grammars.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation (LREC 2006), pages369?372, Genoa, Italy.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics (ACL 2003), pages 423?430,Saporo, Japan.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings of theSeventh Conference on Natural Language Learning(CoNLL-03), pages 180?183, Edmonton, Canada.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof the International Joint Conference on Artificial In-telligence (IJCAI-95), pages 1420?1425, Montreal,Canada.Jean-Christophe Marcadet, Volker Fischer, and ClaireWaast-Richard.
2005.
A transformation-based learn-ing approach to language identification for mixed-lingual text-to-speech synthesis.
In Proceedings ofInterspeech 2005 - ICSLP, pages 2249?2252, Lisbon,Portugal.Beat Pfister and Harald Romsdorfer.
2003.
Mixed-lingual analysis for polyglot TTS synthesis.
InProceedings of Eurospeech 2003, pages 2037?2040,Geneva, Switzerland.Ariel Schwartz and Marti Hearst.
2003.
A simplealgorithm for identifying abbreviation definitions inbiomedical text.
In Proceedings of the Pacific Sym-posium on Biocomputing (PSB 2003), pages 451?462,Kauai, Hawaii.Wojciech Skut, Thorsten Brants, Brigitte Krenn, andHans Uszkoreit.
1998.
A linguistically interpretedcorpus of German newspaper text.
In Proceedings ofthe Conference on Language Resources and Evalua-tion (LREC 1998), pages 705?712, Granada, Spain.Boye Wangensteen.
2002.
Nettbasert nyordsinnsamling.Spra?knytt, 2:17?19.160
