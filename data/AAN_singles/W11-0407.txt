Proceedings of the Fifth Law Workshop (LAW V), pages 56?64,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsA Collaborative Annotation between Human Annotators and a StatisticalParserShun?ya Iwasawa Hiroki Hanaoka Takuya MatsuzakiUniversity of TokyoTokyo, Japan{iwasawa,hkhana,matuzaki}@is.s.u-tokyo.ac.jpYusuke MiyaoNational Institute of InformaticsTokyo, Japanyusuke@nii.ac.jpJun?ichi TsujiiMicrosoft Research AsiaBeijing, P.R.Chinajtsujii@microsoft.comAbstractWe describe a new interactive annotationscheme between a human annotator whocarries out simplified annotations on CFGtrees, and a statistical parser that convertsthe human annotations automatically into arichly annotated HPSG treebank.
In orderto check the proposed scheme?s effectiveness,we performed automatic pseudo-annotationsthat emulate the system?s idealized behaviorand measured the performance of the parsertrained on those annotations.
In addition,we implemented a prototype system and con-ducted manual annotation experiments on asmall test set.1 IntroductionOn the basis of the success of the research on thecorpus-based development in NLP, the demand fora variety of corpora has increased, for use as both atraining resource and an evaluation data-set.
How-ever, the development of a richly annotated cor-pus such as an HPSG treebank is not an easy task,since the traditional two-step annotation, in whicha parser first generates the candidates and then anannotator checks each candidate, needs intensive ef-forts even for well-trained annotators (Marcus et al,1994; Kurohashi and Nagao, 1998).
Among manyNLP problems, adapting a parser for out-domaintexts, which is usually referred to as domain adap-tation problem, is one of the most remarkable prob-lems.
The main cause of this problem is the lackof corpora in that domain.
Because it is difficult toprepare a sufficient corpus for each domain withoutreducing the annotation cost, research on annotationmethodologies has been intensively studied.There has been a number of research projectsto efficiently develop richly annotated corpora withthe help of parsers, one of which is called adiscriminant-based treebanking (Carter, 1997).
Indiscriminant-based treebanking, the annotation pro-cess consists of two steps: a parser first generatesthe parse trees, which are annotation candidates,and then a human annotator selects the most plau-sible one.
One of the most important characteristicsof this methodology is to use easily-understandablequestions called discriminants for picking up the fi-nal annotation results.
Human annotators can per-form annotations simply by answering those ques-tions without closely examining the whole tree.
Al-though this approach has been successful in break-ing down the difficult annotations into a set of easyquestions, specific knowledge about the grammar,especially in the case of a deep grammar, is still re-quired for an annotator.
This would be the bottle-neck to reduce the cost of annotator training and canrestrict the size of annotations.Interactive predictive parsing (Sa?nchez-Sa?ez etal., 2009; Sa?nchez-Sa?ez et al, 2010) is another ap-proach of annotations, which focuses on CFG trees.In this system, an annotator revises the currentlyproposed CFG tree until he or she gets the correcttree by using a simple graphical user interface.
Al-though our target product is a more richly anno-tated treebanks, the interface of CFG can be usefulto develop deep annotations such as HPSG featuresby cooperating with a statistical deep parser.
SinceCFG is easier to understand than HPSG, it can re-56duce the cost of annotator training; non-experts canperform annotations without decent training.
As aresult, crowd-sourcing or similar approach can beadopted and the annotation process would be accel-erated.Before conducting manual annotation, we sim-ulated the annotation procedure for validating oursystem.
In order to check whether the CFG-basedannotations can lead to sufficiently accurate HPSGannotations, several HPSG treebanks were createdwith various qualities of CFG and evaluated by theirHPSG qualities.We further conducted manual annotation experi-ments by two human annotators to evaluate the ef-ficiency of the annotation system and the accuracyof the resulting annotations.
The causes of annota-tion errors were analyzed and future direction of thefurther development is discussed.2 Statistical Deep Parser2.1 HPSGHead-Driven Phrase Structure Grammar (HPSG)is one of the lexicalized grammatical formalisms,which consists of lexical entries and a collection ofschemata.
The lexical entries represent the syntac-tic and semantic characteristics of words, and theschemata are the rules that construct larger phrasesfrom smaller phrases.
Figure 1 shows the mecha-nism of the bottom-up HPSG parsing for the sen-tence ?Dogs run.?
First, a lexical entry is as-signed to each word, and then, the lexical signsfor ?Dogs?
and ?run?
are combined by Subject-Head schema.
In this way, lexical signs and phrasalsigns are combined until the whole sentence be-comes one sign.
Compared to Context Free Gram-mar (CFG), since each sign of HPSG has rich infor-mation about the phrase, such as subcategorizationframe or predicate-argument structure, a corpus an-notated in an HPSG manner is more difficult to buildthan CFG corpus.
In our system, we aim at buildingHPSG treebanks with low-cost in which even non-experts can perform annotations.2.2 HPSG Deep ParserThe Enju parser (Ninomiya et al, 2007) is a statis-tical deep parser based on the HPSG formalism.
Itproduces an analysis of a sentence that includes the264HEAD nounSUBJ <>COMPS <>375Dogs264HEAD verbSUBJ < noun >COMPS <>375Drung?264HEAD verbSUBJ <>COMPS <>375Subject1264HEAD nounSUBJ <>COMPS <>375Headj2664HEAD verbSUBJ < 1 >COMPS <>3775Figure 1: Example of HPSG parsing for ?Dogs run.
?syntactic structure (i.e., parse tree) and the semanticstructure represented as a set of predicate-argumentdependencies.
The grammar design is based onthe standard HPSG analysis of English (Pollard andSag, 1994).
The parser finds a best parse treescored by a maxent disambiguation model using aCKY-style algorithm and beam search.
We useda toolkit distributed with the Enju parser for ex-tracting a HPSG lexicon from a PTB-style treebank.The toolkit initially converts the PTB-style treebankinto an HPSG treebank and then extracts the lexi-con from it.
The HPSG treebank converted from thetest section is also used as the gold standard in theevaluation.2.3 Evaluation MetricsIn the experiments shown below, we evaluate the ac-curacy of an annotation result (i.e., an HPSG deriva-tion on a sentence) by evaluating the accuracy ofthe semantic description produced by the deriva-tion, as well as a more traditional metrics suchas labeled bracketing accuracy of the tree struc-ture.
Specifically, we used labeled and unlabeledprecision/recall/F-score of the predicate-argumentdependencies and the labeled brackets comparedagainst a gold-standard annotation obtained by usingthe Enju?s treebank conversion tool.
A predicate-argument dependency is represented as a tuple of?wp, wa, r?, where wp is the predicate word, wais the argument word, and r is the label of thepredicate-argument relation, such as verb-ARG1(semantic subject of a verb) and prep-MOD (modi-57fiee of a prepositional phrase).
As for the bracketingaccuracies, the label of a bracket is obtained by pro-jecting the sign corresponding to the phrase into asimple phrasal labels such as S, NP, and VP.3 Proposed Annotation SystemIn our system, a human annotator and a statisticaldeep parser cooperate to build a treebank.
Our sys-tem uses CFG as user interface and bridges a gap be-tween CFG and HPSG with a statistical CKY parser.Following the idea of the discriminant-based tree-banking model, the parser first generates candidatetrees and then an annotator selects the correct tree inthe form of a packed forest.
For selecting the correcttree, the annotator only edits a CFG tree projectedfrom an HPSG tree through pre-defined set of oper-ations, to eventually give the constraints onto HPSGtrees.
This is why annotators can annotate HPSGtrees without HPSG knowledge.
The current systemis implemented based on the following client-servermodel.3.1 Client: Annotator InterfaceThe client-side is an annotator?s interface imple-mented with Ajax technique, on which annotator?srevision is carried out through Web-Browser.
Whenthe client-side receives the data of the current besttree from the server-side, it shows an annotator theCFG representation of the tree.
Then, an annotatoradds revisions to the CFG tree using the same GUI,until the current best tree has the CFG structure thatexactly matches the annotators?
interpretation of thesentence.
Finally, the client-side sends the annota-tor?s revision as a CGI query to the server.
Basedon interactive predicative parsing system, two kindsof operations are implemented in our system: ?spanmodification?
and ?label substitution?, here abbrevi-ated as ?S?
and ?L?
operations:?S?
operation modify span(left, right)An annotator can specify that a constituent inthe tree after user?s revision must match a spec-ified span, by sequentially clicking the leafnodes at the left and right boundaries.?L?
operation modify label(pos, label)An annotator can specify that a constituent inthe tree after user?s revision must match a spec-ified label, by inputting a label and clicking thenode position.In addition to ?S?
and ?L?
operations, one moreoperation, ?tree fixation?, abbreviated ?F?, is imple-mented for making annotation more efficient.
Oursystem computes the best tree under the current con-straints, which are specified by the ?S?
and ?L?
op-erations that the annotator has given so far.
It meansother parts of the tree that are not constrained maychange after a new operation by the annotator.
Thischange may lead to a structure that the annotatordoes not want.
To avoid such unexpected changes,an annotator can specify a subtree which he or shedoes not want to change by ?tree fixation?
operation:?F?
operation fix tree(pos = i)An annotator can specify a subtree as correctand not to be changed.
The specified subtreedoes not change and always appears in the besttree.3.2 Server: Parsing ConstraintsIn our annotation system, the server-side carries outthe conversion of annotator?s constraints into HPSGgrammatical constraints on CKY chart and the re-computation of the current best tree under the con-straints added so far.
The server-side works in thefollowing two steps.
The first step is the conversionof the annotator?s revision into a collection of deadedges or dead cells; a dead edge means the edgemust not be a part of the correct tree, and a dead cellmeans all edges in the cell are dead.
As mentionedin the background section, Enju creates a CKY chartduring the parsing where all the terminal and non-terminal nodes are stored with the information of itssign and links to daughter edges.
In our annotationsystem, to change the best tree according to the an-notator?s revision, we determine whether each edgein the chart is either alive or dead.
The server-sidere-constructs the best tree under the constraints thatall the edges used in the tree are alive.
The sec-ond step is the computation of the best tree by re-constructing the tree from the chart, under the con-straint that the best tree contains only the alive edgesas its subconstituents.
Re-construction includes thefollowing recursive process:1.
Start from the root edge.582.
Choose the link which has the highest probabil-ity among the links and whose daughter edgesare all alive.3.
If there is such a link, recursively carry out theprocess for the daughter edge.4.
If all the links from the edge are dead, go backto the previous edge.Note that our system parses a sentence only once,the first time, instead of re-parsing the sentence aftereach revision.
Now, we are going to list the revisionoperations again and explain how the operations areinterpreted as the constraints in the CKY chart.
Inthe description below, label(x) means the CFG-symbol that corresponds to edge x.
Note that thereis in principle an infinite variety of possible HPSGsigns.
The label function maps this multitude ofsigns onto a small set of simple CFG nonterminalsymbols.?S?
operation span(left = i, right = j)When the revision type is ?S?
and the left andright boundary of the specified span is i and jin the CGI query, we add the cells which satisfythe following formula to the list of dead edges.Suppose the sentence length is L, then the setof new dead cells is defined as:{cell(a, b) | 0 ?
a < i,i ?
b < j }?
{cell(c, d) | i+ 1 ?
c ?
j,j + 1 ?
d ?
n },where the first set means the inhibition of theedges that span across the left boundary of thespecified span.
The second set means a similarconditions for the right span.?L?
operation fix label(position = i, label = l)When the revision type is ?L?, the node posi-tion is i and the label is l in the CGI query, wedetermine the set of new dead edges and deadcells as follows:1. let cell(a, b) = the cell including i2.
mark those cells that are generated byspan(a, b) as defined above to be dead,and3.
for each edge e?
in cell(a, b), mark e?to be dead if label(e?)
6= l?F?
operation fix tree(position = i)(a) prob = 0.4 (b) prob = 0.3 (c) prob = 0.2NPNXNPTimeNXfliesPPPXlikeNPDPanNXarrowSNPNXTimeVPVPfliesPPPXlikeNPDPanNXarrowSNPNXNPTimeNXfliesVPVXlikeNPDPanNXarrowFigure 2: Three parse tree candidates of ?Time flies likean arrow.
?When the revision type is ?F?
and the targetnode position is i in the CGI query, we carryout the following process to determine the newdead edges and cells:1. for each edge e in the subtree rooted atnode i,2.
let cell(a, b) = the cell including e3.
mark those cells that are generated byspan(a, b) as defined above to be dead4.
for each edge e?
in cell(a, b), mark e?to be dead if label(e?)
6= label(e)The above procedure adds the constraints sothat the correct tree includes a subtree that hasthe same CFG-tree representation as the sub-tree rooted at i in the current tree.Finally we show how the best tree for the sentence?Time flies like an arrow.?
changes with the anno-tator?s operations.
Let us assume that the chart in-cludes the three trees shown (in the CFG representa-tion) in (Figure 2), and that there are no dead edges.Let us further assume that the probability of eachtree is as shown in the figure and hence the currentbest tree is (a).
If the annotator wants to select (b)as the best tree, s/he can apply ?L?
operation on theroot node.
The operation makes some of the edgesdead, which include the root edge of tree (a) (seeFigure 3).
Accordingly, the best tree is now selectedfrom (b), (c), etc., and tree (b) will be selected as thenext best tree.4 Validation of CFG-based AnnotationBecause our system does not present HPSG anno-tations to the annotators, there is a risk that HPSGannotations are wrong even when their projectionsto CFG trees are completely correct.
Our expecta-59NPTimeNXVPfliesPXVXlikeDPIanINXIarrowINPNX NPPPVPVPNPSNPTimeNXVPfliesPXVXlikeDPIanINXIarrowINPNX NPPPVPVPNPSfix label(root,S)?Figure 3: Chart constraints by ?L?
operation.
Solid linesrepresent the link of the current best tree and dashed linesrepresent the second best one.
Dotted lines stand for anunavailable link due to the death of the source edge.tion is that the stochastic model of the HPSG parserproperly resolves the remaining ambiguities in theHPSG annotation within the constraints given by apart of the CFG trees.
In order to check the validityof this expectation and to measure to what extent theCFG-based annotations can achieve correct HPSGannotations, we performed a pseudo-annotation ex-periment.In this experiment, we used bracketed sentencesin the Brown Corpus (Kuc?era and Francis, 1967),and a court transcript portion of the Manually An-notated Sub-Corpus (MASC) (Ide et al, 2010).
Weautomatically created HPSG annotations that mimicthe annotation results by an ideal annotator in thefollowing four steps.
First, HPSG treebanks forthese sentences are created by the treebank conver-sion program distributed with the Enju parser.
Thisprogram converts a syntactic tree annotated by PennTreebank style into an HPSG tree.
Since this pro-gram cannot convert the sentences that are not cov-ered by the basic design of the grammar, we usedonly those that are successfully converted by theprogram throughout the experiments and consideredthis converted treebank as the gold-standard tree-bank for evaluation.
Second, the same sentences areparsed by the Enju parser and the results are com-pared with the gold-standard treebank.
Then, CFG-level differences between the Enju parser?s outputsand the gold-standard trees are translated into oper-ation sequences of the annotation system.
For ex-ample, ?L?
operation of NX ?
VP at the root nodeis obtained in the case of Figure 4.
Finally, thoseoperation sequences are executed on the annotationsystem and HPSG annotations are produced.total size ave. s. l. convertibleBrown 24,243 18.94 22,214MASC 1,656 14.81 1,353Table 1: Corpus and experimental data information (s. l.means ?sentence length.?
)(a) NXNX PPPX NP(b) VPVP PPPX NPFigure 4: CFG representation of parser output (a) andgold-standard tree (b)4.1 Relationship between CFG and HPSGCorrectnessWe evaluated the automatically produced annota-tions in terms of three measures: the labeled brack-eting accuracies of their projections to CFG trees,the accuracy of the HPSG lexical entry assignmentsto the words, and the accuracy of the semantic de-pendencies extracted from the annotations.
TheCFG-labeled bracketing accuracies are defined inthe same way as the traditional PARSEVAL mea-sures.
The HPSG lexical assignment accuracy isthe ratio of words to which the correct HPSG lex-ical entry is assigned, and the semantic dependencyaccuracy is defined as explained in Section 2.3.
Inthis experiment, we cut off sentences longer than 40words for time reasons.
We split the Brown Cor-pus into three parts: training, development test andevaluation, and evaluated the automatic annotationresults only for the training portion.We created three sets of automatic annotations asfollows:Baseline No operation; default parsing results areconsidered as the annotation results.S-full Only ?S?
operations are used; the tree struc-tures of the resulting annotations should thus beidentical to the gold-standard annotations.SL-full ?S?
and ?L?
operations are used; the la-beled tree structures of the resulting anno-tations should thus be identical to the gold-standard annotations.Before showing the evaluation results, splitting ofthe data should be described here.
Our system as-sumes that the correct tree is included in the parser?s60CKY chart; however, because of the beam-searchlimitation and the incomplete grammar coverage, itdoes not always hold true.
In this paper, such sit-uations are called ?out-chart?.
Conversely, the sit-uations in which the parser does include the cor-rect tree in the CKY chart are ?in-chart?.
The re-sults of ?in-chart?
are here considered to be the re-sults in the ideal situation of the perfect parser.
Inour experimental setting, the training portion of theBrown Corpus has 10,576 ?in-chart?
and 7,208 ?out-chart?
sentences, while the MASC portion has 864?in-chart?
and 489 ?out-chart?
sentences (Table 2).Under ?out-chart?
situations, we applied the opera-tions greedily for calculating S-full and SL-full; thatis, all operations are sequentially applied and an op-eration is skipped when there are no HPSG trees inthe CKY chart after applying that operation.Table 3 shows the results of our three measures:the CFG tree bracketing accuracy, the accuracy ofHPSG lexical entry assignment and that of the se-mantic dependency.
In both of S-full and SL-full,the improvement from the baseline is significant.Especially, SL-full for ?in-chart?
data has almostcomplete agreement with the gold-standard HPSGannotations.
The detailed figures are shown in Ta-ble 4.
Therefore, we can therefore conclude thathigh quality CFG annotations lead to high qualityHPSG annotations when the are combined with agood statistical HPSG parser.4.2 Domain AdaptationWe evaluated the parser accuracy adapted with theautomatically created treebank on the Brown Cor-pus.
In this experiment, we used the adaptation al-gorithm by (Hara et al, 2007), with the same hyper-parameters used there.
Table 5 shows the result ofthe adapted parser.
Each line of this table stands forthe parser adapted with different data.
?Gold?
is theresult adapted on the gold-standard annotations, and?Gold (only covered)?
is that adapted on the golddata which is covered by the original Enju HPSGgrammar that was extracted from the WSJ portionof the Penn Treebank.
?SL-full?
is the result adaptedon our automatically created data.
?Baseline?
is theresult by the original Enju parser, which is trainedonly on the WSJ-PTB and whose grammar was ex-tracted from the WSJ-PTB.
The table shows SL-fullslightly improves the baseline results, which indi-#operationsS L F Avg.
TimeBrown A.
1 122 1 0 1.19 43.32A.
2 91 4 1 0.94 41.77MASC A.
1 275 2 5 2.76 33.33A.
2 52 2 0 0.51 35.13Table 6: The number of operations and annotation timeby human annotators.
?Annotator?
is abbreviated as A.Avg.
is the average number of operations per sentenceand Time is annotation time per sentence [sec.
].cates our annotation system can be useful for do-main adaptation.
Because we used mixed data of?in-chart?
and ?out-chart?
in this experiment, therestill is much room for improvement by increasingthe ratio of the ?in-chart?
sentences using a largerbeam-width.5 Interactive Annotation on aPrototype-systemWe developed an initial version of the annotationsystem described in Section 3, and annotated 200sentences in total on the system.
Half of the sen-tences were taken from the Brown corpus and theother half were taken from a court-debate section ofthe MASC corpus.
All of the sentences were an-notated twice by two annotators.
Both of the anno-tators has background in computer science and lin-guistics.Table 6 shows the statistics of the annotation pro-cedures.
This table indicates that human annotatorsstrongly prefer ?S?
operation to others, and that themanual annotation on the prototype system is at leastcomparable to the recent discriminant-based annota-tion system by (Zhang and Kordoni, 2010), althoughthe comparison is not strict because of the differenceof the text.Table 7 shows the automatic evaluation results.We can see that the interactive annotation gave slightimprovements in all accuracy metrics.
The improve-ments were however not as much as we desired.By classifying the remaining errors in the anno-tation results, we identified several classes of majorerrors:1.
Truly ambiguous structures, which require thecontext or world-knowledge to correctly re-solve them.61in out in+outBrown (train.)
10,576 / 10,394 7,190 / 6,464 17,766 / 16,858MASC 864 / 857 489 / 449 1,353 / 1,306Table 2: The number of ?in-chart?
and ?out-chart?
sentences (total / 1-40 length)in out in+outBrownSL-full 100.00 / 99.31 / 99.60 88.67 / 83.95 / 82.00 94.91 / 92.21 / 92.24S-full 98.46 / 96.64 / 96.83 89.60 / 82.02 / 81.20 94.48 / 89.88 / 90.29Baseline 92.39 / 92.69 / 90.54 82.10 / 78.38 / 73.80 87.78 / 86.07 / 83.54MASCSL-full 100.00 / 99.13 / 99.30 85.91 / 80.75 / 78.80 93.38 / 90.55 / 91.02S-full 98.71 / 96.88 / 96.73 86.95 / 79.14 / 77.43 93.18 / 88.60 / 88.93Baseline 93.98 / 93.51 / 91.56 80.00 / 75.89 / 72.22 87.43 / 85.30 / 83.75Table 3: Evaluation of the automatic annotation sets.
Each cell has the score of CFG F1 / Lex.
Acc.
/ Dep.
F1.CFG tree accuracyBrown MASCA.
1 90.55 / 90.83 / 90.69 90.62 / 90.80 / 90.71A.
2 91.01 / 91.09 / 91.05 91.01 / 91.09 / 91.05Enju 89.70 / 89.74 / 89.72 90.02 / 90.20 / 90.11PAS dependency accuracyBrown MASCA.
1 87.48 / 87.55 / 87.52 86.02 / 86.02 / 86.02A.
2 88.42 / 88.27 / 88.34 85.28 / 91.01 / 85.32Enju 87.12 / 86.91 / 87.01 84.81 / 84.26 / 84.53Table 7: Automatic evaluation of the annotation results(LP / LR / F1)CFG tree accuracyin-chart out-chartA.
1 94.52 / 94.65 / 94.58 83.95 / 84.44 / 84.19A.
2 95.07 / 95.14 / 95.10 84.22 / 84.32 / 84.27Enju 94.44 / 94.37 / 94.40 81.81 / 82.00 / 81.90PAS dependency accuracyin-chart out-chartA.
1 92.85 / 92.85 / 92.85 77.47 / 77.65 / 77.56A.
2 93.34 / 93.34 / 93.34 79.17 / 78.80 / 78.98Enju 92.73 / 92.73 / 92.73 76.57 / 76.04 / 76.30Table 8: Automatic evaluation of the annotation results(LP/LR/F1); in-chart sentences (left-column) and out-chart sentences (right column) both from Brown2.
Purely grammar-dependent analyses, which re-quire in-depth knowledge of the specific HPSGgrammar behind the simplified CFG-tree repre-sentation given to the annotators.3.
Discrepancy between human intuition and theconvention in the HPSG grammar introducedby the automatic conversion.4.
Apparently wrong analysis left untouched dueto the limitation of the annotation system.We suspect some of the errors of type 1 have beencaused by the experimental setting of the annotation;we gave the test sentences randomly drawn fromthe corpus in a randomized order.
This would havemade it difficult for the annotators to interpret thesentences correctly.
We thus expect this kind of er-rors would be reduced by doing the annotation on alarger chunk of text.The second type of the errors are due to the factthat the annotators are not familiar with the detailsof the Enju English HPSG grammar.
For example,one of the annotators systematically chose a struc-ture like (NP (NP a cat) (PP on the mat)).
This struc-ture is however always analysed as (NP a (NP?
cat(PP on the mat))) by the Enju grammar.
The style ofthe analysis implemented in the grammar thus some-times conflicts with the annotators?
intuition and itintroduces errors in the annotation results.Our intention behind the design of the annotationsystem was to make the annotation system more ac-cessible to non-experts and reduce the cost of theannotation.
To reduce the type 2 errors, rather thanthe training of the annotators for a specific gram-mar, we plan to introduce another representationsystem in which the grammar-specific conventionsbecome invisible to the annotators.
For example, theabove-shown difference in the bracketing structuresof a determiner-noun-PP sequence can be hidden byshowing the noun phrase as a ternary branch on thethree children: (NP a cat (PP on the mat)).The third type of the errors are mainly due to therather arbitrary choice of the HPSG analysis intro-duced through the semi-automatic treebank conver-sion used to extract the HPSG grammar.
For in-stance, the Penn Treebank annotates a structure in-cluding an adverb that intervenes an auxiliary verb62Lex-Acc Dep-LP Dep-LR Dep-UP Dep-UR Dep-F1 Dep-EMBrown 99.26 99.61 99.59 99.69 99.67 99.60 95.80MASC 99.13 99.26 99.33 99.42 99.49 99.30 95.68Table 4: HPSG agreement of SL-full for ?in-chart?
data (EM means ?Exact Match.?
)LP LR UP UR F1 EMGold 85.62 85.41 89.70 69.47 85.51 45.07Gold (only covered) 84.32 84.01 88.72 88.40 84.17 42.52SL-full 83.27 82.88 87.93 87.52 83.08 40.19Baseline 82.64 82.20 87.50 87.03 82.42 37.63Table 5: Domain Adaptation Resultsand a following verb as in (VP is (ADVP already)installed).
The attachment direction of the adverb isthus left unspecified.
Such structures are howeverindistinguishably transformed to a binary structurelike (VP (VP?
is already) installed) in the course ofthe conversion to HPSG analysis since there is noway to choose the proper direction only with theinformation given in the source corpus.
This de-sign could be considered as a best-effort, systematicchoice under the insufficient information, but it con-flicts with the annotators?
intuition in some cases.We found in the annotation results that the anno-tators have left apparently wrong analyses on somesentences, either those remaining from the initialoutput proposed by the parser or a wrong structureappeared after some operations by the annotators(error type 4).
Such errors are mainly due to thefact that for some sentences a correct analysis cannotbe found in the parser?s CKY chart.
This can hap-pen either when the correct analysis is not coveredby the HPSG grammar, or the correct analysis hasbeen pruned by the beam-search mechanism in theparser.
To correct a wrong analysis from the insuffi-cient grammar coverage, an expansion of the gram-mar is necessary, either in the form of the expan-sion of the lexicon, or an introduction of a new lex-ical type.
For the other errors from the beam-searchlimitation, there is a chance to get a correct analysisfrom the parser by enlarging the beam size as nec-essary.
The introduction of a new lexical type def-initely requires a deep knowledge on the grammarand thus out of the scope of our annotation frame-work.
The other cases can in principle be handled inthe current framework, e.g., by a dynamic expansionof the lexicon (i.e., an introduction of a new associ-ation between a word and known lexical type), andby a dynamic tuning of the beam size.To see the significance of the last type of the er-ror, we re-evaluated the annotation results on theBrown sentences after classifying them into: (1)those for which the correct analyses were includedin the parser?s chart (in-chart, 65 sentences) and (2)those for which the correct analyses were not in thechart (out-chart, 35 sentences), either because of thepruning effect or the insufficient grammar coverage.The results shown in Table 8 clearly show that thereis a large difference in the accuracy of the annota-tion results between these two cases.
Actually, onthe in-chart sentences, the parser has returned thecorrect analysis as the initial solution for over 50%of the sentences, and the annotators saved it withoutany operations.
Thus, we believe it is quite effectiveto add the above-mentioned functionalities to reducethis type of errors.6 Conclusion and Future WorkWe proposed a new annotation framework for deepgrammars by using statistical parsers.
From the the-oretical point of view, we can achieve significantlyhigh quality HPSG annotations only by CFG annota-tions, and the products can be useful for the domainadaptation task.
On the other hand, preliminary ex-periments of a manual annotation show some diffi-culties about CFG annotations for non-experts, es-pecially grammar-specific ones.
We hence need todevelop some bridging functions reducing such dif-ficulties.
One possible strategy is to introduce an-other representation such as flat CFG than binaryCFG.
While we adopted CFG interface in our firstprototype system, our scheme can be applied to an-other interface such as dependency as long as thereexist some relatedness over syntax or semantics.63ReferencesDavid Carter.
1997.
The treebanker: a tool for super-vised training of parsed corpora.
In Workshop OnComputational Environments For Grammar Develop-ment And Linguistic Engineering, pages 9?15.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an hpsgparser.
In Proceedings of the 10th International Con-ference on Parsing Technologies, pages 11?22, Prague,Czech Republic.Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by the peo-ple.
In Proceedings of the ACL 2010 Conference ShortPapers, pages 68?73, Uppsala, Sweden, July.Sadao Kurohashi and Makoto Nagao.
1998.
Buildinga japanese parsed corpus while improving the parsingsystem.
In Proceedings of the NLPRS, pages 719?724.Henry Kuc?era and W. Nelson Francis.
1967.
Compu-tational Analysis of Present Day American English.Brown University Press, June.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn tree-bank: Annotating predicate argument structure.
InProceedings of the Workshop on Human LanguageTechnology, pages 114?119.Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,and Jun?ichi Tsujii.
2007.
A log-linear model with ann-gram reference distribution for accurate hpsg pars-ing.
In Proceedings of the 10th International Confer-ence on Parsing Technologies, pages 60?68.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez, and Jose?-Miguel Bened??.
2009.
Interactive predictive parsing.In Proceedings of the 11th International Conferenceon Parsing Technologies, pages 222?225.Ricardo Sa?nchez-Sa?ez, Luis A. Leiva, Joan-AndreuSa?nchez, and Jose?-Miguel Bened??.
2010.
Interactivepredictive parsing using a web-based architecture.
InProceedings of the NAACL HLT 2010 DemonstrationSession, pages 37?40.Yi Zhang and Valia Kordoni.
2010.
Discriminant rank-ing for efficient treebanking.
In Coling 2010: Posters,pages 1453?1461, Beijing, China, August.
Coling2010 Organizing Committee.64
