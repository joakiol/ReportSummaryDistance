Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 2?11,Columbus, Ohio, USA June 2008. c?2008 Association for Computational LinguisticsBayesian Learning Over Conflicting Data:Predictions for language changeRebecca MorleyCognitive Science DepartmentJohns Hopkins University3400 N. Charles St.Baltimore, MD 21218morley@cogsci.jhu.eduAbstractThis paper is an analysis of the claim that auniversal ban on certain (?anti-markedness?
)grammars is necessary in order to explain theirnon-occurrence in the languages of the world.To assess the validity of this hypothesis I ex-amine the implications of one sound change (a> ?)
for learning in a specific phonologicaldomain (stress assignment), making explicitassumptions about the type of data that results,and the learning function that computes overthat data.
The preliminary conclusion is thatrestrictions on possible end-point languagesare unneeded, and that the most likely outcomeof change is a lexicon that is inconsistent withrespect to a single generating rule.1 IntroductionThe basic tenet of Evolutionary Phonology is thatthe observed universal commonalities inphonological systems of the world arise from theuniversal commonality of the way listeners andspeakers produce and perceive sound structures(Blevins, 2004).
Diachronic processes operatingvia the transmission of the speech signal act with-out regard for the subsequent system they create.Alternate theories in the tradition of Chomsky ar-gue for universal prohibitions which would serveto ban or repair certain changes just in case theywould result in a ?disallowed?
system (Kiparsky2004, 2006).
In Optimality Theoretic terms, thiswould be a grammar that violates the canonical setof universal markedness constraints.
I will call thisclaim the Universal-Grammar-Delimited Hypothe-sis  Space (UG-Delimited H) Principle.Without this check, Kiparsky argues, commonand natural sound changes (?blind?
EvolutionaryPhonology) would frequently produce unnaturaland in fact unobserved ?anti-markedness?
lan-guages (such as a system in which lower sonorityvowels were stressed in preference to higher so-nority vowels).An analysis of the properties of possiblegrammars is an analysis that involves explicitlycharacterizing the properties of the learner, as wellas of the data to which the learner is exposed.
Thework in this paper is, to my knowledge, the firstattempt to do exactly this kind of analysis, for ex-actly the type of scenario in which a dispreferred,but hypothetically learnable, grammar might arise.Diachronic changes that are caused by factorsoutside of the grammar have the capability of dis-rupting a categorical rule system, introducing ir-regularities into a previously regular pattern.
Theseirregularities may have an ?unnatural?
or anti-markedness character, but typically, they will co-exist alongside remnants of the prior natural pat-tern.
That is the first observation.
The second isthat if learners are allowed to adopt mixed-grammar hypotheses (?co-phonologies?
(Inkelas1997), ?stratal faithfulness?
(Ito and Mester 2001),?lexical indexation?
(Pater 2000)), then under aposterior-maximizing learning model, these hybridsystems are the most likely outcome (rather than acategorical ?anti-markedness?
grammar).I will work through a case study of sonoritysensitive stress, paying special attention to thelexicon that would be produced after a hypotheticalsound change of the type Kiparsky proposes.
Byexamining the output of Bayesian hypothesis test-ing in this domain I will conclude that for the pureanti-markedness grammar to arise, not only is a2certain type of diachronic change necessary, butalso a certain type of non-uniform lexical distribu-tion.
To first approximation, this confluence ofcircumstances appears rather rare, leading me totentatively reject the hypothesis that categoricalbans on allowed grammars are necessary to explainthe distribution of the world?s languages.2 Gujarati PhonologyKiparsky uses Gujarati to provide a concrete illus-tration of the relevant phonological paradigm: asonority-sensitive stress system that respects theposited universal implicational hierarchy.
Thereare eight vowels in Gujarati, corresponding to threesonority tiers: low: (?
), mid: (i,e,?,o,?,u), and high:(a).
The stress system is described as conformingto the following position- and sonority- dependentrules.
[1]   GUJARATI: Sonority & Position -to-Stress?
stress penultimate [a] (the most sonorous vowel)?
otherwise stress ante-penultimate [a]?
otherwise stress final [a]?
otherwise stress penultimate mid-sonority vowel(any of [i,e,?,o,?,u])?
otherwise stress ante-penultimate mid-sonorityvowel?
otherwise stress the penultimate position (whichmust be [?]
(the lowest sonority vowel))This type of system is easily describable withina standard OT framework (Prince and Smolensky1993/2004) that utilizes a universally ordered so-nority scale with respect to the markedness of (ordispreference for) stressing a particular vowel.Crucially, however, the reverse type of system, inwhich lower sonority vowels are the ones that at-tract stress, is so far unattested, and predicted,within the same framework, to be impossible.2.1 Gujarati?In stating his claim about the necessity of intrinsicbans on possible grammars, Kiparsky makes thefollowing assumption: A common and natural typeof sound change is one in which all a?s of a lan-guage change to ??s1.
I will adopt this assumption1In fact, it is not clear how likely an internally motivated la n-guage change of a completely general nature is.
What mightas well for the sake of argument, leaving aside adiscussion of the evidence for how plausible it maybe.
It should be kept in mind that this particularchange is being considered only as a stand-in for aclass of possible sound changes that could producesimilar outcomes with respect to markedness im-plications.A change in vowel quality (with unchangedstress placement) will alter the make-up of theGujarati lexicon, and raise the possibility of a sys-tem in which stress preferentially falls on the low-est-sonority vowel, [?]
(formerly [a], the most so-norous vowel)2.
This new lexicon will, in turn, actas the input to the learner of Gujarati?.
To deter-mine the outcome of learning over this data set,some sort of characterization of the learner?s hy-pothesis space is necessary.
The list in [2] repre-sents the full hypothesis set considered in this pa-per3.
To begin, I will consider only hypotheses 1)-3), leaving aside the discussion of hypotheses 4)and 5) until Section 3.3.
[2]  H :Hypothesis Space1) PENULT: Stress Penultimate Vowel2) GUJARATI: Sonority & Position  -to-Stress3) GUJARATI*: Reversed-Sonority & Position -to-Stress44) NULL(G*/G): GUJARATI* and G UJARATI equallylikely generators of data5) MAX(G*/G): mixed-grammar of GUJARATI* andGUJARATI with variable weightsbe more plausible is that such changes would depend veryheavily on context, with tokens that were less fully realized(e.g., shorter) being more likely to undergo the change thanmore fully /a/-like tokens.
This, of course, would be corre-lated with their stress status.2An alternative traditional generativist account, rather thanadmitting an anti-markedness hypothesis, might propose adifference between stress-attracting ?
?s and non-stress-attracting ?
?s based on differences in their underlying repre-sentations, effectively encoding the diachronic change withinthe synchronic grammar.
This type of analytic bias will im-pede or prevent changes from affecting the rule system(grammar) of a language, and thus it is  not pursued in thepresent work.3This is clearly far from the only way in which the learningproblem can be formulated.
Given that this is, to my knowl-edge, the first study of its kind, a number of somewhat arbi-trary representational decisions had to be made.
For the pur-poses of this work the given H-space is the result of  what Iview as a minimal departure from the standard formalismsboth of  linguistic theory and Bayesian learning.4As in [1], but with the sonority classes reversed.32.2 Evidence to the Learner: Gujarati LexiconThe hypothetical lexicon of Gujarati?
(L?)
dependson the inventory of the old Gujarati (L ).
For agiven possible Gujarati, L is mapped to L?
via thesound change a > ?.
To construct the space of L Istart by making a list of all possible word types,where the type depends on features that are rele-vant to the hypotheses under consideration, namelythe vowel identities.
This listing also correspondsto a particular lexicon LMU?
L; this is the wordinventory under what I will call the Minimal Lexi-con Uniformity assumption: that all types are rep-resented in equal numbers, and each type occursexactly once.
For 3-syllable words and an 8 vowelinventory, there are 83, or 512 distinct types.
For2-syllable words, there are 82, or 64 types.Table 1 lists the word types for 3-syllablewords.
?Case?
refers to the type (vowel make-up)of the word before the hypothetical sound change(where M indicates any of the mid-sonority vowelclass {i,e,?,o,?,u}).
We will restrict ourselves forthe moment to considering only the first three hy-potheses in the space: PENULT(P ), GUJARATI(G),and GUJARATI*(G*)).Case Gujarati ExampleL > L?# typesH1(?,(?,M),a)(M,?,a)(a,?,M)(a,?,(?,a))[p??ik?a?]>[p?rik???]212(M,M,a)(a,M,(M,a,?))[ho?ija??]>[ho?ij??r]84G*3(M,a,(?,M,a))[muba???k]>[mub???
?k]48G*,P4((?,M), M,?)(?,M,M)[t?um?o?t??]>[t?um?o?t??]78G,P5(M,?,M)(M,?,?)[ko?j?ldi]>[ko?j?ldi]42G6(a,a,(a,M))(?,(a,?),(a,?,M))(M,M,M)[aw?a?na?]>[?w???n??
]239G,G*,PTable 1.
Uniform Gujarati Lexicon: three-syllablewords (words taken from de Lacy (2006))Each row represents positive evidence forsome subset of the three hypotheses under consid-eration; the hypotheses consistent with a givencase are specified in the last column below the typecounts.
For example, in Row 3, the word[muba ??
?k] in Gujarati, with stress determined bythe markedness-abiding grammar described in [1]has become [mub???
?k] in Gujarati?.
This formnow exhibits stress on the lowest (rather than thehighest) sonority vowel in the word.
This patternis consistent with the anti-markedness grammarGUJARATI*.
However, the stress placement in thisword is also consistent with the simple positionalgrammar PENULT.
If we indicate the number oftypes that support none of the hypotheses as A(=arbitrary), and the number that support all hy-potheses as N (= neutral), then we can calculate thetotal type counts in support of each hypothesis(A=21; G*=371; G=359; N=239; P=365;T=512).
Note that G*  exceeds P by six wordtypes.3 The Bayesian LearnerThe numbers in Table 1 represent the make-up of apossible lexicon of Gujarati?, namely, LMU?.
Thiswill act as the initial input to our Bayesian learner(for simplicity, all calculations in this section willbe performed only for 3-syllable words).The Bayesian model has been extensively ap-plied to learning scenarios in a number of cognitivedomains (e.g., Chater et al, 2006; Kemp et al,2007; Kording and Wolpert, 2006; Tenenbaum etal., 2007), and involves a fairly minimal and intui-tive apparatus.
Bayes theorem, which provides aformula for computing the posterior probability ofa hypothesis given the data, and thus a method forevaluating competing grammars, is given in (1).
?p(h | d) =p(d | h)p(h)p(d )(1)For the problem at hand, the members of d arestress assignments corresponding to each of the nwords of the lexicon.
The conditional probabilityof a stress assignment diunder hypothesis h ismore properly written as p(di|h,yi), where stressassignment (as can be seen from Table 1) dependson the particular word type yi.
I will assume thatthe conditional probability of each surface stressedform is independent of any other.
The probabilityof the set d given h and y (where h = GUJARATI*,PENULT, or GUJARATI) can then be expanded as theproduct of the probability of each member of d4given h and each member of  y (see Equation (2)).3.1 ?Non-Deterministic?
Hypothesis SpaceApplying Bayes Theorem to the first three hy-potheses of [2] returns a value of p(h|d)=0 for eachgrammar.
To avoid this collapse (due to the exis-tence of contradictory data), let us assign a smallprobability of error (2?)
under each hypothesis.For a given 3-syllable word type, y, there are threestress possibilities: C = {1,2,3}, and the stress classassigned by a given hypothesis Hiis written as afunction of the input word type: Hi(y) ?
C.  For theNon-Deterministic version of the same hypothesis,written as Hi?, stress will be assigned to the con-sistent position (c=Hi(y)) with probability 1-2?,and to either of the two inconsistent positions withprobability ?.
See [3].
[3] Hi?
: Non-Deterministic Version of Hi?p(c |Hi?,y) =1?2?
c = Hi(y)?
c ?
Hi(y)??
?We are assessing the consequences of learningwith no markedness biases, so we will let the priorprobability in Equation (1) be uniform over thehypothesis space.
Since we are concerned with thewinner in any two-hypothesis competition, we willwork with the ratio of their posteriors.
Here thehypotheses GUJARATI*?,  GUJARATI?and P ENULT?are the Non-Deterministic counterparts of the pre-viously introduced hypotheses of the same names,and the numerical values of G*, P and T are ex-tracted from Table 1, under LMU?
(and given at theend of Section 2.1).
?p(GUJARATI*?| d)p(PENULT?| d)=p(di|GUJARATI*?, yi)i?p(di| PENULT?, yi)i???
[di?G*( yi)]?(1?
2?
)[di=G*( yi)]??
[di?P ( yi)]?(1?
2?
)[ di=P ( yi)]?=?
T?G* (1?
2?)G*?
T?P (1?
2?
)P         (2)As we can see from Equation (2), the relativeprobability advantage is highly dependent on themagnitude of ?.
Since ?
is an error term, it shouldremain relatively small.
Within this constraint, wecould allow the learner to fit this parameter basedon maximizing hypothesis likelihood.
For the 3-syllable uniform lexicon, ?MLcomputed with re-spect to GUJARATI* is approximately .14.
Usingthis value in Equation (2) we find that GUJARATI*?wins out over both GUJARATI?and PENULT?bys e v e r a l  o r d e r s  o f  m a g n i t u d e :?p(G*| d)p(P | d)?1.85?104;?p(G*| d)p(G | d)?
3.4 ?108.This initial result seems to provide strong sup-port for The UG-Delimited H  Principle: theGUJARATI* grammar seems overwhelmingly likelyto arise, and yet is unattested.
However, it is in-structive to consider the inherent sensitivity of theBayesian learner to quite small differences be-tween the linguistic hypotheses in question.
A dis-crepancy between data coverage of a mere 6words, as seen in the above case, can lead to a hy-pothesis advantage of four orders of magnitude.And, in fact, a discrepancy of even 1 word can givea posterior advantage on the order of a factor of 5or greater (depending on the value of ?).
This re-sult is the consequence of the extreme probabilitydistribution over only two types of data (consistentand inconsistent -- with values close to 1 in thefirst case, and close to 0 in the second).
Since theprobability of an independent collection of out-comes (a particular input lexicon) is computed viamultiplication, each additional difference in datacoverage compounds the single point case, suchthat the ratio grows exponentially.If this behavior is indeed a problem for ourlinguistic domain (where different sub-regions ofphonological regularity are often observed to co-exist stably in natural language (Inkelas 1997))then there are various means at our disposal tomodify the learning model.
In the following sec-tion I will consider an alternative weighted deci-sion metric; in Section 3.3 I will expand the hy-pothesis space to include mixed-grammar com-petitors; and in Section 4 I will alter the parametersof the learning rule to provide a more stringentthreshold for success in hypothesis competition.3.2 Optimal Bayes ClassifierSo far, we have been implicitly assuming a winner-take-all classification strategy whereby the hy-pothesis with the highest likelihood given the datais the one selected by the learner, and all othersdiscarded.
Let us now consider, instead, the Opti-mal Bayes Classifier which categorizes new in-stances of data by taking a weighted sum of the5predictions of all hypotheses in the space.As expressed in Equation (3), the probabilitythat a new word y will be assigned to category cm(stress syllable m), given the body of training datad ?
p(cm|d,y) ?
is the weighted sum of the prob-ability each hypothesis gives of cmclassification ?p(cm|Hs,y ) ?
where each of these terms isweighted by the a posteriori probability of the par-ticular hypothesis given the training data, p(Hs| d).
?p(cm| d,y) = p(cmHs?|Hs,y)p(Hs| d)       (3)Consider now the situation where there arethree hypotheses in the space: Hi?, Hj?, and Hk?.The formulation of the selector function in Equa-tion (3) allows for the possibility of a ganging-upeffect whereby Hj?and Hk?, even if they individu-ally have lower posterior probability over d thandoes Hi?, can act together to influence the classifi-cation of a new data point y.
We can choose thelexicon in this example so as to showcase the larg-est possible effect these two subordinate rulescould have by making the difference in consistentdata between the (deterministic) hypotheses assmall as possible, such that Hihas a coverage ad-vantage of only one data point over both Hjand Hk.We will also consider those words for which Hjand Hkdiffer from the classification predicted byHi(Hi(y)=c1), but agree with one another in se-lecting c2with the highest probability (Hj(y)=Hk(y)= c2).From Equation (2), with G*-P=1,?p(Hj / k?| d) =?1?2?
p(Hi?
| d)          (4a)Substituting (4a) into Equation (3) gives the prob-ability that classification will occur in line with thedominant hypothesis Hi:?p(c1| d , y) = (1?2?
)P(Hi?| d) +?
?1?2?
P(Hi?
| d )?+ ?
?1?
2?
P(Hi?
| d )             (4b)And the probability that classification will occur inline with the subordinate, but mutually reinforcing,Hjand Hkcan be calculated similarly.The ratio of the probability of categorizing thenew item consistently with Hito that of categoriz-ing consistently with Hjand Hkcan then be shownto be?p(c1| d , y)p(c2| d , y)=6?
2 ?
4?
+13?(1?
2?)
(5)Now take Hi= GUJARATI*, Hj= GUJARATI, and Hk= PENULT; y is a new word of the type in Row 4 ofTable 1.
The gang-up phenomenon, whereGUJARATI and PENULTcollude to move stress awayfrom the position preferred by GUJARATI*, may beseen to have any kind of appreciable effect (where?p(c1| d,y)p(c2| d,y)?1.5 ) only in the region .17 < ?
< .4(relatively large values for ?).
Outside of this re-gion GUJARATI*dominates.
And keep in mind, theadvantage to GUJARATI* only gets higher for largerdifferences in coverage (in Equation (5) only a sin-gle data point separates the three hypotheses), andfor instances of lexical items where GUJARATI andPENULT disagree (Row 5 of Table 1).So far we have seen that the Bayesian frame-work exhibits a potential over-sensitivity whenapplied to problems of the type formulated in thispaper: learning over a space of quasi-categorical,contradictory hypotheses.
This is true whether weconsider learning to result in a single winner-take-all hypothesis, or instead opt for the weighted deci-sion metric of the Optimal Bayes Classifier.
Wewill return to this issue in Section 4.
First, how-ever, I will expand the hypothesis space under con-sideration, in Section 3.3, and introduce, in Section3.4, a non-uniform prior, adding principled biaseson the selection of those different hypotheses.3.3 Mixed-Grammar HypothesesBefore we can assess the performance of the Baye-sian learner with respect to the UG-Delimited HPrinciple we must make sure we consider all po-tential competitor hypotheses that might be betterpredictors of the data than those examined so far.In particular, it is instructive to introduce some-thing like a class of null hypotheses: hybrid gram-mars which explicitly encode equality between anypair of competing alternatives?
ability to explainthe data5.5The effect of mixed-grammar hypotheses can also berealized by allowing a selection procedure over a set of simplegrammars, as described in Section 3.2, but, crucially, with theweights calculated under the assumption that data aregenerated by a combination of grammars (see, for example,the variational model proposed by Yang (1999), or the6I define this class as follows:  the posteriorprobability that the hypothesis NULL(i/j)?assigns toa stress class c is calculated by allotting equalprobability to selecting the Hi?or the Hj?rule toproduce an output of that class:?p(c | NULL(i / j)?, y) = wip(c |Hi?, y) + wjp(c |Hj?, y) (6)where wi= wj=  .5.
From Equation (6) and thedefinition in [3], we can compute the probabilitydistribution of stress assignment c given the appli-cation of NULL(i/j)?to a particular word, y[4] NULL(i/j)?
: ?Null Hypothesis?
?p(c | NULL(i / j)?, y) =1?
2?
c = Hi(y) = Hj(y)1?
?2c = Hi(y) XOR c = Hj(y)?
c ?
Hi(y) &c ?
Hj(y)????
?It can be shown that, for LMU?
(the Gujarati?lexicon generated from the Gujarati minimum uni-form lexicon), the null hypothesis, NULL(G*/G)?,is the decisive winner over GUJARATI*?
(by ap-proximately 30 orders of magnitude).
With thisbroader consideration of the hypothesis space, theanti-markedness grammar is no longer the outcomeof learning.
And it turns out that we can specifyanother hypothesis that gives an even higher likeli-hood over the data.The ?maximum likelihood?
hypotheses arespecified by allowing all three parameters (wi, wj,and ?
(now ?))
in Equation (6) to be estimatedfrom the data.
MAX(i/j)?is defined explicitly belowin [5] for any given weighted combination of Hi?and Hj?.
[5] MAX(i/j)?
: ?Maximum Likelihood?
?p(c |MAX (i / j)?, y) =?
(wi+ wj)(1?
2? )
c = Hi(y) = Hj(y)(1?
2?
)wi+?wjc = Hi(y) &c ?
Hj(y)(1?
2?
)wj+?wic = Hj(y) &c ?
Hi(y)(wi+ wj)?
c ?
Hi(y) &c ?
Hj(y)??????
?When Hi= GUJARATI* and Hj= GUJARATI,MAX(G*/G)?assigns the highest posterior of anywe have seen so far (approximately 56 orders ofmagnitude larger than G*).
This is because, withinthe space of candidates, it gives the highest likeli-hood to the observed data, and the prior probabilityprobabilistic version of Optimality Theory over rankingsutilized by Jarosz (2006)).
(assumed so far to be uniform) plays no role in thiscalculation.
As the hypotheses we are consideringbecome more complicated, however, we are led toconsider an alternative to this assumption, one inwhich hypotheses with longer description lengths,or greater complexity, are penalized (Rissanen1989).3.4 Non-Uniform Prior: Hypothesis DescriptionLengthUnder the uniform prior assumption, only with alexicon in which GUJARATI* accounts for at least44 times as much data as does GUJARATI willMAX(G*/G)?be defeated.
In this section I willshow how that result would be altered by consid-ering a better approximation to the prior probabil-ity distribution over those hypotheses.
MAX(G*/G)?and GUJARATI*?can be seen to differ in a basicway related to the number of parameters and rulesthey must each keep track of.
A domain-independent means of determining a prior prob-ability based on this difference in size, or com-plexity, can be found in the information theoreticnotion of coding cost, or description length.Each hypothesis uses a particular labelingstrategy to encode the input data (which can bequantified by the number of binary pieces of in-formation, or bits needed to transmit that informa-tion to a waiting decoder).
In addition, a certainnumber of bits is needed to encode the hypothesisitself.
The total description length for a string (orset of data) d and a particular hypothesis H is givenby the following general formula for two-partcoding.
?L(d,H ) = L(d |H ) + L(H)(7)The relation of (7) to Bayes Theorem becomesclear when we introduce the fundamental trans-formation from probability to optimal code lengthgiven by?L(x) = ?
logP(x)(8)Intuitively, Equation (8) calls for assigning shorterlength codes to higher probability symbols xwhich, on average, will minimize the code lengthfor a string, d, of symbols drawn from distributionP(x).
The ability to transform between length andprobability allows for the conceptualization of theprior probabilities over the hypothesis space asbiases against complexity.7We can think of the hypotheses in H as deci-sion trees which produce stressed outputs frominput words.
In order to encode such decision treeswe need something like the binary coding schemegiven in Rissanen (1989, section 7.2).
?L(T ) = logkT+ mT?
2kT??????
(9)Here kTis the number of internal (non-terminal)nodes in the tree and mTis the number of leaf (ter-minal) nodes.
Equation (9) provides a measure ofhow much the grammar compresses its input ?
orhow many classes it must keep track of to producethe correct output.
For a series of decisions, basedon querying for a series of features at a series ofinternal nodes, there will be a particular outcome ata particular leaf node.
For the GUJARATI* gram-mar, kT=5 (corresponding to the relevant questionsabout vowel identity listed in definition [1] above),and mT=6 (corresponding to the possible stressdecisions resulting from the answers to each ofthose questions).Additionally, all Non-Deterministic hypothe-ses require the estimation of at least one error term.I will approximate the coding length for a set of kfree parameters (??
?
), estimated over a string oflength n, by Equation (10) (Rissanen 1989, section3.1).?L(?
? )
= k2logn(10)Since I am only interested here in computingthe length associated with the hypotheses them-selves (the negative log of their prior probability),we will focus on the second term of Equation (7),which can be written as the sum of (9) and (10).MAX(G*/G)?consists of a decision tree that istwice as large as that of GUJARATI*?
(since it keepstrack of both GUJARATI*?and GUJARATI?).
Addi-tionally, the combination hypothesis makes use ofone more estimated parameter (wG*).Under LMU?, where n=512 words, the priorprobability ratio6of MAX(G*/G)?to GUJARATI*?is1.7x104.
From this result we can calculate that thetype of lexicon in which the mixed-grammar hy-pothesis would be rejected is one in which theGUJARATI* hypothesis accounts for at least eight6the contribution of the hypotheses lengths,  converted back toprobability via Equation (8)times more data than does GUJARATI (G*/G = 8).This value must be regarded as an approxima-tion due to its dependence on the particular codingscheme used7.
It is, however, likely the best andmost principled estimate of the linguistic-bias-freeprior we can achieve8.Under the information theoretic treatment, itslower probability prior is still not enough to pre-vent MAX(G*/G)?from winning under LMU?
(by 52orders of magnitude over GUJARATI*?).
The pro-ductions of a learner who has converged on thisgrammar would not be obviously consistent with areversed sonority-to-stress output (since manywords would show a stress pattern that is incom-patible with that hypothesis), but neither wouldthose productions be inconsistent with such agrammar (since a (slim) majority of words providepositive evidence for such a hypothesis).
The ty-pological status of such languages will be dis-cussed in the following section.4 Discussion & ConclusionThe foregoing analysis has served to address thequestion of whether the observed frequency of oc-currence (approximately never) of anti-markednesssystems (such as a grammar with a preference forstressing low sonority vowels over high) requiresan active constraint that removes those grammarsfrom the learner?s hypothesis space.
The centralclaim within this paper has been that attempts toanswer this question must involve a careful exami-nation and specification of the learning process, aswell as the inputs to the learner.Given that systems, at any particular time, tend7In practice, a code length exactly equal to the negative log ofthe probability of a particular symbol may be unattainable, andthe relationship in Equation (8) becomes an approximationwhich may be better in some cases than others.
Due to thislimitation, it is not clear how much the exact magnitude of aresult obtained with this method can be relied upon (for a briefdiscussion of this issue see, for example, Brent (1999).
)8An alternative to this approach is to imagine all grammars aspotential mixtures, and to stipulate a prior probability distri-bution over the possible weight values.
Each grammar in thisview is equally complex, but certain weight combinations maybe more likely than others (such as the ?simple?
0/100% distri-bution over weights).
Conceptually this seems at least as rea-sonable as the current approach.
We are still left, however,with the problem of determining the prior probability distribu-tion over the weights, in a manner which, ideally, would beindependent of the problem at hand.8to be in a state in which higher sonority vowelsattract stress (due to assumed perceptual factors),the hypothetical sound change that disrupts thenatural order must act over forms that are origi-nally markedness-abiding.
Thus, there will be aresidue of those forms in the language even afterthe change has occurred (those in which /?/?s notderived from /a/?s fail to attract stress in the pres-ence of mid-sonority vowels).
If this residue issmall enough then the anti-markedness hypothesismight emerge as the winner.
In turn, for this resi-due to be small, the lexicon before the change mustexhibit a certain make-up, such that some wordtypes either fail to appear or occur with muchlower frequency than others.In order to approximate these conditions I cre-ated 1000 (x5) simulated lexicons by sampling(without replacement) from the uniform word in-ventory (LMU) at five different rates; for 3-syllablewords: 1% (=5 types), 3% (=15 types), 5% (=26types), 7% (=36 types), and 10% (=51 types).Higher sampling rates meant a greater likelihoodof reproducing the underlying uniform type distri-bution over the 1000 trials, while lower samplingrates (under-sampling) allowed for a higher likeli-hood of departure from uniformity, and a greaterchance for skewed, or outlier, lexicons to emerge.These simulations were done for the full set ofboth 3-syllable and 2-syllable words (a more real-istic distribution of input to the learner).
To com-bine the two word lengths, with differing numbersof types, I scaled selection from the two classes.
Acursory examination of the online English databaseCELEX (1993) gives a count of 45,652 for 3-syllable words, and 61,738 for 2-syllable words, a1:1.4 relationship.
Using this as a rough guide, andsince the ratio of total types between 3-syllable and2-syllable words is 512:64, a 1:10 scale was used(giving a proportion of 512:640=1:1.25).
Each ofthe five sampling rates maintained this 1:10 scalingfactor, such that the lexicon containing 3-syllableword types sampled at 7%, also contained 2-syllable word types sampled at 70%; this is thelexicon of 36 3-syllable word types (out of a possi-ble total of 512) and 45 2-syllable word types (outof a possible total of 64) (Row 5: [36,45] in Table2).Each lexicon, L, at a particular sampling rate,was transformed to its L ?
counterpart (via thechange a>?
), and the coverage ratio between hy-potheses GUJARATI* and GUJARATI over L ?
wascomputed.
As given at the end of Section 3.4 forthe description-length prior, a value greater thanG*/G = 8 is needed for a GUJARATI* outcome.Here, due to concerns about the sensitivity of theBayesian learner, and the degree of uncertainty inthe calculation of the prior, I relax this criterion.The last four columns of Table 2 correspond tofour (largely arbitrary) values for the G*/G ratiowhich were stipulated as thresholds (or possibleprior probability ratios) that would allow GU-JARATI* to beat MAX(G*/G)?.
Each cell containsthe percentage of anti-markedness outcomes (cal-culated from 1000 runs) for a given threshold, at agiven sampling rate.G*/GSamplingRate[3,2]-syllableword types5 2.5 1.7 1.251%,10% [5,6] 0 0 .4% 6.4%3%,30% [15,19] 0 0 0 .9%5%,50% [26,32] 0 0 0 .1%7%,70% [36,45] 0 0 0 010%,100% [51,64] 0 0 0 0Table 2: Estimated probabilities of learned anti-markedness grammar: under 5 different sampling rates(given as [number of 3-syllable,2-syllable word types]),for four different threshold coverage ratios.The very low occurrence rates of Table 2 showthat changing our assumptions about the make-upof the lexicon (departing from uniformity) do notqualitatively alter the results of the previous sec-tions.
A pure anti-markedness grammar (GU -JARATI*) seems to be a relatively rare outcome ascompared to a mixed-grammar competitor(MAX(G*/G)?
), even under relaxed acceptance cri-teria.The above work relies heavily on the existenceof a residue of natural patterns in a post-soundchange language.
Under circumstances in whichsound change is non-neutralizing (that is, ?
is ab-sent from the inventory of Gujarati before thesound change), there will be no contradictory evi-dence to the learner of Gujarati?
: all data is consis-tent with the GUJARATI* hypothesis.
Furthermore,there is a long-standing intuition in the literaturethat the most likely sound changes might actually9be of this type (Martinet 1955)9.Under these circumstances we might expectGUJARATI* to emerge as the clear winner.
Thiswill depend critically on whether or not we con-sider the lack of conflicting data to be an over-whelming factor in hypothesis selection.
If, in-stead, we maintain our space of non-deterministichypotheses, then there is still competition from themixed-grammar alternatives.
Under the non-neutralizing scenario, Gujarati has 7 vowels (ratherthan 8); for 3-syllable words, all 343 types supportthe G UJARATI*?hypothesis, while 265 are alsoconsistent with PENULT?.
And G*/P = 1.3.
2-syllable words will provide somewhat less of anadvantage to the anti-markedness grammar(49:46~1.13), and with a larger weight (10 timesgreater frequency to approximate the CELEX ra-tios), giving an adjusted ratio of roughly 1.15.Whether this is enough of an advantage to causeGUJARATI*to be selected will depend on the pa-rameters of our learner, as well as the prior prob-ability ratio between the two hypotheses: the dif-ference in complexity between the GUJARATI* rule,which computes stress location based on both po-sition and sonority, and the PENULT rule, whichonly computes over position.What the above discussion illustrates is that theactual form of common or likely sound changescan significantly alter the outcome of analysis.
Ifnon-neutralizing sound changes are the norm, thenthe dispreferred grammar might have a higher pre-dicted likelihood than that calculated here.
Alter-natively, if chain shifts predominate, whereby allthe vowels in the system undergo related incre-mental changes in quality, the outcome might bedifferent again.
And if realistic sound changes op-erate on a word by word basis, as predicted byEvolutionary Phonology, such that results are evenless consistent in terms of sonority class, an evenlower likelihood for a true anti-markedness gram-mar might be the result10.9Thanks to Adam Albright for bringing this to my attention.10Another issue so far undiscussed is the aptness of describingthe GUJARATI* hypothesis as a reversed sonority-to-stressscale.
In either instantiation of Gujarati?
(deriving either fromthe 7- or 8-vowel system) there are only two operable sonoritycategories {MID,?}.
Stressing ?
preferentially over a higher-sonority mid vowel is already dispreferred behavior from auniversalist perspective, but it is qualitatively different than ahypothesis that targets sonority as the deciding factor (ratherthan vowel identity).
This second hypothesis, for example,This work has been a preliminary attempt toaccurately lay out the methodological requirementsfor addressing questions of how grammars arise.Further research ought to be concerned with ex-actly the complications to the question just raised.For present purposes, however, there are two gen-eral points to be made.
The first is that, in order todetermine what any theory predicts in this domain,one has to make assumptions about what consti-tutes a realistic language learner, as well as estab-lish estimates of the normal state of lexical statis-tics.
The second point is that determining thosepredictions tells us what the relevant typologicalfacts are.
The work here suggests that it is the oc-currence, not so much of  pure anti-markednesssystems, but of partial anti-markedness (mixed-grammar) systems that is the critical issue.
It mayturn out to be the case that these systems are alsovery rare, and the over-prediction claim holds in itsrevised form.
However, the true distribution ofthese types of  languages seems far from clear atthe present time, and work will have to be done toestablish the fact of the matter11.AcknowledgmentsThis work was supported by an NSF IGERT grantand a Department of Education Javits Fellowship.I would like to thank Paul Smolensky, Colin Wil-son, and Simon Fischer-Baum for their invaluableassistance.
Thanks also go to the three reviewersof this paper, especially Adam Albright for his ex-tensive and extremely helpful comments.would avoid stressing newly encountered a?s, precisely be-cause of the high sonority of the vowel.
The likelihood ofachieving a true sonority scale reversal seems even lower thanthat of learning the ?stress-??
rule.
This is because the strong-est evidence for a sonority-sensitive scale involves multipletiers or classes of sonority (probably at least three).
However,the more different classes of vowels (the more complicationsto the calculation of stress) the less likely it seems that anindirect sound change (one that does not target sonority itself)will produce a clean reversal of the pattern.
Again, disorder,or proliferating ?co-phonologies?
seem more likely to carry theday.11In the first place, it is not a given that pure anti-markednesssystems are completely  non-occurring (see, for example,Poppe (1960); McLendon (1975); Breen and Pensalfini(1999)).
As for potential mixed-grammar languages, thesemight include systems that have been analyzed as exhibitinghigh degrees of lexical exceptionality, or gone largely un-analyzed due to what is perceived as patternless behavior.10ReferencesBlevins, J.
(2004).
Evolutionary Phonology: the emer-gence of sound patterns.
New York, Cambridge Univer-sity Press.Breen, G. and R. Pensalfini (1999).
"Arrernte: a lan-guage with no syllable onsets."
Linguistic Inquiry 30(1):1-25.Chater, N., J.
B. Tenenbaum, et al (2006).
"Probabilis-tic models of cognition: conceptual foundations.
"Trends in Cognitive Science 10(7): 287-291.Court, C. (1970).
Nasal harmony and some indonesiansound laws.
Pacific Linguistics Series C No.13.
S. A.Wurm and C. Laycock.de Lacy, P. (2006).
Markedness: Reduction and Preser-vation in Phonology, Cambridge University Press.Inkelas, S. (1997).
The theoretical status of morphologi-cally conditioned phonology: a case study of dominanceeffects.
Yearbook of Morphology.
G. Booij and J. vanMarle, Kluwer Academic Publishers: 121-155.Ito, J. and A. Mester (2001).
"Covert generalizations inOptimality Theory: the role of stratal faithfulness con-straints."
Studies in Phonetics, Phonology and Mor-phology 7: 273-299.Jarosz, G. (2006).
Richness of the base and probabilisticunsupervised learning in Optimality Theory.
Proceed-ings of the Eighth Meeting of the ACL Special InterestGroup in Computational Phonology and Morphology,New York City.Kemp, C., A. Perfors, et al (2007).
"Learning overhy-potheses with hierarchical Bayesian models."
Develop-mental Science 10(3): 307-321.Kiparsky, P. (2004).
"Universals constrain change;change results in typological generalizations."
ms.Kiparsky, P. (2006).
"The Amphichronic Program vs.Evolutionary Phonology."
Theoretical Linguistics 32:217-236.Kording, K. P. and D. M. Wolper (2006).
"Bayesiandecision theory in sensorimotor control."
Trends inCognitive Science 10(7): 319-326.Martinet, A.
(1955).
Economie des changements pho-netiques.
Bern, Francke.McLendon, S. (1975).
A Grammar of Eastern Pomo,University of California Press.Mitchell, T. M. (1997).
Machine Learning, McGraw-Hill.Pater, J.
(2000).
"Non-uniformity in English secondaystress: the role of ranked and lexically specific con-straints."
Phonology 17: 237-274.Poppe, N. N. (1960).
Buriat Grammar, Indiana Univer-sity Publications.Prince, A. and P. Smolenksy (1993/2004).
OptimalityTheory, Blackwell Publishing.Rissanen, J.
(1989).
Stochastic Complexity in StatisticalEnquiry, World Scientific Publishing Co.Tenenbaum, J.
B., C. Kemp, et al (2007).
Theory-basedBayesian models of inductive reasoning.
Inductive Rea-soning.
A. Feeney and E. Heit, Cambridge UniversityPress.Yang, C. (1999).
A Selectionist Theory of LanguageAcquisition.
27th Annual Meeting of the Association forComputational Linguistics, College Park, MD.11
