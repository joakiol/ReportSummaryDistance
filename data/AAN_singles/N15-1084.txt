Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 830?840,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsKey Female Characters in Film Have More to Talk About Besides Men:Automating the Bechdel TestApoorv AgarwalComputer Science DepartmentColumbia UniversityNY, USAapoorv@cs.columbia.eduJiehan Zheng?Trinity College of Arts and SciencesDuke UniversityNC, USAjiehan.zheng@duke.eduShruti Vasanth Kamath?Columbia UniversityNY, USAsvk2113@columbia.eduSriram Balasubramanian?FacebookCA, USAgrambler@fb.comShirin Ann DeyColumbia UniversityNY, USAsad2166@columbia.eduAbstractThe Bechdel test is a sequence of threequestions designed to assess the presence ofwomen in movies.
Many believe that be-cause women are seldom represented in filmas strong leaders and thinkers, viewers asso-ciate weaker stereotypes with women.
In thispaper, we present a computational approach toautomate the task of finding whether a moviepasses or fails the Bechdel test.
This allowsus to study the key differences in language useand in the importance of roles of women inmovies that pass the test versus the movies thatfail the test.
Our experiments confirm that inmovies that fail the test, women are in fact por-trayed as less-central and less-important char-acters.1 IntroductionThe Bechdel test is a series of three questions, whichoriginated from Alison Bechdel?s comic ?Dykes toWatch Out For?
(Bechdel, 1986).
The three ques-tions (or tests) are as follows: (T1) are there at leasttwo named women in the movie?
(T2) do thesewomen talk to each other?
and (T3) do these womentalk to each other about something besides a man?If after watching a movie, the viewers answer ?yes?to all three questions, that movie is said to pass theBechdel test.
?These authors contributed equally.
The work was donewhile Jiehan Zheng was at Peddie School and Sriram Balasub-ramanian was at Columbia University.The test was designed to assess the presence ofwomen in movies.
Some researchers have embracedthe test as an effective primary detector for male bias(Scheiner-Fisher and Russell III, 2012).
Due to itsgenerality, the Bechdel test has also been used to as-sess the presence of women in dialogues held on so-cial media platforms such as MySpace and Twitter(Garcia et al, 2014).
Several researchers have notedthat gender inequality roots itself in both the subcon-scious of individuals and the culture of society as awhole (?Zi?zek, 1989; Michel et al, 2011; Garc?
?a andTanase, 2013).
Therefore, combining the Bechdeltest with computational analysis can allow for theexposure of gender inequality over a large body offilms and literature, thus having the potential to alertsociety of the necessity to challenge the status quoof male dominance.In this paper, we investigate the task of automat-ing the Bechdel test.
In doing so, we aim to studythe effectiveness of various linguistic and social net-work analysis features developed for conducting thistask.
Our results show that the features based on so-cial network analysis metrics (such as betweennesscentrality) are most effective.
More specifically, inmovies that fail the test, women are significantlyless centrally connected as compared to movies thatpass the test.
This finding provides support for thelong held belief that women are seldom portrayed asstrong leaders and thinkers in popular media.
Ourresults also show that word unigrams, topic model-ing features, and features that capture mentions ofmen in conversations are less effective.
This maylook like a rather surprising result since the question,830(T3) do these women talk to each other about some-thing besides a man?
seems to be one that linguisticfeatures should be able to answer.
A closer analy-sis suggests why this may be the case.
Consider thescreenplay excerpt in Figure 1 (on the next page).This excerpt is from the movie Hannah and Her Sis-ters, which passes the Bechdel test.
Even thoughthe conversation between named women Mickey andGail mentions a man (He), the conversation is notabout a man.
The conversation is about Mickey?sbrain tumor.
Now consider the following (contrived)conversation between the same characters:Mickey: Ssssss, if i?m in love, I don?tknow what I?m gonna do.Gail: You?re not in love.
Didn?t he tell youthat it was over.Mickey: No, naturallyThis conversation is clearly about a man (or beingin love with a man).
Much like the original conver-sation, this conversation mentions a man only once.The linguistic phenomena that allows us to infer thatthis contrived conversation is about a man is quitecomplex; it requires a deeper semantic analysis andworld knowledge.
First, we need to infer that it be-ing over refers to a relationship.
Relationships typ-ically have two participants.
In order to identify theparticipants, we need to use world knowledge thatrelationships can end and that the person ending therelationship was once part of the relationship, and soon.
Eventually, we are able to conclude that one ofthe main participants of the conversation or the eventbeing discussed is a man.As a first attempt to automate the test, we onlyexperiment with basic linguistic features.
However,we believe that the task itself offers an opportunityfor the development of?
and subsequent evalua-tion of?
rich linguistic features that may be betterequipped for determining the aboutness of conver-sations.The rest of the paper is structured as follows.
Sec-tion 2 reviews the related literature.
Section 3 intro-duces the terminology regarding movie screenplaysthat we use throughout the paper.
Section 4 de-scribes the data and gold standard used for the pur-poses of automating the test.
Sections 5, 6, and 7present our approach, evaluation and results for thethree Bechdel tests, respectively.
We conclude andpresent future direction for research in Section 8.2 RelatedThere has been much work in the computational sci-ences community on studying gender differences inthe way language is used by men versus women(Peersman et al, 2011; Mohammad and Yang, 2011;Bamman et al, 2012; Schwartz et al, 2013; Bam-man et al, 2014; Prabhakaran et al, 2014).
In fact,researchers have proposed linguistic features for su-pervised classifiers that predict the gender of authorsgiven their written text (Koppel et al, 2002; Cor-ney et al, 2002; Cheng et al, 2011).
There has alsobeen a growth in research that utilizes computationaltechniques and big data for quantifying existing gen-der biases in society (Sugimoto et al, 2013; Garciaet al, 2014; Wagner et al, 2015).More closely related to our application is the on-going work in the social sciences community regard-ing the study of gender biases in movie scripts andbooks (Weitzman et al, 1972; Clark et al, 2003;Gooden and Gooden, 2001; McCabe et al, 2011;Chick and Corle, 2012; Smith et al, 2013).
Thiswork has largely depended on manual effort.
Mc-Cabe et al (2011) analyzed the presence of male andfemale characters in titles, and their centralities, in5,618 children?s books.
The authors employed mul-tiple human coders for obtaining the relevant anno-tations.
Smith et al (2013) employed 71 research as-sistants to evaluate 600 films to study gender preva-lence in their scripts.
Our work offers computationaltechniques that may help reduce the manual effortinvolved in carrying out similar social science stud-ies.Recently, Garcia et al (2014) used 213 moviescreenplays for evaluating the correlation of twonovel scores with whether or not movies passed theBechdel test.
However, the main focus of their workwas not to automate the test.
The focus of their workwas to study gender biases in MySpace and Twitter(using these scores).
Nonetheless, we experimentwith these scores and in fact they provide a strongbaseline for automating the task.
Furthermore, weuse our previous work (Agarwal et al, 2014b) toclean noisy screenplays found on the web and carryout the study on a larger data-set of 457 screenplays.831Figure 1: A scene from the movie Hannah and Her Sisters.
The scene shows one conversation between two namedwomen Mickey and Gail.
Tag S denotes scene boundary, C denotes character mention, D denotes dialogue, N denotesscene description, and M denotes meta-data.Researchers in the Natural Language Processing(NLP) community have used movie screenplays fora number of different applications.
Ye and Baldwin(2008) used movie screenplays for evaluating wordsense disambiguation in an effort to automaticallygenerate animated storyboards.
Danescu-Niculescu-Mizil and Lee (2011) utilized movie screenplays forstudying the coordination of linguistic styles in dia-logues.
Bamman et al (2013) used movie plot sum-maries for finding personas of film characters.
Agar-wal et al (2014c) used screenplays for automaticallycreating the xkcd movie narrative charts.
In this pa-per, we use movie screenplays for yet another novelNLP task: automating the Bechdel test.3 Terminology Related to ScreenplaysTuretsky and Dimitrova (2004) described the struc-ture of a movie screenplay as follows: a screenplaydescribes a story, characters, action, setting and di-alogue of a film.
The content of a screenplay fol-lows a (semi) regular format.
Figure 1 shows asnippet of a screenplay from the film Hannah andHer Sisters.
A scene (tag ?S?)
starts with what iscalled the slug line (or scene boundary).
The slugline indicates whether the scene is to take place in-side or outside (INT, EXT), the name of the location(?MICKEY?S OFFICE?
), and can potentially spec-ify the time of day (e.g.
DAY or NIGHT).
Follow-ing the scene boundary, is a scene description (tag?N?).
A scene description is followed by a charactername (tag ?C?
), which is followed by dialogues (tag?D?).
Screenplays also have directions for the cam-era, such as ?CUT TO:, DISSOLVE TO:?.
For lackof a better name, we refer to these as meta-data (tag?M?
).Screenplays are expected to conform to a strictgrammar ?
scene boundaries should be capitalizedand start with markers such as INT./EXT., characternames should be capitalized with an optional (V.O.
)for ?Voice Over?
or (O.S.)
for ?Off-screen.
?, dia-logues and scene descriptions should be indented1ata unique level (i.e.
nothing else in the screenplay isindented at this level).
However, screenplays foundon the web have anomalies in their structures (Gilet al, 2011).
In order to parse screenplays found onthe web, we presented a supervised machine learn-ing approach in Agarwal et al (2014b).
By pars-ing we mean assigning each line of the screenplayone of the following five tags: {S, N, C, D, M}.We showed that a rule based system, often used in1By level of indentation we mean the number of spaces fromthe start of the line to the first non-space character.832the literature (Turetsky and Dimitrova, 2004; Wenget al, 2009; Gil et al, 2011), is not well equippedto handle anomalies in the structure of screenplays.Our supervised models outperformed the regular ex-pressions based baseline by a large and significantmargin (0.69 versus 0.96 macro-F1 measure for thefive classes).
We use these parsed screenplays forthe purposes of this paper.Many of our features designed to automate theBechdel test rely on the definition of a scene and aconversation.
We define them here:Scene: A scene is the span of screenplay that liesbetween two scene boundaries (tag ?S?
).Conversation: A conversation between two or morecharacters is defined as their dialogue exchange inone scene.4 DataThe website bechdeltest.com has reviewedmovies from as long ago as 1892 and as recent as2015.
Over the years, thousands of people have vis-ited the website and assigned ratings to thousands ofmovies: movies that fail the first test are assigneda rating of 0, movies that pass the first test but failthe second test are assigned a rating of 1, moviesthat pass the second test but fail the third test areassigned a rating of 2, and movies that pass all threetests are assigned a rating of 3.
Any visitor who addsa new movie to the list gets the opportunity to ratethe movie.
Subsequent visitors who disagree withthe rating may leave comments stating the reasonfor their disagreement.
The website has a webmas-ter with admin rights to update the visitor ratings.
Ifthe webmaster is unsure or the visitor comments areinconclusive, she sets a flag (called the ?dubious?flag) to true.
For example, niel (webmaster) updatedthe rating for the movie 3 Days to Kill from 1 to 3.2The dubious flag does not show up on the websiteinterface but is available as a meta-data field.
Overthe course of the project, we noticed that the dubiousflag for the movie Up in the Air changed from falseto true.3This provided evidence that the website isactively maintained and moderated by its owners.2http://bechdeltest.com/view/5192/3_days_to_kill/3http://bechdeltest.com/view/578/up_in_the_air/Train & Dev.
Set Test SetFail Pass Fail PassB.
Test 1 26 341 5 85B.
Test 2 128 213 32 53B.
Test 3 60 153 15 38Overall 214 153 52 38Table 1: Distribution of movies for the three tests over thetraining/development and test sets.
B. stands for Bechdel.We crawled a total of 964 movie screenplaysfrom the Internet Movie Script Database (IMSDB).Out of these, only 457 were assigned labels onbechdeltest.com.
We decided to use 367movies for training and development and 90 movies(about 20%) for testing.
Table 1 presents the distri-bution of movies that pass/fail the three tests in ourtraining and test sets.
The distribution shows that amajority of movies fail the test.
In our collection,266 fail while only 191 pass the Bechdel test.5 Test 1: are there at least two namedwomen in the movie?A movie passes the first test if there are two ormore named women in the movie.
We experimentwith several name-to-gender resources for findingthe character?s gender.
If, after analyzing all thecharacters in a movie, we find there are two or morenamed women, we say the movie passes the first test,otherwise it does not.5.1 Resources for Determining GenderIMDB GMAP: The Internet Movie Database(IMDB) provides a full list of the cast and crew formovies.
This list specifies a one-to-one mappingfrom character names to the actors who perform thatrole.
Actors are associated with their gender througha meta-data field.
Using this information, we createdan individual dictionary for each movie that mappedcharacter names to their genders.SSA GMAP: The Social Security Administration(SSA) of the United States has created a publiclyavailable list of first names given to babies born in agiven year, with counts, separated by gender.4Sug-imoto et al (2013) used this resource for assigninggenders to authors of scientific articles.
Prabhakaran4http://www.ssa.gov/oact/babynames/limits.html833Fail Test 1 Pass Test 1Gender Resource P R F1 P R F1 Macro-F1IMDB GMAP 0.35 0.63 0.45 0.97 0.91 0.94 0.71SSA GMAP 0.26 0.21 0.24 0.94 0.95 0.94 0.59STAN GMAP 0.22 0.96 0.36 0.996 0.74 0.85 0.71STAN GMAP+ IMDB GMAP 0.52 0.55 0.54 0.97 0.96 0.96 0.75Table 2: Results for Test 1: ?are there at least two named women in the movie?.et al (2014) used this resource for assigning genderto sender and recipients of emails in the Enron emailcorpus.
The authors noted that a first name may ap-pear several times with conflicting genders.
For ex-ample, the first name Aidyn appears 15 times as amale and 15 times as a female.
For our purposes, weremoved such names from the original list.
The re-sulting resource had 90,000 names, 33,000 with thegender male and 57,000 with the gender female.STAN GMAP: In our experiments, we found bothIMDB GMAP and SSA GMAP to be insufficient.We therefore devised a simple technique for assign-ing genders to named entities using the context oftheir appearance.
This technique is general (not spe-cific to movie screenplays) and may be used for au-tomatically assigning genders to named charactersin literary texts.
The technique is as follows: (1)run a named entity coreference resolution system onthe text, (2) collect all third person pronouns (she,her, herself, he, his, him, himself ) that are resolvedto each entity, and (3) assign a gender based on thegender of the third person pronouns.We used Stanford?s named entity coreference res-olution system (Lee et al, 2013) for finding coref-erences.
Note that the existing coreference sys-tems are not equipped to resolve references withina conversation.
For example, in the conversation be-tween Mickey and Gail (see Figure 1) ?He?
refersto Mickey?s doctor, Dr. Wilkes, who is mentioned byname in an earlier scene (almost 100 lines beforethis conversation).
To avoid incorrect coreferences,we therefore ran the coreference resolution systemonly on the scene descriptions of screenplays.5.2 Results and DiscussionTable 2 presents the results for using various nameto gender mapping resources for the first test.
Sinceit is important for us to perform well on both classes(fail and pass), we report the macro-F1 measure;macro-F1 measure weights the classes equally un-like micro-F1 measure (Yang, 1999).The results show that SSA GMAP performs sig-nificantly5worse than all the other name-to-genderresources.
One reason is that movies have a numberof named characters that have gender different fromthe common gender associated with their names.
Forexample, the movie Frozen (released in 2010) hastwo named women: Parker and Shannon.
Accord-ing to SSA GMAP, Parker is a male, which leads toan incorrect prediction (fail when the movie actuallypasses the first test).The results show that a combination ofSTAN GMAP and IMDB GMAP outperformsall the individual resources by a significant mar-gin.
We combined the resources by taking theirunion.
If a name appeared in both resourceswith conflicting genders, we retained the genderrecorded in IMDB GMAP.
Note that the precision ofIMDB GMAP is significantly higher than the preci-sion of STAN GMAP for the class Fail (0.35 versus0.22).
This has to do with coverage: STAN GMAPis not able to determine the gender of a number ofcharacters and predicts fail when the movie actuallypasses the test.
We expected this behavior as a resultof being able to run the coreference resolution toolonly on the scene descriptions.
Not all charactersare mentioned in scene descriptions.Also note that the precision of IMDB GMAPis significantly lower than the precision ofSTAN GMAP for the class Pass (0.97 versus0.996).
Error analysis revealed two problems withIMDB GMAP.
First, it lists non-named charac-ters (such as Stewardess) along with the namedcharacters in the credit list.
So while the movie A5We use McNemars test with p < 0.05 to report significancethroughout the paper.834Fail Test 2 Pass Test 2Network P R F1 P R F1 Macro-F1CLIQUE 0.55 0.20 0.29 0.65 0.92 0.76 0.57CONSECUTIVE 0.63 0.28 0.39 0.67 0.90 0.77 0.62Table 3: Results for Test 2: ?do these women talk to each other?
?Space Odyssey actually fails the test (it has only onenamed woman, Elena), IMDB GMAP incorrectlydetects Stewardess as another named woman andmakes an incorrect prediction.
Second, certaincharacters are credited with a name different fromthe way they appear in the screenplay.
Followingis a user comment from bechdeltest.com onthe movie Up in the Air that highlights this secondlimitation:Natalie refers to Karen Barnes as ?MissBarnes?
when they first meet.
She is alsonamed later.
Despite the fact that she?scredited as ?Terminated employee?, she?sdefinitely a named female character.The methodology used for finding named womendirectly impacts the performance of our classifierson the next two tests.
For instance, if a method-ology under-predicts the number of named womenin a movie, its chances of failing the next two testsincrease.
In fact, we experimented with all com-binations and found the combination STAN GMAP+IMDB GMAP to outperform other gender resourcesfor the next two tests.
Due to space limitations, wedo not present these results in the paper.
We use thelists of named women and named men generated bySTAN GMAP+ IMDB GMAP for the next two tests.6 Test 2: Do these women talk to eachother?So far, we have parsed screenplays for identifyingcharacter mentions, scene boundaries, and other el-ements of a screenplay (see Figure 1).
We havealso identified the gender of named characters.
Forautomating the second test (do these women talkto each other?)
we experimented with two tech-niques for creating interaction networks of charac-ters.
Consider the following sequence of taggedlines in a screenplay: {S1, C1, C2, C3, S2, C1,.
.
.}.
S1 denotes the first scene boundary, C1 de-notes the first speaking character in the first scene,C2 the second speaking character, and so on.
Oneway of creating an interaction network is to con-nect all the characters that appear between two sceneboundaries (Weng et al, 2009).
Since the charactersC1, C2, and C3 appear between two scene bound-aries (S1 and S2), we connect all the three characterswith pair-wise links.
We call this the CLIQUE ap-proach.
Another way of connecting speaking char-acters is to connect only the ones that appear consec-utively (C1 to C2 and C2 to C3, no link between C1and C3).
We call this the CONSECUTIVE approach.Results presented in Table 3 show that the CON-SEQUITIVE approach performs significantly betterthan the CLIQUE approach.We investigated the reason for an overall low per-formance for this test.
One reason was the over-prediction of named women by our gender resource(labeling Stewardess as a named woman).
Anotherreason was the inconsistent use of scene descrip-tions in screenplays.
Consider the sequence of sceneboundaries, characters, and scene descriptions: {S1,N1, C1, C2, N2, C3, C4, S2, .
.
.}.
While for somescreenplays N2 divided the scene between S1 andS2 into two scenes (S1-N2 and N2-S2), for otherscreenplays it did not.
For the screenplays that itdid, our CONSECUTIVE approach incorrectly con-nected the characters C2 and C3, which led to anover-prediction of characters that talk to each other.Both these reasons contributed to the low recall forthe Fail class.7 Test 3: Do these women talk to eachother about something besides a man?For the third Bechdel test, we experimented withmachine learning models that utilized linguistic fea-tures as well as social network analysis features de-rived from the interaction network of characters.7.1 Feature SetWe considered four broad categories of features:word unigrams (BOW), distribution of conversa-835tions over topics (TOPIC), linguistic features thatcaptured mentions of men in dialogue (LING), andsocial network analysis features (SNA).
We addi-tionally experimented with the two scores proposedby Garcia et al (2014).For BOW, we collected all the words that ap-peared in conversations between pairs of women andnormalized the binary vector by the number of pairsof named women and by the number of conversa-tions they had in a screenplay.
BOW was a fixedfeature vector of length 18,889.The feature set LING consisted of the followingfeatures: (1) the average length of conversationsbetween each pair of named women (2) the num-ber of conversations between each pair of namedwomen, (3) a binary feature that recorded if allconversations between a particular pair of namedwomen mentioned a man, and (4) a binary featurethat recorded if any conversation between a partic-ular pair of named women mentioned a man.
Letus denote these feature vectors by {v1, v2, v3, v4}.Note that the length of these features vectors (|vi| ?
(n2), where n is the number of named women ina movie) may vary from one movie to the other.We converted these variable length vectors intofixed length vectors of length four by using a func-tion, GET MIN MAX MEAN STD(VECTOR), that re-turned the minimum, maximum, mean, and standarddeviation for each vector.
In all, we had 4 ?
4 = 16LING features for each movie.We found multiple instances of conversations thatwere about men but did not explicitly mention aman.
For example, don?t we all fall for those pricks?and which one did you fall in love with?.
We alsofound conversations that mentioned a man explicitlyand were around the same topic (say, love).
For ex-ample, I?m not in love with him, okay!.
In an attemptto capture possible correlations between general top-ics and conversations in which women talked aboutmen, we decided to experiment with features derivedfrom topic models.
We trained a topic model onall the conversations between named women (Bleiet al, 2003; McCallum, 2002).
Before training thetopic model, we converted all the mentions of men toa fixed tag ?MALE?
and all the mentions of womento a fixed tag ?FEMALE?.
For each conversation be-tween every pair of women, we queried the topicmodel for its distribution over the k topics.
Since thenumber of pairs of women and the number of con-versations between them could vary from one movieto the other, we took the average of the k-lengthtopic distributions.
We experimented with k = 2,20, and 50.
Thus the length of the TOPIC featurevector was 72.While the Bechdel test was originally designed toassess the presence of women, it has subsequentlybeen used to comment on the importance of rolesof women in movies.
But does talking about mencorrelate with the importance of their roles?
Tostudy this correlation we designed the following setof SNA features.
We created variable length featurevectors (length equal to number of women) for sev-eral social network analysis metrics (Wasserman andFaust, 1994), all appropriately normalized: (1) de-gree centrality, (2) closeness centrality, (3) between-ness centrality, (4) the number of men a woman wasconnected to, and (5) the number of other women awoman was connected to.
We created two other vari-able length feature vectors (length equal to the num-ber of pairs of women) that recorded (6) the numberof men in common between two women and (7) thenumber of women in common between two women.We converted these variable length feature vectorsto fixed length vectors of length four by using theGET MIN MAX MEAN STD(VECTOR) function de-scribed above.
This constituted 7 ?
4 = 28 of ourSNA features.
We additionally experimented withthe following features: (8) the ratio of the numberof women to the number of men, (9) the ratio of thenumber of women to the total number of characters,(10) the percentage of women that formed a 3-cliquewith a man and another woman, (11, 12, 13) the per-centage of women in the list of five main characters(main based on each of the three notions of central-ities), (14, 15, 16) three boolean features recordingwhether the main character was a women, (17, 18,19) three boolean features recording whether anywoman connected another woman to the main man,and (20, 21, 23) the percentage of women that con-nected the main man to another woman.
In all wehad 28 + 15 = 43 SNA features.As a baseline, we experimented with the featuresproposed by Garcia et al (2014).
The authors pro-posed two scores: BFand BM.
BFwas the ra-tio of {dialogues between female characters that didnot contain mentions of men} over {the total num-836Fail Test 3 Pass Test 3 MacroRow # Kernel Feature Set P R F1 P R F1 F11 Linear Garcia et al (2014) 0.39 0.70 0.50 0.84 0.57 0.67 0.622 Linear BOW 0.40 0.37 0.38 0.74 0.76 0.75 0.573 Linear LING 0.39 0.37 0.37 0.75 0.76 0.75 0.574 Linear TOPIC 0.28 0.29 0.27 0.71 0.70 0.70 0.505 RBF SNA 0.42 0.84 0.56 0.90 0.55 0.68 0.68Table 4: Results for Test 3: ?do these women talk to each other about something besides a man??
Column twospecifies the kernel used with the SVM classifier.Fail Test 3 Pass Test 3 MacroKernel Feature P R F1 P R F1 Macro-F1Linear Garcia et al (2014) 0.72 0.93 0.81 0.81 0.47 0.60 0.73RBF SNA 0.80 0.91 0.85 0.83 0.66 0.73 0.80Table 5: Results on the unseen test set on the end task: does a movie passes the Bechdel Test?ber of dialogues in a movie}.
BMwas the ratioof {dialogues between male characters that did notcontain mentions of women} over {the total numberof dialogues in a movie}.7.2 Evaluation and ResultsThere were 60 movies that failed and 153 moviesthat passed the third test (see Table 1).
We experi-mented with Logistic Regression and Support VectorMachines (SVM) with the linear and RBF kernels.Out of these SVM with linear and RBF kernels per-formed the best.
Table 4 reports the averaged 5-foldcross-validation F1-measures for the best combina-tions of classifiers and feature sets.
For each fold, wepenalized a mistake on the minority class by a factorof 2.55 (153/60), while penalizing a mistake on themajority class by a factor of 1.
This was an impor-tant step and as expected had a significant impact onthe results.
A binary classifier that uses a 0-1 lossfunction optimizes for accuracy.
In a skewed datadistribution scenario where F1-measure is a bettermeasure to report, classifiers optimizing for accu-racy tend to learn a trivial function that classifies allexamples into the same class as the majority class.By penalizing mistakes on the minority class moreheavily, we forced the classifier to learn a non-trivialfunction that achieved a higher F1-measure.Results in Table 4 show that the features derivedfrom social network analysis metrics (SNA) outper-form linguistic features (BOW, LING, and TOPIC)by a significant margin.
SNA features also outper-form the features proposed by Garcia et al (2014)by a significant margin (0.68 versus 0.62).
Variousfeature combinations did not outperform the SNAfeatures.
In fact, all the top feature combinationsthat performed almost as well as the SNA featuresincluded SNA as one of the feature sets.7.3 Evaluation on the End TaskWe used the IMDB GMAP + STAN GMAP gender re-source for the first test, the CONSECUTIVE approachfor creating an interaction network for the secondtest, and compared the performance of the baselineversus the best feature set for the third test.
Table 5presents the results for the evaluation on our unseentest set of 52 movies that failed and 38 movies thatpassed the Bechdel test.
As the results show, our bestfeature and classifier combination outperforms thebaseline by a significant margin (0.73 versus 0.80).Note that the end evaluation is easier than the evalu-ation of each individual test.
Consider a movie thatfails the first test (and thus fails the Bechdel test).At test time, lets say, the movie is mis-predicted andpasses the first two tests.
However, the classifier forthe third test correctly predicts the movie to fail theBechdel test.
Even though the errors propagated allthe way to the third level, these errors are not penal-ized for the purposes of evaluating on the end task.837Figure 2: Distribution of three SNA features (left to right): mean degree centrality, mean closeness centrality, andmean betweenness centrality of named women.
Red histogram is for movies that fail and the Blue histogram is formovies that pass the third Bechdel Test.
The histograms show that the average centralities of women are higher formovies that pass the Bechdel test.7.4 DiscussionWe studied the correlation of our SNA features andthe features proposed by Garcia et al (2014) withthe gold class on the set of 183 movies that pass orfail the third test in our training set.
The most cor-related SNA feature was the one that calculated thepercentage of women who formed a 3-clique with aman and another woman (r = 0.34).
Another highlycorrelated SNA feature was the binary feature thatwas true when the main character was a woman interms of betweenness centrality (r = 0.32).
Severalother SNA features regarding the different notions ofcentralities of women were among the top.
The fea-ture suggested by Garcia et al (2014), BFand BM,were also significantly correlated, with r = 0.27 andr = ?0.23 respectively.Figure 2 shows the distribution of three of ourSNA features: mean degree centrality, mean close-ness centrality, and mean betweenness centrality ofnamed women.
As the distributions show, most ofthe mass for movies that fail the test is towards theleft of the plot, while most of the mass for moviesthat pass is towards the right.
So movies that failthe test tend to have lower centrality measures ascompared to movies that pass the test.
Using ourclassification results, correlation analysis, and visu-alizations of the distributions of the SNA features,we conclude that in fact, movies that fail the testare highly likely to have less centrally connectedwomen.8 Conclusion and Future WorkIn this paper, we introduced a novel NLP task of au-tomating the Bechdel test.
We utilized and studiedthe effectiveness of a wide range of linguistic fea-tures and features derived from social network anal-ysis metrics for the task.
Our results revealed thatthe question, do women talk to each other aboutsomething other than a man, is best answered bynetwork analysis features derived from the interac-tion networks of characters in screenplays.
We werethus able to show a significant correlation betweenthe importance of roles of women in movies withthe Bechdel test.
Indeed, movies that fail the testtend to portray women as less-important and periph-eral characters.To the best of our knowledge, there is no largescale empirical study on quantifying the percentageof children?s books and novels that fail the Bechdeltest.
In the future, we hope to combine some of theideas from this work with our past work on socialnetwork extraction from literary texts (Agarwal andRambow, 2010; Agarwal et al, 2012; Agarwal et al,2013a; Agarwal et al, 2013b; Agarwal et al, 2014a)for presenting a large scale study on children?s bookand novels.AcknowledgmentsWe would like to thank anonymous reviewers fortheir insightful comments.
We would also liketo thank Owen Rambow, Caronae Howell, KapilThadani, Daniel Bauer, and Evelyn Rajan for theirhelpful comments.
We thank Noura Farra for sug-gesting the title for the paper.
The idea originatedfrom a class project (Agarwal, 2013).
We creditMichelle Adriana Marguer Cheripka and Christo-pher I.
Young for the initial idea.
This paper is basedupon work supported in part by the DARPA DEFTProgram.
The views expressed are those of the au-thors and do not reflect the official policy or positionof the Department of Defense or the U.S. Govern-ment.838ReferencesApoorv Agarwal and Owen Rambow.
2010.
Automaticdetection and classification of social events.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 1024?1034, Cambridge, MA, October.
Association for Com-putational Linguistics.Apoorv Agarwal, Augusto Corvalan, Jacob Jensen, andOwen Rambow.
2012.
Social network analysis of al-ice in wonderland.
In Proceedings of the NAACL-HLT2012 Workshop on Computational Linguistics for Lit-erature, pages 88?96, Montr?eal, Canada, June.
Asso-ciation for Computational Linguistics.Apoorv Agarwal, Anup Kotalwar, and Owen Rambow.2013a.
Automatic extraction of social networks fromliterary text: A case study on alice in wonderland.
Inthe Proceedings of the 6th International Joint Confer-ence on Natural Language Processing (IJCNLP 2013).Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, andOwen Rambow.
2013b.
Sinnet: Social interaction net-work extractor from text.
In Sixth International JointConference on Natural Language Processing, page 33.Apoorv Agarwal, Sriramkumar Balasubramanian, AnupKotalwar, Jiehan Zheng, and Owen Rambow.
2014a.Frame semantic tree kernels for social network ex-traction from text.
14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Apoorv Agarwal, Sriramkumar Balasubramanian, JiehanZheng, and Sarthak Dash.
2014b.
Parsing screenplaysfor extracting social networks from movies.
EACL-CLFL 2014, pages 50?58.Apoorv Agarwal, Sarthak Dash, Sriramkumar Balasub-ramanian, and Jiehan Zheng.
2014c.
Using determi-nantal point processes for clustering with applicationto automatically generating and drawing xkcd movienarrative charts.
Academy of Science and Engineering(ASE).Apoorv Agarwal.
2013.
Teaching the basics of nlp andml in an introductory course to information science.In Proceedings of the Fourth Workshop on TeachingNLP and CL, pages 77?84, Sofia, Bulgaria, August.Association for Computational Linguistics.David Bamman, Jacob Eisenstein, and Tyler Schnoebe-len.
2012.
Gender in twitter: Styles, stances, and so-cial networks.
arXiv preprint arXiv:1210.4567.David Bamman, Brendan O?Connor, and Noah A. Smith.2013.
Learning latent personas of film characters.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages 352?361, Sofia, Bulgaria, August.
Association for Compu-tational Linguistics.David Bamman, Jacob Eisenstein, and Tyler Schnoebe-len.
2014.
Gender identity and lexical variation insocial media.
Journal of Sociolinguistics, 18(2):135?160.Alison Bechdel.
1986.
Dykes to watch out for.
FirebrandBooks.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Na Cheng, Rajarathnam Chandramouli, and KP Subbal-akshmi.
2011.
Author gender identification from text.Digital Investigation, 8(1):78?88.Kay Chick and Stacey Corle.
2012.
A gender analysis ofncss notable trade books for the intermediate grades.Social Studies Research and Practice, 7(2):1?14.Roger Clark, Jessica Guilmain, Paul Khalil Saucier, andJocelyn Tavarez.
2003.
Two steps forward, one stepback: The presence of female characters and genderstereotyping in award-winning picture books betweenthe 1930s and the 1960s.
Sex roles, 49(9-10):439?449.Malcolm Corney, Olivier de Vel, Alison Anderson, andGeorge Mohay.
2002.
Gender-preferential text min-ing of e-mail discourse.
In Computer Security Appli-cations Conference, 2002.
Proceedings.
18th Annual,pages 282?289.
IEEE.Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011.Chameleons in imagined conversations: A new ap-proach to understanding coordination of linguisticstyle in dialogs.
In Proceedings of the 2nd Workshopon Cognitive Modeling and Computational Linguis-tics, pages 76?87.
Association for Computational Lin-guistics.David Garc?
?a and Dorian Tanase.
2013.
Measuring cul-tural dynamics through the eurovision song contest.Advances in Complex Systems, 16(08).David Garcia, Ingmar Weber, and Venkata Rama KiranGarimella.
2014.
Gender asymmetries in reality andfiction: The bechdel test of social media.
InternationalConference on Weblogs and Social Media (ICWSM).Sebastian Gil, Laney Kuenzel, and Suen Caroline.
2011.Extraction and analysis of character interaction net-works from plays and movies.
Technical report, Stan-ford University.Angela M Gooden and Mark A Gooden.
2001.
Gen-der representation in notable children?s picture books:1995?1999.
Sex Roles, 45(1-2):89?101.Moshe Koppel, Shlomo Argamon, and Anat Rachel Shi-moni.
2002.
Automatically categorizing written textsby author gender.
Literary and Linguistic Computing,17(4):401?412.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
MIT Press.839Janice McCabe, Emily Fairchild, Liz Grauerholz, Ber-nice A Pescosolido, and Daniel Tope.
2011.
Genderin twentieth-century childrens books patterns of dis-parity in titles and central characters.
Gender & Soci-ety, 25(2):197?226.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K Gray, Joseph PPickett, Dale Hoiberg, Dan Clancy, Peter Norvig,Jon Orwant, et al 2011.
Quantitative analysis ofculture using millions of digitized books.
science,331(6014):176?182.Saif M Mohammad and Tony Wenda Yang.
2011.
Track-ing sentiment in mail: how genders differ on emotionalaxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis (ACL-HLT 2011, pages 70?79.Claudia Peersman, Walter Daelemans, and LeonaVan Vaerenbergh.
2011.
Predicting age and genderin online social networks.
In Proceedings of the 3rdinternational workshop on Search and mining user-generated contents, pages 37?44.
ACM.Vinodkumar Prabhakaran, Emily E Reid, and OwenRambow.
2014.
Gender and power: How gender andgender environment affect manifestations of power.In Proceedings of the 2014 Conference on Empiri-cal Methods in Natural Language Processing, Doha,Qatar, October.
Association for Computational Lin-guistics.Cicely Scheiner-Fisher and William B Russell III.
2012.Using historical films to promote gender equity in thehistory curriculum.
The Social Studies, 103(6):221?225.H.
Andrew Schwartz, Johannes C. Eichstaedt, Mar-garet L. Kern, Lukasz Dziurzynski, Stephanie M. Ra-mones, Megha Agrawal, Achal Shah, Michal Kosin-ski, David Stillwell, Martin E. P. Seligman, andLyle H. Ungar.
2013.
Personality, gender, and agein the language of social media: The open-vocabularyapproach.
PLoS ONE.S.L.
Smith, M. Choueiti, E. Scofield, and K. Pieper.2013.
Gender inequality in 500 popular films: Ex-amining onscreen portrayals and behindthescenes em-ployment patterns in motion pictures released between2007 and 2012.
Media, Diversity, and Social ChangeInitiative: Annenberg School for Communication andJournalism, USC.Cassidy R Sugimoto, Vincent Lariviere, CQ Ni, YvesGingras, and Blaise Cronin.
2013.
Global gender dis-parities in science.
Nature, 504(7479):211?213.Robert Turetsky and Nevenka Dimitrova.
2004.
Screen-play alignment for closed-system speaker identifica-tion and analysis of feature films.
In Multimedia andExpo, 2004.
ICME?04.
2004 IEEE International Con-ference on, volume 3, pages 1659?1662.
IEEE.Claudia Wagner, David Garcia, Mohsen Jadidi, andMarkus Strohmaier.
2015.
It?s a man?s wikipedia?assessing gender inequality in an online encyclopedia.Arxiv preprint arXiv:1501.06307.Stanley Wasserman and Katherine Faust.
1994.
SocialNetwork Analysis: Methods and Applications.
NewYork: Cambridge University Press.Lenore J Weitzman, Deborah Eifler, Elizabeth Hokada,and Catherine Ross.
1972.
Sex-role socialization inpicture books for preschool children.
American jour-nal of Sociology, pages 1125?1150.Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu.
2009.Rolenet: Movie analysis from the perspective of so-cial networks.
Multimedia, IEEE Transactions on,11(2):256?271.Yiming Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Information retrieval,1(1-2):69?90.Patrick Ye and Timothy Baldwin.
2008.
Towards auto-matic animated storyboarding.
In AAAI, pages 578?583.Slavoj?Zi?zek.
1989.
The sublime object of ideology.Verso.840
