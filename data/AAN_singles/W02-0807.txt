Sense Information for Disambiguation:Confluence of Supervised and Unsupervised MethodsKenneth C. LitkowskiCL Research9208 Gue RoadDamascus, MD 20872ken@clres.comAbstractFor SENSEVAL-2, we disambiguated the lexicalsample using two different sense inventories.Official SENSEVAL-2 results were generatedusing WordNet, and separately using the NewOxford Dictionary of English (NODE).
Sinceour initial submission, we have implementedadditional routines and have now examined thedifferences in the features used for making senseselections.
We report here the contribution ofdefault sense selection, idiomatic usage, syntacticand semantic clues, subcategorization patterns,word forms, syntactic usage, context, selectionalpreferences, and topics or subject fields.
We alsocompare the differences between WordNet andNODE.
Finally, we compare these features tothose identified as significant in supervisedlearning approaches.1 IntroductionCL Research?s official submission for SENSEVAL-2used WordNet as the lexical inventory.
Separately,we also used a machine-readable dictionary (TheNew Oxford Dictionary of English, 1998) (NODE),mapping NODE senses automatically into WordNetsenses.
We did not submit these results, since wewere not sure of the feasibility of using onedictionary mapped into another.
Our initial results(Litkowski, 2001)  proved to be much better thananticipated, achieving comparable levels of precisionalthough at lower levels of recall, since not all sensesin NODE mapped into WordNet senses.Subsequently, we examined our results in more detail(Litkowski, 2002), primarily focusing on the qualityof the mapping and its effect on our performanceusing NODE.
This led us to the conclusion that wehad likely performed at a considerably higher levelusing the NODE inventory, with an opportunity foreven better performance as we were able to exploitmuch more information available in NODE.We have now identified what features (i.e., senseinformation) were used in our disambiguation.
Inparticular, we have examined the role of (1) defaultsense selection, (2) idiomatic usage, (3) typing (e.g.,transitivity), (4) syntactic and semantic clues, (5)subcategorization patterns, (6) word form (e.g.,capitalization, tense, or number), (7) selectionalpreferences (for verbs and adjectives), (8) syntacticusage (e.g., nouns as modifiers), (9) context (indefinitions and in examples), and (10) topic area(e.g., subject fields associated with definitions).Our methodology enables us to compare thefeatures relevant to disambiguation in WordNet andin NODE, allowing us to pinpoint differencesbetween the two sense inventories.1In addition,comparing our findings with those identified insupervised machine learning algorithms, we can seepatterns of similarity with our features.In the following sections, we describe ourmethods of dictionary preparat ion, ourdisambiguation techniques, our methodology foranalyzing features and our results.
We discuss thesefindings in terms of what they say about thedifferences between the information available in eachof the two sense inventories, the possiblegeneralizability of our analysis technique, and howour features relate to those used by otherSENSEVAL  participants who used supervisedlearning techniques.
Finally, we describe our futureplans of analysis, based on attempting to merge1We have not yet determined how decisive thesefeatures are in making correct sense selections.
Thepresent study should be viewed as an examination ofsense distinctions in lexical resources.July 2002, pp.
47-53.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sensesupervised and unsupervised word-sensedisambiguation.2 Dictionary PreparationCL Research?s DIMAP (Dictionary MaintenancePrograms) disambiguates open text against WordNetor any other dictionary converted to DIMAP.
Thedictionaries used for disambiguation operate in thebackground (as distinguished from the foregrounddevelopment and maintenance of a dictionary), withrapid lookup to access and examine the multiplesenses of a word after a sentence has been parsed.DIMAP allows multiple senses for each entry, withfields for definitions, usage notes, hypernyms,hyponyms, other semantic relations, and featurestructures containing arbitrary information.For SENSEVAL-2, WordNet was entirelyconverted to alphabetic format for use as thedisambiguation dictionary.
Details of this conversion(which captures all WordNet information) and thecreation of a separate ?phrase?
dictionary for allnoun and verb multiword units (MWUs) aredescribed in Litkowski (2001).
In disambiguation, thephrase dictionary is examined first for a match, withthe full phrase then used to identify the senseinventory rather than a single word.NODE was prepared in a similar manner, withseveral additions.
A conversion program transformedthe MRD files into various fields in DIMAP, thenotable difference being the much richer and moreformal structure (e.g., lexical preferences, grammarfields, and subsensing).
Conversion alsoautomatically created ?kind?
and ?clue?
regularexpression phrases under individual headwords, e.g.,?
(as) happy as a sandboy (or Larry or a clam)?under happy was converted into a collocation patternfor a sense under happy, written ?(as|?)
~ as (asandboy | Larry | a clam)?, with the tilde marking thetarget word.
Further details on this conversion anddefinition parsing to enrich the sense information arealso provided in Litkowski (2001).
After parsing wascompleted, a phrase dictionary was also created forNODE.2The SENSEVAL lexical sample tasks(disambiguating one of 73 target words within a textof several sentences) were run independently againstthe WordNet and NODE sense inventories, with theWordNet results submitted.
To investigate theviability of mapping for WSD, subdictionaries werecreated for each of the lexical sample words.
Foreach word, the subdictionaries consisted of the mainword and all entries identifiable from the phrasedictionary for that word.
(For bar, in NODE, therewere 13 entries where bar was the first word in anMWU and 50 entries where it was the head noun; forbegin, there was only one entry.
)The NODE dictionaries were then mapped intothe WordNet dictionaries (see Litkowski, 1999),using overlap among words and semantic relations.The 73 dictionaries for the lexical sample words gaverise to 1372 WordNet entries and 1722 NODEentries.
Only 491 entries (of which, 418 wereMWUs) were common (i.e., no mappings wereavailable for the remaining 1231 NODE entries, allof which were MWUs); 881 entries in WordNet weretherefore inaccessible through NODE.
For the entriesin common, there was an average of 5.6 senses, ofwhich only 64% were mappable into WordNet, thuscreating our initial impression that use of NODEwould not be feasible.33 Disambiguation TechniquesDetails of the disambiguation process are provided inLitkowski (2001).
In general, for the lexical sample,the sentence containing the target word was firstparsed and the part of speech of the target word wasused to select the sense inventory.
If the tagged wordwas part of an MWU, the MWU's sense inventorywas used.
The dictionary entry for the word was thenaccessed.
Before evaluating the senses, the topic areaof the context provided by the sentence was?established?.
Subject labels for all senses of allcontent words in the context were tallied.Each sense of the target was then evaluated,based on the available information for the sense,including type restrictions such as transitivity,presence of accompanying grammatical constituentssuch as infinitives or complements, selectional2WordNet definitions were not parsed.
An experimentshowed the semantic relations identifiable throughparsing were frequently inconsistent with those inWordNet.3Note that a mapping from WordNet to NODEgenerates similar mismatch statistics.preferences for verbs and adjectives, form restrictionssuch as number and tense, grammatical roles,collocation patterns, contextual clue words,contextual overlap with definitions and examples, andtopical area matches.
Points were given to each senseand the sense with the highest score was selected; incase of a tie, the first sense was selected.The top line of Table 1 shows our official resultsusing WordNet as the disambiguation dictionary,with an overall precision (and recall) of 0.293 at thefine-grained level and 0.367 at the coarse-grainedlevel.
Disambiguating with NODE immediately afterthe official submission and mapping its senses intoWordNet senses achieved comparable levels ofprecision, with a coverage of 75% based on thesenses that could be mapped into WordNet, eventhough the NODE coverage was 100%.Since our original submission, we haveimplemented many additional routines and improvedour NODE mapping to WordNet; our revisedprecision shown in Table 1 are now 0.368 at the fine-grained level and 0.462 at the coarse-grained levelusing WordNet and 0.337 and 0.427 using NODE.Of particular note are the facts that the mapping fromNODE to WordNet is now 89% and that precision iscomparable except for the verbs.In Litkowski (2002), we examined the mappingfrom NODE to WordNet in considerable detail.Several of our findings are pertinent to our analysisof the features affecting disambiguation.
Table 1reflects changes to the automatic mapping along withhand changes.
The automatic mapping changesaccount for the change in coverage.
The handmapping shows that the automatic mapping wasabout 70% accurate.
Interestingly, the hand changesdid not affect precision.
In general, the fact that wewere able to achieve a level of precision comparableto WordNet suggests the most frequent senses of thelexical sample words were able to be disambiguatedand mapped correctly into WordNet.The significant discrepancy between the entries(all MWUs, 1231 entries in NODE not in WordNetand 871 entries in WordNet not in NODE) in partreflects the usual editorial decisions that would befound in examining any two dictionaries.
However,since WordNet is not lexicographically based, manyof the differences are indicative of the idiosyncraticdevelopment of WordNet.
WordNet may identifyseveral types of an entity (e.g., apricot bar andnougat bar), where NODE may use one sense (?anamount of food or another substance formed into aregular narrow block?)
without creating separateentries that follow this regular lexical rule.For the most part, verb phrases containingparticles are equally present in both dictionaries (e.g.,draw out and draw up), but NODE contains severalmore nuanced phrases (e.g., draw in one's horns,draw someone aside, keep one's figure, and pulloneself together).
NODE also contains many idiomswhere a noun is used in a verb phrase (e.g., call it aday, keep one's mouth shut, and go back to nature).About 100 of our disambiguations using NODE wereto MWUs not present in WordNet (20% of ourcoverage gap).Of most significance to the sense mapping is theclassical problem of splitting (attaching moreimportance to differences than to similarities,resulting in more senses) and lumping (attachingmore significance to similarities than to differences,resulting in fewer senses).
Splitting accounts for theremaining 80% gap in our coverage (where NODEidentified senses not present in WordNet).
The effectof lumping is more difficult to assess.
When a NODEdefinition corresponds to more than one sense inWordNet, we may disambiguate correctly in NODE,but receive no score since we have mapped into thewrong definition; the WordNet sense groupings mayallow us to receive credit at the coarse grain, but notat the fine grain.
We have examined this issue inmore detail in Litkowski (2002), with the conclusionthat lumping reduces our NODE score since we areunable to pick out the single WordNet sense answer.More problematic for our mapping was theabsence of crucial information in WordNet.
DelfsTable 1.
Lexical Sample PrecisionRunAdjectives Nouns Verbs TotalItems Fine Coarse Items Fine Coarse Items Fine Coarse Items Fine CoarseWordNet Test 768 0.354 0.354 1726 0.338 0.439 1834 0.225 0.305 4328 0.293 0.367NODE Test 420 0.288 0.288 1403 0.402 0.539 1394 0.219 0.305 3217 0.308 0.405WordNet Test (R) 768 0.435 0.435 1726 0.430 0.535 1834 0.267 0.387 4328 0.368 0.462NODE Test (R) 684 0.472 0.472 1567 0.429 0.537 1605 0.189 0.300 3856 0.337 0.427(2001) described a sense for begin that has aninfinitive complement, but present only in an examplesentence and not explicitly encoded with the usualWordNet verb frame.
Similarly, for train, twosentences were ?tagged to transitive senses despitebeing intransitive because again we were dealing withan implied direct object, and the semantics of thesense that was chosen fit; we just pretended that theobject was there.?
In improving our disambiguationroutines, it will be much more difficult to glean theappropriate criteria for sense selection in WordNetwithout this explicit information than to obtain it inNODE and map it into WordNet.
Much of thisinformation is either not available in WordNet,available only in an unstructured way, only implicitlypresent, or inconsistently present.4 Feature Analysis Methodology4.1 Identifying Disambiguation FeaturesAs indicated above, our disambiguation routinesassign point values based on a judgment of howimportant each feature seems to be.
The weightingscheme is ad-hoc.
For the feature analysis, we simplyrecorded a binary variable for each feature that hadmade a contribution to the final sense selection.
Inparticular, we identified the following features: (1)whether the sense selected was the default (first)sense (i.e., no other features were identified inexamining any of the senses), (2) whether theidentified sense was based on the occurrence of thetarget word in an idiom, (3) whether a type(specifically, transitivity) factored into the senseselection, (4) whether the selected sense had anysyntactic or semantic clues, (5) whether asubcategorization pattern figured into the senseselection, (6) whether the sense had a specified wordform (e.g., capitalization, tense, or number), (7)whether a syntactic usage was relevant (e.g., nounsas modifiers or an adjective being used as a noun,such as ?the blind?
), (8) whether a selectionalpreference was satisfied (for verb subjects andobjects and adjective modificands), (9) whether wewere able to use a Lesk-style context clue from thedefinitions or an example, and (10) topic area (e.g.,subject fields, usage labels, or register labelsassociated with definitions).As the disambiguation algorithm proceeded, werecorded each of the features associated with eachsense.
After a sense was selected, the featuresassociated with that sense were written to a file (as ahexadecimal number) for subsequent analysis.
Wesorted the senses for each target word in the lexicalsample and summarized the features that were usedfor all instances that had the same sense.
We thensummarized the features over all senses and furthersummarized them by part of speech.
These results areshown in Table 2.The first column shows the number of instancesfor each part of speech and overall.
The secondcolumn shows the number of instances where thedisambiguation algorithm selected the default sense.These cases indicate the absence of positiveinformation for selecting a sense and may beconstrued as indicating that the sense inventory maynot make sufficient sense distinctions.
The defaultnumbers are somewhat misleading for verbs, wherethe mere presence of an object (recorded in the ?with?column) sufficed to make a selection ?non-default?.As well, the default selections may indicate that ourdisambiguation does not yet make full use of thedistinctions that are available.
As we makeimprovements in our algorithm, we would expect thenumber of default selections to decrease.Table 2.
Comparative Analysis of Features Used in WordNet and NODE DisambiguationInstance Default Idiom Kind Clue Context Topics Form With As Prefs POSWordNet768 556 79 0 0 190 0 0 15 0 1 Adjectives1754 1140 293 0 0 536 0 0 29 0 0 Nouns1804 436 161 0 2 576 0 0 984 0 0 Verbs4326 2132 533 0 2 1302 0 0 1028 0 1 TotalNODE768 324 81 0 2 249 168 14 11 11 33 Adjectives1754 456 269 14 94 546 364 317 28 136 3 Nouns1804 175 105 61 124 564 285 353 573 187 108 Verbs4326 955 455 75 220 1359 817 684 612 334 144 TotalThe significant difference in the number ofdefault selections between WordNet and NODE is abroad indicator that there is more informationavailable in NODE than in WordNet.
In examiningthe results for individual words, even in cases wherethe ?default?
(or first) sense was being selected, thedecision was being made in NODE based on positiveinformation rather than the absence of information.Generally (but not absolutely), the intent of thecompilers of both WordNet and NODE is that thefirst sense correspond to the most frequent sense.
Therelative importance of the default sense indicated byour results suggests the importance of ensuring thatthis is the case.
In a few instances, the first NODEsense did not correspond to the first WordNet sense,and we were able to obtain a much better resultdisambiguating in NODE than in WordNet by usingan appropriate mapping from NODE to a second orthird WordNet sense.
The significance of the defaultsense is important in the selection of instances in anevaluation such as SENSEVAL; if the instances donot reflect common usage, WSD results may bebiased simply because of the instance selection.The ?idiom?
column indicates those cases wherea phrasal entry was used to provide the senseinventory.
As pointed out above, these correspond tothe MWUs that were created and account for over10% of the lexical instances.The ?kind?
and ?clue?
columns correspond toeither strong or slightly weaker collocational patternsthat have been associated with individual senses.These correspond to similarly named sense attributesused in the Hector database for SENSEVAL-1,which was the experimental basis for NODE.
As canbe seen in the table, these were relevant to the senseselection for about 6.5 percent of the instances forNODE.
We converted several of WordNet?s verbframes into clue format; however, they did not showup as features in our analysis, probably because ourimplementation needs to be improved.
We expect thatfurther improvements will obtain some cases wherethese are relevant in the WordNet disambiguation (aswell as increasing the number of cases where theseare relevant to NODE senses).The context column reflects the significance ofLesk-style information available in the definitions andexamples.
In general, it appears that about a third ofthe lexical instances were able to use thisinformation.
This reflects the extent to which thedictionary compilers are able to provide goodexamples for the individual senses.
Since space islimited for such examples, our results indicate thatthere will an inevitable upper limit of the extent towhich disambiguation can rely on such information(a conclusion also reached by (Haynes 2001)).The potential significance of subject or topicfields associated with individual senses is indicatedby the number of cases where NODE was able to usethis information (nearly 20 percent of the instances).NODE makes extensive use of subject labels,particularly in the MRD.
We included many subjectlabels, usage labels, and register labels in ourWordNet conversion, but these did not surface in ourdisambiguation with WordNet.
They were very rarefor the lexical items used in SENSEVAL.
The valueshown here is similar to the results obtained byMagnini, et al (2001), but their low recall suggeststhat for more common words, there will be a loweropportunity for their use.The word form of a lexical item also emerged asbeing of some significance when disambiguating withNODE, slightly over 16 percent.
In NODE, this iscaptured by such labels as ?often capitalized?
or?often in plural form?.
No comparable information isavailable in WordNet.Subcategorization patterns (indicated under the?with?
column) were very important in bothWordNet (based on the verb frames) and NODE,relevant in 55% and 32% of the sense selections,respectively.
As indicated, the ?with?
category is alsoimportant for nouns.
For the most part, this indicatesthat a given noun sense is usually accompanied by anoun modifier (e.g., ?metal fatigue?
).The ?as?
column corresponds to nouns used asmodifiers, verbs used as adjectives, and adjectivesused as nouns.
These were fairly important for nouns(7.7%) and verbs (10.3%).The final column, ?prefs?, corresponds toselectional preferences for verb subjects and objectsand adjective modificands.
In these cases, a matchoccurred when the head noun in these positions eithermatched literally or was a synonym or within twosynsets in the WordNet hierarchy.
Although theresults were relatively small, this demonstrates theviability of using such preferences.Finally, anomalous entries in the table (e.g.,nouns having subcategorization patterns used in thesense selection) generally correspond to our parserincorrectly assigning a part of speech (i.e., treatingthe noun as a verb sense).4.2 Variation in Disambiguation FeaturesSpace precludes showing the variation in features bylexical item.
The attributes in NODE for individualitems varies considerably and the differences werereflected in which features emerged as important.For adjectives, idiomatic usages were significantfor free, green, and natural.
Topics were importantfor fine, free, green, local, natural, oblique, andsimple, indicating that many senses of these wordshave specialized meanings.
Form was important forblind, arising from the collocation ?the blind?.
Thedefault sense was most prominent for colorless,graceful (with only one sense in NODE), andsolemn.
Context was important for blind, cool, fine,free, green, local, natural, oblique, and simple,suggesting that these words participate in commonexpressions that can be captured well in a few choiceexamples.
Selectional preferences on the modificandswere useful in several instances.For nouns, idioms were important for art, bar,channel, church, circuit, and post.
Clues (i.e., strongcollocations) were important for art, bar, chair, grip,post, and sense.
Topics were important for bar,channel, church, circuit, day, detention, mouth,nation, post, spade, stress, and yew (even thoughyew had only one sense in NODE).
Context wasimportant for art, authority, bar, chair, channel,child, church, circuit, day, detention, facility,fatigue, feeling, grip, hearth, lady, material, mouth,nature, post, and restraint.
The presence ofindividual lexical items in several of these groupingsshows the richness of variations in characteristics,particularly into specialized usages and collocations.For verbs, idioms were important for call, carry,draw, dress, live, play, pull, turn, wash, and work, areflection of the many entries where these words werepaired with a particle.
Form was an important featurefor begin (over 50% of the instances), develop, face,find, leave, match, replace, treat, and work.Subcategorization patterns were important for all theverbs.
However, many verb senses in both WordNetand NODE do not show wide variation in theirsubcategorization patterns and are insufficient inthemselves to distinguish senses.
Strong (?kind?)
andweak (?clue?)
collocations are relatively lessimportant, except for a few verbs (collaborate, serve,and work).
Topics are surprisingly significant forseveral verbs (call, carry, develop, dress, drive, find,play, pull, serve, strike, and train), indicating thepresence of specialized senses.
Context does not varysignificantly among the set of verbs, but it is afeature in one-third of the sense selections.
Finally,selectional preferences on verb subjects and objectsemerged as having some value.5 Generalizability of Feature Analysis,Relation to Supervised Learning, andImplications for Future StudiesThe use of feature analysis has advanced ourperception of the disambiguation process.
To beginwith, by summarizing the features used in the senseselection, the technique identifies overall differencesbetween sense inventories.
While our comments havefocused on information available in NODE, theyreflect only what we have implemented.
Manyopportunities still exist and the results will help usidentify them.In developing our feature analysis techniques, wemade lists of features available for the senses of agiven word.
This gradually gave rise to the notion ofa ?feature signature?
associated with each sense.
Inexamining the set of definitions for each lexical item,an immediate question is how the feature signaturesdiffer from one another.
This allows us to focus onthe issue of adequate sense distinctions: what is itthat distinguishes each sense.The notion of feature signatures also raises thequestion of their correspondence to supervisedlearning techniques such as the feature selection of(Mihalcea & Moldovan, 2001) and the decision listsused in WASPS (Tugwell & Kilgarriff 2001).
Thisraises the possibility of precompiling a senseinventory and revising our disambiguation strategy toidentify the characteristics of an instance?s use andthen simply to perform a boolean conjunction tonarrow the set of viable senses.The use of feature signatures also allows us toexamine our mapping functionality.
As indicatedabove, we are unable to map 10 percent of the sensesfrom NODE to WordNet, and of our mappings,approximately 33 percent have appeared to beinaccurate when examined by hand.
When weexamine the instances where we selected a sense inNODE, but were unable to map to a WordNet sense,we can use these instances either to identify clearcases where there is no WordNet sense.In connection with the use of supervised learningtechniques, participants of other teams have providedus with the raw data with which their systems madetheir sense selections.
The feature arrays from(Mihalcea & Moldovan, forthcoming) identify manyfeatures in common with our set.
For example, theyused the form and part of speech of the target word;this corresponds to our ?form?.
Their collocations,prepositions after the target word, nouns before andafter, and prepositions before and after correspond toour idioms, ?clues?, and ?with?
features.The array of grammatical relations used withWASPS (Tugwell & Kilgarriff,  2001)  (such asbare-noun, plural, passive, ing-complement, noun-modifier, PP-comp) correspond to our ?form?,?clue?, ?with?, and ?as?
features.The data from these teams also identifies bigramsand other context information.
Pedersen (2001) alsoprovided us with the output of several classificationmethods, identifying unigrams and bigrams found tobe significant in sense selection.
These datacorrespond to our ?context?
feature.We have begun to array all these data by sense,corresponding to our detailed feature analysis.
Ourinitial qualitative assessment is that there are strongcorrespondences among the different data set.
Wewill examine these quantitatively to assess thesignificance of the various features.
In addition, whileseveral features are already present in WordNet andNODE, we fully expect that these other results willhelp us to identify features that can be added to theNODE sense inventory.6 ConclusionsOur analysis has identified many characteristics ofsense distinctions, but indicates many difficulties inmaking such distinctions in WordNet (but alsoNODE).
It is questionable whether WSD has beenfully tested without a carefully drawn senseinventory.
A lexicographically-based sense inventoryshows considerable promise and invites the WSDcommunity to pool its resources to come up withsuch an inventory.AcknowledgmentsI wish to thank Oxford University Press for allowingme to use their data, and particularly to Rob Scriven,Judy Pearsall, Glynnis Chantrell, Patrick Hanks,Catherine Soanes, Angus Stevenson, AdamKilgarriff, and James McCracken for their invaluablediscussions, to Rada Mihalcea, Ted Pedersen, andDavid Tugwell for making their data available, andto the anonymous reviewers.ReferencesDelfs, L. (2001, 6 Sep).
Verb keys.
(Personal communication)Haynes, S. (2001, July).
Semantic Tagging Using WordNetExamples.
In Association for Computational LinguisticsSIGLEX Workshop (pp.
79-82).
Toulouse, France.Litkowski, K. C. (2002).
SENSEVAL Word-SenseDisambiguation Using a Different Sense Inventory andMapping to WordNet (CL Research No.
02-01).
.Damascus, MD.Litkowski, K. C. (2001, July).
Use of Machine ReadableDictionaries for Word-Sense in SENSEVAL-2.
InAssociation for Computational Linguistics SIGLEXWorkshop (pp.
107-110).
Toulouse, France.Litkowski, K. C. (1999, June).
Towards a Meaning-FullComparison of Lexical Resources.
In Association forComputational Linguistics SIGLEX Workshop (pp.
30-7).College Park, MD.Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A.
(2001, July).
Using Domain Information for Word SenseDisambiguation.
In Association for ComputationalLinguistics SIGLEX Workshop (pp.
111-4).
Toulouse,France.Mihalcea, R., & Moldovan, D. (2001, July).
Pattern Learningand Active Feature Selection for Word SenseDisambiguation.
In Association for ComputationalLinguistics SIGLEX Workshop (pp.
127-30).
Toulouse,France.Mihalcea, R., & Moldovan, D. (Forthcoming).
Word SenseDisambiguation with Pattern Learning and Active FeatureSelection.
Journal of Natural Language Engineering.The New Oxford Dictionary of English.
(1998) (J.
Pearsall,Ed.).
Oxford: Clarendon Press.Pedersen, T. (2001, July).
Machine Learning with LexicalFeatures: The Duluth Approach to SENSEVAL-2.
InAssociation for Computational Linguistics SIGLEXWorkshop (pp.
139-142).
Toulouse, France.Tugwell, D., & Kilgarriff, A.
(2001, July).
WASP-Bench: ALexicographic Tool Supporting Word SenseDisambiguation.
In Association for ComputationalLinguistics SIGLEX Workshop (pp.
151-4).
Toulouse,France.
