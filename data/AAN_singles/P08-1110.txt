Proceedings of ACL-08: HLT, pages 968?976,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Deductive Approach to Dependency Parsing?Carlos Go?mez-Rodr?
?guezDepartamento de Computacio?nUniversidade da Corun?a, Spaincgomezr@udc.esJohn Carroll and David WeirDepartment of InformaticsUniversity of Sussex, United Kingdom{johnca,davidw}@sussex.ac.ukAbstractWe define a new formalism, based on Sikkel?sparsing schemata for constituency parsers,that can be used to describe, analyze and com-pare dependency parsing algorithms.
Thisabstraction allows us to establish clear rela-tions between several existing projective de-pendency parsers and prove their correctness.1 IntroductionDependency parsing consists of finding the structureof a sentence as expressed by a set of directed links(dependencies) between words.
This is an alterna-tive to constituency parsing, which tries to find a di-vision of the sentence into segments (constituents)which are then broken up into smaller constituents.Dependency structures directly show head-modifierand head-complement relationships which form thebasis of predicate argument structure, but are notrepresented explicitly in constituency trees, whileproviding a representation in which no non-lexicalnodes have to be postulated by the parser.
In addi-tion to this, some dependency parsers are able to rep-resent non-projective structures, which is an impor-tant feature when parsing free word order languagesin which discontinuous constituents are common.The formalism of parsing schemata (Sikkel, 1997)is a useful tool for the study of constituency parserssince it provides formal, high-level descriptionsof parsing algorithms that can be used to provetheir formal properties (such as correctness), es-tablish relations between them, derive new parsersfrom existing ones and obtain efficient implementa-tions automatically (Go?mez-Rodr?
?guez et al, 2007).The formalism was initially defined for context-freegrammars and later applied to other constituency-based formalisms, such as tree-adjoining grammars?Partially supported by Ministerio de Educacio?n y Cienciaand FEDER (TIN2004-07246-C03, HUM2007-66607-C04),Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC-10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc.
daLinguaxe e RI) and Programa de Becas FPU.
(Alonso et al, 1999).
However, since parsingschemata are defined as deduction systems over setsof constituency trees, they cannot be used to de-scribe dependency parsers.In this paper, we define an analogous formalismthat can be used to define, analyze and compare de-pendency parsers.
We use this framework to provideuniform, high-level descriptions for a wide range ofwell-known algorithms described in the literature,and we show how they formally relate to each otherand how we can use these relations and the formal-ism itself to prove their correctness.1.1 Parsing schemataParsing schemata (Sikkel, 1997) provide a formal,simple and uniform way to describe, analyze andcompare different constituency-based parsers.The notion of a parsing schema comes from con-sidering parsing as a deduction process which gener-ates intermediate results called items.
An initial setof items is directly obtained from the input sentence,and the parsing process consists of the application ofinference rules (deduction steps) which produce newitems from existing ones.
Each item contains a pieceof information about the sentence?s structure, and asuccessful parsing process will produce at least onefinal item containing a full parse tree for the sentenceor guaranteeing its existence.Items in parsing schemata are formally definedas sets of partial parse trees from a set denotedTrees(G), which is the set of all the possible par-tial parse trees that do not violate the constraints im-posed by a grammar G. More formally, an item setI is defined by Sikkel as a quotient set associatedwith an equivalence relation on Trees(G).1Valid parses for a string are represented byitems containing complete marked parse trees forthat string.
Given a context-free grammar G =1While Shieber et al (1995) also view parsers as deductionsystems, Sikkel formally defines items and related concepts,providing the mathematical tools to reason about formal prop-erties of parsers.968(N,?, P, S), a marked parse tree for a stringw1 .
.
.
wn is any tree ?
?
Trees(G)/root(?)
=S?yield(?)
= w1 .
.
.
wn2.
An item containing sucha tree for some arbitrary string is called a final item.An item containing such a tree for a particular stringw1 .
.
.
wn is called a correct final item for that string.For each input string, a parsing schema?s deduc-tion steps allow us to infer a set of items, called validitems for that string.
A parsing schema is said tobe sound if all valid final items it produces for anyarbitrary string are correct for that string.
A pars-ing schema is said to be complete if all correct fi-nal items are valid.
A correct parsing schema is onewhich is both sound and complete.
A correct parsingschema can be used to obtain a working implemen-tation of a parser by using deductive engines suchas the ones described by Shieber et al (1995) andGo?mez-Rodr?
?guez et al (2007) to obtain all valid fi-nal items.2 Dependency parsing schemataAlthough parsing schemata were initially defined forcontext-free parsers, they can be adapted to differentconstituency-based grammar formalisms, by findinga suitable definition of Trees(G) for each particularformalism and a way to define deduction steps fromits rules.
However, parsing schemata are not directlyapplicable to dependency parsing, since their formalframework is based on constituency trees.In spite of this problem, many of the dependencyparsers described in the literature are constructive,in the sense that they proceed by combining smallerstructures to form larger ones until they find a com-plete parse for the input sentence.
Therefore, itis possible to define a variant of parsing schemata,where these structures can be defined as items andthe strategies used for combining them can be ex-pressed as inference rules.
However, in order to de-fine such a formalism we have to tackle some issuesspecific to dependency parsers:?
Traditional parsing schemata are used to de-fine grammar-based parsers, in which the parsingprocess is guided by some set of rules which areused to license deduction steps: for example, anEarley Predictor step is tied to a particular gram-mar rule, and can only be executed if such a ruleexists.
Some dependency parsers are also grammar-2wi is shorthand for the marked terminal (wi, i).
These areused by Sikkel (1997) to link terminal symbols to string posi-tions so that an input sentence can be represented as a set oftrees which are used as initial items (hypotheses) for the de-duction system.
Thus, a sentence w1 .
.
.
wn produces a set ofhypotheses {{w1(w1)}, .
.
.
, {wn(wn)}}.Figure 1: Representation of a dependency structure witha tree.
The arrows below the words correspond to its as-sociated dependency graph.based: for example, those described by Lombardoand Lesmo (1996), Barbero et al (1998) and Ka-hane et al (1998) are tied to the formalizations of de-pendency grammar using context-free like rules de-scribed by Hays (1964) and Gaifman (1965).
How-ever, many of the most widely used algorithms (Eis-ner, 1996; Yamada and Matsumoto, 2003) do not usea formal grammar at all.
In these, decisions aboutwhich dependencies to create are taken individually,using probabilistic models (Eisner, 1996) or classi-fiers (Yamada and Matsumoto, 2003).
To representthese algorithms as deduction systems, we use thenotion of D-rules (Covington, 1990).
D-rules takethe form a ?
b, which says that word b can have aas a dependent.
Deduction steps in non-grammar-based parsers can be tied to the D-rules associatedwith the links they create.
In this way, we obtaina representation of the semantics of these parsingstrategies that is independent of the particular modelused to take the decisions associated with each D-rule.?
The fundamental structures in dependency pars-ing are dependency graphs.
Therefore, as itemsfor constituency parsers are defined as sets of par-tial constituency trees, it is tempting to define itemsfor dependency parsers as sets of partial dependencygraphs.
However, predictive grammar-based algo-rithms such as those of Lombardo and Lesmo (1996)and Kahane et al (1998) have operations which pos-tulate rules and cannot be defined in terms of depen-dency graphs, since they do not do any modificationsto the graph.
In order to make the formalism generalenough to include these parsers, we define items interms of sets of partial dependency trees as shown inFigure 1.
Note that a dependency graph can alwaysbe extracted from such a tree.?
Some of the most popular dependency parsingalgorithms, like that of Eisner (1996), work by con-necting spans which can represent disconnected de-pendency graphs.
Such spans cannot be representedby a single dependency tree.
Therefore, our formal-ism allows items to be sets of forests of partial de-pendency trees, instead of sets of trees.969Taking these considerations into account, we de-fine the concepts that we need to describe item setsfor dependency parsers:Let ?
be an alphabet of terminal symbols.Partial dependency trees: We define the set ofpartial dependency trees (D-trees) as the set of finitetrees where children of each node have a left-to-rightordering, each node is labelled with an element of??(?
?N), and the following conditions hold:?
All nodes labelled with marked terminals wi ?(?
?N) are leaves,?
Nodes labelled with terminals w ?
?
do not havemore than one daughter labelled with a markedterminal, and if they have such a daughter node, itis labelled wi for some i ?
N,?
Left siblings of nodes labelled with a marked ter-minal wk do not have any daughter labelled wjwith j ?
k. Right siblings of nodes labelled witha marked terminal wk do not have any daughterlabelled wj with j ?
k.We denote the root node of a partial dependencytree t as root(t).
If root(t) has a daughter node la-belled with a marked terminal wh, we will say thatwh is the head of the tree t, denoted by head(t).
Ifall nodes labelled with terminals in t have a daughterlabelled with a marked terminal, t is grounded.Relationship between trees and graphs: Lett ?
D-trees be a partial dependency tree; g(t), itsassociated dependency graph, is a graph (V,E)?
V ={wi ?
(?
?N) | wi is the label of a node int},?
E ={(wi, wj) ?
(?
?N)2 | C,D are nodes in tsuch that D is a daughter of C, wj the label of adaughter of C, wi the label of a daughter of D}.Projectivity: A partial dependency tree t ?D-trees is projective iff yield(t) cannot be writtenas .
.
.
wi .
.
.
wj .
.
.
where i ?
j.It is easy to verify that the dependency graphg(t) is projective with respect to the linear order ofmarked terminals wi, according to the usual defi-nition of projectivity found in the literature (Nivre,2006), if and only if the tree t is projective.Parse tree: A partial dependency tree t ?D-trees is a parse tree for a given string w1 .
.
.
wnif its yield is a permutation of w1 .
.
.
wn.
If its yieldis exactly w1 .
.
.
wn, we will say it is a projectiveparse tree for the string.Item set: Let ?
?
D-trees be the set of de-pendency trees which are acceptable according to agiven grammar G (which may be a grammar of D-rules or of CFG-like rules, as explained above).
Wedefine an item set for dependency parsing as a setI ?
?, where ?
is a partition of 2?.Once we have this definition of an item set fordependency parsing, the remaining definitions areanalogous to those in Sikkel?s theory of constituencyparsing (Sikkel, 1997), so we will not include themhere in full detail.
A dependency parsing system isa deduction system (I, H,D) where I is a depen-dency item set as defined above, H is a set contain-ing initial items or hypotheses, and D ?
(2(H?I) ?I) is a set of deduction steps defining an inferencerelation `.Final items in this formalism will be those con-taining some forest F containing a parse tree forsome arbitrary string.
An item containing such a treefor a particular string w1 .
.
.
wn will be called a cor-rect final item for that string in the case of nonprojec-tive parsers.
When defining projective parsers, cor-rect final items will be those containing projectiveparse trees for w1 .
.
.
wn.
This distinction is relevantbecause the concepts of soundness and correctnessof parsing schemata are based on correct final items(cf.
section 1.1), and we expect correct projectiveparsers to produce only projective structures, whilenonprojective parsers should find all possible struc-tures including nonprojective ones.3 Some practical examples3.1 Col96 (Collins, 96)One of the most straightforward projective depen-dency parsing strategies is the one described byCollins (1996), directly based on the CYK pars-ing algorithm.
This parser works with dependencytrees which are linked to each other by creatinglinks between their heads.
Its item set is defined asICol96 = {[i, j, h] | 1 ?
i ?
h ?
j ?
n}, where anitem [i, j, h] is defined as the set of forests containinga single projective dependency tree t such that t isgrounded, yield(t) = wi .
.
.
wj and head(t) = wh.For an input string w1 .
.
.
wn, the set of hypothe-ses is H = {[i, i, i] | 0 ?
i ?
n + 1}, i.e., the setof forests containing a single dependency tree of theform wi(wi).
This same set of hypotheses can beused for all the parsers, so we will not make it ex-plicit for subsequent schemata.3The set of final items is {[1, n, h] | 1 ?
h ?
n}:these items trivially represent parse trees for the in-put sentence, where wh is the sentence?s head.
Thededuction steps are shown in Figure 2.3Note that the words w0 and wn+1 used in the definition donot appear in the input: these are dummy terminals that we willcall beginning of sentence (BOS) and end of sentence (EOS)marker, respectively; and will be needed by some parsers.970Col96 (Collins,96):R-Link[i, j, h1][j + 1, k, h2][i, k, h2]wh1 ?
wh2L-Link[i, j, h1][j + 1, k, h2][i, k, h1]wh2 ?
wh1Eis96 (Eisner, 96):Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1, F, F ]R-Link [i, j, F, F ][i, j, T, F ] wi ?
wjL-Link [i, j, F, F ][i, j, F, T ] wj ?
wiCombineSpans[i, j, b, c][j, k, not(c), d][i, k, b, d]ES99 (Eisner and Satta, 99):R-Link [i, j, i] [j + 1, k, k][i, k, k] wi ?
wkL-Link [i, j, i] [j + 1, k, k][i, k, i] wk ?
wiR-Combiner [i, j, i] [j, k, j][i, k, i]L-Combiner [i, j, j] [j, k, k][i, k, k]YM03 (Yamada and Matsumoto, 2003):Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1]R-Link[i, j][j, k][i, k] wj ?
wk L-Link[i, j][j, k][i, k] wj ?
wiLL96 (Lombardo and Lesmo, 96):Initter [(.S), 1, 0] ?
(S)?P Predictor[A(?.B?
), i, j][B(.?
), j + 1, j] B(?
)?PScanner [A(?.
?
?
), i, h?
1] [h, h, h][A(?
?
.?
), i, h] wh IS ACompleter [A(?.B?
), i, j] [B(?.
), j + 1, k][A(?B.?
), i, k]Figure 2: Deduction steps of the parsing schemata for some well-known dependency parsers.As we can see, we use D-rules as side conditionsfor deduction steps, since this parsing strategy is notgrammar-based.
Conceptually, the schema we havejust defined describes a recogniser: given a set of D-rules and an input string wi .
.
.
wn, the sentence canbe parsed (projectively) under those D-rules if andonly if this deduction system can infer a correct finalitem.
However, when executing this schema with adeductive engine, we can recover the parse forest byfollowing back pointers in the same way as is donewith constituency parsers (Billot and Lang, 1989).Of course, boolean D-rules are of limited interestin practice.
However, this schema provides a formal-ization of a parsing strategy which is independentof the way linking decisions are taken in a partic-ular implementation.
In practice, statistical modelscan be used to decide whether a step linking wordsa and b (i.e., having a ?
b as a side condition) isexecuted or not, and probabilities can be attached toitems in order to assign different weights to differentanalyses of the sentence.
The same principle appliesto the rest of D-rule-based parsers described in thispaper.3.2 Eis96 (Eisner, 96)By counting the number of free variables used ineach deduction step of Collins?
parser, we can con-clude that it has a time complexity of O(n5).
Thiscomplexity arises from the fact that a parentlessword (head) may appear in any position in the par-tial results generated by the parser; the complexitycan be reduced to O(n3) by ensuring that parentlesswords can only appear at the first or last positionof an item.
This is the principle behind the parserdefined by Eisner (1996), which is still in wide usetoday (Corston-Oliver et al, 2006; McDonald et al,2005a).The item set for Eisner?s parsing schema isIEis96 = {[i, j, T, F ] | 0 ?
i ?
j ?
n} ?
{[i, j, F, T ] | 0 ?
i ?
j ?
n} ?
{[i, j, F, F ] |0 ?
i ?
j ?
n}, where each item [i, j, T, F ] is de-fined as the item [i, j, j] ?
ICol96, each item[i, j, F, T ] is defined as the item [i, j, i] ?
ICol96,and each item [i, j, F, F ] is defined as the setof forests of the form {t1, t2} such that t1 andt2 are grounded, head(t1) = wi, head(t2) = wj ,and ?k ?
N(i ?
k < j)/yield(t1) = wi .
.
.
wk ?yield(t2) = wk+1 .
.
.
wj .Note that the flags b, c in an item [i, j, b, c] indi-cate whether the words in positions i and j, respec-tively, have a parent in the item or not.
Items withone of the flags set to T represent dependency treeswhere the word in position i or j is the head, whileitems with both flags set to F represent pairs of treesheaded at positions i and j, and therefore correspondto disconnected dependency graphs.Deduction steps4 are shown in Figure 2.
Theset of final items is {[0, n, F, T ]}.
Note that theseitems represent dependency trees rooted at the BOSmarker w0, which acts as a ?dummy head?
for thesentence.
In order for the algorithm to parse sen-tences correctly, we will need to define D-rules toallow w0 to be linked to the real sentence head.3.3 ES99 (Eisner and Satta, 99)Eisner and Satta (1999) define an O(n3) parser forsplit head automaton grammars that can be used4Alternatively, we could consider items of the form [i, i +1, F, F ] to be hypotheses for this parsing schema, so we wouldnot need an Initter step.
However, we have chosen to use a stan-dard set of hypotheses valid for all parsers because this allowsfor more straightforward proofs of relations between schemata.971for dependency parsing.
This algorithm is con-ceptually simpler than Eis96, since it only usesitems representing single dependency trees, avoid-ing items of the form [i, j, F, F ].
Its item set isIES99 = {[i, j, i] | 0 ?
i ?
j ?
n} ?
{[i, j, j] |0 ?
i ?
j ?
n}, where items are defined as inCollins?
parsing schema.Deduction steps are shown in Figure 2, and the setof final items is {[0, n, 0]}.
(Parse trees have w0 astheir head, as in the previous algorithm).Note that, when described for head automatongrammars as in Eisner and Satta (1999), this algo-rithm seems more complex to understand and imple-ment than the previous one, as it requires four differ-ent kinds of items in order to keep track of the stateof the automata used by the grammars.
However,this abstract representation of its underlying seman-tics as a dependency parsing schema shows that thisparsing strategy is in fact conceptually simpler fordependency parsing.3.4 YM03 (Yamada and Matsumoto, 2003)Yamada and Matsumoto (2003) define a determinis-tic, shift-reduce dependency parser guided by sup-port vector machines, which achieves over 90% de-pendency accuracy on section 23 of the Penn tree-bank.
Parsing schemata are not suitable for directlydescribing deterministic parsers, since they work ata high abstraction level where a set of operationsare defined without imposing order constraints onthem.
However, many deterministic parsers can beviewed as particular optimisations of more general,nondeterministic algorithms.
In this case, if we rep-resent the actions of the parser as deduction stepswhile abstracting from the deterministic implemen-tation details, we obtain an interesting nondetermin-istic parser.Actions in Yamada and Matsumoto?s parser createlinks between two target nodes, which act as headsof neighbouring dependency trees.
One of the ac-tions creates a link where the left target node be-comes a child of the right one, and the head of atree located directly to the left of the target nodesbecomes the new left target node.
The other ac-tion is symmetric, performing the same operationwith a right-to-left link.
An O(n3) nondetermin-istic parser generalising this behaviour can be de-fined by using an item set IY M03 = {[i, j] |0 ?
i ?
j ?
n + 1}, where each item [i, j] is de-fined as the item [i, j, F, F ] in IEis96; and the de-duction steps are shown in Figure 2.The set of final items is {[0, n + 1]}.
In order forthis set to be well-defined, the grammar must haveno D-rules of the form wi ?
wn+1, i.e., it must notallow the EOS marker to govern any words.
If thisis the case, it is trivial to see that every forest in anitem of the form [0, n + 1] must contain a parse treerooted at the BOS marker and with yield w0 .
.
.
wn.As can be seen from the schema, this algorithmrequires less bookkeeping than any other of theparsers described here.3.5 LL96 (Lombardo and Lesmo, 96) andother Earley-based parsersThe algorithms in the above examples are based ontaking individual decisions about dependency links,represented by D-rules.
Other parsers, such as thatof Lombardo and Lesmo (1996), use grammars withcontext-free like rules which encode the preferredorder of dependents for each given governor, as de-fined by Gaifman (1965).
For example, a rule of theform N(Det ?
PP ) is used to allow N to have Detas left dependent and PP as right dependent.The algorithm by Lombardo and Lesmo (1996)is a version of Earley?s context-free grammar parser(Earley, 1970) using Gaifman?s dependency gram-mar, and can be written by using an item setILomLes = {[A(?.?
), i, j] | A(??)
?
P ?1 ?
i ?
j ?
n}, where each item [A(?.?
), i, j] rep-resents the set of partial dependency trees rooted atA, where the direct children of A are ?
?, and thesubtrees rooted at ?
have yield wi .
.
.
wj .
The de-duction steps for the schema are shown in Figure 2,and the final item set is {[(S.), 1, n]}.As we can see, the schema for Lombardo andLesmo?s parser resembles the Earley-style parser inSikkel (1997), with some changes to adapt it to de-pendency grammar (for example, the Scanner al-ways moves the dot over the head symbol ?
).Analogously, other dependency parsing schematabased on CFG-like rules can be obtained by mod-ifying context-free grammar parsing schemata ofSikkel (1997) in a similar way.
The algorithm byBarbero et al (1998) can be obtained from the left-corner parser, and the one by Courtin and Genthial(1998) is a variant of the head-corner parser.3.6 Pseudo-projectivityPseudo-projective parsers can generate non-projective analyses in polynomial time by usinga projective parsing strategy and postprocessingthe results to establish nonprojective links.
Forexample, the algorithm by Kahane et al (1998) usesa projective parsing strategy like that of LL96, butusing the following initializer step instead of the972Initter and Predictor:5Initter [A(?
), i, i ?
1] A(?)
?
P ?
1 ?
i ?
n4 Relations between dependency parsersThe framework of parsing schemata can be used toestablish relationships between different parsing al-gorithms and to obtain new algorithms from existingones, or derive formal properties of a parser (such assoundness or correctness) from the properties of re-lated algorithms.Sikkel (1994) defines several kinds of relationsbetween schemata, which fall into two categories:generalisation relations, which are used to obtainmore fine-grained versions of parsers, and filteringrelations, which can be seen as the reverse of gener-alisation and are used to reduce the number of itemsand/or steps needed for parsing.
He gives a formaldefinition of each kind of relation.
Informally, aparsing schema can be generalised from another viathe following transformations:?
Item refinement: We say that P1 ir??
P2 (P2 is anitem refinement of P1) if there is a mapping be-tween items in both parsers such that single itemsin P1 are broken into multiple items in P2 and in-dividual deductions are preserved.?
Step refinement: We say that P1 sr??
P2 if theitem set of P1 is a subset of that of P2 and everysingle deduction step in P1 can be emulated by asequence of inferences in P2.On the other hand, a schema can be obtained fromanother by filtering in the following ways:?
Static/dynamic filtering: P1sf/df????
P2 if the itemset of P2 is a subset of that of P1 and P2 allows asubset of the direct inferences in P16.?
Item contraction: The inverse of item refinement.P1 ic??
P2 if P2 ir??
P1.?
Step contraction: The inverse of step refinement.P1 sc??
P2 if P2 sr??
P1.All the parsers described in section 3 can be re-lated via generalisation and filtering, as shown inFigure 3.
For space reasons we cannot show formalproofs of all the relations, but we sketch the proofsfor some of the more interesting cases:5The initialization step as reported in Kahane?s paper is dif-ferent from this one, as it directly consumes a nonterminal fromthe input.
However, using this step results in an incompletealgorithm.
The problem can be fixed either by using the stepshown here instead (bottom-up Earley strategy) or by adding anadditional step turning it into a bottom-up Left-Corner parser.6Refer to Sikkel (1994) for the distinction between static anddynamic filtering, which we will not use here.4.1 YM03 sr??
Eis96It is easy to see from the schema definitions thatIY M03 ?
IEis96.
In order to prove the relationbetween these parsers, we need to verify that everydeduction step in YM03 can be emulated by a se-quence of inferences in Eis96.
In the case of theInitter step this is trivial, since the Initters of bothparsers are equivalent.
If we write the R-Link step inthe notation we have used for Eisner items, we haveR-Link [i, j, F, F ] [j, k, F, F ][i, k, F, F ] wj ?
wkThis can be emulated in Eisner?s parser by anR-Link step followed by a CombineSpans step:[j, k, F, F ] ` [j, k, T, F ] (by R-Link),[j, k, T, F ], [i, j, F, F ] ` [i, k, F, F ] (by CombineSpans).Symmetrically, the L-Link step in YM03 can beemulated by an L-Link followed by a CombineSpansin Eis96.4.2 ES99 sr??
Eis96If we write the R-Link step in Eisner and Satta?sparser in the notation for Eisner items, we haveR-Link [i, j, F, T ] [j + 1, k, T, F ][i, k, T, F ] wi ?
wkThis inference can be emulated in Eisner?s parseras follows:` [j, j + 1, F, F ] (by Initter),[i, j, F, T ], [j, j + 1, F, F ] ` [i, j + 1, F, F ] (CombineSpans),[i, j + 1, F, F ], [j + 1, k, T, F ] ` [i, k, F, F ] (CombineSpans),[i, k, F, F ] ` [i, k, T, F ] (by R-Link).The proof corresponding to the L-Link step is sym-metric.
As for the R-Combiner and L-Combinersteps in ES99, it is easy to see that they are partic-ular cases of the CombineSpans step in Eis96, andtherefore can be emulated by a single application ofCombineSpans.Note that, in practice, the relations in sections 4.1and 4.2 mean that the ES99 and YM03 parsers aresuperior to Eis96, since they generate fewer itemsand need fewer steps to perform the same deduc-tions.
These two parsers also have the interestingproperty that they use disjoint item sets (one usesitems representing trees while the other uses itemsrepresenting pairs of trees); and the union of thesedisjoint sets is the item set used by Eis96.
Also notethat the optimisation in YM03 comes from contract-ing deductions in Eis96 so that linking operationsare immediately followed by combining operations;while ES99 does the opposite, forcing combiningoperations to be followed by linking operations.4.3 Other relationsIf we generalise the linking steps in ES99 so that thehead of each item can be in any position, we obtain a973Figure 3: Formal relations between several well-known dependency parsers.
Arrows going upwards correspond togeneralisation relations, while those going downwards correspond to filtering.
The specific subtype of relation isshown in each arrow?s label, following the notation in Section 4.correct O(n5) parser which can be filtered to Col96just by eliminating the Combiner steps.From Col96, we can obtain an O(n5) head-cornerparser based on CFG-like rules by an item refine-ment in which each Collins item [i, j, h] is split intoa set of items [A(?.?.?
), i, j, h].
Of course, the for-mal refinement relation between these parsers onlyholds if the D-rules used for Collins?
parser corre-spond to the CFG rules used for the head-cornerparser: for every D-rule B ?
A there must be acorresponding CFG-like rule A ?
.
.
.
B .
.
.
in thegrammar used by the head-corner parser.Although this parser uses three indices i, j, h, us-ing CFG-like rules to guide linking decisions makesthe h indices unnecessary, so they can be removed.This simplification is an item contraction which re-sults in an O(n3) head-corner parser.
From here,we can follow the procedure in Sikkel (1994) torelate this head-corner algorithm to parsers analo-gous to other algorithms for context-free grammars.In this way, we can refine the head-corner parserto a variant of de Vreught and Honig?s algorithm(Sikkel, 1997), and by successive filters we reach aleft-corner parser which is equivalent to the one de-scribed by Barbero et al (1998), and a step contrac-tion of the Earley-based dependency parser LL96.The proofs for these relations are the same as thosedescribed in Sikkel (1994), except that the depen-dency variants of each algorithm are simpler (dueto the absence of epsilon rules and the fact that therules are lexicalised).5 Proving correctnessAnother useful feature of the parsing schemataframework is that it provides a formal way to de-fine the correctness of a parser (see last paragraphof Section 1.1) which we can use to prove that ourparsers are correct.
Furthermore, relations betweenschemata can be used to derive the correctness ofa schema from that of related ones.
In this sec-tion, we will show how we can prove that the YM03and ES99 algorithms are correct, and use that fact toprove the correctness of Eis96.5.1 ES99 is correctIn order to prove the correctness of a parser, we mustprove its soundness and completeness (see section1.1).
Soundness is generally trivial to verify, sincewe only need to check that every individual deduc-tion step in the parser infers a correct consequentitem when applied to correct antecedents (i.e., in thiscase, that steps always generate non-empty itemsthat conform to the definition in 3.3).
The difficultyis proving completeness, for which we need to provethat all correct final items are valid (i.e., can be in-ferred by the schema).
To show this, we will provethe stronger result that all correct items are valid.We will show this by strong induction on thelength of items, where the length of an item ?
=[i, k, h] is defined as length(?)
= k ?
i + 1.
Cor-rect items of length 1 are the hypotheses of theschema (of the form [i, i, i]) which are trivially valid.We will prove that, if all correct items of length mare valid for all 1 ?
m < l, then items of length lare also valid.Let [i, k, i] be an item of length l in IES99 (thus,l = k?
i+1).
If this item is correct, then it containsa grounded dependency tree t such that yield(t) =wi .
.
.
wk and head(t) = wi.By construction, the root of t is labelled wi.
Letwj be the rightmost daughter of wi in t. Since tis projective, we know that the yield of wj must beof the form wl .
.
.
wk, where i < l ?
j ?
k. Ifl < j, then wl is the leftmost transitive dependent ofwj in t, and if k > j, then we know that wk is therightmost transitive dependent of wj in t.Let tj be the subtree of t rooted at wj .
Let t1 bethe tree obtained from removing tj from t. Let t2 be974the tree obtained by removing all the children to theright of wj from tj , and t3 be the tree obtained by re-moving all the children to the left of wj from tj .
Byconstruction, t1 belongs to a correct item [i, l?
1, i],t2 belongs to a correct item [l, j, j] and t3 belongs toa correct item [j, k, j].
Since these three items havea length strictly less than l, by the inductive hypoth-esis, they are valid.
This allows us to prove that theitem [i, k, i] is also valid, since it can be obtainedfrom these valid items by the following inferences:[i, l ?
1, i], [l, j, j] ` [i, j, i] (by the L-Link step),[i, j, i], [j, k, j] ` [i, k, i] (by the L-Combiner step).This proves that all correct items of length l whichare of the form [i, k, i] are correct under the induc-tive hypothesis.
The same can be proved for items ofthe form [i, k, k] by symmetric reasoning, thus prov-ing that the ES99 parsing schema is correct.5.2 YM03 is correctIn order to prove correctness of this parser, we fol-low the same procedure as above.
Soundness isagain trivial to verify.
To prove completeness, weuse strong induction on the length of items, wherethe length of an item [i, j] is defined as j ?
i + 1.The induction step is proven by considering anycorrect item [i, k] of length l > 2 (l = 2 is the basecase here since items of length 2 are generated bythe Initter step) and proving that it can be inferredfrom valid antecedents of length less than l, so it isvalid.
To show this, we note that, if l > 2, eitherwi has at least a right dependent or wk has at least aleft dependent in the item.
Supposing that wi has aright dependent, if t1 and t2 are the trees rooted at wiand wk in a forest in [i, k], we call wj the rightmostdaughter of wi and consider the following trees:v = the subtree of t1 rooted at wj , u1 = the tree ob-tained by removing v from t1, u2 = the tree obtainedby removing all children to the right of wj from v,u3 = the tree obtained by removing all children tothe left of wj from v.We observe that the forest {u1, u2} belongs to thecorrect item [i, j], while {u3, t2} belongs to the cor-rect item [j, k].
From these two items, we can obtain[i, k] by using the L-Link step.
Symmetric reason-ing can be applied if wi has no right dependents butwk has at least a left dependent, and analogously tothe case of the previous parser, we conclude that theYM03 parsing schema is correct.5.3 Eis96 is correctBy using the previous proofs and the relationshipsbetween schemata that we explained earlier, it iseasy to prove that Eis96 is correct: soundness is,as always, straightforward, and completeness can beproven by using the properties of other algorithms.Since the set of final items in Eis96 and ES99 arethe same, and the former is a step refinement of thelatter, the completeness of ES99 directly implies thecompleteness of Eis96.Alternatively, we can use YM03 to prove the cor-rectness of Eis96 if we redefine the set of final itemsin the latter to be of the form [0, n+ 1, F, F ], whichare equally valid as final items since they alwayscontain parse trees.
This idea can be applied to trans-fer proofs of completeness across any refinement re-lation.6 ConclusionsWe have defined a variant of Sikkel?s parsingschemata formalism which allows us to representdependency parsing algorithms in a simple, declar-ative way7.
We have clarified relations betweenparsers which were originally described very differ-ently.
For example, while Eisner presented his algo-rithm as a dynamic programming algorithm whichcombines spans into larger spans, Yamada and Mat-sumoto?s works by sequentially executing parsingactions that move a focus point in the input one po-sition to the left or right, (possibly) creating a de-pendency link.
However, in the parsing schematafor these algorithms we can see (and formally prove)that they are related: one is a refinement of the other.Parsing schemata are also a formal tool that can beused to prove the correctness of parsing algorithms.The relationships between dependency parsers canbe exploited to derive properties of a parser fromthose of others, as we have seen in several examples.Although the examples in this paper are cen-tered in projective dependency parsing, the formal-ism does not require projectivity and can be used torepresent nonprojective algorithms as well8.
An in-teresting line for future work is to use relationshipsbetween schemata to find nonprojective parsers thatcan be derived from existing projective counterparts.7An alternative framework that formally describes some de-pendency parsers is that of transition systems (McDonald andNivre, 2007).
This model is based on parser configurations andtransitions, and has no clear relationship with the approach de-scribed here.8Note that spanning tree parsing algorithms based on edge-factored models, such as the one by McDonald et al (2005b)are not constructive in the sense outlined in Section 2, so theapproach described here does not directly apply to them.
How-ever, other nonprojective parsers such as (Attardi, 2006) followa constructive approach and can be analysed deductively.975ReferencesMiguel A. Alonso, Eric de la Clergerie, David Cabrero,and Manuel Vilares.
1999.
Tabular algorithms forTAG parsing.
In Proc.
of the Ninth Conference on Eu-ropean chapter of the Association for ComputationalLinguistics, pages 150?157, Bergen, Norway.
ACL.Giuseppe Attardi.
2006.
Experiments with a Multilan-guage Non-Projective Dependency Parser.
In Proc.
ofthe Tenth Conference on Natural Language Learning(CoNLL-X), pages 166?170, New York, USA.
ACL.Cristina Barbero, Leonardo Lesmo, Vincenzo Lombarlo,and Paola Merlo.
1998.
Integration of syntacticand lexical information in a hierarchical dependencygrammar.
In Proc.
of the Workshop on DependencyGrammars, pages 58?67, ACL-COLING, Montreal,Canada.Sylvie Billot and Bernard Lang.
1989.
The structure ofshared forest in ambiguous parsing.
In Proc.
of the27th Annual Meeting of the Association for Computa-tional Linguistics, pages 143?151, Vancouver, BritishColumbia, Canada, June.
ACL.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proc.
ofthe 34th annual meeting on Association for Compu-tational Linguistics, pages 184?191, Morristown, NJ,USA.
ACL.Simon Corston-Oliver, Anthony Aue, Kevin Duh, andEric Ringger.
2006.
Multilingual dependency pars-ing using Bayes Point Machines.
In Proc.
of the mainconference on Human Language Technology Confer-ence of the North American Chapter of the Associationof Computational Linguistics, pages 160?167, Morris-town, NJ, USA.
ACL.Jacques Courtin and Damien Genthial.
1998.
Parsingwith dependency relations and robust parsing.
In Proc.of the Workshop on Dependency Grammars, pages 88?94, ACL-COLING, Montreal, Canada.Michael A. Covington.
1990.
A dependency parser forvariable-word-order languages.
Technical Report AI-1990-01, Athens, GA.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proc.
of the 37th annual meet-ing of the Association for Computational Linguisticson Computational Linguistics, pages 457?464, Mor-ristown, NJ, USA.
ACL.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
of the16th International Conference on Computational Lin-guistics (COLING-96), pages 340?345, Copenhagen,August.Haim Gaifman.
1965.
Dependency systems and phrase-structure systems.
Information and Control, 8:304?337.Carlos Go?mez-Rodr?
?guez, Jesu?s Vilares, and Miguel A.Alonso.
2007.
Compiling declarative specificationsof parsing algorithms.
In Database and Expert Sys-tems Applications, volume 4653 of Lecture Notes inComputer Science, pages 529?538, Springer-Verlag.David Hays.
1964.
Dependency theory: a formalism andsome observations.
Language, 40:511?525.Sylvain Kahane, Alexis Nasr, and Owen Rambow.
1998.Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.
In COLING-ACL,pages 646?652.Vincenzo Lombardo and Leonardo Lesmo.
1996.
AnEarley-type recognizer for dependency grammar.
InProc.
of the 16th conference on Computational linguis-tics, pages 723?728, Morristown, NJ, USA.
ACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In ACL ?05: Proc.
of the 43rd Annual Meetingon Association for Computational Linguistics, pages91?98, Morristown, NJ, USA.
ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov and JanHajic?.
2005b.
Non-projective dependency parsing us-ing spanning tree algorithms.
In HLT ?05: Proc.
ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 523?530.
ACL.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the Errors of Data-Driven Dependency ParsingModels.
In Proc.
of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 122?131.Joakim Nivre.
2006.
Inductive Dependency Parsing(Text, Speech and Language Technology).
Springer-Verlag New York, Inc., Secaucus, NJ, USA.Stuart M. Shieber, Yves Schabes, and Fernando C.N.Pereira.
1995.
Principles and implementation of de-ductive parsing.
Journal of Logic Programming, 24:3?36.Klaas Sikkel.
1994.
How to compare the structure ofparsing algorithms.
In G. Pighizzini and P. San Pietro,editors, Proc.
of ASMICS Workshop on Parsing The-ory.
Milano, Italy, Oct 1994, pages 21?39.Klaas Sikkel.
1997.
Parsing Schemata ?
A Frameworkfor Specification and Analysis of Parsing Algorithms.Texts in Theoretical Computer Science ?
An EATCSSeries.
Springer-Verlag, Berlin/Heidelberg/New York.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProc.
of 8th International Workshop on Parsing Tech-nologies, pages 195?206.976
