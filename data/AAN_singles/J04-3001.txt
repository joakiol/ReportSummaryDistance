c?
2004 Association for Computational LinguisticsSample Selection for Statistical ParsingRebecca Hwa?University of PittsburghCorpus-based statistical parsing relies on using large quantities of annotated text as trainingexamples.
Building this kind of resource is expensive and labor-intensive.
This work proposes touse sample selection to find helpful training examples and reduce human effort spent on annotatingless informative ones.
We consider several criteria for predicting whether unlabeled data mightbe a helpful training example.
Experiments are performed across two syntactic learning tasksand within the single task of parsing across two learning models to compare the effect of differentpredictive criteria.
We find that sample selection can significantly reduce the size of annotatedtraining corpora and that uncertainty is a robust predictive criterion that can be easily applied todifferent learning models.1.
IntroductionMany learning tasks for natural language processing require supervised training; thatis, the system successfully learns a concept only if it has been given annotated train-ing data.
For example, while it is difficult to induce a grammar with raw text alone,the task is tractable when the syntactic analysis for each sentence is provided as apart of the training data (Pereira and Schabes 1992).
Current state-of-the-art statisti-cal parsers (Collins 1999; Charniak 2000) are all trained on large annotated corporasuch as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
However,supervised training data are difficult to obtain; existing corpora might not contain therelevant type of annotation, and the data might not be in the domain of interest.
Forexample, one might need lexical-semantic analyses in addition to the syntactic anal-yses in the treebank, or one might be interested in processing languages, domains,or genres for which there are no annotated corpora.
Because supervised training de-mands significant human involvement (e.g., annotating the syntactic structure of eachsentence by hand), creating a new corpus is a labor-intensive and time-consuming en-deavor.
The goal of this work is to minimize a system?s reliance on annotated trainingdata.One promising approach to mitigating the annotation bottleneck problem is touse sample selection, a variant of active learning.
Sample selection is an interactivelearning method in which the machine takes the initiative in selecting unlabeled datafor the human to annotate.
Under this framework, the system has access to a large poolof unlabeled data, and it has to predict how much it can learn from each candidate inthe pool if that candidate is labeled.
More quantitatively, we associate each candidatein the pool with a training utility value (TUV).
If the system can accurately identifythe subset of examples with the highest TUV, it will have located the most beneficial?
Computer Science Department, Pittsburgh, PA 15260.
E-mail: hwa@cs.pitt.edu.Submission received: 14 October 2002; Revised submission received: 30 September 2003; Accepted forpublication: 22 December 2003254Computational Linguistics Volume 30, Number 3training examples, thus freeing the annotators from having to label less informativeexamples.In this article, we apply sample selection to two syntactic learning tasks: traininga prepositional-phrase attachment (PP-attachment) model and training a statisticalparsing model.
We are interested in addressing two main questions.
First, what aregood predictors of a candidate?s training utility?
We propose several predictive criteriaand define evaluation functions based on them to rank the candidates?
utility.
Wehave performed experiments comparing the effect of these evaluation functions onthe size of the training corpus.
We find that, with a judiciously chosen evaluationfunction, sample selection can significantly reduce the size of the training corpus.
Thesecond main question is: Are the predictors consistently effective for different types oflearners?
We compare the predictive criteria both across tasks (between PP-attachmentand parsing) and within a single task (applying the criteria to two parsing models:an expectation-maximization-trained parser and a count-based parser).
We find thatthe learner?s uncertainty is a robust predictive criterion that can be easily applied todifferent learning models.2.
Learning with Sample SelectionUnlike traditional learning systems that receive training examples indiscriminately,a sample selection learning system actively influences its own progress by choosingnew examples to incorporate into its training set.
There are two types of selection algo-rithms: committee-based and single learner.
A committee-based selection algorithmworks with multiple learners, each maintaining a different hypothesis (perhaps per-taining to different aspects of the problem).
The candidate examples that lead to themost disagreements among the different learners are considered to have the highestTUV (Cohn, Atlas, and Ladner 1994; Freund et al 1997).
For computationally intensiveproblems, such as parsing, keeping multiple learners may be impractical.In this work, we focus on sample selection using a single learner that keeps oneworking hypothesis.
Without access to multiple hypotheses, the selection algorithmcan nonetheless estimate the TUV of a candidate.
We identify the following threeclasses of predictive criteria:1.
Problem-space: Knowledge about the problem space may provideinformation about the type of candidates that are particularly plentiful ordifficult to learn.
This criterion focuses on the general attributes of thelearning problem, such as the distribution of the input data andproperties of the learning algorithm, but it ignores the current state ofthe hypothesis.2.
Performance of the hypothesis: Testing the candidates on the currentworking hypothesis shows the type of input data on which thehypothesis may perform weakly.
That is, if the current hypothesis isunable to label a candidate or is uncertain about it, then the candidatemight be a good training example (Lewis and Catlett 1994).
Theunderlying assumption is that an uncertain output is likely to be wrong.3.
Parameters of the hypothesis: Estimating the potential impact that thecandidates will have on the parameters of the current workinghypothesis locates those examples that will change the currenthypothesis the most.255Hwa Sample Selection for Statistical ParsingU is a set of unlabeled candidates.L is a set of labeled training examples.C is the current hypothesis.Initialize:C ?
Train(L).RepeatN ?
Select(n, U, C, f ).U ?
U ?
N.L ?
L ?
Label(N).C ?
Train(L).Until (C is good enough) or (U = ?)
or (cutoff).Figure 1Pseudo code for the sample selection learning algorithm.Figure 1 outlines the single-learner sample selection training loop in pseudocode.Initially, the training set, L, consists of a small number of labeled examples, based onwhich the learner proposes its first hypothesis of the target concept, C. Also availableto the learner is a large pool of unlabeled training candidates, U.
In each trainingiteration, the selection algorithm, Select(n, U, C, f ), ranks the candidates of U accordingto their expected TUVs and returns the n candidates with the highest values.
Thealgorithm predicts the TUV of each candidate, u ?
U, with an evaluation function,f (u, C).
This function may rely on the hypothesis concept C to estimate the utility of acandidate u.
The n chosen candidates are then labeled by human experts and addedto the existing training set.
Running the learning algorithm, Train(L), on the updatedtraining set, the system proposes a new hypothesis regarding the target concept thatis the most compatible with the examples seen thus far.
The loop continues until oneof three stopping conditions is met: The hypothesis is considered to perform wellenough, all candidates are labeled, or an absolute cutoff point is reached (e.g., nomore resources).3.
Sample Selection for Prepositional-Phrase AttachmentOne common source of structural ambiguities arises from syntactic constructs in whicha prepositional phrase might be equally likely to modify the verb or the noun pre-ceding it.
Researchers have proposed many computational models for resolving PP-attachment ambiguities.
Some well-known approaches include rule-based models (Brilland Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximum-entropy model (Ratnaparkhi 1998).
Following the tradition of using learning PP-attachment as a way to gain insight into the parsing problem, we first apply sampleselection to reduce the amount of annotation used in training a PP-attachment model.We use the Collins-Brooks model as the basic learning algorithm and experiment withseveral evaluation functions based on the types of predictive criteria described earlier.Our experiments show that the best evaluation function can reduce the number oflabeled examples by nearly half without loss of accuracy.3.1 A Summary of the Collins-Brooks ModelThe Collins-Brooks model takes prepositional phrases and their attachment classifica-tions as training examples: each is represented as a quintuple of the form (v, n, p, n2, a),where v, n, p, and n2 are the head words of the verb phrase, the object noun phrase, the256Computational Linguistics Volume 30, Number 3subroutine Train(L)foreach ex ?
L doextract (v, n, p, n2, a) from exforeach tuple ?
{(v, n, p, n2), (v, p, n2), (n, p, n2), (v, n, p), (v, p), (n, p), (p, n2), (p)} doCount(tuple) ?
Count(tuple) + 1if a = noun thenCountNP(tuple) ?
CountNP(tuple) + 1subroutine Test(U)foreach u ?
U doextract (v, n, p, n2) from uif Count(v, n, p, n2) > 0 thenprob ?
CountNP(v,n,p,n2)Count(v,n,p,n2)elsif Count(v, p, n2) + Count(n, p, n2) + Count(v, n, p) > 0 thenprob ?
CountNP(v,p,n2)+CountNP(n,p,n2)+CountNP(v,n,p)Count(v,p,n2)+Count(n,p,n2)+Count(v,n,p)elsif Count(v, p) + Count(n, p) + Count(p, n2) > 0 thenprob ?
CountNP(v,p)+CountNP(n,p)+CountNP(p,n2)Count(v,p)+Count(n,p)+Count(p,n2)elsif Count(p) > 0 thenprob ?
CountNP(p)Count(p)else prob ?
1if prob ?
.5 thenoutput nounelse output verbFigure 2The Collins-Brooks PP-attachment classification algorithm.preposition, and the prepositional noun phrase, respectively, and a specifies the attach-ment classification.
For example, (wrote a book in three days, attach-verb) would be anno-tated as (wrote, book, in, days, verb).
The head words can be automatically extracted usinga heuristic table lookup in the manner described by Magerman (1994).
For this learningproblem, the supervision is the one-bit information of whether p should attach to v orto n. In order to learn the attachment preferences of prepositional phrases, the systembuilds attachment statistics for each the characteristic tuple of all training examples.
Acharacteristic tuple is some subset of the four head words in the example, with the con-dition that one of the elements must be the preposition.
Each training example formseight characteristic tuples: (v, n, p, n2), (v, n, p), (v, p, n2), (n, p, n2), (v, p), (n, p), (p, n2), (p).The attachment statistics are a collection of the occurrence frequencies for all the char-acteristic tuples in the training set and the occurrence frequencies for the characteristictuples of those examples determined to attach to nouns.
For some characteristic tuplet, Count(t) denotes the former and CountNP(t) denotes the latter.
In terms of the sampleselection algorithm, the collection of counts represents the learner?s current hypothesis(C in Figure 1).
Figure 2 provides the pseudocode for the Train routine.Once trained, the system can be used to classify test cases based on the statistics ofthe most similar training examples and back off as necessary.
For instance, to determinethe PP-attachment for a test case, the classifier would first consider the ratio of thetwo frequency counts for the four-word characteristic tuple of the test case.
If the tuple257Hwa Sample Selection for Statistical ParsingFigure 3In this example, the classification of the test case preposition is backed off to thetwo-word-tuple level.
In the diagram, each circle represents a characteristic tuple.
A filledcircle denotes that the tuple has occurred in the training set.
The dashed rectangular boxindicates the back-off level on which the classification is made.never occurred in the training example, the classifier would then back off to look at thetest case?s three three-word characteristic tuples.
It would continue to back off further,if necessary.
In the case that the model has no information on any of the characteristictuples of the test case, it would, by default, classify the test case as an instance of nounattachment.
Figure 3 shows using the back-off scheme on a test case.
We describe inthe Test pseudocode routine in Figure 2 the model?s classification procedure for eachback-off level.3.2 Evaluation FunctionsBased on the three classes of predictive criteria discussed in Section 2, we proposeseveral evaluation functions for the Collins-Brooks model.3.2.1 The Problem Space.
One source of knowledge to exploit is our understanding ofthe PP-attachment model and properties of English prepositional phrases.
For instance,we know that the most problematic test cases for the PP-attachment model are thosefor which it has no statistics at all.
Therefore, those data that the system has notyet encountered might be good candidates.
The first evaluation function we define,fnovel(u, C), equates the TUV of a candidate u with its degree of novelty, the numberof its characteristic tuples that currently have zero counts:1fnovel(u, C) =?t?Tuples(u){1 : Count(t) = 00 : otherwiseThis evaluation function has some blatant defects.
It may distort the data distributionso much that the system will not be able to build up a reliable collection of statistics.The function does not take into account the intuition that those data that rarely occur,no matter how novel, probably have overall low training utility.
Moreover, the scoringscheme does not make any distinction between the characteristic tuples of a candidate.1 Note that the current hypothesis C is ignored in evaluation functions of this class because they dependonly on the knowledge about the problem space.258Computational Linguistics Volume 30, Number 3book,on, shelfbook,on,put,book,on,shelfput,on,shelfput,on, on,book,shelfon,put,nounon,on,shelfput,on, shelfon,put,ideaon,nounput,on,shelfput,on, shelfon,on,ideaideaideabook,shelfon,on,wroteon,nounbook,on,shelfbook,on,on,shelf shelfbook,on,on,wrotewrote wroteideaon,nounon,on,on,on,on,topichadideaon,topictopicideahadon,hadideahadtopicu1, u2 u3 u4 u5Figure 4If candidate u1 is selected, a total of 22 tuples can be ignored.
The dashed rectangles show theclassification level before training, and the solid rectangles show the classification level afterthe statistics of u1 have been taken.
The obviated tuples are represented by the filled blackcircles.We know, however, that the PP-attachment classifier is a back-off model that makesits decision based first on statistics of the characteristic tuple with the most words.
Amore sophisticated sampling of the data domain should consider not only the noveltyof the data, but also the frequency of its occurrence, as well as the quality of its charac-teristic tuples.
We define a back-off-model-based evaluation function, fbackoff(u, C), thatscores a candidate u by counting the number of characteristic tuples that would beobviated in all candidates if u were included in the training set.
For example, supposewe have a small pool of five candidates, and we are about to pick the first trainingexample:u1 = (put, book, on, shelf)u2 = (put, book, on, shelf)u3 = (put, idea, on, shelf)u4 = (wrote, book, on, shelf)u5 = (had, idea, on, topic)According to fbackoff, either u1 or u2 would be the best choice.
By selecting either asthe first training example, we could ignore all but the four-word characteristic tuplefor both u1 and u2 (a saving of seven tuples each); since u3 and u4 each have threewords in common with the first two candidates, they would no longer depend ontheir lower four tuples; and although we would also improve the statistics for oneof u5?s tuples (on), nothing could be pruned from u5?s characteristic tuples.
Thus,fbackoff(u1, C) = fbackoff(u2, C) = 7 + 7 + 4 + 4 = 22 (see Figure 4).Under fbackoff, if u1 were chosen as the first example, u2 would lose all its utility,because we could not prune any extra characteristic tuples by using u2.
That is, inthe next round of selection, fbackoff(u2, C) = 0.
Candidate u5 would be the best secondexample because it would now have the most tuples to prune (7 tuples).The evaluation function fbackoff improves upon fnovel in two ways.
First, novel can-didates that occur frequently are favored over those that rarely come up.
As we haveseen in the above example, a candidate that is similar to other candidates can elimi-nate more characteristic tuples all at once.
Second, the evaluation strategy follows theworking principle of the back-off model and discounts lower-level characteristic tuplesthat do not affect the classification process, even if they were ?novel.?
For instance,259Hwa Sample Selection for Statistical Parsingafter selecting u1 as the first training example, we would no longer care about thetwo-word tuples of u4 such as (wrote, on), even though we have no statistics for them.A potential problem with fbackoff is that after all the obvious candidates have beenselected, the function is not very good at differentiating between the remaining can-didates that have about the same level of novelty and occur infrequently.3.2.2 The Performance of the Hypothesis.
The evaluation functions discussed in theprevious section score candidates based on prior knowledge alone, independent of thecurrent state of the learner?s hypothesis and the annotation of the selected trainingexamples.
To attune the selection of training examples to the learner?s progress, anevaluation function might factor in its current hypothesis in predicting a candidate?sTUV.One way to incorporate the current hypothesis into the evaluation function isto score each candidate using the current model, assuming its hypothesis is right.An error-driven evaluation function, ferr, equates the TUV of a candidate with thehypothesis?
estimate of its likelihood to misclassify that candidate (i.e., one minus theprobability of the most-likely class).
If the hypothesis predicts that the likelihood of aprepositional phrase to attach to the noun is 80%, and if the hypothesis is accurate,then there is a 20% chance that it has misclassified.A related evaluation function is one that measures the hypothesis?s uncertaintyacross all classes, rather than focusing on only the most likely class.
Intuitively, if thehypothesis classifies a candidate as equally likely to attach to the verb as to the noun, itis the most uncertain of its answer.
If the hypothesis assigns a candidate to a class witha probability of one, then it is the most certain of its answer.
For the binary-class case,the uncertainty-based evaluation function, func, can be expressed in the same way asthe error-driven function, as a function that is symmetric about 0.5 and monotonicallydecreases if the hypothesis prefers one class over another:2func(u, C) = ferr(u, C)={1 ?
P(noun | u, C) : P(noun | u, C) ?
0.5P(noun | u, C) : otherwise= 0.5 ?
abs(0.5 ?
P(noun | u, C)) (1)In the general case of choosing between multiple classes, ferr and func are different fromone another.
We shall return to this point in Section 4.1.2 when we consider trainingparsers.The potential drawback of the performance-based evaluation functions is that theyassume that the hypothesis is correct.
Selecting training examples based on a poor hy-pothesis is prone to pitfalls.
On the one hand, the hypothesis may be overly confidentabout the certainty of its decisions.
For example, the hypothesis may assign noun toa candidate with a probability of one based on parameter estimates computed from asingle previous observation in which a similar example was labeled as noun.
Despitethe unreliable statistics, this candidate would not be selected, since the hypothesisconsiders this a known case.
Conversely, the hypothesis may also direct the selec-tion algorithm to chase after undecidable cases.
For example, consider prepositionalphrases (PPs) with in as the head.
These PPs occur frequently, and about half of themshould attach to the object noun.
Even though training on more labeled in examples2 As long as it adheres to these criteria, the specific form of the function is irrelevant, since the selectionis not determined by the absolute scores of the candidates, but by their scores relative to each other.260Computational Linguistics Volume 30, Number 3does not improve the model?s performance on future in PPs, the selection algorithmwill keep on requesting more in training examples because the hypothesis remains un-certain about this preposition.3 With an unlucky starting hypothesis, these evaluationfunctions may select uninformative candidates initially.3.2.3 The Parameters of the Hypothesis.
The potential problems with performance-based evaluation function stems from their trust in the model?s diagnosis of its ownprogress.
Another way to incorporate the current hypothesis is to determine how goodit is and what type of examples will improve it the most.
In this section we proposean evaluation function that scores candidates based on their utilities in increasing theconfidence about the parameters of the hypothesis (i.e., the collection of statistics overthe characteristic tuples of the training examples).Training the parameters of the PP-attachment model is similar to empirically de-termining the bias of a coin.
We measure the coin?s bias by repeatedly tossing it andkeeping track of the percentage of times it lands on heads.
The more trials we perform,the more confident we become about our estimation of the bias.
Similarly, in estimatingp, the likelihood of a PP?s attaching to its object noun, we are more confident about theclassification decision based on statistics with higher counts than based on statisticswith lower counts.
A quantitative measurement of our confidence in a statistic is theconfidence interval.
This is a region around the measured statistic, bounding the areawithin which the true statistic may lie.
More specifically, the confidence interval for p,a binomial parameter, is defined asconf int(p?, n) =11 + t2n(p?
+t22n?
t?p?
(1 ?
p?
)n+t24n2)where p?
is the expected value of p based on n trials, and t is a threshold value thatdepends on the number of trials and the level of confidence we desire.
For instance,if we want to be 90% confident that the true statistic p lies within the interval, and p?is based on n = 30 trials, then we set t to be 1.697.4 Applying the confidence intervalconcept to evaluating candidates for the back-off PP-attachment model, we definea function fconf that scores a candidate by taking the average of the lengths of theconfidence interval of each back-off level.
That is,fconf(u, C) =4?l=1|conf int(p?l(u, C), nl(u, C))|4where p?l(u, C) is the probability that model C will attach u to noun at back-off level l,and nl(u, C) is the number of training examples upon which this classification is based.The confidence-based evaluation function has several potential problems.
One ofits flaws is similar to that of fnovel.
In the early stage, fconf picks the same examples3 This phenomenon is particularly acute in the early stages of refining the hypothesis because mostdecisions are based on statistics of the head preposition alone; in the later stages, the hypothesis canusually rely on higher-ordered characteristic tuples that tend to be better classifiers.4 For n ?
120, the values of t can be found in standard statistic textbooks; for n ?
120, t = 1.6576.Because the derivation for the confidence interval equation makes a normality assumption, theequation does not hold for small values of n (cf Larsen and Marx [1986], pp.
277?278).
When n is large,the contributions from the terms in t2n are negligible.
Dropping these terms, we have the t statistic forlarge n, p?
?
t?p?
(1 ?
p?
)/n.261Hwa Sample Selection for Statistical Parsingas fnovel, because we have no confidence in the statistics of novel examples.
Therefore,fconf is also prone to chase after examples that rarely occur to build up the confidenceof some unimportant parameters.
A second problem is that fconf ignores the output ofthe model.
Thus, if candidate A has a confidence interval around [0.6, 1] and candidateB has a confidence interval around [0.4, 0.7], then fconf will prefer candidate A, eventhough training on A will not change the hypothesis?s performance, since the entireconfidence interval is already in the noun zone.3.2.4 Hybrid Function.
The three categories of predictive criteria discussed above arecomplementary, each focusing on a different aspect of the learner?s weakness.
There-fore, it may be beneficial to combine these criteria into one evaluation function.
Forinstance, the deficiency of the confidence-based evaluation function described in theprevious section can be avoided if the confidence interval covering the region aroundthe uncertainty boundary (candidate B in the example just discussed) is weighed moreheavily than one around the end points (candidate A).In this section, we introduce a new function that tries to factor in both the uncer-tainty of the model performance and the confidence of the model parameters.
First, wedefine a function, called area(p?, n), that computes the area under a Gaussian functionN(x,?,?)
with a mean of 0.5 and a standard deviation of 0.1 that is bounded by theconfidence interval as computed by conf int(p?, n) (see Figure 5).5 That is, suppose p?has a confidence interval of [a, b]; thenarea(p?, n) =?
baN(x, 0.5, 0.1)dxComputing area for each back-off level, we define an evaluation function, farea(u, C),as their average.
This function can be viewed as a product of fconf and func.63.3 Experimental ComparisonTo determine the relative merits of the proposed evaluation functions, we compare thelearning curve of training with sample selection according to each function against abaseline of random selection in an empirical study.
The corpus for this comparison isa collection of phrases extracted from the Wall Street Journal (WSJ) Treebank.
We useSection 00 as the development set and Sections 2-23 as the training and test sets.
Weperform 10-fold cross-validation to ensure the statistical significance of the results.
Foreach fold, the training candidate pool contains about 21,000 phrases, and the test setcontained about 2,000 phrases.As shown in Figure 1, the learner generates an initial hypothesis based on a smallset of training examples, L. These examples are randomly selected from the pool ofunlabeled candidates and annotated by a human.
Random sampling ensures that theinitial trained set reflects the distribution of the candidate pool and thus that theinitial hypothesis is unbiased.
Starting with an unbiased hypothesis is important forthose evaluation functions whose scoring metrics are affected by the accuracy of thehypothesis.
In these experiments, L initially contains 500 randomly selected examples.In each selection iteration, all the candidates are scored by the evaluation function,and n examples with the highest TUVs are picked out from U to be labeled and added5 The standard deviation value for the Gaussian is chosen so that more than 98% of the mass of thedistribution is between 0.25 and 0.75.6 Note that we can replace the function in equation (1) with the N(x, 0.5, ?)
without affecting func,because it is also symmetric about 0.5 and monotonically decreasing as the input value moves further.262Computational Linguistics Volume 30, Number 30.4Likelihood of Attach NP1.00Figure 5An example: Suppose that the candidate has a likelihood of 0.4 for noun attachment and aconfidence interval of width 0.1.
Then area computes the area bounded by the confidenceinterval and the Gaussian curve.to L. Ideally, we would like to have n = 1 for each iteration.
In practice, however, it isoften more convenient for the human annotator to label data in larger batches ratherthan one at a time.
In these experiments, we use a batch size of n = 500 examples.We make note of one caveat to this kind of n-best batch selection.
Under ahypothesis-dependent evaluation function, identical examples will receive identicalscores.
Because identical (or very similar) examples tend to address the same defi-ciency in the hypothesis, adding n very similar examples to the training set is unlikelyto lead to big improvements in the hypothesis.
To diversify the examples in each batch,we simulate single-example selection (whenever possible) by reestimating the scoresof the candidates after each selection.
Suppose we have just chosen to add candidatex to the batch.
Then, before selecting the next candidate, we estimate the potentialdecrease in scores of candidates similar to x once it belongs to the annotated trainingset.
The estimation is based entirely on the knowledge that x is chosen, but not onthe classification of x.
Thus, only certain types of evaluation functions are amenableto the reestimation process.
For example, if scores have been assigned by fconf, thenwe know that the confidence intervals of the candidates similar to x must decreaseslightly after learning x.
On the other hand, if scores have been assigned by func, thenwe cannot perceive any changes in the scores of similar candidates without knowingthe true classification of x.3.3.1 Results and Discussion.
This section presents the empirical measurements ofthe model?s performances using training examples selected by different evaluationfunctions.
We compare each proposed function with the baseline of random selection(frand).
The results are graphically depicted from two perspectives.
One (e.g., Figure6(a)?6(c)) plots the learning curves of the functions, showing the relationship betweenthe number of training examples (x-axis) and the performance of the model on testdata (y-axis).
We deem one evaluation function to be better than another if its learningcurve envelopes the other?s.
An alternative way to interpret the results is to focus onthe reduction in training size offered by one evaluation function over another for someparticular performance level.
Figure 6(d) is a bar graph comparing all the evaluation263Hwa Sample Selection for Statistical Parsing747678808284860 5000 10000 15000 20000Classificationaccuracyonthetestset(%)Number of examples in the training setbaselinenoveltybackoff747678808284860 5000 10000 15000 20000Classificationaccuracyonthetestset(%)Number of examples in the training setbaselineuncertaintyconfidence(a) (b)747678808284860 5000 10000 15000 20000Classificationaccuracyonthetestset(%)Number of examples in the training setbaselinearea05,00010,00015,00020,00025,000baselinenoveltybackoffuncertaintyconfidenceareaEvaluation FunctionsNumberofLabeledTrainingExamples(c) (d)Figure 6A comparison of the performance of different evaluation functions: (a) compares the learningcurves of the functions that use knowledge about the problem space (fnovel and fbackoff) withthat of the baseline; (b) compares the learning curves of performance-based function (func andfconf ) with the baseline; (c) compares the learning curve of farea, which combines uncertaintyand confidence, with func, fconf, and the baseline; (d) compares all the functions for the numberof training examples selected at the final performance level (83.8%).functions at the highest performance level.
The graph shows that in order to train amodel that attaches PPs with an accuracy rate of 83.8%, sample selection with fnovelrequires 2,500 fewer examples than the baseline.Compared to fnovel, fbackoff selects more helpful training examples in the early stage.As shown in Figure 6(a), the improvement rate of the model under fbackoff is alwaysat least as fast that for as fnovel.
However, the differences between these two functionsbecome smaller for higher performance levels.
This outcome validates our predictions.Scoring candidates by a combination of their novelty, occurrence frequencies, and thequalities of their characteristic tuples, fbackoff selects helpful early (the first 4,000 or so)training examples.
Then, just as in fnovel, the learning rate remains stagnant for thenext 2,000 poorly selected examples.
Finally, when the remaining candidates all havesimilar novelty values and contain mostly characteristic tuples that occur infrequently,the selection becomes random.Figure 6(b) compares the two evaluation functions that score candidates based onthe current state of the hypothesis.
Although both functions suffer a slow start, theyare more effective than fbackoff at reducing the training set when learning high-qualitymodels.
Initially, because all the unknown statistics are initialized to 0.5, selection basedon func is essentially random sampling.
Only after the hypothesis becomes sufficientlyaccurate (after training on about 5,000 annotated examples) does it begin to make264Computational Linguistics Volume 30, Number 3informed selections.
Following a similar but more exaggerated pattern, the confidence-based function, fconf, also improves slowly at the beginning before finally overtakingthe baseline.
As we noted earlier, because the hypothesis is not confident about novelcandidates, fconf and fnovel tend to select the same early examples.
Therefore, the earlylearning rate of fconf is as poor as that of fnovel.
In the later stage, while fnovel continuesto flounder, fconf can select better candidates based on a more reliable hypothesis.Finally, the best-performing evaluation function is the hybrid approach.
Figure6(c) shows that the learning curve of farea combines the earlier success of func and thelater success of fconf to always outperform the other functions.
As shown in Figure6(d), it requires the least number of examples to achieve the highest performance levelof 83.8%.
Compared to the baseline, farea requires 47% fewer examples to achieve thisperformance level.
From these comparison studies, we conclude that involving thehypothesis in the selection process is a key factor in reducing the size of the trainingset.4.
Sample Selecting for Statistical ParsingIn applying sample selection to training a PP-attachment model, we have observedthat all effective evaluation functions make use of the model?s current hypothesis inestimating the training utility of the candidates.
Although knowledge about the prob-lem space seems to help sharpening the learning curve initially, overall, it is not a goodpredictor.
In this section, we investigate whether these observations hold true for train-ing statistical parsing models as well.
Moreover, in order to determine whether theperformances of the predictive criteria are consistent across different learning modelswithin the same domain, we have performed the study on two parsing models: onebased on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Taka-hashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism(Schabes and Waters 1993; Hwa 1998), and Collins?s Model 2 parser (1997).
Althoughboth models are lexicalized, statistical parsers, their learning algorithms are different.The Collins Parser is a fully supervised, history-based learner that models the pa-rameters of the parser by taking statistics directly from the training data.
In contrast,PLTIG?s expectation-maximization-based induction algorithm is partially supervised;the model?s parameters are estimated indirectly from the training data.As a superset of the PP-attachment task, parsing is a more challenging learningproblem.
Whereas a trained PP-attachment model is a binary classifier, a parser mustidentify the correct syntactic analysis out of all possible parses for a sentence.
Thisclassification task is more difficult than PP-attachment, since the number of possibleparses for a sentence grows exponentially with respect to its length.
Consequently,the annotator?s task is more complex.
Whereas the person labeling the training datafor PP-attachment reveals one unit of information (always choosing between noun orverb), the annotation needed for parser training is usually greater than one unit,7 andthe type of labels varies from sentence to sentence.
Because the annotation complexitydiffers from sentence to sentence, the evaluation functions must strike a balance be-tween maximizing potential informational gain and minimizing the expected amount7 We consider each pair of brackets in the training sentence to be one unit of supervised information,assuming that the number of brackets correlates linearly with the amount of effort spent by the humanannotator.
This correlation is an approximation, however; in real life, adding one pair of brackets to alonger sentence may require more effort than adding a pair of brackets to a shorter one.
To capturebracketing interdependencies at this level, we would need to develop a model of the annotationdecision process and incorporate it as an additional factor in the evaluation functions.265Hwa Sample Selection for Statistical Parsingof annotation exerted.
We propose a set of evaluation functions similar in spirit to thosefor the PP-attachment learner, but extended to accommodate the parsing domain.4.1 Evaluation Functions4.1.1 Problem Space.
Similarly to scoring a PP candidate based on the novelty andfrequencies of its characteristic tuples, we define an evaluation function, flex(w, G) thatscores a sentence candidate, w, based on the novelty and frequencies of word pairco-occurrences:flex(w, G) =?wi,wj?w new(wi, wj)?
coocc(wi, wj)length(w)where w is the unlabeled sentence candidate, G is the current parsing model (whichis ignored by problem-space-based evaluation functions), new(wi, wj) is an indicatorfunction that returns one if we have not yet selected any sentence in which wi and wjco-occurred, and coocc(wi, wj) is a function that returns the number of times that wi co-occurs8 with wj in the candidate pool.
We expect these evaluation functions to be lessrelevant for the parsing domain than for the PP-attachment domain for two reasons.First, because we do not have the actual parses, the extraction of lexical relationshipsis based on co-occurrence statistics, not syntactic relationships.
Second, because thedistribution of words that form lexical relationships is wider and more uniform thanthat of words that form PP characteristic tuples, most word pairs will be novel andappear only once.Another simple evaluation function based on the problem space is one that esti-mates the TUV of a candidate from its sentence length:flen(w, G) = length(w)The intuition behind this function is based on the general observation that longersentences tend to have complex structures and introduce more opportunities for am-biguous parses.
Although these evaluation functions may seem simplistic, they haveone major advantage: They are easy to compute and require little processing time.Because inducing parsing models demands significantly more time than inducing PP-attachment models, it becomes more important that the evaluation functions for pars-ing models be as efficient as possible.4.1.2 The Performance of the Hypothesis.
We previously defined two performance-based evaluation functions: ferr, the model?s estimate of the likelihood that is hasmade a classification error, and func, the model?s estimate of its uncertainty in makingthe classification.
We have shown the two functions to have similar performance forthe PP-attachment task.
This is not the case for statistical parsing, however, becausethe number of possible classes (parse trees) differs from sentence to sentence.
Forexample, suppose we wish to compare one candidate for which the current parsingmodel generated four equally likely parses with another candidate for which the modelgenerated 1 parse with probability of 0.2 and 99 other parses with a probability of0.01 (such that they sum to 0.98).
The error-driven function, ferr, would score the lattercandidate higher because its most likely parse has a lower probability than that of themost likely parse of the former candidate; the uncertainty-based function, func, wouldscore the former candidate higher because the model does not have a strong preference8 We consider two words to be co-occuring if their log-likelihood ratio is greater than some thresholdvalue determined with held-out data.266Computational Linguistics Volume 30, Number 3for one parse over any other.
In this section, we provide a formal definition for bothfunctions.Suppose that a parsing model G generates a candidate sentence w with probabilityP(w | G), and that the set V contains all possible parses that G generated for w. Then,we denote the probability of G?s generating a single parse, v ?
V , as P(v | G) such that?v?VP(v | G) = P(w | G).
The parse chosen for w is the most likely parse in V , denotedas vmax, wherevmax = argmaxv?VP(v | G)Note that P(v | G) reflects the probability of one particular parse tree, v, out of allpossible parse trees for all possible sentences that G can generate.
To compute thelikelihood of a parse?s being the correct parse out of the possible parses of w accordingto G, denoted as P(v | w, G), we need to normalize the tree probability by the sentenceprobability.
So according to G, the likelihood that vmax is the correct parse for w is9P(vmax | w, G) =P(vmax | G)P(w | G)=P(vmax | G)?v?V P(v | G).
(2)Therefore, the error-driven evaluation function is defined asferr(w, G) = 1 ?
P(vmax | w, G)Unlike the error-driven function, which focuses on the most likely parse, theuncertainty-based function takes the probability distribution of all parses into account.To quantitatively characterize its distribution, we compute the entropy of the distri-bution.
That is,H(V) = ?
?v?Vp(v) lg(p(v)) (3)where V is a random variable that can take any possible outcome in set V , and p(v) =Pr(V = v) is the density function.
Further details about the properties of entropy canbe found in textbooks on information theory (e.g., Cover and Thomas 1991).Determining the parse tree for a sentence from a set of possible parses can beviewed as assigning a value to a random variable.
Thus, a direct application of theentropy definition to the probability distribution of the parses for sentence w in Gcomputes its tree entropy, TE(w, G), the expected number of bits needed to encodethe distribution of possible parses for w. However, we may not wish to comparetwo sentences with different numbers of parses by their entropy directly.
If the parseprobability distributions for both sentences are uniform, the sentence with more parseswill have a higher entropy.
Because longer sentences typically have more parses, usingentropy directly would result in a bias toward selecting long sentences.
To normalizefor the number of parses, the uncertainty-based evaluation function, func, is defined asa measurement of similarity between the actual probability distribution of the parsesand a hypothetical uniform distribution for that set of parses.
In particular, we divide9 Note that P(w|v, G) = 1 for any v ?
V , where V is the set of all possible parses for w, because v existsonly when w is observed.267Hwa Sample Selection for Statistical Parsingthe tree entropy by the log of the number of parses:10func(w, G) =TE(w, G)lg(?V?
)We now derive the expression for TE(w, G).
Recall from equation (2) that if Gproduces a set of parses, V , for sentence w, the set of probabilities P(v | w, G) (for allv ?
V) defines the distribution of parsing likelihoods for sentence w:?v?VP(v | w, G) = 1Note that P(v | w, G) can be viewed as a density function p(v) (i.e., the probability ofassigning v to a random variable V).
Mapping it back into the entropy definition fromequation (3), we derive the tree entropy of w as follows:TE(w, G) = H(V)= ?
?v?Vp(v) lg(p(v))= ?
?v?VP(v | G)P(w | G) lg(P(v | G)P(w | G) )= ?
?v?VP(v | G)P(w | G) lg(P(v | G)) +?v?VP(v | G)P(w | G) lg(P(w | G))= ?
1P(w | G)?v?VP(v | G) lg(P(v | G)) + lg(P(w | G))P(w | G)?v?VP(v | G)= ?
1P(w | G)?v?VP(v | G) lg(P(v | G)) + lg(P(w | G))Using the bottom-up, dynamic programming technique (see the appendix for de-tails) of computing inside probabilities (Lari and Young 1990), we can efficiently com-pute the probability of the sentence, P(w | G).
Similarly, the algorithm can be modifiedto compute the quantity?v?VP(v | G) lg(P(v | G)).4.1.3 The Parameters of the Hypothesis.
Although the confidence-based functiongives good TUV estimates to candidates for training PP-attachment models, it is notclear how a similar technique can be applied to training parsers.
Whereas binaryclassification tasks can be described by binomial distributions, for which the confi-dence interval is well defined, a parsing model is made up of many multinomialclassification decisions.
We therefore need a way to characterize the confidence foreach decision as well as a way to combine them into an overall confidence.
Anotherdifficulty is that the complexity of the induction algorithm deters us from reestimat-ing the TUVs of the remaining candidates after selecting each new candidate.
As we10 When func(w, G) = 1, the parser is considered to be the most uncertain about a particular sentence.Instead of dividing tree entropies, one could have computed the Kullback-Leibler distance between thetwo distributions (in which case a score of zero would indicate the highest level of uncertainty).Because the selection is based on relative scores, as long as the function is monotonic, the exact form ofthe function should not have much impact on the outcome.268Computational Linguistics Volume 30, Number 3discussed in Section 3.3, reestimation is important for batched annotation.
Withoutsome means of updating the TUVs after each selection, the learner will not realize thatit has already selected a candidate to train some parameter with low confidence untilthe retraining phase, which occurs only at the end of the batch selection; therefore, itmay continue to select very similar candidates to train the same parameter.
Even if weassume that the statistics can be updated, reestimating the TUVs is a computationallyexpensive operation.
Essentially, all the remaining candidates that share some param-eters with the selected candidate will need to be re-parsed.
For these practical rea-sons, we do not include an evaluation function measuring confidence for the parsingexperiment.4.2 Experiments and ResultsWe compare the effectiveness of sample selection using the proposed evaluation func-tions against a baseline of random selection (frand(w, G) = rand()).
Similarly to previousexperimental designs, the learner is given a small set of annotated seed data from theWSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but withthe labels removed) from which to select new training examples.
All training data arefrom Sections 2?21 of the treebank.
We monitor the learning progress of the parser bytesting it on unseen test sentences.
We use Section 00 for development and Section23 for testing.
This study is repeated for two different models, the PLTIG parser andCollins?s Model 2 parser.4.2.1 An Expectation-Maximization-Based Learner.
In the first experiment, we usean induction algorithm (Hwa 2001a) based on the expectation-maximization (EM)principle that induces parsers for PLTIGs.
The algorithm performs heuristic searchthrough an iterative reestimation procedure to find local optima: sets of values forthe grammar parameters that maximizes the grammar?s likelihood of generating thetraining data.
In principle, the algorithm supports unsupervised learning; however,because the search space has too many local optima, the algorithm tends to convergeon a model that is unsuitable for parsing.
Here, we consider a partially supervisedvariant in which we assume that the learner is given the phrasal boundaries of thetraining sentences but not the label of the constituent units.
For example, the sentenceSeveral fund managers expect a rough market this morning before prices stabilize.
would belabeled as ?
((Several fund managers) (expect ((a rough market) (this morning)) (before(prices stabilize))).)?
Our algorithm is similar to the approach taken by Pereira andSchabes (1992) for inducing PCFG parsers.Because the EM algorithm itself is an iterative procedure, performing sample se-lection on top of an EM-based learner is an extremely computational-intensive process.Here, we restrict the experiments for the PLTIG parsers to a smaller-scale study in thefollowing two aspects.
First, the lexical anchors of the grammar rules are backed off topart-of-speech tags; this restricts the size of the grammar vocabulary to 48.
Second, theunlabeled candidate pool is set to contain 3,600 sentences, which is sufficiently largefor inducing a grammar of this size.
The initial model is trained on 500 labeled seedsentences.
For each selection iteration, an additional 100 sentences are moved fromthe unlabeled pool to be labeled and added to the training set.
After training, theupdated parser is then tested on unseen sentences (backed off to their part-of-speechtags) and compared to the gold standard.
Because the induced PLTIG produces binary-branching parse trees, which have more layers than the gold standard, we measureparsing accuracy in terms of the crossing-bracket metric.
The study is repeated for10 trials, each using a different portion of the full training set, to ensure statisticalsignificance (using pairwise t-test at 95% confidence).269Hwa Sample Selection for Statistical Parsing7677787980815000 10000 15000 20000 25000 30000 35000 40000 45000Classificationaccuracyonthetestset(%)Number of labeled brackets in the training setbaselinelengtherror driventree entropy(a)05,00010,00015,00020,00025,00030,00035,00040,000baselinelengtherrordriventree entropyEvaluation functionsNumberofLabeledBracketsintheTrainingData(b)Figure 7PLTIG parser: (a) A comparison of the evaluation functions?
learning curves.
(b) A comparisonof the evaluation functions for a test performance score of 80%.The results of the experiment are graphically shown in Figure 7.
As with thePP-attachment studies, Figure 7(a) compares the learning curves of the proposed eval-uation functions to that of the baseline.
Note that even though these functions selectexamples in terms of entire sentences, the amount of annotation is measured in thegraphs (x-axis) in terms of the number of brackets rather than sentences.
Unlike inthe PP-attachment case, the amount of effort from the annotators varies significantlyfrom example to example.
A short and simple sentence takes much less time to an-notate than a long and complex sentence.
We address this effect by approximatingthe amount of effort as the number of brackets the annotator needs to label.
Thus,we deem one evaluation function more effective than another if, for the desired levelof performance, the smallest set of sentences selected by the function contains fewerbrackets than that of the other function.
Figure 7(b) compares the evaluation functionsat the final test performance level of 80%.270Computational Linguistics Volume 30, Number 3Qualitatively comparing the learning curves in the figure, we see that with the ap-propriate evaluation function, sample selection does reduce the amount of annotation.Similarly to our findings in the PP-attachment study, the simple problem-space-basedevaluation function, flen, offers only little savings; its performance is nearly indistin-guishable from that of the baseline, for the most part.11 The evaluation functions basedon hypothesis performances, on the other hand, do reduce the amount of annotation inthe training data.
Of the two that we proposed for this category, the tree entropy eval-uation function, func, has a slight edge over the error-driven evaluation function, ferr.For a quantitative comparison, let us consider the set of grammars that achievean average parsing accuracy of 80% on the test sentences.
We consider a grammar tobe comparable to that of the baseline if its mean test score is at least as high as thatof the baseline and if the difference between the means is not statistically significant.The baseline case requires an average of about 38,000 brackets in the training data.
Incontrast, to induce a grammar that reaches the same 80% parsing accuracy with theexamples selected by func, the learner requires, on average, 19,000 training brackets.Although the learning rate of ferr is slower than that of func overall, it seems to havecaught up in the end; it needs 21,000 training brackets, slightly more than func.
Whilethe simplistic sentence length evaluation function, flen, is less helpful, its learning ratestill improves slightly faster than that of the baseline.
A grammar of comparable qualitycan be induced from a set of training examples selected by flen containing an averageof 28,000 brackets.124.2.2 A History-Based Learner.
In the second experiment, the basic learning modelis Collins?s (1997) Model 2 parser, which uses a history-based learning algorithm thattakes statistics directly over the treebank.
As a fully supervised algorithm, it does nothave to iteratively reestimate its parameters and is computationally efficient enoughfor us to carry out a large-scale experiment.
For this set of studies, the unlabeledcandidate pool consists of around 39,000 sentences.
The initial model is trained on500 labeled seed sentences, and at each selection iteration, an additional 100 sentencesare moved from the unlabeled pool into the training set.
The parsing performance onthe test sentences is measured in terms of the parser?s F-score, the harmonic averageof the labeled precision and labeled recall rates over the constituents (Van Rijsbergen1979).13We plot the comparisons between different evaluation functions and the baselinefor the history-based parser in Figure 8.
The examples selected by the problem-space-based functions do not seem to be helpful.
Their learning curves are, for the most part,slightly worse than the baseline.
In contrast, the parsers trained on data selected bythe error-driven and uncertainty-based functions learn faster than the baseline; and asbefore, func performs slightly better than ferr.For the final parsing performance of 88%, the parser requires a baseline training setof 30,500 sentences annotated with about 695,000 constituents.
The same performancecan be achieved with a training set of 20,500 sentences selected by ferr, which containsabout 577,000 annotated constituents; or with a training set of 17,500 sentences selectedby func, which contains about 505,000 annotated constituents, reducing the number ofannotated constituents by 27%.
Comparing the outcome of this experiment with that of11 In this experiment, we have omitted the evaluation function for selecting novel lexical relationships,flex, because the grammar does not use actual lexical anchors.12 In terms of the number of sentences, the baseline frand selected 2,600 sentences; flen selected 1,300sentences; and ferr and func each selected 900 sentences.13 F = 2?LR?LPLR+LP , where LR is the labeled recall score and LP is the labeled precision score.271Hwa Sample Selection for Statistical Parsing788082848688100000 200000 300000 400000 500000 600000 700000 800000 900000Classificationaccuracyonthetestset(%)Number of labeled constituents in the training setbaselinelengthnovel lexerror driventree entropy(a)0100,000200,000300,000400,000500,000600,000700,000800,000baseline novellengtherrordriventree entropyEvaluation FunctionsNumberofLabeledConstituentsintheTrainingData(b)Figure 8Model 2 parser: (a) A comparison of the learning curves of the evaluation functions.
(b) Acomparison of all the evaluation functions at the test performance level of 88%.the experiment involving the EM-based learner, we see that the training data reductionrates are less dramatic than before.
This may be because both func and ferr ignore lexicalitems and chase after sentences containing words that rarely occur.
Recent work byTang, Luo, and Roukos (2002) suggests that a hybrid approach that combines featuresof the problem space and the uncertainty of the parser may result in better performancefor lexicalized parsers.5.
Related WorkSample selection benefits problems in which the cost of acquiring raw data is cheap butthe cost of annotating them is high, as is certainly the case for many supervised learn-ing tasks in natural language processing.
In addition to PP-attachment, as discussedin this article, sample selection has been successfully applied to other classification272Computational Linguistics Volume 30, Number 3applications.
Some examples include text categorization (Lewis and Catlett 1994), basenoun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (EngelsonDagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and wordsense disambiguation (Fujii et al 1998).More challenging are learning problems whose objective is not classification, butgeneration of complex structures.
One example in this direction is applying sampleselection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentencesare paired with their semantic representation using a deterministic shift-reduce parser.A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou,and Roukos (2002).
Their results suggest that the number of training examples can befurther reduced by using a hybrid evaluation function that combines a hypothesis-performance-based metric such as tree entropy (?word entropy?
in their terminology)with a problem-space-based metric such as sentence clusters.Aside from active learning, researchers have applied other learning techniquesto combat the annotation bottleneck problem in parsing.
For example, Hendersonand Brill (2002) consider the case in which acquiring additional human-annotatedtraining data is not possible.
They show that parser performance can be improved byusing boosting and bagging techniques with multiple parsers.
This approach assumesthat there are enough existing labeled data to train the individual parsers.
Anothertechnique for making better use of unlabeled data is cotraining (Blum and Mitchell1998), in which two sufficiently different learners help each other learn by labelingtraining data for one another.
The work of Sarkar (2001) and Steedman, Osborne, etal.
(2003) suggests that co-training can be helpful for statistical parsing.
Pierce andCardie (2001) have shown, in the context of base noun identification, that combiningsample selection and cotraining can be an effective learning framework for large-scaletraining.
Similar approaches are being explored for parsing (Steedman, Hwa, et al2003; Hwa et al 2003).6.
ConclusionIn this article, we have argued that sample selection is a powerful learning techniquefor reducing the amount of human-labeled training data.
Our empirical studies suggestthat sample selection is helpful not only for binary classification tasks such as PP-attachment, but also for applications that generate complex outputs such as syntacticparsing.We have proposed several criteria for predicting the training utility of the unla-beled candidates and developed evaluation functions to rank them.
We have conductedexperiments to compare the functions?
ability to select the most helpful training exam-ples.
We have found that the uncertainty criterion is a good predictor that consistentlyfinds helpful examples.
In our experiments, evaluation functions that factor in theuncertainty criterion consistently outperform the baseline of random selection acrossdifferent tasks and learning algorithms.
For learning a PP-attachment model, the mosthelpful evaluation function is a hybrid that factors in the prediction performance of thehypothesis and the confidence for the values of the parameters of the hypothesis.
Fortraining a parser, we found that uncertainty-based evaluation functions that use treeentropy were the most helpful for both the EM-based learner and the history-basedlearner.The current work points us in several future directions.
First, we shall continueto develop alternative formulations of evaluation functions to improve the learn-ing rates of parsers.
Under the current framework, we did not experiment with anyhypothesis-parameter-based evaluation functions for the parser induction task; how-273Hwa Sample Selection for Statistical Parsingever, hypothesis-parameter-based functions may be feasible under a multilearner set-ting, using parallel machines.
Second, while in this work we focused on selectingentire sentences as training examples, we believe that further reduction in the amountof annotated training data might be possible if the system could ask the annotatorsmore-specific questions.
For example, if the learner is unsure only of a local decisionwithin a sentence (such as a PP-attachment ambiguity), the annotator should not haveto label the entire sentence.In order to allow for finer-grained interactions between the system and the an-notators, we have to address some new challenges.
To begin with, we must weighin other factors in addition to the amount of annotations.
For instance, the learnermay ask about multiple substrings in one sentence.
Even if the total number of la-bels were fewer, the same sentence would still need to be mentally processed by theannotators multiple times.
This situation is particularly problematic when there arevery few annotators, as it becomes much more likely that a person will encounter thesame sentence many times.
Moreover, we must ensure that the questions asked by thelearner are well-formed.
If the learner were simply to present the annotator with somesubstring that it could not process, the substring might not form a proper linguisticconstituent for the annotator to label.
Additionally, we are interested in exploring theinteraction between sample selection and other semisupervised approaches such asboosting, reranking, and cotraining.
Finally, based on our experience with parsing, webelieve that active-learning techniques may be applicable to other tasks that producecomplex outputs such as machine translation.Appendix: Efficient Computation of Tree EntropyAs discussed in Section 4.1.2, for learning tasks such as parsing, the number of possi-ble classifications is so large that it may not be computationally efficient to calculatethe degree of uncertainty using the tree entropy definition.
In the equation for the treeentropy of w (TE(w, G)) presented in Section 4.1.2, the computation requires summingover all possible parses, but the number of possible parses for a sentence grows ex-ponentially with respect to the sentence length.
In this appendix, we show that treeentropy can be efficiently computed using dynamic programming.For illustrative purposes, we describe the computation process using a PCFG ex-pressed in Chomsky normal form.14 The basic idea is to compose the tree entropy ofthe entire sentence from the tree entropy of the subtrees.
The process is similar tothat for computing the probability of the entire sentence from the probabilities of sub-strings (called Inside Probabilities).
We follow the notation convention of Lari andYoung (1990).The inside probability of a nonterminal X generating the substring wi .
.
.wj isdenoted as e(X, i, j); it is the sum of the probabilities of all possible subtrees that haveX as the root and wi .
.
.wj as the leaf nodes.
We define a new function h(X, i, j) torepresent the corresponding entropy for the substring:h(X, i, j) = ?
?x?X ?
?wi...wjP(x | G) lg(P(x | G))where G is the current model.
Under this notation, the tree entropy of a sentence,?v?VP(v | G) lg P(v | G), is denoted as h(S, 1, n).14 That is, every production rule must be in one of two forms: a nonterminal expands into two morenonterminals, or a nonterminal expands into a terminal.274Computational Linguistics Volume 30, Number 3Analogously to the computation of inside probabilities, we compute h(X, i, j) re-cursively.
The base case is when the nonterminal X generates a single token substringwi.
The only possible tree has X at the root, immediately dominating the leaf node wi.Therefore, the tree entropy ish(X, i, i) = e(X, i, i) lg(e(X, i, i))For the general case, h(X, i, j), we must find all rules of the form X ?
YZ, where Y andZ are nonterminals, that have contributed toward X ??
wi .
.
.wj.
To do so, we considerall possible ways dividing up wi .
.
.wj into two pieces such that Y??
wi .
.
.wk andZ ??
wk+1 .
.
.wj:h(X, i, j) =j?1?k=i?
(X?YZ)hY,Z,k(X, i, j)The function hY,Z,k(X, i, j) is a portion of h(X, i, j) that accounts for those parses in whichthe rule X ?
YZ is used and the division point is at word wk.
The nonterminals Y andZ may, in turn, generate their substrings with multiple parses.
Let Y represent the setof parses for Y ??
wi .
.
.wk; let Z represent the set of parses for Z??
wk+1 .
.
.wj; andlet x represent the parse step of X ?
YZ.
Then, there are a total of ?Y?
?
?Z?
parses,and the probability of each parse is P(x)P(y)P(z), where y ?
Y and z ?
Z .
To computehY,Z,k, we need to sum over all possible parses:hY,Z,k(X, i, j) = ?
?y?Y ,z?ZP(x)P(y)P(z) lg(P(x)P(y)P(z))= ?
?y?Y ,z?ZP(x)P(y)P(z)(lg P(x) + lg P(y) + lg P(z))= ?P(x) lg(P(x))e(Y, i, k)e(Z, k + 1, j) + P(x)h(Y, i, k)e(Z, k + 1, j)+P(x)e(Y, i, k)h(Z, k + 1, j)Thus, the tree entropy of the entire sentence can be recursively computed from theentropy values of the substrings.AcknowledgmentsWe thank Joshua Goodman, Lillian Lee,Wheeler Ruml, and Stuart Shieber forhelpful discussions, and Ric Crabbe, PhilipResnik, and the reviewers for theirconstructive comments on this article.Portions of this work have appearedpreviously (Hwa 2000, 2001b); we thank thereviewers of those papers for their helpfulcomments.
Parts of this work was carriedout while the author was a graduate studentat Harvard University, supported by theNational Science Foundation under GrantNo.
IRI 9712068.
The work is also supportedby the Department of Defense contractRD-02-5700, and ONR MURI ContractFCPO.810548265.ReferencesBanko, Michele and Eric Brill.
2001.
Scalingto very very large corpora for naturallanguage disambiguation.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics, Toulouse,France, pages 26?33.Blum, Avrim and Tom Mitchell.
1998.Combining labeled and unlabeled datawith co-training.
In Proceedings of the 1998Conference on Computational LearningTheory, pages 92?100, Madison, WI.Brill, Eric and Philip S. Resnik.
1994.
A rulebased approach to PP attachmentdisambiguation.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING), Kyoto, Japan, pages1198?1204.275Hwa Sample Selection for Statistical ParsingCharniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of the First Meeting of the NorthAmerican Association for ComputationalLinguistics, Seattle.Cohn, David, Les Atlas, and RichardLadner.
1994.
Improving generalizationwith active learning.
Machine Learning,15(2):201?221.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics,pages 16?23, Madrid.Collins, Michael.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania,Philadelphia.Collins, Michael and James Brooks.
1995.Prepositional phrase attachment througha backed-off model.
In Proceedings of theThird Workshop on Very Large Corpora,Cambridge, MA, pages 27?38.Cover, Thomas M. and Joy A. Thomas.1991.
Elements of Information Theory.
JohnWiley, New York.Engelson, Sean P. and Ido Dagan.
1996.Minimizing manual annotation cost insupervised training from corpora.
InProceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics,Santa Cruz, CA, pages 319?326.Freund, Yoav, H. Sebastian Seung, EliShamir, and Naftali Tishby.
1997.
Selectivesampling using the query by committeealgorithm.
Machine Learning,28(2?3):133?168.Fujii, Atsushi, Kentaro Inui, TakenobuTokunaga, and Hozumi Tanaka.
1998.Selective sampling for example-basedword sense disambiguation.
ComputationalLinguistics, 24(4):573?598.Henderson, John C. and Eric Brill.
2000.Bagging and boosting a treebank parser.In Proceedings of the First Meeting of theNorth American Association for ComputationalLinguistics, Seattle, pages 34?41.Hwa, Rebecca.
1998.
An empiricalevaluation of probabilistic lexicalized treeinsertion grammars.
In Proceedings of the36th Annual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, Montreal, volume 1, pages557?563.Hwa, Rebecca.
2000.
Sample selection forstatistical grammar induction.
InProceedings of 2000 Joint SIGDAT Conferenceon Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages45?52, Hong Kong, October.Hwa, Rebecca.
2001a.
Learning ProbabilisticLexicalized Grammars for Natural LanguageProcessing.
Ph.D. thesis, HarvardUniversity, Cambridge, MA.Hwa, Rebecca.
2001b.
On minimizingtraining corpus for parser acquisition.
InProceedings of the ACL 2001 Workshop onComputational Natural Language Learning(ConLL-2001), Toulouse, France, pages84?89.Hwa, Rebecca, Miles Osborne, AnoopSarkar, and Mark Steedman.
2003.Corrected co-training for statisticalparsers.
In Proceedings of the ICMLWorkshop on the Continuum from Labeled toUnlabeled Data in Machine Learning and DataMining at the 20th International Conference ofMachine Learning (ICML-2003),Washington, DC, pages 95?102, August.Joshi, Aravind K., Leon S. Levy, andMasako Takahashi.
1975.
Tree adjunctiongrammars.
Journal of Computer and SystemSciences, 10(1): 136?163.Lari, Karim A. and Steve J.
Young.
1990.The estimation of stochastic context-freegrammars using the inside-outsidealgorithm.
Computer Speech and Language,4:35?56.Larsen, Richard J. and Morris L. Marx.
1986.An Introduction to Mathematical Statisticsand Its Applications.
Prentice-Hall,Englewood Cliffs, NJ.Lewis, David D. and Jason Catlett.
1994.Heterogeneous uncertainty sampling forsupervised learning.
In Proceedings of theEleventh International Conference on MachineLearning, San Francisco, pages 148?156.Magerman, David.
1994.
Natural LanguageParsing as Statistical Pattern Recognition.Ph.D.
thesis, Stanford University,Stanford, CA.Marcus, Mitchell, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Ngai, Grace and David Yarowsky.
2000.Rule writing or annotation: Cost-efficientresource usage for base noun phrasechunking.
In Proceedings of the 38th AnnualMeeting of the Association for ComputationalLinguistics, pages 117?125, Hong Kong,October.Pereira, Fernando C. N. and Yves Schabes.1992.
Inside-outside reestimation frompartially bracketed corpora.
In Proceedingsof the 30th Annual Meeting of the Associationfor Computational Linguistics, pages128?135, Newark, DE.276Computational Linguistics Volume 30, Number 3Pierce, David and Claire Cardie.
2001.Limitations of co-training for naturallanguage learning from large datasets.
InProceedings of the 2001 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-2001), pages 1?9,Pittsburgh, PA.Ratnaparkhi, Adwait.
1998.
Statisticalmodels for unsupervised prepositionalphrase attachment.
In Proceedings of the36th Annual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, Montreal, volume 2, pages1079?1085.Sarkar, Anoop.
2001.
Applying co-trainingmethods to statistical parsing.
InProceedings of the Second Meeting of theNorth American Association forComputational Linguistics, Pittsburgh,pages 175?182, June.Schabes, Yves and Richard Waters.
1993.Stochastic lexicalized context-freegrammar.
In Proceedings of the ThirdInternational Workshop on ParsingTechnologies, Tilburg, The Netherlands,and Durbuy, Belgium, pages 257?266.Steedman, Mark, Rebecca Hwa, StephenClark, Miles Osborne, Anoop Sarkar, JuliaHockenmaier, Paul Ruhlen, Steven Baker,and Jeremiah Crim.
2003.
Exampleselection for bootstrapping statisticalparsers.
In Proceedings of the JointConference of Human Language Technologiesand the Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, Edmonton,Alberta, Canada, pages 236?243.Steedman, Mark, Miles Osborne, AnoopSarkar, Stephen Clark, Rebecca Hwa, JuliaHockenmaier, Paul Ruhlen, Steven Baker,and Jeremiah Crim.
2003.
Bootstrappingstatistical parsers from small datasets.
InProceedings of the Tenth Conference of theEuropean Chapter of the Association forComputational Linguistics, Budapest, pages331?338.Tang, Min, Xiaoqiang Luo, and SalimRoukos.
2002.
Active learning forstatistical natural language parsing.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,Philadelphia, pages 120?127, July.Thompson, Cynthia A., Mary Elaine Califf,and Raymond J. Mooney.
1999.
Activelearning for natural language parsing andinformation extraction.
In Proceedings ofthe Sixteenth International Conference onMachine Learning (ICML-99), pages406?414, Bled, Slovenia.Van Rijsbergen, Cornelis J.
1979.
InformationRetrieval.
Butterworth, London.
