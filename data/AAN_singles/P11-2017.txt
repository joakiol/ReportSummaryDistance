Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 95?100,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJoint Identification and Segmentation of Domain-Specific Dialogue Acts forConversational Dialogue SystemsFabrizio Morbini and Kenji SagaeInstitute for Creative TechnologiesUniversity of Southern California12015 Waterfront Drive, Playa Vista, CA 90094{morbini,sagae}@ict.usc.eduAbstractIndividual utterances often serve multiplecommunicative purposes in dialogue.
Wepresent a data-driven approach for identifica-tion of multiple dialogue acts in single utter-ances in the context of dialogue systems withlimited training data.
Our approach results insignificantly increased understanding of userintent, compared to two strong baselines.1 IntroductionNatural language understanding (NLU) at the levelof speech acts for conversational dialogue systemscan be performed with high accuracy in limited do-mains using data-driven techniques (Bender et al,2003; Sagae et al, 2009; Gandhe et al, 2008, forexample), provided that enough training material isavailable.
For most systems that implement novelconversational scenarios, however, enough exam-ples of user utterances, which can be annotated asNLU training data, only become available once sev-eral users have interacted with the system.
This situ-ation is typically addressed by bootstrapping from arelatively small set of hand-authored utterances thatperform key dialogue acts in the scenario or fromutterances collected from wizard-of-oz or role-playexercises, and having NLU accuracy increase overtime as more users interact with the system and moreutterances are annotated for NLU training.While this can be effective in practice for ut-terances that perform only one of several possiblesystem-specific dialogue acts (often several dozens),longer utterances that include multiple dialogue actspose a greater challenge: the many available combi-nations of dialogue acts per utterance result in sparsecoverage of the space of possibilities, unless a verylarge amount of data can be collected and anno-tated, which is often impractical.
Users of the dia-logue system, whose utterances are collected for fur-ther NLU improvement, tend to notice that portionsof their longer utterances are ignored and that theyare better understood when they express themselveswith simpler sentences.
This results in generation ofdata heavily skewed towards utterances that corre-spond to a single dialogue act, making it difficult tocollect enough examples of utterances with multipledialogue acts to improve NLU, which is preciselywhat would be needed to make users feel more com-fortable with using longer utterances.We address this chicken-and-egg problem with adata-driven NLU approach that segments and iden-tifies multiple dialogue acts in single utterances,even when only short (single dialogue act) utter-ances are available for training.
In contrast to previ-ous approaches that assume the existence of enoughtraining data for learning to segment utterances,e.g.
(Stolcke and Shriberg, 1996), or to align spe-cific words to parts of the formal representation,e.g.
(Bender et al, 2003), our framework requires arelatively small dataset, which may not contain anyutterances with multiple dialogue acts.
This makes itpossible to create new conversational dialogue sys-tem scenarios that allow and encourage users to ex-press themselves with fewer restrictions, without anincreased burden in the collection and annotation ofNLU training data.2 MethodGiven (1) a predefined set of possible dialogue actsfor a specific dialogue system, (2) a set of utterances95each annotated with a single dialogue act label, and(3) a classifier trained on this annotated utterance-label set, which assigns for a given word sequence adialogue act label with a corresponding confidencescore, our task is to find the best sequence of dia-logue acts that covers a given input utterance.
Whileshort utterances are likely to be covered entirely by asingle dialogue act that spans all of its words, longerutterances may be composed of spans that corre-spond to different dialogue acts.bestDialogueActEndingAt(Text,pos) beginif pos < 0 thenreturn ?pos, ?null, 1??
;endS = {};for j = 0 to pos do?c, p?
= classify(words(Text, j, pos));S = S ?
{?j, ?c, p??
};endreturn argmax?k,?c,p??
?S{p ?
p?
: ?h, ?c?, p???
=bestDialogueActEndingAt(Text, k ?
1)};endAlgorithm 1: The function classify(T ) calls thesingle dialogue act classifier subsystem on the in-put text T and returns the highest scoring dia-logue act label c with its confidence score p. Thefunction words(T, i, j) returns the string formedby concatenating the words in T from the ith tothe jth included.
To obtain the best segmenta-tion of a given text, one has to work its way backfrom the end of the text: start by calling ?k, ?c, p?
?= bestDialogueActEndingAt(Text, numWords),where numWords is the number of wordsin Text.
If k > 0 recursively callbestDialogueActEndingAt(Text, k ?
1) to obtainthe optimal dialogue act ending at k ?
1.Algorithm 1 shows our approach for using a sin-gle dialogue act classifier to extract the sequence ofdialogue acts with the highest overall score from agiven utterance.
The framework is independent ofthe particular subsystem used to select the dialogueact label for a given segment of text.
The constraintis that this subsystem should return, for a given se-quence of words, at least one dialogue act label andits confidence level in a normalized range that canbe used for comparisons with subsequent runs.
Inthe work reported in this paper, we use an existingdata-driven NLU module (Sagae et al, 2009), de-veloped for the SASO virtual human dialogue sys-tem (Traum et al, 2008b), but retrained using thedata described in section 3.
This NLU module per-forms maximum entropy multiclass classification,using features derived from the words in the inpututterance, and using dialogue act labels as classes.The basic idea is to find the best segmentation(that is, the one with the highest score) of the portionof the input text up to the ith word.
The base case Siwould be for i = 1 and it is the result of our classi-fier when the input is the single first word.
For anyother i > 1 we construct all word spans Tj,i of theinput text, containing the words from j to i, where1 ?
j ?
i, then we classify each of the Tj,i andpick the best returned class (dialogue act label) Cj,i(and associated score, which in the case of our maxi-mum entropy classifier is the conditional probabilityScore(Cj,i) = P (Cj,i|Tj,i)).
Then we assign to thebest segmentation ending at i, Si, the label Ck,i iff:k = argmax1?h?i(Score(Ch,i) ?
Score(Sh?1))(1)Algorithm 1 calls the classifier O(n2) where nis the number of words in the input text.
Notethat, as in the maximum entropy NLU of Bender etal.
(2003), this search uses the ?maximum approxi-mation,?
and we do not normalize over all possiblesequences.
Therefore, our scores are not true proba-bilities, although they serve as a good approximationin the search for the best overall segmentation.We experimented with two other variations ofthe argument of the argmax in equation 1: (1) in-stead of considering Score(Sh?1), consider onlythe last segment contained in Sh?1; and (2) insteadof using the product of the scores of all segments,use the average score per segment: (Score(Ch,i) ?Score(Sh?1))1/(1+N(Sh?1)) where N(Si) is thenumber of segments in Si.
These variants producesimilar results; the results reported in the next sec-tion were obtained with the second variant.3 Evaluation3.1 DataTo evaluate our approach we used data collectedfrom users of the TACQ (Traum et al, 2008a) dia-96logue system, as described by Artstein et al (2009).Of the utterances in that dataset, about 30% are an-notated with multiple dialogue acts.
The annotationalso contains for each dialogue act the correspond-ing segment of the input utterance.The dataset contains a total of 1,579 utterances.Of these, 1,204 utterances contain only a single di-alogue act, and 375 utterances contain multiple dia-logue acts, according to manual dialogue act anno-tation.
Within the set of utterances that contain mul-tiple dialogue acts, the average number of dialogueacts per utterance is 2.3.The dialogue act annotation scheme uses a totalof 77 distinct labels, with each label correspondingto a domain-specific dialogue act, including somesemantic information.
Each of these 77 labels iscomposed at least of a core speech act type (e.g.wh-question, offer), and possibly also attributes thatreflect semantics in the domain.
For example, thedialogue act annotation for the utterance What isthe strange man?s name?
would be whq(obj:strangeMan, attr: name), reflecting thatit is a wh-question, with a specific object and at-tribute.
In the set of utterances with only one speechact, 70 of the possible 77 dialogue act labels areused.
In the remaining utterances (which containmultiple speech acts per utterance), 59 unique dia-logue act labels are used, including 7 that are notused in utterances with only a single dialogue act(these 7 labels are used in only 1% of those utter-ances).
A total of 18 unique labels are used onlyin the set of utterances with one dialogue act (theselabels are used in 5% of those utterances).
Table 1shows the frequency information for the five mostcommon dialogue act labels in our dataset.The average number of words in utterances withonly a single dialogue act is 7.5 (with a maximumof 34, and minimum of 1), and the average length ofutterances with multiple dialogue acts is 15.7 (max-imum of 66, minimum of 2).
To give a better idea ofthe dataset used here, we list below two examples ofutterances in the dataset, and their dialogue act an-notation.
We add word indices as subscripts in theutterances for illustration purposes only, to facilitateidentification of the word spans for each dialogueact.
The annotation consists of a word interval and aSingle DA Utt.
[%] Multiple DA Utt.
[%]Wh-questions 51 Wh-questions 31Yes/No-questions 14 Offers to agent 24Offers to agent 9 Yes answer 11Yes answer 7 Yes/No-questions 8Greeting 7 Thanks 7Table 1: The frequency of the dialogue act classes mostused in the TACQ dataset (Artstein et al, 2009).
Theleft column reports the statistics for the set of utterancesannotated with a single dialogue act the right those for theutterances annotated with multiple dialogue acts.
Eachdialogue act class typically contains several more specificdialogue acts that include domain-specific semantics (forexample, there are 29 subtypes of wh-questions that canbe performed in the domain, each with a separate domain-specific dialogue act label).dialogue act label1.1.
?
0 his 1 name, 2 any 3 other 4 informa-tion 5 about 6 him, 7 where 8 he 9 lives10?
is labeled with: [0 2] whq(obj:strangeMan, attr: name), [2 7]whq(obj: strangeMan) and [7 10]whq(obj: strangeMan, attr:location).2. ?
0 I 1 can?t 2 offer 3 you 4 money 5 but 6 I 7 can8 offer 9 you 10 protection 11?
is labeled with:[0 5] reject, [5 11] offer(safety).3.2 SetupIn our experiments, we performed 10-fold cross-validation using the dataset described above.
Forthe training folds, we use only utterances with a sin-gle dialogue act (utterances containing multiple dia-logue acts are split into separate utterances), and thetraining procedure consists only of training a max-imum entropy text classifier, which we use as oursingle dialogue act classifier subsystem.For each evaluation fold we run the procedure de-scribed in Section 2, using the classifier obtainedfrom the corresponding training fold.
The segmentspresent in the manual annotation are then alignedwith the segments identified by our system (the1Although the dialogue act labels could be thought of ascompositional, since they include separate parts, we treat themas atomic labels.97alignment takes in consideration both the word spanand the dialogue act label associated to each seg-ment).
The evaluation then considers as correct onlythe subset of dialogue acts identified automaticallythat were successfully aligned with the same dia-logue act label in the gold-standard annotation.We compared the performance of our proposedapproach to two baselines; both use the same max-imum entropy classifier used internally by our pro-posed approach.1.
The first baseline simply uses the single dia-logue act label chosen by the maximum entropyclassifier as the only dialogue act for each ut-terance.
In other words, this baseline corre-sponds to the NLU developed for the SASO di-alogue system (Traum et al, 2008b) by Sagaeet al (2009)2.
This baseline is expected to havelower recall for those utterances that containmultiple dialogue acts, but potentially higherprecision overall, since most utterances in thedataset contain only one dialogue act label.2.
For the second baseline, we treat multiple dia-logue act detection as a set of binary classifica-tion tasks, one for each possible dialogue act la-bel in the domain.
We start from the same train-ing data as above, and create N copies, whereN is the number of unique dialogue acts labelsin the training set.
Each utterance-label pair inthe original training set is now present in all Ntraining sets.
If in the original training set an ut-terance was labeled with the ith dialogue act la-bel, now it will be labeled as a positive examplein the ith training set and as a negative exam-ple in all other training sets.
Binary classifiersfor each N dialogue act labels are then trained.During run-time, each utterance is classified byall N models and the result is the subset of di-alogue acts associated with the models that la-beled the example as positive.
This baseline isexcepted to be much closer in performance toour approach, but it is incapable of determiningwhat words in the utterance correspond to eachdialogue act3.2We do not use the incremental processing version of theNLU described by Sagae et al, only the baseline NLU, whichconsist only of a maximum entropy classifier.3This corresponds to the transformation of a multi-labelP [%] R [%] F [%]Single this 73 77 752ndbl 86 71 781stbl 82 77 80Multiple this 87 66 752ndbl 85 55 671stbl 91 39 55Overall this 78 72 752ndbl 86 64 731stbl 84 61 71Table 2: Performance on the TACQ dataset obtained byour proposed approach (denoted by ?this?)
and the twobaseline methods.
Single indicates the performance whentested only on utterances annotated with a single dialogueact.
Multiple is for utterances annotated with more thanone dialogue act, and Overall indicates the performanceover the entire set.
P stands for precision, R for recall,and F for F-score.3.3 ResultsTable 2 shows the performance of our approach andthe two baselines.
All measures show that the pro-posed approach has considerably improved perfor-mance for utterances that contain multiple dialogueacts, with only a small increase in the number of er-rors for the utterances containing only a single dia-logue act.
In fact, even though more than 70% ofthe utterances in the dataset contain only a single di-alogue act, our approach for segmenting and iden-tifying multiple dialogue acts increases overall F-score by about 4% when compared to the first base-line and by about 2% when compared to the sec-ond (strong) baseline, which suffers from the addi-tional deficiency of not identifying what spans cor-respond to what dialogue acts.
The differences inF-score over the entire dataset (shown in the Over-all portion of Table 2) are statistically significant(p < 0.05).
As a drawback of our approach, itis on average 25 times slower than our first base-line, which is incapable of identifying multiple di-alogue acts in a utterance4.
Our approach is stillabout 15% faster than our second baseline, whichclassification problem into several binary classifiers, describedas PT4 by Tsoumakas and Katakis (?
).4In our dataset, our method takes on average about 102msto process an utterance that was originally labeled with multipledialogue acts, and 12ms to process one annotated with a singledialogue act.9801002003004005000 10 20 30 40 50 60 70Executiontime[ms]Histogram(numberofutterances)Number of words in input textthis1stbl2ndblhistogramFigure 1: Execution time in milliseconds of the classifierwith respect to the number of words in the input text.identifies multiple speech acts, but without segmen-tation, and with lower F-score.
Figure 1 shows theexecution time versus the length of the input text.
Italso shows a histogram of utterance lengths in thedataset, suggesting that our approach is suitable formost utterances in our dataset, but may be too slowfor some of the longer utterances (with 30 words ormore).Figure 2 shows the histogram of the average error(absolute value of word offset) in the start and endof the dialogue act segmentation.
Each dialogue actidentified by Algorithm 1 is associated with a start-ing and ending index that corresponds to the por-tion of the input text that has been classified withthe given dialogue act.
During the evaluation, wefind the best alignment between the manual annota-tion and the segmentation we computed.
For eachof the aligned pairs (i.e.
extracted dialogue act anddialogue act present in the annotation) we computethe absolute error between the starting point of theextracted dialogue act and the starting point of thepaired annotation.
We do the same for the endingpoint and we average the two error figures.
Theresult is binned to form the histogram displayed infigure 2.
The figure also shows the average errorand the standard deviation.
The largest average er-ror happens with the data annotated with multipledialogue acts.
In that case, the extracted segmentshave a starting and ending point that in average aremisplaced by about ?2 words.4 ConclusionWe described a method to segment a given utter-ance into non-overlapping portions, each associated0 1 2 3 4 5 6 7 8 9 10Average error in the starting and ending indexes of each speech act segmentAll data: ?=1.07 ?=1.69Single speech act: ?=0.72 ?=1.12Multiple speech acts: ?=1.64 ?=2.22Figure 2: Histogram of the average absolute error in thetwo extremes (i.e.
start and end) of segments correspond-ing to the dialogue acts identified in the dataset.with a dialogue act.
The method addresses the prob-lem that, in development of new scenarios for con-versational dialogue systems, there is typically notenough training data covering all or most configu-rations of how multiple dialogue acts appear in sin-gle utterances.
Our approach requires only labeledutterances (or utterance segments) corresponding toa single dialogue act, which tends to be the easiesttype of training data to author and to collect.We performed an evaluation using existing dataannotated with multiple dialogue acts for each utter-ance.
We showed a significant improvement in over-all performance compared to two strong baselines.The main drawback of the proposed approach is thecomplexity of the segment optimization that requirescalling the dialogue act classifier O(n2) times withn representing the length of the input utterance.
Thebenefit, however, is that having the ability to identifymultiple dialogue acts in utterances takes us one stepcloser towards giving users more freedom to expressthemselves naturally with dialogue systems.AcknowledgmentsThe project or effort described here has been spon-sored by the U.S. Army Research, Development,and Engineering Command (RDECOM).
State-ments and opinions expressed do not necessarily re-flect the position or the policy of the United StatesGovernment, and no official endorsement should beinferred.
We would also like to thank the anonymousreviewers for their helpful comments.99ReferencesRon Artstein, Sudeep Gandhe, Michael Rushforth, andDavid R. Traum.
2009.
Viability of a simple dialogueact scheme for a tactical questioning dialogue system.In DiaHolmia 2009: Proceedings of the 13th Work-shop on the Semantics and Pragmatics of Dialogue,page 43?50, Stockholm, Sweden, June.Oliver Bender, Klaus Macherey, Franz Josef Och, andHermann Ney.
2003.
Comparison of alignment tem-plates and maximum entropy models for natural lan-guage understanding.
In Proceedings of the tenthconference on European chapter of the Associationfor Computational Linguistics - Volume 1, EACL ?03,pages 11?18, Stroudsburg, PA, USA.
Association forComputational Linguistics.Sudeep Gandhe, David DeVault, Antonio Roque, BilyanaMartinovski, Ron Artstein, Anton Leuski, JillianGerten, and David R. Traum.
2008.
From domainspecification to virtual humans: An integrated ap-proach to authoring tactical questioning characters.In Proceedings of Interspeech, Brisbane, Australia,September.Kenji Sagae, Gwen Christian, David DeVault, andDavid R. Traum.
2009.
Towards natural languageunderstanding of partial speech recognition results indialogue systems.
In Short Paper Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics - Human Language Technolo-gies (NAACL HLT) 2009 conference.Andreas Stolcke and Elizabeth Shriberg.
1996.
Au-tomatic linguistic segmentation of conversationalspeech.
In Proc.
ICSLP, pages 1005?1008.David R. Traum, Anton Leuski, Antonio Roque, SudeepGandhe, David DeVault, Jillian Gerten, Susan Robin-son, and Bilyana Martinovski.
2008a.
Natural lan-guage dialogue architectures for tactical questioningcharacters.
In Army Science Conference, Florida,12/2008.David R. Traum, Stacy Marsella, Jonathan Gratch, JinaLee, and Arno Hartholt.
2008b.
Multi-party, multi-issue, multi-strategy negotiation for multi-modal vir-tual agents.
In IVA, pages 117?130.100
