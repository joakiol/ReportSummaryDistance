Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 177?184Manchester, August 2008Pedagogically Useful Extractive Summaries for Science EducationSebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara SumnerInstitute of Cognitive ScienceDepartment of Computer ScienceUniversity of Colorado at Bouldersebastian.delachica, faisal.ahmad, james.martin,tamara.sumner@colorado.eduAbstractThis paper describes the design andevaluation of an extractive summarizerfor educational science content calledCOGENT.
COGENT extends MEADbased on strategies elicited from an em-pirical study with science domain and in-structional design experts.
COGENTidentifies sentences containing pedagogi-cally relevant concepts for a specific sci-ence domain.
The algorithms pursue ahybrid approach integrating both domainindependent bottom-up sentence scoringfeatures and domain-aware top-down fea-tures.
Evaluation results indicate thatCOGENT outperforms existing summar-izers and generates summaries thatclosely resemble those generated by hu-man experts.
COGENT concept invento-ries appear to also support the computa-tional identification of student miscon-ceptions about earthquakes and plate tec-tonics.1 IntroductionMultidocument summarization (MDS) researchefforts have resulted in significant advancementsin algorithm and system design (Mani, 2001).Many of these efforts have focused on summariz-ing news articles, but not significantly exploredthe research issues arising from processing edu-cational content to support pedagogical applica-tions.
This paper describes our research into theapplication of MDS techniques to educational?
2008.
Licensed under the Creative Commons At-tribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.science content to generate pedagogically usefulsummaries.Knowledge maps are graphical representationsof domain information laid out as networks ofnodes containing rich concept descriptions inter-connected using a fixed set of relationship types(Holley and Dansereau, 1984).
Knowledge mapsare a variant of the concept maps used to capture,assess, and track student knowledge in educationresearch (Novak and Gowin, 1984).
Learningresearch indicates that knowledge maps may beuseful cognitive scaffolds, helping users lackingdomain expertise to understand the macro-levelstructure of an information space (O'Donnell etal., 2002).
Knowledge maps have emerged as aneffective representation to generate conceptualbrowsers that help students navigate educationaldigital libraries, such as the Digital Library forEarth System Education (DLESE.org) (Butcheret al, 2006).
In addition, knowledge maps haveproven useful for domain and instructional ex-perts to capture domain knowledge from digitallibrary resources and to analyze student under-standing for the purposes of providing formativeassessments (Ahmad et al, 2007).Knowledge maps have proven useful both asrepresentations of knowledge for assessmentpurposes and as learning resources for presenta-tion to students.
However, domain knowledgemap construction by experts is an expensiveknowledge engineering activity.
In this paper, wedescribe our progress towards the automatedgeneration of pedagogically useful extractivesummaries from educational texts about a sci-ence domain.
In the context of automated knowl-edge map generation, summary sentences corre-spond to concepts.
While the detection of rela-tionships between concepts is also part of ouroverall research agenda, this paper focuses solelyon concept identification using MDS techniques.The remainder of this paper is organized as fol-177lows.
First, we review related work in the areasof automated concept extraction from texts andextractive summarization.
We then describe theempirical study we have conducted to understandhow domain and instructional design expertsidentify pedagogically important science con-cepts in educational digital library resources.Next, we provide a detailed description of thealgorithms we have designed based on expertstrategies elicited from our empirical study.
Wethen present and discuss our evaluation resultsusing automated summarization metrics and hu-man judgments.
Finally, we present our conclu-sions and future work in this area.2 Related WorkOur work is informed by efforts to automate theacquisition of ontology concepts from text.
On-toLearn (Navigli and Velardi, 2004) extracts do-main terminology from a collection of texts usinga syntactic parse to identify candidate terms thatare filtered based on domain relevance and con-nected using a semantic interpretation based onword sense disambiguation.
The newly identifiedconcepts and relationships are used to update anexisting ontology.
Knowledge Puzzle focuses onn-grams to produce candidate terms filteredbased on term frequency in the input documentsand on the number of relationships associatedwith a given term (Zouaq et al, 2007).
This ap-proach leverages pattern extraction techniques toidentify concepts and relationships.
While theseapproaches produce ontologies useful for compu-tational purposes, the identified concepts are of avery fine granularity and therefore may yieldgraphs not suitable for identifying student mis-conceptions or for presentation back to the stu-dent.
Clustering by committee has also been usedto discover concepts from a text by groupingterms into conceptually related clusters (Lin andPantel, 2002).
The resulting clusters appear to betightly related, but operate at a very fine level ofgranularity.
Our approach focuses on sentencesas units of knowledge to produce concise repre-sentations that may be useful both as computa-tional objects and as learning resources to presentback to the student.
Therefore, extractive sum-marization research also informs our work.Topic representation and topic themes havebeen used to explore promising MDS techniques(Harabagiu and Lacatusu, 2005).
Recent effortsin graph-based MDS have integrated sentenceaffinity, information richness and diversity pen-alties to produce very promising results (Wanand Yang, 2006).
Finally, MEAD is a widelyused multi-document summarization and evalua-tion platform (Radev et al, 2000).
MEAD re-search efforts have resulted in significant contri-butions to support the development of summari-zation applications (Radev et al, 2000).
Whileall these systems have produced promising re-sults in automated evaluations, none have di-rectly targeted educational content as input or thegeneration of pedagogically useful summaries.We are directly building upon MEAD due itsfocus on sentence extraction and its high degreeof modularization.3 Empirical StudyWe have conducted a study to capture how hu-man experts construct and use knowledge maps.In this 10-month study, we examined how ex-perts created knowledge maps from educationaldigital libraries and how they used the maps toassess student work and provide personalizedfeedback.In this paper, we are focusing on the knowl-edge map construction aspects of the study.
Fourgeology and instructional design experts collabo-ratively selected 20 resources from DLESE toconstruct a domain knowledge map on earth-quakes and plates tectonics for high school agelearners.
The experts independently createdknowledge maps of individual resources whichthey collaboratively merged into the final domainknowledge map in a one-day workshop.
The re-sulting domain knowledge map consisted of 564nodes containing domain concepts and 578 rela-tionships.
The concepts consist of 7,846 words,or 5% of the total number of words in the origi-nal resources.
Figure 1 shows a fragment of thedomain knowledge map created by our experts.Figure 1.
Fragment of domain  knowledge mapcreated by domain and instructional expertsExperts created nodes containing concepts ofvarying granularity, including nouns, nounphrases, partial sentences, single sentences, and178multiple sentences.
Our analysis of this domainknowledge map indicates that experts relied oncopying-and-pasting (58%) and paraphrasing(37%) to create most domain concepts.
Only 5%of the nodes could not be traced directly to theoriginal resources.Experts used relationship types in a Zipf-likedistribution with the top 10 relationship typesaccounting for 64% of all relationships.
The top2 relationship types each accounted for morethan 10% of all relationships: elaborations (19%or 110 links) and examples (14% or 78 links).We have established the completeness of thisdomain knowledge map by asking a domain ex-pert to assess its content coverage of nationally-recognized educational goals on earthquakes andplate tectonics for high school age learners usingthe American Association for the Advancementof Science (AAAS) Benchmarks (Project 2061,1993).
The results indicate adequate content cov-erage of the relevant AAAS Benchmarks achievedthrough 82 of the concepts (15%) with the re-maining 482 concepts (85%) providing very de-tailed elaborations of the associated learninggoals.Qualitative analysis of the verbal protocolscaptured during the study indicates that all ex-perts used external sources to construct the do-main knowledge map.
Experts made referencesto their own knowledge (e.g., ?I know that??
),to content learned or taught in geology courses,to other resources used in the study, and to theNational Science Education Standards (NSES), acomprehensive collection of nationally-recognized science learning goals for K-12 stu-dents (National Research Council, 1996).We have examined sentence extraction agree-ment between experts using a kappa measure thataccounts for prevalence of judgments and con-flicting biases amongst experts, called PABA-kappa (Byrt et al, 1993).
The average PABA-kappa value of 0.62 indicates that our expertssubstantially agree on sentence extraction fromdigital library resources.
While this study wasnot designed as an annotation project to supportsummarization evaluation, this level of agree-ment indicates that the concepts selected by theexperts may serve as the reference summary toevaluate the performance of our summarizer.4 Summarizer for Science EducationCreating a knowledge map from a collection ofinput texts involves identifying sentences con-taining important domain concepts, linking con-cepts, and labeling those links.
This paper fo-cuses solely on identifying and extracting peda-gogically relevant sentences as domain concepts.We have designed and implemented an extrac-tive summarizer for educational science content,called COGENT, based on MEAD version 3.11(Radev et al, 2000).
COGENT processes a col-lection of educational digital library resources byfirst preprocessing each resource using Tidy(tidy.sourceforge.net) to fix improperly format-ted HTML code.
COGENT then merges multipleweb pages into a single HTML document andextracts the contents of each resource into a plaintext file.
We have extended MEAD with sen-tence scoring features based on domain content,document structure, and sentence length.4.1 Domain ContentWe have designed two sentence-scoring featuresthat aim to capture the domain content relevanceof each sentence: the educational standardsfeature and the gazetteer feature.We have developed a feature that models howhuman experts used external sources to identifyand extract concepts.
The educational standardsfeature uses the textual description of therelevant AAAS Benchmarks on earthquakes andplate tectonics for high-school age learners andthe associated NSES.
Each sentence receives ascore based on its similarity to the text contentsof the learning goals and educational standardscomputed using a TFIDF (Term Frequency-Inverse Document Frequency) approach  (Saltonand Buckley, 1988).
We have used KinoSearch,a Perl implementation of the Lucene searchengine (lucene.apache.org), to create an indexthat includes the AAAS Benchmarks learninggoal description (boosted by 2), subject (boostedby 8), and keywords (boosted by 2), plus the textof the associated national standards (notboosted).
Sentence scores are based on thesimilarity scores generated by KinoSearch inresponse to a query consisting of the sentencetext.To account for the large number of examplesused by the experts in the domain knowledgemap (14% of all links), we have developed afeature that reflects the number and relevance ofthe geographical names in each sentence.
Earthscience examples often refer to names ofgeographical places, including geologicalformations on the planet.
The gazetteer featureleverages the Alexandria Digital Library (ADL)Gazetteer service (Hill, 2000) to check whethernamed entities identified in each sentence match179entries in the ADL Gazetteer.
A gazetteer is ageoreferencing resource containing informationabout locations and place-names, includinglatitude and longitude as well as type informationabout the corresponding geographical feature.Each sentence receives a score based on a TFIDFapproach where the TF is the number of times aparticular location name appears in the sentenceand the IDF is the inverse of the count ofgazetteer entries matching the location name.
Ifthe ADL Gazetteer returns a large number ofresults for a given place-name, it means there aremany geographical locations identified by thatname.
Our assumption is that unique names maybe more pedagogically relevant.
For example,Ohio receives an IDF score of 0.0625 becausethe ADL Gazetteer contains 16 entries so named,while the Mid-Atlantic Ridge, the distinctiveunderwater mountain range dividing the AtlanticOcean, receives a score of 1.0 as it appears onlyonce.4.2 Document StructureBased on the intuition that the HTML structureof a web site reflects content relevancy, we havedeveloped the hypertext feature.
The hypertextfeature assigns a higher score to sentences con-tained under higher level HTML headings.Heading BonusH1 1/1 = 1.00H2 1/2 = 0.50H3 1/3 = 0.33H4 1/4 = 0.25H5 1/5 = 0.20H6 1/6 = 0.17Table 1.
Hypertext feature heading bonusWithin a given heading level, the hypertextfeature assigns a higher score to sentences thatappear earlier within that level based on bothrelative paragraph order within the heading andrelative sentence position within each paragraph.The equation used to compute the hypertextscore for a sentence is44_1 * _1 * _  _ nosentnoparbonusheadingscorehypertext =where heading_bonus is obtained from Table 1,par_no is the paragraph number within the head-ing, and sent_no is the sentence number withinthe paragraph.
We use the 4 1 x   function to at-tenuate the contributions to the feature score oflater paragraphs and sentences.
Initially, we usedthe same function MEAD uses to modulate itsposition feature ( 2 1 x ), but initial experimenta-tion indicated this function decayed too rapidly,resulting in later sentences being over-penalized.4.3 Sentence LengthTo promote the extraction of sentences contain-ing scientific concepts, we have developed thecontent word density feature.
This feature makesa cut-off decision based on the ratio of contentwords to function words in a sentence.
The con-tent word density feature uses a pre-populatedlist of function words (a stopword list) to calcu-late the ratio of content to function words withineach sentence, keeping sentences that meet orexceed the ratio of 50%.
This cut-off value im-plies that the extracted sentences contain rela-tively more content words than function words.4.4 Sentence Scoring and SelectionWe compute the final score of each sentence byadding the scores obtained for the MEAD defaultconfiguration features (centroid and position) tothe scores for the COGENT features (educationalstandards, gazetteer, and hypertext).
After thesentences have been sorted according to theircumulative scores, we keep sentences that passthe cut-off constraints, including the MEADlength feature equal or greater than 9 and CO-GENT content word density equal or greater than50%.
We use the MEAD cosine re-ranker toeliminate redundant sentences based on a cutoffsimilarity value of 0.7.
Since human experts usedonly 5% of the total word count in the resources,we have configured MEAD to use a 5% wordcompression rate.5 EvaluationWe have evaluated COGENT by processing the20 digital library resources used in the empiricalstudy and comparing its output against the con-cepts identified by the experts.5.1 QualityTo assess the quality of the generated summaries,we have examined three configurations: Random,Default, and COGENT.
The Random configura-tion extracts a random collection of sentencesfrom the input texts.
The Default configurationuses the MEAD default centroid, position andlength (cut-off value of 9) sentence scoring fea-tures.
Finally, the COGENT configuration in-cludes the MEAD default features and the CO-GENT features.
The Default and COGENT con-figurations use the MEAD cosine function with athreshold of 0.7 to eliminate redundant sen-180tences.
All three configurations use a word com-pression factor of 5% resulting in summaries ofvery similar length.For this evaluation, we leverage ROUGE (Linand Hovy, 2003) to address the relative quality ofthe generated summaries based on common n-gram counts and longest common subsequence(LCS).
We report on ROUGE-1 (unigrams),ROUGE-2 (bigrams), ROUGE W-1.2 (weightedLCS), and ROUGE-S* (skip bigrams) as theyappear to correlate well with human judgmentsfor longer multi-document summaries, particu-larly ROUGE-1 (Lin, 2004).
Table 2 shows theresults of this ROUGE-based evaluation includ-ing recall (R), precision (P), and balanced f-measure (F).Random Default COGENTR 0.4855 0.4976 0.6073P 0.5026 0.5688 0.6034 R-1F 0.4939 0.5308 0.6054R 0.0972 0.1321 0.1907P 0.1006 0.1510 0.1895 R-2F 0.0989 0.1409 0.1901R 0.0929 0.0951 0.1185P 0.1533 0.1733 0.1877 R-W-1.2F 0.1157 0.1228 0.1453R 0.2481 0.2620 0.3820P 0.2657 0.3424 0.3772 R-S*F 0.2566 0.2969 0.3796Table 2.
Quality evaluation results (5% wordcompression)COGENT consistently outperforms the Ran-dom and Default baselines based on all four re-ported ROUGE measures.
Given that much ofthe original research efforts on MEAD have cen-tered on news articles, this result is not surpris-ing.
Pedagogical content, such as the educationaldigital library resources used in our work, differsin rhetorical intent, structure and terminologyfrom the news articles leveraged by the MEADresearchers.
However, the COGENT featuresdescribed here are complementary to the defaultMEAD configuration.
COGENT can best becharacterized as a hybrid MDS, integrating bot-tom-up (centroid, position, length, hypertext, andcontent word density) and top-down (educationalstandards and gazetteer) sentence scoring fea-tures.
This hybrid approach reflects our findingsfrom observing expert behaviors for identifyingconcepts from educational digital library re-sources.
We believe the overall improvement inquality scores may be due to the COGENT fea-tures targeting different dimensions of what con-stitutes a pedagogically effective summary thanthe default MEAD features.To characterize the COGENT summary con-tents, one of our research team members manu-ally generated a summary corresponding to thebest case for an extractive summarizer.
This BestCase summary comprises the sentences from thedigital library resources that align to the conceptsselected by the human experts in our empiricalstudy.
Since the experts created concepts of vary-ing granularity, this alignment produces the listof sentences that the experts would have pro-duced if they had only selected single sentencesto create concepts for their domain knowledgemap.
This summary comprises 621 sentencesconsisting of 13,116 words, or about a 9% wordcompression.For this aspect of the evaluation, we have usedROUGE-L, an LCS metric computed usingROUGE.
The ROUGE-L computation examinesthe union LCS between each reference sentenceand all the sentences in the candidate summary.We believe this metric may be well-suited to re-flect the degree of linguistic surface structuresimilarity between summaries.
We postulate thatROUGE-L may be able to account for the explic-itly copy-pasted concepts and to detect the moresubtle similarities with paraphrased concepts inthe expert-generated domain knowledge map.We have also used the content-based evaluationcapabilities of MEAD to report on a cosinemeasure to capture similarity between the candi-date summaries and the reference.
Table 3 showsthe results of this aspect of the evaluation includ-ing recall (R), precision (P), and balanced f-measure (F).Random(5%)Default(5%)COGENT(5%)Best Case(9%)R 0.4814 0.4919 0.6021 0.9669P 0.4982 0.5623 0.5982 0.6256 R-LF 0.4897 0.5248 0.6001 0.7597Cosine 0.5382 0.6748 0.8325 0.9323Table 3.
Content-based evaluation results (wordcompression in parentheses)COGENT consistently outperforms the Ran-dom and Default baselines on both the ROUGE-L and cosine measures.
Given the cosine value of0.8325, it appears COGENT extracts sentencescontaining similar terms  in very similar fre-quency distribution as the experts.The ROUGE-L scores also consistently indi-cate that the COGENT summary may be closerto the reference summary in relative word order-181ing than either the Random or Default configura-tions.
However, the scores for the Best Casesummary reveal two interesting points.
First, theROUGE-L recall score for COGENT (R=0.6021) is lower than that obtained by the BestCase summary (R=0.9669), meaning our sum-marizer appears to be extracting different sen-tences than those selected by the experts.
Giventhe high cosine similarity with the referencesummary (0.8325), we hypothesize that CO-GENT may be selecting sentences that coververy similar concepts to those selected by theexperts only expressed differently.
Second, wewould have expected the ROUGE-L precisionscore for the Best Case configuration to be closerto 1.0.
Instead, the Best Case precision score is0.6256, only a minor improvement over CO-GENT (P=0.5982).
Since the sentences in theBest Case summary come directly from the digi-tal library resources, we hypothesize that expertsmay have used extensive linguistic transforma-tions for paraphrased concepts, resulting in struc-tures that ROUGE-L could not identify as simi-lar.Given the difference in word compression forthe Best Case summary, we have performed anincremental analysis using the ROUGE-L meas-ure shown in Figure 2.ROUGE-L COGENT Evaluation0.000.100.200.300.400.500.600.700.800.901.000 5 10 15 20 25 30MEAD Word Percent CompressionRecall Precision F-MeasureFigure 2.
COGENT ROUGE-L results at differ-ent word compression ratesThis graph shows improved COGENT per-formance in ROUGE-L recall as the length of thesummary increases, while both precision and f-measure degrade.
COGENT can match the recallscores of the Best Case summary (R=0.9669) bymaking the generated summary longer (30%word compression rate or 32,619 words), but theprecision would suffer a sizeable decay(P=0.1558).
For educational applications, morecomprehensive concept inventories (longersummaries) may be better suited for computa-tional purposes, such as pedagogical reasoningabout student understanding, while more succinctinventories (shorter summaries) may be moreappropriate for display to the student.5.2 Pedagogical UtilityWe have evaluated COGENT?s pedagogical util-ity in the context of computationally identifyingstudent scientific misconceptions.
We have de-veloped algorithms that reliably detect incorrectstatements in student essays by comparing anexpert-created domain knowledge map to an ex-pert-created knowledge map of an essay.
Thesealgorithms use textual entailment techniquesbased on a shallow linguistic analysis of knowl-edge map concepts to identify sentences that con-tradict concepts in the domain knowledge map.Initial evaluation results indicate that these algo-rithms identify incorrect statements nearly asadeptly as human experts.ManualExpertAgreementExpertKnowledgeMapsCOGENTConceptInventoryRecall 0.69 0.87 0.93Precision 0.69 0.57 0.57F-Measure 0.69 0.68 0.69Table 4.
Incorrect statement identificationevaluation resultsAs shown in Table 4, the algorithms detect87% of all incorrect statements identified by ex-perts and 57% of the reported incorrect state-ments agree with human judgments on the sametask.
By comparison, experts show 69% overlapon average along both dimensions.
Introducingthe COGENT concept inventory in place of theexpert-created domain knowledge map improvesrecall performance, as the algorithms return 93%of all incorrect statements reported by the ex-perts, while preserving 57% precision.
Theseresults indicate that the generated summary cov-ers the necessary pedagogical concepts to com-putationally identify student scientific miscon-ceptions.Informal sampling of the sentences selected byCOGENT shows the following three importantscience concepts receiving the highest scores:1.
Earthquakes are the result of forces deepwithin the Earth's interior that continuouslyaffect the surface of the Earth.2.
Scientists believed that the movement of theEarth's plates bends and squeezes the rocks atthe edges of the plates.3.
In particular, four major scientific develop-ments spurred the formulation of the plate-182tectonics theory: (1) demonstration of theruggedness and youth of the ocean floor; (2)confirmation of repeated reversals of theEarth magnetic field in the geologic past; (3)emergence of the seafloor-spreading hypothe-sis and associated recycling of oceanic crust;and (4) precise documentation that the world'searthquake and volcanic activity is concen-trated along oceanic trenches and submarinemountain ranges.For a more rigorous analysis of the pedagogi-cal utility of the COGENT concepts, we asked aninstructional expert with domain expertise in ge-ology to evaluate the 326 sentences returned byCOGENT.
The expert used a 5-point Likert scaleto judge whether each concept would be peda-gogically useful in the context of a concept in-ventory on earthquakes and plate tectonicsknowledge for high school age learners.
The ex-pert agreed or strongly agreed that 60% of thesentences would be pedagogically useful, with30% of the sentences being potentially useful andonly 10% of the sentences being judged as notuseful.
These results indicate that COGENT ap-pears to perform quite well at identifying sen-tences that contain information relevant forlearning about the domain.We have also completed an ablation study toidentify the relative contribution of the COGENTfeatures to the quality of the summary.
We havefocused on the cosine metric to capture the over-all similarity between the COGENT concept in-ventory and the concepts from the expert-createdknowledge map.Features CosineAll Features 0.8325(Gazetteer) 0.5545(Hypertext) 0.5575(Educational Standards) 0.8083(Content Word Density) 0.8271Table 5.
Feature ablation evaluation results forCOGENTTable 5 shows the cosine similarity betweenthe concept inventory generated after taking thefeature shown in parentheses out of the summar-izer.
The results are ordered from low-to-highsuch that the feature contributing the most to theall-features cosine score appears at the top of thetable.
Removing either the gazetteer or the hy-pertext feature causes the largest drops in simi-larity indicating the importance of the use of ex-amples and the relevance of document structurefor the quality of the COGENT-generated sum-mary.
Meanwhile both the educational standardsand content word density appear to provide mod-est but useful improvements to the quality of theCOGENT summary.Given that our algorithms have only beenevaluated on the topic of earthquakes and platetectonics for high school age learners, COGENTmay be limited in its ability to transcend domainsdue to its reliance on two domain-aware sentencescoring features: educational standards and gaz-etteer.
However, the educational standards fea-ture may be applicable across other science top-ics because the AAAS Benchmarks and NSESprovide very thorough and detailed coverage of awide range of topics for the Science, Technol-ogy, Engineering, and Math disciplines forgrades K-12.
Only the gazetteer feature wouldneed to be replaced, especially given its signifi-cant contribution to the quality of the generatedsummary as indicated by the results of the abla-tion study.
We believe these results highlight theneed to generalize our approach, perhaps using aclassifier for identifying examples in educationaltexts without resorting to overly domain-specificlanguage resources, such as the ADL Gazetteer.Overall, the evaluation results indicate that ourapproach holds promise for effectively identify-ing concepts for inclusion in the construction of apedagogically useful domain knowledge mapfrom educational science content.6 Conclusions and Future WorkIn this paper, we have presented a multi-document summarization system, COGENT, thatintegrates bottom-up and top-down sentencescoring features to identify pedagogically rele-vant concepts from educational digital libraryresources.
Our results indicate that COGENTgenerates concept inventories that resemble thoseidentified by experts and outperforms existingmulti-document summarization systems.
Wehave also used the COGENT concept inventoryas input to our  misconception identification al-gorithms and the evaluation results indicate thealgorithms perform as well as when using an ex-pert-created domain knowledge map.
In the con-text of generating domain knowledge maps, ournext step is to explore how machine learningtechniques may be employed to connect conceptswith links.Automating the process of creating inventoriesof important pedagogical concepts represents animportant step towards creating scalable intelli-183gent learning and tutoring systems.
We hope ourprogress in this direction may contribute to in-crease the interest within the computational lin-guistics research community in novel educationaltechnology research.AcknowledgmentsThis research is funded in part by the NationalScience Foundation under NSF IIS/ALT Award0537194.
Any opinions, findings, and conclu-sions or recommendations expressed in this ma-terial are those of the author(s) and do not neces-sarily reflect the views of the NSF.ReferencesAhmad, F., de la Chica, S., Butcher, K., Sumner, T.and Martin, J.H.
(2007, June 17-23).
Towardsautomatic conceptual personalization tools.
In Pro-ceedings of the 7th ACM/IEEE-CS Joint Confer-ence on Digital Libraries, (Vancouver, British Co-lumbia, Canada, 2007), pages 452 - 461.Butcher, K.R., Bhushan, S. and Sumner, T. (2006).Multimedia displays for conceptual discovery: in-formation seeking with strand maps.
ACM Multi-media Systems, 11 (3), pages 236-248.Byrt, T., Bishop, J. and Carlin, J.B. (1993).
Bias,prevalence, and kappa.
Journal of Clinical Epide-miology, 46 (5), pages 423-429.Harabagiu, S. and Lacatusu, F. (2005, August 15-19).Topic themes for multi-document summarization.In Proceedings of the 28th Annual InternationalACM SIGIR Conference on Research and Devel-opment in Information Retrieval, (Salvador, Brazil,2005), pages 202-209.Hardy, H., Shimizu, N., Strzalkowski, T., Ting, L.,Wise, G.B.
and Zhang, X.
(2002).
Summarizinglarge document sets using concept-based cluster-ing.
In Proceedings of the Human Language Tech-nology Conference 2002, (San Diego, California,United States, 2002), pages 222-227.Hill, L.L.
(2000, September 18-20).
Core elements ofdigital gazetteers: placenames, categories, andfootprints.
In Proceedings of the 4th EuropeanConference on Digital Libraries, (Lisbon, Portugal,2000), pages 280-290.Holley, C.D.
and Dansereau, D.F.
(1984).
Spatiallearning strategies: Techniques, applications, andrelated issues.
Academic Press, Orlando, Florida.Lin, C.Y.
(2004).
ROUGE: A package for automaticevaluation of summaries.
In Proceedings of theWorkshop on Text Summarization Branches Out,(Barcelona, Spain, 2004).Lin, C.Y.
and Hovy, E. (2003, May-June).
Automaticevaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT-NAACL, (Edmonton,Canada, 2003), pages 71-78.Lin, D. and Pantel, P. (2002, August 24-September 1).Concept discovery from text.
In Proceedings of the19th International Conference on ComputationalLinguistics, (Taipei, Taiwan, 2002), pages 1-7.Mani, I.
(2001).
Automatic Summarization.
Mitkov,R.
(Ed.)
John Benjamins B.V., Amsterdam, TheNetherlands.National Research Council.
(1996).
National ScienceEducation Standards.
National Academy Press,Washington, DC.Navigli, R. and Velardi, P. (2004).
Learning domainontologies from document warehouses and dedi-cated websites.
Computational Linguistics, 30 (2),pages 151-179.Novak, J.D.
and Gowin, D.B.
(1984).
Learning howto learn.
Cambridge University Press, New York,New York.O'Donnell, A.M., Dansereau, D.F.
and Hall, R.H.(2002).
Knowledge maps as scaffolds for cognitiveprocessing.
Educational Psychology Review, 14(1), pages 71-86.Project 2061.
(1993).
Benchmarks for science liter-acy.
Oxford University Press, New York, NewYork, United States.Radev, D.R., Jing, H. and Budzikowska, M. (2000).Centroid-based summarization of multiple docu-ments: sentence extraction, utility-based evalua-tion, and user studies.
In Proceedings of theANLP/NAACL 2000 Workshop on Summariza-tion, (2000), pages 21-30.Salton, G. and Buckley, C. (1988).
Term-weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24 (5), pages 513-523.Wan, X. and Yang, J.
(2006, June 5th-7th).
Improvedaffinity graph based multi-document summariza-tion.
In Proceedings of the Human LanguageTechnology Conference of the North AmericanChapter of the Association for Computational Lin-guistics, (New York City, New York, 2006), pages181-184.Zouaq, A., Nkambou, R. and Frasson, C. (2007, July9-13).
Learning a domain ontology in the Knowl-edge Puzzle project.
In Proceedings of the Fifth In-ternational Workshop on Ontologies and SemanticWeb for E-Learning, (Marina del Rey, California,2007).184
