Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 94?102,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Information Status of Discourse EntitiesMalvina Nissim?Laboratory for Applied OntologyInstitute for Cognitive Science and TechnologyNational Research Council (ISTC-CNR), Roma, Italymalvina.nissim@loa-cnr.itAbstractIn this paper we address the issue of au-tomatically assigning information status todiscourse entities.
Using an annotated cor-pus of conversational English and exploit-ing morpho-syntactic and lexical features,we train a decision tree to classify entitiesintroduced by noun phrases as old, medi-ated, or new.
We compare its performancewith hand-crafted rules that are mainlybased on morpho-syntactic features andclosely relate to the guidelines that hadbeen used for the manual annotation.
Thedecision tree model achieves an overall ac-curacy of 79.5%, significantly outperform-ing the hand-crafted algorithm (64.4%).We also experiment with binary classifica-tions by collapsing in turn two of the threetarget classes into one and retraining themodel.
The highest accuracy achieved onbinary classification is 93.1%.1 IntroductionInformation structure is the way a speaker orwriter organises known and new information intext or dialogue.
Information structure has beenthe subject of numerous and very diverse linguisticstudies (Halliday, 1976; Prince, 1981; Hajic?ova?,1984; Vallduv?
?, 1992; Lambrecht, 1994; Steed-man, 2000, for instance), thus also yielding awide range of terms and definitions (see (Vallduv?
?,?The work reported in this paper was carried out whilethe author was a research fellow at the Institute for Com-municating and Collaborative Systems of the Universityof Edinburgh, United Kingdom, and was supported by aScottish Enterprise Edinburgh-Stanford Link grant (265000-3102-R36766).1992; Kruijff-Korbayova?
and Steedman, 2003) fora discussion).
In the present study, we adopt theterm ?Information Status?, following the defini-tion employed for the annotation of the corpus weuse for our experiments (Nissim et al, 2004).
In-formation status describes to which degree a dis-course entity is available to the hearer, in termsof the speaker?s assumptions about the hearer?sknowledge and beliefs.
Although there is a fineline in the distinction between Information Sta-tus and Information Structure, it is fair to say thatwhereas the latter models wider discourse coher-ence, the former focuses mainly on the local levelof discourse entities.
Section 2 provides more de-tails on how this notion is encoded in our corpus.Information status has generated large interestamong researchers because of its complex interac-tion with other linguistic phenomena, thus affect-ing several Natural Language Processing tasks.Since it correlates with word order and pitch ac-cent (Lambrecht, 1994; Hirschberg and Nakatani,1996), for instance, incorporating knowledge oninformation status would be helpful for naturallanguage generation, and in particular text-to-speech systems.
Sto?ber and colleagues, for ex-ample, ascribe to the lack of such information thelower performance of text-to-speech compared toconcept-to-speech generation, where such knowl-edge could be made directly available to the sys-tem (Sto?ber et al, 2000).Another area where information status can playan important role is anaphora resolution.
A majorobstacle in the resolution of definite noun phraseswith full lexical heads is that only a small pro-portion of them is actually anaphoric (ca.
30%(Vieira and Poesio, 2000)).
Therefore, in the ab-sence of anaphoricity information, a resolutionsystem will try to find an antecedent also for non-94anaphoric definite noun phrases, thus severely af-fecting performance.
There has been recent in-terest in determining anaphoricity before perform-ing anaphora resolution (Ng and Cardie, 2002;Uryupina, 2003), but results have not been en-tirely satisfactory.
Given that old entities are morelikely to be referred to by anaphors, for instance,identification of information status could improveanaphoricity determination.Postolache et al (2005) have recently shownthat learning information structure with high ac-curacy is feasible for Czech.
However, there areyet no studies that explore such a task for English.Exploiting an existing annotated corpus, in this pa-per we report experiments on learning a model forthe automatic identification of information statusin English.2 DataFor our experiments we annotated a portion of thetranscribed Switchboard corpus (Godfrey et al,1992), consisting of 147 dialogues (Nissim et al,2004).1 In the following section we provide a briefdescription of the annotation categories.2.1 AnnotationOur annotation of information status mainly buildson (Prince, 1992), and employs a distinction intoold, mediated, and new entities similar to the workof (Strube, 1998; Eckert and Strube, 2001).All noun phrases (NPs) were extracted as mark-able entities using pre-existing parse information(Carletta et al, 2004).
An entity was annotated asnew if it has not been previously referred to andis yet unknown to the hearer.
The tag mediatedwas instead used whenever an entity that is newlymentioned in the dialogue can be inferred by thehearer thanks to prior or general context.2 Typ-ical examples of mediated entities are generallyknown objects (such as ?the sun?, or ?the Pope?
(Lo?bner, 1985)), and bridging anaphors (Clark,1975; Vieira and Poesio, 2000), where an entityis related to a previously introduced one.
When-ever an entity was neither new nor mediated, it wasconsidered as old.1Switchboard is a collection of spontaneous phone con-versations, averaging six minutes in length, between speakersof American English on predetermined topics.
A third of thecorpus is syntactically parsed as part of the Penn Treebank(Marcus et al, 1993)2This type corresponds to Prince?s (1981; 1992) in-ferrables.In order to account for the complexity of thenotion of information status, the annotation alsoincludes a sub-type classification for old and me-diated entities that provides a finer-grained dis-tinction with information on why a given entity ismediated (e.g., set-relation, bridging) or old (e.g.,coreference, generic pronouns).
In order to testthe feasibility of automatically assigning informa-tion status to discourse entities, we took a modularapproach and only considered the coarser-graineddistinctions for this first study.
Information aboutthe finer-grained subtypes will be used in futurework.In addition to the main categories, we used twomore annotation classes: a tag non-applicable,used for entities that were wrongly extracted in theautomatic selection of markables (e.g.
?course?
in?of course?
), for idiomatic occurrences, and ex-pletive uses of ?it?
; and a tag not-understood to beapplied whenever an annotator did not fully under-stand the text.
Instances annotated with these twotags, as well as all traces, which were left unanno-tated, were excluded from all our experiments.Inter-annotator agreement was measured usingthe kappa (K) statistics (Cohen, 1960; Carletta,1996) on 1,502 instances (three Switchboard dia-logues) marked by two annotators who followedspecific written guidelines.
Given that the taskinvolves a fair amount of subjective judgement,agreement was remarkably high.
Over the threedialogues, the annotation yielded K = .845 forthe old/med/new classification (K = .788 whenincluding the finer-grained subtype distinction).Specifically, ?old?
proved to be the easiest to dis-tinguish, with K = .902; for ?med?
and ?new?agreement was measured at K = .800 and K =.794, respectively.
A value of K > .76 is usuallyconsidered good agreement.
Further details on theannotation process and corpus description are pro-vided in (Nissim et al, 2004)2.2 SetupWe split the 147 dialogues into a training, a de-velopment and an evaluation set.
The training setcontains 40,865 NPs distributed over 94 dialogues,the development set consists of 23 dialogues for atotal of 10,565 NPs, and the evaluation set com-prises 30 dialogues with 12,624 NPs.
Instanceswere randomised, so that occurrences of NPs fromthe same dialogue were possibly split across thedifferent sets.95Table 1 reports the distribution of classes forthe training, development and evaluation sets.
Thedistributions are similar, with a majority of old en-tities, followed by mediated entities, and lastly bynew ones.Table 1: Information status distribution of NPs intraining, development and evaluation setsTRAIN DEV EVALold 19730 (48.3%) 5181 (49.0%) 6049 (47.9%)med 15184 (37.1%) 3762 (35.6%) 4644 (36.8%)new 5951 (14.6%) 1622 (15.4%) 1931 (15.3%)total 40865 (100%) 10565 (100%) 12624 (100%)3 Classification with hand-crafted rulesThe target classes for our classification experi-ments are the annotation tags: old, mediated, andnew.
As baseline, we could take a simple ?most-frequent-class?
assignment that would classify allentities as old, thus yielding an accuracy of 47.9%on the evaluation set (see Table 1).
Although the?all-old?
assumption makes a reasonable baseline,it would not provide a particularly interesting solu-tion from a practical perspective, since a dialogueshould also contain not-old information.
Thus,rather than adopting this simple strategy, we de-veloped a more sophisticated baseline working ona set of hand-crafted rules.This hand-crafted algorithm is based on ratherstraightforward, intuitive rules, partially reflectingthe instructions specified in the annotation guide-lines.
As shown in Figure 1, the top split is theNP type: whether the instance to classify is a pro-noun, a proper noun, or a common noun.
Theother information that the algorithm uses is aboutcomplete or partial string overlapping with respectto the dialogue?s context.
For common nouns wealso consider the kind of determiner (definite, in-definite, demonstrative, possessive, or bare).In order to obtain the NP type information, weexploited the pre-existing morpho-syntactic tree-bank annotation of Switchboard.
Whenever theextraction failed, we assigned a type ?other?
andalways backed-off these cases to old (the most fre-quent class in training data).
Values for the otherfeatures were obtained by simple pattern matchingand NP extraction.Evaluation measures The algorithm?s perfor-mance is evaluated with respect to its general ac-curacy (Acc): the number of correctly classifiedinstances over all assignments.
Moreover, for eachcase NP is a pronounstatus := oldcase NP is a proper nounif first occurrence thenstatus := medelsestatus := oldendifcase NP is a common nounif identical string already mentioned thenstatus := oldelseif partial string already mentioned thenstatus := medelseif determiner is def/dem/poss thenstatus := medelsestatus := newendifendifendifotherwisestatus := oldFigure 1: Hand-crafted rule-based algorithm forthe assignment of information status to NPs.class (c), we report precision (P), recall (R), and f-score (F) thus calculated:Pc =correct assignments of ctotal assignments of cRc =correct assignments of ctotal corpus instances of cFc = 2PcRcPc+RcThe overall accuracy of the rule-based algo-rithm is 65.8%.
Table 2 shows the results for eachtarget class in both the development and evaluationsets.
We discuss results on the latter.Although a very high proportion of old entitiesis correctly retrieved (93.5%), this is done withrelatively low precision (66.7%).
Moreover, bothprecision and recall for the other classes are dis-appointing.
Unsurprisingly, the rules that applyto common nouns (the most ambiguous with re-spect to information status) generate a large num-96Table 2: Per class performance of hand-craftedrules on the development and evaluation setsDEV EVALP R F P R Fold .677 .932 .784 .667 .935 .779med .641 .488 .554 .666 .461 .545new .517 .180 .267 .436 .175 .250ber of false positives.
The rule that predicts anold entity in case of a full previous mention, forexample, has a precision of only 39.8%.
Better,but not yet satisfactory, is the precision of the rulethat predicts a mediated entity for a common nounthat has a previous partial mention (64.7%).
Theworst performing rule is the one that assigns themost frequent class (old) to entities of syntactictype ?other?, with a precision of 35.4%.
To give anidea of the correlation between NP type and infor-mation status, in Table 3 we report the distributionobserved in the evaluation set.Table 3: Distribution of information status overNP types in the evaluation setold med newpronoun 4465 159 13proper 107 198 27common 752 2874 1256other 725 1413 6354 Learning Information StatusOur starting point for the automatic assignmentof information status are the three already intro-duced classes: old, mediated and new.
Addition-ally, we experiment with binary classifications, bycollapsing mediated entities in turn with old andnew ones.For training, developing and evaluating themodel we use the split described in Section 2.2(see Table 1).
Performance is evaluated accord-ing to overall accuracy and per class precision, re-call, and f-score as described in Section 3.
To traina C4.5 decision tree model we use the J48 Wekaimplementation (Witten and Frank, 2000).
Thechoice of features to build the tree is described inthe following section.4.1 FeaturesThe seven features we use are automatically ex-tracted from the annotated data exploiting pre-existing morpho-syntactic markup and using sim-Table 4: Feature set for learning experimentsFEATURE VALUESfull prev mention numericmention time {first,second,more}partial prev mention {yes,no,na}determiner {bare,def,dem,indef,poss,na}NP length numericgrammatical role {subject,subjpass,object,pp,other}NP type {pronoun,common,proper,other}ple pattern matching techniques.
They are sum-marised in Table 4.The choice of features is motivated by the fol-lowing observations.
The information comingfrom partial previous mentions is particularly use-ful for the identification of mediated entities.
Thisshould account specifically for cases of media-tion via set-relations; for example, ?your children?would be considered a partial previous mention of?my children?
or ?your four children?.
The value?na?
stands for ?non-applicable?
and is mainlyused for pronouns.
Full previous mention is likelyto be a good indicator of old entities.
Both full andpartial previous mentions are calculated withineach dialogue without any constraints based ondistance.NP type and determiner type are expected to behelpful for all categories, with pronouns, for in-stance, tending to be old and indefinite NPs beingoften new.
We included the length of NPs (mea-sured in number of words) since linguistic studieshave shown that old entities tend to be expressedwith less lexical material (Wasow, 2002).
In exper-iments on the development data we also includedthe NP string itself, on the grounds that it mightbe of use in cases of general mediated instances(common knowledge entities), such as ?the sun?,?people?, ?Mickey Mouse?, and so on.
However,this feature turned out to negatively affect perfor-mance, and was not included in the final model.4.2 ResultsWith an overall final accuracy of 79.5% on theevaluation set, C4.5 significantly outperforms thehand-crafted algorithm (65.8%).
Although theidentification of old entities is quite successful(F=.928), performance is not entirely satisfactory.This is especially true for the classification of newentities, for which the final f-score is .320, mainlydue to extremely low recall (.223).
Mediated enti-ties, instead, are retrieved with a fairly low preci-sion but higher recall.
Table 5 summarises preci-sion, recall, and f-score for each class.97Table 5: Per class performance of C4.5 on the de-velopment and evaluation setsDEV EVALP R F P R Fold .935 .911 .923 .941 .915 .928med .673 .878 .762 .681 .876 .766new .623 .234 .341 .563 .223 .320The major confusion in the classification arisesbetween mediated and new (the most difficult de-cision to make for human annotators too, see Sec-tion 2.1), which are often distinguished on the ba-sis of world knowledge, not available to the classi-fier.
This is clearly shown by the confusion matrixin Table 6: the highest proportion of mistakes isdue to 1,453 new instances classified as mediated.Also significant is the wrong assignment of me-diated tags to old entities.
Such behaviour of theclassifier is to be expected, given the ?in-between?nature of mediated entities.Table 6: Confusion matrix for evaluation set.C=Classifier tag; G=Gold tagC ?G ?old med newold 5537 452 60med 303 4066 275new 47 1453 4314.3 Classification with two categories onlyGiven the above observations, we collapsed me-diated entities in turn with old ones (focusing ontheir non-newness) or new ones (enhancing theirnon complete givenness), thus reducing the task toa binary classification.Since it appears to be more difficult to distin-guish mediated and new rather than mediated andold (Table 6), we expect the classifier to performbetter when mediated is binned with new ratherthan old.
Also, in the case where mediated and oldentities are collapsed into one single class as op-posed to new ones, the distribution of classes be-comes highly skewed towards old entities (84.7%)so that the learner is likely to lack sufficient infor-mation for identifying new entities.Table 7 shows the final accuracy for the two bi-nary classifications (and the three-way one).
Asexpected, when mediated entities are joint withnew ones, the classifier performs best (93.1%),with high f-scores for both old and new, and is sig-nificantly better than the alternative binary classi-fication (t-test, p < 0.001).
Indeed, the old+medvs new classification is nearly an all-old assign-ment and its overall final accuracy (85.5%) is nota significant improvement over the all-old baseline(84.7%).
Results suggest that mediated NPs aremore similar to new than to old entities and mightprovide interesting feedback for the theoretical as-sumptions underlying the corpus annotation.4.4 Comparison with two categories onlyFor a fair comparison, we performed a two-wayclassification using the hand-crafted algorithm,which had to be simplified to account for the lackof a mediated class.In the case where all mediated instances wherecollapsed together with the old ones, the decisionrules are very simple: pronouns, proper nouns, andcommon nouns that have been previously fully orpartially mentioned are classified as old; first men-tion common nouns are new; everything else isold.
Both precision and recall for old instancesare quite high (.868 and .906 respectively), for aresulting f-score of .887.
Conversely, the perfor-mance on identifying new entities is very poor,with a precision of .337 and a recall of .227, fora combined f-score of .271.
The overall accuracyis .803, and this is significantly lower than the per-formance of C4.5, which achieves an overall accu-racy of .850 (t-test, p < 0.001).When mediated entities are collapsed with newones, rule-based classification is done again witha very basic algorithm derived from the rules inFigure 1: pronouns are old; proper nouns are newif first mention, old otherwise; common nounsthat have been fully previously mentioned are old,otherwise new.
Everything else is new, which inthe training set is now the most frequent class(51.7%).
The overall accuracy of .849 is signif-icantly lower than that achieved by C4.5, whichis .931 (t-test, p < 0.001).
Differently from theprevious case (mediated collapsed with old), theperformance on each class is comparable, with aprecision, recall and f-score of .863, .815, and .838for old and of .838, .881, and .859 for new.5 Discussion5.1 Influence of training sizeIn order to assess the contribution of training sizeto performance, we experimented with increas-98Table 7: Overview of accuracy for hand-craftedrules and C4.5 on three-way and binary classifica-tions on development and evaluation setsDEV EVALclassification rules C4.5 rules C4.5old vs med vs new .658 .796 .644 .795old+med vs new .810 .861 .803 .855old vs med+new .844 .926 .849 .931ingly larger portions of the training data (from 50to 30,000 instances).
For each training size we ranthe classifier 5 times, each with a different ran-domly picked set of instances.
This was done forthe three-way and the two binary classifications.Reported results are always averaged over the 5runs.
Figure 2 shows the three learning curves.Figure 2: Learning curves for three- and two-wayclassificationsThe curve for the three-way classification shows aslight constant improvement, though it appears toreach a plateau after 5,000 instances.
The resultobtained training on the full set (40865 instances)is significantly better only if compared to a train-ing set of 4,000 or less (t-test, p < 0.05).
No othersignificant difference in accuracy can be observed.Increasing the training size over 5,000 instanceswhen learning to classify old+mediated vs newleads to a slight improvement due to the learnerbeing able to identify some new entities.
With asmaller training set the proportion of new entitiesis far too small to be of use.
However, as said, theoverall final accuracy of 85.5% (see Table 7) doesnot significantly improve over the baseline.Table 8: Performance of leave-one-out and single-feature classifiers on three-way classificationFEATUREACCURACYremoved singlefull prev mention .793 .730mention time .795 .730partial prev mention .791 .769determiner .789 .775NP length .793 .733gram role .782 .656NP type .784 .701full set .7955.2 Feature contributionWe are also interested in the contribution of eachsingle feature.
Therefore, we ran the classifieragain, leaving out one feature at a time.
No sig-nificant drop or gain was observed in any of theruns (t-test, p < 0.01), though the worst detri-ments were yielded by removing the grammati-cal role and the NP type.
These two features,however, also appear to be the least informativein single-feature classification experiments, thussuggesting that such information comes very use-ful only when combined with other evidence (seealso Section 5.4.
All results for leave-one-out andsingle-feature classifiers are shown in Table 8.5.3 Error AnalysisThe overwhelming majority of mistakes (1,453,56.1% of all errors) in the three-way classificationstems from classifying as mediated entities thatare in fact new (Table 6).
Significant confusionarises from proper nouns, as they are annotated asmediated or new entities, depending on whetherthey are generally known (such as names of USpresidents, for example), or domain/community-specific (such as the name of a local store that onlythe speaker knows).
This inconsistency in the an-notation might reflect well the actual status of en-tities in the dialogues, but it can be misleading forthe classifier.Another large group of errors is formed by oldentities classified as mediated (452 cases).
This isprobably due to the fact that the first node in thedecision tree is the ?partial mention?
feature (seeFigure 3).
The tree correctly captures the fact thata firstly mentioned entity which has been partiallymentioned before is mediated.
An entity that hasa previous partial mention but also a full previousmention is classified as old only if it is a propernoun or a pronoun, but as mediated if it is a com-mon noun.
This yields a large number of mis-99takes, since many common nouns that have beenpreviously mentioned (both in full and partially)are in fact old.
Another problem with previousmentions is the lack of restriction in distance: weconsider a previous mention any identical mentionof a given NP anywhere in the dialogue, and wehave no means of checking that it is indeed thesame entity that is referred to.
A way to alleviatethis problem might be exploiting speaker turn in-formation.
Using anaphoric chains could also beof help, but see Section 6.5.4 Learnt trees meet hand-crafted rulesThe learnt trees provide interesting insights on theintuitions behind the choice of hand-crafted rules.partial = yes| full <= 1| | det = def: med| | det = indef| | | length <= 2| | | | gramm = subj: med| | | | gramm = subjpassive: new| | | | gramm = obj: med| | | | gramm = pp: med| | | | gramm = other| | | | | type = proper: med| | | | | type = common: new| | | | | type = pronoun: new| | | | | type = other: med| | | length > 2: med| | det = dem| | | gramm = subj.
.
.Figure 3: Top of C.5, full training set, three classesFigure 3 shows the top of C4.5 (trained on the fulltraining set for the three-way classification), whichlooks remarkably different from the rules in Fig-ure 1.
We had based our decision of emphasisingthe importance of the NP type on the linguistic ev-idence that different syntactic realisations reflectdifferent degrees of availability of discourse enti-ties (Givo?n, 1983; Ariel, 1990; Grosz et al, 1995).In the learnt model, however, knowledge about NPtype is only used as subordinate to other features.This is indeed mirrored in the fact that removingNP type information from the feature set causesaccuracy to drop, but a classifier building on NPtype alone performs poorly (see Table 8).3 In-terestingly, though, more informative knowledgeabout syntactic form seems to be derived from thedeterminer type, which helps distinguish degreesof oldness among common nouns.3The NPtype-only classifier assigns old to pronouns andmed to all other types; it never assigns new.5.5 Naive Bayes modelFor additional comparison, we also trained a NaiveBayes classifier with the same experimental set-tings.
Results are significantly worse than C4.5?sin all three scenarios (t-test, p < 0.005), with anaccuracy of 74.6% in the three-way classification,63.3% for old+mediated vs new, and 91.0% for oldvs mediated+new.
The latter distribution appearsagain to be the easiest to learn.6 Related WorkTo our knowledge, there are no other studies on theautomatic assignment of information status in En-glish.
Recently, (Postolache et al, 2005) have re-ported experiments on learning information struc-ture in the Prague TreeBank.
The Czech tree-bank is annotated following the Topic-Focus artic-ulation theory (Hajic?ova?
et al, 1998).
The theo-retical definitions underlying the Prague Treebankand the corpus we are using are different, with theformer giving a more global picture of informa-tion structure, and the latter a more entity-specificone.
For this reason, and due to the fact that Pos-tolache et al?s experiments are on Czech (with afreer word order than English), comparing resultsis not straightforward.Their best system (C4.5 decision tree) achievesan accuracy of 90.69% on the topic/focus identi-fication task.
This result is comparable with theresult we obtain when training and testing on thecorpus where mediated and new entities are notdistinguished (93.1%).
Postolache and colleaguesalso observe a slowly flattening learning curve af-ter a very small amount of data (even 1%, in theircase).
Therefore, they predict an increase in per-formance will mainly come from better featuresrather than more training data.
This is likely to betrue in our case as well, also because our featureset is currently small and we will further benefitfrom incorporating additional features.
Postolacheet al use a larger feature set, which also includescoreference information.
The corpus we use hasmanually annotated coreference links.
However,because we see anaphoricity determination as atask that could benefit from automatic informationstatus assignment, we decided not to exploit thisinformation in the current experiments.
Moreover,we did not want our model to rely too heavily on afeature that is not easy to obtain automatically.1007 Conclusions and Future WorkWe have presented a model for the automatic as-signment of information status in English.
On thethree-way classification into old, mediated, andnew that reflects the corpus annotation tags, thelearnt tree outperforms a hand-crafted algorithmand achieves an accuracy of 79.5%, with high pre-cision and recall for old entities, high recall formediated entities, and a fair precision, but verypoor recall, for new ones.
When we collapsed me-diated and new entities into one category only op-posing this to old ones, the classifier performedwith an accuracy of 93.1%, with high f-scores forboth classes.
Binning mediated and old entities to-gether did not produce interesting results, mainlydue to the highly skewed distribution of the result-ing corpus towards old entities.
This suggests thatmediated entities are more similar to new than toold ones, and might provide interesting feedbackfor the theoretical assumptions underlying the an-notation.
Future work will examine specific casesand investigate how such insights can be used tomake the theoretical framework more accurate.As the first experiments run on English to learninformation status, we wanted to concentrate onthe task itself and avoid noise introduced by au-tomatic processing.
More realistic settings for in-tegrating an information status model in a large-scale NLP system would imply obtaining syntacticinformation via parsing rather than directly fromthe treebank.
Future experiments will assess theimpact of automatic preprocessing of the data.Results are very promising but there is room forimprovement.
First, the syntactic category ?other?is far too large, and finer distinctions must be madeby means of better extraction rules from the trees.Second, and most importantly, we believe that us-ing more features will be the main trigger of higheraccuracy.
In particular, we plan to use additionallexical and relational features derived from knowl-edge sources such as WordNet (Fellbaum, 1998)and FrameNet (Baker et al, 1998) which shouldbe especially helpful in distinguishing mediatedfrom new entities, the most difficult decision tomake.
For example, an entity that is linked inWordNet (within a given depth) and/or FrameNetto a previously introduced one is more likely to bemediated than new.Additionally, we will attempt to exploit dia-logue turns, since knowing which speaker saidwhat is clearly very valuable information.
In asimilar vein, we will experiment with distancemeasures, in terms of turns, sentences, or eventime, for determining when an introduced entitymight stop to be available.We also plan to run experiments on the auto-matic classification of old and mediated subtypes(the finer-grained classification) that is includedin the corpus but that we did not consider for thepresent study (see Section 2.1).
The major benefitof this would be a contribution to the resolution ofbridging anaphora.ReferencesMira Ariel.
1990.
Accessing Noun Phrase An-tecedents.
Routledge, London-New York.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ChristianBoitet and Pete Whitelock, editors, Proceedings ofCOLING-ACL, pages 86?90.Jean Carletta, Shipra Dingare, Malvina Nissim, andTatiana Nikitina.
2004.
Using the NITE XMLToolkit on the Switchboard Corpus to study syntac-tic choice: a case study.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation (LREC2004), Lisbon, May 2004.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: the kappa statistic.
ComputationalLinguistics, 22(2):249?254.Herbert H. Clark.
1975.
Bridging.
In Roger Schankand Bonnie Nash-Webber, editors, Theoretical Is-sues in Natural Language Processing.
The MITPress, Cambridge, MA.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurements, 20:37?46.Miriam Eckert and Michael Strube.
2001.
Dialogueacts, synchronising units and anaphora resolution.Journal of Semantics, 17(1):51?89.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press, Cam-bridge, MA.Talmy Givo?n.
1983.
Introduction.
In Talmy Givo?n,editor, Topic Continuity in Discourse: A Quantita-tive Cross-language Study.
John Benjamins, Ams-terdam/Philadelphia.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus forresearch and development.
In Proceedings ofICASSP-92, pages 517?520.Barbara Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: a framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.101Eva Hajic?ova?, Barbara Partee, and Petr Sgall.
1998.Topic-focus articulation, tripartite structures, and se-mantic content.
In Studies in Linguistics and Philos-ophy, volume 71.
Dordrecht.Eva Hajic?ova?.
1984.
Topic and focus.
In PetrSgall, editor, Contributions to Functional Syntax.Semantics and Language Comprehension (LLSEE16), pages 189?202.
John Benjamins, Amsterdam.M.A.K.
Halliday.
1976.
Notes on transitivity andtheme in English.
Part 2.
Journal of Linguistics,3(2):199?244.Julia Hirschberg and Christine H. Nakatani.
1996.
Aprosodic analysis of discourse segments in directiongiving monologues.
In Proceedings of 34th AnnualMeeting of the Association for Computational Lin-guistics.Ivana Kruijff-Korbayova?
and Mark Steedman.
2003.Discourse and information structure.
Journal ofLogic, Language, and Information, 12:249?259.Knud Lambrecht.
1994.
Information structure andsentence form.
Topic, focus, and the mental repre-sentation of discourse referents.
Cambridge Univer-sity Press, Cambridge.Sebastian Lo?bner.
1985.
Definites.
Journal of Seman-tics, 4:279?326.Mitchell Marcus, Beatrice Santorini, and May AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The Penn treebank.
Computa-tional Linguistics, 19:313?330.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to im-prove coreference resolution.
In Proc of the 19th In-ternational Conference on Computational Linguis-tics; Taipei, Taiwan, pages 730?736.Malvina Nissim, Shipra Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme forinformation status in dialogue.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation (LREC2004), Lisbon, May2004.Oana Postolache, Ivana Kruijff-Korbayova, and Geert-Jan Kruijff.
2005.
Data-driven approaches for in-formation structure identification.
In Proceedingsof Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 9?16, Vancouver, BritishColumbia, Canada, October.
Association for Com-putational Linguistics.Ellen F. Prince.
1981.
Toward a taxonomy of given-new information.
In Peter Cole, editor, RadicalPragmatics.
Academic Press, New York.Ellen Prince.
1992.
The ZPG letter: subjects, definite-ness, and information-status.
In Sandra Thompsonand William Mann, editors, Discourse description:diverse analyses of a fund raising text, pages 295?325.
John Benjamins, Philadelphia/Amsterdam.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA.K.
Sto?ber, P. Wagner, Jo?rg Helbig, S. Ko?ster, D. Stall,M.
Thomas, J. Blauert, W. Hess, R. Hoffmann, andH.
Mangold.
2000.
Speech synthesis using multi-level selection and concatenation of units from largespeech corpora.
In W. Wahlster, editor, Verbmobil:Foundations of Speech-to-Speech Translation, pages519?534.
Springer-Verlag, Berlin.Michael Strube.
1998.
Never look back: An alterna-tive to centering.
In Proceedings of the 17th Interna-tional Conference on Computational Linguistics and36th Annual Meeting of the Association for Com-putational Linguistics, pages 1251?1257, Montre?al,Que?bec, Canada.Olga Uryupina.
2003.
High-precision identification ofdiscourse new and unique noun phrases.
In Proc.
ofthe ACL 2003 Student Workshop, pages 80?86.Enric Vallduv??.
1992.
The Informational Component.Garland, New York.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4).Thomas Wasow.
2002.
Postverbal Behavior.
CSLIPublications.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann, SanDiego, CA.102
