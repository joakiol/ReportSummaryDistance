Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 88?97,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsThe Human Language Project:Building a Universal Corpus of the World?s LanguagesSteven AbneyUniversity of Michiganabney@umich.eduSteven BirdUniversity of Melbourne andUniversity of Pennsylvaniasbird@unimelb.edu.auAbstractWe present a grand challenge to build acorpus that will include all of the world?slanguages, in a consistent structure thatpermits large-scale cross-linguistic pro-cessing, enabling the study of universallinguistics.
The focal data types, bilin-gual texts and lexicons, relate each lan-guage to one of a set of reference lan-guages.
We propose that the ability to trainsystems to translate into and out of a givenlanguage be the yardstick for determin-ing when we have successfully captured alanguage.
We call on the computationallinguistics community to begin work onthis Universal Corpus, pursuing the manystrands of activity described here, as theircontribution to the global effort to docu-ment the world?s linguistic heritage beforemore languages fall silent.1 IntroductionThe grand aim of linguistics is the construction ofa universal theory of human language.
To a com-putational linguist, it seems obvious that the firststep is to collect significant amounts of primarydata for a large variety of languages.
Ideally, wewould like a complete digitization of every humanlanguage: a Universal Corpus.If we are ever to construct such a corpus, it mustbe now.
With the current rate of language loss, wehave only a small window of opportunity beforethe data is gone forever.
Linguistics may be uniqueamong the sciences in the crisis it faces.
The nextgeneration will forgive us for the most egregiousshortcomings in theory construction and technol-ogy development, but they will not forgive us if wefail to preserve vanishing primary language data ina form that enables future research.The scope of the task is enormous.
At present,we have non-negligible quantities of machine-readable data for only about 20?30 of the world?s6,900 languages (Maxwell and Hughes, 2006).Linguistics as a field is awake to the crisis.
Therehas been a tremendous upsurge of interest in doc-umentary linguistics, the field concerned with thethe ?creation, annotation, preservation, and dis-semination of transparent records of a language?
(Woodbury, 2010).
However, documentary lin-guistics alone is not equal to the task.
For example,no million-word machine-readable corpus existsfor any endangered language, even though such aquantity would be necessary for wide-ranging in-vestigation of the language once no speakers areavailable.
The chances of constructing large-scaleresources will be greatly improved if computa-tional linguists contribute their expertise.This collaboration between linguists and com-putational linguists will extend beyond the con-struction of the Universal Corpus to its exploita-tion for both theoretical and technological ends.We envisage a new paradigm of universal linguis-tics, in which grammars of individual languagesare built from the ground up, combining expertmanual effort with the power tools of probabilis-tic language models and grammatical inference.A universal grammar captures redundancies whichexist across languages, constituting a ?universallinguistic prior,?
and enabling us to identify thedistinctive properties of specific languages andfamilies.
The linguistic prior and regularities dueto common descent enable a new economy of scalefor technology development: cross-linguistic tri-angulation can improve performance while reduc-ing per-language data requirements.Our aim in the present paper is to move beyondgeneralities to a concrete plan of attack, and tochallenge the field to a communal effort to cre-ate a Universal Corpus of the world?s languages,in consistent machine-readable format, permittinglarge-scale cross-linguistic processing.882 Human Language Project2.1 Aims and scopeAlthough language endangerment provides ur-gency, the corpus is not intended primarily asa Noah?s Ark for languages.
The aims go be-yond the current crisis: we wish to support cross-linguistic research and technology development atthe largest scale.
There are existing collectionsthat contain multiple languages, but it is rare tohave consistent formats and annotation across lan-guages, and few such datasets contain more than adozen or so languages.If we think of a multi-lingual corpus as con-sisting of an array of items, with columns repre-senting languages and rows representing resourcetypes, the usual focus is on ?vertical?
processing.Our particular concern, by contrast, is ?horizontal?processing that cuts indiscriminately across lan-guages.
Hence we require an unusual degree ofconsistency across languages.The kind of processing we wish to enable ismuch like the large-scale systematic research thatmotivated the Human Genome Project.One of the greatest impacts of havingthe sequence may well be in enablingan entirely new approach to biologicalresearch.
In the past, researchers stud-ied one or a few genes at a time.
Withwhole-genome sequences .
.
.
they canapproach questions systematically andon a grand scale.
They can study .
.
.how tens of thousands of genes and pro-teins work together in interconnectednetworks to orchestrate the chemistry oflife.
(Human Genome Project, 2007)We wish to make it possible to investigate humanlanguage equally systematically and on an equallygrand scale: a Human Linguome Project, as itwere, though we have chosen the ?Human Lan-guage Project?
as a more inviting title for the un-dertaking.
The product is a Universal Corpus,1 intwo senses of universal: in the sense of including(ultimately) all the world?s languages, and in thesense of enabling software and processing meth-ods that are language-universal.However, we do not aim for a collection thatis universal in the sense of encompassing all lan-guage documentation efforts.
Our goal is the con-struction of a specific resource, albeit a very large1http://universalcorpus.org/resource.
We contrast the proposed effort withgeneral efforts to develop open resources, stan-dards, and best practices.
We do not aim to be all-inclusive.
The project does require large-scale col-laboration, and a task definition that is simple andcompelling enough to achieve buy-in from a largenumber of data providers.
But we do not need anddo not attempt to create consensus across the en-tire community.
(Although one can hope that whatproves successful for a project of this scale willprovide a good foundation for future standards.
)Moreover, we do not aim to collect datamerely in the vague hope that it will prove use-ful.
Although we strive for maximum general-ity, we also propose a specific driving ?use case,?namely, machine translation (MT), (Hutchins andSomers, 1992; Koehn, 2010).
The corpus pro-vides a testing ground for the development of MTsystem-construction methods that are dramatically?leaner?
in their resource requirements, and whichtake advantage of cross-linguistic bootstrapping.The large engineering question is how one canturn the size of the task?constructing MT systemsfor all the world?s languages simultaneously?toone?s advantage, and thereby consume dramati-cally less data per language.The choice of MT as the use case is also drivenby scientific considerations.
To explain, we re-quire a bit of preamble.We aim for a digitization of each human lan-guage.
What exactly does it mean to digitize anentire language?
It is natural to think in termsof replicating the body of resources available forwell-documented languages, and the pre-eminentresource for any language is a treebank.
Producinga treebank involves a staggering amount of man-ual effort.
It is also notoriously difficult to obtainagreement about how parse trees should be definedin one language, much less in many languages si-multaneously.
The idea of producing treebanks for6,900 languages is quixotic, to put it mildly.
Butis a treebank actually necessary?Let us suppose that the purpose of a parsetree is to mediate interpretation.
A treebank, ar-guably, represents a theoretical hypothesis abouthow interpretations could be constructed; the pri-mary data is actually the interpretations them-selves.
This suggests that we annotate sentenceswith representations of meanings instead of syn-tactic structures.
Now that seems to take us out ofthe frying pan into the fire.
If obtaining consen-89sus on parse trees is difficult, obtaining consensuson meaning representations is impossible.
How-ever, if the language under consideration is any-thing other than English, then a translation intoEnglish (or some other reference language) is formost purposes a perfectly adequate meaning rep-resentation.
That is, we view machine translationas an approximation to language understanding.Here is another way to put it.
One measure ofadequacy of a language digitization is the abil-ity of a human?already fluent in a referencelanguage?to acquire fluency in the digitized lan-guage using only archived material.
Now it wouldbe even better if we could use a language digiti-zation to construct an artificial speaker of the lan-guage.
Importantly, we do not need to solve the AIproblem: the speaker need not decide what to say,only how to translate from meanings to sentencesof the language, and from sentences back to mean-ings.
Taking sentences in a reference language asthe meaning representation, we arrive back at ma-chine translation as the measure of success.
Inshort, we have successfully captured a language ifwe can translate into and out of the language.The key resource that should be built for eachlanguage, then, is a collection of primary textswith translations into a reference language.
?Pri-mary text?
includes both written documents andtranscriptions of recordings.
Large volumes of pri-mary texts will be useful even without translationfor such tasks as language modeling and unsuper-vised learning of morphology.
Thus, we antici-pate that the corpus will have the usual ?pyrami-dal?
structure, starting from a base layer of unan-notated text, some portion of which is translatedinto a reference language at the document level tomake the next layer.
Note that, for maximally au-thentic primary texts, we assume the direction oftranslation will normally be from primary text toreference language, not the other way around.Another layer of the corpus consists of sentenceand word alignments, required for training andevaluating machine translation systems, and forextracting bilingual lexicons.
Curating such anno-tations is a more specialized task than translation,and so we expect it will only be done for a subsetof the translated texts.In the last and smallest layer, morphology is an-notated.
This supports the development of mor-phological analyzers, to preprocess primary textsto identify morpheme boundaries and recognizeallomorphs, reducing the amount of data requiredfor training an MT system.
This most-refinedtarget annotation corresponds to the interlinearglossed texts that are the de facto standard of anno-tation in the documentary linguistics community.We postulate that interlinear glossed text is suf-ficiently fine-grained to serve our purposes.
Itinvites efforts to enrich it by automatic means:for example, there has been work on parsing theEnglish translations and using the word-by-wordglosses to transfer the parse tree to the object lan-guage, effectively creating a treebank automati-cally (Xia and Lewis, 2007).
At the same time, webelieve that interlinear glossed text is sufficientlysimple and well-understood to allow rapid con-struction of resources, and to make cross-linguisticconsistency a realistic goal.Each of these layers?primary text, translations,alignments, and morphological glosses?seems tobe an unavoidable piece of the overall solution.The fact that these layers will exist in diminishingquantity is also unavoidable.
However, there is animportant consequence: the primary texts will bepermanently subject to new translation initiatives,which themselves will be subject to new align-ment and glossing initiatives, in which each stepis an instance of semisupervised learning (Abney,2007).
As time passes, our ability to enhance thequantity and quality of the annotations will onlyincrease, thanks to effective combinations of auto-matic, professional, and crowd-sourced effort.2.2 PrinciplesThe basic principles upon which the envisionedcorpus is based are the following:Universality.
Covering as many languages aspossible is the first priority.
Progress will begauged against concrete goals for numbers of lan-guages, data per language, and coverage of lan-guage families (Whalen and Simons, 2009).Machine readability and consistency.
?Cover-ing?
languages means enabling machine process-ing seamlessly across languages.
This will sup-port new types of linguistic inquiry and the devel-opment and testing of inference methods (for mor-phology, parsers, machine translation) across largenumbers of typologically diverse languages.Community effort.
We cannot expect a singleorganization to assemble a resource on this scale.It will be necessary to get community buy-in, and90many motivated volunteers.
The repository willnot be the sole possession of any one institution.Availability.
The content of the corpus will beavailable under one or more permissive licenses,such as the Creative Commons Attribution Li-cense (CC-BY), placing as few limits as possibleon community members?
ability to obtain and en-hance the corpus, and redistribute derivative data.Utility.
The corpus aims to be maximally use-ful, and minimally parochial.
Annotation will beas lightweight as possible; richer annotations willwill emerge bottom-up as they prove their utilityat the large scale.Centrality of primary data.
Primary texts andrecordings are paramount.
Secondary resourcessuch as grammars and lexicons are important, butno substitute for primary data.
It is desirable thatsecondary resources be integrated with?if not de-rived from?primary data in the corpus.2.3 What to includeWhat should be included in the corpus?
To someextent, data collection will be opportunistic, butit is appropriate to have a well-defined target inmind.
We consider the following essential.Metadata.
One means of resource identificationis to survey existing documentation for the lan-guage, including bibliographic references and lo-cations of web resources.
Provenance and propercitation of sources should be included for all data.For written text.
(1) Primary documents inoriginal printed form, e.g.
scanned page images orPDF.
(2) Transcription.
Not only optical charac-ter recognition output, but also the output of toolsthat extract text from PDF, will generally requiremanual editing.For spoken text.
(1) Audio recordings.
Bothelicited and spontaneous speech should be in-cluded.
It is highly desirous to have some con-nected speech for every language.
(2) Slow speech?audio transcriptions.?
Carefully respeaking aspoken text can be much more efficient than writ-ten transcription, and may one day yield to speechrecognition methods.
(3) Written transcriptions.We do not impose any requirements on the formof transcription, though orthographic transcriptionis generally much faster to produce than phonetictranscription, and may even be more useful aswords are represented by normalized forms.For both written and spoken text.
(1) Trans-lations of primary documents into a refer-ence language (possibly including commentary).
(2) Sentence-level segmentation and transla-tion.
(3) Word-level segmentation and glossing.
(4) Morpheme-level segmentation and glossing.All documents will be included in primaryform, but the percentage of documents with man-ual annotation, or manually corrected annotation,decreases at increasingly fine-grained levels of an-notation.
Where manual fine-grained annotation isunavailable, automatic methods for creating it (at alower quality) are desirable.
Defining such meth-ods for a large range of resource-poor languages isan interesting computational challenge.Secondary resources.
Although it is possible tobase descriptive analyses exclusively on a text cor-pus (Himmelmann, 2006, p. 22), the followingsecondary resources should be secured if they areavailable: (1) A lexicon with glosses in a referencelanguage.
Ideally, everything should be attested inthe texts, but as a practical matter, there will bewords for which we have only a lexical entry andno instances of use.
(2) Paradigms and phonol-ogy, for the construction of a morphological ana-lyzer.
Ideally, they should be inducible from thetexts, but published grammatical information maygo beyond what is attested in the text.2.4 Inadequacy of existing effortsOur key desideratum is support for automatic pro-cessing across a large range of languages.
No datacollection effort currently exists or is proposed, toour knowledge, that addresses this desideratum.Traditional language archives such as the AudioArchive of Linguistic Fieldwork (UC Berkeley),Documentation of Endangered Languages (MaxPlanck Institute, Nijmegen), the Endangered Lan-guages Archive (SOAS, University of London),and the Pacific And Regional Archive for Digi-tal Sources in Endangered Cultures (Australia) of-fer broad coverage of languages, but the majorityof their offerings are restricted in availability anddo not support machine processing.
Conversely,large-scale data collection efforts by the Linguis-tic Data Consortium and the European LanguageResources Association cover less than one percentof the world?s languages, with no evident plans formajor expansion of coverage.
Other efforts con-cern the definition and aggregation of languageresource metadata, including OLAC, IMDI, and91CLARIN (Simons and Bird, 2003; Broeder andWittenburg, 2006; Va?radi et al, 2008), but this isnot the same as collecting and disseminating data.Initiatives to develop standard formats for lin-guistic annotations are orthogonal to our goals.The success of the project will depend on con-tributed data from many sources, in many differ-ent formats.
Converting all data formats to anofficial standard, such as the RDF-based modelsbeing developed by ISO Technical Committee 37Sub-committee 4 Working Group 2, is simply im-practical.
These formats have onerous syntacticand semantic requirements that demand substan-tial further processing together with expert judg-ment, and threaten to crush the large-scale collab-orative data collection effort we envisage, beforeit even gets off the ground.
Instead, we opt for avery lightweight format, sketched in the next sec-tion, to minimize the effort of conversion and en-able an immediate start.
This does not limit theoptions of community members who desire richerformats, since they are free to invest the effort inenriching the existing data.
Such enrichment ef-forts may gain broad support if they deliver a tan-gible benefit for cross-language processing.3 A Simple Storage ModelHere we sketch a simple approach to storage oftexts (including transcribed speech), bitexts, inter-linear glossed text, and lexicons.
We have beendeliberately schematic since the goal is just to givegrounds for confidence that there exists a general,scalable solution.For readability, our illustrations will includespace-separated sequences of tokens.
However,behind the scenes these could be represented asa sequence of pairs of start and end offsets into aprimary text or speech signal, or as a sequence ofintegers that reference an array of strings.
Thus,when we write (1a), bear in mind it may be imple-mented as (1b) or (1c).
(1) a.
This is a point of order .b.
(0,4), (5,7), (8,9), (10,15), (16,18), .
.
.c.
9347, 3053, 0038, 3342, 3468, .
.
.In what follows, we focus on the minimal re-quirements for storing and disseminating alignedtext, not the requirements for efficient in-memorydata structures.
Moreover, we are agnostic aboutwhether the normalized, tokenized format is storedentire or computed on demand.We take an aligned text to be composed of aseries of aligned sentences, each consisting of asmall set of attributes and values, e.g.
:ID: europarl/swedish/ep-00-01-17/18LANGS: swd engSENT: det ga?ller en ordningsfra?gaTRANS: this is a point of orderALIGN: 1-1 2-2 3-3 4-4 4-5 4-6PROVENANCE: pharaoh-v1.2, ...REV: 8947 2010-05-02 10:35:06 leobfld12RIGHTS: Copyright (C) 2010 Uni...; CC-BYThe value of ID identifies the document and sen-tence, and any collection to which the documentbelongs.
Individual components of the identi-fier can be referenced or retrieved.
The LANGSattribute identifies the source and reference lan-guage using ISO 639 codes.2 The SENT attributecontains space-delimited tokens comprising a sen-tence.
Optional attributes TRANS and ALIGNhold the translation and alignment, if these areavailable; they are omitted in monolingual text.A provenance attribute records any automatic ormanual processes which apply to the record, anda revision attribute contains the version number,timestamp, and username associated with the mostrecent modification of the record, and a rights at-tribute contains copyright and license information.When morphological annotation is available, itis represented by two additional attributes, LEXand AFF.
Here is a monolingual example:ID: example/001LANGS: engSENT: the dogs are barkingLEX: the dog be barkAFF: - PL PL INGNote that combining all attributes of thesetwo examples?that is, combining word-by-wordtranslation with morphological analysis?yieldsinterlinear glossed text.A bilingual lexicon is an indispensable re-source, whether provided as such, induced froma collection of aligned text, or created by merg-ing contributed and induced lexicons.
A bilin-gual lexicon can be viewed as an inventory ofcross-language correspondences between wordsor groups of words.
These correspondences arejust aligned text fragments, albeit much smallerthan a sentence.
Thus, we take a bilingual lexiconto be a kind of text in which each record containsa single lexeme and its translation, represented us-ing the LEX and TRANS attributes we have alreadyintroduced, e.g.
:2http://www.sil.org/iso639-3/92ID: swedishlex/v3.2/0419LANGS: swd engLEX: ordningsfra?gaTRANS: point of orderIn sum, the Universal Corpus is represented asa massive store of records, each representing asingle sentence or lexical entry, using a limitedset of attributes.
The store is indexed for effi-cient access, and supports access to slices identi-fied by language, content, provenance, rights, andso forth.
Many component collections would be?unioned?
into this single, large Corpus, with onlythe record identifiers capturing the distinction be-tween the various data sources.Special cases of aligned text and wordlists,spanning more than 1,000 languages, are Bibletranslations and Swadesh wordlists (Resnik et al,1999; Swadesh, 1955).
Here there are obvioususe-cases for accessing a particular verse or wordacross all languages.
However, it is not neces-sary to model n-way language alignments.
In-stead, such sources are implicitly aligned by virtueof their structure.
Extracting all translations ofa verse, or all cognates of a Swadesh wordlistitem, is an index operation that returns monolin-gual records, e.g.
:ID: swadesh/47 ID: swadesh/47LANGS: fra LANGS: engLEX: chien LEX: dog4 Building the CorpusData collection on this scale is a dauntingprospect, yet it is important to avoid the paraly-sis of over-planning.
We can start immediately byleveraging existing infrastructure, and the volun-tary effort of interested members of the languageresources community.
One possibility is to founda ?Language Commons,?
an open access reposi-tory of language resources hosted in the InternetArchive, with a lightweight method for commu-nity members to contribute data sets.A fully processed and indexed version of se-lected data can be made accessible via a web ser-vices interface to a major cloud storage facility,such as Amazon Web Services.
A common queryinterface could be supported via APIs in multi-ple NLP toolkits such as NLTK and GATE (Birdet al, 2009; Cunningham et al, 2002), and alsoin generic frameworks such as UIMA and SOAP,leaving developers to work within their preferredenvironment.4.1 Motivation for data providersWe hope that potential contributors of data willbe motivated to participate primarily by agree-ment with the goals of the project.
Even some-one who has specialized in a particular languageor language family maintains an interest, we ex-pect, in the universal question?the exploration ofLanguage writ large.Data providers will find benefit in the availabil-ity of volunteers for crowd-sourcing, and tools for(semi-)automated quality control, refinement, andpresentation of data.
For example, a data holdershould be able to contribute recordings and gethelp in transcribing them, through a combinationof volunteer labor and automatic processing.Documentary linguists and computational lin-guists have much to gain from collaboration.
In re-turn for the data that documentary linguistics canprovide, computational linguistics has the poten-tial to revolutionize the tools and practice of lan-guage documentation.We also seek collaboration with communities oflanguage speakers.
The corpus provides an econ-omy of scale for the development of literacy mate-rials and tools for interactive language instruction,in support of language preservation and revitaliza-tion.
For small languages, literacy in the mothertongue is often defended on the grounds that it pro-vides the best route to literacy in the national lan-guage (Wagner, 1993, ch.
8).
An essential ingredi-ent of any local literacy program is to have a sub-stantial quantity of available texts that representfamiliar topics including cultural heritage, folk-lore, personal narratives, and current events.
Tran-sition to literacy in a language of wider commu-nication is aided when transitional materials areavailable (Waters, 1998, pp.
61ff).
Mutual bene-fits will also flow from the development of toolsfor low-cost publication and broadcast in the lan-guage, with copies of the published or broadcastmaterial licensed to and archived in the corpus.4.2 RolesThe enterprise requires collaboration of many in-dividuals and groups, in a variety of roles.Editors.
A critical group are people with suffi-cient engagement to serve as editors for particularlanguage families, who have access to data or areable to negotiate redistribution rights, and overseethe workflow of transcription, translation, and an-notation.93CL Research.
All manual annotation steps needto be automated.
Each step presents a challeng-ing semi-supervised learning and cross-linguisticbootstrapping problem.
In addition, the overallmeasure of success?induction of machine trans-lation systems from limited resources?pushes thestate of the art (Kumar et al, 2007).
Numerousother CL problems arise: active learning to im-prove the quality of alignments and bilingual lex-icons; automatic language identification for low-density languages; and morphology learning.Tool builders.
We need tools for annotation, for-mat conversion, spidering and language identifica-tion, search, archiving, and presentation.
Innova-tive crowd-sourcing solutions are of particular in-terest, e.g.
web-based functionality for transcrib-ing audio and video of oral literature, or setting upa translation service based on aligned texts for alow-density language, and collecting the improvedtranslations suggested by users.Volunteer annotators.
An important reason forkeeping the data model as lightweight as possibleis to enable contributions from volunteers with lit-tle or no linguistic training.
Two models are thevolunteers who scan documents and correct OCRoutput in Project Gutenberg, or the undergraduatevolunteers who have constructed Greek and Latintreebanks within Project Perseus (Crane, 2010).Bilingual lexicons that have been extracted fromaligned text collections might be corrected usingcrowd-sourcing, leading to improved translationmodels and improved alignments.
We also see theUniversal Corpus as an excellent opportunity forundergraduates to participate in research, and fornative speakers to participate in the preservation oftheir language.Documentary linguists.
The collection proto-col known as Basic Oral Language Documentation(BOLD) enables documentary linguists to collect2?3 orders of magnitude more oral discourse thanbefore (Bird, 2010).
Linguists can equip localspeakers to collect written texts, then to carefully?respeak?
and orally translate the texts into a refer-ence language.
With suitable tools, incorporatingactive learning, local speakers could further curatebilingual texts and lexicons.
An early need is pi-lot studies to determine costings for different cat-egories of language.Data agencies.
The LDC and ELRA have a cen-tral role to play, given their track record in obtain-ing, curating, and publishing data with licensesthat facilitate language technology development.We need to identify key resources where negoti-ation with the original data provider, and wherepayment of all preparation costs plus compensa-tion for lost revenue, leads to new material for theCorpus.
This is a new publication model and anew business model, but it can co-exist with theexisting models.Language archives.
Language archives have aspecial role to play as holders of unique materi-als.
They could contribute existing data in its na-tive format, for other participants to process.
Theycould give bilingual texts a distinct status withintheir collections, to facilitate discovery.Funding agencies.
To be successful, the HumanLanguage Project would require substantial funds,possibly drawing on a constellation of public andprivate agencies in many countries.
However, inthe spirit of starting small, and starting now, agen-cies could require that sponsored projects whichcollect texts and build lexicons contribute them tothe Language Commons.
After all, the most effec-tive time to do translation, alignment, and lexiconwork is often at the point when primary data isfirst collected, and this extra work promises directbenefits to the individual project.4.3 Early tasksSeed corpus.
The central challenge, we believe,is getting critical mass.
Data attracts data, and ifone can establish a sufficient seed, the effort willsnowball.
We can make some concrete proposalsas to how to collect a seed.
Language resourceson the web are one source?the Cru?bada?n projecthas identified resources for 400 languages, for ex-ample (Scannell, 2008); the New Testament of theBible exists in about 1200 languages and containsof the order of 100k words.
We hope that exist-ing efforts that are already well-disposed towardelectronic distribution will participate.
We partic-ularly mention the Language and Culture Archiveof the Summer Institute of Linguistics, and theRosetta Project.
The latter is already distributedthrough the Internet Archive and contains materialfor 2500 languages.Resource discovery.
Existing language re-sources need to be documented, a large un-94dertaking that depends on widely distributedknowledge.
Existing published corpora from theLDC, ELRA and dozens of other sources?a totalof 85,000 items?are already documented in thecombined catalog of the Open Language ArchivesCommunity,3 so there is no need to recreate thisinformation.
Other resources can be logged bycommunity members using a public access wiki,with a metadata template to ensure key fields areelicited such as resource owner, license, ISO 639language code(s), and data type.
This informationcan itself be curated and stored in the form of anOLAC archive, to permit search over the union ofthe existing and newly documented items.
Workalong these lines has already been initiated byLDC and ELRA (Cieri et al, 2010).Resource classification.
Editors with knowl-edge of particular language families will catego-rize documented resources relative to the needs ofthe project, using controlled vocabularies.
Thisinvolves examining a resource, determining thegranularity and provenance of the segmentationand alignment, checking its ISO 639 classifi-cations, assigning it to a logarithmic size cate-gory, documenting its format and layout, collect-ing sample files, and assigning a priority score.Acquisition.
Where necessary, permission willbe sought to lodge the resource in the repository.Funding may be required to buy the rights to theresource from its owner, as compensation for lostrevenue from future data sales.
Funding may berequired to translate the source into a referencelanguage.
The repository?s ingestion process isfollowed, and the resource metadata is updated.Text collection.
Languages for which the avail-able resources are inadequate are identified, andthe needs are prioritized, based on linguistic andgeographical diversity.
Sponsorship is soughtfor collecting bilingual texts in high priority lan-guages.
Workflows are developed for languagesbased on a variety of factors, such as availabilityof educated people with native-level proficiencyin their mother tongue and good knowledge ofa reference language, internet access in the lan-guage area, availability of expatriate speakers in afirst-world context, and so forth.
A classificationscheme is required to help predict which work-flows will be most successful in a given situation.3http://www.language-archives.org/Audio protocol.
The challenge posed by lan-guages with no written literature should not beunderestimated.
A promising collection methodis Basic Oral Language Documentation, whichcalls for inexpensive voice recorders and net-books, project-specific software for transcriptionand sentence-aligned translation, network band-width for upload to the repository, and suitabletraining and support throughout the process.Corpus readers.
Software developers will in-spect the file formats and identify high priority for-mats based on information about resource priori-ties and sizes.
They will code a corpus reader, anopen source reference implementation for convert-ing between corpus formats and the storage modelpresented in section 3.4.4 Further challengesThere are many additional difficulties that couldbe listed, though we expect they can be addressedover time, once a sufficient seed corpus is estab-lished.
Two particular issues deserve further com-ment, however.Licenses.
Intellectual property issues surround-ing linguistic corpora present a complex andevolving landscape (DiPersio, 2010).
For users, itwould be ideal for all materials to be available un-der a single license that permits derivative works,commercial use, and redistribution, such as theCreative Commons Attribution License (CC-BY).There would be no confusion about permissibleuses of subsets and aggregates of the collected cor-pora, and it would be easy to view the UniversalCorpus as a single corpus.
But to attract as manydata contributors as possible, we cannot make sucha license a condition of contribution.Instead, we propose to distinguish between:(1) a digital Archive of contributed corpora thatare stored in their original format and made avail-able under a range of licenses, offering preserva-tion and dissemination services to the languageresources community at large (i.e.
the LanguageCommons); and (2) the Universal Corpus, whichis embodied as programmatic access to an evolv-ing subset of materials from the archive underone of a small set of permissive licenses, licenseswhose unions and intersections are understood(e.g.
CC-BY and its non-commercial counterpartCC-BY-NC).
Apart from being a useful service inits own right, the Archive would provide a staging95ground for the Universal Corpus.
Archived cor-pora having restrictive licenses could be evaluatedfor their potential as contributions to the Corpus,making it possible to prioritize the work of nego-tiating more liberal licenses.There are reasons to distinguish Archive andCorpus even beyond the license issues.
The Cor-pus, but not the Archive, is limited to the formatsthat support automatic cross-linguistic processing.Conversely, since the primary interface to the Cor-pus is programmatic, it may include materials thatare hosted in many different archives; it only needsto know how to access and deliver them to the user.Incidentally, we consider it an implementation is-sue whether the Corpus is provided as a web ser-vice, a download service with user-side software,user-side software with data delivered on physicalmedia, or a cloud application with user programsexecuted server-side.Expenses of conversion and editing.
We do nottrivialize the work involved in converting docu-ments to the formats of section 3, and in manu-ally correcting the results of noisy automatic pro-cesses such as optical character recognition.
In-deed, the amount of work involved is one moti-vation for the lengths to which we have gone tokeep the data format simple.
For example, we havedeliberately avoided specifying any particular to-kenization scheme.
Variation will arise as a con-sequence, but we believe that it will be no worsethan the variability in input that current machinetranslation training methods routinely deal with,and will not greatly injure the utility of the Corpus.The utter simplicity of the formats also widens thepool of potential volunteers for doing the manualwork that is required.
By avoiding linguisticallydelicate annotation, we can take advantage of mo-tivated but untrained volunteers such as studentsand members of speaker communities.5 ConclusionNearly twenty years ago, the linguistics commu-nity received a wake-up call, when Hale et al(1992) predicted that 90% of the world?s linguis-tic diversity would be lost or moribund by the year2100, and warned that linguistics might ?go downin history as the only science that presided oblivi-ously over the disappearance of 90 per cent of thevery field to which it is dedicated.?
Today, lan-guage documentation is a high priority in main-stream linguistics.
However, the field of computa-tional linguistics is yet to participate substantially.The first half century of research in compu-tational linguistics?from circa 1960 up to thepresent?has touched on less than 1% of theworld?s languages.
For a field which is justlyproud of its empirical methods, it is time to applythose methods to the remaining 99% of languages.We will never have the luxury of richly annotateddata for these languages, so we are forced to askourselves: can we do more with less?We believe the answer is ?yes,?
and so we chal-lenge the computational linguistics community toadopt a scalable computational approach to theproblem.
We need leaner methods for buildingmachine translation systems; new algorithms forcross-linguistic bootstrapping via multiple paths;more effective techniques for leveraging humaneffort in labeling data; scalable ways to get bilin-gual text for unwritten languages; and large scalesocial engineering to make it all happen quickly.To believe we can build this Universal Corpus iscertainly audacious, but not to even try is arguablyirresponsible.
The initial step parallels earlier ef-forts to create large machine-readable text collec-tions which began in the 1960s and reverberatedthrough each subsequent decade.
Collecting bilin-gual texts is an orthodox activity, and many alter-native conceptions of a Human Language Projectwould likely include this as an early task.The undertaking ranks with the largest data-collection efforts in science today.
It is not achiev-able without considerable computational sophis-tication and the full engagement of the field ofcomputational linguistics.
Yet we require no fun-damentally new technologies.
We can build onour strengths in corpus-based methods, linguis-tic models, human- and machine-supplied annota-tions, and learning algorithms.
By rising to this,the greatest language challenge of our time, weenable multi-lingual technology development at anew scale, and simultaneously lay the foundationsfor a new science of empirical universal linguis-tics.AcknowledgmentsWe are grateful to Ed Bice, Doug Oard, GarySimons, participants of the Language Commonsworking group meeting in Boston, students inthe ?Digitizing Languages?
seminar (University ofMichigan), and anonymous reviewers, for feed-back on an earlier version of this paper.96ReferencesSteven Abney.
2007.
Semisupervised Learning forComputational Linguistics.
Chapman & Hall/CRC.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.
http://nltk.org/book.Steven Bird.
2010.
A scalable method for preservingoral literature from small languages.
In Proceedingsof the 12th International Conference on Asia-PacificDigital Libraries, pages 5?14.Daan Broeder and Peter Wittenburg.
2006.
The IMDImetadata framework, its current application and fu-ture direction.
International Journal of Metadata,Semantics and Ontologies, 1:119?132.Christopher Cieri, Khalid Choukri, Nicoletta Calzo-lari, D. Terence Langendoen, Johannes Leveling,Martha Palmer, Nancy Ide, and James Pustejovsky.2010.
A road map for interoperable language re-source metadata.
In Proceedings of the 7th Interna-tional Conference on Language Resources and Eval-uation (LREC).Gregory R. Crane.
2010.
Perseus Digital Library:Research in 2008/09.
http://www.perseus.tufts.edu/hopper/research/current.Accessed Feb. 2010.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE: anarchitecture for development of robust HLT appli-cations.
In Proceedings of 40th Annual Meetingof the Association for Computational Linguistics,pages 168?175.
Association for ComputationalLinguistics.Denise DiPersio.
2010.
Implications of a permis-sions culture on the development and distributionof language resources.
In FLaReNet Forum 2010.Fostering Language Resources Network.
http://www.flarenet.eu/.Hale, M. Krauss, L. Watahomigie, A. Yamamoto, andC.
Craig.
1992.
Endangered languages.
Language,68(1):1?42.Nikolaus P. Himmelmann.
2006.
Language documen-tation: What is it and what is it good for?
InJost Gippert, Nikolaus Himmelmann, and UlrikeMosel, editors, Essentials of Language Documenta-tion, pages 1?30.
Mouton de Gruyter.Human Genome Project.
2007.
The sciencebehind the Human Genome Project.
http://www.ornl.gov/sci/techresources/Human_Genome/project/info.shtml.Accessed Dec. 2007.W.
John Hutchins and Harold L. Somers.
1992.
An In-troduction to Machine Translation.
Academic Press.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Shankar Kumar, Franz J. Och, and WolfgangMacherey.
2007.
Improving word alignment withbridge languages.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 42?50,Prague, Czech Republic.
Association for Computa-tional Linguistics.Mike Maxwell and Baden Hughes.
2006.
Frontiersin linguistic annotation for lower-density languages.In Proceedings of the Workshop on Frontiers in Lin-guistically Annotated Corpora 2006, pages 29?37,Sydney, Australia, July.
Association for Computa-tional Linguistics.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The Bible as a parallel corpus: Annotatingthe ?book of 2000 tongues?.
Computers and the Hu-manities, 33:129?153.Kevin Scannell.
2008.
The Cru?bada?n Project: Corpusbuilding for under-resourced languages.
In Cahiersdu Cental 5: Proceedings of the 3rd Web as CorpusWorkshop.Gary Simons and Steven Bird.
2003.
The Open Lan-guage Archives Community: An infrastructure fordistributed archiving of language resources.
Liter-ary and Linguistic Computing, 18:117?128.Morris Swadesh.
1955.
Towards greater accuracyin lexicostatistic dating.
International Journal ofAmerican Linguistics, 21:121?137.Tama?s Va?radi, Steven Krauwer, Peter Wittenburg,Martin Wynne, and Kimmo Koskenniemi.
2008.CLARIN: common language resources and technol-ogy infrastructure.
In Proceedings of the Sixth Inter-national Language Resources and Evaluation Con-ference.
European Language Resources Association.Daniel A. Wagner.
1993.
Literacy, Culture, and Devel-opment: Becoming Literate in Morocco.
CambridgeUniversity Press.Glenys Waters.
1998.
Local Literacies: Theory andPractice.
Summer Institute of Linguistics, Dallas.Douglas H. Whalen and Gary Simons.
2009.
En-dangered language families.
In Proceedings of the1st International Conference on Language Docu-mentation and Conservation.
University of Hawaii.http://hdl.handle.net/10125/5017.Anthony C. Woodbury.
2010.
Language documenta-tion.
In Peter K. Austin and Julia Sallabank, edi-tors, The Cambridge Handbook of Endangered Lan-guages.
Cambridge University Press.Fei Xia and William D. Lewis.
2007.
Multilingualstructural projection across interlinearized text.
InProceedings of the Meeting of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL).
Association for ComputationalLinguistics.97
