Proceedings of NAACL HLT 2007, pages 25?32,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsWhat Decisions Have You Made: Automatic Decision Detection inConversational SpeechPei-Yun HsuehSchool of InformaticsUniversity of EdinburghEdinburgh EH9 8WL, UKp.hsueh@ed.ac.ukJohanna MooreSchool of InformaticsUniversity of EdinburghEdinburgh EH9 8WL, UKJ.Moore@ed.ac.ukAbstractThis study addresses the problem of au-tomatically detecting decisions in conver-sational speech.
We formulate the prob-lem as classifying decision-making unitsat two levels of granularity: dialogue actsand topic segments.
We conduct an em-pirical analysis to determine the charac-teristic features of decision-making dia-logue acts, and train MaxEnt models usingthese features for the classification tasks.We find that models that combine lexi-cal, prosodic, contextual and topical fea-tures yield the best results on both tasks,achieving 72% and 86% precision, respec-tively.
The study also provides a quantita-tive analysis of the relative importance ofthe feature types.1 IntroductionMaking decisions is an important aspect of conver-sations in collaborative work.
In the context of meet-ings, the proposed argumentative models, e.g., inPallotta et al (2005) and Rienks et al (2005), havespecified decisions as an essential outcome of meet-ings.
Whittaker et al (2005) have also describedhow reviewing decisions is critical to the re-use ofmeeting recordings.
For example, a new engineerwho just get assigned to a project will need to knowwhat major decisions have been made in previousmeetings.
Unless all decisions are recorded in meet-ing minutes or annotated in the speech recordings, itis difficult to locate the decision points by the brows-ing and playback utilities alone.Banerjee and Rudnicky (2005) have shown thatit is easier for users to retrieve the informationthey seek if the meeting record includes informationabout topic segmentation, speaker role, and meet-ing state (e.g., discussion, presentation, briefing).
Toassist users in identifying or revisiting decisions inmeeting archives, our goal is to automatically iden-tify the dialogue acts and segments where decisionsare made.
Because reviewing decisions is indis-pensable in collaborative work, automatic decisiondetection is expected to lend support to computer-assisted meeting tracking and understanding (e.g.,assisting in the fulfilment of the decisions made inthe meetings) and the development of group infor-mation management applications (e.g., constructinggroup memory).2 Related WorkSpontaneous face-to-face dialogues in meetings vi-olate many assumptions made by techniques pre-viously developed for broadcast news (e.g., TDTand TRECVID), telephone conversations (e.g.,Switchboard), and human-computer dialogues (e.g.,DARPA Communicator).
In order to developtechniques for understanding multiparty dialogues,smart meeting rooms have been built at several insti-tutes to record large corpora of meetings in naturalcontexts, including CMU (Waibel et al, 2001), LDC(Cieri et al, 2002), NIST (Garofolo et al, 2004),ICSI (Janin et al, 2003), and in the context of theIM2/M4 project (Marchand-Mailet, 2003).
Morerecently, scenario-based meetings, in which partic-25ipants are assigned to different roles and given spe-cific tasks, have been recorded in the context ofthe CALO project (the Y2 Scenario Data) (CALO,2003) and the AMI project (Carletta et al, 2005).The availability of meeting corpora has enabledresearchers to begin to develop descriptive modelsof meeting discussions.
Some researchers are mod-elling the dynamics of the meeting, exploiting dia-logue models previously proposed for dialogue man-agement.
For example, Niekrasz et al (2005) usethe Issue-Based Information System (IBIS) model(Kunz and Ritte, 1970) to incorporate the historyof dialogue moves into the Multi-Modal Discourse(MMD) ontology.
Other researchers are modellingthe content of the meeting using the type of struc-tures proposed in work on argumentation.
For ex-ample, Rienks et al (2005) have developed an ar-gument diagramming scheme to visualize the rela-tions (e.g., positive, negative, uncertain) between ut-terances (e.g., statement, open issue), and Marchandet al (2003) propose a schema to model different ar-gumentation acts (e.g., accept, request, reject) andtheir organization and synchronization.
Decisionsare often seen as a by-product of these models.Automatically extracting these argument mod-els is a challenging task.
However, researchershave begun to make progress towards this goal.For example, Gatica et al (2005) and Wrede andShriberg (2003) automatically identify the level ofemotion in meeting spurts (e.g., group level of in-terest, hot spots).
Other researchers have developedmodels for detecting agreement and disagreementin meetings, using models that combine lexical fea-tures with prosodic features (e.g., pause, duration,F0, speech rate) (Hillard et al, 2003) and struc-tural information (e.g., the previous and followingspeaker) (Galley et al, 2004).
More recently, Purveret al (2006) have tackled the problem of detectingone type of decision, namely action items, whichembody the transfer of group responsibility.
How-ever, no prior work has addressed the problem of au-tomatically identifying decision-making units moregenerally in multiparty meetings.
Moreover, no pre-vious research has provided a quantitative accountof the effects of different feature types on the task ofautomatic decision detection.3 Research GoalOur aim is to develop models for automatically de-tecting segments of conversation that contain deci-sions directly from the audio recordings and tran-scripts of the meetings, and to identify the featurecombinations that are most effective for this task.Meetings can be viewed at different levels ofgranularity.
In this study, we first consider how todetect the dialogue acts that contain decision-relatedinformation (DM DAs).
Since it is often difficultto interpret a decision without knowing the currenttopic of discussion, we are also interested in detect-ing decision-making segments at a coarser level ofgranularity: topic segments.
The task of automaticdecision detection can therefore be divided into twosubtasks: detecting DM DAs and detecting decision-making topic segments (DM Segments).In this study we propose to first empiricallyidentify the features that are most characteristic ofdecision-making dialogue acts and then computa-tionally integrate the characteristic features to locatethe DM DAs in meeting archives.
For the latter task,previous research on automatic meeting understand-ing and tracking has commonly utilized a classifica-tion framework, in which variants of generative andconditional models are computed directly from data.In this study, we use a Maximum Entropy (MaxEnt)classifier to combine the decision characteristic fea-tures to predict DM DAs and DM Segments.4 Data4.1 Decision AnnotationIn this study, we use a set of 50 scenario-drivenmeetings (approximately 37,400 dialogue acts) thathave been segmented into dialogue acts and anno-tated with decision information in the AMI meet-ing corpus.
These meetings are driven by a sce-nario, wherein four participants play the role ofProject Manager, Marketing Expert, Industrial De-signer, and User Interface Designer in a design teamin a series of four meetings.
Each series of meet-ing recordings uses four distinctive speakers differ-ent from other series.
The corpus includes manualtranscripts for all meetings.
It also comes with in-dividual sound files recorded by close-talking head-mounted microphones and cross-talking sound filesrecorded by desktop microphones.264.1.1 Decision-Making Dialogue ActsIn fact, it is difficult to determine whether a di-alogue act contains information relevant to any de-cision point without knowing what decisions havebeen made in the meeting.
Therefore, in this studyDM DAs are annotated in a two-phase process:First, annotators are asked to browse through themeeting record and write an abstractive summarydirected to the project manager about the decisionsthat have been made in the meeting.
Next, anothergroup of three annotators are asked to produce ex-tractive summaries by selecting a subset (around10%) of dialogue acts which form a summary of thismeeting for the absent manager to understand whathas transpired in the meeting.Finally, this group of annotators are asked to gothrough the extractive dialogue acts one by one andjudge whether they support any of the sentences inthe decision section of the abstractive summary; if adialogue act is related to any sentence in the decisionsection, a ?decision link?
from the dialogue act tothe decision sentence is added.
For those extracteddialogue acts that do not have any closely relatedsentence, the annotators are not obligated to specifya link.
We then label the dialogue acts that have oneor more decision links as DM DAs.In the 50 meetings we used for the experiments,the annotators have on average found four decisionsper meeting and specified around two decision linksto each sentence in the decision summary section.Overall, 554 out of 37,400 dialogue acts have beenannotated as DM DAs, accounting for 1.4% of all di-alogue acts in the data set and 12.7% of the orginalextractive summary (which is consisted of the ex-tracted dialogue acts).
An earlier analysis has es-tablished the intercoder reliability of the two-phaseprocess at the level of kappa ranging from 0.5 to0.8.
In this round of experiment, for each meetingin the 50-meeting dataset we randomly choose theDM DA annotation of one annotator as the sourec ofits ground truth data.4.1.2 Decision-Making Topic SegmentsTopic segmentation has also been annotated forthe AMI meeting corpus.
Annotators had the free-dom to mark a topic as subordinated (down to twolevels) wherever appropriate.
As the AMI meetingsare scenario-driven, annotators are expected to findthat most topics recur.
Therefore, they are given astandard set of topic descriptions that can be usedas labels for each identified topic segment.
Annota-tors will only add a new label if they cannot find amatch in the standard set.
The AMI scenario meet-ings contain around 14 topic segments per meeting.Each segment lasts on average 44 dialogue acts longand contains two DM DAs.DM Segments are operationalized as topic seg-ments that contain one or more DM DAs.
Over-all, 198 out of 623 (31.78%) topic segments in the50-meeting dataset are DM Segments.
As the meet-ings we use are driven by a predetermined agenda,we expect to find that interlocutors are more likelyto reach decisions when certain topics are broughtup.
Analysis shows that some topics are indeed morelikely to contain decisions than others.
For example,80% of the segments labelled as Costing and 58%of those labelled Budget are DM Segments, whereasonly 7% of the Existing Product segments and noneof the Trend-Watching segments are DM Segments.Functional segments, such as Chitchat, Opening andClosing, almost never include decisions.4.2 Features UsedTo provide a qualitative account of the effect of dif-ferent feature types on the task of automatic decisiondetection, we have conducted empirical analysis onfour major types of features: lexical, prosodic, con-textual and topical features.4.2.1 Lexical FeaturesPrevious research has studied lexical differences(i.e., occurrence counts of N-grams) between var-ious aspects of speech, such as topics (Hsueh andMoore, 2006), speaker gender (Boulis and Osten-dorf, 2005), and story-telling conversation (Gordonand Ganesan, 2005).
As we expect that lexical dif-ferences also exist in DM conversations, we gener-ated language models from the DM Dialogue Acts inthe corpus.
The comparison of the language modelsgenerated from the DM dialogue Acts and the rest ofthe conversations shows that some differences existbetween the two models: (1) decision making con-versations are more likely to contain we than I andYou; (2) in decision-making conversations there aremore explicit mentions of topical words, such as ad-vanced chips and functional design; (3) in decision-27Type FeatureDuration Number of words spoken in current, previous and next subdialogueDuration (in seconds) of current, previous and next subdialoguePause Amount of silence (in seconds) preceding a subdialogueAmount of silence (in seconds) following a subdialogueSpeech rate Number of words spoken per second in current, previous and next subdialogueNumber of syllables per second in current, previous and next subdialogueEnergy Overall energy levelAverage energy level in the first, second, third, and fourth quarter of a subdialoguePitch Maximum and minimum F0, overall slope and varianceSlope and variance at the first 100 and 200 ms and last 100 and 200 ms,at the first and second half, and at each quarter of the subdialogueTable 1: Prosodic features used in this study.making conversations, there are fewer negative ex-pressions, such as I don?t think and I don?t know.In an exploratory study using unigrams, as well asbigrams and trigrams, we found that using bigramsand trigrams does not improve the accuracy of clas-sifying DM DAs, and therefore we include only uni-grams in the set of lexical features in the experimentsreported in Section 6.4.2.2 Prosodic FeaturesFunctionally, prosodic features, i.e., energy, andfundamental frequency (F0), are indicative of seg-mentation and saliency.
In this study, we followShriberg and Stolcke?s (2001) direct modelling ap-proach to manifest prosodic features as duration,pause, speech rate, pitch contour, and energy level.We utilize the individual sound files provided in theAMI corpus.
To extract prosodic features from thesound files, we use the Snack Sound Toolkit to com-pute a list of pitch and energy values delimited byframes of 10 ms, using the normalized cross correla-tion function.
Then we apply a piecewise linearisa-tion procedure to remove the outliers and average thelinearised values of the units within the time frameof a word.
Pitch contour of a dialogue act is ap-proximated by measuring the pitch slope at multi-ple points within the dialogue act, e.g., the first andlast 100 and 200 ms.
The rate of speech is calcu-lated as both the number of words spoken per sec-ond and the number of syllables per second.
Weuse Festival?s speech synthesis front-end to returnphonemes and syllabification information.
An ex-ploratory study has shown the benefits of includingimmediate prosodic contexts, and thus we also in-clude prosodic features of the immediately preced-ing and following dialogue acts.
Table 1 containsa list of automatically generated prosodic featuresused in this study.4.2.3 Contextual FeaturesFrom our qualitative analysis, we expect that con-textual features specific to the AMI corpus, such asthe speaker role (i.e., PM, ME, ID, UID) and meet-ing type (i.e., kick-off, conceptual design, functionaldesign, detailed design) to be characteristic of theDM DAs.
Analysis shows that (1) participants as-signed to the role of PM produce 42.5% of the DMDAs, and (2) participants make relatively fewer de-cisions in the kick-off meetings.
Analysis has alsodemonstrated a difference in the type, the reflexiv-ity1 and the number of addressees, between the DMDAs and the non-DM DAs.
For example, dialogueacts of type inform, suggest, elicit assessment andelicit inform are more likely to be DM DAs.We have also found that immediately precedingand following dialogue acts are important for iden-tifying DM DAs.
For example, stalls and frag-ments preceding and fragments following a DMDA are more likely than for non-DM DAs.2 In1According to the annotation guideline, the reflexivity re-flects on how the group is carrying on the task.
In this case, theinterlocutors pause to evaluate the group performance less oftenwhen it comes to decision making.2STALL is where people start talking before they are ready,or keep speaking when they haven?t figured out what to say;FRAGMENT is the segment which is not really speech or isunclear enough to be transcribed, or where the speaker did not28contrast, there is a lower chance of seeing sug-gest and elicit-type DAs (i.e., elicit-inform, elicit-suggestion, elicit-assessment) in the preceding andfollowing DM DAs.4.2.4 Topical FeaturesAs reported in Section 4.1.2, we find that inter-locutors are more likely to reach decisions when cer-tain topics are brought up.
Also, we expect decision-making conversations to take place towards the endof a topic segment.
Therefore, in this study we in-clude the following features: the label of the currenttopic segment, the position of the DA in a topic seg-ment (measured in words, in seconds, and in %), thedistance to the previous topic shift (both at the top-level and sub-topic level)(measured in seconds), theduration of the current topic segment (both at thetop-level and sub-topic level)(measured in seconds).5 Experiment5.1 Classifying DM DAsDetecting DM DAs is the first step of automatic de-cision detection.
For this purpose, we trained Max-Ent models to classify each unseen sample as ei-ther DM DA (POS) or non-DM DA (NEG).
We per-formed a 5-fold cross validation on the set of 50meetings.
In each fold, we trained MaxEnt mod-els from the feature combinations in the trainingset, wherein each of the extracted dialogue acts hasbeen labelled as either POS or NEG.
Then, themodels were used to classify unseen instances inthe test set as either POS or NEG.
In Section 4.2,we described the four major types of features usedin this study: unigrams (LX1), prosodic (PROS),contextual (CONT), and topical (TOPIC) features.For comparison, we report the naive baseline ob-tained by training the models on the prosodic fea-tures alone, since the prosodic features can be gen-erated fully automatically.
The different combina-tions of features we used for training models canbe divided into the following four groups: (A) us-ing prosodic features alone (BASELINE), (B) us-ing lexical, contextual and topical features alone(LX1, CONT, TOPIC); (C) using all available fea-tures except one of the four types of features (ALL-LX1, ALL-PROS, ALL-CONT, ALL-TOPIC); andget far enough to express the intention.
(D) using all available features (ALL).6 Results6.1 Classifying DM SegmentsDetecting DM segments is necessary for interpret-ing decisions, as it provides information about thecurrent topic of discussion.
Here we combine thepredictions of the DM DAs to classify each unseentopic segment in the test set as either DM Segment(POS) or non-DM Segment (NEG).
Recall that wedefined a DM Segment as a segment that containsone or more hypothesized DM DAs.
The task of de-tecting DM Segments can thus be viewed as that ofdetecting DM Dialogue Acts in a wider window.6.2 EXP1: Classifying DM DAsTable 2 reports the performance on the test set.
Theresults show that models trained with all features(ALL), including lexical, prosodic, contextual andtopical features, yield substantially better perfor-mance than the baseline on the task of detecting DMDAs.
We carried out a one-way ANOVA to exam-ine the effect of different feature combinations onoverall accuracy (F1).
The ANOVA suggests a reli-able effect of feature type (F (9, 286) = 3.44; p <0.001).
Rows 2-4 in Table 2 report the performanceof models in Group B that are trained with a sin-gle type of feature.
Lexical features are the mostpredictive features when used alone.
We performedsign tests to determine whether there are statisticaldifferences among these models and the baseline.We find that when used alone, only lexical features(LX1) can train a better model than the baseline(p < 0.001).
However, none of these models yieldsa comparable performance to the ALL model.To study the relative effect of the different fea-ture types, Rows 5-8 in the table report the perfor-mance of models in Group C, which are trained withall available features except LX1, PROS, CONT andTOPIC features respectively.
The amount of degra-dation in the overall accuracy (F1) of each of themodels in relation to that of the ALL model indi-cates the contribution of the feature type that hasbeen left out of the model.
We performed sign teststo examine the differences among these models andthe ALL model.
We find that the ALL model out-performs all of these models (p < 0.001) except29Exact Match Lenient MatchAccuracy Precision Recall F1 Precision Recall F1BASELINE(PROS) 0.32 0.06 0.1 0.32 0.1 0.15LX1 0.53 0.3 0.38 0.6 0.43 0.5CONT 0 0 0 0 0 0TOPIC 0.49 0.11 0.17 0.57 0.11 0.17ALL-PROS 0.63 0.47 0.54 0.71 0.57 0.63ALL-LX1 0.61 0.34 0.44 0.65 0.43 0.52ALL-CONT 0.66 0.62 0.64 0.69 0.68 0.69ALL-TOPIC 0.72 0.54 0.62 0.7 0.52 0.59ALL 0.72 0.54 0.62 0.76 0.64 0.7Table 2: Effects of different combinations of features on detecting DM DAs.the model trained by leaving out contextual features(ALL-CONT).
A closer investigation of the preci-sion and recall of the ALL-CONT model shows thatthe contextual features are detrimental to recall butbeneficial for precision.
The mixed result is due tothe fact that models trained with contextual featuresare tailored to recognize particular types of DM di-alogue acts.
Therefore, using these contextual fea-tures improves the precision for these types of DMDAs but reduces the overall recognition accuracy.The last three columns of Table 2 are the resultsobtained using a lenient match measure, allowing awindow of 10 seconds preceding and following a hy-pothesized DM DA for recognition.
The better re-sults show that there is room for ambiguity in theassessment of the exact timing of DM DAs.6.3 EXP2: Classifying DM SegmentsAs expected, the results in Table 3 are better thanthose reported in Table 2, achieving at best 83%overall accuracy.The model that combines all fea-tures (ALL) yields significantly better results thanthe baseline.
The ANOVA shows a reliable effect ofdifferent feature types on the task of detecting DMSegments (F (11, 284) = 2.33; p <= 0.01).
Rows2-4 suggest that lexical features are the most pre-dictive in terms of overall accuracy.
Sign tests con-firm the advantage of using lexical features (LX1)over the baseline (PROS) (p < 0.05).
Interest-ingly, the model that is trained with topical featuresalone (TOPIC) yields substantially better precision(p < 0.001).
The increase from 49% precision forthe task of detecting DM DAs (in Table 2) to 91%for that of detecting DM Segments stems from thefact that decisions are more likely to occur in certaintypes of topic segments.
In turn, training modelswith topical features helps eliminate incorrect pre-dictions of DM DAs in these types of topic seg-ments.
However, the accuracy gain of the TOPICmodel on detecting certain types of DM Segmentsdoes not extend to all types of DM Segments.
This isshown by the significantly lower recall of the TOPICmodel over the baseline (p < 0.001).Finally, Rows 5-8 report the performance of themodels in Group (C) on the task of detecting DMSegments.
Sign tests again show that the model thatis trained with all available features (ALL) outper-forms the models that leave out lexical, prosodic,or topical features (p < 0.05).
However, the ALLmodel does not outperform the model that leaves outcontextual features.
In addition, the contextual fea-tures degrade the recall but improve the precisionon the task of detecting DM Segments.
Calculat-ing how much the overall accuracy of the models inGroup C degrades from the ALL model shows thatthe most predictive features are the lexical features,followed by the topical and prosodic features.7 DiscussionAs suggested by the mixed results obtained by themodel that is trained without the contextual features,the two-phase decision annotation procedure (as de-scribed in Section 4.1) may have caused annota-tors to select dialogue acts that serve different func-tional roles in a decision-making process in the setof DM DAs.
For example, in the dialogue shown30Exact MatchAccuracy Precision Recall F1BASELINE(PROS) 0.67 0.39 0.49LX1 0.69 0.69 0.69CONT 0 0 0TOPIC 0.91 0.17 0.29ALL-PROS 0.82 0.76 0.79ALL-LX1 0.79 0.64 0.7ALL-CONT 0.79 0.86 0.83ALL-TOPIC 0.75 0.73 0.74ALL 0.86 0.8 0.82Table 3: Effects of different combinations of featureson detecting DM Segments.in Figure 1, the annotators have marked dialogueact (1), (5), (8), and (11) as the DM DAs relatedto this decision: ?There will be no feature to helpfind the remote when it is misplaced?.
Among thefour DM DAs, (1) describes the topic of what thisdecision is about; (5) and (8) describe the argumentsthat support the decision-making process; (11) in-dicates the level of agreement or disagreement forthis decision.
Yet these DM DAs which play dif-ferent functional roles in the DM process may eachhave their own characteristic features.
Training onemodel to recognize DM DAs of all functional rolesmay have degraded the performance on the classifi-cation tasks.
Developing models for detecting DMDAs that play different functional roles requires alarger scale study to discover the anatomy of gen-eral decision-making discussions.8 Conclusions and Future WorkThis is the first study that aimed to detect segmentsof the conversation that contain decisions.
We have(1) empirically analyzed the characteristic featuresof DM dialogue acts, and (2) computational devel-oped models to detect DM dialogue acts and DMtopic segments, given the set of characteristic fea-tures.
Empirical analysis has provided a qualitativeaccount of the DM-characteristic features, whereastraining the computational models on different fea-ture combinations has provided a quantitative ac-count of the effect of different feature types onthe task of automatic decision detection.
Empiri-cal analysis has exhibited demonstrable differences(1) A: but um the feature that we considered for itnot getting lost.
(2) B: Right.
Well(3) B: were talking about that a little bit(4) B: when we got that email(5) B: and we think that each of these are sodistinctive, that it it?s not just like another piece oftechnology around your house.
(6) B: It?s gonna be somewhere that it can be seen.
(7) A: Mm-hmm.
(8) B: So we?re we?re not thinking that it?s gonnabe as critical to have the loss(9) D: But if it?s like under covers or like in a couchyou still can?t see it.. .
.
(10) A: Okay , that?s a fair evaluation.
(11) A: Um we so we do we?ve decided not toworry about that for now.Figure 1: Example decision-making discussionin the words (e.g., we), the contextual features (e.g.,meeting type, speaker role, dialogue act type), andthe topical features.
The experimental results havesuggested that (1) the model combining all the avail-able features performs substantially better, achiev-ing 62% and 82% overall accuracy on the task ofdetecting DM DAs and that of detecting DM Seg-ments, respectively, (2) lexical features are the bestindicators for both the task of detecting DM DAs andthat of detecting DM Segments, and (3) combiningtopical features is important for improving the pre-cision for the task of detecting DM Segments.Many of the features used in this study require hu-man intervention, such as manual transcriptions, an-notated dialogue act segmentations and labels, anno-tated topic segmentations and labels, and other typesof meeting-specific features.
Our ultimate goal is toidentify decisions using automatically induced fea-tures.
Therefore, studying the performance degra-dation when using the automatically generated ver-sions of these features (e.g., ASR words) is essen-tial for developing a fully automated component ondetecting decisions immediately after a meeting oreven for when a meeting is still in progress.
An-other problem that has been pointed out in Section 6and in Section 7 is the different functional roles ofDM dialogue acts in current annotations.
Purver etal.
(2006) have suggested a hierarchical annotationscheme to accommodate the different aspects of ac-tion items.
The same technique may be applicable31in a more general decision detection task.9 AcknowledgementThis work was supported by the European Union In-tegrated Project AMI (Augmented Multi-party Inter-action, FP6-506811, publication AMI-204).ReferencesS.
Banerjee, C. Rose, and A. I. Rudnicky.
2005.
Thenecessity of a meeting recording and playback system,and the benefit of topic-level annotations to meetingbrowsing.
In Proceedings of the Tenth InternationalConference on Human-Computer Interaction.C.
Boulis and M. Ostendorf.
2005.
A quantitative anal-ysis of lexical differences between genders in tele-phone conversation.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics.
ACM Press.CALO.
2003. http://www.ai.sri.com/project/calo.J.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,I.
McCowan, W. Post, D. Reidsma, and P. Wellner.2005.
The ami meeting corpus: A pre-announcement.In Proceedings of 2nd Joint Workshop on Multi-modal Interaction and Related Machine Learning Al-gorithms.C.
Cieri, D. Miller, and K. Walker.
2002.
Researchmethodologies, observations and outcomes in conver-sational speech data collection.
In Proceedings of theHuman Language Technologies Conference (HLT).M.
Galley, J. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in con-versational speech: Use of bayesian networks to modelpragmatic dependencies.
In Proceedings of the 42ndAnnual Meeting of the ACL.J.
S. Garofolo, C. D. Laprun, M. Michel, V.M.
Stanford,and E. Tabassi.
2004.
The nist meeting room pilotcorpus.
In Proceedings of LREC04.D.
Gatica-Perez, I. McCowan, D. Zhang, and S. Bengio.2005.
Detecting group interest level in meetings.
InIEEE Int.
Conf.
on Acoustics, Speech, and Signal Pro-cessing (ICASSP).Andrew S. Gordon and Kavita Ganesan.
2005.
Auto-mated story extraction from conversational speech.
InProceedings of the Third International Conference onKnowledge Capture (K-CAP 05).D.
Hillard, M. Ostendorf, and E. Shriberg.
2003.
Detec-tion of agreement vs. disagreement in meetings: Train-ing with unlabeled data.
In Proc.
HLT-NAACL.P.
Hsueh and J. Moore.
2006.
Automatic topic segmen-tation and lablelling in multiparty dialogue.
In the firstIEEE/ACM workshop on Spoken Language Technol-ogy (SLT).
IEEE/ACM.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The icsi meeting corpus.In Proceedings of ICASSP-2003, Hong Kong.W.
Kunz and H. W. J. Ritte.
1970.
Issue as elementsof information system.
Technical Report Working Pa-per 131, Institute of Urban and Regional DevelopmentResearch, University of California, Berkeley.S.
Marchand-Mailet.
2003.
Meeting record modeling forenhanced browsing.
Technical report, Computer Vi-sion and Multimedia Lab, Computer Centre, Univer-sity of Geneva, Switzerland.J.
Niekrasz, M. Purver, J. Dowding, and S. Peters.
2005.Ontology-based discourse understanding for a persis-tent meeting assistant.
In Proc.
of the AAAI SpringSymposium.V.
Pallotta, J. Niekrasz, and M. Purver.
2005.
Collab-orative and argumentative models of meeting discus-sions.
In Proceeding of CMNA-05 workshop on Com-putational Models of Natural Arguments in IJCAI 05.M.
Purver, P. Ehlen, and J. Niekrasz.
2006.
Shallowdiscourse structure for action item detection.
In theWorkshop of HLT-NAACL: Analyzing Conversations inText and Speech.
ACM Press.R.
J. Rienks, D. Heylen, and E. van der Weijden.
2005.Argument diagramming of meeting conversations.
InMultimodal Multiparty Meeting Processing Workshopat the ICMI.E.
Shriberg and A. Stolcke.
2001.
Direct modeling ofprosody: An overview of applications in automaticspeech processing.A.
Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf amdT.
Schultz, H. Soltau, H. Yu, and K. Zechner.
2001.Advances in automatic meeting record creation and ac-cess.
In Proceedings of ICASSP.S.
Whittaker, R. Laban, and S. Tucker.
2005.
Analysingmeeting records: An ethnographic study and techno-logical implications.
In Proceedings of MLMI 2005.B.
Wrede and E. Shriberg.
2003.
Spotting hot spots inmeetings: Human judgements and prosodic cues.
InProceedings of EUROSPEECH 2003.32
