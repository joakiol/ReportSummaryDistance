Proceedings of NAACL-HLT 2013, pages 858?867,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAnswer Extraction as Sequence Tagging with Tree Edit DistanceXuchen Yao and Benjamin Van DurmeJohns Hopkins UniversityBaltimore, MD, USAChris Callison-Burch?University of PennsylvaniaPhiladelphia, PA, USAPeter ClarkVulcan Inc.Seattle, WA, USAAbstractOur goal is to extract answers from pre-retrieved sentences for Question Answering(QA).
We construct a linear-chain ConditionalRandom Field based on pairs of questionsand their possible answer sentences, learningthe association between questions and answertypes.
This casts answer extraction as an an-swer sequence tagging problem for the firsttime, where knowledge of shared structure be-tween question and source sentence is incor-porated through features based on Tree EditDistance (TED).
Our model is free of man-ually created question and answer templates,fast to run (processing 200 QA pairs per sec-ond excluding parsing time), and yields an F1of 63.3% on a new public dataset based onprior TREC QA evaluations.
The developedsystem is open-source, and includes an imple-mentation of the TED model that is state of theart in the task of ranking QA pairs.1 IntroductionThe success of IBM?s Watson system for QuestionAnswering (QA) (Ferrucci et al 2010) has illus-trated a continued public interest in this topic.
Wat-son is a sophisticated piece of software engineeringconsisting of many components tied together in alarge parallel architecture.
It took many researchersworking full time for years to construct.
Such re-sources are not available to individual academic re-searchers.
If they are interested in evaluating newideas on some aspect of QA, they must either con-struct a full system, or create a focused subtask?Performed while faculty at Johns Hopkins University.paired with a representative dataset.
We follow thelatter approach and focus on the task of answer ex-traction, i.e., producing the exact answer strings fora question.We propose the use of a linear-chain ConditionalRandom Field (CRF) (Lafferty et al 2001) in or-der to cast the problem as one of sequence taggingby labeling each token in a candidate sentence as ei-ther Beginning, Inside or Outside (BIO) of an an-swer.
This is to our knowledge the first time aCRF has been used to extract answers.1 We uti-lize not only traditional contextual features based onPOS tagging, dependency parsing and Named EntityRecognition (NER), but most importantly, featuresextracted from a Tree Edit Distance (TED) modelfor aligning an answer sentence tree with the ques-tion tree.
The linear-chain CRF, when trained tolearn the associations between question and answertypes, is a robust approach against error propaga-tion introduced in the NLP pipeline.
For instance,given an NER tool that always (i.e., in both train-ing and test data) recognizes the pesticide DDT asan ORG, our model realizes, when a question isasked about the type of chemicals, the correct an-swer might be incorrectly but consistently recog-nized as ORG by NER.
This helps reduce errors in-troduced by wrong answer types, which were esti-mated as the most significant contributer (36.4%)of errors in the then state-of-the-art QA system ofMoldovan et al(2003).The features based on TED allow us to draw the1CRFs have been used in judging answer-bearing sentences(Shima et al 2008; Ding et al 2008; Wang and Manning,2010), but not extracting exact answers from these sentences.858connection between the question and answer sen-tences before answer extraction, whereas tradition-ally the exercise of answer validation (Magnini etal., 2002; Penas et al 2008; Rodrigo et al 2009)has been performed after as a remedy to ensure theanswer is really ?about?
the question.Motivated by a desire for a fast runtime,2 webase our TED implementation on the dynamic-programming approach of Zhang and Shasha(1989), which helps our final system process 200QA pairs per second on standard desktop hardware,when input is syntactically pre-parsed.In the following we first provide background onthe TED model, going on to evaluate our implemen-tation against prior work in the context of questionanswer sentence ranking (QASR), achieving state ofthe art in that task.
We then describe how we cou-ple TED features to a linear-chain CRF for answerextraction, providing the set of features used, and fi-nally experimental results on an extraction datasetwe make public (together with the software) to thecommunity.3 Related prior work is interspersedthroughout the paper.2 Tree Edit Distance ModelTree Edit Distance (?2.1) models have been showneffective in a variety of applications, including tex-tual entailment, paraphrase identification, answerranking and information retrieval (Reis et al 2004;Kouylekov and Magnini, 2005; Heilman and Smith,2010; Augsten et al 2010).
We chose the variantproposed by Heilman and Smith (2010), inspired byits simplicity, generality, and effectiveness.
Our ap-proach differs from those authors in their relianceon a greedy search routine to make use of a complextree kernel.
With speed a consideration, we optedfor the dynamic-programming solution of Zhangand Shasha (1989) (?2.1).
We added new lexical-semantic features ?
(2.2) to the model and then eval-uated our implementation on the QASR task, show-ing strong results ?
(2.3).Feature Descriptiondistance tree edit distance from answersentence to questionrenNounrenVerbrenOther# edits changing POS from or tonoun, verb, or other typesinsN, insV,insPunc,insDet,insOtherPos# edits inserting a noun, verb,punctuation mark, determineror other POS typesdelN, delV, ... deletion mirror of aboveins{N,V,P}ModinsSub, insObjinsOtherRel# edits inserting a modifier for{noun, verb, preposition}, sub-ject, object or other relationsdelNMod, ... deletion mirror of aboverenNMod, ... rename mirror of aboveXEdits # basic edits plus sum of in-s/del/ren editsalignNodes,alignNum,alignN, alignV,alignProper# aligned nodes, and those thatare numbers, nouns, verbs, orproper nounsTable 1: Features for ranking QA pairs.2.1 Cost Design and Edit SearchFollowing Bille (2005), we define an edit script be-tween trees T1, T2 as the edit sequence transformingT1 to T2 according to a cost function, with the totalsummed cost known as the tree edit distance.
Basicedit operations include: insert, delete and rename.With T a dependency tree, we represent each nodeby three fields: lemma, POS and the type of depen-dency relation to the node?s parent (DEP).
For in-stance, Mary/nnp/sub is the proper noun Mary insubject position.Basic edits are refined into 9 types, where thefirst six (INS LEAF, INS SUBTREE, INS, DEL LEAF,DEL SUBTREE, DEL) insert or delete a leaf node, awhole subtree, or a node that is neither a leaf norpart of a whole inserted subtree.
The last three(REN POS, REN DEP, REN POS DEP) serve to re-name a POS tag, dependency relation, or both.2For instance, Watson was designed under the constraint ofa 3 second response time, arising from its intended live use inthe television gameshow, Jeopardy!.3http://code.google.com/p/jacana/859prdplayernn jennifernncapriatinnp 23cdbevbzsubjnmod nmodtennisnnnmodTennis player Jennifer Capriati is 23TreeEdit Distance capriatinnpjennifernnpwhatwpsportnnplayvbzdovbzvmod vmodnmodWhat sport does Jennifer Capriati playinsSubTree:ins(play/vbz/vmod)ins(do/vbz/root)WordNetFigure 1: Edits transforming a source sentence (left) to a question (right).
Each node consists of: lemma, POS tagand dependency relation, with root nodes and punctuation not shown.
Shown includes deletion (?
and strikethroughon the left), alignment (arrows) and insertion (shaded area).
Order of operations is not displayed.
The standard TEDmodel does not capture the alignment between tennis and sport (see Section 2.2).We begin by uniformly assigning basic edits acost of 1.0,4 which brings the cost of a full node in-sertion or deletion to 3 (all the three fields inserted ordeleted).
We allow renaming of POS and/or relationtype iff the lemmas of source and target nodes areidentical.5 When two nodes are identical and thusdo not appear in the edit script, or when two nodesare renamed due to the same lemma, we say they arealigned by the tree edit model (see Figure 1).We used Zhang and Shasha (1989)?s dynamicprogramming algorithm to produce an optimal editscript with the lowest tree edit distance.
The ap-proach explores both trees in a bottom-up, post-order manner, running in time:O(|T1| |T2|min(D1, L1)min(D2, L2))where |Ti| is the number of nodes, Di is the depth,and Li is the number of leaves, with respect to treeTi.Additionally, we fix the cost of stopword renam-ing to 2.5, even in the case of identity, regardlessof whether two stopwords have the same POS tagsor relations.
Stopwords tend to have fixed POS tagsand dependency relations, which often leads to lessexpensive alignments as compared to renaming con-4This applies separately to each element of the tripartitestructure; e.g., deleting a POS entry, inserting a lemma, etc.5This is aimed at minimizing node variations introduced bymorphology differences, tagging or parsing errors.tent terms.
In practice this gave stopwords ?toomuch say?
in guiding the overall edit sequence.The resultant system is fast in practice, processing10,000 pre-parsed tree pairs per second on a contem-porary machine.62.2 TED for Sentence RankingThe task of Question Answer Sentence Ranking(QASR) takes a question and a set of source sen-tences, returning a list sorted by the probabilitylikelihood that each sentence contains an appropri-ate answer.
Prior work in this includes that of:Punyakanok et al(2004), based on mapping syn-tactic dependency trees; Wang et al(2007) utiliz-ing Quasi-Synchronous Grammar (Smith and Eis-ner, 2006); Heilman and Smith (2010) using TED;and Shima et al(2008), Ding et al(2008) and Wangand Manning (2010), who each employed a CRF invarious ways.
Wang et al(2007) made their datasetpublic, which we use here for system validation.
Todate, models based on TED have shown the best per-formance for this task.Our implementation follows Heilman and Smith(2010), with the addition of 15 new features beyondtheir original 33 (see Table 1).
Based on results6In later tasks, feature extraction and decoding will slowdown the system, but the final system was still able to process200 pairs per second.860set source #ques.
#pairs %pos.
len.TRAIN-ALL TREC8-12 1229 53417 12.0 anyTRAIN TREC8-12 94 4718 7.4 ?
40DEV TREC13 82 1148 19.3 ?
40TEST TREC13 89 1517 18.7 ?
40Table 2: Distribution of data, with imbalance towardsnegative examples (sentences without an answer).in DEV, we extract edits in the direction from thesource sentence to the question.In addition to syntactic features, we incorporatedthe following lexical-semantic relations from Word-Net: hypernym and synonym (nouns and verbs); en-tailment and causing (verbs); and membersOf, sub-stancesOf, partsOf, haveMember, haveSubstance,havePart (nouns).
Such relations have been usedin prior approaches to this task (Wang et al 2007;Wang and Manning, 2010), but not in conjunctionwith the model of Heilman and Smith (2010).These were made into features in two ways:WNsearch loosens renaming and alignment withinthe TED model from requiring strict lemma equal-ity to allowing lemmas that shared any of theabove relations, leading to renaming operations suchas REN ...(country, china) and REN ...(sport,tennis); WNfeature counts how many words be-tween the sentence and answer sentence have eachof the above relations, separately as 10 independentfeatures, plus an aggregate count for a total of 11new features beyond the earlier 48.These features were then used to train a logisticregression model using Weka (Hall et al 2009).2.3 QA Sentence Ranking ExperimentWe trained and tested on the dataset from Wang etal.
(2007), which spans QA pairs from TREC QA8-13 (see Table 2).
Per question, sentences withnon-stopword overlap were first retrieved from thetask collection, which were then compared againstthe TREC answer pattern (in the form of Perl regu-lar expressions).
If a sentence matched, then it wasdeemed a (noisy) positive example.
Finally, TRAIN,DEV and TEST were manually corrected for errors.Those authors decided to limit candidate source sen-System MAP MRRWang et al(2007) 0.6029 0.6852Heilman and Smith (2010) 0.6091 0.6917Wang and Manning (2010) 0.5951 0.6951this paper (48 features) 0.6319 0.7270+WNsearch 0.6371 0.7301+WNfeature (11 more feat.)
0.6307 0.7477Table 3: Results on the QA Sentence Ranking task.tences to be no longer than 40 words.7 Keepingwith prior work, those questions with only positiveor negative examples were removed, leaving 94 ofthe original 100 questions for evaluation.The data was processed by Wang et al(2007)with the following tool chain: POS tags via MX-POST (Ratnaparkhi, 1996); parse trees via MST-Parser (McDonald et al 2005) with 12 coarse-grained dependency relation labels; and named enti-ties via Identifinder (Bikel et al 1999).
Mean Av-erage Precision (MAP) and Mean Reciprocal Rank(MRR) are reported in Table 3.
Our implementa-tion gives state of the art performance, and is fur-thered improved by our inclusion of semantic fea-tures drawn from WordNet.83 CRF with TED for Answer ExtractionIn this section we move from ranking source sen-tences, to the next QA stage: answer extraction.Given our competitive TED-based alignment model,the most obvious solution to extraction would be toreport those spans aligned from a source sentenceto a question?s wh- terms.
However, we show thatthis approach is better formulated as a (strongly in-dicative) feature of a larger set of answer extractionsignals.3.1 Sequence ModelFigure 2 illustrates the task of tagging each token ina candidate sentence with one of the following la-7TRAIN-ALL is not used in QASR, but later for answer ex-traction; TRAIN comes from the first 100 questions of TRAIN-ALL.8As the test set is of limited size (94 questions), then whileour MAP/MRR scores are 2.8% ?
5.6% higher than priorwork, this is not statistically significant according to the PairedRandomization Test (Smucker et al 2007), and thus should beconsidered on par with the current state of the art.861prddla yenjri frddlcri tnyiln2l la 3bvzsum o o o o oFigure 2: An example of linear-chain CRF for an-swer sequence tagging.bels: B-ANSWER (beginning of answer), I-ANSWER(inside of answer), O (outside of answer).Besides local POS/NER/DEP features, at each to-ken we need to inspect the entire input to connect theanswer sentence with the question sentence throughtree edits, drawing features from the question andthe edit script, motivating the use of a linear-chainCRF model (Lafferty et al 2001) over HMMs.
Tothe best of our knowledge this is the first time aCRF has been used to label answer fragments, de-spite success in other sequence tagging tasks.3.2 Feature DesignIn this subsection we describe the local and globalfeatures used by the CRF.Chunking We use the POS/NER/DEP tags directlyjust as one would in a chunking task.
Specifically,suppose t represents the current token position andpos[t] its POS tag, we extract unigram, bigram andtrigram features over the local context, e.g., pos[t?2], pos[t ?
2] : pos[t ?
1], and pos[t ?
2] : pos[t ?1] : pos[t].
Similar features are extracted for namedentity types (ner[t]), and dependency relation labels(dep[t]).Our intuition is these chunking features should al-low for learning which types of words tend to beanswers.
For instance, we expect adverbs to be as-signed lower feature weights as they are rarely apart of answer, while prepositions may have differ-ent feature weights depending on their context.
Forinstance, of in kind of silly has an adjective on theright, and is unlikely to be the Beginning of an an-swer to a TREC-style question, as compared to inwhen paired with a question on time, such as seen inan answer in 90 days, where the preposition is fol-lowed by a number then a noun.Feature Descriptionedit=X type of edit feature.
X: DEL,DEL SUBTREE, DEL LEAF,REN POS, REN DEP, REN POS DEPor ALIGN.X pos=?X ner=?X dep=?Delete features.
X is either DEL,DEL SUBTREE or DEL LEAF.
?represents the correspondingPOS/NER/DEP of the current token.Xpos from=?fXpos to=?tXpos f t=?f ?tXner from=?fXner to=?tXner f t=?f ?tXdep from=?fXdep to=?tXdep f t=?f ?tRename features.
X is eitherREN POS, REN DEP orREN POS DEP.
Suppose word f inanswer is renamed to word t inquestion, then ?f and ?t representcorresponding POS/NER/DEP of fand t.align pos=?align ner=?align dep=?Align features.
?
represents thecorresponding POS/NER/DEP of thecurrent token.Table 4: Features based on edit script for answer se-quence tagging.Question-type Chunking features do not capturethe connection between question word and an-swer types.
Thus they have to be combinedwith question types.
For instance, how manyquestions are usually associated with numeric an-swer types.
We encode each major question-type: who, whom, when, where, how many, howmuch, how long, and then for each token, wecombine the question term with its chunking fea-tures described in (most tokens have different fea-tures because they have different POS/NER/DEPtypes).
One feature example of the QA pairhow much/100 dollars for the word 100 would be:qword=how much|pos[t]=CD|pos[t+1]=NNS.
We ex-pect high weight for this feature since it is a goodpattern for matching question type and answer type.Similar features also apply to what, which, why andhow questions, even though they do not indicate ananswer type as clearly as how much does.Some extra features are designed for what/whichquestions per required answer types.
The question862dependency tree is analyzed and the Lexical AnswerType (LAT) is extracted.
The following are someexamples of LAT for what questions:?
color: what is Crips?
gang color??
animal: what kind of animal is an agouti?The extra LAT=?
feature is also used with chunkingfeatures for what/which questions.There is significant prior work in building spe-cialized templates or classifiers for labeling questiontypes (Hermjakob, 2001; Li and Roth, 2002; Zhangand Lee, 2003; Hacioglu and Ward, 2003; Metzlerand Croft, 2005; Blunsom et al 2006; Moschittiet al 2007).
We designed our shallow questiontype features based on the intuitions of these priorwork, with the goal of having a relatively compactapproach that still extracts useful predictive signal.One possible drawback, however, is that if an LAT isnot observed during training but shows up in testing,the sequence tagger would not know which answertype to associate with the question.
In this case itfalls back to the more general qword=?
feature andwill most likely pick the type of answers that aremostly associated with what questions in training.Edit script Our TED module produces an edittrace for each word in a candidate sentence: theword is either deleted, renamed (if there is a wordof the same lemma in the question tree) or strictlyaligned (if there is an identical node in the questiontree).
A word in the deleted edit sequence is a cuethat it could be the answer.
A word being alignedsuggests it is less likely to be an answer.
Thus foreach word we extract features based on its edit type,shown in Table 4.These features are also appended with the token?sPOS/NER/DEP information.
For instance, a deletednoun usually carries higher edit feature weights thanan aligned adjective.Alignment distance We observed that a candidateanswer often appears close to an aligned word (i.e.,answer tokens tend to be located ?nearby?
portionsof text that align across the pair), especially in com-pound noun constructions, restrictive clauses, prepo-sition phrases, etc.
For instance, in the followingpair, the answer Limp Bizkit comes from the leadingcompound noun:?
What is the name of Durst ?s group??
Limp Bizkit lead singer Fred Durst did a lot ...Past work has designed large numbers of specifictemplates aimed at these constructions (Soubbotin,2001; Ravichandran et al 2003; Clark et al 2003;Sneiders, 2002).
Here we use a single general fea-ture that we expect to pick up much of this signal,without the significant feature engineering.Thus we incorporated a simple feature to roughlymodel this phenomenon.
It is defined as the distanceto the nearest aligned nonstop word in the originalword order.
In the above example, the only alignednonstop word is Durst.
Then this nearest alignmentdistance feature for the word Limp is:nearest dist to align(Limp):5This is the only integer-valued feature.
All otherfeatures are binary-valued.
Note this feature doesnot specify answer types: an adverb close to analigned word can also be wrongly taken as a strongcandidate.
Thus we also include a version of thePOS/NER/DEP based feature for each token:?
nearest dist pos(Limp)=NNP?
nearest dist dep(Limp)=NMOD?
nearest dist ner(Limp)=B-PERSON3.3 Overproduce-and-voteWe make an assumption that each sentence producesa candidate answer and then vote among all answercandidates to select the most-voted as the answer tothe original question.
Specifically, this overproduce-and-vote strategy applies voting in two places:1.
If there are overlaps between two answer candi-dates, a partial vote is performed.
For instance,for a when question, if one answer candidate isApril , 1994 and the other is 1994, then besidesthe base vote of 1, both candidates have an ex-tra partial vote of #overlap/#total words = 1/4.
Wecall this adjusted vote.2.
If the CRF fails to find an answer, we still try to?force?
an answer out of the tagged sequence,O?s).
thus forced vote.
Due to its lower credi-bility (the sequence tagger does not think it isan answer), we manually downweight the pre-diction score by a factor of 0.1 (divide by 10).863During what war d id Nimi tz serve ?O O:0.921060 ConantO O:0.991168 hadO O:0.997307 beenO O:0.998570 aO O:0.998608 photographerO O:0.999005 f o rO O:0.877619 AdmO O:0.988293 .O O:0.874101 ChesterO O:0.924568 Nimi tzO O:0.970045 dur ingB?ANS O:0.464799 WorldI?ANS O:0.493715 WarI?ANS O:0.449017 I IO O:0.915448 .Figure 3: A sample sequence tagging output thatfails to predict an answer.
From line 2 on, the firstcolumn is the reference output and the second col-umn is the model output with the marginal probabil-ity for predicated labels.
Note that World War II hasmuch lower probabilities as an O than others.The modified score for an answer candidate is thus:total vote = adjusted vote + 0.1 ?
forced vote.
Tocompute forced vote, we make the following obser-vation.
Sometimes the sequence tagger does not tagan answer in a candidate sentence at all, if thereis not enough probability mass accumulated for B-ANS.
However, a possible answer can still be caughtif it has an ?outlier?
marginal probability.
Figure 3shows an example.
The answer candidate World WarII has a much lower marginal probability as an ?O?but still not low enough to be part of B-ANS/I-ANS.To catch such an outlier, we use Median AbsoluteDeviation (MAD), which is the median of the abso-lute deviation from the median of a data sequence.Given a data sequence x, MAD is defined as:MAD(x) = median(| x?median(x) |)Compared to mean value and standard deviation,MAD is more robust against the influence of out-liers since it does not directly depend on them.
Weselect those words whose marginal probability is 50times of MAD away from the median of the wholesequence as answer candidates.
They contribute tothe forced vote.
Downweight ratio (0.1) and MADSystem Train Prec.% Rec.% F1%CRFTRAIN 55.7 43.8 49.1TRAIN-ALL 67.2 50.6 57.7CRF+WNsearchTRAIN 58.6 46.1 51.6TRAIN-ALL 66.7 49.4 56.8CRF forcedTRAIN 54.5 53.9 54.2TRAIN-ALL 60.9 59.6 60.2CRF forced+WNsearchTRAIN 55.2 53.9 54.5TRAIN-ALL 63.6 62.9 63.3Table 5: Performance on TEST.
?CRF?
only takesvotes from candidates tagged by the sequence tag-ger.
?CRF forced?
(described in ?3.3) further col-lects answer candidates from sentences that CRFdoes not tag an answer by detecting outliers.ratio (50) were hand-tuned on DEV.94 Experiments4.1 QA ResultsThe dataset listed in Table 2 was not designed toinclude an answer for each positive answer sen-tence, but only a binary indicator on whether a sen-tence contains an answer.
We used the answer pat-tern files (in Perl regular expressions) released alongwith TREC8-13 to pinpoint the exact answer frag-ments.
Then we manually checked TRAIN, DEV, andTEST for errors.
TRAIN-ALL already came as a noisydataset so we did not manually clean it, also due toits large size.We trained on only the positive examples ofTRAIN and TRAIN-ALL separately with CRFsuite(Okazaki, 2007).
The reason for training solely withpositive examples is that they only constitute 10% ofall training data and if trained on all, the CRF taggerwas very biased on negative examples and reluctantto give an answer for most of the questions.
TheCRF tagger attempted an answer for about 2/3 of allquestions when training on just positive examples.DEV was used to help design features.
A practi-cal benefit of our compact approach is that an entireround of feature extraction, training on TRAIN andtesting on DEV took less than one minute.
Table 59One might further improve this by leveraging the probabil-ity of a sentence containing an answer from the QA pair rankerdescribed in Section 2 or via the conditional probability of thesequence labels, p(y | x), under the CRF.864reports F1 scores on both the positive and negativeexamples of TEST.Our baseline model, which aligns the questionword with some content word in the answer sen-tence,10 achieves 31.4% in F1.
This model does notrequire any training.
?CRF?
only takes votes fromthose sentences with an identified answer.
It has thebest precision among all models.
?CRF forced?
alsodetects outliers from sentences not tagged with ananswer.
Large amount of training data, even noisy,is helpful.
In general TRAIN-ALL is able to boost theF1 value by 7 ?
8%.
Also, the overgenerate-and-vote strategy, used by the ?forced?
approach, greatlyincreased recall and achieved the best F1 value.We also experimented with the two methods uti-lizing WordNet in Section 2.2 , i.e., WNsearch andWNfeature.
In general, WNsearch helps F1 andyields the best score (63.3%) for this task.
ForWNfeature11 we observed that the CRF model con-verged to a larger objective likelihood with thesefeatures.
However, it did not make a difference inF1 after overgenerate-and-vote.Finally, we found it difficult to do a head-to-headcomparison with other QA systems on this task.12Thus we contribute this dataset to the community,hoping to solicit direct comparisons in the future.Also, we believe our chunking and question-typefeatures capture many intuitions most current QAsystems rely on, while our novel features are basedon TED.
We further conduct an ablation test to com-pare traditional and new QA features.4.2 Ablation TestWe did an ablation test for each of the four types offeatures.
Note that the question type features areused in combination with chunking features (e.g.,qword=how much|pos[t]=CD|pos[t+1]=NN), whilethe chunking feature is defined over POS/NER/DEP10This only requires minimal modification to the originalTED algorithm: the question word is aligned with a certainword in the answer tree instead of being inserted.
Then thewhole subtree headed by the aligned word counts as the answer.11These are binary features indicating whether an answercandidate has a WordNet relation ( c.f.
?2.2) with the LAT.For instance, tennis is a hyponym of the LAT word sport in thewhat sport question in Figure 1.12Reasons include: most available QA systems either retrievesentences from the web, have different preprocessing steps, oreven include templates learned from our test set.CRF Forced CRF ForcedAll 49.1 54.2 -above 3 19.4 25.3-POS 44.7 48.9 -EDIT 44.3 47.5-NER 44.0 50.8 -ALIGN 47.4 51.1-DEP 49.4 54.5 -above 2 40.5 42.0Table 6: F1 based on feature ablation tests.NONE CHUNKING CHUNKING+TEDFeatures Used0102030405060F1(%) 31.440.549.142.054.2F1 with Different FeaturesBaselineCRFCRF forcedFigure 4: Impact of adding features based on chunk-ing and question-type (CHUNKING) and tree edits(TED), e.g., EDIT and ALIGN.separately.
We tested the CRF model with deletionof one of the following features each time:?
POS, NER or DEP.
These features are all com-bined with question types.?
The three of the above.
Deletion of these fea-tures also deletes question type feature implic-itly.?
EDIT.
Features extracted from edit script.?
ALIGN.
Alignment distance features.?
The two of the above, based on the TED model.Table 6 shows the F1 scores of ablation test whentrained on TRAIN.
NER and EDIT are the two singlemost significant features.
NER is important becauseit closely relates question types with answer entitytypes (e.g., qword=who|ner[t]=PERSON).
EDIT isalso important because it captures the syntactic asso-ciation between question tree and answer tree.
Tak-ing out all three POS/NER/DEP features means thechunking and question type features do not fire any-more.
This has the biggest impact on F1.
Note thefeature redundancy here: the question type featuresare combined with all three POS/NER/DEP features865thus taking out a single one does not decrease per-formance much.
However, since TED related fea-tures do not combine question type features, takingout all three POS/NER/DEP features decreases F1 by30%.
Without TED related features (both EDIT andALIGN) F1 also drops more than 10%.Figure 4 is a bar chart showing how much im-provement each feature brings.
While having abaseline model with 31.4% in F1, traditional fea-tures based on POS/DEP/NER and question typesbrings a 10% increase with a simple sequence tag-ging model (second bar labeled ?CHUNKING?
inthe figure).
Furthermore, adding TED based featuresto the model boosted F1 by another 10%.5 ConclusionAnswer extraction is an essential task for any text-based question-answering system to perform.
In thispaper, we have cast answer extraction as a sequencetagging problem by deploying a fast and compactCRF model with simple features that capture manyof the intuitions in prior ?deep pipeline?
approaches.We introduced novel features based on TED thatboosted F1 score by 10% compared with the use ofmore standard features.
Besides answer extraction,our modified design of the TED model is the stateof the art in the task of ranking QA pairs.
Finally,to improve the community?s ability to evaluate QAcomponents without requiring increasingly imprac-tical end-to-end implementations, we have proposedanswer extraction as a subtask worth evaluating inits own right, and contributed a dataset that couldbecome a potential standard for this purpose.
Webelieve all these developments will contribute to thecontinuing improvement of QA systems in the fu-ture.Acknowledgement We thank Vulcan Inc. forfunding this work.
We also thank Michael Heil-man and Mengqiu Wang for helpful discussion anddataset, and the three anonymous reviewers for in-sightful comments.ReferencesNikolaus Augsten, Denilson Barbosa, Michael Bo?hlen,and Themis Palpanas.
2010.
TASM: Top-k Approx-imate Subtree Matching.
In Proceedings of the Inter-national Conference on Data Engineering (ICDE-10),pages 353?364, Long Beach, California, USA, March.IEEE Computer Society.D.M.
Bikel, R. Schwartz, and R.M.
Weischedel.
1999.An algorithm that learns what?s in a name.
Machinelearning, 34(1):211?231.P.
Bille.
2005.
A survey on tree edit distance andrelated problems.
Theoretical Computer Science,337(1):217?239.P.
Blunsom, K. Kocik, and J.R. Curran.
2006.
Questionclassification with log-linear models.
In Proceedingsof the 29th annual international ACM SIGIR confer-ence on Research and development in information re-trieval, pages 615?616.
ACM.Peter Clark, Vinay Chaudhri, Sunil Mishra, Je?ro?meThome?re?, Ken Barker, and Bruce Porter.
2003.
En-abling domain experts to convey questions to a ma-chine: a modified, template-based approach.
InProceedings of the 2nd international conference onKnowledge Capture, pages 13?19, New York, NY,USA.
ACM.Shilin Ding, Gao Cong, Chin yew Lin, and Xiaoyan Zhu.2008.
Using conditional random fields to extract con-texts and answers of questions from online forums.
InIn Proceedings of ACL-08: HLT.D.
Ferrucci, E. Brown, J. Chu-Carroll, J.
Fan, D. Gondek,A.A.
Kalyanpur, A. Lally, J.W.
Murdock, E. Nyberg,J.
Prager, et al2010.
Building Watson: An overviewof the DeepQA project.
AI Magazine, 31(3):59?79.K.
Hacioglu and W. Ward.
2003.
Question classifica-tion with support vector machines and error correctingcodes.
In Proceedings of NAACL 2003, short papers,pages 28?30.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA data min-ing software: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedings ofNAACL 2010, pages 1011?1019, Los Angeles, Cali-fornia.U.
Hermjakob.
2001.
Parsing and question classificationfor question answering.
In Proceedings of the work-shop on Open-domain question answering-Volume 12,pages 1?6.Milen Kouylekov and Bernardo Magnini.
2005.
Recog-nizing textual entailment with tree edit distance algo-rithms.
In PASCAL Challenges on RTE, pages 17?20.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.866In Proceedings of the Eighteenth International Confer-ence on Machine Learning, pages 282?289, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proceedings of ACL 2002, pages 1?7.B.
Magnini, M. Negri, R. Prevete, and H. Tanev.
2002.Is it the right answer?
: exploiting web redundancy foranswer validation.
In Proceedings of ACL 2002, pages425?432.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL 2005, pages 91?98.D.
Metzler and W.B.
Croft.
2005.
Analysis of statisticalquestion classification for fact-based questions.
Infor-mation Retrieval, 8(3):481?504.D.
Moldovan, M. Pas?ca, S. Harabagiu, and M. Surdeanu.2003.
Performance issues and error analysis in anopen-domain question answering system.
ACM Trans-actions on Information Systems (TOIS), 21(2):133?154.A.
Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.2007.
Exploiting syntactic and shallow semantic ker-nels for question answer classification.
In Proceedingsof ACL 2007, volume 45, page 776.Naoaki Okazaki.
2007.
CRFsuite: a fast implementationof Conditional Random Fields (CRFs).A.
Penas, A. Rodrigo, V. Sama, and F. Verdejo.
2008.Testing the reasoning for question answering valida-tion.
Journal of Logic and Computation, 18(3):459?474.Vasin Punyakanok, Dan Roth, and Wen T. Yih.
2004.Mapping Dependencies Trees: An Application toQuestion Answering.
In Proceedings of the 8th In-ternational Symposium on Artificial Intelligence andMathematics, Fort Lauderdale, Florida.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP1996, volume 1, pages 133?142.Deepak Ravichandran, Abraham Ittycheriah, and SalimRoukos.
2003.
Automatic derivation of surface textpatterns for a maximum entropy based question an-swering system.
In Proceedings of NAACL 2003, shortpapers, pages 85?87, Stroudsburg, PA, USA.D.
C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laen-der.
2004.
Automatic web news extraction using treeedit distance.
In Proceedings of the 13th internationalconference on World Wide Web, pages 502?511, NewYork, NY, USA.
ACM.A?.
Rodrigo, A.
Pen?as, and F. Verdejo.
2009.
Overview ofthe answer validation exercise 2008.
Evaluating Sys-tems for Multilingual and Multimodal Information Ac-cess, pages 296?313.H.
Shima, N. Lao, E. Nyberg, and T. Mitamura.
2008.Complex cross-lingual question answering as sequen-tial classification and multi-document summarizationtask.
In Proceedings of NTICIR-7 Workshop, Japan.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,pages 23?30, New York, June.Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance testsfor information retrieval evaluation.
In CIKM ?07:Proceedings of the sixteenth ACM conference on Con-ference on information and knowledge management,pages 623?632, New York, NY, USA.
ACM.E.
Sneiders.
2002.
Automated question answering:template-based approach.
Ph.D. thesis, KTH.Martin M. Soubbotin.
2001.
Patterns of potential answerexpressions as clues to the right answers.
In Proceed-ings of the Tenth Text REtrieval Conference (TREC2001).Mengqiu Wang and Christopher D. Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In Proceedings of ACL 2010, pages 1164?1172,Stroudsburg, PA, USA.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 22?32,Prague, Czech Republic, June.D.
Zhang and W.S.
Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedings ofthe 26th annual international ACM SIGIR conferenceon Research and development in informaion retrieval,pages 26?32.
ACM.K.
Zhang and D. Shasha.
1989.
Simple fast algorithmsfor the editing distance between trees and related prob-lems.
SIAM J.
Comput., 18(6):1245?1262, December.867
