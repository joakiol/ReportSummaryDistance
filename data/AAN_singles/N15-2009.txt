Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 64?70,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsA Preliminary Evaluation of the Impact of Syntactic Structure in SemanticTextual Similarity and Semantic Relatedness TasksNgoc Phuoc An VoFondazione Bruno Kessler,University of TrentoTrento, Italyngoc@fbk.euOctavian PopescuIBM Research, T.J. WatsonYorktown, USo.popescu@us.ibm.comAbstractThe well related tasks of evaluating the Se-mantic Textual Similarity and Semantic Relat-edness have been under a special attention inNLP community.
Many different approacheshave been proposed, implemented and evalu-ated at different levels, such as lexical similar-ity, word/string/POS tags overlapping, seman-tic modeling (LSA, LDA), etc.
However, atthe level of syntactic structure, it is not clearhow significant it contributes to the overall ac-curacy.
In this paper, we make a preliminaryevaluation of the impact of the syntactic struc-ture in the tasks by running and analyzing theresults from several experiments regarding tohow syntactic structure contributes to solvingthese tasks.1 IntroductionSince the introduction of Semantic Textual Simi-larity (STS) task at SemEval 2012 and the Seman-tic Relatedness (SR) task at SemEval 2014, a largenumber of participating systems have been devel-oped to resolve the tasks.1,2The systems must quan-tifiably identify the degree of similarity, relatedness,respectively, for pair of short pieces of text, like sen-tences, where the similarity or relatedness is a broadconcept and its value is normally obtained by aver-aging the opinion of several annotators.
A semanticsimilarity/relatedness score is usually a real numberin a semantic scale, [0-5] in STS, or [1-5] in SR, in1http://www.cs.york.ac.uk/semeval-2012/task62http://alt.qcri.org/semeval2014/task1the direction from no relevance to semantic equiva-lence.
Some examples from the dataset MSRpar ofSTS 2012 with associated similarity scores (by hu-man judgment) are as below:?
The bird is bathing in the sink.
vs. Birdie iswashing itself in the water basin.
(score = 5.0)?
Shares in EDS closed on Thursday at $18.51,a gain of 6 cents.
vs. Shares of EDS closedThursday at $18.51, up 6 cents on the New YorkStock Exchange.
(score = 3.667)?
Vivendi shares closed 3.8 percent up in Paris at15.78 euros.
vs. Vivendi shares were 0.3 per-cent up at 15.62 euros in Paris at 0841 GMT.
(score = 2.6)?
John went horse back riding at dawn with awhole group of friends.
vs. Sunrise at dawnis a magnificent view to take in if you wake upearly enough for it.
(score = 0)From our reading of the literature (Marelli et al,2014b; Agirre et al, 2012; Agirre et al, 2013; Agir-rea et al, 2014), most of STS/SR systems rely onpairwise similarity, such as lexical similarity usingtaxonomies (WordNet (Fellbaum, 1998)) or distri-butional semantic models (LDA (Blei et al, 2003),LSA (Landauer et al, 1998), ESA (Gabrilovich andMarkovitch, 2007), etc), and word/n-grams overlapas main features to train a support vector machines(Joachims, 1998) regression model (supervised), oruse a word-alignment metric (unsupervised) align-ing the two given texts to compute their semanticsimilarity.Intuitively, the syntactic structure plays an impor-tant role for human being to understand the mean-64ing of a given text.
Thus, it also may help to iden-tify the semantic equivalence/relatedness betweentwo given texts.
However, in the STS/SR tasks,very few systems provide evidence of the contribu-tion of syntactic structure in its overall performance.Some systems report partially on this issue, for ex-ample, iKernels (Severyn et al, 2013) carried outan analysis on the STS 2012, but not on STS 2013datasets.
They found that syntactic structure con-tributes 0.0271 and 0.0281 points more to the over-all performance, from 0.8187 to 0.8458 and 0.8468,for adopting constituency and dependency trees, re-spectively.In this paper, we analyze the impact of syntac-tic structure on the STS 2014 and SICK datasetsof STS/SR tasks.
We consider three systems whichare reported to perform efficiently and effectively onprocessing syntactic trees using three proposed ap-proaches Syntactic Tree Kernel (Moschitti, 2006),Syntactic Generalization (Galitsky, 2013) and Dis-tributed Tree Kernel (Zanzotto and Dell?Arciprete,2012).The remainder of the paper is as follows: Section2 introduces three approaches to exploit the syntac-tic structure in STS/SR tasks, Section 3 describesExperimental Settings, Section 4 discusses about theEvaluations and Section 5 is the Conclusions andFuture Work.2 Three Approaches for Exploiting theSyntactic StructureIn this section, we describe three different ap-proaches exploiting the syntactic structure to beused in the STS/SR tasks, which are Syntactic TreeKernel (Moschitti, 2006), Syntactic Generaliza-tion (Galitsky, 2013), and Distributed Tree Ker-nel (Zanzotto and Dell?Arciprete, 2012).
All thesethree approaches learn the syntactic information ei-ther from the dependency parse trees produced bythe Stanford Parser (standard PCFG Parser) (Kleinand Manning, 2003) or constituency parse trees ob-tained by OpenNLP.3The output of each approachis normalized to the standard semantic scale of STS[0-5] or SR [1-5] tasks to evaluate its standalone per-formance, or combined with other features in ourbaseline system for assessing its contribution to the3https://opennlp.apache.orgoverall accuracy by using the same WEKA machinelearning tool (Hall et al, 2009) with as same config-urations and parameters as our baseline systems.2.1 Syntactic Tree Kernel (STK)Given two trees T1 and T2, the functionality of treekernels is to compare two tree structures by comput-ing the number of common substructures betweenT1 and T2 without explicitly considering the wholefragment space.
According to the literature (Mos-chitti, 2006), there are three types of fragments de-scribed as the subtrees (STs), the subset trees (SSTs)and the partial trees (PTs).
A subtree (ST) is a nodeand all its children, but terminals are not STs.
Asubset tree (SST) is a more general structure sinceits leaves need not be terminals.
The SSTs satisfythe constraint that grammatical rules cannot be bro-ken.
When this constraint is relaxed, a more generalform of substructures is obtained and defined as par-tial trees (PTs).Syntactic Tree Kernel (STK) (Moschitti, 2006) isa tree kernels approach to learn the syntactic struc-ture from syntactic parsing information, particularly,the Partial Tree (PT) kernel is proposed as a newconvolution kernel to fully exploit dependency trees.The evaluation of the common PTs rooted in nodesn1 and n2 requires the selection of the shared childsubsets of the two nodes, e.g.
[S [DT JJ N]] and [S[DT N N]] have [S [N]] (2 times) and [S [DT N]] incommon.In order to learn the similarity of syntactic struc-ture, we seek for a corpus which should fulfill thetwo requirements, (1) sentence-pairs contain simi-lar syntactic structure, and with (2) a variety of theirsyntactic structure representations (in their parsingtrees).
However, neither SICK nor STS corpusseems to be suitable.
As the SICK corpus is de-signed for evaluating compositional distributionalsemantic models through semantic relatedness andtextual entailment, the syntactic structure of sen-tence pairs are quite simple and straightforward.
Incontrast, the STS corpus contains several differentdatasets derived from different sources (see Table 1)which carry a large variety of syntactic structure rep-resentations, but lack of learning examples due to nohuman annotation given for syntactic structure sim-ilarity (only annotation for semantic similarity ex-ists); and it is difficult to infer the syntactic structure65similarity from general semantic similarity scoresin STS datasets.
Hence, having assumed that para-phrased pairs would share the same content and sim-ilar syntactic structures, we decide to choose the Mi-crosoft Research Paraphrasing Corpus (Dolan et al,2005) which contains 5,800 sentence pairs extractedfrom news sources on the web, along with humanannotations indicating whether each pair captures aparaphrase/semantic equivalence relationship.4Thiscorpus is split into Training set (4,076 pairs) andTesting set (1,725 pairs).We use Stanford Parser (PCFG Parser) trained onPenn TreeBank (Klein and Manning, 2003) to obtainthe dependency parsing from sentence pairs.
Thenwe use the machine learning tool svm-light-tk 1.2which uses Tree Kernel approach to learn the sim-ilarity of syntactic structure to build a binary clas-sifying model on the Train dataset.5According tothe assumption above, we label paraphrased pairsas 1, -1 otherwise.
We test this model on the Testdataset and obtain the Accuracy of 69.16%, withPrecision/Recall is: 69.04%/97.21%.We apply this model on the STS and SICK datato predict the similarity between sentence pairs.
Theoutput predictions are probability confidence scoresin [-1,1], corresponds to the probability of the la-bel to be positive.
Thus, we convert the predictionvalue into the semantic scale of STS and SR tasks tocompare to the human annotation.
The example data(including train, test, and predictions) of this tool isavailable here.62.2 Syntactic Generalization (SG)Given a pair of parse trees, the Syntactic General-ization (SG) (Galitsky, 2013) finds a set of maxi-mal common subtrees.
Though generalization oper-ation is a formal operation on abstract trees, it yieldssemantics information from commonalities betweensentences.
Instead of only extracting common key-words from two sentences, the generalization op-eration produces a syntactic expression.
This ex-pression maybe semantically interpreted as a com-mon meaning held by both sentences.
This syntacticparse tree generalization learns the semantic infor-4http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd280425http://disi.unitn.it/moschitti/SIGIR-tutorial.htm6http://disi.unitn.it/moschitti/Tree-Kernel.htmmation differently from the kernel methods whichcompute a kernel function between data instances,whereas a kernel function is considered as a similar-ity measure.
Other than the kernel methods, SG isconsidered as structure-based and deterministic, inwhich linguistic features remain their structure, notas value presentations.The toolkit "relevance-based-on-parse-trees" is anopen-source project which evaluates text relevanceby using syntactic parse tree-based similarity mea-sure.7Given a pair of parse trees, it measures thesimilarity between two sentences by finding a set ofmaximal common subtrees, using representation ofconstituency parse trees via chunking.
Each type ofphrases (NP, VP, PRP etc.)
will be aligned and sub-ject to generalization.
It uses the OpenNLP systemto derive dependency trees for generalization (chun-ker and parser).8This tool is made to give as atool for text relevance which can be used as a blackbox, no understanding of computational linguisticsor machine learning is required.
We apply the toolon the SICK and STS datasets to compute the sim-ilarity of syntactic structure of sentence pairs.
Thesimilarity score from this tool is converted into thesemantic scale of STS and SR tasks for comparisonagainst the human annotation.2.3 Distributed Tree Kernel (DTK)Distributed Tree Kernel (DTK) (Zanzotto andDell?Arciprete, 2012) is a tree kernels method usinga linear complexity algorithm to compute vectors fortrees by embedding feature spaces of tree fragmentsin low-dimensional spaces.
Then a recursive algo-rithm is proposed with linear complexity to computereduced vectors for trees.
The dot product amongreduced vectors is used to approximate the originaltree kernel when a vector composition function withspecific ideal properties is used.Firstly, we use Stanford Parser (PCFG Parser)trained on Penn TreeBank (Klein and Manning,2003) to obtain the dependency parsing of sen-tences, and feed them to the software "distributed-tree-kernels" to produce the distributed trees.9Then,we compute the Cosine similarity between the vec-tors of distributed trees of each sentence pair.
This7https://code.google.com/p/relevance-based-on-parse-trees8https://opennlp.apache.org9https://code.google.com/p/distributed-tree-kernels66cosine similarity score is converted to the scale ofSTS and SR for evaluation.3 ExperimentsIn this section, we describe the two corpora we usefor experiments with several different settings toevaluate the contribution of each syntactic structureapproach and in combination with other features inour baseline systems.3.1 DatasetsWe run our experiments on two datasets from twodifferent tasks at SemEval 2014 as follows:?
The SICK dataset (Marelli et al, 2014a) is usedin Task# 1 "Evaluation of compositional dis-tributional semantic models on full sentencesthrough semantic relatedness and textual entail-ment".10It consists of 10,000 English sentencepairs, built from two paraphrase sets: the 8KImageFlickr dataset and the STS 2012 VideoDescriptions dataset.11,12Each sentence pairwas annotated for relatedness score in scale [1-5] and entailment relation.
It is split into threeparts: Trial (500 pairs), Training (4,500 pairs)and Testing (4,927 pairs).?
The STS dataset is used in Task #10 "Mul-tilingual Semantic Textual Similarity" (STSEnglish subtask) which consists of severaldatasets in STS 2012 (Agirre et al, 2012), 2013(Agirre et al, 2013) and 2014 (Agirrea et al,2014).
Each sentence pair is annotated the se-mantic similarity score in the scale [0-5].
Ta-ble 1 shows the summary of STS datasets andsources over the years.
For training, we use alldata in STS 2012 and 2013; and for evaluation,we use STS 2014 datasets.3.2 BaselinesIn order to evaluate the significance of syntacticstructure in the STS/SR tasks, we not only exam-ine the syntactic structure alone, but also combine10http://alt.qcri.org/semeval2014/task111http://nlp.cs.illinois.edu/HockenmaierGroup/data.html12http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=datayear dataset pairs source2012 MSRpar 1500 newswire2012 MSRvid 1500 video descriptions2012 OnWN 750 OntoNotes, WordNet glosses2012 SMTnews 750 Machine Translation evaluation2012 SMTeuroparl 750 Machine Translation evaluation2013 headlines 750 newswire headlines2013 FNWN 189 FrameNet, WordNet glosses2013 OnWN 561 OntoNotes, WordNet glosses2013 SMT 750 Machine Translation evaluation2014 headlines 750 newswire headlines2014 OnWN 750 OntoNotes, WordNet glosses2014 Deft-forum 450 forum posts2014 Deft-news 300 news summary2014 Images 750 image descriptions2014 Tweet-news 750 tweet-news pairsTable 1: Summary of STS datasets in 2012, 2013, 2014.it with some features learned from common ap-proaches, such as bag-of-words, pairwise similarity,n-grams overlap, etc.
Therefore, we use two base-line systems for evaluations, the weak and the strongones.
The weak baseline is the basic one used forevaluation in all the STS tasks, namely tokencos.It uses the bag-of-words approach which representseach sentence as a vector in the multidimensionaltoken space (each dimension has 1 if the token ispresent in the sentence, 0 otherwise) and computesthe cosine similarity between vectors.Besides the weak baseline, we use DKPro Simi-larity (B?r et al, 2012) as a strong baseline whichis an open source software and intended to use asa baseline-system in the share task STS at *SEM2013.13It uses a simple log-linear regression model(about 18 features), to combine multiple text simi-larity measures of varying complexity ranging fromsimple character/word n-grams and common subse-quences to complex features such as Explicit Se-mantic Analysis vector comparisons and aggrega-tion of word similarity based on lexical-semantic re-sources (WordNet and Wiktionary).14,154 Evaluations and DiscussionsIn this section, we present twelve different set-tings for experimenting the contribution of syntacticstructure individually and in combination with typi-13https://code.google.com/p/dkpro-similarity-asl/wiki/SemEval201314http://wordnet.princeton.edu15http://en.wiktionary.org/wiki/Wiktionary:Main_Page67Settings deft- deft- headlines images OnWN tweet- STS2014 SICK-forum news news Mean testTokencos (0) 0.353 0.596 0.510 0.513 0.406 0.654 0.5054 0.501DKPro (1) 0.4314 0.7089 0.6887 0.7671 0.8125 0.6932 0.6836 0.6931STK (2) 0.1163 0.2369 0.0374 -0.1125 0.0865 -0.0296 0.0558 0.0757SG (3) 0.2816 0.3808 0.4078 0.4449 0.4934 0.5487 0.4262 0.4498DTK (4) 0.0171 0.1 -0.0336 -0.109 0.0359 -0.0986 -0.0147 0.2657STK & SG & DTK 0.2402 0.3886 0.3233 0.2419 0.4066 0.4489 0.3416 0.4822(0) & (2) 0.3408 0.5738 0.4817 0.4184 0.4029 0.6016 0.4699 0.5074(0) & (3) 0.3735 0.5608 0.5367 0.5432 0.4813 0.6736 0.5282 0.522(0) & (4) 0.3795 0.6343 0.5399 0.5096 0.4504 0.6539 0.5279 0.5018(0), (2), (3) & (4) 0.3662 0.5867 0.5265 0.464 0.4758 0.6407 0.51 0.5252(1) & (2) 0.4423 0.7019 0.6919 0.7653 0.8122 0.7105 0.6874 0.7239(1) & (3) 0.4417 0.7067 0.6844 0.7636 0.812 0.6777 0.6810 0.6948(1) & (4) 0.4314 0.7089 0.6887 0.7671 0.8125 0.6932 0.6836 0.6953(1), (2), (3) & (4) 0.4495 0.7032 0.6902 0.7627 0.8115 0.6974 0.6857 0.7015Table 2: Experiment Results on STS 2014 and SICK datasets.cal similarity features to the overall performance ofcomputing similarity/relatedness score on SICK andSTS datasets.
The results reported here are obtainedwith Pearson correlation, which is the official mea-sure used in both tasks.16We have some discussionsfrom the results in Table 2 as below:Baseline comparison.
The strong baselineDKPro is superior than the bag-of-word baseline onmost of datasets (both STS and SICK), except thetweet-news where their performances are close asthe tweet-news dataset contains little or no syntac-tic information compared to others.Individual approach evaluation.
Each syntacticapproach is weaker than both baselines.
Though theSTK and DTK both use the tree kernel approach,just different representations, the performance issimilar only on the dataset images.
The STK stillperforms better than DTK on most of STS datasets,but much lower on SICK dataset.
This is reason-able as the SICK dataset is created for evaluatingdistributional semantics which suits the DTK ap-proach.
Both approaches have some negative resultson STS datasets; especially, both methods obtainnegative correlation on two datasets "images" and"tweet-news".
It seems that both methods struggle tolearn the semantic information (in parsing) extracted16http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficientfrom these two datasets.
Moreover, due to the factthat Twitter data is informal text which carries lotof noise created by users, and very different fromformal text from other STS datasets, the syntacticapproach does not seem to capture correct meaning,thus, the result confirms that syntactic approach isnot suitable and beneficial for social media text.In contrast, the SG performs better than other twoapproaches to obtain better correlation with humanjudgment; yet it is still below the bag-of-word base-line (only better on OnWN dataset).
Hence, usingany of these syntactic approaches is not sufficientto solve the STS/SR task as its performance is stilllower than the weak baseline.
Some examples withgold-standard and system scores as below:?
Blue and red plane in mid-air flight.
vs. A blueand red airplane while in flight.
(gold=4.8;STK=3.418; DTK=3.177; SG=3.587)?
Global online #education is a key to democra-tizing access to learning and overcoming soci-etal ills such as poverty vs.
Op-Ed Columnist:Revolution Hits the Universities (gold=0.6;STK=3.054; DTK=3.431; SG=2.074)?
you are an #inspiration!
#Keepfighting vs.The front lines in fight for women (gold=0.4;STK=3.372; DTK=3.479; SG=2.072)?
CGG - 30 die when bus plunges off cliff inNepal vs. 30 killed as bus plunges off cliff68in Nepal (gold=5; STK=3.155; DTK=3.431;SG=3.402)The combination of three approaches.
Thesethree methods do not collaborate well on STSdatasets, it even decreases the overall performanceof the best method SG by a large margin of 8%.However, it improves the result on SICK dataset bya medium margin around 4%.
Finally, the combi-nation of three methods still returns a lower resultthan the weak baseline.
Thus, this combination ofsyntactic approaches alone cannot solve the STS/SRtasks.Combination with bag-of-word approach.
Thecombination of syntactic information and bag-of-word approach more or less improves the perfor-mance over the weak baseline.?
The STK does not improve but has negative im-pact to the overall performance on STS with adecrease of 4%.
However, it gains a small im-provement on SICK of 1%.?
Though the DTK returns 3.5% better resultthan STK on STS and slightly improves theperformance on SICK for less than 1%, it is0.5% lower than the weak baseline.?
The SG improves the performance 2-12% onmost of STS and SICK datasets.
It performs 4-8% better than the weak baseline, but still dra-matically 11-14% lower than the DKPro base-line.?
The combination of three methods with thebag-of-word results 3-8% better performancethan the weak baseline on STS/SICK datasets.However, this combination brings negative ef-fect of 0.5% to the overall result on STS incomparison to the performance of SG.Combination with DKPro.
Perhaps DKProbaseline consists of several strong features whichmake syntactic features insignificant in the combi-nation.
Hence, using a strong baseline like DKPro isnot a good way to evaluate the significance of syn-tactic information.?
The STK gains small improvement on SICK(3%) and some STS datasets (1%), whereasother datasets remain unchanged.?
The DTK does not have any effect to the resultof DKPro standalone.
This shows that DTK hasno integration with DKPro features.?
The SG only makes slight improvement onSICK (0.2%) and deft-forum (1%), whereas lit-tle decrease on other datasets.
This shows thatSG does not collaborate well with DKPro ei-ther.?
On STS, this total combination returns fewsmall improvements around 1% on somedatasets deft-forum, headlines, tweet-news andmean value, whereas 1-3% better on SICKdataset.In conclusion, despite the fact that we experimentdifferent methods to exploit syntactic information ondifferent datasets derived from various data sources,the results in Table 2 confirms the positive impactof syntactic structure in the overall performance onSTS/SR tasks.
However, syntactic structure does notalways work well and effectively on any dataset, itrequires a certain level of syntactic presentation inthe corpus to exploit.
In some cases, applying syn-tactic structure on poor-structured data may causenegative effect to the overall performance.5 Conclusions and Future WorkIn this paper, we deploy three different approachesto exploit and evaluate the impact of syntactic struc-ture in the STS/SR tasks.
We use a freely avail-able STS system, DKPro, which is using simi-larity features for computing the semantic similar-ity/relatedness scores as a strong baseline.
We alsoevaluate the contribution of each syntactic structureapproach and different combinations between themand the typical similarity approach in the baseline.From our observation, in the mean time with recentproposed approaches, the results in Table 2 showsthat the syntactic structure does contribute individu-ally and together with typical similarity approachesfor computing the semantic similarity/relatednessscores between given sentence pairs.
However, com-pared to the baselines, the contribution of syntac-tic structure is not significant to the overall perfor-mance.
For future work, we may expect to see moreeffective ways for exploiting and learning syntacticstructure to have better contribution into the overallperformance in the STS/SR tasks.69ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theFirst Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Eval-uation, pages 385?393.
Association for ComputationalLinguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013. sem 2013 shared task:Semantic textual similarity, including a pilot on typed-similarity.
In In* SEM 2013: The Second Joint Con-ference on Lexical and Computational Semantics.
As-sociation for Computational Linguistics.
Citeseer.Eneko Agirrea, Carmen Baneab, Claire Cardiec, DanielCerd, Mona Diabe, Aitor Gonzalez-Agirrea, WeiweiGuof, Rada Mihalceab, German Rigaua, and JanyceWiebeg.
2014.
Semeval-2014 task 10: Multilingualsemantic textual similarity.
SemEval 2014, page 81.Daniel B?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation, pages 435?440.
Association for Computational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Bill Dolan, Chris Brockett, and Chris Quirk.
2005.
Mi-crosoft research paraphrase corpus.
Retrieved March,29:2008.Christiane Fellbaum.
1998.
WordNet.
Wiley Online Li-brary.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In IJCAI, volume 7, pages1606?1611.Boris Galitsky.
2013.
Machine learning of syntac-tic parse trees for search and classification of text.Engineering Applications of Artificial Intelligence,26(3):1072?1091.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.
2009.The weka data mining software: an update.
ACMSIGKDD explorations newsletter, 11(1):10?18.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
Springer.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Thomas K Landauer, Peter W Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse processes, 25(2-3):259?284.M Marelli, S Menini, M Baroni, L Bentivogli,R Bernardi, and R Zamparelli.
2014a.
A SICK curefor the evaluation of compositional distributional se-mantic models.
In Proceedings of LREC 2014, Reyk-javik (Iceland): ELRA.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zampar-elli.
2014b.
Semeval-2014 task 1: Evaluation of com-positional distributional semantic models on full sen-tences through semantic relatedness and textual entail-ment.
SemEval-2014.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Machine Learning: ECML 2006, pages 318?329.Springer.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013. ikernels-core: Tree kernel learningfor textual similarity.
Atlanta, Georgia, USA, page 53.Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.2012.
Distributed tree kernels.
arXiv preprintarXiv:1206.4607.70
