Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 96?102,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsRelation Extraction from Community Generated Question-Answer PairsDenis SavenkovEmory Universitydsavenk@emory.eduWei-Lwun LuGoogleweilwunlu@google.comJeff DaltonGooglejeffdalton@google.comEugene AgichteinEmory Universityeugene@mathcs.emory.eduAbstractCommunity question answering (CQA) web-sites contain millions of question and answer(QnA) pairs that represent real users?
inter-ests.
Traditional methods for relation extrac-tion from natural language text operate overindividual sentences.
However answer text issometimes hard to understand without know-ing the question, e.g., it may not name the sub-ject or relation of the question.
This workpresents a novel model for relation extrac-tion from CQA data, which uses discourse ofQnA pairs to predict relations between entitiesmentioned in question and answer sentences.Experiments on 2 publicly available datasetsdemonstrate that the model can extract from?20% to?40% additional relation triples, notextracted by existing sentence-based models.1 IntroductionRecently all major search companies have adoptedknowledge bases (KB), and as a result users nowcan get rich structured data as answers to some oftheir questions.
However, even the largest existingknowledge bases, such as Freebase (Bollacker et al,2008), DPpedia (Auer et al, 2007), NELL (Carlsonet al, 2010), Google Knowledge Graph etc., whichstore billions of facts about millions of entities, arefar from being complete (Dong et al, 2014).
A lotof information is hidden in unstructured data, suchas natural language text, and extracting this infor-mation for knowledge base population (KBP) is anactive area of research (Surdeanu and Ji, 2014).One particularly interesting source of unstruc-tured text data is CQA websites (e.g.
Yahoo!
An-swers,1Answers.com,2etc.
), which became very1http://answers.yahoo.com/2http://www.answers.compopular resources for question answering.
The in-formation expressed there can be very useful, forexample, to answer future questions (Shtok et al,2012), which makes it attractive for knowledge basepopulation.
Although some of the facts mentionedin QnA pairs can also be found in some other textdocuments, another part might be unique (e.g.
inClueweb3about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answersdocuments cannot be found in other documents).There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., theytypically consider sentences independently and ig-nore the discourse of QnA pair text.
However, of-ten it is impossible to understand the answer withoutknowing the question.
For example, in many casesusers simply give the answer to the question with-out stating it in a narrative sentence (e.g.
?What does?xoxo?
stand for?
Hugs and kisses.?
), in some othercases the answer contains a statement, but some im-portant information is omitted (e.g.
?What?s the cap-ital city of Bolivia?
Sucre is the legal capital, thoughthe government sits in La Paz?
).In this work we propose a novel model for rela-tion extraction from CQA data, that uses discourseof a QnA pair to extract facts between entities men-tioned in question and entities mentioned in answersentences.
The conducted experiments confirm thatmany of such facts cannot be extracted by existingsentence-based techniques and thus it is beneficial tocombine their outputs with the output of our model.2 ProblemThis work targets the problem of relation extractionfrom QnA data, which is a collection of (q, a) pairs,3http://www.lemurproject.org/clueweb12/96where q is a question text (can contain multiple sen-tences) and a is the corresponding answer text (canalso contain multiple sentences).
By relation in-stance r we mean an ordered binary relation betweensubject and object entities, which is commonly rep-resented as [subject, predicate, object] triple.
Forexample, the fact that Brad Pitt married AngelinaJolie can be represented as [Brad Pitt, married to,Angelina Jolie].
In this work we use Freebase, anopen schema-based KB, where all entities and pred-icates come from the fixed alphabets E and P cor-respondingly.
Let e1and e2be entities that are men-tioned together in a text (e.g.
in a sentence, or e1in a question and e2in the corresponding answer),we will call such an entity pair with the correspond-ing context a mention.
The same pair of entitiescan be mentioned multiple times within the corpus,and for all mentions i = 1, ..., n the goal is to pre-dict the expressed predicate (zi?
P ) or to say thatnone applies (zi= ?).
Individual mention predic-tions z1, ..., znare combined to infer a set of rela-tions y = {yi?
P} between the entities e1and e2.3 ModelsOur models for relation extraction from QnA dataincorporates the topic of the question and can berepresented as a graphical model (Figure 1).
Eachmention of a pair of entities is represented with aset of mention-based features x and question-basedfeatures xt.
A multinomial latent variable z repre-sents a relation (or none) expressed in the mentionand depends on the features and a set of weightswxfor mention-based and wtfor question-based fea-tures: z?
= argmaxz?P?
?p(z|x, xt, wx, wt).
To estimatethis variable we use L2-regularized multinomial lo-gistic regression model, trained using the distant su-pervision approach for relation extraction (Mintz etal., 2009), in which mentions of entity pairs relatedin Freebase are treated as positive instances for thecorresponding predicates, and negative examples aresampled from mentions of entity pairs which arenot related by any of the predicates of interest.
Fi-nally, to predict a set of possible relations y betweenthe pair of entities we take logical OR of individualmention variables z, i.e.
yp= ?Mi=1[zi= p, p ?
P ],where M is the number of mentions of this pair ofentities.yPzxxtwtwx|Q|MNFigure 1: QnA-based relation extraction model platediagram.
N - number of different entity pairs, M -number of mentions of an entity pair, |Q| - numberof questions where an entity pair is mentioned, x andxt- mention-based and question-based features, wand wt- corresponding feature weights, latent vari-ables z - relation expressed in an entity pair mention,latent variables y - relations between entity pair3.1 Sentence-based baseline modelExisting sentence-based relation extraction modelscan be applied to individual sentences of a QnApair and will work well for complete statements, e.g.
?Who did Brad Pitt marry?
Brad Pitt and AngelinaJolie married at secret ceremony?.
In sentence-based scenario, when the set of question-based fea-tures is empty, the above model corresponds tothe Mintz++ baseline described in Surdeanu et al(2012), which was shown to be superior to the orig-inal model of Mintz et al (2009), is easier to trainthan some other state of the art distant supervisionmodels and produces comparable results.3.2 Sentence-based model with questionfeaturesIn many cases an answer statement is hard to in-terpret correctly without knowing the correspondingquestion.
To give the baseline model some knowl-edge about the question, we include question fea-tures (Table 1), which are based on dependency treeand surface patterns of a question sentence.
This97Table 1: Examples of features used for relation extraction for ?When was Mariah Carey born?
MariahCarey was born 27 March 1970?Sentence-based modelDependency path between entities [PERSON]?nsubjpass(born)tmod?
[DATE]Surface pattern [PERSON] be/VBD born/VBN [DATE]Question features for sentence-based modelQuestion template when [PERSON] bornDependecy path from a verb to the question word (when)?advmod(born)Question word + dependency tree root when+bornQnA-based modelQuestion template + answer entity type Q: when [PERSON] born A:[DATE]Dependency path from question word to entity Q:(when)?advmod(born)nsubj?
[PERSON]and answer entity to the answer tree root A: (born)tmod?
[DATE]Question word, dependency root and answer pattern Q: when+born A:born [DATE]information can help the model to account for thequestion topic and improve predictions in some am-biguous situations.3.3 QnA-based modelThe QnA model for relation extraction is inspiredby the observation, that often an answer sentence donot mention one of the entities at all, e.g., ?Whenwas Isaac Newton born?
December 25, 1642 Wool-sthorpe, England?.
To tackle this situation we makethe following assumption about the discourse of aQnA pair: an entity mentioned in a question is re-lated to entities in the corresponding answer and thecontext of both mentions can be used to infer the re-lation predicate.
Our QnA-based relation extractionmodel takes an entity from a question sentence andentity from the answer as a candidate relation men-tion, represents it with a set features (Table 1) andpredicts a possible relation between them similar tosentence-based models.
The features are conjunc-tions of various dependency tree and surface patternsof question and answer sentences, designed to cap-ture their topics and relation.4 Experiments4.1 DatasetsFor experiments we used 2 publicly available CQAdatasets: Yahoo!
Answers Comprehensive Ques-tions and Answers4and a crawl of WikiAnswers54http://webscope.sandbox.yahoo.com/catalog.php?datatype=l5http://wiki.answers.com(Fader et al, 2014).
The Yahoo!
Answersdataset contains 4,483,032 questions (3,894,644 inEnglish) with the corresponding answers collectedon 10/25/2007.
The crawl of WikiAnswers has30,370,994 question clusters, tagged by WikiAn-swers users as paraphrases, and only 3,386,256 themhave answers.
From these clusters we used all possi-ble pairs of questions and answers (19,629,443 pairsin total).For each QnA pair we applied tokenization,sentence detection, named entity tagger, parsingand coreference resolution from Stanford CoreNLP(Manning et al, 2014).
Our cascade entity link-ing approach is similar to Chang et al (2011) andconsidered all noun phrase and named entity men-tions as candidates.
First all named entity mentionsare looked up in Freebase names and aliases dictio-nary.
The next two stages attempt to match mentiontext with dictionary of English Wikipedia concepts(Spitkovsky and Chang, 2012) and its normalizedversion.
Finally for named entity mentions we tryspelling correction using Freebase entity names dic-tionary.
We didn?t disambiguate entities and insteadtook top-5 ids for each coreference cluster (using thep(entity|phrase) score from the dictionary or num-ber of existing Freebase triples).
All pairs of entities(or entity and date) in a QnA pair that are directlyrelated6in Freebase were annotated with the corre-sponding relations.6We also consider some paths that come through a mediatornode, e.g./people/person/spouse s./people/marriage/spouse98Table 2: Yahoo!
Answers and WikiAnswers datasets statisticsY!A WANumber of QnA pairs 3.8M 19.6MAverage question length (in chars) 56.67 47.03Average answer length (in chars) 335.82 24.24Percent of QnA pairs with answers that do not have any verbs 8.8% 18.9%Percent of QnA pairs with at least one pair of entities related in Freebase 11.7% 27.5%Percent of relations between entity pairs in question sentences only 1.6 % 3.1%Percent of relations between entity pairs in question and answer sentences only 28.1% 46.4%Percent of relations between entity pairs in answer sentences only 38.6% 12.0%Table 2 gives some statistics on the datasets usedin this work.
The analysis of answers that do nothave any verbs show that ?8.8% of all QnA pairsdo not state the predicate in the answer text.
Thepercentage is higher for WikiAnswers, which hasshorter answers on average.
Unfortunately, for manyQnA pairs we were unable to find relations betweenthe mentioned entities (for many of them no or fewentities were resolved to Freebase).
Among thoseQnA pairs, where some relation was annotated, welooked at the location of related entities.
In Yahoo!Answers dataset 38.6% (12.0% for WikiAnswers) ofrelated entities are mentioned in answer sentencesand can potentially be extracted by sentence-basedmodel, and 28.1% (46.4% for WikiAnswers) be-tween entities mentioned in question and answersentences, which are not available to the baselinemodel and our goal is to extract some of them.4.2 Experimental setupFor our experiments we use a subset of 29 Freebasepredicates that have enough unique instances anno-tated in our corpus, e.g.
date of birth, profession,nationality, education institution, date of death, dis-ease symptoms and treatments, book author, artistalbum, etc.
We train and test the models on eachdataset separately.
Each corpus is randomly split fortraining (75%) and testing (25%).
Knowledge basefacts are also split into training and testing sets (50%each).
QnA and sentence-based models predict la-bels for each entity pair mention, and we aggregatemention predictions by taking the maximum scorefor each predicate.
We do the same aggregation toproduce a combination of QnA- and sentence-basedmodels, i.e., all extractions produced by the modelsare combined and if there are multiple extractions ofthe same fact we take the maximum score as the finalconfidence.
The precision and recall of extractionsare evaluated on a test set of Freebase triples, i.e.
anextracted triple is considered correct if it belongs tothe test set of Freebase triples, which are not usedfor training (triples used for training are simply ig-nored).
Note, that this only provides a lower boundon the model performance as some of the predictedfacts can be correct and simply missing in Freebase.4.3 ResultsFigure 2 shows Precision-Recall curves for QnA-based and sentence-based baseline models and somenumeric results are given in Table 3.
As 100% recallwe took all pairs of entities that can be extracted byeither model.
It is important to note, that since someentity pairs occur exclusively inside the answer sen-tences and some in pairs of question and answer sen-tences, none of the individual models is capable ofachieving 100% recall, and maximum possible re-calls for QnA- and sentence-based models are dif-ferent.Results demonstrate that from 20.5% to 39.4% ofcorrect triples extracted by the QnA-based model arenot extracted by the baseline model, and the com-bination of both models is able to achieve higherprecision and recall.
Unfortunately, comparison ofsentence-based model with and without question-based features (Figure 2) didn?t show a significantdifference.5 Error analysis and future workTo get an idea of typical problems of QnA-basedmodel we sampled and manually judged extractedhigh confidence examples that are not present in99Table 3: Extraction results for QnA- and sentence-based models on both datasetsYahoo!
Answers WikiAnswersQnA Sentence Combined QnA Sentence CombinedF-1 score 0.219 0.276 0.310 0.277 0.297 0.332Number of correct extractions 3229 5900 7428 2804 2288 3779Correct triples not extracted by other model 20.5% 56.5% - 39.4% 25.8% -Figure 2: Precision-Recall curves for QnA-basedvs sentence-based models and sentence-based modelwith and without question featuresFreebase (and thus are considered incorrect forprecision-recall analysis).The major reason (40%) of false positive extrac-tions is errors in entity linking.
For example: ?Whois Tim O?Brien?
He was born in Austin on October1, 1946?.
The model was able to correctly extract[Tim O?Brien, date of birth, October 1, 1946], how-ever Tim O?Brien was linked to a wrong person.
Ina number of cases (16%) our discourse model turnsout to be too simple and fails for answers, that men-tion numerous additional information, e.g.
?How oldis Madonna really?
...Cher was born on 20 May1946 which makes her older that Madonna...?.
Apossible solution would be to either restrict QnA-based model to cases when no additional informa-tion is present or design a better discourse modelwith deeper analysis of the answer sentence and itspredicates and arguments.
Some mistakes are due todistant supervision errors, for example for the mu-sic.composition.composer predicate our model ex-tracts singers as well as composers (which are inmany cases the same).Of course, there are a number of cases, whenour extractions are indeed correct, but are eithermissing (33%) or contradicting with Freebase (8%).An example of an extracted fact, that is missingin Freebase is ?Who is Wole Soyinka?
He studiedat the University College, Ibadan(1952-1954) andthe University of Leeds (1954-1957)?, and [WoleSoyinka, institution, University of Leeds] is cur-rently not present in Freebase.
Contradictions withFreebase occur because of different precision lev-els (?pianist?
vs ?jazz pianist?, city vs county, etc.
),different calendars used for dates or ?incorrect?
in-formation provided by the user.
An example, whenexisting and extracted relation instance are differentin precision is:?Who is Edward Van Vleck?
EdwardVan Vleck was a mathematician born in Middle-town, Connecticut?
we extract [Edward Van Vleck,place of birth, Middletown], however the Freebasecurrently has USA as his place of birth.The problem of ?incorrect?
information providedin the answer is very interesting and worth special100attention.
It has been studied in CQA research, e.g.
(Shah and Pomerantz, 2010), and an example ofsuch QnA pair is: ?Who is Chandrababu Naidu?Nara Chandra Babu Naidu (born April 20, 1951)?.Other authoritative resources on the Web give April20, 1950 as Chandrababu Naidu?s date of birth.
Thisraises a question of trust to the provided answer andexpertise of the answerer.
Many questions on CQAwebsites belong to the medical domain, e.g.
peo-ple asking advices on different health related topics.How much we can trust the answers provided to ex-tract them into the knowledge base?
We leave thisquestion to the future work.Finally, we have seen that only a small fractionof available QnA pairs were annotated with exist-ing Freebase relations, which shows a possible lim-itation of Freebase schema.
A promising directionfor future work is automatic extraction of new pred-icates, which users are interested in and which canbe useful to answer more future questions.6 Related workRelation extraction from natural language text hasbeen an active area of research for many years, anda number of supervised (Snow et al, 2004), semi-supervised (Agichtein and Gravano, 2000) and un-supervised (Fader et al, 2011) methods have beenproposed.
These techniques analyze individual sen-tences and can extract facts stated in them using syn-tactic patterns, sentence similarity, etc.
This workfocus on one particular type of text data, i.e.
QnApairs, and the proposed algorithm is designed to ex-tract relations between entities mentioned in ques-tion and answer sentences.Community question-answering data has been asubject of active research during the last decade.Bian et al (2008) and Shtok et al (2012) show howsuch data can be used for question answering, anarea with a long history of research, and numer-ous different approaches proposed over the decades(Kolomiyets and Moens, 2011).
One particular wayto answer questions is to utilize structured KBs andperform semantic parsing of questions to transformnatural language questions into KB queries.
Berantet al (2013) proposed a semantic parsing model thatcan be trained from QnA pairs, which are much eas-ier to obtain than correct KB queries used previ-ously.
However, unlike our approach, which takesnoisy answer text provided by a CQA website user,the work of Berant et al (2013) uses manually cre-ated answers in a form of single or lists of KB enti-ties.
Later Yao and Van Durme (2014) presented aninformation extraction inspired approach, that pre-dicts which of the entities related to an entity inthe question could be the answer to the question.The key difference of this work from question an-swering is that our relation extraction model doesn?ttarget question understanding problem and doesn?tnecessarily extract the answer to the question, butrather some knowledge it can infer from a QnA pair.Many questions on CQA websites are not factoid,and there are many advice and opinion questions,which simply cannot be answered with a KB en-tity or a list of entities.
However, it is still possi-ble to learn some information from them (e.g.
from?What?s your favorite Stephen King book?
The DarkHalf is a pretty incredible book?
we can learn thatthe Dark Half is a book by Stephen King).
In ad-dition, answers provided by CQA users often con-tain extra information, which can also be useful (e.g.from ?Where was Babe Ruth born?
He was born inBaltimore, Maryland on February 6th, 1895?
we canlearn not only place of birth, but also date of birth ofBabe Ruth).7 ConclusionIn this paper we proposed a model for relation ex-traction from QnA data, which is capable of predict-ing relations between entities mentioned in questionand answer sentences.
We conducted experimentson 2 publicly available CQA datasets and showedthat our model can extract triples not available to ex-isting sentence-based techniques and can be effec-tively combined with them for better coverage of aknowledge base population system.AcknowledgmentsThis work was funded by the Google FacultyResearch Award.
We gratefully thank EvgeniyGabrilovich and Amar Subramanya for numerousvaluable and insightful discussions, and anonymousreviewers for useful comments on the work.101ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM Conference on Dig-ital Libraries, DL ?00, pages 85?94, New York, NY,USA.
ACM.S?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.
2007.Dbpedia: A nucleus for a web of open data.
Springer.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP?13, pages 1533?1544.Jiang Bian, Yandong Liu, Eugene Agichtein, andHongyuan Zha.
2008.
Finding the right facts in thecrowd: Factoid question answering over social media.In Proceedings of the 17th International Conferenceon World Wide Web, WWW ?08, pages 467?476, NewYork, NY, USA.
ACM.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A Col-laboratively Created Graph Database for StructuringHuman Knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Management ofData, SIGMOD ?08.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E.R.
Hr-uschka Jr., and T.M.
Mitchell.
2010.
Toward an Archi-tecture for Never-Ending Language Learning.
In Pro-ceedings of the Conference on Artificial Intelligence(AAAI), AAAI?10, pages 1306?1313.
AAAI Press.Angel X Chang, Valentin I Spitkovsky, Eneko Agirre, andChristopher D Manning.
2011.
Stanford-ubc entitylinking at tac-kbp, again.
In Proceedings of Text Anal-ysis Conference, TAC?11.Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, WilkoHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,Shaohua Sun, and Wei Zhang.
2014.
Knowledgevault: A web-scale approach to probabilistic knowl-edge fusion.
In Proceedings of the 20th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, KDD ?14, pages 601?610, NewYork, NY, USA.
ACM.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open informationextraction.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?11, pages 1535?1545, Stroudsburg, PA,USA.
Association for Computational Linguistics.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated and ex-tracted knowledge bases.
In Proceedings of the 20thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?14, pages1156?1165, New York, NY, USA.
ACM.Oleksandr Kolomiyets and Marie-Francine Moens.
2011.A survey on question answering technology froman information retrieval perspective.
Inf.
Sci.,181(24):5412?5434, December.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, ACL ?09.Chirag Shah and Jefferey Pomerantz.
2010.
Evaluat-ing and predicting answer quality in community qa.In Proceedings of the 33rd international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 411?418.
ACM.Anna Shtok, Gideon Dror, Yoelle Maarek, and IdanSzpektor.
2012.
Learning from the past: Answeringnew questions with past answers.
In Proceedings ofthe 21st International Conference on World Wide Web,WWW ?12, pages 759?768, New York, NY, USA.ACM.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
Advances in Neural Information Process-ing Systems 17.Valentin I Spitkovsky and Angel X Chang.
2012.
Across-lingual dictionary for english wikipedia con-cepts.
In LREC, pages 3168?3175.Mihai Surdeanu and Heng Ji.
2014.
Overview of the en-glish slot filling track at the tac2014 knowledge basepopulation evaluation.
In Proc.
Text Analysis Confer-ence (TAC2014).Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, andChristopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In Proceed-ings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, EMNLP-CoNLL?12, pages 455?465, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Xuchen Yao and Benjamin Van Durme.
2014.
Informa-tion extraction over structured data: Question answer-ing with freebase.
In Proceedings of ACL, ACL?14.102
