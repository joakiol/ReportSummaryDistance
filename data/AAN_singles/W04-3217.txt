Automatic Analysis of Plot for Story RewritingHarry HalpinSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotland, UKH.Halpin@ed.ac.ukJohanna D. MooreSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotland, UKJ.Moore@ed.ac.ukJudy RobertsonSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWScotland, UKjudyr@inf.ed.ac.ukAbstractA method for automatic plot analysis of narrativetexts that uses components of both traditional sym-bolic analysis of natural language and statisticalmachine-learning is presented for the story rewrit-ing task.
In the story rewriting task, an exemplarstory is read to the pupils and the pupils rewrite thestory in their own words.
This allows them to prac-tice language skills such as spelling, diction, andgrammar without being stymied by content creation.Often the pupil improperly recalls the story.
Ourmethod of automatic plot analysis enables the tu-toring system to automatically analyze the student?sstory for both general coherence and specific miss-ing events.1 IntroductionStoryStation is an intelligent tutoring system cre-ated to provide personalized attention and detailedfeedback to children ages 10-12 on their writing(Roberston and Wiemar-Hastings, 2002).
Writingis viewed as a skill-based task, with skills beingelements of writing such as spelling, diction, andplot development.
Each writing skill is associatedwith an animated agent that provides online help.Evaluations of StoryStation show that children en-joy the personalized encouragement and construc-tive comments that StoryStation provides (Robert-son and Cross, 2003).
StoryStation was designedby researchers in conjunction with two teachers anda group of students.
However, both students andteachers indicated StoryStation would be signifi-cantly improved if it were enhanced with an agentthat could give feedback about the plot of a story.Here we describe how techniques from symbolicnatural language processing and statistical machine-learning were used to tackle the problem of auto-mated plot analysis for StoryStation.2 The Story Rewriting TaskIn the story rewriting task, pupils rewrite a story intheir own words, allowing them to focus on theirwriting ability instead of plot formulation.
This taskis currently used in Scottish schools and thus it waschosen to be the first feature of the plot analysisagent.
We collected a corpus of 103 stories rewrittenby children from classes at primary schools in Scot-land.
Pupils were told a story, an exemplar story,by a storyteller and were asked to rewrite the storyin their own words.1 The automated plot analysisprogram must be able to give a general rating of thequality of the rewritten story?s plot and be able todetermine missing or incorrect events.
The generalrating can be used by the teacher to find out whichpupils are in need of attention, while the more spe-cific details can be used by an animated agent inStoryStation to remind the student of specific eventsand characters they have forgotten or misused.3 Plot RatingsThe stories were rated for plot by three differentraters.
A story-teller (Rater B) ranked all of the sto-ries.
Two others (Rater A, a teacher, and Rater C)ranked the stories as well, although Rater A rankedonly half.
The following scale, devised by a teacherwith over forty years of experience, was used.1.
Excellent: An excellent story shows that thereader understands the ?point?
of the story andshould demonstrate some deep understandingof the plot.
The pupil should be able to retrieveall the important links and, not all the details,but the right details.2.
Good: A good story shows that the pupil waslistening to the story, and can recall the main1The exemplar story used in our corpus was ?Nils?
Ad-venture,?
a story from ?The Wonderful Adventures of Nils?
(Lagerloff, 1907).Class Probability Number of Class1 (Excellent) 0.175 182 (Good) 0.320 333 (Fair) 0.184 194 (Poor) 0.320 33Table 1: Distribution of Story Ratingsevents and links in the plot.
However, thepupil shows no deeper understanding of theplot, which can often be detected by the pupilleaving out an important link or emphasizingthe wrong details.3.
Fair: A fair story shows that the pupil is miss-ing more than one link or chunk of the story,and not only lacks an understanding of the?point?
but also lacks recall of vital parts ofthe story.
A fair story does not really flow.4.
Poor: A poor story has definite problems withrecall of events, and is missing substantialamount of the plot.
Characters will be misiden-tified and events confused.
Often the childwrites on the wrong subject or starts off recit-ing only the beginning of the story.Rater B and Rater A had an agreement of 39%while Rater B and Rater C had an agreement of77%.
However, these numbers are misleading as therating scale is ordinal and almost all the disagree-ments were the result of grading a story either onerank better or worse.
In particular Rater A usuallymarked incomplete stories as poor while the otherraters assigned partial credit.
To evaluate the relia-bility of the grades both Cronbach?s ?
and Kendall?s?b were used, since these statistics take into accountordinal scales and inter-rater reliability.
BetweenRater A and B there was a Cronbach?s ?
statisticof .86 and a Kendall?s ?b statistic of .72.
BetweenRater B and C there was a Cronbach?s ?
statisticof .93 and Kendall?s ?b statistic of .82.
These statis-tics show our rating scheme to be fairly reliable.
Asthe most qualified expert to rate all the stories, RaterB?s ratings were used as the gold standard.
The dis-tribution of plot ratings are given in Table 1.4 A Minimal Event CalculusThe most similar discourse analysis program to theone needed by StoryStation is the essay-gradingcomponent of ?Criterion?
by ETS technologies(Burstein et al, 2003), which is designed to anno-tate parts of an essay according to categories suchas ?Thesis, ?Main Points,?
?Support,?
and ?Con-clusion.?
Burstein et.
al.
(2003) uses RhetoricalStructure Theory to parse the text into discourse re-lations based on satellites and nuclei connected byrhetorical relations.
Moore and Pollack (1992) notethat Rhetorical Structure Theory conflates the infor-mational (the information being conveyed) and in-tentional (the effects on the reader?s beliefs or atti-tudes) levels of discourse.
Narratives are primarilyinformational, and so tend to degenerate to long se-quences of elaboration or sequence relations.
Sincein the story rewriting task the students are attempt-ing to convey information about the narrative, un-like the primarily persuasive task of an essay, oursystem focuses on the informational level as embod-ied by a simplified event calculus.
Another tutoringsystem similar to ours is the WHY physics tutoringsystem (Rose et al, 2002).We formulate only three categories to describestories: events, event names, and entities.
This for-mulation keeps the categories from being arbitraryor exploding in number.
Entities are both animatecharacters, such as ?elves?
and ?storks,?
and inani-mate objects like ?sand?
and ?weather.?
Nouns arethe most common type of entities.
Events are com-posed of the relationships among entities, such as?the boy becomes an elf,?
which is composed of a?boy?
and ?elf?
interacting via ?becoming,?
whichwe call the event name.
This is because the useof such verbs is an indicator of the presence of anevent in the story.
In this manner events are relation-ships labeled with an event name, and entities arearguments to these relationships as in propositionallogic.
Together these can form events such as be-come(boy,elf), and this formulation maps partiallyonto Shanahan?s event calculus which has beenused in other story-understanding models (Mueller,2003).
The key difference between an event calcu-lus and a collection of propositions is that time isexplicitly represented in the event calculus.Each story consists of a group of events that arepresent in the story, e1...eh.
Each event consists ofan event name, a time variable t, and a set of enti-ties arranged in an ordered set n1...na.
An eventmust contain one and only one event name.
Theevent names are usually verbs, while the entitiestend to be, but are not exclusively, nouns.
Time ismade explicit through a variable t. Normally, theShanahan event calculus has a series of predicatesto deal with relations of achievements, accomplish-ments, and other types of temporal relations (Shana-han, 1997), however our calculus does not use thesesince it is difficult to extract these from ungrammati-cal raw text automatically.
A story?s temporal orderis a partial ordering of events as denoted by theirtime variable t. When incorporating a set of entitiesinto an event, a superscript is used to keep the enti-ties distinct, as n13 is entity 1 in event 3.
An entitymay appear in multiple events, such as entity 1 ap-pearing in event 3 (n13) and in event 5 (n15).
The plotof a story can then be considered an event structureof the following form if it has h events:e1(t1, (n11, n21, ...na1)), ...., eh(th, (n2h, n4h...nch))Where time t1 ?
t2 ?
...th.
An example from arewritten story is ?Nils found a coin and he walkedround a sandy beach.
He talked to the stork.
Askeda question.?
This is represented by an event struc-ture as:find(t = 1(Nils, coin)),walk(t = 1, (Nils, sand, beach)),talk(t = 2, (stork, Nils)),ask(t = 3, (question))Note that the rewritten stories are often ungram-matical.
A sentence may map onto one, multiple, orno events.
Two stories match if they are composedof the same ordering of events.5 Extracting the Event CalculusThe event calculus can be extracted from raw textby layering NLP modules using an XML-basedpipeline.
Our main constraint was that the text of thepupil was rarely grammatical, restricting our choiceof NLP components to those that did not require acorrect parse or were in any other ways dependenton grammatical sentences.
At each level of process-ing, an XML-enabled natural language processingcomponent can add mark-up to the text, and use anymark-up that the previous components made.
Alllayers in the pipeline are fully automatic.
For ourpipeline we used LT-TTT (Language TechnologyText Tokenization Toolkit) (Grover et al, 2000).Once words are tokenized and sentence boundariesdetected by LT-TTT, LT-POS tags the words usingthe Penn Treebank tag-set without parsing the sen-tences.
While a full parse could be generated by astatistical parser, such parses would likely be incor-rect for the ungrammatical sentences often gener-ated by the pupils (Charniak, 2000).
Pronouns areresolved using a cascading rule-based approach di-rectly inspired by the CogNIAC algorithm (Bald-win, 1997) with two variations.
First, it resolves indistinct cascades for singular and then plural pro-nouns.
Second, it resolves using only the Cog-NIAC rules that can be determined using Penn Tree-bank tags.
The words are lemmatized using an aug-mented version of the SCOL Toolset and sentencesare chunked using the Cass Chunker (Abney, 1995).There is a trade-off between this chunking approachthat works on ungrammatical sentences and one thatrequires a full parse such as those using dependencygrammars.
The Cass Chunker is highly precise,but often inaccurate and misses relations and enti-ties that are not in a chunk.
In its favor, those tu-ples in chunks that it does identify are usually cor-rect.
SCOL extracts tuples from the chunks to deter-mine the presence of events, and the remaining ele-ments in the chunk are inspected via rules for enti-ties.
Time is explicitly identified using a variation ofthe ?now point?
algorithm (Allen, 1987).
We mapeach event?s time variable to a time-line, assumingthat events occur in the order in which they appearin the text.
While temporal ordering of events ishard (Mani and Wilson, 2003), given that childrenof this age tend to use a single tense throughout thenarrative and that in narratives events are presentedin order (Hickmann, 2003), this simple algorithmshould suffice for ordering in the domain of chil-dren?s stories.6 Plot Comparison AlgorithmSince the story rewriting task involves imperfect re-call, story events will likely be changed or left outby the pupil.
The story rewriting task involves thestudents choosing their own diction and expressingtheir own unique mastery of language, so variationin how the fundamental elements of the story arerewritten is to be expected.
To deal with these is-sues, an algorithm had to be devised that takes theevent structure of the rewritten story and comparesit to the event structure of the exemplar story, whiledisregarding the particularities of diction and gram-mar.
The problem is one of credit allocation for thesimilarity of rewritten events to the exemplar event.The words used in the events of the two story mod-els may differ.
The exemplar story model mightuse the event see(Nils,stork), but a rewritten storymay use the word ?bird?
instead of the more preciseword ?stork.?
However, since the ?bird?
is refer-ring to the stork in the exemplar story, partial creditshould be assigned.
A plot comparison algorithmwas created that uses abstract event calculus repre-sentations of plot and the text of the rewritten story,taking into account temporal order and word simi-larity.
The exemplar story?s event structure is cre-ated by applying the event extraction pipeline to thestoryteller?s transcript.The Plot Comparison Algorithm is given in Fig-ure 1.
In the pseudo-code, E of size h and R of sizej are the event structures of the exemplar story andrewritten story respectively, with the names of eachof their events denoted as e and r. The set of entitiesof each event are denoted as Ne and Nr respectively.T is the lemmatized tokens of the rewritten story?sraw text.
WordNet(x) denotes the synset of x.
The?now point?
of the rewritten story is t, and featureset is f , which has an index of i.
The index i isincremented every time f is assigned a value.
1 de-notes an exact match, 2 a WordNet synset match, 3a match in the text, and 0 a failure to find any match.The Plot Comparison Algorithm essentially iter-ates through the exemplar story looking for matchesof the events in the rewritten story.
To find if twoevents are in or out of order the rewritten story hasa ?now point?
that serves as the beginning of its it-eration.
Each event of the event structure of the ex-emplar story is matched against each event of therewritten story starting at the ?now point?
and us-ing the exact text of the event name.
If that matchfails a looser match is attempted by giving the eventnames of the rewritten story to WordNet and see-ing if a match to the resultant synset succeeds (Fell-baum, 1998).
If either match attempt succeeds, thealgorithm attempts to match entities in the samefashion and the ?now point?
of the rewritten storyis incremented.
Thus the algorithm does not looksback in the rewritten story for a match.
If the eventmatch fails, one last attempt is made by checkingthe event name or entity against every lemmatizedtoken in the entire rewritten text.
If this fails, a fail-ure is recorded.
The results of the algorithm are canbe used as a feature set for machine-learning.
Theevent calculus extraction pipeline and the Plot Com-parison Algorithm can produce event calculus rep-resentations of any English text and compare them.They have been tested on other stories that do nothave a significant corpus of rewritten stories.
Thenumber of events for an average rewritten story inour corpus was 26, with each event having an aver-age of 1 entity.Included in Figure 2 is sample output from ouralgorithm given the exemplar story model ea and arewritten story rb whose text is as follows: Nils tookthe coin and tossed it away, cause it was worthless.A city appeared and so he walked in.
Everywherewas gold and the merchant said Buy this Only onecoin Nils has no coin.
So he went to get the coin hethrew away but the city vanished just like that rightbehind him.
Nils asked the bird Hey where the citygo?
Let?s go home.Due to space limitations, we only display selectedevents from the transcript and their most likelymatch from the rewritten story in Figure 2.
The out-put of the feature set would be the concatenation inorder of every value of fe.Algorithm6.1: PLOTCOMPARE(E, R, T )t?
1i?
0for ex ?
e1 to ehdo for ry ?
rt to rjdo????????????????????????????????????????????????????????????
?if ex = rythen fi ?
1 and t?
t + 1else if ex ?
WORDNET(ry)then fi ?
2 and t?
t + 1if fi = 1 or 2then??????????????????????
?for each n ?
Neif n ?
N rthen fi ?
1else if n ?
WORDNET(Nr)then fi ?
2else if n ?
Tthen fi ?
3else fi ?
0else if ex ?
Tthen fi ?
3else fi ?
0Figure 1: Plot Comparison Algorithmea rb fethrow(Nils, coin) toss(coin) 2, 3, 1see(Nils, city) appear(city) 0, 3, 3enter(Nils, city) walk(Nils) 0, 3, 3ask(Nils, merchant) say(merchant) 0, 3, 3say(Nils) say(merchant) 1, 3leave(Nils) go(Nils) 2, 1disappear(city) vanish(city) 2, 1inquire(Nils, stork) ask(Nils, bird) 2, 1, 2fly(stork) go(home) 0, 3Figure 2: Example of Plot Algorithm7 Learning the Significance of EventsMachine-learning is crucial to our experiment, as itwill allow our model to discriminate what eventsand words in a rewritten story are good predictorsof plot quality as rated by a human expert.
Wehave restricted our feature set to the results of thePlot Comparison Algorithm and LSA scores, as wedescribe below.
Other possible features, such asthe grammatical correctness and the number of con-junctives, are dealt with by other agents in StoryS-tation.
We are focusing on plot recall quality asopposed to general writing quality.
Two differentmachine-learning algorithms with differing assump-tions were used.
These are by no means exhaus-tive of the options, and extensive tests have beendone with other algorithms.
Further experimentsare needed to understand the precise nature of therelations between the feature set and machine learn-ing algorithms.
All results were created by ten-foldcross validation over the rated stories, which is es-pecially important given our small corpus size.7.1 Nearest Neighbors using LSAWe can classify the stories without using the re-sults of the Plot Comparison Algorithm, and insteaduse only their statistical attributes.
Latent SemanticAnalysis (LSA) provides an approximation of ?se-mantic?
similarity based on the hypothesis that thesemantics of a word can be deduced from its contextin an entire document, leading to useful coherencyscores when whole documents are compared (Foltzet al, 1998).
LSA compares the text of each rewrit-ten story in the corpus for similarity to the transcriptof the exemplar story in a subspace produced byreducing the dimensionality of the TASA 12 gradeUSA reading-level to 200.
This dimensionality wasdiscovered through experimentation to be our prob-lem?s optimal parameters for LSA given the rangeof choices originally used by Landauer (1997).
Thestories can be easily classified by grouping them to-gether based on LSA similarity scores alone, andthis technique is embodied in the simple K-NearestNeighbors (K-NN) learner.
K-NN makes no para-metric assumptions about the data and uses no for-mal symbolic features other than an LSA similarityscore.
For K-NN k = 4 gave the best results overa large range of k, and we expect this k would beideal for stories of similar length.As shown in Table 2, despite its simplicity this al-gorithm performs fairly well.
It is not surprising thatfeatures based primarily on word distributions suchas LSA could correctly discriminate the non-poorfrom the poor rewritten stories.
Some good rewrit-ten stories closely resemble the exemplar story al-most word for word, and so share the same worddistribution with the exemplar story.
Poor rewrittenstories usually have little resemblance to the exem-plar story, and so have a drastically different worddistribution.
The high spread of error in classifyingstories is shown in the confusion matrix in Table 3.This leads to unacceptable errors such as excellentstories being classified as poor stories.7.2 Hybrid Model with Naive BayesBy using both LSA scores and event structures asfeatures for a statistical machine learner, a hybridmodel of plot rating can be created.
In hybrid mod-Class Precision Recall F-score1 (Excellent) 0.11 0.17 0.132 (Good) 0.42 0.46 0.443 (Fair) 0.30 0.16 0.214 (Poor) 0.83 0.76 0.79Table 2: K-Nearest Neighbors Precision and RecallClass 1 2 3 41 (Excellent) 3 10 4 12 (Good) 13 15 2 33 (Fair) 9 6 3 14 (Poor) 2 5 1 25Table 3: K-Nearest Neighbors: Confusion Matrixels a formal symbolic model (the event calculus-based results of a Plot Comparison Algorithm) en-ters a mutually beneficial relationship with a statis-tical model of the data (LSA), mediated by a ma-chine learner (Naive Bayes).
One way to combineLSA similarity scores and the results of the eventstructure is by using the Naive Bayes (NB) ma-chine learner.
NB makes the assumptions of bothparametrization and Conditional Independence.The recall and precision per rank is given in Ta-ble 4, and it is clear that while no stories are clas-sified as excellent at all, the majority of good andpoor stories are identified correctly.
As shown bythe confusion matrix in Table 5, NB does not de-tect excellent stories and it collapses the distinctionbetween good and excellent stories.
Compared toK-NN with LSA, NB shows less spread in its er-rors, although it does confuse some poor stories asgood and one excellent story as fair.
Even thoughit mistakenly classifies some poor stories as good,for many teachers this is better than misidentifyinga good story as a poor story.The raw accuracy results over all classes of themachine learning algorithms are summarized in Ta-ble 6.
Note that average human rater agreementis the average agreement between Rater A and C(whose agreement ranged from 39% to 77%), sinceRater B?s ratings were used as the gold standard.This average also assumes Rater A would have con-tinued marking at the same accuracy for the com-Class Precision Recall F-Score1 (Excellent) 0.00 0.00 0.002 (Good) 0.43 0.88 0.583 (Fair) 0.45 0.26 0.334 (Poor) 0.92 0.67 0.77Table 4: Naive Bayes Precision and RecallClass 1 2 3 41 (Excellent) 0 17 1 02 (Good) 1 29 2 13 (Fair) 0 13 5 14 (Poor) 0 8 3 22Table 5: Naive Bayes Confusion MatrixMachine Learner Percentage CorrectK-NN (LSA) 44.66%ID3 DT (Events) 40.78%NB (LSA + Events) 54.37%Rater Agreement 58.37%Table 6: Machine Learner Comparisonplete corpus.
DT refers to an ID3 Decision Treealgorithm that creates a purely symbolic machine-learner whose feature set was only the results of thePlot Comparison Algorithm (Quinlan, 1986).
It per-formed worse than K-NN and thus the details arenot reported any further.
Using NB and combiningthe LSA scores with the results of the Plot Com-parison Algorithm produces better raw performancethan K-NN.
Recall of 54% for NB may seem dis-appointing, but given that the raters only have anaverage agreement of 58%, the performance of themachine learner is reasonable.
So if the machine-learner had a recall of 75% it would be suspect.Statistics to compare the results given the ordinalnature of our rating scheme are shown in Table 7.8 DiscussionFrom these experiments as shown in Table 6 wesee that the type of machine learner and the par-ticular features are important to correctly classifychildren?s stories.
Inspection of the results showsthat separating good and excellent stories from poorstories is best performed by Naive Bayes.
For ourapplication, teachers have indicated that the classi-fication of an excellent or good story as a poor oneis considered worse than the classifying of a fairor even poor story as good.
Moreover, it uses theevent-based results of the Plot Comparison Algo-rithm so that the agent in StoryStation may use theseresults to inform the student what precise events andentities are missing or misused.
NB is fast enough toprovide possible feedback in real time and its abil-ity to separate poor stories from good and excellentstories would allow it to be used in classrooms.
Italso has comparable raw accuracy to average humanagreement as shown in Table 6, although it makesmore errors than humans in classifying a story offby more than one class off as shown by the statisticsMachine Learner Cronbach?s ?
Kendall?s ?bNB to Rater B .78 .59Rater A to Rater B .86 .72Rater C to Rater B .93 .82Table 7: Statistical Comparisonin Table 7.
The results most in its favor are shownhighlighted in Table 5.
It separates with few errorsboth excellent and good stories from the majority ofpoor stories.While the event calculus captures some of the rel-evant defining characteristics of stories, it does notcapture all of them.
The types of stories that give themachine learners the most difficulty are those whichare excellent and fair.
One reason is that these sto-ries are less frequent in the training data than poorand good stories.
Another reason is that there arefeatures particular to these stories that are not ac-counted for by an event structure or LSA.
Both ex-cellent stories and fair stories rely on very subtlefeatures to distinguish them from good and poor sto-ries.
Good stories were characterized in the ratingcriteria as ?parroting off of the main events,?
andthe event calculus naturally is good at identifyingthis.
Poor stories have ?definite problems with therecall of events,?
and so are also easily identified.However, fair stories show both a lack of ?under-standing of the point?
and ?do not really flow?
whilethe excellent story shows an ?understanding of thepoint.?
These characteristics involve relations suchas the ?point?
of the story and connections betweenevents.
These ideas of ?flow?
and ?point?
are muchmore difficult to analyze automatically.9 ConclusionDue to its practical focus, the plot analysis of oursystem is very limited in nature, focusing on justthe story rewriting task.
Traditionally ?deep?
rep-resentation systems have attempted to be powerfulgeneral-purpose story understanding or generationsystems.
A general plot analysis agent would bemore useful than our current system, which is suc-cessful by virtue of the story rewriting task beingless complex than full story understanding.
How-ever, our system fulfills an immediate need in theStoryStation application, in contrast to more tra-ditional story-understanding and story-generationsystems, which are usually used as testing groundsfor theoretical ideas in artificial intelligence.
Thesystem was tested and developed using a small man-ually collected corpus of a single rewritten story.While previous researchers who worked on thisproblem felt that the small size of the corpus mademachine-learning unusable, the results shows thatwith careful feature selection and relatively simplealgorithms empirical methods can be made to work.We expect that our technique can be generalized tolarger corpora of diverse types.Our hybrid system uses both LSA and eventstructures to classify plot quality.
The use of eventstructures in classifying stories allows us to de-tect whether particular crucial characters and eventshave been left out of the rewritten story.
Separatingthe students who have written good plots from thosewho have done so poorly is a boon to the teachers,since often it is the students who have the most dif-ficulty with plot that are least likely to ask a teacherfor help.
StoryStation is now being used in twoschools as part of their classroom writing instruc-tion over the course of the next year.
Results fromthis study will be instrumental in shaping the futureof the plot analysis system in StoryStation and theexpansion of the current system into a general pur-pose plot analysis system for other writing tasks.ReferencesSteven Abney.
1995.
Chunks and dependencies:Bringing processing evidence to bear on syntax.In Jennifer Cole, Georgia Green, and Jerry Mor-gan, editors, Computational Linguistics and theFoundations of Linguistic Theory, pages 145?164.James Allen.
1987.
Natural Language Understand-ing.
Menlo Park, CA, Benjamin/Cummings Pub-lishing.Breck Baldwin.
1997.
CogNIAC : A High Preci-sion Pronoun Resolution Engine.Jill Burstein, Daniel Marcu, and Kevin Knight.2003.
Finding the WRITE Stuff: AutomaticIdentification of Discourse Structure in StudentEssays.
IEEE Intelligent Systems, pages 32?39.Eugene Charniak.
2000.
A Maximum-Entropy In-spired Parser.
In Proceedings of the North Amer-ican Association for Computational Linguistics.Christine Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherencewith Latent Semantic Analysis.
Discourse Pro-cesses, 25(2&3):285?307.Claire Grover, Colin Matheson, Andrei Mikheev,and Marc Moens.
2000.
LT TTT - A FlexibleTokenisation Tool.
In Proceedings of the SecondLanguage Resources and Evaluation Conference.Maya Hickmann.
2003.
Children?s Discourse: per-son, space and time across language.
CambridgeUniversity Press, Cambridge, UK.Selma Lagerloff.
1907.
The Wonderful Adventuresof Nils.
Doubleday, Page, and Company, GardenCity, New York.Thomas.
Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The Latent SemanticAnalysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Re-view.I.
Mani and G. Wilson.
2003.
Robust temporal pro-cessing of the news.
In In Proceedings of Associ-ation for Computational Linguistics.Johanna D. Moore and Martha Pollack.
1992.A problem for RST: The need for multi-leveldiscourse analysis.
Computational Linguistics,18(4):537?544.Erik T. Mueller.
2003.
Story understandingthrough multi-representation model construction.In Graeme Hirst and Sergei Nirenburg, editors,Text Meaning: Proceedings of the HLT-NAACL2003 Workshop, pages 46?53, East Stroudsburg,PA.
Association for Computational Linguistics.Ross Quinlan.
1986.
Induction of decision trees.
InMachine Learning, volume 1.
Kluwer AcademicPress.Judy Roberston and Peter Wiemar-Hastings.
2002.Feedback on children?s stories via multiple inter-face agents.
In International Conference on In-telligent Tutoring Systems, Biarritz, France.Judy Robertson and Beth Cross.
2003.
Children?sperceptions about writing with their teacher andthe StoryStation learning environment.
Narrativeand Interactive Learning Environments: SpecialIssue of International Journal of Continuing En-gineering Education and Life-long Learning.C.
Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-tava, and K. VanLehn.
2002.
A hybrid languageunderstanding approach for robust selection of tu-toring goals.
In International Conference on In-telligent Tutoring Systems, Biarritz, France.Murray Shanahan.
1997.
Solving the Frame Prob-lem.
MIT Press, Cambridge, MA.
