Dependency Parsing of Modern StandardArabic with Lexical and Inflectional FeaturesYuval Marton?Nuance CommunicationsNizar Habash?
?Center for Computational LearningSystems, Columbia UniversityOwen Rambow?Center for Computational LearningSystems, Columbia UniversityWe explore the contribution of lexical and inflectional morphology features to dependencyparsing of Arabic, a morphologically rich language with complex agreement patterns.
Using con-trolled experiments, we contrast the contribution of different part-of-speech (POS) tag sets andmorphological features in two input conditions: machine-predicted condition (in which POS tagsand morphological feature values are automatically assigned), and gold condition (in which theirtrue values are known).
We find that more informative (fine-grained) tag sets are useful in thegold condition, but may be detrimental in the predicted condition, where they are outperformedby simpler but more accurately predicted tag sets.
We identify a set of features (definiteness,person, number, gender, and undiacritized lemma) that improve parsing quality in the predictedcondition, whereas other features are more useful in gold.
We are the first to show that functionalfeatures for gender and number (e.g., ?broken plurals?
), and optionally the related rationality(?humanness?)
feature, are more helpful for parsing than form-based gender and number.
Wefinally show that parsing quality in the predicted condition can dramatically improve by trainingin a combined gold+predicted condition.
We experimented with two transition-based parsers,MaltParser and Easy-First Parser.
Our findings are robust across parsers, models, and inputconditions.
This suggests that the contribution of the linguistic knowledge in the tag sets andfeatures we identified goes beyond particular experimental settings, and may be informative forother parsers and morphologically rich languages.1.
IntroductionFor Arabic?as for other morphologically rich languages?the role of morphology isoften expected to be essential in syntactic modeling, and the role of word order is lessimportant than in morphologically poorer languages such as English.
Morphology?
Nuance Communications, 505 First Ave. S, Suite 700, Seattle, WA 98104.
E-mail: yuvalmarton@gmail.com.??
Center for Computational Learning, Columbia University.
E-mail: habash@ccls.columbia.edu.?
Center for Computational Learning, Columbia University.
E-mail: rambow@ccls.columbia.edu.Submission received: October 1, 2011; revised submission received: June 16, 2012; accepted for publication:August 3, 2012.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 1interacts with syntax in two ways: agreement and assignment.
In agreement, there iscoordination between the morphological features of two words in a sentence basedon their syntactic configuration (e.g., subject?verb or noun?adjective agreement inGENDER and/or NUMBER).
In assignment, specific morphological feature values areassigned in certain syntactic configurations (e.g., CASE assignment for the subject ordirect object of a verb).1Parsing model design aims to come up with features that best help parsers learnthe syntax and choose among different parses.
The choice of optimal linguistic featuresdepends on three factors: relevance, redundancy, and accuracy.
A feature has relevanceif it is useful in making an attachment (or labeling) decision.
A particular feature mayor may not be relevant to parsing.
For example, the GENDER feature may help parsethe Arabic phrase / bAb AlsyAr Aljdyd/Aljdyd (?door the-carthe-newmasc.sg/fem.sg [lit.
]),2 using syntactic agreement: if the-new is masculine (),it should attach to the masculine door, resulting in the meaning ?the car?s new door?
;if the-new is feminine ( ), it should attach to the feminine the-car, resulting in ?thedoor of the new car.?
Conversely, the ASPECT feature does not constrain any syntacticdecision.
Even if relevant, a feature may not necessarily contribute to optimal perfor-mance because it may be redundant with other features that surpass it in relevance.
Forexample, as we will see, the DET and STATE features alone both help parsing becausethey help identify the idafa construction, but they are redundant with each other and theDET feature is more helpful because it also helps with adjectival modification of nouns.Finally, the accuracy of automatically predicting the feature values (ratio of correctpredictions out of all predictions) of course affects the value of a feature on unseen text.Even if relevant and non-redundent, a feature may be hard to predict with sufficientaccuracy by current technology, in which case it will be of little or no help for parsing,even if helpful when its gold values are provided.
As we will see, the CASE feature isvery relevant and not redundant, but it cannot be predicted with high accuracy andoverall it is not useful.Different languages vary with respect to which features may be most helpful givenvarious tradeoffs among these three factors.
In the past, it has been shown that if wecan recognize the relevant morphological features in assignment configurations wellenough, then they contribute to parsing accuracy.
For example, modeling CASE in Czechimproves Czech parsing (Collins et al1999): CASE is relevant, not redundant, and canbe predicted with sufficient accuracy.
It has been more difficult showing that agreementmorphology helps parsing, however, with negative results for dependency parsing inseveral languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin2008; Nivre 2009).In this article we investigate morphological features for dependency parsing ofModern Standard Arabic (MSA).
For MSA, the space of possible morphological featuresis fairly large.
We determine which morphological features help and why.
We furtherdetermine the upper bound for their contribution to parsing quality.
Similar to previous1 Other morphological features, such as MOOD or ASPECT, do not interact with syntax at all.
Note also thatwe do not commit to a specific linguistic theory with these terms; hence, other theoretical terms such asthe Minimalist feature checking may be used here just as well.2 All Arabic transliterations are presented in the HSB transliteration scheme (Habash, Soudi, andBuckwalter 2007): (alphabetically) Abt?jHxd?rzs?SDTD??
?fqklmnhwy and the additional symbols: ?
, ?,A?, A?, w?
, y?
,  , ?
, a , u , i , ?
, ?
, u?
, ??
.162Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Featuresresults, assignment features, specifically CASE, are very helpful in MSA, though onlyunder gold conditions: Because CASE is rarely explicit in the typically undiacritizedwritten MSA, it has a dismal accuracy rate, which makes it useless when used in amachine-predicted (real, non-gold) condition.
In contrast with previous results, weshow agreement features are quite helpful in both gold and predicted conditions.
Thisis likely a result of MSA having a rich agreement system, covering both verb?subjectand noun?adjective relations.
The result holds for both the MaltParser (Nivre 2008) andthe Easy-First Parser (Goldberg and Elhadad 2010).Additionally, almost all work to date in MSA morphological analysis and part-of-speech (POS) tagging has concentrated on the morphemic form of the words.
Often,however, the functional morphology (which is relevant to agreement, and relates tothe meaning of the word) is at odds with the ?surface?
(form-based) morphology; awell-known example of this are the ?broken?
(irregular) plurals of nominals.
We showthat by modeling the functional morphology rather than the form-based morphology,we obtain a further increase in parsing performance (again, both when using gold andwhen using predicted POS and morphological features).
To our knowledge, this workis the first to use functional morphology features in MSA processing.As a further contribution of this article, we show that for parsing with pre-dicted POS and morphological features, training on a combination of gold and pre-dicted POS and morphological feature values outperforms the alternative trainingscenarios.The article is structured as follows.
We first present relevant Arabic linguistic facts,their representation in the annotated corpus we use, and variations of abstractionthereof in several POS tag sets (Section 2).
We follow with a survey of related work(Section 3), and describe our basic experiments in Section 4.
We first explore the con-tribution of various POS tag sets, (form-based) morphological features, and promisingcombinations thereof, to Arabic dependency parsing quality?in straightforward fea-ture engineering design and combination heuristics.
We also explore more sophisticatedfeature engineering for the determiner (DET) feature.
In Section 5, we proceed to anextended exploration of functional features.
This includes using functional NUMBERand GENDER feature values, instead of form-based values; using the non-form-basedrationality (RAT) feature; and combinations thereof.
We additionally consider the appli-cability of our results to a different parser (Section 6) and consider combining gold andpredicted data for training (Section 7).
Section 8 presents a result validation on unseentest data, as well as an analysis of parsing error types under different conditions.
Weconclude and provide a download link to our model in Section 9.
Last, we include anappendix with further explorations of PERSON feature engineering, ?binning?
of Arabicnumber constructions according to their complex syntactic patterns, and embeddinguseful morphological features in the POS tag set.
Much of Sections 2?5 was presentedin two previous publications (Marton, Habash, and Rambow 2010, 2011).
This articleextends that previous work by:1. evaluating all our parsing models in both gold and non-gold conditions (wherebefore this was true for only select models in Sections 4?5),2. using a newer version of our Arabic functional morphology resource (Section 5),3. evaluating several of our most notable parsing models with an additional parser(Section 6),4. exploring two additional training methods, as already mentioned above (Sec-tion 7), and163Computational Linguistics Volume 39, Number 15. providing an extended discussion and comparison of several notable and bestperforming models, including analyses of their performance per dependency tag(Section 8).2.
Experimental Data and Relevant Linguistic ConceptsIn this section, we present the linguistic concepts relevant to our discussion of Arabicparsing, and the data we use for our experiments.
We start with the central concept ofthe morpheme followed by the more abstract concepts of the lexeme and lexical andinflectional features.
Throughout this section, we use the term feature in its linguisticsense, as opposed to its machine learning sense that we use in Section 4.
Discussions ofthe challenges of form-based (morpheme-based) versus functional features on the onehand, and morpho-syntactic interactions on the other hand, follow.
Finally, we presentthe annotated corpus we use, and the various POS tag sets, that are extracted from thiscorpus (in varying degrees of abstraction and lexicalization), and which we use in therest of the article.2.1 MorphemesWords can be described in terms of their morphemes (atomic units bearing mean-ing); in Arabic, in addition to concatenative prefixes and suffixes, there are templatic(non-contiguous) morphemes called root and pattern.
The root is typically a tripletof consonants (a.k.a.
radicals).
The pattern is a template made of vowels, sometimesadditional consonants, and place-holders for the root radicals.
The root conveys somebase meaning, which patterns may modify in various ways.
A combination of a rootand a pattern is called a stem.
More on root and pattern can be found in Section 2.2.Arabic also includes a set of clitics that are tokenized in all Arabic treebanks, with theexception of the Arabic definite article,  Al+ (?the?
), which is kept attached to the stem.We consider the definite article a prefix, and its presence affects the value of the DETfeature in models containing it (see Section 4.3).
An example of morphological analysisto the level of morphemes is the word  !yu+kAtib+uwn (?they correspond?
); ithas one prefix and one suffix (which at a deeper level may be viewed together as onecircumfix), in addition to a stem composed of the root " k-t-b (?writing related?)
andthe pattern 1A2i3.32.2 Lexeme, Lexical Features, and Inflectional FeaturesArabic words can also be described in terms of lexemes and inflectional features.
Wedefine the lexeme as the set of word forms that only vary inflectionally among eachother.
A lemma is one of these word forms, used for representing the lexeme word set.For example, Arabic verb lemmas are third-person masculine singular perfective.
Weexplore using both a diacritized LEMMA feature, and an undiacritized lemma (hereafterLMM).
Just as the lemma abstracts over inflectional morphology, the root abstractsover both inflectional and derivational morphology and thus provides a deeper levelof lexical abstraction, indicating the ?core?
meaning of the word.
The pattern is agenerally complementary abstraction, sometimes indicating semantic notions such as3 The digits in the pattern correspond to the positions where root radicals are inserted.164Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Featurescausation and reflexiveness, among other things.
We use the pattern of the lemma, notof the word form.
We group the ROOT, PATTERN, LEMMA, and LMM in our discussionas lexical features (see Section 4.4).
Nominal lexemes can be further classified into twogroups: denoting rational (i.e., human) entities, or irrational (i.e., non-human) entities.The rationality (or RAT) feature interacts with syntactic agreement and other inflectionalfeatures (discussed next); as such, we group it with those features in this article.The inflectional features define the space of variations of the word forms associatedwith a lexeme.
Words4 vary along nine dimensions: GENDER, NUMBER, and PERSON (fornominals and verbs); ASPECT, VOICE, and MOOD (for verbs); and CASE, STATE (constructstate, idafa), and the attached definite article proclitic DET (for nominals).
Inflectionalfeatures abstract away from the specifics of morpheme forms.
Some inflectional featuresaffect more than one morpheme in the same word.
For example, changing the value ofthe ASPECT feature in the earlier example from imperfective to perfective yields theword form  # kAtab+uwA (?they corresponded?
), which differs in terms of prefix,suffix, and pattern.2.3 Form-Based vs. Functional FeaturesSome inflectional features, specifically gender and number, are expressed using dif-ferent morphemes in different words (even within the same POS).
There are foursound gender-number suffixes in Arabic:5 +?
(null morpheme) for masculine singular,+ + for feminine singular, + +wn for masculine plural, and + +At for feminineplural.
Form-based GENDER and NUMBER feature values are set only according to thesefour morphemes (and a few others, ignored for simplicity).
There are exceptions andalternative ways to express GENDER and NUMBER, however, and functional featurevalues take them into account: Depending on the lexeme, plurality can be expressedusing sound plural suffixes or using a pattern change together with singular suffixes.A sound plural example is the word pair $%/ $% Hafiyd+a/Hafiyd+At (?grand-daughter/granddaughters.)
On the other hand, the plural of the inflectionally andmorphemically feminine singular word &'( madras+a (?school?)
is the word )(madAris+?
(?schools?
), which is feminine and plural inflectionally, but has a masculinesingular suffix.
This irregular inflection, known as broken plural, is similar to the Englishmouse/mice, but is much more common in Arabic (over 50% of plurals in our trainingdata).
A similar inconsistency appears in feminine nominals that are not inflectedusing sound gender suffixes, for example, the feminine form of the masculine singu-lar adjective *?zraq+?
(?blue?)
is +  zarqA?+?
not &+* *?zraq+a.
To address thisinconsistency in the correspondence between inflectional features and morphemes, andinspired by Smr?
(2007), we distinguish between two types of inflectional features: form-based (a.k.a.
surface, or illusory) features and functional features.6Most available Arabic NLP tools and resources model morphology using form-based (?surface?)
inflectional features, and do not mark rationality; this includes thePenn Arabic Treebank (PATB) (Maamouri et al2004), the Buckwalter morphologicalanalyzer (Buckwalter 2004), and tools using them such as the Morphological Analysisand Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash,Rambow, and Roth 2012).
The Elixir-FM analyzer (Smr?
2007) readily provides the4 PATB-tokenized words; see Section 2.5.5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity.6 Note that the functional and form-based feature values for verbs always coincide.165Computational Linguistics Volume 39, Number 1functional inflectional number feature, but not full functional gender (only for adjectivesand verbs but not for nouns), nor rationality.
In this article, we use an in-house systemwhich provides functional gender, number, and rationality features (Alkuhlani andHabash 2012).
See Section 5.2 for more details.2.4 Morpho-Syntactic InteractionsInflectional features and rationality interact with syntax in two ways.
In agreementrelations, two words in a specific syntactic configuration have coordinated values forspecific sets of features.
MSA has standard (i.e., matching value) agreement for subject?verb pairs on PERSON, GENDER, and NUMBER, and for noun?adjective pairs on NUMBER,GENDER, CASE, and DET.
There are, however, three very common cases of exceptionalagreement: Verbs preceding subjects are always singular, adjectives of irrational pluralnouns are always feminine singular, and verbs whose subjects are irrational pluralare also always feminine singular.
See the example in Figure 1: the adjective, ,Al?kyAt (?smart?
), of the feminine plural (and rational) $% HafiydAt (?granddaugh-ters?)
is feminine plural; but the adjective, &( -AlHkwmy (?the-governmental?
),of the feminine plural (and irrational) )( madAris (?schools?)
is feminine singular.This exceptional agreement is orthogonal to the form-function inconsistency discussedearlier.
In other words, having a sound or broken plural has no bearing on whether thenoun is rational or not?and hence whether an adjectival modifier should agree with itby being feminine-singular or -plural.
Note also that all agreement rules, including theexceptional agreement rules, refer to functional number and gender, not to form-basednumber and gender.VRB./0 t?ml?work?MODPRT1+ fy?in?OBJNOM)23AlmdArs?the-schools?MODNOM&( -AlHkwmy?the-governmental?SBJNOM$% HfydAt?granddaughters?MODNOM, Al?kyAt?smart?IDFNOM4! AlkAtb?the-writer?Figure 1CATiB Annotation example.
&( -)231+, 4!$% ./0 t?ml HfydAt AlkAtbAl?kyAt fy AlmdArs AlHkwmy (?The writer?s smart granddaughters work for public schools?
).The words in the tree are presented in the Arabic reading direction (from right to left).166Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesMSA exhibits assignment relations in CASE and STATE marking.
Different typesof dependents have different CASE, for example, verbal subjects are always markedNOMINATIVE (for a discussion of case in MSA, see Habash et al[2007]).
STATE is amarker on nouns; when a noun heads an idafa construction, its STATE is (?construct?
).CASE and STATE are rarely explicitly manifested in undiacritized MSA.
The DET featureplays an important role in distinguishing between N-N construct (idafa), in which onlythe last noun bears the definite article,7 and N-A (noun-adjectival modifier), in whichboth elements generally exhibit agreement in definiteness (and agreement in otherfeatures, too).
Although only N-N may be followed by additional N elements in Idafarelation, both constructions may be followed by one or more adjectival modifiers.Lexical features do not constrain syntactic structure as inflectional features do.Instead, bilexical dependencies are used to model semantic relations that often are theonly way to disambiguate among different possible syntactic structures.2.5 Corpus, CATiB Format, and the CATIB6 POS Tag SetWe use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009).
Specifically,we use the portion converted from Part 3 of the PATB to the CATiB format, which en-riches the CATiB dependency trees with full PATB morphological information.
CATiB?sdependency representation is based on traditional Arabic grammar and emphasizessyntactic case relations.
It has a reduced POS tag set consisting of six tags only (hence-forth CATIB6).
The tags are: NOM (non-proper nominals including nouns, pronouns,adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS(passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX(punctuation).
CATiB uses a standard set of eight dependency relations: SBJ and OBJfor subject and (direct or indirect) object, respectively (whether they appear pre- or post-verbally); IDF for the idafa (possessive) relation; MOD for most other modifications; andother less common relations that we will not discuss here.
For other PATB-based POStag sets, see Sections 2.6 and 2.7.The CATiB Treebank uses the word segmentation of the PATB.
It splits off severalcategories of orthographic clitics, but not the definite article + Al+ (?the?).
In all ofthe experiments reported in this article, we use the gold segmentation.
Tokenization in-volves further decisions on the segmented token forms, such as spelling normalization,which we only briefly touch on here (in Section 4.1).
An example CATiB dependencytree is shown in Figure 1.
For the corpus statistics, see Table 1.
For more information onCATiB, see Habash and Roth (2009) and Habash, Faraj, and Roth (2009).2.6 Core POS Tag SetsLinguistically, words have associated POS tags, e.g., ?verb?
or ?noun,?
which furtherabstract over morphologically and syntactically similar lexemes.
Traditional Arabicgrammars often describe a very general three-way distinction into verbs, nominals,and particles.
In comparison, the tag set of the Buckwalter Morphological Analyzer(Buckwalter 2004) used in the PATB has a core POS set of 44 tags (CORE44) before mor-phological extension.8 Cross-linguistically, a core set containing around 12 tags is often7 We ignore the rare ?false idafa?
construction (Habash 2010, p. 102).8 The 44 tags in CORE44 are based on the tokenized version of Arabic words.
There are 34 untokenized coretags as used in MADA+TOKAN (Habash, Rambow, and Roth 2012).167Computational Linguistics Volume 39, Number 1assumed as a ?universal tag set?
(Rambow et al2006; Petrov, Das, and McDonald 2012).We have adapted the list from Rambow et al(2006) for Arabic, and call it here CORE12.
Itcontains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun(PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle(PRT), abbreviation (AB), and punctuation (PNX).
The CATIB6 tag set can be viewed asa further reduction, with the exception that CATIB6 contains a passive voice tag (a mor-phological feature); this tag constitutes only 0.5% of the tags in the training, however.2.7 Extended POS Tag SetsThe notion of ?POS tag set?
in natural language processing usually does not refer toa core set.
Instead, the Penn English Treebank (PTB) uses a set of 46 tags, includingnot only the core POS, but also the complete set of morphological features (this tag setis still fairly small since English is morphologically impoverished).
In PATB-tokenizedMSA, the corresponding type of tag set (core POS extended with a complete descriptionof morphology) would contain upwards of 2,000 tags, many of which are extremelyrare (in our training corpus of about 300,000 words, we encounter only 430 POS tagswith complete morphology).
Therefore, researchers have proposed tag sets for MSAwhose size is similar to that of the English PTB tag set, as this has proven to be auseful size computationally.
These tag sets are hybrids in the sense that they are neithersimply the core POS, nor the complete morphologically enriched tag set, but insteadthey selectively enrich the core POS tag set with only certain morphological features.A more detailed discussion of the various available Arabic tag sets can be found inHabash (2010).The following are the various tag sets we use in this article: (a) the core POS tagsets CORE44 and the newly introduced CORE12; (b) CATiB Treebank tag set (CATIB6)(Habash and Roth 2009) and its newly introduced extension of CATIBEX created usingsimple regular expressions on word form, indicating particular morphemes such as theprefix  Al+ or the suffix  +wn; this tag set is the best-performing tag set for Arabicon predicted values as reported in Section 4; (c) the PATB full tag set with completemorphological tag (BW) (Buckwalter 2004); and two extensions of the PATB reducedtag set (PENN POS, a.k.a.
RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), bothoutperforming it: (d) Kulick, Gabbard, and Marcus (2006)?s tag set (KULICK), size 43,one of whose most important extensions is the marking of the definite article clitic, and(e) Diab and Benajiba?s (in preparation) EXTENDED RTS tag set (ERTS), which marksgender, number, and definiteness, size 134.3.
Related WorkMuch work has been done on the use of morphological features for parsing of morpho-logically rich languages.
Collins et al(1999) report that an optimal tag set for parsingCzech consists of a basic POS tag plus a CASE feature (when applicable).
This tag set(size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set(size ?3000+).
They also report that the use of gender, number, and person featuresdid not yield any improvements.
The results for Czech are the opposite of our resultsfor Arabic, as we will see.
This may be due to CASE tagging having a lower errorrate in Czech (5.0%) (Hajic?
and Vidov?-Hladk?
1998) compared with Arabic (?14.0%,see Table 3).
Similarly, Cowan and Collins (2005) report that the use of a subset ofSpanish morphological features (number for adjectives, determiners, nouns, pronouns,and verbs; and mode for verbs) outperforms other combinations.
Our approach is168Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Featurescomparable to their work in terms of its systematic exploration of the space of mor-phological features.
We also find that the number feature helps for Arabic.
Lookingat Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima?an (2007) reportthat extending POS and phrase structure tags with definiteness information helpsunlexicalized PCFG parsing.As for work on Arabic (MSA), results have been reported on the PATB (Kulick,Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Depen-dency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habashand Roth 2009).
Recently, Green and Manning (2010) analyzed the PATB for annotationconsistency, and introduced an enhanced split-state constituency grammar, includinglabels for short idafa constructions and verbal or equational clauses.
Nivre (2008) reportsexperiments on Arabic parsing using his MaltParser (Nivre et al2007), trained on thePADT.
His results are not directly comparable to ours because of the different treebankrepresentations, even though all the experiments reported here were performed usingthe MaltParser.Our results agree with previous work on Arabic and Hebrew in that marking thedefinite article is helpful for parsing.
We go beyond previous work, however, andexplore additional lexical and inflectional features.
Previous work with MaltParser inRussian, Turkish, and Hindi showed gains with CASE but not with agreement features(Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009).Our work is the first to show gains using agreement in MaltParser and in Arabicdependency parsing, and the first to use functional features for this task.
Furthermore,we demonstrate that our results carry over successfully to another parser, the Easy-FirstParser (Goldberg and Elhadad 2010) (Section 6).Hohensee and Bender (2012) have conducted a study on dependency parsing for21 languages using features that encode whether the values for certain attributes areequal or not for a node and its governor.
These features are potentially powerful,because they generalize to the very notion of agreement, away from the specific valuesof the attributes on which agreement occurs.9 We expect this kind of feature to yieldlower gains for Arabic, unless: one uses functional feature values (such as those used here for the first time inArabic NLP), one uses yet another representation level to account for the otherwise non-identityagreement patterns of irrational plurals, one handles the loss of overt number agreement in constructions such as VS(where the verb precedes its subject), and one adequately represents the otherwise ?inverse?
number agreement (a phe-nomenon common to other Semitic languages, such as Hebrew, too).4.
Basic Parsing ExperimentsWe examined a large space of settings.
In all our experiments, we contrasted the resultsobtained using machine-predicted input with the results obtained using gold input (the9 We do not relate to specific results in their study because it has been brought to our attention thatHohensee and Bender (2012) are in the process of rechecking their code for errors, and rerunning theirexperiments (personal communication).169Computational Linguistics Volume 39, Number 1upper bound for using these features).
We started by looking at individual features(including POS tag sets) and their prediction accuracy.
We then explored various featurecombinations in a hill-climbing fashion.
We examined these issues in the followingorder:1. the contribution of POS tag sets to the parsing quality, as a function of the amountof information encoded in the tag set, using (a) gold input, and (b) machine-predicted POS tags;2. the contribution of numerous inflectional features in a controlled fashion, using (c)gold input and (d) machine-predicted input; (e) the prediction accuracy of eachinflectional feature;3. the contribution of the lexical features in a similar fashion, again using (f) gold inputand (g) predicted input; (h) the prediction accuracy of each lexical feature;4.
(i) certain feature combinations and (j) the embedding of the best combination in thePOS tag set; and5.
(k) further feature engineering of select useful features.In Section 5 we explore using functional (instead of form-based) feature values.
InSection 6 we repeat key experiments with another parser, illustrating the robustnessof our findings across these frameworks.
In Section 7 we explore alternative trainingmethods, and their impact on key models.All results are reported mainly in terms of labeled attachment accuracy score (theparent word and the type of dependency relation to it, abbreviated as LAS), which is alsoused for greedy (hill-climbing) decisions for feature combination.
Unlabeled attachmentaccuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS)are also given.
For statistical significance, we use McNemar?s test on non-gold LAS, asimplemented by Nilsson and Nivre (2008).
We denote p < 0.05 and p < 0.01 with + and++, respectively.4.1 Data Sets and ParserFor all the experiments reported in this article, we used the training portion of PATBPart 3 v3.1 (Maamouri et al2004), converted to the CATiB Treebank format, as men-tioned in Section 2.5.
We used the same training / devtest split as in Zitouni, Sorensen,and Sarikaya (2006); and we further split the devtest into two equal parts: a devel-opment (dev) set and a blind test set.
For all experiments, unless specified otherwise,we used the dev set.10 We kept the test unseen (?blind?)
during training and modeldevelopment.
Statistics about this split (after conversion to the CATiB dependencyformat) are given in Table 1.For all experiments reported in this section we used the syntactic dependencyparser MaltParser v1.3 (Nivre 2003, 2008; K?bler, McDonald, and Nivre 2009), atransition-based parser with an input buffer and a stack, which uses SVM classifiers10 We use the term ?dev set?
to denote a non-blind test set, used for model development (feature selectionand feature engineering).
We do not perform further weight optimization (which, if done, is done on aseparate ?tuning set?
).170Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 1Penn Arabic Treebank part 3 v3.1 data split.split # tokens # sentences sentence length(avg.
# tokens)training 341,094 11,476 29.7dev 31,208 1,043 29.9unseen test 29,944 1,007 29.7TOTAL 402,246 13,526 29.7to predict the next state in the parse derivation.
All experiments were done using theNivre ?eager?
algorithm.11There are five default attributes in the MaltParser terminology for each token in thetext: word ID (ordinal position in the sentence), word-form, POS tag, head (parent wordID), and deprel (the dependency relation between the current word and its parent).There are default MaltParser features (in the machine learning sense),12 which are thevalues of functions over these attributes, serving as input to the MaltParser internalclassifiers.
The most commonly used feature functions are the top of the input buffer(next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); followingitems on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).
Hence MaltParserfeatures are defined as POS tag at stk[0], word-form at buf[0], and so on.
K?bler,McDonald, and Nivre (2009) describe a ?typical?
MaltParser model configuration ofattributes and features.13 Starting with it, in a series of initial controlled experiments,we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] forPOS tags.
For features of new MaltParser-attributes (discussed later), we used buf[0] +stk[0].
We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]),ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively,dependents of the specified argument).
This new MaltParser configuration resulted ingains of 0.3?1.1% in labeled attachment accuracy (depending on the POS tag set) overthe default MaltParser configuration.
We also experimented with using normalizedword-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as iscommon in parsing and statistical machine translation literature, but it resulted in asmall decrease in performance, so we settled on using non-normalized word-forms.
Allexperiments reported here were conducted using this new configuration.
To recap, it hasthe following MaltParser attributes (machine learning features): 4 word-form attributes,7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre?eager?
algorithm), totaling 16 attributes and two more for every new feature describedin Section 4.3 and on (e.g., CASE).11 Nivre (2008) reports that non-projective and pseudo-projective algorithms outperform the ?eager?projective algorithm in MaltParser, but our training data did not contain any non-projectivedependencies.
The Nivre ?standard?
algorithm is also reported there to do better on Arabic, but in apreliminary experimentation, it did slightly worse than the ?eager?
one, perhaps due to the highpercentage of right branching (left headed structures) in our Arabic training set?an observation alreadynoted in Nivre (2008).12 The terms feature and attribute are overloaded in the literature.
We use them in the linguistic sense, unlessspecifically noted otherwise, e.g., MaltParser feature(s).13 It is slightly different from the default configuration.171Computational Linguistics Volume 39, Number 1Table 2Parsing performance with each POS tag set, on gold and predicted input.
LAS = labeledattachment accuracy (dependency + relation).
UAS = unlabeled attachment accuracy(dependency only).
LS = relation label prediction accuracy.
LAS diff = difference between labeledattachment accuracy on gold and predicted input.
POS acc = POS tag prediction accuracy.tag set gold predicted gold-pred.
POS tag setLAS UAS LS LAS UAS LS LAS diff.
acc.
sizeCATIB6 81.0 83.7 92.6 78.3 82.0 90.6 ?2.7 97.7 6CATIBEX 82.5 85.0 93.4 79.7 83.3 91.4 ?2.8 97.7 44CORE12 82.9 85.4 93.5 78.7 82.5 90.6 ?4.2 96.3 12CORE44 82.7 85.2 93.3 78.4 82.2 90.4 ?4.3 96.1 40ERTS 83.0 85.2 93.8 78.9 82.6 91.0 ?4.0 95.5 134KULICK 83.6 86.0 94.0 79.4 83.2 91.1 ?4.2 95.7 32BW 84.0 85.8 94.8 72.6 77.9 86.5 ?11.4 81.8 4304.2 The Effect of POS Tag Richness on Parsing QualityIn this section, we compare the effect on parsing quality of a number of POS tag setsvarying in their richness, in both gold and predicted settings.Gold POS tag values.
We turn first to the contribution of POS information to parsingquality, as a function of the amount of information encoded in the POS tag set (i.e., therelevance of a tag set).
A first rough estimation for the amount of information is the actualtag set size, as it appears in the training data.
For this purpose we compared the POStag sets discussed in sections 2.6 and 2.7.
In optimal conditions (using gold POS tags),the richest tag set (BW) is indeed the best performer (84.0%), and the poorest (CATIB6) isthe worst (81.0%).
Mid-size tag sets are in the high (82%), with the notable exception ofKULICK, which does better than ERTS, in spite of having one fourth the tag set size; more-over, it is the best performer in unlabeled attachment accuracy (86.0%), in spite of beingless than tenth the size of BW.
Our extended mid-size tag set, CATIBEX, was a mid-levelperformer as expected.
Columns 2?4 in Table 2 show results with gold input, and therightmost column shows the number of tag types actually occurring in the training data.Predicted POS tag values.
So far we discussed optimal (gold) conditions.
But in prac-tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags asinput, as opposed to gold (human-annotated) tags.14 The more informative the tag set,the less accurate the tag prediction might be, so the effect on overall parsing qualityis unclear.
Put differently, we are interested in the tradeoff between relevance and accu-racy.
Therefore, we repeated the experiments with POS tags predicted by the MADAtoolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012)15 (see Table 2,14 Some parsers predict POS tags internally, instead of receiving them as input, but this is not the case in thisarticle.15 We use MADA v3.1 in all of our experiments.
We note that MADA v3.1 was tuned on the samedevelopment set that we use for making our parsing model choices; ideally, we would have chosen adifferent development set for our work on parsing, but we thought it would be best to use MADA as ablack box component (for past and future comparability), and did not have sufficient data to carve outfrom a second development set (while retaining a test set).
We do not take this as a major concern for ourresults.
In fact, although MADA was tuned to maximize its core POS accuracy (the untokenized versionof CORE44), CORE44 did not yield best parsing quality on MADA-predicted input (see Table 2).172Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Featurescolumns 5?7).
It turned out that BW, the best gold performer but with lowest POS pre-diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performerwith predicted tags.
The simplest tag set, CATIB6, and its extension, CATIBEX, benefitedfrom the highest POS prediction accuracy (97.7%), and their performance suffered theleast.
CATIBEX was the best performer with predicted POS tags.
Performance drop andPOS prediction accuracy are given in columns 8 and 9.These results suggest that POS tag set accuracy is as important to parsing quality,if not more important, than its relevance.
In other words, when designing a parsingmodel, one might want to consider that in the tradeoff, mediocre accuracy may be worsethan mediocre relevance.
Later we see a similar trend for other features as well (e.g.,CASE in Section 4.3).
In Section 7 we also present a training method that largely mitigates(but doesn?t resolve) this issue of mediocre accuracy of relevant features.4.3 Inflectional Features and Their Contribution to Parsing QualityExperimenting with inflectional features is especially important in Arabic parsing,as it is morphologically rich.
In order to explore the contribution of inflectional andlexical information in a controlled manner, we focused on the best performing core(?morphology-free?)
POS tag set, CORE12, as baseline; using three different set-ups,we added nine inflectional features (with either gold values, or with values predictedby MADA): DET (presence of determiner), PERSON, ASPECT, VOICE, MOOD, GENDER,NUMBER, STATE, and CASE.
For a brief reminder and examples for each feature, see therightmost column in Table 3, or for more details refer back to Section 2.In set-up All, we augmented the baseline model with all nine features (as nineadditional MaltParser attributes); in set-up Sep, we augmented the baseline model witheach of these features, one at a time, separately; and in set-up Greedy, we combinedthem in a greedy heuristic (since the entire feature space is too vast to exhaust): startingwith the most gainful feature from Sep, adding the next most gainful feature, keepingit if it helped, or discarding it otherwise, and repeating this heuristics through the leastgainful feature.
See Table 4.Gold feature values.
We applied the three setups (All, Sep, and Greedy) with gold POStags and gold morphological tags, to examine the contribution of the morphologicalfeatures in optimal conditions.
The top left section of Table 4 shows that applying allinflectional features together yields gains over the baseline.
Examining the contributionof each feature separately (second top left Sep section), we see that CASE, followed bySTATE and DET, were the top contributors.
Performance of CASE is the notable differencefrom the predicted conditions (see following discussion).
No single feature outper-formed the All set-up in gold.
Surprisingly, only CASE and STATE helped in the Greedyset-up (85.4%, our highest result in gold), although one might expect feature DET to havehelped, too (since it is highly relevant: It participates in agreement, and interacts withthe idafa construction).
This shows that there is redundancy in the information providedby DET on the one hand and CASE and STATE on the other, presumably because bothsets of feature help identify the same construction, idafa.Predicted feature values.
We re-applied the three set-ups with predicted feature values(right-hand side half of Table 4).
Set-up All hurts performance on the machine-predictedinput.
This can be explained if one examines the prediction accuracy of each feature (tophalf, third section of Table 3).
Features which are not predicted with very high accuracy,such as CASE (86.3%), can dominate the negative contribution, even though they are173Computational Linguistics Volume 39, Number 1Table 3Prediction accuracy, value set sizes, descriptions, and value examples of features used in thiswork.
Accuracy was measured over the development set.
* = The set includes a ?N/A?
value(s).feature acc.
set size comments and examplesnormalizedword-form99.3 29,737 collapse certain spelling variations into a single representation, e.g., 15A?ly(?automatic?)
and 15A?l?
(?to?)
are collapsed into 15 Alynon-normalizedword-form98.9 29,980 ?raw?
input (except for PATB segmentation), e.g., the uncollapsed formsaboveLEMMA(diacritized)96.7 16,837 abstraction over inflected forms, e.g., the lemma of 4!( makAtib (?offices?
)is 4-( maktab (?office?
)LMM 98.3 15,305 undiacritized lemma (lemma with vowels and other diacritics removed),e.g., 4-( mktb for the example above.ROOT 98.4 9,646 further abstraction over inflection and patterns; typically a consonanttriplet, a.k.a.
radicals, e.g.,  " k t b (?writing-related?
)PATTERN 97.0 338 sequence of vowels and consonants with placeholders for the root radicals,e.g., ma12a3 (?location-related?
); typically a derivational modification to thebasic meaning of the root, such as a location or instrument, but inflectionalvariations such as aspect, voice, number and gender also exist; we use thepattern of the lemma, not the inflected form, which may differ in cases suchas broken pluralsDET 99.6 3* presence of the determiner morpheme  AlPERSON 99.1 4* first, second, or third person (or N/A)ASPECT 99.1 5* perfective, imperfective and imperative for verbs (or N/A)VOICE 98.9 4* active or passive voice for verbs (or N/A)MOOD 98.6 5* indicative, subjunctive, jussive for verbs (or N/A)GENDER 99.3 3* (form-based) masculine or feminine (or N/A)NUMBER 99.5 4* (form-based) singular, dual, or plural (or N/A)STATE 95.6 4* construct (head of idafa), definite, or indefinite (or N/A)CASE 86.3 5* nominative, accusative or genitive (or N/A)NUMDGT 99.5 7* a NUMBER feature with digit token representation; see Section A.3NUMDGTBIN 99.5 12* a NUMBER feature with number ?binning?
according to syntactic agreementpatterns; see Section A.3FNNUM 99.2 6* a functional NUMBER feature, using ElixirFM; see Section 5.1FNNUMDGT 99.2 7* a functional NUMBER feature with digit token representation, usingElixirFM; see Sections 5.1 and A.3FNNUMDGTBIN 99.2 12* a functional NUMBER feature with number ?binning?
according to syntacticagreement patterns, using ElixirFM; see Sections 5.1 and A.3FN*GENDER 98.6 6* a functional GENDER feature, using our in-house resource; see Section 5.2FN*NUM 99.0 7* a functional NUMBER feature, using our in-house resource; see Section 5.2FN*NUMDGTBIN 99.0 13* a functional NUMBER feature with number ?binning?
according to syntacticagreement patterns, using our in-house resource; see Sections 5.2 and A.3RAT 95.6 5* rationality (humanness) feature; rational, irrational, ambiguous, unknownor N/A; using our in-house resource; see Section 5.2PNG ?
?
abbrev.
for PERSON, NUMBER, and GENDER (a.k.a.
?-features); similarly forPGFN*NGR ?
?
abbrev.
for functional NUMBER, GENDER, and RAT; similarly for FN*NGtop contributors, that is, highly relevant, in optimal (gold) conditions (see previousparagraph).
The determiner feature (DET), followed by the STATE feature, were topindividual contributors in set-up Sep.
Adding the features that participate in agreement,namely, DET and the PNG features (PERSON, NUMBER, GENDER), in the Greedy set-up,yielded a 1.4% gain over the CORE12 baseline.
These results suggest that for a successfulfeature combination, one should take into account not only the relevance of the features,but also their accuracy.174Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 4CORE12 POS tag set with morphological inflectional features.
Left half: Using gold POS tag andfeature values.
In it: Top part (All ): Adding all nine inflectional features to CORE12.
Second part(Sep): Adding each feature separately to CORE12.
Third part (Greedy): Greedily adding nextbest feature from Sep, and keeping it if improving score.
Right half: Same as left half, but withpredicted POS tag and feature values.
Statistical significance tested only on predicted (non-gold)input, against the CORE12 baseline.gold POS and feature values predicted POS and feature valuesSet-up CORE12+.
.
.
LAS UAS LS CORE12+.
.
.
LAS UAS LSAll (baseline repeated) 82.9 85.4 93.5 (baseline repeated) 78.7 82.5 90.6+ all 9 infl.
features 85.2 86.6 95.3 + all 9 infl.
features 77.9 82.1 90.0Sep+CASE 84.6 86.3 95.0 +DET 79.8++ 83.2 91.5+STATE 84.2 86.4 94.4 +STATE 79.4++ 82.9 91.2+DET 84.0 86.2 94.2 +GENDER 78.8 82.4 90.8+NUMBER 83.1 85.5 93.6 +PERSON 78.7 82.5 90.7+PERSON 83.1 85.4 93.7 +NUMBER 78.7 82.4 90.6+VOICE 83.1 85.4 93.6 +VOICE 78.6 82.4 90.6+MOOD 83.1 85.5 93.5 +ASPECT 78.6 82.4 90.5+ASPECT 83.0 85.4 93.5 +MOOD 78.5 82.4 90.5+GENDER 83.0 85.2 93.6 +CASE 75.8 80.2 88.5Greedy+CASE+STATE 85.4 86.9 95.5 +DET+STATE 79.4++ 82.8 91.2+CASE+STATE+DET 85.2 86.7 95.4 +DET+GENDER 79.9++ 83.2 91.7+CASE+STATE+NUMBER 85.4 86.9 95.5 +DET+GENDER+PERSON 79.9++ 83.2 91.7+CASE+STATE+PERSON 85.3 86.8 95.4 +DET+PNG 80.1++ 83.3 91.8+CASE+STATE+VOICE 85.3 86.8 95.4 +DET+PNG+VOICE 80.0++ 83.2 91.7+CASE+STATE+MOOD 85.2 86.7 95.4 +DET+PNG+ASPECT 80.0++ 83.2 91.8+CASE+STATE+ASPECT 85.2 86.8 95.4 +DET+PNG+MOOD 80.0++ 83.2 91.8+CASE+STATE+GENDER 85.3 86.8 95.4 ?4.4 Lexical Features and Their Contribution to Parsing QualityNext, we experimented with adding the lexical features, which involve semantic ab-straction to some degree: the diacritized LEMMA, the undiacritized lemma (LMM), theROOT, and the PATTERN (which is the pattern of the LEMMA).
A notable advantageof lexical abstraction is that it reduces data sparseness, and explicitly ties togethersemantically related words.
We experimented with the same set-ups as above: All, Sep,and Greedy.Gold feature values.
The left-hand side half of Table 5 shows that adding all fourfeatures yielded gains similar to adding a lemma feature separately.
With gold tags,however, no proper subset of the lexical features beats the set of all lexical features.Predicted feature values.
The right-hand side of Table 5 shows that adding all fourfeatures yielded a minor gain in set-up All.
LMM was the best single contributor, closelyfollowed by ROOT in Sep. CORE12+LMM+ROOT (with or without LEMMA) was the bestgreedy combination in set-up Greedy, and also provides the best performance of allexperiments with lexical features only.
Due to the high redundancy of LEMMA and LMM(only 0.01% absolute gain when adding LEMMA in the Greedy set-up, which appearslarger only due to rounding in the table), we do not consider LEMMA in feature combina-tions from this point on.
Note, however, that LEMMA?and all the lexical features?arepredicted with high accuracy (top half, second section of Table 3).175Computational Linguistics Volume 39, Number 1Table 5Models with lexical morpho-semantic features.
Top: Adding all lexical features together on topof the CORE12 baseline.
Center: Adding each feature separately.
Bottom: Greedily adding bestfeatures from previous part, on predicted input.
Statistical significance tested only on predicted(non-gold) input, against the CORE12 baseline.set-up CORE12+.
.
.
gold predictedLAS UAS LS LAS UAS LSAll CORE12 (baseline repeated) 82.9 85.4 93.5 78.7 82.5 90.6+ all lexical features 83.4 85.5 93.9 78.9 82.5 90.8Sep+LMM (lemma without diacritics) 83.3 85.5 93.8 79.0+ 82.5 90.8+ROOT 83.2 85.5 93.7 78.9+ 82.6 90.7+LEMMA 83.4 85.5 93.8 78.8 82.4 90.7+PATTERN 83.1 85.5 93.6 78.6 82.4 90.6Greedy +LMM+ROOT 83.3 85.5 93.9 79.0++ 82.6 90.9+LMM+ROOT+LEMMA 83.3 85.4 93.8 79.1++ 82.6 90.9+LMM+ROOT+PATTERN 83.4 85.5 93.9 78.9 82.6 90.84.5 Inflectional and Lexical Feature Combination and Its Contribution toParsing QualityWe now combine morphological and lexical features.
Following the same greedyheuristic as in the previous sections, we augmented the best inflection-based modelCORE12+DET+PNG with lexical features, and found that the undiacritized lemma (LMM)improved performance on predicted input (80.2%) (see Table 6).
Adding more lexicalfeatures does not help, however, suggesting that some of the information in the lexicalfeatures is redundant with the information in the morphological features.
See the Ap-pendix, Section A.1, for our attempt to extend the tag set by embedding the best featurecombination in it.4.6 Additional Feature EngineeringSo far we have experimented with morphological feature values as extracted fromthe PATB (gold) or predicted by MADA; we also used the same MaltParser featureconfiguration for all added features (i.e., stk[0] + buf[0]).
It is likely, however, that froma machine-learning perspective, representing similar categories with the same tag, orTable 6Models with inflectional and lexical morphological features together (predicted value-guidedheuristic).
Statistical significance tested only on predicted input, against the CORE12 baseline.tag set gold predictedLAS UAS LS LAS UAS LSCORE12+DET+PNG (rep.) 84.2 86.2 94.5 80.1++ 83.3 91.8CORE12+DET+PNG+LMM 84.4 86.4 94.6 80.2++ 83.3 91.9CORE12+DET+PNG+LMM +ROOT 84.3 86.3 94.6 80.1++ 83.3 91.8CORE12+DET+PNG+LMM +PATTERN 84.4 86.3 94.6 80.0++ 83.2 91.8176Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 7Models with re-engineered DET and PERSON inflectional features.
Statistical significance testedonly on predicted input, against the CORE12 baseline.model (POS tag set and infl.
feature) gold predictedLAS UAS LS LAS UAS LSCORE12+DET (repeated) 84.0 86.2 94.2 79.8++ 83.2 91.5CORE12+DET2 84.1 86.4 94.3 80.1++ 83.5 91.7CORE12+DET+PNG+LMM (repeated) 84.4 86.4 94.6 80.2++ 83.3 91.9CORE12+DET2+PNG+LMM 84.6 86.5 94.7 80.2++ 83.4 91.9taking into account further-away tokens in the sentence, may be useful for learning.Therefore, we next experimented with modifying some inflectional features that provedmost useful in predicted input.As DET may help disambiguate N-N / N-A constructions (and N-N-N, N-A-A, .
.
.
,see Section 2), we attempted modeling the DET values of previous and next elements(as MaltParser?s stk[1] + buf[1], in addition to the modeled stk[0] + buf[0]).
This vari-ant, denoted DET2, indeed helps: When added to the CORE12 baseline model, DET2improves non-gold parsing quality by more than 0.3%, compared to DET, as shown inTable 7.
This variant yields a small improvement also when used in combination withthe PNG and LMM features, as shown in the second part of Table 7?but only in gold.These results suggest an intricate interaction between the extended relevance of thedeterminer feature, and its redundancy with the PNG features (and note that all fea-tures involved are predicted with high accuracy).
A possible explanation might be thatform-based feature representation is inherently inadequate here, and therefore its highaccuracy may not be very indicative.
We explore non-form-based (functional) featurerepresentation in Section 5.
For more on our feature engineering, see the Appendix,Section A.2.5.
Parsing Experiments with Functional FeaturesSection 4 explored the contribution of various POS tag sets, (form-based) morphologicalfeatures, and promising combinations thereof, to Arabic dependency parsing quality?in straightforward feature engineering design and combination heuristics.
This sectionexplores more sophisticated feature engineering: using functional NUMBER and GENDERfeature values, instead of form-based values; using the non-form-based rationality (RAT)feature; and combinations thereof.
For additional experiments regarding alternativerepresentation for digit tokens, and the ?binning?
Arabic number constructions accord-ing to their complex syntactic patterns, see the Appendix, Section A.3.5.1 Functional Feature Representation for Broken Plurals (using ElixirFM)The NUMBER feature we have thus far extracted from PATB with MADA only reflectsform-based (as opposed to functional) values, namely, broken plurals are marked assingular.
This might have a negative effect for learning generalizations over the complexagreement patterns in MSA, beyond memorization of word pairs seen together in177Computational Linguistics Volume 39, Number 1training.
To address this issue, one can use the Arabic morphological tool ElixirFM16(Smr?
2007).
For each given word form, it outputs a list of possible analyses, eachcontaining a lemma and a functional NUMBER (and other features).
We replaced thesurface NUMBER value for all nominals marked as singular in our data with ElixirFM?sfunctional value, using the MADA-predicted lemma to disambiguate multiple ElixirFManalyses.
These experiments are denoted with FNNUM.
In training, of the lemma typessent to ElixirFM for analysis, about 20% received no analysis (OOV).
A manual observa-tion of a small sample revealed that at least half of those were proper names (and hencetheir NUMBER value would have stayed singular).
Almost 9% of the ElixirFM-analyzedtypes (over 7% of the tokens) changed their NUMBER value.
In the dev set, the OOVrate was less than 9%, and almost 11% of the ElixirFM-analyzed types changed theirNUMBER value.
This amounts to 4.4% of all tokens.We used ElixirFM to determine the values for FNNUM, the functional numberfeature.
We used this feature in our best model so far, CORE12+DET+PNG+LMM, insteadof the form-based NUMBER feature.17 The ElixirFM-based models yielded small gainsof up to 0.1% over this best model on predicted input.
We then modified the ElixirFM-based best model to use the enhanced DET2 feature.
This variation yielded a similarlysmall gain, altogether less than 0.2% from its ElixirFM-free counterparts.5.2 Functional Gender and Number Features, and the Rationality FeatureThe ElixirFM lexical resource used previously provided functional NUMBER featurevalues but no functional GENDER values, nor RAT (rationality, or humanness) values.To address this issue, we use a version of the PATB3 training and dev sets manuallyannotated with functional gender, number, and rationality (Alkuhlani and Habash2011).18 This is the first resource providing all three features (ElixirFm only providesfunctional number, and to some extent functional gender).
We conducted experimentswith gold features to assess the potential of these features, and with predicted fea-tures, obtained from training a simple maximum likelihood estimation classifier on thisresource (Alkuhlani and Habash 2012).19 The first part of Table 8 shows that the RAT(rationality) feature is very relevant (in gold), but suffers from low accuracy (no gainsin machine-predicted input).
The next two parts show the advantages of functionalgender and number (denoted with a FN* prefix) over their surface-based counterparts.The fourth part of the table shows the combination of these functional features withthe other features that participated in the best combination so far (LMM, the extendedDET2, and PERSON); without RAT, this combination is at least as useful as its form-basedcounterpart, in both gold and predicted input; adding RAT to this combination yields0.4% (absolute) gain in gold, offering further support to the relevance of the rationalityfeature, but a slight decrease in predicted input, presumably due to insufficient accuracyagain.
The last part of the table revalidates the gains achieved with the best controlledfeature combination, using CATIBEX?the best performing tag set with predicted in-put.
Note, however, that the 1% (absolute) advantage of CATIBEX (without additionalfeatures) over the morphology-free CORE12 on machine-predicted input (Table 2) has16 http://sourceforge.net/projects/elixir-fm.17 We also applied the manipulations described in Section A.3 to FNNUM, giving us the variantsFNNUMDGT and FNNUMDGTBIN, which we tested similarly.18 In this article, we use a newer version of the corpus by Alkuhlani and Habash (2011) than the one weused in Marton, Habash, and Rambow (2011).19 The paper by Alkuhlani and Habash (2012) presents additional, more sophisticated models that we donot use in this article.178Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 8Models with functional features: GENDER, NUMBER, rationality (RAT).
FN* = functionalfeature(s) based on Alkuhlani and Habash (2011); GN = GENDER+NUMBER; GNR = GENDER+NUMBER+RAT.
Statistical significance tested only for CORE12+.
.
.
models on predicted input,against the CORE12 baseline.model (POS tag set and features) gold predictedLAS UAS LS LAS UAS LSCORE12 (repeated) 82.9 85.4 93.5 78.7 82.5 90.6+FN*RATIONAL 83.7 85.8 94.0 78.7 82.5 90.7+GENDER (repeated) 83.0 85.2 93.6 78.8 82.4 90.8+FN*GENDER 83.3 85.5 93.7 78.9+ 82.6 90.9+NUMBER (repeated) 83.1 85.5 93.6 78.7 82.4 90.6+FN*NUMBER 83.3 85.6 93.7 78.9+ 82.5 90.7+DET2+LMM+PNG (repeated) 84.6 86.5 94.7 80.2++ 83.4 91.9+DET2+LMM+PERSON +FN*NGR 85.0 86.7 94.9 80.3++ 83.7 91.6+DET2+LMM+PERSON +FN*NG 84.6 86.5 94.7 80.4++ 83.5 91.9CATIBEX+DET2+LMM+PERSON+FN*NGR 84.1 85.9 94.4 80.7 84.0 91.9CATIBEX+DET2+LMM+PERSON+FN*NG 83.5 85.4 94.1 80.7 83.7 92.2shrunk with these functional feature combinations to 0.3%.
We take it as further supportto the relevance of our functional morphology features, and their partial redundancywith the form-based morphological information embedded in the CATIBEX POS tags.6.
Evaluation of Results with Easy-First ParserIn this section, we validate the contribution of key tag sets and morphological features?and combinations thereof?using a different parser: the Easy-First Parser (Goldberg andElhadad 2010).
As in Section 4, all models are evaluated on both gold and non-gold(machine-predicted) feature values.The Easy-First Parser is a shift-reduce parser (as is MaltParser).
Unlike MaltParser,however, it does not attempt to attach arcs ?eagerly?
as early as possible (as in previoussections), or at the latest possible stage (an option we abandoned early on in preliminaryexperiments).
Instead, the Easy-First Parser keeps a stack of partially built treelets, andattaches them to one another in order of confidence (from high confidence, ?easy?attachment, to low, as estimated by the classifier).
Labeling the relation arcs is donein a second pass, with a separate training step, after all attachments have been decided(the code for which was added after the publication of Goldberg and Elhadad (2010),which only included an unlabeled attachment version).Setting machine-learning features for Easy-First Parser is not as simple and elegantas for MaltParser, but it gives the feature designer greater flexibility.
For example, thePOS tag can be dynamically split (or not) according to the token?s word-form and/orthe already-built attachment treelets, whereas in MaltParser, one can meld severalfeatures into a single complex feature only if applied unconditionally to all tokens.The Easy-First Parser?s first version comes with the code for the features used in itsfirst publication.
These include POS tag splitting and feature melding for prepositionalattachment chains (e.g., parent-preposition-child).
For greater control of the contribu-tion of the various POS tag and morphological features in the experiments, and for179Computational Linguistics Volume 39, Number 1a better ?apples-to-apples?
comparison with MaltParser (as used here), we disabledthese features, and instead used features (and selected feature melding) that were asequivalent to MaltParser as possible.Table 9 shows results with Easy-First Parser.
Results with Easy-First Parser areconsistently higher than the corresponding results with MaltParser, with similar trendsfor the various features?
contribution: Functional GENDER and NUMBER features con-tribute more than their form-based counterparts, in both gold and predicted conditions;rationality (RAT) as a single feature on top of the POS tag set helps in gold (and withEasy-First Parser, also in predicted conditions)?but when used in combination withPERSON, LMM, functional GENDER, and NUMBER, it actually slightly lowers parsingscores in predicted conditions (but with Easy-First Parser, it helps in gold conditions);DET is the most useful single feature in predicted conditions (from those we tried here);and the best performing model in predicted conditions is the same as with MaltParser:CORE12+DET+LMM+PERSON+FN*NG.20As before, we see that the patterns of gain achieved with the ?morphology-free?CORE12 hold also for CATIBEX, the best performing tag set on predicted input.
Inter-estingly, with this parser, the greater 1.6% (absolute) advantage of CATIBEX (withoutadditional features) over the morphology-free CORE12 on machine-predicted input(compare with only 1% in MaltParser in Table 2) has shrunk completely with thesefunctional feature combinations.
This suggests that Easy-First Parser is more resilientto accuracy errors (presumably due to its design to make less ambiguous decisionsearlier), and hence can take better advantage of the relevant information encoded inour functional morphology features.7.
Combined Gold and Predicted Features for TrainingSo far, we have only evaluated models trained on gold POS tag set and morphologicalfeature values.
Some researchers, however, including Goldberg and Elhadad (2010),train on predicted feature values instead.
It makes sense that training on predictedfeatures yields better scores for evaluation on predicted features, since the trainingbetter resembles the test.
But we argue that it also makes sense that training on acombination of gold and predicted features (one copy of each) might do even better,because good predictions of feature values are reinforced (since they repeat the goldpatterns), whereas noisy predicted feature values are still represented in training (inpatterns that do not repeat the gold).21 To test our hypothesis, we start this section bycomparing three variations: Training on gold feature values (as has been the case so far) Training on predicted feature values (as in Goldberg and Elhadad 2010) Training on the novel combination of gold and predicted features (denoted belowas g+p)20 Recall that DET2 was only defined for MaltParser, and not for the Easy-First Parser.21 Although conceived independently, this hypothesis resembles self-training (McClosky, Charniak, andJohnson 2006), where the parser is re-trained on its own predicted parsing output, together with theoriginal labeled training data.
Note, however, that we re-train on gold and predicted feature values (e.g.,POS tag, GENDER, or NUMBER), but we always use gold training data for HEAD and DEPREL.
In both casesthe parsers seem to benefit from training data (features) that better resemble the test data, while retainingbias toward the gold and correctly predicted data.180Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 9Select models trained using the Easy-First Parser.
Statistical significance tested only forCORE12.
.
.
models on predicted input: significance of the Easy-First Parser CORE12 baselinemodel against its MaltParser counterpart; and significance of all other CORE12+.
.
.
modelsagainst the Easy-First Parser CORE12 baseline model.model (POS tag set and features) gold predictedLAS UAS LS LAS UAS LSCORE12 (MaltParser baseline, repeated) 82.9 85.4 93.5 78.7 82.5 90.6CORE12 (Easy-First Parser) 83.5 86.0 93.9 79.6++ 83.5 91.3CORE12+NUMBER 83.3 85.7 94.0 79.5 83.4 91.3CORE12+FN*NUMBER 83.5 85.9 94.0 79.8 83.6 91.4CORE12+GENDER 83.5 86.0 94.0 79.5 83.5 91.3CORE12+FN*GENDER 83.6 86.1 94.0 79.7 83.6 91.3CORE12+RAT 84.2 86.4 94.4 79.6 83.6 91.3CORE12+DET 84.3 86.7 94.5 80.6++ 84.1 92.2CORE12+LMM 83.6 85.8 94.1 79.7 83.5 91.5CORE12+DET+LMM+PNG 84.8 86.9 94.9 81.1++ 84.4 92.3CORE12+DET+LMM+PERSON+FN*NG 84.9 86.9 94.8 81.4++ 84.7 92.4CORE12+DET+LMM+PERSON+FN*NGR 85.1 87.1 94.9 81.2++ 84.7 92.1CATIBEX 83.1 85.6 94.0 81.2 84.6 92.5CATIBEX+DET+LMM+PERSON+FN*NG 83.5 85.8 94.2 81.4 84.6 92.7CATIBEX+DET+LMM+PERSON+FN*NGR 83.9 85.9 94.7 81.1 84.6 92.5The first two parts of Table 10 show that, as expected, training on gold featurevalues yields better scores when evaluated on gold, too (although later we see this isnot always the case).
More interestingly, when evaluated on predicted feature values,training on predicted feature values yields better parsing scores than when training ongold, and training on g+p yields best scores, in support of our hypothesis.
Therefore,in the rest of the table (and in the rest of the experiments), we apply the g+p trainingvariant to the best models so far, both in MaltParser and Easy-First Parser.
The next partin Table 10 shows that this trend is consistent also with the best feature combinations sofar.
Interestingly, the RAT feature contributes to improvement only in the g+p condition,presumably because of its low prediction accuracy.In Table 11, we repeated most of these experiments with other tag sets: CATIBEXand BW (best performers on predicted and gold input, respectively).
We can see inthis table that the same trends hold for these POS tag sets as well.
Interestingly, the?morphology-free?
CORE12 (in Table 10) outperforms CATIBEX here (Table 11), mak-ing CORE12+DET2+LMM+PERSON+FN*NGR our best MaltParser model on predictedfeature values.
Similarly, the Easy-First Parser model CORE12+DET+LMM+PERSON+FN*NG outperforms its CATIBEX counterpart (CATIBEX+DET+LMM+PERSON+FN*NG),resulting in our best model on the dev set in machine-predicted condition (82.7%).22The richest POS tag set, BW, which is also the worst predicted tag set and worstperformer on predicted input, had the most dramatic gains from using g+p: more than22 See Section 9 for download information.181Computational Linguistics Volume 39, Number 1Table 10Alternatives to training on gold-only feature values.
Top: Select MaltParser CORE12+.
.
.
modelsre-trained on predicted or gold + predicted feature values.
Bottom: Similar models to the tophalf, with the Easy-First Parser.
Statistical significance tested only for CORE12+.
.
.
models onpredicted input: significance of the MaltParser models from the MaltParser CORE12 baselinemodel, and significance of the Easy-First Parser models from the Easy-First Parser CORE12baseline.model (POS tag set and features) gold predictedLAS UAS LS LAS UAS LSMaltParser:CORE12 (gold train, repeated) 82.9 85.4 93.5 78.7 82.5 90.6CORE12 predicted train 82.4 85.0 93.2 79.8++ 83.2 91.4CORE12 g+p 82.7 85.2 93.5 80.0++ 83.4 91.6CORE12+DET+LMM+PNG (gold train, repeated) 84.4 86.4 94.6 80.2++ 83.3 91.9CORE12+DET+LMM+PNG predicted train 84.1 86.1 94.3 81.6++ 84.4 92.8CORE12+DET+LMM+PNG g+p 84.2 86.1 94.5 81.7++ 84.5 92.9CORE12+DET2+LMM+PERSON+FN*NGR 85.0 86.7 94.9 80.3++ 83.7 91.6(gold train, repeated)CORE12+DET2+LMM+PERSON+FN*NG 84.6 86.5 94.7 80.4++ 83.5 91.9(gold train, repeated)CORE12+DET2+LMM+PERSON+FN*NG g+p 84.4 86.3 94.6 81.8++ 84.6 93.0CORE12+DET2+LMM+PERSON+FN*NGR g+p 84.7 86.5 94.7 81.9++ 84.7 93.0Easy-First Parser:CORE12 (gold train, repeated) 83.5 86.0 93.9 79.6 83.5 91.3CORE12 g+p 83.6 86.1 94.1 80.8++ 84.4 92.3CORE12+DET+LMM+PNG g+p 84.8 86.9 94.7 82.5++ 85.5 93.3CORE12+DET+LMM+PERSON+FN*NG g+p 84.9 86.9 94.9 82.7++ 85.7 93.3CORE12+DET+LMM+PERSON+FN*NGR g+p 85.2 87.2 95.0 82.6++ 85.7 93.25% (absolute) for LAS on predicted input with MaltParser (and over 3% with Easy-First Parser).
Although much improved, BW models?
performance still lags behind theleading models.The results in Tables 10 and 11 suggest that our g+p training method is superior tothe alternatives (independently of parser choice) due to making the parser more resilientto lower accuracy in the input.
It also suggests that g+p training enables the parser tobetter exploit relevant data when represented in ?cleaner?
separate features, as opposedto when the POS tags are split into ambiguous form-based cases as in CATIBEX.
Futureexperimentation is needed in order to test this latter conjecture.8.
Result Validation and Discussion8.1 Validating Results on an Unseen Test SetOnce experiments on the development set were done, we ran the best performing form-based non-gold-based models from Section 4 on a previously unseen test set.
This set182Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 11Alternatives to training on gold-only feature values for CATIBEX and BW tag sets.
Top: SelectMaltParser models re-trained on predicted or gold + predicted feature values.
Bottom: Similarmodels to the top half, with the Easy-First Parser.
(Statistical significance was tested only forCORE12+.
.
.
models ?
none here).model (POS tag set and features) gold predictedLAS UAS LS LAS UAS LSMaltParser:CATIBEX (gold train, repeated) 82.5 85.0 93.4 79.7 83.3 91.4CATIBEX g+p 82.3 84.8 93.3 80.4 83.6 92.0CATIBEX+DET2+LMM+PERSON+FN*NG 83.5 85.4 94.1 80.7 83.7 92.2(gold train, repeated)CATIBEX+DET2+LMM+PERSON+FN*NGR 84.1 85.9 94.4 80.7 84.0 91.9(gold train)CATIBEX+DET2+LMM+PERSON+FN*NG g+p 83.2 85.3 93.9 81.3 84.2 92.6CATIBEX+DET2+LMM+PERSON+FN*NGR g+p 83.7 85.7 94.2 81.4 84.3 92.6BW (gold train, repeated) 84.0 85.8 94.8 72.6 77.9 86.5BW g+p 83.9 85.7 94.7 77.8 81.4 90.3BW+DET2+LMM+PERSON+FN*NG g+p 84.8 86.4 95.1 79.4 82.6 91.2BW+DET2+LMM+PERSON+FN*NGR g+p 85.1 86.6 95.2 79.5 82.7 91.2Easy-First Parser:CATIBEX (gold train, repeated) 83.1 85.6 94.0 81.2 84.6 92.5CATIBEX g+p 82.5 85.1 93.8 81.2 84.4 92.9CATIBEX+DET+LMM+PERSON+FN*NG 83.5 85.8 94.2 81.4 84.6 92.7(gold train, repeated)CATIBEX+DET+LMM+PERSON+FN*NGR 83.9 85.9 94.7 81.1 84.6 92.5(gold train, repeated)CATIBEX+DET+LMM+PNG g+p 83.4 85.7 94.3 82.1 85.0 93.3CATIBEX+DET+LMM+PERSON+FN*NG g+p 83.6 85.8 94.4 82.0 84.9 93.4CATIBEX+DET+LMM+PERSON+FN*NGR g+p 83.9 86.0 94.6 82.2 85.3 93.2BW (gold train, repeated) 84.9 86.6 95.6 77.5 82.2 90.1BW g+p 84.4 86.2 95.3 80.7 84.1 92.5BW+DET+LMM+PERSON+FN*NG g+p 84.8 86.5 95.5 81.1 84.2 92.8BW+DET+LMM+PERSON+FN*NGR g+p 85.1 86.7 95.6 81.2 84.4 92.9Table 12Results on PATB3-TEST for form-based models which performed best on PATB3-DEV ?predicted input.
Statistical significance tested on the PATB3-TEST set, only for MaltParserCORE12+.
.
.
models against the MaltParser CORE12 baseline model output.model (POS tag set and morph.
features) LAS UAS LSCORE12 77.3 81.0 90.1CORE12+DET+PNG 78.6 81.7 91.1CORE12+DET+LMM+PNG 79.1++ 82.1 91.4CATIBEX 78.5 81.8 91.0CATIBEX+DET+LMM+PNG 79.3 82.4 91.6183Computational Linguistics Volume 39, Number 1is the test split of part 3 of the PATB (hereafter PATB3-TEST; see Table 1 for details).Table 12 shows that the same trends held on this set too, with even greater relative gains,up to almost 2% absolute gains.We then also revalidated the contribution of the best performing models fromSections 5?7 on PATB3-TEST.
Here, too, the same trends held.
Results are shown inTable 13.8.2 Best Results on Length-Filtered InputFor better comparison with work of others, we adopt the suggestion made by Greenand Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.We report these filtered results in Table 14.
Filtered results are consistently higher (asexpected).
Results are about 0.9% absolute higher on the development set, and about0.6% higher on the test set.
The contribution of the RAT feature across sets is negligible(or small and unstable), resulting in less than 0.1% absolute loss on the dev set, butabout 0.15% gain on the test set.
For clarity and conciseness, we only show the bestmodel (with RAT) in Table 14.8.3 Error AnalysisWe perform two types of error analyses.
First, we analyze the attachment accuracyby attachment relation type on PATB3-DEV.
Our hypothesis is that the syntactic re-lations which are involved in agreement or assignment configurations will show animprovement when the relevant morphological features are used, but other syntacticTable 13Results on PATB3-TEST for models that performed best on PATB3-DEV ?
predicted input.
UsingMaltParser, unless indicated otherwise.
g+p = trained on combination of gold and predictedinput (instead of gold-only).
Statistical significance tested only for CORE12+.
.
.
models: ForMaltParser CORE12+.
.
.
models against the MaltParser CORE12 baseline model output, and forEasy-First Parser CORE12+.
.
.
models against the Easy-First Parser CORE12 baseline modeloutput.POS tag set LAS UAS LSCORE12 (repeated) 77.3 81.0 90.1CORE12+DET2+LMM+PG+FNNUMDGTBIN 79.3++ 82.3 91.4CORE12+DET2+LMM+PERSON +FN*NGR 78.9+ 82.3 91.0CORE12+DET2+LMM+PERSON +FN*NG 79.1++ 82.1 91.4CORE12+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 81.0++ 84.0 92.7CORE12+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 80.9++ 83.9 92.8CATIBEX 78.5 81.8 91.0CATIBEX+DET2+LMM+PG+FNNUMDGTBIN 79.4 82.5 91.6CATIBEX+DET2+LMM+PERSON +FN*NGR 79.3 82.6 91.3CATIBEX+DET2+LMM+PERSON +FN*NG 79.3 82.4 91.5CATIBEX+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 79.5 83.0 91.9CATIBEX+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 79.6 82.8 92.1BW 72.1 77.2 86.3BW+DET2+LMM+PERSON +FN*NGR g+p, Easy-First Parser 79.6 82.7 92.2BW+DET2+LMM+PERSON +FN*NG g+p, Easy-First Parser 79.7 82.9 92.3184Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 14Results for best performing model on PATB3-DEV and PATB3-TEST for sentences up to 70 tokenslong (predicted input).model evaluated on LAS UAS LSCORE12+DET+LMM+PERSON+FN*NGR g+p, PATB3-DEV 83.6 86.5 93.5Easy-First ParserCORE12+DET+LMM+PERSON+FN*NGR g+p, PATB3-TEST 81.7 84.6 92.8Easy-First Parserrelations will not.
Second, we analyze the grammaticality of the obtained parse treeswith respect to agreement and assignment phenomena.
Here, our hypothesis is thatwhen using morphological features, the grammaticality of the obtained parse trees willincrease.Attachment accuracy by relation type.
Our first hypothesis is illustrated in Figure 2.On the left, we see the parse provided by our baseline system (MaltParser using onlyCORE12), which has two errors: The node labeled 6?yAm (?days?)
should be the subject, not the object, of themain verb 7( mrt (?passed?).
Morphologically, 6?yAm is masculine plural,and 7( mrt (?passed?)
is feminine singular, obeying the agreement pattern un-der which a non-rational subject following the verb always triggers a femininesingular verbal form. The node labeled )823  Almhnds (?the engineer?)
should not be in an idafa (gen-itive construction) dependency with its governor .(7  Alzmyl (?the colleague?
),but in a modifier relation (a sort of apposition, in this case).
This must be the casebecause a noun that is the head of an idafa construction cannot have a definitedeterminer, as is the case here.Both errors could be corrected (to the correct form as in our best model, on the right-hand side of Figure 2) if functional morphological features were available to the parser,including the rationality feature, and if the parser could learn the agreement rule fornon-rational subjects, as well as the requirement that the head of an idafa constructioncannot have a definite article.Our first hypothesis is generally borne out.
We discuss three conditions in moredetail:1.
Using morphological features with the MaltParser and training on gold tags(Table 15).2.
Using morphological features with the MaltParser and training on a combinationof gold and predicted tags (Table 16).3.
Using morphological features with the Easy-First parser and training on a combi-nation of gold and predicted tags (Table 17).In all cases, for controlled investigation, we compare the error reduction resultingfrom adding morphological features to a ?morphology-free?
baseline, which in allcases we take to be the MaltParser trained on the gold CORE12, and evaluated on185Computational Linguistics Volume 39, Number 1BaselineV7( mrt?passed?MODP19: ?l??upon?OBJN$%A?xtfA?
?disappearance?IDFN.(7  Alzmyl?the colleague?<<<IDF>>>N)823Almhnds?the engineer?MODPN.
.
.<<<OBJ>>>N6?yAm?days?BestV7( mrt?passed?MODP19: ?l??upon?OBJN$%A?xtfA?
?disappearance?IDFN.(7  Alzmyl?the colleague?MODN)823Almhnds?the engineer?MODPN.
.
.SBJN6?yAm?days?Figure 2Error analysis example.
.
.
.
)823  .(7  $%19: 67( mrt ?yAm ?l?
A?xtfA?
Alzmyl Almhnds.
.
.
(?Several days have passed since the disappearance of the colleague the engineer .
.
.
?
), asparsed by the baseline system using only CORE12 (left) and as using the best performing model(right).
Bad predictions are marked with <<< .
.
.
>>>.
The words in the tree are presented inthe Arabic reading direction (from right to left).machine-predicted input (except for Table 17, where the Easy-First Parser is trained andevaluated instead).We start out by investigating the behavior of MaltParser, using all gold tags fortraining.
The accuracy by relation type is shown in Table 15.
Using just CORE12, wesee that some attachments (subject, modifications) are harder than others (objects,idafa).
We see that by adding LMM, all attachment types improve a little bit; this isas expected, because this feature provides a slight lexical abstraction.
We then addfeatures designed to improve idafa and those relations subject to agreement, subject,and nominal modification (DET2, PERSON, NUMBER, GENDER).
We see that, as expected,subject, nominal modification, and idafa reduce error by substantial margins (errorreduction over CORE12 is greater than 10%; in the case of idafa it is 21.8%), and all otherrelations (including object and prepositional attachment) improve to a lesser degree(error reduction of 7.1% or less).
We assume that the non-agreement relations (object186Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesTable 15Training the MaltParser on gold tags, accuracy by gold attachment type (selected): subject,object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by apreposition, idafa, and overall results (repeated).model (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF totalCORE12 67.9 90.4 72.0 70.3 94.5 78.7CORE12 + LMM 68.8 90.4 72.6 70.9 94.6 79.0CORE12 + DET2+LMM+PNG 71.7 91.0 74.9 72.4 95.5 80.2CORE12 + DET2+LMM+PERSON +FN*NG 72.3 91.0 75.6 72.7 95.5 80.4CORE12 + DET2+LMM+PERSON +FN*NGR 71.9 91.2 74.5 73.2 95.3 80.2and prepositional attachment) improve because of the overall improvement in theparse due to the improvements in the other relations.When we move to the functional features, using functional number and gender,we see a further reduction in the agreement-related attachments, namely, subject andnominal modification (error reductions over baseline of 13.7% and 12.9%, respectively).Idafa decreases slightly (because this relation is not affected by the functional features),whereas object stays the same.
Surprisingly, prepositional attachment also improves,with an error reduction of 8.1%.
Again, we can only explain this by proposing that theimprovement in nominal modification attachment has the indirect effect of ruling outsome bad prepositional attachments as well.We then add the rationality feature (last line of Table 15).
We now see that allrelations affected by agreement or assignment perform worse than without the ratio-nality feature.
In contrast, all other relations improve.
The decrease in performance canbe explained by the fact that the rationality (RAT) feature is not predicted with highaccuracy; because it interacts directly with agreement, and because we are training ongold annotation, the models trained do not correspond to the seen data.
We expectrationality to contribute when we look at training that includes predicted features.
(We have no explanation for the improvement in the other relations.
)We now turn to training the MaltParser on a combination of gold and predictedPOS and morphological feature values (g+p; Section 7).
The accuracy by relation isshown in Table 16.
The table repeats (in the first row) the results for the MaltParsertrained only using gold CORE12 features.
First, we see that using the same single feature,but training on gold and predicted tags, we obtain an across-the-board improvement,Table 16Training the MaltParser on gold and predicted tags, accuracy by gold attachment type (selected):subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun)by a preposition, idafa, and overall results (repeated).model (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF totalCORE12 67.9 90.4 72.0 70.3 94.5 78.7CORE12 g+p 70.9 91.0 73.5 70.2 94.7 80.0CORE12 + DET+LMM+PNG g+p 73.7 91.6 76.6 72.8 96.3 81.7CORE12 + DET2+LMM+PERSON 74.3 91.8 77.4 72.9 95.2 81.8+FN*NG g+pCORE12 + DET2+LMM+PERSON 74.8 91.6 77.4 73.5 95.5 81.9+FN*NGR g+p187Computational Linguistics Volume 39, Number 1Table 17Training the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type(selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb ora noun) by a preposition, idafa, and overall results (repeated).model (POS and morphological features) SBJ OBJ MOD-N MOD-Prep IDF totalCORE12 70.8 90.7 73.1 71.4 94.2 79.6CORE12 g+p 73.3 91.2 74.6 71.4 95.0 80.8CORE12 + DET+LMM+PERSON+FN*NG g+p 76.4 91.9 77.9 73.2 96.2 82.7CORE12 + DET+LMM+PERSON+FN*NGR g+p 76.2 91.9 78.1 73.2 95.9 82.6with error reductions between 3.6% and 9.3%, with no apparent patterns.
(Prepositionalmodifications even show a slight decline in attachment accuracy).
This row (using onlyCORE12 and training on gold and predicted) now becomes our baseline for subsequentdiscussion of error reduction.
If we then add the form-based features, we again find thatthe error rate decrease for subject, nominal modification, and idafa (the relations affectedby agreement and assignment) is greater than that for the other relations; with this train-ing corpus, however, the separation is not as stark, with subject decreasing its error rateby 9.6% and prepositional modification by 8.7%.
Notably, idafa shows the greatest errorrate reduction we have seen so far: 30.2%.
When we turn to functional features, we againsee a further increase in performance across the board.
And, as expected, the penalty forusing the rationality feature disappears because we have trained on predicted featuresas well.
In fact the improvement due to rationality specifically benefits the relationsaffected by agreement and assignment, with subject reducing error by 13.4% now,nominal modification by 14.7%, and idafa by 34.0%.
The tree on the right in Figure 2is the parse tree returned by this model, and both the subject and the idafa relation arecorrectly analyzed.
Note that the increase in the accuracy of idafa is probably not relatedto the interaction of syntax and morphology in assignment, because assignment in theidafa construction is not affected by rationality.
Instead, we suspect that the parser canexploit the very different profile of the rationality feature in the dependent node of theidafa and modification constructions.
Looking just at nominals, we see in the gold corpusthat 62% of the dependents in a modification relation have no inherent rationality (thisis the case notably for adjectives), whereas this number for idafa is only 18%.
In contrast,the dependent of an idafa is irrational 66% of the time, whereas for modification thatnumber is only 16%.Finally, we turn to the use of the Easy-First Parser (Section 6).
The accuracy byrelation is shown in Table 17.
When we switch from MaltParser to Easy-First Parser,we get an overall error reduction of 4.2%, which is reflected fairly evenly among therelations, with two outliers: subjects improve by 9.0%, whereas idafa increases its errorrate by 5.5%!
We do not have an immediate analysis for this behavior, because idafais usually considered an ?easy?
relation (no word can intervene between the linkedwords), as reflected in the high accuracy numbers for this relation.
Furthermore, whenwe inspect the unlabeled accuracy scores (not shown here), we see that the unlabeledattachment score for idafa also decreases.
Thus, we must reject a plausible hypothesis,namely, that the parser gets the relations right but the labeler (which in the Easy-FirstParser is a separate, second-pass module) gets the labels wrong.
When we train theEasy-First Parser on gold and predicted, we see a similar improvement pattern over justtraining on gold as we did with the MaltParser; one exception is that the idafa relationimproves greatly again.
Finally, we add the functional morphological features (training188Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Featureson gold and predicted).
Again, the pattern we observe (by comparing error reductionagainst using Easy-First Parser trained only using CORE12 on gold and predicted) arevery similar to the pattern we observed with the MaltParser in the same conditions.One difference stands out, however: whereas the MaltParser can exploit the rationalityfeature when trained on gold and predicted, the Easy-First Parser cannot.
Object andprepositional modification perform identically with or without rationality, but subjectand idafa perform worse; only nominal modification performs better (with overall per-formance decreasing).
If we inspect the unlabeled attachment scores for subjects, we dodetect an increase in accuracy (from 85.0% to 85.4%); perhaps the parser can exploit therationality feature, but the labeler cannot.Grammaticality of parse trees.
We now turn to our second type of error analysis, theevaluation of the grammaticality of the parse trees in terms of gender and numberagreement patterns.
We use the agreement checker code developed by Alkuhlani andHabash (2011) and evaluate our baseline (MaltParser using only CORE12), best perform-ing model (Easy-First Parser using CORE12 + DET+LMM+PERSON+FN*NGR g+p), andthe gold reference.
The agreement checker verifies, for all verb?nominal subject relationsand noun?adjective relations found in the tree, whether the agreement conditions aremet or not.
The accuracy number reflects the percentage of such relations found whichmeet the agreement criteria.
Note that we use the syntax given by the tree, not thegold syntax.
For all three trees, however, we used gold morphological features forthis evaluation even when those features were not used in the parsing task.
This isbecause we want to see to what extent the predicted morphological features help findthe correct syntactic relations, not whether the predicted trees are intrinsically coherentgiven possibly false predicted morphology.
The results can be found in Table 18.
We notethat the grammaticality of the gold corpus is not 100%; this is approximately equallydue to errors in the checking script and to annotation errors in the gold standard.We take the given grammaticality of the gold corpus as a topline for this analysis.Nominal modification has a smaller error band between baseline and gold comparedwith subject?verb agreement.
We assume this is because subject?verb agreement is morecomplex (it depends on their relative order), and because nominal modification canhave multiple structural targets, only one of which is correct, although all, however,are plausible from the point of view of agreement.
The error reduction relative to thegold topline is 62% and 76% for nominal agreement and verb agreement, respectively.Thus, we see that our second hypothesis?that the use of morphological features willreduce grammaticality errors in the resulting parse trees with respect to agreementphenomena?is borne out.In summary, we see that not only do morphological (and functional morpholog-ical features in particular) improve parsing, but they improve parsing in the wayTable 18Analysis of grammaticality of agreement relations between verb and subject and between a nounand a nominal modifier (correct agreement in percent).model (POS and morphological features) noun-modifier subject-verbGold 97.8 98.1MaltParser using CORE12 95.2 88.6Easy-First Parser using CORE12 + 96.8 95.8DET+LMM+PERSON+FN*NGR g+p189Computational Linguistics Volume 39, Number 1that we expect: (a) those relations affected by agreement and assignment contributemore than those that are not, and (b) agreement errors in the resulting parse trees arereduced.9.
Conclusions and Future WorkWe explored the contribution of different morphological features (both inflectionaland lexical) to dependency parsing of Arabic.
Starting with form-based morphologicalfeatures, we find that definiteness (DET), PERSON, NUMBER, GENDER, and undiacritizedlemma (LMM) are most helpful for Arabic dependency parsing on predicted (non-gold) input.
We further find that functional gender, number, and rationality features(FN*GENDER, FN*NUMBER, RAT) improve over form-based-only morphological fea-tures, as expected when considering the complex agreement rules of Arabic.
To ourknowledge, this is the first result in Arabic NLP using functional morphological fea-tures, and showing an improvement over form-based features.This article presented a large number of results.
We summarize them next.1.
We observe a tradeoff among the three factors (relevance, redundancy, and ac-curacy) of morphological features in parsing quality.
The best performing tag set(BW) under the gold condition (i.e., it is very relevant) is worst under the machine-predicted condition, because of its dismal prediction accuracy rate.
The tag setwith highest prediction accuracy (CATIB6) does not necessarily yield the bestresults in dependency parsing accuracy, because it is not very relevant.
A simpleextension of CATIB6, however, that improves its relevance (CATIBEX) but retainssufficient accuracy improves the overall parsing quality.2.
Lexical features do help parsing, and the most helpful in predicted condition isthe undiacritized lemma (LMM) feature.
Although LMM is more ambiguous thanthe diacritized LEMMA feature, it has half the error rate of LEMMA which makes ita more reliable (accurate) feature.
When using LMM, LEMMA is highly redundant(and vice versa).3.
GENDER and NUMBER and their functional variants are the most useful for parsingin predicted condition.
This is a result of their high relevance and their highprediction accuracy.
In contrast, CASE and STATE are the best performers in thegold condition (i.e., highly relevant) but not in the predicted condition (whereCASE is actually the worst feature).
The rationality (RAT) feature is more helpfulin the gold condition, which suggests it is relevant; its associated parsing resultsin predicted condition are not as good, however.
Presumably, this is because of itslower prediction accuracy.4.
When evaluating in the machine-predicted input condition, training on data withgold and predicted morphological features (g+p training) consistently improvesresults over training on gold.
This novel technique most likely addresses thenegative effect of feature prediction error by introducing the common errors tothe parsing model in training.
A side effect of it is that using correct predictionsby the parser is reinforced, because constructions with correctly predicted valuesappear twice as often in g+p training.5.
All of these results carry over successfully to another parser (Easy-First Parser),suggesting the insights are not specific to MaltParser.190Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features6.
Our best model was trained with the Easy-First Parser, containing the follow-ing features: CORE12+DET+LMM+PERSON+FN*NGR, with g+p feature valuesfor training.
We make this model available, together with the source code.23Although we only experimented with Arabic dependency parsing, we believe thatthe evaluation framework we presented and many of our conclusions will carry over toother languages (particularly, Semitic and morphology-rich languages) and syntacticrepresentations (e.g., phrase structure).
Some of our conclusions are more languageindependent (e.g., those involving the use of predicted training conditions).In future work, we intend to improve the prediction of functional morphologicalfeatures?especially RAT?in order to improve dependency parsing accuracy in pre-dicted condition.
We also intend to investigate how these features can be integrated intoother parsing frameworks; we expect them to help independently of the framework.
Theability to represent the relevant morphological information in a manner that is useful toattachment decisions is, of course, crucial to improving parsing quality.AcknowledgmentsThis work was supported by the DARPA GALE program, contract HR0011-08-C-0110.
Y. Martonperformed most of the work on this paper while he was at the Center for Computational LearningSystems at Columbia University and at the IBM Watson Research Center.
We thank Joakim Nivrefor his useful remarks, Ryan Roth for his help with MADA, and Sarah Alkuhlani for her help withfunctional features.
We also thank three anonymous reviewers for thoughtful comments.ReferencesAlkuhlani, Sarah and Nizar Habash.
2011.A corpus for modeling morpho-syntacticagreement in Arabic: Gender, numberand rationality.
In Proceedings of the 49thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 357?362, Portland, OR.Alkuhlani, Sarah and Nizar Habash.
2012.Identifying broken plurals, irregulargender, and rationality in Arabic text.
InProceedings of the 13th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 675?685,Avignon.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings ofComputational Natural Language Learning(CoNLL), pages 149?164, New York, NY.Buckwalter, Timothy A.
2004.
BuckwalterArabic Morphological AnalyzerVersion 2.0.
Linguistic Data Consortium,University of Pennsylvania, 2002.LDC Catalog No.
: LDC2004L02,ISBN 1-58563-324-0.Collins, Michael, Jan Hajic, Lance Ramshaw,and Christoph Tillmann.
1999.
A statisticalparser for Czech.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 505?512, College Park, MD.Cowan, Brooke and Michael Collins.2005.
Morphology and reranking forthe statistical parsing of Spanish.In Proceedings of Human LanguageTechnology (HLT) and the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 795?802,Morristown, NJ.Dada, Ali.
2007.
Implementation ofArabic numerals and their syntax inGF.
In Proceedings of the Workshop onComputational Approaches to SemiticLanguages, pages 9?16, Prague.Diab, Mona.
2007.
Towards an optimalPOS tag set for modern standard Arabicprocessing.
In Proceedings of RecentAdvances in Natural Language Processing(RANLP), pages 91?96, Borovets.Diab, Mona and Yassine Benajiba.
(in preparation).
From raw text to basephrase chunks: The new generation ofAMIRA Tools for the processing ofModern Standard Arabic.Diab, Mona, Kadri Hacioglu, and DanielJurafsky.
2004.
Automatic tagging ofArabic text: From raw text to base phrase23 Available for downloading at http://www1.ccls.columbia.edu/ CATiB/parser.191Computational Linguistics Volume 39, Number 1chunks.
In Proceedings of the 4th Meeting ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL) -Human Language Technology (HLT),pages 149?152, Boston, MA.Eryigit, G?lsen, Joakim Nivre, and KemalOflazer.
2008.
Dependency parsing ofTurkish.
Computational Linguistics,34(3):357?389.Goldberg, Yoav and Michael Elhadad.
2010.An efficient algorithm for easy-firstnon-directional dependency parsing.In Proceedings of Human LanguageTechnology (HLT): The North AmericanChapter of the Association for ComputationalLinguistics (NAACL), pages 742?750,Los Angeles, CA.Green, Spence and Christopher D. Manning.2010.
Better Arabic parsing: Baselines,evaluations, and analysis.
In Proceedings ofthe 23rd International Conference onComputational Linguistics (COLING),pages 394?402, Beijing.Habash, Nizar.
2010.
Introduction to ArabicNatural Language Processing.
Morgan &Claypool Publishers.Habash, Nizar, Reem Faraj, and Ryan Roth.2009.
Syntactic Annotation in theColumbia Arabic Treebank.
In Proceedingsof MEDAR International Conference onArabic Language Resources and Tools,pages 125?135, Cairo.Habash, Nizar, Ryan Gabbard, OwenRambow, Seth Kulick, and Mitch Marcus.2007.
Determining case in Arabic:Learning complex linguistic behaviorrequires complex linguistic features.
InProceedings of the 2007 Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP-CoNLL),pages 1,084?1,092, Prague.Habash, Nizar and Owen Rambow.
2005.Arabic tokenization, part-of-speechtagging and morphologicaldisambiguation in one fell swoop.In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 573?580, Ann Arbor, MI.Habash, Nizar, Owen Rambow, and RyanRoth.
2012.
MADA+TOKAN Manual.Technical Report CCLS-12-01, ColumbiaUniversity, New York, NY.Habash, Nizar and Ryan Roth.
2009.
CATiB:The Columbia Arabic treebank.
InProceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 221?224,Suntec.Habash, Nizar, Abdelhadi Soudi, andTim Buckwalter.
2007.
On Arabictransliteration.
In A. van den Bosch andA.
Soudi, editors, Arabic ComputationalMorphology: Knowledge-based and EmpiricalMethods, pages 15?22.
Springer, Berlin.Hajic?, Jan and Barbora Vidov?-Hladk?.
1998.Tagging inflective languages: Predictionof morphological categories for a rich,structured tagset.
In Proceedings of theInternational Conference on ComputationalLinguistics (COLING) - the Association forComputational Linguistics (ACL),pages 483?490, Stroudsburg, PA.Hohensee, Matt and Emily M. Bender.2012.
Getting more from morphologyin multilingual dependency parsing.In Proceedings of the 2012 Conference of theNorth American Chapter of the Associationfor Computational Linguistics: HumanLanguage Technologies, pages 315?326,Montr?al.K?bler, Sandra, Ryan McDonald, and JoakimNivre.
2009.
Dependency Parsing.
SynthesisLectures on Human LanguageTechnologies.
Morgan and ClaypoolPublishers.Kulick, Seth, Ryan Gabbard, and MitchMarcus.
2006.
Parsing the Arabic Treebank:Analysis and improvements.
In Proceedingsof the Treebanks and Linguistic TheoriesConference, pages 31?42, Prague.Maamouri, Mohamed, Ann Bies, Timothy A.Buckwalter, and Wigdan Mekki.
2004.The Penn Arabic Treebank: Building alarge-scale annotated Arabic corpus.In Proceedings of the NEMLAR Conference onArabic Language Resources and Tools,pages 102?109, Cairo.Marton, Yuval, Nizar Habash, and OwenRambow.
2010.
Improving Arabicdependency parsing with inflectional andlexical morphological features.
InProceedings of Workshop on StatisticalParsing of Morphologically Rich Languages(SPMRL) at the 11th Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL) -Human Language Technology (HLT),pages 13?21, Los Angeles, CA.Marton, Yuval, Nizar Habash, and OwenRambow.
2011.
Improving Arabicdependency parsing with lexical andinflectional surface and functionalfeatures.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 1,586?1,596,Portland, OR.192Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional FeaturesMcClosky, David, Eugene Charniak, andMark Johnson.
2006.
Effective self-trainingfor parsing.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL) -Human Language Technology (HLT),pages 152?159, Brooklyn, New York.Nilsson, Jens and Joakim Nivre.
2008.MaltEval: An evaluation and visualizationtool for dependency parsing.
In Proceedingsof the sixth Conference on Language Resourcesand Evaluation (LREC), pages 161?166,Marrakech.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.In Proceedings of the 8th InternationalConference on Parsing Technologies(IWPT), pages 149?160, Nancy.Nivre, Joakim.
2008.
Algorithms fordeterministic incremental dependencyparsing.
Computational Linguistics,34(4):513?553.Nivre, Joakim.
2009.
Parsing Indianlanguages with MaltParser.
In Proceedingsof the ICON09 NLP Tools Contest:Indian Language Dependency Parsing,pages 12?18, Hyderabad, India.Nivre, Joakim, Igor M. Boguslavsky,and Leonid K. Iomdin.
2008.Parsing the SynTagRus Treebank ofRussian.
In Proceedings of the 22ndInternational Conference on ComputationalLinguistics (COLING), pages 641?648,Manchester.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gulsen Eryigit, SandraKubler, Svetoslav Marinov, and ErwinMarsi.
2007.
MaltParser: A language-independent system for data-drivendependency parsing.
Natural LanguageEngineering, 13(2):95?135.Petrov, Slav, Dipanjan Das, and RyanMcDonald.
2012.
A universalpart-of-speech tagset.
In Proceedings of theConference on Language Resources andEvaluation (LREC), pages 2,089?2,096.Rambow, Owen, Bonnie Dorr, David Farwell,Rebecca Green, Nizar Habash, StephenHelmreich, Eduard Hovy, Lori Levin,Keith J. Miller, Teruko Mitamura,Florence Reeder, and SiddharthanAdvaith.
2006.
Parallel syntacticannotation of multiple languages.In Proceedings of the Fifth Conference onLanguage Resources and Evaluation (LREC),pages 559?564, Genoa.Smr?, Otakar.
2007.
Functional ArabicMorphology.
Formal System andImplementation.
Ph.D. thesis, CharlesUniversity, Prague.Tsarfaty, Reut and Khalil Sima?an.
2007.Three-dimensional parametrization forparsing morphologically rich languages.In Proceedings of the 10th InternationalConference on Parsing Technologies (IWPT),pages 156?167, Morristown, NJ.Zitouni, Imed, Jeffrey S. Sorensen, and RuhiSarikaya.
2006.
Maximum entropy basedrestoration of Arabic diacritics.
InProceedings of the 21st InternationalConference on Computational Linguistics(COLING) and the 44th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 577?584, Sydney.193Computational Linguistics Volume 39, Number 1A.
Appendix: Additional Feature EngineeringThe following sections describe additional experiments, with negative or small gains,presented here for completeness.A.1 Embedding Morphological Features Within the POS TagsAfter discovering our best form-based feature combination, we explored whether mor-phological data should be added to an Arabic parsing model as stand-alone machinelearning features, or whether they should be used to enhance and extend a POS tag set.We created a new POS tag set, CORE12EX, size 81 (and 96.0% prediction accuracy), byextending the CORE12 tag set with the features that most improved the CORE12 baseline:DET and the PNG-features.
But CORE12EX did worse than its non-extended (but feature-enhanced) counterpart, CORE12+DET+PNG.
Another variant, CORE12EX+DET+PNG,which used both the extended tag set and the additional DET and PNG-features, didnot improve over CORE12+DET+PNG either.A.2 Extended PERSON FeatureAfter extending the determiner feature (DET2), the next gainful feature that we couldalter was PERSON.
We changed the values of proper names from ?N/A?
to ?3?(third-person).
But this change resulted in a slight decrease in performance, so it wasabandoned.A.3 Digit Tokens and Number BinningDigit tokens (e.g., 4, as opposed to four) are marked singular by default.
They don?tshow surface agreement with a noun, even though the corresponding number-wordtoken would.
Therefore we replaced the digit tokens?
NUMBER value with ?N,?
anddenoted these experiments with NUMDGT.24We further observe that MSA displays complex agreement patterns with num-bers (Dada 2007).
Therefore, we alternatively experimented with binning the digittokens?
NUMBER value accordingly: the number 0 and numbers ending with 00 the number 1 and numbers ending with 01 the number 2 and numbers ending with 02 the numbers 3?10 and those ending with 03?10 the numbers, and numbers ending with, 11?99 all other number tokens (e.g., 0.35 or 7/16)We denoted these experiments with NUMDGTBIN.
Almost 1.5% of the tokens aredigit tokens in the training set, and 1.2% in the dev set.Number binning did not have a consistent contribution in either gold or predictedvalue conditions (results not shown), so it was abandoned as well.24 We didn?t mark the number-words because in our training data there were fewer than 30 lemmas offewer than 2,000 such tokens, and hence presumably their agreement patterns can be more easily learned.194
