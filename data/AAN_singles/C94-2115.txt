Drawing  P ic tures  w i th  Natura l  Language and Direct  Man ipu la t ionMayumi Hiyoshi and Hideo ShimazuInformation Technology Research Laboratories, NEC Corporation4-1-1 Miyazaki, Miyamae Kawasaki, 216 Japan{hiyoshi, shimazu}?j oke.
c\]_.nec, co. jpAbstractA multimodal user interface allows usersto communicate with computers using mul-tiple modalities, such as a mouse, a key-board or voice, in various combined ways.This paper discusses a multimodal drawingtool, whereby the user can use a mouse,a keyboard and voice effectively.
Also,it describes an interpretation method, bywhich the system integrates voice inputsand pointing inputs using context.1 IntroductionThis paper describes an experimental implementa~tion of a multimodal interface.
Specifically, the au-thors have developed a multimodal drawing tool.
Themultimodal drawing tool allows users to draw pic-tures by using multiple modalities; mouse, keyboardand voice input, in various combined ways.Recently, most user interfaces tend to be based ona direct manipulation method.
However the directmanipulation method is not always better than otherways.
The direct manipulation method is not par-ticularly applicable, when mentioning several opera-tions together and operating an object which is notdisplayed.
Also, it compels a user to point to a targetobject correctly with a pointing device.
On the otherhand, voice inputs have some advantages, ince a us-er can feel free to speak at any time, and the user canuse the voice input while simultaneously using otherdevices.
A combination of such different modalitiesoffers an interface which is easy for the user to use.Many multimodal systems, which integrate natu-ral language inputs and pointing inputs, have beendeveloped \[2\]\[1\]\[5\]\[4\].
In those systems, tbe user usesnatural language mainly supported by the pointinginputs.
However, when the user has to communicatewith the computer frequently, in such a system asdrawing tool, it is ,lot effective for the user to alwaysspeak while working.A prototype system for a multimodal drawing toolhas been developed, whereby the user can use voiceinputs unrestrainedly and effectively, that is, the usercan choose a modality unrestrainedly, and can use thevoice inputs only when the user wants to do so.
Illsuch a system, input data come ill at random withmultiple modalities.
Tile multimodal system must beable to handle such several kinds of input data.2 Multimodal Inputs in DrawingToolsThis section describes requirements o develop gen-eral drawing interfaces.
In existing drawing tools,a mouse is a major input device.
In addition, somedrawing tools assign functions to some keys on a key-board to reduce inconvenience in menu operations.Issues regarding such interfaces are as follows:?
It is troublesome to input a function.
Becausea user uses a mouse, both to select a menu andto draw a figure, tile user has to move a cursormany times from a menu area to a canvas onwhich figures are placed.?
It is troublesome to look for a menu item.
In pro-portion to increasing functions increment, menuitems also increase.
So, it becomes increasing-ly difficult to look for a specific objective menuitem.?
It is troublesome to continuously move a handfrom a mouse to a keyboard.?
It is not possible to express plural requirementssimultaneously.
For example, when a user wantsto delete plural figure objects, the user has tochoose the objects one by one.?
The user has to point to an object correctly.
Forexample, when the user wants to choose a lineobject on a display, the user has to move a cursorjust above the line and click the mouse button.If the point shifts slightly, the object is not se-lected.By adding voice input functions to such an inputenvironmeut, it becomes possible to solve these first722three issues.
That  is, by means of operation with thevoice input, a user can concentrate on drawing, andmenu search and any labor required by changing deevices becomes unnecessary.t, br overcoming the rest of these issues, more con-it*vance is needed.
The authors attempted to developa mull*modal drawing tool, operable with both voiceinlmts and pointing inputs, which has tire followingtime*ions.?
A user can choose a modality (lUOHSe or voice)unrestrainedly, which means that the user canuse the voice inputs only when the user wantsto do so.
Also, the user can use both modal*tiesin various comhiued wws.
For example, the us-er says "this", while pointing to one of severalobjects.?
Plural requests can be e?pressed simultaneous-ly ( ex.
"change the (-ok)r of all line objects togreen").
So, the operation elticiency will be im-1)roved.?
A user can shorten voice inputs (ex.
"n,ovehere") or omit mouse i)ointing events b,%sed onthe situation, if the omitted concepts are able tobe inferred l?om eoute?t.
For example, the.
us-er can utter "this", as a reference for previouslyoperated objects, without a mouse pointing.?
Ambiguous pointings are possible.
When a userwants to choose an object \['rein ~;uliollg those ona display, tire nser can indicate it rouglfly with abrief desrription, using the voice input.
For ex..aml)le , a user points at a spot near a target ob-ject and utters "line", whc'reby the nearest "line"obje.ct to the spot in selected.
Or, a us(;,' pointsat objects piled n I) and says "circle", then onlythe "circle" objects among the piled /,t1 objectsare selected.'
lb realize these time*ions, it in necessary to solvethe following new problems.1.
Matching pointing inputs with voice inputs.hr tire proposed sysi.em, since pointing eventsmay olteu occur independently, it is difficult tojudge wtmther or not an event is at* indepen-dent input or whethe.r it follows a related voiceinput.
So, an interpretation wherein the voiceinput and pointing event are connected in theorder of arrival is not sufficient \[4\].
Therefore,a pointing event shouht be basically handled asan independent event.
Then, the event is pickedout from input history afl, erward, when the sys~tern judges that the event relates to the li)llowingvoice input.2.
Solving several input data ambiguities.in the l)revious mouse based system, ambiguousinputs do not o<'.cur, because tim system requiresthat a user selects menus and target objects e~-plieitly and exactly.
F, ven if the voice inl)ut func-tion in added in such a system~ it is possible toif)roe the user to give a detailed verbal sequencefor the operation without ambiguity.
However,when the time*ion becomes more sopltisticated,it is dilficult for the user to outline the user's in-tention in detail verbally.
So, it is necessary tobe able to interpret he ambiguous user's input.Several multimodal systems have been developedto solve these problems.
For example, Hayes \[4\] p roposed the first issue to be addressed, but the definitesolution was not addressed.
()()hen \[3\] presented asolution for the first issue, by utilizing context.
How-ever, the solution is not sufficient for application todrawing tools, because it was presented only for querysystems.
The following section describes a prototypesystem for a multimodal drawing tool.
Next, solu-tions for these problems are presented.3 Mu l t lmoda l  Drawing  Too l3.1 System Const ruct ionA l)rototype system for a nmltimodal drawing toolw~Ls develol)ed as a case study on a mull*modal coin-municatiou system.
Figure I shows a system imageof the prototype system, by which the user drawsl)ictures using mouse, keyboard and voice.
This sys-tem was developed on a SUN workstation using theX-window system and was written in Prolog.
Voiceinput data is recognized on a personal colnputer, andthe recognition result is sent to the workstation.Figure t: Syst, em hnage3.2 ln ter lhce  Examph~sFigure 2 shows a screen image, of the system.
Theuser can draw pi<'tures with a combination (11' mouseand voice~ as using a mouse only.
Input examples ayefollows:723~ Mouse InputMouse ~ t Handler Inputf KeyboardKeyboard "tlnput HandlerInput~f Voice InputVoice ~t Handler InputDisplayDisplay ~ Handler j -Input Integrator -1Figure 3: Multimodal Drawing Tool StructureDrawing 1 Tooliiiiii!iiiiii iiiiiiii!iil,i,i!,i,i i iiii!iiii!.
.
.
.
.
.
.
.
.
.
.
.
, : :  : .
.
.
.
.
, : .
.
, .
.
.il.iii.liiii iii!i\[ii.iiiiliiiiiiii iiiiiiiiliiiiiiiii iiiiiii i!i!
i i!
i i!
i i i i i!
i i i!
i i i i i i i \[!i i i!
i i i i!
i}!i!i!
:~!iii!iNi~i~iii\ [ \ ]  MM-De'aw \[ \ ]i~i~i~i!~iiiiii.i!i::i!::.~i!!ii~!.i.!~i::!i::!i~i~iiii~i:~:~i!
: i !i~i!~i i!i .
!
ii!i'!i~i::!i::iiii::iiii':::i:ii!~:ii!!:.:.ii:~i!~ii:.!:!
!.i::: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :.~~iiiiiiiilNii~i!iiii ~ .
.
.
.
.
.
.
.
~i~ E, Vi;o,,,o,, ............ \[\]:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::ii:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::~~Figure 2: Screen hnage?
if a user wants to move an existing circle objectto some point, the user says "Move this circlehere", while pointing at a circle object and adestination point.
The system moves the circleobject to the specified point.?
If a user wants to choose an existing line objectamong several objects one upon another, the us-er can say "line" while pointing at a point nearthe line object.
The system chooses the nearestline object to the point.?
If the user wants to draw a red circle, the usercan say "red circle".
The system changes a cur-rent color mode to red and changes the drawingmode to the circle mode.3.3 System St ructureFigure 3 shows a prototype system structure.
Thesystem includes Mouse Input Handler, Keyboard Iu-put Handler, Voice input Handler, Drawing Tool andInput Integrator.Each Input Handler receives mouse input events,keyboard input events and voice input events, andsends them to the Input Integrator.The Input Integrator receives an input messagefrom each input handler, then interprets the message,using voice scripts, which show voice input patternsand sequences of operations related to the pattern-s, as well as mouse scripts, which show mouse inputpatterns and sequences of operations.
When the in-put data matches one of the input patterns in thescripts, the Input Integrator executes the sequence ofoperations related to the pattern.
That  is, the In-put Integrator sends some messages to Drawing Toolto carry out the sequences of operations.
If the in-put data matches a part of one of the input patternsin the scripts, tile Input Integrator waits for a nextinput.
Then, a combination of previous input data724and the new input data is examined.
Otherwise, theinterpretation fails.
The Input Integrator may referto the Drawing Tool.
For example, it refers to theDrawing 'reel for iirformation regarding an object ata specific position.Tile Drawing Toot manages attributes for tignreobjects and current status, such as color, size, linewidth, etc.
Also, it executes drawing and editing tip-erations, according to requests from the Input lute-grater, and it; sends the editing results to the DisplayHandler.
The l)isplay Handler modilies the expres-sion on the display.4 Multimode Data InterpretationThis section describes in detail interpretation meth-ods for tile multimodal inputs used in the drawingtool.4.1 Matching Pointing Inputs  wi th  VoiceInputsIn conwmtional multimodal systems, all a,laphoricreferences in voice inputs bring about pointing input-s, and individual pointing input is connected to anyanaphorie refi~renee.
However, in our system, a nsercan operate with either a pointing input only, a voiceinput only or a colnbination of pointing events andvoice inputs.
Because a pointing event may often oc-cur independently, when a l/ointing event does occur,the system cannot judge whe.ther tile event is an inde-pendent input or whether it follows the related voiceinput.
Furthermore, tile user can utter "this", as ref-erence to an object, operated immediately before tileutterance.
So, an interpretation that the voice inputand pointing event are connected only in the orderof arrival is not ,mtficient.
In the proposed system,a pointing event is basically handled as an indepen-dent event.
Then, the event is picked out from inputhistory afterward, when the system judges that theevent relates to the following voice input.
Further-more, the system has to interpret the voice inputsusing context ( ex.
previous operated object).In the proposed system, pointing inputs fl'om startto end of a voice input are kept in a queue.
Whenthe voice input ends, the system binds phrases inthe voice input and the pointing inputs in the queue.First, the system comliares the number of anaphoriereferences in the voice input and the mmdier of point-lug inputs in tile queue.
Figure 4 shows timing datafor a voice input and pointing inputs.
In Case(l)~the number of anaphoric references in the voice in-put and the number of pointing inputs.
In the othercases, a pointing input is lacking.
When a pointinginput is lacking, the following three possiDle causesare considered.?
The relative pointing event occurred before thevoice input, and it was bandied previously(Case(2) in Fig.
4).?
The Iirst anaphoric reference is "this" as refer-ence to all object which was operated immedi-ately before the voice input (Case(a) in Fig.
4).I 'l'he relative pointing event will occur after thevoice input (Case(4)in Fig.
4).i mCase(l) ', move this here 'Voice input l ~EaE~ EEZZCiPointing Input /k ,; tI iCase(2) ~ move this here 'Voice Input ~ ~Pointing Input _ Z~ A , |Case(a)Voice InputPointing input', move this here ',IZZZE3 ETZZZ\] EZZZ1 iA _ <Object>'.
.
.
~tCase(4) \]Voice InputPointing Input _J imove this here? '
A .. .
.
.
.
.
.
, .
i~  tFigure 4: Timing data for voice input and pointinginputsThe interpretation steps are as follows.1.
'\['he system examines an input immediately be-fore the voice input.
\[f it is a pointing event,the event is used for interpretation.
That is, theevent is added at the top of the pointing queue.2.
When tile above operation fails and the tirstanaphorie references is "this", then the systempicks up the object operated immediately before,if such exists.
The object information is addedat the top of the pointing queue.3.
Otherwise, tile system waits for the next point-ing input.
The inlmt is added onto the last ofthe pointing queue.
When a time out occurs, theinterpretation fails, due to a lack of a pointingevent.If the system can obtain tile necessary informa-tion, it binds the anaphorie references in the voiceinput and pointing event and object information intile pointing queue in the order of arrival.7254.2 Solv ing Input  Data AmbiguityIn a conventional mouse based system, there is nosemantic ambiguity.
Such systems require a user toselect menus and target objects and to edit the ob-jects explicitly and exactly.
Even if the voice inputfunction is added in such a system, the user can beforced to utter operations without ambiguity.
How-ever, when the function becomes more sophisticated,it is difficult for the user to utter the user's intentionsin detail.
So, it is necessary to be able to interpretthe user's ambiguous input.
In a multimodal drawingtool, such our system, one of the most essential inputambiguities is led by ambiguous I)ointings.l~br example, if a user says "character string", thereare three possible interpretations: "the user wants toedit one of the existing character strings", "the userwants to choose one of the existing character strings"and "the user wants to write a new character string"In this example, the system interprets using thefollowing heuristic rules.?
If a pointing event does not exist immediatelybefore, the system changes a drawing mode tothe character string input mode.?
If a pointing event upon a string object existsjust before the voice input, then the system addsthe character string object to a current selection;a group of objects selected currently.?
When a pointing event exists immediately beforethe voice input and there is a character stringobject near the position of the user's point (ex.within a radius of five ram.
fi'om the position),then the character string object is added to acurrent selection.?
When a pointing event exists and there is nocharacter string object near the position, thenthe mode is changed to the character string inputmode at the position.Naturally, "character string" in these heuristicsrules can be replaced by other figure types.
If thisheuristic rule is not perfect, the interpretation maybe different from the user's intention.
In such a case,it is important for a user to return from the errorcondition with minimum effort.
For example, assumethat a user, who wants to choose one of "characterstring" objects, says "character string" and pointson the display, but the distance between the pointedposition and the "character string" object is greaterthan the predefined threshold.
Then, according tothe above rules, the result of the system's interprc-ration will be to input a new character string at theposition, and the drawing mode changes to the char-acter string input mode.
In this case, the user wishesto turn back to a state which the user intended withmininmm elfort.
The system must return to the statein which tlle character string input mode is canceledand the nearest "character string" object is selected.A solution is for the user to utter "select" only.
Then,the system understands that it's interpretation waswrong and interprets that "select" means "select acharacter string object" using current context.5 ConclusionFor implementing a multimodal system, based on di-rect nmnipulation system, tile system has to use notonly pointing events concurrently with a voice input,but must also use the context, such as input historyor information regarding the current operated object,ms information for binding to the voice input.
Fur-thermore, it is important o solve any ambiguity ininputs.
This paper discussed these problems, anddescribed an interpretation nmthod using a drawingtool example.
Furthermore, a prototype system fora multimodal drawing tool has been implemented.Much future work remains, but we believe that theseelaborate interpretations may become bases of userfi.iendly multimodal interfaces.AcknowledgementsA part of this study was conducted under theFRIEND21 national study project.References\[1\] Allgayer, J., Jansen-Winkeln, 1~., reddig, C., andReithing N., "Bidirectional use of knowledge inthe multi-modal NL access system XTRA' ,  IJ-CAI'89, pp.1491-1497, 1989.\[2\] Bolt, R.A., "I~ut-That.-There: Voice and Ges--ture at the Graphics Interface", ComputerGraphics 14, 3, 1980.\[3\] Cohen, P.R., Dalrymple, M., Moran, D.B.,Pereira, F.C.N., et al, "Synergistic Use of DirectManipulation and Natural Language", Proc.
ofCHI-88, 1989.\[4\] Hayes, P.J., "Steps towards Integrating nat-ural Language and Graphical Interaction forKnowledge-based Systems", Advances in ArtiIi-cial Intelligence- lI, Elsevier Science Publishers,1987.\[5\] Wahlster, W., "User and discourse models formultilnodal communication", in J.W.
Sullivanand S.W.
Tyler, editors, Intelligent User Inter-faces, chapter3, ACM Press Frontiers Series~ Ad-dison Wesley Publishing, 1989.726
