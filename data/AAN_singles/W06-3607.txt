Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49?56,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsRe-Ranking Algorithms for Name TaggingHeng Ji Cynthia Rudin Ralph GrishmanDept.
of Computer Science Center for Neural Science and CourantInstitute of Mathematical SciencesDept.
of Computer ScienceNew York UniversityNew York, N.Y. 10003hengji@cs.nyu.edu rudin@nyu.edu grishman@cs.nyu.eduAbstractIntegrating information from differentstages of an NLP processing pipeline canyield significant error reduction.
We dem-onstrate how re-ranking can improve nametagging in a Chinese information extrac-tion system by incorporating informationfrom relation extraction, event extraction,and coreference.
We evaluate three state-of-the-art re-ranking algorithms (MaxEnt-Rank, SVMRank, and p-Norm Push Rank-ing), and show the benefit of multi-stagere-ranking for cross-sentence and cross-document inference.1 IntroductionIn recent years, re-ranking techniques have beensuccessfully applied to enhance the performanceof NLP analysis components based on generativemodels.
A baseline generative model produces N-best candidates, which are then re-ranked using arich set of local and global features in order toselect the best analysis.
Various supervised learn-ing algorithms have been adapted to the task of re-ranking for NLP systems, such as MaxEnt-Rank(Charniak and Johnson, 2005; Ji and Grishman,2005), SVMRank (Shen and Joshi, 2003), VotedPerceptron (Collins, 2002; Collins and Duffy,2002; Shen and Joshi, 2004), Kernel Based Meth-ods (Henderson and Titov, 2005), and RankBoost(Collins, 2002; Collins and Koo, 2003; Kudo et al,2005).These algorithms have been used primarilywithin the context of a single NLP analysis com-ponent, with the most intensive study devoted toimproving parsing performance.
The re-rankingmodels for parsing, for example, normally rely onstructures generated within the baseline parseritself.
Achieving really high performance for someanalysis components, however, requires that wetake a broader view, one that looks outside a sin-gle component in order to bring to bear knowl-edge from the entire NL analysis process.
In thispaper we will demonstrate the potential of thisapproach in enhancing the performance of Chi-nese name tagging within an information extrac-tion application.Combining information from other stages in theanalysis pipeline allows us to incorporate informa-tion from a much wider context, spanning the en-tire document and even going across documents.This will give rise to new design issues; we willexamine and compare different re-ranking algo-rithms when applied to this task.We shall first describe the general setting andthe special characteristics of re-ranking for nametagging.
Then we present and evaluate three re-ranking algorithms ?
MaxEnt-Rank, SVMRankand a new algorithm, p-Norm Push Ranking ?
forthis problem, and show how an approach based onmulti-stage re-ranking can effectively handle fea-tures across sentence and document boundaries.2 Prior Work2.1 RankingWe will describe the three state-of-the-art super-vised ranking techniques considered in this work.Later we shall apply and evaluate these algorithmsfor re-ranking in the context of name tagging.Maximum Entropy modeling (MaxEnt) hasbeen extremely successful for many NLP classifi-49cation tasks, so it is natural to apply it to re-ranking problems.
(Charniak and Johnson, 2005)applied MaxEnt to improve the performance of astate-of-art parser; also in (Ji and Grishman, 2005)we used it to improve a Chinese name tagger.Using SVMRank, (Shen and Joshi, 2003)achieved significant improvement on parse re-ranking.
They compared two different samplecreation methods, and presented an efficient train-ing method by separating the training samples intosubsets.The last approach we consider is a boosting-style approach.
We implement a new algorithmcalled p-Norm Push Ranking (Rudin, 2006).
Thisalgorithm is a generalization of RankBoost(Freund et al 1998) which concentrates specifi-cally on the top portion of a ranked list.
The pa-rameter ?p?
determines how much the algorithmconcentrates at the top.2.2 Enhancing Named Entity TaggersThere have been a very large number of NE taggerimplementations since this task was introduced atMUC-6 (Grishman and Sundheim, 1996).
Mostimplementations use local features and a unifyinglearning algorithm based on, e.g., an HMM, Max-Ent, or SVM.
Collins (2002) augmented a baselineNE tagger with a re-ranker that used only local,NE-oriented features.
Roth and Yih (2002) com-bined NE and semantic relation tagging, butwithin a quite different framework (using a linearprogramming model for joint inference).3 A Framework for Name Re-Ranking3.1 The Information Extraction PipelineThe extraction task we are addressing is that of theAutomatic Content Extraction (ACE)1 evaluations.The 2005 ACE evaluation had 7 types of entities,of which the most common were PER (persons),ORG (organizations), LOC (natural locations) andGPE (?geo-political entities?
?
locations which arealso political units, such as countries, counties,and cities).
There were 6 types of semantic rela-tions, with 18 subtypes.
Examples of these rela-tions are ?the CEO of Microsoft?
(anorganization-affiliation relation), ?Fred?s wife?
(a1 The ACE task description can be found athttp://www.itl.nist.gov/iad/894.01/tests/ace/personal-social relation), and ?a military base inGermany?
(a located relation).
And there were 8types of events, with 33 subtypes, such as ?KurtSchork died in Sierra Leone yesterday?
(a Dieevent), and ?Schweitzer founded a hospital in1913?
(a Start-Org event).To extract these elements we have developed aChinese information extraction pipeline that con-sists of the following stages:?
Name tagging and name structure parsing(which identifies the internal structure of somenames);?
Coreference resolution, which links "men-tions" (referring phrases of selected semantictypes) into "entities": this stage is a combina-tion of high-precision heuristic rules andmaximum entropy models;?
Relation tagging, using a K-nearest-neighboralgorithm to identify relation types and sub-types;?
Event patterns, semi-automatically extractedfrom ACE training corpora.3.2 Hypothesis Representation and Genera-tionAgain, the central idea is to apply the baselinename tagger to generate N-Best multiple hypothe-ses for each sentence; the results from subsequentcomponents are then exploited to re-rank thesehypotheses and the new top hypothesis is outputas the final result.In our name re-ranking model, each hypothesisis an NE tagging of the entire sentence.
For ex-ample, ?<PER>John</PER> was born in<GPE>New York</GPE>.?
is one hypothesisfor the sentence ?John was born in New York?.We apply a HMM tagger to identify four namedentity types: Person, GPE, Organization and Loca-tion.
The HMM tagger generally follows theNymble model (Bikel et al 1997), and uses best-first search to generate N-Best hypotheses.
It alsocomputes the ?margin?, which is the differencebetween the log probabilities of the top two hy-potheses.
This is used as a rough measure of con-fidence in the top hypothesis.
A large marginindicates greater confidence that the first hypothe-sis is correct.
The margin also determines thenumber of hypotheses (N) that we will store.
Us-ing cross-validation on the training data, we de-termine the value of N required to include the best50hypothesis, as a function of the margin.
We thendivide the margin into ranges of values, and set avalue of N for each range, with a maximum of 30.To obtain the training data for the re-rankingalgorithm, we separate the name tagging trainingcorpus into k folders, and train the HMM nametagger on k-1 folders.
We then use the HMM togenerate N-Best hypotheses H = {h1, h2,?,hN} foreach sentence in the remaining folder.
Each hi inH is then paired with its NE F-measure, measuredagainst the key in the annotated corpus.We define a ?crucial pair?
as a pair of hypothe-ses such that, according to F-Measure, the firsthypothesis in the pair should be more highlyranked than the second.
That is, if for a sentence,the F-Measure of hypothesis hi is larger than thatof hj, then (hi, hj) is a crucial pair.3.3 Re-Ranking FunctionsWe investigated the following three different for-mulations of the re-ranking problem:?
Direct Re-Ranking by ScoreFor each hypothesis hi, we attempt to learn a scor-ing function f : H ?
R, such that f(hi) > f(hj) if theF-Measure of hi is higher than the F-measure of hj.?
Direct Re-Ranking by ClassificationFor each hypothesis hi, we attempt to learn f : H?
{-1, 1}, such that f(hi) = 1 if hi has the top F-Measure among H; otherwise f(hi) = -1.
This canbe considered a special case of re-ranking byscore.?
Indirect Re-Ranking FunctionFor each ?crucial?
pair of hypotheses (hi, hj), welearn f : H ?
H ?
{-1, 1}, such that f(hi, hj) = 1 ifhi is better than hj; f (hi, hj) = -1 if hi is worse thanhj.
We call this ?indirect?
ranking because weneed to apply an additional decoding step to pickthe best hypothesis from these pair-wise compari-son results.4 Features for Re-Ranking4.1 Inferences From Subsequent StagesInformation extraction is a potentially symbioticpipeline with strong dependencies between stages(Roth and Yih, 2002&2004; Ji and Grishman,2005).
Thus, we use features based on the outputof four subsequent stages ?
name structure parsing,relation extraction, event patterns, and coreferenceanalysis ?
to seek the best hypothesis.We included ten features based on name struc-ture parsing to capture the local informationmissed by the baseline name tagger such as detailsof the structure of Chinese person names.The relation and event re-ranking features arebased on matching patterns of words or constitu-ents.
They serve to correct name boundary errors(because such errors would prevent some patternsfrom matching).
They also exert selectional pref-erences on their arguments, and so serve to correctname type errors.
For each relation argument, weincluded a feature whose value is the likelihoodthat relation appears with an argument of that se-mantic type (these probabilities are obtained fromthe training corpus and binned).
For each eventpattern, a feature records whether the types of thearguments match those required by the pattern.Coreference can link multiple mentions ofnames provided they have the same spelling(though if a name has several parts, some may bedropped) and same semantic type.
So if theboundary or type of one mention can be deter-mined with some confidence, coreference can beused to disambiguate other mentions, by favoringhypotheses which support more coreference.
Tothis end, we incorporate several features based oncoreference, such as the number of mentions re-ferring to a name candidate.Each of these features is defined for individualname candidates; the value of the feature for ahypothesis is the sum of its values over all namesin the hypothesis.
The complete set of detailedfeatures is listed in (Ji and Grishman, 2006).4.2 Handling Cross-Sentence Features byMulti-Stage Re-RankingCoreference is potentially a powerful contributorfor enhancing NE recognition, because it providesinformation from other sentences and even docu-ments, and it applies to all sentences that includenames.
For a name candidate, 62% of its corefer-ence relations span sentence boundaries.
How-ever, this breadth poses a problem because itmeans that the score of a hypothesis for a given51sentence may depend on the tags assigned to thesame names in other sentences.2Ideally, when we re-rank the hypotheses for onesentence S, the other sentences that include men-tions of the same name should already have beenre-ranked, but this is not possible because of themutual dependence.
Repeated re-ranking of a sen-tence would be time-consuming, so we haveadopted an alternative approach.
Instead of incor-porating coreference evidence with all other in-formation in one re-ranker, we apply two re-rankers in succession.In the first re-ranking step, we generate newrankings for all sentences based on name structure,relation and event features, which are all sentence-internal evidence.
Then in a second pass, we ap-ply a re-ranker based on coreference between thenames in each hypothesis of sentence S and thementions in the top-ranking hypothesis (from thefirst re-ranker) of all other sentences.3  In this way,the coreference re-ranker can propagate globally(across sentences and documents) high-confidencedecisions based on the other evidence.
In our finalMaxEnt Ranker we obtained a small additionalgain by further splitting the first re-ranker intothree separate steps: a name structure based re-ranker, a relation based re-ranker and an eventbased re-ranker; these were incorporated in anincremental structure.4.3 Adding Cross-Document InformationThe idea in coreference is to link a name mentionwhose tag is locally ambiguous to another men-tion that is unambiguously tagged based on localevidence.
The wider a net we can cast, the greaterthe chance of success.
To cast the widest net pos-sible, we have used cross-document coreferencefor the test set.
We cluster the documents using across-entropy metric and then treat the entire clus-ter as a single document.We take all the name candidates in the top Nhypotheses for each sentence in each cluster T toconstruct a ?query set?
Q.
The metric used for theclustering is the cross entropy H(T, d) between thedistribution of the name candidates in T and2 For in-document coreference, this problem could be avoided if the tagging ofan entire document constituted a hypothesis, but that would be impractical ?
avery large N would be required to capture sufficient alternative taggings in anN-best framework.3 This second pass is skipped for sentences for which the confidence in the tophypothesis produced by the first re-ranker is above a threshold.document d. If H(T, d) is smaller than a thresholdthen we add d to T. H(T, d) is defined by:???
?=QxxdprobxTprobdTH ),(log),(),( .We built these clusters two ways: first, justclustering the test documents; second, by aug-menting these clusters with related documentsretrieved from a large unlabeled corpus (withdocument relevance measured using cross-entropy).5 Re-Ranking AlgorithmsWe have been focusing on selecting appropriateranking algorithms to fit our application.
Wechoose three state-of-the-art ranking algorithmsthat have good generalization ability.
We nowdescribe these algorithms.5.1 MaxEnt-Rank5.1.1  Sampling and PruningMaximum Entropy models are useful for the taskof ranking because they compute a reliable rank-ing probability for each hypothesis.
We have triedtwo different sampling methods ?
single samplingand pairwise sampling.The first approach is to use each single hy-pothesis hi as a sample.
Only the best hypothesisof each sentence is regarded as a positive sample;all the rest are regarded as negative samples.
Ingeneral, absolute values of features are not goodindicators of whether a hypothesis will be the besthypothesis for a sentence; for example, a co-referring mention count of 7 may be excellent forone sentence and poor for another.
Consequently,in this single-hypothesis-sampling approach, weconvert each feature to a Boolean value, which istrue if the original feature takes on its maximumvalue (among all hypotheses) for this hypothesis.This does, however, lose some of the detail aboutthe differences between hypotheses.In pairwise sampling we used each pair of hy-potheses (hi, hj) as a sample.
The value of a fea-ture for a sample is the difference between itsvalues for the two hypotheses.
However, consid-ering all pairs causes the number of samples togrow quadratically (O(N2)) with the number ofhypotheses, compared to the linear growth withbest/non-best sampling.
To make the training and52test procedures more efficient, we prune the datain several ways.We perform pruning by beam setting, removingcandidate hypotheses that possess very low prob-abilities from the HMM, and during training wediscard the hypotheses with very low F-measurescores.
Additionally, we incorporate the pruningtechniques used in (Chiang 2005), by which anyhypothesis with a probability lower than?timesthe highest probability for one sentence is dis-carded.
We also discard the pairs very close inperformance or probability.5.1.2 DecodingIf f is the ranking function, the MaxEnt modelproduces a probability for each un-pruned ?cru-cial?
pair: prob(f(hi, hj) = 1), i.e., the probabilitythat for the given sentence, hi is a better hypothe-sis than hj.
We need an additional decoding step toselect the best hypothesis.
Inspired by the cachingidea and the multi-class solution proposed by(Platt et al 2000), we use a dynamic decodingalgorithm with complexity O(n) as follows.We scale the probability values into three types:CompareResult (hi, hj) = ?better?
if prob(f(hi, hj) =1) >?1, ?worse?
if prob(f(hi, hj) = 1) <?2, and?unsure?
otherwise, where ?1??2.
4Prunefor i = 1 to nNum = 0;for j = 1 to n and j?iIf CompareResult(hi, hj) = ?worse?Num++;if Num>?then discard hi from HSelectInitialize: i = 1, j = nwhile (i<j)if CompareResult(hi, hj) = ?better?discard hj from H;j--;else if CompareResult(hi, hj) = ?worse?discard hi from H;i++;else break;4 In the final stage re-ranker we use?1=?2 so that we don?t generate theoutput of ?unsure?, and one hypothesis is finally selected.OutputIf the number of remaining hypotheses in H is 1,then output it as the best hypothesis; else propa-gate all hypothesis pairs into the next re-ranker.5.2 SVMRankWe implemented an SVM-based model, whichcan theoretically achieve very low generalizationerror.
We use the SVMLight package (Joachims,1998), with the pairwise sampling scheme as forMaxEnt-Rank.
In addition we made the followingadaptations: we calibrated the SVM outputs, andseparated the data into subsets.To speed up training, we divided our trainingsamples into k subsets.
Each subset contains N(N-1)/k pairs of hypotheses of each sentence.In order to combine the results from these dif-ferent SVMs, we must calibrate the function val-ues; the output of an SVM yields a distance to theseparating hyperplane, but not a probability.
Wehave applied the method described in (Shen andJoshi, 2003), to map SVM?s results to probabili-ties via a sigmoid.
Thus from the kth SVM, we getthe probability for each pair of hypotheses:)1),(( =jik hhfprob ,namely the probability of hi being better than hj.Then combining all k SVMs?
results we get:?
==kjikji hhfprobhhZ )1),((),( .So the hypothesis hi with maximal value is cho-sen as the top hypothesis:?jjihhhZi)),((maxarg .5.3 P-Norm Push RankingThe third algorithm we have tried is a generalboosting-style supervised ranking algorithm calledp-Norm Push Ranking (Rudin, 2006).
We de-scribe this algorithm in more detail since it is quitenew and we do not expect many readers to be fa-miliar with it.The parameter ?p?
determines how much em-phasis (or ?push?)
is placed closer to the top of theranked list, where p?1.
The p-Norm Push Rankingalgorithm generalizes RankBoost (take p=1 forRankBoost).
When p is set at a large value, therankings at the top of the list are given higher pri-ority (a large ?push?
), at the expense of possiblymaking misranks towards the bottom of the list.53Since for our application, we do not care about therankings at the bottom of the list (i.e., we do notcare about the exact rank ordering of the bad hy-potheses), this algorithm is suitable for our prob-lem.
There is a tradeoff for the choice of p; largerp yields more accurate results at the very top ofthe list for the training data.
If we want to considermore than simply the very top of the list, we maydesire a smaller value of p. Note that larger valuesof p also require more training data in order tomaintain generalization ability (as shown both bytheoretical generalization bounds and experi-ments).
If we want large p, we must aim to choosethe largest value of p that allows generalization,given our amount of training data.
When we areworking on the first stage of re-ranking, we con-sider the whole top portion of the ranked list, be-cause we use the rank in the list as a feature forthe next stage.
Thus, we have chosen the valuep1=4 (a small ?push?)
for the first re-ranker.
Forthe second re-ranker we choose p2=16 (a large?push?
).The objective of the p-Norm Push Ranking al-gorithm is to create a scoring function f: H?Rsuch that for each crucial pair (hi, hj), we shallhave f(hi) > f(hj).
The form of the scoring functionis f(hi) = ?
?kgk(hi), where gk is called a weakranker: gk : H ?
[0,1].
The values of ?k are de-termined by the p-Norm Push algorithm in an it-erative way.The weak rankers gk are the features describedin Section 4.
Note that we sometimes allow thealgorithm to use both gk and g?k(hi)=1-gk(hi) asweak rankers, namely when gk has low accuracyon the training set; this way the algorithm itselfcan decide which to use.As in the style of boosting algorithms, real-valued weights are placed on each of the trainingcrucial pairs, and these weights are successivelyupdated by the algorithm.
Higher weights aregiven to those crucial pairs that were misranked atthe previous iteration, especially taking into ac-count the pairs near the top of the list.
At eachiteration, one weak ranker gk is chosen by the al-gorithm, based on the weights.
The coefficient ?kis then updated accordingly.6 Experiment Results6.1 Data and ResourcesWe use 100 texts from the ACE 04 training corpusfor a blind test.
The test set included 2813 names:1126 persons, 712 GPEs, 785 organizations and190 locations.
The performance is measured viaPrecision (P), Recall (R) and F-Measure (F).The baseline name tagger is trained from 2978texts from the People?s Daily news in 1998 andalso 1300 texts from ACE training data.The 1,071,285 training samples (pairs of hy-potheses) for the re-rankers are obtained from thename tagger applied on the ACE training data, inthe manner described in Section 3.2.We use OpenNLP5 for the MaxEnt-Rank ex-periments.
We use SVMlight (Joachims, 1998) forSVMRank, with a linear kernel and the soft mar-gin parameter set to the default value.
For the p-Norm Push Ranking, we apply 33 weak rankers,i.e., features described in Section 4.
The numberof iterations was fixed at 110, this number waschosen by optimizing the performance on a devel-opment set of 100 documents.6.2 Effect of Pairwise SamplingWe have tried both single-hypothesis and pairwisesampling (described in section 5.1.1) in MaxEnt-Rank and p-Norm Push Ranking.
Table 1 showsthat pairwise sampling helps both algorithms.MaxEnt-Rank benefited more from it, with preci-sion and recall increased 2.2% and 0.4% respec-tively.Model P R FSingle Sampling 89.6 90.2 89.9MaxEnt-Rank Pairwise Sampling 91.8 90.6 91.2Single Sampling 91.4 89.6 90.5p-NormPush Pairwise Sampling 91.2 90.8 91.0Table 1.
Effect of Pairwise Sampling6.3 Overall PerformanceIn Table 2 we report the overall performance forthese three algorithms.
All of them achieved im-provements on the baseline name tagger.
MaxEntyields the highest precision, while p-Norm PushRanking with p2 = 16 yields the highest recall.A larger value of ?p?
encourages the p-NormPush Ranking algorithm to perform better near thetop of the ranked list.
As we discussed in section5 http://maxent.sourceforge.net/index.html545.3, we use p1 = 4 (a small ?push?)
for the first re-ranker and p2 = 16 (a big ?push?)
for the secondre-ranker.
From Table 2 we can see that p2 = 16obviously performed better than p2 = 1.
In general,we have observed that for p2 ?16, larger p2 corre-lates with better results.Model P R FBaseline  87.4 87.6 87.5MaxEnt-Rank 91.8 90.6 91.2SVMRank 89.5 90.1 89.8p-Norm Push Ranking (p2 =16) 91.2 90.8 91.0p-Norm Push Ranking(p2 =1, RankBoost)89.3 89.7 89.5Table 2.
Overall PerformanceThe improved NE results brought better per-formance for the subsequent stages of informationextraction too.
We use the NE outputs from Max-Ent-Ranker as inputs for coreference resolver andrelation tagger.
The ACE value6 of entity detec-tion (mention detection + coreference resolution)is increased from 73.2 to 76.5; the ACE value ofrelation detection is increased from 34.2 to 34.8.6.4 Effect of Cross-document InformationAs described in Section 4.3, our algorithm incor-porates cross-document coreference information.The 100 texts in the test set were first clusteredinto 28 topics (clusters).
We then apply cross-document coreference on each cluster.
Comparedto single document coreference, cross-documentcoreference obtained 0.5% higher F-Measure, us-ing MaxEnt-Ranker, improving performance for15 of these 28 clusters.These clusters were then extended by selecting84 additional related texts from a corpus of 15,000unlabeled Chinese news articles (using a cross-entropy metric to select texts).
24 clusters gavefurther improvement, and an overall 0.2% furtherimprovement on F-Measure was obtained.6.5 EfficiencyModel Training TestMaxEnt-Rank 7 hours 55 minutesSVMRank 48 hours 2 hoursp-Norm Push Ranking 3.2 hours 10 minutesTable 3.
Efficiency Comparison6 The ACE04 value scoring metric can be found at:http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdfIn Table 3 we summarize the running time ofthese three algorithms in our application.7 DiscussionWe have shown that the other components of anIE pipeline can provide information which cansubstantially improve the performance of an NEtagger, and that these improvements can be real-ized through a variety of re-ranking algorithms.MaxEnt re-ranking using binary sampling and p-Norm Push Ranking proved about equally effec-tive.7  p-Norm Push Ranking was particularly ef-ficient for decoding (about 10 documents /minute), although no great effort was invested intuning these procedures for speed.We presented methods to handle cross-sentenceinference using staged re-ranking and to incorpo-rate additional evidence through document clus-tering.An N-best / re-ranking strategy has proven ef-fective for this task because with relatively smallvalues of N we are already able to include highly-rated hypotheses for most sentences.
Using thevalues of N we have used throughout (dependenton the margin of the baseline HMM, but neverabove 30), the upper bound of N-best performance(if we always picked the top-scoring hypothesis)is 97.4% recall, 96.2% precision, F=96.8%.Collins (2002) also applied re-ranking to im-prove name tagging.
Our work has addressed bothname identification and classification, while hisonly evaluated name identification.
Our re-rankerused features from other pipeline stages, while hiswere limited to local features involving lexicalinformation and 'word-shape' in a 5-token window.Since these feature sets are essentially disjoint, itis quite possible that a combination of the twocould yield even further improvements.
His boost-ing algorithm is a modification of the method in(Freund et al, 1998), an adaptation of AdaBoost,whereas our p-Norm Push Ranking algorithm canemphasize the hypotheses near the top, matchingour objective.Roth and Yih (2004) combined informationfrom named entities and semantic relation tagging,adopting a similar overall goal but using a quitedifferent approach based on linear programming.7 The features were initially developed and tested using the MaxEnt re-ranker,so it is encouraging that they worked equally well with the p-Norm PushRanker without further tuning.55They limited themselves to name classification,assuming the identification given.
This may be anatural subtask for English, where capitalization isa strong indicator of a name, but is much less use-ful for Chinese, where there is no capitalization orword segmentation, and boundary errors on nameidentification are frequent.
Expanding their ap-proach to cover identification would have greatlyincreased the number of hypotheses and madetheir approach slower.
In contrast, we adjust thenumber of hypotheses based on the margin in or-der to maintain efficiency while minimizing thechance of losing a high-quality hypothesis.In addition we were able to capture selectionalpreferences (probabilities of semantic types asarguments of particular semantic relations ascomputed from the corpus), whereas Roth and Yihlimited themselves to hard (boolean) type con-straints.AcknowledgmentThis material is based upon work supported by theDefense Advanced Research Projects Agency un-der Contract No.
HR0011-06-C-0023, and the Na-tional Science Foundation under Grant IIS-00325657 and a postdoctoral research fellowship.Any opinions, findings and conclusions expressedin this material are those of the authors and do notnecessarily reflect the views of the U. S. Govern-ment.ReferencesDaniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance Learning Name-finder.
Proc.ANLP1997.
pp.
194-201.
Washington, D.C.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt DiscriminativeReranking.
Proc.
ACL2005.
pp.
173-180.
Ann Arbor,USADavid Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
Proc.ACL2005.
pp.
263-270.
Ann Arbor, USAMichael Collins.
2002.
Ranking Algorithms forNamed-Entity Extraction: Boosting and the VotedPerceptron.
Proc.
ACL 2002. pp.
489-496Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
Proc.ACL2002.
pp.
263-270.
Philadelphia, USAMichael Collins and Terry Koo.
2003.
DiscriminativeReranking for Natural Language Parsing.
Journal ofAssociation for Computational Linguistics.
pp.
175-182.Yoav Freund, Raj Iyer, Robert E. Schapire and YoramSinger.
1998.
An efficient boosting algorithm forcombining  preferences.
Machine Learning: Pro-ceedings of the Fifteenth International Conference.pp.
170-178Ralph Grishman and Beth Sundheim.
1996.
Messageunderstanding conference - 6: A brief history.
Proc.COLING1996,.
pp.
466-471.
Copenhagen.James Henderson and Ivan Titov.
2005.
Data-DefinedKernels for Parse Reranking Derived from Probabil-istic Models.
Proc.
ACL2005.
pp.
181-188.
Ann Ar-bor, USA.Heng Ji and Ralph Grishman.
2005.
Improving NameTagging by Reference Resolution and Relation De-tection.
Proc.
ACL2005.
pp.
411-418.
Ann Arbor,USA.Heng Ji and Ralph Grishman.
2006.
Analysis and Re-pair of Name Tagger Errors.
Proc.
ACL2006(POSTER).
Sydney, Australia.Thorsten Joachims.
1998.
Making large-scale supportvector machine learning practical.
Advances in Ker-nel Methods: Support Vector Machine.
MIT Press.Taku Kudo, Jun Suzuki and Hideki Isozaki.
2005.Boosting-based Parse Reranking Derived fromProbabilistic Models.
Proc.
ACL2005.
pp.
189-196.Ann Arbor, USA.John Platt, Nello Cristianini, and John Shawe-Taylor.2000.
Large margin dags for multiclass classifica-tion.
Advances in Neural Information ProcessingSystems 12. pp.
547-553Dan Roth and Wen-tau Yih.
2004.
A Linear Program-ming Formulation for Global Inference in NaturalLanguage Tasks.
Proc.
CONLL2004.
pp.
1-8Dan Roth and Wen-tau Yih.
2002.
Probabilistic Rea-soning for Entity & Relation Recognition.
Proc.COLING2002.
pp.
835-841Cynthia Rudin.
2006.
Ranking with a p-Norm Push.Proc.
Nineteenth Annual  Conference on Computa-tional Learning Theory (CoLT 2006), Pittsburgh,Pennsylvania.Libin Shen and Aravind K. Joshi.
2003.
An SVMBased Voting Algorithm with Application to ParseReRanking.
Proc.
HLT-NAACL 2003 workshop onAnalysis of Geographic References.
pp.
9-16Libin Shen and Aravind K. Joshi.
2004.
Flexible Mar-gin Selection for Reranking with Full Pairwise Sam-ples.
Proc.IJCNLP2004.
pp.
446-455.
Hainan Island,China.56
