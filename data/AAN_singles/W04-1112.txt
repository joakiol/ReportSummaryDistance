Chinese Term Extraction from Web Pages Based on Compound wordProductivityHiroshi NakagawaInformation Technology Center, TheUniversity of Tokyo7-3-1 Hongou, BunkyoTokyo, JAPAN, 113-0033nakagawa@dl.itc.u-tokyo.ac.jpHiroyuki Kojima*, Akira Maeda**Faculty of Economics, The Universityof Tokyo7-3-1 Hongou, BunkyoTokyo, JAPAN, 113-0033* kojima@e.u-tokyo.ac.jp,**maeda@lib.u-tokyo.ac.jpAbstractIn this paper, we propose an automatic termrecognition system for Chinese.
Our idea isbased on the relation between a compoundword and its constituents that are simple wordsor individual Chinese character.
Moreprecisely, we basically focus on how manywords/characters adjoin the word/character inquestion to form compound words.
We alsotake into account the frequency of term.
Weevaluated word based method and characterbased method with several Chinese Web pages,resulting in precision of 75% for top tencandidate terms.1 IntroductionAutomatic term recognition, ATR in short, aimsat extracting domain specific terms from a corpusor Web pages.
Domain specific terms are termsthat express the concept specifically defined in thegiven domain.
They are required to have a uniquemeaning in order for efficient communicationabout the topic of the domain.
It is, however,difficult to decide automatically whether they areunique.
So we put this issue aside.
In terms offeasibility, their grammatical status is important,for instance part of speeches.
Although they arenot necessarily confined to simple words where?simple word?
means a word which could not befurther divided into shorter and more basic words,the majority of them are actually compound words,.Thus, we here focus on both of simple andcompound words.In terms of text length, even one Web pagewhich is not long gives us a number of domainspecific vocabulary like ?national library?, ?librarypolicy?
if the Web page is about libraries.
If weexpand domain specific terms to this extent, thebig portion of domain specific terms are compoundwords.
Obviously, the majority of compoundwords consist of relatively small number of distinctsimple words.
In this situation, it is natural to payattention to the relation among compound wordsand their constituent simple words.
(Kageura and Umino 1996) proposed animportant feature of domain specific terms calledtermhood which refers to the degree that alinguistic unit is related to a domain-specificconcept.
Presumably, it is necessary to develop anATR method that calculates termhood of each termcandidate extracted from a domain corpus thatusually consists of a number of documents.
Manyworks of ATR use statistics of term candidatedistribution in a corpus such as term frequency tocalculate the termhood of every term candidate.This frequency based methods, however, heavilydepend on the size of corpus.
Thus we do notexpect a good result if we extract domain specificterms from one or a few Web pages.
If we shift ourfocus from a corpus based statistics like frequencyto term space that consists of all term candidates,we expect better result of extracted terms evenfrom one Web page because of the followingreason: A set of term candidates has its ownstructure like relations between compound wordsand their constituent simple words as stated before.The statistical information about these relationscomes from more microscopic structure than termfrequency.
Thus, if we utilize more informationfrom term space, it is reasonable to expect betterperformance in extracting from a small text likeone Web page.
Without this kind of information,we will be suffering from the shortage ofinformation for ATR.Now look at frequency based information andinformation inherent with term space more closely.Even though several kinds of statistics about actualuse in a corpus such as term frequency give a goodapproximation of termhood.
They are notnecessarily meanings in a writer's mind.
On thecontrary, the statistics of term space can reflect themeaning in a writer?s mind because it is up to awriter?s decision how to make a compound wordterm to express a complicated concept usingsimple word terms as its components.
Moreprecisely, if a certain simple word, say N,expresses the basic concept of a domain that thedocument treats, the writer of the document, weexpect, uses N not only many times but in variousways.
One of typical way of this kind is that he/shecomposes quite a few compound words using Nand uses these compound words in documentshe/she writes.
For this reason, we have to focus onthe relation among simple words and compoundwords when pursuing new ATR methods.One of the attempts to make use of this relationhas been done by Nakagawa and Mori (2003).Their method is based on the number of distinctsimple words that come left or right of a simpleword term to make up compound word terms.
Inthis paper, we apply their method to deal with Webpages written in Chinese.In this paper, section 2 gives the background ofATR methods.
In section 3 we introduce ATRmethod developed by Nakagawa and Mori(2003).Section 4, 5 and 6 are for how to apply theirmethod to Chinese language and evaluation of twoproposed method: 1) Word based method usingChinese morphological analyzer ICTCLAS(Zhang,Yu, Xiong and Liu.
2003), 2) Stop character basedmethod.2 Background2.1 Typical Procedures of Automatic TermRecognitionAn ATR procedure consists of two proceduresin general.
The first one is a procedure ofextracting term candidates from a corpus.
Thesecond procedure is to assign each term candidatea score that indicates how likely the term candidateis a term to be recognized.
Then all candidates areranked according to their scores.
In the remainingpart of this section, we describe the background ofa candidate extraction procedure and a scoringprocedure respectively.2.2 Candidates ExtractionIn term candidates extraction from the given textcorpus, we mainly focus on compound words aswell as simple words.
To extract compound wordswhich are promising term candidates and at thesame time to exclude undesirable strings such as?is a?
or ?of the?, the frequently used method is tofilter out the words that are the member of a stop-word-list.The structure of complex term is anotherimportant factor for automatic term candidateextraction.
A syntactic structure that is the result ofparsing is focused on in many works.
Since wefocus on these complex structures, the first task inextracting term candidates is a morphologicalanalysis including part of speech (POS) tagging.There are no explicit word boundary marker inChinese, we first have to do morphologicalanalysis which segments out words from asentence and does POS tagging at the same time.After POS tagging, the complex structuresmentioned above are extracted as term candidates.Previous studies have proposed many promisingways for this purpose, for instance, Smadja andMcKeown (1990), and Frantzi and Ananiadou(1996) tried to treat more general structures likecollocations.2.3 ScoringThe next step of ATR is to assign each termcandidate its score in order to rank them indescending order of termhood.
Many researchershave sought the definition of term candidate?sscore which approximates termhood.
In fact, manyof those proposals make use of statistics of actualuse in a corpus such as term frequency which is sopowerful and simple that many researchers directlyor indirectly have used it.
The combination of termfrequency and inverse document frequency is alsowell studied i.e.
(Uchimoto et al2000), (Fukushigeand Noguchi 2000).
On the other hand, severalscoring methods that are neither directly norheavily based on frequency of term candidateshave been proposed.
Among those, Ananiadou etal.
proposed C-value (Frantzi and Ananiadou1996) which counts how independently the givencompound word is used in the given corpus.Hisamitsu (2000) proposes a way to measuretermhood which estimates how far the documentcontaining given term is different from thedistribution of documents not containing the giventerm.
However, the method proposed by Nakagawaand Mori (2003) outperforms these methods interms of NTCIR1 TMREC task(Kageura, et al1999).2.4 Chinese Term ExtractionAs for Chinese language NLP, very many worksabout word segmentation were published i.e.
(Maand Xia 2003).
Nevertheless the term ?Termextraction?
has not yet been used for Chinese NLP,key words extraction have been a target for a longtime.
For instance, key words extraction from newsarticles (Li.
et al 2003) is the recent result whichuses frequency and length of character string forscoring.
Max-duplicated string based method(Yang and Li.
2002) is also promising.
In spite ofprevious research efforts, there have been noattempt so far to apply the relation between simpleand compound words to Chinese term extraction,and that is exactly what we propose in this paper.3 Scoring methods with Simple wordBigrams3.1 Simple word BigramsThe relation between a simple word andcomplex words that include the simple word isvery important in terms of term space structure.Nevertheless, to my knowledge, this relation hasnot been paid enough attention so far except for themethod proposed by Nakagawa and Mori (2003).In this paper, taking over their works, we focus oncompound words among the various types ofcomplex terms.
In technical documents, themajority of domain-specific terms are noun phrasesor compound words consisting of small sizevocabulary of simple words.
This observationleads to a new scoring methods that measures howmany distinct compound words contain the simpleword in question as their part in a given documentor corpus.
Here, suppose the situation wheresimple word: N occurs with other simple words asa part of many compound words shown in Figure 1where [N M] means bigram of noun N and M.[LN1  N] (#L1)           [N  RN1](#R1)[LN2  N] (#L2)           [N  RN2](#R2):                              :[LNn  N](#Ln)           [N  RNm](#Rm)Figure 1.
Noun Bigram and their FrequencyIn Figure 1, [LNi  N] (i=1,..,n) and [N  RNj](j=1,...,m) are simple word bigrams which make (apart of) compound words.
#Li and #Rj (i=1,..,n andj=1,..,m) mean the frequency of the bigram [LNiN] and [N RNj] in the corpus respectively.
Notethat since we depict only bigrams, compoundwords like [LNi N RNj]  which contains [LNi  N]and/or [N RNj] as their parts might actually occurin a corpus.
Note that this noun trigram might be apart of longer compound words.
We show anexample of a set of noun bigrams.
Suppose that weextract compound words including ?trigram?
asterm candidates from a corpus as shown in thefollowing example.Example 1.trigram statistics, word trigram, class trigram, wordtrigram, trigram acquisition, word trigram statistics,character trigramThen, noun bigrams consisting of a simple word?trigram?
are shown in Figure 2 where the numberbetween ( and ) shows the frequency in the corpus.word  trigram (3)      trigram statistics (2)class trigram (1)      trigram acquisition (1)character trigram(1)Figure 2.
An example of noun bigramNow we focus on and utilize simple wordbigrams to define the scoring function.
Note thatwe are only concerned with simple word bigramsand not with a simple word per se because, asstated before, we are concerned with the relationbetween a compound word and its componentsimple words.3.2 Scoring Function3.2.1 Score of simple wordSince there are infinite number of scoringfunctions based on [LNi N] or [N RNj], we hereconsider the following simple but representativescoring functions.#LDN(N) and #RDN(N) : These are the numberof distinct simple words which directly precede orsucceed N. These coincide with ?n?
and ?m?
inFigure 1 respectively.
For instance, in an exampleshown in Figure 2, #LDN(trigram)=3,#RDN(trigram)=2.Using #LDN and #RDN we define LN(N) andRN(N): These are based on the number ofoccurrence of each noun bigram, and defined for[LNi N] and [N RNj] as follows respectively.
?==LDN(N)#1iLi)(#LN(N)                             (1)?==RDN(N)#1jRj)(#RN(N)                             (2)LN(N) and RN(N) are the frequencies of nounsthat directly precede or succeed N. For instance, inan example shown in Figure 2, LN(trigram)=5, andRN(trigram)=3.Let?s think about the background of thesescoring functions.
#LDN(N) and #RDN(N), wherewe do not take into account the frequency of eachnoun bigram but take into account the number ofdistinct nouns that adjoin to N to make compoundwords.
That indicates how linguistically anddomain dependently productive the noun:N is in agiven corpus.
That means that if N presents a keyand/or basic concept of the domain treated by thecorpus, writers in that domain work out manydistinct compound words with N to express morecomplicated concepts.
On the other hand, as forLN(N) and RN(N), we also focus on frequency ofeach noun bigram as well.
In other words, statisticbias in actual use of noun:N is, this time, one ofour main concern.
For example, in Figure 2,LN(trigram,2)=11, and RN(trigram,2)=5.
Inconclusion, since LN(N) and RN(N) convey moreinformation than #LDN(N) and #RDN(N), weadopt LN(N) and RN(N) in this research.3.2.2 Score of compound wordsThe next thing to do is expanding those scoringfunctions for simple word to the scoring functionsfor compound words.
We adopt a geometric meanfor this purpose.
Now think of a compound word :CN = N1 N2?N L, where Ni (i= 1,.., L) is a simpleword.
Then a geometric mean: LR of CN isdefined as follows.L21L1iii 1))1)(RN(N)(LN(N(CN)LR???????
?++= ?=(3)For instance, if we use LN(N) and RN(N) inexample 1, LR(trigram) = )15()13( +?+  = 4.90.LR does not depend on the length of CN where?length?
means the number of simple words thatconsist of CN.
This is because since we have notyet had any idea about the relation between theimportance of a compound word and a length ofthe compound word, it is fair to treat all compoundwords, including simple words, equally no matterhow long or short each compound word is.3.2.3 Combining LR and Frequency of NounsWe still have not fully utilized the informationabout statistics of actual use in a corpus in thebigram based methods described in 3.2.1 and 3.2.2.Among various kinds of information about actualuse, the important and basic one is the frequency ofsingle-and compound words that occurindependently.
The term ?independently?
meansthat the left and right adjacent words are not nouns.For instance, ?word patterns?
occurs independentlyin ?we use the word patterns which occur in thissentence.?
Since the scoring functions proposed in3.2.1 is noun bigram statistics, the number of thiskind of independent occurrences of nounsthemselves have not been used so far.
If we takethis information into account, the better results areexpected.
Thus, if a simple word or a compoundword occurs independently, the score of the nounis simply multiplied by the number of itsindependent occurrences.
We call this new scoringfunction as FLR(CN) which is defined as follows.if N occurs independentlythen f(CN)(CN)LR(CN)LRF ?=where f(CN) means the number of independentoccurrences of noun CN                                (4)4 Term Extraction for Chinese based onMorphological AnalysisIf we try to apply the scoring method proposedin section 3 directly to a Chinese text, every wordshould be POS tagged because we extract multi-word unit of several types of POS tag sequences ascandidates of domain specific terms.
For this weneed a Chinese morphological analyzer becauseChinese is an agglutinative language.
Actually, weuse Chinese morphological analyzer:ICTCLAS(Zhang and Liu 2004).
As termcandidates, we extract compound word: MWUhaving the following POS tag sequence expressedin (5).
A multi-word-unit: MWU is defined by thefollowing CFG rules where the right hand sides areexpressed as a regular expression.MWU <-- [ag a]* [ng  n  nr  ns  nt  nz  nx  vnan  i  j]+MWU <-- MWU?b [ng  n  nr  ns  nt  nz  nx  vnan  i  j]+MWU <-- [ag a]+ [u k] MWUMWU <-- MWU (u|k|he-2|yu-3) MWU(5)where ?ag?, ?a?, ?n?
?, ?u?
are all tags used inICTCLAS.Roughly speaking (5) means an adjectivefollowed by the repetition of [adjective nounparticle] followed by a noun.
The problem is theambiguity of POS tagging because the same wordis very often used verb as well as noun.
In addition,unknown words like newly appeared proper namesalso impairs the accuracy.
Due to this problemcaused by morphological analyzer, the accuracy isdegraded.Once we segment out word sequencesconforming the above POS tag sequences, wecalculate LN and RN of each component word.
Incalculation of LN and RN, a word whose POS is c,u or k is omitted.
In other words, if a wordsequence ?w1 w2 w3?
where POS of w2 is c u or k,then we calculate RN of w1 and LN of w3 byregarding the word sequence as ?w1 w3.
?Then we combine LN and RN of each word tocalculate FLR by definition of (3) and (4) to sortall extracted candidates in descending order ofFLR.We apply the proposed methods to 30 Webpages from People?s Daily news.
The areas aresocial, international and IT related news.
Theaverage length is 592.6 characters.
Firstly, weextract relevant terms by hand from each newsarticle and use it as the gold standard.
The averagenumber of gold standard terms per one newsparticle is 15.9 words.
Secondly, we extract termsfrom each news article and sort them in descendingorder by the proposed method and evaluate themby a precision of top N terms defined as follows.CT(K)= 1  if Kth term is one of the goldstandard terms.0   otherwiseKiCTKprecisionKi?
==1)()(                             (6)where N is the number of the gold standardterms, and in our experiment, N=20.
Precision(K),where K=1,..,20, are shown in Figure 3 as ?Strict.
?We also use another precision rate precision?which is not strict and defined as follows.CTpart(K)= 1 if one of gold standard terms.is a part of Kth term0  otherwiseKiCTpartKprecisionKi?
==1)()('                    (7)These are also shown in Figure 3 as ?Partly.
?Figure 3.
Strict and partly precision of word basedextraction method.From Figure 3, we see that If we pick up the tenhighest ranked terms, about 75% of them meet thegold standard.
The case we loosen the definition ofprecision shows better than the strict case of (6)but the difference is not so large.
That means thatthe proposed word based ranking method worksvery well to extract important Chinese terms fromnews articles.5 Character based Term ExtractionThere are several reasons why we would like todevelop a term extraction system withoutmorphological analyzer.The first reason is that the accuracy ofmorphological analyzer is, in spite of the greatadvancement of these years, still around 95%(GuoDong and Jian 2003).The second reason is that there possibly existterminologies with unexpected POS sequences.
Ifwe deal only with academic papers or technicaldocuments, we expect POS sequences ofterminologies with high accuracy.
However, if weconsider terminology extraction from Web pages,the possibility of unexpected POS sequence mayrise.The third reason is language independency.Currently proposed and/or used morphologicalanalyzers heavily depend either upon thesophisticated linguistic knowledge about the targetlanguage or upon a big size corpus of the targetlanguage if machine learning is employed.
Theselinguistic resources, however, are not alwaysavailable.Due to these reasons, we also developed termcandidate extraction system which does not use amorphological analyzer.
Instead of morphologicalanalyzer, we try to employ a stop word list.
InChinese, as stop words, we find many characterunigrams and bigrams because one Chinesecharacter conveys larger amount of informationthan a character of Latin alphabet.
They are partlyshown in Appendix A.As term candidates, we simply extract characterstrings between two stop words that are nearesteach other within a sentence.
Obviously, thecharacter strings thus extracted are not necessarilymeaningful compound words.
Therefore we cannotdirectly use these strings as words to calculate LNand RN function.
Back to the idea that Chinesecharacters are ideograms, we come up to the ideathat we calculate LN and RN of each characterappearing within every character strings extractedas candidates.
An example is shown in Figure 4.LN(zuo-4)=3                  RN(zuo-4)=2Figure 4.
LN and RN of Chinese character zuo-4In calculation of LN and RN, we neglectcharacters whose POS are c ,u or k as same as wedid in morphological analyzer based method.Once we calculate LN and RN of each character,FLR of every character string is calculated asdefined by (3) and (4) to sort them in descendingorder of FLR.Actually this idea is very similar with left andright entropy used to extract two character Chinesewords from a corpus (Luo and Sun.
2003).However what we would like to extract is a set oflonger compound words or even phrases used in aWeb page.
Moreover we only use the Web pageand do not use any other language resources suchas a big corpus at all due to the reason describedabove in this section.We evaluate the proposed character basedextraction method against the same Web pagesfrom People?s Daily news used in MorphologicalAnalysis based method described in Section 4.
Wealso use the same gold standard terms described inSection 4 for evaluation.
The strict and partlyprecision defined by (6) and (7) are used.
Theresult is shown in Figure 5.Figure 5.
Strict and partly precision of character basedextraction method.Comparing Figure 3 with Figure 5, apparentlythe result of extracted terms of word based methodis better than that of character based method.However, it does not necessarily mean that thecharacter based term extraction is inferior.If you take a glance at the stop word list ofAppendix A, it seems that several of the stopwords are selected mainly from words in auxiliaryverbs, pronouns, adverbs, particles, prepositions,conjunctions, exclamations, onomatopoeic wordsand punctuation marks.
However, in reality, ourselection is based rather on meaning, usage andgenerally frequency of use than parts of speech.Thus some of them are not function words butcontent words in order to exclude non-domain-specific words.
Actually, the stop words are notonly character unigram but character bigram.Because Chinese character is ideograph and eachcharacter may have plural meanings, it is difficultonly to use character unigram as a stop word inChinese.Our method based on these viewpoints resultedin getting an interesting consequence.
We show anexample of news article and extracted terms from itby this method in Appendix B and Appendix C.This news article is entitled ?The Culture ofTibetan Web Site is formally created.?
Let?s take alook at an underlined sentence in this short articleand underlined terms extracted from there.
Thissentence says: According to the introduction, TheCulture of Tibetan Web Site is a site of specialpure culture for the purpose of ?investigating theessence of Tibetan culture, showing the scale ofTibetan culture and raising the spirit of Tibetanculture?.
In the case of method based on stop wordlist, we can extract compound term of?investigating the essence of Tibetan culture?, ?showing the scale ofTibetan culture ( )?, ?raising thespirit of Tibetan culture ( )?
andso on from this sentence.
On the contrary, by theterm extraction method based on morphologicalanalysis, gerund , for example, ?showing( )?and ?raising ( ), can not be extracted.We said that the majority of domain specificterms are noun phrases or compound wordsconsisting of small size vocabulary of simplewords as stated in section 3.
So we especiallyhave paid attention to relation among nouns.However most of Chinese nouns can also beused as verbs.
Moreover inflection of Chineseverbs can hardly be recognized visually.
It isdifficult to distinguish verb from noun bymorphological analysis.
Certainly ICTCLASclassifies the character that has meaning of bothverb and noun into the category of vn (verb andnoun).
But gerunds and verbal noun infinitives arenot contained in vn.
For instance, ?
?
means notonly ?write a letter?
but ?writing letter.?
Thus wehave to pay attention to verbs in Chinese too.
Onlyby tuning up stop word list, we can take gerundsand verbal noun infinitives into account to someextent.
Appendix C shows one of the evidence ofthis observation.6 ConclusionIn this paper, we apply automatic termrecognition system based on FLR proposed byNakagawa and Mori (2003) to Chinese Web pagesbecause the term extraction from small text likeone Web page is the future oriented topic.
Weproposed two methods: word based and characterbased extraction and ranking using the compoundword productivity of simple words.
Since theaccuracies of term recognition are around 60% fortop 1,000 term candidates in NTCIR TMRECtask(Kageura et al1999), the result of 75%accuracy of top ten candidates is a good start.ReferencesAnaniadou, S. 1994.
A Methodology for AutomaticTerm Recognition.
In Proceedings of 14thInternational Conference on ComputationalLinguistics :1034 - 1038.GuoDong Zhou and Jian Su.
2003.
A Chinese EfficientAnalyser Integrating Word Segmentation, Part-Of-Speech Tagging, Partial Parsing and Full Parsing, InProceedings of The Second SIGHAN Workshop, onChinese Language Processing .ACL2003 :78-83Frantzi, T.K.
and Ananiadou, S. 1996.
Extracting nestedcollocations.
In Proceedings of 16th InternationalConference on Computational Linguistics :41-46.Fukushige, Y. and N. Nogichi.
2000.
Statistical andlinguistic approaches to automatic term recognitionNTCIR experiments at Matsushita.
Terminology6(2) :257-286Hua-PingZhang, Hong-KuYu, De-Yi Xiong and QunLiu.
2003.
HHMM-based Chinese Lexical AnalyzerICTCLAS.
In Proceedings of The Second SIGHANWorkshop, on Chinese LanguageProcessing .ACL2003 :184-187Hisamitsu, T, 2000.
A Method of Measuring TermRepresentativeness.
In Proceedings of 18thInternational Conference on ComputationalLinguistics :320-326.Kageura, K. and Umino, B.
1996.
Methods of automaticterm recognition: a review.
Terminology  3(2) :259-289.Kageura, K. et al 1999.
TMREC Task: Overview andEvaluation.
In Proceedings of the First NTCIRWorkshop on Research in Japanese Text Retrievaland Term Recognition :411-440.Shengfen Luo and Maosong Sun.
2003.
Two-CharacterChinese Word Extraction Based on Hybrid ofInternal and Contextual Measures.
In Proceedings ofThe Second SIGHAN Workshop, on ChineseLanguage Processing .ACL2003 :24-30Qing Ma and Fei Xia.
2003.
Proceedings of The SecondSIGHAN Workshop, on Chinese LanguageProcessing .ACL2003, SapproSujian Li,Houfeng Wang,Shiwen Yu and ChengshengXin.
2003.
News-Oriented Automatic ChineseKeyword Indexing.
In Proceedings of The SecondSIGHAN Workshop, on Chinese LanguageProcessing .ACL2003 :92-97Nakagawa, H. and Tatsunori Mori.
2003.
AutomaicTerm Recognition based on Statistics of Compoundwords and their Components, Terminology 9(2) :201-219Smadja, F.A.
and Mckeown, K.R.
1990.
Automaticallyextracting and representing collocations for languagegeneration.
In Proceedings of the 28th AnnualMeetings of the Association for ComputationalLinguistics :252-259.Uchimoto, K., S.Sekine, M. Murata, H.Ozaku and H.Isahara.
2000.
Term recognition using corpora fromdifferent fields.
Terminology 6(2) :233-256Wenfeng Yang and Xing Li.
2002.
Chinese keywordextraction based on max-duplicated strings of thedocuments.
In Proceedings of the 25th annualinternational ACM SIGIR conference on Researchand develop-ment in information retrieval, pp.
439-440.Kevin Zhang and Qun Liu.
2004.
ICTCLAS.http://www.nlplab.cn/zhangle/morphix-nlp /manual/node12.htmlAppendix A: A part of stop word listAppendix B: An example of news articleAppendix C: Terms extracted from AppendixB.Word Based (Top 10 terms with score ofequation (4) )Character Based(Top 11 terms with scoreequation (4) )
