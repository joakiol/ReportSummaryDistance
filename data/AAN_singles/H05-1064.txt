Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 507?514, Vancouver, October 2005. c?2005 Association for Computational LinguisticsHidden?Variable Models for Discriminative RerankingTerry KooMIT CSAILmaestro@mit.eduMichael CollinsMIT CSAILmcollins@csail.mit.eduAbstractWe describe a new method for the repre-sentation of NLP structures within rerank-ing approaches.
We make use of a condi-tional log?linear model, with hidden vari-ables representing the assignment of lexi-cal items to word clusters or word senses.The model learns to automatically makethese assignments based on a discrimina-tive training criterion.
Training and de-coding with the model requires summingover an exponential number of hidden?variable assignments: the required sum-mations can be computed efficiently andexactly using dynamic programming.
Asa case study, we apply the model toparse reranking.
The model gives an F?measure improvement of ?
1.25% be-yond the base parser, and an ?
0.25%improvement beyond the Collins (2000)reranker.
Although our experiments arefocused on parsing, the techniques de-scribed generalize naturally to NLP struc-tures other than parse trees.1 IntroductionA number of recent approaches in statistical NLPhave focused on reranking algorithms.
In rerank-ing methods, a baseline model is used to generate aset of candidate output structures for each input intraining or test data.
A second model, which typi-cally makes use of more complex features than thebaseline model, is then used to rerank the candidatesproposed by the baseline.
Reranking approacheshave given improvements in accuracy on a numberof NLP problems including parsing (Collins, 2000;Charniak and Johnson, 2005), machine translation(Och and Ney, 2002; Shen et al, 2004), informa-tion extraction (Collins, 2002), and natural languagegeneration (Walker et al, 2001).The success of reranking approaches dependscritically on the choice of representation used by thereranking model.
Typically, each candidate struc-ture (e.g., each parse tree in the case of parsing) ismapped to a feature?vector representation.
Previouswork has generally relied on two approaches to rep-resentation: explicitly hand?crafted features (e.g., inCharniak and Johnson (2005)) or features definedthrough kernels (e.g., see Collins and Duffy (2002)).This paper describes a new method for the rep-resentation of NLP structures within reranking ap-proaches.
We build on the intuition that lexical itemsin natural language often fall into word clusters (forexample, president and chairman might belong tothe same cluster) or fall into distinct word senses(e.g., bank might have two distinct senses).
Ourmethod involves a hidden?variable model, wherethe hidden variables correspond to an assignmentof words to either clusters or word?senses.
Lexicalitems are automatically assigned their hidden valuesusing unsupervised learning within a discriminativereranking approach.We make use of a conditional log?linear modelfor our task.
Formally, hidden variables withinthe log?linear model consist of global assignments,where a global assignment entails an assignment ofevery word in the sentence to some hidden clusteror sense value.
The number of such global assign-ments grows exponentially fast with the length ofthe sentence being processed.
Training and decod-ing with the model requires summing over the ex-ponential number of possible global assignments, amajor technical challenge in our model.
We showthat the required summations can be computed ef-ficiently and exactly using dynamic?programmingmethods (i.e., the belief propagation algorithm forMarkov random fields (Yedidia et al, 2003)) undercertain restrictions on features in the model.Previous work on reranking has made heavy useof lexical statistics, but has treated lexical items asatoms.
The motivation for our method comes fromthe observation that statistics based on lexical itemsare critical, but that these statistics suffer consid-erably from problems of data sparsity and word?507sense polysemy.
Our model has the ability to allevi-ate data sparsity issues by learning to assign wordsto word clusters, and can mitigate problems withword?sense polysemy by learning to assign lexicalitems to underlying word senses based upon con-textual information.
A critical difference betweenour method and previous work on unsupervised ap-proaches to word?clustering or word?sense discov-ery is that our model is trained using a discriminativecriterion, where the assignment of words to clustersor senses is driven by the reranking task in question.As a case study, in this paper we focus on syn-tactic parse reranking.
We describe three modeltypes that can be captured by our approach.
Thefirst method emulates a clustering operation, wherethe aim is to place similar words (e.g., president andchairman) into the same cluster.
The second methodemulates a refinement operation, where the aim is torecover distinct senses underlying a single word (forexample, distinct senses underlying the noun bank).The third definition makes use of an existing ontol-ogy (i.e., WordNet (Miller et al, 1993)).
In this casethe set of possible hidden values for each word cor-responds to possible WordNet senses for the word.In experimental results on the Penn Wall StreetJournal treebank parsing domain, the hidden?variable model gives an F?measure improvement of?
1.25% beyond a baseline model (the parser de-scribed in Collins (1999)), and gives an ?
0.25%improvement beyond the reranking approach de-scribed in Collins (2000).
Although the experimentsin this paper are focused on parsing, the techniqueswe describe generalize naturally to other NLP struc-tures such as strings or labeled sequences.
We dis-cuss this point further in Section 6.1.2 Related WorkVarious machine?learning methods have been usedwithin reranking tasks, including conditional log?linear models (Ratnaparkhi et al, 1994; Johnson etal., 1999), boosting methods (Collins, 2000), vari-ants of the perceptron algorithm (Collins, 2002;Shen et al, 2004), and generalizations of support?vector machines (Shen and Joshi, 2003).
There havebeen several previous approaches to parsing usinglog?linear models and hidden variables.
Riezleret al (2002) describe a discriminative LFG pars-ing model that is trained on standard (syntax only)treebank annotations by treating each tree as a fullLFG analysis with an observed c-structure and hid-den f -structure.
Clark and Curran (2004) present analternative CCG parsing approach that divides eachCCG parse into a dependency structure (observed)and a derivation (hidden).
More recently, Matsuzakiet al (2005) introduce a probabilistic CFG aug-mented with hidden information at each nontermi-nal, which gives their model the ability to tailor it-self to the task at hand.
The form of our model isclosely related to that of Quattoni et al (2005), whodescribe a hidden?variable model for object recog-nition in computer vision.The approaches of Riezler et al, Clark and Cur-ran, and Matsuzaki et al are similar to our ownwork in that the hidden variables are exponentialin number and must be handled with dynamic?programming techniques.
However, they differ fromour approach in the definition of the hidden variables(the Matsuzaki et al model is the most similar).
Inaddition, these three approaches don?t use rerank-ing, so their features must be restricted to local scopein order to allow dynamic?programming approachesto training.
Finally, these approaches use Viterbior other approximations during decoding, somethingour model can avoid (see section 6.2).In some instantiations, our model effectively clus-ters words into categories.
Our approach differsfrom standard word clustering in that the cluster-ing criteria is directly linked to the reranking objec-tive, whereas previous word?clustering approaches(e.g.
Brown et al (1992) or Pereira et al (1993))have typically leveraged distributional similarity.
Inother instantiations, our model establishes word?sense distinctions.
Bikel (2000) has done previouswork on incorporating the WordNet hierarchy intoa generative parsing model; however, this approachrequires data with word?sense annotations whereasour model deals with word?sense ambiguity throughunsupervised discriminative training.3 The Hidden?Variable ModelIn this section we describe a hidden?variable modelbased on conditional log?linear models.
Each sen-tence si for i = 1 .
.
.
n in our training data has aset of ni candidate parse trees ti,1, .
.
.
, ti,ni , whichare the output of an N?best baseline parser.
Eachcandidate parse has an associated F?measure score,508indicating its similarity to the gold?standard parse.Without loss of generality, we define ti,1 to be theparse with the highest F?measure for sentence si.Given a candidate parse tree ti,j , the hidden?variable model assigns a domain of hidden val-ues to each word in the tree.
For example, thehidden?value domain for the word bank could be{bank1, bank2, bank3} or {NN1,NN2,NN3}.
De-tailed descriptions of the domains we used are givenin Section 4.1.
Formally, if ti,j spans m words thenthe hidden?value domains for each word are the setsH1(ti,j), .
.
.
,Hm(ti,j).
A global hidden?value as-signment, which attaches a hidden value to everyword in ti,j , is written h = (h1, .
.
.
, hm) ?
H(ti,j),where H(ti,j) = H1(ti,j)?
.
.
.
?Hm(ti,j) is the setof all possible global assignments for ti,j .We define a feature?based representation ?
suchthat ?
(ti,j ,h) ?
Rd is a vector of feature occur-rence counts that describes candidate parse ti,j withglobal assignment h ?
H(ti,j).
We write ?k fork = 1 .
.
.
d to denote the kth component of the vec-tor ?.
Each component of the feature vector is thecount of some substructure within (ti,j ,h).
For ex-ample, ?12 and ?101 could be defined as follows:?12(ti,j ,h) =Number of times the word theoccurs with hidden value the3and part of speech tag DT in(ti,j ,h).
?101(ti,j ,h) =Number of times CEO1 ap-pears as the subject of owns2in (ti,j ,h)(1)We use a parameter vector ?
?
Rd to define alog?linear distribution over candidate trees togetherwith global hidden?value assignments:p(ti,j ,h | si,?)
=e?
(ti,j ,h)???j?,h??H(ti,j?
)e?(ti,j?
,h?)?
?By marginalizing out the global assignments, we ob-tain a distribution over the candidate parses alone:p(ti,j | si,?)
=?h?H(ti,j)p(ti,j ,h | si,?)
(2)Later in this paper we will describe how to trainthe parameters of the model by minimizing the fol-lowing loss function?which is the negative log?likelihood of the training data?with respect to ?:L(?)
= ?
?ilog p(ti,1 | si,?
)= ?
?ilog?h?H(ti,1) p(ti,1,h | si,?
)(3)saw withPP(with)VP(saw)S(saw)a telescopeNP(telescope)the boyNP(boy)The manNP(man)sawmanThe thewithboy telescopeaFigure 1: A sample parse tree and its dependency tree.3.1 Local Feature VectorsNote that the number of possible global assignments(i.e., |H(ti,j)|) grows exponentially fast with respectto the number of words spanned by ti,j .
This posesa problem when training the model, or when calcu-lating the probability of a parse tree through Eq.
2.This section describes how to address this difficultyby restricting features to sufficiently local scope.
InSection 3.2 we show that this restriction allows effi-cient training and decoding of the model.The restriction to local feature?vectors makes useof the dependency structure underlying the parsetree ti,j .
Formally, for tree ti,j , we define the cor-responding dependency tree D(ti,j) to be a set ofedges between words in ti,j , where (u, v) ?
D(ti,j)if and only if there is a head?modifier dependencybetween words u and v. See Figure 1 for an exam-ple dependency tree.
We restrict the definition of?
in the following way1.
If w, u and v are wordindices, we introduce single?variable local featurevectors ?
(ti,j , w, hw) ?
Rd and pairwise local fea-ture vectors ?
(ti,j , u, v, hu, hv) ?
Rd.
The globalfeature vector ?
(ti,j ,h) is then decomposed into asum over the local feature vectors:?
(ti,j ,h) =?1?w?m?
(ti,j , w, hw) +?(u,v)?D(ti,j)?
(ti,j , u, v, hu, hv)(4)Notice that the second sum, over pairwise localfeature vectors, respects the dependency structureD(ti,j).
Section 3.2 describes how this decompo-sition of ?
leads to an efficient and exact dynamic?programming approach that, during training, allowsus to calculate the gradient ?L??
and, during testing,allows us to find the most probable candidate parse.In our implementation, each dimension of the lo-cal feature vectors is an indicator function signalingthe presence of a feature, so that a sum over localfeature vectors in a tree gives the occurrence count1Note that the restriction on local feature vectors only con-cerns the inclusion of hidden values; features may still observearbitrary structure within the underlying parse tree ti,j .509of features in that tree.
For instance, define?12(ti,j , w, hw) =rhw = the3 and tree ti,j assigns wordw to part?of?speech DTz?101(ti,j , u, v, hu, hv) =s(hu, hv) = (CEO1, owns2)and tree ti,j places (u, v) ina subject?verb relationship{where the notation JPK signifies a 0/1 indicator ofpredicate P .
When summed over the tree, these defi-nitions of ?12 and ?101 yield global features ?12 and?101 as given in the previous example (see Eq.
1).3.2 Training the ModelWe now describe how the loss function in Eq.
3 canbe optimized using gradient descent.
The gradientof the loss function is given by:?L??
= ?
?iF (ti,1,?)
+?i,jp(ti,j | si,?
)F (ti,j ,?
)where F (ti,j ,?)
=?h?H(ti,j)p(ti,j ,h | si,?
)p(ti,j | si,?)?
(ti,j ,h)is the expected value of the feature vector producedby parse tree ti,j .
As we remarked earlier, |H(ti,j)|is exponential in size so direct calculation of eitherp(ti,j | si,?)
or F (ti,j ,?)
is impractical.
However,using the feature?vector decomposition in Eq.
4, wecan rewrite the key functions of ?
as follows:p(ti,j | si,?)
=Zi,j?j?Zi,j?F (ti,j ,?)
=?1 ?
w ?
mhw ?
Hw(ti,j)p(ti,j , w, hw)?
(ti,j , w, hw) +?
(u, v) ?
D(ti,j)hu ?
Hu(ti,j)hv ?
Hv(ti,j)p(ti,j , u, v, hu, hv)?
(ti,j , u, v, hu, hv)where p(ti,j , w, hw) and p(ti,j , u, v, hu, hv) aremarginalized probabilities and Zi,j is the associatednormalization constant:Zi,j =?h?H(ti,j)e?
(ti,j ,h)?
?p(ti,j , w, x) =?h |hw=xp(ti,j ,h | si,?
)p(ti,j , u, v, x, y) =?h |hu=x,hv=yp(ti,j ,h | si,?
)The three quantities above can be computed with be-lief propagation (Yedidia et al, 2003), a dynamic?programming technique that is efficient2 and exact2The running time of belief propagation varies linearly withthe number of nodes in D(ti,j) and quadratically with the car-dinality of the largest hidden?value domain.when the graph D(ti,j) is a tree, which is the casein our parse reranking model.
Having calculatedthe gradient in this way, we minimize the loss usingstochastic gradient descent3 (LeCun et al, 1998).4 Features for Parse RerankingThe previous section described hidden?variablemodels for discriminative reranking.
We now de-scribe features for the parse reranking problem.
Wefocus on the definition of hidden?value domains andlocal feature vectors in the reranking model.4.1 Hidden?Value Domains and Local FeaturesEach word in a parse tree is given a domain of pos-sible hidden values by the hidden?variable model.Models with widely varying behavior can be createdby changing the way these domains are defined.
Inparticular, in this section we will see how differentdefinitions of the domains give rise to the three mainmodel types: clustering, refinement, and mappinginto a pre?built ontology such as WordNet.As illustration, consider a simple approach thatsplits each word into a domain of three word?sensehidden values (e.g., the word bank would yield thedomain {bank1, bank2, bank3}).
In this approach,each word receives a domain of hidden values thatis not shared with any other word.
The model isthen able to distinguish several different usages foreach word, emulating a refinement operation.
Analternative approach is to split each word?s part?of?speech tag into several sub?tags (e.g., bank wouldyield {NN1,NN2,NN3}).
This approach assigns thesame domain to many words; for instance, singularnouns such as bond, market, and bank all receive thesame domain.
The behavior of the model then emu-lates a clustering operation.Figure 2 shows the single?variable and pairwisefeatures used in our experiments.
The featuresare shown with hidden variables corresponding toword?specific hidden values, such as shares1 orbought3.
In our experiments, we made use of fea-tures such as those in Figure 2 in combination withthe following four definitions of the hidden?value3We also performed some experiments using the conjugategradient descent algorithm (Johnson et al, 1999).
However, wedid not find a significant difference between the performance ofeither method.
Since stochastic gradient descent was faster andrequired less memory, our final experiments used the stochasticgradient method.510bought yesterday1.5m shares in heavy tradingVBD(bought) PP(in) NP(yesterday)NP(shares)S(bought)VP(bought)PSfrag replacementsPairwise features generated for (shares1, bought3) =(shares1, bought3, Dependency: VBD, NP, VP, Right, +Adj, -CC)(shares1, bought3, Rule: VP?
VBDhead, NPmod, PP, NP)(shares1, bought3, Gpar Rule: S ?
VP ?
VBDhead, NPmod, PP, NP)Single?variable features generated for (shares1) =(shares1)(shares1, Word: shares)(shares1, POS: NN)(shares1, Word: shares, POS: NN)(shares1, Highest NT: NP)(shares1, Word: shares, Highest NT: NP)(shares1, POS: NN, Highest NT: NP)(shares1, Word: shares, POS: NN, Highest NT: NP)(shares1, Up Path: NP, VP, S)(shares1, Word: shares, Up Path: NP, VP, S)(shares1, Down Path: NP, NN)(shares1, Word: shares, Down Path: NP, NN)(shares1, Head Rule: NP ?
CD, CD, NNShead)(shares1, Word: shares, Head Rule: NP?
CD, CD, NNShead)(shares1, Mod Rule: VP ?
VBDhead, NPmod, PP, NP)(shares1, Word: shares, Mod Rule: VP?
VBDhead, NPmod, PP, NP)(shares1, Head Gpar Rule: VP ?
NP?
CD, CD, NNShead)(shares1, Word: shares, Head Gpar Rule: VP?
NP?
CD, CD, NNShead)(shares1, Mod Gpar Rule: S ?
VP?
VBDhead, NPmod, PP, NP)(shares1, Word: shares, Mod Gpar Rule: S ?
VP ?
VBDhead, NPmod, PP, NP)Figure 2: The features used in our model.
Weshow the single?variable features produced for hidden valueshares1 and the pairwise features produced for hidden values(shares1, bought3), in the context of the given parse fragment.Highest NT = highest nonterminal, Up Path = sequence of ances-tor nonterminals, Down Path = sequence of headed nonterminals,Head Rule = rules headed by the word, Mod Rule = rule in whichword acts as modifier, Head/Mod Gpar Rule = Head/Mod Rule plusgrandparent nonterminal.domains (in each case we give the model type thatresults from the definition?clustering, refinement,or pre?built ontology?in parentheses):Lexical (Refinement) Each word is split intothree sub?values.
See Figure 2 for an example offeatures generated for this choice of domain.Part?of?Speech (Clustering) The part?of?speech tag of each word is split into five sub?values.In Figure 2, the word shares would be assignedthe domain {NNS1, .
.
.
,NNS5}, and the word boughtwould have the domain {VBD1, .
.
.
, VBD5}.Highest Nonterminal (Clustering) The high-est nonterminal to which each word propagates asa headword is split into five sub?values.
In Figure 2the word bought yields domain {S1, .
.
.
, S5}, whilein yields {PP1, .
.
.
, PP5}.Supersense (Pre?Built Ontology) We borrowthe idea of using WordNet lexicographer filenamesas broad ?supersenses?
from Ciaramita and John-son (2003).
For each word, we split each of itssupersenses into three sub?supersenses.
If no su-persenses are available, we fall back to splittingthe part?of?speech into five sub?values.
For ex-ample, shares has the supersenses noun.possession,noun.act and noun.artifact, which yield the do-main {noun.possession1, noun.act1, noun.artifact1, .
.
.noun.possession3, noun.act3, noun.artifact3}.
On theother hand, in does not have any WordNet super-senses, so it is assigned the domain {IN1, .
.
.
, IN5}.4.2 The Final Feature SetsWe created eight feature sets by combining thefour hidden?value domains above with two alterna-tive definitions of dependency structures: standardhead?modifier dependencies and ?sibling dependen-cies.?
When using sibling dependencies, connec-tions are established between the headwords of ad-jacent siblings.
For instance, the head?modifierdependencies produced by the tree fragment inFigure 2 are (bought, shares), (bought, in), and(bought, yesterday), while the corresponding siblingdependencies are (bought, shares), (shares, in), and(in, yesterday).4.3 Mixed ModelsThe different hidden?variable models display vary-ing strengths and weaknesses.
We created mixturesof different models using a weighted average:log p(ti,j |si) =M?m=1?m log pm(ti,j |si,?m)?Z(si)where Z(si) is a normalization constant that can beignored, as it does not affect the ranking of parses.The ?m weights are determined through optimiza-tion of parsing scores on a development set.5 Experimental ResultsWe trained and tested the model on data from thePenn Treebank (Marcus et al, 1994).
Candidateparses were produced by an N?best version of theCollins (1999) parser.
Our training data consists ofTreebank Sections 2?21, divided into a training cor-pus of 35,540 sentences and a development corpusof 3,676 sentences.
In later experiments, we madeuse of a secondary development corpus of 1,914 sen-tences from Section 0.
Sections 22?24, containing5,455 sentences, were held out as the test set.For each of the eight feature sets described inSection 4.2, we used the stochastic gradient descent511Section 22 Section 23 Section 24 TotalLR LP LR LP LR LP LR LPC99 89.12 89.20 88.14 88.56 87.17 87.97 88.19 88.60MIX 90.43 90.61 89.25 89.69 88.46 89.29 89.41 89.87C2K 90.27 90.62 89.43 89.97 88.56 89.58 89.46 90.07MIX+ 90.57 90.79 89.80 90.27 88.78 89.73 89.78 90.29Table 1: The results on Sections 22?24.
LR = Labeled Recall,LP = Labeled Precision.method to optimize the parameters of the model.
Wecreated various mixtures of the eight models usingthe weighted?average technique described in Sec-tion 4.3, testing the accuracy of each mixture on thesecondary development set.
Our final model was amixture of three of the eight possible models: super-sense hidden values with sibling trees, lexical hid-den values with sibling trees, and highest nontermi-nal hidden values with normal head?modifier trees.Our final tests evaluated four models.
The twobaseline models are the Collins (1999) base parser,C99, and the Collins (2000) reranker, C2K.
The firstnew model is the MIX model, which is a combina-tion of the C99 base model with the three modelsdescribed above.
The second new model, MIX+, iscreated by augmenting MIX with features from themethod in C2K.
Table 1 shows the results.
TheMIX model obtains an F?measure improvement of?
1.25% over the C99 baseline, an improvement thatis comparable to the C2K reranker.
The MIX+ modelyields an improvement of ?
0.25% beyond C2K.We tested the significance of 8 comparisons cor-responding to the results in Table 1 using the signtest4: we tested MIX vs. C99 on Sections 22, 23, and24 individually, as well as on Sections 22?24 takenas a whole; we also tested MIX+ vs. C2K on these 4test sets.
Of the 8 comparisons, all showed signif-icant improvements at the level p ?
0.01 with theexception of one test, MIX+ vs. C2K on Section 24.6 Discussion6.1 Applying the Model to Other NLP TasksIn this section, we discuss how hidden?variablemodels might be applied to other NLP problems, andin particular to structures other than parse trees.
To4The input to the sign test is a set of sentences with judge-ments for each sentence indicating whether model 1 gives abetter parse than model 2, model 2 gives a better parse thanmodel 1, or models 1 and 2 give equal quality parses.
Whenusing the sign test, for each sentence in question we calculatethe F?measure at the sentence level for the two models beingcompared, deriving the required judgement from these scores.summarize the model, the major components of theapproach are as follows:?
We assume some set of candidate structures ti,j ,which are to be reranked by the model.
Each struc-ture ti,j has ni,j words w1, .
.
.
, wni,j , and each wordwk has a set Hk(ti,j) of possible hidden values.?
We assume a graph D(ti,j) for each ti,j that de-fines possible interactions between hidden variablesin the model.
We assume some definition of localfeature vectors, which consider either single hiddenvariables, or pairs of hidden variables that are con-nected by an edge in D(ti,j).The approach can be instantiated in several wayswhen applying the model to other NLP tasks.
Wehave already seen that by changing the definitionof the hidden?value domains Hk(ti,j), we can de-rive models with widely varying behavior.
In ad-dition, there is no requirement that the hidden vari-ables only be associated with words in the structure;the hidden variables could be associated with otherunits.
For example, in speech recognition hiddenvariables could be associated with phonemes ratherthan words, and in Chinese word segmentation, hid-den variables could be associated with individualcharacters rather than words.NLP tasks other than parsing involve structuresti,j that are not necessarily parse trees.
For example,in speech recognition candidates are simply strings(utterances); in tagging tasks candidates are labeledsequences (e.g., sentences labeled with part?of?speech tag sequences); in machine translation can-didate structures may be source?language/target?language pairs, along with alignment structuresspecifying the correspondence between words in thetwo languages.
Sentences and labeled sequences arein a sense simplifications of the parsing case, wherea natural choice for the underlying graph D(ti,j)would be an N th order Markov structure, where eachword depends on the previous N words.
Machinetranslation alignments are a more interesting type ofstructure, where the choice ofD(ti,j) might actuallydepend on the alignment between the two sentences.As a final note, there is some flexibility in thechoice of D(ti,j).
In the case that D(ti,j) is a treebelief propagation is exact.
In the more general casewhere D(ti,j) contains cycles, there are alternativealgorithms that are either exact (Cowell et al, 1999)or approximate (Yedidia et al, 2003).5126.2 Packed Representations and LocalityOne natural extension of our reranker is to adapt it tocandidate parses represented as a packed parse for-est, so that it can operate on the base parser?s fulloutput instead of a limited N -best list.
However,as we described in Section 3.1, our features are lo-cally scoped with respect to hidden?variable interac-tions but unrestricted regarding information derivedfrom the underlying candidate parses, which posesa problem for the use of packed representations.For instance, the Up/Down Path features (see Figure2) enumerate the vertical sequences of nontermi-nals that extend above and below a given headword.We could restrict the features to local scope on thecandidate parses, allowing dynamic?programmingto be used to train the model with a packed rep-resentation.
However, even with these restrictions,finding argmaxt?h p(t,h | s,?)
is NP?hard, andthe Viterbi approximation argmaxt,h p(t,h | s,?)?
or other approximations ?
would have to be used(see Matsuzaki et al (2005)).6.3 Empirical Analysis of the Hidden ValuesOur model makes no assumptions about the interpre-tation of the hidden values assigned to words: dur-ing training, the model simply learns a distributionover global hidden?value assignments that is usefulin improving the log?likelihood of the training data.Intuitively, however, we expect that the model willlearn to make hidden?value assignments that are rea-sonable from a linguistic standpoint.
In this sectionwe describe some empirical observations concern-ing hidden values assigned by the model.We established a corpus of parse trees withhidden?value annotations, as follows.
First, we findthe optimal parameters ??
on the training set.
Forevery sentence si in the training set, we then use??
to find t?i , the most probable candidate parse un-der the model.
Finally, we use ??
to decode h?i ,the most probable global assignment of hidden val-ues, for each parse tree t?i .
We created a corpus of(t?i ,h?i ) pairs for the feature set defined by part?of?speech hidden?value domains and standard depen-dency structures.
The remainder of this section de-scribes trends for several of the most common part?of?speech categories in the corpus.As a first example, consider the hidden valuesfor the part?of?speech VB (infinitival verb).
In themajority of cases, words tagged VB either modify amodal verb tagged MD (e.g., in the new rate will/MDbe/VB payable) or the infinitival marker to (e.g., in inan effort to streamline/VB bureaucracy).
The statis-tics of our corpus reflect this distinction.
In 11,546cases of the VB1 hidden value, 10,652 cases mod-ified to, and 81 cases modified modals tagged MD.In contrast, in 11,042 cases of the VB2 value, thenumbers were 8,354 and 599 for modification ofmodals and to respectively, showing the oppositepreference.
This polarization of hidden values al-lows modifiers to the VB (e.g., payable in the newrate will be payable) to be sensitive to whether theverb is modifying a modal or to.In a related case, the hidden values for the part?of?speech TO (corresponding to the word to) alsoshow that the model is learning useful structure.Consider cases where to heads a clause which mayor may not have a subject (e.g., in it expects ?its salesto remain steady?
vs. a proposal ?to ease reportingrequirements?).
We find that for hidden values TO1and TO5 together, 946 out of 976 cases have a sub-ject.
In contrast, for the hidden value TO4, only 29out of 10,148 cases have a subject.
This splittingof the TO part?of?speech allows modifiers to to, orwords modified by to, to be sensitive to the presenceor absence of a subject in the clause headed by to.Finally, consider the hidden values for the part?of?speech NNS (plural noun).
In this case, the modeldistinguishes contexts where a plural noun acting asthe head of a noun?phrase is or isn?t modified by apost?modifier (such as a prepositional phrase or rel-ative clause).
For hidden value NNS3, 12,003 outof the 12,664 instances in our corpus have a post?modifier, but for hidden value NNS5, only 4,099 ofthe 39,763 occurrences have a post?modifier.
Sim-ilar contextual effects were observed for other nouncategories such as singular or proper nouns.7 Conclusions and Future ResearchThe hidden?variable model is a novel method forrepresenting NLP structures in the reranking frame-work.
We can obtain versatile behavior from themodel simply by manipulating the definition of thehidden?value domains, and we have experimentedwith models that emulate word clustering, word re-finement, and mappings from words into an existingontology.
In the case of the parse reranking task,513the hidden?variable model achieves reranking per-formance comparable to the reranking approach de-scribed by Collins (2000), and the two rerankers canbe combined to yield an additive improvement.Future work may consider the use of hidden?value domains with mixed contents, such as a do-main that contains 3 refinement?oriented lexical val-ues and 3 clustering?oriented part?of?speech val-ues.
These mixed values would allow the hidden?variable model to exploit interactions between clus-tering and refinement at the level of words and de-pendencies.
Another area for future research is toinvestigate the use of unlabeled data within the ap-proach, for example by making use of clusters de-rived from large amounts of unlabeled data (e.g., seeMiller et al (2004)).
Finally, future work may applythe models to NLP tasks other than parsing.AcknowledgementsWe would like to thank Regina Barzilay, IgorMalioutov, and Luke Zettlemoyer for their manycomments on the paper.
We gratefully acknowl-edge the support of the National Science Founda-tion (under grants 0347631 and 0434222) and theDARPA/SRI CALO project (through subcontractNo.
03-000215).ReferencesDaniel Bikel.
2000.
A statistical model for parsing and word?sense disambiguation.
In Proceedings of EMNLP.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-nifer C. Lai, and Robert L. Mercer.
1992.
Class?based n?gram models of natural language.
Computational Linguis-tics, 18(4):467?479.Eugene Charniak and Mark Johnson.
2005.
Coarse?to?fine n?best parsing and maxent discriminative reranking.
In Pro-ceedings of the 43rd ACL.Massimiliano Ciaramita and Mark Johnson.
2003.
Supersensetagging of unknown nouns in wordnet.
In EMNLP 2003.Stephen Clark and James R. Curran.
2004.
Parsing the wsjusing ccg and log?linear models.
In ACL, pages 103?110.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL 2002.Michael Collins.
1999.
Head?Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of the 17th ICML.Michael Collins.
2002.
Ranking algorithms for named entityextraction: Boosting and the voted perceptron.
In ACL 2002.Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, andDavid J. Spiegelhalter.
1999.
Probabilistic Networks andExpert Systems.
Springer.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, andStefan Riezler.
1999.
Estimators for stochastic ?unification?based?
grammars.
In Proceedings of the 37th ACL.Y.
LeCun, L. Bottou, Y. Bengio, and P. Haffner.
1998.Gradient?based learning applied to document recognition.Proceedings of the IEEE, 86(11):2278?2324, November.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpusof english: The penn treebank.
Computational Linguistics,19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005.Probabilistic cfg with latent annotations.
In ACL.George A. Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine Miller.
1993.
Five papers onwordnet.
Technical report, Stanford University.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.Name tagging with word clusters and discriminative train-ing.
In HLT?NAACL, pages 337?342.Franz Josef Och and Hermann Ney.
2002.
Discriminative train-ing and maximum entropy models for statistical machinetranslation.
In Proceedings of the 40th ACL, pages 295?302.Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of english words.
In Proceedingsof the 31st ACL, pages 183?190.Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2005.Conditional random fields for object recognition.
In NIPS17.
MIT Press.Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward.
1994.A maximum entropy model for parsing.
In ICSLP 1994.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard S.Crouch, John T. Maxwell III, and Mark Johnson.
2002.Parsing the wall street journal using a lexical?functionalgrammar and discriminative estimation techniques.
In ACL.Libin Shen and Aravind K. Joshi.
2003.
An svm?based vot-ing algorithm with application to parse reranking.
In WalterDaelemans and Miles Osborne, editors, Proceedings of the7th CoNLL, pages 9?16.
Edmonton, Canada.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.
Dis-criminative reranking for machine translation.
In HLT?NAACL, pages 177?184.Marilyn A. Walker, Owen Rambow, and Monica Rogati.
2001.Spot: A trainable sentence planner.
In NAACL.Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,2003.
Exploring Artificial Intelligence in the New Millen-nium, chapter 8: ?Understanding Belief Propagation and ItsGeneralizations?, pages 239?236.
Science & TechnologyBooks.514
