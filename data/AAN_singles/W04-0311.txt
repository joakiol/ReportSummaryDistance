Dynamic Dependency ParsingMichael DaumNatural Language Systems GroupDepartment of Computer ScienceUniversity of Hamburgmicha@nats.informatik.uni-hamburg.deAbstractThe inherent robustness of a system might be animportant prerequisite for an incremental pars-ing model to the effect that grammaticality re-quirements on full sentences may be suspendedor allowed to be violated transiently.
How-ever, we present additional means that allow thegrammarian to model prefix-analyses by alter-ing a grammar for non-incremental parsing in acontrolled way.
This is done by introducing un-derspecified dependency edges that model theexpected relation between already seen and yetunseen words during parsing.
Thus the basicframework of weighted constraint dependencyparsing is extended by the notion of dynamicdependency parsing.1 IntroductionIn an incremental mode of operation, a parserworks on a prefix of a prolonging utterance, try-ing to compute prefix-analyses while having tocope with a growing computational effort.
Thissituation gives rise at least to the following ques-tions:(1) Which provisions can be made to acceptprefix-analyses transiently given a modelof language that describes complete sen-tences?
(2) How shall prefix-analyses look like?
(3) How can the complexity of incrementalparsing be bounded?We will introduce underspecified dependencyedges, called nonspec dependency edges, to theframework of weighted constraint dependencygrammar (WCDG) (Schro?der, 2002).
These areused to encode an expected function of a wordalready seen but not yet integrated into the restof the parse tree during incremental parsing.In WCDG, parse trees are annotated byconstraint violations that pinpoint deviationsfrom grammatical requirements or preferences.Hence weighted constraints are a means to de-scribe a graded grammaticality discretion by de-scribing the inherent ?costs?
of accepting an im-perfect parse tree.
Thus parsing follows princi-ples of economy when repairing constraint vio-lations as long as reducing costs any further isjustified by its effort.The following sections revise the basic ideasof applying constraint optimization to naturallanguage parsing and extend it to dynamic de-pendency parsing.2 From static to dynamic constraintsatisfactionWe begin by describing the standard constraintsatisfaction problem (CSP), then extend it intwo different directions commonly found in theliterature: (a) to constraint optimization prob-lems (COP) and (b) to dynamic constraint sat-isfaction problems (DynCSP) aggregating bothto dynamic constraint optimization problems(DynCOP) which is motivated by our currentapplication to incremental parsing1.2.1 Constraint SatisfactionConstraint satisfaction is defined as being theproblem of finding consistent values for a fixedset of variables given all constraints betweenthose values.
Formally, a constraint satisfac-tion problem (CSP) can be viewed as a triple(X, D, C) where X = {x1, .
.
.
, xn} is a fi-nite set of variables with respective domainsD = {D1, .
.
.
, Dn}, and a set of constraintsC = {C1, .
.
.
, Ct}.
A constraint Ci is defined asa relation defined on a subset of variables, called1Note that we don?t use the common abbreviationsfor dynamic constraint satisfaction problems DCSP in fa-vor of DynCSP in order to distinguish if from distributedconstraint satisfaction problems which are called DCSPsalso.
Likewise we use DynCOP instead of DCOP, thelatter of which is commonly known as distributed con-straint optimization problems.the scope, restricting their simultaneous assign-ment.
Constraints defined on one variable arecalled unary ; constraints on two variables arebinary.
We call unary and binary constraints lo-cal constraints as their scope is very restricted.Constraints of wider scope are classified non-local.
Especially those involving a full scopeover all variables are called context constraints.The ?local knowledge?
of a CSP is encoded ina constraint network (CN) consisting of nodesbundling all values of a variable consistent withall unary constraints.
The edges of a CN de-pict binary constraints between the connectedvariables.
So a CN is a compact representation(of a superset) of all possible instantiations.
Asolution of a CSP is a complete instantiation ofvariables ?x1, .
.
.
, xn?
with values ?di1 , .
.
.
, din?with dik ?
Dk found in a CN that is consistentwith all constraints.Principles of processing CSPs have been de-veloped in (Montanari, 1974), (Waltz, 1975)and (Mackworth, 1977).2.2 Constraint OptimizationIn many problem cases no complete instantia-tion exists that satisfies all constraints: eitherwe get stuck by solving only a part of the prob-lem or constraints need to be considered defea-sible for a certain penalty.
Thus finding a so-lution becomes a constraint optimization prob-lem (COP).
A COP is denoted as a quadruple(X, D, C, f), where (X, D, C) is a CSP and fis a cost function on (partial) variable instan-tiations.
f might be computed by multiply-ing the penalties of all violated constraints.
Asolution of a COP is a complete instantiation,where f(?di1 , .
.
.
, din?)
is optimal.
This termbecomes zero if the penalty of at least one vio-lated constraint is zero.
These constraints arecalled hard, those with a penalty greater zeroare called soft.An more precise formulation of COPs (alsocalled partial constraint satisfaction problems),can be found in (Freuder and Wallace, 1989).2.3 Dynamic Constraint SatisfactionThe traditional CSP and COP framework isonly applicable to static problems, where thenumber of variables, the values in their domainsand the constraints are all known in advance.
Ina dynamically changing environment these as-sumptions don?t hold any more as new variables,new values or new constraints become availableover time.
A dynamic constraint satisfactionproblem (DynCSP) is construed as a series ofCSPs P0, P1, .
.
.
that change periodically overtime by loss of gain of values, variables or con-straints (Pi+1 = Pi + ?Pi+1).
For each problemchange ?Pi+1 we try to find a solution change?Si+1 such that Si+1 = Si + ?Si+1 is a solutionto Pi+1.
The legitimate hope is that this is moreefficient than solving Pi+1 the naive way fromscratch whenever things change.This notation is consistent with previous onesfound in in (Dechter and Dechter, 1988) and(Wire?n, 1993).2.4 Dynamic Constraint OptimizationMost notions of DynCSPs in the literature arean extension of the classical CSP that use hardconstraints exclusively.
To model the aimedapplication of incremental parsing however, westill like to use weighted constraints.
There-fore we define dynamic constraint optimizationproblems (DynCOP) the same way DynCSPswere defined on the basis of CSPs as a seriesof COPs P0, P1, .
.
.
that change over time.
Inaddition to changing variables, values and con-straints we are concerned with changes of thecost function as well.
In particular, variable in-stantiations evaluated formerly might now bejudged differently.
As this could entail seri-ous computational problems we try to keepchanges in the cost function monotonic, thatis re-evaluation shall only give lower penaltiesthan before, i.e.
instantiations that become in-consistent once don?t get consistent later onagain.3 Basic Dependency ParsingUsing constraint satisfaction techniques for nat-ural language parsing was introduced first in(Maruyama, 1990) by defining a constraint de-pendency grammar (CDG) that maps nicely onthe notion of a CSP.
A CDG is a quadruple(?, R, L, C), where ?
is a lexicon of knownwords, R is a set of roles of a word.
A role rep-resents a level of language like ?SYN?
or ?SEM?.L is a set of labels for each role (e.g.
{?SUBJ?,?OBJ?
}, {?AGENT?,?PATIENT?
}), and C is aconstraint grammar consisting of atomic logicalformulas.
Now, the only thing that is left inorder to match a CDGs to a CSPs is to definevariables and their possible values.
For eachword of an utterance and for each role we al-locate one variable that can take values of theform ei,j = ?r, wi, l, wj?
with r ?
R, wi, wj ?
?and l ?
L. ei,j is called the dependency edgebetween word form wi and wj labeled with l onthe description level r. A dependency edge ofthe form ei,root is called the root edge.
Hencea dependency tree of an utterance of length nis a set of dependency edges s = {ei,j | i ?
{1, .
.
.
, n} , j ?
{1, .
.
.
, n} ?
{root} , i 6= j}.From this point on parsing natural languagehas become a matter of constraint processingas can be found in the CSP literature (Dechter,2001).4 Weighted Dependency ParsingIn (Schro?der, 2002) the foundations of depen-dency parsing have been carried over to COPsusing weighted constraint dependency grammars(WCDG), a framework to model language usingall-quantified logical formulas on dependencystructures.
Penalties for constraint violationsaren?t necessarily static once, but can be lexi-calized or computed arithmetically on the ba-sis of the structure under consideration.
Thefollowing constraints are rather typical once re-stricting the properties of subject edges:{X:SYN} : SUBJ-init : 0.0 :X.label = SUBJ ->( X@cat = NN | X@cat = NE |X@cat = FM | X@cat = PPER ) &X^cat = VVFIN;{X:SYN} : SUBJ-dist : 2.9 / X.length :X.label = SUBJ -> X.length < 3;Both constraints have a scope of one depen-dency edge on the syntax level ({X:SYN}).
Theconstraint SUBJ-init is a hard constraint stat-ing that every dependency edge labeled SUBJshall have a nominal modifier and a finiteverb as its modifiee.
The second constraintSUBJ-dist is a soft one, such as every edgewith label SUBJ attached more than two wordsaway induces a penalty calculated by the term2.9 / X.length.
Note, that the maximal edgelength in SUBJ-dist is quite arbitrary andshould be extracted from a corpus automati-cally as well as the grade of increasing penal-ization.
A realistic grammar consists of about500 such handwritten constraints like the cur-rently developed grammar for German (Daumet al, 2003).The notation used for constraints in this pa-per is expressing valid formulas interpretable bythe WCDG constraint system.
The followingdefinitions explain some of the primitives thatare part of the constraint language:?
X is a variable for a dependency edge of theform ei,j = ?r, wi, l, wj?,REFSEM SYNsemanticconstraints syntaxconstraintsreferenceconstraintssyntax?semanticconstraintssyntax?referenceconstraintssemantic?referenceconstraintsLexiconChunker TaggerOntologyFigure 1: Architecture of WCDG?
X@word (X^word) refers to the word formwi ?
?
(wj ?
?)?
X@id (X@id) refers to the position i (j)?
X.label refers to the label l ?
L?
X@cat (X^cat) refers to the POS-tag of themodifier (modifiee)?
root(X^id) ?
true iff wj = root?
X.length is defined as |i ?
j|.A complete definition can be found in (Schro?deret al, 1999).Figure (1) outlines the overall architecture ofthe system consisting of a lexicon component,ontologies and other external shallow parsingcomponents, all of which are accessed via con-straints affecting the internal constraint net-work as far as variables are in their scope.
Whilea static parsing model injects all variables intothe ring in Figure (1) once and then waits forall constraints to let the variable instantiationssettle in a state of equilibrium, a dynamic opti-mization process will add and remove variablesfrom the current scope repeatedly.5 Dynamic Dependency ParsingAs indicated, the basic parsing model in WCDGis a two stage process: first building up a con-straint network given an utterance and secondconstructing an optimal dependency parse.
In adynamic system like an incremental dependencyparser these two steps are repeated in a loopwhile consuming all bits from the input thatcomplete a sentence over time.
In principle, theproblem of converting the static parsing modelinto a dynamic one should only be a questionof repetitive updating the constraint network ina subtle way.
Additionally, information aboutthe internal state of the ?constraint optimizer?itself, which is not stored in the constraint net,shall not get lost during consecutive iterationsas it (a) might participate in the update heuris-tics of the first phase and (b) the parsing effortduring all previous loops might affect the cur-rent computation substantially.
We will comeback to this argument in Section 8.Basically, changes to the constraint networkare only allowed after the parser has emitted aparse tree.
This is acceptable if the parser it-self is interruptible providing the best parse treefound so far.
An interrupt may occur eitherfrom ?outside?
or from ?inside?
by the parser it-self taking into account the number of pendingnew words not yet added.
So it either may in-tegrate a new word as soon as it arrives or waituntil further hypotheses have been checked.
Astransformation based parsing has strong any-time properties, these heuristics can be imple-mented as part of a termination criterion be-tween increments easily.6 Modeling expectations usingnonspec6.1 MotivationAnalyzing sentence prefixes with a static parser,that i.e.
is not aware of the sentence being a pre-fix, will yield at least a penalty for a fragmen-tary representation.
To get such a result at all,the parser must allow partial parse trees.
Theconstraints S-init and frag illustrate modelingof normal and fragmentary dependency trees.
{X:SYN} : S-init : 0.0 :X.label = S -> root(X^id) &(X@cat = VVFIN | X@cat = VMFIN |X@cat = VAFIN | ... );{X:SYN} : frag : 0.001 :root(X^id) -> X.label = S;Constraint S-init restricts all edges with la-bel S to be finite verbs pointing to root .
But ifsome dependency edge is pointing to root andis not labeled with S then constraint frag is vi-olated and induces a penalty of 0.001.
So everyfragment in sentence (2a) that can not be inte-grated into the rest of the dependency tree willincrease the penalty of the structure by three or-ders of magnitude.
A constraint optimizer willtry to avoid an analysis with an overall penaltyof at least 1?12 and will search for another struc-ture better than that.
Modeling language ina way that (2a) in fact turns out as the op-timal solution is therefore difficult.
Moreover,the computational effort could be avoided if apartial tree is promised to be integrated laterwith fewer costs.The only way to prevent a violation of fragin WCDG is either by temporarily switching itoff completely or, preferably, by replacing theroot attachment with a nonspec dependency asshown in (2b), thereby preventing the prerequi-sites of frag in the first place while remainingrelevant for ?normal?
dependency edges.A prefix-analysis like (2a) might turn out tobe cognitively implausible as well, as humansexpect a proper NP-head to appear as long asno other evidence forces an adjective to be nom-inalized.
Such a thesis can be modeled usingnonspec dependency edges.6.2 DefinitionWe now extend the original definition ofWCDG, so that a dependency tree is devised ass = {ei,j | i ?
{1, .
.
.
, n} ?
{?
}, j ?
{1, .
.
.
, n} ?
{root , ?
}, i 6= j}.
We will use the notation w?to denote any unseen word.
A dependency edgemodifying w?
is written as ei,?, and an edge ofthe form e?,i denotes a dependency edge of w?modifying an already seen word.
ei,?
and e?,iare called nonspec dependency edges.Selective changes to the semantics of the con-straint language have been made to accom-plish nonspec dependency edges.
So given twoedges ei1,i2 = ?r, wi1 , l?, wi2?
and ej1,j2 =the big blue bouncingDET ADJ ADJ ADJthe big blue bouncing w?DETADJADJADJ(a)(b)(c)the big blue bouncing ball w?DET ADJ ADJADJ SUBJFigure 2: Example sentence prefix?r, wj1 , l?
?, wj2?
with X ?
ei1,i2 and Y ?
ej1,j2 :?
X^id = Y^id ?
false iff wi2 6= wj2 ?
?
?wi2 = wj2 = w?, and true otherwise?
X.length ?
|i1 ?
i2| iff wi1 , wi2 ?
?, andn+1 iff wi2 = w?, (n: length of the currentsentence prefix)?
X^cat = ?POS ?
tag?
?
false iff wi2 = w??
nonspec(X^id) ?
true iff wi2 = w??
spec(X^id) ?
true iff wi2 ?
?6.3 PropertiesAlthough every nonspec dependency in Figure(2b) points to the same word w?, two nonspecdependency edges are not taken to be connectedat the top (X^id = Y^id ?
false) as we don?tknow yet whether wi and wj will be modifyingthe same word in the future.In general, the above extension to the con-straint language is reasonable enough to fit intothe notion of static parsing, that is a grammartailored for incremental parsing can still be usedfor static parsing.
An unpleasant consequenceof nonspec is, that more error-cases might occurin an already existing constraint grammar forstatic parsing that was not written with nonspecdependency edges in mind.
Therefore we intro-duced guard-predicates nonspec() and spec()that complete those guard-predicates alreadypart of the language (e.g.
root() and exists()).These must be added by the grammarian to pre-vent logical error-cases in a constraint formula.Looking back at the constraints we?ve dis-cussed so far, the constraints SUBJ-init andS-init have to be adjusted to become nonspec-aware because referring to the POS-tag is notpossible if the dependency edge under consider-ation is of the form ei,?
or e?,i.
Thus a prefix-analysis like (2c) is inducing a hard violationof SUBJ-init.
We have to rewrite SUBJ-init toallow (2c) as follows:{X:SYN} : SUBJ-init : 0.0 :X.label = SUBJ ->( nonspec(X@id) |X@cat = NN | X@cat = NE |X@cat = FM | X@cat = PPER) &( nonspec(X^id) | X^cat = VVFIN );When all constraints have been checked forlogical errors due to a possible nonspec depen-dency edge, performance of the modified gram-mar will not have changed for static parsing butwill accept nonspec dependency edges.Using the nonspec() predicate, we are able towrite constraints that are triggered by nonspecedges only being not pertinent for ?normal?edges.
For example we like to penalize nonspecdependency edges the older they become dur-ing the incremental course and thereby allow acheaper structure to take over seamlessly.
Thiscan easily be achieved with a constraint likenonspec-dist, similar to SUBJ-dist:{X:SYN} : nonspec-dist : 1.9 / X.length :nonspec(X^id) -> X.length < 2;The effect of nonspec-dist is, that a certainamount of penalty is caused by ?SYN, the, DET,w??
and ?SYN, big, ADJ, w??
in (2b).
Figure (2c)illustrates the desired prefix-analysis in the nextloop when nonspec edges become pricey due totheir increased attachment length.
In a real-life constraint grammar (2c) will be optimal ba-sically because the head of the NP occurred,therefore overruling every alternative nonspecdependency edges that crosses the head.
Thelatter alternative structure will either cause aprojectivity violation with all other non-headcomponents of the NP that are still linked to thehead or cause an alternative head to be electedwhen becoming available.7 Dynamic Constraint Networksnonspec dependency edges play an importantrole when updating a constraint network to re-flect the problem change ?Pi .
Maintaining theconstraint network in the first phase is crucialfor the overall performance as a more sophisti-cated strategy to prune edges might compensatecomputational effort in the second phase.Figure (3) illustrates a sentence of threewords being processed one word wi per time-point ti as follows:1. for each edge e of the form ej,?
or e?,j , (j <i) recompute penalty f(?e?).
If its penaltydrops below ?, then remove e. Otherwisederive edge e?
on the basis of e2.
add new edges ei,?
and e?,i to the CN as faras f(?ei,??)
< ?
and f(?e?,i?)
< ?3.
remove each edge e from the CN if it?s lo-cal penalty is lower than the penalty of thebest parse so far.The parameter ?
is a penalty threshold thatdetermines the amount of nonspec edges beingpruned.
Any remaining nonspec edge indicateswhere the constraint network remains extensiblew1 w?e1,?e?,1t1w1 w2 w?e1,?e1,2e?,1e2,1e2,?e?,2t2w1 w2 w3 w?e1,?e1,2e1,3e?,1e2,1e3,1e2,?e2,3e?,2e3,2e3,?e?,3t3Figure 3: Incremental update of a constraint networkand provides an upper estimate of any futureedge derived from it.
This holds only if someprerequisites of monotony are guaranteed:?
The penalty of a parse will always be lowerthan each of the penalties on its depen-dency edges (guaranteed by the multiplica-tive cost function).?
Each nonspec edge must have a penaltythat is an upper boundary of each depen-dency edge that will be derived from it:f(?e?,i1?)
>= f(?ei2,i1?)
andf(?ei1,??)
>= f(?ei1,i2?)
with (i1 < i2).Only then will pruning of nonspec depen-dency edges be correct.?
As a consequence the overall penalties ofprefix-analyses degrade monotonically overtime: f(si) >= f(si+1)Note, that the given strategy to update theconstraint network does not take the struc-ture of the previous prefix-analysis into accountbut only works on the basis of the completeconstraint network.
Nevertheless, the previousparse tree is used as a starting point for thenext optimization step, so that near-by parsetrees will be constructed within a few transfor-mation steps using the alternatives licensed bythe constraint network.8 The OptimizerSo far we discussed the first phase of a dynamicdependency parser building up a series of prob-lems P0, P1, .
.
.
changing Pi using ?Pi+1 in termsof maintaining a dynamic constraint network.In the second phase ?the optimizer?
tries to ac-commodate to those changes by constructingSi+1 on the basis of Si and Pi+1.WCDG offers a decent set of methods to com-pute the second phase, one of which implementsa guided local search (Daum and Menzel, 2002).The key idea of GLS is to add a heuristicssitting on top of a local search procedure by in-troducing weights for each possible dependencyedge in the constraint network.
Initially beingzero, weights are increased steadily if a localsearch settles in a local optimum.
By augment-ing the cost function f with these extra weights,further transformations are initiated along thegradient of f .
Thus every weight of a depen-dency edge resembles an custom-tailored con-straint whose penalty is learned during search.The question now to be asked is, how weightsacquired during the incremental course of pars-ing influence GLS.
The interesting propertyis that the weights of dependency edges inte-grated earlier will always tend to be higher thanweights of most recently introduced dependencyedges as a matter of saturation.
Thus keepingold weights will prevent GLS from changing olddependency edges and encourage transformingnewer dependency edges first.
Old dependencyedges will not be transformed until more re-cent constraint violations have been removed orold structures are strongly deprecated recently.This is a desirable behavior as it stabilizes for-mer dependency structures with no extra provi-sions to the base mechanism.
Transformationswill be focused on the most recently added de-pendency edges.
This approach is comparableto a simulated annealing heuristics where trans-formations are getting more infrequent due to adeclining ?temperature?.Another very successful implementation of?the optimizer?
in WCDG is called Frobbing(Foth et al, 2000) which is a transformationbased parsing technique similar to taboo search.One interesting feature of Frobbing is its abilityto estimate an upper boundary of the penalty ofany structure using a certain dependency edgeand a certain word form.
In an incrementalparsing mode the penalty limit of a nonspec de-pendency edge will then be an estimate of anystructure derived from it and thereby provide agood heuristics to prune nonspec edges fallingbeyond ?
during the maintenance of the con-straint network.9 ConclusionIncremental parsing using weighted constraintoptimization has been classified as a special caseof dynamic dependency parsing.The idea of nonspec dependency edges hasbeen described as a means of expressing ex-pectations during the incremental process.
Wehave argued that (a) nonspec dependency edgesare more adequate to model prefix-analyses and(b) offer a computational advantage comparedto a parser that models the special situation ofa sentence prefix only by means of violated con-straints.While completing the notion of dynamic de-pendency parsing, we assessed the consequencesof an incremental parsing mode to the mostcommonly used optimization methods used inWCDG.Further research will need to add the notionof DynCSP to the WCDG system as well asan adaption and completion of an existing con-straint grammar.
This will allow an in-depthevaluation of dynamic dependency parsing withand without nonspec dependency edges giventhe optimization methods currently available.Experiments will be conducted to acquire pars-ing times per increment that are then comparedto human reading times.AcknowledgmentsThis research has been partially supported byDeutsche Forschungsgemeinschaft under grantMe 1472/4-1.ReferencesMichael Daum and Wolfgang Menzel.
2002.Parsing natural language using guided localsearch.
In F. van Harmelen, editor, Proc.
15thEuropean Conference on Artificial Intelli-gence, Amsterdam.
IOS Press.Michael Daum, Kilian Foth, and Wolfgang Men-zel.
2003.
Constraint based integration ofdeep and shallow parsing techniques.
In Pro-ceedings 11th Conference of the EuropeanChapter of the ACL, Budapest, Hungary.Rina Dechter and Avi Dechter.
1988.
Be-lief Maintenance in Dynamic Constraint Net-works.
In 7th Annual Conference of theAmerican Association of Artificial Intelli-gence, pages 37?42.Rina Dechter.
2001.
Constraint Processing.Morgan Kaufmann, September.Kilian Foth, Wolfgang Menzel, and IngoSchro?der.
2000.
A transformation-basedparsing technique with anytime properties.In Proc.
4th International Workshop on Pars-ing Technologies, pages 89?100, Trento, Italy.Eugene C. Freuder and Richard J. Wallace.1989.
Partial constraint satisfaction.
InProc.
11th International Joint Conference onArtificial Intelligence (IJCAI-89), volume 58,pages 278?283, Detroit, Michigan, USA.A.
K. Mackworth.
1977.
Consistency in net-works of relations.
Artificial Intelligence.8:99-118.Hiroshi Maruyama.
1990.
Structure disam-biguation with constraint propagation.
InProc.
the 28th Annual Meeting of the ACL,pages 31?38, Pittsburgh.U.
Montanari.
1974.
Networks of constraints:Fundamental properties and applications topicture processing.
Inform.
Sci., 7:95-132.Ingo Schro?der, Kilian A. Foth, and MichaelSchulz.
1999.
[X]cdg Benutzerhandbuch.Technical Report Dawai-HH-13, Universita?tHamburg.Ingo Schro?der.
2002.
Natural Language Parsingwith Graded Constraints.
Ph.D. thesis, Dept.of Computer Science, University of Hamburg,Germany.David Waltz.
1975.
Understanding line draw-ings of scenes with shadows.
In P. H. Win-ston, editor, The Psychology of Computer Vi-sion.
McGraw?Hill, New York.Mats Wire?n.
1993.
Bounded incremental pars-ing.
In Proc.
6th Twente Workshop onLanguage Technology, pages 145?156, En-schede/Netherlands.
