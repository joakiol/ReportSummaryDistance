Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390?396,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDifficult Cases: From Data to Learning, and BackBeata Beigman Klebanov?Educational Testing Service660 Rosedale RoadPrinceton, NJ 08541bbeigmanklebanov@ets.orgEyal Beigman?Liquidnet Holdings Inc.498 Seventh AvenueNew York, NY 10018e.beigman@gmail.comAbstractThis article contributes to the ongoing dis-cussion in the computational linguisticscommunity regarding instances that aredifficult to annotate reliably.
Is it worth-while to identify those?
What informa-tion can be inferred from them regardingthe nature of the task?
What should bedone with them when building supervisedmachine learning systems?
We addressthese questions in the context of a sub-jective semantic task.
In this setting, weshow that the presence of such instancesin training data misleads a machine learnerinto misclassifying clear-cut cases.
Wealso show that considering machine lear-ning outcomes with and without the diffi-cult cases, it is possible to identify specificweaknesses of the problem representation.1 IntroductionThe problem of cases that are difficult for anno-tation received recent attention from both the the-oretical and the applied perspectives.
Such itemsmight receive contradictory labels, without a clearway of settling the disagreement.
Beigman andBeigman Klebanov (2009) showed theoreticallythat hard cases ?
items with unreliable annota-tions ?
can lead to unfair benchmarking resultswhen found in test data, and, in worst case, to adegradation in a machi74ne learner?s performanceon easy, uncontroversial instances if found in thetraining data.
Schwartz et al (2011) provided anempirical demonstration that the presence of suchdifficult cases in dependency parsing evaluations1The work presented in this paper was done when the firstauthor was a post-doctoral fellow at Northwestern University,Evanston, IL and the second author was a visiting assistantprofessor at Washington University, St. Louis, MO.leads to unstable benchmarking results, as diffe-rent gold standards might provide conflicting an-notations for such items.
Reidsma and Carletta(2008) demonstrated by simulation that systema-tic disagreements between annotators negativelyimpact generalization ability of classifiers builtusing data from different annotators.
Oosten etal.
(2011) showed that judgments of readabilityof the same texts by different groups of expertsare sufficiently systematically different to hampercross-expert generalization of readability classi-fiers trained on annotations from different groups.Rehbein and Ruppenhofer (2011) discuss the ne-gative impact of systematic simulated annotationinconsistencies on active learning performance ona word-sense disambiguation task.In this paper, we address the task of classify-ing words in a text as semantically new or old.Using multiple annotators, we empirically identifyinstances that show substantial disagreement be-tween annotators.
We then discuss those both fromthe linguistic perspective, identifying some char-acteristics of such cases, and from the perspec-tive of machine learning, showing that the pres-ence of difficult cases in the training data misleadsthe machine learner on easy, clear-cut cases ?
aphenomenon termed hard case bias in Beigmanand Beigman Klebanov (2009).
The main con-tribution of this paper is in providing additionalempricial evidence in support of the argument putforward in the literature regarding the need to payattention to problematic, disagreeable instances inannotated data ?
not only from the linguistic per-spective, but also from a machine learning one.2 DataThe task considered here is that of classifying firstoccurrences of words in a text as semantically oldor new.
One of goals of the project is to inves-tigate the relationship between various kinds ofnon-novelty in text, and, in particular, the rela-390tionship between semantic non-novelty (conceptu-alized as semantic association with some preced-ing word in the text), the information structure interms of given and new information, and the cog-nitive status of discourse entities (Postolache et al,2005; Birner and Ward, 1998; Gundel et al, 1993;Prince, 1981).
If an annotator identified an asso-ciative tie from the target word back to some otherword in the text, the target word is thereby classi-fied as semantically old (class 1, or positive); if noties were identified, it is classified as new (class 0,or negative).For the project, annotations were collected for10 texts of various genres, where annotators wereasked, for every first appearance of a word in atext, to point out previous words in the text thatare semantically or associatively related to it.
Alldata was annotated by 22 undergraduate and grad-uate students in various disciplines who were re-cruited for the task.
During outlier analysis, datafrom two annotators was excluded from considera-tion, while 20 annotations were retained.
This taskis fairly subjective, with inter-annotator agreement?=0.45 (Beigman Klebanov and Shamir, 2006).Table 1 shows the number and proportion of in-stances that received the ?semantically old?
(1) la-bel from i annotators, for 0?
i ?
20.
The first col-umn shows the number of annotators who gave thelabel ?semantically old?
(1).
Column 2 shows thenumber and proportion of instances that receivedthe label 1 from the number of annotators shown incolumn 1.
Column 3 shows the split into item dif-ficulty groups.
We note that while about 20% ofthe instances received a unanimous 0 annotationand about 12% of the instances received just one 1label out of 20 annotators, the remaining instancesare spread out across various values of i. Reasonsfor this spread include intrinsic difficulty of someof the items, as well as attention slips.
Since anno-tators need to consider the whole of the precedingtext when annotating a given word, maintainingfocus is a challenge, especially for words that firstappear late in the text.Our interest being in difficult, disagreeablecases, we group the instances into 5 bands accor-ding to the observed level of disagreement andthe tendency in the majority of the annotations.Thus, items with at most two label 1 annotationsare clearly semantically new, while those with atleast 17 (out of 20) are clearly semantically old.The groups Hard 0 and Hard 1 contain instances# 1s # instances group(proportion)0 476 (.20) Easy 01 271 (.12) (.40)2 191 (.08)3 131 (.06) Hard 04 106 (.05) (.25)5 76 (.03)6 95 (.04)7 85 (.04)8 78 (.03)9 60 (.03) Very10 70 (.03) Hard11 60 (.03) (.08)12 57 (.02) Hard 113 63 (.03) (.13)14 68 (.03)15 49 (.02)16 65 (.03)17 60 (.03) Easy 118 72 (.03) (.14)19 94 (.04)20 99 (.04)Table 1: Sizes of subsets by levels of agreement.with at least a 60% majority classification, whilethe middle class ?
Very Hard ?
contains instancesfor which it does not appear possible to even iden-tify the overall tendency.In what follows, we investigate the learnabi-lity of the classification of semantic novelty fromvarious combinations of easy, hard, and very harddata.3 Experimental Setup3.1 Training PartitionsThe objective of the study is to determine the use-fulness of instances of various types in the trainingdata for semantic novelty classification.
In parti-cular, in light of Beigman and Beigman Klebanov(2009), we want to check whether the presence ofless reliable data (hard cases) in the training setadversely impacts performance on the highly reli-able data (easy cases).
We therefore test separatelyon easy and hard cases.We ran 25 rounds of the following experiment.All easy cases are randomly split 80% (train) and20% (test), all hard cases are split into train andtest sets in the same proportions.
Then various391parts of the training data are used to train the 5 sys-tems described in Table 2.
We build models usingeasy data; hard data; easy and hard data; easy,hard, and very hard data; easy data and a weightedsample of the hard data.
The labels for very harddata were assigned by flipping a fair coin.System Easy Hard Very HardE +H +E+H + +E+H+VH + + +E+H100w+ sample1Table 2: The 5 training regimes used in the experi-ment, according to the parts of the data utilized fortraining.3.2 Machine LearningWe use linear Support Vector Machines classifieras implemented in SVMLight (Joachims, 1999).Apart from being a popular and powerful ma-chine learning method, linear SVM is one of thefamily of classifiers analyzed in Beigman andBeigman Klebanov (2009), where they are theo-retically shown to be vulnerable to hard case biasin the worst case.To represent the instances, we use two featuresthat capture semantic relatedness between words.One feature uses Latent Semantic Analysis (Deer-wester et al, 1990) trained on the Wall Street Jour-nal articles to quantify the distributional similarityof two words, the other uses an algorithm basedon WordNet (Miller, 1990) to calculate seman-tic relatedness, combining information from boththe hierarchy and the glosses (Beigman Klebanov,2006).
For each word, we calculate LSA (Word-Net) relatedness score for this word with each pre-ceding word in the text, and report the highest pair-wise score as the LSA (WordNet) feature value forthe given word.
The values of the features canbe thought of as quantifying the strength of theevidence for semantic non-newness that could beobtained via a distributional or a dictionary-basedmethod.1The weight corresponds to the number of people whomarked the item as 1, for hard cases.
We take a weightedsample of 100 hard cases.4 ResultsWe calculate the accuracy of every system sepa-rately on the easy and hard test data.
Table 3 showsthe results.Train Test-E Test-HAcc Rank Acc RankE 0.781 1 0.643 2E+H 0.764 2 0.654 1E+H+VH 0.761 2 0.650 1,2H 0.620 3 0.626 3E+H100w0.779 1 0.645 2Table 3: Accuracy and ranking for semantic no-velty classification for systems built using varioustraining data and tested on easy (Test-E) and hard(Test-H) cases.
Systems with insignificant differ-ences in performance (paired t-test, n=25, p>0.05)are given the same rank.We observe first the performance of the systemtrained solely on hard cases (H in Table 3).
Thissystem shows the worst performance, both on theeasy test and on the hard test.
In fact, this systemfailed to learn anything about the positive class in24 out of the 25 runs, classifying all cases as nega-tive.
It is thus safe to conclude that in the featurespace used here the supervision signal in the hardcases is too weak to guide learning.The system trained solely on easy cases (E inTable 3) significantly outperforms H both on theeasy and on the hard test.
That is, easy cases aremore informative about the classification of hardcases than the hard cases themselves.
This showsthat at least some hard cases pattern similarly tothe easy ones in the feature space; SVM failed tosingle them out when trained on hard cases alone,but they are learnable from the easy data.The system that trained on all cases ?
both easyand hard ?
attains the best performance on hardcases but yields to E on the easy test (Test-E).
Thisdemonstrates what Beigman and Beigman Kle-banov (2009) called hard case bias ?
degradationin test performance on easy cases due to hard casesin the training data.
The negative effect of usinghard cases in training data can be mitigated if weonly use a small sample of them (system E+H100w);yet neither this nor other schemes we tried ofselectively incorporating hard cases into trainingdata produced an improvement over E when testedon easy cases (Test-E).3925 Discussion5.1 Beyond worst caseBeigman and Beigman Klebanov (2009) per-formed a theoretical analysis showing that hardcases could lead to hard case bias where hard caseshave completely un-informative labels, with pro-bability of p=0.5 for either label.
These corre-spond to very hard cases in our setting.
Accordingto Table 3, it is indeed the case that adding thevery hard cases hurts performance, but not signif-icantly so ?
compare results for E+H vs E+H+VHsystems.Our results suggest that un-informative labelsare not necessary for the hard case bias to sur-face.
The instances grouped under Hard 1 havethe probability of p=0.66 for class 1 and the in-stances grouped under Hard 0 have the probabi-lity of p=0.71 for class 0.
Thus, while the labelsare somewhat informative, it is apparently the casethat the hard instances are distributed sufficientlydifferently in the feature space from the easy caseswith the same label to produce a hard case bias.Inspecting the distribution of hard cases (Fig-ure 1), we note that hard cases do not followthe worst case pattern analyzed in Beigman andBeigman Klebanov (2009), where they were con-centrated in an area of the feature space that wasremoved far from the separation plane, a malig-nant but arguably unlikely scenario (Dligach et al,2010).
Here, hard cases are spread both close andfar from the plane, yet their distribution is suffi-ciently different from that of the easy cases to pro-duce hard case bias during learning.Hard cases00.10.20.30.40.50.60.70.80 0.2 0.4 0.6 0.8 1LSA scoreWordNet scoreEasy Separator Hard "-"Easy+Hard Separator Hard "+"Figure 1: Hard cases with separators learned fromeasy and easy+hard training data.5.2 The nature of hard casesFigure 1 plots the hard instances in the two-dimensional feature space: Latent Semantic Anal-ysis score is shown on x-axis, and WordNet-basedscore is shown on the y-axis.
The red lines showthe linear separator induced when the system istrained on easy cases only (system E in Table 3),whereas the green line shows the separator in-duced when the system is trained on both easy andhard cases (system E+H).It is apparent from the figure that the differencein the distributions of the easy and the hard caseslead to a lower threshold for LSA score whenWordNet score is zero and a higher threshold ofWordNet score when LSA score is zero in hardvs easy cases.
That is, the system exposed to hardcases learned to trust LSA more and to trust Word-Net less when determining that an instance is se-mantically old than a system that saw only easycases at train time.The tendency to trust WordNet less yields animprovement in precision (92.1% for system E+Hon Test-E class 1 data vs 84% for system E onTest-E class 1 data), which comes at a cost of adrop in recall (42.2% vs 53.3%) on easy positivecases.
This suggests that high WordNet scores thatare not supported by distributional evidence are asource of Hard 0 cases that made the system morecautious when relying on WordNet scores.The pattern of low LSA score and high Word-Net score often obtains for rare senses of words:Distributional evidence typically points away fromthese senses, but they can be recovered throughdictionary definitions (glosses) in WordNet.An example of hard 0 case involves a homony-mous rare sense.
Deck is used in the observationdeck sense in one of the texts.
However, it wasfound to be highly related to buy by WordNet-based measure through the notion of illegal ?
buyin the sense of bribe and deck in the sense of apacket of illegal drugs.
This is clearly a spuri-ous connection that makes deck appear semanti-cally associated with preceding material, whereasannotators largely perceived it as new.Exposure to such cases at training time leads thesystem to forgo handling rare senses that lack dis-tributional evidence, thus leading to misclassifica-tion of easy positive cases that exhibit a similarpattern.
Thus, stall and market are both used in thesales outlet sense in one of the text.
They come outhighly related by WordNet measure; yet in the 68393instances of stall in the training data for LSA thehomonymous verbal usage predominates.
Simi-larly, partner is overwhelmingly used in the busi-ness partner sense in the WSJ data, hence wife andpartner come out distributionally unrelated, whilethe WordNet based measure successfully recoversthese connections.Our features, while rich enough to diagnosea rare sense (low LSA score and high WordNetscore), do not provide information regarding theappropriateness of the rare sense in context.
Shortof full scale word sense disambiguation, we expe-rimented with the idea of taking the second highestpairwise score as the value of the WordNet fea-ture, under the assumption that an appropriate raresense is likely to be related to multiple words inthe preceding text, while a spurious rare sense isless likely to be accidentally related to more thanone preceding word.
We failed to improve per-formance, however; it is thus left for future workto enrich the representation of the problem so thatcases with inappropriate rare senses can be differ-entiated from the appropriate ones.
In the contextof the current article, the identification of a parti-cular weakness in the representation is an addedvalue of the analysis of the machine learning per-formance with and without the difficult cases.6 Related WorkReliability of annotation is a concern widelydiscussed in the computational linguistics litera-ture (Bayerl and Paul, 2011; Beigman Klebanovand Beigman, 2009; Artstein and Poesio, 2008;Craggs and McGee Wood, 2005; Di Eugenio andGlass, 2004; Carletta, 1996).
Ensuring high re-liability is not always feasible, however; the ad-vent of crowdsourcing brought about interest inalgorithms for recovering from noisy annotations:Snow et al (2008), Passonneau and Carpenter(2013) and Raykar et al (2010) discuss methodsfor improving over annotator majority vote whenestimating the ground truth from multiple noisyannotations.A situation where learning from a small num-ber of carefully chosen examples leads to a betterperformance in classifiers is discussed in the ac-tive learning literature (Schohn and Cohn, 2000;Cebron and Berthold, 2009; Nguyen and Smeul-ders, 2004; Tong and Koller, 2001).
Recent workin the proactive active learning and multi-expertactive learning paradigms incorporates considera-tions of item difficulty and annotator expertise intoan active learning scheme (Wallace et al, 2011;Donmez and Carbonell, 2008).In information retrieval, one line of work con-cerns the design of evaluation schemes that reflectdifferent levels of document relevance to a givenquery (Kanoulas and Aslam, 2009; Sakai, 2007;Kek?al?ainen, 2005; Sormunen, 2002; Voorhees,2001; J?arvelin and Kek?al?ainen, 2000; Voorhees,2000).
J?arvelin and Kek?al?ainen (2000) consider,for example, a tiered evaluation scheme, whereprecision and recall are reported separately for ev-ery level of relevance, which is quite analogousto the idea of testing separately on easy and hardcases as employed here.
The graded notion ofrelevance addressed in the information retrievalresearch assumes a coding scheme where peopleassign documents into one of the relevance tiers(Kek?al?ainen, 2005; Sormunen, 2002).
In our case,the graded notion of semantic novelty is a possibleexplanation for the observed pattern of annotatorresponses.7 ConclusionThis article contributes to the ongoing discussionin the computational linguistics community re-garding instances that are difficult to annotate re-liably ?
how to identify those, and what to dowith them once identified.
We addressed this is-sue in the context of a subjective semantic task.In this setting, we showed that the presence ofdifficult instances in training data misleads a ma-chine learner into misclassifying clear-cut, easycases.
We also showed that considering machinelearning outcomes with and without the difficultcases, it is possible to identify specific weaknessesof the problem representation.
Our results alignwith the literature suggesting that difficult casesin training data can be disruptive (Beigman andBeigman Klebanov, 2009; Schwartz et al, 2011;Rehbein and Ruppenhofer, 2011; Reidsma andCarletta, 2008); yet we also show that investigat-ing their impact on the learning outcomes in somedetail can provide insight about the task at hand.The main contribution of this paper is there-fore in providing additional empirical evidence insupport of the argument put forward in the litera-ture regarding the need to pay attention to prob-lematic, disagreeable instances in annotated data?
both from the linguistic and from the machinelearning perspectives.394ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Petra Saskia Bayerl and Karsten Ingmar Paul.
2011.What determines inter-coder agreement in manualannotations?
a meta-analytic investigation.
Comput.Linguist., 37(4):699?725, December.Eyal Beigman and Beata Beigman Klebanov.
2009.Learning with Annotation Noise.
In Proceedingsof the 47th Annual Meeting of the Association forComputational Linguistics, pages 280?287, Singa-pore, August.Beata Beigman Klebanov and Eyal Beigman.
2009.From Annotator Agreement to Noise Models.
Com-putational Linguistics, 35(4):493?503.Beata Beigman Klebanov and Eli Shamir.
2006.Reader-based exploration of lexical cohesion.
Lan-guage Resources and Evaluation, 40(2):109?126.Beata Beigman Klebanov.
2006.
Measuring Seman-tic Relatedness Using People and WordNet.
In Pro-ceedings of the Human Language Technology Con-ference of the NAACL, Companion Volume: ShortPapers, pages 13?16, New York City, USA, June.Association for Computational Linguistics.Betty Birner and Gregory Ward.
1998.
InformationStatus and Non-canonical Word Order in English.Amsterdam/Philadelphia: John Benjamins.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Nicolas Cebron and Michael Berthold.
2009.
Activelearning for object classification: From explorationto exploitation.
Data Mining and Knowledge Dis-covery, 18:283?299.Richard.
Craggs and Mary McGee Wood.
2005.
Eval-uating Discourse and Dialogue Coding Schemes.Computational Linguistics, 31(3):289?296.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journalof the American Society For Information Science,41:391?407.Barbara Di Eugenio and Michael Glass.
2004.
Thekappa statistic: a second look.
Computational Lin-guistics, 30(1):95?101.Dmitriy Dligach, Rodney Nielsen, and Martha Palmer.2010.
To Annotate More Accurately or to AnnotateMore.
In Proceedings of the 4th Linguistic Annota-tion Workshop, pages 64?72, Uppsala, Sweden, July.Pinar Donmez and Jaime G. Carbonell.
2008.
Proac-tive learning: Cost-sensitive active learning withmultiple imperfect oracles.
In Proceedings of the17th ACM Conference on Information and Knowl-edge Management, CIKM ?08, pages 619?628, NewYork, NY, USA.
ACM.Jeanette Gundel, Nancy Hedberg, and Ron Zacharski.1993.
Cognitive status and the form of referring ex-pressions in discourse.
Language, 69:274?307.Kalervo J?arvelin and Jaana Kek?al?ainen.
2000.
IREvaluation Methods for Retrieving Highly RelevantDocuments.
In Proceedings of the 23th Annual In-ternational Conference on Research and Develop-ment in Information Retrieval, pages 41?48, Athens,Greece, July.Thorsten Joachims.
1999.
Advances in KernelMethods - Support Vector Learning.
In Bern-hard Schlkopf, Christopher Burges, and AlexanderSmola, editors, Making large-scale SVM learningpractical, pages 169?184.
MIT Press.Evangelos Kanoulas and Javed Aslam.
2009.
Empir-ical Justification of the Gain and Discount Functionfor nDCG .
In Proceedings of the 19th ACM Confer-ence on Information and Knowledge Management,pages 611?620, Hong Kong, November.Jaana Kek?al?ainen.
2005.
Binary and graded relevancein IR evaluations ?
Comparison of the effects onranking of IR systems.
Information Processing andManagement, 41:1019?1033.George Miller.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4):235?312.Hieu Nguyen and Arnold Smeulders.
2004.
Ac-tive Learning Using Pre-clustering.
In Proceedingsof 21st International Conference on Machine Lear-ning, pages 623?630, Banff, Canada, July.Philip Oosten, Vronique Hoste, and Dries Tanghe.2011.
A posteriori agreement as a quality mea-sure for readability prediction systems.
In Alexan-der Gelbukh, editor, Computational Linguistics andIntelligent Text Processing, volume 6609 of Lec-ture Notes in Computer Science, pages 424?435.Springer Berlin Heidelberg.Rebecca J. Passonneau and Bob Carpenter.
2013.
Thebenefits of a model of annotation.
In Proceedings ofthe 7th Linguistic Annotation Workshop and Interop-erability with Discourse, pages 187?195, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Oana Postolache, Ivana Kruijff-Korbayova, and Geert-Jan Kruijff.
2005.
Data-driven approaches for in-formation structure identification.
In Proceedingsof Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 9?16, Vancouver, BritishColumbia, Canada, October.395Ellen Prince.
1981.
Toward a taxonomy of given-newinformation.
In Peter Cole, editor, Radical Prag-matics, pages 223?255.
Academic Press.Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-ardo Hermosillo Valadez, Charles Florin, Luca Bo-goni, and Linda Moy.
2010.
Learning from crowds.J.
Mach.
Learn.
Res., 11:1297?1322, August.Ines Rehbein and Josef Ruppenhofer.
2011.
Evaluat-ing the impact of coder errors on active learning.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 43?51, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Dennis Reidsma and Jean Carletta.
2008.
ReliabilityMeasurement without Limits.
Computational Lin-guistics, 34(3):319?326.Tetsuya Sakai.
2007.
On the reliability of informationretrieval metrics based on graded relevance.
Infor-mation Processing and Management, 43:531?548.Greg Schohn and David Cohn.
2000.
Less is more:Active Learning with Support Vector Mfachines.
InProceedings of 17th International Conference onMachine Learning, pages 839?846, San Francisco,July.Roy Schwartz, Omri Abend, Roi Reichart, and AriRappoport.
2011.
Neutralizing linguistically prob-lematic annotations in unsupervised dependencyparsing evaluation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 663?672, Stroudsburg, PA,USA.
Association for Computational Linguistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for nat-ural language tasks.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 254?263, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Eero Sormunen.
2002.
Liberal relevance criteria ofTREC ?
Counting on negligible documents?
InProceedings of the 25th Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval, pages 324?330, Tampere,Finland, August.Simon Tong and Daphne Koller.
2001.
Support Vec-tor Machine active learning with applications to textclassification.
Journal of Machine Learning Re-search, 2:45?66.Ellen Voorhees.
2000.
Variations in relevance judge-ments and the measurement of retrieval effective-ness.
Information Processing and Management,36:697?716.Ellen Voorhees.
2001.
Evaluation by highly relevantdocuments.
In Proceedings of the 24th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 74?82,New Orleans, LA, USA, September.B.
Wallace, K. Small, C. Brodley, and T. Trikalinos,2011.
Who Should Label What?
Instance Alloca-tion in Multiple Expert Active Learning, chapter 16,pages 176?187.396
