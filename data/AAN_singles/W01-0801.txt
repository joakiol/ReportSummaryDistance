Corpus-Based Methods in Natural Language Generation: Friend or Foe?Extended AbstractOwen RambowAT&T Labs ?
ResearchFlorham Park, NJ, USArambow@research.att.comIn computational linguistics, the 1990s werecharacterized by the rapid rise to prominence ofcorpus-based methods in natural language under-standing (NLU).
These methods include statis-tical and machine-learning and approaches.
Innatural language generation (NLG), in the meantime, there was little work using statistical andmachine learning approaches.
Some researchersfelt that the kind of ambiguities that appeared toprofit from corpus-based approaches in NLU didnot exist in NLG: if the input is adequately speci-fied, then all the rules that map to a correct out-put can also be explicitly specified.
However,this paper will argue that this view is not cor-rect, and NLG can and does profit from corpus-based methods.
The resistance to corpus-basedapproaches in NLG may have more to do with thefact that in many NLG applications (such as re-port or description generation) the output to begenerated is extremely limited.
As is the casewith NLU, if the language is limited, hand-craftedmethods are adequate and successful.
Thus, it isnot a surprise that the first use of corpus-basedtechniques, at ISI (Knight and Hatzivassiloglou,1995; Langkilde and Knight, 1998) was moti-vated by the use of NLG not in ?traditional?
NLGapplications, but in machine translation, in whichthe range of output language is (potentially) muchlarger.In fact, the situations in NLU and NLG donot actually differ with respect to the notion ofambiguity.
Though it is not a trivial task, wecan fully specify a grammar such that the gen-erated text is not ungrammatical.
But the prob-lem for NLG is not specifying a grammar, but de-termining which part of the grammar to use: togive a simple example, a give-event can be gen-erated with the double-object frame (give Mary abook) or with a prepositional object (give a bookto Mary).
We can easily specify the syntax ofthese two constrictions.
What we need to knowis when to choose which.
But the situation is ex-actly the same in NLU: the problem is knowingwhich grammar rules to use when during analy-sis.
Thus, just as the mapping from input to outputis ambiguous in NLU, it is ambiguous in NLG,not because the grammar is wrong, but because itleaves too many options.
The difference is that inNLG, different outputs differ not in whether theyare correct (as is the case in NLU), but in whetherthey are appropriate or felicitous in a given con-text.
Thus, the need for corpus-based approachesis less apparent.Determining which linguistic forms are appro-priate in what contexts is a hard task.
The intro-spective grammaticality judgment that (perhaps)is legitimate in the study of syntax is method-ologically suspect in the study of language use incontext, and most work in linguistic pragmaticsis in fact corpus-based, such as Prince?s work us-ing the Watergate transcripts and similar corpora(Prince, 1981).
Thus, it is clear that the role ofcorpus-based methods in NLG is not to displacetraditional methods, but rather to accelerate them.If indeed corpus-based methods are necessary inany case, we may as well use automated proce-dures for discovering regularities; we no longerneed to use multi-colored pencils to mark up pa-per copies.
For the researcher, there is enough leftto do: the corpus-based techniques still requirelinguistic research in order to determine whichfeatures to code for (i.e., what linguistic phenom-ena to count).
To the extent that corpus-basedmethods fail currently, it is largely because we aresubstituting easily codable features for those thatare more difficult to code, or because we are sim-ply coding the wrong features.
It is not becausethere is some hidden truth which traditional lin-guistic methodologies have access to but corpus-based methods do not, because they are not in factin opposition to each other.Finally, the emphasis on evaluation that thecorpus-based techniques in NLU have broughtwith them have often aroused animosity in theNLG community.
Evaluation is necessary fordevelopment purposes when using corpus-basedtechniques: it is easy to generate many differ-ent hypotheses, and we need to be able to chooseamong them.
Since this is crucial, increased at-tention needs to be paid to evaluation in gen-eration (Bangalore et al, 2000; Rambow et al,2001).
But again, the situation is in fact not dif-ferent from a traditional linguistic methodology:theories about language use in context need to bedefeasible on empirical grounds and hence needto be evaluated against a corpus.
Of course, thechoice of evaluation corpus is an important one,and the costs associated with compiling and an-notating corpora can greatly impact the choice ofevaluation corpus and hence the evaluation.In conclusion, NLG has nothing to fear fromcorpus-based methods.
Instead, the NLG com-munity can continue to provide a test-bed for lin-guists to exercise their theories (to a much greaterextent than can NLU).
The difference is that ev-eryone can now start using computers.ReferencesSrinivas Bangalore, Owen Rambow, and Steve Whit-taker.
2000.
Evaluation metrics for generation.
InProceedings of the First International Natural Lan-guage Generation Conference (INLG2000), MitzpeRamon, Israel.K.
Knight and V. Hatzivassiloglou.
1995.
Two-level,many-paths generation.
In 33rd Meeting of the As-sociation for Computational Linguistics (ACL?95).Irene Langkilde and Kevin Knight.
1998.
The prac-tical value of n-grams in generation.
In Proceed-ings of the Ninth International Natural LanguageGeneration Workshop (INLG?98), Niagara-on-the-Lake, Ontario.Ellen F. Prince.
1981.
Topicalization, focus move-ment and yiddish movement: A pragmatic dif fer-entiation.
In D. Alford, editor, Proceedings of theSeventh Annual Meeting of the Berkely LinguisticsSociety, pages 249?264.
BLS.Owen Rambow, Monica Rogati, and Marilyn Walker.2001.
Evaluating a trainable sentence planner for aspoken dialogue system.
In 39th Meeting of the As-sociation for Computational Linguistics (ACL?01),Toulouse, France.
