Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1?8,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsA Syntax-Directed Translator with Extended Domain of LocalityLiang HuangDept.
of Comp.
& Info.
Sci.Univ.
of PennsylvaniaPhiladelphia, PA 19104lhuang3@cis.upenn.eduKevin KnightInfo.
Sci.
Inst.Univ.
of Southern CaliforniaMarina del Rey, CA 90292knight@isi.eduAravind JoshiDept.
of Comp.
& Info.
Sci.Univ.
of PennsylvaniaPhiladelphia, PA 19104joshi@linc.cis.upenn.eduAbstractA syntax-directed translator first parsesthe source-language input into a parse-tree, and then recursively converts the treeinto a string in the target-language.
Wemodel this conversion by an extended tree-to-string transducer that have multi-leveltrees on the source-side, which gives oursystem more expressive power and flexi-bility.
We also define a direct probabil-ity model and use a linear-time dynamicprogramming algorithm to search for thebest derivation.
The model is then ex-tended to the general log-linear frame-work in order to rescore with other fea-tures like n-gram language models.
Wedevise a simple-yet-effective algorithm togenerate non-duplicate k-best translationsfor n-gram rescoring.
Initial experimen-tal results on English-to-Chinese transla-tion are presented.1 IntroductionThe concept of syntax-directed (SD) translationwas originally proposed in compiling (Irons, 1961;Lewis and Stearns, 1968), where the source programis parsed into a tree representation that guides thegeneration of the object code.
Following Aho andUllman (1972), a translation, as a set of string pairs,can be specified by a syntax-directed translationschema (SDTS), which is essentially a synchronouscontext-free grammar (SCFG) that generates twolanguages simultaneously.
An SDTS also induces atranslator, a device that performs the transformationinduces implementsSD translator(source parser + recursive converter)specifies translation(string relation)SD translation schema(synchronous grammar)Figure 1: The relationship among SD concepts,adapted from (Aho and Ullman, 1972).??????SNP(1)?
VPVB(2)?
NP(3)?,SVB(2)?
NP(1)?
NP(3)??????
?Figure 2: An example of complex reordering repre-sented as an STSG rule, which is beyond any SCFG.from input string to output string.
In this context, anSD translator consists of two components, a source-language parser and a recursive converter which isusually modeled as a top-down tree-to-string trans-ducer (Ge?cseg and Steinby, 1984).
The relationshipamong these concepts is illustrated in Fig.
1.This paper adapts the idea of syntax-directedtranslator to statistical machine translation (MT).We apply stochastic operations at each node of thesource-language parse-tree and search for the bestderivation (a sequence of translation steps) that con-verts the whole tree into some target-language stringwith the highest probability.
However, the structuraldivergence across languages often results in non-isomorphic parse-trees that is beyond the power ofSCFGs.
For example, the S(VO) structure in Englishis translated into a VSO word-order in Arabic, an in-stance of complex reordering not captured by any1SCFG (Fig.
2).To alleviate the non-isomorphism problem, (syn-chronous) grammars with richer expressive powerhave been proposed whose rules apply to larger frag-ments of the tree.
For example, Shieber and Sch-abes (1990) introduce synchronous tree-adjoininggrammar (STAG) and Eisner (2003) uses a syn-chronous tree-substitution grammar (STSG), whichis a restricted version of STAG with no adjunctions.STSGs and STAGs generate more tree relations thanSCFGs, e.g.
the non-isomorphic tree pair in Fig.
2.This extra expressive power lies in the extended do-main of locality (EDL) (Joshi and Schabes, 1997),i.e., elementary structures beyond the scope of one-level context-free productions.
Besides being lin-guistically motivated, the need for EDL is also sup-ported by empirical findings in MT that one-levelrules are often inadequate (Fox, 2002; Galley et al,2004).
Similarly, in the tree-transducer terminology,Graehl and Knight (2004) define extended tree trans-ducers that have multi-level trees on the source-side.Since an SD translator separates the source-language analysis from the recursive transformation,the domains of locality in these two modules are or-thogonal to each other: in this work, we use a CFG-based Treebank parser but focuses on the extendeddomain in the recursive converter.
Following Gal-ley et al (2004), we use a special class of extendedtree-to-string transducer (xRs for short) with multi-level left-hand-side (LHS) trees.1 Since the right-hand-side (RHS) string can be viewed as a flat one-level tree with the same nonterminal root from LHS(Fig.
2), this framework is closely related to STSGs:they both have extended domain of locality on thesource-side, while our framework remains as a CFGon the target-side.
For instance, an equivalent xRsrule for the complex reordering in Fig.
2 would beS(x1:NP, VP(x2:VB, x3:NP))?
x2 x1 x3While Section 3 will define the model formally,we first proceed with an example translation fromEnglish to Chinese (note in particular that the in-verted phrases between source and target):1Throughout this paper, we will use LHS and source-sideinterchangeably (so are RHS and target-side).
In accordancewith our experiments, we also use English and Chinese as thesource and target languages, opposite to the Foreign-to-Englishconvention of Brown et al (1993).
(a) the gunman was [killed]1 by [the police]2 .parser ?
(b)SNP-CDTtheNNgunmanVPVBDwasVP-CVBNkilledPPINbyNP-CDTtheNNpolicePUNC.r1, r2 ?
(c) qiangshouVPVBDwasVP-CVBNkilledPPINbyNP-CDTtheNNpolice?r3 ?
(d) qiangshou beiNP-CDTtheNNpoliceVBNkilled?r5 ?
r4 ?
(e) qiangshou bei [jingfang]2 [jibi]1 ?Figure 3: A synatx-directed translation process forExample (1).
(1) the gunman was killed by the police .qiangshou[gunman]bei[passive]jingfang[police]jibi[killed]?.Figure 3 shows how the translator works.
The En-glish sentence (a) is first parsed into the tree in (b),which is then recursively converted into the Chinesestring in (e) through five steps.
First, at the rootnode, we apply the rule r1 which preserves the top-level word-order and translates the English periodinto its Chinese counterpart:(r1) S (x1:NP-C x2:VP PUNC (.)
) ?
x1 x2 ?2Then, the rule r2 grabs the whole sub-tree for ?thegunman?
and translates it as a phrase:(r2) NP-C ( DT (the) NN (gunman) )?
qiangshouNow we get a ?partial Chinese, partial English?
sen-tence ?qiangshou VP ??
as shown in Fig.
3 (c).
Ourrecursion goes on to translate the VP sub-tree.
Herewe use the rule r3 for the passive construction:(r3)VPVBDwasVP-Cx1:VBN PPINbyx2:NP-C?
bei x2 x1which captures the fact that the agent (NP-C, ?thepolice?)
and the verb (VBN, ?killed?)
are alwaysinverted between English and Chinese in a passivevoice.
Finally, we apply rules r4 and r5 which per-form phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chi-nese string in (e).2 Previous WorkIt is helpful to compare this approach with recent ef-forts in statistical MT.
Phrase-based models (Koehnet al, 2003; Och and Ney, 2004) are good at learn-ing local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling the re-orderings of phrases themselves, especially betweenlanguage pairs with very different word-order.
Thisis because the generative capacity of these modelslies within the realm of finite-state machinery (Ku-mar and Byrne, 2003), which is unable to processnested structures and long-distance dependencies innatural languages.Syntax-based models aim to alleviate this prob-lem by exploiting the power of synchronous rewrit-ing systems.
Both Yamada and Knight (2001) andChiang (2005) use SCFGs as the underlying model,so their translation schemata are syntax-directed asin Fig.
1, but their translators are not: both systemsdo parsing and transformation in a joint search, es-sentially over a packed forest of parse-trees.
To thisend, their translators are not directed by a syntac-tic tree.
Although their method potentially consid-ers more than one single parse-tree as in our case,the packed representation of the forest restricts thescope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, sothat the latter can have an extended domain of local-ity.
In addition, our translator also enjoys a speed-up by this decoupling, with each of the two stageshaving a smaller search space.
In fact, the recursivetransfer step can be done by a a linear-time algo-rithm (see Section 5), and the parsing step is alsofast with the modern Treebank parsers, for instance(Collins, 1999; Charniak, 2000).
In contrast, theirdecodings are reported to be computationally expen-sive and Chiang (2005) uses aggressive pruning tomake it tractable.
There also exists a compromisebetween these two approaches, which uses a k-bestlist of parse trees (for a relatively small k) to approx-imate the full forest (see future work).Besides, our model, as being linguistically mo-tivated, is also more expressive than the formallysyntax-based models of Chiang (2005) and Wu(1997).
Consider, again, the passive example in ruler3.
In Chiang?s SCFG, there is only one nonterminalX, so a corresponding rule would be?
was X(1) by X(2), bei X(2) X(1) ?which can also pattern-match the English sentence:I was [asleep]1 by [sunset]2 .and translate it into Chinese as a passive voice.
Thisproduces very odd Chinese translation, because here?was A by B?
in the English sentence is not a pas-sive construction.
By contrast, our model appliesrule r3 only if A is a past participle (VBN) and Bis a noun phrase (NP-C).
This example also showsthat, one-level SCFG rule, even if informed by theTreebank as in (Yamada and Knight, 2001), is notenough to capture a common construction like thiswhich is five levels deep (from VP to ?by?
).There are also some variations of syntax-directedtranslators where dependency structures are usedin place of constituent trees (Lin, 2004; Ding andPalmer, 2005; Quirk et al, 2005).
Although theyshare with this work the basic motivations and simi-lar speed-up, it is difficult to specify re-ordering in-formation within dependency elementary structures,so they either resort to heuristics (Lin) or a sepa-rate ordering model for linearization (the other two3works).2 Our approach, in contrast, explicitly mod-els the re-ordering of sub-trees within individualtransfer rules.3 Extended Tree-to-String TranducersIn this section, we define the formal machinery ofour recursive transformation model as a special caseof xRs transducers (Graehl and Knight, 2004) thathas only one state, and each rule is linear (L) andnon-deleting (N) with regarding to variables in thesource and target sides (henth the name 1-xRLNs).Definition 1.
A 1-xRLNs transducer is a tuple(N,?,?,R) where N is the set of nonterminals, ?is the input alphabet, ?
is the output alphabet, andR is a set of rules.
A rule in R is a tuple (t, s, ?)where:1.
t is the LHS tree, whose internal nodes are la-beled by nonterminal symbols, and whose fron-tier nodes are labeled terminals from ?
or vari-ables from a set X = {x1, x2, .
.
.};2.
s ?
(X ??)?
is the RHS string;3. ?
is a mapping from X to nonterminals N .We require each variable xi ?
X occurs exactly oncein t and exactly once in s (linear and non-deleting).We denote ?
(t) to be the root symbol of tree t.When writing these rules, we avoid notational over-head by introducing a short-hand form from Galleyet al (2004) that integrates the mapping into the tree,which is used throughout Section 1.
Following TSGterminology (see Figure 2), we call these ?variablenodes?
such as x2:NP-C substitution nodes, sincewhen applying a rule to a tree, these nodes will bematched with a sub-tree with the same root symbol.We also define |X | to be the rank of the rule, i.e.,the number of variables in it.
For example, rules r1and r3 in Section 1 are both of rank 2.
If a rule hasno variable, i.e., it is of rank zero, then it is called apurely lexical rule, which performs a phrasal trans-lation as in phrase-based models.
Rule r2, for in-stance, can be thought of as a phrase pair ?the gun-man, qiangshou?.Informally speaking, a derivation in a transduceris a sequence of steps converting a source-language2Although hybrid approaches, such as dependency gram-mars augmented with phrase-structure information (Alshawi etal., 2000), can do re-ordering easily.r1r2 r3r4 r5r1r2 r6r4 r7r5(a) (b)Figure 4: (a) the derivation in Figure 3; (b) anotherderviation producing the same output by replacingr3 with r6 and r7, which provides another way oftranslating the passive construction:(r6) VP ( VBD (was) VP-C (x1:VBN x2:PP ) )?
x2 x1(r7) PP ( IN (by) x1:NP-C )?
bei x1tree into a target-language string, with each step ap-plying one tranduction rule.
However, it can alsobe formalized as a tree, following the notion ofderivation-tree in TAG (Joshi and Schabes, 1997):Definition 2.
A derivation d, its source and targetprojections, noted E(d) and C(d) respectively, arerecursively defined as follows:1.
If r = (t, s, ?)
is a purely lexical rule (?
= ?
),then d = r is a derivation, where E(d) = t andC(d) = s;2.
If r = (t, s, ?)
is a rule, and di is a (sub-)derivation with the root symbol of its sourceprojection matches the corresponding substitu-tion node in r, i.e., ?
(E(di)) = ?
(xi), thend = r(d1, .
.
.
, dm) is also a derivation, whereE(d) = [xi 7?
E(di)]t and C(d) = [xi 7?C(di)]s.Note that we use a short-hand notation [xi 7?
yi]tto denote the result of substituting each xi with yiin t, where xi ranges over all variables in t.For example, Figure 4 shows two derivations forthe sentence pair in Example (1).
In both cases, thesource projection is the English tree in Figure 3 (b),and the target projection is the Chinese translation.Galley et al (2004) presents a linear-time algo-rithm for automatic extraction of these xRs rulesfrom a parallel corpora with word-alignment andparse-trees on the source-side, which will be usedin our experiments in Section 6.44 Probability Models4.1 Direct ModelDeparting from the conventional noisy-channel ap-proach of Brown et al (1993), our basic model is adirect one:c?
= argmaxcPr(c | e) (2)where e is the English input string and c?
is thebest Chinese translation according to the translationmodel Pr(c | e).
We now marginalize over all En-glish parse trees T (e) that yield the sentence e:Pr(c | e) =??
?T (e)Pr(?, c | e)=??
?T (e)Pr(?
| e) Pr(c | ?)
(3)Rather than taking the sum, we pick the best tree ?
?and factors the search into two separate steps: pars-ing (4) (a well-studied problem) and tree-to-stringtranslation (5) (Section 5):??
= argmax?
?T (e)Pr(?
| e) (4)c?
= argmaxcPr(c | ??)
(5)In this sense, our approach can be considered asa Viterbi approximation of the computationally ex-pensive joint search using (3) directly.
Similarly, wenow marginalize over all derivationsD(??)
= {d | E(d) = ??
}that translates English tree ?
into some Chinesestring and apply the Viterbi approximation again tosearch for the best derivation d?:c?
= C(d?)
= C(argmaxd?D(??
)Pr(d)) (6)Assuming different rules in a derivation are ap-plied independently, we approximate Pr(d) asPr(d) =?r?dPr(r) (7)where the probability Pr(r) of the rule r is estimatedby conditioning on the root symbol ?
(t(r)):Pr(r) = Pr(t(r), s(r) | ?
(t(r)))= c(r)?r?:?(t(r?))=?
(t(r)) c(r?
)(8)where c(r) is the count (or frequency) of rule r inthe training data.4.2 Log-Linear ModelFollowing Och and Ney (2002), we extend the directmodel into a general log-linear framework in orderto incorporate other features:c?
= argmaxcPr(c | e)?
?
Pr(c)?
?
e?
?|c| (9)where Pr(c) is the language model and e?
?|c| is thelength penalty term based on |c|, the length of thetranslation.
Parameters ?, ?, and ?
are the weightsof relevant features.
Note that positive ?
preferslonger translations.
We use a standard trigram modelfor Pr(c).5 Search AlgorithmsWe first present a linear-time algorithm for searchingthe best derivation under the direct model, and thenextend it to the log-linear case by a new variant ofk-best parsing.5.1 Direct Model: Memoized RecursionSince our probability model is not based on the noisychannel, we do not call our search module a ?de-coder?
as in most statistical MT work.
Instead, read-ers who speak English but not Chinese can view it asan ?encoder?
(or encryptor), which corresponds ex-actly to our direct model.Given a fixed parse-tree ?
?, we are to searchfor the best derivation with the highest probability.This can be done by a simple top-down traversal(or depth-first search) from the root of ??
: at eachnode ?
in ?
?, try each possible rule r whose English-side pattern t(r) matches the subtree ???
rooted at ?,and recursively visit each descendant node ?i in ??
?that corresponds to a variable in t(r).
We then col-lect the resulting target-language strings and plugthem into the Chinese-side s(r) of rule r, gettinga translation for the subtree ???
.
We finally take thebest of all translations.With the extended LHS of our transducer, theremay be many different rules applicable at one treenode.
For example, consider the VP subtree inFig.
3 (c), where both r3 and r6 can apply.
As a re-sult, the number of derivations is exponential in thesize of the tree, since there are exponentially many5decompositions of the tree for a given set of rules.This problem can be solved by memoization (Cor-men et al, 2001): we cache each subtree that hasbeen visited before, so that every tree node is visitedat most once.
This results in a dynamic program-ming algorithm that is guaranteed to run in O(npq)time where n is the size of the parse tree, p is themaximum number of rules applicable to one treenode, and q is the maximum size of an applicablerule.
For a given rule-set, this algorithm runs in timelinear to the length of the input sentence, since pand q are considered grammar constants, and n isproportional to the input length.
The full pseudo-code is worked out in Algorithm 1.
A restrictedversion of this algorithm first appears in compilingfor optimal code generation from expression-trees(Aho and Johnson, 1976).
In computational linguis-tics, the bottom-up version of this algorithm resem-bles the tree parsing algorithm for TSG by Eisner(2003).
Similar algorithms have also been proposedfor dependency-based translation (Lin, 2004; Dingand Palmer, 2005).5.2 Log-linear Model: k-best SearchUnder the log-linear model, one still prefers tosearch for the globally best derivation d?:d?
= argmaxd?D(??)Pr(d)?
Pr(C(d))?e?
?|C(d)| (10)However, integrating the n-gram model with thetranslation model in the search is computationallyvery expensive.
As a standard alternative, ratherthan aiming at the exact best derivation, we searchfor top-k derivations under the direct model usingAlgorithm 1, and then rerank the k-best list with thelanguage model and length penalty.Like other instances of dynamic programming,Algorithm 1 can be viewed as a hypergraph searchproblem.
To this end, we use an efficient algo-rithm by Huang and Chiang (2005, Algorithm 3)that solves the general k-best derivations problemin monotonic hypergraphs.
It consists of a normalforward phase for the 1-best derivation and a recur-sive backward phase for the 2nd, 3rd, .
.
.
, kth deriva-tions.Unfortunately, different derivations may have thesame yield (a problem called spurious ambiguity),due to multi-level LHS of our rules.
In practice, thisresults in a very small ratio of unique strings amongtop-k derivations.
To alleviate this problem, deter-minization techniques have been proposed by Mohriand Riley (2002) for finite-state automata and ex-tended to tree automata by May and Knight (2006).These methods eliminate spurious ambiguity by ef-fectively transforming the grammar into an equiva-lent deterministic form.
However, this transforma-tion often leads to a blow-up in forest size, which isexponential to the original size in the worst-case.So instead of determinization, here we present asimple-yet-effective extension to the Algorithm 3 ofHuang and Chiang (2005) that guarantees to outputunique translated strings:?
keep a hash-table of unique strings at each vertexin the hypergraph?
when asking for the next-best derivation of a ver-tex, keep asking until we get a new string, andthen add it into the hash-tableThis method should work in general for anyequivalence relation (say, same derived tree) that canbe defined on derivations.6 ExperimentsOur experiments are on English-to-Chinese trans-lation, the opposite direction to most of the recentwork in SMT.
We are not doing the reverse directionat this time partly due to the lack of a sufficientlygood parser for Chinese.6.1 Data PreparationOur training set is a Chinese-English parallel corpuswith 1.95M aligned sentences (28.3M words on theEnglish side).
We first word-align them by GIZA++,then parse the English side by a variant of Collins(1999) parser, and finally apply the rule-extractionalgorithm of Galley et al (2004).
The resulting ruleset has 24.7M xRs rules.
We also use the SRI Lan-guage Modeling Toolkit (Stolcke, 2002) to train aChinese trigram model with Knesser-Ney smooth-ing on the Chinese side of the parallel corpus.Our evaluation data consists of 140 short sen-tences (< 25 Chinese words) of the Xinhua portionof the NIST 2003 Chinese-to-English evaluation set.Since we are translating in the other direction, weuse the first English reference as the source inputand the Chinese as the single reference.6Algorithm 1 Top-down Memoized Recursion1: function TRANSLATE(?
)2: if cache[?]
defined then .
this sub-tree visited before?3: return cache[?
]4: best?
05: for r ?
R do .
try each rule r6: matched, sublist?
PATTERNMATCH(t(r), ?)
.
tree pattern matching7: if matched then .
if matched, sublist contains a list of matched subtrees8: prob?
Pr(r) .
the probability of rule r9: for ?i ?
sublist do10: pi, si ?
TRANSLATE(?i) .
recursively solve each sub-problem11: prob?
prob ?
pi12: if prob > best then13: best?
prob14: str ?
[xi 7?
si]s(r) .
plug in the results15: cache[?]?
best, str .
caching the best solution for future use16: return cache[?]
.
returns the best string with its prob.6.2 Initial ResultsWe implemented our system as follows: for each in-put sentence, we first run Algorithm 1, which returnsthe 1-best translation and also builds the derivationforest of all translations for this sentence.
Then weextract the top 5000 non-duplicate translated stringsfrom this forest and rescore them with the trigrammodel and the length penalty.We compared our system with a state-of-the-artphrase-based system Pharaoh (Koehn, 2004) on theevaluation data.
Since the target language is Chi-nese, we report character-based BLEU score insteadof word-based to ensure our results are indepen-dent of Chinese tokenizations (although our lan-guage models are word-based).
The BLEU scoresare based on single reference and up to 4-gram pre-cisions (r1n4).
Feature weights of both systems aretuned on the same data set.3 For Pharaoh, we use thestandard minimum error-rate training (Och, 2003);and for our system, since there are only two in-dependent features (as we always fix ?
= 1), weuse a simple grid-based line-optimization along thelanguage-model weight axis.
For a given language-model weight ?, we use binary search to find the bestlength penalty ?
that leads to a length-ratio closest3In this sense, we are only reporting performances on thedevelopment set at this point.
We will report results tuned andtested on separate data sets in the final version of this paper.Table 1: BLEU (r1n4) score resultssystem BLEUPharaoh 25.5direct model (1-best) 20.3log-linear model (rescored 5000-best) 23.8to 1 against the reference.
The results are summa-rized in Table 1.
The rescored translations are betterthan the 1-best results from the direct model, but stillslightly worse than Pharaoh.7 Conclusion and On-going WorkThis paper presents an adaptation of the clas-sic syntax-directed translation with linguistically-motivated formalisms for statistical MT.
Currentlywe are doing larger-scale experiments.
We are alsoinvestigating more principled algorithms for inte-grating n-gram language models during the search,rather than k-best rescoring.
Besides, we will extendthis work to translating the top k parse trees, insteadof committing to the 1-best tree, as parsing errorscertainly affect translation quality.7ReferencesA.
V. Aho and S. C. Johnson.
1976.
Optimal code gen-eration for expression trees.
J. ACM, 23(3):488?501.Alfred V. Aho and Jeffrey D. Ullman.
1972.
The The-ory of Parsing, Translation, and Compiling, volume I:Parsing.
Prentice Hall, Englewood Cliffs, New Jersey.Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
Computa-tional Linguistics, 26(1):45?60.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
of NAACL, pages 132?139.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of the 43rdACL.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Thomas H. Cormen, Charles E. Leiserson, Ronald L.Rivest, and Clifford Stein.
2001.
Introduction to Al-gorithms.
MIT Press, second edition.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probablisitic synchronous dependency in-sertion grammars.
In Proceedings of the 43rd ACL.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of ACL(companion volume), pages 205?208.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In In Proc.
of EMNLP.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.F.
Ge?cseg and M. Steinby.
1984.
Tree Automata.Akade?miai Kiado?, Budapest.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In HLT-NAACL, pages 105?112.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of the Nineth InternationalWorkshop on Parsing Technologies (IWPT-2005), 9-10October 2005, Vancouver, Canada.E.
T. Irons.
1961.
A syntax-directed compiler for AL-GOL 60.
Comm.
ACM, 4(1):51?55.Aravind Joshi and Yves Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, volume 3, pages 69?
124.
Springer, Berlin.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL, pages 127?133.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proc.
of AMTA, pages 115?124.Shankar Kumar and William Byrne.
2003.
A weightedfinite state transducer implementation of the alignmenttemplate model for statistical machine translation.
InProc.
of HLT-NAACL, pages 142?149.P.
M. Lewis and R. E. Stearns.
1968.
Syntax-directedtransduction.
Journal of the ACM, 15(3):465?488.Dekang Lin.
2004.
A path-based transfer model for ma-chine translation.
In Proceedings of the 20th COLING.Jonathan May and Kevin Knight.
2006.
A better n-bestlist: Practical determinization of weighted finite treeautomata.
Submitted to HLT-NAACL 2006.Mehryar Mohri and Michael Riley.
2002.
An efficientalgorithm for the n-best-strings problem.
In Proceed-ings of the International Conference on Spoken Lan-guage Processing 2002 (ICSLP ?02), Denver, Col-orado, September.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
of ACL.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30:417?449.Franz Och.
2003.
Minimum error rate training for statis-tical machine translation.
In Proc.
of ACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the 43rd ACL.Stuart Shieber and Yves Schabes.
1990.
Synchronoustree-adjoining grammars.
In Proc.
of COLING, pages253?258.Andrea Stolcke.
2002.
Srilm: an extensible languagemodeling toolkit.
In Proc.
of ICSLP.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proc.
of ACL.8
