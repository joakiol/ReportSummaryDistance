Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 172?179,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCrowdsourcing Document Relevance Assessment with Mechanical TurkCatherine Grady and Matthew LeaseSchool of InformationUniversity of Texas at Austin{cgrady,ml}@ischool.utexas.eduAbstractWe investigate human factors involved in de-signing effective Human Intelligence Tasks(HITs) for Amazon?s Mechanical Turk1.
Inparticular, we assess document relevance tosearch queries via MTurk in order to evaluatesearch engine accuracy.
Our study varies fourhuman factors and measures resulting experi-mental outcomes of cost, time, and accuracyof the assessments.
While results are largelyinconclusive, we identify important obstaclesencountered, lessons learned, related work,and interesting ideas for future investigation.Experimental data is also made publicly avail-able for further study by the community2.1 IntroductionEvaluating accuracy of new search algorithms onever-growing information repositories has becomeincreasingly challenging in terms of the time andexpense required by traditional evaluation tech-niques.
In particular, while the Cranfield evalua-tion paradigm has proven remarkably effective fordecades (Voorhees, 2002), enormous manual effortis involved in assessing topic relevance of many dif-ferent documents to many different queries.
Conse-quently, there has been significant recent interest indeveloping more scalable evaluation methodology.This has included developing robust accuracy met-rics using few assessments (Buckley and Voorhees,2004), inferring implicit relevance assessments from1http://aws.amazon.com/mturk2http://www.ischool.utexas.edu/?ml/datauser behavior (Joachims, 2002), more carefully se-lecting documents for assessment (Aslam and Pavlu,2008; Carterette et al, 2006), and leveraging crowd-sourcing (Alonso et al, 2008).We build on this line of work to investigat-ing crowdsourcing-based relevance assessment viaMTurk.
While MTurk has quickly become popularas a means of obtaining data annotations quickly andinexpensively (Snow et al, 2008), relatively little at-tention has been given to addressing human-factorsinvolved in crowdsourcing and their impact on re-sultant cost, time, and accuracy of the annotationsobtained (Mason and Watts, 2009).
The advent ofcrowdsourcing has led to many researchers, whosework might otherwise fall outside the realm ofhuman-computer interaction (HCI), suddenly find-ing themselves creating HITs for MTurk and therebydirectly confronting important issues of interface de-sign and usability which could significantly impactthe quality or quantity of annotations they obtain.
Asimilar observation has been made recently regard-ing the importance of effective HCI for obtainingquality answers from users in a social search set-ting (Horowitz and Kamvar, 2010).Our overarching hypothesis is that better address-ing human factors in HIT design can yield signifi-cantly reduce cost, reduce time, and/or increase ac-curacy of the annotations obtained via crowdsourc-ing.
Such improvement could come through a va-riety of complimentary effects, such as attractingmore or better workers, incentivizing them to do bet-ter work, better explaining the task to be performedand reducing confusion, etc.
While the results ofthis study are largely inconclusivewith regard to our172experimental hypothesis, other contributions of thework are identified in the abstract above.2 BackgroundTo evaluate search accuracy in the Cranfieldparadigm (Voorhees, 2002), a predefined set of doc-uments (e.g., web pages) are typically manually as-sessed for relevance with respect to some fixed setof topics.
Each topic corresponds to some staticinformation need of a hypothetical user.
Becauselanguage allows meaning to be conveyed in vari-ous ways and degrees of brevity, each topic can beexpressed via a myriad of different queries.
Ta-ble 1 shows the four topics used in our study whichwere generated by NIST for TREC3.
We do usethe paragraph-length ?narrative?
queries under an(untested) assumption that they are overly complexand technical for a layman assessor.
Instead, weuse (1) the short keyword ?title?
queries and (2)more verbose and informative ?description?
queries,which are typically expressed as a one-sentencequestion or statement.NIST has typically invested significant time train-ing annotators, something far less feasible in acrowdsourced setting.
NIST has also typically em-ployed a single human assessor per topic to en-sure consistent topic interpretation and relevance as-sessment.
One downside of this practice is limitedscalability of annotation, particularly in a crowd-sourced setting.
When multiple annotators havebeen used, previous studies have also found rela-tively low inner-annotator agreement for relevanceassessment due to the highly subjective nature ofrelevance (Voorhees, 2002).
Thus in addition to re-ducing time and cost of assessment, crowdsourcingmay also enable us to improve assessment accuracyby integrating assessment decisions by a commit-tee of annotators.
This is particularly important forgenerating reusable test collections for benchmark-ing.
Practical costs involved in relevance assess-ment based on standard pooling methods is signif-icant and becoming increasingly prohibitive as col-lection sizes grow (Carterette et al, 2009).MTurk allows ?requesters?
to crowdsource largenumbers of HITs online which workers can search,browse, preview, accept, and complete or abandon.3http://trec.nist.gov3.
Joint Ventures.
Document will announce a new jointventure involving a Japanese company.13.
Mitsubishi Heavy Industries Ltd.
Document refers toMitusbishi Heavy Industries Ltd.68.
Health Hazards from Fine-Diameter Fibers.
Docu-ment will report actual studies, or even unsubstantiated con-cerns about the safety to manufacturing employees and in-stallation workers of fine-diameter fibers used in insulationand other products.78.
Greenpeace.
Document will report activity by Green-peace to carry out their environmental protection goals.Table 1: The four TREC topics used in our study.
Topicnumber and <title> field are shown in bold.
Remain-ing text constitutes the description (<desc>) field.With regard to measuring the impact of differentdesign alternatives on resulting HIT effectiveness,MTurk provides requesters with many useful statis-tics regarding completion of their HITs.
Some ef-fects cannot be measured, however, such as whenHITs are skipped, when HITs are viewed in searchresults but not selected, and other outcomes whichcould usefully inform effective HIT design.3 MethodologyOur study investigated how varying certain aspectsof HIT design affected annotation accuracy andtime, as well as the relationship between expenseand these outcomes.
In particular, workers wereasked to make binary assessments regarding the rel-evance of various documents to different queries.3.1 Experimental VariablesWe varied four simple aspects of HIT design:?
Query: <title> vs. <desc>?
Terminology: HIT title of ?binary relevance judg-ment?
(technical) vs. ?yes/no decision?
(layman)?
Pay: $0.01 vs. $0.02?
Bonus: no bonus offered vs. $0.02The Query is clearly central to relevance assess-ment since it provides the annotator?s primary ba-sis for judging relevance.
Since altering a querycan have enormous impact on the assessment, andbecause we were testing the ability of MechanicalTurk workers to replicate assessments made previ-ously by TREC assessors, we preserved wording of173the queries as they appeared in the original TRECtopics (see ?2).
We hypothesized that the greater de-tail found in the topic description vs. the title wouldimprove accuracy with some corresponding increasein HIT completion time (longer query to read, attimes with more stilted language, and more specificrelevance criteria requiring more careful reading ofdocuments).
An alternative hypothesis would bethat a very conscientious worker might take longerwrestling with a vague title query.Terminology: the HIT title is arguably one of aHIT?s more prominent features since it is one of thefirst (and often the only) description of a HIT a po-tential worker sees.
An attractive title could conceiv-ably draw workers to a task while an unattractive onecould repel them.
Besides the simple variation stud-ied here, future experiments could test other aspectsof title formulation.
For example, greater specificityas to the content of documents or topics within theHIT could attract workers that are knowledgeable orinterested in a particular subject.
Additionally, a titlethat indicates a task is for research purposes mightattract workers motivated to contribute to society.Pay: the base pay rate has obvious implicationsfor attracting workers and incentivizing them to doquality work.
While anecdotal knowledge suggestedthe ?going rate?
for simple HITs was about $0.02,we started at the lowest possible rate and increasedfrom there.
Although higher pay rates are certainlymore attractive to legitimate workers, they also tendto attract more spammers, so determining appropri-ate pay is something of a careful balancing act.Bonus: Two important questions are 1) How doesknowing that one could receive a bonus affect per-formance on the current HIT?, and 2) How does ac-tually receiving a bonus affect performance on fu-ture HITs?
We focused on the first question.
Whenbonuses were offered, we both advertised this factin the HIT title (see Title 4 above) and appended thefollowing statement to the instructions: ?
[b]onuseswill be given for good work with good explana-tions of the reasoning behind your relevance assess-ment.?
If a worker?s explanation made clear whyshe made the relevance judgment she did, bonuseswere awarded regardless of the assessment?s correct-ness with regard to ground truth.
Decisions to awardbonus pay were made manually (see ?5).3.2 Experimental ConstantsVarious factors kept constant in our study could alsobe interesting to investigate in future work:?
Description: the worker may optionally view abrief description of the task before accepting theHIT.
For all HITs, our description was simply: ?
(1)Decide whether a document is relevant to a topic, 2)Click ?relevant?
or ?not relevant?, and 3) Submit?.?
Keywords: HITs were advertised for search viakeywords ?judgment, document, relevance, search??
Duration: once accepted, all HITs had to be com-pleted within one hour?
Approval Rate: workers had to have a 95% ap-proval rate to accept our HITs?
HIT approval: all HITs were accepted, but ap-proval was not immediate to suggest that HITs werebeing carefully reviewed before pay was awarded?
Feedback to workers: none givenMore careful selection of high-interest Keywords(e.g., ?easy?
or ?fun?)
may be a surprisingly effec-tive way to attract more workers.
It would be veryinteresting to analyze the query logs for keywordsused by Workers in searching for HITs of interest.Omar Alonso suggests workers should alwaysbe paid (personal communication).
Given the lowcost involved, keeping Workers individually happyavoids the effort of having to justify rejections to an-gry Workers, maintains one?s reputation for attract-ing Workers, and still allows problematic workers tobe filtered out in future batches.3.3 Experimental OutcomesWith regard to outcomes, we were principally in-terested in measuring accuracy, time, and expense.Base statistics, such as the completion time of a par-ticular HIT, allowed us to compute derived statis-tics like averages per topic, per Worker, per Batch,per experimental variable, etc.
We could then alsolook for correlations between outcomes as well asbetween experimental variables and outcomes.Accuracy was measured by simply computing theannotator mean accuracy with regard to ?groundtruth?
binary relevance labels from NIST.
A vari-ety of other possibilities exist, such as deciding bi-nary annotations by majority vote and comparingthese to ground truth.
Recent work has explored en-semble methods for weighting and combining anno-174Topic Relevant Non-Relevant3 48, 55, 84, 120 8513 28, 30 *193*, 84, 11768 157, 163, 170, 182, 18678 *9978* 134, 166, 167,*0062*Table 2: Documents assessed per topic, along with ?true?binary relevance judgments according to official TRECNIST annotation.
Document prefixes used in table: (3and 13) WSJ920324-, except *WSJ920323-0193*,(68 and 78) AP901231- except *FBIS4-9978* and*WSJ920324-0062*.
Only one document, 84, wasshared across queries (3 and 13).# Name Query Term.
Pay Bonus1 Baseline title BRJ $0.01 -2 P=0.02 title BRJ $0.02 -3 T=yes/no title yes/no $0.01 -4 Q=desc.
desc.
yes/no $0.01 -5 B=0.02 title yes/no $0.01 $0.02Table 3: Experimental matrix.
Batches 2 and 3 changedone variable with respect to Batch 1.
Batches 4 and 5changed one variable with respect to Batch 3.
Terminol-ogy varied as specified in ?3.
For batch 5, 23 bonuseswere awarded at total cost of $0.46.tations (Snow et al, 2008; Whitehill et al, 2009)which also could have been used like majority vote.As for time, we measured HIT completion time(from acceptance to completion) and Batch com-pletion time (from publishing the Batch to all itsHITs being completed).
We only anecdotally mea-sured our own time required to generate HIT de-signs, shepherd the Batches, assess outcomes, etc.Cost was measured solely with respect to whatwas paid to Workers and does not include overheadcosts charged by Amazon (?2).
We also did not ac-count for the cost of our own salaries, equipment, orother indirect expenses associated with the work.3.4 Additional DetailsAssessment was performed on XML documentstaken from the TREC TIPSTER collection of newsarticles.
Documents were simply presented as textafter simple pre-processing; a better alternative forthe future would be to associate an attractive stylesheet with the XML to enhance readability and at-tractiveness of HITs.
Relatively little pre-processingHITs per Worker01020304050600102030405060Figure 1: Number of HITs completed by each workerwas performed: (1) XML tags were replaced withHTML, (2) document ID, number, and TREC-related info was commented out, and (3) paragraphtags were added to break up text.Our basic HIT layout was based on a pre-existingtemplate for assessing binary relevance provided byOmar Alonso (personal communication).
This tem-plate reflected several useful design decisions likehaving HITs be self-contained rather than referringto content at an external URL, a design previouslyfound to be effective (Alonso et al, 2008).4 EvaluationWe performed five batch evaluations, shown in Ta-ble 3.
For each of the four topics shown in Ta-ble 1, five documents were assessed (Table 2), andten assessments (one per HIT) were collected foreach document.
Each batch therefore consisted of4 ?
5 ?
10 = 200 HITs, for an overall total of 1000HITs.
Document length varied from 162 words to2129 words per document (including HTML tagsand single-character tokens).
Each HIT required theworker to make a single binary relevance judgment(i.e.
relevant or non-relevant) for a given query-document pair.
In all cases, ?ground truth?
wasavailable to us in the form of prior relevance assess-ments created by NIST.
149 unique Workers com-1750 10 20 30 40 50 600.00.20.40.60.81.0#HITsAccuracyFigure 2: HITs completed vs. accuracy achieved showsnegligible direct correlation: Pearson |?| < 0.01.0 200 400 6000.00.20.40.60.81.0SecondsAccuracyFigure 3: HIT completion time vs. accuracy achievedshows negligible direct correlation: Pearson |?| ?
0.06.pleted the 1000 HITs, with some Workers complet-ing far more HITs than others (Figure 1).We did not restrict Workers from accepting HITsfrom different batches, and some Workers even par-ticipated in all 5 Batches.
Since in some cases asingle Worker assessed the same query-documentpair multiple times, our results likely reflect unan-ticipated effects of training or fatigue (see ?5).Statistical significance was measured via a two-tailed unpaired t-test.
The only significant outcomesobserved were increase in comment length and num-ber of comments for higher-payingor bonus batches.We note p-values < 0.05 where they occur.Maximum accuracy of 70.5% was achieved withBatch 3, which featured use of Title query andyes/no response.
Similar accuracy of 69.5% wasalso achieved in both Batch 1 and 2.
Accuracy fellin Batch 4 (using the Description query) to 66.5%,and fell further to 64% in Batch 5, which featuredbonuses.
With regard to varying use of Title vs. De-scription query (Batches 1-3,5 vs. 4), accuracy forthe Title query HITs was 68.4% vs. the 66.5% re-ported above for Batch 4.
Thus use of Descriptionqueries was not observed to lead to more accurateassessments.
HIT completion time was also highestfor Batch 4, with workers taking an average of 72sto complete a HIT, vs. mean HIT completion time of63s over the four Title query batches.The number of unique workers (UW) per Batchgives some sense of how attractive a Batch was,where a high number could alternatively suggestmany workers were attracted (positive) or incentiveswere too weak to encourage a few Workers to domany HITs (negative).
UW in batches 1-4 rangedfrom 64-72.
This fell to 38 UW in Batch 5 (bonusbatch), perhaps indicating that workers were incen-tivized to do more HITs to earn bonuses.
At thesame time that the number of workers went down,the accuracy per worker went up, with the averageworker judging 3.37 documents correctly, comparedto a range of 2.10 - 2.20 correct answers per aver-age worker for Batches 1-3 and 1.85 correct answersper average worker for Batch 4 (which, interestingly,had slightly more UWs than the other batches).176Subset Cost Batch Completion Time HIT Completion Time#B HITs noB withB Total MeanH MeanB sdB Total MeanH MeanB sdHQuery 3 5 250 $3.00 $3.14 N/A N/A N/A N/A 16127 64.50 3225.4 92.48Query 13 5 250 $3.00 $3.14 N/A N/A N/A N/A 17148 68.59 3429.6 139.08Query 68 5 250 $3.00 $3.06 N/A N/A N/A N/A 14880 59.52 2976 111.23Query 78 5 250 $3.00 $3.12 N/A N/A N/A N/A 17117 68.46 3423.4 122.89Pay=$0.01 4 800 $8.00 $8.46 1078821 1348.52 269705.25 47486.7 54379 67.97 13594.75 123.57Pay=$0.02 1 200 $4.00 $4.00 386324 1931.62 386324 N/A 10893 54.465 10893 88.87Title 4 800 $10.00 $10.46 1227585 1534.48 306896.25 67820.58 50968 63.71 12742 117.14Desc.
1 200 $4.00 $4.00 237560 1187.8 237560 N/A 14304 71.52 14304 119.20No Bonus 4 800 $10.00 $10.00 1124799 1405.99 281199.75 70347.43 51966 64.95 12991.5 111.32Bonus 1 200 $2.00 $2.46 340346 1701.73 340346 N/A 13306 66.53 13306 139.97Batch 1 1 200 $2.00 $2.00 249921 1249.60 249921 N/A 13935 69.67 13935 130.66Batch 2 1 200 $4.00 $4.00 386324 1931.62 386324 N/A 10893 54.46 10893 88.87Batch 3 1 200 $2.00 $2.00 250994 1254.97 250994 N/A 12834 64.17 12834 102.01Batch 4 1 200 $2.00 $2.00 237560 1187.8 237560 N/A 14304 71.52 14304 119.20Batch 5 1 200 $2.00 $2.46 340346 1701.73 340346 N/A 13306 66.53 13306 139.97All 5 1000 $12.00 $12.46 1465145 1465.14 293029 66417.07 65272 65.272 13054.4 117.54Table 4: Preliminary analysis 1.
Column labels: #B: Number of Batches, # HITs, noB: Cost without bonuses, withB:Cost with bonuses, Total, MeanH/B: Mean per-HIT/Batch, sdB/H: std-deviation across Batches/HITs.Recall that bonuses were awarded wheneverWorkers provided clear justification of their judg-ments (whether or not those judgments matchedground truth).
In 74% of these cases (17 of the 23HITs awarded bonuses), relevance assessments werecorrect.
Thus there may be a useful correlation to ex-ploit provided practical heuristics exist for automat-ically distinguishing quality feedback from spam.Feedback length might serve as a more practi-cal alternative to measuring quality while still cor-relating with accuracy.
Mean comment length forBatches 2 and 5 was 38.6 and 28.1 characters percomment, whereas Batches 1, 3, and 4 had meancomment lengths of 13.9, 12.7, and 19.3 charac-ters per comment.
The mean difference in commentlength between Batch 2 and Batch 1 was 24.7 char-acters (p<0.01), 25.9 characters between Batches2 and 3 (p<0.01), and 19.3 characters betweenBatches 2 and 4 (p<0.01).
Batch 5 and Batch 1 hada mean comment-length difference of 14.2 charac-ters (p<0.01), and Batches 5 and 3 differed by 15.4characters (p<0.01).
Thus higher-paying HITs orHITs with bonus opportunities may correlate withgreater Worker effort.
Batches 2 (pay=$0.02) and 5(bonus batch) garnered the highest number of com-ments, with each averaging 0.37 comments per HIT.In contrast, Batches 1, 3, and 4 averaged only 0.21,0.18, and 0.23 comments per HIT, or a difference of0.16 (p<0.01 ), 0.19 (p<0.01), and 0.14 (p<0.01)comments, respectively.5 DiscussionHow to control for the same worker participatingin multiple experiments.
We found many of thesame workers completed HITs in multiple batches,compromising our experimental control and likelyintroducing effects of training or fatigue.
It doesnot appear that MTurk provides an easy way to pre-venting this; one can block a worker from doingjobs, but blocking is more of a tool to prevent poorperformance.
It is also construed as a punishment:workers?
ratings can be negatively affected by block-ing.
Because of this, blocking is not a substitutefor a mechanism that simply allows requesters tohideHITs or otherwise disallow repeat workers fromcompleting HITs.
It would be nice to develop a sim-ple mechanism for automatically ensuring each ex-periment involves a different set of workers.Automatic HIT validation.
MTurk does not ap-pear to automatically ensure a submitted HIT wasactually completed, i.e.
a worker can submit a HITwithout having actually done anything.
While thesubmitted HIT can be rejected and re-requested,building some trivial validation of HITs to catchsuch cases automatically appears worthwhile.Automatic bonus pay.
For Batch 5 (which in-cluded bonus pay), one of the authors spent an hourmanually processing/evaluating worker annotationsand feedback, distributing bonus pay for 23 of the200 HITs.
While some time is certainly well spentin manually analyzing annotations and feedback, the177Subset Accuracy Unique Workers HPW Feedback Given Feedback Length#Correct MeanH MeanB sdH Total MeanH MeanB Acc Mean Total MeanH MeanH sdQuery 3 144 0.58 28.8 0.50 84 0.34 16.8 1.71 2.98 60 0.24 17.91 42.44Query 13 191 0.76 38.2 0.43 88 0.35 17.6 2.17 2.84 71 0.28 25.04 55.82Query 68 183 0.73 36.6 0.44 83 0.33 16.6 2.20 3.01 69 0.28 21.08 44.69Query 78 162 0.65 32.4 0.48 83 0.33 16.6 1.95 3.01 76 0.30 26.02 56.52Pay=$0.01 541 0.68 135.25 0.47 137 0.17 34.25 3.95 5.84 201 0.25 18.49 42.52Pay=$0.02 139 0.70 139 0.46 64 0.32 64 2.17 3.13 75 0.38 38.60 71.53Title 547 0.68 136.75 0.47 132 0.17 33 4.14 6.06 229 0.29 23.32 51.23Desc.
133 0.67 133 0.47 72 0.36 72 1.85 2.78 47 0.24 19.29 46.40No Bonus 552 0.69 138 0.46 121 0.15 30.25 4.56 6.61 201 0.25 21.12 50.26Bonus 128 0.64 128 0.48 38 0.19 38 3.37 5.26 75 0.38 28.09 50.21Batch 1 139 0.70 139 0.46 66 0.33 66 2.11 3.03 42 0.21 13.90 37.62Batch 2 139 0.70 139 0.46 64 0.32 64 2.17 3.13 75 0.38 38.60 71.53Batch 3 141 0.71 141 0.46 64 0.32 64 2.20 3.13 37 0.19 12.67 31.96Batch 4 133 0.67 133 0.47 72 0.36 72 1.85 2.78 47 0.24 19.29 46.40Batch 5 128 0.64 128 0.48 38 0.19 38 3.37 5.26 75 0.38 28.09 50.21All 680 0.68 136 0.47 149 0.15 29.8 4.56 6.71 276 0.28 22.51 50.30Table 5: Preliminary analysis 2.
Column labels: HPW: HITs per worker, MeanH/B: Mean per-HIT/Batch, sd(H):std-deviation (across HITs), Acc: mean worker accuracy.
Feedback length is in characters.disparity in cost of our own salaries vs. bonus ex-penses suggests decisions on bonus pay should beautomated if possible (and it likely pays to err on theside of being generous).
Of course, automated bonusdistribution may negatively affect quality of workif, for example, any string of characters in the feed-back box yields bonus pay and workers catch on tothis.
Similarly, automation may fail to reward trulyvaluable qualitative feedback from workers which isharder to automatically assess than simply evaluat-ing worker accuracy on known examples.6 Future WorkAssessing relevance of Web pages.
In the near-term, we will be using MTurk to evaluate search ac-curacy of systems participating in the TREC 2010Relevance Feedback Track.
This will involve ad-dressing several significant challenges: (1) achiev-ing scalable evaluation, (2) protecting workers frommalicious attack pages while maintaining assess-ment accuracy, (3) addressing issues of Web spam,and (4) handling issues of unknown mature contentworkers may encounter during assessment.With regard to (1), we will be scaling upCranfield-based relevance assessment to supportsearch evaluation on the massive ClueWeb09 Webcrawl4.
As for (2), many Web pages containingattack code designed to compromise the viewer?scomputer, and in a crowdsourced environment we4http://boston.lti.cs.cmu.edu/Data/clueweb09cannot ensure all workers have installed the latestsecurity patches for their Web browsers.
Varioustradeoffs may be involved between security and us-ability in pre-renderingWeb pages to assess as staticimages, creating a ?safe-viewer?
applet, etc.
Webspam (3) can be annoying to workers and therebyimpact the quality of their work, wastes time andmoney since spam is never relevant to any query bydefinition, and spam detection is conceptually a dis-tinct task and ought to be handled as such.
In theshort term, we may simply ask workers to not onlydecide relevance vs. non-relevance, but to simulta-neously differentiate non-relevant content from non-relevant spam, but a better solution would be prefer-able.
Mature content (4) is similar to spam but canbe far worse than annoying to workers, touches onlegal issues, and inability to filter it could signifi-cantly reduce the number of workers willing to ac-cept HITs which may contain it.
Our short-term so-lution will likely be to perform some simple pre-filtering and simply warn workers they may en-counter such content, but this solution is not ideal.Varying number of annotations in proportionto annotator agreement.
While we collected afixed number of relevance assessments for eachquery-document pair, it may be both more efficientand more effective to collect few assessments wheninner-annotator agreement is high and proportion-ally more assessments when greater disagreementexists between annotators (Von Ahn et al, 2008).178Graded vs. binary relevance.
We want asses-sors to be both maximally informative and max-imally consistent, and there is an inherent trade-off here.
Allowing assessors to make graded rel-evance judgments corresponds to the intuitive no-tion that relevance is typically not a binary propo-sition.
Evaluation of commercial search engines to-day often reports use of a five-point graded scale,and such graded feedback allows us to better distin-guish relative effectiveness of different search algo-rithms at a finer scale.
However, the right number ofrelevance levels to assess is unclear, and too manywould likely involve making overly nuanced judg-ments that could overwhelm assessors and lead tolow inner-annotator agreement.
We may similarlyask assessors to further differentiate relevance judg-ment from cases of ?I don?t know?
and ?this HITseems broken?.
There is also the possibility of in-ducing graded relevance levels from binary judg-ments, such as by averaging and rescaling.
The util-ity could be measured by comparing benchmark al-gorithms using the explicit or induced assessments.Evaluating annotation accuracy with regardto ground-truth labels vs. task accuracy.
Whilemuch research with MTurk has measured accuracyin terms of reproducing a ground-truth label, ulti-mately we are not interested in the labels themselvesbut rather in what we can do with them.
Rele-vance assessment in particular suffers from notori-ously low inner-annotator agreement.
Consequently,one alternative to comparing against ?ground-truth?labels would be to evaluate the ability of crowd-sourced labels for effectively distinguish betweendifferent benchmark algorithms.Crowd demographics.
While it is typically sug-gested that experts produce superior annotations,there are important questions of effects from whois judging the annotations.
For example, if you wantto know if the general public will think a particu-lar web page is relevant to a particular query, moreuseful assessments might be obtained from a laymanthan from someone who builds search engines for aliving.
This also suggests another reason why it mayeven be preferable in some circumstances for crowd-source annotations to disagree with ?ground-truth?expert labels.
It also raises questions about gener-ality of system comparisons based on expert labelswhen systems are to be used by the general public.ReferencesOmar Alonso, Daniel E. Rose, and Benjamin Stewart.2008.
Crowdsourcing for relevance evaluation.
SIGIRForum, 42(2):9?15.J.
Aslam and V. Pavlu.
2008.
A practical sampling strat-egy for efficient retrieval evaluation.
Technical report,Northeastern University.C.
Buckley and E.M. Voorhees.
2004.
Retrieval evalu-ation with incomplete information.
In Proceedings ofthe 27th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 25?32.
ACM New York, NY, USA.B.
Carterette, J. Allan, and R. Sitaraman.
2006.
Minimaltest collections for retrieval evaluation.
In Proceed-ings of the 29th annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 268?275.B.
Carterette, V. Pavlu, E. Kanoulas, J.A.
Aslam, andJ.
Allan.
2009.
If I Had a Million Queries.
In Pro-ceedings of the 31st European Conference on Infor-mation Retrieval, pages 288?300.D.
Horowitz and S.D.
Kamvar.
2010.
The Anatomy of aLarge-Scale Social Search Engine.
In Proc.
of the 19thinternational conference on World wide web (WWW).Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD ?02: Proceedings ofthe 8th SIGKDD international conference on Knowl-edge discovery and data mining, pages 133?142.W.
Mason and D.J.
Watts.
2009.
Financial incentivesand the performance of crowds.
In Proceedings ofthe ACM SIGKDDWorkshop on Human Computation,pages 77?85.
ACM.R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263.
Association forComputational Linguistics.L.
Von Ahn, B. Maurer, C. McMillen, D. Abraham, andM.
Blum.
2008. recaptcha: Human-based charac-ter recognition via web security measures.
Science,321(5895):1465.E.M.
Voorhees.
2002.
The philosophy of information re-trieval evaluation.
Lecture Notes in Computer Science,pages 355?370.J.
Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movel-lan.
2009.
Whose Vote Should Count More: OptimalIntegration of Labels from Labelers of Unknown Ex-pertise.
Proceedings of the 2009 Neural InformationProcessing Systems (NIPS) Conference.179
