Probabilistic Parsing for German using Sister-Head DependenciesAmit DubeyDepartment of Computational LinguisticsSaarland UniversityPO Box 15 11 5066041 Saarbru?cken, Germanyadubey@coli.uni-sb.deFrank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKkeller@inf.ed.ac.ukAbstractWe present a probabilistic parsing modelfor German trained on the Negra tree-bank.
We observe that existing lexicalizedparsing models using head-head depen-dencies, while successful for English, failto outperform an unlexicalized baselinemodel for German.
Learning curves showthat this effect is not due to lack of trainingdata.
We propose an alternative model thatuses sister-head dependencies instead ofhead-head dependencies.
This model out-performs the baseline, achieving a labeledprecision and recall of up to 74%.
This in-dicates that sister-head dependencies aremore appropriate for treebanks with veryflat structures such as Negra.1 IntroductionTreebank-based probabilistic parsing has been thesubject of intensive research over the past few years,resulting in parsing models that achieve both broadcoverage and high parsing accuracy (e.g., Collins1997; Charniak 2000).
However, most of the ex-isting models have been developed for English andtrained on the Penn Treebank (Marcus et al, 1993),which raises the question whether these modelsgeneralize to other languages, and to annotationschemes that differ from the Penn Treebank markup.The present paper addresses this question byproposing a probabilistic parsing model trained onNegra (Skut et al, 1997), a syntactically annotatedcorpus for German.
German has a number of syn-tactic properties that set it apart from English, andthe Negra annotation scheme differs in important re-spects from the Penn Treebank markup.
While Ne-gra has been used to build probabilistic chunkers(Becker and Frank, 2002; Skut and Brants, 1998),the research reported in this paper is the first attemptto develop a probabilistic full parsing model for Ger-man trained on a treebank (to our knowledge).Lexicalization can increase parsing performancedramatically for English (Carroll and Rooth, 1998;Charniak, 1997, 2000; Collins, 1997), and the lexi-calized model proposed by Collins (1997) has beensuccessfully applied to Czech (Collins et al, 1999)and Chinese (Bikel and Chiang, 2000).
However, theresulting performance is significantly lower than theperformance of the same model for English (see Ta-ble 1).
Neither Collins et al (1999) nor Bikel andChiang (2000) compare the lexicalized model to anunlexicalized baseline model, leaving open the pos-sibility that lexicalization is useful for English, butnot for other languages.This paper is structured as follows.
Section 2 re-views the syntactic properties of German, focusingon its semi-flexible wordorder.
Section 3 describestwo standard lexicalized models (Carroll and Rooth,1998; Collins, 1997), as well as an unlexicalizedbaseline model.
Section 4 presents a series of experi-ments that compare the parsing performance of thesethree models (and several variants) on Negra.
Theresults show that both lexicalized models fail to out-perform the unlexicalized baseline.
This is at oddswith what has been reported for English.
Learningcurves show that the poor performance of the lexi-calized models is not due to lack of training data.Section 5 presents an error analysis for Collins?s(1997) lexicalized model, which shows that thehead-head dependencies used in this model fail tocope well with the flat structures in Negra.
We pro-pose an alternative model that uses sister-head de-pendencies instead.
This model outperforms the twooriginal lexicalized models, as well as the unlexical-ized baseline.
Based on this result and on the reviewof the previous literature (Section 6), we argue (Sec-tion 7) that sister-head models are more appropriatefor treebanks with very flat structures (such as Ne-gra), typically used to annotate languages with semi-free wordorder (such as German).2 Parsing German2.1 Syntactic PropertiesGerman exhibits a number of syntactic propertiesthat distinguish it from English, the language thathas been the focus of most research in parsing.Prominent among these properties is the semi-freeLanguage Size LR LP SourceEnglish 40,000 87.4% 88.1% (Collins, 1997)Chinese 3,484 69.0% 74.8% (Bikel and Chiang, 2000)Czech 19,000 ?- 80.0% ?- (Collins et al, 1999)Table 1: Results for the Collins (1997) model forvarious languages (dependency precision for Czech)wordorder, i.e., German wordorder is fixed in somerespects, but variable in others.
Verb order is largelyfixed: in subordinate clauses such as (1a), both thefinite verb hat ?has?
and the non-finite verb kom-poniert ?composed?
are in sentence final position.
(1) a. WeilbecauseerergesternyesterdayMusikmusickomponiertcomposedhat.has?Because he has composed music yesterday.?b.
Hat er gestern Musik komponiert?c.
Er hat gestern Musik komponiert.In yes/no questions such as (1b), the finite verb issentence initial, while the non-finite verb is sen-tence final.
In declarative main clauses (see (1c)), onthe other hand, the finite verb is in second position(i.e., preceded by exactly one constituent), while thenon-finite verb is final.While verb order is fixed in German, the orderof complements and adjuncts is variable, and influ-enced by a variety of syntactic and non-syntacticfactors, including pronominalization, informationstructure, definiteness, and animacy (e.g., Uszkor-eit 1987).
The first position in a declarative sen-tence, for example, can be occupied by various con-stituents, including the subject (er ?he?
in (1c)), theobject (Musik ?music?
in (2a)), an adjunct (gestern?yesterday?
in (2b)), or the non-finite verb (kom-poniert ?composed?
in (2c)).
(2) a. Musik hat er gestern komponiert.b.
Gestern hat er Musik komponiert .c.
Komponiert hat er gestern Musik.The semi-free wordorder in German means that acontext-free grammar model has to contain morerules than for a fixed wordorder language.
For tran-sitive verbs, for instance, we need the rules S !V NP NP, S !
NP V NP, and S !
NP NP V toaccount for verb initial, verb second, and verb finalorder (assuming a flat S, see Section 2.2).2.2 Negra Annotation SchemeThe Negra corpus consists of around 350,000 wordsof German newspaper text (20,602 sentences).
Theannotation scheme (Skut et al, 1997) is modeled to acertain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences.
Most impor-tantly, Negra follows the dependency grammar tra-dition in assuming flat syntactic representations:(a) There is no S !
NP VP rule.
Rather, the sub-ject, the verb, and its objects are all sisters of eachother, dominated by an S node.
This is a way ofaccounting for the semi-free wordorder of German(see Section 2.1): the first NP within an S need notbe the subject.
(b) There is no SBAR !
Comp S rule.
Mainclauses, subordinate clauses, and relative clauses allshare the category S in Negra; complementizers andrelative pronouns are simply sisters of the verb.
(c) There is no PP !
P NP rule, i.e., the prepo-sition and the noun it selects (and determiners andadjectives, if present) are sisters, dominated by aPP node.
An argument for this representation is thatprepositions behave like case markers in German; apreposition and a determiner can merge into a singleword (e.g., in dem ?in the?
becomes im).Another idiosyncrasy of Negra is that it assumesspecial coordinate categories.
A coordinated sen-tence has the category CS, a coordinate NP has thecategory CNP, etc.
While this does not make theannotation more flat, it substantially increases thenumber of non-terminal labels.
Negra also containsgrammatical function labels that augment phrasaland lexical categories.
Example are MO (modifier),HD (head), SB (subject), and OC (clausal object).3 Probabilistic Parsing Models3.1 Probabilistic Context-Free GrammarsLexicalization has been shown to improve pars-ing performance for the Penn Treebank (e.g., Car-roll and Rooth 1998; Charniak 1997, 2000; Collins1997).
The aim of the present paper is to test if thisfinding carries over to German and to the Negra cor-pus.
We therefore use an unlexicalized model as ourbaseline against which to test the lexicalized models.More specifically, we used a standard proba-bilistic context-free grammar (PCFG; see Charniak1993).
Each context-free rule RHS !
LHS is anno-tated with an expansion probability P(RHSjLHS).The probabilities for all rules with the same lefthandside have to sum to one, and the probability of aparse tree T is defined as the product of the prob-abilities of all rules applied in generating T .3.2 Carroll and Rooth?s Head-LexicalizedModelThe head-lexicalized PCFG model of Carroll andRooth (1998) is a minimal departure from the stan-dard unlexicalized PCFG model, which makes itideal for a direct comparison.1A grammar rule LHS !
RHS can be written asP !
C1 .
.
.Cn, where P is the mother category, andC1 .
.
.Cn are daughters.
Let l(C) be the lexical head1Charniak (1997) proposes essentially the same model; wewill nevertheless use the label ?Carroll and Rooth model?
as weare using their implementation (see Section 4.1).of the constituent C. The rule probability is then de-fined as (see also Beil et al 2002):P(RHSjLHS) = Prule(C1 .
.
.CnjP, l(P))(3)n?i=1Pchoice(l(Ci)jCi,P, l(P))Here Prule(C1 .
.
.CnjP, l(P)) is the probability thatcategory P with lexical head l(P) is expanded by therule P !
C1 .
.
.Cn, and Pchoice(l(C)jC,P, l(P)) is theprobability that the (non-head) category C has thelexical head l(C) given that its mother is P with lex-ical head l(P).3.3 Collins?s Head-Lexicalized ModelIn contrast to Carroll and Rooth?s (1998) approach,the model proposed by Collins (1997) does not com-pute rule probabilities directly.
Rather, they are gen-erated using a Markov process that makes certain in-dependence assumptions.
A grammar rule LHS !RHS can be written as P !
Lm .
.
.L1 H R1 .
.
.Rnwhere P is the mother and H is the head daughter.Let l(C) be the head word of C and t(C) the tag ofthe head word of C. Then the probability of a rule isdefined as:P(RHSjLHS) = P(Lm .
.
.L1 H R1 .
.
.RnjP)(4)= Ph(HjP)Pl(Lm .
.
.L1jP,H)Pr(R1 .
.
.RnjP,H)= Ph(HjP)m?i=0Pl(LijP,H,d(i))n?i=0Pr(RijP,H,d(i))Here, Ph is the probability of generating the head,and Pl and Pr are the probabilities of generating thenonterminals to the left and right of the head, re-spectively; d(i) is a distance measure.
(L0 and R0 arestop categories.)
At this point, the model is still un-lexicalized.
To add lexical sensitivity, the Ph, Pr andPl probability functions also take into account headwords and their POS tags:P(RHSjLHS) = Ph(HjP,t(P), l(P))(5)m?i=0Pl(Li,t(Li), l(Li)jP,H,t(H), l(H),d(i))n?i=0Pr(Ri,t(Ri), l(Ri)jP,H,t(H), l(H),d(i))4 Experiment 1This experiment was designed to compare the per-formance of the three models introduced in thelast section.
Our main hypothesis was that the lex-icalized models will outperform the unlexicalizedbaseline model.
Another prediction was that addingNegra-specific information to the models will in-crease parsing performance.
We therefore tested amodel variant that included grammatical function la-bels, i.e., the set of categories was augmented by thefunction tags specified in Negra (see Section 2.2).Adding grammatical functions is a way of deal-ing with the wordorder facts of German (see Sec-tion 2.1) in the face of Negra?s very flat annota-tion scheme.
For instance, subject and object NPshave different wordorder preferences (subjects tendto be preverbal, while objects tend to be postver-bal), a fact that is captured if subjects have the la-bel NP-SB, while objects are labeled NP-OA (ac-cusative object), NP-DA (dative object), etc.
Alsothe fact that verb order differs between subordinateand main clauses is captured by the function labels:the former are labeled S, while the latter are labeledS-OC (object clause), S-RC (relative clause), etc.Another idiosyncrasy of the Negra annotation isthat conjoined categories have separate labels (S andCS, NP and CNP, etc.
), and that PPs do not containan NP node.
We tested a variant of the Carroll andRooth (1998) model that takes this into account.4.1 MethodData Sets All experiments reported in this paperused the treebank format of Negra.
This format,which is included in the Negra distribution, was de-rived from the native format by replacing crossingbranches with traces.
We split the corpus into threesubsets.
The first 18,602 sentences constituted thetraining set.
Of the remaining 2,000 sentences, thefirst 1,000 served as the test set, and the last 1000 asthe development set.
To increase parsing efficiency,we removed all sentences with more than 40 words.This resulted in a test set of 968 sentences and adevelopment set of 975 sentences.
Early versionsof the models were tested on the development set,and the test set remained unseen until all parameterswere fixed.
The final results reported this paper wereobtained on the test set, unless stated otherwise.Grammar Induction For the unlexicalized PCFGmodel (henceforth baseline model), we used theprobabilistic left-corner parser Lopar (Schmid,2000).
When run in unlexicalized mode, Lopar im-plements the model described in Section 3.1.
Agrammar and a lexicon for Lopar were read off theNegra training set, after removing all grammaticalfunction labels.
As Lopar cannot handle traces, thesewere also removed from the training data.The head-lexicalized model of Carroll and Rooth(1998) (henceforth C&R model) was again realizedusing Lopar, which in lexicalized mode implementsthe model in Section 3.2.
Lexicalization requires thateach rule in a grammar has one of the categories onits righthand side annotated as the head.
For the cate-gories S, VP, AP, and AVP, the head is marked in Ne-gra.
For the other categories, we used rules to heuris-tically determine the head, as is standard practice forthe Penn Treebank.The lexicalized model proposed by Collins (1997)(henceforth Collins model) was re-implemented byone of the authors.
For training, empty categorieswere removed from the training data, as the modelcannot handle them.
The same head finding strategywas applied as for the C&R model.In this experiment, only head-head statistics wereused (see (5)).
The original Collins model usessister-head statistics for non-recursive NPs.
This willbe discussed in detail in Section 5.Training and Testing For all three models, themodel parameters were estimated using maximumlikelihood estimation.
Both Lopar and the Collinsmodel use various backoff distributions to smooththe estimates.
The reader is referred to Schmid(2000) and Collins (1997) for details.
For the C&Rmodel, we used a cutoff of one for rule frequenciesPrule and lexical choice frequencies Pchoice (the cutoffvalue was optimized on the development set).We also tested variants of the baseline model andthe C&R model that include grammatical functioninformation, as we hypothesized that this informa-tion might help the model to handle wordorder vari-ation more adequately, as explained above.Finally, we tested variant of the C&R model thatuses Lopar?s parameter pooling feature.
This fea-ture makes it possible to collapse the lexical choicedistribution Pchoice for either the daughter or themother categories of a rule (see Section 3.2).
Wepooled the estimates for pairs of conjoined and non-conjoined daughter categories (S and CS, NP andCNP, etc.
): these categories should be treated as thesame daughters; e.g., there should be no differencebetween S !NP V and S!CNP V. We also pooledthe estimates for the mother categories NPs and PPs.This is a way of dealing with the fact that there is noseparate NP node within PPs in Negra.Lopar and the Collins model differ in their han-dling of unknown words.
In Lopar, a POS tag distri-bution for unknown words has to be specified, whichis then used to tag unknown words in the test data.The Collins model treats any word seen fewer thanfive times in the training data as unseen and uses anexternal POS tagger to tag unknown words.
In orderto make the models comparable, we used a uniformapproach to unknown words.
All models were runon POS-tagged input; this input was created by tag-ging the test set with a separate POS tagger, for bothknown and unknown words.
We used TnT (Brants,2000), trained on the Negra training set.
The taggingaccuracy was 97.12% on the development set.In order to obtain an upper bound for the perfor-mance of the parsing models, we also ran the parserson the test set with the correct tags (as specified inNegra), again for both known and unknown words.We will refer to this mode as ?perfect tagging?.All models were evaluated using standard PAR-SEVAL measures.
We report labeled recall (LR)labeled precision (LP), average crossing brackets(CBs), zero crossing brackets (0CB), and two or lesscrossing brackets (2CB).
We also give the cover-age (Cov), i.e., the percentage of sentences that theparser was able to parse.4.2 ResultsThe results for all three models and their variantsare given in Table 2, for both TnT tags and per-fect tags.
The baseline model achieves 70.56% LRand 66.69% LP with TnT tags.
Adding grammaticalfunctions reduces both figures slightly, and cover-age drops by about 15%.
The C&R model performsworse than the baseline, at 68.04% LR and 60.07%LP (for TnT tags).
Adding grammatical functionagain reduces performance slightly.
Parameter pool-ing increases both LR and LP by about 1%.
TheCollins models also performs worse than the base-line, at 67.91% LR and 66.07% LP.Performance using perfect tags (an upper boundof model performance) is 2?3% higher for the base-line and for the C&R model.
The Collins modelgains only about 1%.
Perfect tagging results in a per-formance increase of over 10% for the models withgrammatical functions.
This is not surprising, as theperfect tags (but not the TnT tags) include grammat-ical function labels.
However, we also observe a dra-matic reduction in coverage (to about 65%).4.3 DiscussionWe added grammatical functions to both the base-line model and the C&R model, as we predictedthat this would allow the model to better capture thewordorder facts of German.
However, this predic-tion was not borne out: performance with grammat-ical functions (on TnT tags) was slightly worse thanwithout, and coverage dropped substantially.
A pos-sible reason for this is sparse data: a grammar aug-mented with grammatical functions contains manyadditional categories, which means that many moreparameters have to be estimated using the sametraining set.
On the other hand, a performance in-crease occurs if the tagger also provides grammati-cal function labels (simulated in the perfect tags con-dition).
However, this comes at the price of an unac-ceptable reduction in coverage.When training the C&R model, we included avariant that makes use of Lopar?s parameter pool-ing feature.
We pooled the estimates for conjoineddaughter categories, and for NP and PP mother cat-egories.
This is a way of taking the idiosyncrasies ofthe Negra annotation into account, and resulted in asmall improvement in performance.The most surprising finding is that the best per-formance was achieved by the unlexicalized PCFGTnT tagging Perfect taggingLR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB CovBaseline 70.56 66.69 1.03 58.21 84.46 94.42 72.99 70.00 0.88 60.30 87.42 95.25Baseline + GF 70.45 65.49 1.07 58.02 85.01 79.24 81.14 78.37 0.46 74.25 95.26 65.39C&R 68.04 60.07 1.31 52.08 79.54 94.42 70.79 63.38 1.17 54.99 82.21 95.25C&R + pool 69.07 61.41 1.28 53.06 80.09 94.42 71.74 64.73 1.11 56.40 83.08 95.25C&R + GF 67.66 60.33 1.31 55.67 80.18 79.24 81.17 76.83 0.48 73.46 94.15 65.39Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23Table 2: Results for Experiment 1: comparison of lexicalized and unlexicalized models (GF: grammaticalfunctions; pool: parameter pooling for NPs/PPs and conjoined categories)0 20 40 60 80 100percent of training corpus45505560657075f-scoreunlexicalized PCFGlexicalized PCFG (Collins)lexicalized PCFG (C&R)Figure 1: Learning curves for all three modelsbaseline model.
Both lexicalized models (C&R andCollins) performed worse than the baseline.
This re-sults is at odds with what has been found for En-glish, where lexicalization is standardly reported toincrease performance by about 10%.
The poor per-formance of the lexicalized models could be due toa lack of sufficient training data: our Negra trainingset contains approximately 18,000 sentences, and istherefore significantly smaller than the Penn Tree-bank training set (about 40,000 sentences).
Negrasentences are also shorter: they contain, on average,15 words compared to 22 in the Penn Treebank.We computed learning curves for the unmodifiedvariants (without grammatical functions or parame-ter pooling) of all three models (on the developmentset).
The result (see Figure 1) shows that there is noevidence for an effect of sparse data.
For both thebaseline and the C&R model, a fairly high f-scoreis achieved with only 10% of the training data.
Aslow increase occurs as more training data is added.The performance of the Collins model is even lessaffected by training set size.
This is probably due tothe fact that it does not use rule probabilities directly,but generates rules using a Markov chain.5 Experiment 2As we saw in the last section, lack of training data isnot a plausible explanation for the sub-baseline per-formance of the lexicalized models.
In this experi-ment, we therefore investigate an alternative hypoth-esis, viz., that the lexicalized models do not copePenn NegraNP 2.20 3.08PP 2.03 2.66Penn NegraVP 2.32 2.59S 2.22 4.22Table 3: Average number of daughters for the gram-matical categories in the Penn Treebank and Negrawell with the fact that Negra rules are so flat (seeSection 2.2).
We will focus on the Collins model, asit outperformed the C&R model in Experiment 1.An error analysis revealed that many of the errorsof the Collins model in Experiment 1 are chunkingerrors.
For example, the PP neben den Mitteln desTheaters should be analyzed as (6a).
But instead theparser produces two constituents as in (6b)):(6) a.
[PP nebenapartdentheMittelnmeans[NP destheTheaters]]theater?s?apart from the means of the theater?.b.
[PP neben den Mitteln] [NP des Theaters]The reason for this problem is that neben is the headof the constituent in (6), and the Collins model usesa crude distance measure together with head-headdependencies to decide if additional constituentsshould be added to the PP.
The distance measure isinadequate for finding PPs with high precision.The chunking problem is more widespread thanPPs.
The error analysis shows that other con-stituents, including Ss and VPs, also have the wrongboundary.
This problem is compounded by the factthat the rules in Negra are substantially flatter thanthe rules in the Penn Treebank, for which the Collinsmodel was developed.
Table 3 compares the averagenumber of daughters in both corpora.The flatness of PPs is easy to reduce.
As detailedin Section 2.2, PPs lack an intermediate NP projec-tion, which can be inserted straightforwardly usingthe following rule:(7) [PP P .
.
. ]
!
[PP P [NP .
.
.
]]In the present experiment, we investigated if parsingperformance improves if we test and train on a ver-sion of Negra on which the transformation in (7) hasbeen applied.In a second series of experiments, we investigateda more general way of dealing with the flatness ofC&R Collins Charniak CurrentHead sister category X X XHead sister head word X X XHead sister head tag X XPrev.
sister category X X XPrev.
sister head word XPrev.
sister head tag XTable 4: Linguistic features in the current modelcompared to the models of Carroll and Rooth(1998), Collins (1997), and Charniak (2000)Negra, based on Collins?s (1997) model for non-recursive NPs in the Penn Treebank (which are alsoflat).
For non-recursive NPs, Collins (1997) does notuse the probability function in (5), but instead sub-stitutes Pr (and, by analogy, Pl) by:Pr(Ri,t(Ri), l(Ri)jP,Ri?1,t(Ri?1), l(Ri?1),d(i))(8)Here the head H is substituted by the sister Ri?1(and Li?1).
In the literature, the version of Pr in (5)is said to capture head-head relationships.
We willrefer to the alternative model in (8) as capturingsister-head relationships.Using sister-head relationships is a way of coun-teracting the flatness of the grammar productions;it implicitly adds binary branching to the grammar.Our proposal is to extend the use of sister-head re-lationship from non-recursive NPs (as proposed byCollins) to all categories.Table 4 shows the linguistic features of the result-ing model compared to the models of Carroll andRooth (1998), Collins (1997), and Charniak (2000).The C&R model effectively includes category infor-mation about all previous sisters, as it uses context-free rules.
The Collins (1997) model does not usecontext-free rules, but generates the next categoryusing zeroth order Markov chains (see Section 3.3),hence no information about the previous sisters isincluded.
Charniak?s (2000) model extends this tohigher order Markov chains (first to third order), andtherefore includes category information about previ-ous sisters.The current model differs from all theseproposals: it does not use any information about thehead sister, but instead includes the category, headword, and head tag of the previous sister, effectivelytreating it as the head.5.1 MethodWe first trained the original Collins model on a mod-ified versions of the training test from Experiment 1in which the PPs were split by applying rule (7).In a second series of experiments, we tested arange of models that use sister-head dependenciesinstead of head-head dependencies for different cat-egories.
We first added sister-head dependencies forNPs (following Collins?s (1997) original proposal)and then for PPs, which are flat in Negra, and thussimilar in structure to NPs (see Section 2.2).
Thenwe tested a model in which sister-head relationshipsare applied to all categories.In a third series of experiments, we trained mod-els that use sister-head relationships everywhere ex-cept for one category.
This makes it possible to de-termine which sister-head dependencies are crucialfor improving performance of the model.5.2 ResultsThe results of the PP experiment are listed in Ta-ble 5.
Again, we give results obtained using TnT tagsand using perfect tags.
The row ?Split PP?
containsthe performance figures obtained by including splitPPs in both the training and in the testing set.
Thisleads to a substantial increase in LR (6?7%) and LP(around 8%) for both tagging schemes.
Note, how-ever, that these figures are not directly comparable tothe performance of the unmodified Collins model: itis possible that the additional brackets artificially in-flate LR and LP.
Presumably, the brackets for splitPPs are easy to detect, as they are always adjacent toa preposition.
An honest evaluation should thereforetrain on the modified training set (with split PPs),but collapse the split categories for testing, i.e., teston the unmodified test set.
The results for this evalu-ation are listed in rows ?Collapsed PP?.
Now there isno increase in performance compared to the unmod-ified Collins model; rather, a slight drop in LR andLP is observed.Table 5 also displays the results of our exper-iments with the sister-head model.
For TnT tags,we observe that using sister-head dependencies forNPs leads to a small decrease in performance com-pared to the unmodified Collins model, resulting in67.84% LR and 65.96% LP.
Sister-head dependen-cies for PPs, however, increase performance sub-stantially to 70.27% LR and 68.45% LP.
The high-est improvement is observed if head-sister depen-dencies are used for all categories; this results in71.32% LR and 70.93% LP, which corresponds to animprovement of 3% in LP and 5% in LR comparedto the unmodified Collins model.
Performance withperfect tags is around 2?4% higher than with TnTtags.
For perfect tags, sister-head dependencies leadto an improvement for NPs, PPs, and all categories.The third series of experiments was designed todetermine which categories are crucial for achiev-ing this performance gain.
This was done by train-ing models that use sister-head dependencies for allcategories but one.
Table 6 shows the change in LRand LP that was found for each individual category(again for TnT tags and perfect tags).
The highestdrop in performance (around 3%) is observed whenthe PP category is reverted to head-head dependen-cies.
For S and for the coordinated categories (CS,TnT tagging Perfect taggingLR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB CovUnmod.
Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23Split PP 73.84 73.77 0.82 62.89 88.98 95.11 75.93 75.27 0.77 65.36 89.03 93.79Collapsed PP 66.45 66.07 0.89 66.60 87.04 95.11 68.22 67.32 0.94 66.67 85.88 93.79Sister-head NP 67.84 65.96 0.75 65.85 88.97 95.11 71.54 70.31 0.60 68.03 93.33 94.60Sister-head PP 70.27 68.45 0.69 66.27 90.33 94.81 73.20 72.44 0.60 68.53 93.21 94.50Sister-head all 71.32 70.93 0.61 69.53 91.72 95.92 73.93 74.24 0.54 72.30 93.47 95.21Table 5: Results for Experiment 2: performance for models using split phrases and sister-head dependenciesCNP, etc.
), a drop in performance of around 1% eachis observed.
A slight drop is observed also for VP(around 0.5%).
Only minimal fluctuations in perfor-mance are observed when the other categories areremoved (AP, AVP, and NP): there is a small effect(around 0.5%) if TnT tags are used, and almost noeffect for perfect tags.5.3 DiscussionWe showed that splitting PPs to make Negra lessflat does not improve parsing performance if test-ing is carried out on the collapsed categories.
How-ever, we observed that LR and LP are artificially in-flated if split PPs are used for testing.
This findinggoes some way towards explaining why the parsingperformance reported for the Penn Treebank is sub-stantially higher than the results for Negra: the PennTreebank contains split PPs, which means that thereare lot of brackets that are easy to get right.
The re-sulting performance figures are not directly compa-rable to figures obtained on Negra, or other corporawith flat PPs.2We also obtained a positive result: we demon-strated that a sister-head model outperforms the un-lexicalized baseline model (unlike the C&R modeland the Collins model in Experiment 1).
LR wasabout 1% higher and LP about 4% higher than thebaseline if lexical sister-head dependencies are usedfor all categories.
This holds both for TnT tags andfor perfect tags (compare Tables 2 and 5).
We alsofound that using lexical sister-head dependencies forall categories leads to a larger improvement than us-ing them only for NPs or PPs (see Table 5).
Thisresult was confirmed by a second series of experi-ments, where we reverted individual categories backto head-head dependencies, which triggered a de-crease in performance for all categories, with the ex-ception of NP, AP, and AVP (see Table 6).On the whole, the results of Experiment 2 are atodds with what is known about parsing for English.The progression in the probabilistic parsing litera-ture has been to start with lexical head-head depen-dencies (Collins, 1997) and then add non-lexical sis-2This result generalizes to Ss, which are also flat in Negra(see Section 2.2).
We conducted an experiment in which weadded an SBAR above the S. No increase in performance wasobtained if the evaluation was carried using collapsed Ss.TnT tagging Perfect tagging?LR ?LP ?LR ?LPPP ?3.45 ?1.60 ?4.21 ?3.35S ?1.28 0.11 ?2.23 ?1.22Coord ?1.87 ?0.39 ?1.54 ?0.80VP ?0.72 0.18 ?0.58 ?0.30AP ?0.57 0.10 0.08 ?0.07AVP ?0.32 0.44 0.10 0.11NP 0.06 0.78 ?0.15 0.02Table 6: Change in performance when reverting tohead-head statistics for individual categoriester information (Charniak, 2000), as illustrated inTable 4.
Lexical sister-head dependencies have onlybeen found useful in a limited way: in the originalCollins model, they are used for non-recursive NPs.Our results show, however, that for parsing Ger-man, lexical sister-head information is more im-portant than lexical head-head information.
Only amodel that replaced lexical head-head with lexicalsister-head dependencies was able to outperform abaseline model that uses no lexicalization.3 Basedon the error analysis for Experiment 1, we claim thatthe reason for the success of the sister-head model isthe fact that the rules in Negra are so flat; using asister-head model is a way of binarizing the rules.6 Comparison with Previous WorkThere are currently no probabilistic, treebank-trained parsers available for German (to our knowl-edge).
A number of chunking models have been pro-posed, however.
Skut and Brants (1998) used Ne-gra to train a maximum entropy-based chunker, andreport LR and LP of 84.4% for NP and PP chunk-ing.
Using cascaded Markov models, Brants (2000)reports an improved performance on the same task(LR 84.4%, LP 88.3%).
Becker and Frank (2002)train an unlexicalized PCFG on Negra to performa different chunking task, viz., the identification oftopological fields (sentence-based chunks).
They re-port an LR and LP of 93%.The head-lexicalized model of Carroll and Rooth(1998) has been applied to German by Beil et al3It is unclear what effect bi-lexical statistics have on thesister-head model; while Gildea (2001) shows bi-lexical statis-tics are sparse for some grammars, Hockenmaier and Steedman(2002) found they play a greater role in binarized grammars.
(1999, 2002).
However, this approach differs in thenumber of ways from the results reported here: (a) ahand-written grammar (instead of a treebank gram-mar) is used; (b) training is carried out on unan-notated data; (c) the grammar and the training setcover only subordinate and relative clauses, not un-restricted text.
Beil et al (2002) report an evaluationusing an NP chunking task, achieving 92% LR andLP.
They also report the results of a task-based eval-uation (extraction of sucategorization frames).There is some research on treebank-based pars-ing of languages other than English.
The work byCollins et al (1999) and Bikel and Chiang (2000)has demonstrated the applicability of the Collins(1997) model for Czech and Chinese.
The perfor-mance reported by these authors is substantiallylower than the one reported for English, which mightbe due to the fact that less training data is avail-able for Czech and Chinese (see Table 1).
This hy-pothesis cannot be tested, as the authors do notpresent learning curves for their models.
However,the learning curve for Negra (see Figure 1) indicatesthat the performance of the Collins (1997) modelis stable, even for small training sets.
Collins et al(1999) and Bikel and Chiang (2000) do not comparetheir models with an unlexicalized baseline; henceit is unclear if lexicalization really improves parsingperformance for these languages.
As Experiment 1showed, this cannot be taken for granted.7 ConclusionsWe presented the first probabilistic full parsingmodel for German trained on Negra, a syntacticallyannotated corpus.
This model uses lexical sister-head dependencies, which makes it particularly suit-able for parsing Negra?s flat structures.
The flatnessof the Negra annotation reflects the syntactic proper-ties of German, in particular its semi-free wordorder.In Experiment 1, we applied three standard pars-ing models from the literature to Negra: an un-lexicalized PCFG model (the baseline), Carrolland Rooth?s (1998) head-lexicalized model, andCollins?s (1997) model based on head-head depen-dencies.
The results show that the baseline modelachieves a performance of up to 73% recall and 70%precision.
Both lexicalized models perform substan-tially worse.
This finding is at odds with what hasbeen reported for parsing models trained on the PennTreebank.
As a possible explanation we consideredlack of training data: Negra is about half the size ofthe Penn Treebank.
However, the learning curves forthe three models failed to produce any evidence thatthey suffer from sparse data.In Experiment 2, we therefore investigated an al-ternative hypothesis: the poor performance of thelexicalized models is due to the fact that the rules inNegra are flatter than in the Penn Treebank, whichmakes lexical head-head dependencies less usefulfor correctly determining constituent boundaries.Based on this assumption, we proposed an alterna-tive model hat replaces lexical head-head dependen-cies with lexical sister-head dependencies.
This canthe thought of as a way of binarizing the flat rules inNegra.
The results show that sister-head dependen-cies improve parsing performance not only for NPs(which is well-known for English), but also for PPs,VPs, Ss, and coordinate categories.
The best perfor-mance was obtained for a model that uses sister-headdependencies for all categories.
This model achievesup to 74% recall and precision, thus outperformingthe unlexicalized baseline model.It can be hypothesized that this finding carriesover to other treebanks that are annotated with flatstructures.
Such annotation schemes are often usedfor languages that (unlike English) have a free orsemi-free wordorder.
Testing our sister-head modelon these languages is a topic for future research.ReferencesBecker, Markus and Anette Frank.
2002.
A stochastic topological parser of Ger-man.
In Proceedings of the 19th International Conference on ComputationalLinguistics.
Taipei.Beil, Franz, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth.
1999.Inside-outside estimation of a lexicalized PCFG for German.
In Proceedingsof the 37th Annual Meeting of the Association for Computational Linguistics.College Park, MA.Beil, Franz, Detlef Prescher, Helmut Schmid, and Sabine Schulte im Walde.
2002.Evaluation of the Gramotron parser for German.
In Proceedings of the LRECWorkshop Beyond Parseval: Towards Improved Evaluation Measures for Pars-ing Systems.
Las Palmas, Gran Canaria.Bikel, Daniel M. and David Chiang.
2000.
Two statistical parsing models appliedto the Chinese treebank.
In Proceedings of the 2nd ACL Workshop on ChineseLanguage Processing.
Hong Kong.Brants, Thorsten.
2000.
TnT: A statistical part-of-speech tagger.
In Proceedingsof the 6th Conference on Applied Natural Language Processing.
Seattle.Carroll, Glenn and Mats Rooth.
1998.
Valence induction with a head-lexicalizedPCFG.
In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing.
Granada.Charniak, Eugene.
1993.
Statistical Language Learning.
MIT Press, Cambridge,MA.Charniak, Eugene.
1997.
Statistical parsing with a context-free grammar and wordstatistics.
In Proceedings of the 14th National Conference on Artificial Intel-ligence.
AAAI Press, Cambridge, MA.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedingsof the 1st Conference of the North American Chapter of the Association forComputational Linguistics.
Seattle.Collins, Michael.
1997.
Three generative, lexicalised models for statistical pars-ing.
In Proceedings of the 35th Annual Meeting of the Association for Com-putational Linguistics and the 8th Conference of the European Chapter of theAssociation for Computational Linguistics.
Madrid.Collins, Michael, Jan Hajic?, Lance Ramshaw, and Christoph Tillmann.
1999.
Astatistical parser for Czech.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics.
College Park, MA.Gildea, Daniel.
2001.
Corpus variation and parser performance.
In Proceedingsof the Conference on Empirical Methods in Natural Language Processing.Pittsburgh.Hockenmaier, Julia and Mark Steedman.
2002.
Generative models for statisticalparsing with combinatory categorial grammar.
In Proceedings of 40th AnnualMeeting of the Association for Computational Linguistics.
Philadelphia.Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993.Building a large annotated corpus of English: The Penn Treebank.
Compu-tational Linguistics 19(2).Schmid, Helmut.
2000.
LoPar: Design and implementation.
Ms., Institute forComputational Linguistics, University of Stuttgart.Skut, Wojciech and Thorsten Brants.
1998.
A maximum-entropy partial parser forunrestricted text.
In Proceedings of the 6th Workshop on Very Large Corpora.Montre?al.Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit.
1997.
Anannotation scheme for free word order languages.
In Proceedings of the 5thConference on Applied Natural Language Processing.
Washington, DC.Uszkoreit, Hans.
1987.
Word Order and Constituent Structure in German.
CSLIPublications, Stanford, CA.
