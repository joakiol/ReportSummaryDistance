Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 124?131, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsECNUCS: Measuring Short Text Semantic Equivalence Using MultipleSimilarity MeasurementsTian Tian ZHUDepartment of Computer Science andTechnologyEast China Normal University51111201046@student.ecnu.edu.cnMan LAN?Department of Computer Science andTechnologyEast China Normal Universitymlan@cs.ecnu.edu.cnAbstractThis paper reports our submissions to theSemantic Textual Similarity (STS) task in?SEM Shared Task 2013.
We submitted threeSupport Vector Regression (SVR) systems incore task, using 6 types of similarity mea-sures, i.e., string similarity, number similar-ity, knowledge-based similarity, corpus-basedsimilarity, syntactic dependency similarity andmachine translation similarity.
Our third sys-tem with different training data and differentfeature sets for each test data set performs thebest and ranks 35 out of 90 runs.
We also sub-mitted two systems in typed task using stringbased measure and Named Entity based mea-sure.
Our best system ranks 5 out of 15 runs.1 IntroductionThe task of semantic textual similarity (STS) is tomeasure the degree of semantic equivalence betweentwo sentences, which plays an increasingly impor-tant role in natural language processing (NLP) ap-plications.
For example, in text categorization (Yangand Wen, 2007), two documents which are moresimilar are more likely to be grouped in the sameclass.
In information retrieval (Sahami and Heil-man, 2006), text similarity improves the effective-ness of a semantic search engine by providing in-formation which holds high similarity with the inputquery.
In machine translation (Kauchak and Barzi-lay, 2006), sentence similarity can be applied forautomatic evaluation of the output translation andthe reference translations.
In question answering(Mohler and Mihalcea, 2009), once the question andthe candidate answers are treated as two texts, theanswer text which has a higher relevance with thequestion text may have higher probability to be theright one.The STS task in ?SEM Shared Task 2013 consistsof two subtasks, i.e., core task and typed task, andwe participate in both of them.
The core task aimsto measure the semantic similarity of two sentences,resulting in a similarity score which ranges from 5(semantic equivalence) to 0 (no relation).
The typedtask is a pilot task on typed-similarity between semi-structured records.
The types of similarity to bemeasured include location, author, people involved,time, events or actions, subject and description aswell as the general similarity of two texts (Agirre etal., 2013).In this work we present a Support Vector Re-gression (SVR) system to measure sentence seman-tic similarity by integrating multiple measurements,i.e., string similarity, knowledge based similarity,corpus based similarity, number similarity and ma-chine translation metrics.
Most of these similari-ties are borrowed from previous work, e.g., (Ba?r etal., 2012), (S?aric et al 2012) and (de Souza et al2012).
We also propose a novel syntactic depen-dency similarity.
Our best system ranks 35 out of90 runs in core task and ranks 5 out of 15 runs intyped task.The rest of this paper is organized as follows.
Sec-tion 2 describes the similarity measurements used inthis work in detail.
Section 3 presents experimentsand the results of two tasks.
Conclusions and futurework are given in Section 4.1242 Text Similarity MeasurementsTo compute semantic textual similarity, previouswork has adopted multiple semantic similarity mea-surements.
In this work, we adopt 6 types ofmeasures, i.e., string similarity, number similarity,knowledge-based similarity, corpus-based similar-ity, syntactic dependency similarity and machinetranslation similarity.
Most of them are borrowedfrom previous work due to their superior perfor-mance reported.
Besides, we also propose two syn-tactic dependency similarity measures.
Totally weget 33 similarity measures.
Generally, these simi-larity measures are represented as numerical valuesand combined using regression model.2.1 PreprocessingGenerally, we perform text preprocessing before wecompute each text similarity measurement.
Firstly,Stanford parser1 is used for sentence tokenizationand parsing.
Specifically, the tokens n?t and ?m arereplaced with not and am.
Secondly, Stanford POSTagger2 is used for POS tagging.
Thirdly, Natu-ral Language Toolkit3 is used for WordNet basedLemmatization, which lemmatizes the word to itsnearest base form that appears in WordNet, for ex-ample, was is lemmatized as is, not be.Given two short texts or sentences s1 and s2, wedenote the word set of s1 and s2 as S1 and S2, thelength (i.e., number of words) of s1 and s2 as |S1|and |S2|.2.2 String SimilarityIntuitively, if two sentences share more strings, theyare considered to have higher semantic similarity.Therefore, we create 12 string based features in con-sideration of the common sequence shared by twotexts.Longest Common sequence (LCS).
The widelyused LCS is proposed by (Allison and Dix, 1986),which is to find the maximum length of a com-mon subsequence of two strings and here the sub-sequence need to be contiguous.
In consideration ofthe different length of two texts, we compute LCS1http://nlp.stanford.edu/software/lex-parser.shtml2http://nlp.stanford.edu/software/tagger.shtml3http://nltk.org/similarity using Formula (1) as follows:SimLCS =Length of LCSmin(|S1|, |S2|)(1)In order to eliminate the impacts of various formsof word, we also compute a Lemma LCS similarityscore after sentences being lemmatized.word n-grams.
Following (Lyon et al 2001), wecalculate the word n-grams similarity using the Jac-card coefficient as shown in Formula (2), where p isthe number of n-grams shared by s1 and s2, q and rare the number of n-grams not shared by s1 and s2,respectively.Jacc = pp + q + r (2)Since we focus on short texts, here only n=1,2,3,4is used in this work.
Similar with LCS, we also com-pute a Lemma n-grams similarity score.Weighted Word Overlap (WWO).
(S?aric et al2012) pointed out that when measuring sentencesimilarity, different words may convey different con-tent information.
Therefore, we consider to assignmore importance to those words bearing more con-tent information.
To measure the importance of eachword, we use Formula (3) to calculate the informa-tion content for each word w:ic(w) = ln?w?
?C freq(w?
)freq(w) (3)where C is the set of words in the corpus andfreq(w) is the frequency of the word w in the cor-pus.
To compute ic(w), we use the Web 1T 5-gramCorpus4, which is generated from approximatelyone trillion word tokens of text from Web pages.Obviously, the WWO scores between two sen-tences is non-symmetric.
The WWO of s2 by s1 isgiven by Formula (4):Simwwo(s1, s2) =?w?S1?S2 ic(w)?w?
?S2 ic(w?
)(4)Likewise, we can get Simwwo(s2, s1) score.Then the final WWO score is the harmonic mean ofSimwwo(s1, s2) and Simwwo(s2, s1).
Similarly, weget a Lemma WWO score as well.4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T131252.3 Knowledge Based SimilarityKnowledge based similarity approaches rely ona semantic network of words.
In this workall knowledge-based word similarity measures arecomputed based on WordNet.
For word similarity,we employ four WordNet-based similarity metrics:the Path similarity (Banea et al 2012); the WUPsimilarity (Wu and Palmer, 1994); the LCH similar-ity (Leacock and Chodorow, 1998); the Lin similar-ity (Lin, 1998).
We adopt the NLTK library (Bird,2006) to compute all these word similarities.In order to determine the similarity of sentences,we employ two strategies to convert the word simi-larity into sentence similarity, i.e., (1) the best align-ment strategy (align) (Banea et al 2012) and (2) theaggregation strategy (agg) (Mihalcea et al 2006).The best alignment strategy is computed as below:Simalign(s1, s2) =(?
+?|?|i=1 ?i) ?
(2|S1||S2|)|S1| + |S2|(5)where ?
is the number of shared terms between s1and s2, list ?
contains the similarities of non-sharedwords in shorter text, ?i is the highest similarityscore of the ith word among all words of the longertext.
The aggregation strategy is calculated as be-low:Simagg(s1, s2) =?w?S1(maxSim(w, S2) ?
ic(w))?w?
{S1} ic(w)(6)where maxSim(w,S2) is the highest WordNet-based score between word w and all words of sen-tence S2.
To compute ic(w), we use the same cor-pus as WWO, i.e., the Web 1T 5-gram Corpus.
Thefinal score of the aggregation strategy is the mean ofSimagg(s1, s2) and Simagg(s2, s1).
Finally we get8 knowledge based features.2.4 Corpus Based SimilarityLatent Semantic Analysis (LSA) (Landauer et al1997).
In LSA, term-context associations are cap-tured by means of a dimensionality reduction op-eration performing singular value decomposition(SVD) on the term-by-context matrix T , where Tis induced from a large corpus.
We use the TASAcorpus5 to obtain the matrix and compute the word5http://lsa.colorado.edu/similarity using cosine similarity of the two vectorsof the words.
After that we transform word similar-ity to sentence similarity based on Formula (5).Co-occurrence Retrieval Model (CRM) (Weeds,2003).
CRM is based on a notion of substitutabil-ity.
That is, the more appropriate it is to substituteword w1 in place of word w2 in a suitable naturallanguage task, the more semantically similar theyare.
The degree of substitutability of w2 with w1is dependent on the proportion of co-occurrences ofw1 that are also the co-occurrences of w2, and theproportion of co-occurrences of w2 that are also theco-occurrences of w1.
Following (Weeds, 2003), theCRM word similarity is computed using Formula(7):SimCRM (w1, w2) =2 ?
|c(w1) ?
c(w2)||c(w1)| + |c(w2)|(7)where c(w) is the set of words that co-occur withw.
We use the 5-gram part of the Web 1T 5-gramCorpus to obtain c(w).
If two words appear in one5-gram, we will treat one word as the co-occurringword of each other.
To obtain c(w), we propose twomethods.
In the first CRM similarity, we only con-sider the word w with |c(w)| > 200, and then takethe top 200 co-occurring words ranked by the co-occurrence frequency as its c(w).
To relax restric-tions, we also present an extended CRM (denotedby ExCRM), which extends the CRM list that all wwith |c(w)| > 50 are taken into consideration, butthe maximum of |c(w)| is still set to 200.
Finally,these two CRM word similarity measures are trans-formed to sentence similarity using Formula (5).2.5 Syntactic Dependency SimilarityAs (S?aric et al 2012) pointed out that dependencyrelations of sentences often contain semantic infor-mation, in this work we propose two novel syntacticdependency similarity features to capture their pos-sible semantic similarity.Simple Dependency Overlap.
First we measure thesimple dependency overlap between two sentencesbased on matching dependency relations.
StanfordParser provides 53 dependency relations, for exam-ple:nsubj(remain ?
16, leader ?
4)dobj(return ?
10, home ?
11)126where nsubj (nominal subject) and dobj (direct ob-ject) are two dependency types, remain is the gov-erning lemma and leader is the dependent lemma.Two syntactic dependencies are considered equalwhen they have the same dependency type, govern-ing lemma, and dependent lemma.Let R1 and R2 be the set of all dependency rela-tions in s1 and s2, we compute Simple DependencyOverlap using Formula (8):SimSimDep(s1, s2) =2 ?
|R1 ?
R2| ?
|R1||R2||R1| + |R2|(8)Special Dependency Overlap.
Several types of de-pendency relations are believed to contain the pri-mary content of a sentence.
So we extract three rolesfrom those special dependency relations, i.e., pred-icate, subject and object.
For example, from abovedependency relation dobj, we can extract the objectof the sentence, i.e., home.
For each of these threeroles, we get a similarity score.
For example, to cal-culate Simpredicate, we denote the sets of predicatesof two sentences as Sp1 and Sp2.
We first use LCH tocompute word similarity and then compute sentencesimilarity using Formula (5).
Similarly, the Simsubjand Simobj are obtained in the same way.
In the endwe average the similarity scores of the three roles asthe final Special Dependency Overlap score.2.6 Number SimilarityNumbers in the sentence occasionally carry similar-ity information.
If two sentences contain differentsets of numbers even though their sentence structureis quite similar, they may be given a low similarityscore.
Here we adopt two features following (S?aricet al 2012), which are computed as follow:log(1 + |N1| + |N2|) (9)2 ?
|N1 ?
N2|/(|N1| + |N2|) (10)where N1 and N2 are the sets of all numbers in s1and s2.
We extract the number information fromsentences by checking if the POS tag is CD (cardinalnumber).2.7 Machine Translation SimilarityMachine translation (MT) evaluation metrics are de-signed to assess whether the output of a MT sys-tem is semantically equivalent to a set of referencetranslations.
The two given sentences can be viewedas one input and one output of a MT system, thenthe MT measures can be used to measure their se-mantic similarity.
We use the following 6 lexicallevel metrics (de Souza et al 2012): WER, TER,PER, NIST, ROUGE-L, GTM-1.
All these measuresare obtained using the Asiya Open Toolkit for Auto-matic Machine Translation (Meta-) Evaluation6.3 Experiment and Results3.1 Regression ModelWe adopt LIBSVM7 to build Support Vector Regres-sion (SVR) model for regression.
To obtain the op-timal SVR parameters C, g, and p, we employ gridsearch with 10-fold cross validation on training data.Specifically, if the score returned by the regressionmodel is bigger than 5 or less than 0, we normalizeit as 5 or 0, respectively.3.2 Core TaskThe organizers provided four different test sets toevaluate the performance of the submitted systems.We have submitted three systems for core task, i.e.,Run 1, Run 2 and Run 3.
Run 1 is trained on alltraining data sets with all features except the num-ber based features, because most of the test data donot contain number.
Run 2 uses the same feature setsas Run 1 but different training data sets for differenttest data as listed in Table 1, where different trainingdata sets are combined together as they have simi-lar structures with the test data.
Run 3 uses differentfeature sets as well as different training data sets foreach test data.
Table 2 shows the best feature setsused for each test data set, where ?+?
means the fea-ture is selected and ?-?
means not selected.
We didnot use the whole feature set because in our prelimi-nary experiments, some features performed not wellon some training data sets, and they even reducedthe performance of our system.
To select features,we trained two SVR models for each feature, onewith all features and another with all features exceptthis feature.
If the first model outperforms the sec-ond model, this feature is chosen.Table 3 lists the performance of these three sys-tems as well as the baseline and the best results on6http://nlp.lsi.upc.edu/asiya/7http://www.csie.ntu.edu.tw/ cjlin/libsvm/127Test TrainingHeadline MSRparOnWN+FNWN MSRpar+OnWNSMT SMTnews+SMTeuroparlTable 1: Different training data sets used for each test data settype Features Headline OnWN and FNWN SMTLCS + + -Lemma LCS + + -String N-gram + 1+2gram 1gramBased Lemma N-gram + 1+2gram 1gramWWO + + +Lemma WWO + + +Path,WUP,LCH,Lin + + +Knowledge +alighBased Path,WUP,LCH,Lin + + ++ic-weightedCorpus LSA + + +Based CRM,ExCRM + + +Simple Dependency + + +Syntactic OverlapDependency Special Dependency + - +OverlapNumber Number + - -WER - + +TER - + +PER + + +MT NIST + + -ROUGE-L + + +GTM-1 + + +Table 2: Best feature combination for each data setSystem Mean Headline OnWN FNWN SMTBest 0.6181 0.7642 0.7529 0.5818 0.3804Baseline 0.3639 0.5399 0.2828 0.2146 0.2861Run 1 0.3533 0.5656 0.2083 0.1725 0.2949Run 2 0.4720 0.7120 0.5388 0.2013 0.2504Run 3 (rank 35) 0.4967 0.6799 0.5284 0.2203 0.3595Table 3: Final results on STS core taskSTS core task in ?SEM Shared Task 2013.
For thethree runs we submitted to the task organizers, Run3 performs the best results and ranks 35 out of 90runs.
Run 2 performs much better than Run 1.
It in-dicates that using different training data sets for dif-ferent test sets indeed improves results.
Run 3 out-performs Run 2 and Run 1.
It shows that our featureselection process for each test data set does help im-128prove the performance too.
From this table, we findthat different features perform different on differentkinds of data sets and thus using proper feature sub-sets for each test data set would make improvement.Besides, results on the four test data sets are quitedifferent.
Headline always gets the best result oneach run and OnWN follows second.
And resultsof FNWN and SMT are much lower than Headlineand OnWN.
One reason of the poor performance ofFNWN may be the big length difference of sentencepairs.
That is, sentence from WordNet is short whilesentence from FrameNet is quite longer, and somesamples even have more than one sentence (e.g.
?do-ing as one pleases or chooses?
VS ?there exist anumber of different possible events that may happenin the future in most cases, there is an agent involvedwho has to consider which of the possible events willor should occur a salient entity which is deeply in-volved in the event may also be mentioned?).
Asa result, even though the two sentences are similarin meaning, most of our measures would give lowscores due to quite different sentence length.In order to understand the contributions of eachsimilarity measurement, we trained 6 SVR regres-sion models based on 6 types on MSRpar data set.Table 4 presents the Pearson?s correlation scoresof the 6 types of measurements on MSRpar.
Wecan see that the corpus-based measure achieves thebest, then the knowledge-based measure and the MTmeasure follow.
Number similarity performs sur-prisingly well, which benefits from the property ofdata set that MSRpar contains many numbers in sen-tences and the sentence similarity depends a lot onthose numbers as well.
The string similarity is notas good as the knowledge-based, the corpus-basedand the MT similarity because of its disability of ex-tracting semantic characteristics of sentence.
Sur-prisingly, the Syntactic dependency similarity per-forms the worst.
Since we only extract two featuresbased on sentence dependency, they may not enoughto capture the key semantic similarity informationfrom the sentences.3.3 Typed TaskFor typed task, we also adopt a SVR model foreach type.
Since several previous similarity mea-sures used for core task are not suitable for evalu-ation of the similarity of people involved, time pe-Features resultsstring 0.4757knowledge-based 0.5640corpus-based 0.5842syntactic dependency 0.3528number 0.5278MT metrics 0.5595Table 4: Pearson correlation of features of the six aspectson MSRparriod, location and event or action involved, we addtwo Named Entity Recognition (NER) based fea-tures.
Firstly we use Stanford NER8 to obtain per-son, location and date information from the wholetext with NER tags of ?PERSON?, ?LOCATION?and ?DATE?.
Then for each list of entity, we get twofeature values using the following two formulas:SimNER Num(L1NER, L2NER) =min(|L1NER|, |L2NER|)max(|L1NER|, |L2NER|)(11)SimNER(L1NER, L2NER) =Num(equalpairs)|L1NER| ?
|L2NER|(12)where LNER is the list of one entity type fromthe text, and for two lists of NERs L1NER andL2NER, there are |L1NER| ?
|L2NER| NER pairs.Num(equalpairs) is the number of equal pairs.Here we expand the condition of equivalence: twoNERs are considered equal if one is part of another(e.g.
?John Warson?
VS ?Warson?).
Features andcontent we used for each similarity are presented inTable 5.
For the three similarities: people involved,time period, location, we compute the two NERbased features for each similarity with NER type of?PERSON?, ?LOCATION?
and ?DATE?.
And forevent or action involved, we add the above 6 NERfeature scores as its feature set.
The NER based sim-ilarity used in description is the same as event or ac-tion involved but only based on ?dcDescription?
partof text.
Besides, we add a length feature in descrip-tion, which is the ratio of shorter length and longerlength of descriptions.8http://nlp.stanford.edu/software/CRF-NER.shtml129Type Features Content usedauthor string based (+ knowledge based for Run2) dcCreatorpeople involved NER based whole texttime period NER based whole textlocation NER based whole textevent or action involved NER based whole textsubject string based (+ knowledge based for Run2) dcSubjectdescription string based, NER based,length dcDescriptionGeneral the 7 similarities aboveTable 5: Feature sets and content used of 8 type similarities of Typed dataWe have submitted two runs.
Run 1 uses onlystring based and NER based features.
Besides fea-tures used in Run 1, Run 2 also adds knowledgebased features.
Table 6 shows the performance ofour two runs as well as the baseline and the best re-sults on STS typed task in ?SEM Shared Task 2013.Our Run 1 ranks 5 and Run 2 ranks 7 out of 15 runs.Run 2 performed worse than Run 1 and the possiblereason may be the knowledge based method is notsuitable for this kind of data.
Furthermore, since weonly use NER based features which involves threeentities for these similarities, they are not enough tocapture the relevant information for other types.4 ConclusionIn this paper we described our submissions to theSemantic Textual Similarity Task in ?SEM SharedTask 2013.
For core task, we collect 6 types of simi-larity measures, i.e., string similarity, number sim-ilarity, knowledge-based similarity, corpus-basedsimilarity, syntactic dependency similarity and ma-chine translation similarity.
And our Run 3 with dif-ferent training data and different feature sets for eachtest data set ranks 35 out of 90 runs.
For typed task,we adopt string based measure, NER based mea-sure and knowledge based measure, our best systemranks 5 out of 15 runs.
Clearly, these similarity mea-sures are not quite enough.
For the core task, in ourfuture work we will consider the measures to eval-uate the sentence difference as well.
For the typedtask, with the help of more advanced IE tools to ex-tract more information regarding different types, weneed to propose more methods to evaluate the simi-larity.AcknowledgmentsThe authors would like to thank the organizers andreviewers for this interesting task and their helpfulsuggestions and comments, which improved the fi-nal version of this paper.
This research is supportedby grants from National Natural Science Foundationof China (No.60903093), Shanghai Pujiang TalentProgram (No.09PJ1404500), Doctoral Fund of Min-istry of Education of China (No.20090076120029)and Shanghai Knowledge Service Platform Project(No.ZF1213).ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Lloyd Allison and Trevor I Dix.
1986.
A bit-stringlongest-common-subsequence algorithm.
InformationProcessing Letters, 23(5):305?310.Carmen Banea, Samer Hassan, Michael Mohler, andRada Mihalcea.
2012.
Unt: A supervised synergisticapproach to semantic text similarity.
pages 635?642.First Joint Conference on Lexical and ComputationalSemantics (*SEM).Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
pages 435?440.
First Joint Conference on Lex-ical and Computational Semantics (*SEM).Steven Bird.
2006.
Nltk: the natural language toolkit.
InProceedings of the COLING/ACL on Interactive pre-sentation sessions, pages 69?72.
Association for Com-putational Linguistics.130System general author people time location event subject description meanBest 0.7981 0.8158 0.6922 0.7471 0.7723 0.6835 0.7875 0.7996 0.7620Baseline 0.6691 0.4278 0.4460 0.5002 0.4835 0.3062 0.5015 0.5810 0.4894Run 1 0.6040 0.7362 0.3663 0.4685 0.3844 0.4057 0.5229 0.6027 0.5113Run 2 0.6064 0.5684 0.3663 0.4685 0.3844 0.4057 0.5563 0.6027 0.4948Table 6: Final results on STS typed taskJose?
Guilherme C de Souza, Matteo Negri, Trento Povo,and Yashar Mehdad.
2012.
Fbk: Machine trans-lation evaluation and word similarity metrics for se-mantic textual similarity.
pages 624?630.
First JointConference on Lexical and Computational Semantics(*SEM).David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedings ofthe main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 455?462.
Association for Computational Linguistics.Thomas K Landauer, Darrell Laham, Bob Rehder, andMissy E Schreiner.
1997.
How well can passagemeaning be derived without using word order?
a com-parison of latent semantic analysis and humans.
InProceedings of the 19th annual meeting of the Cog-nitive Science Society, pages 412?417.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th inter-national conference on Machine Learning, volume 1,pages 296?304.
San Francisco.Caroline Lyon, James Malcolm, and Bob Dickerson.2001.
Detecting short passages of similar text in largedocument collections.
In Proceedings of the 2001Conference on Empirical Methods in Natural Lan-guage Processing, pages 118?125.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the na-tional conference on artificial intelligence, volume 21,page 775.
Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999.Michael Mohler and Rada Mihalcea.
2009.
Text-to-textsemantic similarity for automatic short answer grad-ing.
In Proceedings of the 12th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 567?575.
Association for Computa-tional Linguistics.Mehran Sahami and Timothy D Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In Proceedings of the 15th interna-tional conference on World Wide Web, pages 377?386.ACM.Frane S?aric, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic.
2012.
Takelab: Systemsfor measuring semantic text similarity.
pages 441?448.
First Joint Conference on Lexical and Compu-tational Semantics (*SEM).Julie Elizabeth Weeds.
2003.
Measures and applicationsof lexical distributional similarity.
Ph.D. thesis, Cite-seer.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics, pages 133?138.
Association for Computa-tional Linguistics.Cha Yang and Jun Wen.
2007.
Text categorization basedon similarity approach.
In Proceedings of Interna-tional Conference on Intelligence Systems and Knowl-edge Engineering (ISKE).131
