Col lect ion of Spontaneous Speech for the ATIS Domain  andComparat ive Analyses of Data  Col lected at MIT  and TI 1Joseph Polifroni, Stephanie Seneff, and Victor W. ZueSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts In titute of TechnologyCambridge, Massachusetts 02139ABSTRACTAs part of our development of a spoken language system inthe ATIS domain, we have begun a smail-scale ffort in collectingspontaneous speech data.
Our procedure differs from the one usedat 'i~xas Instruments (TI) in many respects, the most importantbeing the reliance on an existing system, rather than a wizard, toparticipate in data collection.
Over the past few months, we havecollected over 3,600 spontaneously generated sentences from 100subjects.
This paper documents our data collection process, andmakes ome comparative anaiyses of our data with those collectedat TI.
The advantages a well as disadvantages of this method ofdata collection will be discussed.INTRODUCTIONAWLS, or Air Travel Information Service, is the desig-nated common task of the DARPA Spoken Language Sys-tems (SLS) Program \[8\].
As part of our development of aspoken language system in this domain, we have recently be-gun a small-scale effort in collecting spontaneous speech data.This effort is motivated partly by our desire to contribute tothe data collection efforts already underway elsewhere \[4,2,1\],so that more data can be available to the community soonerfor system development, training, and evaluation.
In addi-tion, we were interested in exploring various alternatives ofthe data collection procedure itself.
It is our belief that we asa community do not fully understand how goal-directed spon-taneous peech should best be collected.
This is not surpris-ing, since we have little experience in this area.
Nevertheless,data collection is an impori~ant area of research for the SLSProgram, since the type of data that we collect will directlyaffect the capabilities of systems that we develop, and theevaluations that we can perform.
Therefore, we thought itwould be appropriate to experiment with different aspects ofthis process.
There is evidence that even very small changesin the procedure, such as the instructions to the subject, candrastically alter the nature of the data collected \[l\].The paper is organized as follows.
We will first discuss1This research was supported by DARPA under Contract N00014-89-.11-1332, monitored through the Office of Naval Research.some methodological considerations that led to the particularcollection procedure that we adopted.
We will then brieflydescribe the procedure itself.
This will be followed by somecomparative analyses of a subset of the data that we havecollected with those collected at Texas Instruments (TI).
Im-plications of our findings will be discussed.DATA COLLECT IONAs is the case with other efforts \[4~2,1\], our data are col-lected under simulation.
Nevertheless, we wanted the simula-tion to reflect as much as possible the system that we are de-veloping.
In this section, we will briefly describe some deslgnissues and document the actual collection process.
Furtherdetails can be found elsewhere \[7\].Methodological ConsiderationsWhile many years may pass before we are able to buildsystems with capabilities approaching those of humans, webelieve strongly that it should soon be possible to developfunctioning systems with limited capabilities.
The successfuldevelopment of such systems will partly depend on our abilityto train subjects to stay within the restricted omain of thesystem.
Therefore, we should try to collect data intention-ally restricting the user in ways that closely match systemcapability.
In this section we will describe some aspects ofour data collection paradigm that support his viewpoint.Wizard vs. System By far the most important differencebetween the data collection procedures at TI and MIT is theway system simulation is conducted uring data collection.TI made use of a "wizard" paradigm, in which a highly skilledexperimenter interprets what was spoken, converts it into aform that enables database access, and produces an answerfor the subject \[4,2\].
Based on our previous positive expe-rience with collecting spontaneous speech for a different do-main \[10\], we decided to explore an alternative paradigm fromthe one used at TI, in which we make use of the system underdevelopment to do most of the work.
That is, prior to thebeginning of data collection, the natural language componentis developed to the point where it has reasonable coverage o-r360Subject :  Show f l ights  from Ph i lade lph ia  to  Denver serv ing  lunch or d inner  on February second and a l so  show the i r  fa res .ATIS Response:.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(RAW DISPLAY) (PROCESSED DISPLAY)These are the flights from Philadelphia to Denver servinglunch and dinner on Friday February 2.AIRLINE FLIGHT AIRLINE NUMBER FROM TO DEPARTURE ARRIVAL STOPS MEALSCODE NUMBER SERVEDDL 1083 DELTA 1083 PHL DEN 12:30 P.M. 4:15 P.M. 1 LUNCH SNACKUA 355 UNITED 355 PHL DEN 5:53 P.M. 7:55 P.M. 0 DINNERCO 1631 CONT 1631 PHL DEN 6:00 P.M. 8:37 P.M. 0 DINNER.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(RAW DISPLAY) (PROCESSED DISPLAY)FROM TO DEPARTURE ARRIVAL STOPS MEALAIRPORT AIRPORT TIME TIME CODEPHL DEN 1230 1615 1 LSPHL DEN 1753 1955 0 DPBL DEN 1800 2037 0 DAIRLINE FLIGHT RESTRICT ONE WAY END TRIP FARECODE NUMBER CODE COST COST CLASSCO 1631 470 940 YCO 1631 706 1412 YDL 1083 420 840 YDL 1083 630 1260 FUA 355 470 940 YUA 355 706 1412 FThese are the fares for the flights from Philadelphia to Denverserving lunch and dinner on Friday February 2.AIRLINE NUMBER RESTRICTION ONE WAY ROUND TRIP FARE CLASSCONT 1631 NONE $470,00 $940.00 Y: COACH CLASSCONT 1631 NONE $706.00 $1412.00 F: FIRST CLASSDELTA 1083 NONE $420,00 $840.00 Y: COACH CLASSDELTA 1083 NOHE $630.00 $1260.00 F: FIRST CLASSUNITED 355 NONE $470.00 $940.00 Y: COACH CLASSUNITED 355 NONE $706.00 $1412.00 F: FIRST CLASSFigure 1: Comparison of the displays as returned from the OAG database (left panels) and those presented to the subject (right panels)for a query.the possible queries.
In addition, the system must be able toautomatically translate the text into a query to the database,and return the information to the subject.
Once such a sys-tem is available, data collection is accomplished by havingthe experimenter, a fast and accurate typist, type verbatimto the system what was spoken, after removing spontaneousspeech disfluencies.
The actual interpretation and responsegeneration is accomplished by the system without further hu-man intervention.
If the sentence cannot be understood bythe system, an error message is produced to help the subjectmake appropriate modifications.Another feature of our paradigm is that the underlyingsystem can be improved incrementally using the data col-lected thus far.
The resulting expansion in system capabil-ities permit us to accommodate more complex sentences aswell as those that previously failed.Displays One of the considerations that led to the se-lection of ATm as the common task is the realization that,since most people have planned air travel at one time or an-other, there will be no shortage of subjects familiar with thetask.
Since the average traveller is not likely to be knowledge-able of the format and display of the Official Airline Guide(OAG), we have translated many of the cryptic symbols andabbreviations that OAG uses into easily recognizable words.We believe that this change has the positive ffect of helpingthe subject focus on the travel planning aspect, and not beconfused by the cryptic displays that are intended for moreexperienced users.
In fact, we try to keep the displayed infor-mation at a minimum in order to encourage verbal problemsolving.
In general, we only display the airline, flight number,origination and destination cities, departure and arrival time,and the number of stops.
Additional columns of informationare included only when specifically requested.Figure 1 illustrates the difference between the raw dis-plays returned from the OAG database and the ones that wepresent o the subjects by applying some post-processing tothe raw display.
The query is "Show flights from Philadel-phia to Denver serving lunch or dinner on February secondand also show their fares."
Note that the airlines, meal codes,and fare codes in the processed isplays (shown in the right-hand panels) have all been translated into words as much aspossible while keeping the displays manageable on a screen, eThe military-time displays for departure and arrival timeshave also been converted to more familiar forms to facilitateinterpretation.
Furthermore, under the TI data collectionscheme the answer is assembled as one large table.
However,we break it up into two answers, one for the flights and onefor the fares.System Feedback Our system provides explicit feedbackto the subject in the form of text and synthetic speech, para-phrasing its understanding of the sentence.
This feature isillustrated in Figure 1 in the right-hand panels, immediatelyabove the display tables.
By providing confirmation to thesubject of what was understood, the system greatly reducesthe confusion and frustration that may arise later on in thedialogue caused by an earlier error in the system's responses.In addition, the generation of a verbal response implicitly en-courages the notion of human/machine interactive dialogue.Interactive Dialogue We believe that, for the ATIS sys-tem to be truly useful, the user must be able to carry out an2Some of the headings in the raw display have been reformatted sothat, the table will stay within the width of this page.361interactive dialogue with the system, in the same way that atraveller would with a travel agent.
Our data collection pro-cedure therefore ncourages natural dialogue by incorporat-ing some primitive discourse capabilities, allowing subjects tomake indirect as well as direct anaphoric references, and frag-mentary responses where appropriate.
In some sessions, weeven use a version of the system that plays an active role inguiding the subject hrough flight reservations.
Details of theinteractive dialogue capabilities of our system are describedin a companion paper \[9\].Data  Co l lec t ion  ProcessThe data are collected in an office environment where theambient noise is approximately 60 dB SPL, measured on theC scale.
The subject sits in front of a display monitor, witha window slaved to a Lisp process running on the experi-menter's Sun workstation located in a nearby room.
Theexperimenter's typing is hidden from the subject to avoidunnecessary distractions.
A push-to-talk mechanism is usedto collect the data.
The subject is instructed to hold downa mouse button while talking, and release the button whendone.
The resulting speech is captured both by a SennheiserHMD-224 noise-cancelling microphone and a Crown PZMdesk-top microphone, digitized simultaneously.Prior to the session, the subject is given a one-page de-scription of the task, a one-page summary of the system'sknowledge base, and three sets of scenarios \[7\].
The first setcontains imple tasks such as finding the earliest flight fromone city to another that serves a meal, and is intended as a"warm-up" exercise.
The second set involves more complextasks, and includes the official ATIS scenarios.
Finally, thesubject is asked to make up a scenario and attempt o solveit.
The subjects are instructed to choose a pre-determinednumber of scenarios from each category.
In addition, theyare asked to clearly delineate ach scenario with the com-mands "begin scenario x" and "end scenario x," where x is anumber, so that we can keep track of discourse utilization.
Atypical session lasts about 40 minutes, including initial taskfamiliarization and final questionnaire.For several initial data collection sessions, the first twoauthors took turns serving as the experimenter.
Once webegan daily sessions, however, it was possible to hire a part-time helper to serve as the scheduler, experimenter, and tran-scriber.
The experimenter can hear everything that the sub-ject says and can communicate with the subject via a two-waymicrophone/speaker hook-up.
However, the experimenterrarely communicates with the subject once the experimentis under way.
The digitized speech is played back to the ex-perimenter, allowing him/her to confirm that the recordingwas successful.
The voice input during the session, minusdisfluencies, is typed verbatim to ATIS by the experimenter,and saved a.utomatically in a computer log.
The system re-sponse is generated automatically from this text, and is alsorecorded into the log.
The system's response typically takesless than 10 seconds after the text has been entered.
At alater time, the experimenter listens again to each digitizedsentence and inserts false starts and non-speech events intothe orthography to form a detailed orthographic transcrip-tion, following the conventions described in \[6\].There are basically three ways that the system can fail,each of which provides a distinct error message.
If the sen-tence contains unknown words, then the system identifies tothe subject he words that it doesn't know.
If it knows all thewords, but the sentence fails to parse, then it identifies thepoint in the sentence where the parse failed.
Finally, it mayparse the sentence but fail to produce a response due to, forinstance, an incorrect database query.
In that case, it simplysays, "I ran into an error trying to evaluate this sentence.
"Our long-term goal is to make error messages sufficiently in-formative that the subject knows how to best rephrase thequery.
By examining how subjects react to the various kindsof error messages, we hope to improve the overall usability ofthe system.
Figure 2 illustrates the data collection processwith a simple dialogue between a subject and ATIS.SCENARIO: "Find the earliest (or latest) flight from onecity to another that serves a meal of your choice.
"Subject: BEGIN SCENARIO TWOSubject: SHOW ME THE FLIGHTS FROM BOSTON TO ATLANTAAtis: These are the flights from Boston to Atlanta.AIRLINE NUMBER FROM TO DEPARTURE ARRIVAL STOPSDELTA 225 BOS ATL 6:30 A.M 9:13 A.M. 0EASTERN 147 BOS ATL 7:00 A.M. 9:46 A.M. 0DELTA 219 BOS ATL 8:15 A.M. 11:00 A.M. 0EASTERN 643 BOS ATL 8:32 A.M. 11:26 A.M. 0DELTA 547 BOS ATL 9:55 A.M. 12:35 P.M. 0EASTERN 145 EOS ATL 10:35 A.M. 1:27 P.M. 0DELTA 708 BOS ATL 11:56 A.M. 2:35 P.M. 0EASTERN 143 BOS ATL 12:40 P.M. 3:25 P.M. 0EASTERN 646 BOS ATL 2:21 P.M. 5:09 P.M. 0DELTA 975 EOS ATL 3:20 P.M. 6:04 P.M. 0EASTERN 141 BOS ATL 4:05 P.M. 7:00 P.M. 0EASTERN 149 BOS ATL 8:58 P.M. 8:49 P.M. 0DELTA 323 BOS ATL 6:50 P.M. 9:28 P.M. 0DELTA 54 EOS ATL 8:24 P.M. 10:88 P.M. 0Subject: PLEASE SHOW ME THE THREE EARLIEST FLIGHTSAtis: I'm sorry I Eot stuck just before the words"earliest fliEhts".Subject: WHICH ONE IS THE EARLIEST FLIGHTAtis: The earliest flight from Boston to Atlanta?AIRLINE ~UMBER FROM TO DEPARTURE ARRIVAL STOPSDELTA 225 BOS ATL 6:30 A.M 9:13 A.M. 0Subject: DOES THIS FLIGHT SERVE BREAKFASTAtis: Yes, the earliest flight from Boston to Atlanta doesserve breakfast.AIRLINE NUMBER FROM TO DEPARTURE ARRIVAL STOPS MEALSDELTA 225 BOS ATL 6:30 A.M. 9:13 AiM.
0 BREAKFASTSubject: END SCENARIO TWOFigure 2: An example log for one scenario created by a subjectSubjects are recruited from the general vicinity of MIT.No restrictions in age or sex are imposed, nor do we insist362that they be native speakers.
For their efforts, each subjectis given a gift certificate at a popular Chinese restaurant oran ice-cream parlor.
Presently, we are collecting data fromtwo to three subjects per day.COMPARATIVE  ANALYSESTo facilitate system development, training, and testing,we arbitrarily partitioned part of the collected ata into train-ing, development-test, and test sets, as summarized in Ta-ble 1.
All the comparative analyses reported in this sectionare based on our designated training set and the TI trainingset, the latter defined as the total amount of training datareleased by TI prior to June 1990.Data Set \ [#  Speakers\]Trainingi Development-TestTest4110 37110 324Sentences1582'Table 1: Designation of various data sets collected at MIT.General Characteristics Table 2 compares ome generalstatistics of the data in the TI and MIT training set.
Onthe average, the wizard paradigm used at TI can collect 25sentences over approximately 40 minutes, for a yield of 39sentences per hour \[2\].
In contrast, we were able to collect anaverage of about 39 sentences in approximately 45 minutes,for a yield of 53 sentences per hour.
Our higher yield ispresumably due to the fact that the system can respond muchfaster than a wizard; the process of translating the sentencesinto an NLParse command \[2\] by hand can sometimes bequite time-consuming: Note that the yields in both cases donot include the generation of the ancillary files, which is anessential task performed after data collection.Variables TI Data MIT Data-# Speakers 31 41Sentences 774 1582Ave.
# Sentences/Speaker 25.0 38.6Ave.
.# Words/Sentence 10.65 9.14\]% of Table Clarification Sentences 25 1i Ave. ~ Words/Second 1.18 2.04Table 2: General statistics of the training data collected at TIand MIT.The average number of words per sentence for the MITdata is 15% fewer than that for the TI data.
The shortersentences in the MIT data can be due to several reasons.The system's inability to deal with longer sentences and thefeedback that it provides may coerce the subject into makingshorter sentences.
The limited display may discourage theconstruction of lengthy and sometimes contorted sentencesthat attempt o solve the scenarios expeditiously.
The inter-active nature of problem solving may encourage the user totake a "divide-and-conquer" attitude and ask simpler ques-tions.
Closer examination of the data reveals that the stan-dard deviation on sentence length is very different betweenthe two data sets (o'TI = 5.53 and aMiT  = 3.68).
We suspectthat this is primarily due to the preponderance of short sym-bol clarification sentences uch as "What does EA mean?
"in the TI data, along with occasional very long sentences.Table 2 shows that 25% of the TI sentences deal with tableclarification compared to only 1% of our sentences.
In fact, 8of our 16 table clarification sentences concern airline code ab-breviations.
They were collected from earlier sessions whenthe display was still somewhat cryptic.
Once we made someextremely simple changes in the display, such sentences nolonger appeared.The speaking rate of the MIT sentences was more than70% higher that that of the TI sentences.
We believe thatthe speaking rate of the TI sentences (70 words/minute) isunnaturally low.
This may be due to the insertion of manypauses, or the fact that the subjects imply spoke tentatively,due to their unfamiliarity with the task.
Acoustic analysis isclearly needed before we can know for certain.System Growth Rate Figure 3 compares the size of thelexicon, i.e., the number of unique words, as a function of thenumber of training sentences collected at TI and MIT.
TheFigure shows that the vocabulary size grows at a much slowerrate (about 20 words per 100 training sentences) for the MITAWlS data than the TI data (about 50 words per 100 trainingsentences).
Also included on the Figure for reference is aplot of the growth rate for our VOYAGER.
corpus, which wascollected using the same paradigm as we have used for ATIS.A previous comparison of the TI data and the MIT VOYAGER.data \[5\] led to the conclusion that the VOYAGER.
domain  wasintrinsically more restricted.
Since the MIT ATIS data aremore similar to the VOYAGER.
data, it may be the case that amore critical factor was the data collection paradigm.
Thus,one may argue that our data collection procedure is betterable to encourage the subjects to stay within the domain.
Aslow growth rate may also be an indication that the trainingdata is more representative of the unseen test data.As further evidence that our training data is represen-tative of the data that the system is likely to see, Table 3compares the system's performance on the MIT training anddevelopment-test sets.
The similarities in performance be-tween the two data sets is striking, suggesting that the systemis able to generalize well from training data.
Since the sys-tem can deal with over 70% of the sentences, we feel that thesubject is not likely to be overly frustrated by the system'sinability to deal with the remaining sentences.
This also re-flects the apparent ability of subjects to adjust their speechso as to stay generally within the domain of the system.Disfluencies Table 4 compares the occurrence of spon-taneous peech disfluencies in the two data sets.
We define3630oiIu.=_iz80O6004O020C J J J F "  I .
.
.
.
MKATIS Data IP~ I .
.
.
.
.
.
.
MIT VOYAGER Data I00 200 400 600 800 1000 1200 1400 1600Number of Training SentencesFigure 3: The size of the lexicon as a function of the number oftraining sentences for the TI and MIT ATIS training sets, as wellas the MIT VOYAGER training set.% of Sentences Training Set Development-Test Setwith New Words 9.3 8.6with NL failure 16.9Parsed 73.819.771.7Table 3: System performance on the training and develop-ment-test sets.
Evaluation on the training set was conductedafter the system had been trained on these sentences, whereasthe development-test se represents unseen data.% of Sentences TI Data \[MIT Datawith filled pauses 8.1 I 1.3with lexical false starts i 6.0 \] 2.8with linguistic false starts 5.9 1.0Table 4: Analyses of disfluencies in the training data collectedat TI and MIT.lexical false starts as the appearance of a partial word andlinguistic false starts as the appearance ofone or more extra-neous whole words.
Again, our analyses how quite a differ-ence between the two data sets along all dimensions.
A totalof 73 filled pauses appear in 63 (or 8.1%) of the TI sentences,whereas only 25 appear in 21 (or 1.3%) of the MIT sentences.Similarly, it is twice as likely to find a sentence with a lexicalfalse start in the TI data as is in the MIT data, and almostsix times more likely for a linguistic false start.DISCUSSIONBy far the most important feature of our data collectionprocess is that the system under development is used in placeof a wizard.
We believe that this "system-assisted" paradigmoffers several advantages.
Since the system used to collectthe data is under continual development, we can periodicallyreplace it with an improved version, where the improvementwill be guided by the data already collected.
If, for example,we observe that subjects frequently use a certain linguisticconstruct, then we can modify the system to provide thatcapability.
Alternatively, we may decide that the capabilityis outside the domain of expertise of the system (for example,booking flights).
We can then try to keep the subject ':inbounds" by experimenting with different subject instructions.Furthermore, this type of data collection provides relativelyrealistic sample data of human-machine interaction.
Thisdistinguishes it from wizard data, where the human wizardwill answer any query that the back-end can answer.
Webelieve that by providing the subject with a more realisticsituation, where there are a significant number of queries thatthe system cannot handle, we can gather better data aboutthe possible training effects of the system on the subject,the subject's ability to adapt to the system, and the systemcapabilities to provide useful diagnostics on its limitations.By combining data collection and system development intoclosely coupled cycles, we can potentially ensure that thetype of data that we collect is appropriate for the systemthat we want to develop, thus increasing the efficiency ofsystem development.Our data collection procedure is also cost effective.
Sincethe experimenter plays a very passive role during data collec-tion, we eliminate the need for a highly skilled, and presum-ably expensive, person to interpret what was said and coercethe back-end to generate the necessary responses.
This hasthe dual effect of freeing the researchers toconcentrate on sys.tem development, and reducing the cost of data collection.We estimate that the unburdened cost for data collection, in-cluding the subject, the experimenter, and the generation ofthe correct ranscription% is about $0.85 per sentence.
Sub-sequent categorization f the sentences, and the generation ofthe reference answers will add another $2.30 to each sentence,although this may not be needed for all the data collected.The disadvantage of this method of data collection is thatthe baseline system is constantly evolving.
This has severaleffects.
First, data collected in an earlier session may notbe comparable to data collected in a later session.
This isnot important if one merely wishes to collect as much spon-taneous peech within a given domain as possible.
However,it can create a consistency problem if the data are used fortraining at a later stage.
For example, in an earlier session,the system may provide an error message stating that it doesnot understand a particular word, whereas in a later sessionit may be able to handle the identical query with no prob-lem.
The system response can also change from a diagnosticmessage to a request for further information or clarification.The result is that a dialogue collected at one stage in systemdevelopment may not be coherent at a later stage in systemdevelopment.
This is a problem for development of dialoguehandling.
However, it may be possible to handle this by eval-364uating the system after "resetting the context" based on theactual system response, as suggested in \[3\].
Despite thesedifficulties, we feel that the ease of data collection with thismethodology by far outweighs any disadvantages.Comparative analyses of the data collected at TI and MITreveal significant differences in many dimensions.
Given themany ways in which the two procedures differ, it is not al-ways easy to attribute the discrepancies to one single fac-tor.
One of the most striking differences between the TIand MIT data is the fraction of sentences dealing with ta-ble clarification.
One may argue that these sentences areunnecessary by-products of the cryptic display format, andthey contribute very little to the problem of providing race-ful human/machine interface for travel planning.
By a verysimple change in the display format, we were able to reducethis type of question by 25 foldl Similarly, a change in datacollection procedure can reduce the number of spontaneousspeech phenomena several fold.
It is therefore conceivablethat we can minimize the occurrence of spontaneous speechphenomena in veal systems.
These effects again underscorethe importance of collecting the type of data that is as closelymatched as feasible to the capabilities of the system that weare developing.While we have used our own natural language system,TINA, for data  collection, it is important o note that thechoice was primarily motivated by convenience and availabil-ity.
Clearly, any functioning natural anguage system couldbe freely substituted.
In fact, a richer pool of data wouldprobably arise if data were collected independently at severalsites, each of which used their own system for the back-endresponses.At this writing, we have collected 3,690 sentences from102 subjects.
Orthographic transcriptions for all of the sen-tences are available, as are the categorizations and referenceanswers for the development-test and test sets.ACKNOWLEDGEMENTWe would like to acknowledge the help of Claudia Sears,who served as experimenter and transcriber for much of thedata, Victoria Palay, who helped in the coordination of thedata collection, and Bridget Bly of SRI, whom we hired as aconsultant in order to generate some of the ancillary files forthe development-test and test sets.REFERENCES\[1\] Bly, B., P. Price, S. Tepper, E. Jackson, and V. Abrash,"Designing the Human Machine Interface in the ATIS Do-main," Proc.
Third Darpa Speech and Language Workshop,pp.
136-140, Hidden Valley, PA, June 1990.\[2\] Hemphill, C. T., J. J. Godfrey, and G. R.. Doddington, "TheATIS Spoken Language System Pilot Corpus," Proc.
ThirdDarpa Speech and Language Workshop, p. 96-101, HiddenValley, PA , June 1990.\[3\]\[4\]\[5\]\[6\]\[7\]\[s\]\[9\]\[10\]Hirschman, L., D. A. Dahl, D. P. McKay, L. M.
Norton,L., and M. C. Linebarger, "Beyond Class A: A Proposalfor Automatic Evaluation of Discourse," Proc.
Third DarpaSpeech and Language Workshop, p. 109-113,Hidden Valley,PA, June 1990.Kowtko, J. C. and P. J.
Price, "Data Collection and Analysisin the Air Travel Planning Domain," Proc.
Second DarpaSpeech and Language Workshop, p. 119-125, Harwichport,MA, October 1989.Norton, L. M., D. A. Dahl, D. P. McKay, L. Hirschman, M.C.
Linebarger, D. Magerman, and C. N. Ball, "Managementand Evaluation of Interactive Dialog in the Air Travel Do-main," Proc.
Third DaTa Speech and Language Workshop,pp.
141-146, Hidden Valley, PA , June 1990.Polifroni, J. and M. Soclof, "Conventions for Transcrib-ing Spontaneous Speech Events in the VOYAGER.
Corpus,"DARPA SLS Note 6, Spoken Language Systems Group, MITLaboratory for Computer Science, Cambridge, MA, Febru-ary, 1990.Polifroni, J., S. Seneff, V. W. Zue, and L. Hirschman, , "AwlsData Collection at MIT," DAR.PA SLS Note 8, Spoken Lan-guage Systems Group, MIT Laboratory for Computer Sci-ence, Cambridge, MA, November, 1990.Price P., "Evaluation of Spoken Language Systems: TheATIS Domain," Proc.
Third Darpa Speech and LanguageWorkshop, p. 91-95, Hidden Valley, PA , June 1990.Seneff, S., L. Hirschman, and V. W. Zue, "Interactive Prob-lem Solving and Dialogue in the ATIS Domain," These Pro-ceedings.Zue, V., N. Daly, J.
Glass, H. Leung, M. Phillips, J. Po-lJfroni, S. Seneff, and M. Soclof, "The Collection and Pre-lim_inary Analysis of a Spontaneous Speech Database," Proc.Second Darpa Speech and Language Workshop, p. 126-134,Harwichport, MA, October 1989.365
