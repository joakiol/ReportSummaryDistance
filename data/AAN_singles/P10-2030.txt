Proceedings of the ACL 2010 Conference Short Papers, pages 162?167,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPredicate Argument Structure Analysis using Transformation-basedLearningHirotoshi Taira Sanae Fujita Masaaki NagataNTT Communication Science Laboratories2-4, Hikaridai, Seika-cho, Souraku-gun, Kyoto 619-0237, Japan{taira,sanae}@cslab.kecl.ntt.co.jp nagata.masaaki@lab.ntt.co.jpAbstractMaintaining high annotation consistencyin large corpora is crucial for statisticallearning; however, such work is hard,especially for tasks containing semanticelements.
This paper describes predi-cate argument structure analysis using?transformation-based learning.
An advan-tage of transformation-based learning isthe readability of learned rules.
A dis-advantage is that the rule extraction pro-cedure is time-consuming.
We presentincremental-based, transformation-basedlearning for semantic processing tasks.
Asan example, we deal with Japanese pred-icate argument analysis and show sometendencies of annotators for constructinga corpus with our method.1 IntroductionAutomatic predicate argument structure analysis(PAS) provides information of ?who did whatto whom?
and is an important base tool forsuch various text processing tasks as machinetranslation information extraction (Hirschman etal., 1999), question answering (Narayanan andHarabagiu, 2004; Shen and Lapata, 2007), andsummarization (Melli et al, 2005).
Most re-cent approaches to predicate argument structureanalysis are statistical machine learning methodssuch as support vector machines (SVMs)(Pradhanet al, 2004).
For predicate argument struc-ture analysis, we have the following represen-tative large corpora: FrameNet (Fillmore et al,2001), PropBank (Palmer et al, 2005), and Nom-Bank (Meyers et al, 2004) in English, the Chi-nese PropBank (Xue, 2008) in Chinese, theGDA Corpus (Hashida, 2005), Kyoto Text CorpusVer.4.0 (Kawahara et al, 2002), and the NAISTText Corpus (Iida et al, 2007) in Japanese.The construction of such large corpora is strenu-ous and time-consuming.
Additionally, maintain-ing high annotation consistency in such corporais crucial for statistical learning; however, suchwork is hard, especially for tasks containing se-mantic elements.
For example, in Japanese cor-pora, distinguishing true dative (or indirect object)arguments from time-type argument is difficult be-cause the arguments of both types are often ac-companied with the ?ni?
case marker.A problem with such statistical learners as SVMis the lack of interpretability; if accuracy is low, wecannot identify the problems in the annotations.We are focusing on transformation-based learn-ing (TBL).
An advantage for such learning meth-ods is that we can easily interpret the learnedmodel.
The tasks in most previous research aresuch simple tagging tasks as part-of-speech tag-ging, insertion and deletion of parentheses in syn-tactic parsing, and chunking (Brill, 1995; Brill,1993; Ramshaw and Marcus, 1995).
Here we ex-periment with a complex task: Japanese PASs.TBL can be slow, so we proposed an incremen-tal training method to speed up the training.
Weexperimented with a Japanese PAS corpus with agraph-based TBL.
From the experiments, we in-terrelated the annotation tendency on the dataset.The rest of this paper is organized as follows.Section 2 describes Japanese predicate structure,our graph expression of it, and our improvedmethod.
The results of experiments using theNAIST Text Corpus, which is our target corpus,are reported in Section 3, and our conclusion isprovided in Section 4.2 Predicate argument structure andgraph transformation learningFirst, we illustrate the structure of a Japanese sen-tence in Fig.
1.
In Japanese, we can divide a sen-tence into bunsetsu phrases (BP).
A BP usuallyconsists of one or more content words and zero,162BPCW FWKare no tabe ta okashiHe ?sCW FWeat PAST snackwa kinouTOP buy PASTkatCW FW CW FWThe snack he ate is one I bought at the store yesterday.Kareno tabeta okashiwa kinou misede katta.SentenceSyntactic dependency between bunsetsusPRED: PredicateBPBPBPBPyesterdaymise de tashop atBPCWCW FWBP: Bunsetsu phrasePREDARGARGARGARGPREDNom.
Acc.Time Acc.Loc.CW: Content WordFW: Functional WordARG: ArgumentNom: NominativeAcc: AccusativeTime: TimeLoc: LocationArgument TypesDat: DativeFigure 1: Graph expression for PASone, or more than one functional words.
Syn-tactic dependency between bunsetsu phrases canbe defined.
Japanese dependency parsers such asCabocha (Kudo and Matsumoto, 2002) can extractBPs and their dependencies with about 90% accu-racy.Since predicates and arguments in Japanese aremainly annotated on the head content word ineach BP, we can deal with BPs as candidates ofpredicates or arguments.
In our experiments, wemapped each BP to an argument candidate nodeof graphs.
We also mapped each predicate to apredicate node.
Each predicate-argument relationis identified by an edge between a predicate and anargument, and the argument type is mapped to theedge label.
In our experiments below, we definedfive argument types: nominative (subjective), ac-cusative (direct objective), dative (indirect objec-tive), time, and location.
We use five transforma-tion types: a) add or b) delete a predicate node, c)add or d) delete an edge between an predicate andan argument node, e) change a label (= an argu-ment type) to another label (Fig.
2).
We explainthe existence of an edge between a predicate andan argument labeled t candidate node as that thepredicate and the argument have a t type relation-ship.Transformation-based learning was proposedby (Brill, 1995).
Below we explain our learn-ing strategy when we directly adapt the learningmethod to our graph expression of PASs.
First, un-structured texts from the training data are inputted.After pre-processing, each text is mapped to aninitial graph.
In our experiments, the initial graphhas argument candidate nodes with correspondingBPs and no predicate nodes or edges.
Next, com-a) `Add Pred Node?PREDBP BP BPPREDBP BP BPb) `Delete Pred Node?ARGPREDNom.ARGPREDc) `Add Edge?d) `Delete Edge?Nom.ARGPREDAcc.ARGPREDe) `Change Edge Label?Figure 2: Transform typesparing the current graphs with the gold standardgraph structure in the training data, we find the dif-ferent statuses of the nodes and edges among thegraphs.
We extract such transformation rule candi-dates as ?add node?
and ?change edge label?
withconstraints, including ?the corresponding BP in-cludes a verb?
and ?the argument candidate and thepredicate node have a syntactic dependency.?
Theextractions are executed based on the rule tem-plates given in advance.
Each extracted rule isevaluated for the current graphs, and error reduc-tion is calculated.
The best rule for the reductionis selected as a new rule and inserted at the bottomof the current rule list.
The new rule is applied tothe current graphs, which are transferred to othergraph structures.
This procedure is iterated untilthe total errors for the gold standard graphs be-come zero.
When the process is completed, therule list is the final model.
In the test phase, we it-eratively transform nodes and edges in the graphsmapped from the test data, based on rules in themodel like decision lists.
The last graph after allrule adaptations is the system output of the PAS.In this procedure, the calculation of error reduc-tion is very time-consuming, because we have tocheck many constraints from the candidate rulesfor all training samples.
The calculation order isO(MN), where M is the number of articles andN is the number of candidate rules.
Additionally,an edge rule usually has three types of constraints:?pred node constraint,?
?argument candidate nodeconstraint,?
and ?relation constraint.?
The num-ber of combinations and extracted rules are muchlarger than one of the rules for the node rules.Ramshaw et al proposed an index-based efficientreduction method for the calculation of error re-duction (Ramshaw and Marcus, 1994).
However,in PAS tasks, we need to check the exclusivenessof the argument types (for example, a predicate ar-gument structure does not have two nominative ar-163guments), and we cannot directly use the method.Jijkoun et al only used candidate rules that hap-pen in the current and gold standard graphs andused SVM learning for constraint checks (Jijkounand de Rijke, 2007).
This method is effectivefor achieving high accuracy; however, it loses thereadability of the rules.
This is contrary to our aimto extract readable rules.To reduce the calculations while maintainingreadability, we propose an incremental methodand describe its procedure below.
In this proce-dure, we first have PAS graphs for only one arti-cle.
After the total errors among the current andgold standard graphs become zero in the article,we proceed to the next article.
For the next article,we first adapt the rules learned from the previousarticle.
After that, we extract new rules from thetwo articles until the total errors for the articles be-come zero.
We continue these processes until thelast article.
Additionally, we count the number ofrule occurrences and only use the rule candidatesthat happen more than once, because most suchrules harm the accuracy.
We save and use theserules again if the occurrence increases.3 Experiments3.1 Experimental SettingsWe used the articles in the NAIST Text Cor-pus version 1.4?
(Iida et al, 2007) based on theMainichi Shinbun Corpus (Mainichi, 1995), whichwere taken from news articles published in theJapanese Mainichi Shinbun newspaper.
We usedarticles published on January 1st for training ex-amples and on January 3rd for test examples.Three original argument types are defined in theNAIST Text Corpus: nominative (or subjective),accusative (or direct object), and dative (or indi-rect object).
For evaluation of the difficult anno-tation cases, we also added annotations for ?time?and ?location?
types by ourselves.
We show thedataset distribution in Table 1.
We extracted theBP units and dependencies among these BPs fromthe dataset using Cabocha, a Japanese dependencyparser, as pre-processing.
After that, we adaptedour incremental learning to the training data.
Weused two constraint templates in Tables 2 and 3for predicate nodes and edges when extracting therule candidates.Table 1: Data distributionTraining Test# of Articles 95 74# of Sentences 1,129 687# of Predicates 3,261 2,038# of Arguments 3,877 2,468Nom.
1,717 971Acc.
1,012 701Dat.
632 376Time 371 295Loc.
145 125Table 4: Total performances (F1-measure (%))Type System P R F1Pred.
Baseline 89.4 85.1 87.2Our system 91.8 85.3 88.4Arg.
Baseline 79.3 59.5 68.0Our system 81.9 62.4 70.83.2 ResultsOur incremental method takes an hour.
In com-parison, the original TBL cannot even extract onerule in a day.
The results of predicate and argu-ment type predictions are shown in Table 4.
Here,?Baseline?
is the baseline system that predicts theBSs that contain verbs, adjectives, and da formnouns (?to be?
in English) as predicates and pre-dicts argument types for BSs having syntacticaldependency with a predicted predicate BS, basedon the following rules: 1) BSs containing nomina-tive (ga) / accusative (wo) / dative (ni) case mark-ers are predicted to be nominative, accusative, anddative, respectively.
2) BSs containing a topic casemarker (wa) are predicted to be nominative.
3)When a word sense category from a Japanese on-tology of the head word in BS belongs to a ?time?or ?location?
category, the BS is predicted to be a?time?
and ?location?
type argument.
In all preci-sion, recall, and F1-measure, our system outper-formed the baseline system.Next, we show our system?s learning curve inFig.
3.
The number of final rules was 68.
Thisindicates that the first twenty rules are mainly ef-fective rules for the performance.
The curve alsoshows that no overfitting happened.
Next, weshow the performance for every argument type inTable 5.
?TBL,?
which stands for ?transformation-based learning,?
is our system.
In this table,the performance of the dative and time types im-proved, even though they are difficult to distin-guish.
On the other hand, the performance of thelocation type argument in our system is very low.Our method learns rules as decreasing errors of164Table 2: Predicate node constraint templatesPred.
Node Constraint Template Rule ExampleConstraint Description Pred.
Node Constraint Operationpos1 noun, verb, adjective, etc.
pos1=?ADJECTIVE?
add pred nodepos2 independent, attached word, etc.
pos2=?DEPENDENT WORD?
del pred nodepos1 & pos2 above two features combination pos1=?VERB?
& pos2=?ANCILLARY WORD?
add pred node?da?
da form (copula) ?da form?
add pred nodelemma word base form lemma=?%?
add pred nodeTable 3: Edge constraint templatesEdge Constraint Template Rule ExampleArg.
Cand.
Pred.
Node Relation Edge Constraint OperationConst.
Const.
Const.FW (=func.word)?
dep(arg?
pred) FW of Arg.
=?wa(TOP)?
& dep(arg?
pred) add NOM edge?
FW dep(arg?
pred) FW of Pred.
=?na(ADNOMINAL)?
& dep(arg?
pred)add NOM edgeSemCat(=semanticcategory)?
dep(arg?
pred) SemCat of Arg.
= ?TIME?
& dep(arg?
pred) add TIME edgeFW passive form dep(arg?
pred) FW of Arg.
=?ga(NOM) & Pred.
: passive form chg edge labelNOM?
ACC?
kform (= typeof inflectedform)?
kform of Pred.
= continuative ?ta?
form add NOM edgeSemCat Pred.
SemCat ?
SemCat of Arg.
= ?HUMAN?
& Pred.
SemCat= ?PHYSICAL MOVE?add NOM edge01020304050607080F1-measure (%)10 20 30 40 50 60070rulesFigure 3: Learning curves: x-axis = number ofrules; y-axis: F1-measure (%)all arguments, and the performance of the locationtype argument is probably sacrificed for total errorreduction because the number of location type ar-guments is much smaller than the number of otherargument types (Table 1), and the improvement ofthe performance-based learning for location typearguments is relatively low.
To confirm this, weperformed an experiment in which we gave therules of the baseline system to our system as initialrules and subsequently performed our incremen-tal learning.
?Base + TBL?
shows the experiment.The performance for the location type argumentimproved drastically.
However, the total perfor-mance of the arguments was below the originalTBL.
Moreover, the ?Base + TBL?
performancesurpassed the baseline system.
This indicates thatour system learned a reasonable model.Finally, we show some interesting extractedrules in Fig.
4.
The first rule stands for an ex-pression where the sentence ends with the per-formance of something, which is often seen inJapanese newspaper articles.
The second and thirdrules represent that annotators of this dataset tendto annotate time types for which the semantic cate-gory of the argument is time, even if the argumentlooks like the dat.
type, and annotators tend to an-notate dat.
type for arguments that have an dat.165if BP contains the word `%?
,Add Pred.
NodePREDDat.
/ TimeARGPREDif func.
wd.
is `DAT?
case,Rule No.20 CW`%?BPRule No.15Time / Dat.ARGPREDRule No.16Change Edge LabelChange Edge LabelDat.
?TimeSemCat is `Time?ExampleExample??
?BPCWBPCWBPCWkotae-ta hito-wa87%-deanswer-ed people-TOP87%-be`People who answered are 87%?PRED7?
?BPCWBPCW7ka-nistaato-suru7th DAT start will`will start on the 7th?ARGPREDFWFW FW FW???
??
?????
?
?TimeARGPREDDat.Rule No.16 is appliedFigure 4: Examples of extracted rulesTable 5: Results for every arg.
type (F-measure(%))System Args.
Nom.
Acc.
Dat.
Time Loc.Base 68.0 65.8 79.6 70.5 51.5 38.0TBL 70.8 64.9 86.4 74.8 59.6 1.7Base + TBL 69.5 63.9 85.8 67.8 55.8 37.4type case marker.4 ConclusionWe performed experiments for Japanese predicateargument structure analysis using transformation-based learning and extracted rules that indicate thetendencies annotators have.
We presented an in-cremental procedure to speed up rule extraction.The performance of PAS analysis improved, espe-cially, the dative and time types, which are difficultto distinguish.
Moreover, when time expressionsare attached to the ?ni?
case, the learned modelshowed a tendency to annotate them as dative ar-guments in the used corpus.
Our method has po-tential for dative predictions and interpreting thetendencies of annotator inconsistencies.AcknowledgmentsWe thank Kevin Duh for his valuable comments.ReferencesEric Brill.
1993.
Transformation-based error-drivenparsing.
In Proc.
of the Third International Work-shop on Parsing Technologies.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Charles J. Fillmore, Charles Wooters, and Collin F.Baker.
2001.
Building a large lexical databankwhich provides deep semantics.
In Proc.
of the Pa-cific Asian Conference on Language, Informationand Computation (PACLING).Kouichi Hashida.
2005.
Global document annotation(GDA) manual.
http://i-content.org/GDA/.Lynette Hirschman, Patricia Robinson, LisaFerro, Nancy Chinchor, Erica Brown,Ralph Grishman, and Beth Sundheim.1999.
Hub-4 Event?99 general guidelines.http://www.itl.nist.gov/iaui/894.02/related projects/muc/.Ryu Iida, Mamoru Komachi, Kentaro Inui, and YujiMatsumoto.
2007.
Annotating a Japanese text cor-pus with predicate-argument and coreference rela-tions.
In Proc.
of ACL 2007 Workshop on LinguisticAnnotation, pages 132?139.Valentin Jijkoun and Maarten de Rijke.
2007.
Learn-ing to transform linguistic graphs.
In Proc.
ofthe Second Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing(TextGraphs-2), pages 53?60.
Association for Com-putational Linguistics.166Daisuke Kawahara, Sadao Kurohashi, and KoichiHashida.
2002.
Construction of a Japaneserelevance-tagged corpus (in Japanese).
Proc.
of the8th Annual Meeting of the Association for NaturalLanguage Processing, pages 495?498.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
of the 6th Conference on Natural LanguageLearning 2002 (CoNLL 2002).Mainichi.
1995.
CD Mainichi Shinbun 94.
NichigaiAssociates Co.Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,and Fred Popowich.
2005.
Description ofSQUASH, the SFU question answering summaryhandler for the DUC-2005 summarization task.
InProc.
of DUC 2005.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBank project:An interim report.
In Proc.
of HLT-NAACL 2004Workshop on Frontiers in Corpus Annotation.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion answering based on semantic structures.
InProc.
of the 20th International Conference on Com-putational Linguistics (COLING).M.
Palmer, P. Kingsbury, and D. Gildea.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Sameer Pradhan, Waybe Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proc.
ofthe Human Language Technology Conference/NorthAmerican Chapter of the Association of Computa-tional Linguistics HLT/NAACL 2004.Lance Ramshaw and Mitchell Marcus.
1994.
Explor-ing the statistical derivation of transformational rulesequences for part-of-speech tagging.
In The Bal-ancing Act: Proc.
of the ACL Workshop on Com-bining Symbolic and Statistical Approaches to Lan-guage.Lance Ramshaw and Mitchell Marcus.
1995.
Textchunking using transformation-based learning.
InProc.
of the third workshop on very large corpora,pages 82?94.Dan Shen and Mirella Lapata.
2007.
Using se-mantic roles to improve question answering.
InProc.
of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processingand Computational Natural Language Learning(EMNLP/CoNLL), pages 12?21.Nianwen Xue.
2008.
Labeling Chinese predicateswith semantic roles.
Computational Linguistics,34(2):224?255.167
