Proceedings of NAACL-HLT 2013, pages 1041?1050,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsIdentifying Intention Posts in Discussion ForumsZhiyuan Chen, Bing Liu Meichun Hsu, Malu Castellanos,Riddhiman GhoshDepartment of Computer Science HP LabsUniversity of Illinois at Chicago Palo Alto, CA 94304, USAChicago, IL 60607, USA {meichun.hsu, malu.castellanos,riddhiman.ghosh}@hp.com czyuanacm@gmail.com,liub@cs.uic.eduAbstractThis paper proposes to study the problem ofidentifying intention posts in online discus-sion forums.
For example, in a discussion fo-rum, a user wrote ?I plan to buy a camera,?which indicates a buying intention.
This in-tention can be easily exploited by advertisers.To the best of our knowledge, there is still noreported study of this problem.
Our researchfound that this problem is particularly suitedto transfer learning because in different do-mains, people express the same intention insimilar ways.
We then propose a new transferlearning method which, unlike a generaltransfer learning algorithm, exploits severalspecial characteristics of the problem.
Exper-imental results show that the proposed meth-od outperforms several strong baselines,including supervised learning in the targetdomain and a recent transfer learning meth-od.1 IntroductionSocial media content is increasingly regarded asan information gold mine.
Researchers have stud-ied many problems in social media, e.g., senti-ment analysis (Pang & Lee, 2008; Liu, 2010) andsocial network analysis (Easley & Kleinberg,2010).
In this paper, we study a novel problemwhich is also of great value, namely, intentionidentification, which aims to identify discussionposts expressing certain user intentions that can beexploited by businesses or other interested parties.For example, one user wrote, ?I am looking for abrand new car to replace my old Ford Focus?.Identifying such intention automatically can helpsocial media sites to decide what ads to display sothat the ads are more likely to be clicked.This work focuses on identifying user postswith explicit intentions.
By explicit we mean thatthe intention is explicitly stated in the text, noneed to deduce (hidden or implicit intention).
Forexample, in the above sentence, the author clearlyexpressed that he/she wanted to buy a car.
On theother hand, an example of an implicit sentence is?Anyone knows the battery life of iPhone??
Theperson may or may not be thinking about buyingan iPhone.To our knowledge, there is no reported study ofthis problem in the context of text documents.
Themain related work is in Web search, where user(or query) intent classification is a major issue(Hu et al 2009; Li, 2010; Li, Wang, & Acero,2008).
Its task is to determine what the user issearching for based on his/her keyword queries (2to 3 words) and his/her click data.
We will discussthis and other related work in Section 2.We formulate the proposed problem as a two-class classification problem since an applicationmay only be interested in a particular intention.We define intention posts (positive class) as theposts that explicitly express a particular intentionof interest, e.g., the intention to buy a product.The other posts are non-intention posts (negativeclass).
Note that we do not exploit intention spe-cific knowledge since our aim is to propose a ge-neric method applicable to different types ofintentions.There is an important feature about this prob-lem which makes it amenable to transfer learningso that we do not need to label data in every do-main.
That is, for a particular kind of intentionsuch as buying, the ways to express the intentionin different domains are often very similar.
This1041fact can be exploited to build a classifier based onlabeled data in some domains and apply it to anew/target domain without labeling any trainingdata in the target domain.
However, this problemalso has some special difficulties that existinggeneral transfer learning methods do not dealwith.
The two special difficulties of the proposedproblem are as follows:1.
In an intention post, the intention is typicallyexpressed in only one or two sentences whilemost sentences do not express intention, whichprovide very noisy data for classifiers.
Fur-thermore, words/phrases used for expressingintention are quite limited compared to othertypes of expressions.
These mean that the setof shared (or common) features in differentdomains is very small.
Most of the existing ad-vanced transfer learning methods all try to ex-tract and exploit these shared features.
Thesmall number of such features in our taskmakes it hard for the existing methods to findthem accurately, which in turn learn poorerclassifiers.2.
As mentioned above, in different domains, theways to express the same intention are oftensimilar.
This means that only the positive (in-tention) features are shared among differentdomains, while features indicating the negativeclass in different domains are very diverse.
Wethen have an imbalance problem, i.e., theshared features are almost exclusively featuresindicating the positive class.
To ourknowledge, none of the existing transfer learn-ing methods deals with this imbalance problemof shared features, which also results in inaccu-rate classifiers.We thus propose a new transfer learning (or do-main adaptation) method, called Co-Class, which,unlike a general transfer learning method, is ableto deal with these difficulties in solving the prob-lem.
Co-Class works as follows: we first build aclassifier   using the labeled data from existingdomains, called the source data, and then applythe classifier to classify the target (domain) data(which is unlabeled).
Based on the target data la-beled by  , we perform a feature selection on thetarget data.
The selected set of features is used tobuild two classifiers, one (  ) from the labeledsource data and one (  ) from the target datawhich has been labeled by  .
The two classifiers(   and   ) then work together to perform classi-fication of the target data.
The process then runsiteratively until the labels assigned to the targetdata stabilize.
Note that in each iteration bothclassifiers are built using the same set of featuresselected from the target domain in order to focuson the target domain.
The proposed Co-Class ex-plicitly deals with the difficulties mentionedabove (see Section 3).
Our experiments using fourreal-life data sets extracted from four forum dis-cussion sites show that Co-Class outperforms sev-eral strong baselines.
What is also interesting isthat it works even better than fully supervisedlearning in the target domain itself, i.e., using bothtraining and test data in the target domain.
It alsooutperforms a recent state-of-the-art transferlearning method (Tan et al 2009), which hasbeen successfully applied to the NLP task of sen-timent classification.In summary, this paper makes two main contri-butions:1.
It proposes to study the novel problem of inten-tion identification.
User intention is an im-portant type of information in social mediawith many applications.
To our knowledge,there is still no reported study of this problem.2.
It proposes a new transfer learning method Co-Class which is able to exploit the above twokey issues/characteristics of the problem inbuilding cross-domain classifiers.
Our experi-mental results demonstrate its effectiveness.2 Related WorkAlthough we have not found any paper studyingintention classification of social media posts, thereare some related works in the domain of Websearch, where user or query intent classification isa major issue (Hu et al 2009; Li, 2010; Li et al2008).
The task there is to classify a query submit-ted to a search engine to determine what the useris searching for.
It is different from our problembecause they classify based on the user-submittedkeyword queries (often 2 to 3 words) togetherwith the user?s click-through data (which repre-sent the user?s behavior).
Such intents are typical-ly implicit because people usually do not issue asearch query like ?I want to buy a digital cam-era.?
Instead, they may just type the keywords?digital camera?.
Our interest is to identify explic-it intents expressed in full text documents (forumposts).
Another related problem is online com-mercial intention (OCI) identification (Dai et al10422006; Hu et al 2009), which focuses on capturingcommercial intention based on a user query andweb browsing history.
In this sense, OCI is still auser query intent problem.In NLP, (Kanayama & Nasukawa, 2008) stud-ied users?
needs and wants from opinions.
Forexample, they aimed to identify the user needsfrom sentences such as ?I?d be happy if it isequipped with a crisp LCD.?
This is clearly dif-ferent from our explicit intention to buy or to usea product/service, e.g., ?I plan to buy a new TV.
?Our proposed Co-Class technique is related totransfer learning or domain adaptation.
The pro-posed method belongs to ?feature representationtransfer" from source domain to target domain(Pan & Yang, 2010).
Aue & Gamon (2005) triedtraining on a mixture of labeled reviews from oth-er domains where such data are available and teston the target domain.
This is basically one of ourbaseline methods 3TR-1TE in Section 4.
Theirwork does not do multiple iterations and does notbuild two separate classifiers as we do.
Some re-lated methods were also proposed in (W. Dai,Xue, Yang & Yu, 2007; Tan et al 2007; Yang, Si& Callan, 2006).
More sophisticated transferlearning methods try to find common features inboth the source and target domains and then try tomap the differences of the two domains (Blitzer,Dredze, & Pereira, 2007; Pan, et al2010; Bolle-gala, Weir & Carroll, 2011; Tan et al 2009).Some researchers also used topic modeling ofboth domains to transfer knowledge (Gao & Li,2011; He, Lin & Alani, 2011).
However, none ofthese methods deals with the two prob-lems/difficulties of our task.
Co-Class tacklesthem explicitly and effectively (Section 4).The proposed Co-Class method is also relatedto Co-Training method in (Blum & Mitchell,1998).
We will compare them in detail in Section3.3.3 The Proposed TechniqueWe now present the proposed technique.
Our ob-jective is to perform classification in the targetdomain by utilizing labeled data from the sourcedomains.
We use the term ?source domains?
aswe can combine labeled data from multiple sourcedomains.
The target domain has no labeled data.Only the source domain data are labeled.To deal with the first problem in Section 1 (i.e.,the difficulty of finding common features acrossdifferent domains), Co-Class avoids it by using anEM-based method to iteratively transfer from thesource domains to the target domain while ex-ploiting feature selection in the target domain tofocus on important features in the target domain.Since our ideas are developed starting from theEM (Expectation Maximization) algorithm and itsshortcomings, we now introduce EM.3.1 EM AlgorithmEM (Dempster, Laird, & Rubin, 1977) is a popu-lar class of iterative algorithms for maximum like-lihood estimation in problems with incompletedata.
It is often used to address missing values inthe data by computing expected values using ex-isting values.
The EM algorithm consists of twosteps, the Expectation step (E-step) and the Max-imization step (M-step).
E-step basically fills inthe missing data, and M-step re-estimates the pa-rameters.
This process iterates until convergence.Since our target data have no labels, which can betreated as missing values/data, the EM algorithmnaturally applies.
For text classification, each iter-ation of EM (Nigam, McCallum, Thrun, & Mitch-ell, 2000) usually uses the na?ve Bayes (NB)classifier.
Below, we first introduce the NB classi-fier.Given a set of training documents  , each doc-ument      is an ordered list of words.
We useto denote the word in the position   of   ,where each word is from the vocabulary| | , which is the set of all words con-sidered in classification.
We also have a set ofclasses         representing positive and neg-ative classes.
For classification, we compute theposterior probability      |   .
Based on theBayes rule and multinomial model, we have:(1)and with Laplacian smoothing:(2)where          is the number of times that theword    occurs in document   , and   (  |  )is the probability of assigning class    to   .Assuming that word probabilities are independentgiven a class, we have the NB classifier:?
???
?????
|V|s|D|i ijis|D|i ijitjt d|cd,wN|V|d|cd,wNc|w1 11))Pr(())Pr((1)?r(||)|(r)(r||1DdccDi ijj?
?
??
?1043(3)The EM algorithm basically builds a classifieriteratively using NB and both the labeled sourcedata and the unlabeled target data.
However, themajor shortcoming is that the feature set, evenwith feature selection, may fit the labeled sourcedata well but not the target data because the targetdata has no labels to be used in feature selection.Feature selection is shown to be very importantfor this application as we will see in Section 4.3.2 FS-EMBased on the discussion above, the key to solvethe problem of EM is to find a way to reflect thefeatures in the target domain during the iterations.We propose two alternatives, FS-EM (FeatureSelection EM) and Co-Class (Co-Classification).This sub-section presents FS-EM.EM can select features only before iterationsusing the labeled source data and keep using thesame features in each iteration.
However, thesefeatures only fit the labeled source data but not thetarget data.
We then propose to select featuresduring iterations, i.e., after each iteration, we re-do feature selection.
For this, we use the predictedclasses of the target data.
In na?ve Bayes, we de-fine the predicted class for document    as|    (4)The detailed algorithm for FS-EM is given inFigure 1.
First, we select a feature set from thelabeled source data    and then build an initialNB classifier (lines 1 and 2).
The feature selectionis based on Information Gain, which will be intro-duced in Section 3.4.
After that, we classify eachdocument in the target data    to obtain its pre-dicted class (lines 4-6).
A new target data setis produced in line 7, which is    with addedclasses (predicted in line 5).
Line 8 selects a newfeature set   from the data    (which is discussedbelow), from which a new classifier   is built(line 9).
The iteration stops when the predictedclasses of    do not change any more (line 10).We now turn to the data set   , which can beformed with one of the two methods:1.2.The first method (called FS-EM1) merges thelabeled source data    and the target data(with predicted classes).
However, this methoddoes not work well because the labeled sourcedata can dominate    and the target domain fea-tures are still not well represented.The second method (     ), denoted as FS-EM2, selects features from the target domain dataonly based on the predicted classes.
The clas-sifiers are built in iterations (lines 3-10) using on-ly the target domain data.
The weakness of this isthat it completely ignores the labeled source dataafter initialization, but the source data does con-tain some valuable information.
Our final pro-posed method Co-Class is able to solve thisproblem.3.3 Co-ClassCo-Class is our final proposed algorithm.
It con-siders both the source labeled data and the targetdata with predicted classes.
It uses the idea of FS-EM, but is also inspired by Co-Training in (Blum& Mitchell, 1998).
It additionally deals with thesecond issue identified in Section 1 (i.e., the im-balance of shared positive and negative features).Co-Training is originally designed for semi-supervised learning to learn from a small labeledand a large unlabeled set of training examples,which assumes the set of features in the data canbe partitioned into two subsets, and each subset issufficient for building an accurate classifier.
Theproposed Co-Class model is similar to Co-Training in that it also builds two classifiers.However, unlike Co-Training, Co-Class does notpartition the feature space.
Instead, one classifieris built based on the target data with predictedclasses (  ), and the other classifier is built usingonly the source labeled data (  ).
Both classifiersuse the same features (this is an important point)that are selected from the target data    only, inorder to focus on the target domain.
The finalclassification is based on both classifiers.
Fur-thermore, Co-Training only uses the data from thesame domain.The detailed Co-Class algorithm is given inFigure 2.
Lines 1-6 are the same as lines 1, 2 and4-7 in FS-EM.
Line 8 selects new features   from.
Two na?ve Bayes classifiers,    and   , arethen built using the source data    and predictedtarget data    respectively with the same set of?
???
????????
||1||1 ,||1 ,)|(r)(r)|(r)(r)|(r Crdk rkdrdk jkdjij iiiicwccwcdc1044features   (lines 9-10).
Lines 11-13 classify eachtarget domain document    using the two classifi-ers.
(             ) is the aggregate function tocombine the results of two classifiers.
It is definedas:(             )  {This aims to deal with the imbalanced featureproblem.
As discussed before, the expressions forstating a particular intention (e.g., buying) arevery similar across domains but the non-intentionexpressions across domains are highly diverse,which result in strong positive features and weaknegative features.
We then need to restrict thepositive class by requiring both classifiers to givepositive predictions.
If we use the method in Co-Training (multiplying the probabilities of the twoNB classifiers), the classification results deterio-rate from iteration to iteration because the positiveclass recall gets higher and higher due to strongpositive features, but the precision gets lower andlower.Since we build and use two classifiers for thefinal classification, we call the method Co-Class,short for Co-Classification.
Co-Class is differentfrom EM (Nigam et al 2000) in two main aspects.First, it integrates feature selection into the itera-tions, which has not been done before.
Featureselection refines features to enhance the correla-tion between the features and classes.
Second, twoclassifiers are built based on different domainsand combined to improve the classification.
Onlyone classifier is built in existing EM methods,which gives poorer results (Section 4).3.4 Feature SelectionAs feature selection is important for our task, webriefly introduce the Information Gain (IG) meth-od given in (Yang & Pedersen, 1997), which is apopular feature selection algorithm for text classi-fication.
IG is based on entropy reflecting the pu-rity of the categories or classes by knowing thepresence or absence of each feature, which is de-fined as:?
??????
?ffmiiimiii fcPfcPfPcPcPfIG, 11)|(log)|()()(log)()(Using the IG value of each feature  , all fea-tures can be ranked.
As in normal classificationtasks, the common practice is to use a set of topranked features for classification.4 EvaluationWe have conducted a comprehensive set of exper-iments to compare the proposed Co-Class methodwith several strong baselines, including a state-of-the-art transfer learning method.4.1 Experiment SettingsDatasets: We created 4 different domain datasetscrawled from 4 different forum discussion sites:Cellphone: http://www.howardforums.com/forums.phpElectronics: http://www.avsforum.com/avs-vb/Camera: http://forum.digitalcamerareview.com/Algorithm FS-EMInput:  Labeled data    and unlabeled data1   Select a feature set   based on IG from   ;2   Learn an initial na?ve Bayes classifier   frombased on   (using Equations (1) and (2));3   repeat4       for each document    in    do5                  ;   // predict the class of    using6       end7       Produce data    based on predicted class of   ;8       Select a new feature set   from   ;9       Learn a new classifier   onbased on the new feature set  ;10 until the predicted classes of    stabilize11 Return the classifier   from the last iteration.Figure 1 ?
The FS-EM algorithmAlgorithm Co-ClassInput:  Labeled data    and unlabeled data1   Select a feature set   based on IG from   ;2   Learn an initial na?ve Bayes classifier   frombased on   (using Equations (1) and (2));3  for each document    in    do4              ;   // predict the class of    using5    end6   Produce data    based on the predicted class of   ;7   repeat8       Select a new feature set   from   ;9       Build a na?ve Bayes classifier    using   and   ;10     Build a na?ve Bayes classifier    using   and   ;11     for each document    in    do12             (             ); // Aggregate function13     end14     Produce data    based on predicted class of   ;15 until the prediction classes of    stabilize16 Return classifiers    and    from the last iteration.Figure 2 ?
The Co-Class algorithm1045TV: http://www.avforums.com/forums/tvs/For our experiments, we are interested in the in-tention to buy, which is our intention or positiveclass.
For each dataset, we manually labeled 1000posts.Labeling: We initially labeled about one fifth ofposts by two human annotators.
We found theirlabels highly agreed.
We then used only one anno-tator to complete the remaining labeling.
The rea-son for the strong labeling agreement is that weare interested in only explicit buying intentions,which are clearly expressed in each post, e.g., ?Iam in the market for a new smartphone.?
There islittle ambiguity or subjectivity in labeling.To ensure that the task is realistic, for all da-tasets we keep their original class distributions asthey are extracted from their respective websitesto reflect the real-life situation.
The intention classis always the minority class, which makes it muchharder to predict due to the imbalanced class dis-tribution.
Table 1 gives the statistics of each da-taset.
On average, each post contains about 7.5sentences and 122 words.
We have made the da-tasets used in this paper publically available at thewebsites of the first two authors.Evaluation measures: For all experiments, weuse precision, recall and F1-score as the evalua-tion measures.
They are suitable because our ob-jective is to identify intention posts.4.2 One Domain LearningThe objective of our work is to classify the targetdomain instances without labeling any target do-main data.
To set the background, we first givethe results of one domain learning, i.e., assumingthat there is labeled training data in the target do-main (which is the traditional fully supervisedlearning).
We want to see how the results of Co-Class compare with the fully supervised learning.For this set of experiments, we use na?ve Bayesand SVM.
For na?ve Bayes, we use the Lingpipeimplementation (http://alias-i.com/lingpipe/).
ForSVM, we use SVMLight (Joachims, 1999) from(http://svmlight.joachims.org/) with the linearkernel as it has been shown by many researchersthat linear kernel is sufficient for text classifica-tion (Joachims, 1998; Yang and Liu, 1999).During labeling, we observed that the intentionin an intention (positive) post is often expressed inthe first few or the last few sentences.
Hence, wetried to use the full post (denoted by Full), the first5 sentences (denoted by (5, 0)), and first 5 and last5 sentences (denoted by (5, 5)).
We also experi-mented with the first 3 sentences, and first 3 andlast 3 sentences but their results were poorer.The experiments were done using 10-fold crossvalidation.
For the number of selected features,we tried 500, 1000, 1500, 2000, 2500 and all.
Wealso tried unigrams, bigrams, trigrams, and 4-grams.
To compare na?ve Bayes with SVM, wetried each combination, i.e.
number of featuresand n-grams, and found the best model for eachmethod.
We found that na?ve Bayes works bestwhen using trigrams with 1500 selected features.Bigrams with 1000 features are the best combina-tion for SVM.
Figure 3 shows the comparison ofthe best results (F1-scores) of na?ve Bayes andSVM.From Figure 3, we make the following observa-tions:1.
SVM does not do well for this task.
We tunedthe parameters of SVM, but the results weresimilar to the default setting, and all wereworse than na?ve Bayes.
We believe the mainreason is that the data for this application ishighly noisy because apart from one or two in-tention sentences, other sentences in an inten-tion post have little difference from those in anon-intention post.
SVM does not perform wellwith very noisy data.
When there are datapoints far away from their own classes, SVMDatasetNo.
ofIntentionNo.
ofNon-IntentionTotal No.of postsCellphone 184 816 1000Electronics 280 720 1000Camera 282 718 1000TV 263 737 1000Table 1: Datasets statistics with the buy intentionFigure 3 ?
Na?ve Bayes vs. SVM1046tends to be strongly affected by such points(Wu & Liu, 2007).
Na?ve Bayes is more robustin the presence of noise due to its probabilisticnature.2.
SVM using only the first few and/or last fewsentences performs better than using full postsbecause full posts have more noise.
However,it is still worse than na?ve Bayes.3.
For na?ve Bayes, using full posts and the first 5and last 5 (5, 5) sentences give similar results,which is not surprising as (5, 5) has almost allthe information needed.
Without using the last5 sentence (5, 0), the results are poorer.We also found that without feature selection (us-ing all features), the results are markedly worsefor both na?ve Bayes and SVM.
This is under-standable (as we discussed earlier) because mostwords and sentences in both intention and non-intention posts are very similar.
Thus, feature se-lection is highly desirable for this application.Effect of different combinations: Table 2 givesthe detailed F1-score results of na?ve Bayes withbest results in different n-grams (with best numberof features).
We can see that using trigrams pro-duces the best results on average, but bigrams and4-grams are quite similar.
It turns out that usingtrigrams with 1500 selected features performs thebest.
SVM results are not shown as they are poor-er.In summary, we say that na?ve Bayes is moresuitable than SVM for our application and featureselection is crucial.
In our experiments reportedbelow, we will only use na?ve Bayes with featureselection.4.3 Evaluation of Co-ClassWe now compare Co-Class with the baselinemethods listed below.
Note that for this set of ex-periments, the source data all contain labeledposts from three domains and the target data con-tain unlabeled posts in one domain.
That is, foreach target domain, we merge three other domainsfor training and the target domain for testing.
Forexample, for the target of ?Cellphone?, the modelis built using the data from the other three do-mains (i.e., ?Electronics?, ?Camera?
and ?TV?
).The results are the classification of the model onthe target domain ?Cellphone?.
Several strongbaselines are described as follows:3TR-1TE: Use labeled data from three do-mains to train and then classify the target (test)domain.
There is no iteration.
This method wasused in (Aue & Gamon, 2005).EM: This is the algorithm in Section 3.1.
Thecombined data from three domains are used as thelabeled source data.
The data of the remaining onedomain are used as the unlabeled target data,which is also used as the test data (since it is unla-beled).ANB: This is a recent transfer learning method(Tan et al 2009).
ANB uses frequently co-occurring entropy (FCE) to pick out generalizable(or shared) features that occur frequently in boththe source and target domains.
Then, a weightedtransfer version of na?ve Bayes classifier is ap-plied.
We chose this method for comparison as itis a recent method, also based on na?ve Bayes, andhas been applied to the NLP task of sentimentNa?ve Bayes(n-grams, features)Cellphone Electronics Camera TVFull 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5Unigrams, 2000 59.91 55.21 56.76 71.31 70.10 71.24 71.57 71.53 75.78 74.96 74.45 74.13Bigrams, 1500 61.97 54.29 59.17 70.71 71.46 72.48 77.02 74.12 77.38 79.76 77.71 79.72Trigrams, 1500 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.924-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88Table 2: One-domain learning using na?ve Bayes with n-grams (with best no.
of features)Na?ve Bayes(n-grams, features)Cellphone Electronics Camera TVFull 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5Trigrams, 2000 57.98 57.60 58.67 71.85 69.74 71.51 74.45 73.58 74.24 74.07 71.34 73.65Trigrams, 2500 58.08 57.48 59.12 72.27 69.65 71.82 76.15 73.64 76.31 74.02 71.25 73.49Trigrams, 3000 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73Trigrams, 3500 56.60 56.81 57.21 71.86 70.40 72.24 77.17 74.85 76.68 74.25 71.10 73.374-grams, 2000 58.94 51.94 57.72 72.03 71.98 73.05 79.84 75.09 79.46 79.12 76.61 79.88Table 3: F1-scores of 3TR-1TE with trigrams and different no.
of features1047classification, which to some extend is related tothe proposed task of intention classification.
ANBwas also shown to perform better than EM andna?ve Bayes transfer learning method (Dai et al2007).We look at the results of 3TR-1TE first, whichare shown in Table 3.
Due to space limitations, weonly show the trigrams F1-scores as they performthe best on average.
Table 3 gives the number offeatures with trigrams.
We can observe that onaverage using 3000 features gives the best F1-score results.
It has 1000 more features than onedomain learning because we now combine threedomains (3000 posts) for training and thus moreuseful features.From Table 3, we observe that the F1-score re-sults of 3TR-1TE are worse than those of one do-main learning (Table 2), which is intuitivebecause no training data are used from the targetdomain.
But the results are not dramatically worsewhich indicate that there are some common fea-tures in different domains, meaning people ex-pressing the same intention in similar ways.Since we found that trigrams with 3000 featuresperform the best on average, we run EM, FS-EM1, FS-EM2 and Co-Class based on trigramswith 3000 features.
For the baseline ANB, wetuned the parameters using a development set(1/10 of the training data).
We found that select-ing 2000 generalizable/shared features gives thebest results (the default is 500 in (Tan et al2009)).
We kept ANB?s other original parametervalues.
The F1-scores (averages over all 4 da-tasets) with the number of iterations are shown inFigure 4.
Iteration 0 is the result of 3TR-1TE.From Figure 4, we can make the following obser-vations:1.
EM makes a little improvement in iteration 1.After that, the results deteriorate.
The gain ofiteration 1 shows that incorporating the targetdomain data (unlabeled) is helpful.
However,the selected features from source domains canonly fit the labeled source data but not the tar-get data, which was explained in Section 3.1.2.
ANB improves slightly from iteration 1 to iter-ation 6, but the results are all worse than thoseof Co-Class.
We checked the generaliza-ble/shared features of ANB and found that theywere not suitable for our problem since theywere mainly adjectives, nouns and sentimentverbs, which do not have strong correlationwith intentions.
This shows that it is hard tofind the truly shared features indicating inten-tions.
Furthermore, ANB?s results are almostthe same as those of EM.3.
FS-EM2 behaves similarly to FS-EM1.
Aftertwo iterations, the results start to deteriorate.Selecting features only from the target domainmakes sense since it can reflect target domaindata well.
However, it also becomes worsewith the increased number of iterations, due tostrong positive features.
With increased itera-tions, positive features get stronger due to theimbalanced feature problem discussed in Sec-tion 1.4.
Co-Class performs much better than all othermethods.
With the increased number of itera-tions, the results actually improve.
Startingfrom iteration 7, the results stabilize.
Co-Classsolves the problem of strong positive featuresby requiring strong conditions for positiveclassification and focusing on features in thetarget domain only.
Although the detailed re-sults of precision and recall are not shown, theCo-Class model actually improves the F1-scoreby improving both the precision and recall.Significance of improvement: We now discussthe significance of improvements by comparingthe results of Co-Class with other models.
Table 4summarizes the results among the models.
ForCo-Class, we use the converged models at itera-tion 7.
We also include the One Domain learningresults which are from fully supervised classifica-tion in the target domains with trigrams and 1500features.
The results of 3TR-1TE, EM, ANB, FS-EM1, and FS-EM2 are obtained based on theirsettings which give the best results in Figure 4.Figure 4 ?
Comparison EM, ANB, FS-EM1, FS-EM2,and Co-Class across iterations (0 is 3TR-1TE)1048It is clear from Table 4 that Co-Class is the bestmethod in general.
It is even better than the fullysupervised One-Domain learning, although theirresults are not strictly comparable because One-Domain learning uses training and test data fromthe same domain via 10-fold cross validation,while all other methods use one domain as the testdata (the labeled data are from the other three do-mains).
One possible reason is that the labeleddata are much bigger than those in One-Domainlearning, which contain more expressions of buy-ing intention.
Note that FS-EM1 and FS-EM2work slightly better than Co-Class in domain?Camera?
because it is the least noisy domainwith very short posts while other domains (assource data) are quite noisy.
With good qualitydata, FS-EM1 and FS-EM2 (also proposed in thispaper) can do slightly better than Co-Class.
Statis-tical paired t-test shows that Co-Class performssignificantly better than baseline methods 3TR-1TE, EM, ANB and FS-EM1 at the confidencelevel of 95%, and better than FS-EM2 at the con-fidence level of 94%.Effect of the number of training domains: Inour experiments above, we used 3 source domaindata and tested on one target domain.
We nowshow what happens if we use only one or twosource domain data and test on one target domain.We tried all possible combinations of source andtarget data.
Figure 5 gives the average results overthe four target/test domains.
We can see that usingmore source domains is better due to more labeleddata.
With more domains, Co-Class also improvesmore over 3TR-1TE.5 ConclusionThis paper studied the problem of identifying in-tention posts in discussion forums.
The problemhas not been studied in the social media context.Due to special characteristics of the problem, wefound that it is particularly suited to transfer learn-ing.
A new transfer learning method, called Co-Class, was proposed to solve the problem.
Unlikea general transfer learning method, Co-Class candeal with two specific difficulties of the problemto produce more accurate classifiers.
Our experi-mental results show that Co-Class outperformsstrong baselines including classifiers trained usinglabeled data in the target domains and classifiersfrom a state-of-the-art transfer learning method.AcknowledgmentsThis work was supported in part by a grant fromNational Science Foundation (NSF) under grantno.
IIS-1111092, and a grant from HP Labs Inno-vation Research Program.ReferencesAue, A., & Gamon, M. (2005).
Customizing SentimentClassifiers to New Domains: A Case Study.
Pro-ceedings of Recent Advances in Natural LanguageProcessing (RANLP).Blitzer, J., Dredze, M., & Pereira, F. (2007).
Biog-raphies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment Classification.Proceedings of Annual Meeting of the Associationfor Computational Linguistics (ACL).ModelCellphone Electronics Camera TVFull 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5 Full 5,0 5,5One-Domain 61.50 55.78 60.15 71.38 71.07 71.61 77.66 75.71 78.74 80.24 75.66 79.923TR-1TE 56.74 56.94 56.74 72.27 70.76 72.43 77.62 74.65 77.62 75.64 71.65 74.73EM 60.28 59.59 60.45 70.47 69.90 71.33 79.38 77.01 80.31 74.96 70.76 74.31ANB 62.53 59.29 62.41 66.58 68.29 68.36 78.37 77.49 78.83 78.70 75.73 78.26FS-EM1 59.01 57.69 59.41 70.75 71.74 72.00 80.58 76.13 80.37 79.29 73.75 77.34FS-EM2 59.54 60.09 61.33 71.19 72.09 72.07 80.14 77.93 81.09 78.90 74.21 77.53Co-Class 62.69 61.10 62.69 73.38 73.23 73.95 79.69 74.65 78.66 81.12 76.40 81.60Table 4: F1-score results of One-Domain, 3TR-1TE, EM, ANB, FS-EM1, FS-EM2, and Co-ClassFigure 5 ?
Effect of number of source domainsusing 3TR-1TE and Co-Class.1049Blum, A., & Mitchell, T. (1998).
Combining Labeledand Unlabeled Data with Co-Training.
COLT: Pro-ceedings of the eleventh annual conference on Com-putational learning theory.Bollegala, D., Weir, D. J., & Carroll, J.
(2011).
UsingMultiple Sources to Construct a Sentiment SensitiveThesaurus for Cross-Domain Sentiment Classifica-tion.
Proceedings of Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Dai, H. K., Zhao, L., Nie, Z., Wen, J. R., Wang, L., &Li, Y.
(2006).
Detecting online commercial inten-tion (OCI).
Proceedings of the 15th internationalconference on World Wide Web (WWW).Dai, W., Xue, G., Yang, Q., & Yu, Y.
(2007).
Transfer-ring naive bayes classifiers for text classification.
InProceedings of the 22nd AAAI Conference on Artifi-cial Intelligence (AAAI).Dempster, A., Laird, N., & Rubin, D. (1977).
Maxi-mum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.Series B, 39(1), 1?38.Easley, D., & Kleinberg, J.
(2010).
Networks, Crowds,and Markets: Reasoning About a Highly ConnectedWorld.
Cambridge University Press.Gao, S., & Li, H. (2011).
A cross-domain adaptationmethod for sentiment classification using probabilis-tic latent analysis.
Proceedings of the 20th ACM in-ternational conference on Information andknowledge management (CIKM).He, Y., Lin, C., & Alani, H. (2011).
AutomaticallyExtracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification.
Proceedings ofthe 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL).Hu, D. H., Shen, D., Sun, J.-T., Yang, Q., & Chen, Z.(2009).
Context-Aware Online Commercial Inten-tion Detection.
Proceedings of the 1st Asian Confer-ence on Machine Learning: Advances in MachineLearning (ACML).Hu, J., Wang, G., Lochovsky, F., tao Sun, J., & Chen,Z.
(2009).
Understanding user?s query intent withwikipedia.
Proceedings of the 18th internationalconference on World wide web (WWW).Joachims, T. (1998).
Text Categorization with SupportVector Machines: Learning with Many RelevantFeatures.
European Conference on Machine Learn-ing (ECML).Joachims, T. (1999).
Making large-Scale SVM Learn-ing Practical.
Advances in Kernel Methods - SupportVector Learning.
MIT Press.Kanayama, H., & Nasukawa, T. (2008).
Textual De-mand Analysis: Detection of Users?
Wants andNeeds from Opinions.
Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (COLING).Li, X.
(2010).
Understanding the Semantic Structure ofNoun Phrase Queries.
Proceedings of Annual Meet-ing of the Association for Computational Linguistics(ACL).Li, X., Wang, Y.-Y., & Acero, A.
(2008).
Learningquery intent from regularized click graphs.
Proceed-ings of the 31st annual international ACM SIGIRconference on Research and development in infor-mation retrieval (SIGIR).Liu, B.
(2010).
Sentiment Analysis and Subjectivity.(N.
Indurkhya & F. J. Damerau, Eds.)
Handbook ofNatural Language Processing, 2nd ed.Nigam, K., McCallum, A. K., Thrun, S., & Mitchell, T.(2000).
Text Classification from Labeled and Unla-beled Documents using EM.
Mach.
Learn., 39(2-3),103?134.Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z.(2010).
Cross-domain sentiment classification viaspectral feature alignment.
Proceedings of the 19thinternational conference on World wide web(WWW).Pan, S. J., & Yang, Q.
(2010).
A Survey on TransferLearning.
IEEE Trans.
Knowl.
Data Eng., 22(10),1345?1359.Pang, B., & Lee, L. (2008).
Opinion mining and senti-ment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2), 1?135.Tan, S., Cheng, X., Wang, Y., & Xu, H. (2009).
Adapt-ing Naive Bayes to Domain Adaptation for Senti-ment Analysis.
Proceedings of the 31th EuropeanConference on IR Research on Advances in Infor-mation Retrieval (ECIR).Tan, S., Wu, G., Tang, H., & Cheng, X.
(2007).
A nov-el scheme for domain-transfer problem in the con-text of sentiment analysis.
Proceedings of thesixteenth ACM conference on Conference on infor-mation and knowledge management (CIKM).Wu, Y., & Liu, Y.
(2007).
Robust truncated-hinge-losssupport vector machines.
Journal of the AmericanStatistical Association, 102(479), 974?983.Yang, H., Si, L., & Callan, J.
(2006).
KnowledgeTransfer and Opinion Detection in the TREC 2006Blog Track.
Proceedings of TREC.Yang, Y., & Liu, X.
(1999).
A re-examination of textcategorization methods.
Proceedings of the 22ndannual international ACM SIGIR conference on Re-search and development in information retrieval(SIGIR).Yang, Y., & Pedersen, J. O.
(1997).
A ComparativeStudy on Feature Selection in Text Categorization.Proceedings of the Fourteenth International Con-ference on Machine Learning (ICML).1050
