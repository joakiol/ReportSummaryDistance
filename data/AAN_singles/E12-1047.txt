Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 460?470,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsEfficient Parsing with Linear Context-Free Rewriting SystemsAndreas van CranenburghHuygens ING & ILLC, University of AmsterdamRoyal Netherlands Academy of Arts and SciencesPostbus 90754, 2509 LT The Hague, the Netherlandsandreas.van.cranenburgh@huygens.knaw.nlAbstractPrevious work on treebank parsing withdiscontinuous constituents using LinearContext-Free Rewriting systems (LCFRS)has been limited to sentences of up to 30words, for reasons of computational com-plexity.
There have been some results onbinarizing an LCFRS in a manner that min-imizes parsing complexity, but the presentwork shows that parsing long sentences withsuch an optimally binarized grammar re-mains infeasible.
Instead, we introduce atechnique which removes this length restric-tion, while maintaining a respectable accu-racy.
The resulting parser has been appliedto a discontinuous treebank with favorableresults.1 IntroductionDiscontinuity in constituent structures (cf.
figure 1& 2) is important for a variety of reasons.
Forone, it allows a tight correspondence betweensyntax and semantics by letting constituent struc-ture express argument structure (Skut et al 1997).Other reasons are phenomena such as extraposi-tion and word-order freedom, which arguably re-quire discontinuous annotations to be treated sys-tematically in phrase-structures (McCawley, 1982;Levy, 2005).
Empirical investigations demon-strate that discontinuity is present in non-negligibleamounts: around 30% of sentences contain dis-continuity in two German treebanks (Maier andS?gaard, 2008; Maier and Lichte, 2009).
Re-cent work on treebank parsing with discontinuousconstituents (Kallmeyer and Maier, 2010; Maier,2010; Evang and Kallmeyer, 2011; van Cranen-burgh et al 2011) shows that it is feasible todirectly parse discontinuous constituency anno-tations, as given in the German Negra (Skut et alSBARQSQVPWHNP MD NP VB .What should I do ?Figure 1: A tree with WH-movement from the Penntreebank, in which traces have been converted to dis-continuity.
Taken from Evang and Kallmeyer (2011).1997) and Tiger (Brants et al 2002) corpora, orthose that can be extracted from traces such as inthe Penn treebank (Marcus et al 1993) annota-tion.
However, the computational complexity issuch that until now, the length of sentences neededto be restricted.
In the case of Kallmeyer andMaier (2010) and Evang and Kallmeyer (2011) thelimit was 25 words.
Maier (2010) and van Cranen-burgh et al(2011) manage to parse up to 30 wordswith heuristics and optimizations, but no further.Algorithms have been suggested to binarize thegrammars in such a way as to minimize parsingcomplexity, but the current paper shows that thesetechniques are not sufficient to parse longer sen-tences.
Instead, this work presents a novel formof coarse-to-fine parsing which does alleviate thislimitation.The rest of this paper is structured as follows.First, we introduce linear context-free rewritingsystems (LCFRS).
Next, we discuss and evalu-ate binarization strategies for LCFRS.
Third, wepresent a technique for approximating an LCFRSby a PCFG in a coarse-to-fine framework.
Lastly,we evaluate this technique on a large corpus with-out the usual length restrictions.460ROOTSVPPROAV VAFIN NN NN VVPP $.Danach habe Kohlenstaub Feuer gefangen .Afterwards had coal dust fire caught .Figure 2: A discontinuous tree from the Negra corpus.Translation: After that coal dust had caught fire.2 Linear Context-Free RewritingSystemsLinear Context-Free Rewriting Systems (LCFRS;Vijay-Shanker et al 1987; Weir, 1988) subsumea wide variety of mildly context-sensitive for-malisms, such as Tree-Adjoining Grammar (TAG),Combinatory Categorial Grammar (CCG), Min-imalist Grammar, Multiple Context-Free Gram-mar (MCFG) and synchronous CFG (Vijay-Shankerand Weir, 1994; Kallmeyer, 2010).
Furthermore,they can be used to parse dependency struc-tures (Kuhlmann and Satta, 2009).
Since LCFRSsubsumes various synchronous grammars, they arealso important for machine translation.
This makesit possible to use LCFRS as a syntactic backbonewith which various formalisms can be parsed bycompiling grammars into an LCFRS, similar to theTuLiPa system (Kallmeyer et al 2008).
As allmildly context-sensitive formalisms, LCFRS areparsable in polynomial time, where the degreedepends on the productions of the grammar.
In-tuitively, LCFRS can be seen as a generalizationof context-free grammars to rewriting other ob-jects than just continuous strings: productions arecontext-free, but instead of strings they can rewritetuples, trees or graphs.We focus on the use of LCFRS for parsing withdiscontinuous constituents.
This follows up onrecent work on parsing the discontinuous anno-tations in German corpora with LCFRS (Maier,2010; van Cranenburgh et al 2011) and work onparsing the Wall Street journal corpus in whichtraces have been converted to discontinuous con-stituents (Evang and Kallmeyer, 2011).
In the caseof parsing with discontinuous constituents a non-ROOT(ab)?
S(a) $.(b)S(abcd)?
VAFIN(b) NN(c) VP2(a, d)VP2(a, bc)?
PROAV(a) NN(b) VVPP(c)PROAV(Danach)?
VAFIN(habe)?
NN(Kohlenstaub)?
NN(Feuer)?
VVPP(gefangen)?
$.(.)?
Figure 3: The productions that can be read off from thetree in figure 2.
Note that lexical productions rewrite to, because they do not rewrite to any non-terminals.terminal may cover a tuple of discontinuous stringsinstead of a single, contiguous sequence of termi-nals.
The number of components in such a tupleis called the fan-out of a rule, which is equal tothe number of gaps plus one; the fan-out of thegrammar is the maximum fan-out of its production.A context-free grammar is a LCFRS with a fan-outof 1.
For convenience we will will use the rulenotation of simple RCG (Boullier, 1998), whichis a syntactic variant of LCFRS, with an arguablymore transparent notation.A LCFRS is a tuple G = ?N,T, V, P, S?.
Nis a finite set of non-terminals; a function dim :N ?
N specifies the unique fan-out for every non-terminal symbol.
T and V are disjoint finite setsof terminals and variables.
S is the distinguishedstart symbol with dim(S) = 1.
P is a finite set ofrewrite rules (productions) of the form:A(?1, .
.
.
?dim(A))?B1(X11 , .
.
.
, X1dim(B1)).
.
.
Bm(Xm1 , .
.
.
, Xmdim(Bm))for m ?
0, where A, B1, .
.
.
, Bm ?
N ,each Xij ?
V for 1 ?
i ?
m, 1 ?
j ?
dim(Aj)and ?i ?
(T ?
V )?
for 1 ?
i ?
dim(Ai).Productions must be linear: if a variable occursin a rule, it occurs exactly once on the left handside (LHS), and exactly once on the right hand side(RHS).
A rule is ordered if for any two variablesX1 andX2 occurring in a non-terminal on the RHS,X1 precedes X2 on the LHS iff X1 precedes X2on the RHS.Every production has a fan-out determined bythe fan-out of the non-terminal symbol on the left-hand side.
Apart from the fan-out productions also461have a rank: the number of non-terminals on theright-hand side.
These two variables determinethe time complexity of parsing with a grammar.
Aproduction can be instantiated when its variablescan be bound to non-overlapping spans such thatfor each component ?i of the LHS, the concatena-tion of its terminals and bound variables forms acontiguous span in the input, while the endpointsof each span are non-contiguous.As in the case of a PCFG, we can read off LCFRSproductions from a treebank (Maier and S?gaard,2008), and the relative frequencies of productionsform a maximum likelihood estimate, for a prob-abilistic LCFRS (PLCFRS), i.e., a (discontinuous)treebank grammar.
As an example, figure 3 showsthe productions extracted from the tree in figure 2.3 BinarizationA probabilistic LCFRS can be parsed using a CKY-like tabular parsing algorithm (cf.
Kallmeyer andMaier, 2010; van Cranenburgh et al 2011), butthis requires a binarized grammar.1 Any LCFRScan be binarized.
Crescenzi et al(2011) state?while CFGs can always be reduced to rank two(Chomsky Normal Form), this is not the case forLCFRS with any fan-out greater than one.?
How-ever, this assertion is made under the assumption ofa fixed fan-out.
If this assumption is relaxed thenit is easy to binarize either deterministically or, aswill be investigated in this work, optimally witha dynamic programming approach.
Binarizing anLCFRS may increase its fan-out, which results inan increase in asymptotic complexity.
Considerthe following production:X(pqrs)?
A(p, r) B(q) C(s) (1)Henceforth, we assume that non-terminals on theright-hand side are ordered by the order of theirfirst variable on the left-hand side.
There are twoways to binarize this production.
The first is fromleft to right:X(ps)?XAB(p) C(s) (2)XAB(pqr)?A(p, r) B(q) (3)This binarization maintains the fan-out of 1.
Thesecond way is from right to left:X(pqrs)?A(p, r) XBC(q, s) (4)XBC(q, s)?B(q) C(s) (5)1Other algorithms exist which support n-ary productions,but these are less suitable for statistical treebank parsing.This binarization introduces a production witha fan-out of 2, which could have been avoided.After binarization, an LCFRS can be parsed inO(|G| ?
|w|p) time, where |G| is the size of thegrammar, |w| is the length of the sentence.
The de-gree p of the polynomial is the maximum parsingcomplexity of a rule, defined as:parsing complexity := ?+ ?1 + ?2 (6)where ?
is the fan-out of the left-hand side and?1 and ?2 are the fan-outs of the right-hand sideof the rule in question (Gildea, 2010).
As Gildea(2010) shows, there is no one to one correspon-dence between fan-out and parsing complexity: itis possible that parsing complexity can be reducedby increasing the fan-out of a production.
In otherwords, there can be a production which can be bi-narized with a parsing complexity that is minimalwhile its fan-out is sub-optimal.
Therefore we fo-cus on parsing complexity rather than fan-out inthis work, since parsing complexity determines theactual time complexity of parsing with a grammar.There has been some work investigating whetherthe increase in complexity can be minimized ef-fectively (Go?mez-Rodr?
?guez et al 2009; Gildea,2010; Crescenzi et al 2011).More radically, it has been suggested that thepower of LCFRS should be limited to well-nestedstructures, which gives an asymptotic improve-ment in parsing time (Go?mez-Rodr?
?guez et al2010).
However, there is linguistic evidence thatnot all language use can be described in well-nested structures (Chen-Main and Joshi, 2010).Therefore we will use the full power of LCFRS inthis work?parsing complexity is determined bythe treebank, not by a priori constraints.3.1 Further binarization strategiesApart from optimizing for parsing complexity, forlinguistic reasons it can also be useful to parsethe head of a constituent first, yielding so-calledhead-driven binarizations (Collins, 1999).
Addi-tionally, such a head-driven binarization can be?Markovized?
?i.e., the resulting production can beconstrained to apply to a limited amount of hor-izontal context as opposed to the full context inthe original constituent (e.g., Klein and Manning,2003), which can have a beneficial effect on accu-racy.
In the notation of Klein and Manning (2003)there are two Markovization parameters: h andv.
The first parameter describes the amount of462XBA X C Y D E0 1 2 3 4 5originalp = 4, ?
= 2XXB,C,D,EBXC,D,EXD,EA X C Y D E0 1 2 3 4 5right branchingp = 5, ?
= 2XXB,C,D,EXB,C,DXB,CBA X C Y D E0 1 2 3 4 5optimalp = 4, ?
= 2XXBB XEXDA X C Y D E0 1 2 3 4 5head-drivenp = 5, ?
= 2XXDXAXBBA X C Y D E0 1 2 3 4 5optimal head-drivenp = 4, ?
= 2Figure 4: The four binarization strategies.
C is the head node.
Underneath each tree is the maximum parsingcomplexity and fan-out among its productions.horizontal context for the artificial labels of a bi-narized production.
In a normal form binarization,this parameter equals infinity, because the bina-rized production should only apply in the exactsame context as the context in which it originallybelongs, as otherwise the set of strings acceptedby the grammar would be affected.
An artificiallabel will have the form XA,B,C for a binarizedproduction of a constituent X that has coveredchildren A, B, and C of X .
The other extreme,h = 1, enables generalizations by stringing partsof binarized constituents together, as long as theyshare one non-terminal.
In the previous example,the label would become just XA, i.e., the pres-ence of B and C would no longer be required,which enables switching to any binarized produc-tion that has covered A as the last node.
Limit-ing the amount of horizontal context on which aproduction is conditioned is important when thetreebank contains many unique constituents whichcan only be parsed by stringing together differentbinarized productions; in other words, it is a wayof dealing with the data sparseness about n-aryproductions in the treebank.The second parameter describes parent annota-tion, which will not be investigated in this work;the default value is v = 1 which implies only in-cluding the immediate parent of the constituentthat is being binarized; including grandparents is away of weakening independence assumptions.Crescenzi et al(2011) also remark thatan optimal head-driven binarization allows forMarkovization.
However, it is questionablewhether such a binarization is worthy of the nameMarkovization, as the non-terminals are not intro-duced deterministically from left to right, but inan arbitrary fashion dictated by concerns of pars-ing complexity; as such there is not a Markovprocess based on a meaningful (e.g., temporal) or-dering and there is no probabilistic interpretationof Markovization in such a setting.To summarize, we have at least four binarizationstrategies (cf.
figure 4 for an illustration):1. right branching: A right-to-left binarization.No regard for optimality or statistical tweaks.2.
optimal: A binarization which minimizes pars-ing complexity, introduced in Gildea (2010).Binarizing with this strategy is exponential inthe resulting optimal fan-out (Gildea, 2010).3. head-driven: Head-outward binarization withhorizontal Markovization.
No regard for opti-mality.4.
optimal head-driven: Head-outward binariza-tion with horizontal Markovization.
Min-imizes parsing complexity.
Introduced inand proven to be NP-hard by Crescenzi et al(2011).3.2 Finding optimal binarizationsAn issue with the minimal binarizations is thatthe algorithm for finding them has a high compu-tational complexity, and has not been evaluatedempirically on treebank data.2 Empirical inves-tigation is interesting for two reasons.
First ofall, the high computational complexity may notbe relevant with constant factors of constituents,which can reasonably be expected to be relativelysmall.
Second, it is important to establish whetheran asymptotic improvement is actually obtainedthrough optimal binarizations, and whether thistranslates to an improvement in practice.Gildea (2010) presents a general algorithm tobinarize an LCFRS while minimizing a given scor-ing function.
We will use this algorithm with twodifferent scoring functions.2Gildea (2010) evaluates on a dependency bank, but doesnot report whether any improvement is obtained over a naivebinarization.4631101001000100001000003 4 5 6 7 8 9FrequencyParsing complexityright branchingoptimalFigure 5: The distribution of parsing complexityamong productions in binarized grammars read off fromNEGRA-25.
The y-axis has a logarithmic scale.The first directly optimizes parsing complexity.Given a (partially) binarized constituent c, the func-tion returns a tuple of scores, for which a linearorder is defined by comparing elements startingfrom the most significant (left-most) element.
Thetuples contain the parsing complexity p, and thefan-out ?
to break ties in parsing complexity; ifthere are still ties after considering the fan-out, thesum of the parsing complexities of the subtrees ofc is considered, which will give preference to a bi-narization where the worst case complexity occursonce instead of twice.
The formula is then:opt(c) = ?p, ?, s?The second function is the similar except thatonly head-driven strategies are accepted.
A head-driven strategy is a binarization in which the headis introduced first, after which the rest of the chil-dren are introduced one at a time.opt-hd(c) ={?p, ?, s?
if c is head-driven??,?,??
otherwiseGiven a (partial) binarization c, the score shouldreflect the maximum complexity and fan-out inthat binarization, to optimize for the worst case, aswell as the sum, to optimize the average case.
Thisaspect appears to be glossed over by Gildea (2010).Considering only the score of the last production ina binarization produces suboptimal binarizations.3.3 ExperimentsAs data we use version 2 of the Negra (Skut et al1997) treebank, with the common training, devel-1101001000100001000003 4 5 6 7 8 9FrequencyParsing complexityhead-drivenoptimal head-drivenFigure 6: The distribution of parsing complexity amongproductions in Markovized, head-driven grammars readoff from NEGRA-25.
The y-axis has a logarithmic scale.opment and test splits (Dubey and Keller, 2003).Following common practice, punctuation, whichis left out of the phrase-structure in Negra, is re-attached to the nearest constituent.In the course of experiments it was discoveredthat the heuristic method for punctuation attach-ment used in previous work (e.g., Maier, 2010;van Cranenburgh et al 2011), as implemented inrparse,3 introduces additional discontinuity.
Weapplied a slightly different heuristic: punctuationis attached to the highest constituent that contains aneighbor to its right.
The result is that punctuationcan be introduced into the phrase-structure with-out any additional discontinuity, and thus withoutartificially inflating the fan-out and complexity ofgrammars read off from the treebank.
This newheuristic provides a significant improvement: in-stead of a fan-out of 9 and a parsing complexity of19, we obtain values of 4 and 9 respectively.The parser is presented with the gold part-of-speech tags from the corpus.
For reasons of effi-ciency we restrict sentences to 25 words (includ-ing punctuation) in this experiment: NEGRA-25.A grammar was read off from the training partof NEGRA-25, and sentences of up to 25 wordsin the development set were parsed using the re-sulting PLCFRS, using the different binarizationschemes.
First with a right-branching, right-to-leftbinarization, and second with the minimal bina-rization according to parsing complexity and fan-3Available from http://www.wolfgang-maier.net/rparse/downloads.
Retrieved March 25th, 2011464right optimalbranching optimal head-driven head-drivenMarkovization v=1, h=?
v=1, h=?
v=1, h=2 v=1, h=2fan-out 4 4 4 4complexity 8 8 9 8labels 12861 12388 4576 3187clauses 62072 62097 53050 52966time to binarize 1.83 s 46.37 s 2.74 s 28.9 stime to parse 246.34 s 193.94 s 2860.26 s 716.58 scoverage 96.08 % 96.08 % 98.99 % 98.73 %F1 score 66.83 % 66.75 % 72.37 % 71.79 %Table 1: The effect of binarization strategies on parsing efficiency, with sentences from the development section ofNEGRA-25.out.
The last two binarizations are head-drivenand Markovized?the first straightforwardly fromleft-to-right, the latter optimized for minimal pars-ing complexity.
With Markovization we are forcedto add a level of parent annotation to tame theincrease in productivity caused by h = 1.The distribution of parsing complexity (mea-sured with eq.
6) in the grammars with differentbinarization strategies is shown in figure 5 and6.
Although the optimal binarizations do seemto have some effect on the distribution of parsingcomplexities, it remains to be seen whether thiscan be cashed out as a performance improvementin practice.
To this end, we also parse using thebinarized grammars.In this work we binarize and parse withdisco-dop introduced in van Cranenburgh et al(2011).4 In this experiment we report scores of the(exact) Viterbi derivations of a treebank PLCFRS;cf.
table 1 for the results.
Times represent CPUtime (single core); accuracy is given with a gener-alization of PARSEVAL to discontinuous structures,described in Maier (2010).Instead of using Maier?s implementation of dis-continuous F1 scores in rparse, we employ a vari-ant that ignores (a) punctuation, and (b) the rootnode of each tree.
This makes our evaluation in-comparable to previous results on discontinuousparsing, but brings it in line with common practiceon the Wall street journal benchmark.
Note thatthis change yields scores about 2 or 3 percentagepoints lower than those of rparse.Despite the fact that obtaining optimal bina-4All code is available from: http://github.com/andreasvc/disco-dop.rizations is exponential (Gildea, 2010) and NP-hard (Crescenzi et al 2011), they can be computedrelatively quickly on this data set.5 Importantly, inthe first case there is no improvement on fan-outor parsing complexity, while in the head-drivencase there is a minimal improvement because of asingle production with parsing complexity 15 with-out optimal binarization.
On the other hand, theoptimal binarizations might still have a significanteffect on the average case complexity, rather thanthe worst-case complexities.
Indeed, in both casesparsing with the optimal grammar is faster; in thefirst case, however, when the time for binariza-tion is considered as well, this advantage mostlydisappears.The difference in F1 scores might relate to theefficacy of Markovization in the binarizations.
Itshould be noted that it makes little theoreticalsense to ?Markovize?
a binarization when it is nota left-to-right or right-to-left binarization, becausewith an optimal binarization the non-terminals ofa constituent are introduced in an arbitrary order.More importantly, in our experiments, thesetechniques of optimal binarizations did not scaleto longer sentences.
While it is possible to obtainan optimal binarization of the unrestricted Negracorpus, parsing long sentences with the resultinggrammar remains infeasible.
Therefore we need tolook at other techniques for parsing longer sen-tences.
We will stick with the straightforward5The implementation exploits two important optimiza-tions.
The first is the use of bit vectors to keep track of whichnon-terminals are covered by a partial binarization.
The sec-ond is to skip constituents without discontinuity, which areequivalent to CFG productions.465head-driven, head-outward binarization strategy,despite this being a computationally sub-optimalbinarization.One technique for efficient parsing of LCFRS isthe use of context-summary estimates (Kallmeyerand Maier, 2010), as part of a best-first parsingalgorithm.
This allowed Maier (2010) to parsesentences of up to 30 words.
However, the calcu-lation of these estimates is not feasible for longersentences and large grammars (van Cranenburghet al 2011).Another strategy is to perform an online approx-imation of the sentence to be parsed, after whichparsing with the LCFRS can be pruned effectively.This is the strategy that will be explored in thecurrent work.4 Context-free grammar approximationfor coarse-to-fine parsingCoarse-to-fine parsing (Charniak et al 2006) isa technique to speed up parsing by exploiting theinformation that can be gained from parsing withsimpler, coarser grammars?e.g., a grammar witha smaller set of labels on which the original gram-mar can be projected.
Constituents that do notcontribute to a full parse tree with a coarse gram-mar can be ruled out for finer grammars as well,which greatly reduces the number of edges thatneed to be explored.
However, by changing justthe labels only the grammar constant is affected.With discontinuous treebank parsing the asymp-totic complexity of the grammar also plays a majorrole.
Therefore we suggest to parse not just witha coarser grammar, but with a coarser grammarformalism, following a suggestion in van Cranen-burgh et al(2011).This idea is inspired by the work of Barthe?lemyet al(2001), who apply it in a non-probabilisticsetting where the coarse grammar acts as a guide tothe non-deterministic choices of the fine grammar.Within the coarse-to-fine approach the techniquebecomes a matter of pruning with some probabilis-tic threshold.
Instead of using the coarse gram-mar only as a guide to solve non-deterministicchoices, we apply it as a pruning step which alsodiscards the most suboptimal parses.
The basicidea is to extract a grammar that defines a supersetof the language we want to parse, but with a fan-out of 1.
More concretely, a context-free grammarcan be read off from discontinuous trees that havebeen transformed to context-free trees by the pro-cedure introduced in Boyd (2007).
Each discontin-uous node is split into a set of new nodes, one foreach component; for example a node NP2 will besplit into two nodes labeled NP*1 and NP*2 (likeBarthe?lemy et al we mark components with anindex to reduce overgeneration).
Because Boyd?stransformation is reversible, chart items from thisgrammar can be converted back to discontinuouschart items, and can guide parsing of an LCFRS.This guiding takes the form of a white list.
Af-ter parsing with the coarse grammar, the result-ing chart is pruned by removing all items thatfail to meet a certain criterion.
In our case thisis whether a chart item is part of one of the k-bestderivations?we use k = 50 in all experiments (asin van Cranenburgh et al 2011).
This has simi-lar effects as removing items below a thresholdof marginalized posterior probability; however,the latter strategy requires computation of outsideprobabilities from a parse forest, which is moreinvolved with an LCFRS than with a PCFG.
Whenparsing with the fine grammar, whenever a newitem is derived, the white list is consulted to seewhether this item is allowed to be used in furtherderivations; otherwise it is immediately discarded.This coarse-to-fine approach will be referred to asCFG-CTF, and the transformed, coarse grammarwill be referred to as a split-PCFG.Splitting discontinuous nodes for the coarsegrammar introduces new nodes, so obviously weneed to binarize after this transformation.
On theother hand, the coarse-to-fine approach requires amapping between the grammars, so after reversingthe transformation of splitting nodes, the resultingdiscontinuous trees must be binarized (and option-ally Markovized) in the same manner as those onwhich the fine grammar is based.To resolve this tension we elect to binarize twice.The first time is before splitting discontinuousnodes, and this is where we introduce Markoviza-tion.
This same binarization will be used for thefine grammar as well, which ensures the modelsmake the same kind of generalizations.
The sec-ond binarization is after splitting nodes, this timewith a binary normal form (2NF; all productionsare either unary, binary, or lexical).Parsing with this grammar proceeds as fol-lows.
After obtaining an exhaustive chart fromthe coarse stage, the chart is pruned so as to onlycontain items occurring in the k-best derivations.When parsing in the fine stage, each new item is466SBA X C Y D E0 1 2 3 4 5SSASBBSCSDSEA X C Y D E0 1 2 3 4 5SSASBB*0 SC*0 B*1 SC*1SDSEA X C Y D E0 1 2 3 4 5SSASBB*0 SB : SC*0,B*1,SC*1SC*0 SB : B*1,SC*1B*1 SC*1SDSEA X C Y D E0 1 2 3 4 5Figure 7: Transformations for a context-free coarse grammar.
From left to right: the original constituent,Markovized with v = 1, h = 1, discontinuities resolved, normal form (second binarization).model train dev test rules labels fan-out complexitySplit-PCFG 17988 975 968 57969 2026 1 3PLCFRS 17988 975 968 55778 947 4 9Disco-DOP 17988 975 968 2657799 702246 4 9Table 2: Some statistics on the coarse and fine grammars read off from NEGRA-40.looked up in this pruned coarse chart, with multi-ple lookups if the item is discontinuous (one foreach component).To summarize, the transformation happens infour steps (cf.
figure 7 for an illustration):1.
Treebank tree: Original (discontinuous) tree2.
Binarization: Binarize discontinuous tree, op-tionally with Markovization3.
Resolve discontinuity: Split discontinuousnodes into components, marked with indices4.
2NF: A binary normal form is applied; all pro-ductions are either unary, binary, or lexical.5 EvaluationWe evaluate on Negra with the same setup as insection 3.3.
We report discontinuous F1 scores aswell as exact match scores.
For previous results ondiscontinuous parsing with Negra, see table 3.
Forresults with the CFG-CTF method see table 4.We first establish the viability of the CFG-CTFmethod on NEGRA-25, with a head-driven v = 1,h = 2 binarization, and reporting again the scoresof the exact Viterbi derivations from a treebankPLCFRS versus a PCFG using our transformations.Figure 8 compares the parsing times of LCFRSwith and without the new CFG-CTF method.
Thegraph shows a steep incline for parsing with LCFRSdirectly, which makes it infeasible to parse longersentences, while the CFG-CTF method is faster for0510152025303540450 5 10 15 20 25cputime(s)Sentence lengthPLCFRSCFG-CTF (Split-PCFG?
PLCFRS)Figure 8: Efficiency of parsing PLCFRS with and with-out coarse-to-fine.
The latter includes time for bothcoarse & fine grammar.
Datapoints represent the aver-age time to parse sentences of that length; each lengthis made up of 20?40 sentences.sentences of length > 22 despite its overhead ofparsing twice.The second experiment demonstrates the CFG-CTF technique on longer sentences.
We restrict thelength of sentences in the training, developmentand test corpora to 40 words: NEGRA-40.
As a firststep we apply the CFG-CTF technique to parse witha PLCFRS as the fine grammar, pruning away allitems not occurring in the 10,000 best derivations467words PARSEVAL Exact(F1) matchDPSG: Plaehn (2004) ?
15 73.16 39.0PLCFRS: Maier (2010) ?
30 71.52 31.65Disco-DOP: van Cranenburgh et al(2011) ?
30 73.98 34.80Table 3: Previous work on discontinuous parsing of Negra.words PARSEVAL Exact(F1) matchPLCFRS, dev set ?
25 72.37 36.58Split-PCFG, dev set ?
25 70.74 33.80Split-PCFG, dev set ?
40 66.81 27.59CFG-CTF, PLCFRS, dev set ?
40 67.26 27.90CFG-CTF, Disco-DOP, dev set ?
40 74.27 34.26CFG-CTF, Disco-DOP, test set ?
40 72.33 33.16CFG-CTF, Disco-DOP, dev set ?
73.32 33.40CFG-CTF, Disco-DOP, test set ?
71.08 32.10Table 4: Results on NEGRA-25 and NEGRA-40 with the CFG-CTF method.
NB: As explained in section 3.3, theseF1 scores are incomparable to the results in table 3; for comparison, the F1 score for Disco-DOP on the dev set?
40 is 77.13 % using that evaluation scheme.from the PCFG chart.
The result shows that thePLCFRS gives a slight improvement over the split--pcfg, which accords with the observation that thelatter makes stronger independence assumptionsin the case of discontinuity.In the next experiments we turn to an all-fragments grammar encoded in a PLCFRS usingGoodman?s (2003) reduction, to realize a (dis-continuous) Data-Oriented Parsing (DOP; Scha,1990) model?which goes by the name of Disco-DOP (van Cranenburgh et al 2011).
This providesan effective yet conceptually simple method toweaken the independence assumptions of treebankgrammars.
Table 2 gives statistics on the gram-mars, including the parsing complexities.
The finegrammar has a parsing complexity of 9, whichmeans that parsing with this grammar has com-plexity O(|w|9).
We use the same parameters asvan Cranenburgh et al(2011), except that unlikevan Cranenburgh et al we can use v = 1, h = 1Markovization, in order to obtain a higher cover-age.
The DOP grammar is added as a third stage inthe coarse-to-fine pipeline.
This gave slightly bet-ter results than substituting the the DOP grammarfor the PLCFRS stage.
Parsing with NEGRA-40took about 11 hours and 4 GB of memory.
Thesame model from NEGRA-40 can also be used toparse the full development set, without length re-strictions, establishing that the CFG-CTF methodeffectively eliminates any limitation of length forparsing with LCFRS.6 ConclusionOur results show that optimal binarizations areclearly not the answer to parsing LCFRS efficiently,as they do not significantly reduce parsing com-plexity in our experiments.
While they providesome efficiency gains, they do not help with themain problem of longer sentences.We have presented a new technique for large-scale parsing with LCFRS, which makes it possibleto parse sentences of any length, with favorableaccuracies.
The availability of this technique maylead to a wider acceptance of LCFRS as a syntacticbackbone in computational linguistics.AcknowledgmentsI am grateful to Willem Zuidema, Remko Scha,Rens Bod, and three anonymous reviewers forcomments.468ReferencesFranc?ois Barthe?lemy, Pierre Boullier, Philippe De-schamp, and E?ric de la Clergerie.
2001.
Guidedparsing of range concatenation languages.
InProc.
of ACL, pages 42?49.Pierre Boullier.
1998.
Proposal for a natural lan-guage processing syntactic backbone.
Techni-cal Report RR-3342, INRIA-Rocquencourt, LeChesnay, France.
URL http://www.inria.fr/RRRT/RR-3342.html.Adriane Boyd.
2007.
Discontinuity revisited: Animproved conversion to context-free representa-tions.
In Proceedings of the Linguistic Annota-tion Workshop, pages 41?44.Sabine Brants, Stefanie Dipper, Silvia Hansen,Wolfgang Lezius, and George Smith.
2002.
TheTiger treebank.
In Proceedings of the workshopon treebanks and linguistic theories, pages 24?41.Eugene Charniak, Mark Johnson, M. Elsner,J.
Austerweil, D. Ellis, I. Haxton, C. Hill,R.
Shrivaths, J. Moore, M. Pozar, et al2006.Multilevel coarse-to-fine PCFG parsing.
In Pro-ceedings of NAACL-HLT, pages 168?175.Joan Chen-Main and Aravind K. Joshi.
2010.
Un-avoidable ill-nestedness in natural language andthe adequacy of tree local-mctag induced depen-dency structures.
In Proceedings of TAG+.
URLhttp://www.research.att.com/?srini/TAG+10/papers/chenmainjoshi.pdf.Michael Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. the-sis, University of Pennsylvania.Pierluigi Crescenzi, Daniel Gildea, AandreaMarino, Gianluca Rossi, and Giorgio Satta.2011.
Optimal head-driven parsing complex-ity for linear context-free rewriting systems.
InProc.
of ACL.Amit Dubey and Frank Keller.
2003.
Parsing ger-man with sister-head dependencies.
In Proc.
ofACL, pages 96?103.Kilian Evang and Laura Kallmeyer.
2011.PLCFRS parsing of English discontinuous con-stituents.
In Proceedings of IWPT, pages 104?116.Daniel Gildea.
2010.
Optimal parsing strategiesfor linear context-free rewriting systems.
InProceedings of NAACL HLT 2010., pages 769?776.Carlos Go?mez-Rodr?
?guez, Marco Kuhlmann, andGiorgio Satta.
2010.
Efficient parsing of well-nested linear context-free rewriting systems.
InProceedings of NAACL HLT 2010., pages 276?284.Carlos Go?mez-Rodr?
?guez, Marco Kuhlmann, Gior-gio Satta, and David Weir.
2009.
Optimal reduc-tion of rule length in linear context-free rewrit-ing systems.
In Proceedings of NAACL HLT2009, pages 539?547.Joshua Goodman.
2003.
Efficient parsing ofDOP with PCFG-reductions.
In Rens Bod,Remko Scha, and Khalil Sima?an, editors, Data-Oriented Parsing.
The University of ChicagoPress.Laura Kallmeyer.
2010.
Parsing Beyond Context-Free Grammars.
Cognitive Technologies.Springer Berlin Heidelberg.Laura Kallmeyer, Timm Lichte, Wolfgang Maier,Yannick Parmentier, Johannes Dellert, and Kil-ian Evang.
2008.
Tulipa: Towards a multi-formalism parsing environment for grammarengineering.
In Proceedings of the Workshopon Grammar Engineering Across Frameworks,pages 1?8.Laura Kallmeyer and Wolfgang Maier.
2010.
Data-driven parsing with probabilistic linear context-free rewriting systems.
In Proceedings of the23rd International Conference on Computa-tional Linguistics, pages 537?545.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
of ACL,volume 1, pages 423?430.Marco Kuhlmann and Giorgio Satta.
2009.
Tree-bank grammar techniques for non-projective de-pendency parsing.
In Proceedings of EACL,pages 478?486.Roger Levy.
2005.
Probabilistic models of wordorder and syntactic discontinuity.
Ph.D. thesis,Stanford University.Wolfgang Maier.
2010.
Direct parsing of discon-tinuous constituents in German.
In Proceedingsof the SPMRL workshop at NAACL HLT 2010,pages 58?66.Wolfgang Maier and Timm Lichte.
2009.
Charac-terizing discontinuity in constituent treebanks.469In Proceedings of Formal Grammar 2009, pages167?182.
Springer.Wolfgang Maier and Anders S?gaard.
2008.
Tree-banks and mild context-sensitivity.
In Proceed-ings of Formal Grammar 2008, page 61.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large an-notated corpus of english: The penn treebank.Computational linguistics, 19(2):313?330.James D. McCawley.
1982.
Parentheticals anddiscontinuous constituent structure.
LinguisticInquiry, 13(1):91?106.Oliver Plaehn.
2004.
Computing the most prob-able parse for a discontinuous phrase structuregrammar.
In Harry Bunt, John Carroll, and Gior-gio Satta, editors, New developments in parsingtechnology, pages 91?106.
Kluwer AcademicPublishers, Norwell, MA, USA.Remko Scha.
1990.
Language theory and languagetechnology; competence and performance.
InQ.A.M.
de Kort and G.L.J.
Leerdam, editors,Computertoepassingen in de Neerlandistiek,pages 7?22.
LVVN, Almere, the Netherlands.Original title: Taaltheorie en taaltechnologie;competence en performance.
Translation avail-able at http://iaaa.nl/rs/LeerdamE.html.Stuart M. Shieber.
1985.
Evidence against thecontext-freeness of natural language.
Linguis-tics and Philosophy, 8:333?343.Wojciech Skut, Brigitte Krenn, Thorten Brants,and Hans Uszkoreit.
1997.
An annotationscheme for free word order languages.
In Pro-ceedings of ANLP, pages 88?95.Andreas van Cranenburgh, Remko Scha, andFederico Sangati.
2011.
Discontinuous data-oriented parsing: A mildly context-sensitive all-fragments grammar.
In Proceedings of SPMRL,pages 34?44.K.
Vijay-Shanker and David J. Weir.
1994.
Theequivalence of four extensions of context-freegrammars.
Theory of Computing Systems,27(6):511?546.K.
Vijay-Shanker, David J. Weir, and Aravind K.Joshi.
1987.
Characterizing structural descrip-tions produced by various grammatical for-malisms.
In Proc.
of ACL, pages 104?111.David J. Weir.
1988.
Characterizing mildlycontext-sensitive grammar formalisms.Ph.D.
thesis, University of Pennsylvania.URL http://repository.upenn.edu/dissertations/AAI8908403/.470
