Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 107?112,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsLexical Acquisition for Opinion Inference:A Sense-Level Lexicon of Benefactive and Malefactive EventsYoonjung Choi1, Lingjia Deng2, and Janyce Wiebe1,21Department of Computer Science2Intelligent Systems ProgramUniversity of Pittsburghyjchoi@cs.pitt.edu, lid29@pitt.edu, wiebe@cs.pitt.eduAbstractOpinion inference arises when opinionsare expressed toward states and eventswhich positive or negatively affect entities,i.e., benefactive and malefactive events.This paper addresses creating a lexicon ofsuch events, which would be helpful to in-fer opinions.
Verbs may be ambiguous,in that some meanings may be benefac-tive and others may be malefactive or nei-ther.
Thus, we use WordNet to create asense-level lexicon.
We begin with seedsenses culled from FrameNet and expandthe lexicon using WordNet relationships.The evaluations show that the accuracy ofthe approach is well above baseline accu-racy.1 IntroductionOpinions are commonly expressed in many kindsof written and spoken text such as blogs, reviews,new articles, and conversation.
Recently, therehave been a surge in reserach in opinion analy-sis (sentiment analysis) research (Liu, 2012; Pangand Lee, 2008).While most past researches have mainly ad-dressed explicit opinion expressions, there are afew researches for implicit opinions expressed viaimplicatures.
Deng and Wiebe (2014) showedhow sentiments toward one entity may be prop-agated to other entities via opinion implicaturerules.
Consider The bill would curb skyrocketinghealth care costs.
Note that curb costs is bad forthe object costs since the costs are reduced.
Wecan reason that the writer is positive toward theevent curb since the event is bad for the objecthealth care costs which the writer expresses an ex-plicit negative sentiment (skyrocketing).
We canreason from there that the writer is positive towardthe bill, since it is the agent of the positive event.These implicature rules involve events that pos-itively or negatively affect the object.
Such eventsare called malefactive and benefactive, or, for easeof writing, goodFor (gf ) and badFor (bf ) (here-after gfbf).
The list of gfbf events and their polari-ties (gf or bf) are necessary to develop a fully auto-matic opinion inference system.
On first thought,one might think that we only need lists of gfbfwords.
However, it turns out that gfbf terms maybe ambiguous ?
a single word may have both gfand bf meanings.Thus, in this work, we take a sense-level ap-proach to acquire gfbf lexicon knowledge, lead-ing us to employ lexical resources with fine-grained sense rather than word representations.For that, we adopt an automatic bootstrappingmethod which disambiguates gfbf polarity at thesense-level utilizing WordNet, a widely-used lex-ical resource.
Starting from the seed set manuallygenerated from FrameNet, a rich lexicon in whichwords are organized by semantic frames, we ex-plore how gfbf terms are organized in WordNet viasemantic relations and expand the seed set basedon those semantic relations.The expanded lexicon is evaluated in two ways.First, the lexicon is evaluated against a corpus thathas been annotated with gfbf information at theword level.
Second, samples from the expandedlexicon are manually annotated at the sense level,which gives some idea of the prevalence of gfbflexical ambiguity and provides a basis for sense-level evaluation.
Also, we conduct the agreementstudy.
The results show that the expanded lexi-con covers more than half of the gfbf instancesin the gfbf corpus, and the system?s accuracy, asmeasured against the sense-level gold standard, issubstantially higher than baseline.
In addition, inthe agreement study, the annotators achieve goodagreement, providing evidence that the annotationtask is feasible and that the concept of gfbf givesus a natural coarse-grained grouping of senses.1072 The GFBF CorpusA corpus of blogs and editorials about the Afford-able Care Act, a controversial topic, was manu-ally annotated with gfbf information by Deng etal.
(2013)1.
This corpus provides annotated gfbfevents and the agents and objects of the events.
Itconsists of 134 blog posts and editorials.
Becausethe Affordable Health Care Act is a controversialtopic, the data is full of opinions.
In this corpus,1,411 gfbf instances are annotated, each includinga gfbf event, its agent, and its object (615 gf in-stances and 796 bf instances).
196 different wordsappear in gf instances and 286 different words ap-pear in bf instances; 10 words appear in both.3 Sense-Level GFBF AmbiguityA word may have one or more meanings.
Forthat, we use WordNet2, which is a large lexicaldatabase of English (Miller et al., 1990).
In Word-Net, nouns, verbs, adjectives, and adverbs are or-ganized by semantic relations between meanings(senses).
We assume that a sense is exactly oneof gf, bf, or neither.
Since words often have morethan one sense, the polarity of a word may or maynot be consistent, as the following WordNet exam-ples show.?
A word with only gf senses: encourageS1: (v) promote, advance, boost, further, en-courage (contribute to the progress or growthof)S2: (v) encourage (inspire with confidence;give hope or courage to)S3: (v) encourage (spur on)?
A word with only bf senses: assaultS1: (v) assail, assault, set on, attack (attacksomeone physically or emotionally)S2: (v) rape, ravish, violate, assault, dis-honor, dishonour, outrage (force (someone)to have sex against their will)S3: (v) attack, round, assail, lash out, snipe,assault (attack in speech or writing)All senses of encourage are good for the object,and all senses of assault are bad for the object.The polarity is always same regardless of sense.In such cases, for our purposes, which particularsense is being used does not need to be determinedbecause any instance of the word will be good for1Available at http://mpqa.cs.pitt.edu/corpora/gfbf/2WordNet, http://wordnet.princeton.edu/(bad for); that is, word-level approaches can workwell.
However, word-level approaches are not ap-plicable for all the words.
Consider the following:?
A word with gf and neutral senses: inspireS3: (v) prompt, inspire, instigate (serve as theinciting cause of)S4: (v) cheer, root on, inspire, urge, barrack,urge on, exhort, pep up (spur on or encourageespecially by cheers and shouts)S6: (v) inhale, inspire, breathe in (draw in(air))?
A word with bf and neutral senses: neutral-izeS2: (v) neutralize, neutralise, nullify, negate(make ineffective by counterbalancing the ef-fect of)S6: (v) neutralize, neutralise (make chemi-cally neutral)The words inspire and neutralize both have 6senses (we list a subset due to space limitations).For inspire, while S3 and S4 are good for the ob-ject, S6 doesn?t have any polarity, i.e., it is a neu-tral (we don?t think of inhaling air as good for theair).
Also, while S2 of neutralize is bad for theobject, S6 is neutral (neutralizing a solution justchanges its pH).
Thus, if word-level approachesare applied using these words, some neutral in-stances may be incorrectly classified as gf or bfevents.?
A word with gf and bf senses: fightS2: (v) fight, oppose, fight back, fight down,defend (fight against or resist strongly)S4: (v) crusade, fight, press, campaign, push,agitate (exert oneself continuously, vigor-ously, or obtrusively to gain an end or engagein a crusade for a certain cause or person; bean advocate for)As mentioned in Section 2, 10 words are ap-peared in both gf and bf instances.
Since onlywords and not senses are annotated in the corpus,such conflicts arise.
These 10 words account for9.07% (128 instances) of all annotated instances.One example is fight.
In the corpus instance fightfor a piece of legislation, fight is good for the ob-ject, a piece of legislation.
This is S4.
However,in the corpus instance we need to fight this repeal,the meaning of fight here is S2, so fight is bad forthe object, this repeal.108Thesefore, approaches for determining the gfbfpolarity of an instance that are sense-level insteadof word-level promise to have higher precision.4 Lexicon AcquisitionIn this section, we develop a sense-level gfbf lex-icon by exploiting WordNet.
The method boot-straps from a seed lexicon and iteratively followsWordNet relations.
We consider only verbs.4.1 Seed LexiconTo preserve the corpus for evaluation, we createda seed set that is independent from the corpus.
Anannotator who didn?t have access to the corpusmanually selected gfbf words from FrameNet3inthe light of semantic frames.
The annotator found592 gf words and 523 bf words.
Decomposingeach word into its senses in WordNet, there are1,525 gf senses and 1,154 bf senses.
83 words ex-tracted from FrameNet overlap with gfbf instancesin the corpus.
For independence, those words werediscarded.
Among the senses of the remainingwords, we randomly choose 200 gf senses and 200bf senses.4.2 Expansion MethodIn WordNet, verb senses are arranged into hier-archies, that is, verb senses towards the bottomof the trees express increasingly specific manners.Thus, we can follow hypernym relations to moregeneral senses and troponym relations to more spe-cific verb senses.
Since the troponym relationrefers to a specific elaboration of a verb sense, wehypothesized that troponyms of a synset tends tohave its same polarity (i.e., gf or bf).
We only con-sider the direct troponyms in a single iteration.
Al-though the hypernym is a more general term, wehypothesized that direct hypernyms tend to havethe the same or neutral polarity, but not the oppo-site polarity.
Also, the verb groups are promising;even though the coverage is incomplete, we expectthe verb groups to be the most helpful.WordNet Similarity4, is a facility that provides avariety of semantic similarity and relatedness mea-sures based on information found in the Word-Net lexical database.
We choose Jiang&Conrath(1997) (jcn) method which has been found to beeffective for such tasks by NLP researchers.
Whentwo concetps aren?t related at all, it returns 0.
The3FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/4WN Similarity, http://wn-similarity.sourceforge.net/more they are related, the higher the value is re-tuned.
We regarded words with similarity valuesgreater than 1.0 to be similar words.Beginning with its seed set, each lexicon (gf andbf) is expanded iteratively.
On each iteration, foreach sense in the current lexicon, all of its directtroponyms, direct hypernyms, and members of thesame verb group are extracted and added to thelexicon for the next iteration.
Similarity, for eachsense, all words with above-threshold jcn valuesare added.
For new senses that are extracted forboth the gf and bf lexicons, we ignore such senses,since there is conflicting evidence (recall that weassume a sense has only one polarity, even if aword may have senses of different polarities).4.3 Corpus EvaluationIn this section, we use the gfbf annotations in thecorpus as a gold standard.
The annotations in thecorpus are at the word level.
To use the annota-tions as a sense-level gold standard, all the sensesof a word marked gf (bf) in the corpus are con-sidered to be gf (bf).
While this is not ideal, thisallows us to evaluate the lexicon against the onlycorpus evidence available.The 196 words that appear in gf instances inthe corpus have a total of 897 senses, and the 286words that appear in bf instances have a total of1,154 senses.
Among them, 125 senses are con-flicted: a sense of a word marked gf in the corpuscould be a member of the same synset as a senseof a word marked bf in the corpus.
For a more reli-able gold-standard set, we ignored these conflictedsenses.
Thus, the gold-standard set contains 772 gfsenses and 1,029 bf senses.Table 1 shows the results after five iterations oflexicon expansion.
In total, the gf lexicon contains4,157 senses and the bf lexicon contains 5,071senses.
The top half gives the results for the gflexicon and the bottom half gives the results forthe bf lexicon.
In the table, gfOverlap means theoverlap between the senses in the lexicon in thatrow and the gold-standard gf set, while bfOverlapis the overlap between the senses in the lexicon inthat row and the gold-standard bf set.
That is, ofthe 772 senses in the gf gold standard, 449 (58%)are in the gf expanded lexicon while 105 (14%)are in the bf expanded lexicon.Accuracy (Acc) for gf is calculated as #gfOver-lap / (#gfOverlap + #bfOverlap) and bf is calcu-lated as #bfOverlap / (#gfOverlap + #bfOverlap).109goodFor#senses #gfOverlap #bfOverlap AccTotal 4,157 449 176 0.72WN Sim 1,073 134 75 0.64Groups 242 69 24 0.74Troponym 4,084 226 184 0.55Hypernym 223 75 33 0.69badFor#senses #gfOverlap #bfOverlap AccTotal 5,071 105 562 0.84WN Sim 1,008 34 190 0.85Groups 255 11 86 0.89Troponym 4,258 66 375 0.85Hypernym 286 16 77 0.83Table 1: Results after lexicon expansionOverall, accuracy is higher for the bf than thegf lexicon.
The results in the table are brokendown by semantic relation.
Note that the individ-ual counts do not sum to the totals because sensesof different words may actually be the same sensein WordNet.
The results for the bf lexicon are con-sistently high over all semantic relations.
The re-sults for the gf lexicon are more mixed, but all re-lations are valuable.The WordNet Similarity is advantageous be-cause it detects similar senses automatically, somay provide coverage beyond the semantic rela-tions coded in WordNet.Overall, the verb group is the most informativerelation, as we suspected.Although the gf-lexicon accuracy for the tro-ponym relation is not high, it has the advantageis that it yields the most number of senses.
Itslower accuracy doesn?t support our original hy-pothesis.
We first thought that verbs lower down inthe hierarchy would tend to have the same polar-ity since they express specific manners character-izing an event.
However, this hypothesis is wrong.Even though most troponyms have the same polar-ity, there are many exceptions.
For example, pro-tect#v#1, which means the first sense of the verbprotect, has 18 direct troponyms such as coverfor#v#1, overprotect#v#2, and so on.
protect#v#1is a gf event because the meaning is ?shieldingfrom danger?
and most troponyms are also gfevents.
However, overprotect#v#2, which is oneof troponyms of protect#v#1, is a bf event.For the hypernym relation, the number of de-tected senses is not large because many were al-ready detected in previous iterations (in general,there are fewer nodes on each level as hypernymlinks are traversed).4.4 Sense Annotation EvaluationFor a more direct evaluation, two annotators, whoare co-authors, independently annotated a sampleof senses.
We randomly selected 60 words amongthe following classes: 10 pure gf words (i.e., allsenses of the words are classified by the expan-sion method, and all senses are put into the gf lex-icon), 10 pure bf words, 20 mixed words (i.e., allsenses of the words are classified by the expan-sion method, and some senses are put into the gflexicon while others are put into the bf lexicon),and 20 incomplete words (i.e., some senses of thewords are not classified by the expansion method).The total number of senses is 151; 64 sensesare classified as gf, 56 senses are classified as bf,and 31 senses are not classified.
We included moremixed than pure words to make the results of thestudy more informative.
Further, we wanted to in-cluded non-classified senses as decoys for the an-notators.
The annotators only saw the sense en-tries from WordNet.
They didn?t know whetherthe system classified a sense as gf or bf or whetherit didn?t classify it at all.Table 2 evaluates the lexicons against the man-ual annotations, and in comparison to the ma-jority class baseline.
The top half of the tableshows results when treating Anno1?s annotationsas the gold standard, and the bottom half showsthe results when treating Anno2?s as the gold stan-dard.
Among 151 senses, Anno1 annotated 56senses (37%) as gf, 51 senses (34%) as bf, and44 senses (29%) as neutral.
Anno2 annotated 66senses (44%) as gf, 55 senses (36%) as bf, and30 (20%) senses as neutral.
The incorrect casesare divided into two sets: incorrect opposite con-sists of senses that are classified as the oppositepolarity by the expansion method (e.g., the senseis classified into gf, but annotator annotates it asbf), and incorrect neutral consists of senses thatthe expansion method classifies as gf or bf, but theannotator marked it as neutral.
We report the accu-racy and the percentage of cases for each incorrectcase.
The accuracies substantially improve overbaseline for both annotators and for both classes.In Table 3, we break down the results into gfbfclasses.
The gf accuracy measures the percentageof correct gf senses out of all senses annotated asgf according to the annotations (same as bf accu-racy).
As we can see, accuracy is higher for thebf than the gf.
The conclusion is consistent withwhat we have discovered in Section 4.3.110By Anno1, 8 words are detected as mixedwords, that is, they contain both gf and bf senses.By Anno2, 9 words are mixed words (this set in-cludes the 8 mixed words of Anno1).
Amongthe randomly selected 60 words, the proportion ofmixed words range from 13.3% to 15%, accordingto the two annotators.
This shows that gfbf lexicalambiguity does exist.To measure agreement between the annotators,we calculate two measures: percent agreement and?
(Artstein and Poesio, 2008).
?
measures theamount of agreement over what is expected bychance, so it is a stricter measure.
Percent agree-ment is 0.84 and ?
is 0.75.accuracy % incorrect % incorrect base-opposite neutral lineAnno1 0.53 0.16 0.32 0.37Anno2 0.57 0.24 0.19 0.44Table 2: Results against sense-annotated datagf accuracy bf accuracy baselineAnno1 0.74 0.83 0.37Anno2 0.68 0.74 0.44Table 3: Accuracy broken down for gfbf5 Related WorkLexicons are widely used in sentiment analysisand opinion extraction.
There are several previ-ous works to acquire or expand sentiment lexi-cons such as (Kim and Hovy, 2004), (Strapparavaand Valitutti, 2004), (Esuli and Sebastiani, 2006),(Gyamfi et al., 2009), (Mohammad and Turney,2010) and (Peng and Park, 2011).
Such senti-ment lexicons are helpful for detecting explicitlystated opinions, but are not sufficient for recog-nizing implicit opinions.
Inferred opinions oftenhave opposite polarities from the explicit senti-ment expressions in the sentence; explicit senti-ments must be combined with benefactive, male-factive state and event information to detect im-plicit sentiments.
There are few previous worksclosest to ours.
(Feng et al., 2011) build con-notation lexicons that list words with connotativepolarity and connotative predicates.
Goyal et al.
(2010) generate a lexicon of patient polarity verbsthat imparts positive or negative states on their pa-tients.
Riloff et al.
(2013) learn a lexicon of nega-tive situation phrases from a corpus of tweets withhashtag ?sarcasm?.Our work is complementary to theirs in thattheir acquisition methods are corpus-based, whilewe acquire knowledge from lexical resources.Further, all of their lexicons are word level whileours are sense level.
Finally, the types of entriesamong the lexicons are related but not the same.Ours are specifically designed to support the au-tomatic recognition of implicit sentiments in textthat are expressed via implicature.6 Conclusion and Future WorkIn this paper, we developed a sense-level gfbflexicon which was seeded by entries culled fromFrameNet and then expanded by exploiting se-mantic relations in WordNet.
Our evaluationsshow that such lexical resources are promising forexpanding such sense-level lexicons.
Even thoughthe seed set is completely independent from thecorpus, the expanded lexicon?s coverage of thecorpus is not small.
The accuracy of the expandedlexicon is substantially higher than baseline accu-racy.
Also, the results of the agreement study arepositive, providing evidence that the annotationtask is feasible and that the concept of gfbf givesus a natural coarse-grained grouping of senses.However, there is still room for improvement.We believe that gf/bf judgements of word sensescould be effectively crowd-sourced; (Akkaya etal., 2010), for example, effectively used Ama-zon Mechanical Turk (AMT) for similar coarse-grained judgements.
The idea would be to use au-tomatic expansion methods to create a sense-levellexicon, and then have AMT workers judge theentries in which we have least confidence.
Thiswould be much more time- and cost-effective.The seed sets we used are small - only 400 totalsenses.
We believe it will be worth the effort tocreate larger seed sets, with the hope to mine manyadditional gfbf senses from WordNet.To exploit the lexicon to recognize sentiments ina corpus, the word-sense ambiguity we discoveredneeds to be addressed.
There is evidence that theperformance of word-sense disambiguation sys-tems using a similar coarse-grained sense inven-tory is much better than when the full sense inven-tory is used (Akkaya et al., 2009; Akkaya et al.,2011).
That, coupled with the fact that our studysuggests that many words are unambiguous withrespect to the gfbf distinction, makes us hopefulthat gfbf information may be practically exploitedto improve sentiment analysis in the future.1117 AcknowledgmentsThis work was supported in part by DARPA-BAA-12-47 DEFT grant #12475008.ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InProceedings of EMNLP 2009, pages 190?199.Cem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon mechanical turkfor subjectivity word sense disambiguation.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Cre-ating Speech and Language Data with Amazon?sMechanical Turk, pages 195?203.Cem Akkaya, Janyce Wiebe, Alexander Conrad, andRada Mihalcea.
2011.
Improving the impact ofsubjectivity word sense disambiguation on contex-tual opinion analysis.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning, pages 87?96.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.Linguist., 34(4):555?596, December.Lingjia Deng and Janyce Wiebe.
2014.
Sentimentpropagation via implicature constraints.
In Proceed-ings of EACL.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.2013.
Benefactive/malefactive event and writer atti-tude annotation.
In Proceedings of 51st ACL, pages120?125.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of 5th LREC, pages417?422.Song Feng, Ritwik Bose, and Yejin Choi.
2011.
Learn-ing general connotation of words using graph-basedalgorithms.
In Proceedings of EMNLP, pages 1092?1103.Amit Goyal, Ellen Riloff, and Hal DaumeIII.
2010.Automatically producing plot unit representationsfor narrative text.
In Proceedings of EMNLP, pages77?86.Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and CemAkkaya.
2009.
Integrating knowledge for subjectiv-ity sense labeling.
In Proceedings of NAACL HLT2009, pages 10?18.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
In Proceedings of COLING.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of 20thCOLING, pages 1367?1373.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Wordnet: An on-line lexical database.
InternationalJournal of Lexicography, 13(4):235?312.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: Us-ing mechanical turk to create an emotion lexicon.
InProceedings of the NAACL-HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135.Wei Peng and Dae Hoon Park.
2011.
Generate adjec-tive sentiment dictionary for social media sentimentanalysis using constrained nonnegative matrix fac-torization.
In Proceedings of ICWSM.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-dra De Silva, Nathan Gilbert, and Ruihong Huang.2013.
Sarcasm as contrast between a positive sen-timent and negative situation.
In Proceedings ofEMNLP, pages 704?714.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-affect: An affective extension of wordnet.In Proceedings of 4th LREC, pages 1083?1086.112
