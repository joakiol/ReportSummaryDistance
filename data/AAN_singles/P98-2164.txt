On the Evaluation and Comparison of Taggers: the Effect ofNoise in Testing Corpora.Llufs Padr6 and Llufs M~rquezDep.
LSI.
Technical University of Cataloniac / Jo rd i  Girona 1-3.
08034 Barcelona{padro, lluism}@l si.
upc.
esAbstractThis paper addresses the issue of POS taggerevaluation.
Such evaluation is usually per-formed by comparing the tagger output witha reference test corpus, which is assumed to beerror-free.
Currently used corpora contain noisewhich causes the obtained performance to be adistortion of the real value.
We analyze to whatextent this distortion may invalidate the com-parison between taggers or the measure of theimprovement given by a new system.
The mainconclusion is that a more rigorous testing exper-imentation setting/designing is needed to reli-ably evaluate and compare tagger accuracies.1 Int roduct ion  and Mot ivat ionPart of Speech (POS) Tagging is a quite welldefined NLP problem, which consists of assign-ing to each word in a text the proper mor-phosyntactic tag for the given context.
Al-though many words are ambiguous regardingtheir POS, in most cases they can be completelydisambiguated taking into account an adequatecontext.
Successful taggers have been built us-ing several approaches, uch as statistical tech-niques, symbolic machine learning techniques,neural networks, etc.
The accuracy reported bymost current aggers ranges from 96-97% to al-most 100% in the linguistically-motivated Con-straint Grammar environment.Unfortunately, there have been very few di-rect comparisons ofalternative taggers 1 on iden-tical test data.
However, in most current papersit is argued that the performance of some tag-gers is better than others as a result of somekind of indirect comparisons between them.
WeI One of the exceptions is the work by (Samuelssonand Voutilainen, 1997), in which a very strict comparisonbetween taggers is performed.think that there are a number of not enoughcontrolled/considered factors that make theseconclusions dubious in most cases.In this direction, the present paper aims topoint out some of the difficulties arising whenevaluating and comparing tagger performancesagainst a reference test corpus, and to makesome criticism about common practices followedby the NLP researchers in this issue.The above mentioned factors can affect ei-ther the evaluation or the comparison process.Factors affecting the evaluation process are: (1)Training and test experiments are usually per-formed over noisy corpora which distorts the ob-tained results, (2) performance figures are toooften calculated from only a single or very smallnumber of trials, though average results frommultiple trials are crucial to obtain reliable esti-mations of accuracy (Mooney, 1996), (3) testingexperiments are usually done on corpora withthe same characteristics a  the training data-usually a small fresh portion of the trainingcorpus- but no serious attempts have been donein order to determine the reliability of the re-sults when moving from one domain to another(Krovetz, 1997), and (4) no figures about com-putational effort -space/time complexity- areusually reported, even from an empirical per-spective.
A factors affecting the comparisonprocess is that comparisons between taggers areoften indirect, while they should be comparedunder the same conditions in a multiple-trialexperiment with statistical tests of significance.For these reasons, this paper calls for a dis-cussion on POS taggers evaluation, aiming toestablish a more rigorous test experimentationsetting/designing, indispensable to extract reli-able conclusions.
As a starting point, we willfocus only on how the noise in the test corpuscan affect the obtained results.9972 No ise  in the  tes t ing  corpusFrom a machine learning perspective, the rele-vant noise in the corpus is that of non system-atically mistagged words (i.e.
different annota-tions for words appearing in the same syntac-t ic/semantic ontexts).Commonly used annotated corpora havenoise.
See, for instance, the following examplesfrom the Wall Street Journal (wsJ)  corpus:Verb participle forms are sometimes tagged assuch (VBN) and also as adjectives (JJ) in othersentences with no structural differences:la) ... fa i l ing_VBG to_TO voluntar i ly_RBsubmit_VB the_DT requested_VBN?nformation_~N ...ib) ... a_DT la rge J J  sample_NN of_INmarried_2J women_b~IS with__ IN  a t_ INleast_3JS one_CD child_NN .
.
.Another structure not coherently tagged arenoun chains when the nouns (NN) are ambigu-ous and can be also adjectives (J J):2a) ... Mr._NNP Hahn_NNP , _ ,  the_DT62-year-oldJJ chairman_NN and_CCchief_NN executive_JJ officer_NN of_INGeorgia-Pac i f ic_NNP Corp._NNP .
.
.2b) ... Burger_NNP King_NNP's_POS chief_JJ executive_NN o\]ficer_NN ,_,Barry_NNP Gibbons_NNP ,_, stars_VBZin_.IN ads_NNS saying_VBG ...The noise in the test set produces a wrongestimation of accuracy, since correct answers arecomputed as wrong and vice-versa.
In followingsections we will show how this uncertainty in theevaluation may be, in some cases, larger thanthe reported improvements from one system toanother, so invalidating the conclusions of thecomparison.3 Mode l  Set t ingTo study the appropriateness of the choicesmade by a POS tagger, a reference tagging mustbe selected and assumed to be correct in or-der to compare it with the tagger output.
Thisis usually done by assuming that the disam-biguated test corpora being used contains theright POS disambiguation.
This approach isquite right when the tagger error rate is largerenough than the test corpus error rate, never-theless, the current POS taggers have reached aperformance level that invalidates this choice,since the tagger error rate is getting too closeto the error rate of the test corpus.Since we want to study the relationship be-tween the tagger error rate and the test corpuserror rate, we have to establish an absolute ref-erence point.
Although (Church, 1992) ques-tions the concept of correct analysis, (Samuels-son and Voutilainen, 1997) establish that thereexists a -statistically significant- absolute cor-rect disambiguation, respect o which the errorrates of either the tagger or the test corpus canbe computed.
What  we will focus on is howdistorted is the tagger error rate by the use ofa noisy test corpus as a reference.The cases we can find when evaluating theperformance of a certain tagger are presentedin table 1.
OK/--aOK stand for a r ight/wrongtag (respect o the absolute correct disambigua-tion).
When both the tagger and the test cor-pus have the correct tag, the tag is correctlyevaluated as right.
When the test corpus hasthe correct tag and the tagger gets it wrong,the occurrence is correctly evaluated as wrong.But problems arise when the test corpus hasa wrong tag: If the tagger gets it correctly, itis evaluated as wrong when it should be right(false negative).
If the tagger gets it wrong, itwill be rightly evaluated as wrong if the errorcommited by the tagger is other than the er-ror in the test corpus, but wrongly evaluatedas right (false positive) if the error is the same.Table 1 shows the computation of the percent-corpus tagger eval: right eval: wrongOK c OK t (1 -C) tOK c "aOK t - (1 -C) (1 - t )"aOK c OK t - Cu~OKc ~OKt  C(1-u)p C(1-u)(1-p)Table 1: Possible cases when evaluating a tagger.ages of each case.
The meanings of the usedvariables are:C: Test corpus error rate.
Usually an estima-tion is supplied with the corpus.t: Tagger performance rate on words rightlytagged in the test corpus.
It can be seen asP(OKtIOKc).u: Tagger performance rate on words wronglytagged in the test corpus.
It can be seen asP(OKtbOKc).998p: Probabil ity that the tagger makes the sameerror as the test corpus, given that both geta wrong tag.x: Real performance of the tagger, i.e.
whatwould be obtained on an error-free test set.K: Observed performance of the tagger, com-puted on the noisy test corpus.For simplicity, we will consider only perfor-mance on ambiguous words.
Considering unam-biguous words will make the analysis more com-plex, since it should be taken into account hatneither the behaviour of the tagger (given by u,t, p) nor the errors in the test corpus (given byc) are the same on ambiguous and unambiguouswords.
Nevertheless, this is an issue that mustbe further addressed.If we knew each one of the above proportions,we would be able to compute the real perfor-mance of our tagger (x) by adding up the OKtrows from table 1, i.e.
the cases in which thetagger got the right disambiguation indepen-dently from the tagging of the test set:x=(1-C) t+Cu (1)The equation of the observed performancecan also be extracted from table 1, adding upwhat is evaluated as right:K=(1-C) t+C(1-u)p  (2)The relationship between the real and the ob-served performance is derived from 1 and 2:x=K-C(1 -u)p+CuSince only K and C are known (or approxi-mately estimated) we can not compute the realperformance of the tagger.
All we can do is toestablish some reasonable bounds for t, u andp, and see in which range is x.Since all variables are probabilities, they arebounded in \[0, 1\].
We also can assume 2 thatK > C. We can use this constraints and theabove equations to bound the values of all vari-ables.
From 2, we obtain:u= 1 K - t (1 -C)  K - t (1 -C)  K -C(1 -u)p, p -  , t=  Cp C(l-u) 1-CThus, u will be maximum when p and t aremaximum (i.e.
1).
This gives an upper bound2In the cases we are interested in -that is, currentsystems- the tagger observed performance, I(, is over90%, while the corpus error rate, C, is below 10%.for u of (1 -K) /C .
When t=0,  u will rangein \[-oo, 1 -K /C \ ]  depending on the value of p.Since we are assuming K > C, the most informa-tive lower bound for u keeps being zero.
Simi-larly, p is minimum when t = 1 and u = 0.
Whent = 0 the value for p will range in \[K/C, +c~\]depending on u.
Since K > C, the most infor-mative upper bound for p is still 1.
Finally, twill be maximum when u - 1 and p = 0, andminimum when u=O and p=l .
Summarizing:0 <u<min{1,~ -~} (3){ K+C-ll ma= 0, ~ j <p_<l  (4)1 -CSince the values of the variables are mutuallyconstrained, it is not possible that, for instance,u and t have simultaneously their upper boundvalues (if (1 - I ( ) /C< 1 then K/ (1 -C)  > 1 andviceversa).
Any bound which is out of \[0, 1\] isnot informative and the appropriate boundary,0 or 1, is then used.
Note that the lower boundfor t will never be negative under the assump-tion K > C.Once we have established these bounds, wecan use equation 1 to compute the range for thereal performance value of our tagger: x will beminimum when u and t are mimmum, whichproduces the following bounds:~,,,, = Xr-Cp (6)K+C if K_< 1 -Cx,,,~= = 1--g+c-1 if K > 1 -C  (7)pAs an example, let's suppose we evaluate a tag-ger on a test corpus which is known to containabout 3% of errors (C=0.03) ,  and obtain a re-ported performance of 93% 3 (K= 0.93).
In thiscase, equations 6 and 7 yield a range for thereal performance x that varies from \[0.93, 0.96\]when p=O to \[0.90, 0.96\] when p= 1.This results suggest hat although we observea performance of K,  we can not be sure of howwell is our tagger performing without takinginto account the values of t, u and p.It is also obvious that the intervals in theabove example are too wide, since they con-sider all the possible parameter values, evenwhen they correspond to very unlikely param-~This is a realistic case obtained by (M?rquez andPadr6 , 1997) tagger.
Note that 93% is the accuracy onambiguous words (the equivalent overall accuracy wasabout 97%).999eter combinations 4.
In section 4 we will try tonarrow those intervals, limiting the possibilitiesto reasonable cases.4 Reasonab le  Bounds  fo r  the  BasicParametersIn real cases, not all parameter combinationswill be equally likely.
In addition, the boundsfor the values of t, u and p are closely relatedto the similarities between the training and testcorpora.
That is, if the training and test sets areextracted from the same corpus, they will prob-ably contain the same kind of errors in the samekind of situations.
This may cause the trainingprocedure to learn the errors -especially if theyare systematic-  and thus the resulting taggerwill tend to make the same errors that appearin the test set.
On the contrary, if the train-ing and test sets come from different sources-sharing only the tag set-  the behaviour of theresulting tagger will not depend on the right orwrong tagging of the test set.We can try to establish narrower bounds forthe parameters than those obtained in section 3.First of all, the value of t is already con-strained enough, due to its high contribution(1 -C)  to the value of K,  which forces t totake a value close to K. For instance, apply-ing the boundaries in equation 5 to the caseC- -0.03 and K--0.93,  we obtain that t belongsto \[0.928, 0.959\].The range for u can be slightly narrowed con-sidering the following: In the case of indepen-dent test and training corpora, u will tend tobe equal to t. Otherwise, the more biased to-wards the corpus errors is the language model,the lower u will be.
Note than u > t would meanthat the tagger disambiguates better the noisycases than the correct ones.
Concerning to thelower bound, only in the case that all the errorsin the training and test corpus were systematic(and thus can be learned) could u reach zero.However, not only this is not a likely Situation,but also requires a perfect-learning tagger.
Itseems more reasonable that, in normal cases, er-rors will be random, and the tagger will behave4For instance, it is not reasonable that  u=0,  whichwould mean that  the tagger never  disambiguates cor-rectly a wrong word in the corpus, or p- -  1, which wouldmean that  it a lways  makes the same error when bothare wrong.randomly on the noisy occurrences.
This yieldsa lower bound for u of 1/a, being a the averageambiguity ratio for ambiguous words.The reasonable bounds for u are thus_1 <_ u < min  t,aFinally, the value of p has similar constraintsto those of u.
If the test and training corporaare independent, the probability of making thesame error, given that both are wrong, will bethe random 1/ (a -1 ) .
If the corpora are notindependent, he errors that can be learned bythe tagger will cause p to rise up to (potentially)1.
Again, only in the case that all errors wheresystematic, could p reach 1.Then, the reasonable bounds for p are:{ 1 K+C-1}max < p < 1a - l '  C - -5 On 'Compar ing  TaggerPerformancesAs stated above, knowing which are the reason-able limits for the u, p and t parameters enablesus to compute the range in which the real per-formance of the tagger can vary.So, given two different aggers T1 and T2, andprovided we know the values for the test corpuserror rate and the observed performance of bothcases (C1, C~, K1, Ks), we can compare themby matching the reasonable intervals for the re-spective real performances xl and x2.From a conservative position, we cannotstrongly state than one of the taggers performsbetter than the other when the two intervalsoverlap, since this implies a chance that the realperformances of both taggers are the same.The following real example has been ex-tracted from (M?rquez and Padrd , 1997): Thetagger T1 uses only bigram information and hasan observed performance on ambiguous wordsK1 = 0.9135 (96.86% overall).
The tagger 2"2uses trigrams and automatically acquired con-text constraints and has an accuracy of K2 =0.9282 (97.39% overall).
Both taggers have beenevaluated on a corpus (wsJ)  with an estimatederror rate 5 C1 = C2 = 0.03.
The average ambigu-ity ratio of the ambiguous words in the corpusis a=2.5  tags/word.5The (wsJ)  corpus error rate is est imated over allwords.
We are assunfing that  the errors d ist r ibuteuniformly among all words, a l though ambiguous words1000These data yield the following range of rea-sonable intervals for the real performance of thetaggers.for pi=(1/a)=0.4 Ixx E [91.35, 94.05]x2 ?
[92:82, 95.60]for pi = lxl E [90.75, 93.99]x2 E [92.22, 95.55]The same information is included in figure 1which presents the reasonable accuracy intervalsfor both taggers, for p ranging from 1/a = 0.4 to1 (the shadowed part corresponds to the over-lapping region between intervals).1I " I I I I I% accqracy1/a=0.4 I .....90 91 92 93 94 95 (X)Figure 1: Reasonable intervals for both taggersThe obtained intervals have a large overlapregion which implies that there are reasonableparameter combinations that could cause thetaggers to produce different observed perfor-mances though their real accuracies were verysimilar.
From this conservative approach, wewould not be able to conclude that the tagger7"2 is better than T1, even though the 95% con-fidence intervals for the observed performancesdid allow us to do so.6 D iscuss ionThe presented analysis of the effects of noise inthe test corpus on the evaluation of POS taggersleads us to conclude that when a tagger is eval-uated as better than another using noisy testcorpus, there are reasonable chances that theyare in fact very similar but one of them is justadapting better than the other to the noise inthe corpus.probably have a higher error rate.
Nevertheless, a highervalue for C would cause the intervals to be wider and tooverlap even more.We believe that the widespread practice ofevaluating taggers against a noisy test corpushas reached its limit, since the performance ofcurrent aggers is getting too close to the errorrate usually found in test corpora.An obvious olution -and maybe not as costlyas one might think, since small test sets properlyused may yield enough statistical evidence- isusing only error-free test corpora.
Another pos-sibility is to further study the influence of noisein order to establish a criterion -e.g.
a thresh-old depending on the amount of overlapping be-tween intervals- to decide whether a given tag-ger can be considered better than another.There is still much to be done in this direc-tion.
This paper does not intend to establisha new evaluation method for POS tagging, butto point out that there are some issues -such asthe noise in test corpus- that have been paid lit-tle attention and are more important than whatthey seem to be.Some of the issues that should be further con-sidered are: The effect of noise on unambigu-ous words; the reasonable intervals for overallreal performance; the -probably- different val-ues of C, p, u and t for ambiguous/unambiguouswords; how to estimate the parameter values ofthe evaluated tagger in order to constrain asmuch as possible the intervals; the statisticalsignificance of the interval overlappings; a moreinformed (and less conservative) criterion to re-ject/accept the hypothesis that both taggers aredifferent, etc.Re ferencesChurch, K.W.
1992.
Current Practice in Part ofSpeech Tagging and Suggestions for the Future.In Simmons (ed.
), Sbornik praci: In Honor ofHenry Kudera.
Michigan Slavic Studies.Krovetz, R. 1997.
Homonymy and Polysemy inInformation Retrieval.
In Proceedings of jointE/A CL meeting.M~trquez, L. and Padr6, L. 1997.
A Flexible POSTagger Using an Automatically Acquired Lan-guage Model.
In Proceedings of joint E/ACLmeeting.Mooney, R.J. 1996.
Comparative Experiments onDisambiguating Word Senses: An Illustration ofthe Role of Bias in Machine Learning.
In Proceed-ings of EMNLP'96 conference.Samuelsson, C. and Voutilainen, A.
1997.
Compar-ing a Linguistic and a Stochastic Tagger.
In Pro-ceedings of joint E/A CL meeting.1001ResumAquest article versa sobre l'avaluaci6 de desam-biguadors morfosint~ctics.
Normalment, l'ava-luaci6 es fa comparant la sortida del desam-biguador arab un corpus de refer~ncia, que sesuposa lliure d'errors.
De tota manera, els cor-pus que s'usen habitualment contenen soroll quecausa que el rendiment que s'obt~ dels desam-biguadors igui una distorsi6 del valor real.
Enaquest article analitzem fins a quin punt aques-ta distorsi6 pot invalidar la comparaci6 entredesambiguadors o la mesura de la millora apor-tada per un nou sistema.
La conclusi6 princi-pal ~s que cal establir procediments alternatiusd'experimentaci6 mils rigorosos, per poder ava-luar i comparar fiablement les precisions delsdesambiguadors morfosint?ctics.LaburtenaArtikulu hau desanbiguatzaile morfosintak-tikoen ebaluazioaren inguruan datza.
Nor-malean, ebaluazioa, desanbiguatzailearen irte-era eta ustez errorerik gabeko erreferentziakocorpus bat konparatuz egiten da.
Hala ere, maizcorpusetan erroreak egoten dira eta horrek de-sanbiguatzailearen emaitzaren benetako balioaneragina izaten du.
Artikulu honetan, hainzuzen ere, horixe aztertuko dugu, alegia, zerneurritan distortsio horrek jar dezakeen auzitandesanbiguatzaileen arteko konparazioa edo sis-tema berri batek ekar dezakeen hobekuntza-maila.
Konklusiorik nagusiena hauxe da: de-sanbiguatzaile morfosintaktikoak ztertzeko etamodu ziurrago batez konparatu ahal izateko,azterketa-bideak sakonagoak eta zehatzagoakizan beharko liratekeela.1002
