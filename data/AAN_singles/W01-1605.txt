Building a Discourse-Tagged Corpus in the Framework ofRhetorical Structure TheoryLynn CarlsonDepartment of DefenseFt.
George G. MeadeMD 20755lmcarlnord@aol.comDaniel MarcuInformation Sciences InstituteUniversity of S. CaliforniaMarina del Rey, CA 90292marcu@isi.eduMary Ellen OkurowskiDepartment of DefenseFt.
George G. MeadeMD 20755meokuro@romulus.ncsc.milAbstractWe describe our experience indeveloping a discourse-annotatedcorpus for community-wide use.Working in the framework ofRhetorical Structure Theory, we wereable to create a large annotatedresource with very high consistency,using a well-defined methodology andprotocol.
This resource is madepublicly available through theLinguistic Data Consortium to enableresearchers to develop empiricallygrounded, discourse-specificapplications.1 IntroductionThe advent of large-scale collections ofannotated data has marked a paradigm shift inthe research community for natural languageprocessing.
These corpora, now also common inmany languages, have accelerated developmentefforts and energized the community.Annotation ranges from broad characterizationof document-level information, such as topic orrelevance judgments (Voorhees and Harman,1999; Wayne, 2000) to discrete analysis of awide range of linguistic phenomena.
However,rich theoretical approaches to discourse/textanalysis (Van Dijk and Kintsch, 1983; Meyer,1985; Grosz and Sidner, 1986; Mann andThompson, 1988) have yet to be applied on alarge scale.
So far, the annotation of discoursestructure of documents has been appliedprimarily to identifying topical segments(Hearst, 1997), inter-sentential relations(Nomoto and Matsumoto, 1999; Ts?ou et al,2000), and hierarchical analyses of smallcorpora (Moser and Moore, 1995; Marcu et al,1999).In this paper, we recount our experience indeveloping a large resource with discourse-levelannotation for NLP research.
Our main goal inundertaking this effort was to create a referencecorpus for community-wide use.
Two essentialconsiderations from the outset were that thecorpus needed to be consistently annotated, andthat it would be made publicly available throughthe Linguistic Data Consortium for a nominalfee to cover distribution costs.
The paperdescribes the challenges we faced in building acorpus of this level of complexity and scope ?including selection of theoretical approach,annotation methodology, training, and qualityassurance.
The resulting corpus contains 385documents of American English selected fromthe Penn Treebank (Marcus et al, 1993),annotated in the framework of RhetoricalStructure Theory.
We believe this resourceholds great promise as a rich new source of text-level information to support multiple lines ofresearch for language understandingapplications.2 FrameworkTwo principle goals underpin the creation of thisdiscourse-tagged corpus: 1) The corpus shouldbe grounded in a particular theoretical approach,and 2) it should be sufficiently large enough tooffer potential for wide-scale use ?
includinglinguistic analysis, training of statistical modelsof discourse, and other computational linguisticapplications.
These goals necessitated a numberof constraints to our approach.
The theoreticalframework had to be practical and repeatableover a large set of documents in a reasonableamount of time, with a significant level ofconsistency across annotators.
Thus, ourapproach contributes to the community quitedifferently from detailed analyses of specificdiscourse phenomena in depth, such asanaphoric relations (Garside et al, 1997) orstyle types (Leech et al, 1997); analysis of asingle text from multiple perspectives (Mannand Thompson, 1992); or illustrations of atheoretical model on a single representative text(Britton and Black, 1985; Van Dijk and Kintsch,1983).Our annotation work is grounded in theRhetorical Structure Theory (RST) framework(Mann and Thompson, 1988).
We decided touse RST for three reasons:?
It is a framework that yields rich annotationsthat uniformly capture intentional, semantic,and textual features that are specific to agiven text.?
Previous research on annotating texts withrhetorical structure trees (Marcu et al,1999) has shown that texts can be annotatedby multiple judges at relatively high levelsof agreement.
We aimed to produceannotation protocols that would yield evenhigher agreement figures.?
Previous research has shown that RST treescan play a crucial role in building naturallanguage generation systems (Hovy, 1993;Moore and Paris, 1993; Moore, 1995) andtext summarization systems (Marcu, 2000);can be used to increase the naturalness ofmachine translation outputs (Marcu et al2000); and can be used to build essay-scoring systems that provide students withdiscourse-based feedback (Burstein et al,2001).
We suspect that RST trees can beexploited successfully in the context ofother applications as well.In the RST framework, the discoursestructure of a text can be represented as a treedefined in terms of four aspects:?
The leaves of the tree correspond to textfragments that represent the minimal unitsof the discourse, called elementarydiscourse units?
The internal nodes of the tree correspond tocontiguous text spans?
Each node is characterized by its nuclearity?
a nucleus indicates a more essential unit ofinformation, while a satellite indicates asupporting or background unit ofinformation.?
Each node is characterized by a rhetoricalrelation that holds between two or morenon-overlapping, adjacent text spans.Relations can be of intentional, semantic, ortextual nature.Below, we describe the protocol that we usedto build consistent RST annotations.2.1 Segmenting Texts into UnitsThe first step in characterizing the discoursestructure of a text in our protocol is to determinethe elementary discourse units (EDUs), whichare the minimal building blocks of a discoursetree.
Mann and Thompson (1988, p. 244) statethat ?RST provides a general way to describethe relations among clauses in a text, whether ornot they are grammatically or lexicallysignalled.?
Yet, applying this intuitive notion tothe task of producing a large, consistentlyannotated corpus is extremely difficult, becausethe boundary between discourse and syntax canbe very blurry.
The examples below, whichrange from two distinct sentences to a singleclause, all convey essentially the same meaning,packaged in different ways:1.
[Xerox Corp.?s third-quarter net incomegrew 6.2% on 7.3% higher revenue.]
[Thisearned mixed reviews from Wall Streetanalysts.]2.
[Xerox Corp?s third-quarter net incomegrew 6.2% on 7.3% higher revenue,] [whichearned mixed reviews from Wall Streetanalysts.]3.
[Xerox Corp?s third-quarter net incomegrew 6.2% on 7.3% higher revenue,][earning mixed reviews from Wall Streetanalysts.]4.
[The 6.2% growth of Xerox Corp.?s third-quarter net income on 7.3% higher revenueearned mixed reviews from Wall Streetanalysts.
]In Example 1, there is a consequentialrelation between the first and second sentences.Ideally, we would like to capture that kind ofrhetorical information regardless of the syntacticform in which it is conveyed.
However, asexamples 2-4 illustrate, separating rhetoricalfrom syntactic analysis is not always easy.
It isinevitable that any decision on how to bracketelementary discourse units necessarily involvessome compromises.Reseachers in the field have proposed anumber of competing hypotheses about whatconstitutes an elementary discourse unit.
Whilesome take the elementary units to be clauses(Grimes, 1975; Givon, 1983; Longacre, 1983),others take them to be prosodic units(Hirschberg and Litman, 1993), turns of talk(Sacks, 1974), sentences (Polanyi, 1988),intentionally defined discourse segments (Groszand Sidner, 1986), or the ?contextually indexedrepresentation of information conveyed by asemiotic gesture, asserting a single state ofaffairs or partial state of affairs in a discourseworld,?
(Polanyi, 1996, p.5).
Regardless of theirtheoretical stance, all agree that the elementarydiscourse units are non-overlapping spans oftext.Our goal was to find a balance betweengranularity of tagging and ability to identifyunits consistently on a large scale.
In the end,we chose the clause as the elementary unit ofdiscourse, using lexical and syntactic clues tohelp determine boundaries:5.
[Although Mr. Freeman is retiring,] [he willcontinue to work as a consultant forAmerican Express on a project basis.]wsj_13176.
[Bond Corp., a brewing, property, mediaand resources company, is selling many ofits assets] [to reduce its debts.
]wsj_0630However, clauses that are subjects, objects,or complements of a main verb are not treated asEDUs:7.
[Making computers smaller often meanssacrificing memory.]wsj_23878.
[Insurers could see claims totaling nearly$1 billion from the San Franciscoearthquake.
]wsj_0675Relative clauses, nominal postmodifiers, orclauses that break up other legitimate EDUs, aretreated as embedded discourse units:9.
[The results underscore Sears?s difficulties][in implementing the ?everyday lowpricing?
strategy?]wsj_110510.
[The Bush Administration,] [trying to bluntgrowing demands from Western Europe fora relaxation of controls on exports to theSoviet bloc,] [is questioning?
]wsj_2326Finally, a small number of phrasal EDUs areallowed, provided that the phrase begins with astrong discourse marker, such as because, inspite of, as a result of, according to.
We optedfor consistency in segmenting, sacrificing somepotentially discourse-relevant phrases in theprocess.2.2 Building up the Discourse StructureOnce the elementary units of discourse havebeen determined, adjacent spans are linkedtogether via rhetorical relations creating ahierarchical structure.
Relations may bemononuclear or multinuclear.
Mononuclearrelations hold between two spans and reflect thesituation in which one span, the nucleus, is moresalient to the discourse structure, while the otherspan, the satellite, represents supportinginformation.
Multinuclear relations hold amongtwo or more spans of equal weight in thediscourse structure.
A total of 53 mononuclearand 25 multinuclear relations were used for thetagging of the RST Corpus.
The final inventoryof rhetorical relations is data driven, and isbased on extensive analysis of the corpus.Although this inventory is highly detailed,annotators strongly preferred keeping a higherlevel of granularity in the selections available tothem during the tagging process.
More extensiveanalysis of the final tagged corpus willdemonstrate the extent to which individualrelations that are similar in semantic contentwere distinguished consistently during thetagging process.The 78 relations used in annotating thecorpus can be partitioned into 16 classes thatshare some type of rhetorical meaning:Attribution, Background, Cause, Comparison,Condition, Contrast, Elaboration, Enablement,Evaluation, Explanation, Joint, Manner-Means,Topic-Comment, Summary, Temporal, Topic-Change.
For example, the class Explanationincludes the relations evidence, explanation-argumentative, and reason, while Topic-Comment includes problem-solution, question-answer, statement-response, topic-comment, andcomment-topic.
In addition, three relations areused to impose structure on the tree: textual-organization, span, and same-unit  (used to linkparts of units separated by embedded units orspans).3 Discourse Annotation TaskOur methodology for annotating the RSTCorpus builds on prior corpus work in theRhetorical Structure Theory framework byMarcu et al (1999).
Because the goal of thiseffort was to build a high-quality, consistentlyannotated reference corpus, the task requiredthat we employ people as annotators whoseprimary professional experience was in the areaof language analysis and reporting, provideextensive annotator training, and specify arigorous set of annotation guidelines.3.1 Annotator Profile and TrainingThe annotators hired to build the corpus were allprofessional language analysts with priorexperience in other types of data annotation.They underwent extensive hands-on training,which took place roughly in three phases.During the orientation phase, the annotatorswere introduced to the principles of RhetoricalStructure Theory and the discourse-tagging toolused for the project (Marcu et  al., 1999).
Thetool enables an annotator to segment a text intounits, and then build up a hierarchical structureof the discourse.
In this stage of the training, thefocus was on segmenting hard copy texts intoEDUs, and learning the mechanics of the tool.In the second phase, annotators began toexplore interpretations of discourse structure, byindependently tagging a short document, basedon an initial set of tagging guidelines, and thenmeeting as a group to compare results.
Theinitial focus was on resolving segmentationdifferences, but over time this shifted toaddressing issues of relations and nuclearity.These exploratory sessions led to enhancementsin the tagging guidelines.
To reinforce newrules, annotators re-tagged the document.During this process, we regularly tracked inter-annotator agreement (see Section 4.2).
In thefinal phase, the annotation team concentrated onways to reduce differences by adopting someheuristics for handling higher levels of thediscourse structure.
Wiebe et al (1999) presenta method for automatically formulating a singlebest tag when multiple judges disagree onselecting between binary features.
Because ourannotators had to select among multiple choicesat each stage of the discourse annotationprocess, and because decisions made at onestage influenced the decisions made duringsubsequent stages, we could not apply Wiebe etal.
?s method.
Our methodology for determiningthe ?best?
guidelines was much more of aconsensus-building process, taking intoconsideration multiple factors at each step.
Thefinal tagging manual, over 80 pages in length,contains extensive examples from the corpus toillustrate text segmentation, nuclearity, selectionof relations, and discourse cues.
The manual canbe downloaded from the following web site:http://www.isi.edu/~marcu/discourse.The actual tagging of the corpus progressedin three developmental phases.
During the initialphase of about four months, the team created apreliminary corpus of 100 tagged documents.This was followed by a one-month reassessmentphase, during which we measured consistencyacross the group on a select set of documents,and refined the annotation rules.
At this point,we decided to proceed by pre-segmenting all ofthe texts on hard copy, to ensure a higher overallquality to the final corpus.
Each text was pre-segmented by two annotators; discrepancieswere resolved by the author of the taggingguidelines.
In the final phase (about six months)all 100 documents were re-tagged with the newapproach and guidelines.
The remainder of thecorpus was tagged in this manner.3.2 Tagging StrategiesAnnotators developed different strategies foranalyzing a document and building up thecorresponding discourse tree.
There were twobasic orientations for document analysis ?
hardcopy or graphical visualization with the tool.Hard copy analysis ranged from jotting of notesin the margins to marking up the document intodiscourse segments.
Those who preferred agraphical orientation performed their analysissimultaneously with building the discoursestructure, and were more likely to build thediscourse tree in chunks, rather thanincrementally.We observed a variety of annotation stylesfor the actual building of a discourse tree.
Twoof the more representative styles are illustratedbelow.1.
The annotator segments the text one unit ata time, then incrementally builds up the(26)*elaboration-object-attribute-embedded+attribution-embedded(17) (18) (21)19-20 22-23 24-2517-21 22-26explanation-argumentative consequence-ssame-unit(25)purpose(24)17-26 elaboration-additional(16)example+(22) (23)17-18attribution 19-21 attribution(19) (20)*Figure 1: Discourse sub-tree for multiple sentencesdiscourse tree by immediately attaching thecurrent node to a previous node.
Whenbuilding the tree in this fashion, theannotator must anticipate the upcomingdiscourse structure, possibly for a largespan.
Yet, often an appropriate choice ofrelation for an unseen segment may not beobvious from the current (rightmost) unitthat needs to be attached.
That is whyannotators typically used this approach onshort documents, but resorted to otherstrategies for longer documents.2.
The annotator segments multiple units at atime, then builds discourse sub-trees foreach sentence.
Adjacent sentences are thenlinked, and larger sub-trees begin toemerge.
The final tree is produced bylinking major chunks of the discoursestructure.
This strategy allows the annotatorto see the emerging discourse structure moreglobally; thus, it was the preferred approachfor longer documents.Consider the text fragment below, consistingof four sentences, and 11 EDUs:[Still, analysts don?t expect the buy-back tosignificantly affect per-share earnings in theshort term.
]16 [The impact won?t be that great,]17[said Graeme Lidgerwood of First BostonCorp.
]18 [This is in part because of the effect]19[of having to average the number of sharesoutstanding,]20 [she said.
]21 [In addition,]22 [Mrs.Lidgerwood said,]23 [Norfolk is likely to drawdown its cash initially]24 [to finance thepurchases]25 [and thus forfeit some interestincome.
]26 wsj_1111The discourse sub-tree for this text fragmentis given in Figure 1.
Using Style 1 the annotator,upon segmenting unit [17], must anticipate theupcoming example relation, which spans units[17-26].
However, even if the annotator selectsan incorrect relation at that point, the tool allowsgreat flexibility in changing the structure of thetree later on.Using Style 2, the annotator segments eachsentence, and builds up corresponding sub-treesfor spans [16], [17-18], [19-21] and [22-26].
Thesecond and third sub-trees are then linked via anexplanation-argumentative relation, after which,the fourth sub-tree is linked via an elaboration-additional relation.
The resulting span [17-26] isfinally attached to node [16] as an examplesatellite.4 Quality AssuranceA number of steps were taken to ensure thequality of the final discourse corpus.
Theseinvolved two types of tasks: checking thevalidity of the trees and tracking inter-annotatorconsistency.4.1 Tree Validation ProceduresAnnotators reviewed each tree for syntactic andsemantic validity.
Syntactic checking involvedensuring that the tree had a single root node andcomparing the tree to the document to check formissing sentences or fragments from the end ofthe text.
Semantic checking involved reviewingnuclearity assignments, as well as choice ofrelation and level of attachment in the tree.
Alltrees were checked with a discourse parser andtree traversal program which often identifiederrors undetected by the manual validationprocess.
In the end, all of the trees workedsuccessfully with these programs.4.2 Measuring ConsistencyWe tracked inter-annotator agreement duringeach phase of the project, using a methoddeveloped by Marcu et al (1999) for computingkappa statistics over hierarchical structures.
Thekappa coefficient (Siegel and Castellan, 1988)has been used extensively in previous empiricalstudies of discourse (Carletta et al, 1997;Flammia and Zue, 1995; Passonneau andLitman, 1997).
It measures pairwise agreementamong a set of coders who make categoryjudgments, correcting for chance expectedagreement.
The method described in Marcu etal.
(1999) maps hierarchical structures into setsof units that are labeled with categorialjudgments.
The strengths and shortcomings ofthe approach are also discussed in detail there.Researchers in content analysis (Krippendorff,1980) suggest that values of kappa > 0.8 reflectvery high agreement, while values between 0.6and 0.8 reflect good agreement.Table 1 shows average kappa statisticsreflecting the agreement of three annotators atvarious stages of the tasks on selecteddocuments.
Different sets of documents werechosen for each stage, with no overlap indocuments.
The statistics measure annotationreliability at four levels: elementary discourseunits, hierarchical spans, hierarchical nuclearityand hierarchical relation assignments.At the unit level, the initial (April 00) scoresand final (January 01) scores representagreement on blind segmentation, and areshown in boldface.
The interim June andNovember scores represent agreement on hardcopy pre-segmented texts.
Notice that even withpre-segmenting, the agreement on units is not100% perfect, because of human errors thatoccur in segmenting with the tool.
As Table 1shows, all levels demonstrate a markedimprovement from April to November (whenthe final corpus was completed), ranging fromabout 0.77 to 0.92 at the span level, from 0.70 to0.88 at the nuclearity level, and from 0.60 to0.79 at the relation level.
In particular, whenrelations are combined into the 16 rhetorically-related classes discussed in Section 2.2, theNovember results of the annotation process areextremely good.
The Fewer-Relations columnshows the improvement in scores on assigningTable 1: Inter-annotator agreement ?
periodic results for three taggersTaggers Units Spans Nuclearity Relations Fewer-RelationsNo.
ofDocsAvg.
No.EDUsA, B, E(Apr 00)0.874407 0.772147 0.705330 0.601673 0.644851 4 128.750000A, B, E(Jun 00)0.952721 0.844141 0.782589 0.708932 0.739616 5 38.400002A, E(Nov 00)0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668B, E(Nov 00)0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713A, B(Nov 00)1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001A, B, E(Jan 01)0.971613 0.899971 0.855867 0.755539 0.782312 5 68.599998relations when they are grouped in this manner,with November results ranging from 0.78 to0.82.
In order to see how much of theimprovement had to do with pre-segmenting, weasked the same three annotators to annotate fivepreviously unseen documents in January,without reference to a pre-segmented document.The results of this experiment are given in thelast row of Table 1, and they reflect only a smalloverall decline in performance from theNovember results.
These scores reflect verystrong agreement and represent a significantimprovement over previously reported results onannotating multiple texts in the RST framework(Marcu et al, 1999).Table 2 reports final results for all pairs oftaggers who double-annotated four or moredocuments, representing 30 out of the 53documents that were double-tagged.
Results arebased on pre-segmented documents.Our team was able to reach a significantlevel of consistency, even though they faced anumber of challenges which reflect differencesin the agreement scores at the various levels.While operating under the constraints typical ofany theoretical approach in an appliedenvironment, the annotators faced a task inwhich the complexity increased as support fromthe guidelines tended to decrease.
Thus, whilerules for segmenting were fairly precise,annotators relied on heuristics requiring morehuman judgment to assign relations andnuclearity.
Another factor is that the cognitivechallenge of the task increases as the tree takesshape.
It is relatively straightforward for theannotator to make a decision on assignment ofnuclearity and relation at the inter-clausal level,but this becomes more complex at the inter-sentential level, and extremely difficult whenlinking large segments.This tension between task complexity andguideline under-specification resulted from thepractical application of a theoretical model on abroad scale.
While other discourse theoreticalapproaches posit distinctly different treatmentsfor various levels of the discourse (Van Dijk andKintsch, 1983; Meyer, 1985), RST relies on astandard methodology to analyze the documentat all levels.
The RST relation set is rich and theconcept of nuclearity, somewhat interpretive.This gave our annotators more leeway ininterpreting the higher levels of the discoursestructure, thus introducing some stylisticdifferences, which may prove an interestingavenue of future research.5 Corpus DetailsThe RST Corpus consists of 385 Wall StreetJournal articles from the Penn Treebank,representing over 176,000 words of text.
Inorder to measure inter-annotator consistency, 53of the documents (13.8%) were double-tagged.The documents range in size from 31 to 2124words, with an average of 458.14 words perdocument.
The final tagged corpus contains21,789 EDUs with an average of 56.59 EDUsper document.
The average number of words perEDU is 8.1.The articles range over a variety of topics,including financial reports, general intereststories, business-related news, cultural reviews,editorials, and letters to the editor.
In selectingthese documents, we partnered with theLinguistic Data Consortium to select PennTreebank texts for which the syntacticbracketing was known to be of high caliber.Thus, the RST Corpus provides an additionallevel of linguistic annotation to supplementexisting annotated resources.Table 2: Inter-annotator agreement ?
final results fox six taggersTaggers Units Spans Nuclearity Relations Fewer-RelationsNo.
ofDocsAvg.
No.EDUsB, E 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713A, E 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668A, B 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001A, C 0.950962 0.840187 0.782688 0.676564 0.711109 4 116.500000A, F 0.952342 0.777553 0.694634 0.597302 0.624908 4 26.500000A, D 1.000000 0.868280 0.801544 0.720692 0.769894 4 23.250000For details on obtaining the corpus,annotation software, tagging guidelines, andrelated documentation and resources,  see:http://www.isi.edu/~marcu/discourse.6 DiscussionA growing number of groups have developed orare developing discourse-annotated corpora fortext.
These can be characterized both in terms ofthe kinds of features annotated as well as by thescope of the annotation.
Features may includespecific discourse cues or markers, coreferencelinks, identification of rhetorical relations, etc.The scope of the annotation refers to the levelsof analysis within the document, and can becharacterized as follows:?
sentential: annotation of features at theintra-sentential or inter-sentential level, at asingle level of depth  (Sundheim, 1995;Tsou et al, 2000; Nomoto and Matsumoto,1999; Rebeyrolle, 2000).?
hierarchical: annotation of features atmultiple levels, building upon lower levelsof analysis at the clause or sentence level(Moser and Moore, 1995; Marcu, et al1999)?
document-level: broad characterization ofdocument structure such as identification oftopical segments (Hearst, 1997), linking oflarge text segments via specific relations(Ferrari, 1998; Rebeyrolle, 2000), ordefining text objects with a text architecture(Pery-Woodley and Rebeyrolle, 1998).Developing corpora with these kinds of richannotation is a labor-intensive effort.
Buildingthe RST Corpus involved more than a dozenpeople on a full or part-time basis over a one-year time frame (Jan. ?
Dec. 2000).
Annotationof a single document could take anywhere from30 minutes to several hours, depending on thelength and topic.
Re-tagging of a large numberof documents after major enhancements to theannotation guidelines was also time consuming.In addition, limitations of the theoreticalapproach became more apparent over time.Because the RST theory does not differentiatebetween different levels of the tree structure, afairly fine-grained set of relations operatesbetween EDUs and EDU clusters at the macro-level.
The procedural knowledge available at theEDU level is likely to need further refinementfor higher-level text spans along the lines ofother work which posits a few macro-levelrelations for text segments, such as Ferrari(1998) or Meyer (1985).
Moreover, using theRST approach, the resultant tree structure, like atraditional outline, imposed constraints thatother discourse representations (e.g., graph)would not.
In combination with the treestructure, the concept of nuclearity also guidedan annotator to capture one of a number ofpossible stylistic interpretations.
We ourselvesare eager to explore these aspects of the RST,and expect new insights to appear throughanalysis of the corpus.We anticipate that the RST Corpus will bemultifunctional and support a wide range oflanguage engineering applications.
The addedvalue of multiple layers of overt linguisticphenomena enhancing the Penn Treebankinformation can be exploited to advance thestudy of discourse, to enhance languagetechnologies such as text summarization,machine translation or information retrieval, orto be a testbed for new and creative naturallanguage processing techniques.ReferencesBruce Britton and John Black.
1985.Understanding Expository Text.
Hillsdale, NJ:Lawrence Erlbaum Associates.Jill Burstein, Daniel Marcu, Slava Andreyev,and Martin Chodorow.
2001.
Towardsautomatic identification of discourse elements inessays.
In Proceedings of the 39th AnnualMeeting of the Association for ComputationalLinguistics, Toulouse, France.Jean Carletta, Amy Isard, Stephen Isard,Jacqueline Kowtko, Gwyneth Doherty-Sneddon,and Anne Anderson.
1997.
The reliability of adialogue structure coding scheme.Computational Linguistics 23(1): 13-32.Giacomo Ferrari.
1998.
Preliminary stepstoward the creation of a discourse and textresource.
In Proceedings of the FirstInternational Conference on LanguageResources and Evaluation (LREC 1998),Granada, Spain, 999-1001.Giovanni Flammia and Victor Zue.
1995.Empirical evaluation of human performance andagreement in parsing discourse constituents inspoken dialogue.
In Proceedings of the 4thEuropean Conference on SpeechCommunication and Technology, Madrid, Spain,vol.
3, 1965-1968.Roger Garside, Steve Fligelstone and SimonBotley.
1997.
Discourse Annotation: AnaphoricRelations in Corpora.
In Corpus annotation:Linguistic information from computer textcorpora, edited by R. Garside, G. Leech, and T.McEnery.
London: Longman, 66-84.Talmy Givon.
1983.
Topic continuity indiscourse.
In Topic Continuity in Discourse: aQuantitative Cross-Language Study.Amsterdam/Philadelphia: John Benjamins, 1-41.Joseph Evans Grimes.
1975.
The Thread ofDiscourse.
The Hague, Paris: Mouton.Barbara Grosz and Candice Sidner.
1986.Attentions, intentions, and the structure ofdiscourse.
Computational Linguistics, 12(3):175-204.Marti Hearst.
1997.
TextTiling: Segmentingtext into multi-paragraph subtopic passages.Computational Linguistics 23(1): 33-64.Julia Hirschberg and Diane Litman.
1993.Empirical studies on the disambiguation of cuephrases.
Computational Linguistics 19(3): 501-530.Eduard Hovy.
1993.
Automated discoursegeneration using discourse structure relations.Artificial Intelligence 63(1-2): 341-386.Klaus Krippendorff.
1980.
Content Analysis:An Introduction to its Methodology.
BeverlyHills, CA: Sage Publications.Geoffrey Leech, Tony McEnery, and MartinWynne.
1997.
Further levels of annotation.
InCorpus Annotation: Linguistic Information fromComputer Text Corpora, edited by R. Garside,G.
Leech, and T. McEnery.
London: Longman,85-101.Robert Longacre.
1983.
The Grammar ofDiscourse.
New York: Plenum Press.William Mann and Sandra Thompson.
1988.Rhetorical structure theory.
Toward a functionaltheory of text organization.
Text, 8(3): 243-281.William Mann and Sandra Thompson, eds.1992.
Discourse Description: Diverse LinguisticAnalyses of a Fund-raising Text.Amsterdam/Philadelphia: John Benjamins.Daniel Marcu.
2000.
The Theory andPractice of Discourse Parsing andSummarization.
Cambridge, MA: The MITPress.Daniel Marcu, Estibaliz Amorrortu, andMagdelena Romera.
1999.
Experiments inconstructing a corpus of discourse trees.
InProceedings of the ACL Workshop on Standardsand Tools for Discourse Tagging, College Park,MD, 48-57.Daniel Marcu, Lynn Carlson, and MakiWatanabe.
2000.
The automatic translation ofdiscourse structures.
Proceedings of the FirstAnnual Meeting of the North American Chapterof the Association for ComputationalLinguistics, Seattle, WA, 9-17.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: the PennTreebank, Computational Linguistics 19(2),313-330.Bonnie Meyer.
1985.
Prose Analysis:Purposes, Procedures, and Problems.
InUnderstanding Expository Text, edited by B.Britton and J.
Black.
Hillsdale, NJ: LawrenceErlbaum Associates, 11-64.Johanna Moore.
1995.
Participating inExplanatory Dialogues: Interpreting andResponding to Questions in Context.Cambridge, MA: MIT Press.Johanna Moore and Cecile Paris.
1993.Planning text for advisory dialogues: capturingintentional and rhetorical information.Computational Linguistics 19(4): 651-694.Megan Moser and Johanna Moore.
1995.Investigating cue selection and placement intutorial discourse.
Proceedings of the 33rdAnnual Meeting of the Association forComputational Linguistics, Cambridge, MA,130-135.Tadashi Nomoto and Yuji Matsumoto.
1999.Learning discourse relations with active dataselection.
In Proceedings of the Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,College Park, MD, 158-167.Rebecca Passonneau and Diane Litman.1997.
Discourse segmentation by human andautomatic means.
Computational Linguistics23(1): 103-140.Marie-Paule Pery-Woodley and JosetteRebeyrolle.
1998.
Domain and genre insublanguage text: definitional microtexts inthree corpora.
In Proceedings of the FirstInternational Conference on LanguageResources and Evaluation (LREC-1998),Granada, Spain, 987-992.Livia Polanyi.
1988.
A formal model of thestructure of discourse.
Journal of Pragmatics12: 601-638.Livia Polanyi.
1996.
The linguistic structureof discourse.
Center for the Study of Languageand Information.
CSLI-96-200.Josette Rebeyrolle.
2000.
Utilisation decontextes d?finitoires pour l?acquisition deconnaissances ?
partir de textes.
In ActesJourn?es Francophones d?Ing?nierie de laConnaissance (IC?2000), Toulouse, IRIT, 105-114.Harvey Sacks, Emmanuel Schegloff, andGail Jefferson.
1974.
A simple systematics forthe organization of turntaking in conversation.Language 50: 696-735.Sidney Siegal and N.J. Castellan.
1988.Nonparametric Statistics for the BehavioralSciences.
New York: McGraw-Hill.Beth Sundheim.
1995.
Overview of results ofthe MUC-6 evaluation.
In Proceedings of theSixth Message Understanding Conference(MUC-6), Columbia, MD, 13-31.Benjamin K. T?sou, Tom B.Y.
Lai, SamuelW.K.
Chan, Weijun Gao, and Xuegang Zhan.2000.
Enhancement of Chinese discoursemarker tagger with C.4.5.
In  Proceedings of theSecond Chinese Language ProcessingWorkshop, Hong Kong, 38-45.Teun A.
Van Dijk and Walter Kintsch.
1983.Strategies of Discourse Comprehension.
NewYork: Academic Press.Ellen Voorhees and Donna Harman.
1999.The Eighth Text Retrieval Conference (TREC-8).
NIST Special Publication 500-246.Charles Wayne.
2000.
Multilingual topicdetection and tracking: successful researchenabled by corpora and evaluation.
InProceedings of the Second InternationalConference on Language Resources andEvaluation (LREC-2000), Athens, Greece,1487-1493.Janyce Wiebe, Rebecca Bruce, and ThomasO?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics.College Park, MD, 246-253.
