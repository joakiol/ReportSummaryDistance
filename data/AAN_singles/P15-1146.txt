Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1512?1522,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAdding Semantics to Data-Driven ParaphrasingEllie Pavlick1Johan Bos2Malvina Nissim2Charley Beller3Benjamin Van Durme4Chris Callison-Burch11Computer and Information Science Department, University of Pennsylvania2Center for Language and Cognition Groningen, University of Groningen4Human Language Technology Center of Excellence, Johns Hopkins University3IBM Watson GroupAbstractWe add an interpretable semantics tothe paraphrase database (PPDB).
To date,the relationship between phrase pairsin the database has been weakly de-fined as approximately equivalent.
Weshow that these pairs represent a vari-ety of relations, including directed entail-ment (little girl/girl) and exclusion (no-body/someone).
We automatically assignsemantic entailment relations to entries inPPDB using features derived from pastwork on discovering inference rules fromtext and semantic taxonomy induction.
Wedemonstrate that our model assigns theserelations with high accuracy.
In a down-stream RTE task, our labels rival relationsfrom WordNet and improve the coverageof a proof-based RTE system by 17%.1 MotivationA basic precursor to language understanding is theability to recognize when two expressions meanthe same thing.
Different expressions of the sameinformation is the central problem addressed byparaphrasing and the closely related task of rec-ognizing textual entailment (RTE).
In RTE, a sys-tem is given two pieces of text, often called thetext (T) and the hypothesis (H), and asked to de-termine whether T entails H, T contradicts H, orT and H are unrelatable (Figure 1).
In contrast,data-driving paraphrasing typically sidesteps de-veloping a clear definition of ?meaning the samething?
and instead ?assume[s] paraphrasing is acoherent notion and concentrate[s] on devices thatcan produce paraphrases?
(Barzilay, 2003).
Re-cent work on paraphrase extraction has resultedin enormous paraphrase collections (Lin and Pan-tel, 2001; Dolan et al, 2004; Ganitkevitch etal., 2013), but the usefulness of these collectionsRiots in Denmark were sparked by 12 editorialcartoons that were offensive to Muhammad.12 ?
Twelveeditorial cartoons A illustrationsoffensive A insultingMuhammad ?
the prophetsparked A causedriots A unrestin Denmark | in JordanTwelve illustrations insulting the prophetcaused unrest in Jordan.Figure 1: An example sentence pair for the RTE task.
In orderfor a system to conclude that the premise (top) does not entailthe hypothesis (bottom), it should recognize that sparked im-plies caused but that in Denmark precludes in Jordan.
Thesephrase-level entailment relationships are modeled by naturallogic.is limited by the fast-and-loose treatment of themeaning of paraphrases.
One concrete defini-tion that is sometimes used for paraphrases re-quires that they be bidirectionally entailing (An-droutsopoulos and Malakasiotis, 2010).
That is,in terms of RTE, it is assumed that if P is a para-phrase of Q, then P entails Q and Q entails P. Inreality, paraphrases are often more nuanced (Bha-gat and Hovy, 2013), and the entries in most para-phrase resources certainly do not match this def-inition.
For instance, Lin and Pantel (2001) ex-tracted 12 million ?inference rules?
from mono-lingual text by exploiting shared dependency con-texts.
Their method learns paraphrases that aretruly meaning equivalent, but it just as readilylearns contradictory pairs such as hX rises, X fallsi.Ganitkevitch et al (2013) extract over 150 mil-lion paraphrase rules by pivoting through foreigntranslations.
This bilingual method often learnshypernym/hyponym pairs, e.g.
due to variationin the discourse structure of translations (Callison-1512Equivalent Entailment Exclusion Other relation Unrelatedlook at/watch little girl/girl close/open swim/water girl/playa person/someone kuwait/country minimal/significant husband/marry to found/partyclean/cleanse tower/building boy/young girl oil/oil price profit/yearaway/out the cia/agency nobody/someone country/patriotic man/talkdistant/remote sneaker/footwear blue/green drive/vehicle car/familythe phone/the telephone heroin/drug france/germany family/home holiday/serieslast autumn/last fall doe/deer least three/least two basketball/court green/tennisillegal entry/smuggling typhoon/storm child/mother playing/toy sunday/tourapprove/to ratify seriously injure/injure in front/on the side islamic/jihad city/southalliance of/coalition between sunglasses/glasses oppose/support delay/time back/viewTable 1: Examples of different types of entailment relations appearing in PPDB.Burch, 2007), and unrelated pairs, e.g.
due to mis-alignments or polysemy in the foreign language.The unclear semantics severely limits the ap-plicability of paraphrase resources to natural lan-guage understanding (NLU) tasks.
Some effortshave been made to identify directionality of para-phrases (Bhagat et al, 2007; Kotlerman et al,2010), but tasks like RTE require even richer se-mantic information.
For example, in the T/H pairshown in Figure 1, a system needs informationnot only about equivalent words (12/twelve) andasymmetric entailments (riots/unrest), but also se-mantic exclusion (Denmark/Jordan).
Such lexicalentailment relations are captured by natural logic,a formalism which views natural language itselfas a meaning representation, eschewing externalrepresentations such as First Order Logic (FOL).This is a great fit for automatically extracted para-phrases, since the phrase pairs themselves can beused as the semantic representation with minimaladditional annotation.
But as is, paraphrase re-sources lack such annotation.As a result, NLU systems rely on manually builtresources like WordNet, which are limited in cov-erage and often lead to incorrect inferences (Ka-plan and Schubert, 2001).
In fact, in the mostrecent RTE challenge, over half of the submittedsystems used WordNet (Pontiki et al, 2014).
Eventhe NatLog system (MacCartney and Manning,2007), which popularized natural logic for RTE,relied on WordNet and did not solve the problemof assigning natural logic relations at scale.The main contributions of this paper are:?
We add a concrete, interpretable semanticsto the Paraphrase Database (PPDB) (Ganitke-vitch et al, 2013), the largest paraphrase re-source currently available.
We give each en-try in the database a label describing the en-tailment relationship between the phrases.?
We develop a statistical model to predictthese relations.
The enormous size of PPDB?over 77 million phrase pairs!?
makes it im-possible to perform this task manually.
Ourwide range of monolingual and bilingual fea-tures results in high intrinsic accuracy.?
We demonstrate improvements to a proof-based RTE system, showing that our auto-matic labels increase the number of proofsthat it is able to find by 17%, while maintain-ing the same accuracy as when using gold-standard, manual labels.2 Related WorkLexical entailment resources Approaches toparaphrase identification have exploited signalfrom distributional contexts (Lin and Pantel, 2001;Szpektor et al, 2004), comparable corpora (Dolanet al, 2004; Xu et al, 2014), and graph structures(Berant et al, 2011; Brockett et al, 2013).
Theseapproaches are scalable, but they often assume thatall relations are equivalence relations (Madnaniand Dorr, 2010).
Several efforts have attemptedto build or augment lexical ontologies automati-cally, to discover other types of lexical relationslike hypernyms.
Most of these approaches rely onlexico-syntactic patterns.
Hearst (1992) searchedfor hand-written patterns (e.g.
?an X is a Y?)
in alarge corpus in order to learn taxonomic relationsbetween nouns.
Snow et al (2006) used depen-dency parses to automatically learn such patterns,which they used to augment WordNet with newhypernym relations.
Similar monolingual signalshave been used to learn fine-grained relationshipsbetween verbs, such as enablement and happens-before (Chklovski and Pantel, 2004; Hashimoto etal., 2009).Recognizing Textual Entailment The sharedRTE tasks (Dagan et al, 2006) have been a spring-board for research in natural language inference,1513Figure 2: Distribution of entailment relations in different sizes of PPDB.
Distributions are estimated from our manual annota-tions of randomly sampled pairs.
PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent),compared to only 700K in PPDB-S (where the majority type is equivalent).using data motivated by the applications to infor-mation retrieval, information extraction, summa-rization, machine translation evaluation, and morerecently, question answering (Giampiccolo et al,2007) and essay grading (Clark et al, 2013).
RTEsystems vary considerably in their choice of rep-resentation and inference procedure.
In the mostrecent shared task on RTE, some systems useddeep logical representations of text, allowing themto invoke theorem provers (Bjerva et al, 2014)or Markov Logic Networks (Beltagy et al, 2014)to perform the inference, while others used shal-lower representations, relying on machine learn-ing to perform inference (Lai and Hockenmaier,2014; Zhao et al, 2014).
Systems based on naturallogic (MacCartney and Manning, 2007) use natu-ral language as a representation, but still performinference using a structured algebra rather than astatistical model.
Regardless of the inference pro-cedure, improvements to external lexical resourcescan improve RTE systems across the board (Clarket al, 2007).3 The Paraphrase Database (PPDB)PPDB is currently the largest available collectionof paraphrases.
Compared to other paraphraseresources such as the DIRT database (12 mil-lion rules) (Lin and Pantel, 2001) and the MSRparaphrase phrase table (13 million) (Dolan etal., 2004), PPDB contains over 150 million para-phrase rules covering three paraphrase types?
lex-ical (single word), phrasal (multiword), and syn-tactic restructuring rules.
We focus on lexical andphrasal paraphrases, of which there are over 77million rules.
Of these, a large fraction are trueparaphrases?
either equivalent (distant/remote) orasymmetric entailment (girl/little girl)?
but manyare not.
PPDB contains some pairs which arerelated by semantic exclusion (nobody/someone),some of which are related by something other thanentailment (swim/water), and some which are sim-ply unrelated (car/family).
Table 1 gives examplesof pairs in PPDB falling into each of these cate-gories.PPDB is released in six sizes (S, M, L, XL,XXL and XXXL), which fall roughly on a con-tinuum from highest precision and lowest recall tolowest average precision and highest recall.
Fig-ure 2 shows how the distribution of entailment re-lations differs across the sizes of PPDB.1Our goalis to make these relations explicit, by providingannotations for each phrase pair.
Because of theenormous scale of PPDB, this annotation must bedone automatically.4 Selection of ParaphrasesIn this paper we focus on paraphrases pairs fromPPDB that occur in RTE data.
We use the recentSICK dataset from in the 2014 SemEval RTE chal-lenge (Marelli et al, 2014) for our experiments.The data consists of 10K sentences split roughlyevenly into training and testing sets.
The sen-tence pairs are labeled using a 3-way entailmentclassification: ENTAILMENT, (29%) CONTRADIC-TION (15%), or NEUTRAL (56%).
We considerall phrase pairs from PPDB hp1, p2i up to threewords in length such that there is some T/H sen-tence pair in which p1appears in T and p2appears1These distributions were estimated based on a randomsample of pairs drawn from each size of PPDB, annotated onMTurk as described in Section 51514Lexical We use the lemmas, POS tags, and phrase lengths of p1and p2, the substrings shared by p1and p2,and the Levenstein, Jaccard, and Hamming distances between p1and p2.Distributional Given a dependency context vectors for p1and p2, we compute the number of shared contexts, andthe Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between thevectors.Paraphrase We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilitiesas computed in Bannard and Callison-Burch (2005).
We refer the reader to Ganitkevitch and Callison-Burch (2014) for a complete description of all of the features included with PPDB.Translation We include the number of foreign language ?pivots?
(translations) shared by p1and p2for each of 24languages used in the construction of PPDB, as a fraction of the total number of translations observedfor each of p1and p2.Path We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) whichare observed between p1and p2in the Annotated Gigaword corpus (Napoles et al, 2012).WordNet We include binary features indicating whether WordNet classifies p1and p2according to any of thefollowing relations: synonym, hypernym, hyponym, antonym, holonym, meronym, cause, entailment,derivationally-related, similar-to, also-see, or attribute.Figure 3: Summary of features extracted for each phrase pair hp1, p2i.
Full descriptions of the features used are given in thesupplementary material.in H. Roughly 55% of the word types and 5% ofthe phrase (bigram and trigram) types in the SICKdata appear in PPDB.
This gives us a list of 9,600pairs, half from the training sentences, which weuse for development in Section 6, and half fromthe test sentences, which we use for evaluation inSection 7.The SICK data has a relatively small vocabu-lary, with 86% of words types and <1% of thephrase types covered by WordNet.
Still, over halfof the words in SICK which are covered by PPDBdo not appear in WordNet.
In general, PPDB cov-ers a much larger vocabulary (1.6MM words) thandoes WordNet (155K words), and we expect thepotential benefit of using PPDB in addition to orin place of WordNet to be larger on datasets withricher vocabularies.5 Entailment RelationsWe use the relations from Bill MacCartney?sthesis on natural language inference as the basisfor our categorization of relations (MacCartney,2009).
He outlines 7 basic entailment relation-ships:2Equivalence (P?Q): 8x[P(x) $ Q(x)]Forward Entailment (P@Q): 8x[P(x) !
Q(x)]Reverse Entailment (PAQ): 8x[Q(x) !
P(x)]Negation (P?Q): 8x [P(x) $ ?
Q(x)]Alternation (P|Q): 8x ?
[P(x) ^ Q(x)]Cover (P^Q): 8x[P(x) _ Q(x)]Independence (P#Q): All other cases.2To further clarify the definitions here: ?negation?
is XOR(exclusive disjunction), ?alternation?
is NAND, and ?cover?is OR (inclusive disjunction)These relations are based on the theory of natu-ral logic, meaning they are defined between pairsof natural language expressions rather than requir-ing an external formal representation.
This makesthem an ideal fit for the phrase pairs in in PPDBand similar automatically-constructed paraphraseresources.Nat.
ThisMTurk descriptionLog.
work?
?
X is the same as Y@ @ X is more specific than/is a type of YA A X is more general than/encompasses Y?
?X is the opposite of Y| X is mutually exclusive with Y#?
X is related in some other way to Y# X is not related to YTable 2: Column 1 gives the semantics of each label underMacCartney?s Natural Logic.
Column 2 gives the notationwe use throughout the remainder of this paper.
Column 3gives the description that was shown to Turkers.Annotation We use Amazon Mechanical Turk(MTurk) to collect labels for our phrase pairs.
Weasked workers to choose between the options showin Table 2, which represent a modified versionof MacCartney?s relations.
We replace negation(?)
with the weaker notion of ?opposites,?
effec-tively merging it with the alternation (|) relation;we split the independent (#) class into two cases:truly independent phrases and phrases which arerelated by something other than entailment (whichwe denote ?).
We omit the cover (^) relation en-tirely, as its practicality is not obvious.
We showeach pair to 5 workers, taking the majority label astruth.
Each HIT consisted of two control questionstaken from WordNet.
Workers achieved good ac-curacies on our controls (82% overall) and moder-1515Cosine Similarity Monolingual (symmetric) Monolingual (asymmetric) BilingualA shades/the shade ?
large/small A boy/little boy ?
dad/fatherA yard/backyard ?
few/several A man/two men A some kid/child# each other/man ?
different/same A child/three children ?
a lot of/manyA picture/drawing ?
other/same ?
is playing/play ?
female/woman?
practice/target ?
put/take A side/both sides ?
male/manTable 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailmentlabels.
Column 1 is cosine similarity based on dependency contexts.
Column 2 is based on Lin (1998), column 3 on Weeds(2004), and column 4 is a novel feature.
Precise definitions of each metric are given in the supplementary material.ate levels of agreement (Fleiss?s ?
= 0.56) (Landisand Koch, 1977).
For a fuller discussion of theannotation, refer to the supplementary material.6 Automatic ClassificationWe aim to build a classifier to automatically assignentailment types to entries in the PPDB, and todemonstrate that it performs well both intrinsicallyand extrinsically.
We fix the direction of the@ andA relations to create a single class and train a lo-gistic regression classifier to distinguish betweenthe 5 classes {#,?,A,?,?}.
We compute vari-ety of basic lexical features and WordNet features(summarized in Figure 3).
We categorize the re-maining features into two broad groups: monolin-gual features, which are based on observed usagein the Annotated Gigaword corpus (Napoles et al,2012), and bilingual features, which are based ontranslation probabilities observed in bilingual par-allel corpora.
Full descriptions of all the featuresused are provided in the supplementary material.6.1 Monolingual featuresPath features Snow et al (2004) used lexico-syntactic patterns to mine taxonomic relations(hypernyms and hyponyms) between noun pairs.They were able to verify the earlier work of Hearst(1992) which found that certain patterns, e.g.
Xand other Y, are strong indicators of hypernymy.Using similar path features, we learn new patternsto differentiate between more subtle relations.
Forexample, we learn the pattern separate X from Y ishighly indicative of the ?
relation.
We learn thatthe pattern X including Y suggests A more than itsuggests ?
whereas the pattern X known as Y sug-gests ?
more than A.
Table 4 gives examples ofsome of the paths most indicative of the ?
relation.Distributional features Lin and Pantel (2001)attempted to mine inference rules from text byfinding paths in a dependency tree which connectthe same nouns.
The intuition is that good para-phrases should tend to modify and be modified byin X and in Y in foods and in beveragesseparate X from Y separate the old from the youngto X and/or to Y to the left or to the rightfrom X to Y from 7 a.m. to 10 p.m.more/less X than Y more harm than goodTable 4: Top paths associated with the ?
class.the same words.
Given context vectors, Lin andPantel (2001) used a symmetric similarity met-ric (Lin, 1998) to find candidate paraphrases.
Webuild dependency context vectors for each wordin our data and compute both symmetric as wellas more recently proposed asymmetric similaritymeasures (Weeds et al, 2004; Szpektor and Da-gan, 2008; Clarke, 2009), which are potentiallybetter suited for identifying A paraphrases.
Ta-ble 3 gives a comparison of the pairs which areconsidered ?most similar?
according to several ofthese metrics.6.2 Bilingual featuresWe explore a variety of bilingual features, whichwe expect to provide complimentary signal to themonolingual features.
Each pair in PPDB is asso-ciated with several paraphrase probabilities, whichare based on the probabilities of aligning eachword to the foreign ?pivot?
phrase (a foreign trans-lation shared by the two phrases), computed asdescribed in Bannard and Callison-Burch (2005).We also compute the total number of shared for-eign translations for each phrase pair.
Table 3shows the highest ranked pairs by this bilingualsimilarity score, in comparison to several of themonolingual scores.6.3 AnalysisTable 5 shows an ablation analysis.
The bilingualfeatures are especially important for distinguish-ing the?
class, and the path and WordNet featuresare important for the ?
class.
The lexical featuresshow strong performance across the board; this isoften because they capture negation words (e.g.no) and substring features (little boy @ boy).1516Table 1mono Predicted label ?
(using monolingual features)Predicted label ?
(using bilingual features)Predicted label ?
(using all features)ind syn hyp exl oth ?
?
?
# ~ ?
?
?
# ~ ?
?
?
# ~syn 1 3 1 0 0 4 ?
58% 20% 4% 15% 3% 62% 21% 5% 4% 8% 83% 10% 0% 2% 4%hyp 2 3 7 0 1 13 ?
20% 51% 3% 18% 7% 27% 5% 7% 7% 54% 6% 76% 2% 7% 8%exl 1 1 1 1 0 4 ?
26% 14% 37% 17% 6% 6% 14% 30% 36% 14% 2% 8% 73% 13% 3%ind 14 2 2 0 1 20 # 8% 13% 2% 71% 6% 1% 7% 6% 78% 8% 1% 4% 2% 88% 6%oth 3 1 2 0 2 10 ~ 15% 21% 5% 36% 23% 8% 19% 9% 30% 35% 5% 10% 3% 18% 64%biind syn hyp exl othsyn 0 3 1 0 0 4hyp 2 7 1 2 13 24exl 1 0 1 1 1 4ind 15 0 1 1 2 20oth 3 1 2 1 3 10bothind syn hyp exl othsyn 9 368 46 1 19 443hyp 97 83 1004 29 108 1321exl 49 9 29 275 13 375ind 1730 15 82 35 114 1976oth 169 48 97 33 609 956True labelFigure 4: Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingualfeatures (paraphrase and translation).
True labels are shown along rows, predicted along columns.
The matrix is normalizedalong rows, so that the predictions for each (true) class sum to 100%.
The confusion matrices reflect classifier?s performanceon held-out phrase pairs from the SICK test set.  F1 when excludingAll Lex.
Dist.
Path Para.
Tran.
WN# 79 -2.0 -0.2 -1.2 -1.7 -0.2 -0.1?
57 -3.5 +0.2 -0.7 -2.4 -3.7 +0.5A 68 -4.6 -0.3 -0.8 -0.8 -0.7 -1.6?
49 -4.0 -0.8 -2.9 +0.3 -0.0 -2.2?
51 -4.9 -0.5 -0.7 -1.2 -0.9 -0.3Table 5: F1 measure (?100) achieved by entailment classifierusing 10-fold cross validation on the training data.Table 3 shines some light onto the differencesbetween monolingual and bilingual similarities.While the monolingual asymmetric metrics aregood for identifying A pairs, the symmetric met-rics consistently identify ?
pairs; none of themonolingual scores we explored were effectivein making the subtle distinction between ?
pairsand the other types of paraphrase.
In contrast,the bilingual similarity metric is fairly precisefor identifying ?
pairs, but provides less infor-mation for distinguishing between types of non-equivalent paraphrase.
These differences are fur-ther exhibited in the confusion matrices shown inFigure 4; when the classifier is trained using onlymonolingual features, it misclassifies 26% of ?pairs as ?, whereas the bilingual features makethis error only 6% of the time.
On the other hand,the bilingual features completely fail to predict theA class, calling over 80% of such pairs ?
or ?.7 Evaluation7.1 Intrinsic EvaluationWe test the performance of our classifier intrinsi-cally, through its ability to reproduce the humanlabels for the phrase pairs from the SICK test sen-tences.
Table 7 shows the precision and recallachieved by the classifier for each of our 5 en-tailment classes.
The classifier is able to achievean overall 79% accuracy, reaching >70% preci-sion while maintaining good levels of recall on allclasses.True Pred.
N Example misclassifications?
# 169 boy/little, an empy/the air# ?
114 little/toy, color/hairA ?
108 drink/juice, ocean/surfA # 97 in front of/the face of, vehicle/horseA ?
83 cat/kitten, pavement/sidewalk?
A 46 big/grand, a girl/a young ladyA ?
29 kid/teenager, no small/a large?
A 29 old man/young man, a car/a window# ?
15 a person/one, a crowd/a large?
# 9 he is/man is, photo/still?
?
1 girl is/she isTable 6: Example misclassifications from some of the mostfrequent and most interesting error categories.Figure 4 shows the classifier?s confusion ma-trix and Table 6 shows some examples of commonand interesting error cases.
The majority of errors(26%) come from confusing the ?
class with the# class.
This mistake is not too concerning froman RTE perspective since ?
can be treated as aspecial case of # (Section 5).
There are very fewcases in which the classifier makes extreme errors,e.g.
confusing ?
with ?
or with #; some interest-ing examples of such errors arise when the phrasescontain pronouns (e.g.
girl ?
she) or when therelation uses a highly infrequent word sense (e.g.photo ?
still).7.2 The Nutcracker RTE SystemTo further test our classifier, we evaluate the use-fulness of the automatic entailment predictions ina downstream RTE task.
We run our experimentsusing Nutcracker, a state-of-the-art RTE systembased on formal semantics (Bjerva et al, 2014).1517Figure 5: ENTAILMENTFigure 6: CONTRADICTION Figure 7: NEUTRALFigure 8: F1 measures achieved by Nutcracker on SICK test data when using various KBs.
Baselines are in gray, this workin blue, human references in gold.
PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to beequivalent.
PPDB-H refers to a run in which manual labels were used to generate axioms.
PPDB+ refers to runs in whichthe automatic classifications were used to generate axioms.
In some cases, better proof coverage causes NC to find incorrectproofs, illustrated by the decreased performance on CONTRADICTION when using PPDB-H. For example, using PPDB-H, NCfinds an inconsistency for the pair Someone is not playing piano./A person is playing a keyboard.
Using the PPDB+, in whichpiano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL.Freq.
Precision Recall F score# 39% 84.22 87.55 85.85?
8% 70.36 83.07 76.19A 26% 79.81 76.00 77.85?
7% 73.73 73.33 73.53?
19% 70.57 63.70 66.96Table 7: F1 measure (?100) achieved by entailment classifieron the held out phrase pairs from the sentences in SICK test.In the SemEval 2014 RTE challenge, this systemperformed in the top 5 out of the more than 20 par-ticipating systems (Marelli et al, 2014).Given a text/hypothesis (T/H) pair, Nutcracker(NC) uses the Boxer parser (Bos, 2008) to producea formal semantic representation of both T and H,which it translates into standard first-order logic.The logical formulae are passed to an off-the-shelftheorem prover, which searches for a logical en-tailment, and to a model builder, which attempts tofind a logical contradiction.
By default, when thesystem fails to find a proof for either entailment orinconsistency, it predicts the most frequent class(in our case, NEUTRAL).
Therefore, NC reliesheavily on lexical entailment resources in orderto improve the recall of the theorem prover andmodel builder.Baselines The most frequent class baseline isachieved by labeling every sentence pair as NEU-TRAL, and results in an accuracy of 56%.
Astronger baseline is obtained by running NC alone,without any external axioms; in this case, wordsare only equivalent if they are lemma-identical.As an additional baseline, we generate a ?basic?Acc.
# Proofs CoverageMFC 56.4 0 0%NC alone 74.3 878 17.8%+ WN 77.5 1,051 21.3%+ PPDB-XL 77.5 1,091 22.1%+ PPDB+ 78.0 1,197 24.3%+ WN, PPDB+ 78.4 1,230 25.0%+ WN, PPDB-H 78.6 1,232 25.0%Table 8: Nutcracker?s overall system accuracy and proof cov-erage when using different sources of axioms.
Coverage ismeasured as the percent of sentence pairs for which NC?stheorem prover or model builder is able to find a completelogical proof of either entailment or contradiction.
When NCfails to find either type of proof, it guesses the most frequentclass, NEUTRAL.
NC alone uses no axioms.
PPDB+ refersto the axioms generated automatically using the classifier de-scribed in this paper.
PPDB-H refers axioms generated usingthe human labels on which the classifier was trained.PPDB-XL3knowledge base (KB), which consistsexclusively of axioms expressing synonym rela-tionships.
I.e.
for every pair of phrases hp1, p2i inPPDB-XL, the PPDB-XL KB contains the equiv-alence axiom syn(p1, p2).
We also generate theWordNet (WN) KB, which is the default used byNC.
This KB consists of axioms for all synonyms,antonyms, and hypernyms in WN, which generatesyn, isnota, and isa axioms, respectively.PPDB+ We convert our classifier?s predictionsinto a set of axioms for NC.
When our classifierpredicts?we generate an syn axiom, when it pre-dicts A we generate an isa axiom, and when itpredicts ?
we generate an isnota axiom.
# and?
do not generate any axioms.
To handle the di-rectionality of the A relation, we run the classifier3We generated basic KBs for all six sizes of PPDB, butXL performed best.1518True PPDB+ WN Text/Hypothesis pairENTAIL.
ENTAIL.
NEUTRAL A bride in a white dress is running/A girl in a white dress is running.ENTAIL.
NEUTRAL ENTAIL.
A lemur is biting a person?s finger./An animal is biting a person?s finger.CONTRA.
CONTRA.
NEUTRAL Someone is playing a piano./There is no one playing a piano.CONTRA.
NEUTRAL CONTRA.
There is no man pouring oil into a pan./A man is pouring oil into a skillet.Table 9: Examples of T/H pairs for which the system?s prediction differed when using PPDB+ vs. WN.over every pair in both directions, and we choosewhichever direction and relation receives the high-est confidence score to be the final prediction.
Werefer to this set of automatically-predicted axiomsas PPDB+.To calibrate our improvements, we also gener-ate a KB using the human labels collected fromMTurk, which we refer to as PPDB-Human orPPDB-H.Results Table 8 reports NC?s overall predictionaccuracy and the number of proofs found when us-ing each of the described KBs.
Figure 8 shows theperformance in terms of the precision and recallachieved for each of the three entailment classes:ENTAILMENT, CONTRADICTION, and NEUTRAL.Table 9 provides some examples of T/H pairs onwhich predictions differed using the PPDB+ com-pared to the WN KB, and Figure 9 shows someillustrative misclassifications.Our automatic labels result in a 4% improve-ment in accuracy over the baseline of using NCalone (Figure 8), and a 15 point improvement in F1measure for the entailment class (Table 8).
By allperformance measures, PPDB+ also outperformsWordNet as a source of axioms for NC.
More-over, adding PPDB+ to WordNet gives a 17% rel-ative increase in the number of proofs found com-pared to using WordNet alne (Table 8).
Theseadditional proofs lead NC to make a greater num-ber of correct predictions for the ?right reasons?(i.e.
finding a proof/contradiction) rather than bylucky guessing (recall NC guesses the most fre-quent class when it cannot find a proof).For comparison, we run the same experimentsusing a KB of oracle human labels in place ofthe predicted labels in PPDB+.
Using PPDB+,NC comes very close to the performance achievedwhen using PPDB-Human, demonstrating thatthe automatically generated PPDB+ provides asmuch utility to the end-to-end system as does agold-standard resource.8 Data ReleaseUpon publication, we are releasing a new PPDBfully annotated with semantic relations.
We arealso releasing the set of 14K manually labeledphrase pairs occurring in RTE data, and our soft-ware for extracting features and running the clas-sifier, so that researchers can apply our model totheir own paraphrase collections.
This will consti-tute the largest lexical entailment resources avail-able, while also offering new fine-grained anno-tation necessary for challenging NLU tasks.
Anevaluation of the predicted relations appearing inthe entire Paraphrase Database (not just those oc-curring in RTE data) is given in the supplementarymaterial.9 ConclusionWe argue that a significant failing of recent workon data-driven paraphrasing is the weak definitionof paraphrases as being more-or-less equivalent.In this paper, we show how a clear concept of se-mantics can be applied to large-scale paraphraseresources.
In particular, the entailment relationsgiven by natural logic are a great fit for paraphraseresources, since natural logic operates on pairs ofnatural language expressions (like the entries inPPDB).
By classifying paraphrase entries with en-tailment relations, we provide them with an inter-pretable semantics.
Our classifier uses extensivefeature sets to scale natural logic to the enormousnumber of phrase pairs in PPDB.
We rigorouslyevaluate our model, demonstrating high accuracyon an intrinsic task.
On an extrinsic RTE task, ourmodel?s predictions allow an RTE system to find17% more proofs and achieve a higher overall ac-curacy than when using WordNet?s manual rela-tions.
Our new release of PPDB, annotated withsemantic entailments, will dramatically improvePPDB?s utility for NLU tasks.Acknowledgements This research was sup-ported by the Allen Institute for Artificial Intel-ligence (AI2), the Human Language TechnologyCenter of Excellence (HLTCOE), and by giftsfrom the Alfred P. Sloan Foundation, Google, and1519# ?
A ?
?38% 8% 26% 7% 18%#1730 9 97 49 16940%(clear,very) (cover,front) (hand,male) (drive,park) (child,park)(exhibit,hold) (photo,still) (man,police) (female,man) (crowded,many)(walk,woman) (woman who,woman with) (mountain,side) (flag,ship) (note,write)?15 368 83 9 4810%(a big,very) (a small,the little) (a gun,a weapon) (another man,one man) (a child,kid in)(a lot,long) (away,out) (a weapon,gun) (bike,biking) (and hold,and take)(face a,front of) (block,slab) (legs,leg) (young girl,young woman) (his arms,his hands)A82 46 1004 29 9724%(device,guy) (a call,phone call) (camera,webcam) (a car,a window) (a lady,girl)(something,talk) (a group,bunch of) (kid,other child) (a female,a man) (field,playing)(the man,the phone) (another man,man) (kid,the daughter) (arms,his hands) (girl,the lady)?35 1 29 275 337%(a ball,a man) (girl is,she is) (a boy,a teenager) (cat,dog) (dog,owner)(a boy,little) (a kid,daughter) (morning,night) (ground,water)(number,woman) (kid,little girl) (type,write) (hat,vest)?114 19 108 13 60917%(leg,soccer) (chef,cook) (cut,saw) (a boat,sail) (ice,rink)(perform,run) (fight,match) (face,hair) (dress,suit) (snow,snowy)(sail,water) (race,ride) (the kid,the little) (light,the dark) (study by,study the)Figure 9: Confusion matrix for classifier (with all features) on SICK test set.
True labels and their distribution are shown alongthe columns, predicted along the rows.Facebook.
This material is based in part on re-search sponsored by the NSF under grant IIS-1249516 and DARPA under agreement numberFA8750-13-2-0017 (the DEFT program).
TheU.S.
Government is authorized to reproduce anddistribute reprints for Governmental purposes.The views and conclusions contained in this pub-lication are those of the authors and should not beinterpreted as representing official policies or en-dorsements of DARPA or the U.S. Government.The authors would like to thank Peter Clark,Bill MacCartney, Patrick Pantel and the anony-mous reviews for their thoughtful suggestions.ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entail-ment methods.
Journal of Artificial Intelligence Re-search, 38.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 597?604.Regina Barzilay.
2003.
Information fusion for multi-document summarization: paraphrasing and gener-ation.
Ph.D. thesis, Columbia University.Islam Beltagy, Stephen Roller, Gemma Boleda, Ka-trin Erk, and Raymond J Mooney.
2014.
UTexas:Natural language semantics using distributional se-mantics and probabilistic logic.
SemEval 2014, page796.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages610?619.Rahul Bhagat and Eduard Hovy.
2013.
What is a para-phrase?
Computational Linguistics, 39.Rahul Bhagat, Patrick Pantel, Eduard H Hovy, andMarina Rey.
2007.
Ledir: An unsupervised algo-rithm for learning directionality of inference rules.In EMNLP-CoNLL, pages 161?170.
Citeseer.Johannes Bjerva, Johan Bos, Rob van der Goot, andMalvina Nissim.
2014.
The meaning factory: For-mal semantics for recognizing textual entailmentand determining semantic similarity.
SemEval 2014,page 642.Johan Bos.
2008.
Wide-coverage semantic analy-sis with boxer.
In Johan Bos and Rodolfo Del-monte, editors, Semantics in Text Processing.
STEP2008 Conference Proceedings, Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Christopher John Brockett, Stanley Kok, and Dengy-ong Zhou.
2013.
Locating paraphrases through uti-lization of a multipartite graph, July 9.
US Patent8,484,016.Chris Callison-Burch.
2007.
Paraphrasing and Trans-lation.
Ph.D. thesis, University of Edinburgh, Edin-burgh, Scotland.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the web for fine-grained semantic verbrelations.
In EMNLP, volume 2004, pages 33?40.Peter Clark, William R. Murray, John Thompson, PhilHarrison, Jerry Hobbs, and Christiane Fellbaum.2007.
On the role of lexical and world knowledgein rte3.
In Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing, RTE?07, pages 54?59.1520Peter Clark, Myroslava O Dzikovska, Rodney DNielsen, Chris Brew, Claudia Leacock, Danilo Gi-ampiccolo, Luisa Bentivogli, Ido Dagan, and Hoa TDang.
2013.
Semeval-2013 task 7: The joint stu-dent response analysis and 8th recognizing textualentailment challenge.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: an overview.
In Proceedings ofthe Workshop on Geometrical Models of NaturalLanguage Semantics, pages 112?119.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Machine Learning Challenges.
Eval-uating Predictive Uncertainty, Visual Object Classi-fication, and Recognising Tectual Entailment, pages177?190.
Springer.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th international conference onComputational Linguistics, page 350.Juri Ganitkevitch and Chris Callison-Burch.
2014.
Themultilingual paraphrase database.
In The 9th editionof the Language Resources and Evaluation Confer-ence, Reykjavik, Iceland, May.
European LanguageResources Association.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages758?764, Atlanta, Georgia, June.
Association forComputational Linguistics.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third PASCAL recog-nizing textual entailment challenge.
In Proceedingsof the ACL-PASCAL workshop on textual entailmentand paraphrasing, pages 1?9.Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,Stijn De Saeger, Masaki Murata, and Jun?ichiKazama.
2009.
Large-scale verb entailment acqui-sition from the web.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 3-Volume 3, pages 1172?1181.
Association for Computational Linguistics.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th Conference on Computational Linguistics -Volume 2, COLING ?92, pages 539?545.Aaron N Kaplan and Lenhart K Schubert.
2001.Measuring and improving the quality of worldknowledge extracted from wordnet.
University ofRochester, Rochester, NY.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Alice Lai and Julia Hockenmaier.
2014.
Illinois-LH: Adenotational and distributional approach to seman-tics.
SemEval 2014, page 329.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.biometrics, pages 159?174.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
Dis-covery of Inference Rules from Text.
In Proceed-ings of the seventh ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 323?328.
ACM.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational linguistics-Volume 2, pages 768?774.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing, RTE ?07, pages 193?200.Bill MacCartney.
2009.
Natural language inference.Ph.D.
thesis, Citeseer.Nitin Madnani and Bonnie J. Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Computational Linguistics, 36.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014.
Semeval-2014 task 1: Evaluation ofcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
SemEval-2014.Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Pro-ceedings of the Joint Workshop on Automatic Knowl-edge Base Construction and Web-scale KnowledgeExtraction, pages 95?100.Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis,Ion Androutsopoulos, John Pavlopoulos, and SureshManandhar.
2014.
Semeval-2014 task 4: Aspectbased sentiment analysis.
Proceedings of SemEval,Dublin, Ireland.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS, volume 17, pages 1297?1304.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association for Compu-tational Linguistics, ACL-44, pages 801?808.Idan Szpektor and Ido Dagan.
2008.
Learning en-tailment rules for unary templates.
In Proceedingsof the 22Nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08, pages849?856.1521Idan Szpektor, Hristo Tanev, Dr Dagan, BonaventuraCoppola, et al 2004.
Scaling web-based acquisitionof entailment relations.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th InternationalConference on Computational Linguistics, COLING?04.Wei Xu, Alan Ritter, Chris Callison-Burch, William B.Dolan, and Yangfeng Ji.
2014.
Extracting lexicallydivergent paraphrases from Twitter.
Transactions ofthe Association for Computational Linguistics, 2.Jiang Zhao, Tian Tian Zhu, and Man Lan.
2014.
Ecnu:One stone two birds: Ensemble of heterogenousmeasures for semantic relatedness and textual entail-ment.
SemEval 2014, page 271.1522
