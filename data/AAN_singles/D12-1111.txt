Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1212?1222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsPolarity Inducing Latent Semantic AnalysisWen-tau Yih Geoffrey Zweig John C. PlattMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{scottyih,gzweig,jplatt}@microsoft.comAbstractExisting vector space models typically mapsynonyms and antonyms to similar word vec-tors, and thus fail to represent antonymy.
Weintroduce a new vector space representationwhere antonyms lie on opposite sides of asphere: in the word vector space, synonymshave cosine similarities close to one, whileantonyms are close to minus one.We derive this representation with the aid of athesaurus and latent semantic analysis (LSA).Each entry in the thesaurus ?
a word sensealong with its synonyms and antonyms ?
istreated as a ?document,?
and the resulting doc-ument collection is subjected to LSA.
The keycontribution of this work is to show how to as-sign signs to the entries in the co-occurrencematrix on which LSA operates, so as to inducea subspace with the desired property.We evaluate this procedure with the Grad-uate Record Examination questions of (Mo-hammed et al 2008) and find that the methodimproves on the results of that study.
Furtherimprovements result from refining the sub-space representation with discriminative train-ing, and augmenting the training data withgeneral newspaper text.
Altogether, we im-prove on the best previous results by 11 pointsabsolute in F measure.1 IntroductionVector space representations have proven usefulacross a wide variety of text processing applicationsranging from document clustering to search rele-vance measurement.
In these applications, text isrepresented as a vector in a multi-dimensional con-tinuous space, and a similarity metric such as co-sine similarity can be used to measure the related-ness of different items.
Vector space representationshave been used both at the document and word lev-els.
At the document level, they are effective forapplications including information retrieval (Saltonand McGill, 1983; Deerwester et al 1990), docu-ment clustering (Deerwester et al 1990; Xu et al2003), search relevance measurement (Baeza-Yatesand Ribiero-Neto, 1999) and cross-lingual docu-ment retrieval (Platt et al 2010).
At the word level,vector representations have been used to measureword similarity (Deerwester et al 1990; Turney andLittman, 2005; Turney, 2006; Turney, 2001; Lin,1998; Agirre et al 2009; Reisinger and Mooney,2010) and for language modeling (Bellegarda, 2000;Coccaro and Jurafsky, 1998).
While quite success-ful, these applications have typically been consistentwith a very general notion of similarity in whichbasic association is measured, and finer shades ofmeaning need not be distinguished.
For example,latent semantic analysis might assign a high degreeof similarity to opposites as well as synonyms (Lan-dauer and Laham, 1998; Landauer, 2002).Independent of vector-space representations, anumber of authors have focused on identifying dif-ferent kinds of relatedness.
At the simplest level,we may wish to distinguish between synonyms andantonyms, which can be further differentiated.
Forexample, in synonymy, we may wish to distinguishhyponyms and hypernyms.
Moreover, Cruse (1986)notes that numerous kinds of antonymy are possible,for example antipodal pairs like ?top-bottom?
or1212gradable opposites like ?light-heavy.?
Work in thisarea includes (Turney, 2001; Lin et al 2003; Tur-ney and Littman, 2005; Turney, 2006; Curran andMoens, 2002; van der Plas and Tiedemann, 2006;Mohammed et al 2008; Mohammed et al 2011).Despite the existence of a large amount of relatedwork in the literature, distinguishing synonyms andantonyms is still considered as a difficult open prob-lem in general (Poon and Domingos, 2009).In this paper, we fuse these two strands of re-search in an attempt to develop a vector space rep-resentation in which the synonymy and antonymyare naturally differentiated.
We follow Schwab etal.
(2002) in requiring a representation in whichtwo lexical items in an antonymy relation should lieat opposite ends of an axis.
However, in contrastto the logical axes used previously, we desire thatantonyms should lie at the opposite ends of a spherelying in a continuous and automatically induced vec-tor space.
To generate this vector space, we presenta novel method for assigning both negative and pos-itive values to the TF-IDF weights used in latent se-mantic analysis.To determine these signed values, we exploit theinformation present in a thesaurus.
The result is avector space representation in which synonyms clus-ter together, and the opposites of a word tend to clus-ter together at the opposite end of a sphere.This representation provides several advantagesover the raw thesaurus.
First, by finding the itemsmost and least similar to a word, we are able to dis-cover new synonyms and antonyms.
Second, as dis-cussed in Section 5, the representation provides anatural starting point for gradient-descent based op-timization.
Thirdly, as we discuss in Section 6, it isstraightforward to embed new words into the derivedsubspace by using information from a large unsuper-vised text corpus such as Wikipedia.The remainder of this paper is organized as fol-lows.
Section 2 describes previous work.
Section 3presents the classical LSA approach and analyzessome of its limitations.
In Section 4 we present ourpolarity inducing extension to LSA.
Section 5 fur-ther extends the approach by optimizing the vectorspace representation with supervised discriminativetraining.
Section 6 describes the proposed method ofembedding new words in the thesaurus-derived sub-space.
The experimental results of Section 7 indi-cate that the proposed method outperforms previousapproaches on a GRE test of closest-opposites (Mo-hammed et al 2008).
Finally, Section 8 concludesthe paper.2 Related WorkThe detection of antonymy has been studied in anumber of previous papers.
Mohammed et al(2008)approach the problem by combining informationfrom a published thesaurus with corpus statistics de-rived from the Google n-gram corpus (Brants andFranz, 2006).
Their method consists of two mainsteps: first, detecting contrasting word categories(e.g.
?WORK?
vs. ?ACTIVITY FOR FUN?)
andthen determining the degree of antonymy.
Cate-gories are defined by a thesaurus; contrasting cat-egories are found by using affix rules (e.g., un- &dis-) and WordNet antonymy links.
Words belong-ing to contrasting categories are treated as antonymsand the degree of contrast is determined by distri-butional similarity.
Mohammed et al(2008) alsoprovides a publicly available dataset for detection ofantonymy, which we have adopted.
This work hasbeen extended in (Mohammed et al 2011) to in-clude a study of antonymy based on crowd-sourcingexperiments.Turney (2008) proposes a unified approach tohandling analogies, synonyms, antonyms and asso-ciations by transforming the last three cases intocases of analogy.
A supervised learning methodis then used to solve the resulting analogical prob-lems.
This is evaluated on a set of 136 ESL ques-tions.
Lin et al(2003) builds on (Lin, 1998) andidentifies antonyms as semantically related wordswhich also happen to be found together in a databasein pre-identified phrases indicating opposition.
Linet al(2003) further note that whereas synonymswill tend to translate to the same word in anotherlanguage, antonyms will not.
This observation isused to select antonyms from amongst distribution-ally similar words.
Antonymy is used in (de Si-mone and Kazakov, 2005) for document clusteringand (Harabagiu et al 2006) to find contradiction.The automatic detection of synonyms has beenmore extensively studied.
Lin (1998) presentsa thorough comparison of word-similarity metricsbased on distributional similarity, where this is de-1213termined from co-occurrence statistics in depen-dency triples extracted by parsing a large dataset.Related studies are described in (Curran and Moens,2002; van der Plas and Bouma, 2005).
Later, vander Plas and Tiedemann (2006) extend the use ofmultilingual data present in Lin et al(2003) by mea-suring distributional similarity based on the contextsthat a word occurs in once translated into a new lan-guage.
This is used to improve the precision/recallcharacteristics on synonym pairs.
Structured infor-mation can be important in determining relatedness,and thesauri and Wikipedia links have been studiedin (Milne and Witten, 2008; Jarmasz and Szpakow-icz, 2003).
Combinations of approaches are studiedin (Turney et al 2003).Vector-space models and latent semantic analysisin particular have a long history of use in synonymdetection, which in fact was suggested in some ofthe earliest LSA papers.
Deerwester et al(1990)defines a metric for measuring word similarity basedon LSA, and it has been used in (Landauer and Du-mais, 1997; Landauer et al 1998) to answer wordsimilarity questions derived from the Test of Englishas a Foreign Language (TOEFL).
Turney (2001)proposes the use of point-wise mutual information inconjunction with LSA, and again presents results onsynonym questions derived from the TOEFL.
Vari-ants of vector space models are further analyzedin (Turney and Littman, 2005; Turney, 2006; Tur-ney and Pantel, 2010).3 Latent Semantic AnalysisLatent Semantic Analysis (Deerwester et al 1990)is a widely used method for representing words anddocuments in a low dimensional vector space.
Themethod is based on applying singular value decom-position (SVD) to a matrix W which indicates theoccurrence of words in documents.
To performLSA, one proceeds as follows.
The input is a col-lection of d documents which are expressed in termsof words from a vocabulary of size n. These docu-ments may be actual documents such as newspaperarticles, or simply notional documents such as sen-tences, or any other collection in which words aregrouped together.
Next, a d?n document-term ma-trix W is formed1.
At its simplest form, the ijthentry contains the number of times word j has oc-curred in document i ?
its term frequency or TFvalue.
More conventionally, the entry is weightedby some notion of the importance of word j, for ex-ample the negative logarithm of the fraction of doc-uments that contain it, resulting in a TF-IDF weight-ing (Salton et al 1975).
The similarity between twodocuments can be computed using the cosine simi-larity of their corresponding row vectors:sim(x,y) =x ?
y?
x ??
y ?Similarly, the cosine similarity of two column vec-tors can be used to judge the similarity of the corre-sponding words.
Finally, to obtain a subspace repre-sentation of dimension k, W is decomposed asW ?
USV Twhere U is d ?
k, V T is k ?
n, and S is a k ?
kdiagonal matrix.
In applications, k  n and k  d;for example one might have a 50, 000 word vocab-ulary and 1, 000, 000 documents and use a 300 di-mensional subspace representation.An important property of SVD is that the columnsof SV T ?
which now represent the words ?
behavesimilarly to the original columns of W , in the sensethat the cosine similarity between two columns inSV T approximates the cosine similarity between thecorresponding columns in W .
This follows fromthe observation that W TW = V S2V T , and the factthat the ijth entry of W TW is the dot product ofthe ith and jth columns (words) in W .
We willuse this observation subsequently in the derivationof polarity-inducing LSA.
For efficiency, we nor-malize the columns of SV T to unit length, allow-ing the cosine similarity between two words to becomputed with a single dot-product; this also has theproperty of mapping each word to a point on a multi-dimensional sphere.A second important property of LSA is that in theword representations which result can by viewed asthe result of applying a projection matrix U to theoriginal vectors as:UTW = SV T1(Bellegarda, 2000) constructs the transpose of this, but wehave found it convenient in data processing for documents torepresent rows.1214In Section 5, we will viewU simply as a d?k matrixlearned through gradient descent so as to optimizean objective function.3.1 Limitation of LSAWord similarity as determined by LSA assigns highvalues to words which tend to co-occur in doc-uments.
However, as noted by (Landauer andLaham, 1998; Landauer, 2002), there is no no-tion of antonymy; words with low or negative co-sine scores are simply unrelated.
In comparison,words with high cosine similarity scores are typi-cally semantically related, which includes both syn-onyms and antonyms, as contrasting words often co-occur (Murphy and Andrew, 1993; Mohammed etal., 2008).
To illustrate this, we have performedSVD with the aid of the Encarta thesaurus developedby Bloomsbury Publishing Plc.
This thesaurus con-tains approximately 47k word senses and a vocab-ulary of 50k words and phrases.
Each ?document?is taken to be the thesaurus entry for a word-sense,including synonyms and antonyms.
For example,the word ?admirable?
induces a document consist-ing of {admirable, estimable, commendable, vener-able, good, splendid, worthy, marvelous, excellent,unworthy}.
Note that the last word in this set is itsantonym.
Performing SVD on this set of thesaurusderived ?meaning-documents?
results in a subspacerepresentation for each word.
This form of LSA issimilar to the use of Wikipedia in (Gabrilovich andMarkovitch, 2007).Table 1 shows some words, their original the-saurus documents, and the most and least similarwords in the LSA subspace.
Several properties areapparent:?
The vector-space representation of words isable to identify related words that are not ex-plicitly present in the original thesaurus.
Forexample, ?meritorious?
for ?admirable?
?
ar-guably better than any of the words given in thethesaurus itself.?
Similarity is based on co-occurrence, so theco-occurrence of antonyms in the thesaurus-derived documents induces their presence asLSA-similar words.
For example, ?con-temptible?
is identified as similar to ?ad-mirable.?
In the case of ?mourning,?
oppositesacrimony rancor goodwill affectionacrimony 1 1 1 1affection 1 1 1 1Table 2: The W matrix for two thesaurus entries inits original form.
Rows represent documents; columnswords.acrimony rancor goodwill affectionacrimony 1 1 -1 -1affection -1 -1 1 1Table 3: The W matrix for two thesaurus entries in itspolarity-inducing form.such as ?joy?
and ?elation?
actually dominatethe list of LSA-similar words.?
The LSA-least-similar words have no relation-ship at all to the word they are least-similar to.For example, the least-similar word to ?consid-ered?
is ?ready-made-meal.
?In the next section, we will present a method forinducing polarity in LSA subspaces, where oppositewords will tend to have negative cosine similarities,analogous to the positive similarities of synonyms.Thus, the least-similar words to a given word will beits opposites.4 Polarity Inducing LSAWe modify LSA so that we may exploit a thesaurusto embed meaningful axes in the induced subspacerepresentation.
Words with opposite meaning willlie at opposite positions on a sphere.
Recall that thecosine similarity between word-vectors in the orig-inal matrix W are preserved in the subspace repre-sentation of words.
Thus, if we construct the originalmatrix so that the columns representing antonymswill tend to have negative cosine similarities whilecolumns representing synonyms will tend to havepositive similarities, we will achieve the desired be-havior.This can be achieved by negating the TF-IDF en-tries for the antonyms of a word when constructingW from the thesaurus, which is illustrated in Ta-bles 2 and 3.
The two rows in these tables corre-spond to thesaurus entries for the sense-categories1215Word Thesaurus Entry LSA Most-Similar Words LSA Least-Similar Wordsadmirable estimable, commendable,venerable, good, splen-did, worthy, marvelous,excellent, unworthycommendable, creditable,laudable, praiseworthy,worthy, meritorious,scurvy, contemptible,despicable, estimableeasy-on-the-eye, peace-keeper, peace-lover,conscientious-objector,uninviting, dishy, dessert,pudding, seductiveconsidered careful, measured, well-thought-out, painstaking,rashcalculated, premeditated,planned, tactical, strate-gic, thought-through, in-tentional, fortuitous, pur-poseful, unpremeditatedready-made-meal, ready-meal, disposed-to, apt-to,wild-animals, big-game,game-birds, game-fish,rugger, rugbymourning grief, bereavement, sor-row, sadness, lamenta-tion, woe, grieving, exul-tationsorrowfulness, anguish,exultation, rejoicing, ju-bilation, glee, heartache,travail, joy, elationmuckiness, turn-the-corner, impassibility,filminess, pellucidity,limpidity, sheernessTable 1: LSA on a thesaurus.
Thesaurus entries include antonyms in italics.?acrimony,?
and ?affection.?
The thesaurus entriesinduce two ?documents?
containing the words andtheir synonyms and antonyms.
The complete set ofwords is acrimony, rancor, goodwill, affection.
Forsimplicity, we assume that all TF-IDF weights are1.
In the original LSA formulation, we have the rep-resentation of Table 2.
?Rancor?
is listed as a syn-onym of ?acrimony,?
which has ?goodwill?
and ?af-fection?
as its antonyms.
This results in the first row.Note that the cosine similarity between every pair ofwords (columns) is 1.Table 3 shows the polarity-inducing representa-tion.
Here, the cosine similarity between synony-mous words (columns) is 1, and the cosine similaritybetween antonymous words is -1.
Since LSA tendsto preserve cosine similarities between words, in theresulting subspace we may expect to find meaning-ful axes, where opposite senses map to opposite ex-tremes.
We refer to this as polarity-inducing LSA orPILSA.In Table 4, we show the PILSA-similar andPILSA-least-similar words for the same words as inTable 1.
We see now that words which are leastsimilar in the sense of having the lowest cosine-similarity are indeed opposites.
In this table gen-erally the most similar words have similarities in therange of 0.7 to 1.0 and the least similar in the rangeof -0.7 to -1.0.5 Discriminative TrainingAlthough the cosine similarity of LSA-derived wordvectors are generally very effective in applicationssuch as judging the relevance of words or docu-ments, or detecting antonyms as in our construction,the process of singular value decomposition in LSAdoes not explicitly try to achieve such goals.
In thissection, we see that when supervised training data isavailable, the projection matrix of LSA can be en-hanced through a discriminative training techniqueexplicitly designed to create a representation suitedto a specific task.Because LSA is closely related to principle com-ponent analysis (PCA), extensions of PCA such ascanonical correlation analysis (CCA) and orientedprinciple component analysis (OPCA) can leveragethe labeled data and produce the projection matrixthrough general eigen-decomposition (Platt et al2010).
Along this line of work, Yih et al(2011)proposed a Siamese neural network approach calledS2Net, which tunes the projection matrix directlythrough gradient descent, and has shown to outper-form other methods in several tasks.
Below we de-scribe briefly this technique and explain how weadopt it for the task of antonym detection.The goal of S2Net is to learn a concept vectorrepresentation of the original sparse term vectors.Although such transformation can be non-linear ingeneral, its current design chooses the model formto be a linear projection matrix, which is identical to1216Word PILSA-Similar Words PILSA-Least-Similar Wordsadmirable commendable, creditable, laudable,praiseworthy, worthy, meritorious, es-timable, deserving, tiptop, valuedscurvy, contemptible, despicable,lamentable, shameful, reprehensible,unworthy, disgraceful, discreditable,undeservingconsidered calculated, premeditated, planned, tac-tical, strategic, thought-through, inten-tional, purposeful, intended, psycho-logicalfortuitous, unpremeditated, unconsid-ered, off-your-own-bat, unintended,undirected, objectiveless, hit-and-miss,unforced, involuntarymourning sorrowful, doleful, sad, miserable,wistful, pitiful, wailing, sobbing,heavy-hearted, forlornsmiley, happy, blissful, wooden, mirth-ful, joyful, deadpan, fulfilled, straight-faced, contentTable 4: PILSA on a thesaurus.
Thesaurus entries are as in Table 1.that of LSA, PCA, OPCA or CCA.
Given a d-by-1input vector f , the model of S2Net is a d-by-k ma-trix A = [aij ]d?k, which maps f to a k-by-1 outputvector g = AT f .
The fact that the transformationcan be viewed as a two-layer neural network leadsto the method?s name.What differentiates S2Net from other approachesis its loss function and optimization process.
Inthe ?parallel text?
setting, the labeled data con-sists of pairs of similar text objects such as doc-uments.
The objective of the training process isto assign higher cosine similarities to these pairscompared to others.
More specifically, suppose thetraining set consists of m pairs of raw input vectors{(fp1 , fq1), (fp2 , fq2), ?
?
?
, (fpm , fqm)}.
Given a pro-jection matrix A, the similarity score of any pair ofobjects is simA(fpi , fqj ) = cosine(AT fpi ,AT fqj ).Let ?ij = simA(fpi , fqi) ?
simA(fpi , fqj ) be thedifference of the similarity scores of (fpi , fqi) and(fpi , fqj ).
The learning procedure tries to increase?ij by using the following logistic loss:L(?ij ;A) = log(1 + exp(??
?ij)),where ?
is a scaling factor that adjusts the loss func-tion2.
The loss of the whole training set is thus:1m(m?
1)?1?i,j?m,i 6=jL(?ij ;A)Parameter learning (i.e., tuning A) can be done2As suggested in (Yih et al 2011), ?
is set to 10 in ourexperiments.by standard gradient-based methods, such as L-BFGS (Nocedal and Wright, 2006).The original setting of S2Net can be directly ap-plied to finding synonymous words, where the train-ing data consists of pairs of vectors representingtwo synonyms.
It is also easy to modify the lossfunction to apply it to the antonym detection prob-lem.
We first sample pairs of antonyms from thethesaurus to create the training data.
The raw inputvector f of a selected word is its corresponding col-umn vector of the document-term matrix W (Sec-tion 3) after inducing polarity (Section 4).
Wheneach pair of vectors in the training data representstwo antonyms, we can redefine ?ij by flipping thesign: ?ij = simA(fpi , fqj ) ?
simA(fpi , fqi), andleave others unchanged.
As the loss function encour-ages ?ij to be larger, an antonym pair will tend tohave a smaller cosine similarity than other pairs.
Be-cause S2Net uses a gradient descent technique and anon-convex objective function, it is sensitive to ini-tialization, and we have found that the PILSA pro-jection matrix U (Section 3) provides an excellentstarting point.
As illustrated in Section 7, learningthe word vectors with S2Net produces a significantimprovement over PILSA alone.6 Extending PILSA to Out-of-thesaurusWordsWhile PILSA is effective at representing synonymand antonym information, in its pure form, it is lim-ited to the vocabulary of the thesaurus.
In order toextend PILSA to operate on out-of-thesaurus words,1217we employ a two-stage strategy.
We first conductsome lexical analysis and try to match an unknownword to one or more in-thesaurus words in their lem-matized forms.
If no such match can be found,we then attempt to find semantically related in-thesaurus words by leveraging co-occurrence statis-tics from general text data.
These two steps are de-scribed in detail below.6.1 Matching via Lexical AnalysisWhen a target word is not included in a thesaurus, itis quite often that some of its morphological varia-tions are covered.
For example, although the Encartathesaurus does not have the word ?corruptibility,?it does contain other forms like ?corruptible?
and?corruption.?
Replacing the out-of-thesaurus targetword with these morphological variations may alterthe part-of-speech but typically does not change themeaning3.Given an out-of-thesaurus target word, we firstapply a morphological analyzer for English devel-oped by Minnen et al(2001), which removes theinflectional affixes and returns the lemma.
If thelemma still does not exist in the thesaurus, we thenapply Porter?s stemmer (Porter, 1980) and checkwhether the target word can match any of the in-thesaurus words in their stemmed forms.
A sim-ple rule that checks whether removing hyphens fromwords can lead to a match and whether the targetword occurs as part of a compound word in the the-saurus is applied when both morphological analysisand stemming fail to find a match.
When there aremore than one matched words, the centroid of theirPILSA vectors is used to represent the target word.When there is only one matched word, the matchedword is treated as the target word.6.2 Leveraging General Text DataIf no words in the thesaurus can be linked to thetarget word through the simple lexical analysis pro-cedure, we try to find matched words by creatinga context vector space model from a large docu-ment collection, and then mapping from this spaceto the PILSA space.
We use contexts because of thedistributional hypothesis ?
words that occur in thesame contexts tend to have similar meaning (Harris,3The rules we use based on lexical analysis are moderatelyconservative to avoid mistakes like mapping hopeless to hope.1954).
When a word is not in the thesaurus but ap-pears in the corpus, we predict its PILSA vector rep-resentation from the context vector space model byusing its k-nearest neighbors which are in the the-saurus and consistent with each other.6.2.1 Context Vector Space ModelGiven a corpus of documents, we construct theraw context vectors as follows.
For each target word,we first create a bag of words by collecting all theterms within a window of [-10,+10] centered at eachoccurrence of the target word in the corpus.
Thenon-identical terms form a term-vector, where eachterm is weighted using its TF-IDF value.
We thenperform LSA on the context-word matrix.
The se-mantic similarity/relatedness of two words can thenbe determined using the cosine similarity of theircorresponding LSA word vectors.
In the followingtext, we refer to this LSA context vector space modelas the corpus space, in contrast to the PILSA the-saurus space.6.2.2 Embedding Out-of-Vocabulary WordsGiven the context space model, we may use alinear regression or a k-nearest neighbors approachto embed out-of-thesaurus words into the thesaurus-space representation.
However, as near words in thecontext space may be synonyms in addition to othersemantically related words (including antonyms),such approaches can potentially be noisy.
For ex-ample, words like ?hot?
and ?cold?
may be closeto each other in the context space due to their sim-ilar usage in text.
An affine transform cannot ?tearspace?
and map them to opposite poles in the the-saurus space.Therefore, we propose a revised k-nearest neigh-bors approach.
Suppose we are interested in an out-of-thesaurus word w. We first find K-nearest in-thesaurus neighbors to w in the context space.
Wethen select a subset of k members of these K wordssuch that the pairwise similarity of each of the kmembers with every other member is positive.
Thethesaurus-space centroid of these k items is com-puted as w?s representation.
This procedure has theproperty that the k nearby words used to form theembedding of a non-thesaurus word are selected tobe consistent with each other.
In practice, we usedK = 10 and k = 3, which requires only around12181000 pairwise computations even done in a brute-force way.
To provide a concrete example, if wehad the out-of-thesaurus word ?sweltering?
with in-thesaurus neighbors ?hot, cold, burning, scorching,...?
the procedure would return the centroid of ?hot,burning, scorching?
and exclude ?cold.
?7 Experimental ValidationIn this section, we present our experimental resultson applying PILSA and its extensions to answeringthe closest-opposite GRE questions.7.1 Data ResourcesThe primary thesaurus we use is the Encarta The-saurus developed by Bloomsbury Publishing Plc4.Our version of this has approximately 47k wordsenses and a vocabulary of 50k words, and con-tains 125,724 pairs of antonyms.
To experiment withthe effect of using a different thesaurus, we usedWordNet as an information source.
Each synset inWordNet maps to a row in the document-term ma-trix; synonyms in a synset are weighted with posi-tive TFIDF values, and antonyms are weighted neg-ative TFIDF values.
Entries corresponding to otherwords in the vocabulary are 0.
WordNet providessignificantly greater coverage with approximately227k synsets involving multiple words, and a vo-cabulary of about 190k words.
However, it is alsomuch sparser, with 5.3 words per sense on averageas opposed to 10.3 in the thesaurus, and has only62,821 pairs of antonyms.
As general text data foruse in embedding out-of-vocabulary words, we useda Nov-2010 dump of English Wikipedia, which con-tains approximately 917M words.7.2 Development and Test DataFor testing, we use the closest-opposite questionsfrom GRE tests provided by (Mohammed et al2008).
Each question contains a target word andfive choices, and asks which of the choice words hasthe most opposite meaning to the target word.
Twodatasets are made publicly available by Mohammadet al(2008): the development set, which consists of162 questions, and the test set, which has 950 ques-tions5.
We considered making our own, more exten-4http://www.bloomsbury.com/5http://www.umiacs.umd.edu/?saif/WebDocs/LC-data/{devset,testset}.txtDimensions Bloomsbury Prec.
WordNet Prec.50 0.778 0.475100 0.850 0.563200 0.856 0.569300 0.863 0.625400 0.843 0.625500 0.843 0.613750 0.830 0.6131000 0.837 0.5442000 0.784 0.5193000 0.778 0.494Table 5: The performance of PILSA vs. the number of di-mensions when applied to the closest-opposite questionsfrom the GRE development set.
Out of the 162 ques-tions, using the Bloomsbury thesaurus data we are ableto answer 153 of them.
Using 300 dimensions gives thebest precision (132/153 = 0.863).
This dimension set-ting is also optimal when using the WordNet data, whichanswers 100 questions correctly out of the 160 attempts(100/160 = 0.625).sive, test ?
for example one which would require theuse of sentence context to choose between relatedyet distributionally different antonyms (e.g.
?little,small?
as antonyms of ?big?)
but chose to stick to apreviously used benchmark.
This allows the directcomparison with previously reported methods.Some of these questions contain very rarely usedtarget or choice words, which are not included inthe thesaurus vocabulary.
In order to provide a faircomparison to existing methods, we do not try torandomly answer these questions.
Instead, when thetarget word is out of vocabulary, we skip the wholequestion.
When the target word is in vocabulary butone or more choices are unknown words, we ignorethose unknown words and pick the word with thelowest cosine similarity from the rest as the answer.The results of our methods are reported in precision(the number of questions answered correctly dividedby the number of questions attempted), recall (thenumber of questions answered correctly divided bythe number of all questions) and F1 (the harmonicmean of precision and recall)6.
We now turn to anin-depth evaluation.6Precision/recall/F1 were used in (Mohammed et al 2008)as when their system ?did not find any evidence of antonymybetween the target and any of its alternatives, then it refrainedfrom attempting that question.?
We adopt this convention toprovide a fair comparison to their system.1219Dev.
Set Test SetPrec Rec F1 Prec Rec F1WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77Bloomsbury PILSA + S2Net 0.89 0.84 0.86 0.84 0.77 0.80Bloomsbury PILSA + S2Net + Embedding 0.88 0.87 0.87 0.81 0.80 0.81(Mohammed et al 2008) 0.76 0.66 0.70 0.76 0.64 0.70Table 6: The overall results.
PILSA performs LSA on the signed TF-IDF vectors.7.3 Basic PILSAWhen applying PILSA, we need to determine thenumber of dimensions in the projected space.
Eval-uated on the GRE development set, Table 5 showsthe precision of PILSA, using two different trainingdatasets, Bloomsbury and WordNet, at different di-mensions.The Bloomsbury-based system is able to answer153 questions, and the best dimension setting is300, which answers 132 questions correctly and thusarchives 0.863 in precision.
In contrast, the largervocabulary in WordNet helps the system answer 160questions but the quality is not as good.
We finddimensions 300 and 400 are equally good, whereboth answer 100 questions correctly (0.625 in pre-cision)7.
Because a lower number of dimensionsis preferred for saving storage space and computingtime, we choose 300 as the number of dimensions inPILSA.We now compare the proposed methods.
All re-sults are summarized in Table 6.
When evaluated onthe GRE test set, the Bloomsbury thesaurus-basedmethods (Lines 4?7) attempted 865 questions.
Theprecision, recall and F1 of the Bloomsbury-basedPILSA model (Line 6) are 0.81, 0.74 and 0.77,which are all better than the best reported methodin (Mohammed et al 2008)8.
In contrast, theWordNet-based methods (Lines 1?3) attempted 9367Note that the number of questions attempted is not a func-tion of the number of dimensions.8We take a conservative approach and assume that skippedquestions are answered incorrectly.
The difference is statisti-cally significant at 99% confidence level using a binomial test.questions.
However, consistent with what we ob-served on the development set, the WordNet-basedmodel is inferior.
Its precision, recall and F1 onthe test set are 0.60, 0.60 and 0.60 (Line 3).
Al-though the quality of the data source plays an im-portant role, we need to emphasize that performingLSA using our polarity inducing construction is infact a critical step in enhancing the model perfor-mance.
For example, directly using the antonym setsin the Bloomsbury thesaurus gives 0.59 in F1 (Line4), while using cosine similarity on the signed vec-tors prior to LSA only reaches 0.60 in F1 (Line 5).7.4 Improving Precision with DiscriminativeTrainingBuilding on the success of the unsupervised PILSAmodel, we refine the projection matrix.
As describedin Section 5, we take the PILSA projection matrixas the initial model in S2Net and train the modelusing 20,517 pairs of antonyms sampled from theBloomsbury thesaurus.
A separate sample of 5,000antonym pairs is used as the validation set for hyper-parameter tuning in regularization.
Encouragingly,we found that the already strong results of PILSAcan indeed be improved, which gives 3 more pointsin both precision (0.84), recall (0.77) and F1 (0.80).7.5 Improving Recall with Unsupervised DataWe next evaluate our approach of extending theword coverage with the help of an external text cor-pus, as well as the lexical analysis procedure.
Usingthe Bloomsbury PILSA-S2Net thesaurus space andthe Wikipedia corpus space, our method increases1220recall by 3 points on the test set.
Compared to the in-vocabulary only setting, it attempted 75 more ques-tions (865?
940) and had 33 of them correctly an-swered.While the accuracy on these questions is muchhigher than random, the fact that it is substantiallybelow the precision of the original indicates someroom for improvement.
We notice that the out-of-thesaurus words are either offensive words excludedin the thesaurus (e.g., moronic) or some very rarelyused words (e.g., froward).
When the lexical analy-sis procedure fails to match the target word to somein-thesaurus words, the context vector embeddingapproach solves the former case, but has difficultyin handling the latter.
The main reason is that suchwords occur very infrequently in a general corpus,which result in significant uncertainty in their se-mantic vectors.
Other than using a much largercorpus, approaches that leverage character n-gramsmay help.
We leave this as future work.8 ConclusionIn this paper we have tackled the problem of find-ing a vector-space representation of words where,by construction, synonyms and antonyms are easyto distinguish.
Specifically, we have defined a wayof assigning sign to the entries in the co-occurrencematrix on which LSA operates, such that synonymswill tend to have positive cosine similarity, andantonyms will tend to have negative similarities.
Tothe best of our knowledge, our method of inducingpolarity to the document-term matrix before apply-ing LSA is novel and has shown to effectively pre-serve and generalize the synonymous/antonymousinformation in the projected space.
With this vectorspace representation, we were able to bring to bearthe machinery of discriminative training in order tofurther optimize the word representations.
Finally,by using the notion of closeness in this space, wewere able to embed new out-of-vocabulary wordsinto the space.
On a standard test set, the proposedmethods improved the F measure by 11 points abso-lute over previous results.AcknowledgmentsWe thank Susan Dumais for her thoughtful com-ments, Silviu-Petru Cucerzan for preparing theWikipedia data, and Saif Mohammed for sharing theGRE datasets.
We are also grateful to the anony-mous reviewers for their useful suggestions.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceedingsof HLT-NAACL, pages 19?27.Ricardo Baeza-Yates and Berthier Ribiero-Neto.
1999.Modern Information Retrieval.
Addison-Wesley.J.
Bellegarda.
2000.
Exploiting latent semantic informa-tion in statistical language modeling.
Proceedings ofthe IEEE, 88(8).Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium.N.
Coccaro and D. Jurafsky.
1998.
Towards better in-tegration of semantic predictors in statistical languagemodeling.
In Proceedings, International Conferenceon Spoken Language Processing (ICSLP-98).D.
A. Cruse.
1986.
Lexical Semantics.
Cambridge Uni-versity Press.James R. Curran and Marc Moens.
2002.
Improvementsin automatic thesaurus extraction.
In Proceedings ofthe ACL-02 workshop on Unsupervised lexical acqui-sition - Volume 9, pages 59?66.
Association for Com-putational Linguistics.Thomas de Simone and Dimitar Kazakov.
2005.
Usingwordnet similarity and antonymy relations to aid doc-ument retrieval.
In Recent Advances in Natural Lan-guage Processing (RANLP).S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(96).E.
Gabrilovich and S. Markovitch.
2007.
Computing se-mantic relatedness using wikipedia-based explicit se-mantic analysis.
In AAAI Conference on Artificial In-telligence (AAAI).Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast and contradiction in text pro-cessing.
In AAAI Conference on Artificial Intelligence(AAAI).Zelig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Mario Jarmasz and Stan Szpakowicz.
2003.
Rogets the-saurus and semantic similarity.
In Proceedings of theInternational Conference on Recent Advances in Nat-ural Language Processing (RANLP-2003).1221Thomas Landauer and Susan Dumais.
1997.
A solutionto plato?s problem: The latent semantic analysis the-ory of the acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2), pages 211?240.T.K.
Landauer and D. Laham.
1998.
Learning human-like knowledge by singular value decomposition: Aprogress report.
In Neural Information ProcessingSystems (NIPS).Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25, pages 259?284.T.K.
Landauer.
2002.
On the computational basis oflearning and cognition: Arguments from lsa.
Psychol-ogy of Learning and Motivation, 41:43?84.Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.2003.
Identifying synonyms among distributionallysimilar words.
In International Joint Conference onArtificial Intelligence (IJCAI).Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics - Volume 2, ACL ?98, pages 768?774, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.David Milne and Ian H. Witten.
2008.
An effective low-cost measure of semantic relatedness obtained fromwikipedia links.
In Proceedings of the AAAI 2008Workshop on Wikipedia and Artificial Intelligence.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of english.
Natural Lan-guage Engineering, 7(3):207?223.Saif Mohammed, Bonnie Dorr, and Graeme Hirst.
2008.Computing word pair antonymy.
In Empirical Meth-ods in Natural Language Processing (EMNLP).Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2011.
Measuring degrees of seman-tic opposition.
Technical report, National ResearchCouncil Canada.Gregory L. Murphy and Jane M. Andrew.
1993.
Theconceptual basis of antonymy and synonymy in adjec-tives.
Journal of Memory and Language, 32(3):1?19.Jorge Nocedal and Stephen Wright.
2006.
NumericalOptimization.
Springer, 2nd edition.John Platt, Kristina Toutanova, and Wen-tau Yih.
2010.Translingual document representations from discrimi-native projections.
In Proceedings of EMNLP, pages251?261.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Empirical Methods in Nat-ural Language Processing (EMNLP).Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InProceedings of HLT-NAACL, pages 109?117.Gerard Salton and Michael J. McGill.
1983.
Introductionto Modern Information Retrieval.
McGraw Hill.G.
Salton, A. Wong, and C. S. Yang.
1975.
A VectorSpace Model for Automatic Indexing.
Communica-tions of the ACM, 18(11).D.
Schwab, M. Lafourcade, and V. Prince.
2002.Antonymy and conceptual vectors.
In InternationalConference on Computational Linguistics (COLING).Peter Turney and Michael Littman.
2005.
Corpus-basedlearning of analogies and semantic relations.
MachineLearning, 60 (1-3), pages 251?278.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, (37).Peter D. Turney, Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining independentmodules to solve multiple-choice synonym and anal-ogy problems.
In Recent Advances in Natural Lan-guage Processing (RANLP).Peter D. Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In European Conference onMachine Learning (ECML).P.
D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Peter Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In In-ternational Conference on Computational Linguistics(COLING).Lonneke van der Plas and Gosse Bouma.
2005.
Syntac-tic contexts for finding semantically similar words.
InProceedings of the Meeting of Computational Linguis-tics in the Netherlands 2004 (CLIN).Lonneke van der Plas and Jo?rg Tiedemann.
2006.
Find-ing synonyms using automatic word alignment andmeasures of distributional similarity.
In Proceedingsof the COLING/ACL on Main conference poster ses-sions, COLING-ACL ?06, pages 866?873.
Associa-tion for Computational Linguistics.Wei Xu, Xin Liu, and Yihong Gong.
2003.
Documentclustering based on non-negative matrix factorization.In Proceedings of the 26th annual international ACMSIGIR conference on Research and development in in-formaion retrieval, pages 267?273, New York, NY,USA.
ACM.Wen-tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning discriminativeprojections for text similarity measures.
In Proceed-ings of the Fifteen Conference on Computational Nat-ural Language Learning (CoNLL), pages 247?256,Portland, Oregon, USA.1222
