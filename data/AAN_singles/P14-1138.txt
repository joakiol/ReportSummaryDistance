Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470?1480,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRecurrent Neural Networks for Word Alignment ModelAkihiro Tamura?, Taro Watanabe, Eiichiro SumitaNational Institute of Information and Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPANa-tamura@ah.jp.nec.com,{taro.watanabe, eiichiro.sumita}@nict.go.jpAbstractThis study proposes a word alignmentmodel based on a recurrent neural net-work (RNN), in which an unlimitedalignment history is represented by re-currently connected hidden layers.
Weperform unsupervised learning usingnoise-contrastive estimation (Gutmannand Hyv?arinen, 2010; Mnih and Teh,2012), which utilizes artificially generatednegative samples.
Our alignment model isdirectional, similar to the generative IBMmodels (Brown et al, 1993).
To overcomethis limitation, we encourage agreementbetween the two directional models byintroducing a penalty function that en-sures word embedding consistency acrosstwo directional models during training.The RNN-based model outperformsthe feed-forward neural network-basedmodel (Yang et al, 2013) as well as theIBM Model 4 under Japanese-Englishand French-English word alignmenttasks, and achieves comparable transla-tion performance to those baselines forJapanese-English and Chinese-Englishtranslation tasks.1 IntroductionAutomatic word alignment is an important task forstatistical machine translation.
The most classicalapproaches are the probabilistic IBM models 1-5(Brown et al, 1993) and the HMM model (Vogelet al, 1996).
Various studies have extended thosemodels.
Yang et al (2013) adapted the Context-Dependent Deep Neural Network for HMM (CD-DNN-HMM) (Dahl et al, 2012), a type of feed-forward neural network (FFNN)-based model, to?The first author is now affiliated with KnowledgeDiscovery Research Laboratories, NEC Corporation, Nara,Japan.the HMM alignment model and achieved state-of-the-art performance.
However, the FFNN-basedmodel assumes a first-order Markov dependencefor alignments.Recurrent neural network (RNN)-based modelshave recently demonstrated state-of-the-art per-formance that outperformed FFNN-based modelsfor various tasks (Mikolov et al, 2010; Mikolovand Zweig, 2012; Auli et al, 2013; Kalchbrennerand Blunsom, 2013; Sundermeyer et al, 2013).An RNN has a hidden layer with recurrent con-nections that propagates its own previous signals.Through the recurrent architecture, RNN-basedmodels have the inherent property of modelinglong-span dependencies, e.g., long contexts, in in-put data.
We assume that this property would fitwith a word alignment task, and we propose anRNN-based word alignment model.
Our modelcan maintain and arbitrarily integrate an alignmenthistory, e.g., bilingual context, which is longerthan the FFNN-based model.The NN-based alignment models are super-vised models.
Unfortunately, it is usually dif-ficult to prepare word-by-word aligned bilingualdata.
Yang et al (2013) trained their model fromword alignments produced by traditional unsuper-vised probabilistic models.
However, with thisapproach, errors induced by probabilistic mod-els are learned as correct alignments; thus, gen-eralization capabilities are limited.
To solve thisproblem, we apply noise-contrastive estimation(NCE) (Gutmann and Hyv?arinen, 2010; Mnihand Teh, 2012) for unsupervised training of ourRNN-based model without gold standard align-ments or pseudo-oracle alignments.
NCE artifi-cially generates bilingual sentences through sam-plings as pseudo-negative samples, and then trainsthe model such that the scores of the original bilin-gual sentences are higher than those of the sam-pled bilingual sentences.Our RNN-based alignment model has a direc-1470tion, such as other alignment models, i.e., from f(source language) to e (target language) and frome to f .
It has been proven that the limitation maybe overcome by encouraging two directional mod-els to agree by training them concurrently (Ma-tusov et al, 2004; Liang et al, 2006; Grac?a et al,2008; Ganchev et al, 2008).
The motivation forthis stems from the fact that model and generaliza-tion errors by the two models differ, and the mod-els must complement each other.
Based on thismotivation, our directional models are also simul-taneously trained.
Specifically, our training en-courages word embeddings to be consistent acrossalignment directions by introducing a penalty termthat expresses the difference between embeddingof words into an objective function.
This con-straint prevents each model from overfitting to aparticular direction and leads to global optimiza-tion across alignment directions.This paper presents evaluations of Japanese-English and French-English word alignment tasksand Japanese-to-English and Chinese-to-Englishtranslation tasks.
The results illustrate that ourRNN-based model outperforms the FFNN-basedmodel (up to +0.0792 F1-measure) and the IBMModel 4 (up to +0.0703 F1-measure) for the wordalignment tasks.
For the translation tasks, ourmodel achieves up to 0.74% gain in BLEU as com-pared to the FFNN-based model, which matchesthe translation qualities of the IBM Model 4.2 Related WorkVarious word alignment models have been pro-posed.
These models are roughly clustered intotwo groups: generative models, such as those pro-posed by Brown et al (1993), Vogel et al (1996),and Och and Ney (2003), and discriminative mod-els, such as those proposed by Taskar et al (2005),Moore (2005), and Blunsom and Cohn (2006).2.1 Generative Alignment ModelGiven a source language sentence fJ1= f1, ..., fJand a target language sentence eI1= e1, ..., eI,fJ1is generated by eI1via the alignment aJ1=a1, ..., aJ.
Each ajis a hidden variable indicat-ing that the source word fjis aligned to the targetword eaj.
Usually, a ?null?
word e0is added tothe target language sentence and aJ1may containaj= 0, which indicates that fjis not aligned toany target word.
The probability of generating thesentence fJ1from eI1is defined asp(fJ1|eI1) =?aJ1p(fJ1, aJ1|eI1).
(1)The IBM Models 1 and 2 and the HMM modeldecompose it into an alignment probability paanda lexical translation probability ptasp(fJ1, aJ1|eI1) =J?j=1pa(aj|aj?1, j)pt(fj|eaj).
(2)The three models differ in their definition of align-ment probability.
For example, the HMM modeluses an alignment probability with a first-orderMarkov property: pa(aj|aj?
aj?1).
In addition,the IBM models 3-5 are extensions of these, whichconsider the fertility and distortion of each trans-lated word.These models are trained using the expectation-maximization algorithm (Dempster et al, 1977)from bilingual sentences without word-level align-ments (unlabeled training data).
Given a specificmodel, the best alignment (Viterbi alignment) ofthe sentence pair (fJ1, eI1) can be found asa?J1= argmaxaJ1p(fJ1, aJ1|eI1).
(3)For example, the HMM model identifies theViterbi alignment using the Viterbi algorithm.2.2 FFNN-based Alignment ModelAs an instance of discriminative models, we de-scribe an FFNN-based word alignment model(Yang et al, 2013), which is our baseline.
AnFFNN learns a hierarchy of nonlinear featuresthat can automatically capture complex statisti-cal patterns in input data.
Recently, FFNNs havebeen applied successfully to several tasks, such asspeech recognition (Dahl et al, 2012), statisticalmachine translation (Le et al, 2012; Vaswani etal., 2013), and other popular natural language pro-cessing tasks (Collobert and Weston, 2008; Col-lobert et al, 2011).Yang et al (2013) have adapted a type of FFNN,i.e., CD-DNN-HMM (Dahl et al, 2012), to theHMM alignment model.
Specifically, the lexicaltranslation and alignment probability in Eq.
2 arecomputed using FFNNs assNN(aJ1|fJ1, eI1) =J?j=1ta(aj?
aj?1|c(eaj?1))?tlex(fj, eaj|c(fj), c(eaj)), (4)1471LookupLayerHidden LayerOutput LayerInput fj-1 eL L L L L Lhtanh(H?
+BH)O?
+BOaj-1t   ( ,    |      ,      )fj eaj ef j-1j+1lexz0z1fj fj+1 eaj eaj+1z0z1aj-1aj+1Figure 1: FFNN-based model for computing a lex-ical translation score of (fj, eaj)where taand tlexare an alignment score and a lex-ical translation score, respectively, sNNis a scoreof alignments aJ1, and ?c(a word w)?
denotes acontext of word w. Note that the model uses non-probabilistic scores rather than probabilities be-cause normalization over all words is computa-tionally expensive.
The model finds the Viterbialignment using the Viterbi algorithm, similar tothe classic HMM model.
Note that alignmentsin the FFNN-based model are also governed byfirst-order Markov dynamics because an align-ment score depends on the previous alignmentaj?1.Figure 1 shows the network structure with onehidden layer for computing a lexical translationprobability tlex(fj, eaj|c(fj), c(eaj)).
The modelconsists of a lookup layer, a hidden layer, and anoutput layer, which have weight matrices.
Themodel receives a source and target word with theircontexts as inputs, which are words in a prede-fined window (the window size is three in Fig-ure 1).
First, the lookup layer converts each in-put word into its word embedding by looking upits corresponding column in the embedding ma-trix (L), and then concatenates them.
Let Vf(orVe) be a set of source words (or target words) andM be a predetermined embedding length.
L is aM ?
(|Vf|+ |Ve|) matrix1.
Word embeddings aredense, low dimensional, and real-valued vectorsthat can capture syntactic and semantic propertiesof the words (Bengio et al, 2003).
The concate-nation (z0) is then fed to the hidden layer to cap-ture nonlinear relations.
Finally, the output layerreceives the output of the hidden layer (z1) andcomputes a lexical translation score.1We add a special token ?unk?
to handle unknown wordsand ?null?
to handle null alignments to Vfand VeThe computations in the hidden and output layerare as follows2:z1= f(H ?
z0+ BH), (5)tlex= O ?
z1+ BO, (6)where H , BH, O, and BOare |z1| ?
|z0|, |z1| ?
1,1?|z1|, and 1?1 matrices, respectively, and f(x)is an activation function.
Following Yang et al(2013), a ?hard?
version of the hyperbolic tangent,htanh(x)3, is used as f(x) in this study.The alignment model based on an FFNN isformed in the same manner as the lexical trans-lation model.
Each model is optimized by mini-mizing the following ranking loss with a marginusing stochastic gradient descent (SGD)4, wheregradients are computed by the back-propagationalgorithm (Rumelhart et al, 1986):loss(?)
=?
(f ,e)?Tmax{0, 1?
s?
(a+|f , e)+s?
(a?|f ,e)}, (7)where ?
denotes the weights of layers in themodel, T is a set of training data, a+is the goldstandard alignment, a?is the incorrect alignmentwith the highest score under ?, and s?denotes thescore defined by Eq.
4 as computed by the modelunder ?.3 RNN-based Alignment ModelThis section proposes an RNN-based alignmentmodel, which computes a score for alignments aJ1using an RNN:sNN(aJ1|fJ1, eI1) =J?j=1tRNN(aj|aj?11, fj, eaj), (8)where tRNNis the score of an alignment aj.
Theprediction of the j-th alignment ajdepends on allpreceding alignments aj?11.
Note that the pro-posed model also uses nonprobabilistic scores,similar to the FFNN-based model.The RNN-based model is illustrated in Figure2.
The model consists of a lookup layer, a hid-den layer, and an output layer, which have weight2Consecutive l hidden layers can be used: zl= f(Hl?zl?1+ BHl).
For simplicity, this paper describes the modelwith 1 hidden layer.3htanh(x) = ?1 for x < ?1, htanh(x) = 1 for x > 1,and htanh(x) = x for others.4In our experiments, we used a mini-batch SGD insteadof a plain SGD.1472O?
+BOhtanh(H?
+R?
+BH )t    ( |    ,    ,    )Lookup LayerHiddenLayerOutput LayerInputL Ldaj fjRNN eajj-1a1fj eajyjyj-1yjdxjxj dyj-1Figure 2: RNN-based alignment modelmatrices L, {Hd, Rd, BdH}, and {O,BO}, respec-tively.
Each matrix in the hidden layer (Hd, Rd,and BdH) depends on alignment, where d denotesthe jump distance from aj?1to aj: d = aj?aj?1.
In our experiments, we merge distancesthat are greater than 8 and less than -8 into thespecial ??8?
and ??-8?
distances, respectively.Specifically, the hidden layer has weight matrices{H?
?8, H?7, ?
?
?
, H7, H?8, R?
?8, R?7, ?
?
?
,R7, R?8, B?
?8H, B?7H, ?
?
?
, B7H, B?8H} and com-putes yjusing the corresponding matrices of thejump distance d.The Viterbi alignment is determined using theViterbi algorithm, similar to the FFNN-basedmodel, where the model is sequentially appliedfrom f1to fJ5.
When computing the score of thealignment between fjand eaj, the two words areinput to the lookup layer.
In the lookup layer, eachof these words is converted to its word embedding,and then the concatenation of the two embeddings(xj) is fed to the hidden layer in the same manneras the FFNN-based model.
Next, the hidden layerreceives the output of the lookup layer (xj) andthat of the previous hidden layer (yj?1).
The hid-den layer then computes and outputs the nonlinearrelations between them.
Note that the weight ma-trices used in this computation are embodied bythe specific jump distance d. The output of the hid-den layer (yj) is copied and fed to the output layerand the next hidden layer.
Finally, the output layercomputes the score of aj(tRNN(aj|aj?11, fj, eaj))from the output of the hidden layer (yj).
Note thatthe FFNN-based model consists of two compo-5Strictly speaking, we cannot apply the dynamic pro-gramming forward-backward algorithm (i.e., the Viterbi al-gorithm) due to the long alignment history of yi.
Thus, theViterbi alignment is computed approximately using heuristicbeam search.nents: one is for lexical translation and the otheris for alignment.
The proposed RNN produces asingle score that is constructed in the hidden layerby employing the distance-dependent weight ma-trices.Specifically, the computations in the hidden andoutput layer are as follows:yj= f(Hd?
xj+ Rd?
yj?1+ BdH), (9)tRNN= O ?
yj+ BO, (10)where Hd, Rd, BdH, O, and BOare |yj| ?
|xj|,|yj| ?
|yj?1|, |yj| ?
1, 1 ?
|yj|, and 1 ?
1 matri-ces, respectively.
Note that |yj?1| = |yj|.
f(x) isan activation function, which is a hard hyperbolictangent, i.e., htanh(x), in this study.As described above, the RNN-based model hasa hidden layer with recurrent connections.
Underthe recurrence, the proposed model compactly en-codes the entire history of previous alignments inthe hidden layer configuration yi.
Therefore, theproposed model can find alignments by taking ad-vantage of the long alignment history, while theFFNN-based model considers only the last align-ment.4 TrainingDuring training, we optimize the weight matricesof each layer (i.e., L, Hd, Rd, BdH, O, and BO)following a given objective using a mini-batchSGD with batch size D, which converges fasterthan a plain SGD (D = 1).
Gradients are com-puted by the back-propagation through time algo-rithm (Rumelhart et al, 1986), which unfolds thenetwork in time (j) and computes gradients overtime steps.
In addition, an l2 regularization termis added to the objective to prevent the model fromoverfitting the training data.The RNN-based model can be trained by asupervised approach, similar to the FFNN-basedmodel, where training proceeds based on the rank-ing loss defined by Eq.
7 (Section 2.2).
However,this approach requires gold standard alignments.To overcome this drawback, we propose an un-supervised method using NCE, which learns fromunlabeled training data.4.1 Unsupervised LearningDyer et al (2011) presented an unsupervisedalignment model based on contrastive estimation(CE) (Smith and Eisner, 2005).
CE seeks to dis-criminate observed data from its neighborhood,1473which can be viewed as pseudo-negative samples.Dyer et al (2011) regarded all possible align-ments of the bilingual sentences, which are givenas training data (T ), and those of the full transla-tion search space (?)
as the observed data and itsneighborhood, respectively.We introduce this idea to a ranking loss withmargin asloss(?)
= max{0, 1??(f+,e+)?TE?[s?
(a|f+, e+)]+?(f+,e?)??E?[s?
(a|f+, e?
)]}, (11)where ?
is a set of all possible alignments given(f , e), E?[s?]
is the expected value of the scoress?on ?, e+denotes a target language sentence inthe training data, and e?denotes a pseudo-targetlanguage sentence.
The first expectation term isfor the observed data, and the second is for theneighborhood.However, the computation for ?
is prohibitivelyexpensive.
To reduce computation, we employNCE, which uses randomly sampled sentencesfrom all target language sentences in ?
as e?, andcalculate the expected values by a beam searchwith beam width W to truncate alignments withlow scores.
In our experiments, we set W to 100.In addition, the above criterion is converted to anonline fashion asloss(?)
=?f+?Tmax{0, 1?
EGEN[s?
(a|f+, e+)]+1N?e?EGEN[s?
(a|f+, e?
)]}, (12)where e+is a target language sentence aligned tof+in the training data, i.e., (f+, e+) ?
T , e?isa randomly sampled pseudo-target language sen-tence with length |e+|, and N denotes the num-ber of pseudo-target language sentences per sourcesentence f+.
Note that |e+| = |e?|.
GEN is asubset of all possible word alignments ?, which isgenerated by beam search.In a simple implementation, each e?is gener-ated by repeating a random sampling from a set oftarget words (Ve) |e+| times and lining them upsequentially.
To employ more discriminative neg-ative samples, our implementation samples eachword of e?from a set of the target words that co-occur with fi?
f+whose probability is above athreshold C under the IBM Model 1 incorporatingl0prior (Vaswani et al, 2012).
The IBM Model1 with l0prior is convenient for reducing transla-tion candidates because it generates more sparsealignments than the standard IBM Model 1.4.2 Agreement ConstraintsBoth of the FFNN-based and RNN-based modelsare based on the HMM alignment model, and theyare therefore asymmetric, i.e., they can representone-to-many relations from the target side.
Asym-metric models are usually trained in each align-ment direction.
The model proposed by Yang etal.
(2013) is no exception.
However, it has beendemonstrated that encouraging directional mod-els to agree improves alignment performance (Ma-tusov et al, 2004; Liang et al, 2006; Grac?a et al,2008; Ganchev et al, 2008).Inspired by their work, we introduce an agree-ment constraint to our learning.
The constraintconcretely enforces agreement in word embed-dings of both directions.
The proposed methodtrains two directional models concurrently basedon the following objective by incorporating apenalty term that expresses the difference betweenword embeddings:argmin?FE{loss(?FE) + ???LEF?
?LFE?
}, (13)argmin?EF{loss(?EF) + ???LFE?
?LEF?
}, (14)where ?FE(or ?EF) denotes the weights of lay-ers in a source-to-target (or target-to-source) align-ment model, ?Ldenotes weights of a lookup layer,i.e., word embeddings, and ?
is a parameter thatcontrols the strength of the agreement constraint.???
indicates the norm of ?.
2-norm is used in ourexperiments.
Equations 13 and 14 can be appliedto both supervised and unsupervised approaches.Equations 7 and 12 are substituted into loss(?
)in supervised and unsupervised learning, respec-tively.
The proposed constraint penalizes overfit-ting to a particular direction and enables two di-rectional models to optimize across alignment di-rections globally.Our unsupervised learning procedure is summa-rized in Algorithm 1.
In Algorithm 1, line 2 ran-domly samples D bilingual sentences (f+, e+)Dfrom training data T .
Lines 3-1 and 3-2 gener-ate N pseudo-negative samples for each f+ande+based on the translation candidates of f+ande+found by the IBM Model 1 with l0prior,1474Algorithm 1 Training AlgorithmInput: ?1FE, ?1EF, training data T , MaxIter,batch size D, N , C, IBM1, W , ?1: for all t such that 1 ?
t ?MaxIter do2: {(f+, e+)D}?sample(D,T )3-1: {(f+, {e?
}N)D}?nege({(f+, e+)D}, N,C, IBM1)3-2: {(e+, {f?
}N)D}?negf({(f+, e+)D}, N,C, IBM1)4-1: ?t+1FE?update((f+, e+, {e?
}N)D, ?tFE, ?tEF,W, ?
)4-2: ?t+1EF?update((e+, f+, {f?
}N)D, ?tEF, ?tFE,W, ?
)5: end forOutput: ?MaxIter+1EF, ?MaxIter+1FETrain Dev TestBTEC 9 K 0 960Hansards 1.1 M 37 447FBISNIST03240 K 878919NIST04 1,597IWSLT 40 K 2,501 489NTCIR 3.2 M 2,000 2,000Table 1: Size of experimental datasetsIBM1 (Section 4.1).
Lines 4-1 and 4-2 update theweights in each layer following a given objective(Sections 4.1 and 4.2).
Note that ?tFEand ?tEFareconcurrently updated in each iteration, and ?tEF(or ?tFE) is employed to enforce agreement be-tween word embeddings when updating ?tFE(or?tEF).5 Experiment5.1 Experimental DataWe evaluated the alignment performance of theproposed models with two tasks: Japanese-English word alignment with the Basic TravelExpression Corpus (BTEC) (Takezawa et al,2002) and French-English word alignment withthe Hansard dataset (Hansards) from the 2003NAACL shared task (Mihalcea and Pedersen,2003).
In addition, we evaluated the end-to-endtranslation performance of three tasks: a Chinese-to-English translation task with the FBIS corpus(FBIS), the IWSLT 2007 Japanese-to-Englishtranslation task (IWSLT ) (Fordyce, 2007), andthe NTCIR-9 Japanese-to-English patent transla-tion task (NTCIR) (Goto et al, 2011)6.Table 1 shows the sizes of our experimentaldatasets.
Note that the development data wasnot used in the alignment tasks, i.e., BTEC6We did not evaluate the translation performance on theHansards data because the development data is very smalland performance is unreliable.and Hansards, because the hyperparameters ofthe alignment models were set by preliminarysmall-scale experiments.
The BTEC data isthe first 9,960 sentence pairs in the training datafor IWSLT , which were annotated with wordalignment (Goh et al, 2010).
We split thesepairs into the first 9,000 for training data andthe remaining 960 as test data.
All the data inBTEC is word-aligned, and the training data inHansards is unlabeled data.
In FBIS, we usedthe NIST02 evaluation data as the developmentdata, and the NIST03 and 04 evaluation data astest data (NIST03 and NIST04).5.2 Comparing MethodsWe evaluated the proposed RNN-based alignmentmodels against two baselines: the IBM Model4 and the FFNN-based model with one hiddenlayer.
The IBM Model 4 was trained by pre-viously presented model sequence schemes (Ochand Ney, 2003): 15H53545, i.e., five iterations ofthe IBM Model 1 followed by five iterations of theHMM Model, etc., which is the default setting forGIZA++ (IBM4).
For the FFNN-based model,we set the word embedding length M to 30, thenumber of units of a hidden layer |z1| to 100, andthe window size of contexts to 5.
Hence, |z0| is300 (30?5?2).
Following Yang et al (2013), theFFNN-based model was trained by the supervisedapproach described in Section 2.2 (FFNNs).For the RNN-based models, we set M to 30and the number of units of each recurrent hid-den layer |yj| to 100.
Thus, |xj| is 60 (30 ?
2).The number of units of each layer of the FFNN-based and RNN-based models and M were setthrough preliminary experiments.
To demonstratethe effectiveness of the proposed learning meth-ods, we evaluated four types of RNN-based mod-els: RNNs, RNNs+c, RNNu, and RNNu+c,where ?s/u?
denotes a supervised/unsupervisedmodel and ?+c?
indicates that the agreement con-straint was used.In training all the models except IBM4, theweights of each layer were initialized first.
Forthe weights of a lookup layer L, we preliminarilytrained word embeddings for the source and targetlanguage from each side of the training data.
Wethen set the word embeddings to L to avoid fallinginto local minima.
Other weights were randomlyinitialized to [?0.1, 0.1].
For the pretraining, we1475Alignment BTEC HansardsIBM4 0.4859 0.9029FFNNs(I) 0.4770 0.9020RNNs(I) 0.5053+0.9068RNNs+c(I) 0.5174+0.9202+RNNu0.5307+0.9037RNNu+c0.5562+0.9275+FFNNs(R) 0.8224 -RNNs(R) 0.8798+-RNNs+c(R) 0.8921+-Table 2: Word alignment performance (F1-measure)used the RNNLM Toolkit7(Mikolov et al, 2010)with the default options.
We mapped all wordsthat occurred less than five times to the special to-ken ?unk?.
Next, each weight was optimized us-ing the mini-batch SGD, where batch size D was100, learning rate was 0.01, and an l2regulariza-tion parameter was 0.1.
The training stopped after50 epochs.
The other parameters were set as fol-lows: W , N and C in the unsupervised learningwere 100, 50, and 0.001, respectively, and ?
forthe agreement constraint was 0.1.In the translation tasks, we used the Mosesphrase-based SMT systems (Koehn et al, 2007).All Japanese and Chinese sentences were seg-mented by ChaSen8and the Stanford Chinese seg-menter9, respectively.
In the training, long sen-tences with over 40 words were filtered out.
Usingthe SRILM Toolkits (Stolcke, 2002) with modifiedKneser-Ney smoothing, we trained a 5-gram lan-guage model on the English side of each trainingdata for IWSLT and NTCIR, and a 5-gram lan-guage model on the Xinhua portion of the EnglishGigaword corpus for FBIS.
The SMT weightingparameters were tuned by MERT (Och, 2003) inthe development data.5.3 Word Alignment ResultsTable 2 shows the alignment performance bythe F1-measure.
Hereafter, MODEL(R) andMODEL(I) denote the MODEL trained fromgold standard alignments and word alignmentsfound by the IBM Model 4, respectively.
InHansards, all models were trained from ran-7http://www.fit.vutbr.cz/?imikolov/rnnlm/8http://chasen-legacy.sourceforge.jp/9http://nlp.stanford.edu/software/segmenter.shtmldomly sampled 100 K data10.
We evaluatedthe word alignments produced by first applyingeach model in both directions and then combin-ing the alignments using the ?grow-diag-final-and?
heuristic (Koehn et al, 2003).
The signif-icance test on word alignment performance wasperformed by the sign test with a 5% significancelevel.
?+?
in Table 2 indicates that the compar-isons are significant over corresponding baselines,IBM4 and FFNNs(R/I).In Table 2, RNNu+c, which includes all ourproposals, i.e., the RNN-based model, the unsu-pervised learning, and the agreement constraint,achieves the best performance for both BTECand Hansards.
The differences from the base-lines are statistically significant.Table 2 shows that RNNs(R/I) outperformsFFNNs(R/I), which is statistically significantin BTEC.
These results demonstrate that captur-ing the long alignment history in the RNN-basedmodel improves the alignment performance.
Wediscuss the difference of the RNN-based model?seffectiveness between language pairs in Section6.1.
Table 2 also shows that RNNs+c(R/I) andRNNu+cachieve significantly better performancethan RNNs(R/I) and RNNuin both tasks, re-spectively.
This indicates that the proposed agree-ment constraint is effective in training better mod-els in both the supervised and unsupervised ap-proaches.In BTEC, RNNuand RNNu+csignificantlyoutperform RNNs(I) and RNNs+c(I), respec-tively.
The performance of these models is com-parable with Hansards.
This indicates that ourunsupervised learning benefits our models becausethe supervised models are adversely affected byerrors in the automatically generated training data.This is especially true when the quality of trainingdata, i.e., the performance of IBM4, is low.5.4 Machine Translation ResultsTable 3 shows the translation performance by thecase sensitive BLEU4 metric11(Papineni et al,2002).
Table 3 presents the average BLEU of threedifferent MERT runs.
In NTCIR and FBIS,each alignment model was trained from the ran-10Due to high computational cost, we did not use all thetraining data.
Scaling up to larger datasets will be addressedin future work.11We used mteval-v13a.pl as the evaluation tool(http://www.itl.nist.gov/iad/mig/tests/mt/2009/).1476Alignment IWSLT NTCIRFBISNIST03 NIST04IBM4all46.4727.91 25.90 28.34IBM4 27.25 25.41 27.65FFNNs(I) 46.38 27.05 25.45 27.61RNNs(I) 46.43 27.24 25.47 27.56RNNs+c(I) 46.51 27.12 25.55 27.73RNNu47.05?27.79?25.76?27.91?RNNu+c46.97?27.76?25.84?28.20?Table 3: Translation performance (BLEU4(%))domly sampled 100 K data, and then a translationmodel was trained from all the training data thatwas word-aligned by the alignment model.
In ad-dition, for a detailed comparison, we evaluated theSMT system where the IBM Model 4 was trainedfrom all the training data (IBM4all).
The sig-nificance test on translation performance was per-formed by the bootstrap method (Koehn, 2004)with a 5% significance level.
?*?
in Table 3 in-dicates that the comparisons are significant overboth baselines, i.e., IBM4 and FFNNs(I).Table 3 also shows that better word align-ment does not always result in better translation,which has been discussed previously (Yang et al,2013).
However, RNNuand RNNu+coutper-form FFNNs(I) and IBM4 in all tasks.
Theseresults indicate that our proposals contribute to im-proving translation performance12.
In addition,Table 3 shows that these proposed models arecomparable to IBM4allin NTCIR and FBISeven though the proposed models are trained fromonly a small part of the training data.6 Discussion6.1 Effectiveness of RNN-based AlignmentModelFigure 3 shows word alignment examples fromFFNNsand RNNs, where solid squares indi-cate the gold standard alignments.
Figure 3 (a)shows that RRNsadequately identifies compli-cated alignments with long distances comparedto FFNNs(e.g., jaggy alignments of ?have youbeen learning?
in Fig 3 (a)) because RNNscap-tures alignment paths based on long alignment his-tory, which can be viewed as phrase-level align-ments, while FFNNsemploys only the last align-ment.In French-English word alignment, the most12We also confirmed the effectiveness of our models on theNIST05 and NTCIR-10 evaluation data.HowlonghaveyoubeenlearningEnglish?????
???
?????
??
??????
??
??
?
????
??
?????
(a) Japanese-English Alignmenttheyalsohavearoletoplayinfoodchain.theeux aussiont un r?le ?
jouerdansla cha?nealimentaire.
(b) French-English Alignment?????????????
: FFNN  (R)s: RNN  (R)s?
: FFNN  (I)s: RNN  (I)sFigure 3: Word alignment examplesAlignment 40 K 9 K 1 KIBM4 0.5467 0.4859 0.4128RNNu+c0.6004 0.5562 0.4842RNNs+c(R) - 0.8921 0.6063Table 4: Word alignment performance on BTECwith various sized training datavaluable clues are located locally because Englishand French have similar word orders and theiralignment has more one-to-one mappings thanJapanese-English word alignment (Figure 3).
Fig-ure 3 (b) shows that both RRNsand FFNNswork for such simpler alignments.
Therefore,the RNN-based model has less effect on French-English word alignment than Japanese-Englishword alignment, as indicated in Table 2.6.2 Impact of Training Data SizeTable 4 shows the alignment performance onBTEC with various training data sizes, i.e., train-ing data for IWSLT (40 K), training data forBTEC (9 K), and the randomly sampled 1 Kdata from the BTEC training data.
Note thatRNNs+c(R) cannot be trained from the 40 K databecause the 40 K data does not have gold standard1477Alignment BTEC HansardsFFNNs(I) 0.4770 0.9020FFNNs+c(I) 0.4854+0.9085+FFNNu0.5105+0.9026FFNNu+c0.5313+0.9144+FFNNs(R) 0.8224 -FFNNs+c(R) 0.8367+-Table 5: Word alignment performance of variousFFNN-based models (F1-measure)word alignments.Table 4 demonstrates that the proposed RNN-based model outperforms IBM4 trained from theunlabeled 40 K data by employing either the 1K labeled data or the 9 K unlabeled data, whichis less than 25% of the training data for IBM4.Consequently, the SMT system using RNNu+ctrained from a small part of training data canachieve comparable performance to that usingIBM4 trained from all training data, which isshown in Table 3.6.3 Effectiveness of UnsupervisedLearning/Agreement ConstraintsThe proposed unsupervised learning and agree-ment constraints can be applied to any NN-basedalignment model.
Table 5 shows the alignmentperformance of the FFNN-based models trainedby our supervised/unsupervised approaches (s/u)with and without our agreement constraints.
InTable 5, ?+c?
denotes that the agreement con-straint was used, and ?+?
indicates that thecomparison with its corresponding baseline, i.e.,FFNNs(I/R), is significant in the sign test with a5% significance level.Table 5 shows that FFNNs+c(R/I) andFFNNu+cachieve significantly better perfor-mance than FFNNs(R/I) and FFNNu, respec-tively, in both BTEC and Hansards.
In addi-tion, FFNNuand FFNNu+csignificantly out-perform FFNNs(I) and FFNNs+c(I), respec-tively, in BTEC.
The performance of these mod-els is comparable in Hansards.
These resultsindicate that the proposed unsupervised learningand agreement constraint benefit the FFNN-basedmodel, similar to the RNN-based model.7 ConclusionWe have proposed a word alignment model basedon an RNN, which captures long alignment his-tory through recurrent architectures.
Furthermore,we proposed an unsupervised method for trainingour model using NCE and introduced an agree-ment constraint that encourages word embeddingsto be consistent across alignment directions.
Ourexperiments have shown that the proposed modeloutperforms the FFNN-based model (Yang et al,2013) for word alignment and machine translation,and that the agreement constraint improves align-ment performance.In future, we plan to employ contexts composedof surrounding words (e.g., c(fj) or c(eaj) in theFFNN-based model) in our model, even thoughour model implicitly encodes such contexts in thealignment history.
We also plan to enrich eachhidden layer in our model with multiple layersfollowing the success of Yang et al (2013), inwhich multiple hidden layers improved the perfor-mance of the FFNN-based model.
In addition, wewould like to prove the effectiveness of the pro-posed method for other datasets.AcknowledgmentsWe thank the anonymous reviewers for their help-ful suggestions and valuable comments on the firstversion of this paper.ReferencesMichael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint Language and TranslationModeling with Recurrent Neural Networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A Neural Probabilistic Lan-guage Model.
Journal of Machine Learning Re-search, 3:1137?1155.Phil Blunsom and Trevor Cohn.
2006.
DiscriminativeWord Alignment with Conditional Random Fields.In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Lin-guistics, pages 65?72.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics,19(2):263?311.Ronan Collobert and Jason Weston.
2008.
A Uni-fied Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning.
In1478Proceedings of the 25th International Conference onMachine Learning, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural Language Processing (Almost) fromScratch.
Journal of Machine Learning Research,12:2493?2537.George E. Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-Dependent Pre-trained Deep Neu-ral Networks for Large Vocabulary Speech Recog-nition.
Audio, Speech, and Language Processing,IEEE Transactions on, 20(1):30?42.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical So-ciety, Series B, 39(1):1?38.Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.Smith.
2011.
Unsupervised Word Alignment withArbitrary Features.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, pages 409?419.Cameron S. Fordyce.
2007.
Overview of the IWSLT2007 Evaluation Campaign.
In Proceedings of the4th International Workshop on Spoken LanguaegTranslation, pages 1?12.Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.2008.
Better Alignments = Better Translations?
InProceedings of the 46th Annual Conference of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 986?993.Chooi-Ling Goh, TaroWatanabe, Hirofumi Yamamoto,and Eiichiro Sumita.
2010.
Constraining a Gen-erative Word Alignment Model with DiscriminativeOutput.
IEICE Transactions, 93-D(7):1976?1983.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the PatentMachine Translation Task at the NTCIR-9 Work-shop.
In Proceedings of the 9th NTCIR Workshop,pages 559?578.Jo?ao V. Grac?a, Kuzman Ganchev, and Ben Taskar.2008.
Expectation Maximization and PosteriorConstraints.
In Advances in Neural InformationProcessing Systems 20, pages 569?576.Michael Gutmann and Aapo Hyv?arinen.
2010.
Noise-Contrastive Estimation: A New Estimation Principlefor Unnormalized Statistical Models.
In Proceed-ings of the 13st International Conference on Artifi-cial Intelligence and Statistics, pages 297?304.Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentContinuous Translation Models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference: North American Chapter of the Associ-ation for Computational Linguistics, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constrantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics on In-teractive Poster and Demonstration Sessions, pages177?180.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 388?395.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous Space Translation Models withNeural Networks.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 39?48.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by Agreement.
In Proceedings of the MainConference on Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 104?111.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric Word Alignments for StatisticalMachine Translation.
In Proceedings of the 20th In-ternational Conference on Computational Linguis-tics, pages 219?225.Rada Mihalcea and Ted Pedersen.
2003.
An Evalua-tion Exercise for Word Alignment.
In Proceedingsof the HLT-NAACL 2003 Workshop on Building andUsing Parallel Texts: Data Driven Machine Trans-lation and Beyond, pages 1?10.Tomas Mikolov and Geoffrey Zweig.
2012.
Con-text Dependent Recurrent Neural Network Lan-guage Model.
In Proceedings of the 4th IEEE Work-shop on Spoken Language Technology, pages 234?239.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network based Language Model.
InProceedings of 11th Annual Conference of the Inter-national Speech Communication Association, pages1045?1048.Andriy Mnih and Yee Whye Teh.
2012.
A Fast andSimple Algorithm for Training Neural ProbabilisticLanguage Models.
In Proceedings of the 29th In-ternational Conference on Machine Learning, pages1751?1758.1479Robert C. Moore.
2005.
A Discriminative Frameworkfor Bilingual Word Alignment.
In Proceedings ofHuman Language Technology Conference and Con-ference on Empirical Methods in Natural LanguageProcessing, pages 81?88.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.D.
E. Rumelhart, G. E. Hinton, and R. J. Williams.1986.
Learning Internal Representations by ErrorPropagation.
In D. E. Rumelhart and J. L. McClel-land, editors, Parallel Distributed Processing, pages318?362.
MIT Press.Noah A. Smith and Jason Eisner.
2005.
ContrastiveEstimation: Training Log-Linear Models on Unla-beled Data.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics, pages 354?362.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,pages 901?904.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,Ben Freiberg, Ralf Schl?uter, and Hermann Ney.2013.
Comparison of Feedforward and RecurrentNeural Network Language Models.
In IEEE Inter-national Conference on Acoustics, Speech, and Sig-nal Processing, pages 8430?8434.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-aya, Hirofumi Yamamoto, and Seiichi Yamamoto.2002.
Toward a Broad-coverage Bilingual Corpusfor Speech Translation of Travel Conversations inthe Real World.
In Proceedings of the 3rd Interna-tional Conference on Language Resources and Eval-uation, pages 147?152.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A Discriminative Matching Approach toWord Alignment.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 73?80.Ashish Vaswani, Liang Huang, and David Chiang.2012.
Smaller Alignment Models for Better Trans-lations: Unsupervised Word Alignment with the l0-norm.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 311?319.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with Large-ScaleNeural Language Models Improves Translation.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1387?1392.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based Word Alignment in Statisti-cal Translation.
In Proceedings of the 16th Inter-national Conference on Computational Linguistics,pages 836?841.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-hai Yu.
2013.
Word Alignment Modeling with Con-text Dependent Deep Neural Network.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 166?175.1480
