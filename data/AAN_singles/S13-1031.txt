Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 216?220, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsCFILT-CORE: Semantic Textual Similarity using Universal NetworkingLanguageAvishek DanIIT BombayMumbai, Indiaavishekdan@cse.iitb.ac.inPushpak BhattacharyyaIIT BombayMumbai, Indiapb@cse.iitb.ac.inAbstractThis paper describes the system that was sub-mitted in the *SEM 2013 Semantic TextualSimilarity shared task.
The task aims to findthe similarity score between a pair of sen-tences.
We describe a Universal Network-ing Language (UNL) based semantic extrac-tion system for measuring the semantic simi-larity.
Our approach combines syntactic andword level similarity measures along with theUNL based semantic similarity measures forfinding similarity scores between sentences.1 IntroductionSemantic Textual Similarity is the task of findingthe degree of semantic equivalence between a pairof sentences.
The core Semantic Textual Similar-ity shared task of *SEM 2013 (Agirre et al 2013)is to generate a score in the range 0-5 for a pairof sentences depending on their semantic similar-ity.
Textual similarity finds applications in infor-mation retrieval and it is closely related to textualentailment.
Universal Networking Language (UNL)(Uchida, 1996) is an ideal mechanism for seman-tics representation.
Our system first converts thesentences into a UNL graph representation and thenmatches the graphs to generate the semantic relat-edness score.
Even though the goal is to judge sen-tences based on their semantic relatedness, our sys-tem incorporates some lexical and syntactic similar-ity measures to make the system robust in the faceof data sparsity.Section 2 give a brief introduction to UNL.
Sec-tion 3 decribes the English Enconverter developedFigure 1: UNL Graph for ?The boy chased the dog?Figure 2: UNL Graph for ?The dog was chased by theboy?by us.
Section 4 discusses the various similaritymeasures used for the task.
Section 5 mentionsthe corpus used for training and testing.
Section 6describes the method used to train the system andSection 7 presents the results obtained on the taskdatasets.2 Universal Networking LanguageUniversal Networking Language (UNL) is an inter-lingua that represents a sentence in a language inde-pendent, unambiguous form.
The three main build-ing blocks of UNL are relations, universal wordsand attributes.
UNL representations have a graphicalstructure with concepts being represented as nodes(universal words) and interactions between conceptsbeing represented by edges (relations) between thenodes.
Figure 1 shows the UNL graph correspond-216ing to the sentence ?The boy chased the dog.?
Theconversion from a source language to UNL is calledenconversion.
The reverse process of generating anatural language sentence from UNL is called de-conversion.
The enconversion process is markedlymore difficult than the deconversion process due tothe inherent ambiguity and idiosyncracy of naturallanguage.UNL representation captures the semantics inde-pendent of the structure of the language.
Figures1 and 2 show the UNL representation of two struc-turally different sentences which convey the samemeaning.
The UNL graph structure remains thesame with an additional attribute on the main verbof figure 2 indicating the voice of the sentence.2.1 Universal WordsUniversal words (UWs) are language independentconcepts that are linked to various language re-sources.
The UWs used by us are linked tothe Princeton WordNet and various other languageWordNet synsets.
UWs consist of a head wordwhich is the word in its lemma form.
For example,in figure 2 the word chased is shown in its lemmaform as chased.
The head word is followed by aconstraint list which is used to disambiguate it.
Forexample, chase icl (includes) pursue indicates thatchase as a type of pursuing is indicated here.
Com-plex concepts are represented by hypernodes, whichare UNL graphs themselves.2.2 RelationsRelations are two place functions that imdicate therelationship between UWs.
Some of the commonlyused relations are agent (agt), object (obj), instru-ment (ins), place (plc).
For example, in figure 1 therelation agt between boy and chase indicates that theboy is the doer of the action.2.3 AttributeAttributes are one place functions that convey vari-ous morphological and pragmatic information.
Forexample, in figure 1 the attribute past indicates thatthe verb is in the past tense.3 UNL GenerationThe conversion from English to UNL involves aug-menting the sentence with various factors such asPOS tags, NER tags and dependency parse treerelations and paths.
The suitable UW generationis achieved through a word sense disambiguation(WSD) system trained on a tourism corpus.
TheWSD system maps the words to Wordnet 2.1 synsetids.
The attribute and relation generation is achievedthrough a combination of rule-base and classifierstrained on a small corpus.
We use a nearest neighborclassifier trained on the EOLSS corpus for generat-ing relations.
The attributes are generated by con-ditional random fields trained on the IGLU corpus.The attribute generation is a word level phenomena,hence attributes for complex UWs cannot be gener-ated by the classifiers.
The steps are described indetail.3.1 Parts of Speech TaggingThe Stanford POS tagger using the WSJ corpustrained PCFG model is used to tag the sentences.Penn Treebank style tags are generated.3.2 Word Sense DisambiguationA Supervised Word Sense Disambiguation (WSD)tool trained in Tourism domain is used.
The WSDsystem takes a sequence of tagged words and pro-vides the WordNet synset ids of all nouns, verbs, ad-jectives and adverbs in the sequence.
The accuracyof the system is depends on the length of the inputsentence.3.3 Named Entity RecognitionStanford Named Entity Recognizer is used to tag thewords in the sentence.
The tags may be PERSON,LOCATION or ORGANIZATION.3.4 Parsing and Clause MarkingStanford Parser is used to parse the sentences.
Rulesbased on the constituency parse are used to identifythe clause boundaries.
The dependency parse is usedfor clause type detection.
It is also used in the laterstages of UNL generation.The clauses are converted into separate sim-ple sentences for further processing.
Independentclauses can be trivially separated since they havecomplete sentential structure of their own.
Depen-dent clauses are converted into complete sentencesusing rules based on the type of clause.
For exam-ple, for the sentence, That he is a good sprinter, is217known to all, containing a nominal clause, the sim-ple sentences obtained are he is a good sprinter andit is known to all.
Here the dependent clause is re-placed by the anaphora it to generate the sentencecorresponding to the main clause.3.5 UW GenerationWordNet synset ids obtained from the WSD systemand the parts of speech tags are used to generate theUWs.
The head word is the English sentence in itslemma form.
The constraint list is generated fromthe WordNet depending on the POS tag.3.6 Relation GenerationRelations are generated by a combination of rulebase and corpus based techniques.
Rules are writ-ten using parts of speech tags, named entity tags andparse dependency relations.
The corpus based tech-niques are used when insufficient rules exist for re-lation generation.
We use a corpus of about 28000sentences consisting of UNL graphs for WordNetglosses obtained from the UNDL foundation.
Thistechnique tries to find similar examples from the cor-pus and assigns the observed relation label to thenew part of the sentence.3.7 Attribute GenerationAttributes are a combination of morphological fea-tures and pragmatic information.
Attribute genera-tion can be considered to be a sequence labeling taskon the words.
A conditional random field trained onthe corpus described in section 5.1 is used for at-tribute generation.4 Similarity MeasuresWe broadly define three categories of similaritymeasures based on our classification of perceptionof similarity.4.1 Word based Similarity MeasureWord based similarity measures consider the sen-tences as sets-of-words.
These measures are mo-tivated by our view that sentences having a lot ofcommon words will appear quite similar to a humanuser.
The sentences are tokenized using StanfordParser.
The Jaccard coefficient (Agirre and Ghoshand Mooney, 2000) compares the similarity or diver-sity of two sets.
It is the ratio of size of intersectionto the size of union of two sets.
We define a newmeasure based on the Jaccard similarity coefficientthat captures the relatedness between words.
Thetokens in the set are augmented with related wordsfrom Princeton WordNet.
(Pedersen and Patward-han and Michelizzi, 2004) As a preprocessing step,all the tokens are stemmed using WordNet Stemmer.For each possible sense of each stem, its synonyms,antonyms, hypernyms and holonyms are added tothe set as applicable.
For example, hypernyms areadded only when the token appears as a noun or verbin the WordNet.
The scoring function used is definedasExtJSim(S1, S2) =|ExtS1 ?
ExtS2||S1 ?
S2|The following example illustrates the intuition be-hind this similarity measure.?
I am cooking chicken in the house.?
I am grilling chicken in the kitchen.The measure generates a similarity score of 1since grilling is a kind of cooking (hypernymy) andkitchen is a part of house (holonymy).4.2 Syntactic Similarity MeasuresStructural similarity as an indicator of textual sim-ilarity is captured by the syntactic similarity mea-sures.
Parses are obtained for the pair of Englishsentences using Stanford Parser.
The parser is run onthe English PCFG model.
The dependency graphs ofthe two sentences are matched to generate the simi-larity score.
A dependency graph consists of a num-ber of dependency relations of the form dep(word1,word2) where dep is the type of relation and word1and word2 are the words between which the rela-tion holds.
A complete match of a dependency re-lation contributes 1 to the score whereas a match ofonly the words in the relation contributes 0.75 to thescore.SynSim(S1, S2) =|S1 ?
S2||S1 ?
S2|+ 0.75?
?a?S1,b?S2[[a.w1 = b.w1&a.w2 = b.w2]]|S1 ?
S2|Here S1 and S2 represent the set of dependencyrelations.218An extended syntactic similarity measure inwhich exact word matchings are replaced by a matchwithin a set formed by extending the word with re-lated words as described in 4.1 is also used.4.3 Semantic Similarity MeasureSemantic similarity measures try to capture the sim-ilarity in the meaning of the sentences.
The UNLgraphs generated for the two sentences are comparedusing the formula given below.
In addition, syn-onymy is no more used for enriching the word banksince UWs by design are mapped to synsets, henceall synonyms are equivalent in a UNL graph.SemSim(S1, S2) =|S1 ?
S2||S1 ?
S2|+?a?S1,b?S2(0.75?
[[a.w1 = b.w1&a.w2 = b.w2]]|S1 ?
S2|+ 0.75?
[[a.r = b.r&a.Ew1 = b.Ew1&a.Ew2 = b.Ew2]]|S1 ?
S2|+0.6 ?
[[a.Ew1 = b.Ew1&a.Ew2 = b.Ew2]]|S1 ?
S2|)5 CorpusThe system is trained on the Semantic Textual Sim-ilarity 2012 task data.
The training dataset consistsof 750 pairs from the MSR-Paraphrase corpus, 750sentences from the MSR-Video corpus and 734 pairsfrom the SMTeuroparl corpus.The test set contains headlines mined from sev-eral news sources mined by European Media Moni-tor, sense definitions from WordNet and OntoNotes,sense definitions from WordNet and FrameNet, sen-tences from DARPA GALE HTER and HyTER,where one sentence is a MT output and the other isa reference translation.Each corpus contains pairs of sentences with anassociated score from 0 to 5.
The scores are givenbased on whether the sentences are on different top-ics (0), on the same topic but have different con-tent (1), not equivalent but sharing some details (2),roughly equivalent with some inportant informationmissing or differing (3), mostly important while dif-fering in some unimportant details (4) or completelyequivalent (5).Table 1: ResultsCorpus CFILT Best ResultsHeadlines 0.5336 0.7642OnWN 0.2381 0.7529FNWN 0.2261 0.5818SMT 0.2906 0.3804Mean 0.3531 0.61816 TrainingThe several scores are combined by training a Lin-ear Regression model.
We use the inbuilt libaries ofWeka to learn the weights.
To compute the proba-bility of a test sentence pair, the following formulais used.score(S1, S2) = c+5?i=1?iscorei(S1, S2)7 ResultsThe test dataset contained many very long sentenceswhich could not be parsed by the Stanford parserused by the UNL system.
In addition, the perfor-mance of the WSD system led to numerous falsenegatives.
Hence erroneous output were produced inthese cases.
In these cases, the word based similar-ity measures somewhat stabilized the scores.
Table1 summarizes the results.The UNL system is not robust enough to han-dle large sentences with long distance relationshipswhich leads to poor performance on the OnWN andFNWN datasets.8 Conclusion and Future WorkThe approach discussed in the paper shows promisefor the small sentences.
The ongoing developmentof UNL is expected to improve the accuracy of thesystem.
Tuning the scoring parameters on a develop-ment set instead of arbitrary values may improve re-sults.
A log-linear model instead of the linear com-bination of scores may capture the relationships be-tween the scores in a better way.ReferencesEneko Agirre and Daniel Cer and Mona Diab and AitorGonzalez-Agirre and Weiwei Guo.
*SEM 2013219Shared Task: Semantic Textual Similarity, including aPilot on Typed-Similarity.
*SEM 2013: The SecondJoint Conference on Lexical and Computational Se-mantics.
Association for Computational Linguistics.Hiroshi Uchida.
UNL: Universal Networking Lan-guageAn Electronic Language for Communica-tion, Understanding, and Collaboration.
1996.UNU/IAS/UNL Center, Tokyo.Alexander Strehl and Joydeep Ghosh and RaymondMooney Impact of similarity measures on web-pageclustering.
2000.
Workshop on Artificial Intelligencefor Web Search (AAAI 2000).Ted Pedersen and Siddharth Patwardhan and JasonMichelizzi WordNet:: Similarity: measuring the re-latedness of concepts.
2004.
Demonstration Papersat HLT-NAACL 2004.
Association for ComputationalLinguistics.220
