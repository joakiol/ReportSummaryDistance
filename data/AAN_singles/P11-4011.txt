Proceedings of the ACL-HLT 2011 System Demonstrations, pages 62?67,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsPrototyping virtual instructors from human-human corporaLuciana BenottiPLN Group, FAMAFNational University of Co?rdobaCo?rdoba, Argentinaluciana.benotti@gmail.comAlexandre DenisTALARIS team, LORIA/CNRSLorraine.
Campus scientifique, BP 239Vandoeuvre-le`s-Nancy, Francealexandre.denis@loria.frAbstractVirtual instructors can be used in several ap-plications, ranging from trainers in simulatedworlds to non player characters for virtualgames.
In this paper we present a novelalgorithm for rapidly prototyping virtual in-structors from human-human corpora withoutmanual annotation.
Automatically prototyp-ing full-fledged dialogue systems from cor-pora is far from being a reality nowadays.
Ouralgorithm is restricted in that only the virtualinstructor can perform speech acts while theuser responses are limited to physical actionsin the virtual world.
We evaluate a virtual in-structor, generated using this algorithm, withhuman users.
We compare our results bothwith human instructors and rule-based virtualinstructors hand-coded for the same task.1 IntroductionVirtual human characters constitute a promisingcontribution to many fields, including simulation,training and interactive games (Kenny et al, 2007;Jan et al, 2009).
The ability to communicate usingnatural language is important for believable and ef-fective virtual humans.
Such ability has to be goodenough to engage the trainee or the gamer in the ac-tivity.
Nowadays, most conversational systems oper-ate on a dialogue-act level and require extensive an-notation efforts in order to be fit for their task (Rieserand Lemon, 2010).
Semantic annotation and ruleauthoring have long been known as bottlenecks fordeveloping conversational systems for new domains.In this paper, we present novel a algorithm forgenerating virtual instructors from automatically an-notated human-human corpora.
Our algorithm,when given a task-based corpus situated in a virtualworld, generates an instructor that robustly helps auser achieve a given task in the virtual world of thecorpus.
There are two main approaches toward au-tomatically producing dialogue utterances.
One isthe selection approach, in which the task is to pickthe appropriate output from a corpus of possible out-puts.
The other is the generation approach, in whichthe output is dynamically assembled using somecomposition procedure, e.g.
grammar rules.
The se-lection approach to generation has only been usedin conversational systems that are not task-orientedsuch as negotiating agents (Gandhe and Traum,2007), question answering characters (Kenny et al,2007), and virtual patients (Leuski et al, 2006).
Ouralgorithm can be seen as a novel way of doing robustgeneration by selection and interaction managementfor task-oriented systems.In the next section we introduce the corpora usedin this paper.
Section 3 presents the two phases ofour algorithm, namely automatic annotation and di-alogue management through selection.
In Section 4we present a fragment of an interaction with a vir-tual instructor generated using the corpus and thealgorithm introduced in the previous sections.
Weevaluate the virtual instructor in interactions withhuman subjects using objective as well as subjec-tive metrics.
We present the results of the evaluationin Section 5.
We compare our results with both hu-man and rule-based virtual instructors hand-codedfor the same task.
Finally, Section 6 concludes thepaper proposing an improved virtual instructor de-signed as a result of our error analysis.622 The GIVE corpusThe Challenge on Generating Instructions in Vir-tual Environments (GIVE; Koller et al (2010)) isa shared task in which Natural Language Gener-ation systems must generate real-time instructionsthat guide a user in a virtual world.
In this paper, weuse the GIVE-2 Corpus (Gargett et al, 2010), a cor-pus of human instruction giving in virtual environ-ments.
We use the English part of the corpus whichconsists of 63 American English written discoursesin which one subject guided another in a treasurehunting task in 3 different 3D worlds.The task setup involved pairs of human partners,each of whom played one of two different roles.
The?direction follower?
(DF) moved about in the vir-tual world with the goal of completing a treasurehunting task, but had no knowledge of the map ofthe world or the specific behavior of objects withinthat world (such as, which buttons to press to opendoors).
The other partner acted as the ?directiongiver?
(DG), who was given complete knowledge ofthe world and had to give instructions to the DF toguide him/her to accomplish the task.The GIVE-2 corpus is a multimodal corpus whichconsists of all the instructions uttered by the DG, andall the object manipulations done by the DF with thecorresponding timestamp.
Furthermore, the DF?sposition and orientation is logged every 200 mil-liseconds, making it possible to extract informationabout his/her movements.3 The unsupervised conversational modelOur algorithm consists of two phases: an annotationphase and a selection phase.
The annotation phaseis performed only once and consists of automaticallyassociating the DG instruction to the DF reaction.The selection phase is performed every time the vir-tual instructor generates an instruction and consistsof picking out from the annotated corpus the mostappropriate instruction at a given point.3.1 The automatic annotationThe basic idea of the annotation is straightforward:associate each utterance with its corresponding re-action.
We assume that a reaction captures the se-mantics of its associated instruction.
Defining re-action involves two subtle issues, namely boundarydetermination and discretization.
We discuss theseissues in turn and then give a formal definition ofreaction.We define the boundaries of a reaction as follows.A reaction rk to an instruction uk begins right af-ter the instruction uk is uttered and ends right beforethe next instruction uk+1 is uttered.
In the follow-ing example, instruction 1 corresponds to the reac-tion ?2, 3, 4?, instruction 5 corresponds to ?6?, andinstruction 7 to ?8?.DG(1): hit the red you see in the far roomDF(2): [enters the far room]DF(3): [pushes the red button]DF(4): [turns right]DG(5): hit far side greenDF(6): [moves next to the wrong green]DG(7): noDF(8): [moves to the right green and pushes it]As the example shows, our definition of bound-aries is not always semantically correct.
For in-stance, it can be argued that it includes too muchbecause 4 is not strictly part of the semantics of 1.Furthermore, misinterpreted instructions (as 5) andcorrections (e.g., 7) result in clearly inappropriateinstruction-reaction associations.
Since we want toavoid any manual annotation, we decided to use thisnaive definition of boundaries anyway.
We discussin Section 5 the impact that inappropriate associa-tions have on the performance of a virtual instructor.The second issue that we address here is dis-cretization of the reaction.
It is well known that thereis not a unique way to discretize an action into sub-actions.
For example, we could decompose action 2into ?enter the room?
or into ?get close to the doorand pass the door?.
Our algorithm is not dependenton a particular discretization.
However, the samediscretization mechanism used for annotation has tobe used during selection, for the dialogue managerto work properly.
For selection (i.e., in order to de-cide what to say next) any virtual instructor needsto have a planner and a planning domain represen-tation, i.e., a specification of how the virtual worldworks and a way to represent the state of the virtualworld.
Therefore, we decided to use them in orderto discretize the reaction.Now we are ready to define reaction formally.
LetSk be the state of the virtual world when uttering in-63struction uk, Sk+1 be the state of the world whenuttering the next utterance uk+1 and D be the plan-ning domain representation.
The reaction to uk isdefined as the sequence of actions returned by theplanner with Sk as initial state, Sk+1 as goal stateand D as planning domain.The annotation of the corpus then consists of au-tomatically associating each utterance to its (dis-cretized) reaction.3.2 Selecting what to say nextIn this section we describe how the selection phase isperformed every time the virtual instructor generatesan instruction.The instruction selection algorithm consists infinding in the corpus the set of candidate utterancesC for the current task plan P ; P being the se-quence of actions returned by the same planner andplanning domain used for discretization.
We defineC = {U ?
Corpus | U.Reaction is a prefix of P}.In other words, an utterance U belongs to C if thefirst actions of the current plan P exactly match thereaction associated to the utterance.
All the utter-ances that pass this test are considered paraphrasesand hence suitable in the current context.While P does not change, the virtual instructoriterates through the set C, verbalizing a different ut-terance at fixed time intervals (e.g., every 3 seconds).In other words, the virtual instructor offers alterna-tive paraphrases of the intended instruction.
WhenP changes as a result of the actions of the DF, C isrecalculated.It is important to notice that the discretizationused for annotation and selection directly impactsthe behavior of the virtual instructor.
It is crucialthen to find an appropriate granularity of the dis-cretization.
If the granularity is too coarse, manyinstructions in the corpus will have an empty asso-ciated reaction.
For instance, in the absence of therepresentation of the user orientation in the planningdomain (as is the case for the virtual instructor weevaluate in Section 5), instructions like ?turn left?and ?turn right?
will have empty reactions makingthem indistinguishable during selection.
However,if the granularity is too fine the user may get into sit-uations that do not occur in the corpus, causing theselection algorithm to return an empty set of candi-date utterances.
It is the responsibility of the virtualinstructor developer to find a granularity sufficientto capture the diversity of the instructions he wantsto distinguish during selection.4 A virtual instructor for a virtual worldWe implemented an English virtual instructor forone of the worlds used in the corpus collection wepresented in Section 2.
The English fragment of thecorpus that we used has 21 interactions and a totalof 1136 instructions.
Games consisted on averageof 54.2 instructions from the human DG, and tookabout 543 seconds on average for the human DF tocomplete the task.On Figures 1 to 4 we show an excerpt of an in-teraction between the system and a real user that wecollected during the evaluation.
The figures show a2D map from top view and the 3D in-game view.
InFigure 1, the user, represented by a blue character,has just entered the upper left room.
He has to pushthe button close to the chair.
The first candidate ut-terance selected is ?red closest to the chair in front ofyou?.
Notice that the referring expression uniquelyidentifies the target object using the spatial proxim-ity of the target to the chair.
This referring expres-sion is generated without any reasoning on the tar-get distractors, just by considering the current stateof the task plan and the user position.Figure 1: ?red closest to the chair in front of you?After receiving the instruction the user gets closerto the button as shown in Figure 2.
As a result of thenew user position, a new task plan exists, the set ofcandidate utterances is recalculated and the systemselects a new utterance, namely ?the closet one?.The generation of the ellipsis of the button or the64Figure 2: ?the closet one?Figure 3: ?good?Figure 4: ?exit the way you entered?chair is a direct consequence of the utterances nor-mally said in the corpus at this stage of the task plan(that is, when the user is about to manipulate this ob-ject).
From the point of view of referring expressionalgorithms, the referring expression may not be op-timal because it is over-specified (a pronoun wouldbe preferred as in ?click it?
), Furthermore, the in-struction contains a spelling error (?closet?
insteadof ?closest?).
In spite of this non optimality, the in-struction led our user to execute the intended reac-tion, namely pushing the button.Right after the user clicks on the button (Figure 3),the system selects an utterance corresponding to thenew task plan.
The player position stayed the sameso the only change in the plan is that the button nolonger needs to be pushed.
In this task state, DGsusually give acknowledgements and this then whatour selection algorithm selects: ?good?.After receiving the acknowledgement, the userturns around and walks forward, and the next actionin the plan is to leave the room (Figure 4).
The sys-tem selects the utterance ?exit the way you entered?which refers to the previous interaction.
Again, thesystem keeps no representation of the past actionsof the user, but such utterances are the ones that arefound at this stage of the task plan.5 Evaluation and error analysisIn this section we present the results of the evalu-ation we carried out on the virtual instructor pre-sented in Section 4 which was generated using thedialogue model algorithm introduced in Section 3.We collected data from 13 subjects.
The partici-pants were mostly graduate students; 7 female and6 male.
They were not English native speakers butrated their English skills as near-native or very good.The evaluation contains both objective measureswhich we discuss in Section 5.1 and subjective mea-sures which we discuss in Section 5.2.5.1 Objective metricsThe objective metrics we extracted from the logs ofinteraction are summarized in Table 1.
The tablecompares our results with both human instructorsand the three rule-based virtual instructors that weretop rated in the GIVE-2 Challenge.
Their results cor-respond to those published in (Koller et al, 2010)which were collected not in a laboratory but con-necting the systems to users over the Internet.
Thesehand-coded systems are called NA, NM and Saar.We refer to our system as OUR.65Human NA Saar NM OURTask success 100% 47% 40% 30% 70%Canceled 0% 24% n/a 35% 7%Lost 0% 29% n/a 35% 23%Time (sec) 543 344 467 435 692Mouse actions 12 17 17 18 14Utterances 53 224 244 244 194Table 1: Results for the objective metricsIn the table we show the percentage of games thatusers completed successfully with the different in-structors.
Unsuccessful games can be either can-celed or lost.
To ensure comparability, time untiltask completion, number of instructions received byusers, and mouse actions are only counted on suc-cessfully completed games.In terms of task success, our system performs bet-ter than all hand-coded systems.
We duly notice that,for the GIVE Challenge in particular (and proba-bly for human evaluations in general) the successrates in the laboratory tend to be higher than the suc-cess rate online (this is also the case for completiontimes) (Koller et al, 2009).In any case, our results are preliminary given theamount of subjects that we tested (13 versus around290 for GIVE-2), but they are indeed encouraging.In particular, our system helped users to identify bet-ter the objects that they needed to manipulate in thevirtual world, as shown by the low number of mouseactions required to complete the task (a high numberindicates that the user must have manipulated wrongobjects).
This correlates with the subjective evalu-ation of referring expression quality (see next sec-tion).We performed a detailed analysis of the instruc-tions uttered by our system that were unsuccessful,that is, all the instructions that did not cause the in-tended reaction as annotated in the corpus.
From the2081 instructions uttered in the 13 interactions, 1304(63%) of them were successful and 777 (37%) wereunsuccessful.Given the limitations of the annotation discussedin Section 3.1 (wrong annotation of correction ut-terances and no representation of user orientation)we classified the unsuccessful utterances using lexi-cal cues into 1) correction (?no?,?don?t?,?keep?, etc.
),2) orientation instruction (?left?, ?straight?, ?behind?,etc.)
and 3) other.
We found that 25% of the unsuc-cessful utterances are of type 1, 40% are type 2, 34%are type 3 (1% corresponds to the default utterance?go?
that our system utters when the set of candidateutterances is empty).
Frequently, these errors led tocontradictions confusing the player and significantlyaffecting the completion time of the task as shown inTable 1.
In Section 6 we propose an improved virtualinstructor designed as a result of this error analysis.5.2 Subjective metricsThe subjective measures were obtained from re-sponses to the GIVE-2 questionnaire that was pre-sented to users after each game.
It asked users to ratedifferent statements about the system using a contin-uous slider.
The slider position was translated to anumber between -100 and 100.
As done in GIVE-2, for negative statements, we report the reversedscores, so that in Tables 2 and 3 greater numbersare always better.
In this section we compare our re-sults with the systems NA, Saar and NM as we didin Section 5.1, we cannot compare against human in-structors because these subjective metrics were notcollected in (Gargett et al, 2010).The GIVE-2 Challenge questionnaire includestwenty-two subjective metrics.
Metrics Q1 to Q13and Q22 assess the effectiveness and reliability ofinstructions.
For almost all of these metrics we gotsimilar or slightly lower results than those obtainedby the three hand-coded systems, except for threemetrics which we show in Table 2.
We suspect thatthe low results obtained for Q5 and Q22 relate tothe unsuccessful utterances identified and discussedin Section 5.1.
The high unexpected result in Q6 isprobably correlated with the low number of mouseactions mentioned in Section 5.1.NA Saar NM OURQ5: I was confused about which direction to go in29 5 9 -12Q6: I had no difficulty with identifying the objects thesystem described for me18 20 13 40Q22: I felt I could trust the system?s instructions37 21 23 0Table 2: Results for the subjective measures assessing theefficiency and effectiveness of the instructionsMetrics Q14 to Q20 are intended to assess the nat-66uralness of the instructions, as well as the immer-sion and engagement of the interaction.
As Table 3shows, in spite of the unsuccessful utterances, oursystem is rated as more natural and more engaging(in general) than the best systems that competed inthe GIVE-2 Challenge.NA Saar NM OURQ14: The system?s instructions sounded robotic-4 5 -1 28Q15: The system?s instructions were repetitive-31 -26 -28 -8Q16: I really wanted to find that trophy-11 -7 -8 7Q17: I lost track of time while solving the task-16 -11 -18 16Q18: I enjoyed solving the task-8 -5 -4 4Q19: Interacting with the system was really annoying8 -2 -2 4Q20: I would recommend this game to a friend-30 -25 -24 -28Table 3: Results for the subjective measures assessing thenaturalness and engagement of the instructions6 Conclusions and future workIn this paper we presented a novel algorithm forrapidly prototyping virtual instructors from human-human corpora without manual annotation.
Usingour algorithm and the GIVE corpus we have gener-ated a virtual instructor1 for a game-like virtual en-vironment.
We obtained encouraging results in theevaluation with human users that we did on the vir-tual instructor.
Our system outperforms rule-basedvirtual instructors hand-coded for the same task bothin terms of objective and subjective metrics.
It isimportant to mention that the GIVE-2 hand-codedsystems do not need a corpus but are tightly linkedto the GIVE task.
Our algorithm requires human-human corpora collected on the target task and en-vironment, but it is independent of the particular in-struction giving task.
For instance, it could be usedfor implementing game tutorials, real world naviga-tion systems or task-based language teaching.In the near future we plan to build a new versionof the system that improves based on the error anal-ysis that we did.
For instance, we plan to change1Demo at cs.famaf.unc.edu.ar/?luciana/give-OURour discretization mechanism in order to take orien-tation into account.
This is supported by our algo-rithm although we may need to enlarge the corpuswe used so as not to increase the number of situa-tions in which the system does not find anything tosay.
Finally, if we could identify corrections auto-matically, as suggested in (Raux and Nakano, 2010),we could get another increase in performance, be-cause we would be able to treat them as correctionsand not as instructions as we do now.In sum, this paper presents a novel way of au-tomatically prototyping task-oriented virtual agentsfrom corpora who are able to effectively and natu-rally help a user complete a task in a virtual world.ReferencesSudeep Gandhe and David Traum.
2007.
Creating spo-ken dialogue characters from corpora without annota-tions.
In Proceedings of Interspeech, Belgium.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 corpus ofgiving instructions in virtual environments.
In Proc.
ofthe LREC, Malta.Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,and David Traum.
2009.
A virtual tour guide forvirtual worlds.
In Proc.
of IVA, pages 372?378, TheNetherlands.
Springer-Verlag.Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-ton Leuski, and Albert A. Rizzo.
2007.
Virtual pa-tients for clinical therapist skills training.
In Proc.
ofIVA, pages 197?210, France.
Springer-Verlag.Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Sara Dalzel-Job, JohannaMoore, and Jon Oberlander.
2009.
Validating theweb-based evaluation of nlg systems.
In Proc.
of ACL-IJCNLP, Singapore.Alexander Koller, Kristina Striegnitz, Andrew Gargett,Donna Byron, Justine Cassell, Robert Dale, JohannaMoore, and Jon Oberlander.
2010.
Report on the sec-ond challenge on generating instructions in virtual en-vironments (GIVE-2).
In Proc.
of INLG, Dublin.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective questionanswering characters.
In Proc.
of SIGDIAL, pages 18?27, Australia.
ACL.Antoine Raux and Mikio Nakano.
2010.
The dynamicsof action corrections in situated interaction.
In Proc.of SIGDIAL, pages 165?174, Japan.
ACL.Verena Rieser and Oliver Lemon.
2010.
Learning hu-man multimodal dialogue strategies.
Natural Lan-guage Engineering, 16:3?23.67
