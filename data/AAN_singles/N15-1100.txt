Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 984?989,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsWord Embedding-based Antonym Detection using Thesauri andDistributional InformationMasataka Ono, Makoto Miwa, Yutaka SasakiDepartment of Advanced Science and TechnologyToyota Technological Institute2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan{sd12412, makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jpAbstractThis paper proposes a novel approach to trainword embeddings to capture antonyms.
Wordembeddings have shown to capture synonymsand analogies.
Such word embeddings, how-ever, cannot capture antonyms since they de-pend on the distributional hypothesis.
Ourapproach utilizes supervised synonym andantonym information from thesauri, as wellas distributional information from large-scaleunlabelled text data.
The evaluation results onthe GRE antonym question task show that ourmodel outperforms the state-of-the-art sys-tems and it can answer the antonym questionsin the F-score of 89%.1 IntroductionWord embeddings have shown to capture synonymsand analogies (Mikolov et al, 2013b; Mnih andKavukcuoglu, 2013; Pennington et al, 2014).
Wordembeddings have also been effectively employedin several tasks such as named entity recogni-tion (Turian et al, 2010; Guo et al, 2014), adjectivalscales (Kim and de Marneffe, 2013) and text classi-fication (Le and Mikolov, 2014).
Such embeddingstrained based on distributional hypothesis (Harris,1954), however, often fail to recognize antonymssince antonymous words, e.g.
strong and weak, oc-cur in similar contexts.
Recent studies focuses onlearning word embeddings for specific tasks, suchas sentiment analysis (Tang et al, 2014) and de-pendency parsing (Bansal et al, 2014; Chen et al,2014).
These motivate a new approach to learn wordembeddings to capture antonyms.Recent studies on antonym detection have shownthat thesauri information are useful in distinguishingantonyms from synonyms.
The state-of-the-art sys-tems achieved over 80% in F-score on GRE antonymtests.
Yih et al (2012) proposed a Polarity Induc-ing Latent Semantic Analysis (PILSA) that incor-porated polarity information in two thesauri in con-structing a matrix for latent semantic analysis.
Theyadditionally used context vectors to cover the out-of-vocabulary words; however, they did not use wordembeddings.
Recently, Zhang et al (2014) pro-posed a Bayesian Probabilistic Tensor Factorization(BPTF) model to combine thesauri information andexisting word embeddings.
They showed that theusefulness of word embeddings but they used pre-trained word embeddings.In this paper, we propose a novel approachto construct word embeddings that can captureantonyms.
Unlike the previous approaches, our ap-proach directly trains word embeddings to representantonyms.
We propose two models: a Word Em-bedding on Thesauri information (WE-T) model anda Word Embeddings on Thesauri and Distributionalinformation (WE-TD) model.
The WE-T model re-ceives supervised information from synonym andantonym pairs in thesauri and infers the relationsof the other word pairs in the thesauri from the su-pervised information.
The WE-TD model incorpo-rates corpus-based contextual information (distribu-tional information) into the WE-T model, which en-ables the calculation of the similarities among in-vocabulary and out-of-vocabulary words.984co-occurrenceRelations in Thesauri Relations inferred by WE-TDistributional informationRelations inferredby WE-TDdispersegarnerscatternucleatedispersegarnerscattersynonymantonymdispersegarnerscatterWE-T(?
2.1)WE-TD(?
2.2)dispersegarnerscatternucleateFigure 1: Overview of our approach.
When we use the thesauri directly, disperse and garner are known to be antony-mous and disperse and scatter are known to be synonymous, but the remaining relations are unknown.
WE-T infersindirect relations among words in thesauri.
Furthermore, WE-TD incorporates distributional information, and therelatedness among in-vocabulary and out-of-vocabulary words (nucleate here) are obtained.2 Word embeddings for antonymsThis section explains how we train word embed-dings from synonym and antonym pairs in thesauri.We then explain how to incorporate distributional in-formation to cover out-of-vocabulary words.
Fig-ure 1 illustrates the overview of our approach.2.1 Word embeddings using thesauriinformationWe first introduce a model to train word embeddingsusing thesauri information alone, which is called theWE-T model.
We embed vectors to words in the-sauri and train vectors to represent synonym andantonym pairs in the thesauri.
More concretely, wetrain the vectors by maximizing the following objec-tive function:?w?V?s?Swlog ?
(sim(w, s))+?
?w?V?a?Awlog ?
(?sim(w, a))(1)V is the vocabulary in thesauri.
Swis a set of syn-onyms of a word w, and Awis a set of antonyms ofa word w.
?
(x) is the sigmoid function11+e?x.
?
isa parameter to balance the effects of synonyms andantonyms.
sim(w1, w2) is a scoring function thatmeasures a similarity between two vectors embed-ded to the corresponding words w1and w2.
We usethe following asymmetric function for the scoringfunction:sim(w1, w2) = vw1?
vw2+ bw1(2)vwis a vector embedded to a word w and bwis ascalar bias term corresponding to w. This similarityscore ranges from minus infinity to plus infinity andthe sigmoid function in Equation (1) scales the scoreinto the [0, 1] range.The first term of Equation (1) denotes the sum ofthe similarities between synonym pairs.
The secondterm of Equation (1) denotes the sum of the dissimi-larities between antonym pairs.
By maximizing thisobjective, synonym and antonym pairs are tuned tohave high and low similarity scores respectively, andindirect antonym pairs, e.g., synonym of antonym,will also have low similarity scores since the em-beddings of the words in the pairs will be dissimi-lar.
We use AdaGrad (Duchi et al, 2011) to maxi-mize this objective function.
AdaGrad is an onlinelearning method using a gradient-based update withautomatically-determined learning rate.2.2 Word embeddings using thesauri anddistributional informationNow we explain a model to incorporate corpus-based distributional information into the WE-Tmodel, which is called the WE-TD model.We hereby introduce Skip-Gram with NegativeSampling (SGNS) (Mikolov et al, 2013a), whichthe WE-TD model bases on.
Levy and Goldberg(2014) shows the objective function for SGNS can985be rewritten as follows.
?w?V?c?V{#(w, c) log ?
(sim(w, c))+ k#(w)P0(c) log ?
(?sim(w, c))}(3)The first term represents the co-occurrence pairswithin a context window of C words preceding andfollowing target words.
#(w, c) stands for the num-ber of appearances of a target word w and its con-text c. The second term represents the negative sam-pling.
k is a number of negatively sampled words foreach target word.
#p(w) is the number of appear-ances of w as a target word, and its negative contextc is sampled from a modified unigram distributionP0(Mikolov et al, 2013a).
We employ the subsam-pling (Mikolov et al, 2013a), which discards wordsaccording to the probability of P (w) = 1 ?
?tp(w).p(w) is the proportion of occurrences of a word win the corpus, and t is a threshold to control the dis-card.
When we use a large-scale corpus directly, theeffects of rare words are dominated by the effects offrequent words.
Subsampling alleviates this prob-lem by discarding frequent words more often thanrare words.To incorporate the distributional information intothe WE-T model, we propose the following objec-tive function, which simply adds this objective func-tion to Equation 1 with an weight ?:?
{?w?V?s?Swlog ?
(sim(w, s))+?
?w?V?a?Awlog ?
(?sim(w, a))}+?w?V?c?V{#(w, c) log ?
(sim(w, c))+k#(w)P0(c) log ?
(?sim(w, c))}(4)This function can be further arranged as?w?V?c?V{Aw,clog ?
(sim(w, c))+ Bw,clog ?
(?sim(w, c))}(5)Here, the coefficientsAw,candBw,care sums of cor-responding coefficients in Equation 4.
These termscan be pre-calculated by using the number of ap-pearances of contextual word pairs, unigram distri-butions, and synonym and antonym pairs in thesauri.The objective is maximized by using AdaGrad.We skip some updates according to the coefficientsAw,cand Bw,cto speed up the computation; weignore the terms with extremely small coefficients(< 10?5) and we sample the terms according to thecoefficients when the coefficients are less than 1.3 Experiments3.1 Evaluation settingsThis section explains the task setting, resource fortraining, parameter settings, and evaluation metrics.3.1.1 GRE antonym question taskWe evaluate our models and compare them withother existing models using GRE antonym ques-tion dataset originally provided by Mohammad etal.
(2008).
This dataset is widely used to evaluatethe performance of antonym detection.
Each ques-tion has a target word and five candidate words, andthe system has to choose the most contrasting wordto the target word from the candidate words (Mo-hammad et al, 2013).
All the words in the questionsare single-token words.
This dataset consists of twoparts, development and test, and they have 162 and950 questions, respectively.
Since the test part con-tains 160 development data set, We will also reportresults on 790 (950-160) questions following Mo-hammad et al (2013).In evaluating our models on the questions, we firstcalculated similarities between a target word and itscandidate words.
The similarities were calculatedby averaging asymmetric similarity scores using thesimilarity function in Equation 2.
We then chose aword which had the lowest similarity among them.When the model did not contain any words in a ques-tion, the question was left unanswered.3.1.2 Resource for trainingFor supervised dataset, we used synonym andantonym pairs in two thesauri: WordNet (Miller,1995) and Roget (Kipfer, 2009).
These pairs wereprovided by Zhang et al (2014)1.
There were 52,760entries (words), each of which had 11.7 synonymson average, and 21,319 entries, each of which had6.5 antonyms on average.1https://github.com/iceboal/word-representations-bptf986Dev.
Set Test Set (950) Test Set (790)Prec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
FEncarta lookup?0.65 0.61 0.63 0.61 0.56 0.59 ?
?
?WordNet & Roget lookup?1.00 0.49 0.66 0.98 0.45 0.62 0.98 0.45 0.61WE-T 0.92 0.71 0.80 0.90 0.72 0.80 0.90 0.72 0.80WordNet + Affix heuristics0.79 0.66 0.72 ?
?
?
0.77 0.63 0.69+ Adjacent category annotation?WE-D 0.09 0.08 0.09 0.08 0.07 0.07 0.07 0.07 0.07Encarta PILSA0.88 0.87 0.87 0.81 0.80 0.81 ?
?
?+ S2Net + Embedding?WordNet & Roget BPTF?0.88 0.88 0.88 0.82 0.82 0.82 ?
?
?WE-TD 0.92 0.91 0.91 0.90 0.88 0.89 0.89 0.87 0.88Table 1: Results on the GRE antonym question task.
?is from Yih et al (2012),?is from Zhang et al (2014), and?is from Mohammad et al (2013).
?slightly differs from the result in Zhang et al (2014) since thesauri can containmultiple candidates as antonyms and the answer is randomly selected for the candidates.Error Type Description# ExampleErrors Target Gold PredictedContrastingPredicted answer is contrasting,7reticence loquaciousness stormbut not antonym.
dussuade exhort extolDegreeBoth answers are antonyms, but gold3 postulate verify rejecthas a higher degree of contrast.Incorrect gold Gold answer is incorrect.
2 flinch extol advanceWrong Gold and predicted answers are1 hapless fortunate happyexpansion both in the expanded thesauri.Incorrect Predicted answer is not contrasting.
1 sessile obile ceasingTotal 14 ?
?
?Table 2: Error types by WE-TD on the development set.We obtained raw texts fromWikipedia on Novem-ber 2013 for unsupervised dataset.
We lowercasedall words in the text.3.1.3 Parameter settingsThe parameters were tuned using the developmentpart of the dataset.
In training the WE-T model, thedimension of embeddings was set to 300, the num-ber of iteration of AdaGrad was set to 20, and theinitial learning rate of AdaGrad was set to 0.03.
?in Equation 1 were set to 3.2, according to the pro-portion of the numbers of synonym and antonympairs in the thesauri.
In addition to these parameters,when we trained the WE-TD model, we added thetop 100,000 frequent words appearing in Wikipediainto the vocabulary.
The parameter ?
was set to 100,the number of negative sampling k was set as 5, thecontext window size C was set to 5, the thresholdfor subsampling2was set to 10?8.3.1.4 Evaluation metricsWe used the F-score as a primary evaluation met-ric following Zhang et al (2014).
The F-score is theharmonic mean of precision and recall.
Precision isthe proportion of correctly answered questions overanswered questions.
Recall is the proportion of cor-rectly answered questions over the questions.2This small threshold is because this was used to balance theeffects of supervised and unsupervised information.9873.2 ResultsTable 1 shows the results of our models on the GREantonym question task.
This table also shows theresults of previous systems (Yih et al, 2012; Zhanget al, 2014; Mohammad et al, 2013) and modelstrained on Wikipedia without thesauri (WE-D) forthe comparison.The low performance of WE-D illuminates theproblem of distributional hypothesis.
Word em-beddings trained by using distributional informationcould not distinguish antonyms from synonyms.Our WE-T model achieved higher performancethan the baselines that only look up thesauri.
Inthe thesauri information we used, the synonyms andantonyms have already been extended for the origi-nal thesauri by some rules such as ignoring part ofspeech (Zhang et al, 2014).
This extension con-tributes to the larger coverage than the original syn-onym and antonym pairs in the thesauri.
This im-provement shows that our model not only capturesthe information of synonyms and antonyms pro-vided by the supervised information but also infersthe relations of other word pairs more effectivelythan the rule-based extension.Our WE-TD model achieved the highest scoreamong the models that use both thesauri and distri-butional information.
Furthermore, our model hassmall differences in the results on the developmentand test parts compared to the other models.3.3 Error AnalysisWe analyzed the 14 errors on the development set,and summarized the result in Table 2.Half of the errors (i.e., seven errors) were causedin the case that the predicted word is contrasting tosome extent but not antonym (?Contrasting?).
Thismight be caused by some kind of semantic drift.
Inorder to predict these gold answers correctly, con-straints of the words, such as part of speech and se-lectional preferences, need to be used.
For example,?venerate?
usually takes ?person?
as its object, while?magnify?
takes ?god.?
Three of the errors werecaused by the degree of contrast of the gold and thepredicted answers (?Degree?).
The predicted wordcan be regarded as an antonym but the gold answeris more appropriate.
This is because our model doesnot consider the degree of antonymy, which is out ofour focus.
One of the questions in the errors had anincorrect gold answer (?Incorrect gold?).
We foundthat in one case both gold and predicted answers arein the expanded antonym dictionary (?Wrong expan-sion?).
In expanding dictionary entries, the gold andpredicted answers were both included in the wordlist of an antonym entries.
In one case, the predictedanswer was simply wrong (?Incorrect?
).4 ConclusionsThis paper proposed a novel approach that trainsword embeddings to capture antonyms.
We pro-posed two models: WE-T andWE-TDmodels.
WE-T trains word embeddings on thesauri information,and WE-TD incorporates distributional informationinto the WE-T model.
The evaluation on the GREantonym question task shows that WE-T can achievea higher performance over the thesauri lookup base-lines and, by incorporating distributional informa-tion, WE-TD showed 89% in F-score, which out-performed the conventional state-of-the-art perfor-mances.
As future work, we plan to extend our ap-proaches to obtain word embeddings for other se-mantic relations (Gao et al, 2014).ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 809?815, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Wenliang Chen, Yue Zhang, and Min Zhang.
2014.
Fea-ture embedding for dependency parsing.
In Proceed-ings of COLING 2014, the 25th International Confer-ence on Computational Linguistics: Technical Papers,pages 816?826, Dublin, Ireland, August.
Dublin CityUniversity and Association for Computational Lin-guistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch, 12:2121?2159, July.Bin Gao, Jiang Bian, and Tie-Yan Liu.
2014.
Wordrep:A benchmark for research on learning word represen-tations.
ICML 2014Workshop on Knowledge-PoweredDeep Learning for Text Mining.988Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.2014.
Revisiting embedding features for simple semi-supervised learning.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 110?120, Doha, Qatar,October.
Association for Computational Linguistics.Zellig S Harris.
1954.
Distributional structure.
Word,10(23):146?162.Joo-Kyung Kim andMarie-Catherine de Marneffe.
2013.Deriving adjectival scales from continuous space wordrepresentations.
In Proceedings of the 2013 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1625?1630, Seattle, Washington, USA,October.
Association for Computational Linguistics.Barbara Ann Kipfer.
2009.
Roget?s 21st Century The-saurus.
Philip Lief Group, third edition edition.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
In Tony Jebaraand Eric P. Xing, editors, Proceedings of the 31st In-ternational Conference on Machine Learning (ICML-14), pages 1188?1196.
JMLR Workshop and Confer-ence Proceedings.Omer Levy and Yoav Goldberg.
2014.
Neural word em-bedding as implicit matrix factorization.
In Z. Ghahra-mani, M.Welling, C. Cortes, N.D. Lawrence, and K.Q.Weinberger, editors, Advances in Neural InformationProcessing Systems 27, pages 2177?2185.
Curran As-sociates, Inc.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013a.
Distributed representa-tions of words and phrases and their compositionality.In C.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahra-mani, and K.Q.
Weinberger, editors, Advances in Neu-ral Information Processing Systems 26, pages 3111?3119.
Curran Associates, Inc.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751, Atlanta, Georgia, June.Association for Computational Linguistics.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Commun.
ACM, 38(11):39?41, November.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastive es-timation.
In Z. Ghahramani, M. Welling, C. Cortes,N.D.
Lawrence, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems 26,pages 2265?2273.
Curran Associates, Inc.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008.Computing word-pair antonymy.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 982?991, Honolulu,Hawaii, October.
Association for Computational Lin-guistics.Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2013.
Computing lexical contrast.Computational Linguistics, 39(3):555?590, Septem-ber.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1532?1543, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,and Bing Qin.
2014.
Learning sentiment-specificword embedding for twitter sentiment classification.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 1555?1565, Baltimore, Mary-land, June.
Association for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 384?394, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Wen-tau Yih, Geoffrey Zweig, and John Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1212?1222, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Jingwei Zhang, Jeremy Salwen, Michael Glass, and Al-fio Gliozzo.
2014.
Word semantic representations us-ing bayesian probabilistic tensor factorization.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages1522?1531.
Association for Computational Linguis-tics.989
