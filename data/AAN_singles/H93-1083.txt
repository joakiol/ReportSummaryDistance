Segment-Based Acoustic Modelsfor Continuous Speech RecognitionMari Ostendorf J. Robin RohlicekBoston  Univers i ty  BBN Inc.Boston,  MA 02215 Cambr idge,  MA 02138PROJECT GOALSThe goal of this project is to develop improved acousticmodels for speaker-independent recognition of continu-ous speech, together with emeient search algorithms ap-propriate for use with these models.
The current workon acoustic modeling is focussed on stochastic, segment-based models that capture the time correlation of a se-quence of observations (feature vectors) that correspondto a phoneme, hierarchical stochastic models that cap-ture higher level intra-utterance orrelation, and multi-pass search algorithms for implementing these more com-plex models.
This research as been jointly sponsored byDARPA and NSF under NSF grant IPd-8902124 and byDARPA and ONR under ONR grant N00014-92-J-1778.RECENT RESULTS?
Implemented ifferent auditory-based signal pro-ceasing algorithms and evaluated their use in recog-nition on the TIMIT corpus, finding no performancegains relative to cepstral parameters probably dueto the non-Gaussian ature of auditory features.?
Improved the score combination technique for N-Best rescoring, through normalizing scores by sen-tence length to obtain more robust weights that al-leviate problems associated with test set mismatch.?
Further investigated agglomerative and divisiveclustering methods for estimating robust context.-dependent models, and introduced a new clusteringcriterion based on a likelihood ratio test; obtaineda slight improvement in performance with an asso-ciated reduction in storage costs of a factor of two.?
Extended the classification and segmentation scor-ing formalism to handle context-dependent modelswithout requiring the assumption of independenceof features between phone segments (using maxi-mum entropy methods); evaluated ifferent segmen-tation scores with results suggesting more work isneeded in this area.?
Evaluated a new distribution mapping, which ledto an 8% reduction in error on the development testset but no improvement on other test sets.?
Investigated the use of different phone sets andprobabilistic multiple-pronunciation networks; noimprovements were obtained on the RM corpus,though there may be gains in another domain.?
Extended the two level segment/rnicrosegment for-malism to application in word recognition usingcontext-dependent models; evaluated the trade-offsassociated with modeling trajectories vs. (non-tied)microsegment mixtures, finding that mixtures aremore useful for context-independent modeling butrepresentation of a trajectory is more useful forcontext-dependent modeling.?
Investigated the use of tied mixtures at the framelevel (as opposed to the microsegment level), evalu-ating different covariance assumptions and trainingconditions; developed new, faster mixture trainingalgorithms; and achieved a 20% reduction in worderror over our previous best results on the ResourceManagement task.
Current SSM performance ratesare 3.6% word error on the Oct89 test set and 7.3%word error on the Sep92 test set.PLANS FOR THE COMING YEAR?
Continue work in the classification and segmenta-tion scoring paradigm; demonstrate improvementsassociated with novel models and/or features.?
Port the BU recognition system to the Wall StreetJournal (WSJ) task, 5000 word vocabulary.?
Develop a stochastic formalism for modeling intra-utterance dependencies assuming a hierarchicalstructure.?
Investigate unsupervised adaptation in the WSJtask domain.?
Investigate m~ulti-pass earch algorithms that use alattice rather .
'than N-Best representation f recog-nition hypotheses.389
