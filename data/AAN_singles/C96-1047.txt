Finite-state phrase parsing by rule sequencesMarc Vilain and David DayThe MITRE Corporation202 Burlington Rd.Bedford, MA 01720 USAmbv@mitre.org, day@mitre.orgAbstractWe present a novel approach to parsing phrasegrammars based on Eric Brill's notion of rulesequences.
The basic framework we describe hassomewhat less power than a finite-state machine,and yet achieves high accuracy on standard phraseparsing tasks.
The rule language is simple, whichmakes it easy to write rules.
Further, this simpli-city enables the automatic acquisition of phrase-parsing rules through an error-reduction strategy.This paper explores an approach to syntactic analysisthat is unconventional in several respects.
To beginwith, we are concerned not so much with the tradi-tional goal of analyzing the comprehensive structure ofcomplete sentences, as much as with assigning partialstructure to parts of sentences.
The fragment of interesthere is demonstrably a subset of the regular sets, andwhile these languages are traditionally analyzed withfinite-state automata, our approach relies instead on therule sequence architecture defined by Eric Brill.Why restrict ourselves to the finite-state case?
Somelinguistic phenomena re easier to model with regularsets than context-free grammars.
Proper names are acase in point, since their syntactic distribution partiallyoverlaps that of noun phra~ses in general; as this overlapis only partial, name analysis within a full context-freegrammar is cumbersome, and some approaches havetaken to include finite-state name parsers as a front-endto a principal context-free parsing stage (Jacobs et alI99i).
Proper names are of further interest, since theiridentifi cation is independently motivated as valuable toboth information retrieval and extraction (Sundheim~996).
Further, several promising recent approaches toinformation extraction rely on little more than finite-state machines to perform the entire extraction analysis(Appelt et al I993 , Grishman I995).Why approach this problem with rule sequences?
Inthis paper we maka the case that rule sequences succeedat this task through their simplicity and speed.
Mostimportant, they support mixed-mode acquisition: therules are both easy for an engineer to write and easy tolearn automatically.Rule sequencesAs part of our work in information extraction, we havebeen extensively exploring the use of rule sequences.Our information extraction prototype, Alembic, is infact based on a pipeline of rule sequence processors thatrun the gamut from part-of-speech tagging, to phraseidentification, to sentence parsing, to inference(Aberdeeen et al I995).
In each case, the underlyingmethod is identical.
Processing takes place bysequentially relabeling the corpus under consideration.Each sequential step is driven by a rule that attempts topatch residual errors left in place in the preceding steps.The patching process as a whole is itself preceded by aninitial labeling phase that provides an approximatelabeling as a starting point for rule application.This patching architecture, illustrated in Fig.
1, wascodified by Eric Brill, who first exploited it for part-of-speech tagging (Brill I993).
In the part-of-speech appli-cation, initial labeling is provided by lexicon lookup:lexemes are initially tagged with the most common partof speech assigned to them in a training corpus.
Thisinitial labeling is refined by two sets of transformations.Morphological transformations relabel the initial(default) tagging of those words that failed to be foundin the lexicon.
The morphological rules arc followed bycontextual transformations: these rules inspect lexica\[context to relabel lexemes that are ambiguous withrespect o part-of-speech.
In effect, the morphologicaltransformations patch errors that were due to gaps inthe lexicon, and the contextual rules patch errors thatwere due to the initial assignment of a lexeme's mostcommon tag.Phrase identification: some examplesSequencing, patching, and simplicity, the hallmarks ofBrill's part-of-speech tagger, are also characteristic ofour phrase parser.
In our approach, phrases are initiallybuilt around word sequences that meet certain lexical orpart-of-speech criteria.
The sequenced phrase-findingrules then grow the boundaries of phrases or set theirlabel, according to a repertory of simple lexical andcontextual tests.
For example, the following rule assignsa label of oa(; to an unlabeled phrase just in case thephrase is ended by the word "Inc."(def-phraserlabeJ NONE ; phrase is currently; unlabelledright-wd-1 lexeme "inc." ; rightmost word in the; phrase is "inc."labebaction ORG) ; change the phrase's label,; but not its boundariesNow, consider the following partially labelled string:<none>Donald F. DeScenza</none>, analyst with<none>Nomura Securities Inc.</none>274text Initial ~ ( Labelled text ) __  >C Finaltext@nprocessed ) ?
lexlconlabelling:lookup j~ transformat lons:~?
morphological rules JFigure 1: Brill's rule sequence architecture asapplied to partmf-speech tagging.
)The SGML markup delimits phrases whose boun-daries were identified by the initial phrase-finding pass.Of these phrases, the second successfully triggers theexample rule, yielding the following relabeled string.<none>Donald F. PeScenza</none>, analyst with<org>Nomura Securities Inc.</org>The rule, which seems both as obvious as walkingand as fool-proof comes from the name-findinigprocessor we developed for our participation i  the 6 mMessage Understanding Conference (MtJC-6).
As itturns out, though, the rule is in fact not error-proof,and causes both errors of omission (i.e.
recall errors)and commission (i.e.
precision errors).
Consider thecase of "Volkswagen of America Inc." Because theinitial phrase labeling is only approximate, the string isbroken into two sub-phr~es separated by "of".<none>golkswagen</none> of <none>AmericaInc,</none>The example rule designates the partial phrase"America Inc." as an out;, a precision error because ofits partiality, ,and fails to produce an otto label spanningthe entire string (a recall error).<none>golkswagen<lnone> of <org>America Inc.</org>This problem is patched by a subsequent name-finding rule, namely the following.
(def-phraseelabel ORGleft-wd-1 test country?left-ctxt-I lexeme "og'le%-ctxt-2 phrase NONEbounds-action MERGElabbel-ac~ion ORG); this is an organization; is the leftmost lexeme;in the phrase on a list; of country words?
; to the left of the; phrase is the word "og'; tothe left of that is an; unlabelled phrase; merge the entire left; contextinto the OIZG,; phrase and allThe first two clauses of the rule are antecedents hatlook for phrases uch as "America inc." The next twoclauses are further antecedents hat look to the left ofthe phrase for contextual patterns of form"<non~>,.
,</none> of".The final two clauses incorporate the left contextwholesale into the triggering phrase, yielding:<org>golkswagen of America Inc.</org>This rule effectively patches tile errors caused by itspredecessor in the rule sequence, and simultaneouslyeliminates both a recall and a precision error.The phrase finderWith these examples as background, we may nowturn our attention to the technical details of the phrasefinding process.
As noted above, this process occurs intwo main steps, an initial labeling pass followed by theapplication of a rule sequence.Initial phrase labelingThe initial labeling process eeds the phrase-finderwith candidate phrases.
These candidate phrases neednot be any more than approximations, in partictdar, itis not necessary for these candidates to have whollyaccurate boundaries, as their left and right edges can beadjusted later by means of patching rules.
It is also notneccssatT for these candidates to be unfragmented, asfragments can be reassembled later, just as with "Volks-wagen of America Inc." Further, applications thatrequire multiple types of phrase labels, need not choosesuch a label during the initial phrase-finding pass.What is important is that the initial phrase identifi-cation Fred the cores of phrases reliably, even if completephrases arc not identified.
That is, it must partiallyalign some kind of candidate phrase ~ for every phrase(~ that is actually present in the input.
Extending aconcept from information retrieval, this amounts tomaximizing what we might call initial recall, i.e.,lit= I (1) I I / I (i) I,where (IJ is the set of actual phrases in a test set, K is theset of candidate phrases generated by the initialphrasing passs, and cI) I is tile set of those (D < q~ that arcpartially aligned with some 1( c K.The general strategy we have adopted for findinginitial phrase seeds is to look for either runs of lcxcmesin a fixed word list or runs of lexemcs that have beentagged a certain way by our part-of-speech tagger.1)iffercnt instantiations of this general strategy forinitial phrase labeling naturally arise for differentphrase-finding tasks.
For example, on the classic"proper names" task in mixed-case text, we havcachieved good results starting from runs of lexemestagged with Nm, or m'~ps, the Penn Treebank propernoun tags.
This strategy achieves the desired highinitial recall R I , as these tags are well-correlated withbona fide proper nanles ~md are reliably produced inmixed-case text by our part-of-speech tagger.
Thisstrategy does not yield quite as good initial precision(i.e., it yields false positives) for a number of rcasons,such as the fragmentation problcms noted above, e.g.,golkswagen/NNP of/IN America/NNP Inc./NNPOnce again, though, these initial precision errors arcreadily addressed by patching rules.275Clauee type Syntax DefinitionContextual testsPhrase-internaltestsLabel testActionsleft-ctx~-l, ef~-ctxt-2right-ctxt~l, rig ht-ctxt-2le%-wd-1, left-wd-2right-wd-1, right-wd-2wd-anywd-spanlabellabel-actionbounds~actionTest one place (resp.
two places) to the left of the phraseTest one place (resp.
two places) to the right of the phraseTest first (resp.
second) word of phraseTest last (resp.
next-to-last) word of phraseTest each word of phrase in succession.
Succeeds if any word in thephrase passes the test.Test entire string spanned by phraseTest phrase's labelSets the label of the phraseModify the phrase's !eft or right boundariesTable h Repertory of unary rule clauses.Phrase-finding rulesA phrase-finding rule in our framework is made up ofseveral clauses.
The corc of the rule consists of clausesthat test thc lexical context around a candidatc phrase 1<or that test lcxcmcs spanned by 1(.
The repertory ofthese test loci is given in "Fable 1.
At any given locus, atest may either search for a particular lcxcmc, match alexeme against a closed word list, match a part ofspeech, or match a phrase of a given type.
Most rulesalso test the label of thc candidate phrase 1(.The unary contextual tests in the table may also bccombincd to form binary or ternary tests.
For example,combining I,EVT-C'IXW-I and i~mrr-cwxa'-z clauses yieldsa rule that tests for the left bigram contcxt.
This wasdone in the ore  defragmentation rule described earlier.A rule also contains at least one action clause, eithera clause that sets the label of the phrase, or one thatmodifies the boundaries of the phrase.
Finally, somerule actions actually introduce new phrases that embedthe candidate mad its test context; this allows one tobuild non-recursive parse trees.Phrase rule interpreterThe phrase rule interpreter implements the rulelanguage in a straightforward way.
Given a documentto be analyzed, it proceeds through a rule sequence onerule r at a time, and attempts to apply r to every phrasein every sentence in the document.
The interpreter firstattempts to match the test label of r to the label of thecandidate phrase.
If this test succeeds, then theinterpreter attempts to satisfy the rule's contextual testsin the context of the candidate.
If these test succeed,then the rule's bounds and label actions are executed.Beyond this, the only real complexity arises withphrase-finding tasks that require one to maintain atemporary lexicon.
The clearest such example is propername identification.
Indeed, short name forms (e.g.,"Detroit Diesel") can sometimes only be identifiedcorrectly once their component terms have been foundas part of the complete naxne (e.g., "Detroit DieselCorp.").
The converse is also true, as short forms ofperson names (e.g., "Mr. Olatunji") can help identifyfitll nanm forms ( e.g., "Babatunde Olatunji").The interprcter maintains a temporary lexicon on adocument-by-document basis.
Every time theinterpreter changes the label of a phrase $, pairs of form<Z, "c> are added to the lexicon, where ~ is a lcxcmc in~, and "c is the label with which (~ is tagged.
Thislexicon is then exploited to form the associationsbetween short and long proper name forms (through anextension to the rule repertory defined above).Correspondence to the regular setsIt is straightforward to prove that this approachrecognizes a subset of the regular sets, so we will onlysketch the outline of such a proof here.
The proofproceeds inductivcly by constructing a finite statemachinc bt that accepts exactly those strings whichreceive a certain label in the phrase-finding processunder a given rule sequence Z.
We consider each rule pin Z in order, and correspondingly elaborate themachine so as to reproduce the rule's effect.To begin with, consider that the initial phraselabeling proceeds by building phrases around lexemes0~ 1 ..... fz n in a designated word list or by finding runsof certain parts of speech ~t 1 ..... 7Zm.
The machine thatreproduces this initial labeling is thuspl/rq ..... p n/n1pl/nm ..... p n/nm Pl/nl ..... p n/rqAs usual, node labeled "S" is thc start state, and anynode drawn with two circles is ,an accepting state.
TheP i /~i  arc labels stand for all lcxemes in the lexicon thatmay be labeled with the part of speech gJ'The induction step in the construction procccdsfrom ~l.bl , the machine built to reproduce Z up l~hroughrule l\] bl in the sequence, and adds additional states andarcs so as to reproduce Z up through ruh'.
p i.For example, say Pi tests for the presence of a lexemeto the left of a phrase and e~tends the phrase'slxaundaries to include )v. We extend the machine bt to276encode this rule by replacing ~'s current start state Swith a new one S', and adding a ~, transition from S' tothe former start state S. ThusbecomesPv ,U>l@ > O-  - ->0For a rule I~ that tests whether a phrase contains acertain lcxcme ~'i, wc construct an "acccptor" machincthat accepts any string with )~i in its midst.CoCONoting that the regular sets are closed trader inter-section, wc them proceed to build the machine that"intersects" the acccptor with bli.Other rule patterns arc handled with constructionsof a similar flavor--space considerations preclude theirdescription hcre.
Note, howcw:r, that extending thefl:amework with a temporary lexicon makcs it trans-finite-state, lqnally, as with all semi-parsers, themachines we construct in this way must actually beinterpreted as transducers, not just acceptors.Learning rule sequences automaticallyOur experience with writing rule sequences by lt,-md inthis approach as been very positive.
"\['he rule patternsthcmselves are simple, and the fact that they arcsequenced localizes their effccts mid reduccs the scopeof their interactions.
These hand-engineeringadvantages are also conferred upon learning programsthat attcmpt o acquire these rules atttomatica\[ly.The approach we have taken towards discoveringphrase rule sequences automatically is a maximumerror-reduction scheme for selecting the next rule in asequence.
This approach originated with Brill's workon part-of-speech tagging and bracketing (Brill i993).Brill's rule learning algorithm"\['he search for a rule sequence in a given trainingcorpus begins hy first applying the initial labelingfunction, just as would be the case in running acomplete sequence.
Following this, the learningprocedurc needs to consider every rule that can possiblyapply at this juncture, which itself is a function of therule schema laaaguage.
For each such applicable rule *;the learner considers the possible improvement inphrase labeling conferred by r in the current state.
Therule that most reduces the residual error in the trainingdata is selected as the next rule in the sequence.This generate-and-test cycle is contimmd until astopping criterion is reached, which is usually taken asthe point where performance improvement falls below athreshold, or ceases altogether.
Other a\[ternativcsinclude setting a strict limit on the number of ruleslearned, or cross-testing the performance improvementof a rule on a corpus distinct from the training set.The rule search spaceThe language of phrase rules supports a large number ofpossible rules that the phrase rule learner might need toconsider at any one time.
Take one of our smallcrtraining sets, in which there arc ~9I sentences consistingof 6,8IZ word tokens, with z,o77 unique word types.
(ionsidcring only lexical rules (those that look forparticular words), this means that there are as many asI8,693 possibh', unary lexical rules (%077 x 9 ruleschemata), mad IZ,941,787 binat T lexical rules (?.,o77 z x3 simple bigram rule schemata) in the search space.However, by inverting the process, and tabulating onlythose lexical contexts that actually appear in thetraining texts, this search spacc is reduced to z,:.I 9unal T lcxical rules and 854 binary lexical rules.There are two substantively different kinds of rulesto acquire: rules that only change the label of a phrase,and those that change the boundary of a phrase.
Thelatter prcsent a problem \[:or accurately estimating theimprovement of a rule, since sometimes the boundaryrealignment necessary to fix a phrase problem exceedsthe amount by which a single rule can move aboundary--namely, two lexemcs.
For thcse phrascs tobe fixed there will have to be more than one rule tonudge the appropriate phrase botmdaries over.
Wehandle this through a heuristic scoring ftmction thatestimates the wtluc of moving a boundary in such cases.Error estimation methodsA rule that fixes a problem in some cases might wellintroduce rrors in some other cases.
This kind of over-generalization can occur early in the learning process, asnew rules need only improve over an approximateinitial labcting.
The extent o which a candidate rule isrewarded for its specificity and penalized for its over-generalization can have a strong effect on the finalperformance of the rule sequences discovered.We explored the use of three different types ofscoring metrics for use in selecting the "best" of thecompeting rules to add to the sequence.
Initially wemade use of a simple arithmetic difference metric, y -  s,wimrc y (for yield) is the number of additional correctphrase labelings that would be introduced if a rule wereto be added to the rule sequence, and s (for sacrifice) isthe number of new mistaken labelings that would bcintroduced by the addition of the rule.
'\['his is Brill'soriginal metric, but note that it does not differentiatebetween rules whose overall improvement is identical,but whose rate of over-generalization is not.
Forexample, a rule whose yield is IOO and sacrifice is 7 ?
istreated as equally valuable as one whose yield is only 3 ?but which introduces uo overgeneralization at all(sacrifice = o).
This can lead to the selection of low-precision rules, and while small numbers of precisionerrors may be patched, wholesale precision problemsmake subsequent improvement more difficult.277Scoring metric Training TestRecall Precision P&R Recall Precision P&RArithmetic (y-s) 88.8 8I.z 8+8 87.2 79.0 82.
9Log likelihood 81.9 85.7 78.4 8t.o 73.4 77.0F measure, ~:o.8 86.
3 8z.
9 84.
5 85.0 8I.
5 83.zTable 2: Comparative contributions of three scoring measures after 100 learning epochs.
(Training on i495 sentences from the MUc-6 named entities task).The next measure we investigated was oneadvocated by Dunning (I993) which uses a log like-lihood measure for estimating the significance of rareevents in small populations.
This measure did notimprove predsion or recall in the learned sequences.The third scoring measure we investigated was theF-measure (VanRijsbergen 1979), which was introducedin information retrieval to compute a weighted combi-nation of recall and precision.
The F-measure is alsoused extensively in evaluating information extractionsystems at MUG (Chinchor I995).
It is defined as:F = (32 + 1)PR(3 2 +P)RThis measure is conservative in the sense that itsvalue is closer to precision, p, or recall, R, depending onwhich is lower.
By manipulating the ~ paraaneter one isable to control for the relative importance of recall orprecision.
Preliminary exploration shows that a ~ of 0.8seems to boost precision with no significant loss in thelong-term recall or F-measure of the rule sequences.Table z summariz~es the contributions of these threeerror measures towards learning rule sequences for theMUC-6 named entities task (for task details, see below).EvaluationWe have applied this rule sequence approach to avariety of realistic tasks.
These largely arose as part ofour information extraction efforts, and have been eitherdirectly or indirecdy evaluated in the context of twoevaluation conferences: MUC-6 and Mffl' (for Multi-lingual Entity Tagging).
In this paper, we willprimarily report on evaluation conducted in the contextof the MuC-6 named entities task (Sundheim I995).
1The named entities task attempts to measure theability to identify the basic building blocks of mostnewswire analysis applications, e.g., named entities uchas persons, organizations, and geographical locations.Also measured is the identification of some numericexpressions (money and percentiles), dates, and times.This task has become a classic application for finite-state pre-parsers, and indeed our work was in partmotivated by the success that has been achieved by suchsystems in past information extraction evaluations.We have applied a variety of techniques towards thistask.
The easy cases of dates mid times are identified bya separate pre-processor, leaving numeric expressions1We have also measured performance on several syntacticconstructs, (e.g., the so-called noun group), and on semanticsubgrammars, (e.o<, person-title-organization appositions).
(also easy) and "proper names" (the interesting hardpart) to be treated by the rule sequence processor.Hand-crafted RulesWe first approached this task as an engineeringproblem, and wrote a rule sequence by hand to identifythese named entities.
The rule sequence comprises I45named-entity rules, Iz rules for expressions of moneyand percentiles, and 6I rules for geographical comple-ments (as in "Hyundai of Canada").
In addition, therules refer to a few morphological predicates and someshort word lists--one such list, for example lists wordsdesignating business subsidiaries, e.g., "unit".
Theinitial phrase labeling for the proper name cases isimplemented by accumulating runs of NNP- and NNeS-tagged lexemes.
A similar strategy is used for numberexpressions, using numeric tags.The performance of our hand-crafted rule sequenceis summarized in Table 3, below, which gives compo-nent scores on the Mt3c-6 blind test set.
The mostinteresting measures are those for the difficult propername cases.
Our performance here is high, especiallyfor person names.
Our lowest score is on organizationalnames, but note that the system lacks any extensiveorganization ame list.
Aside from ten hard-wirednames, all names are found from first principles.
Onthe easy numeric expressions, performancc is ahnostperfect--precision appears poor for percentiles, but thisis due to an artifact of the testing procedure.
2Machine-crafted RulesTo evaluate the performance of our learning algorithm,we attempted to reproduce substantially the sameenvironment as is used for the hand-crafted rules.
Thelearner had access to the same predefined word lists,including the less-than-perfect TU'S'tmR gazetteer.Further, we only acquired rules for the hardest cases,namely the person, organization, and location phrases.We cut offrule acquisition after the iooth rule.The results for this acquired rule set are surprisinglyencouraging.
As Table 3 shows, these rules achievedhigher recall on the very hardest phrase type(organization) than their hand-crafted counterparts,albeit at a cost in precision.
Overall, however, themachine-crafted rules still lag behind.
When weincorporated them into our information extraction2Our performance vis-a-vis other MUC-6 participantsplaced us in the top third of participating systems.
Except forthe absolute highest performer, all these top-tercile systemswere statistically not distinguishable from each other.278Phrase type NOrganization 419Person 34gl,ocation m 9Money 74Percent ~6All phrases zt5 oHand-crafted rulesRecall Precision85 8794 9494 8799 97tO0 6 79 ~ 9 zOverall t,'= 91.2Machine-learned rulesRecall Preckion87 7978 79D 6888 83Overall F= 85.2Table 3: Performance on the MUC-6 named entities blind tcst.system, the machinc-learned rules achieved an overallnamed cntitics F-score of 85.2, compared to the 91.2achieved by the hand-crafted rttlcs, it should be noted,however, that the system loaded with these machine-crafted rules still outpcrfimned about a third of systemsparticipating in the MUc-6 evaluation.Mult i l ingual evaluation (MH')After the Muc-6 evahtation, the namcd entity task wasextended in various ways to make it more applicablecross-linguistically.
Predictably, this was followed by anew round of evaluations: Mv:r. The target languages intltis case were Spanish, Chinese, and Japanese.
Weapplied our approach m all three.The Mt{'l' cvahtation rcquircd actual system perfor-mance resuhs to be kept strictly ,-monymotts, whichprecludes our reporting here any scores as specific as wehave cited for English.
What wc may legitimatelyreport, however, is that wc have effectively reproducedor bettered our hand-engineered English results in theSpanish mid Japanese t~ks, despite having no nativespeakers of either language (and only the most rudi-mentary reading sldlls in Kanji).
In both cases, we wered~le to exploit part-of-speech tagging and some existingword lists fbr person names and locations.For Chinese, although we had available a wordsegmentcr, we had neither part-o6speech tagger, norword lists, nor even the elementary reading skills wehad for Japanese.
As a result, we had to rely ahnostentirely on the learning procedure to acquire any rulesequences.
1)cspitc thcse impediments, wc cmnc doseto reproducing our results with thc English machinc-lcarned named entidcs rule sequcncc.DiscussionWhat is most encouraging about this approach is howwell it performs on so many dimensions.
We have onlyreported here on nature-finding tasks, but early invcsti-gations in other areas arc encouraging as well.
Withrule sequences that parse noun groups, for instance, wehope to reproduce the utility of other rulc-scqucnceapproaches to text chunking (Ramshaw & MarcusI995).
We are also excited by the promise of thelearning proccdure, not just because it learns goodrules, but dso because the rules it learns can be freelyintermixed with hand-cngineered rules.
This mixed-mode acquisition is unique among natural languagelearning proccdurcs, mid we put it to good use inbuilding our multilingual name-tagging sequences.l)espitc rcsuhs that comparc favorably to those ofmore mature systems, this work is still in its infancy.We still have much to explore, especially with thelearning procedure, lndccd, while the lcamcr induces/'tile sequences that pcrfi~rm well in tim aggrcgatc,individual rules clearly show their mechanical genesis.For instm~cc, whcn the learner must break tics betweenidentically-scoring rule candidates, it often does so inlhlguistically clumsy ways.
At times, the learner mayacquire a good contextual pattern, but may bc unable toextend it to closcly-related cases that would occurnaturally m a linguist.We belicve thcsc problems arc solvable in the ncar~term, and wc have partial solutions in place already.
Asour tcclmiques mature, this validates not only ottrparticular approach Io phrase-finding, but the wholefield of language processing through rule sequences.ReferencesAberdeen, J., Burger, J., Day, D., llirsehman, \].,Robinson, P., & Vilain, M. t995.
"Description of theAlembic" system used for MIJC-6".
Ill Prcdgs.
of'MUC-6,(\]olumbia MD.Appch, I).
E., ttobbs, J. R., Bear, J., Israel, D., &Tyson, M. I993.
"I;AsTUS: A finite-state processor forinformation extraction fi'om rcd-world text."
in Prcdgs.q'  IJCAt-93, Chantb&y, France.Brill, E. 093.
A corpus-based approach m languagelearning.
1)octoral 1)issertation, Univ.
of Pennsylvania.Chinchor, N. 094.
"M uc- 5 evaluation metrics".
InPrcdgs.
t~'MUC-5, Baltimore, Ml3.Dunning, T. 093 .
"Accurate methods for thestatistics of surprise and coincidence".
Comput.
Ling 19 .Grishmml, R. 095- "The NVu system fin" MtJC-6, orwhere's the syntax?"
Ill Prcdgs.
of MOO-6, Cohunbia Ml3.Jacobs, P. S., Krupka, G., &Rau,  L. 199i.
"I.exico-semantic pattern-matching as a companion m parsing".in Prcdgs.
of the Fourth DaUeA Speech and Nat.
Lang.Workshop, San Marco, CA: Morgan Kaufinan.Ramshaw, I.. c/r Marcus, M. 095.
"Text chunkingusing transformation-based l arning".
\[n Preys.
of 3rdWkshp on Very Large Corpora, (;ambridge, MA.Sundhcim, B.
095.
"Named entity task definition".In Prcdgs.
e~MUC-6, Columbia MD.Van Rijsbergen, ('.J.
I979.
Information Retrieval.London: Buttcrsworth.2 7 9
