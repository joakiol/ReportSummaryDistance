Bilingual Parsing with Factored Estimation:Using English to Parse KoreanDavid A. Smith and Noah A. SmithDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{d,n}asmith@cs.jhu.eduAbstractWe describe how simple, commonly understood statisti-cal models, such as statistical dependency parsers, proba-bilistic context-free grammars, and word-to-word trans-lation models, can be effectively combined into a uni-fied bilingual parser that jointly searches for the best En-glish parse, Korean parse, and word alignment, wherethese hidden structures all constrain each other.
Themodel used for parsing is completely factored into thetwo parsers and the TM, allowing separate parameter es-timation.
We evaluate our bilingual parser on the PennKorean Treebank and against several baseline systemsand show improvements parsing Korean with very lim-ited labeled data.1 IntroductionConsider the problem of parsing a language L for whichannotated resources like treebanks are scarce.
Supposewe have a small amount of text data with syntactic an-notations and a fairly large corpus of parallel text, forwhich the other language (e.g., English) is not resource-impoverished.
How might we exploit English parsers toimprove syntactic analysis tools for this language?One idea (Yarowsky and Ngai, 2001; Hwa et al, 2002)is to project English analysis onto L data, ?through?word-aligned parallel text.
To do this, we might use anEnglish parser to analyze the English side of the paralleltext and a word-alignment algorithm to induce word cor-respondences.
By positing a coupling of English syntaxwith L syntax, we can induce structure on the L side ofthe parallel text that is in some sense isomorphic to theEnglish parse.We might take the projection idea a step farther.
Astatistical English parser can tell us much more than thehypothesized best parse.
It can be used to find everyparse admitted by a grammar, and also scores of thoseparses.
Similarly, translation models, which yield wordalignments, can be used in principle to score competingalignments and offer alternatives to a single-best align-ment.
It might also be beneficial to include the predic-tions of an L parser, trained on any available annotatedL data, however few.This paper describes how simple, commonly under-stood statistical models?statistical dependency parsers,probabilistic context-free grammars (PCFGs), and wordtranslation models (TMs)?can be effectively combinedinto a unified framework that jointly searches for the bestEnglish parse, L parse, and word alignment, where thesehidden structures are all constrained to be consistent.This inference task is carried out by a bilingual parser.At present, the model used for parsing is completely fac-tored into the two parsers and the TM, allowing separateparameter estimation.First, we discuss bilingual parsing (?2) and show howit can solve the problem of joint English-parse, L-parse,and word-alignment inference.
In ?3 we describe param-eter estimation for each of the factored models, includ-ing novel applications of log-linear models to Englishdependency parsing and Korean morphological analysis.
?4 presents Korean parsing results with various mono-lingual and bilingual algorithms, including our bilingualparsing algorithm.
We close by reviewing prior work inareas related to this paper (?5).2 Bilingual parsingThe joint model used by our bilingual parser is an in-stance of a stochastic bilingual multitext grammar (2-MTG), formally defined by Melamed (2003).
The 2-MTG formalism generates two strings such that eachsyntactic constituent?including individual words?inone side of the bitext corresponds either to a constituentin the other side or to ?.Melamed defines bilexicalized MTG (L2MTG), whichis a synchronous extension of bilexical grammars suchas those described in Eisner and Satta (1999) and appliesthe latter?s algorithmic speedups to L2MTG-parsing.Our formalism is not a precise fit to either unlexical-ized MTG or L2MTG since we posit lexical dependencystructure only in one of the languages (English).
The pri-mary rationale for this is that we are dealing with only asmall quantity of labeled data in language L and there-fore do not expect to be able to accurately estimate itslexical affinities.
Further, synchronous parsing is in prac-tice computationally expensive, and eliminating lexical-ization on one side reduces the run-time of the parserfrom O(n8) to O(n7).
Our parsing algorithm is a simpletransformation of Melamed?s R2D parser that eliminateshead information in all Korean parser items.The model event space for our stochastic ?half-bilexicalized?
2-MTG consists of rewrite rules of the fol-lowing two forms, with English above and L below:(X[h1]A ?h1h2),(X[h1]A ?Y [h1]Z[c1]BC)where upper-case symbols are nonterminals and lower-case symbols are words (potentially ?).
One approachto assigning a probability to such a rule is to make anindependence assumption, for example:Prbi(X[h]A ?Y [h1]Z[c]BC)=PrEnglish(X[h] ?
Y [h1]Z[c1]) ?
PrL(A ?
BC)There are two powerful reasons to model the bilingualgrammar in this factored way.
First, we know of no tree-aligned corpora from which bilingual rewrite probabili-ties could be estimated; this rules out the possibility ofsupervised training of the joint rules.
Second, separat-ing the probabilities allows separate estimation of theprobabilities?resulting in two well-understood param-eter estimation tasks which can be carried out indepen-dently.1This factored modeling approach bears a strong re-semblance to the factored monolingual parser of Kleinand Manning (2002), which combined an English depen-dency model and an unlexicalized PCFG.
The generativemodel used by Klein and Manning consisted of multiply-ing the two component models; the model was thereforedeficient.We go a step farther, replacing the deficient genera-tive model with a log-linear model.
The underlying pars-ing algorithm remains the same, but the weights are nolonger constrained to sum to one.
(Hereafter, we assumeweights are additive real values; a log-probability is anexample of a weight.)
The weights may be estimatedusing discriminative training (as we do for the Englishmodel, ?3.1) or as if they were log-probabilities, usingsmoothed maximum likelihood estimation (as we do forthe Korean model, ?3.3).
Because we use this model onlyfor inference, it is not necessary to compute a partitionfunction for the combined log-linear model.In addition to the two monolingual syntax models, weadd a word-to-word translation model to the mix.
Inthis paper we use a translation model to induce only asingle best word matching, but in principle the transla-tion model could be used to weight all possible word-word links, and the parser would solve the joint align-ment/parsing problem.2As a testbed for our experiments, the Penn KoreanTreebank (KTB; Han et al, 2002) provides 5,083 Ko-rean constituency trees along with English translationsand their trees.
The KTB also analyzes Korean wordsinto their component morphemes and morpheme tags,which allowed us to train a morphological disambigua-tion model.To make the most of this small corpus, we performedall our evaluations using five-fold cross-validation.
Dueto the computational expense of bilingual parsing, we1Of course, it might be the case that some information is knownabout the relationship between the two languages.
In that case, our log-linear framework would allow the incorporation of additional bilingualproduction features.2Although polynomial, we found this to be too computationally de-manding to do with our optimal parser in practice, but with pruningand/or A?
heuristics it is likely to be feasible.produced a sub-corpus of the KTB limiting English sen-tence length to 10 words, or 27% of the full data.
We thenrandomized the order of sentences and divided the datainto five equal test sets of 280 sentences each (?1,700Korean words, ?2,100 English words).
Complementingeach test set, the remaining data were used for trainingsets of increasing size to simulate various levels of datascarcity.3 Parameter estimationWe now describe parameter estimation for the four com-ponent models that combine to make our full system (Ta-ble 1).3.1 English syntax modelOur English syntax model is based on weighted bilexi-cal dependencies.
The model predicts the generation ofa child (POS tag, word) pair, dependent upon its parent(tag, word) and the tag of the parent?s most recent childon the same side (left or right).
These events correspondquite closely to the parser described by Eisner?s (1996)model C, but instead of the rules receiving conditionalprobabilities, we use a log-linear model and allow arbi-trary weights.
The model does not predict POS tags; itassumes they are given, even in test.Note that the dynamic program used for inferenceof bilexical parses is indifferent to the origin of therule weights; they could be log-probabilities or arbitrarynumbers, as in our model.
The parsing algorithm neednot change to accommodate the new parameterization.In this model, the probability of a (sentence, tree) pair(E, T ) is given by:Pr(E, T ) =exp (f(E, T ) ?
?
)?E?,T ?
exp(f(E?, T ?)
?
?
)(1)where ?
are the model parameters and f is a vector func-tion such that fi is equal to the number of times a feature(e.g., a production rule) fires in (E, T ).Parameter estimation consists of selecting weights ?to maximize the conditional probability of the correctparses given observed sentences:3?iPr(Ti|Si) =?iexp (f(Ei, Ti) ?
?
)?T ?
exp(f(Ei, T?)
?
?
)(2)Another important advantage of moving to log-linearmodels is the simple handling of data sparseness.
Thefeature templates used by our model are shown in Ta-ble 2.
The first feature corresponds to the fully-describedchild-generation event; others are similar but less infor-mative.
These ?overlapping?
features offer a kind ofbackoff, so that each child-generation event?s weight re-ceives a contribution from several granularities of de-scription.Feature selection is done by simple thresholding: if afeature is observed 5 times or more in the training set,its weight is estimated; otherwise its weight is locked at3This can be done using iterative scaling or gradient-based numeri-cal optimization methods, as we did.Model Formalism Estimation RoleEnglish syntax (?3.1) bilexical dependency discriminative estimation combines with Koreangrammar syntax for bilingual parsingKorean morphology (?3.2) two-sequence discriminative estimation best analysis used as inputtrigram model over a lattice to TM training and to parsingKorean syntax (?3.3) PCFG smoothed MLE combines with Englishsyntax for bilingual parsingTranslation model (?3.4) IBM models 1?4, unsupervised estimation best analysis used asboth directions (approximation to EM) input to bilingual parsingTable 1: A summary of the factored models described in this paper and their interactions.
?TP ,WP , TA, TC ,WC , D?
?TP , TA, TC , D?
?TP , TA, TC ,WC , D?
?TP ,WP , TA, TC , D?
?TP ,WP , stop, TC , D?
?TP , stop, TC , D?Table 2: Feature templates used by the English depen-dency parser.
TX is a tag and WX is a word.
P indi-cates the parent, A the previous child, and C the next-generated child.
D is the direction (left or right).
Thelast two templates correspond to stopping.0.
If a feature is never seen in training data, we give itthe same weight as the minimum-valued feature from thetraining set (?min).
To handle out-of-vocabulary (OOV)words, we treat any word seen for the first time in thefinal 300 sentences of the training corpus as OOV.
Themodel is smoothed using a Gaussian prior with unit vari-ance on every weight.Because the left and right children of a parent are in-dependent of each other, our model can be described asa weighted split head automaton grammar (Eisner andSatta, 1999).
This allowed us to use Eisner and Satta?sO(n3) parsing algorithm to speed up training.4 Thisspeedup could not, however, be applied to the bilingualparsing algorithm since a split parsing algorithm will pre-clude inference of certain configurations of word align-ments that are allowed by a non-split parser (Melamed,2003).We trained the parser on sentences of 15 words orfewer in the WSJ Treebank sections 01?21.5 99.49% de-pendency attachment accuracy was achieved on the train-ing set, and 76.68% and 75.00% were achieved on sec-tions 22 and 23, respectively.
Performance on the En-glish side of our KTB test set was 71.82% (averagedacross 5 folds, ?
= 1.75).This type of discriminative training has been appliedto log-linear variants of hidden Markov models (Laffertyet al, 2001) and to lexical-functional grammar (Johnsonet al, 1999; Riezler et al, 2002).
To our knowledge, ithas not been explored for context-free models (includ-ing bilexical dependency models like ours).
A review4Our split HAG?s head automaton states correspond to the POS tagsof the dependent words; this makes the head automaton deterministicand offers an additional speedup.5The parser does not model POS-tags; we assume they are known.Head words in the WSJ corpus were obtained using R. Hwa?sconst2dep tool.of discriminative approaches to parsing can be found inChiang (2003).3.2 Korean morphological analysisA Korean word typically consists of a head morphemefollowed by a series of closed-class dependent mor-phemes such as case markers, copula, topicalizers, andconjunctions.
Since most of the semantic content re-sides in the leading head morpheme, we eliminate forword alignment all trailing morphemes, which reducesthe KTB?s vocabulary size from 10,052 to 3,104.Existing morphological processing tools for many lan-guages are often unweighted finite-state transducers thatencode the possible analyses for a surface form word.One such tool, klex, is available for Korean (Han,2004).Unfortunately, while the unweighted FST describesthe set of valid analyses, it gives no way to chooseamong them.
We treat this as a noisy channel: Koreanmorpheme-tag pairs are generated in sequence by someprocess, then passed through a channel that turns theminto Korean words (with loss of information).
The chan-nel is given by the FST, but without any weights.
Toselect the best output, we model the source process.We model the sequence of morphemes and their tagsas a log-linear trigram model.
Overlapping trigram, bi-gram, and unigram features provide backoff informationto deal with data sparseness (Table 3).
For each trainingsentence, we used the FST-encoded morphological dic-tionary to construct a lattice of possible analyses.
Thelattice has a ?sausage?
form with all paths joining be-tween each word.We train the feature weights to maximize the weightof the correct path relative to all paths in the lattice.
Incontrast, Lafferty et al (2001) train to maximize the theprobability of the tags given the words.
Over trainingsentences, maximize:?iPr(Ti,Mi|lattice)=?iexp(f(Ti,Mi) ?
?)?
(T ?,M ?
)?lattice exp(f(T?,M ?)
?
?
)(3)where Ti is the correct tagging for sentence i, Mi is thecorrect morpheme sequence.There are a few complications.
First, the coverage ofthe FST is of course not universal; in fact, it cannot ana-lyze 4.66% of word types (2.18% of tokens) in the KTB.
?Ti?2,Mi?2, Ti?1,Mi?1, Ti,Mi?
?Ti?
?Ti?2,Mi?2, Ti?1,Mi?1, Ti?
?Ti,Mi?
?Ti?2, Ti?1,Mi?1, Ti,Mi?
?Ti?1, Ti?
?Ti?2, Ti?1,Mi?1, Ti, ?
?Ti?1, Ti,Mi?
?Ti?2, Ti?1, Ti,Mi?
?Ti?1,Mi?1, Ti, ?
?Ti?1,Mi?1, Ti,Mi?
?Ti?2, Ti?1, Ti?Table 3: Feature templates used by the Korean morphol-ogy model.
Tx is a tag, Mx is a morpheme.We tag such words as atomic common nouns (the mostcommon tag).
Second, many of the analyses in the KTBare not admitted by the FST: 21.06% of correct analy-ses (by token) are not admitted by the FST; 6.85% donot have an FST analysis matching in the first tag andmorpheme, 3.63% do not have an FST analysis matchingthe full tag sequence, and 1.22% do not have an analysismatching the first tag.
These do not include the 2.18%of tokens with no analysis at all.
When this happened intraining, we added the correct analysis to the lattice.To perform inference on new data, we construct a lat-tice from the FST (adding in any analyses of the wordseen in training) and use a dynamic program (essentiallythe Viterbi algorithm) to find the best path through thelattice.
Unseen features are given the weight ?min.
Ta-ble 4 shows performance on ambiguous tokens in train-ing and test data (averaged over five folds).3.3 Korean syntax modelBecause we are using small training sets, parameter esti-mates for a lexicalized Korean probabilistic grammar arelikely to be highly unreliable due to sparse data.
There-fore we use an unlexicalized PCFG.
Because the POStags are given by the morphological analyzer, the PCFGneed not predict words (i.e., head morphemes), only POStags.Rule probabilities were estimated with MLE.
Sinceonly the sentence nonterminal S was smoothed (usingadd-0.1), the grammar could parse any sequence of tagsbut was relatively sparse, which kept bilingual run-timedown.
6When we combine the PCFG with the other models todo joint bilingual parsing, we simply use the logs of thePCFG probabilities as if they were log-linear weights.A PCFG treated this way is a perfectly valid log-linearmodel; the exponentials of its weights just happen to sat-isfy certain sum-to-one constraints.In the spirit of joint optimization, we might have alsocombined the Korean morphology and syntax modelsinto one inference task.
We did not do this, largely out ofconcerns over computational expense (see the discussionof translation models in ?3.4).
This parser, independentof the bilingual parser, is evaluated in ?4.6We also found that this type of smoothing and smoothing all non-terminals gave indistinguishable results on monolingual parsing.
Al-ternatively, we could have trained the PCFG discriminatively (treatingthe PCFG rules as log-linear features), but because our training sets aresmall we do not expect such training to be very different from trainingthe PCFG as a generative model with probabilities.3.4 Translation modelIn our bilingual parser, the English and Korean parses aremediated through word-to-word translational correspon-dence links.
Unlike the syntax models, the translationmodels were trained without the benefit of labeled data.We used the GIZA++ implementation of the IBM statisti-cal translation models (Brown et al, 1993; Och and Ney,2003).To obtain reliable word translation estimates, wetrained on a bilingual corpus in addition to the KTBtraining set.
The Foreign Broadcast Information Servicedataset contains about 99,000 sentences of Korean and72,000 of English translation.
For our training, we ex-tracted a relatively small parallel corpus of about 19,000high-confidence sentence pairs.As noted above, Korean?s productive agglutinativemorphology leads to sparse estimates of word frequen-cies.
We therefore trained our translation models af-ter replacing each Korean word with its first morphemestripped of its closed-class dependent morphemes, as de-scribed in ?3.2.The size of the translation tables made optimal bilin-gual parsing prohibitive by exploding the number ofpossible analyses.
We therefore resorted to usingGIZA++?s hypothesized alignments.
Since the IBMmodels only hypothesize one-to-many alignments fromtarget to source, we trained using each side of the bitextas source and target in turn.
We could then produce twokinds of alignment graphs by taking either the intersec-tion or the union of the links in the two GIZA++ align-ment graphs.
All words not in the resulting alignmentgraph are set to align to ?.Our bilingual parser deals only in one-to-one align-ments (mappings); the intersection graph yields a map-ping.
The union graph yields a set of links which maypermit different one-to-one mappings.
Using the uniongraph therefore allows for flexibility in the word align-ments inferred by the bilingual parser, but this comes atcomputational expense (because more analyses are per-mitted).Even with over 20,000 sentence pairs of training data,the hypothesized alignments are relatively sparse.
Forthe intersection alignments, an average of 23% of non-punctuation Korean words and 17% of non-punctuationEnglish words have a link to the other language.
For theunion alignments, this improves to 88% for Korean and22% for English.A starker measure of alignment sparsity is the accu-racy of English dependency links projected onto Korean.Following Hwa et al (2002), we looked at dependencylinks in the true English parses from the KTB where boththe dependent and the head were linked to words on theKorean side using the intersection alignment.
Note thatHwa et al used not only the true English trees, but alsohand-produced alignments.
If we hypothesize that, if En-glish words i and j are in a parent-child relationship,then so are their linked Korean words, then we infer anincomplete dependency graph for the Korean sentenceswhose precision is around 49%?53% but whose recall isTraining sentences All tags All morphemes First tag First morphemeTraining set accuracy 32 91.14 (1.41) 94.25 (2.59) 91.14 (1.41) 95.74 (2.49)on ambiguous tokens 64 89.76 (0.34) 93.39 (1.12) 89.76 (0.34) 95.23 (1.43)128 88.19 (0.91) 92.48 (1.25) 88.38 (1.08) 94.43 (1.02)512 83.69 (0.94) 89.59 (0.27) 85.03 (1.08) 91.95 (0.21)1024 82.55 (0.68) 89.28 (0.30) 84.22 (0.77) 91.67 (0.19)Test set accuracy 32 59.34 (2.52) 53.13 (2.09) 72.81 (1.96) 84.99 (3.11)on ambiguous tokens 64 59.34 (2.41) 54.76 (1.64) 72.68 (1.79) 85.54 (2.03)128 60.85 (2.15) 57.20 (2.01) 74.44 (1.17) 86.29 (1.14)512 63.99 (2.02) 63.24 (1.28) 75.14 (0.86) 85.82 (1.01)1024 65.26 (1.85) 66.03 (1.72) 75.22 (1.25) 85.62 (1.08)Table 4: Korean morphological analysis accuracy on ambiguous tokens in the training and test sets: means (andstandard deviations) are shown over five-fold cross-validation.
Over 65% of word tokens are ambiguous.
The accuracyof the first tag in each word affects the PCFG and the accuracy of the first morpheme affects the translation model(under our aggressive morphological lemmatization).an abysmal 2.5%?3.6%.
74 EvaluationHaving trained each part of the model, we bring themtogether in a unified dynamic program to perform infer-ence on the bilingual text as described in ?2.
In order toexperiment easily with different algorithms, we imple-mented all the morphological disambiguation and pars-ing models in this paper in Dyna, a new language forweighted dynamic programming (Eisner et al, 2004).For parameter estimation, we used the complementaryDynaMITE tool.
Just as CKY parsing starts with wordsin its chart, the dynamic program chart for the bilingualparser is seeded with the links given in the hypothesizedword alignment.All our current results are optimal under the model,but as we scale up to more complex data, we might in-troduce A?
heuristics or, at the possible expense of opti-mality, a beam search or pruning techniques.
Our agendadiscipline is uniform-cost search, which guarantees thatthe first full parse discovered will be optimal?if none ofthe weights are positive.
In our case we are maximizingsums of negative weights, as if working with log proba-bilities.8When evaluating our parsing output against the testdata from the KTB, we do not claim credit for the sin-gle outermost bracketing or for unary productions.
Sinceunary productions do not translate well from language tolanguage (Hwa et al, 2002), we collapse them to theirlower nodes.4.1 Baseline systemsWe compare our bilingual parser to several baseline sys-tems.
The first is the Korean PCFG trained on the small7We approximated head-words in the Korean gold-standard treesby assuming all structures to be head-final, with the exception of punc-tuation.
That is, the head-words of sister constituents will elect theright-most, non-punctuation word among them as the head.8In fact the English syntax model is not constrained to have non-positive weights, but we decrement every parameter by ?max.
For agiven sentence, this will reduce every possible parse?s weight by a con-stant value, since the same number of features fire in every parse; thus,the classification properties of the parser are not affected.KTB training sets, as described in ?3.3.
We also considerWu?s (1997) stochastic inversion transduction grammar(SITG) as well as strictly left- and right-branching trees.We report the results of five-fold cross-validation withthe mean and standard deviation (in parentheses).Since it is unlexicalized, the PCFG parses sequencesof tags as output by the morphological analysis model.By contrast, we can build translation tables for the SITGdirectly from surface words?and thus not use any la-beled training data at all?or from the sequence of headmorphemes.
Experiments showed, however, that theSITG using words consistently outperformed the SITGusing morphemes.
We also implemented Wu?s tree-transformation algorithm to turn full binary-branchingSITG output into flatter trees.
Finally, we can provideextra information to the SITG by giving it a set of En-glish bracketings that it must respect when constructingthe joint tree.
To get an upper bound on performance, weused the true parses from the English side of the KTB.Only the PCFG, of course, can be evaluated on la-beled bracketing (Table 6).
Although labeled precisionand recall on test data generally increase with more train-ing data, the slightly lower performance at the highesttraining set size may indicate overtraining of this simplemodel.
Unlabeled precision and recall show continuedimprovement with more Korean training data.Even with help from the true English trees, the unsu-pervised SITGs underperform PCFGs trained on as fewas 32 sentences, with the exception of unlabeled recall inone experiment.
It seems that even some small amountof knowledge of the language helps parsing.
Crossingbrackets for the flattened SITG parses are understandablylower.4.2 Bilingual parsingThe output of our bilingual parser contains three types ofconstituents: English-only (aligned to ?
), Korean-only(aligned to ?
), and bilingual.
The Korean parse inducedby the Korean-only and bilingual constituents is filteredso constituents with intermediate labels (generated by thebinarization process) are eliminated.A second filter we consider is to keep only the (re-maining) bilingual constituents corresponding to an En-glish head word?s maximal span.
This filter will elimi-nate constituents whose English correspondent is a headword with some (but not all) of its dependents.
Such par-tial English constituents are by-products of the parsingand do not correspond to the modeled syntax.With good word alignments, the English parser canhelp disambiguate Korean phrase boundaries and over-come erroneous morphological analyses (Table 5).
Re-sults without and with the second filter are shown inTable 7.
Because larger training datasets lead to largerPCFGs (with more rules), the grammar constant in-creases.
Our bilingual parser implementation is on thecusp of practicality (in terms of memory requirements);when the grammar constant increased, we were unableto parse longer sentences.
Therefore the results given forbilingual parsing are on reduced test sets, where a lengthfilter was applied: sentences with |E| + |F | > ?
wereremoved, for varying values of ?
.4.3 DiscussionWhile neither bilingual parser consistently beats thePCFG on its own, they offer slight, complementary im-provements on small training datasets of 32 and 64 sen-tences (Table 7).
The bilingual parser without the En-glish head span filter gives a small recall improvementon average at similar precision.
Neither of these differ-ences is significant when measured with a paired-samplet-test.In contrast, the parser with the English head span filtersacrifices significantly on recall for a small but signifi-cant gain in precision at the 0.01 level.
Crossing brack-ets at all levels are significantly lower with the Englishhead span filter.
We can describe this effect as a filteringof Korean constituents by the English model and wordalignments.
Constituents that are not strongly evident onthe English side are simply removed.
On small train-ing datasets, this effect is positive: although good con-stituents are lost so that recall is poor compared to thePCFG, precision and crossing brackets are improved.As one would expect, as the amount of training dataincreases, the advantage of using a bilingual parservanishes?there is no benefit from falling back on theEnglish parser and word alignments to help disambiguatethe Korean structure.
Since we have not pruned oursearch space in these experiments, we can be confidentthat all variations are due to the influence of the transla-tion and English syntax models.Our approach has this principal advantage: the variousmorphology, parsing, and alignment components can beimproved or replaced easily without needing to retrainthe other modules.
The low dependency projection re-sults (?3.4), in conjunction with our modest overall gains,indicate that the alignment/translation model should re-ceive the most attention.
In all the bilingual experiments,there is a small positive correlation (0.3), for sentencesat each length, between the proportion of Korean wordsaligned to English and measures of parsing accuracy.
Im-proved English parsers?such as Collins?
models?havealso been implemented in Dyna, the dynamic program-ming framework used here (Eisner et al, 2004).5 Prior workCombining separately trained systems and then search-ing for an (ideally) optimal solution is standard prac-tice in statistical continuous speech recognition (Jelinek,1998) and statistical machine translation (Brown et al,1990).
Composition is even more of a staple in finite-state frameworks (Knight and Graehl, 1998).
Finally,factored models involving parses have been used to guidesearch.
Charniak et al (2003) combine separatelytrained parse production probabilities with translationprobabilities to prune a parse forest hypothesized by thetranslation model.
As discussed in ?2, Klein and Man-ning (2002) guide their parser?s search using a combina-tion of separate unlexicalized PCFG and lexical depen-dency models.The extent to which assumptions about similarity ofsyntax across languages are empirically valid has re-ceived attention in a few pilot studies.
Fox (2002) hasconsidered English and French, and Hwa et al (2002) in-vestigate Chinese and English.
Xia et al (2000) comparethe rule templates of lexicalized tree adjoining grammarsextracted from treebanks in English, Chinese, and Ko-rean.
In the context of machine translation, Dorr (1994)investigated divergences between two languages?
struc-tures.Some proposals have sidestepped the empirical issueentirely.
Wu (1997) and Alshawi et al (2000) used un-supervised learning on parallel text to induce syntacticanalysis that was useful for their respective applicationsin phrasal translation extraction and speech translation,though not necessarily similar to what a human anno-tator would select.
Note a point of divergence of theSITG from our bilingual parsing system: SITG only al-lows words, but not higher structures, to match null in theother language and thus requires that the trees in parallelsentences be isomorphic.
Yamada and Knight (2001) in-troduced tree-to-string alignment on Japanese data, andGildea (2003) performed tree-to-tree alignment on theKorean Treebank, allowing for non-isomorphic struc-tures; he applied this to word-to-word alignment.
Fi-nally, inspired by these intuitive notions of translationalcorrespondence, Cherry and Lin (2003) include depen-dency features in a word alignment model to improvenon-syntactic baseline systems.In more formal work, Melamed (2003) proposesmultitext grammars and algorithms for parsing them.Shieber and Schabes (1990) describe a synchronous treeadjoining grammar.
While both of these formalisms re-quire bilingual grammar rules, Eisner (2003) describesan algorithm for learning tree substitution grammarsfrom unaligned trees.Working on the Penn Korean Treebank, Sarkar andHan (2002) made a single training/test split and used91% of the sentences to train a morphological disam-biguator and lexicalized tree adjoining grammar (LTAG)based parsing system.For a monolingual approach to training a parser withscarce resources, see (Steedman et al, 2003), who applyco-training and corrected co-training to bootstrapping anEnglish parser starting with 1000 parsed training sen-Truth [TOP [NP ngyen.tay/NNC kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/DAN to/NNC] ken.sel/NNC] ./SFN]PCFG [TOP ngyen.tay/VV [S [NP kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN]]Bilingual [TOP [NP ngyen.tay/VV kong.pyeng/NNC1 cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN2]Translation The regimental1 engineer company constructed that road .2Truth [TOP [NP ku/DAN sa.lam/NNC] [NP ceng.chi/NNC kwun.kwan/NNC] ?/SFN]PCFG [TOP [VP [NP ku/DAN sa.lam/NNC ceng.chi/NNC] kwun.kwan/NNC] ?/SFN]Bilingual [TOP [NP ku/DAN1 sa.lam/NNC] [NP ceng.chi/NNC2 kwun.kwan/NNC3] ?/SFN4]Translation He1 is a political2 officer3 ?4Table 5: The gold-standard parse, PCFG parse, bilingual parse, and English translation for two selected test sentences.GIZA-aligned words are coindexed with subscripts.
The bilingual parser recovers from erroneous morphologicaltagging in the first sentence and finds the proper NP bracketing in the second.Method Training Unlabeled Unlabeled Labeled Labeled CrossingSentences Precision Recall Precision Recall BracketsPCFG training 32 57.03 (5.45) 78.45 (5.71) 51.13 (6.14) 70.26 (6.40) 0.71 (0.22)64 54.96 (4.98) 76.91 (6.71) 46.94 (4.38) 65.69 (5.99) 0.72 (0.25)128 52.60 (3.15) 73.20 (4.97) 43.46 (3.34) 60.48 (5.14) 0.82 (0.18)512 50.82 (1.46) 70.98 (2.00) 39.47 (2.49) 55.12 (3.42) 0.87 (0.06)1024 50.25 (0.82) 70.31 (1.32) 37.93 (1.45) 53.07 (2.16) 0.89 (0.04)PCFG test 32 43.63 (4.40) 45.96 (5.38) 31.67 (3.47) 33.36 (4.19) 1.27 (0.16)64 45.90 (2.30) 46.68 (2.92) 34.29 (2.35) 34.91 (3.22) 1.18 (0.12)128 48.07 (4.14) 48.47 (4.45) 36.39 (3.37) 36.68 (3.50) 1.15 (0.14)512 50.88 (2.97) 51.89 (2.92) 38.10 (3.22) 38.82 (2.68) 1.10 (0.10)1024 51.15 (2.17) 52.65 (1.74) 37.47 (1.89) 38.58 (1.64) 1.12 (0.08)SITG ?
30.65 (1.97) 45.22 (3.43) ?
?
1.93 (0.17)Flat SITG ?
41.78 (1.98) 33.59 (3.36) ?
?
0.94 (0.08)SITG w/Eng.
constit.
?
36.28 (0.70) 52.68 (1.03) ?
?
1.60 (0.07)Flat SITG w/Eng.
constit.
?
42.55 (1.32) 30.64 (1.37) ?
?
0.77 (0.06)L-branching ?
25.62 (1.07) 35.83 (1.39) ?
?
2.04 (0.04)R-branching ?
27.59 (1.03) 38.60 (1.75) ?
?
2.06 (0.11)Table 6: Baseline parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-validation.
The SITG system is evaluated on test data, but is trained without labeled data; the SITG with English treesuses true treebank English parses to constrain the search and thus represents an upper bound.
The table shows meansand standard deviations for five-fold cross-validation.
The best test results in each column are in bold.Method Max.
|E| + |F | Training Unlabeled Unlabeled Labeled Labeled CrossingTest Sen.
Length Sentences Precision Recall Precision Recall BracketsPCFG 20 32 44.19 (4.41) 46.51 (5.32) 32.10 (3.47) 33.78 (4.14) 1.23 (0.16)20 64 46.39 (2.45) 47.03 (3.01) 34.69 (2.40) 35.20 (3.22) 1.15 (0.11)18 128 49.86 (4.83) 49.63 (4.74) 37.78 (3.74) 37.60 (3.61) 1.03 (0.13)17 512 53.89 (3.60) 54.60 (3.73) 40.61 (3.84) 41.10 (3.19) 0.87 (0.11)15 1024 57.87 (3.75) 59.39 (3.35) 43.92 (3.52) 45.07 (3.26) 0.61 (0.09)Bilingual 20 32 44.17 (3.97) 47.10 (4.81) 31.67 (3.65) 33.78 (4.29) 1.22 (0.14)parsing 20 64 46.30 (2.46) 47.73 (2.83) 34.14 (2.60) 35.23 (3.35) 1.15 (0.12)18 128 48.75 (3.64) 49.51 (4.08) 36.95 (2.65) 37.52 (2.92) 1.04 (0.10)17 512 52.77 (3.92) 54.21 (4.42) 39.73 (3.68) 40.78 (3.56) 0.88 (0.12)15 1024 56.70 (4.79) 58.85 (4.10) 43.09 (4.24) 44.71 (3.69) 0.60 (0.12)Bilingual 20 32 45.65 (5.81) 28.83 (4.35) 32.92 (4.60) 20.82 (3.53) 0.72 (0.11)parsing, 20 64 47.15 (2.88) 28.73 (1.79) 34.65 (2.36) 21.14 (1.73) 0.68 (0.08)English 18 128 49.65 (4.52) 28.74 (2.30) 38.62 (3.69) 22.35 (1.76) 0.59 (0.09)head span 17 512 52.03 (4.21) 29.47 (2.71) 39.80 (2.92) 22.51 (1.32) 0.50 (0.08)filter 15 1024 54.78 (5.20) 29.74 (1.91) 42.01 (5.05) 22.78 (1.84) 0.34 (0.09)Table 7: Bilingual parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-validation.
Bold-faced numbers in the bilingual parsers indicate significant improvements on the PCFG baseline usingthe paired-sample t-test at the 0.01 level.tences.
Although this technique has interesting proper-ties, our combined optimization should be more stablesince it does not involve iterative example selection.6 ConclusionWe have presented a novel technique for merging sim-ple, separately trained models for Korean parsing, En-glish dependency parsing, and word translation, and opti-mizing the joint result using dynamic programming.
Weshowed small but significant improvements for Koreanparsers trained on small amounts of labeled data.7 AcknowledgementsWe would like to thank Elliott Dra?bek, Jason Eisner,Eric Goldlust, Philip Resnik, Charles Schafer, DavidYarowsky, and the reviewers for their comments and as-sistance and Chung-hye Han, Na-Rae Han, and AnoopSarkar for their help with the Korean resources.
Thiswork was supported under a National Science Founda-tion Graduate Research Fellowship and a Fannie andJohn Hertz Foundation Fellowship.ReferencesH.
Alshawi, S. Bangalore, and S. Douglas.
2000.
Learn-ing dependency translation models as collections offinite-state head transducers.
Computational Linguis-tics, 26(1):45?60.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.Roossin.
1990.
A statistical approach to machine trans-lation.
Computational Linguistics, 16(2):79?85.P.
E. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statistical ma-chine translation: Parameter estimation.
ComputationalLinguistics, 19(2):263?311.E.
Charniak, K. Knight, and K. Yamada.
2003.
Syntax-based language models for machine translation.
InProc.
MT Summit IX.C.
Cherry and D. Lin.
2003.
A probability model to im-prove word alignment.
In Proc.
ACL.D.
Chiang.
2003.
Mildly context-sensitive grammars forestimating maximum entropy models.
In Proc.
FormalGrammar.B.
J. Dorr.
1994.
Machine translation divergences: A for-mal description and proposed solution.
ComputationalLinguistics, 20(4):597?633.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexicalcontext-free grammars and head automaton grammars.In Proc.
ACL.J.
Eisner, E. Goldlust, and N. A. Smith.
2004.
Dyna:A declarative language for implementing dynamic pro-grams.
In ACL Companion Vol.J.
Eisner.
1996.
An empirical comparison of probabil-ity models for dependency grammar.
Technical ReportIRCS-96-11, U. Penn.J.
Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In ACL Companion Vol.H.
J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
EMNLP.D.
Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proc.
ACL.C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer.2002.
Penn Korean Treebank: Development and evalu-ation.
In Proc.
Pacific Asian Conf.
Language and Comp.N.-R. Han.
2004.
Klex: Finite-state lexical trans-ducer for Korean.
http://wave.ldc.upenn.edu/Catalog/-CatalogEntry.jsp?catalogId=LDC2004L01.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.Evaluating translational correspondence using annota-tion projection.
In Proc.
ACL.F.
Jelinek.
1998.
Statistical Methods for Speech Recogni-tion.
MIT Press, Cambridge, MA.M.
Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-zler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In Proc.
ACL.D.
Klein and C. D. Manning.
2002.
Fast exact naturallanguage parsing with a factored model.
In NIPS.K.
Knight and J. Graehl.
1998.
Machine transliteration.Computational Linguistics, 24(4).J.
Lafferty, A. McCallum, and F. C. N. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ICML.I.
D. Melamed.
2003.
Multitext grammars and syn-chronous parsers.
In Proc.
HLT-NAACL.F.-J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
ComputationalLinguistics, 29(1):19?51.S.
Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,and M. Johnson.
2002.
Parsing the WSJ using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proc.
ACL.A.
Sarkar and C.-H. Han.
2002.
Statistical morphologicaltagging and parsing of Korean with an LTAG grammar.In Proc.
TAG+6, pages 48?56.S.
M. Shieber and Y. Schabes.
1990.
Synchronous tree-adjoining grammars.
In Proc.
ACL, pages 253?258.M.
Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003.Example selection for bootstrapping statistical parsers.In Proc.
HLT-NAACL.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Compu-tational Linguistics, 23(3):377?404.F.
Xia, C.-H. Han, M. Palmer, and A. Joshi.
2000.
Com-paring lexicalized treebank grammars extracted fromChinese, Korean, and English corpora.
In Proc.
2ndChinese Language Processing Workshop.K.
Yamada and K. Knight.
2001.
A syntax-based statisti-cal translation model.
In Proc.
ACL.D.
Yarowsky and G. Ngai.
2001.
Inducing multilingualPOS taggers and NP bracketers via robust projectionacross aligned corpora.
In Proc.
NAACL.
