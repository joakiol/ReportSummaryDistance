Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 278?287,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCreative Language Retrieval:A Robust Hybrid of Information Retrieval and Linguistic CreativityTony VealeSchool of Computer Science and Informatics,University College Dublin,Belfield, Dublin D4, Ireland.Tony.Veale@UCD.ieAbstractInformation retrieval (IR) and figurativelanguage processing (FLP) could scarcelybe more different in their treatment of lan-guage and meaning.
IR views language asan open-ended set of mostly stable signswith which texts can be indexed and re-trieved, focusing more on a text?s potentialrelevance than its potential meaning.
Incontrast, FLP views language as a systemof unstable signs that can be used to talkabout the world in creative new ways.There is another key difference: IR is prac-tical, scalable and robust, and in daily useby millions of casual users.
FLP is neitherscalable nor robust, and not yet practicalenough to migrate beyond the lab.
This pa-per thus presents a mutually beneficial hy-brid of IR and FLP, one that enriches IRwith new operators to enable the non-literalretrieval of creative expressions, and whichalso transplants FLP into a robust, scalableframework in which practical applicationsof linguistic creativity can be implemented.1 IntroductionWords should not always be taken at face value.Figurative devices like metaphor can communicatefar richer meanings than are evident from a super-ficial ?
and perhaps literally nonsensical ?
reading.Figurative Language Processing (FLP) thus uses avariety of special mechanisms and representations,to assign non-literal meanings not just to meta-phors, but to similes, analogies, epithets, puns andother creative uses of language (see Martin, 1990;Fass, 1991; Way, 1991; Indurkhya, 1992; Fass,1997; Barnden, 2006; Veale and Butnariu, 2010).Computationalists have explored heterodoxsolutions to the procedural and representationalchallenges of metaphor, and FLP more generally,ranging from flexible representations (e.g.
thepreference semantics of Wilks (1978) and the col-lative semantics of Fass (1991, 1997)) to processesof cross-domain structure alignment (e.g.
structuremapping theory; see Gentner (1983) and Falken-hainer et al 1989) and even structural inversion(Veale, 2006).
Though thematically related, eachapproach to FLP is broadly distinct, giving com-putational form to different cognitive demands ofcreative language: thus, some focus on inter-domain mappings (e.g.
Gentner, 1983) while oth-ers focus more on intra-domain inference (e.g.
Ba-rnden, 2006).
However, while computationallyinteresting, none has yet achieved the scalability orrobustness needed to make a significant practicalimpact outside the laboratory.
Moreover, suchsystems tend to be developed in isolation, and arerarely designed to cohere as part of a larger frame-work of creative reasoning (e.g.
Boden, 1994).In contrast, Information Retrieval (IR) is bothscalable and robust, and its results translate easilyfrom the laboratory into practical applications (e.g.see Salton, 1968; Van Rijsbergen, 1979).
WhereasFLP derives its utility and its fragility from its at-tempts to identify deeper meanings beneath thesurface, the widespread applicability of IR stemsdirectly from its superficial treatment of language278and meaning.
IR does not distinguish betweencreative and conventional uses of language, orbetween literal and non-literal meanings.
IR is alsoremarkably modular: its components are designedto work together interchangeably, from stemmersand indexers to heuristics for query expansion anddocument ranking.
Yet, because IR treats all lan-guage as literal language, it relies on literalmatching between queries and the texts that theyretrieve.
Documents are retrieved precisely be-cause they contain stretches of text that literallyresemble the query.
This works well in the main,but it means that IR falls flat when the goal of re-trieval is not to identify relevant documents but toretrieve new and creative ways of expressing agiven idea.
To retrieve creative language, and to bepotentially surprised or inspired by the results, oneneeds to facilitate a non-literal relationship be-tween queries and the texts that they match.The complementarity of FLP and IR suggests aproductive hybrid of both paradigms.
If the mostrobust elements of FLP are used to provide newnon-literal query operators for IR, then IR can beused to retrieve potentially new and creative waysof speaking about a topic from a large text collec-tion.
In return, IR can provide a stable, robust andextensible platform on which to use these opera-tors to build FLP systems that exhibit linguisticcreativity.
In the next section we consider the re-lated work on which the current realization ofthese ideas is founded, before presenting a specifictrio of new semantic query operators in section 3.We describe three simple but practical applicationsof this creative IR paradigm in section 4.
Empiricalsupport for the FLP intuitions that underpin ournew operators is provided in section 5.
The paperconcludes with some closing observations aboutfuture goals and developments in section 6.2 Related Work and IdeasIR works on the premise that a user can turn aninformation need into an effective query by antici-pating the language that is used to talk about agiven topic in a target collection.
If the collectionuses creative language in speaking about a topic,then a query must also contain the seeds of thiscreative language.
Veale (2004) introduces the ideaof creative information retrieval to explore how anIR system can itself provide a degree of creativeanticipation, acting as a mediator between the lit-eral specification of a meaning and the retrieval ofcreative articulations of this meaning.
This antici-pation ranges from simple re-articulation (e.g.
atext may implicitly evoke ?Qur?an?
even if it onlycontains ?Muslim bible?)
to playful allusions andepithets (e.g.
the CEO of a rubber company may bepunningly described as a ?rubber baron?).
A crea-tive IR system may even anticipate out-of-dictionary words, like chocoholic and sexoholic.Conventional IR systems use a range of queryexpansion techniques to automatically bolster auser?s query with additional keywords or weights,to permit the retrieval of relevant texts it might nototherwise match (e.g.
Vernimb, 1977; Voorhees,1994).
Techniques vary, from the use of stemmersand morphological analysis to the use of thesauri(such as WordNet; see Fellbaum, 1998; Voorhees,1998) to pad a query with synonyms, to the use ofstatistical analysis to identify more appropriatecontext-sensitive associations and near-synonyms(e.g.
Xu and Croft, 1996).
While some techniquesmay suggest conventional metaphors that have be-come lexicalized in a language, they are unlikely toidentify relatively novel expressions.
Crucially,expansion improves recall at the expense of overallprecision, making automatic techniques even moredangerous when the goal is to retrieve results thatare creative and relevant.
Creative IR must balancea need for fine user control with the statisticalbreadth and convenience of automatic expansion.Fortunately, statistical corpus analysis is an ob-vious area of overlap for IR and FLP.
Distribu-tional analyses of large corpora have been shownto produce nuanced models of lexical similarity(e.g.
Weeds and Weir, 2005) as well as context-sensitive thesauri for a given domain (Lin, 1998).Hearst (1992) shows how a pattern like ?Xs andother Ys?
can be used to construct more fluid,context-specific taxonomies than those providedby WordNet (e.g.
?athletes and other celebrities?suggests a context in which athletes are viewed asstars).
Mason (2004) shows how statistical analysiscan automatically detect and extract conventionalmetaphors from corpora, though creative meta-phors still remain a tantalizing challenge.
Hanks(2005) shows how the ?Xs like A, B and C?
con-struction allows us to derive flexible ad-hoc cate-gories from corpora, while Hanks (2006) arguesfor a gradable conception of metaphoricity basedon word-sense distributions in corpora.279Veale and Hao (2007) exploit the simile frame?as X as Y?
to harvest a great many commonsimiles and their underlying stereotypes from theweb (e.g.
?as hot as an oven?
), while Veale andHao (2010) show that the pattern ?about as X as Y?retrieves an equally large collection of creative (ifmostly ironic) comparisons.
These authors demon-strate that a large vocabulary of stereotypical ideas(over 4000 nouns) and their salient properties (over2000 adjectives) can be harvested from the web.We now build on these results to develop a setof new semantic operators, that use corpus-derivedknowledge to support finely controlled non-literalmatching and automatic query expansion.3 Creative Text RetrievalIn language, creativity is always a matter of con-strual.
While conventional IR queries articulate aneed for information, creative IR queries articulatea need for expressions to convey the same meaningin a fresh or unusual way.
A query and a matchingphrase can be figuratively construed to have thesame meaning if there is a non-literal mappingbetween the elements of the query and the ele-ments of the phrase.
In creative IR, this non-literalmapping is facilitated by the query?s explicit use ofsemantic wildcards (e.g.
see Mihalcea, 2002).The wildcard * is a boon for power-users of theGoogle search engine, precisely because it allowsusers to focus on the retrieval of matching phrasesrather than relevant documents.
For instance, * canbe used to find alternate ways of instantiating aculturally-established linguistic pattern, or ?snow-clone?
: thus, the Google queries ?In * no one canhear you scream?
(from Alien), ?Reader, I * him?
(from Jane Eyre) and ?This is your brain on *?
(from a famous TV advert) find new ways inwhich old patterns have been instantiated for hu-morous effect on the Web.
On a larger scale, Vealeand Hao (2007) used the * wildcard to harvest websimiles, but reported that harvesting cultural datawith wildcards is not a straightforward process.Google and other engines are designed to maxi-mize document relevance and to rank results ac-cordingly.
They are not designed to maximize thediversity of results, or to find the largest set ofwildcard bindings.
Nor are they designed to findthe most commonplace bindings for wildcards.Following Guilford?s (1950) pioneering work,diversity is widely considered a key component inthe psychology of creativity.
By focusing on thephrase level rather than the document level, and byreturning phrase sets rather than document sets,creative IR maximizes diversity by finding asmany bindings for its wildcards as a text collectionwill support.
But we need more flexible and pre-cise wildcards than *.
We now consider three va-rieties of semantic wildcards that build on insightsfrom corpus-linguistic approaches to FLP.3.1 The Neighborhood Wildcard     ?XSemantic query expansion replaces a query term Xwith a set {X, X1, X2, ?, Xn} where each Xiisrelated to X by a prescribed lexico-semantic rela-tionship, such as synonymy, hyponymy ormeronymy.
A generic, lightweight resource likeWordNet can provide these relations, or a richerontology can be used if one is available (e.g.
seeNavigli and Velardi, 2003).
Intuitively, each queryterm suggests other terms from its semantic neigh-borhood, yet there are practical limits to this intui-tion.
Ximay not be an obvious or natural substitutefor X.
A neighborhood can be drawn too small,impacting recall, or too large, impacting precision.Corpus analysis suggests an approach that isboth semantic and pragmatic.
As noted in Hanks(2005), languages provide constructions for build-ing ad-hoc sets of items that can be consideredcomparable in a given context.
For instance, a co-ordination of bare plurals suggests that two ideasare related at a generic level, as in ?priests andimams?
or ?mosques and synagogues?.
More gen-erally, consider the pattern ?X and Y?, where X andY are proper-names (e.g., ?Zeus and Hera?
), or Xand Y are inflected nouns or verbs with the sameinflection (e.g., the plurals ?cats and dogs?
or theverb forms ?kicking and screaming?).
Millions ofmatches for this pattern can be found in the Google3-grams (Brants and Franz, 2006), allowing us tobuild a map of comparable terms by linking theroot-forms of X and Y with a similarity score ob-tained via a WordNet-based measure (e.g.
see Bu-danitsky and Hirst (2006) for a good selection).The pragmatic neighborhood of a term X can bedefined as {X, X1, X2, ?, Xn}, so that for eachXi, the Google 3-grams contain ?X+inf andXi+inf?
or ?X+inf and Xi+inf?.
The boundaries ofneighborhoods are thus set by usage patterns: if ?Xdenotes the neighborhood of X, then ?artist280matches not just artist, composer and poet, butstudio,  portfolio and gallery, and many otherterms that are semantically dissimilar but prag-matically linked to artist.
Since each Xi?
?X isranked by similarity to X, query matches can alsobe ranked by similarity.When X is an adjective, then ?X matches anyelement of {X, Xi, X2, ?, Xn}, where each Xipragmatically reinforces X, and X pragmaticallyreinforces each Xi.
To ensure X and Xireally aremutually reinforcing adjectives, we use the double-ground simile pattern ?as X and Xias?
to harvest{X1, ?, Xn} for each X.
Moreover, to maximizerecall, we use the Google API (rather than Googlengrams) to harvest suitable bindings for X and Xifrom the web.
For example, @witty = {charming,clever, intelligent, entertaining, ?, edgy, fun}.3.2 The Cultural Stereotype Wildcard   @XDickens claims in A Christmas Carol that ?thewisdom of a people is in the simile?.
Similes ex-ploit familiar stereotypes to describe a less familiarconcept, so one can learn a great deal about a cul-ture and its language from the similes that have themost currency (Taylor, 1954).
The wildcard @ Xbuilds on the results of Veale and Hao (2007) toallow creative IR queries to retrieve matches onthe basis of cultural expectations.
This foundationprovides a large set of adjectival features (over2000) for a larger set of nouns (over 4000) denot-ing stereotypes for which these features are salient.If N is a noun, then @N matches any elementof the set {A1, A2, ?, An}, where each Aiis anadjective denoting a stereotypical property of N.For example, @diamond matches any element of{transparent, immutable, beautiful, tough,  expen-sive, valuable, shiny, bright, lasting, desirable,strong, ?, hard} .
If A is an adjective, then @ Amatches any element of the set {N1, N2, ?, Nn},where each Niis a noun denoting a stereotype forwhich A is a culturally established property.
Forexample, @tall matches any element of {giraffe,skyscraper, tree, redwood, tower, sunflower, light-house, beanstalk,  rocket, ?, supermodel}.Stereotypes crystallize in a language as clich?s,so one can argue that stereotypes and clich?s arelittle or no use to a creative IR system.
Yet, asdemonstrated in Fishlov (1992), creative languageis replete with stereotypes, not in their clich?dguises, but in novel and often incongruous combi-nations.
The creative value of a stereotype lies inhow it is used, as we?ll show later in section 4.3.3 The Ad-Hoc Category Wildcard    ^XBarsalou (1983) introduced the notion of an ad-hoc category, a cross-cutting collection of oftendisparate elements that cohere in the context of aspecific task or goal.
The ad-hoc nature of thesecategories is reflected in the difficulty we have innaming them concisely: the cumbersome ?things totake on a camping trip?
is Barsalou?s most citedexample.
But ad-hoc categories do not replacenatural kinds; rather, they supplement an existingsystem of more-or-less rigid categories, such as thecategories found in WordNet.The semantic wildcard ^C matches C and anyelement of {C1, C2, ?, Cn},  where each Ciis amember of the category named by C. ^C can de-note a fixed category in a resource like WordNet oreven Wikipedia; thus, ^fruit matches any memberof {apple, orange, pear, ?, lemon} and ^animalany member of {dog, cat, mouse, ?, deer,  fox}.Ad-hoc categories arise in creative IR when theresults of a query ?
or more specifically, the bind-ings for a query wildcard ?
are funneled into a newuser-defined category.
For instance, the query?^fruit juice?
matches any phrase in a text collec-tion that denotes a named fruit juice, from ?lemonjuice?
to ?pawpaw juice?.
A user can now funnelthe bindings for ^fruit in this query into an ad-hoccategory juicefruit, to gather together those fruitsthat are used for their juice.
Elements of ^juicefruitare ranked by the corpus frequencies discovered bythe original query; low-frequency juicefruit mem-bers in the Google ngrams include coffee, raisin,almond, carob and soybean.
Ad-hoc categoriesallow users of IR to remake a category system intheir own image, and create a new vocabulary ofcategories to serve their own goals and interests, aswhen ?^food pizza?
is used to suggest disparatemembers for the ad-hoc category pizzatopping.The more subtle a query, the more disparate theelements it can funnel into an ad-hoc category.
Wenow consider how basic semantic wildcards can becombined to generate even more diverse results.3.4 Compound OperatorsEach wildcard maps a query term onto a set of ex-281pansion terms.
The compositional semantics of awildcard combination can thus be understood inset-theoretic terms.
The most obvious and usefulcombinations of ?, @ and ^ are described below:??
Neighbor-of-a-neighbor: if ?X matches anyelement of {X, X1, X2, ?, Xn} then ?
?X matchesany of ?X ?
?X1?
?
?
?Xn, where the rankingof Xijin ?
?X is a function of the ranking of Xiin?X and the ranking of Xijin ?Xi.
Thus, ?
?artistmatches far more terms than ?artist, yielding morediversity, more noise, and more creative potential.
@@  Stereotype-of-a-stereotype: if @X matchesany element of {X1, X2, ?, Xn} then @@Xmatches any of @X1?
@X2?
?
?
@Xn.
Forinstance, @@diamond matches any stereotypethat shares a salient property with diamond, and@@sharp matches any salient property of anynoun for which sharp is a stereotypical property.?
@ Neighborhood-of-a-stereotype: if @X matchesany element of {X1, X2, ?, Xn} then ?
@ Xmatches any of ?X1?
?X2?
?
?
?Xn.
Thus,?
@cunning matches any term in the pragmaticneighborhood of a stereotype for cunning, while?
@knife matches any property that mutually rein-forces any stereotypical property of knife@?
Stereotypes-in-a-neighborhood: if ?X matchesany of {X, X1, X2, ?, Xn} then @?X matches anyof @X ?
@X1?
?
?
@Xn.
Thus, @?corpsematches any salient property of any stereotype inthe neighborhood of corpse, while @?fast matchesany stereotype noun with a salient property that issimilar to, and reinforced by, fast.
?^ Neighborhood-of-a-category: if ^C matchesany of {C, C1, C2, ?, Cn} then ?^C matches anyof ?C ?
?C1?
?
?
?Cn.^?
Categories-in-a-neighborhood: if ?X matchesany of {X, X1, X2, ?, Xn} then ^?X matches anyof ^X ?
^X1?
?
?
^Xn.
@^   Stereotypes-in-a-category: if ^C matches anyof {C, C1, C2, ?, Cn} then @^C matches any of@C ?
@C1?
?
?
@Cn.^@ Members-of-a-stereotype-category: if @ Xmatches any element of {X1, X2, ?, Xn} then^@X matches any of ^X1?
^X2?
?
?
^Xn.So ^@strong matches any member of a category(such as warrior) that is stereotypically strong.4 Applications of Creative RetrievalThe Google ngrams comprise a vast array of ex-tracts from English web texts, of 1 to 5 words inlength (Brants and Franz, 2006).
Many extracts arewell-formed phrases that give lexical form to manydifferent ideas.
But an even greater number ofngrams are not linguistically well-formed.
TheGoogle ngrams can be seen as a lexicalized ideaspace, embedded within a larger sea of noise.Creative IR can be used to explore this idea space.Each creative query is a jumping off point in aspace of lexicalized ideas that is implied by a largecorpus, with each successive match leading theuser deeper into the space.
By turning matches intoqueries, a user can perform a creative explorationof the space of phrases and ideas (see Boden,1994) while purposefully sidestepping the noise ofthe Google ngrams.
Consider the pleonastic query?Catholic ?pope?.
Retrieved phrases include, indescending order of lexical similarity, ?Catholicpresident?, ?Catholic politician?, ?Catholic king?,?Catholic emperor?
and ?Catholic patriarch?.Suppose a user selects ?Catholic king?
: the newquery ?Catholic ?king?
now retrieves ?Catholicqueen?, ?Catholic court?, ?Catholic knight?
,?Catholic kingdom?
and ?Catholic throne?.
Thesubsequent query ?Catholic ?kingdom?
in turnretrieves ?Catholic dynasty?
and ?Catholic army?,among others.
In this way, creative IR allows auser to explore the text-supported ramifications ofa metaphor like Popes are Kings (e.g., if popes arekings, they too might have queens, command ar-mies, found dynasties, or sit on thrones).Creative IR gives users the tools to conducttheir own explorations of language.
The morewildcards a query contains, the more degrees offreedom it offers to the explorer.
Thus, the query?
?scientist ?s ?laboratory?
uncovers a plethora ofanalogies for the relationship between scientistsand their labs: matches in the Google 3-grams in-clude ?technician?s workshop?, ?artist?s studio?,?chef?s kitchen?
and ?gardener?s greenhouse?.2824.1 Metaphors with AristotleFor a term X, the wildcard ?X suggests those otherterms that writers have considered to be compara-ble to X, while ?
?X extrapolates beyond the cor-pus evidence to suggest an even larger space ofpotential comparisons.
A meaningful metaphor canbe constructed for X by framing X with anystereotype to which it is pragmatically comparable,that is, any stereotype in ?X.
Collectively, thesestereotypes can impart the properties @?X to X.Suppose one wants to metaphorically ascribethe property P to X.
The set @P contains thosestereotypes for which P is culturally salient.
Thus,close metaphors for X (what MacCormac (1985)dubs epiphors) in the context of P are suggested by?X ?
@P.  More distant metaphors (MacCormacdubs these diaphors) are suggested by ?
?X ?
@P.For instance, to describe a scholar as wise, one canuse poet, yogi, philosopher or rabbi as compari-sons.
Yet even a simple metaphor will impart otherfeatures to a topic.
If ^PSdenotes the ad-hoc setof additional properties that may be inferred for Xwhen a stereotype S is used to convey property P,then ^PS= ?P ?
@@P. The query ?^PSX?
nowfinds corpus-attested elements of ^PSthat canmeaningfully be used to modify X.These IR formulations are used by Aristotle, anonline metaphor generator, to generate targetedmetaphors that highlight a property P in a topic X.Aristotle uses the Google ngrams to supply valuesfor ?X, ?
?X, ?P and ^PS.
The system can be ac-cessed at: www.educatedinsolence.com/aristotle4.2 Expressing Attitude with Idiom SavantOur retrieval goals in IR are often affective in na-ture: we want to find a way of speaking about atopic that expresses a particular sentiment and car-ries a certain tone.
However, affective categoriesare amongst the most cross-cutting structures inlanguage.
Words for disparate ideas are groupedaccording to the sentiments in which they are gen-erally held.
We respect judges but dislike critics;we respect heroes but dislike killers; we respectsharpshooters but dislike snipers; and respect re-bels but dislike insurgents.
It seems therefore thatthe particulars of sentiment are best captured by aset of culture-specific ad-hoc categories.We thus construct two ad-hoc categories,^posword and ^negword, to hold the most obvi-ously positive or negative words in Whissell?s(1989) Dictionary of Affect.
We then grow thesecategories to include additional reinforcing ele-ments from their pragmatic neighborhoods,?^posword and ?^negword.
As these categoriesgrow, so too do their neighborhoods, allowing asimple semi-automated bootstrapping process tosignificantly grow the categories over several it-erations.
We construct two phrasal equivalents ofthese categories, ^posphrase and ^negphrase,using the queries ?^posword - ^pastpart?
(e.g.,matching ?high-minded?
and ?sharp-eyed?)
and?^negword - ^pastpart?
(e.g., matching ?flat-footed?
and ?dead-eyed?)
to mine affective phrasesfrom the Google 3-grams.
The resulting ad-hoccategories (of ~600 elements each) are manuallyedited to fix any obvious mis-categorizations.Idiom Savant is a web application that uses^posphrase and ^negphrase to suggest flatteringand insulting epithets for a given topic.
The query?^posphrase ?X?
retrieves phrases for a topic Xthat put a positive spin on a related topic to whichX is sometimes compared, while ?^negphrase?X?
conversely imparts a negative spin.
Thus, forpolitician, the Google 4-grams provide the flatter-ing epithets ?much-needed leader?, ?awe-inspiringleader?, ?hands-on boss?
and ?far-sighted states-man?, as well as insults like ?power-mad leader?,?back-stabbing boss?, ?ice-cold technocrat?
and?self-promoting hack?.
Riskier diaphors can beretrieved via ?^posphrase ??X?
and ?^negphrase?
?
X ?.
Idiom Savant is accessible online at:www.educatedinsolence.com/idiom-savant/4.3 Poetic Similes with The Jigsaw BardThe well-formed phrases of a large corpus can beviewed as the linguistic equivalent of objets trou-v?s in art: readymade or ?found?
objects that mighttake on fresh meanings in a creative context.
Thephrase ?robot fish?, for instance, denotes a more-or-less literal object in the context of autonomousrobotic submersibles, but can also be used to con-vey a figurative meaning as part of a creative com-parison (e.g., ?he was as cold as a robot fish?
).Fishlov (1992) argues that poetic comparisonsare most resonant when they combine mutually-reinforcing (if distant) ideas, to create memorableimages and evoke nuanced feelings.
Building onFishlov?s argument, creative IR can be used to turn283the readymade phrases of the Google ngrams intovehicles for creative comparison.
For a topic X anda property P, simple similes of the form ?X is as Pas S?
are easily generated, where S ?
@P ?
?
?X.Fishlov would dub these non-poetic similes(NPS).
However, the query ?
?P @P?
will retrievecorpus-attested elaborations of stereotypes in @Pto suggest similes of the form ?X is as P as P1S?,where P1?
?P.
These similes exhibit elements ofwhat Fishlov dubs poetic similes (PS).
Why say?as cold as a fish?
when you can say ?as cold as awet fish?, ?a dead haddock?, ?a wet January?, ?afrozen corpse?, or ?a  heartless robot??
Complexqueries can retrieve more creative combinations, so?
@P @P?
(e.g.
?robot fish?
or ?snow storm?
forcold), ?
?P @P @P?
(e.g.
?creamy chocolatemousse?
for rich) and ?
@P - ^pastpart @P?
(e.g.
?snow-covered graveyard?
and ?bullet-riddledcorpse?
for cold) each retrieve ngrams that blendtwo different but overlapping stereotypes.Blended properties also make for nuancedsimiles of the form ?as P and ?P as S?, where S ?
@P ?
@?P.
While one can be ?as rich as a fatking?, something can be ?as rich and enticing as achocolate truffle?, ?a chocolate brownie?, ?achocolate fruitcake?, and even ?a chocolate king?.The Jigsaw Bard is a web application thatharnesses the readymades of the Google ngrams toformulate novel similes from existing phrases.
Bymapping blended properties to ngram phrases thatcombine multiple stereotypes, the Bard expands itsgenerative scope considerably, allowing this appli-cation to generate hundreds of thousands of evoca-tive comparisons.
The Bard can be accessed onlineat: www.educatedinsolence.com/jigsaw/5 Empirical EvaluationThough ^ is the most overtly categorical of ourwildcards, all three wildcards ?
?, @  and ^ ?
arecategorical in nature.
Each has a semantic orpragmatic membership function that maps a termonto an expansion set of related members.
Themembership functions for specific uses of ^ arecreated in an ad-hoc fashion by the users that ex-ploit it; in contrast, the membership functions foruses of @  and ?
are derived automatically, viapattern-matching and corpus analysis.
Nonetheless,ad-hoc categories in creative IR are often popu-lated with the bindings produced by uses of @ and?
and combinations thereof.
In a sense, ?X and@X  and their variations are themselves ad-hoccategories.
But how well do they serve as catego-ries?
Are they large, but noisy?
Or too small, withlimited coverage?
We can evaluate the effective-ness of ?
and @ , and indirectly that of ^ too, bycomparing the use of ?
and @ as category buildersto a hand-crafted gold standard like WordNet.Other researchers have likewise used WordNetas a gold standard for categorization experiments,and we replicate here the experimental set-up ofAlmuhareb and Poesio (2004, 2005), which is de-signed to measure the effectiveness of web-acquired conceptual descriptions.
Almuhareb andPoesio choose 214 English nouns from 13 ofWordNet?s upper-level semantic categories, andproceed to harvest property values for these con-cepts from the web using the Hearst-like pattern?a|an|the * C is|was?.
This pattern yields a com-bined total of 51,045 values for all 214 nouns;these values are primarily adjectives, such as hotand black for coffee, but noun-modifiers of C arealso allowed, such as fruit for cake.
They also har-vest 8934 attribute nouns, such as temperature andcolor, using the query ?the * of the C is|was?
.These values and attributes are then used as thebasis of a clustering algorithm to partition the 214nouns back into their original 13 categories.
Com-paring these clusters with the original WordNet-based groupings, Almuhareb and Poesio report acluster accuracy of 71.96% using just values likehot and black (51,045 values), an accuracy of64.02% using just attributes like temperature andcolor (8,934 attributes), and an accuracy of 85.5%using both together (a combined 59,979 features).How concisely and accurately does @X de-scribe a noun X for purposes of categorization?
Let^AP denote the set of 214 WordNet nouns used byAlmuhareb and Poesio.
Then @^AP denotes a setof 2,209 adjectival properties; this should be con-trasted with the space of 51,045 adjectival valuesused by Almuhareb and Poesio.
Using the sameclustering algorithm over this feature set, @ Xachieves a clustering accuracy (as measured viacluster purity) of 70.2%, compared to 71.96% forAlmuhareb and Poesio.
However, when @X  isused to harvest a further set of attribute nouns forX, via web queries of the form ?the P  * of X ?
(where P ?
@X), then @ X augmented with thisadditional set of attributes (like hands for surgeon)284produces a larger space of 7,183 features.
This inturn yields a cluster accuracy of 90.2% whichcontrasts with Almuhareb and Poesio?s 85.5% for59,979 features.
In either case, @X produces com-parable clustering quality to Almuhareb and Poe-sio, with just a small fraction of the features.So how concisely and accurately does ?X de-scribe a noun X for purposes of categorization?While @X denotes a set of salient adjectives, ?Xdenotes a set of comparable nouns.
So this time,?^AP denotes a set of 8,300 nouns in total, to actas a feature space for the 214 nouns of Almuhareband Poesio.
Remember, the contents of each ?X,and of ?^AP overall, are determined entirely bythe contents of the Google 3-grams; the elementsof ?X are not ranked in any way, and all are treatedas equals.
When the 8,300 features in ?^AP areclustered into 13 categories, the resulting clustershave a purity of 93.4% relative to WordNet.
Thepragmatic neighborhood of X, ?X, appears to be anaccurate and concise proxy for the meaning of X.What about adjectives?
Almuhareb and Poe-sio?s set of 214 words does not contain adjectives,and besides, WordNet does not impose a categorystructure on its adjectives.
In any case, the role ofadjectives in the applications of section 4 is largelyan affective one: if X is a noun, then one musthave confidence that the adjectives in @X are con-sonant with our understanding of X, and if P is aproperty, that the adjectives in ?P evoke much thesame mood and sentiment as P. Our evaluation of@X and ?P should thus be an affective one.So how well do the properties in @X captureour sentiments about a noun X?
Well enough toestimate the pleasantness of X from the adjectivesin @ X, perhaps?
Whissell?s (1989) dictionary ofaffect provides pleasantness ratings for a sizeablenumber of adjectives and nouns (over 8,000 wordsin total), allowing us to estimate the pleasantnessof X as a weighted average of the pleasantness ofeach Xiin @X (the weights here are web frequen-cies for the similes that underpin @ in section 3.2).We thus estimate the affect of all stereotype nounsfor which Whissell also records a score.
A two-tailed Pearson test (p < 0.05) shows a positive cor-relation of 0.5 between these estimates and thepleasantness scores assigned by Whissell.
In con-trast, estimates based on the pleasantness of adjec-tives found in corresponding WordNet glossesshow a positive correlation of just 0.278.How well do the elements of ?P capture oursentiments toward an adjective P?
After all, wehypothesize that the adjectives in ?P are highlysuggestive of P, and vice versa.
Aristotle and theJigsaw Bard each rely on ?P to suggest adjectivesthat evoke an unstated property in a metaphor orsimile, or to suggest coherent blends of properties.When we estimate the pleasantness of each adjec-tive P in Whissell?s dictionary via the weightedmean of the pleasantness of adjectives in ?P (againusing web frequencies as weights), a two-tailedPearson test (p < 0.05) shows a correlation of 0.7between estimates and actual scores.
It seems ?Pdoes a rather good job of capturing the feel of P.6 Concluding RemarksCreative information retrieval is not a single appli-cation, but a paradigm that allows us to conceiveof many different kinds of application for crea-tively manipulating text.
It is also a tool-kit forimplementing such an application, as shown herein the cases of Aristotle, Idiom Savant and JigsawBard.The  wildcards @, ?
and ^ allow users to for-mulate their own task-specific ontologies of ad-hoccategories.
In a fully automated application, theyprovide developers with a simple but powerful vo-cabulary for describing the range and relationshipsof the words, phrases and ideas to be manipulated.The @ , ?
and ^ wildcards are just a start.
Weexpect other aspects of figurative language to beincorporated into the framework whenever theyprove robust enough for use in an IR context.
Inthis respect, we aim to position Creative IR as anopen, modular platform in which diverse results inFLP, from diverse researchers, can be meaning-fully integrated.
One can imagine wildcards formatching potential puns, portmanteau words andother novel forms, as well as wildcards for figura-tive processes like metonymy, synecdoche, hyper-bolae and even irony.
Ultimately, it is hoped thatcreative IR can serve as a textual bridge betweenhigh-level creativity and the low-level creativepotentials that are implicit in a large corpus.AcknowledgmentsThis work was funded in part by Science Founda-tion Ireland (SFI), via the Centre for Next Genera-tion Localization.
(CNGL).285ReferencesAlmuhareb, A. and Poesio, M. (2004).
Attribute-Basedand Value-Based Clustering: An Evaluation.
In Proc.of EMNLP 2004.
Barcelona.Almuhareb, A. and Poesio, M. (2005).
Concept Learn-ing and Categorization from the Web.
In Proc.
of the27thAnnual meeting of the Cognitive Science Society.Barnden, J.
A.
(2006).
Artificial Intelligence, figurativelanguage and cognitive linguistics.
In: G. Kristian-sen, M. Achard, R. Dirven, and F. J. Ruiz de Men-doza Ibanez (Eds.
), Cognitive Linguistics: CurrentApplication and Future Perspectives, 431-459.
Ber-lin: Mouton de Gruyter.Barsalou, L. W. (1983).
Ad hoc categories.
Memory andCognition, 11:211?227.Boden, M. (1994).
Creativity: A Framework for Re-search, Behavioural & Brain Sciences 17(3):558-568.Brants, T. and Franz, A.
(2006).
Web 1T 5-gram Ver.
1.Linguistic Data Consortium.Budanitsky, A. and Hirst, G. (2006).
Evaluating Word-Net-based Measures of Lexical Semantic Related-ness.
Computational Linguistics, 32(1):13-47.Falkenhainer, B., Forbus, K. and Gentner, D. (1989).Structure-Mapping Engine: Algorithm and Exam-ples.
Artificial Intelligence, 41:1-63.Fass, D. (1991).
Met*: a method for discriminatingmetonymy and metaphor by computer.
Computa-tional Linguistics, 17(1):49-90.Fass, D. (1997).
Processing Metonymy and Metaphor.Contemporary Studies in Cognitive Science & Tech-nology.
New York: Ablex.Fellbaum, C. (1998).
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge.Fishlov, D. (1992).
Poetic and Non-Poetic Simile:Structure, Semantics, Rhetoric.
Poetics Today, 14(1),1-23.Gentner, D. (1983), Structure-mapping: A TheoreticalFramework.
Cognitive Science 7:155?170.Guilford, J.P. (1950) Creativity, American Psychologist5(9):444?454.Hanks, P. (2005).
Similes and Sets: The English Prepo-sition ?like?.
In: Blatn?, R. and Petkevic, V.
(Eds.
),Languages and Linguistics: Festschrift for Fr.
Cer-mak.
Charles University, Prague.Hanks, P. (2006).
Metaphoricity is gradable.
In: AnatolStefanowitsch and Stefan Th.
Gries (Eds.
), Corpus-Based Approaches to Metaphor and Metonymy,.
17-35.
Berlin: Mouton de Gruyter.Hearst, M. (1992).
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of the 14thInt.
Conf.on Computational Linguistics, pp 539?545.Indurkhya, B.
(1992).
Metaphor and Cognition: Studiesin Cognitive Systems.
Kluwer Academic Publishers,Dordrecht: The Netherlands.Lin, D. (1998).
Automatic retrieval and clustering ofsimilar words.
In Proc.
of the 17thinternational con-ference on Computational linguistics, 768-774.MacCormac, E. R. (1985).
A Cognitive Theory of Meta-phor.
MIT Press.Martin, J. H. (1990).
A Computational Model of Meta-phor Interpretation.
New York: Academic Press.Mason, Z. J.
(2004).
CorMet: A Computational, Cor-pus-Based Conventional Metaphor Extraction Sys-tem, Computational Linguistics, 30(1):23-44.Mihalcea, R. (2002).
The Semantic Wildcard.
In Proc.of the LREC Workshop on Creating and Using Se-mantics for Information Retrieval and Filtering.
Ca-nary Islands, Spain, May 2002.Navigli, R. and Velardi, P. (2003).
An Analysis of On-tology-based Query Expansion Strategies.
In Proc.
ofthe workshop on Adaptive Text Extraction and Min-ing (ATEM 2003), at ECML 2003, the 14thEuropeanConf.
on Machine Learning, 42?49Salton, G. (1968).
Automatic Information Organizationand Retrieval.
New York: McGraw-Hill.Taylor, A.
(1954).
Proverbial Comparisons and Similesfrom California.
Folklore Studies 3.
Berkeley: Uni-versity of California Press.Van Rijsbergen, C. J.
(1979).
Information Retrieval.Oxford: Butterworth-Heinemann.Veale, T. (2004).
The Challenge of Creative Informa-tion Retrieval.
Computational Linguistics and Intelli-gent Text Processing: Lecture Notes in ComputerScience, Volume 2945/2004, 457-467.Veale, T. (2006).
Re-Representation and Creative Anal-ogy: A Lexico-Semantic Perspective.
New Genera-tion Computing 24, pp 223-240.Veale, T. and Hao, Y.
(2007).
Making Lexical Ontolo-gies Functional and Context-Sensitive.
In Proc.
ofthe 46thAnnual Meeting of the Assoc.
of Computa-tional Linguistics.Veale, T. and Hao, Y.
(2010).
Detecting Ironic Intent inCreative Comparisons.
In Proc.
of ECAI?2010, the19th European Conference on Artificial Intelligence.286Veale, T. and Butnariu, C. (2010).
Harvesting and Un-derstanding On-line Neologisms.
In: Onysko, A. andMichel, S.
(Eds.
), Cognitive Perspectives on WordFormation.
393-416.
Mouton De Gruyter.Vernimb, C. (1977).
Automatic Query Adjustment inDocument Retrieval.
Information Processing &Management.
13(6):339-353.Voorhees, E. M. (1994).
Query Expansion Using Lexi-cal-Semantic Relations.
In the proc.
of SIGIR 94, the17th International Conference on Research and De-velopment in Information Retrieval.
Berlin: Springer-Verlag, 61-69.Voorhees, E. M. (1998).
Using WordNet for text re-trieval.
WordNet, An Electronic Lexical Database,285?303.
The MIT Press.Way, E. C. (1991).
Knowledge Representation andMetaphor.
Studies in Cognitive systems.
Holland:Kluwer.Weeds, J. and Weir, D. (2005).
Co-occurrence retrieval:A flexible framework for lexical distributional simi-larity.
Computational Linguistics, 31(4):433?475.Whissell, C. (1989).
The dictionary of affect in lan-guage.
R. Plutchnik & H. Kellerman (Eds.)
Emotion:Theory and research.
NY: Harcourt Brace, 113-131.Wilks, Y.
(1978).
Making Preferences More Active,Artificial Intelligence 11.Xu, J. and Croft, B. W. (1996).
Query expansion usinglocal and global document analysis.
In Proc.
of the19thannual international ACM SIGIR conference onResearch and development in information retrieval.287
