A Noisy-Channel Approach to Question AnsweringAbdessamad Echihabi and Daniel MarcuInformation Sciences InstituteDepartment of Computer ScienceUniversity of Southern California4676 Admiralty Way, Suite 1001Marina Del Rey, CA 90292{echihabi,marcu}@isi.eduAbstractWe introduce a probabilistic noisy-channel model for question answering andwe show how it can be exploited in thecontext of an end-to-end QA system.
Ournoisy-channel system outperforms a state-of-the-art rule-based QA system that usessimilar resources.
We also show that themodel we propose is flexible enough toaccommodate within one mathematicalframework many QA-specific resourcesand techniques, which range from theexploitation of WordNet, structured, andsemi-structured databases to reasoning,and paraphrasing.1 IntroductionCurrent state-of-the-art Question Answering (QA)systems are extremely complex.
They contain tensof modules that do everything from informationretrieval, sentence parsing (Ittycheriah andRoukos, 2002; Hovy et al, 2001; Moldovan et al2002), question-type pinpointing (Ittycheriah andRoukos, 2002; Hovy et al, 2001; Moldovan et al2002), semantic analysis (Xu et al, Hovy et al,2001; Moldovan et al 2002), and reasoning(Moldovan et al 2002).
They access externalresources such as the WordNet (Hovy et al, 2001,Pasca and Harabagiu, 2001, Prager et al, 2001),the web (Brill et al, 2001), structured, and semi-structured databases (Katz et al, 2001; Lin, 2002;Clarke, 2001).
They contain feedback loops,ranking, and re-ranking modules.
Given theircomplexity, it is often difficult (and sometimesimpossible) to understand what contributes to theperformance of a system and what doesn?t.In this paper, we propose a new approach toQA in which the contribution of various resourcesand components can be easily assessed.
Thefundamental insight of our approach, whichdeparts significantly from the current architectures,is that, at its core, a QA system is a pipeline ofonly two modules:?
An IR engine that retrieves a set of Mdocuments/N sentences that may containanswers to a given question Q.?
And an answer identifier module that givena question Q and a sentence S (from the setof sentences retrieved by the IR engine)identifies a sub-string SA of S that is likelyto be an answer to Q and assigns a score toit.Once one has these two modules, one has a QAsystem because finding the answer to a question Qamounts to selecting the sub-string SA of highestscore.
Although this view is not made explicit byQA researchers, it is implicitly present in allsystems we are aware of.In its simplest form, if one accepts a wholesentence as an answer (SA = S), one can assess thelikelihood that a sentence S contains the answer toa question Q by measuring the cosine similaritybetween Q and S. However, as research in QAdemonstrates, word-overlap is not a good enoughmetric for determining whether a sentence containsthe answer to a question.
Consider, for example,the question ?Who is the leader of France??
Thesentence ?Henri Hadjenberg, who is the leader ofFrance?s Jewish community, endorsed confrontingthe specter of the Vichy past?
overlaps with allquestion terms, but it does not contain the correctanswer; while the sentence ?Bush later met withFrench President Jacques Chirac?
does not overlapwith any question term, but it does contain thecorrect answer.To circumvent this limitation of word-basedsimilarity metrics, QA researchers have developedmethods through which they first map questionsand sentences that may contain answers indifferent spaces, and then compute the ?similarity?between them there.
For example, the systemsdeveloped at IBM and ISI map questions andanswer sentences into parse trees and surface-based semantic labels and measure the similaritybetween questions and answer sentences in thissyntactic/semantic space, using QA-motivatedmetrics.
The systems developed by CYC and LCCmap questions and answer sentences into logicalforms and compute the ?similarity?
between themusing inference rules.
And systems such as thosedeveloped by IBM and BBN map questions andanswers into feature sets and compute thesimilarity between them using maximum entropymodels that are trained on question-answercorpora.
From this perspective then, thefundamental problem of question answering is thatof finding spaces where the distance betweenquestions and sentences that contain correctanswers is small and where the distance betweenquestions and sentences that contain incorrectanswers is large.In this paper, we propose a new space and anew metric for computing this distance.
Beinginspired by the success of noisy-channel-basedapproaches in applications as diverse as speechrecognition (Jelinek, 1997), part of speech tagging(Church, 1988), machine translation (Brown et al,1993), information retrieval (Berger and Lafferty,1999), and text summarization (Knight and Marcu,2002), we develop a noisy channel model for QA.This model explains how a given sentence SA thatcontains an answer sub-string A to a question Qcan be rewritten into Q through a sequence ofstochastic operations.
Given a corpus of question-answer pairs (Q, SA), we can train a probabilisticmodel for estimating the conditional probabilityP(Q | SA).
Once the parameters of this model arelearned, given a question Q and the set ofsentences ?
returned by an IR engine, one can findthe sentence Si ?
?
and an answer in it Ai,j bysearching for the Si,Ai,j that maximizes theconditional probability P(Q | Si,Ai,j).In Section 2, we first present the noisy-channelmodel that we propose for this task.
In Section 3,we describe how we generate training examples.
InSection 4, we describe how we use the learnedmodels to answer factoid questions, we evaluatethe performance of our system using a variety ofexperimental conditions, and we compare it with arule-based system that we have previously used inseveral TREC evaluations.
In Section 5, wedemonstrate that the framework we propose isflexible enough to accommodate a wide range ofresources and techniques that have been employedin state-of-the-art QA systems.2 A noisy-channel for QAAssume that we want to explain why ?1977?
insentence S in Figure 1 is a good answer for thequestion ?When did Elvis Presley die??
To do this,we build a noisy channel model that makes explicithow answer sentence parse trees are mapped intoquestions.
Consider, for example, the automaticallyderived answer sentence parse tree in Figure 1,which associates to nodes both syntactic andshallow semantic, named-entity-specific tags.
Inorder to rewrite this tree into a question, weassume the following generative story:1.
In general, answer sentences are much longerthan typical factoid questions.
To reduce thelength gap between questions and answers andto increase the likelihood that our models canbe adequately trained, we first make a ?cut?
inthe answer parse tree and select a sequence ofwords, syntactic, and semantic tags.
The ?cut?is made so that every word in the answersentence or one of its ancestors belongs to the?cut?
and no two nodes on a path from a wordto the root of the tree are in the ?cut?.
Figure 1depicts graphically such a cut.2.
Once the ?cut?
has been identified, we markone of its elements as the answer string.
InFigure 1, we decide to mark DATE as theanswer string (A_DATE).3.
There is no guarantee that the number of wordsin the cut and the number of words in thequestion match.
To account for this, westochastically assign to every element si in acut a fertility according to table n(?
| si).
Wedelete elements of fertility 0 and duplicateelements of fertility 2, etc.
With probability p1we also increment the fertility of an invisibleword NULL.
NULL and fertile words, i.e.words with fertility strictly greater than 1enable us to align long questions with shortanswers.
Zero fertility words enable us to alignshort questions with long answers.4.
Next, we replace answer words (including theNULL word) with question words according tothe table t(qi | sj).5.
In the last step, we permute the question wordsaccording to a distortion table d, in order toobtain a well-formed, grammatical question.The probability P(Q | SA) is computed bymultiplying the probabilities in all the steps of ourgenerative story (Figure 1 lists some of the factorsspecific to this computation.)
The readers familiarwith the statistical machine translation (SMT)literature should recognize that steps 3 to 5 arenothing but a one-to-one reproduction of thegenerative story proposed in the SMT context byBrown et al (see Brown et al, 1993 for a detailedmathematical description of the model and theformula for computing the probability of analignment and target string given a source string).1Figure 1: A generative model for QuestionansweringTo simplify our work and to enable us exploitexisting off-the-shelf software, in the experimentswe carried out in conjunction with this paper, weassumed a flat distribution for the two steps in our1 The distortion probabilities depicted in Figure 1 are asimplification of the distortions used in the IBM Model 4model by Brown et al (1993).
We chose this watered downrepresentation only for illustrative purposes.
Our QA systemimplements the full-blown Model 4 statistical model describedby Brown et algenerative story.
That is, we assumed that it isequally likely to take any cut in the tree andequally likely to choose as Answer anysyntactic/semantic element in an answer sentence.3 Generating training and testingmaterial3.1 Generating training casesAssume that the question-answer pair in Figure 1appears in our training corpus.
When this happens,we know that 1977 is the correct answer.
Togenerate a training example from this pair, wetokenize the question, we parse the answersentence, we identify the question terms andanswer in the parse tree, and then we make a "cut"in the tree that satisfies the following conditions:a) Terms overlapping with the question arepreserved as surface textb) The answer is reduced to its semantic orsyntactic class prefixed with the symbol ?A_?c) Non-leaves, which don?t have any questionterm or answer offspring, are reduced to theirsemantic or syntactic class.d) All remaining nodes (leaves) are preservedas surface text.Condition a) ensures that the question termswill be identified in the sentence.
Condition b)helps learn answer types.
Condition c) brings thesentence closer to the question by compactingportions that are syntactically far from questionterms and answer.
And finally the importance oflexical cues around question terms and answermotivates condition d).
For the question-answerpair in Figure 1, the algorithm above generates thefollowing training example:Q: When did Elvis Presley die ?SA: Presley died PP PP in A_DATE, andSNT.Figure 2 represents graphically the conditionsthat led to this training example being generated.Our algorithm for generating training pairsimplements deterministically the first two steps inour generative story.
The algorithm is constructedso as to be consistent with our intuition that agenerative process that makes the question andanswer as similar-looking as possible is most likelyto enable us learn a useful model.
Each question-answer pair results in one training example.
It isthe examples generated through this procedure thatwe use to estimate the parameters of our model.Figure 2: Generation of QA examples for training.3.2 Generating test casesAssume now that the sentence in Figure 1 isreturned by an IR engine as a potential candidatefor finding the answer to the question ?When didElvis Presley die??
In this case, we don?t knowwhat the answer is, so we assume that anysemantic/syntactic node in the answer sentence canbe the answer, with the exception of the nodes thatsubsume question terms and stop words.
In thiscase, given a question and a potential answersentence, we generate an exhaustive set ofquestion-answer test cases, each test case labelingas answer (A_) a different syntactic/semantic node.Here are some of the test cases we consider for thequestion-answer pair in Figure 1:Q: When did Elvis Presley die ?SA1: Presley died A_PP PP PP , and SNT .Q:  When did Elvis Presley die ?SAi: Presley died PP PP in A_DATE, andSNT .Q:  When did Elvis Presley die ?SAj: Presley died PP PP PP , and NPreturn by A_NP NP .If we learned a good model, we would expect it toassign a higher probability to P(Q | Sai) than to P(Q| Sa1) and P(Q | Saj).4 Experiments4.1 Training DataFor training, we use three different sets.
(i) TheTREC9-10 set consists of the questions used atTREC9 and 10.
We automatically generateanswer-tagged sentences using the TREC9 and 10judgment sets, which are lists of answer-documentpairs evaluated as either correct or wrong.
Forevery question, we first identify in the judgmentsets a list of documents containing the correctanswer.
For every document, we keep only thesentences that overlap with the question terms andcontain the correct answer.
(ii) In order to havemore variation of sentences containing the answer,we have automatically extended the first data setusing the Web.
For every TREC9-10question/answer pair, we used our Web-based IRto retrieve sentences that overlap with the questionterms and contain the answer.
We call this data setTREC9-10Web.
(iii) The third data set consists of2381 question/answer pairs collected fromhttp://www.quiz-zone.co.uk.
We use the samemethod to automatically enhance this set byretrieving from the web sentences containinganswers to the questions.
We call this data setQuiz-Zone.
Table 1 shows the size of the threetraining corpora:Training Set # distinct questions # question-answer pairsTREC9-10 1091 18618TREC9-10Web 1091 54295Quiz-Zone 2381 17614Table 1: Size of Training CorporaTo train our QA noisy-channel model, we applythe algorithm described in Section 3.1 to generatetraining cases for all QA pairs in the three corpora.To help our model learn that it is desirable to copyanswer words into the question, we add to eachcorpus a list of identical dictionary word pairs wi-wi.
For each corpus, we use GIZA (Al-Onaizan etal., 1999), a publicly available SMT package thatimplements the IBM models (Brown et al, 1993),to train a QA noisy-channel model that mapsflattened answer parse trees, obtained using the?cut?
procedure described in Section 3.1, intoquestions.4.2 Test DataWe used two different data sets for the purpose oftesting.
The first set consists of the 500 questionsused at TREC 2002; the second set consists of 500questions that were randomly selected from theKnowledge Master (KM) repository(http://www.greatauk.com).
The KM questionstend to be longer and quite different in stylecompared to the TREC questions.th e  fa i t h fu l  r e tu r n  b y  th eh u n d r e d s  e a c h  y e a r  tom a r k  th e  a n n iv e r s a r yo f  a  h e a r t  d i s e a s e  a t  G r a c e l a n dS N TN P  P PP r e s le yd i e d  P Pin  1 9 7 7S N T,  .a n d  P PC o n d i t io n  a )C o n d i t io n  b )C o n d i t io n  d )C o n d i t io n  c )4.3 A noisy-channel-based QA systemOur QA system is straightforward.
It has only twomodules: an IR module, and an answer-identifier/ranker module.
The IR module is thesame we used in previous participations at TREC.As the learner, the answer-identifier/ranker moduleis also publicly available ?
the GIZA package canbe configured to automatically compute theprobability of the Viterbi alignment between aflattened answer parse tree and a question.For each test question, we automatically generate aweb query and use the top 300 answer sentencesreturned by our IR engine to look for an answer.For each question Q and for each answer sentenceSi, we use the algorithm described in Section 3.2 toexhaustively generate all Q- Si,Ai,j pairs.
Hence weexamine all syntactic constituents in a sentence anduse GIZA to assess their likelihood of being acorrect answer.
We select the answer Ai,j thatmaximizes P(Q | Si,Ai,j) for all answer sentences Siand all answers Ai,j that can be found in listretrieved by the IR module.
Figure 3 depictsgraphically our noisy-channel-based QA system.Figure 3: The noisy-channel-based QA system.4.4 Experimental ResultsWe evaluate the results by generatingautomatically the mean reciprocal rank (MRR)using the TREC 2002 patterns and QuizZoneoriginal answers when testing on TREC 2002 andQuizZone test sets respectively.
Our baseline is astate of the art QA system, QA-base, which wasranked from second to seventh in the last 3 years atTREC.
To ensure a fair comparison, we use thesame Web-based IR system in all experiments withno answer retrofitting.
For the same reason, we usethe QA-base system with the post-processingmodule disabled.
(This module re-ranks theanswers produced by QA-base on the basis of theirredundancy, frequency on the web, etc.)
Table 2summarizes results of different combinations oftraining and test sets:Trained on\Tested on TREC 2002 KMA = TREC9-10 0.325 0.108B = A + TREC9-10Web 0.329 0.120C = B + Quiz-Zone 0.354 0.132QA-base 0.291 0.128Table 2: Impact of training and test sets.For the TREC 2002 corpus, the relatively lowMRRs are due to the small answer coverage of theTREC 2002 patterns.
For the KM corpus, therelatively low MRRs are explained by two factors:(i) for this corpus, each evaluation pattern consistsof only one string ?
the original answer; (ii) theKM questions are more complex than TRECquestions (What piece of furniture is associatedwith Modred, Percival, Gawain, Arthur, andLancelot?
).It is interesting to see that using only theTREC9-10 data as training (system A in Table 2),we are able to beat the baseline when testing onTREC 2002 questions; however, this is not truewhen testing on KM questions.
This can beexplained by the fact that the TREC9-10 trainingset is similar to the TREC 2002 test set while it issignificantly different from the KM test set.
Wealso notice that expanding the training to TREC9-10Web (System B) and then to Quiz-Zone (SystemC) improved the performance on both test sets,which confirms that both the variability acrossanswer tagged sentences  (Trec9-10Web) and theabundance of distinct questions (Quiz-Zone)contribute to the diversity of a QA training corpus,and implicitly to the performance of our system.5 Framework flexibilityAnother characteristic of our framework is itsflexibility.
We can easily extend it to span otherquestion-answering resources and techniques thathave been employed in state-of-the art QAsystems.
In the rest of this section, we assess theimpact of such resources and techniques in thecontext of three case studies.5.1 Statistical-based ?Reasoning?The LCC TREC-2002 QA system (Moldovan etal., 2002) implements a reasoning mechanism forjustifying answers.
In the LCC framework,T estquestionQS i,Ai,jQ A M odeltrainedusingG IZAS x,Ax,y=  argm ax (P(Q  | S i,Ai,j))A  =  A x,yG IZAS 1S mS 1,A1,1S 1,A1,vS m ,Am ,1S m ,Am ,wIRquestions and answers are first mapped into logicalforms.
A resolution-based module then proves thatthe question logically follows from the answerusing a set of axioms that are automaticallyextracted from the WordNet glosses.
For example,to prove the logical form of ?What is the age of oursolar system??
from the logical form of the answer?The solar system is 4.6 billion years old.
?, theLCC theorem prover shows that the atomicformula that corresponds to the question term?age?
can be inferred from the atomic formula thatcorresponds to the answer term ?old?
using anaxiom that connects ?old?
and ?age?, because theWordNet gloss for ?old?
contains the word ?age?.Similarly, the LCC system can prove that ?Votingis mandatory for all Argentines aged over 18?provides a good justification for the question?What is the legal age to vote in Argentina?
?because it can establish through logical deductionusing axioms induced from WordNet glosses that?legal?
is related to ?rule?, which in turn is relatedto ?mandatory?
; that ?age?
is related to ?aged?
;and that ?Argentine?
is related to ?Argentina?.
It isnot difficult to see by now that these logicalrelations can be represented graphically asalignments between question and answer terms(see Figure 4).Figure 4: Gloss-based reasoning as word-levelalignment.The exploitation of WordNet synonyms, which ispart of many QA systems (Hovy et al, 2001;Prager et al, 2001; Pasca and Harabagiu, 2001), isa particular case of building such alignmentsbetween question and answer terms.
For example,using WordNet synonymy relations, it is possibleto establish a connection between ?U.S.?
and?United States?
and between ?buy?
and ?purchase?in the question-answer pair (Figure 5), thusincreasing the confidence that the sentencecontains a correct answer.Figure 5: Synonym-based alignment.The noisy channel framework we proposed in thispaper can approximate the reasoning mechanismemployed by LCC and accommodate theexploitation of gloss- and synonymy-basedrelations found in WordNet.
In fact, if we had avery large training corpus, we would expect suchconnections to be learned automatically from thedata.
However, since we have a relatively smalltraining corpus available, we rewrite the WordNetglosses into a dictionary by creating word-pairentries that establish connections between allWordnet words and the content words in theirglosses.
For example, from the word ?age?
and itsgloss ?a historic period?, we create the dictionaryentries ?age - historic?
and ?age ?
period?.
Toexploit synonymy relations, for every WordNetsynset Si, we add to our training data all possiblecombinations of synonym pairs Wi,x-Wi,y.Our dictionary creation procedure is a crudeversion of the axiom extraction algorithmdescribed by Moldovan et al (2002); and ourexploitation of the glosses in the noisy-channelframework amounts to a simplified, statisticalversion of the semantic proofs implemented byLCC.
Table 3 shows the impact of WordNetsynonyms (WNsyn) and WordNet glosses(WNgloss) on our system.
Adding WordNetsynonyms and glosses improved slightly theperformance on the KM questions.
On the otherhand, it is surprising to see that the performancehas dropped when testing on TREC 2002questions.Trained on\Tested on TREC 2002 KMC 0.354 0.132C+WNsyn 0.345 0.138C + WNgloss 0.343 0.136Table 3: WordNet synonyms and glosses impact.5.2 Question reformulationHermjakob et al (2002) showed thatreformulations (syntactic and semantic) improvethe answer pinpointing process in a QA system.To make use of this technique, we extend ourtraining data set by expanding every question-answer pair Q-SA to a list (Qr-SA), Qr ?
?
where ?is the set of question reformulations.
2   We alsoexpand in a similar way the answer candidates inthe test corpus.
Using reformulations improved the2 We are grateful to Ulf Hermjakob for sharing hisreformulations with us.In 1867, Secretary of State William H. Seward arranged forthe United-States to purchase Alaska for 2 cents per acre.What year did the U.S. buy Alaska?What  is the legal age to vote in Argentina?Voting  is mandatory for all Argentines aged over 18performance of our system on the TREC 2002 testset while it was not beneficial for the KM test set(see Table 4).
We believe this is explained by thefact that the reformulation engine was fine tunedon TREC-specific questions, which aresignificantly different from KM questions.Trained on\Tested on TREC 2002 KMC 0.354 0.132C+reformulations 0.365 0.128Table 4: Reformulations impact.5.3 Exploiting data in structured -and semi-structured databasesStructured and semi-structured databases wereproved to be very useful for question-answeringsystems.
Lin (2002) showed through his federatedapproach that 47% of TREC-2001 questions couldbe answered using Web-based knowledge sources.Clarke et al (2001) obtained a 30% improvementby using an auxiliary database created from webdocuments as an additional resource.
We adopteda different approach to exploit external knowledgebases.In our work, we first generated a naturallanguage collection of factoids by mining differentstructured and semi-structured databases (WorldFact Book, Biography.com, WordNet?).
Thegeneration is based on manually written question-factoid template pairs, which are applied on thedifferent sources to yield simple natural languagequestion-factoid pairs.
Consider, for example, thefollowing two factoid-question template pairs:Qt1: What is the capital of _c?St1: The capital of _c is capital(_c).Qt2: How did _p die?St2: _p died of causeDeath(_p).Using extraction patterns (Muslea, 1999), weapply these two templates on the World Fact Bookdatabase and on biography.com pages to instantiatequestion and answer-tagged sentence pairs such as:Q1: What is the capital of Greece?S1: The capital of Greece is Athens.Q2: How did Jean-Paul Sartre die?S2: Jean-Paul Sartre died of a lungailment.These question-factoid pairs are useful both intraining and testing.
In training, we simply add allthese pairs to the training data set.
In testing, forevery question Q, we select factoids that overlapsufficiently enough with Q as sentences thatpotentially contain the answer.
For example, giventhe question ?Where was Sartre born??
we willselect the following factoids:1-Jean-Paul Sartre was born in 1905.2-Jean-Paul Sartre died in 1980.3-Jean-Paul Sartre was born in Paris.4-Jean-Paul Sartre died of a lungailment.Up to now, we have collected about 100,000question-factoid pairs.
We found out that thesepairs cover only 24 of the 500 TREC 2002questions.
And so, in order to evaluate the value ofthese factoids, we reran our system C on these 24questions and then, we used the question-factoidpairs as the only resource for both training andtesting as described earlier (System D).
Table 5shows the MRRs for systems C and D on the 24questions covered by the factoids.System 24 TREC 2002 questionsC 0.472D 0.812Table 5: Factoid impact on system performance.It is very interesting to see that system Doutperforms significantly system C. This showsthat, in our framework, in order to benefit fromexternal databases, we do not need any additionalmachinery (question classifiers, answer typeidentifiers, wrapper selectors, SQL querygenerators, etc.)
All we need is a one-timeconversion of external structured resources tosimple natural language factoids.
The results inTable 5 also suggest that collecting naturallanguage factoids is a useful research direction: ifwe collect all the factoids in the world, we couldprobably achieve much higher MRR scores on theentire TREC collection.6 ConclusionIn this paper, we proposed a noisy-channel modelfor QA that can accommodate within a unifiedframework the exploitation of a large number ofresources and QA-specific techniques.
We believethat our work will lead to a better understanding ofthe similarities and differences between theapproaches that make up today?s QA researchlandscape.
We also hope that our paper will reducethe high barrier to entry that is explained by thecomplexity of current QA systems and increase thenumber of researchers working in this field:because our QA system uses only publiclyavailable software components (an IR engine; aparser; and a statistical MT system), it can beeasily reproduced by other researchers.However, one has to recognize that the reliance ofour system on publicly available components is notideal.
The generative story that our noisy-channelemploys is rudimentary; we have chosen it onlybecause we wanted to exploit to the best extentpossible existing software components (GIZA).The empirical results we obtained are extremelyencouraging: our noisy-channel system is alreadyoutperforming a state-of-the-art rule-based systemthat took many person years to develop.
It isremarkable that a statistical machine translationsystem can do so well in a totally different context,in question answering.
However, buildingdedicated systems that employ more sophisticated,QA-motivated generative stories is likely to yieldsignificant improvements.Acknowledgments.
This work was supported bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering forIntelligence (AQUAINT) Program under contractnumber MDA908-02-C-0007.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation.
Fi-nal Report, JHU Summer Workshop.Adam L. Berger, John D. Lafferty.
1999.
InformationRetrieval as Statistical Translation.
In Proceedings ofthe SIGIR 1999, Berkeley, CA.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,Andrew Ng.
2001.
Data-Intensive QuestionAnswering.
In Proceedings of the TREC-2001Conference, NIST.
Gaithersburg, MD.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263--312.Kenneth W. Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProceedings of the Second Conference on AppliedNatural Language Processing, Austin, TX.Charles L. A. Clarke, Gordon V. Cormack, Thomas R.Lynam, C. M. Li, G. L. McLearn.
2001.
WebReinforced Question Answering (MultiTextExperiments for TREC 2001).
In Proceedings of theTREC-2001Conference, NIST.
Gaithersburg, MD.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural Language BasedReformulation Resource and Web Exploitation forQuestion Answering.
In Proceedings of the TREC-2002 Conference, NIST.
Gaithersburg, MD.Edward H. Hovy, Ulf Hermjakob, Chin-Yew Lin.
2001.The Use of External Knowledge in Factoid QA.
InProceedings of the TREC-2001 Conference, NIST.Gaithersburg, MD.Abraham Ittycheriah and Salim Roukos.
2002.
IBM'sStatistical Question Answering System-TREC 11.
InProceedings of the TREC-2002 Conference, NIST.Gaithersburg, MD.Frederick Jelinek.
1997.
Statistical Methods for SpeechRecognition.
MIT Press, Cambridge, MA.Boris Katz, Deniz Yuret, Sue Felshin.
2001.
Omnibase:A universal data source interface.
In MIT ArtificialIntelligence Abstracts.Kevin Knight, Daniel Marcu.
2002.
Summarizationbeyond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence139(1): 91-107.Jimmy Lin.
2002.
The Web as a Resource for QuestionAnswering: Perspective and Challenges.
In LREC2002, Las Palmas, Canary Islands, Spain.Dan  Moldovan, Sanda Harabagiu, Roxana Girju, PaulMorarescu, Finley Lacatusu, Adrian Novischi,Adriana Badulescu, Orest Bolohan.
2002.
LCC Toolsfor Question Answering.
In Proceedings of theTREC-2002 Conference, NIST.
Gaithersburg, MD.Ion Muslea.
1999.
Extraction Patterns for InformationExtraction Tasks: A Survey.
In Proceedings ofWorkshop on Machine Learning and InformationExtraction (AAAI-99), Orlando, FL.Marius Pasca, Sanda Harabagiu, 2001.
The InformativeRole of WordNet in Open-Domain QuestionAnswering.
In Proceedings of the NAACL 2001Workshop on WordNet and Other Lexical Resources,Carnegie Mellon University, Pittsburgh PA.John M. Prager, Jennifer Chu-Carroll, Krysztof Czuba.2001.
Use of WordNet Hypernyms for AnsweringWhat-Is Questions.
In Proceedings of the TREC-2002 Conference, NIST.
Gaithersburg, MD.Jinxi Xu, Ana Licuanan, Jonathan May, Scott Miller,Ralph Weischedel.
2002.
TREC 2002 QA at BBN:Answer Selection and Confidence Estimation.
InProceedings of the TREC-2002 Conference, NIST.Gaithersburg, MD.
