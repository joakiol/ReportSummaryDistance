Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392?402,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsTowards Situated Dialogue: Revisiting Referring Expression GenerationRui Fang, Changsong Liu, Lanbo She, Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State University, East Lansing, MI, 48824, USA{fangrui,cliu,shelanbo,jchai}@cse.msu.eduAbstractIn situated dialogue, humans and agents havemismatched capabilities of perceiving theshared environment.
Their representationsof the shared world are misaligned.
Thusreferring expression generation (REG) willneed to take this discrepancy into consider-ation.
To address this issue, we developeda hypergraph-based approach to account forgroup-based spatial relations and uncertain-ties in perceiving the environment.
Our em-pirical results have shown that this approachoutperforms a previous graph-based approachwith an absolute gain of 9%.
However, whilethese graph-based approaches perform effec-tively when the agent has perfect knowledgeor perception of the environment (e.g., 84%),they perform rather poorly when the agent hasimperfect perception of the environment (e.g.,45%).
This big performance gap calls for newsolutions to REG that can mediate a sharedperceptual basis in situated dialogue.1 IntroductionSituated human robot dialogue has received increas-ing attention in recent years.
In situated dialogue,robots/artificial agents and their human partners areco-present in a shared physical world.
Robots needto automatically perceive and make inference of theshared environment.
Due to its limited perceptualand reasoning capabilities, the robot?s representationof the shared world is often incomplete, error-prone,and significantly mismatched from that of its humanpartner?s.
Although physically co-present, a jointperceptual basis between the human and the robotcannot be established (Clark and Brennan, 1991).Thus, referential communication between the hu-man and the robot becomes difficult.How this mismatched perceptual basis affects ref-erential communication in situated dialogue was in-vestigated in our previous work (Liu et al 2012).In that work, the main focus is on reference resolu-tion: given referential descriptions from human part-ners, how to identify referents in the environmenteven though the robot only has imperfect percep-tion of the environment.
Since robots need to col-laborate with human partners to establish a joint per-ceptual basis, referring expression generation (REG)becomes an equally important problem in situateddialogue.
Robots have much lower perceptual capa-bilities of the environment than humans.
How cana robot effectively generate referential descriptionsabout the environment so that its human partner canunderstand which objects are being referred to?There has been a tremendous amount of workon referring expression generation in the last twodecades (Dale, 1995; Krahmer and Deemter, 2012).However, most existing REG algorithms were devel-oped and evaluated under the assumption that agentsand humans have access to the same kind of domaininformation.
For example, many experimental se-tups (Gatt et al 2007; Viethen and Dale, 2008;Golland et al 2010; Striegnitz et al 2012) weredeveloped based on a visual world for which the in-ternal representation is assumed to be known andcan be represented symbolically.
However, this as-sumption no longer holds in situated dialogue withrobots.
There are two important distinctions in situ-ated dialogue.
First, the perfect knowledge of the en-vironment is not available to the agent ahead of time.The agent needs to automatically make inferences toconnect recognized lower-level visual features with392symbolic labels or descriptors.
Both recognition andinference are error-prone and full of uncertainties.Second, in situated dialogue the agent and the hu-man have mismatched representations of the envi-ronment.
The agent needs to take this difference intoconsideration to identify the most reliable featuresfor REG.
Given these two distinctions, it is not clearwhether state-of-the-art REG approaches are appli-cable under mismatched perceptual basis in situateddialogue.To address this issue, this paper revisits the prob-lem of REG in the context of mismatched percep-tual basis.
We extended a well known graph-basedapproach (Krahmer et al 2003) that has shownto be effective in previous work (Gatt and Belz,2008; Gatt et al 2009).
We incorporated uncer-tainties in perception into cost functions.
We fur-ther extended regular graph representation into hy-pergraph representation to account for group-basedspatial relations that are important for visual descrip-tions (Dhande, 2003; Tenbrink and Moratz, 2003;Funakoshi et al 2006; Liu et al 2012).
Our em-pirical results demonstrate that both enhancementslead to about a 9% absolute performance gain com-pared to the original approach.
However, whileour approache performs effectively when the agenthas perfect knowledge or perception of the environ-ment (e.g., 84%), it performs poorly under the mis-matched perceptual basis (e.g., 45%).
This perfor-mance gap calls for new solutions for REG that arecapable of mediating mismatched perceptual basis.In the following sections, we first describe ourhypergraph-based representations and illustrate howuncertainties from automated perception can be in-corporated.
We then describe an empirical study us-ing Amazon Mechanical Turks for evaluating gener-ated referring expressions.
Finally we present evalu-ation results and discuss potential future directions.2 Related WorkSince the Full Brevity algorithm (Dale, 1989), manyapproaches have been developed and evaluated forREG (Dale, 1995; Krahmer and Deemter, 2012),such as the incremental algorithm (Dale, 1995),the locative algorithm (Kelleher and Kruijff, 2006),and graph-based approaches (Krahmer et al 2003;Croitoru and Van Deemter, 2007).
Most of these ap-proaches assume the agent has access to a completesymbolic representation of the domain.
While theseapproaches work well for many applications involv-ing user interfaces, the question is whether they canbe extended to the situation where the agent has in-complete or incorrect knowledge and needs to makeinference about the domain or the world.Recently, there has been increasing interest inREG for visual objects (Roy, 2002; Golland et al2010; Mitchell et al 2013).
Some work (Gollandet al 2010) uses visual scenes that are generated bycomputer graphics and thus the internal representa-tion of the scene is known.
Some other work focuseson the connection between lower-level visual fea-tures and symbolic descriptors for REG (Roy, 2002;Mitchell et al 2013).
However, most work assumesno vision recognition errors.
It is well establishedthat automated recognition of visual scenes is ex-tremely challenging.
This process is error-proneand full of uncertainties.
It is not clear whetherthe existing approaches can be extended to the sit-uation where the agent has imperfect perception ofthe shared environment.An earlier work by Horacek (Horacek, 2005)has looked into the problem of mismatched knowl-edge between conversation partners for REG.
Theapproach is a direct extension of the incremental al-gorithm (Dale, 1995).
However, this work only pro-vides a proof of concept example to illustrate theidea.
No empirical evaluation was given.All these previous works have motivated ourpresent investigation.
We are interested in REG un-der mismatched perceptual basis between conversa-tion partners, where the agent has imperfect percep-tion and knowledge of the shared environment.
Inparticular, we took a well-studied graph-based ap-proach (Krahmer et al 2003) and extended it to in-corporate group spatial relations and uncertaintiesassociated with automated perception of the envi-ronment.
The reason we chose a graph-based ap-proach is that graph representations are widely usedin the fields of computer vision (CV) and patternrecognition to represent spatially rich scenes.
Never-theless, the findings from this investigation provideinsight to other approaches.393(a) an original scene (b) the corresponding impoverishedsceneFigure 1: An original scene and its impoverished scene processed by CV algorithm3 Hypergraph-based REGTowards mediating a shared perceptual basis in sit-uated dialogue, our previous work (Liu et al 2012)has conducted experiments to study referential com-munication between partners with mismatched per-ceptual capabilities.
We simulated mismatched ca-pabilities by making an original scene (Figure 1(a))available to a director (simulating higher perceptualcalibre) and a corresponding impoverished scene(Figure 1(b)) available to a matcher (simulating low-ered perceptual calibre).
The impoverished sceneis created by re-rendering automated recognition re-sults of the original scene by a CV algorithm.
Anexample of the original scene and an impoverishedscene is shown in Figure 1.
Using this setup, the di-rector and the matcher were instructed to collaboratewith each other on some naming games.
Throughthese games, they collected data on how partnerswith mismatched perceptual capabilities collaborateto ground their referential communication.The setup in (Liu et al 2012) is intended to sim-ulate situated dialogue between a human (like thedirector) and a robot (like the matcher).
The robothas a significantly lowered ability in perception andreasoning.
The robot?s internal representation of theshared world will be much like the impoverishedscene which contains many recognition errors.
Thedata from (Liu et al 2012; Liu et al 2013) showsthat different strategies were used by conversationpartners to produce referential descriptions.
Besidesdirectly describing attributes or binary relations witha relatum, they often use group-based descriptions(e.g., a cluster of four objects on the right).
This ismainly due to the fact that some objects are simplynot recognizable to the matcher.
Binary spatial rela-tionships sometimes are difficult to describe the tar-get object, so the matcher must resort to group infor-mation to distinguish the target object from the restof the objects.
For example, suppose the matcherneeds to describe the target object 5 in Figure 1(b),he/she may have to start by indicating the group ofthree objects at the bottom and then specify the re-lationship (i.e., top) of the target object within thisgroup.The importance of group descriptions has beenshown not only here, but also in previous workson REG (Funakoshi et al 2004; Funakoshi et al2006; Weijers, 2011).
While the original graph-based approach can effectively represent attributesand binary relations between objects (Krahmer et al2003), it is insufficient to capture within-group orbetween-group relations.
Therefore, to address thelow perceptual capabilities of artificial agents, we in-troduce hypergraphs to represent the shared environ-ment.
Our approach has two unique characteristicscompared to previous graph-based approaches: (1)A hypergraph representation is more general thana regular graph.
Besides attributes and binary re-lations, it can also represent group-based relations.
(2) Unlike previous work, here the generation of hy-pergraphs are completely driven by automated per-ception of the environment.
This is done by incor-porating uncertainties in perception and reasoninginto cost functions associated with graphs.
Next we394give a detailed account on hypergraph representa-tion, cost functions incorporating uncertainties, andthe search algorithm for REG.3.1 Hypergraph RepresentationA directed hypergraph G (Gallo et al 1993) is atuple of the form: G = ?X, A?, in whichX = {xm}A = {ai = (ti, hi) | ti ?
X, hi ?
X}Similar to regular graphs, a hypergraph consistsof a set of nodes X and a set of arcs A. However,different from regular graphs, each arc in A is con-sidered as a hyperarc in the sense that it can capturerelations between any two subsets of nodes: a tail(ti) and a head (hi).
Therefore, a hypergraph is ageneralization of a regular graph.
It becomes a reg-ular graph if the cardinalities of both the tail and thehead are restricted to one for all hyperarcs.
Whileregular graphs are commonly used to represent bi-nary relations between two nodes, hypergraphs pro-vide a more general representation for n-ary rela-tions among multiple nodes.We use hypergraphs to represent the agent?s per-ceived physical environment (also called scene hy-pergraphs).
More specifically, each perceived ob-ject is represented by a node in the graph.
Each per-ceived visual attribute of an object (e.g., color, size,type information) or a group of objects (e.g., num-ber of objects in the group, location) is captured bya self-looping hyperarc.
Hyperarcs are also used tocapture the spatial relations between any two subsetsof nodes, whether it is a relation between two ob-jects, or between two groups of objects, or betweenone or more objects within a group of objects.For example, Figure 2 shows a hypergraph cre-ated for part of the impoverished scene shown inFigure 1(b) (i.e., the upper right corner includingobjects 7, 8, 9, 11, and 13).
One important char-acteristic is that, because the graph is created basedon an automated vision recognition system, the val-ues of an attribute or a relation in the hypergraphare numeric (except for the type attribute).
For ex-ample, the value of the color attribute is the RGBdistribution extracted from the corresponding visualobject, the value of the size attribute is the width andheight of the bounding box and the value of the lo-cation attribute is a function of spatial coordinates.These numerical features will be further converted tosymbolic labels with certain confidence scores (de-scribed later in Section 3.3.2).3.2 Hypergraph PruningThe perceived visual scene can be represented as acomplete hypergraph, in which any pair of two sub-sets of nodes are connected by a hyperarc.
However,such a complete hypergraph is not only inefficientbut also unnecessary.
Instead of keeping all possiblen-ary relations (i.e., hyperarcs), we only retain thoserelations that are likely used by humans to producereferring expressions, based on two heuristics.The first heuristic is based on perceptual prin-ciples, also called the Gestalt Laws of perception(Sternberg, 2003), which describe how people groupvisually similar objects into entities or groups.
Twowell known principles of perceptual grouping areproximity and similarity (Wertheimer, 1938): ob-jects that lie close together are often perceived asgroups; objects of similar shape, size or color aremore likely to form groups than objects differingalong these dimensions.
Based on these two prin-ciples, previous works have developed different al-gorithms for perceptual grouping (Thrisson, 1994;Gatt, 2006).
In our investigation, we adopted Gatt?salgorithm (Gatt, 2006), which has shown to be moreaccurate for spatial grouping.
Given the resultsfrom spatial grouping, we only retain hyperarcs thatrepresent spatial relations between two objects, be-tween two perceived groups, between one object anda perceived group, or between one object and thegroup it belongs to.The second heuristic is based on the observationthat, given a certain orientation, people tend to use arelatum that is closer to the referent than more dis-tant relata.
In other words, it is less likely to refer toan object relative to a distant relatum when there isa closer relatum.
For example, when referring to thestapler (object 9 in Figure 1(a) ), it is more likely touse ?the stapler above the battery?
than ?the staplerabove the cellphone?.
Based on this observation, weprune the hypergraphs by only retaining hyperarcsbetween an object and their closest relata for eachpossible orientation.Figure 2 shows the resulting hypergraph for rep-resenting a subset of objects (7, 8, 9, 11, and 13) inFigure 1(a).395Figure 2: An example of hypergraph representing the per-ceived scene (a partial scene only including object 7, 8,9, 11, 13 for Figure 1(a)).3.3 Symbolic Descriptors for AttributesAs mentioned earlier, the values of attributes of ob-jects and their relations are numerical in nature.
Inorder for the agent to generate natural language de-scriptions, the first step is to assign symbolic labelsor descriptors to those attributes and relations.
Nextwe describe how we use a lexicon with grounded se-mantics in this process.3.3.1 Lexicon with Grounded SemanticsGrounded semantics provides a bridge to connectsymbolic labels or words with lower level visual fea-tures (Harnad, 1990).
Previous work has developedvarious approaches for grounded semantics mainlyfor the reference resolution task, i.e., identifying vi-sual objects in the environment given language de-scriptions (Dhande, 2003; Gorniak and Roy, 2004;Tenbrink and Moratz, 2003; Siebert and Schlangen,2008; Liu et al 2012).
For the referring expressiongeneration task here, we also need a lexicon withgrounded semantics.In our lexicon, the semantics of each categoryof words is defined by a set of semantic groundingfunctions that are parameterized on visual features.For example, for the color category it is defined as amultivariate Gaussian distribution based on the RGBdistribution.
Specific words such as green, red, orblue have different means and co-variances as thefollowing:color : red = fr(~vcolor) = N(~vcolor | ?1,?1)color : green = fg(~vcolor) = N(~vcolor | ?2,?2)color : blue = fb(~vcolor) = N(~vcolor | ?3,?3)The above functions define how likely a set of rec-ognized visual features (i.e., ~vcolor) describing thecolor dimensions (i.e., RGB distribution) is to matchthe color terms red, green, and blue.For the spatial relation terms such as above, be-low, left, right, the semantic grounding functionstake both vertical and horizontal coordinates of twoobjects, as follows 1:spatialRel : above(a, b) = fabove(~valoc, ~vbloc)={1?
|xa?xb|400 if ya < yb;0 otherwise.Using the above convention, we have defined se-mantic grounding functions for size category words(e.g., small and big) and absolute position words(e.g., top, below, left, and right).
In addition, we useobject recognition models (Zhang and Lu, 2002) todefine class type category words such as apple andorange used in our domain.3.3.2 Attribute Descriptors and Cost FunctionsGiven the lexicon with grounded semantics as de-scribed above, the numerical attributes captured inthe scene hypergraph can be converted to symbolicdescriptors.
For each attribute (e.g., color) or re-lation, the corresponding visual feature vector (i.e.,~vcolor) is plugged into the semantic grounding func-tions for the corresponding category of words.
Theword that best describes the attribute is chosen as thedescriptor for that attribute.
For example, given anRGB color distribution ~vcolor, we can find the colordescriptor as follows:color : w?
= argmaxred,green,bluefw(~vcolor),For each attribute or relation, we can find a bestdescriptor in this manner.
In addition, we also ob-tain a numerical value (returned from the semantic1The size of the overall scene is 800x800.396grounding functions) that measures how well thisdescriptor describes the corresponding visual fea-tures.
Intuitively, one would choose a descriptor thatclosely matches the visual features.
Based on thisintuition, we define the cost for each attribute A asthe following:cost(A) = 1?
fw?
( ~vA)where w?
is the best descriptor for the attribute.Given an attribute, the better the descriptormatches the extracted visual features, the lower thecost of the corresponding hyperarc.3.4 Graph Matching for REGNow the hypergraph representing the perceived en-vironment has symbolic descriptors for its attributesand relations together with corresponding costs.Given this representation, REG can be formulated asa graph matching algorithm similar to that describedin (Krahmer et al 2003).
We use the same Branchand Bound algorithm described in (Krahmer et al2003).
In this approach, a hypothesis hypergraph(starting with one node representing the target ob-ject) is gradually expanded by adding in a least costhyperarc from the scene hypergraph.
At each ex-pansion, the hypothesis graph is matched against thescene hypergraph to decide whether it matches anynodes other than the target node in the scene hyper-graph.
The expansion stops if the hypothesis graphdoes not cover any other nodes except for the targetnode.
At this point, the hypothesis graph captures allthe content (e.g., attributes and relations) required touniquely describe the target object.
We then applya set of simple generation templates to generate thesurface form of referring expressions based on thehypothesis graph.4 Empirical Evaluations4.1 Evaluation SetupTo evaluate the performance of this hypergraph-based approach to REG, we conducted a compara-tive study using crowd-sourcing.
More specifically,we created 48 different scenes similar to that in Fig-ure 1(a).
Each scene has 13 objects on average andthere are 621 objects in total.
For each of thesescenes, we applied a CV algorithm (Zhang and Lu,2002) and generated scene hypergraphs as describedin Section 3.1.
We then use different generationstrategies (varied in terms of graph representationsand cost functions, to be explained in Section 4.2) toautomatically generate referring expressions to referto each object.To evaluate the quality of these generated refer-ring expressions, we applied Amazon MechanicalTurk to solicit feedback from the crowd 2.
Throughan interface, we displayed an original scene and gen-erated referring expressions (from different genera-tion strategies) in a random order.
We asked eachturk to select the object in the scene that he/she be-lieved was the one referred to by the shown refer-ring expression (i.e., reference identification task).Each referring expression received three votes fromthe crowd.
In total, 217 turks participated in our ex-periment.4.2 Generation StrategiesWe applied a set of different strategies to generatereferring expressions for each object.
The variationslie in two dimensions: (1) different graph repre-sentations: using a hypergraph to represent the per-ceived scene as described in Section 3.1 versus us-ing a regular graph as introduced in (Krahmer et al2003); and (2) different cost functions for attributesand relations: cost functions that have been used inprevious works (Theune et al 2007; Krahmer et al2008) and cost functions that incorporate uncertain-ties of perception as described in Section 3.3.2.Cost functions play an important role in graph-based approaches (Krahmer et al 2003).
Previousworks have examined different types of cost func-tions (Theune et al 2007; Krahmer et al 2008;Theune et al 2011).
We adopted some commonlyused cost functions from previous work togetherwith the cost functions defined here.
In particular,we experimented with the following different costfunctions:Simple Cost: The costs for all hyperarcs are set to1.
With this cost function, the graph-based algorithmresembles the Full Brevity algorithm of Dale (Dale,2To control the quality of crowdsourcing, we recruited par-ticipants based on the following criteria: Participants?
locationsare limited to the United States.
Approval rate for each partic-ipant?s previous work is greater than or equal to 95%, and thenumber of each participant?s previous approved work is greaterthan or equal to 1000.3971992) in that a shortest distinguishing description ispreferred.Absolute Preferred: The costs for hyperarcs rep-resenting absolute attributes (e.g., type, color, andposition) are set to 1.
The costs for relative at-tributes (e.g., size) and relations are set to 2.
Thiscost function mimics human?s preference for abso-lute attributes over relative ones (Dale, 1995).Relative Preferred: The costs for hyperarcs repre-senting absolute attributes are set to 2 and for rela-tive attributes and relations are set to 1.
This costfunction has been applied previously to emphasizethe importance of spatial relations in REG (Viethenand Dale, 2008).Uncertainty Based: The costs for all hyperarcs aredefined by incorporating uncertainties from percep-tion as described in Section 3.3.2.Uncertainty Relative Preferred: To emphasize theimportance of spatial relations as demonstrated insituated interaction (Tenbrink and Moratz, 2003;Kelleher and Kruijff, 2006), the costs for hyperarcsrepresenting relative attributes and relations are di-vided by 3.
This cost function will allow the algo-rithm to prefer spatial relations through the reducedcost.Note that we only tested a few (not all) com-monly used cost functions proposed by previouswork (Krahmer et al 2003; Theune et al 2007;Krahmer et al 2008; Theune et al 2011).
For ex-ample, we did not include the stochastic cost func-tion which is defined based on the frequencies of at-tribute selection from the training data (Krahmer etal., 2003).
On the one hand, we did not have a largeset of human descriptions of the impoverished sceneto learn the stochastic cost.
On the other hand, itis not clear whether human strategies of describingthe impoverished scene should be used to representoptimal strategies for the robot.
Nevertheless, theabove different cost functions will allow us to eval-uate whether incorporating perceptual uncertaintieswill make a difference in the REG performance.4.3 Evaluation ResultsAs mentioned earlier, each generated referring ex-pression received three independent votes regardingits referent from the crowd.
The referent with themost votes is taken as the predicted referent and isused for evaluation.
If all three votes are differ-Cost Function Regular Graph HypergraphSimple Costs 33.2% 33.3%Absolute Preferred 30.1% 30.3%Relative Preferred 31.1% 35.4%Uncertainty Based 35.7% 37.5%Uncertainty Rel.
Prefer.
36.7% 45.2%Table 1: Results with different cost functionsent, then by default, it is deemed that the referentis not correctly identified for that expression.
Weuse the accuracy of the referential identification task(i.e., the percentage of generated referring expres-sions where the referents are correctly identified) asthe metric to evaluate different generation strategiesillustrated in Section 4.2.4.3.1 The Role of Cost FunctionsTable 1 shows the results based on different costfunctions and different graph representations.
Thereare several observations.First, when the agent does not have perfect knowl-edge of the environment and has to automaticallyinfer the environment as in our setting here, costfunctions based on uncertainties of perception leadto better results.
This occurs for both regular graphsand hypergraphs.
This result is not surprising andindicates that cost functions should be tied to theagent?s ability to perceive and infer the environment.The uncertainty based cost functions allow the agentto prefer reliable attributes or relations.Second, consistent with previous work (Viethenand Dale, 2008), we observed the importance of spa-tial relations.
Especially when the perceived worldis full of uncertainties, spatial relations tend to bemore reliable.
In particular, as shown in Table 1,using hypergraphs enables generating group-basedrelations and results in significantly better perfor-mance (45.2%) compared to regular graphs (36.7%)(p = 0.002).Note that our current cost function only includesuncertainties of the agent?s own perception in a sim-plistic form.
When humans and agents have mis-matched perceptual basis, the human?s model ofcomprehension and tolerance of inaccurate descrip-tion could play a role in REG.
Incorporating humanmodels in the cost function will require in-depth em-pirical studies and we will leave that to our future398work.4.3.2 The Role of Imperfect PerceptionTo further understand the role of hypergraphs inmediating mismatched perceptions between humansand agents, we created a perfect scene regular graphand a perfect scene hypergraph (representing theagent?s perfect knowledge of the environment) foreach of the 48 scenes used in the experiments.
Ineach of these scene graphs, the attribute and rela-tion descriptors are manually provided.
We fur-ther applied the Absolute Preferred cost function(which has shown competitive performance in previ-ous work) to generate referring expressions for eachobject.
Again, each referring expression receivedthree votes from the crowd.Table 2 shows the results comparing two con-ditions: (1) REs generated (by the Absolute Pre-ferred cost function) based on the perfect graphswhich represent the agent?s perfect knowledge andperception of the environment; and (2) REs gener-ated based on automatically created graphs (by theUncertainty Relative Preferred cost function) whichrepresent the agent?s imperfect knowledge of theenvironment as a result of automated recognitionand inference.
The result shows that given perfectknowledge of the environment, hypergraphs onlyperform marginally better than the regular graphs(p = 0.07).
Given imperfect knowledge of the envi-ronment, hypergraphs significantly outperforms theregular graphs by taking advantage of spatial group-ing information (p = 0.002).
It is worthwhile tomention that currently we use spatial proximity toidentify groups.
However, the hypergraph based ap-proach is not restricted to spatial grouping.
In the-ory, it can represent any type of group based on dif-ferent similarity criteria.Furthermore, our result shows that the graph-based approaches perform quite competitively underthe condition of perfect knowledge and perception.Although evaluated on different data sets, this resultis consistent with results from previous work (Gattand Belz, 2008; Gatt et al 2009).
However, what ismore interesting here is that while graph-based ap-proaches perform well when the agent has perfectknowledge of the environment, as its human part-ner, these approaches literally fall apart with closeto 40% performance degradation when applied toEnvironment Regular Graph HypergraphPefect Perception 80.4% 84.2%Imperfect Perception 36.7% 45.2%Table 2: Results of comparing perfect perception and im-perfect perception of the shared world.the situation where the agent?s representation of theshared world is problematic and full of mistakes.These results indicate that REG for automati-cally perceived scenes can be extremely challeng-ing.
Many errors result from automated perceptionand reasoning that will affect the internal representa-tion of the world and thus the generated REs.
In ourexperiments here, we applied a very basic CV algo-rithm which resulted in rather poor performance inour data: overall, 60.3% of objects in the originalscene are mis-recognized, and 10.5% of objects aremis-segmented.
We think this poor CV performancerepresents a more challenging problem.Some errors such as recognition errors can be by-passed using our current approach based on hyper-graphs.
For example, in Figure 1 target object 9 (astapler) and 13 (a key) are mis-recognized as a cupand a pen.
Using our hypergraph-based approach,for the target object 9, instead of generating ?a smallcup?
(as in the case of using regular graphs), ?a grayobject on the top within a cluster of four objects?is generated.
For the target object 13, instead of ?apen?
as generated by regular graphs, ?a small objecton the right within a cluster of 4?
is generated.
Evenwith recognition errors, these group-based descrip-tions will allow the listener to identify target objectsin their representation correctly.
Nevertheless, manyprocessing errors cannot be handled by our currentapproach.
For example, an object can be mistak-enly segmented into multiple parts or several objectscan be mistakenly grouped into one object.
In addi-tion, our current semantic grounding functions aresimple.
Sometimes they do not provide correct de-scriptors for the extracted visual features.
More so-phisticated functions that better reflect human?s vi-sual perception (Regier, 1996; Mojsilovic, 2005;Mitchell et al 2011) should be pursued in the fu-ture.399Minimum Effort Extra EffortPefect Perception 84.2% 88.1%Imperfect Perception 45.2% 51.5%Table 3: Results of comparing minimum effort and extraeffort using hypergraphs4.3.3 The Role of Extra EffortWhile REG systems have a tendency to produceminimal descriptions, recent psycholinguistic stud-ies have shown that speakers do not necessarily fol-low the Grice?s maxim of quantity, and they tendto provide redundant properties in their descrip-tions (Jordan and Walker, 2000; Belke and Meyer,2002; Arts et al 2011).
With this in mind, weconducted a very simple evaluation on the role ofextra effort.
Once a set of descriptors are selectedbased on the minimum cost, one additional descrip-tor (with the least cost among the remaining at-tributes or relations) is added to the referential de-scription.
We once again solicited the crowd feed-back to this set of expressions generated by extraeffort.
Each expression again received three votesfrom the crowd.Table 3 shows the results by comparing minimumeffort with extra effort when using hypergraphs togenerate REs.
As indicated here, extra effort (byadding one additional descriptor) leads to more com-prehensible REs with 3.9% improvement under per-fect perception and 6.3% improvement under imper-fect perception (both are significant, p < 0.05).
Theimprovement is larger under imperfect perception.This seems to indicate that exploring extra effort inREG could help mediate mismatched perceptions insituated dialogue.
However, more understanding onhow to engage in such extra effort will be requiredin the future.5 ConclusionIn situated dialogue, humans and agents have mis-matched perceptions of the shared environment.
Tofacilitate successful referential communication be-tween a human and an agent, the agent needs to takesuch discrepancies into consideration and generatereferential descriptions that can be understood byits human partner.
With this in mind, we re-visitedthe problem of referring expression generation in thecontext of mismatched perceptions between humansand agents.
In particular, we applied and extendedthe state of the art graph-based approach (Krahmeret al 2003) in this new setting.
Our empirical re-sults have shown that, to address the agent?s limitedperceptual capability, REG algorithms will need totake into account the uncertainties in perception andreasoning.
Group-based information appears morereliable and thus should be modeled by an approachthat deals with automated perception of spatiallyrich scenes.While graph-based approaches have shown effec-tive for the situation where the agent has completeknowledge of the environment, as its human part-ner, these approaches are often inadequate when hu-mans and agents have mismatched representationsof the shared world.
Our empirical results here callfor new solutions to address the mismatched per-ceptual basis.
Previous work indicated that referen-tial communication is a collaborative process (Clarkand Wilkes-Gibbs, 1986; Heeman and Hirst, 1995).Conversation partners make extra effort to collab-orate with each other.
For the situation with mis-matched perceptual basis, a potential solution thusshould go beyond the objective of generating a mini-mum description, and towards a collaborative modelwhich incorporates immediate feedback from theconversation partner (Edmonds, 1994).6 AcknowledgmentsThis work was supported by N00014-11-1-0410from the Office of Naval Research and IIS-1208390from the National Science Foundation.ReferencesAnja Arts, Alfons Maes, Leo Noordman, and CarelJansen.
2011.
Overspecification facilitates objectidentification.
Journal of Pragmatics, 43(1):361?374.E.
Belke and A. S. Meyer.
2002.
Tracking the timecourse of multidimensional stimulus discrimination:Analyses of viewing patterns and processing timesduring ?same?-?different?
decisions.
European Jour-nal of Cognitive Psychology, 14(2):237?266.H.H.
Clark and S.E.
Brennan.
1991.
Grounding in com-munication.
Perspectives on socially shared cognition,13:127?149.H.
H Clark and D Wilkes-Gibbs.
1986.
Referring as acollaborative process.
Cognition, 22:1?39.400Madalina Croitoru and Kees Van Deemter.
2007.
A con-ceptual graph approach to the generation of referringexpressions.
In Proceedings of the 20th internationaljoint conference on Artifical intelligence, IJCAI?07,pages 2456?2461.Robert Dale.
1989.
Cooking up referring expressions.In Proceedings of the 27th annual meeting on Associ-ation for Computational Linguistics, ACL ?89, pages68?75, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Robert Dale.
1992.
Generating Referring Expressions:Constructing Descriptions in a Domain of Objects andProcesses.
The MIT Press,Cambridge, Massachusetts.Robert Dale.
1995.
Computational interpretations of thegricean maxims in the generation of referring expres-sions.
Cognitive Science, 19:233?263.Sheel Sanjay Dhande.
2003.
A computational model toconnect gestalt perception and natural language.
InMasters thesis, Massachusetts Institure of Technology.Philip G. Edmonds.
1994.
Collaboration on reference toobjects that are not mutually known.
In Proceedingsof the 15th conference on Computational linguistics -Volume 2, COLING ?94, pages 1118?1122, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,and Takenobu Tokunaga.
2004.
Generation of relativereferring expressions based on perceptual grouping.
InCOLING.Kotaro Funakoshi, Satoru Watanabe, and TakenobuTokunaga.
2006.
Group-based generation of referringexpressions.
In INLG, pages 73?80.Giorgio Gallo, Giustino Longo, Stefano Pallottino, andSang Nguyen.
1993.
Directed hypergraphs and ap-plications.
Discrete applied mathematics, 42(2):177?201.Albert Gatt and Anja Belz.
2008.
Attribute selection forreferring expression generation: new algorithms andevaluation methods.
In Proceedings of the Fifth In-ternational Natural Language Generation Conference,INLG ?08, pages 50?58, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Albert Gatt, Ielka van der Sluis, and Kees van Deemter.2007.
Evaluating algorithms for the generation of re-ferring expressions using a balanced corpus.
In Pro-ceedings of the Eleventh European Workshop on Nat-ural Language Generation, ENLG ?07, pages 49?56,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Albert Gatt, Anja Belz, and Eric Kow.
2009.
The tuna-reg challenge 2009: overview and evaluation results.In Proceedings of the 12th European Workshop onNatural Language Generation, ENLG ?09, pages 174?182, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Albert Gatt.
2006.
Structuring knowledge for referencegeneration: A clustering algorithm.
In Proceedings ofthe 11th Conference of the European Chapter of theAssociation for Computational Linguistics, Associa-tion for Computational Linguistics, pages 321?328.Dave Golland, Percy Liang, and Dan Klein.
2010.
Agame-theoretic approach to generating spatial descrip-tions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?10, pages 410?419, Stroudsburg, PA, USA.Association for Computational Linguistics.Peter Gorniak and Deb Roy.
2004.
Grounded seman-tic composition for visual scenes.
Journal of ArtificialIntelligence Research, 21:429?470.Stevan Harnad.
1990.
The symbol grounding problem.Physica D, 42:335?346.Peter A. Heeman and Graeme Hirst.
1995.
Collaboratingon referring expressions.
Computational Linguistics,21:351?382.Helmut Horacek.
2005.
Generating referential descrip-tions under conditions of uncertainty.
In Proceedingsof the 10th European Workshop on Natural LanguageGeneration (ENLG) pages 58-67, Aberdeen, UK.Pamela W Jordan and Marilyn Walker.
2000.
Learningattribute selections for non-pronominal expressions.In Proceedings of the 38th Annual Meeting on Asso-ciation for Computational Linguistics, pages 181-190.John D. Kelleher and Geert-Jan M. Kruijff.
2006.
In-cremental generation of spatial referring expressionsin situated dialog.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Com-putational Linguistics, ACL-44, pages 1041?1048,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Emiel Krahmer and Kees Van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.computational linguistics, 38(1):173?218.Emiel Krahmer Krahmer, Sebastiaan van Erk, and Andre?Verleg.
2003.
Graph-based generation of referringexpressions.
Computational Linguistics, 29(1):53?72,March.Emiel Krahmer, Mariet Theune, Jette Viethen, and IrisHendrickx.
2008.
Graph: The costs of redundancyin referring expressions.
In In Proceedings of the 5thInternational Conference on Natural Language Gen-eration, Salt Fork OH, USA.Changsong Liu, Rui Fang, and Joyce Y. Chai.
2012.
To-wards mediating shared perceptual basis in situated di-alogue.
In Proceedings of the 13th Annual Meeting of401the Special Interest Group on Discourse and Dialogue,SIGDIAL ?12, pages 140?149, Stroudsburg, PA, USA.Association for Computational Linguistics.Changsong Liu, Rui Fang, Lanbo She, and Joyce Y. Chai.2013.
Modeling collaborative referring for situatedreferential grounding.
In The 14th Annual SIGdialMeeting on Discourse and Dialogue.Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2011.
Two approaches for generating size modifiers.In Proceedings of the 13th European Workshop onNatural Language Generation, ENLG ?11, pages 63?70, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2013.
Generating expressions that refer to visible ob-jects.
In Proceedings of NAACL-HLT 2013, pages1174-1184.Aleksandra Mojsilovic.
2005.
A computational modelfor color naming and describing color compositionof images.
IEEE Transactions on Image Processing,14:690 ?
699.Terry Regier.
1996.
The human semantic potential.
TheMIT Press,Cambridge, Massachusetts.Deb Roy.
2002.
Learning visually grounded words andsyntax of natural spoken language.
Evolution of Com-munication, 4.Alexander Siebert and David Schlangen.
2008.
A sim-ple method for resolution of definite reference in ashared visual context.
In Proceedings of the 9th SIG-dial Workshop on Discourse and Dialogue, SIGdial?08, pages 84?87, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Robert Sternberg.
2003.
Cognitive Psychology,ThirdEdition.
Thomson Wadsworth.Kristina Striegnitz, Hendrik Buschmeier, and StefanKopp.
2012.
Referring in installments: a corpusstudy of spoken object references in an interactive vir-tual environment.
In Proceedings of the Seventh In-ternational Natural Language Generation Conference,INLG ?12, pages 12?16, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Thora Tenbrink and Reinhard Moratz.
2003.
Group-based spatial reference in linguistic human-robot in-teraction.
Spatial Cognition and Computation, 6:63?106.Marie?t Theune, Pascal Touset, Jette Viethen, and EmielKrahmer.
2007.
Cost-based attribute selection for gre(graph-sc/graph-fp).
In Proceedings of the MT SummitXI Workshop on Using Corpora for NLG: LanguageGeneration and Machine Translation (UCNLG+MT).Marie?t Theune, Ruud Koolen, Emiel Krahmer, andSander Wubben.
2011.
Does size matter ?
how muchdata is required to train a reg algorithm?
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 660?664, Portland, Oregon, USA,June.
Association for Computational Linguistics.Kristinn R. Thrisson.
1994.
Simulated perceptual group-ing: An application to human-computer interaction.
InProceedings of the Sixteenth Annual Conference of theCognitive Science Society, pages 876?881.Jette Viethen and Robert Dale.
2008.
The use of spa-tial relations in referring expression generation.
InProceedings of the Fifth International Natural Lan-guage Generation Conference, INLG ?08, pages 59?67, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.S.
Weijers.
2011.
Referring expressions with groups aslandmarks.
volume 15.
University of Twente.Max Wertheimer.
1938.
Laws of organization in per-ceptual forms.
A Source Book of Gestalt Psychology.Routledge and Kegan Paul, London.Dengsheng Zhang and Guojun Lu.
2002.
An integratedapproach to shape based image retrieval.
In Proc.of 5th Asian Conference on Computer Vision (ACCV,pages 652?657.402
