Proceedings of the Workshop on BioNLP, pages 46?54,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtraction of Named Entities from Tables in Gene Mutation LiteratureWern Wong?, David Martinez?
?, Lawrence Cavedon???
?NICTA Victoria Research Laboratory?Dept of Computer Science and Software EngineeringThe University of Melbourne{wongwl,davidm,lcavedon}@csse.unimelb.edu.auAbstract We investigate the challenge of extract-ing information about genetic mutations from ta-bles, an important source of information in scien-tific papers.
We use various machine learning algo-rithms and feature sets, and evaluate performance inextracting fields associated with an existing hand-created database of mutations.
We then show howclassifying tabular information can be leveraged forthe task of named entity detection for mutations.1Keywords Information extraction; tables;biomedical applications.1 IntroductionWe are interested in applying information extractionand text mining techniques to aiding the construc-tion of databases of biomedical information, in par-ticular information about genetic mutations.
Suchdatabases are currently constructed by hand: a long,involved, time-consuming and human-intensive pro-cess.
Each paper considered for inclusion in thedatabase must be read, the interesting data identifiedand then entered by hand into a database.2However, the biomedical domain throws up manynew and serious challenges to information extractionand text mining.
Unusual terminology and under-developed standards for nomenclature present prob-lems for tokenisation and add complexity to stan-dard information extraction tasks, such as named en-tity recognition (NER).
A lack of resources (at least1A short version of this paper was presented at the Aus-tralasian Document Computing Symposium, 2008.
All copy-rights from that event were retained by the authors.2Karamis et al(2008) illustrate how even simple tools canhave an impact on improving the database-curation process.compared to other domains), such as collections ofannotated full-text documents and relevance judge-ments for various tasks, are a bottleneck to develop-ing and evaluating the core techniques required.In this paper, we report on work performed onextracting information from tables in biomedicalresearch papers.
Tables present a succinct andinformation-rich format for providing information,and are particularly important when reporting re-sults in biological and medical research papers.For example, the Human Genome Variation Society(HGVS), in its general recommendations for muta-tion nomenclature, recommends making use of tab-ular listings when several changes are described ina manuscript.3 A specific premise of our work isthat the highly-structured nature of tabular informa-tion allows leverage of some techniques that are notso sensitive to the well-reported problems inherentin biomedical terminology, which complicate NERtasks in this domain.
In particular, we describeinitial techniques for extending NER performancethrough the analysis of tables: columns/rows areclassified as containing items of the entities of inter-est, thereby allowing those entities to be recognizedas of the target type.
Since a significant amount ofsuch entities may be found in tables in biomedicalscientific papers, this can have positive impact onthe performance of base NER techniques.NER tools specifically targeted at recognisingmutations have been developed (e.g.
(Horn et al,2004; Baker and Witte, 2006; Caporaso et al, 2007;Lee et al, 2007)); however, they only detect a sub-class of mutations, so-called single-point mutations,3http://www.hgvs.org/mutnomen/recs.html#general46i.e.
those that affect a single base.
MutationFinder(Caporaso et al, 2007) is the only publicly availabletool, built with around 700 automatically-generatedrules (both for different nomenclatures and naturallanguage).
However, most of the mutations thatwe find in our dataset are not point mutations ordo not follow point-mutation nomenclature, limitingthe usefulness of MutationFinder (and related tools)over our document collection.In the next section, we describe the setting of ourtask, the Mismatch Repair (MMR) Database, andoutline the task of extraction from tables.
In Sec-tion 3, we describe the preparation of our documentcollection, and in Section 4, we analyse the amountof mutation-related information that is in the associ-ated tables.
Section 5 describes the main task, whichis classifying table rows and columns as containingmutations, and Section 6 leverages this technique todetect mutations of interest to the MMR Database.We discuss the results in Section 7.2 BackgroundIn this section, we discuss the MMR database?thesetting for our task and from which we constructour document collection?and previous approachesto table processing.2.1 The MMR DatabaseOur extraction task is grounded in the specific con-text of the Mismatch Repair (MMR) Database com-piled at the Memorial University of Newfoundland(Woods et al, 2007)?a database of known geneticmutations related to hereditary non-polyposis col-orectal cancer (HNPCC), a hereditary form of bowelcancer.
The MMR Database contains informationon genetic mutations known to be related to HN-PCC, along with links to the research papers fromwhich the database has been constructed.4 From thedatabase and its links to papers, we were able to con-struct a collection of tables related to HNPCC muta-tions, and then use the MMR database records them-selves as a gold standard for evaluating our tech-niques.
As of May 2008, the MMR database con-tained a total of 5,491 records on mutations that oc-4I.e.
a team of geneticists manually trawled the biomedicalliterature for information on HNPCC-related mutation informa-tion, and added links to any papers relevant to those mutationsin the context of HNPCC.cur on any one of four genes that have been identi-fied as related to colon cancer.
An example recordfrom the MMR database is the following:MLH1 | Exon13 | c.1491delG | Yamamoto et al | 9500462Respectively, this record contains: gene; exon;mutation; citation of the paper the information wassourced from;5 and the paper?s PubMedID.
Thesefields are important because they contain informa-tion researchers are directly interested in (gene,exon, mutation) and the paper said information wasfound in.
Note that if a gene/mutation pair is refer-enced in multiple papers, then there are correspond-ingly multiple entries in the database.
Conversely, ifa single paper mentions multiple (relevant) genes,then that paper is mentioned in multiple databaserecords.2.2 Table ProcessingAn important but less-researched sub-problem intext mining is information extraction from tables.This is particularly important in the biomedical do-main since much important data is present in tabu-lar form, such as experimental results, relations be-tween entities, and other information that may notbe contained elsewhere in the text.
For example, thetable shown in Figure 1 (taken from an article in ourcollection) contains much of the same data that waspresent in database records, in a similar format.Tabular information extraction can be divided intotwo broad sub-tasks:?
table detection: identifying tables within docu-ments;?
table processing: extraction of data from tables.Several systems have been developed to handle bothtasks, some are designed only to handle table de-tection, and others focus only on extracting data.Both machine learning and heuristic / rule-based ap-proaches have been proposed.Table detection techniques depend heavily on theinput format.
Most work that tackles this problemtends to assume one homogeneous input format, buttables generally come in one of two varieties:65This field has been abbreviated.
We have also omitted fieldssuch as ?internal id?.6We don?t consider the possibility of processing bitmaps orother images from scanned documents.47Figure 1: Sample table containing mutation information related to HNPCC?
raw text tables: generally ASCII text inmonospace font, delimited by whitespaceand/or special characters;?
rich text tables: those formatted using LaTeX,PDF, HTML and other such formats.Tables in plain text tend to be more difficult todetect, as the detection system must be sensitive towhitespace and symbols used to align cells in tables.Efforts to handle rich text formats generally focus onHTML-based representations.
Raw HTML is easierto parse than raw LaTeX or PDF, and most formatsare easily converted to HTML.
HTML tables cantheoretically be trivially detected using <table>tags.
However, Lerman et al(2004) note that inHTML files taken from the web, only a fraction oftabular data was presented using <table> tags, andthose tags were also used to format multi-columntext, images and other non-table applications.
Hurst(2001) attests that less than 30% of HTML tables onthe web contain actual tabular content; for many, theHTML table tags are often used simply for format-ting purposes.Zanibbi et al(2004) present a survey of tablerecognition in general.
Of greatest relevance to ushere are approaches that adopt a machine learningapproach to detecting and/or extracting table data.Cohen et al(2002) use features based on HTMLtable tags, number of rows and columns of spe-cific lengths, and ratios of single-square cells to to-tal number of cells, to perform table detection, andthen form a geometric representation of the data us-ing algorithms based on table-rendering techniquesimplemented by browsers.Pinto, Wei, and their colleagues have used condi-tional random fields (CRFs) to both detect and pro-cess tables simultaneously.
Pinto et al(2003) com-pare the output of their CRF system with a previ-ous effort using hidden Markov machines (HMMs).These systems use features such as: presence ofwhitespace of varying length (different lengths ofwhitespace are used as separate features); domain-specific lexical features (such as month names, yearstrings, specified keywords); separator characters(e.g.
?+?, ?-?, etc).
In subsequent work they developa system for performing question answering over ta-ble data (Wei et al, 2004) by treating each extracteddata cell as a discrete document.To our knowledge, no previous system has at-tempted to extract data from tables in biomedicalliterature.
This is possibly because of a combina-tion of the lack of resources for this domain (e.g.48collections of full-text documents; relevance judge-ments), as well as the lesser focus on text miningin general in this area.
As will be seen in the nextsection, the vagaries of the construction of our col-lection of tables means we were effectively able toignore the issue of table detection and focus directlyon the problem of processing.3 Experimental SettingOur experiments were designed to identify mentionsof mutations in the biomedical literature, focusingon tabular content.
In this section, we first describeour target dataset, built from the hand-curated MMRdatabase (Woods et al, 2007); we then explain thetable extraction process; finally, we introduce thetask design.3.1 Mutation Mention DatasetWe relied on the MMR Database and MEDLINE inorder to build our test collection.
First we collectedall the information available in the hand-curatedMMR records, obtaining a total of 5,491 mutationslinked to 719 distinct PubMedIDs7.Our next step was to crawl the full-text articlesfrom MEDLINE.
We used an automatic crawler thatfollowed the links from the PubMed interface, anddownloaded those papers that had a full-text HTMLversion, and which contained at least one content ta-ble.The tables were then extracted from the full textHTML files.
It is important to note that the tableswere already present as links to separate HTML filesrather than being presented as inline tables, makingthis process easier.
Papers that did not contain tablesin HTML format were discarded.Our final collection consisted of 70 papers out ofthe original 719 PubMedIDs.
Some of the paperswere not available in full text, and for others ourcrawling script failed to extract the full version.
Ourapproach was conservative, and our collection couldbe augmented in the future, but we decided to fo-cus on this dataset for the experiments presented inthis paper.
This set of articles is linked to 717 MMRrecords (mutations), which constitutes our gold stan-dard hand-curated annotation.
The collection con-tains 197 tables in all.7Data was downloaded from the web interface in May 2008.3.2 Table extractionOnce scraped, the tables were then pre-processedinto a form that more readily allowed experimenta-tion.
The tables were therefore split into three parts:column headers, row headers, and data cells.
Thiswas done based on the HTML formatting, whichwas consistent throughout the data set as the tableswere automatically generated.The first step was to deconstruct the HTML ta-bles into nested lists of cells based on HTML ta-ble tags.
Inconsistencies introduced by colspan androwspan attributes were resolved by replicating acell?s contents across its spanned lengths.
That is, acell with colspan=3 would be duplicated across thethree columns, and likewise for cells spanning mul-tiple rows.
Single-cell rows at the top or bottom of atable were assumed to be captions and discarded.The remaining HTML was stripped, save for thefollowing tags which contained important informa-tion:?
img tags were replaced by their alternate text,where available.
Such images often representa mathematical symbol, which is important in-formation to retain;?
hr tags proved to be an important indicator fordividing header cells from data cells.Tables were broken up into row headers, columnheaders, and data cells by making use of the hr tags,denoting horizontal lines, to detect column headers.Such tags tend to be present as a separator betweencolumn header cells and data cells; in fact, the onlytables in our collection that did not have the separa-tors did not have column headers either.
The hr tagswere subsequently stripped after this use.
Detectingrow headers was performed by checking if the topleft cell of the table was blank, a pattern which oc-curred in all row-major tables.
The vast majority oftables had column headers rather than row headers,although some had both and a small proportion hadonly row headers.
We acknowledge that this pro-cessing may be specific to the vagaries of the specificformat of the HTML generation used by PubMed(from which we sourced the tables).
However, ourwhole task is specific to this domain; further, our fo-cus is on the extraction task rather than the actualdetection of row/column headers.49Class Class Freq.
Cell Freq.Gene 64 1,618Exon 48 1,004Codon 23 435Mutation 90 2,174Statistic 482 8,788Other 576 14,324Total 1,283 28,343Table 1: Frequency per class and number of cells in thecollection.3.3 Task DesignIn order to extract mutation information fromtables, we first performed classification of fullcolumns/rows into relevant entities.
The content of acolumn (or row, depending on whether the table wasrow- or column-oriented) tends to be homogeneous;this allowed us to build classifiers that can identifyfull vectors of relevant entities in a single step.
Werefer to this task as table vector classification.We identified the following classes as relevant:Gene, Exon, Mutation, Codon, and Statistic.
Thefirst four were chosen directly from the MMRDatabase.
We decided to include ?Statistic?
after in-specting the tabular dataset, since we found that thisprovides relevant information about the importanceof a given mutation.
Of the five classes, Mutationis the most informative for our final information ex-traction goal.The next step was to hand-annotate the headersof the 197 tables in our collection by using the fiveclasses and the class ?Other?
as the tagset.
Someheaders belonged to more that one class, since theclasses were collapsed into a single field of the ta-ble.
The frequency per class and the number of cells,across the collection of tables, is shown in Table 1.3.4 EvaluationWe evaluated our systems in two ways:?
Header classification: performance of differentsystems on predicting the classes of each col-umn/row of the tables;?
Mutation extraction: recall of our system overthe subset of the hand-curated MMR database.Evaluation for the header classification step wasperformed using precision, recall and f-score, micro-averaged amongst the classes.
Micro-averaging in-volves multiplying the score of a class by the numberof instances of the class in the gold standard, and di-viding by the total number of instances.
For the ma-chine learning algorithms, evaluation was performedusing 10-fold cross-validation.
For mutation extrac-tion we focus on a single class, and produce recalland a lower-bound on precision.4 Mutation Mentions in TablesIn order to determine the value of processing tab-ular data for mutation-mining purposes, we ob-tained a sample of 100 documents that were hand-annotated by curators prior to their introduction inthe database?the curators highlighted relevant mu-tations found in each paper.
We found that for 59of the documents, only the tabular parts of the paperwere selected; 33 of the documents had only textualparts highlighted; and for 8 documents both tablesand text were selected.
This is an indicator of theimportance of tabular data in this context.Our next step was to measure the amount of in-formation that we could potentially extract from thetables in our collection.
Since we are interested inmutations, we extracted all cells from the vectorsthat were manually annotated as ?Mutation?
in or-der to compare them to the goldstandard, and mea-sure the recall.
This comparison was not straight-forward, because mutation mentions have differentnomenclatures.
Ideally we would normalise the dif-ferent references into a standard form, and then per-form the comparison.
However, normalisation is acomplex process in itself, and we resorted to evalu-ation by hand at this point.We found that 198 of the 717 goldstandard muta-tions were present in tables (28%).
This is a signif-icant amount, taking into account that their extrac-tion should be much easier than parsing the raw text.We also tested MutationFinder over the full text, andfound that only 6 of the goldstandard mutations wereretrieved (0.8%), which indicates that point mutationidentification is not sufficient for this task.Finally, we measured the amount of informationthat could be extracted by a simple string look-upsystem separately over the tabular and textual parts50of the articles.
We were looking for mutation men-tions that correspond exactly to the goldstandardrecord from each article, which meant that mentionsin different nomenclatures would be missed.
Wefound that a total of 177 mentions (24.7%) could befound with the same spelling; of those 142 (80.1%)were found in tables only, and the remaining 35(20.9%) were found in both tables and text; i.e., nomention was found in text only.These results indicate that we can find relevant in-formation in tables that is not easy to detect in run-ning text.5 Table Vector ClassificationWe built automatic classifiers to detect relevant en-tities in tables.
Two separate approaches were at-tempted for vector classification: applying heuristicrules, and machine learning (ML) techniques.
Theseare described here, along with an analysis of theirperformance.5.1 Heuristic BaselineAs a baseline method, we approached the task ofclassifying headers by matching the header string tothe names of the classes in a case-insensitive man-ner.
When the class name was found as a substringof the header, the class would be assigned to it.
Forexample, a header string such as ?Target Mutation?would be assigned the class ?Mutation?.
Some head-ers had multiple annotations (E.g.
?Gene/Exon?
).For better recall, we also matched synonyms forthe class ?Mutation?
(the terms ?Variation?
and?Missense?)
and the class ?Statistic?
(the terms?No.
?, ?Number?
and ?%?).
For the remainingclasses we did not identify other obvious synonyms.The results are shown in Table 2.
Precisionwas reasonably high for the ?Codon?, ?Exon?
and?Statistic?
classes.
However, this was not the casefor ?Mutation?, and this illustrates that differenttypes of information are provided under this head-ing; illustrative examples include the heading ?Mu-tation Detected?
on a ?Gene?
vector, or the heading?Germline Mutation?
referring to ?Statistics?.
Therecall was also low for ?Mutation?
and most otherclasses, showing that more sophisticated approachesare required in order to exploit the information con-tained in the tables.
Notice also that the micro-Class Precision Recall FScoreGene 0.537 0.620 0.575Exon 0.762 0.615 0.681Codon 0.850 0.654 0.739Mutation 0.283 0.301 0.292Statistic 0.911 0.324 0.478Other 0.581 0.903 0.707Micro Avg.
0.693 0.614 0.651Table 2: Naive Baseline results across the differentclasses and micro-averagedClass Precision Recall FScoreGene 0.537 0.611 0.571Exon 0.762 0.615 0.681Codon 0.850 0.654 0.739Mutation 0.600 0.452 0.515Statistic 0.911 0.340 0.495Other 0.579 0.910 0.708Micro Avg.
0.715 0.633 0.672Table 3: Results integrating MutationFinder across thedifferent classes and micro-averagedaverage is highly biased by the classes ?Statistic?and ?Others?, since they contain most of the test in-stances.Our second step was to build a more informedclassifier for the class ?Mutation?
using the pointmutation NER system MutationFinder (Caporaso etal., 2007).
We applied this tool to the text in thetable-cells, and identified which table-vectors con-tained at least one mutation mention.
These vectorswere also classified as mutations.
The results areshown in Table 3.
This approach caused the ?Muta-tion?
results to improve, but the overall f-score val-ues are still in the range 50%-70%.We considered other heuristic rules that couldbe applied, such as looking for different kinds ofpatterns for each class: for instance, numbers for?Exon?, or the normalised form c.N[symbol]N formutation, or trying to match against term lists (e.g.using Gene dictionaries).
Future work will exploreextending the ML approach below with featuressuch as these.515.2 Classification TechniquesFor the ML experiments we used the Weka (Wittenand Frank, 2005) toolkit, as it contains a wide se-lection of in-built algorithms.
We selected a varietyof well-known approaches in order to obtain a betterpicture of the overall performance.
As a baseline, weapplied the majority class from the training data toall test instances.
We applied the following ML sys-tems:8 Naive Bayes (NB); Support Vector Machines(SVM); Propositional Rule Learner (JRip); and De-cision Trees (J48).
We did not tune the parameters,and relied on the default settings.In order to define our feature sets, we used thetext in the headers and cells of the tables, withouttokenisation.
Other possible sources of information,such as captions or the running text referring to thetable were not employed at this stage.
We appliedfour feature sets:?
Basic (Basic): Four basic features, consistingof the header string, the average and mediancell lengths, and a binary feature indicatingwhether the data in the cells was numeric.?
Cell Bag-of-Words (C bow): Bag of wordsover the tokens in the table cells.?
Header Bag-of-Words (H bow): Bag ofwords over the tokens in the header strings.?
Header + Cell Bag-of-Words (HC bow):Combination of bags of words formed by thetokens in headers and cells, represented as sep-arate types of features.The micro-averaged results of the different learn-ing methods and feature sets are shown in Table 4.Regarding the feature sets, we can see that the bestperformance is obtained by using the headers as bag-of-words, while the content of the cells seems to betoo sparse to guide the learning methods.
SVM isthe best algorithm for this dataset, with JRip and J48following, and NB performing worst of the four inmost cases.Overall, the results show that the ML approachis superior to the baselines when using the headerbag of words feature to classify the relevant entities.8We applied a number of other ML algorithms as well, butthese showed significantly lesser performance.Method Feature SetsBasic C bow H bow HC bowMj.
Cl.
0.288NB 0.614 0.454 0.678 0.581SVM 0.717 0.599 0.839 0.816JRip 0.564 0.493 0.790 0.749J48 0.288 0.532 0.793 0.782Table 4: Results for ML Algorithms - Micro-AveragedFScores.
Mj.Cl.
: Majority Class.
The best results percolumn are given in bold.Class Precision Recall FScoreGene 0.778 0.737 0.757Exon 0.786 0.707 0.745Codon 0.833 0.882 0.857Mutation 0.656 0.679 0.667Statistic 0.919 0.853 0.885Other 0.82 0.884 0.850Micro Avg 0.839 0.841 0.839Table 5: Results for SVM and the feature set H bow perclass and micro-averaged.SVM is able to reach a high f-score of 83.9%, whichhas been found to be significantly better than the bestbaseline after applying a paired t-test (p-value under0.0001).We break down the results per class in Table 5,using the outputs from SVM and feature-set H bow.We can see that all classes show an improvementover the heuristic baselines.
There is a big increasefor the classes ?Gene?
and ?Statistic?, and all classesexcept mutation are above 70% f-score.
?Muta-tion?
is the most difficult class to predict, but itstill reaches 66.7% f-score, which can be helpful forsome tasks, as we explore in the next section.6 Automatic Mutation ExtractionWe applied the results of our classifier to a practi-cal application, i.e., the detection of mutations inthe literature for the MMR Database project.
Ta-ble vector classification allows us to extract lists ofcandidate mutation names from tables to be addedto the database.
We would like a system with highrecall that identifies all relevant candidates, but alsoacceptable precision so that not all the tables need to52System Mut.
Found TP % in MMR Rec.Automatic 1,702 153 9.0 77.3Gold standard 1,847 198 10.7 100Table 6: Results for Mutation detection.
TP indicates thenumber of true positives, ?% in MMR?
shows the per-centage of positives found in the database.be hand-checked.In order to test the viability of this approach, wemeasured the results of the system in detecting theexisting hand-curated mutations in MMR.
We cal-culated the recall in retrieving those mutations, andalso the rate of false positives; however, note thatwe also consider as false positives those valid muta-tions that were not relevant for MMR, and thereforethe reported precision is artificially low.Results for the automatic extraction and the gold-standard annotation are given in Table 6.
As ex-pected, there is a high rate of false positives in thegoldstandard and automatic systems; this shows thatmost of the mutations detected are not relevant forthe MMR database.
More interestingly, we wereable to retrieve 77.3% of relevant mutation mentionsautomatically using the ML approach, which corre-sponds to 21.3% of all the hand-curated data.The vector classifier discriminates 1,702 mutationcells out of a total of 27,700 unique cells in the tablecollection, and it effectively identifies 153 out of the198 relevant mutations present in the tabular data.This means that we only need to hand-check 6.1%of the tabular content to retrieve 77.3% of relevantmutations, saving the curators a significant amountof time.
The classifiers could also be biased towardshigher recall by parameter tuning?this is an area forfurther investigation.Finally, after the evaluation process we observedthat many false mutation candidates could be re-moved by discarding those that do not contain twoconsecutive digits or any of the following n-grams:?c.
?, ?p.
?, ?>?, ?del?, ?ins?, ?dup?.
This heuristic re-duces the number of mutation candidates from 1,702to 989 with no cost in recall.7 DiscussionWhile this is early work, our preliminary results onthe task of identifying relevant entities from genemutation literature show that targeting tables can bea fruitful approach for text mining.
By relying onML methods and simple bag-of-words features, wewere able to achieve good performance over a num-ber of selected entities, well above header word-matching baselines.
This allowed us to identify listsof mentions of relevant entities with minimal effort.An advantage of our approach is that the annotationof examples for training and evaluation is consider-ably easier, since many entities can be annotated ina single step, opening the way to faster annotation ofother entities of interest in the biomedical domain.The approach of using table vector classificationfor the named entity task also has promise.
In partic-ular, the wide variety and non-standard terminologyof biomedical entities (i.e.
genes, proteins, muta-tions) is one of the challenges to NER in this do-main.
However, since a column of homogeneousinformation may include representatives of the het-erogeneous nomenclature schemes, classification ofa whole column or row potentially helps nullify theeffect of the terminological variability.For future work, we plan to study different typesof features for better representing the entities tar-geted in this work.
Specially for mutation mentions,we observed that the presence of certain ngrams (e.g.?del?)
can be a strong indicator for this class.
An-other issue we plan to address is that of the normal-isation of mutation mentions into a standard form,for which we have started developing a collectionof regular expressions.
Another of our goals is toincrease the size of our dataset of articles by im-proving our web crawler, and by hand-annotatingthe retrieved table vectors for further experimenta-tion.
Finally, we also aim to explore the potential ofusing tabular data for NER of different entities in thebiomedical domain, such as gene mentions.Acknowledgements NICTA is funded by the Aus-tralian Government as represented by the Depart-ment of Broadband, Communications and the Dig-ital Economy and the Australian Research Councilthrough the ICT Centre of Excellence program.Thanks to Mike Woods and his colleagues at theMemorial University of Newfoundland for makingthe MMR database and their curation data availableto us.
Eric Huang wrote several of the scripts men-tioned in Section 3 for creating the table collection.53ReferencesC.
J. O. Baker and R. Witte.
2006.
Mutation mining?aprospector?s tale.
J. of Information Systems Frontiers,8(1):45?57.J.
G. Caporaso, W. A. Baumgartner Jr., D. A. Randolph,K.
B. Cohen, and L. Hunter.
2007.
Mutationfinder: Ahigh-performance system for extracting point mutationmentions from text.
Bioinformatics, 23(14):1862?1865.W.
W. Cohen, M. Hurst, and L. S. Jensen.
2002.
A flex-ible learning system for wrapping tables and lists inhtml documents.
In WWW ?02: Proc.
11th Int?l Conf.on World Wide Web, pages 232?241, Honolulu.F.
Horn, A. L. Lau, and F. E. Cohen.
2004.
Auto-mated extraction of mutation data from the literature:Application of MuteXt to g protein-coupled recep-tors and nuclear hormone receptors.
Bioinformatics,20(4):557?568.M.
Hurst.
2001.
Layout and language: Challengesfor table understanding on the web.
Technical report,WhizBang!Labs.N.
Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-chos, C. Gasperin, R. Drysdale, and T. Briscoe.
2008.Natural language processing in aid of flybase curators.BMC Bioinformatics, 9:193?204.Lawrence C. Lee, Florence Horn, and Fred E. Cohen.2007.
Automatic extraction of protein point muta-tions using a graph bigram association.
PLoS Com-putational Biology, 3(2):e16+, February.K.
Lerman, L. Getoor, S. Minton, and C. Knoblock.2004.
Using the structure of web sites for automaticsegmentation of tables.
In SIGMOD?04, pages 119?130, Paris.D.
Pinto, A. McCallum, X. Wei, and W. B. Croft.
2003.Table extraction using conditional random fields.
InSIGIR ?03, pages 235?242.X.
Wei, W.B.
Croft, and D. Pinto.
2004.
Questionanswering performance on table data.
Proceedingsof National Conference on Digital Government Re-search.I.
H. Witten and E. Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.M.O.
Woods, P. Williams, A. Careen, L. Edwards,S.
Bartlett, J. McLaughlin, and H. B. Younghusband.2007.
A new variant database for mismatch repairgenes associated with lynch syndrome.
Hum.
Mut.,28:669?673.R.
Zanibbi, D. Bolstein, and J. R. Cordy.
2004.
A surveyof table recognition.
Int?l J. on Document Analysisand Recognition, 7(1).54
