Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Speech Understanding Frameworkthat Uses Multiple Language Models and Multiple Understanding Models?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno?Graduate School of Informatics, Kyoto UniversityYoshida-Hommachi, Sakyo, Kyoto606-8501, Japan{katumaru, komatani}@kuis.kyoto-u.ac.jp{ogata, okuno}@kuis.kyoto-u.ac.jp?Honda Research Institute Japan Co., Ltd.8-1 Honcho, Wako, Saitama351-0188, Japan{nakano, funakoshi}@jp.honda-ri.comAbstractThe optimal combination of language model(LM) and language understanding model(LUM) varies depending on available trainingdata and utterances to be handled.
Usually, alot of effort and time are needed to find the op-timal combination.
Instead, we have designedand developed a new framework that usesmultiple LMs and LUMs to improve speechunderstanding accuracy under various situa-tions.
As one implementation of the frame-work, we have developed a method for select-ing the most appropriate speech understand-ing result from several candidates.
We usetwo LMs and three LUMs, and thus obtain sixcombinations of them.
We empirically showthat our method improves speech understand-ing accuracy.
The performance of the oracleselection suggests further potential improve-ments in our system.1 IntroductionThe speech understanding component in a spokendialogue system consists of an automatic speechrecognition (ASR) component and a language un-derstanding (LU) component.
To develop a speechunderstanding component, we need to prepare anASR language model (LM) and a language under-standing model (LUM) for the dialogue domainof the system.
There are many types of LMssuch as finite-state grammars and N-grams, andmany types of LUMs such as finite-state transduc-ers (FST), weighted finite-state transducers (WFST),and keyphrase-extractors (extractor).
Selecting asuitable combination of LM and LUM is necessaryfor robust speech understanding against various userutterances.Conventional studies of speech understandinghave investigated which LM and LUM give the bestperformance by using fixed training and test datasuch as the Air Travel Information System (ATIS)corpus.
However, in real system development, re-sources such as training data for statistical modelsand efforts to write finite-state grammars vary ac-cording to the available human resources or budgets.Domain-dependent training data are particularly dif-ficult to obtain.
Therefore, in conventional systemdevelopment, system developers determine the typesof LM and LUM by trial and error.
Every LM andLUM has some advantages and disadvantages, so itis difficult for a single combination of LM and LUMto gain high accuracy except in a situation involv-ing a lot of training data and effort.
Therefore, usingmultiple speech understanding methods is a more ef-fective approach.In this paper, we propose a speech understand-ing framework called ?Multiple Language modelsand Multiple Understanding models (MLMU)?, inwhich multiple LMs and LUMs are used, to achievebetter performance under the various developmentsituations.
It selects the best speech understandingresult from the multiple results generated by arbi-trary combinations of LMs and LUMs.So far there have been several attempts to im-prove ASR and speech understanding using mul-tiple speech recognizers and speech understandingmodules.
ROVER (Fiscus, 1997) tried to improveASR accuracy by integrating the outputs of multi-ple ASRs with different acoustic and language mod-133LM 1utterance LM 2LM nresultconfidenceintegrationcomponentLUcomponent speechunderstandingresultsASRcomponentLUM n LUM 2LUM 1LM: Language ModelLUM: Language Understanding ModelFigure 1: Flow of speech understanding in MLMUels.
The work is different from our study in the fol-lowing two points: it does not deal with speech un-derstanding, and it assumes that each ASR is well-developed and achieves high accuracy for a varietyof speech inputs.
Eckert et al (1996) used multipleLMs to deal with both in-grammar utterances andout-of-grammar utterances, but did not mention lan-guage understanding.
Hahn et al (2008) used mul-tiple LUMs, but just a single language model.2 Speech Understanding FrameworkMLMUMLMU is a framework by which system developerscan use multiple speech understanding methods bypreparing multiple LMs and multiple LUMs.
Fig-ure 1 illustrates the flow of speech understanding inMLMU.
System developers list available LMs andLUMs for each system?s domain, and the systemunderstands utterances by using these models.
Theframework selects one understanding result frommultiple results or calculates a confidence score ofthe result by using the generated multiple under-standing results.MLMU can improve speech understanding for thefollowing reason.
The performance of each speechunderstanding (a combination of LM and LUM)might not be very high when either training data forthe statistical model or available expertise and ef-fort for writing grammar are insufficient.
In suchcases, some utterances might not be covered by thesystem?s finite-state grammar LM, and probabilityestimation in the statistical models may not be verygood.
Using multiple speech understanding mod-els is expected to solve this problem because eachmodel has different specialities.
For example, finite-state grammar LMs and FST-based LUMs achievehigh accuracy in recognizing and understanding in-grammar utterances, whereas out-of-grammar utter-ances are covered by N-gram models and LUMsbased on WFST and keyphrase-extractors.
There-fore it is more possible that the understanding resultsof MLMU will include the correct result than a casewhen a single understanding model is used.The understanding results of MLMU will be help-ful in many ways.
We used them to achieve betterunderstanding accuracy by selecting the most reli-able one.
This selection is based on features con-cerning ASR results and language understanding re-sults.
It is also possible to delay the selection, hold-ing multiple understanding result candidates thatwill be disambiguated as the dialogue proceeds (Bo-hus, 2004).
Furthermore, confidence scores, whichenable an efficient dialogue management (Komataniand Kawahara, 2000), can be calculated by rankingthese results or by voting on them, by using multi-ple speech understanding results.
The understandingresults can be used in the discourse understandingmodule and the dialogue management module.
Theycan choose one of the understanding results depend-ing on the dialogue situation.3 Implementation3.1 Available Language Models and LanguageUnderstanding ModelsWe implemented MLMU as a library of RIME-TK, which is a toolkit for building multi-domainspoken dialogue systems (Nakano et al, 2008).With the current implementation, developers can usethe following LMs:1.
A LM based on finite-state grammar (FSG)2.
A domain-dependent statistical N-gram model(N-gram)and the following LUMs:1.
Finite-state transducer (FST)2.
Weighted FST (WFST)3.
Keyphrase-extractor (extractor).System developers can use multiple finite-state-grammar-based LMs or N-gram-based LMs, and134also multiple FSTs and WFSTs.
They can specifythe combination for each domain by preparing LMsand LUMs.
They can specify grammar models whensufficient human labor is available for writing gram-mar, and specify statistical models when a corpus fortraining models is available.3.2 Selecting Understanding Result based onASR and LU FeaturesWe also implemented a mechanism for selecting oneof the understanding results as the best hypothesis.The mechanism chooses the result with the highestestimated probability of correctness.
Probabilitiesare estimated for each understanding result by usinglogistic regression, which uses several ASR and LUfeatures.We define Pi as the probability that speech under-standing result i is correct, and we select one resultbased on argmaxiPi.
We denote each speech un-derstanding result as i (i = 1,. .
.
,6).
We constructeda logistic regression model for Pi.
The regressionfunction can be written as:Pi = 11 + exp(?
(ai1Fi1 + .
.
.
+ aimFim + bi)) .
(1)The coefficients ai1, .
.
.
, aim, bi were fitted us-ing training data.
The independent variablesFi1, Fi2, ..., Fim are listed in Table 1.
In the table,n indicates the number of understanding results, thatis, n = 6 in this paper?s experiment.
Here, we denotethe features as Fi1, Fi2, ..., Fim.Features from Fi1 to Fi3 represent characteristicsof ASR results.
The acoustic scores were normal-ized by utterance durations in seconds.
These fea-tures are used for verifying its ASR result.
Featuresfrom Fi4 to Fi9 represent characteristics of LU re-sults.
Features from Fi4 to Fi6 are defined on thebasis of the concept-based confidence scores (Ko-matani and Kawahara, 2000).4 Preliminary ExperimentWe conducted a preliminary experiment to show thepotential of the framework by using the two LMsand three LUMs noted in Section 3.1.Table 1: Features from speech understanding result iFi1: acoustic score of ASRFi2: difference between Fi1 and acoustic scoreof ASR for utterance verificationFi3: utterance duration [sec.
]Fi4: average confidence scores for concepts in iFi5: average of Fi4 ( 1n?ni Fi4)Fi6: proportion of Fi4 (Fi4 /?ni Fi5)Fi7: average # concepts ( 1n?ni #concepti)Fi8: max.
# concepts (max (#concepti) )Fi9: min.
# concepts (min (#concepti) )4.1 Preparing LMs and LUMsThe finite-state grammar rules were written in sen-tence units manually.
A domain-dependent statisti-cal N-gram model was trained on 10,000 sentencesrandomly generated from the grammar.
The vocab-ulary sizes of the grammar LM and the domain-dependent statistical LM were both 278.
Wealso used a domain-independent statistical N-grammodel for obtaining acoustic scores for utteranceverification, which was trained onWeb texts (Kawa-hara et al, 2004).
Its vocabulary size was 60,250.The grammar used in the FST was the same as theFSG used as one of the LMs, which was manuallywritten by a system developer.
The WFST-based LUwas based on a method to estimate WFST parame-ters with a small amount of data (Fukubayashi et al,2008).
Its parameters were estimated by using 105utterances of just one user.
The keyphrase extrac-tor extracts as many concepts as possible from anASR result on the basis of a grammar while ignor-ing words that do not match the grammar.4.2 Target Data for EvaluationWe used 3,055 utterances in the rent-a-car reserva-tion domain (Nakano et al, 2007).
We used Julius(ver.
4.0.2) as the speech recognizer and a 3000-state phonetic tied-mixture (PTM) triphone modelas the acoustic model1.
ASR accuracy in mora ac-curacy when using the FSG and the N-gram modelwere 71.9% and 75.5% respectively.
We used con-cept error rates (CERs) to represent the speech un-derstanding accuracy, which is calculated as fol-1http://julius.sourceforge.jp/135Table 2: CERs [%] for each speech understandingmethodspeech understanding method(LM + LUM) CER(1) FSG + FST 26.9(2) FSG + WFST 29.9(3) FSG + extractor 27.1(4) N-gram + FST 35.2(5) N-gram + WFST 25.3(6) N-gram + extractor 26.0selection from (1) through (6) (our method) 22.7oracle selection 13.5lows:CER = # error concepts#concepts in utterances .
(2)We manually annotated whether an understandingresult of each utterance was correct or not, andused them as training data to fit the coefficientsai1, .
.
.
, aim, bi.4.3 Evaluation in Concept Error RatesWe fitted the coefficients of regression functions andselected understanding results with a 10-fold crossvalidation.
Table 2 lists the CERs based on combi-nations of single LM and LUM and by our method.Of all combinations of single LM and LUM, the bestaccuracy was obtained with (5) (N-gram + WFST).Our method improved by 2.6 points over (5).
Al-though we achieved a lower CER, we used a lotof data to estimate logistic regression coefficients.Such a large amount of data may not be available in areal situation.
We will conduct more experiments bychanging the amount of training data.
Table 2 alsoshows the accuracy of the oracle selection, whichselected the best speech understanding result man-ually.
The CER of the oracle selection was 13.5%,a significant improvement compared to all combina-tions of a LM and LUM.
There is no combination ofa LM and LUM whose understanding results werenot selected at all in the oracle selection and ourmethod?s selection.
These results show that usingmultiple LMs and multiple LUMs can potentiallyimprove speech understanding accuracy.5 Ongoing workWe will conduct more experiments in other domainsor with other resources to evaluate the effectivenessof our framework.
We plan to investigate the casein which a smaller amount of the training data isused to estimate the coefficients of the logistic re-gressions.
Furthermore, finding a way to calculateconfidence scores of speech understanding results ison our agenda.ReferencesDan Bohus.
2004.
Error awareness and recovery intask-oriented spoken dialogue systems.
Ph.D. thesis,Carnegie Mellon University.Wieland Eckert, Florian Gallwitz, and Heinrich Nie-mann.
1996.
Combining stochastic and linguistic lan-guage models for recognition of spontaneous speech.In Proc.
ICASSP, pages 423?426.Jonathan G. Fiscus.
1997.
A post-processing systemto yield reduced word error rates: Recognizer Out-put Voting Error Reduction (ROVER).
In Proc.
ASRU,pages 347?354.Yuichiro Fukubayashi, Kazunori Komatani, MikioNakano, Kotaro Funakoshi, Hiroshi Tsujino, TetsuyaOgata, and Hiroshi G. Okuno.
2008.
Rapid prototyp-ing of robust language understanding modules for spo-ken dialogue systems.
In Proc.
IJCNLP, pages 210?216.Stefan Hahn, Patrick Lehnen, and Hermann Ney.
2008.System combination for spoken language understand-ing.
In Proc.
Interspeech, pages 236?239.Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-sunobu Itou, and Kiyohiro Shikano.
2004.
Recentprogress of open-source LVCSR Engine Julius andJapanese model repository.
In Proc.
ICSLP, pages3069?3072.Kazunori Komatani and Tatsuya Kawahara.
2000.Flexible mixed-initiative dialogue management usingconcept-level confidence measures of speech recog-nizer output.
In Proc.
COLING, volume 1, pages 467?473.Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-jino.
2007.
Analysis of user reactions to turn-takingfailures in spoken dialogue systems.
In Proc.
SIGdial,pages 120?123.Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, andHiroshi Tsujino.
2008.
A framework for building con-versational agents based on a multi-expert model.
InProc.
SIGdial, pages 88?91.136
