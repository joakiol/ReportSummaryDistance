Sense Discrimination with Parallel CorporaNancy IdeDept.
of Computer ScienceVassar CollegePoughkeepsie,New York 12604-0520USAide@cs.vassar.eduTomaz ErjavecDept.
of Intelligent SystemsInstitute "Jozef Stefan"Jamova 39,SI-1000 LjubljanaSLOVENIAtomaz.erjavec@ijs.siDan TufisRACAIRomanian AcademyCasa Academiei,Calea 13 Septembrie 13,Bucharest 74311, ROMANIAtufis@racai.roAbstractThis paper describes an experiment thatuses translation equivalents derived fromparallel corpora to determine sensedistinctions that can be used for automaticsense-tagging and other disambiguationtasks.
Our results show that sensedistinctions derived from cross-lingualinformation are at least as reliable as thosemade by human annotators.
Because ourapproach is fully automated through all itssteps, it could provide means to obtainlarge samples of ?sense-tagged?
datawithout the high cost of humanannotation.1 IntroductionIt is well known that the most nagging issue forword sense disambiguation (WSD) is the definitionof just what a word sense is.
At its base, theproblem is a philosophical and linguistic one that isfar from being resolved.
However, work inautomated language processing has led to efforts tofind practical means to distinguish word senses, atleast to the degree that they are useful for naturallanguage processing tasks such as summarization,document retrieval, and machine translation.Resnik and Yarowsky (1997) suggest that for thepurposes of WSD, the different senses of a wordcould be determined by considering only sensedistinctions that are lexicalized cross-linguistically.In particular, they propose that some set of targetlanguages be identified, and that the sensedistinctions to be considered for languageprocessing applications and evaluation be restrictedto those that are realized lexically in someminimum subset of those languages.
This ideawould seem to provide an answer, at least in part,to the problem of determining different senses of aword: intuitively, one assumes that if anotherlanguage lexicalizes a word in two or more ways,there must be a conceptual motivation.
If we lookat enough languages, we would be likely to find thesignificant lexical differences that delimit differentsenses of a word.Several studies have used parallel texts for WSD(e.g., Gale et al, 1993; Dagan et al, 1991; Daganand Itai, 1994) as well as to define semanticproperties of and relations among lexemes (Dyvik,1998).
More recently, two studies have examinedthe use of cross-lingual lexicalization as a criterionfor validating sense distinctions: Ide (1999) usedtranslation equivalents derived from alignedversions of Orwell?s Nineteen Eighty-Four amongfive languages from four different languagesfamilies, while Resnik and Yarowsky (2000) usedtranslations generated by native speakers presentedwith isolated sentences in English.
In both of thesestudies, translation information was used tovalidate sense distinctions provided in lexiconssuch as WordNet (Miller et al, 1990).
Althoughthe results are promising, especially for coarse-grained sense distinctions, they rest on theacceptance of a previously established set ofsenses.
Given the substantial divergences amongsense distinctions in dictionaries and lexicons,together with the ongoing debate within the WSDcommunity concerning which sense distinctions, ifany, are appropriate for language processingapplications, fitting cross-linguistic information topre-established sense inventories may not be theoptimal approach.July 2002, pp.
54-60.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word SenseThis paper builds on previously reported work (Ideet al, 2001) that uses translation equivalentsderived from a parallel corpus to determine sensedistinctions that can be used to automaticallysense-tag the data.
Our results show that sensedistinctions derived from cross-lingual informationare at least as reliable as those made by humanannotators.
Our approach therefore provides apromising means to automatically identify sensedistinctions.2 MethodologyWe conducted a study using parallel, alignedversions of George Orwell's Nineteen Eighty-Four(Erjavec and Ide, 1998) in seven languages:English, Romanian, Slovene, Czech, Bulgarian,Estonian, and Hungarian.
The study involveslanguages from four language families (Germanic,Romance, Slavic, and Finno-Ugric),  threelanguages from the same family (Czech, Sloveneand Bulgarian), as well as two  non-Indo-Europeanlanguages (Estonian and Hungarian).
AlthoughNineteen Eighty-Four, (ca.
100,000 words),  is awork of fiction, Orwell's prose is not highlystylized and, as such, it provides a reasonablesample of modern, ordinary language that is nottied to a given topic or sub-domain (which is thecase for newspapers, technical reports, etc.
).Furthermore, the translations of the text seem to berelatively faithful to the original: over 95% of thesentence alignments in the full parallel corpus ofseven languages are one-to-one (Priest-Dorman, etal., 1997).2.1 Preliminary ExperimentWe constructed a multilingual lexicon based on theOrwell corpus, using a method outlined in Tufisand Barbu (2001, 2002).
The complete EnglishOrwell contains 7,069 different lemmas, while thecomputed lexicon comprises 1,233 entries, out ofwhich 845 have (possibly multiple) translationequivalents in all languages.
We then conducted apreliminary study using a subset of 33 nounscovering a range of frequencies and degrees ofambiguity (Ide, et al, 2001).For each noun in the sample, we extracted allsentences from the English Nineteen Eighty-Fourcontaining the lemma in question, together with theparallel sentences from each of the six translations.The aligned sentences were automatically scannedto extract translation equivalents.1A vector wasthen created for each occurrence, representing allpossible lexical translations in the six parallelversions: if a given word is used to translate thatoccurrence, the vector contains a 1 in thecorresponding position, 0 otherwise.
The vectorsfor each ambiguous word were fed to anagglomerative clustering algorithm (Stolcke,1996), where the resulting clusters are taken torepresent different senses and sub-senses of theword in question.The clusters produced by the algorithm werecompared with sense assignments made by twohuman annotators on the basis of WordNet 1.6.2Inorder to compare the algorithm results with theannotators?
sense assignments, we normalized thedata as follows: for each annotator and thealgorithm, each of the 33 words was represented asa vector of length n(n-1)/2, where n is the numberof occurrences of the word in the corpus.
Thepositions in the vector represent a ?yes-no?assignment for each pair of occurrences, indicatingwhether or not they were judged to have the samesense (the same WordNet sense for the annotators,and the same cluster for the algorithm).Representing the clustering algorithm results in thisform required some means to ?flatten?
the clusterhierarchies, which typically extend to 5 or 6 levels,to conform more closely to the completely flatWordNet-based data.
Therefore, clusters with aminimum distance value (as assigned by theclustering algorithm) at or below 1.7 werecombined, and each leaf of the resulting collapsedtree was treated as a different sense.
This yielded aset of sense distinctions for each word roughlysimilar in number to those assigned by theannotators.3The cluster output for glass  in Figure 1 is anexample of the results obtained from the clusteringalgorithm.
For clarity, the occurrences have beenmanually labeled with WordNet 1.6 senses (Figure2).
The tree shows that the algorithm correctly1Sentences in which more than one translation equivalentappears were eliminated (cca.
5% of the translations).2Originally, the annotators attempted to group occurrenceswithout reference to an externally defined sense set, but thisproved to be inordinately difficult and produced highlyvariable results and was eventually abandoned.3We used the number of senses annotators assigned ratherthan the number of WordNet senses as a guide to determinethe minimum distance cutoff, because many WordNet sensesare not represented in the corpus.grouped occurrences corresponding to WordNetsense 1 (a solid material) in one of the two mainbranches, and those corresponding to sense 2(drinking vessel) in the other.
The top group isfurther divided into two sub-clusters, the lower ofwhich refer to a looking glass and a magnifyingglass, respectively.
While this is a particularly clearexample of good results from the clusteringalgorithm, results for other words are, for the mostpart, similarly reasonable.Figure 1 : Output of the clustering algorithm1.
a brittle transparent solid withirregular atomic structure2.
a glass container for holding liquidswhile drinking3.
the quantity a glass will hold4.
a small refracting telescope5.
a mirror; usually a ladies' dressingmirror6.
glassware collectively; "She collectedold glass"Figure 2 : WordNet 1.6 senses for glass (noun)The results of the first experiment are summarizedin Table 1, which shows the percentage ofagreement between the cluster algorithm and eachannotator, between the two annotators, and for thealgorithm and both annotators taken together.4Thepercentages are similar to those reported in earlierwork; for example, Ng et al (1999) achieved a rawpercentage score of 58% agreement amongannotators tagging nouns with WordNet 1.6 senses.Cluster/Annotator 1 66.7%Cluster/Annotator 2 63.6%Annotator 1/Annotator 2 76.3%Cluster/Annotator 1/ Annotator 2 53.4%Table 1 : Levels of agreement2.2 Second experimentComparison of sense differentiation achieved usingtranslation equivalents, as determined by theclustering algorithm, with those assigned by humanannotators suggests that use of translationequivalents for word sense tagging anddisambiguation is worth pursuing.
Agreementlevels are comparable to (and in some cases higherthan) those obtained in earlier studies tagging withWordNet senses.
Furthermore, the pairwisedifference in agreement between the humanannotators and the annotators and the clusteringalgorithm is only 10-13%, which is also similar toscores obtained in other studies.In the second phase, the experiment was broadenedto include 76 nouns from the multi-lingual lexicon,including words with varying ambiguity (the rangein number of WordNet senses is 2 to 29, average7.09) and semantic characteristics (e.g., abstract vs.concrete: ?thought?, ?stuff?, ?meaning?, ?feeling?vs.
?hand?, ?boot?, ?glass?, ?girl?, etc.).
We chosenouns that occur a minimum of 10 times in thecorpus, have no undetermined translations and atleast five different translations in the six non-English languages, and have the log likelihoodscore of at least 18; that is:LL(TT, TS)  =?
?= =21ij21in*2j*j**i**ijn*nn*nlog?
18where nijstands for the number of times TTand TShave been seen together in aligned sentences, ni*and n*jstand for the number occurrences of TTandTS,respectively, and n**represents the total4We computed raw percentages only; common measures ofannotator agreement such as the Kappa statistic (Carletta,1996) proved to be inappropriate for our two-category (?yes-no?)
classification scheme._____|-> (1)|-----|     |-> (1)|     |_____|---> (1)|           |___|-> (1)|               |-> (1)|         |---> (1)|----|         |            _|-> (1)|    |         |         |-| |-> (1)|    |     |---|       |-| |-> (1)|    |     |   |     |-| |-> (1)|    |-----|   |   |-| |-> (1)|--|          |   |---| |-> (1)|  |          |       |-> (1)|  |          |___|---> (6)|  |              |___|-----> (1)|  |                  |-----> (1)|  |     _____|-----> (1)-|  |----|     |-----> (5)|       |-----> (4)|      |---> (2)|  |---|      _|-> (2)|  |   |   |-| |-> (2)|  |   |---| |-> (2)|--|       |-> (2)|   |-----> (2)|   |           ___|-----> (2)|---|     |----|   |-----> (2)|     |    |    _|-> (2)|     |    |---| |-> (2)|-----|        |-> (2)|     ____|-> (3)|----|    |-> (2)|     _|-> (2)|----| |-> (2)|-> (2)number of potential translation equivalents in theparallel corpus.
The LL score is set at a maximumvalue to ensure high precision for the extractedtranslation equivalents, which minimizes senseclustering errors due to incorrect word alignment.Table 2 summarizes the data.No.
of words 76No.
of example sentences 2399Average examples/word 32No.
of senses (annotator 1) 241No.
of senses (annotator 2) 280No.
of senses (annotator 3) 213No.
of senses (annotator 4) 232No.
of senses (all annotators) 345Average senses per word 4.53Percentage of annotator agreement:Full agreement (4/4) 54.2775% agreement (3/4) 28.1350% agreement (2/4) 16.92No agreement 0.66Table 2 : Summary of the dataIn this second experiment, we increased thenumber of annotators to four.
The results of theclustering algorithm and the sense assignmentsmade by the human annotators were normalizeddifferently than in the earlier experiment, byignoring sense numbers and interpreting theannotators?
sense assignments as clusters only.
Tosee why this was necessary, consider the followingset of sense assignments for the seven occurrencesof ?youth?
in Nineteen Eighty-Four:OCC 1 2 3 4 5 6 7Ann13 1 6 3 6 3 1Ann22 1 4 2 6 2 1Agreement is 43%; however, both annotatorsclassify occurrences 1, 4, and 6  as having the samesense, although each assigned a different sensenumber to the group.
If we ignore sense numbersand consider only the annotators?
?clusters?, theagreement rate is much higher,5and the data ismore comparable to that obtained from the clusteralgorithm.We also addressed the issue of the appropriatepoint at which to cut off the clustering by thealgorithm.
Our use of a pre-defined minimum5In fact, the only remaining disagreement is that Annotator 1assigns occurrences 3 and 5 together, whereas Annotator 2assigns a different sense to occurrence 3?in effect, Annotator2 makes a finer distinction than Annotator 1 betweenoccurrences 3 and 5.distance value to determine the number of clusters(senses) in the earlier experiment  yielded varyingresults for different words (especially words withsignificantly different numbers of translationequivalents) and we sought a more principledmeans to determine the cut-off value.
Theclustering algorithm was therefore modified tocompute the correct number of clustersautomatically by halting the clustering processwhen the number of clusters reached a valuesimilar to the average number obtained by theannotators.6As criteria, we used the minimumdistance between existing clusters at each iteration,which determines the two clusters to be joined,where minimum distance is computed between twovectors v1, v2length n as:?
(v1(i) - v2(i))2i=1n?Best results were obtained when the clustering wasstopped at the point where:(dist(k)-dist(k+1))/dist(k+1) < 0.12where dist(k) is the minimal distance between twoclusters at the kth iteration step.We defined a ?gold standard?
annotation by takingthe majority vote of the four annotators (in case ofties, the annotator closest to the majority vote inthe greatest number of cases was considered to beright).
Using this heuristic, the clustering algorithmassigned the same number of senses as the goldstandard for 41 words.
However, overall agreementwas much worse (67.9%) than when the number ofclusters was pre-specified.
The vast majority ofclustering errors occurred when sense distributionsare skewed; we therefore added a post-processingphase in which the smallest clusters are eliminatedand their members included in the largest clusterwhen the number of occurrences in the largestcluster is at least ten times that of any othercluster.7With this new heuristic, the algorithm produced thesame number of clusters as the gold standard foronly 15 words, but overall agreement reached74.6%.
Mismatching clusters typically included6In principle, the upper limit for the number of senses for aword is the number of senses in WordNet 1.6; however, therewas no case in which all WordNet senses appeared in the text.7The factor of 10 is a conservative threshold; additionalexperiments might yield evidence for a lower value.only one element.
There were only five words forwhich a difference in the  number of clustersassigned by the gold standard vs. the algorithmsignificantly contributed to the 2.7% depreciationin agreement.We also experimented with eliminating the data for?non-contributing?
languages  (i.e., languages forwhich there is only one translation for the targetword); this was ultimately abandoned because itworsened results by amplifying the effect ofsynonymous translations in other languages.Finally, we compared the use of weighted vs.unweighted clustering algorithms (see, e.g.,Yarowsky and Florian, 1999) and determined thatresults were improved using weighted clustering.The clusters produced by each pair of classifiers(human or machine) were mapped for maximumoverlap; differences were considered asdivergences.
The agreement between two differentclassifications was computed as the number ofcommon occurrences in the corresponding clustersof the two classifications divided by the totalnumber of the occurrences of the target word.
Forexample, the word movement occurs 40 times inthe corpus; both the ?gold standard?
and thealgorithm identified four clusters, but thedistribution of the 40 occurrences was substantiallydifferent, as summarized in Table 3.
Thirty-four ofthe 40 occurrences appear in the clusters commonto the two classifications; therefore, the agreementrate is 85%.CLUSTER 1 2 3 4Gold standard 28 6 3 3Algorithm 25 7 6 2Intersection 24 6 3 1Table 3 : Gold standard vs. algorithm clustering formovement2.3 ResultsThe results of our second experiment aresummarized in Table 4, which gives the agreementrate between baseline clustering (B), in which it isassumed all occurrences are labeled with the samesense; each pair of human annotators (1-4); thegold standard (G); and the clustering algorithm(A).
The table shows that agreement rates amongthe human annotators, as compared to thosebetween the algorithm and all but one annotator,are not significantly different, and that thealgorithm?s highest level of agreement is with thebaseline.
This is not surprising because of thesecond heuristic used.
However, the second bestagreement rate for the algorithm is with the goldstandard, which suggests that sense distinctionsdetermined using the algorithm are almost asreliable as sense distinctions determined manually.The agreement of the algorithm with the goldstandard falls slightly below that of the humanannotators, but is still well within the range ofacceptability.
Also, given that the gold standardwas computed on the basis of the humanannotations, it is understandable that theseannotations do better than the algorithm.1 2 3 4 G AB71.1 65.1 76.3 74.1 75.5 81.5178.1 75.6 83.1 88.6 74.4271.3 75.9 82.5 66.9377.3 82.1 77.1490.4 75.9G77.3Table 4 Agreement rates among baseline, the fourannotators, gold standard, and the algorithm3 Discussion and Further WorkOur results show that sense distinctions based ontranslation variants from parallel corpora aresimilar to those obtained from human annotators,which suggests several potential applications.Because our approach is fully automated throughall its steps, it could be used to automaticallyobtain large samples of ?sense-differentiated?
datawithout the high cost of human annotation.Although our method does not choose senseassignments from a pre-defined list, most languageprocessing applications (e.g.
information retrieval)do not require this knowledge; they need only theinformation that different occurrences of a givenword are used in the same or a different sense.A by-product of applying our method is that oncewords in a text in one language are tagged usingthis method, different senses of the correspondingtranslations in the parallel texts are also identified,potentially providing a source of information foruse in other language processing tasks and forbuilding resources in the parallel languages (e.g.,WordNets for the Eastern European languages inour study).
In addition, if different senses of targetwords are identified in parallel texts, contextualinformation for different senses of a word can begathered for use in disambiguating other, unrelatedtexts.
The greatest obstacle to application of thisapproach is, obviously, the lack of parallel corpora:existing freely available parallel corpora includingseveral languages are typically small (e.g., theOrwell), domain dependent (e.g.
the MULTEXTJournal of the Commission (JOC) corpus; Ide andV?ronis, 1994), and/or represent highly stylizedlanguage (e.g.
the Bible; Resnik et al, 1999).Appropriate parallel data including Asianlanguages  is virtually non-existent.
Given that ourmethod applies only to words for which differentsenses are lexicalized differently in at least oneother language, its broad application depends onthe future availability of large-scale parallelcorpora including a variety of language types.Many studies have pointed out that coarser-grainedsense distinctions can be assigned more reliably byhuman annotators than finer distinctions such asthose in WordNet.
In our study, the granularity ofthe sense distinctions was largely ignored, exceptinsofar as we attempted to cut off the number ofclusters produced by the algorithm at a valuesimilar to the number identified by the annotators.The sense distinctions derived from the clusteringalgorithm are hierarchical, often identifying four orfive levels of refinement, whereas the WordNetsense distinctions are organized as a flat list withno indication of their degree of relatedness.
Ourattempt to flatten the cluster data in fact loses muchinformation about the relatedness of senses.8As aresult, both annotators and the clustering algorithmare penalized as much for failing to distinguishfine-grained as coarse-grained distinctions.
We arecurrently exploring two possible sources ofinformation about sense relatedness: the output ofthe clustering algorithm itself, and WordNethypernyms, which may not only improve but alsobroaden the applicability of our method.8Interestingly, the clustering for ?glass?
in Figure 1 revealsadditional sub-groupings that are not distinguished inWordNet:  the top sub-group of the top cluster includesoccurrences that deal with some physical aspect of the material(?texture of?, ?surface of?, ?rainwatery?, ?soft?, etc.).
In thelower cluster, the two main sub-groups distinguish a (drinking)glass as a manipulatable object (by washing, holding, on ashelf, etc.)
from its sense as a vessel (mainly used as the objectof ?pour into?, ?fill?, ?take/pick up?, etc.
or modified by?empty?, ?of gin?, etc.
).We note in our data that although it is notstatistically significant, there is some correlation (-.51) between the number of WordNet senses for aword and overall agreement levels.
The lowestoverall agreement levels were for ?line?
(29senses), ?step?
(10), position (15), ?place?
(17),and ?corner?
(11).
Perfect agreement was achievedfor several words with under 5 senses, e.g., ?hair?
(5), ?morning?
(4), ?sister?
(4), ?tree?
(2), and?waist?
(2)?all of which were judged by both theannotators and the algorithm to occur in only onesense in the text.
On the other hand, agreementlevels for some words with under five WordNetsenses had low agreement: e.g., ?rubbish?
(2),?rhyme?
(2), ?destruction?
(3), and ?belief?
(3).Because both the algorithm (which baseddistinctions on translations) and the humanannotators (who used WordNet senses) had lowagreement in these cases, the WordNet sensedistinctions may be overly fine-grained and,possibly, irrelevant to many language processingtasks.We continue to explore the viability of our methodto automatically determine sense distinctionscomparable to those achieved by humanannotators.
We are currently exploring methods torefine the clustering results as well as theircomparison to results obtained from humanannotators (e.g., the Gini Index  [Boley, et al,1999]).4 ConclusionThe results reported here represent a first step indetermining the degree to which automatedclustering based on translation equivalents can beused to differentiate word senses.
Our work so farindicates that the method is promising and couldprovide a significant means to automaticallyacquire sense-differentiated data in multiplelanguages.
Our current results suggest that coarse-grained agreement is the best that can be expectedfrom humans, and that our method is capable ofduplicating sense differentiation at this level.5 AcknowledgementsOur thanks go to Arianna Schlegel, ChristinePerpetua, and Lindsay Schulz who annotated thedata, and to Ion Radu who modified the clusteringalgorithm.
We would also like to thank theanonymous reviewers for their comments andsuggestions.
All errors, of course, remain our own.6 ReferencesBoley D., Gini, M, Gross, R., Han, S.,.Hastings, K and Karypis, G., Kumar, V.,Mobasher, B, Moore, J.
(1999) Partitioning-BasedClustering for Web Document Categorization.Decision Support Systems, 27:3, 329-341.Carletta, J.
(1996).
Assessing Agreement onClassification Tasks: The Kappa Statistic.Computational Linguistics, 22:2, 249-254.Dagan, I. and Itai, A.
(1994).
Word sensedisambiguation using a second languagemonolingual corpus.
Computational Linguistics,20:4, 563-596.Dagan, I., Itai, A., and Schwall, U.
(1991).
Twolanguages are more informative than one.Proceedings of the 29th Annual Meeting of theACL, 18-21 Berkeley, California, 130-137.Dyvik, H. (1998).
Translations as SemanticMirrors.
Proceedings of Workshop Multilingualityin the Lexicon II, ECAI 98, Brighton, UK, 24-44.Erjavec, T. and Ide, N. (1998).
TheMULTEXT-EAST Corpus.
Proceedings of theFirst International Conference on LanguageResources and Evaluation, Granada, 971-74.Gale, W. A., Church, K. W. and Yarowsky, D.(1993).
A method for disambiguating word sensesin a large corpus.
Computers and the Humanities,26, 415-439.Ide, N. (1999).
Cross-lingual sensedetermination: Can it work?
Computers and theHumanities, 34:1-2,  223-34.Ide, N., Erjavec, T., and Tufis, D. (2001).Automatic sense tagging using parallel corpora.Proceedings of the Sixth Natural LanguageProcessing Pacific Rim Symposium, Tokyo,  83-89.Ide, N., V?ronis, J.
(1994).
Multext(Multilingual Tools and Corpora).
Proceedings ofthe 14th International Conference onComputational Linguistics, COLING?94, Kyoto,90-96.Miller, G. A., Beckwith, R. T. Fellbaum, C. D.,Gross, D. and Miller, K. J.
(1990).
WordNet: Anon-line lexical database.
International Journal ofLexicography, 3:4, 235-244.Ng, H. T., Lim, C. Y., Foo, S. K. (1999).
ACase Study on Inter-Annotator Agreement forWord Sense Disambiguation.
Proceedings of theACL SIGLEX Workshop: Standardizing LexicalResources, College Park, MD, USA, 9-13.Priest-Dorman, G.; Erjavec, T.; Ide, N. andPetkevic, V. (1997).
Corpus Markup.
COP Project106 MULTEXT-East D2.3 F.Resnik, P. and Yarowsky, D. (2000).Distinguishing systems and distinguishing senses:New evaluation methods for word sensedisambiguation.
Journal of Natural LanguageEngineering, 5(2): 113-133.Resnik, P., Broman Olsen, M., Diab, M. (1999).Creating a Parallel Corpus from the Book of 2000Tongues.
Computers and the Humanities, 33:1-2.129-153.Resnik, Philip and Yarowsky, David (1997).
Aperspective on word sense disambiguation methodsand their evaluation.
ACL-SIGLEX WorkshopTagging Text with Lexical Semantics: Why, What,and How?
Washington, D.C., 79-86.Stolcke, Andreas (1996) Cluster 2.9.http://www.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/software/cluster-2.9.tar.Z.Tufis, D., Barbu, A.-M. (2001) AutomaticConstruction of Translation Lexicons.
In V.Kluew,C.
D'Attellis N. Mastorakis (eds.)
Advances inAutomation, Multimedia and Modern ComputerScience, WSES Press, 156-172Tufis, D., Barbu, A.-M. (2002), Revealingtranslators knowledge: statistical methods inconstructing practical multilingual lexicons forlanguage and speech processing.
InternationalJournal of Speech Technology (to appear).Yarowsky, D., Florian.
R. (1999).
Taking theload off the conference chairs: towards a digitalpaper-routing assistant.
Proceedings of the JointSIGDAT Conference on Empirical Methods in NLPand Very Large Corpora, 220-230.
