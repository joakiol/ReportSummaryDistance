Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 250?256,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsDeep Multilingual Correlation for Improved Word EmbeddingsAng Lu1, Weiran Wang2, Mohit Bansal2, Kevin Gimpel2, and Karen Livescu21Department of Automation, Tsinghua University, Beijing, 100084, Chinalva11@mails.tsinghua.edu.cn2Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA{weiranwang, mbansal, kgimpel, klivescu}@ttic.eduAbstractWord embeddings have been found usefulfor many NLP tasks, including part-of-speechtagging, named entity recognition, and pars-ing.
Adding multilingual context when learn-ing embeddings can improve their quality,for example via canonical correlation analysis(CCA) on embeddings from two languages.
Inthis paper, we extend this idea to learn deepnon-linear transformations of word embed-dings of the two languages, using the recentlyproposed deep canonical correlation analy-sis.
The resulting embeddings, when eval-uated on multiple word and bigram similar-ity tasks, consistently improve over monolin-gual embeddings and over embeddings trans-formed with linear CCA.1 IntroductionLearned word representations are widely used inNLP tasks such as tagging, named entity recogni-tion, and parsing (Miller et al, 2004; Koo et al,2008; Turian et al, 2010; Ta?ckstro?m et al, 2012;Huang et al, 2014; Bansal et al, 2014).
The ideain such representations is that words with similarcontext have similar meaning, and hence shouldbe nearby in a clustering or vector space.
Con-tinuous representations are learned with neural lan-guage models (Bengio et al, 2003; Mnih and Hin-ton, 2007; Mikolov et al, 2013) or spectral meth-ods (Deerwester et al, 1990; Dhillon et al, 2011).The context used to learn these representations istypically the set of nearby words of each word oc-currence.
Prior work has found that adding transla-tional context results in better representations (Diaband Resnik, 2002; Ta?ckstro?m et al, 2012; Bansal etal., 2012; Zou et al, 2013).
Recently, Faruqui andDyer (2014) applied canonical correlation analysis(CCA) to word embeddings of two languages, andfound that the resulting embeddings represent wordsimilarities better than the original monolingual em-beddings.In this paper, we follow the same intuition asFaruqui and Dyer (2014) but rather than learning lin-ear transformations with CCA, we permit the cor-related information to lie in nonlinear subspaces ofthe original embeddings.
We use the recently pro-posed deep canonical correlation analysis (DCCA)technique of Andrew et al (2013) to learn non-linear transformations of two languages?
embed-dings that are highly correlated.
We evaluate ourDCCA-transformed embeddings on word similaritytasks like WordSim-353 (Finkelstein et al, 2001)and SimLex-999 (Hill et al, 2014), and also onthe bigram similarity task of Mitchell and Lapata(2010) (using additive composition), obtaining con-sistent improvements over the original embeddingsand over linear CCA.
We also compare tuning crite-ria and ensemble methods for these architectures.2 MethodWe assume that we have initial word embeddings fortwo languages, denoted by random vectors x ?
RDxand y ?
RDy, and a set of bilingual word pairs.
Ourgoal is to obtain a representation for each languagethat incorporates useful information from both x andy.
We consider the two input monolingual word em-beddings as different views of the same latent se-mantic signal.
There are multiple ways to incor-porate multilingual information into word embed-dings.
Here we follow Faruqui and Dyer (2014) intaking a CCA-based approach, in which we projectthe original embeddings onto their maximally corre-lated subspaces.
However, instead of relying on lin-ear correlation, we learn more powerful non-lineartransformations of each view via deep networks.Canonical Correlation Analysis A popularmethod for multi-view representation learning iscanonical correlation analysis (CCA; Hotelling,1936).
Its objective is to find two vectors u ?
RDx250and v ?
RDysuch that projections of the two viewsonto these vectors are maximally (linearly) corre-lated:maxu?RDx,v?RDyE[(u?x)(v?y)]?E [(u?x)2]?E [(v?y)2]=u??xyv?u??xxu?v?
?yyv(1)where ?xyand ?xxare the cross-view and within-view covariance matrices.
(1) is extended to learnmulti-dimensional projections by optimizing thesum of correlations in all dimensions, subject todifferent projected dimensions being uncorrelated.Given sample pairs {(xi,yi)}Ni=1, the empirical es-timates of the covariance matrices are?
?xx=1N?Ni=1xix?i+ rxI,?
?yy=1N?Ni=1yiy?i+ ryIand?
?xy=1N?Ni=1xiy?iwhere (rx, ry) > 0 areregularization parameters (Hardoon et al, 2004;De Bie and De Moor, 2003).
Then the optimal k-dimensional projection mappings are given in closedform via the rank-k singular value decomposition(SVD) of the Dx?Dymatrix???1/2xx??xy??
?1/2yy.2.1 Deep Canonical Correlation AnalysisA linear feature mapping is often not sufficientlypowerful to faithfully capture the hidden, non-linearrelationships within the data.
Recently, Andrew etal.
(2013) proposed a nonlinear extension of CCAusing deep neural networks, dubbed deep canonicalcorrelation analysis (DCCA) and illustrated in Fig-ure 1.
In this model, two (possibly deep) neuralnetworks f and g are used to extract features fromeach view, and trained to maximize the correlationsbetween outputs in the two views, measured by alinear CCA step with projection mappings (u,v).The neural network weights and the linear projec-tions are optimized together using the objectivemaxWf,Wg,u,vu??fgv?u??ffu?v?
?ggv, (2)where Wfand Wgare the weights of the two net-works and ?fg, ?ffand ?ggare covariance ma-trices computed for {f(xi),g(yi)}Ni=1in the sameway as CCA.
The final transformation is the com-position of the neural network and CCA projection,e.g., u?f(x) for the first view.
Unlike CCA, DCCAword vector 2English Germanword vector 1View 1View2uvfgfoulfoulawfuluglyprettycharmingcutegorgeousmarvelousmagnificentelegantsplendidhidousbeastlygrotesquehorridschrecklichenha?sslicheziemlichbezauberndercleverblondenwunderbarengro?artigeelegantehervorragendeabscheulichengebotgroteskaufzukla?renFigure 1: Illustration of deep CCA.does not have a closed-form solution, but the param-eters can be learned via gradient-based optimization,with either batch algorithms like L-BFGS as in (An-drew et al, 2013) or a mini-batch stochastic gradientdescent-like approach as in (Wang et al, 2015).
Wechoose the latter in this paper.An alternative nonlinear extension of CCA is ker-nel CCA (KCCA) (Lai and Fyfe, 2000; Vinokourovet al, 2003), which introduces nonlinearity throughkernels.
DCCA scales better with data size, asKCCA involves the SVD of an N ?
N matrix.
An-drew et al (2013) showed that DCCA achieves bet-ter correlation on held-out data than CCA/KCCA,and Wang et al (2015) found that DCCA outper-forms CCA and KCCA on a speech recognition task.3 ExperimentsWe use English and German as our two languages.Our original monolingual word vectors are the sameas those used by Faruqui and Dyer (2014).
Theyare 640-dimensional and are estimated via latentsemantic analysis on the WMT 2011 monolingualnews corpora.1We use German-English translationpairs as the input to CCA and DCCA, using thesame set of 36K pairs as used by Faruqui and Dyer.These pairs contain, for each of 36K English wordtypes, the single most frequently aligned Germanword.
They were obtained using the word alignerin cdec (Dyer et al, 2010) run on the WMT06-10 news commentary corpora and Europarl.
Aftertraining, we apply the learned CCA/DCCA projec-tion mappings to the original English word embed-dings (180K words) and use these transformed em-beddings for our evaluation tasks.3.1 Evaluation TasksWe compare our DCCA-based embeddings to theoriginal word vectors and to CCA-based em-1www.statmt.org/wmt11/251beddings on several tasks.
We use WordSim-353 (Finkelstein et al, 2001), which contains 353English word pairs with human similarity ratings.It is divided into WS-SIM and WS-REL by Agirreet al (2009) to measure similarity and relatedness.We also use SimLex-999 (Hill et al, 2014), a newsimilarity-focused dataset consisting of 666 nounpairs, 222 verb pairs, and 111 adjective pairs.
Fi-nally, we use the bigram similarity dataset fromMitchell and Lapata (2010) which has 3 subsets,adjective-noun (AN), noun-noun (NN), and verb-object (VN), and dev and test sets for each.
For thebigram task, we simply add the word vectors outputby CCA or DCCA to get bigram vectors.2All task datasets contain pairs with human sim-ilarity ratings.
To evaluate embeddings, we com-pute cosine similarity between the two vectors ineach pair, order the pairs by similarity, and com-pute Spearman?s correlation (?)
between the model?sranking and human ranking.3.2 TrainingWe normalize the 36K training pair vectors to unitnorm (as also done by Faruqui and Dyer).
Wethen remove the per-dimension mean and standarddeviation of this set of training pairs, as is typi-cally done in neural network training (LeCun et al,1998).
We do the same to the original 180K Eng-lish word vectors (normalize to unit norm, removethe mean/standard deviation of the size-36K train-ing set), then apply our CCA/DCCA mappings tothese 180K vectors.
The resulting 180K vectors arefurther normalized to zero mean before cosine simi-larities between test pairs are computed, as also doneby Faruqui and Dyer.For both CCA and DCCA, we tune theoutput dimensionality among factors in{0.2, 0.4, 0.6, 0.8, 1.0} of the original embed-ding dimension (640), and regularization (rx, ry)from {10?6, 10?5, 10?4, 10?3}, based on the 7tuning tasks discussed below.For DCCA, we use standard deep neural net-works with rectified linear units and tune thedepth (1 to 4 hidden layers) and layer widths (in{128, 256, 512, 1024, 2048, 4096}) separately foreach language.
For optimization, we use stochastic2We also tried multiplication but it performed worse.
In fu-ture work, we will directly train on bigram translation pairs.gradient descent (SGD) as described by Wang et al(2015).
We tune SGD hyperparameters on a smallgrid, choosing a mini-batch size of 3000, learningrate of 0.0001, and momentum of 0.99.3.3 TuningOur main results are based on tuning hyperparame-ters (of CCA/DCCA) on 7 word similarity tasks.3We perform additional experiments in which wetune on the development sets for the bigram tasks.We set aside WS-353, SimLex-999, and the test setsof the bigram tasks as held-out test sets.
We considertwo tuning criteria:BestAvg: Choose the hyperparameters with the bestaverage performance across the 7 tuning tasks.
Thisis the only tuning criterion used for CCA.MostBeat: For DCCA, choose the hyperparametersthat beat the best CCA embeddings on a maximumnumber of the 7 tasks; to break ties here, choose thehyperparameters with the best average performance.The idea is that we want to find a setting that gener-alizes to many tasks.We also consider simple ensembles by averagingthe cosine similarities from the three best settingsunder each of these two criteria.3.4 ResultsTable 1 shows our main results on the word and bi-gram similarity tasks.
All values are Spearman?scorrelation (?).
We show the original word vectorresults, the best-tuned CCA setting (CCA-1), the en-semble of the top-3 CCA settings (CCA-Ens), andthe same for DCCA (with both tuning criteria).
TheDCCA results show an overall improvement on mosttasks over linear CCA (all of the shaded DCCA re-sults are better than all corresponding CCA results).Each of our tuning criteria for DCCA performswell, and almost always better than CCA.
BestAvgis better on some tasks while MostBeat is better onothers; we report both here to bring attention to andpromote discussion about the effects of tuning meth-ods when learning representations in the absence ofsupervision or in-domain tuning data.In Table 2, we report additional bigram similarityresults obtained by tuning on the dev sets of the bi-3RG-65 (Rubenstein and Goodenough, 1965), MC-30(Miller and Charles, 1991), MTurk-287 (Radinsky et al, 2011),MTurk-771, MEN (Bruni et al, 2014), RareWord (Luong et al,2013), and YP-130 (Yang and Powers, 2006).252Embeddings WS-353 WS-SIM WS-REL SL-999 AN NN VN Avg DimOriginal 46.7 56.3 36.6 26.5 26.5 38.1 34.1 32.9 640CCA-1 67.2 73.0 63.4 40.7 42.4 48.1 37.4 42.6 384CCA-Ens 67.5 73.1 63.7 40.4 42.0 48.2 37.8 42.7 384DCCA-1 (BestAvg) 69.6 73.9 65.6 38.9 35.0 40.9 41.3 39.1 128DCCA-Ens (BestAvg) 70.8 75.2 67.3 41.7 42.4 45.7 40.1 42.7 128DCCA-1 (MostBeat) 68.6 73.5 65.7 42.3 44.4 44.7 36.7 41.9 384DCCA-Ens (MostBeat) 69.9 74.4 66.7 42.3 43.7 47.4 38.8 43.3 384Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text fordetails).
Shading indicates a result that matches or improves the best linear CCA result; boldface indicatesthe best result in a given column.
See Section 3.4 for discussion on NN results.Embeddings AN NN VN AvgCCA 42.4 48.1 37.4 42.6Deep CCA 45.5 47.1 45.1 45.9Table 2: Bigram results, tuned on bigram dev sets.gram tasks themselves (as provided by Mitchell andLapata), since the 7 tuning tasks are not particularlyrelated to the bigram test sets.
We see that DCCAcan achieve even stronger improvements over CCAand overall using these related dev sets.We note that the performance on the NN taskdoes not improve.
The typical variance of annota-tor scores for each bigram pair was larger for theNN dataset than for the other bigram datasets, sug-gesting noisier annotations.
Also, we found that theNN annotations often reflected topical relatednessrather than functional similarity, e.g., television setand television programme are among the most simi-lar noun-noun bigrams.
We expect that multilingualinformation would help embeddings to more closelyreflect functional similarity.For DCCA, we found that the best-performingnetworks were typically asymmetric, with 1 to 2 lay-ers on the English side and 2 to 4 on the Germanside.
The best network structure on the bigram VNdevelopment set is 640-128-128 for the English viewand 640-128-512-128 for the German view, with afinal CCA projection layer with dimensionality 128for each language.4 DiscussionNormalization and Evaluation We note that thecosine similarity (and thus Spearman?s ?)
between apair of words is not invariant to the series of simple(affine) transformations done by the normalizationsin our procedure.
For their baseline, Faruqui andDyer (2014) did not remove the standard deviationbetter with DCCA worse with DCCAarrive come author creatorlocate find leader managerway manner buddy companionrecent new crowd bunchtake obtain achieve succeedboundary border attention interestwin accomplish join addcontemplate think mood emotionTable 3: Highly-similar pairs in SimLex-999 thatimproved/degraded the most under DCCA.
Pairs aresorted in decreasing order according to the amountof improvement/degradation.of the 36K training set for the 180K English vocabu-lary during testing.
We have accidentally found thatthis normalization step alone greatly improves theperformance of the original vectors.For example, the WS-353 correlation improvesfrom 46.7 to 67.1, essentially matching the linearCCA correlations, though DCCA still outperformsthem both.
This indicates that the cosine similarityis not stable, and it is likely better to learn a dis-tance/similarity function (using labeled tuning data)atop the learned features such that similarities be-tween selected pairs will match the human similari-ties, or such that the rankings will match.Error Analysis We analyze high-similarity wordpairs that change the most with DCCA, as comparedto both linear CCA and the original vectors.For a word pair w, we use r(w) to refer to itssimilarity rank, subscripting it whether it is com-puted according to human ratings (rh) or if basedon cosine similarity via the original vectors (ro),CCA-1 (rc), or DCCA-1 MostBeat (rd).
We define?a(w) = |ra(w) ?
rh(w)| and compute ?
(w) =253Original CCA-1 DCCA-1 (MostBeat)Figure 2: t-SNE visualization of synonyms (green) and antonyms (red, capitalized) of dangerous.
?d(w) ?
(?c(w) + ?o(w)).
If ?
(w) < 0, thenword pair w was closer to the human ranking usingDCCA.
Table 3 shows word pairs from SimLex-999with high human similarity ratings (?
7 out of 10);column 1 shows pairs with smallest ?
values, andcolumn 2 shows pairs with largest ?
values.Among pairs in column 1, many contain wordswith several senses.
Using bilingual information islikely to focus on the most frequent sense in the bi-text, due to our use of the most frequently-alignedGerman word in each training pair.
By contrast,using only monolingual context is expected to findan embedding that blends the contextual informationacross all word senses.Several pairs from column 2 show hypernymrather than paraphrase relationships, e.g., author-creator and leader-manager.
Though these pairs arerated as highly similar by annotators, linear CCAmade them less similar than the original vectors, andDCCA made them less similar still.
This matchesour intuition that bilingual information should en-courage paraphrase-like similarity and thereby dis-courage the similarity of hypernym-hyponym pairs.Visualizations We visualized several synonym-antonym word lists and often found that DCCAmore cleanly separated synonyms from antonymsthan CCA or the original vectors.
An example ofthe clearest improvement is shown in Fig.
2.5 Related workPrevious work has successfully used translationalcontext for word representations (Diab and Resnik,2002; Zhao et al, 2005; Ta?ckstro?m et al, 2012;Bansal et al, 2012; Faruqui and Dyer, 2014), includ-ing via hand-designed vector space models (Peirs-man and Pado?, 2010; Sumita, 2000) or via unsuper-vised LDA and LSA (Boyd-Graber and Blei, 2009;Zhao and Xing, 2006).There have been other recent deep learning ap-proaches to bilingual representations, e.g., based ona joint monolingual and bilingual objective (Zouet al, 2013).
There has also been recent interestin learning bilingual representations without usingword alignments (Chandar et al, 2014; Gouws et al,2014; Koc?isky` et al, 2014; Vulic and Moens, 2013).This research is also related to early examples oflearning bilingual lexicons using monolingual cor-pora (Koehn and Knight, 2002; Haghighi et al,2008); the latter used CCA to find matched wordpairs.
Irvine and Callison-Burch (2013) used a su-pervised learning method with multiple monolingualsignals.
Finally, other work on CCA and spectralmethods has been used in the context of other typesof views (Collobert and Weston, 2008; Dhillon et al,2011; Klementiev et al, 2012; Chang et al, 2013).6 ConclusionWe have demonstrated how bilingual informationcan be incorporated into word embeddings via deepcanonical correlation analysis (DCCA).
The DCCAembeddings consistently outperform linear CCAembeddings on word and bigram similarity tasks.Future work could compare DCCA to other non-linear approaches discussed in ?5, compare differ-ent languages as multiview context, and extend toaligned phrase pairs, and to unaligned data.AcknowledgmentsWe are grateful to Manaal Faruqui for sharing re-sources, and to Chris Dyer, David Sontag, Lyle Un-gar, and anonymous reviewers for helpful input.254ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pacsca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceedingsof HLT-NAACL.Galen Andrew, Raman Arora, Jeff Bilmes, and KarenLivescu.
2013.
Deep canonical correlation analysis.In Proceedings of ICML.Mohit Bansal, John DeNero, and Dekang Lin.
2012.
Un-supervised translation sense clustering.
In Proceed-ings of NAACL-HLT.M.
Bansal, K. Gimpel, and K. Livescu.
2014.
Tailoringcontinuous word representations for dependency pars-ing.
In Proceedings of ACL.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155,March.Jordan Boyd-Graber and David M Blei.
2009.
Multilin-gual topic models for unaligned text.
In Proceedingsof UAI.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
J. Artif.
Intell.Res.
(JAIR), 49:1?47.Sarath Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas Raykar,and Amrita Saha.
2014.
An autoencoder approach tolearning bilingual word representations.
In Proceed-dings of NIPS.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of EMNLP.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Tijl De Bie and Bart De Moor.
2003.
On the regular-ization of canonical correlation analysis.
Int.
Sympos.ICA and BSS, pages 785?790.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.2011.
Multi-view learning of word embeddings viaCCA.
In Proceedings of NIPS.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of ACL.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,P.
Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proceedings of ACL.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
In Proceedings of EACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The conceptrevisited.
In Proceedings of WWW.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2014.
Bilbowa: Fast bilingual distributed repre-sentations without word alignments.
arXiv preprintarXiv:1410.2455.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL.David R. Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.
Neu-ral Computation, 16(12):2639?2664, December.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377, December.Fei Huang, Arun Ahuja, Doug Downey, Yi Yang, YuhongGuo, and Alexander Yates.
2014.
Learning repre-sentations for weakly supervised natural language pro-cessing tasks.
Computational Linguistics, 40(1).Ann Irvine and Chris Callison-Burch.
2013.
Supervisedbilingual lexicon induction with multiple monolingualsignals.
In Proceedings of HLT-NAACL, pages 518?523.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.Toma?s?
Koc?isky`, Karl Moritz Hermann, and Phil Blun-som.
2014.
Learning bilingual word representa-tions by marginalizing alignments.
arXiv preprintarXiv:1405.0947.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In Pro-ceedings of the ACL Workshop on Unsupervised Lexi-cal Acquisition.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL.P.
L. Lai and C. Fyfe.
2000.
Kernel and nonlinearcanonical correlation analysis.
Int.
J. Neural Syst.,10(5):365?377, October.Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Mu?ller.
1998.
Efficient backprop.
volume2551524 of Lecture Notes in Computer Science, pages 9?50, Berlin.
Springer-Verlag.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better word representations with re-cursive neural networks for morphology.
In Proceed-ings of CoNLL.TomasMikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeffrey Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InProceedings of NIPS.George A Miller and Walter G Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andcognitive processes, 6(1):1?28.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In Proceedings of HLT-NAACL.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1439.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphicalmodels for statistical languagemodelling.
InProceedings of ICML.Yves Peirsman and Sebastian Pado?.
2010.
Cross-lingual induction of selectional preferences with bilin-gual vector spaces.
In Proceedings of HLT-NAACL.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch.
2011.
A word at a time: com-puting word relatedness using temporal semantic anal-ysis.
In Proceedings of WWW.Herbert Rubenstein and John B Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 8(10):627?633.Eiichiro Sumita.
2000.
Lexical transfer using a vector-space model.
In Proceedings of ACL.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of HLT-NAACL.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.Alexei Vinokourov, Nello Cristianini, and John Shawe-Taylor.
2003.
Inferring a semantic representation oftext via cross-language correlation analysis.
In Pro-ceedings of NIPS.Ivan Vulic and Marie-Francine Moens.
2013.
A studyon bootstrapping bilingual vector spaces from non-parallel data (and nothing else).
In Proceedings ofEMNLP.Weiran Wang, Raman Arora, Karen Livescu, and JeffBilmes.
2015.
Unsupervised learning of acoustic fea-tures via deep canonical correlation analysis.
In Pro-ceedings of ICASSP.Dongqiang Yang and David MW Powers.
2006.
Verbsimilarity on the taxonomy of wordnet.
Proceedingsof GWC-06, pages 121?128.Bing Zhao and Eric P Xing.
2006.
Bitam: Bilin-gual topic admixture models for word alignment.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 969?976.
ACL.Bing Zhao, Eric P Xing, and Alex Waibel.
2005.
Bilin-gual word spectral clustering for statistical machinetranslation.
In Proceedings of the ACL Workshop onBuilding and Using Parallel Texts.Will Y Zou, Richard Socher, Daniel M Cer, and Christo-pher D Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingsof EMNLP.256
