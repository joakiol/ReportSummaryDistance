Experiments on the Automatic Induction ofGerman Semantic Verb ClassesSabine Schulte im Walde?Universita?t des SaarlandesThis article presents clustering experiments on German verbs: A statistical grammar model forGerman serves as the source for a distributional verb description at the lexical syntax?semanticsinterface, and the unsupervised clustering algorithm k-means uses the empirical verb propertiesto perform an automatic induction of verb classes.
Various evaluation measures are applied tocompare the clustering results to gold standard German semantic verb classes under differentcriteria.
The primary goals of the experiments are (1) to empirically utilize and investigate thewell-established relationship between verb meaning and verb behavior within a cluster analysisand (2) to investigate the required technical parameters of a cluster analysis with respect to thisspecific linguistic task.
The clustering methodology is developed on a small-scale verb set andthen applied to a larger-scale verb set including 883 German verbs.1.
MotivationSemantic verb classes generalize over verbs according to their semantic properties,that is, they capture large amounts of verb meaning without defining the idiosyncraticdetails for each verb.
The classes refer to a general semantic level, and idiosyncraticlexical semantic properties of the verbs are either added to the class description orleft underspecified.
Examples for semantic verb classes are Position verbs such as liegen?lie?, sitzen ?sit?, stehen ?stand?, and Manner of Motion with a Vehicle verbs such as fahren?drive?, fliegen ?fly?, rudern ?row?.
Manual definitions of semantic verb classes existfor several languages, the most dominant examples concerning English (Levin 1993;Baker, Fillmore, and Lowe 1998) and Spanish (Va?zquez et al 2000).
On the one hand,verb classes reduce redundancy in verb descriptions since they encode the commonproperties of verbs.
On the other hand, verb classes can predict and refine properties ofa verb that received insufficient empirical evidence, with reference to verbs in the sameclass: Under this criterion, a verb classification is especially useful for the pervasiveproblem of data sparseness in NLP, where little or no knowledge is provided for rareevents.
For example, the English verb classification by Levin (1993) has been used inNLP applications such as word sense disambiguation (Dorr and Jones 1996), machinetranslation (Dorr 1997), document classification (Klavans and Kan 1998), and subcat-egorization acquisition (Korhonen 2002).
To my knowledge, no comparable Germanverb classification is available so far; therefore, such a classification would provide aprincipled basis for filling a gap in available lexical knowledge.?
Department of Computational Linguistics, Saarbru?cken, Germany.
E-mail: schulte@coli.uni-sb.de.Submission received: 1 September 2003; revised submission received: 5 September 2005; accepted forpublication: 10 November 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 2How can we obtain a semantic classification of verbs while avoiding tedious manualdefinitions of the verbs and the classes?
Few resources are semantically annotatedand provide semantic information off-the-shelf such as FrameNet (Baker, Fillmore,and Lowe 1998; Fontenelle 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005).Instead, the automatic construction of semantic classes typically benefits from a long-standing linguistic hypothesis that asserts a tight connection between the lexicalmeaning of a verb and its behavior: To a certain extent, the lexical meaning of a verbdetermines its behavior, particularly with respect to the choice of its arguments(Pinker 1989; Levin 1993; Dorr and Jones 1996; Siegel and McKeown 2000; Merlo andStevenson 2001; Schulte im Walde and Brew 2002; Lapata and Brew 2004).
Even thoughthe meaning?behavior relationship is not perfect, we can make this prediction: If weinduce a verb classification on the basis of verb features describing verb behavior, thenthe resulting behavior classification should agree with a semantic classification to acertain extent (yet to be determined).
The aim of this work is to utilize this predictionfor the automatic acquisition of German semantic verb classes.The verb behavior itself is commonly captured by the diathesis alternation of verbs:alternative constructions at the syntax?semantics interface that express the same or asimilar conceptual idea of a verb (Lapata 1999; Schulte im Walde 2000; McCarthy 2001;Merlo and Stevenson 2001; Joanis 2002).
Consider example (1), where the most commonalternations of the Manner of Motion with a Vehicle verb fahren ?drive?
are illustrated.
Theconceptual participants are a vehicle, a driver, a passenger, and a direction.
In (a), thevehicle is expressed as the subject in a transitive verb construction, with a prepositionalphrase indicating the direction.
In (b), the driver is expressed as the subject in a tran-sitive verb construction, with a prepositional phrase indicating the direction.
In (c), thedriver is expressed as the subject in a transitive verb construction, with an accusativenoun phrase indicating the vehicle.
In (d), the driver is expressed as the subject in aditransitive verb construction, with an accusative noun phrase indicating the passenger,and a prepositional phrase indicating the direction.
Even if a certain participant is notrealized within an alternation, its contribution might be implicitly defined by the verb.For example, in the German sentence in (a) the driver is not expressed overtly, but weknow that there is a driver, and in (b) and (d) the vehicle is not expressed overtly, butwe know that there is a vehicle.
Verbs in the same semantic class are expected to overlapin their alternation behavior to a certain extent.
For example, the Manner of Motion witha Vehicle verb fliegen ?fly?
alternates between (a) such as in Der Airbus A380 fliegt nachNew York ?The Airbus A380 flies to New York?, (b) in marked cases as in Der a?lterePilot fliegt nach London ?The older pilot flies to London?, (c) as in Pilot Schulze fliegt eineBoing 747 ?Pilot Schulze flies a Boing 747?, and (d) as in Der Pilot fliegt seine Passagiere nachThailand ?The pilot flies his passengers to Thailand?
; the Manner of Motion with a Vehicleverb rudern ?row?
alternates between (b) such as in Anna rudert u?ber den See ?Anna rowsover the lake?, (c) such as in Anna rudert das blaue Boot ?Anna rows the blue boat?, and (d)such as in Anna rudert ihren kleinen Bruder u?ber den See ?Anna rows her little brother overthe lake?.Example 1(a) Der Wagen fa?hrt in die Innenstadt.
?The car drives to the city centre.?
(b) Die Frau fa?hrt nach Hause.
?The woman drives home.
?160Schulte im Walde Induction of German Semantic Verb Classes(c) Der Filius fa?hrt einen blauen Ferrari.
?The son drives a blue Ferrari.?
(d) Der Junge fa?hrt seinen Vater zum Zug.
?The boy drives his father to the train.
?We decided to use diathesis alternations as an approach to characterizing verb behavior,and to use the following verb features to stepwise describe diathesis alternations: (1)syntactic structures, which are relevant for capturing argument functions; (2) preposi-tions, which are relevant to distinguish, for example, directions from locations; and (3)selectional preferences, which concern participant roles.
A statistical grammar modelserves as the source for an empirical verb description for the three levels at the syntax?semantics interface.
Based on the empirical feature description, we then perform acluster analysis of the German verbs using k-means, a standard unsupervised hardclustering technique as proposed by Forgy (1965).
The clustering outcome cannot be aperfect semantic verb classification, since the meaning?behavior relationship on whichthe clustering relies is not perfect, and the clustering method is not perfect for the am-biguous verb data.
However, our primary goal is not necessarily to obtain the optimalclustering result, but rather to assess the linguistic and technical conditions that arecrucial for a semantic cluster analysis.
More specifically, (1) we perform an empiricalinvestigation of the relationship between verb meaning and verb behavior (that is, Canwe use the meaning?behavior relationship of verbs to induce verb classes, and to whatextent does the meaning?behavior relationship hold in the experiments?
), and (2) weinvestigate which technical parameters are suitable for the natural language task.
Theresulting clustering methodology can then be applied to a larger-scale verb set.The plan of the article is as follows.
Section 2 describes the experimental setupwith respect to (1) gold standard verb classes for 168 German verbs, (2) the statisticalgrammar model that provides empirical lexical information for German verbs at thesyntax?semantics interface, and (3) the clustering algorithm and evaluation methods.Section 3 performs preliminary clustering experiments on the German gold standardverbs, and Section 4 presents an application of the clustering technique in a large-scaleexperiment.
Section 5 discusses related work, and Section 6 presents the conclusionsand outlook for further work.2.
Experimental Setup2.1 German Semantic Verb ClassesA set of 168 German verbs was manually classified into 43 concise semantic verb classes.The verb class labels refer to the common semantic properties of the verbs in a classat a general conceptual level, and the idiosyncratic lexical semantic properties of theverbs are left underspecified.
The German verbs are provided with a coarse translationinto English, given here in brackets; we do not attempt to define subtle differencesin meaning or usage.
The translated verb senses only refer to the respective semanticclass; if the verb translations in one class are too similar to distinguish among them,a common translation is given.
Even though the classification is primarily based onsemantic intuition and not on facts about syntactic behavior, the verbs grouped inone class share certain aspects of their behavior.
(Please note that this overlap doesnot necessarily transfer to the English translations.)
This agreement corresponds to the161Computational Linguistics Volume 32, Number 2long-standing linguistic hypothesis that asserts a tight connection between the meaningcomponents of a verb and its behavior (Pinker 1989; Levin 1993).The purpose of the manual classification is to evaluate the reliability and perfor-mance of the clustering experiments.
The following facts refer to empirically relevantproperties of the classification: The class size is between 2 and 7, with an average of3.9 verbs per class.
Eight verbs are ambiguous with respect to class membership andmarked by subscripts.
The classes include both high- and low-frequency verbs in orderto exercise the clustering technology in both data-rich and data-poor situations: Thecorpus frequencies of the verbs range from 8 to 71,604 (within 35 million words of aGerman newspaper corpus, cf.
Section 2.2).
The class labels are given on two semanticlevels: coarse labels such as Manner of Motion are subdivided into finer labels, such asLocomotion, Rotation, Rush, Vehicle, Flotation.
The fine-grained labels are relevant for theclustering experiments, as the numbering indicates.
As mentioned before, the classifi-cation is primarily based on semantic intuition, not on facts about syntactic behavior.As an extreme example, the Support class (23) contains the verb unterstu?tzen, whichsyntactically requires a direct object, together with the verbs dienen, folgen, and helfen,which dominantly subcategorize for an indirect object.
The classification was checkedto ensure lack of bias, so class membership is not disproportionately made up of high-frequency verbs, low-frequency verbs, strongly ambiguous verbs, verbs from specificsemantic areas, and so forth.The classification deliberately sets high standards for the automatic inductionprocess: It would be easier (1) to define the verb classes on a purely syntactic basis,since syntactic properties are easier to obtain automatically than semantic features,or (2) to define larger classes of verbs, so that the distinction between the classes isnot based on fine-grained verb properties, or (3) to disregard clustering complicationssuch as verb ambiguity and low-frequency verbs.
But the overall goal is not to achievea perfect clustering on the given 168 verbs but to investigate both the potential andthe limits of our clustering methodology that combines easily available data with asimple algorithm.
The task cannot be solved completely, but we can investigate thebounds.The classification is defined as follows:1.
Aspect: anfangen, aufho?ren, beenden, beginnen, enden (start, stop,finish, begin, end)2.
Propositional Attitude: ahnen, denken, glauben, vermuten, wissen(guess, think, believe, assume, know)?
Desire3.
Wish: erhoffen, wollen, wu?nschen (hope, want, wish)4.
Need: bedu?rfen, beno?tigen, brauchen (all: need/require)5.
Transfer of Possession (Obtaining): bekommen, erhalten, erlangen,kriegen (all: receive/obtain)?
Transfer of Possession (Giving)6.
Gift: geben, leihen, schenken, spenden, stiften, vermachen,u?berschreiben (give, borrow, present, donate, donate,bequeath, sign over)7.
Supply: bringen, liefern, schicken, vermitteln1, zustellen(bring, deliver, send, convey, deliver)162Schulte im Walde Induction of German Semantic Verb Classes?
Manner of Motion8.
Locomotion: gehen, klettern, kriechen, laufen, rennen,schleichen, wandern (go, climb, creep, walk, run, sneak,wander)9.
Rotation: drehen, rotieren (turn around, rotate)10.
Rush: eilen, hasten (both: hurry)11.
Vehicle: fahren, fliegen, rudern, segeln (drive, fly, row, sail)12.
Flotation: flie?en, gleiten, treiben (float, glide, float)?
Emotion13.
Origin: a?rgern, freuen (be annoyed, be happy)14.
Expression: heulen1, lachen1, weinen (cry, laugh, cry)15.
Objection: a?ngstigen, ekeln, fu?rchten, scheuen (frighten,disgust, fear, be afraid)16.
Facial Expression: ga?hnen, grinsen, lachen2, la?cheln, starren(yawn, grin, laugh, smile, stare)17.
Perception: empfinden, erfahren1, fu?hlen, ho?ren, riechen, sehen,wahrnehmen (feel, experience, feel, hear, smell, see, perceive)18.
Manner of Articulation: flu?stern, rufen, schreien (whisper, shout, scream)19.
Moaning: heulen2, jammern, klagen, lamentieren (all: wail/moan/complain)20.
Communication: kommunizieren, korrespondieren, reden, sprechen,verhandeln (communicate, correspond, talk, talk, negotiate)?
Statement21.
Announcement: anku?ndigen, bekanntgeben, ero?ffnen,verku?nden (all: announce)22.
Constitution: anordnen, bestimmen, festlegen (arrange,determine, constitute)23.
Promise: versichern, versprechen, zusagen (ensure,promise, promise)24.
Observation: bemerken, erkennen, erfahren2, feststellen, realisieren,registrieren (notice, realize, get to know, observe, realize, realize)25.
Description: beschreiben, charakterisieren, darstellen1, interpretieren(describe, characterize, describe, interpret)26.
Presentation: darstellen2, demonstrieren, pra?sentieren, veranschaulichen,vorfu?hren (present, demonstrate, present, illustrate, demonstrate)27.
Speculation: gru?beln, nachdenken, phantasieren, spekulieren (muse,think about, fantasize, speculate)28.
Insistence: beharren, bestehen1, insistieren, pochen (all: insist)29.
Teaching: beibringen, lehren, unterrichten, vermitteln2 (all: teach)?
Position30.
Bring into Position: legen, setzen, stellen (lay, set, put upright)31.
Be in Position: liegen, sitzen, stehen (lie, sit, stand)163Computational Linguistics Volume 32, Number 232.
Production: bilden, erzeugen, herstellen, hervorbringen, produzieren(all: generate/produce)33.
Renovation: dekorieren, erneuern, renovieren, reparieren (decorate,renew, renovate, repair)34.
Support: dienen, folgen1, helfen, unterstu?tzen (serve, follow, help,support)35.
Quantum Change: erho?hen, erniedrigen, senken, steigern, vergro?
?ern,verklenern (increase, decrease, decrease, increase, enlarge,diminish)36.
Opening: o?ffnen, schlie?en1 (open, close)37.
Existence: bestehen2, existieren, leben (exist, exist, live)38.
Consumption: essen, konsumieren, lesen, saufen, trinken (eat, consume,read, booze, drink)39.
Elimination: eliminieren, entfernen, exekutieren, to?ten, vernichten(eliminate, delete, execute, kill, destroy)40.
Basis: basieren, beruhen, gru?nden, stu?tzen (all: be based on)41.
Inference: folgern, schlie?en2 (conclude, infer)42.
Result: ergeben, erwachsen, folgen2, resultieren (all: follow/result)43.
Weather: blitzen, donnern, da?mmern, nieseln, regnen, schneien(lightning, thunder, dawn, drizzle, rain, snow)The evidence used in the class creation process?including the choice of theverbs?was provided by subjective conceptual knowledge, monolingual and bilingualdictionary entries and corpus searches.
Interannotator agreement has therefore not beenaddressed, but the classes were created in close relation to the English classificationby Levin (1993) (as far as the English classes have German counterparts) and agreewith the German verb classification by Schumacher (1986), as far as the relevant verbsare covered by his semantic ?fields?.
To overcome the drawback of a subjective classdefinition, the classification was accompanied by a detailed class description.
Thischaracterization is closely related to Fillmore?s scenes-and-frames semantics (Fillmore1977, 1982), as computationally utilized in FrameNet (Baker, Fillmore, and Lowe 1998;Fontenelle 2003); there is no reference to the German FrameNet version (Erk, Kowalski,and Pinkal 2003)?as one might expect?just because the German version itself hadjust started to be developed.
The frame-semantic class definition contains a prose scenedescription, predominant frame participant and modification roles, and frame variantsdescribing the scene.
The frame roles have been developed on the basis of a largeGerman newspaper corpus from the 1990s (cf.
Section 2.2).
They capture the scenedescription with idiosyncratic participant names and demarcate major and minor roles.Since a scene might be activated by a number of frame embeddings, the predominantframe variants from the corpus are listed, marked with participating roles, and at leastone example sentence for each verb utilizing the respective frame is given.
The corpusexamples are annotated and illustrate the idiosyncratic combinations of lexical verbmeaning and conceptual constructions to capture variations in verb sense.
Example 2presents a verb class description for the class of Aspect verbs.
For further class descrip-164Schulte im Walde Induction of German Semantic Verb Classestions, the reader is referred to Schulte im Walde (2003a, pages 27?103).
Verbs allowing aframe variant are marked by ?+,?
verbs allowing the frame variant only in company ofan additional adverbial modifier are marked by ?+adv,?
and verbs not allowing a framevariant are marked by ??.?
In the case of ambiguity, frame variants are only given forthe senses of the verbs with respect to the class label.
The frame variants with their rolesmarked represent the alternation potential of the verbs.
For example, the causative?inchoative alternation assumes the syntactic embeddings nXaY and nY, indicating thatthe alternating verbs are realized by a transitive frame type (containing a nominativeNP ?n?
with role X and an accusative NP ?a?
with role Y) and the correspondingintransitive frame type (with a nominative NP ?n?
only, indicating the same role Y as forthe transitive accusative).
Passivization of a verb?frame combination is indicated by [P].Appendix 6 lists all possible frame variants with illustrative examples.
Note that the cor-pus examples are given in the old German spelling version, before the spelling reformin 1998.Semantic verb classes have been defined for several languages, for example, as theearlier mentioned lexicographic resource FrameNet for English (Baker, Fillmore, andLowe 1998; Fontenelle 2003) and German (Erk, Kowalski, and Pinkal 2003); the lexicalsemantic ontology WordNet for English (Miller et al 1990; Fellbaum 1998); EuroWordNet(Vossen 2004) for Dutch, Italian, Spanish, French, German, Czech, and Estonian, andfurther languages as listed in WordNets in the World (Global WordNet Association,www.globalwordnet.org); syntax?semantics based verb classes for English (Levin 1993),Spanish (Va?zquez et al 2000), and French (Saint-Dizier 1998).Example 2Aspect Verbs: anfangen, aufho?ren, beenden, beginnen, endenScene: [E An event] begins or ends, either internally caused or externally caused by[I an initiator].
The event may be specified with respect to [T tense], [L location],[X an experiencer], or [R a result].Frame Roles: I(nitiator), E(vent)Modification Roles: T(emporal), L(ocal), (e)X(periencer), R(esult)Frame Participating Verbs and Corpus ExamplesnE + anfangen, aufho?ren, beginnen / +adv enden / ?
beendenNunNowaberthoughmu?must[E der Dialog]the dialoganfangen.beginErstFirstmu?must[E das Morden]the killingaufho?ren.stop[E Der Gottesdienst]The servicebeginnt.begins[E Das Schuljahr]The school yearbeginntbegins[T im Februar].in February[X Fu?r die Flu?chtlinge]For the fugitivesbeginntbeginsnunnow[E ein Wettlauf gegen die Zeit].a race against time[E Die Ferien]The vacationsendenend[R mit einem gro?en Fest].with a big party[E Druckkunst]The art of typesetting......endetends[R beim guten Buch].with a good book[E Der Informationstag]The information day......endetfinishes[T um 14 Uhr].at 2pm165Computational Linguistics Volume 32, Number 2nI + anfangen, aufho?ren / ?
beenden, beginnen, enden......da?that[I er]he[T pu?nktlich]in timeanfing.beginsJetztNowko?nnencan[I wir]wenichtnoteinfachjustaufho?ren.stopVielleichtMaybesollteshould[I ich]Iaufho?renstopundandnochyetstudieren.studynI + anfangen, beenden, beginnen / ?
aufho?ren, endenaE NachdemAfter[I wir]we[E die Sache]the thingangefangen haben,have started[I Die Polizei]The policebeendetestopped[E die Gewaltta?tigkeiten].the violence[T Nach dem Abi]After the Abiturbeginntbegins[I Jens]Jens[L in Frankfurt]in Frankfurt[E seine Lehre]his apprenticeship......nI + anfangen, beenden, beginnen / ?
aufho?ren, endenaE WennIf[E die Arbeiten]the work[T vor dem Bescheid]before the notificationangefangen werdenis started......[P] Wa?hrendWhile[X fu?r Senna]for Senna[E das Rennen]the racebeendet warwas finished............ehebefore[E eine milita?rische Aktion]a military actionbegonnen wirdis begun...nI + anfangen, aufho?ren, beginnen / ?
beenden, endeniE [I Ich]Ihabehaveangefangen,started[E Hemden zu schneidern].shirts to make......da?that[I der Alkoholiker]the alcoholicaufho?rtstops[E zu trinken].to drinkIn dieser StimmungIn this moodbegannenbegan[I Ma?nner]men[E Tango zu tanzen]tango to dance...nI + anfangen, aufho?ren, beginnen / ?
beenden, endenpE : mit ErstOnlyalswhen[I der versammelte Hofstaat]the gathered royal household[E mit Klatschen]with applauseanfing,began[I Der Athlet]The athlete......kanncan......[E mit seinem Sport]with his sportsaufho?ren.stop[I Man]Onebeginnestarts[E mit eher katharsischen Werken].with rather catharsic worksnI +anfangen, aufho?ren, beginnen / ?
beenden, endenpE : mit UndAnd[E mit den Umbauarbeiten]with the reconstruction workko?nntecouldangefangen werden.be begun[P] [E Mit diesem ungerechten Krieg]With this unjust warmu?mustsofortimmediatelyaufgeho?rt werden.be stopped[T Vorher]Beforedu?rfemust[E mit der Auflo?sung]with the closingnichtnotbegonnen werden.be started2.2 Empirical Distributions for German VerbsWe developed, implemented, and trained a statistical grammar model for German thatis based on the framework of head-lexicalized, probabilistic, context-free grammars.The idea originates from Charniak (1997), with this work using an implementationby Schmid (2000) for a training corpus of 35 million words from a collection of largeGerman newspaper corpora from the 1990s, including Frankfurter Rundschau, StuttgarterZeitung, VDI-Nachrichten, die tageszeitung, German Law Corpus, Donaukurier, and Com-puterzeitung.
The statistical grammar model provides empirical lexical information,specializing in but not restricted to the subcategorization behavior of verbs.
Details of166Schulte im Walde Induction of German Semantic Verb Classesthe implementation, training, and exploitation of the grammar model can be found inSchulte im Walde (2003a, chapter 3).The German verbs are represented by distributional vectors, with features andfeature values in the distribution being acquired from the statistical grammar.
The dis-tributional description is based on the hypothesis that ?each language can be describedin terms of a distributional structure, that is, in terms of the occurrence of parts relativeto other parts?
(cf.
Harris 1968).
The verbs are described distributionally on three levelsat the syntax?semantics interface, each level refining the previous level.
The first levelD1 encodes a purely syntactic definition of verb subcategorization, the second levelD2 encodes a syntactico-semantic definition of subcategorization with prepositionalpreferences, and the third level D3 encodes a syntactico-semantic definition of sub-categorization with prepositional and selectional preferences.
Thus, the refinement ofverb features starts with a purely syntactic definition and incrementally adds semanticinformation.
The most elaborated description comes close to a definition of verb alterna-tion behavior.
We decided on this three-step procedure of verb descriptions because theresulting clusters and particularly the changes in clusters that result from a change offeatures should provide insight into the meaning?behavior relationship at the syntax?semantics interface.For D1, the statistical grammar model provides frequency distributions for Ger-man verbs over 38 purely syntactic subcategorization frames (cf.
Appendix 6).
Basedon these frequencies, we can also calculate the probabilities.
For D2, the grammarprovides frequencies for the different kinds of prepositional phrases within a frametype; probabilities are computed by distributing the joint probability of a verb and aPP frame over the prepositional phrases according to their frequencies in the corpus.Prepositional phrases are referred to by case and preposition, such as mitDat, fu?rAcc.
Thestatistical grammar model does not learn the distinction between PP arguments and PPadjuncts perfectly.
Therefore, we did not restrict the PP features to PP arguments, butto 30 PPs according to ?reasonable?
appearance in the corpus, as defined by the 30 mostfrequent PPs that appear with at least 10 different verbs.
The subcategorization frameinformation for D1 and D2 has been evaluated: Schulte im Walde (2002b) describes theinduction of a subcategorization lexicon from the grammar model for a total of 14,229verbs with a frequency between 1 and 255,676 in the training corpus, and Schulte imWalde (2002a) performs an evaluation of the subcategorization data against manuallycreated dictionary entries and shows that the lexical entries have potential for addingto and improving manual verb definitions.For the refinement of D3, the grammar provides selectional preference informationat a fine-grained level: It specifies the possible argument realizations in the form oflexical heads, with reference to a specific verb?frame?slot combination.
Obviously, wewould run into a sparse data problem if we tried to incorporate selectional preferencesinto the verb descriptions at such a specific level.
We are provided with detailed infor-mation at the nominal level, but we need a generalization of the selectional preferencedefinition.
A widely used resource for selectional preference information is the semanticontology WordNet (Miller et al 1990; Fellbaum 1998); the University of Tu?bingen hasdeveloped the German version of WordNet, GermaNet (Hamp and Feldweg 1997; Kunze2000).
The hierarchy is realized by means of synsets, sets of synonymous nouns, whichare organized by multiple inheritance hyponym/hypernym relationships.
A noun canappear in several synsets, according to its number of senses.
The German noun hierar-chy in GermaNet is utilized for the generalization of selectional preferences: For eachnoun in a verb?frame?slot combination, the joint frequency is divided over the differentsenses of the noun and propagated up the hierarchy.
In case of multiple hypernym167Computational Linguistics Volume 32, Number 2synsets, the frequency is divided again.
The sum of frequencies over all top synsetsequals the total joint frequency.
Repeating the frequency assignment and propagationfor all nouns appearing in a verb?frame?slot combination, the result defines a frequencydistribution of the verb?frame?slot combination over all GermaNet synsets.
To restrictthe variety of noun concepts to a general level, only the frequency distributions over thetop GermaNet nodes1 are considered: Lebewesen ?creature?, Sache ?thing?, Besitz ?prop-erty?, Substanz ?substance?, Nahrung ?food?, Mittel ?means?, Situation ?situation?, Zustand?state?, Struktur ?structure?, Physis ?body?, Zeit ?time?, Ort ?space?, Attribut ?attribute?,Kognitives Objekt ?cognitive object?, Kognitiver Prozess ?cognitive process?.
Since the 15nodes are mutually exclusive and the node frequencies sum to the total joint verb-framefrequency, we can use their frequencies to define a probability distribution.Are selectional preferences equally necessary and informative for all frame types?For example, selectional preferences for the direct object are expected to vary stronglywith respect to the subcategorizing verb (because the direct object is a highly frequentargument type across all verbs and verb classes), but selectional preferences for asubject in a transitive construction with a nonfinite clause are certainly less interestingfor refinement (because this frame type is more restricted with respect to the verbs itis subcategorized for).
We empirically investigated which of the overall frame rolesmay be realized by different selectional preferences and are therefore relevant andinformative for a selectional preference distinction.
As a result, in parts of the clusteringexperiments we will concentrate on a specific choice of frame-slot combinations to berefined by selectional preferences (with the relevant slots underlined): ?n?, ?na?, ?nd?,?nad?, ?ns-dass.
?Table 1 presents three verbs from different classes and their 10 most frequent frametypes at the three levels of verb definition and their probabilities.
D1 for beginnen ?begin?defines ?np?
and ?n?
as the most probable frame types.
After splitting the ?np?
probabilityover the different PP types in D2, a number of prominent PPs are left, the time indicat-ing umAcc and nachDat, mitDat referring to the begun event, anDat as date, and inDat asplace indicator.
It is obvious that not all PPs are argument PPs, but adjunct PPs alsorepresent a part of the verb behavior.
D3 illustrates that typical selectional preferencesfor beginner roles are Situation ?situation?, Zustand ?state?, Zeit ?time?, Sache ?thing?.D3 has the potential to indicate verb alternation behavior, for example, ?na(Situation)?refers to the same role for the direct object in a transitive frame as ?n(Situation)?
inan intransitive frame.
essen ?eat?
as an object-drop verb shows strong preferences forboth intransitive and transitive usage.
As desired, the argument roles are dominatedby Lebewesen ?creature?
for ?n?
and ?na?
and Nahrung ?food?
for ?na?.
fahren ?drive?chooses typical manner of motion frames (?n,?
?np,?
?na?)
with the refining PPs beingdirectional (inAcc, zuDat, nachDat) or referring to a means of motion (mitDat, inDat, aufDat).The selectional preferences show correct alternation behavior: Lebewesen ?creature?
inthe object drop case for ?n?
and ?na,?
Sache ?thing?
in the inchoative/causative case for?n?
and ?na?.In addition to the absolute verb descriptions above, a simple smoothing techniqueis applied to the feature values.
The goal of smoothing is to create more uniformdistributions, especially with regard to adjusting zero values, but also for assimilatinghigh and low frequencies and probabilities.
The smoothed distributions are particularlyinteresting for distributions with a large number of features, since they typically contain1 Since GermaNet had not been completed when we used the hierarchy, we manually added a fewhypernym definitions.168Schulte im Walde Induction of German Semantic Verb ClassesTable 1Example distributions of German verbs.DistributionVerb D1 D2 D3beginnen np 0.43 n 0.28 n(Situation) 0.12?begin?
n 0.28 np:umAcc 0.16 np:umAcc(Situation) 0.09ni 0.09 ni 0.09 np:mitDat(Situation) 0.04na 0.07 np:mitDat 0.08 ni(Lebewesen) 0.03nd 0.04 na 0.07 n(Zustand) 0.03nap 0.03 np:anDat 0.06 np:anDat(Situation) 0.03nad 0.03 np:inDat 0.06 np:inDat(Situation) 0.03nir 0.01 nd 0.04 n(Zeit) 0.03ns-2 0.01 nad 0.03 n(Sache) 0.02xp 0.01 np:nachDat 0.01 na(Situation) 0.02essen na 0.42 na 0.42 na(Lebewesen) 0.33?eat?
n 0.26 n 0.26 na(Nahrung) 0.17nad 0.10 nad 0.10 na(Sache) 0.09np 0.06 nd 0.05 n(Lebewesen) 0.08nd 0.05 ns-2 0.02 na(Lebewesen) 0.07nap 0.04 np:aufDat 0.02 n(Nahrung) 0.06ns-2 0.02 ns-w 0.01 n(Sache) 0.04ns-w 0.01 ni 0.01 nd(Lebewesen) 0.04ni 0.01 np:mitDat 0.01 nd(Nahrung) 0.02nas-2 0.01 np:inDat 0.01 na(Attribut) 0.02fahren n 0.34 n 0.34 n(Sache) 0.12?drive?
np 0.29 na 0.19 n(Lebewesen) 0.10na 0.19 np:inAcc 0.05 na(Lebewesen) 0.08nap 0.06 nad 0.04 na(Sache) 0.06nad 0.04 np:zuDat 0.04 n(Ort) 0.06nd 0.04 nd 0.04 na(Sache) 0.05ni 0.01 np:nachDat 0.04 np:inAcc(Sache) 0.02ns-2 0.01 np:mitDat 0.03 np:zuDat(Sache) 0.02ndp 0.01 np:inDat 0.03 np:inAcc(Lebewesen) 0.02ns-w 0.01 np:aufDat 0.02 np:nachDat(Sache) 0.02persuasive zero values and severe outliers.
Chen and Goodman (1998) present a conciseoverview of smoothing techniques, with specific emphasis on language modeling.
Wedecided to apply the smoothing algorithm referred to as additive smoothing: The smooth-ing is performed simply by adding 0.5 to all verb features, that is, the joint frequencyof each verb v and feature xi is changed by freq?
(v, xi) = freq(v, xi) + 0.5.
The total verbfrequency is adapted to the changed feature values, representing the sum of all verbfeature values: vfreq?
=?i freq?
(v, xi).
Smoothed probability values are based on thesmoothed frequency distributions.2.3 Clustering Algorithm and Evaluation TechniquesClustering is a standard procedure in multivariate data analysis.
It is designed toallow exploration of the inherent natural structure of the data objects, where objectsin the same cluster are as similar as possible and objects in different clusters are asdissimilar as possible.
Equivalence classes induced by the clusters provide a means for169Computational Linguistics Volume 32, Number 2generalizing over the data objects and their features.
The clustering of the German verbsis performed by the k-means algorithm, a standard unsupervised clustering techniqueas proposed by Forgy (1965).
With k-means, initial verb clusters are iteratively reorgan-ized by assigning each verb to its closest cluster and recalculating cluster centroids untilno further changes take place.
Applying the k-means algorithm assumes (1) that verbsare represented by distributional vectors and (2) that verbs that are closer to each otherin a mathematically defined way are also more similar to each other in a linguisticway.
k-Means depends on the following parameters: (1) The number of clusters is notknown beforehand, so the clustering experiments investigate this parameter.
Relatedto this parameter is the level of semantic concept: The more verb clusters are found,the more specific the semantic concept, and vice versa.
(2) k-means is sensitive to theinitial clusters, so the initialization is varied according to how much preprocessing weinvest: Both random clusters and hierarchically preprocessed clusters are used as initialclusters for k-means.
In the case of preprocessed clusters, the hierarchical clusteringis performed as bottom-up agglomerative clustering with the following criteria formerging the clusters: single linkage (minimal distance between nearest neighbor verbs),complete linkage (minimal distance between furthest neighbor verbs), average distancebetween verbs, distance between cluster centroids, and Ward?s method (minimizing thesum of squares when merging clusters).
The merging method influences the shape ofthe clusters; for example, single linkage causes a chaining effect in the shape of theclusters, and complete linkage creates compact clusters.
(3) In addition, there are severalpossibilities for defining the similarity between distributional vectors.
But which bestfits the idea of verb similarity?
Table 2 presents an overview of relevant similaritymeasures that are applied in the experiments.
x and y refer to the verb object vectors,their subscripts to the verb feature values.
The Minkowski metric can be applied tofrequencies and probabilities.
It is a generalization of the two well-known instancesq = 1 (Manhattan distance) and q = 2 (Euclidean distance).
The Kullback?Leibler divergence(KL) is a measure from information theory that determines the inefficiency of assuminga model probability distribution given the true distribution (Cover and Thomas 1991).The KL divergence is not defined in case yi = 0, so the probability distributions needto be smoothed.
Two variants of KL, information radius and skew divergence, perform adefault smoothing.
Both variants can tolerate zero values in the distribution becausethey work with a weighted average of the two distributions compared.
Lee (2001) hasshown that the skew divergence is an effective measure for distributional similarity inNLP.
Similarly to Lee?s method, we set the weight w for the skew divergence to 0.9.
Thecosine measures the similarity of the two object vectors x and y by calculating the cosineof the angle between the feature vectors.
The cosine measure can be applied to frequencyand probability values.
For a detailed description of hierarchical clustering techniquesand an intuitive interpretation of the similarity measures, the reader is referred to, forexample, Kaufman and Rousseeuw (1990).There is no agreed standard method for evaluating clustering experiments andresults, but a variety of evaluation measures from diverse areas such as theoreticalstatistics, machine vision, and Web-page clustering are generally applicable.
We usedthe following two measures for the evaluation: (1) Hatzivassiloglou and McKeown(1993) define and evaluate a cluster analysis of adjectives, based on common clustermembership of object pairs in the clustering C and the manual classification M. Recalland precision numbers are calculated in the standard way, with true positives thenumber of common pairs in M and C, false positives the number of pairs in C, butnot M, and false negatives the number of pairs in M, but not C. We use the f -score pairF(as harmonic mean between recall and precision), which provides an easy to understand170Schulte im Walde Induction of German Semantic Verb ClassesTable 2Data similarity measures.Measure DefinitionMinkowski metric / Lq norm Lq(x, y) =q?
?ni=1 |xi ?
yi|qManhattan distance / L1 norm L1(x, y) =?ni=1 |xi ?
yi|Euclidean distance / L2 norm L2(x, y) =?
?ni=1(xi ?
yi)2KL divergence / relative entropy D(x||y) =?ni=1 xi ?
logxiyiInformation radius IRad(x, y) = D(x|| x+y2 ) + D(y||x+y2 )Skew divergence Skew(x, y) = D(x||w ?
y + (1 ?
w) ?
x)Cosine cos(x, y) =?ni=1 xi ?
yi?
?ni=1 x2i ??
?ni=1 y2ipercentage.
(2) The adjusted Rand index is a measure of agreement versus disagreementbetween object pairs in clusterings that provides the most appropriate reference toa null model (Hubert and Arabie 1985); cf.
equation (1).
The agreement in the twopartitions is represented by a contingency table C ?
M: tij denotes the number of verbscommon to classes Ci in the clustering partition C and Mj in the manual classificationM; the marginals ti.
and t.j refer to the number of objects in Ci and Mj, respectively; theexpected number of common object pairs attributable to a particular cell ?Ci, Mj?
in thecontingency table is defined by(ti.2)(t.j2)/(n2).
The upper bound for Randadj is 1, the lowerbound is mostly 0, with only extreme cases below zero.Randadj(C, M) =?i,j(tij2)?
?i (ti.2 )?j (t.j2 )(n2)12 (?i(ti.2)+?j(t.j2)) ?
?i (ti.2 )?j (t.j2 )(n2)(1)The above two measures were chosen as a result of comparing various evaluationmeasures and their properties with respect to the linguistic task (Schulte im Walde2003a, chapter 4).3.
Preliminary Clustering ExperimentsThe 168 German verbs are associated with distributional vectors over frame types andassigned to initial clusters.
Then k-means is allowed to run for as many iterations asit takes to reach a fixed point, and the resulting clusters are interpreted and evaluatedagainst the manual classes.
The verbs are described by D1?D3, and each level refers tofrequencies and probabilities, with original and smoothed values.
The initial clustersfor k-means are generated either randomly or by a preprocessing cluster analysis, thatis, hierarchical clustering as described in Section 2.3.
For random cluster initializationthe verbs are randomly assigned to a cluster, with cluster numbers between 1 and thenumber of manual classes.
The experiments are performed with the number of k clustersbeing fixed to the number of gold standard classes (43); optimization of the number ofclusters is addressed in Section 3.4.171Computational Linguistics Volume 32, Number 23.1 Baseline and Upper BoundThe experiment baseline refers to 50 random clusterings: The verbs are randomly as-signed to a cluster (with a cluster number between 1 and the number of manual classes),and the resulting clustering is evaluated by the evaluation measures.
The baseline valueis the average value of the 50 repetitions.
The upper bound of the experiments (the?optimum?)
refers to the evaluation values on the manual classification; the manualclassification is adapted before calculating the upper bound by randomly deletingadditional senses (i.e., more than one sense) of a verb, so as to leave only one sensefor each verb, since k-means as a hard clustering algorithm cannot model ambiguity.Table 3 lists the baseline and upper bound values for the clustering experiments.3.2 Experiment ResultsThe following tables present the results of the clustering experiments.
Tables 4 to 7each concentrate on one technical parameter of the clustering process; Tables 8 to 10then focus on performing clustering with a fixed parameter set, in order to vary thelinguistically interesting parameters concerning the feature choice for the verbs.
Allsignificance tests have been performed with ?2, df = 1, ?
= 0.05.Table 4 illustrates the effect of the distribution units (frequencies and probabilities)on the clustering result.
The experiments use distributions on D1 and D2 with randomand preprocessed initialization, and the cosine as similarity measure (since it worksfor both distribution units).
To summarize the results, neither the differences betweenfrequencies and probabilities nor between original and smoothed values are significant.Table 5 illustrates the usage of different similarity measures.
As before, the exper-iments are performed for D1 and D2 with random and preprocessed initialization.The similarity measures are applied to the relevant probability distributions (as thedistribution unit that can be used for all measures).
The tables point out that thereis no best-performing similarity measure in the clustering processes.
On the largerfeature set, the Kullback?Leibler variants information radius and skew divergence tendto outperform all other similarity measures.
In fact, the skew divergence is the onlymeasure that shows significant differences for some parameter settings, as compared toall other measures except information radius.
In further experiments, we will thereforeconcentrate on the two Kullback?Leibler variants.Tables 6 and 7 compare the effects of varying the initialization of the k-meansalgorithm.
The experiments are performed for D1 and D2 with probability distrib-utions, using the similarity measures information radius and skew divergence.
Forrandom and hierarchical initialization, we cite both the evaluation scores for thek-means initial cluster analysis (i.e., the output clustering from the random assignmentor the preprocessing hierarchical analysis), and for the k-means result.
The manualcolumns in the tables refer to a cluster analysis where the initial clusters provided toTable 3k-means experiment baseline and upper bound.Evaluation Baseline OptimumPairF 2.08 95.81Randadj ?0.004 0.909172Schulte im Walde Induction of German Semantic Verb ClassesTable 4Comparing distributions on D1 and D2.Distribution: D1 Distribution: D2Probability Frequency Probability FrequencyEval Initial Original Smoothed Original Smoothed Original Smoothed Original SmoothedPairF Random 12.67 12.72 14.06 14.14 14.98 15.37 14.82 15.07H-Ward 11.40 11.70 11.56 11.37 10.57 13.71 11.65 9.98Randa Random 0.090 0.090 0.102 0.102 0.104 0.113 0.107 0.109H-Ward 0.079 0.081 0.080 0.076 0.065 0.096 0.075 0.056Table 5Comparing similarity measures on D1 and D2.Similarity MeasureD1 D2Eval Initial Cos L1 Eucl IRad Skew Cos L1 Eucl IRad SkewPairF Random 12.67 13.11 13.85 14.19 14.13 14.98 15.20 16.10 16.15 18.01H-Ward 11.40 13.65 12.88 13.07 12.64 10.57 15.51 13.11 17.49 19.30Randa Random 0.090 0.094 0.101 0.101 0.105 0.104 0.109 0.123 0.118 0.142H-Ward 0.079 0.099 0.093 0.097 0.094 0.065 0.116 0.092 0.142 0.158k-means are the manual classification, that is, the gold standard.
An optimal clusteranalysis should realize the ?perfect?
clustering and not perform any reorganization ofthe clusters.
In the experiments, k-means does perform iterations, so the clustering resultis suboptimal.
This finding is caused by the syntax?semantics mismatches, which wedeliberately included in the definition of the gold standard (recall, e.g., that unterstu?tzenis syntactically very different compared to the other three Support verbs).
In addition,the results not only show that the feature sets are suboptimal, but also that the lossin quality is less for the linguistically refined feature level D2 compared to D1, as wewould have hoped.
For random clustering initialization to k-means, the tables presentboth the best and the average clustering results.
The best results are paired with theevaluation of their initial clusters, that is, the random clusterings.
As the tables show, theinitial clusters receive low evaluation scores.
Typically, the clusterings consist of clusterswith rather homogeneous numbers of verbs, but the perturbation within the clustersis high, as expected.
k-means is able to cope with the high degree of perturbation:The resulting clusters improve significantly and are comparable with those based onpreprocessed hierarchical clustering; this competitiveness vanishes with an increasingnumber of features.
The average values of the random initialization experiments areclearly below the best ones, but not significantly different.
Cluster analyses as based onagglomerative hierarchical clustering with single-linkage amalgamation are evaluated aspoor compared to the gold standard.
This result is probably due to the chaining effectin the clustering, which is characteristic for single linkage; the effect is observable inthe analysis, which typically contains one very large cluster and many clusters withfew verbs, mostly singletons.
k-means obviously cannot compensate for this strongbias in cluster sizes (and their respective centroids); the reorganization improves theclusterings, but the result is still worse than for any other initialization.
With averagedistance and centroid distance amalgamation, both the clusterings and the evaluationresults are less extreme than with single linkage since the chaining effect is smoothed.173Computational Linguistics Volume 32, Number 2Table 6Comparing clustering initializations on D1.k-means InitializationRandomEval Distance Manual Best AvgPairF IRad 18.56 2.16 ?
14.19 11.78Skew 20.00 1.90 ?
14.13 12.17Randa IRad 0.150 ?0.004 ?
0.101 0.078Skew 0.165 ?0.005 ?
0.105 0.083k-means InitializationHierarchicalEval Distance Single Complete Average Centroid WardPairF IRad 4.80 ?
12.73 9.43 ?
10.16 10.83 ?
11.33 8.77 ?
11.88 12.76 ?
13.07Skew 4.81 ?
13.04 11.50 ?
11.00 11.68 ?
11.41 8.83 ?
11.45 12.44 ?
12.64Randa IRad 0.000 ?
0.088 0.055 ?
0.065 0.067 ?
0.072 0.039 ?
0.079 0.094 ?
0.097Skew 0.000 ?
0.090 0.077 ?
0.072 0.075 ?
0.073 0.041 ?
0.072 0.092 ?
0.094The overall results are better than for single linkage, but only slightly improved byk-means.
Hierarchical clusters as based on complete-linkage amalgamation are morecompact, and result in a closer fit to the gold standard than the previous methods.The hierarchical initialization is only slightly improved by k-means; in some casesthe k-means output is worse than its hierarchical initialization.
Ward?s method seemsto work best on hierarchical clusters and k-means initialization.
The cluster sizes aremore balanced and correspond to compact cluster shapes.
As for complete linkage,k-means improves the clusterings only slightly; in some cases the k-means output isworse than its hierarchical initialization.
A cluster analysis based on Ward?s hierarchicalclusters performs best of all the applied methods, especially with an increasing numberof features.
The similarity of Ward?s clusters (and similarly complete linkage clusters)Table 7Comparing clustering initializations on D2.k-means InitializationRandomEval Distance Manual Best AvgPairF IRad 40.23 1.34 ?
16.15 13.37Skew 47.28 2.41 ?
18.01 14.07Randa IRad 0.358 0.001 ?
0.118 0.093Skew 0.429 ?0.002 ?
0.142 0.102k-means InitializationHierarchicalEval Distance Single Complete Average Centroid WardPairF IRad 5.06 ?
11.12 15.37 ?
14.44 10.50 ?
10.64 9.16 ?
12.90 17.86 ?
17.49Skew 5.20 ?
10.64 15.21 ?
13.81 10.02 ?
10.02 9.04 ?
10.91 15.86 ?
15.23Randa IRad 0.003 ?
0.063 0.114 ?
0.105 0.059 ?
0.060 0.045 ?
0.082 0.145 ?
0.142Skew 0.004 ?
0.063 0.115 ?
0.102 0.054 ?
0.054 0.042 ?
0.064 0.158 ?
0.158174Schulte im Walde Induction of German Semantic Verb Classesand k-means is not by chance, since these methods aim to optimize the same criterion,the sum of distances between the verbs and their respective cluster centroids.
Notethat for D2, Ward?s method actually significantly outperforms all other initializationmethods, complete linkage significantly outperforms all but Ward?s.
Between singlelinkage, average and centroid distance, there are no significant differences.
For D1, thereare no significant differences between the initializations.The low scores in the tables might be surprising to the reader, but they reflect thedifficulty of the task.
As mentioned before, we deliberately set high demands for thegold standard, especially with reference to the fine-grained, small classes.
Comparedto related work (cf.
Section 5), our results achieve lower scores because the task ismore difficult; for example, Merlo and Stevenson (2001) classify 60 verbs into 3 classes,and Siegel and McKeown (2000) classify 56 verbs into 2 classes, as compared to ourclustering, which assigns 168 verbs to 43 classes.
The following illustrations shouldprovide an intuition about the difficulty of the task:1.
In a set of additional experiments, a random choice of a reduced numberof 5/10/15/20 classes from the gold standard is performed.
The verbsfrom the respective gold standard classes are clustered with the optimalparameter set (see Table 8), which results in a pairwise f-score PairF of22.19%.
The random choice and the cluster analysis are repeated 20 timesfor each reduced gold standard size of 5/10/15/20 classes, and theaverage PairF is calculated: The results are 45.27/35.64/30.30/26.62%,respectively.
This shows that the clustering results are much better (withthe same kind of data and features and the same algorithm) when appliedto a smaller number of verbs and classes.2.
Imagine a gold standard of three classes with four members each, forexample, {{a, b, c, d}, {e, f, g, h}, {i, j, k, l}}.
If a cluster analysis of theseelements into three clusters resulted in an almost perfect choice of {{a, b,c, d, e}, {f, g, h}, {i, j, k, l}} where only e is assigned to a ?wrong?
class,the pairwise precision is 79%, the recall is 83%, and pairF is 81%, so thedecrease of pairF with only one mistake is almost 20%.
If another clusteranalysis resulted in a choice with just one more mistake such as {{a, b,c, d, e, i}, {f, g, h}, {j, k, l}} where i is also assigned to a ?wrong?
class, theresult decreases by almost another 20%, to a precision of 57%, a recall of67%, and pairF of 62%.
The results show how much impact a few mistakesmay have on the pairwise f-score of the results.In addition to defining a difficult task, we also chose strong evaluation measures:Evaluating pairs of objects results in lower numbers than evaluating the individualobjects.
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen,Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct clusterwith respect to the gold standard class of the majority of cluster members.
That is, ina first step each induced verb cluster is assigned a gold standard class according towhich class captures the majority of the cluster members.
In a second step, each verbin a cluster is evaluated as correct or wrong with respect to its gold standard class,and accuracy/purity of the whole clustering is calculated as the proportion of correctverbs divided by the total number of verbs.
If we applied this measure to our optimalclustering with a pairwise f-score PairF of 22.19%, we achieve an accuracy of 51.19%;if we applied the measure to the above random choices of gold standard classes with5/10/15/20 classes, we achieve accuracies of 68.20/60.73/57.82/55.48%.175Computational Linguistics Volume 32, Number 2The last series of experiments applies the algorithmic insights from the previousexperiments to a linguistic variation of parameters (cf.
Schulte im Walde 2003b).
Theverbs are described by probability distributions on different levels of linguistic infor-mation (frames, prepositional phrases, and selectional preferences).
A preprocessinghierarchical cluster analysis is performed by Ward?s method, and k-means is appliedto re-organize the clusters.
Similarities are measured by the skew divergence.
Table 8presents the first results comparing D1, D2, and D3, either on specified frame slots (?n,??na,?
?nd,?
?nad,?
?ns-dass?
), on all noun phrase slots (NP), or on all noun phrase andprepositional phrase slots (NP?PP).
The number of features in each experiment is givenin square brackets.
The table demonstrates that a purely syntactic verb description givesrise to a verb clustering clearly above the baseline.
Refining the coarse subcategorizationframes with prepositional phrases considerably improves the verb clustering results.Adding selectional preferences to the verb description further improves the clusteringresults, but the improvement is not as persuasive as in the first step, when refiningthe purely syntactic verb descriptions with prepositional information.
The differencebetween D1 and D2 is significant, but neither the difference between D2 and D3 (inany variation) nor the differences between the variants of D3 are significant.
In the caseof adding role information to all NP (and all PP) slots, the problem might be causedby sparse data, but for the linguistically chosen subset of argument slots we assumeadditional linguistic reasons are directly relevant to the clustering outcome.In order to choose the most informative frame roles for D3, we varied the selectionalpreference slots by considering only single slots for refinements, or small combinationsof argument slots.
The variations should provide insight into the contribution of slotsand slot combinations to the clustering.
The experiments are performed on probabilitydistributions for D3; all other parameters were chosen as above.
Table 9 shows thatrefining only a single slot (the underlined slot in the respective frame type) in additionto the D2 definitions results in little or no improvement.
There is no frame-slot typethat consistently improves results, but success depends on the parameter instantiation.The results do not match our linguistic intuitions: For example, we would expect thearguments in the two highly frequent intransitive ?na?
and transitive ?na?
frames withvariable semantic roles to provide valuable information with respect to their selectionalpreferences, but only those in ?na?
actually improve D2.
However, a subject in a transi-tive construction with a non-finite clause ?ni?, which is less variable with respect to verbsand roles, does work better than ?n?.
In Table 10, selected slots are combined to defineselectional preference information, for example, n/na means that the nominative slotin ?na?, and both the nominative and accusative slot in ?na?
are refined by selectionalpreferences.
It is obvious that the clustering effect does not represent a sum of itsparts, for example, both the information in ?na?
and in ?na?
improve Ward?s clusteringbased on D2 (cf.
Table 9), but it is not the case that ?na?
improves the clustering, too.Table 8Comparing feature descriptions.DistributionD1 D2 D3 D3 NP D3 NP?PPEval [38] [183] [288] [906] [2,726]PairF 12.64 18.81 22.19 19.29 21.11Randadj 0.094 0.151 0.182 0.158 0.176176Schulte im Walde Induction of German Semantic Verb ClassesTable 9Comparing selectional preference slot definitions.DistributionD3Eval D2 n na na nad nad nadPairF 18.81 16.22 21.15 20.19 17.82 15.13 19.48Randadj 0.151 0.125 0.176 0.164 0.144 0.115 0.161DistributionD3Eval D2 nd nd np ni nr ns-2 ns-dassPairF 18.81 18.88 17.92 16.77 18.26 17.22 15.55 19.29Randadj 0.151 0.152 0.143 0.133 0.148 0.136 0.121 0.156As in Table 9, there is no combination of selectional preference frame definitions thatconsistently improves the results.
On the contrary, some additional D3 informationmakes the result significantly worse, for example, ?nad?.
The specific combination ofselectional preferences as determined preexperimentally actually achieves the overallbest results, better than any other slot combination, and better than refining all NP slotsor refining all NP and all PP slots in the frame types (cf.
Table 8).3.3 Experiment InterpretationFor illustrative purposes, we present representative parts of the cluster analysis as basedon the following parameters: The clustering initialization is obtained from a hierarchicalanalysis of the German verbs (Ward?s amalgamation method), the number of clustersbeing the number of manual classes (43); the similarity measure is the skew divergence.The cluster analysis is based on the verb description on D3, with selectional roles forTable 10Comparing selectional preference frame definitions.DistributionD3Eval D2 n na n/na nad n/na/nadPairF 18.81 16.22 17.82 17.00 13.36 16.05Randadj 0.151 0.125 0.137 0.128 0.088 0.118DistributionD3Eval D2 nd n/na/nd n/na/nad/nd np/ni/nr/ns-2/ns-dassPairF 18.81 18.48 16.48 20.21 16.73Randadj 0.151 0.150 0.124 0.161 0.131177Computational Linguistics Volume 32, Number 2?n,?
?na,?
?nd,?
?nad,?
?ns-dass.?
We compare the clusters with the respective clusters byD1 and D2.
(a) nieseln regnen schneien ?
Weather(b) da?mmern ?
Weather(c) beginnen enden ?
Aspectbestehen existieren ?
Existenceliegen sitzen stehen ?
Positionlaufen ?
Manner of Motion: Locomotion(d) kriechen rennen ?
Manner of Motion: Locomotioneilen ?
Manner of Motion: Rushgleiten ?
Manner of Motion: Flotationstarren ?
Facial Expression(e) klettern wandern ?
Manner of Motion: Locomotionfahren fliegen segeln ?
Manner of Motion: Vehicleflie?en ?
Manner of Motion: Flotation(f) festlegen ?
Constitutionbilden ?
Productionerho?hen senken steigern vergro?
?ern verkleinern ?
Quantum Change(g) to?ten ?
Eliminationunterrichten ?
Teaching(h) geben ?
Transfer of Possession (Giving): GiftThe weather verbs in cluster (a) strongly agree in their syntactic expression on D1 anddo not need D2 or D3 refinements for an improved class constitution.
da?mmern in cluster(b) is ambiguous between a weather verb and expressing a sense of understanding;this ambiguity is already idiosyncratically expressed in D1 frames, so da?mmern is neverclustered together with the other weather verbs by D1?D3.
Manner of Motion, Existence,Position, and Aspect verbs are similar in their syntactic frame usage and thereforemerged together by D1, but adding PP information distinguishes the respective verbclasses: Manner of Motion verbs primarily demand directional PPs, Aspect verbs aredistinguished by patient mitDat and time and location prepositions, and Existence andPosition verbs are distinguished by locative prepositions, with Position verbs show-ing more PP variation.
The PP information is essential for distinguishing these verbclasses, and the coherence is partly destroyed by D3: Manner of Motion verbs (from thesubclasses Locomotion, Rotation, Rush, Vehicle, Flotation) are captured well by clusters (d)and (e), since they use common alternations, but cluster (c) merges Existence, Position,and Aspect verbs because verb-idiosyncratic demands on selectional roles destroy theD2 class demarcation.
Still, the verbs in cluster (c) are close in their (more general con-ceptual) semantics, with a common sense of (bringing into versus being in) existence.laufen fits into the cluster with its sense of ?function.?
Cluster (f) contains most verbsof Quantum Change, together with one verb of Production and Constitution each.
Thecommon conceptual level of this cluster therefore refers to a quantum change includingthe quantum change from zero to something (as for the two verbs festlegen, ?constitute,?and bilden, ?found?).
The verbs in this cluster typically subcategorize for a direct object,alternating with a reflexive usage, ?nr?
and ?npr?
with mostly aufAcc and umAcc.
The178Schulte im Walde Induction of German Semantic Verb Classesselectional preferences help to distinguish this cluster: The verbs agree in demanding athing or situation as subject, and various objects such as attribute, cognitive object, state,structure, or thing as object.
Without selectional preferences (on D1 and D2), the changeof quantum verbs are not found together with the same degree of purity.
There are verbsas in cluster (g) whose properties are correctly stated as similar by D1?D3, so a commoncluster is justified, but the verbs only have coarse common meaning components; in thiscase to?ten ?kill?
and unterrichten ?teach?
agree in an action of one person or institutiontowards another.
geben in cluster (h) represents a singleton.
Syntactically, this is causedby being the only verb with a strong preference for ?xa.?
From the meaning point ofview, this specific frame represents an idiomatic expression, only possible with geben.An overall interpretation of the clustering results gives insight into the relationshipbetween verb properties and clustering outcome.
(1) The fact that there are verbs thatare clustered semantically on the basis of their corpus-based and knowledge-basedempirical properties indicates (a) a relationship between the meaning components ofthe verbs and their behavior and (b) that the clustering algorithm is able to benefit fromthe linguistic descriptions and to abstract away from the noise in the distributions.
(2)Low-frequency verbs were a problem in the clustering experiments.
Their distributionsare noisier than those for more frequent verbs, so they typically constitute noisy clusters.
(3) As known beforehand, verb ambiguity cannot be modeled by the hard clusteringalgorithm k-means.
Ambiguous verbs were typically assigned either (a) to one of thecorrect clusters or (b) to a cluster whose verbs have distributions that are similar tothe ambiguous distribution, or (c) to a singleton cluster.
(4) The interpretation of theclusterings unexpectedly points to meaning components of verbs that have not beendiscovered by the manual classification.
An example verb is laufen, expressing not onlya Manner of Motion but also a kind of existence when used in the sense of operation.
Thediscovery effect should be more impressive with an increasing number of verbs, sincemanual judgement is more difficult, and also with a soft clustering technique, wheremultiple cluster assignment is enabled.
(5) In a similar way, the clustering interpretationexhibits semantically related verb classes, that is, verb classes that are separated inthe manual classification, but semantically merged in a common cluster.
For example,Perception and Observation verbs are related in that all the verbs express an observation,with the Perception verbs additionally referring to a physical ability, such as hearing.
(6)Related to the preceding issue, the manual verb classes as defined are demonstrated asdetailed and subtle.
Compared to a more general classification that would appropriatelymerge several classes, the clustering confirms that we defined a difficult task with subtleclasses.
We were aware of this fact but preferred a fine-grained classification, sinceit allows insight into verb and class properties.
In this way, verbs that are similar inmeaning are often clustered incorrectly with respect to the gold standard.To come to the main point, what exactly is the nature of the meaning?behaviorrelationship?
(1) Already a purely syntactic verb description allows a verb clusteringclearly above the baseline.
The result is a (semantic) classification of verbs that agree intheir syntactic frame definitions, for example, most of the Support verbs.
The clusteringfails for semantically similar verbs that differ in their syntactic behavior, for example,unterstu?tzen, which belongs to the Support verbs but demands an accusative ratherthan a dative object.
In addition, it fails for syntactically similar verbs that are clus-tered together even though they do not exhibit semantic similarity; for example, manyverbs from different semantic classes subcategorize for an accusative object, so they arefalsely clustered together.
(2) Refining the syntactic verb information with prepositionalphrases is helpful for the semantic clustering, not only in the clustering of verbs wherethe PPs are obligatory, but also in the clustering of verbs with optional PP arguments.179Computational Linguistics Volume 32, Number 2The improvement underlines the linguistic fact that verbs that are similar in theirmeaning agree either on a specific prepositional complement (e.g., glauben/denken anAcc)or on a more general kind of modification, for example, directional PPs for Manner ofMotion verbs.
(3) Defining selectional preferences for arguments improves the clusteringresults further, but the improvement is not as persuasive as when refining the purelysyntactic verb descriptions with prepositional information.
For example, selectionalpreferences help demarcate the Quantum Change class because the respective verbs agreein their structural as well as selectional properties.
But in the Consumption class, essenand trinken have strong preferences for a food object, whereas konsumieren allows awider range of object types.
In contrast, there are verbs that are very similar in theirbehavior, especially with respect to a coarse definition of selectional roles, but they donot belong to the same fine-grained semantic class, for example, to?ten and unterrichten.The effect could be due to (a) noisy or (b) sparse data, but the basic verb descriptionsappear reliable with respect to their desired linguistic content, and Table 8 illustratesthat even with little added information the effect exists (e.g., refining few arguments by15 selectional roles results in 253 instead of 178 features, so the magnitude of featurenumbers does not change).
Why do we encounter an indeterminism concerning theencoding and effect of verb features, especially with respect to selectional preferences?The meaning of verbs comprises both properties that are general for the respectiveverb classes, and idiosyncratic properties that distinguish the verbs from each other.As long as we define the verbs by those properties that represent the common parts ofthe verb classes, a clustering can succeed.
But by stepwise refining the verb descriptionand including lexical idiosyncrasy, the emphasis on the common properties vanishes.From a theoretical point of view, the distinction between common and idiosyncraticfeatures is obvious, but from a practical point of view there is no perfect choice forthe encoding of verb features.
The feature choice depends on the specific propertiesof the desired verb classes, and even if classes are perfectly defined on a commonconceptual level, the relevant level of behavioral properties of the verb classes mightdiffer.
Still, for a large-scale classification of verbs, we need to specify a combinationof linguistic verb features as a basis for the clustering.
Which combination do wechoose?
Both the theoretical assumption of encoding features of verb alternation as verbbehavior and the practical realization by encoding syntactic frame types, prepositionalphrases, and selectional preferences seem promising.
In addition, we aimed at a (ratherlinguistically than technically based) choice of selectional preferences that represents auseful compromise for the conceptual needs of the verb classes.
Therefore, this choice offeatures best utilizes the meaning?behavior relationship and will be applied in a large-scale clustering experiment (cf.
Section 4).3.4 Optimizing the Number of ClustersIt is not a goal of this article to optimize the number of clusters in the cluster analysis.We are not interested in the question of whether, for example, 40, 42, 43, or 45 clustersrepresent the best semantic classification of 168 verbs.
But there are two reasons whyit is interesting and relevant to investigate the properties of clusterings with respect todifferent numbers of clusters.
(1) The clustering methodology should basically work theway we expect it to work, that is, the evaluation of the results should show deficienciesfor extreme numbers of clusters, but (possibly several) optimal values for variousnumbers of clusters in between.
(2) Even if we do not check for an exact number ofclusters, we should check the magnitude of the number of clusters, since the clustering180Schulte im Walde Induction of German Semantic Verb Classesmethodology might be successful in capturing a rough verb classification with few verbclasses but not a fine-grained classification with many subtle distinctions.Figure 1 illustrates the clustering results for the series of cluster analyses asperformed by k-means with hierarchical clustering initialization (Ward?s method) onprobability distributions, with skew divergence as the similarity measure.
The featuredescription refers to D2.
The number of clusters is varied from 1 through the numberof verbs (168), and the results are evaluated by Randadj.
A range of numbers of clustersis determined as optimal (71) or near-optimal (approx.
58?78).
The figure demonstratesthat having performed experiments on the parameters for clustering, it is worthwhileexploring additional parameters: The optimal result is 0.188 for 71 clusters as comparedto 0.158 for 43 clusters reported previously.4.
Large-Scale Clustering ExperimentsSo far, all clustering experiments were performed on a small scale, preliminary set of168 manually chosen German verbs.
One goal of this article was to develop a clusteringmethodology with respect to the automatic acquisition of a large-scale German verbclassification.
We therefore apply the insights on the theoretical relationship betweenverb meaning and verb behavior and our findings regarding the clustering parametersto a considerably larger amount of verb data.We extracted all German verbs from our statistical grammar model that appearedwith an empirical frequency of between 500 and 10,000 in the training corpus (cf.Section 2.2).
This selection resulted in a total of 809 verbs, including 94 verbs fromthe preliminary set of 168 verbs.
We added the missing verbs from the preliminaryset, resulting in a total of 883 German verbs.
The feature description of the Germanverbs refers to the probability distribution over the coarse syntactic frame types, withFigure 1Varying the number of clusters (evaluation: Randadj).181Computational Linguistics Volume 32, Number 2prepositional phrase information on the 30 chosen PPs and selectional preferences forour empirically most successful combination ?n,?
?na,?
?nd,?
?nad,?
and ?ns-dass.?
As inprevious clustering experiments, the features are stepwise refined.
k-means is providedhierarchical clustering initialization (based on Ward?s method), with the similarity mea-sure being skew divergence.
The number of clusters is set to 100, which correspondsto an average of 8.83 verbs per cluster, that is, not too fine-grained clusters but stillpossible to interpret.
The preliminary set of 168 verbs is a subset of the large-scaleset in order to provide an ?auxiliary?
evaluation of the clustering results: Consideringonly the manually chosen verbs in the clustering result, this partial cluster analysis isevaluated against the gold standard of 43 verb classes.
Results were not expected tomatch the results of our clustering experiments using only the preliminary verb set,but to provide an indication of how different cluster analyses can be compared witheach other.Tables 11 to 13 present the clustering results for the large-scale verb set for D1?D3 inthe rightmost columns, citing the evaluation scores of the initial (hierarchical) clustersand the resulting k-means clusters.
The subset of the 168 gold standard verbs is scatteredover 72 of the 100 resulting clusters.
The results are compared to our previous resultsfor the 168 verbs in 43 clusters, and to the case where those 168 verbs are clusteredinto 72 hierarchical classes.
The large-scale clustering results once more confirm thegeneral insights (1) that the stepwise refinement of features improves the clustering and(2) that Ward?s hierarchical clustering is seldom improved by the k-means application.In addition, several of the large-scale cluster analyses were quite comparable with theclustering results using the small-scale set of verbs, especially when compared to 72clusters.In the following, we present example clusters from the optimal large-scale clusteranalysis (according to the above evaluation): Ward?s hierarchical cluster analysis basedon subcategorization frames, PPs, and selectional preferences, without running k-meanson the hierarchical clustering.
Some clusters are extremely good with respect to the se-mantic overlap of the verbs, some clusters contain a number of similar verbs mixed withsemantically different verbs, and for some clusters it is difficult to recognize commonelements of meaning.
The verbs that we think are semantically similar are marked inbold face.
(1) abschneiden ?cut off?, anziehen ?dress?, binden ?bind?, entfernen ?remove?,tunen ?tune?, wiegen ?weigh?
(2) aufhalten ?detain?, aussprechen ?pronounce?, auszahlen ?pay off?, durchsetzen?achieve?, entwickeln ?develop?, verantworten ?be responsible?, verdoppeln?double?, zuru?ckhalten ?keep away?, zuru?ckziehen ?draw back?, a?ndern?change?Table 11Large-scale clustering on D1.Small-Scale Large-ScaleEval 43 Clusters 72 Clusters 72 ClustersPairF 12.44 ?
12.64 10.83 ?
11.73 12.15 ?
12.88Randadj 0.092 ?
0.094 0.084 ?
0.091 0.094 ?
0.102182Schulte im Walde Induction of German Semantic Verb ClassesTable 12Large-scale clustering on D2.Small-Scale Large-ScaleEval 43 Clusters 72 Clusters 72 ClustersPairF 18.64 ?
18.81 17.56 ?
18.81 18.22 ?
16.96Randadj 0.148 ?
0.151 0.149 ?
0.161 0.152 ?
0.142Table 13Large-scale clustering on D3 with n/na/nd/nad/ns-dass.Small-Scale Large-ScaleEval 43 Clusters 72 Clusters 72 ClustersPairF 22.86 ?
22.19 19.47 ?
20.48 19.92 ?
15.06Randadj 0.190 ?
0.182 0.165 ?
0.174 0.170 ?
0.115(3) anho?ren ?listen?, auswirken ?affect?, einigen ?agree?, lohnen ?be worth?,verhalten ?behave?, wandeln ?promenade?
(4) abholen ?pick up?, ansehen ?watch?, bestellen ?order?, erwerben ?purchase?,holen ?fetch?, kaufen ?buy?, konsumieren ?consume?, verbrennen ?burn?,verkaufen ?sell?
(5) anschauen ?watch?, erhoffen ?wish?, vorstellen ?imagine?, wu?nschen ?wish?,u?berlegen ?think about?
(6) danken ?thank?, entkommen ?escape?, gratulieren ?congratulate?
(7) beschleunigen ?speed up?, bilden ?constitute?, darstellen ?illustrate?, decken?cover?, erfu?llen ?fulfil?, erho?hen ?raise?, erledigen ?fulfil?, finanzieren?finance?, fu?llen ?fill?, lo?sen ?solve?, rechtfertigen ?justify?, reduzieren?reduce?, senken ?lower?, steigern ?increase?, verbessern ?improve?,vergro?
?ern ?enlarge?, verkleinern ?make smaller?, verringern ?decrease?,verschieben ?shift?, verscha?rfen ?intensify?, versta?rken ?intensify?,vera?ndern ?change?
(8) ahnen ?guess?, bedauern ?regret?, befu?rchten ?fear?, bezweifeln ?doubt?,merken ?notice?, vermuten ?assume?, wei?en ?whiten?, wissen ?know?
(9) anbieten ?offer?, bieten ?offer?, erlauben ?allow?, erleichtern ?facilitate?,ermo?glichen ?make possible?, ero?ffnen ?open?, untersagen ?forbid?,veranstalten ?arrange?, verbieten ?forbid?
(10) argumentieren ?argue?, berichten ?report?, folgern ?conclude?, hinzufu?gen?add?, jammern ?moan?, klagen ?complain?, schimpfen ?rail?, urteilen ?judge?
(11) basieren ?be based on?, beruhen ?be based on?, resultieren ?result from?,stammen ?stem from?
(12) befragen ?interrogate?, entlassen ?release?, ermorden ?assassinate?, erschie?en?shoot?, festnehmen ?arrest?, to?ten ?kill?, verhaften ?arrest?183Computational Linguistics Volume 32, Number 2(13) beziffern ?amount to?, scha?tzen ?estimate?, veranschlagen ?estimate?
(14) entschuldigen ?apologize?, freuen ?be glad?, wundern ?be surprised?,a?rgern ?be annoyed?
(15) nachdenken ?think about?, profitieren ?profit?, reden ?talk?, spekulieren?speculate?, sprechen ?talk?, tra?umen ?dream?, verfu?gen ?decree?,verhandeln ?negotiate?
(16) mangeln ?lack?, nieseln ?drizzle?, regnen ?rain?, schneien ?snow?Clusters (1) to (3) are examples where the verbs do not share elements of meaning.In the overall cluster analysis, such semantically incoherent clusters tend to be ratherlarge, that is, with more than 15?20 verb members.
Clusters (4) to (7) are examplesof clusters where some of the verbs show overlap in meaning, but also contain con-siderable noise.
Cluster (4) mainly contains verbs of buying and selling, cluster (5)contains verbs of wishing, cluster (6) contains verbs of expressing approval, and cluster(7) contains verbs of quantum change.
Clusters (8) to (16) are examples of clusters wheremost or all verbs show a strong similarity in their semantic concept.
Cluster (8) containsverbs expressing a propositional attitude; the underlined verbs, in addition, indicate anemotion.
The only unmarked verb wei?en can be explained, since some of its inflectedforms are ambiguous with respect to their base verb: either wei?en or wissen, a verbthat belongs to the Aspect verb class.
The verbs in cluster (9) describe a scene wheresomebody or some situation makes something possible (in the positive or negativesense); the only exception verb is veranstalten.
The verbs in cluster (10) are connectedmore loosely, all referring to a verbal discussion, with the underlined verbs denoting anegative, complaining way of utterance.
In cluster (11) all verbs refer to a basis, in cluster(12) the verbs describe the process from arresting to releasing a suspect, and cluster (13)contains verbs of estimating an amount of money.
In cluster (14), all verbs except forentschuldigen refer to an emotional state (with some origin for the emotion).
The verbsin cluster (15) except for profitieren all indicate thought (with or without talking) abouta certain matter.
Finally in cluster (16), we can recognize the same weather verb clusteras in the previously discussed small-scale cluster analyses.We experimented with two variations in the clustering setup: (1) For the selectionof the verb data, we considered a random choice of German verbs in approximatelythe same magnitude of number of verbs (900 verbs plus the preliminary verb set),but without any restriction on the verb frequency.
The clustering results are?both onthe basis of the evaluation and on the basis of a manual inspection of the resultingclusters?much worse than in the preceding cluster analysis, since the large number oflow-frequency verbs destroys the clustering.
(2) The number of target clusters was setto 300 instead of 100, that is, the average number of verbs per cluster was 2.94 insteadof 8.83.
The resulting clusters are numerically slightly worse than in the precedingcluster analysis, but easier for inspection and therefore a preferred basis for a large-scale resource.
Several of the large, semantically incoherent clusters are split into smallerand more coherent clusters, and the formerly coherent clusters often preserved theirconstitution.
To present one example, the following cluster from the 100-cluster analysisanzeigen ?announce?, aufkla?ren ?clarify?, beeindrucken ?impress?, befreien ?free?,begeistern ?inspire?, beruhigen ?calm down?, entta?uschen ?disappoint?, retten?save?, schu?tzen ?protect?, sto?ren ?disturb?, u?berraschen ?surprise?, u?berzeugen?persuade?184Schulte im Walde Induction of German Semantic Verb Classesis split into the following four clusters from the 300-cluster analysis:(a) anzeigen ?announce?, aufkla?ren ?clarify?
(b) beeindrucken ?impress?, entta?uschen ?disappoint?, u?berraschen ?surprise?,u?berzeugen ?persuade?
(c) befreien ?free?, beruhigen ?calm down?, retten ?save?, schu?tzen ?protect?,sto?ren ?disturb?
(d) begeisternwhere cluster (a) shows a loose semantic coherence of declaration, the verbs in cluster(b) are semantically very similar and describe an emotional impact of somebody or asituation on a person, and the verbs in cluster (c) show a protective (and the negation:nonprotective) influence of one person towards another.Summarizing, the large-scale clustering experiment results in a mixture of semanti-cally coherent and incoherent verb classes.
Semantically incoherent verb classes andclustering mistakes need to be split into finer and more coherent clusters, or to befiltered from the classification.
Semantically coherent verb classes need little manualcorrection as a lexical resource.
Interestingly, the coherence in verb classes refers todifferent criteria on meaning coherence, such as synonymy (e.g., reduzieren ?reduce?
andverringern ?decrease?
), antonymy (e.g., reduzieren ?reduce?
and erho?hen ?raise?
), situationaloverlap (e.g., emotional state containing freuen ?be glad?
and a?rgern ?be annoyed?
), andparticipation in a common process/script (e.g., bestellen ?order?, kaufen ?buy?, verkaufen?sell?, and abholen ?pick up?).5.
Related WorkThe following paragraphs describe related classification and clustering experiments onthe automatic induction of verb classes.
The classifications refer to different class crite-ria, for example, aspectual properties (Siegel and McKeown 2000), syntactic categories(Merlo and Stevenson 2001; Merlo et al 2002; Tsang, Stevenson, and Merlo 2002), and?most similar to my approach?semantic categories (Schulte im Walde 2000; Joanis 2002).The soft clustering approaches indicate how we might extend our hard clustering toverb ambiguity, now that we have determined the relevant set of verb features.Siegel and McKeown (2000) used three supervised and one unsupervised machine-learning algorithm to perform an automatic aspectual classification of English verbs.
(1) For the supervised classification, 97,973 parsed sentences from medical dischargesummaries were used to extract frequencies for verbs on 14 linguistic indicators, suchas manner adverb, duration in PP, past tense, and perfect tense.
Logistic regression,decision tree induction, and genetic programming were applied to the verb data todistinguish states and events.
Comparing the ability of the learning methods to combinethe linguistic indicators was claimed to be difficult, as they rank differently dependingon the classification task and evaluation criteria.
Decision trees achieved an accuracyof 93.9%, as compared to the uninformed baseline of 83.8%.
(2) For the unsupervisedclustering, 14,038 distinct verb?object pairs of varying frequencies were extracted from75,289 parsed novel sentences.
A random partition of the set of verbs was improved bya hill-climbing method, which improved the partition by moving a verb to the clusterthat decreases the sum of distances most.
For a small set of 56 verbs whose frequencyin the verb?object pairs was larger than 50, Siegel and McKeown (2000) claimed onthe basis of an evaluation of 19 verbs that their clustering algorithm discriminated185Computational Linguistics Volume 32, Number 2event verbs from stative verbs.
Overall, they performed a comparably simpler task thanpresented in this article, since the aspectual class criteria can be defined more objectivelyand more clearly than semantic criteria based on situational similarity.
Their choiceof features delimited their class criteria well, and they were able to achieve excellentresults.In previous work on English, Schulte im Walde (2000) clustered 153 verbs into 30verb classes taken from Levin (1993), using unsupervised hierarchical clustering.
Theverbs were described by distributions over subcategorization frames as extracted frommaximum-probability parses using a robust statistical parser, and completed by assign-ing WordNet classes as selectional preferences to the frame arguments.
Using Levin?sverb classification as a basis for evaluation, 61% of the verbs were classified correctlyinto semantic classes.
The clustering was most successful when utilizing syntactic sub-categorization frames enriched with PP information; selectional preferences decreasedthe performance of the clustering approach.
The detailed encoding and therefore sparsedata made the clustering worse with the selectional preference information.Merlo and Stevenson (2001) presented an automatic classification of three types ofEnglish intransitive verbs, based on argument structure and crucially involving the-matic relations.
They selected 60 verbs with 20 verbs from each verb class, comprisingunergatives, unaccusatives, and object-drop verbs.
The verbs in each verb class showsimilarities with respect to their argument structure, in that they all can be used both astransitives and intransitives.
Therefore, argument structure alone does not distinguishthe classes, and subcategorization information is refined by thematic relations.
Merloand Stevenson defined verb features based on linguistic heuristics that describe thethematic relations between subject and object in transitive and intransitive verb usage.The features included heuristics for transitivity, causativity, animacy, and syntactic fea-tures.
For example, the degree of animacy of the subject argument roles was estimatedas the ratio of occurrences of pronouns to all subjects for each verb, based on theassumption that unaccusatives occur less frequently with an animate subject comparedto unergative and object-drop verbs.
Each verb was described by a five-feature vector,and the vector descriptions were fed into a decision tree algorithm.
Compared witha baseline performance of 33.9%, the decision trees classified the verbs into the threeclasses with an accuracy of 69.8%.
Further experiments demonstrated the contributionof the different features within the classification.
Compared to the current article, Merloand Stevenson (2001) performed a simpler task and classified a smaller number of 60verbs into only three classes.
The features of the verbs were restricted to those thatshould capture the basic differences between the verb classes, in line with the idea thatthe feature choice depends on the specific properties of the desired verb classes.
Butusing the same classification methodology for a large-scale experiment with an enlargednumber of verbs and classes faces more problems.
For example, Joanis (2002) reportedan extension of their work that used 802 verbs from 14 classes from Levin (1993).
He de-fined an extensive feature space with 219 core features (such as part of speech, auxiliaryfrequency, syntactic categories, and animacy as above) and 1,140 selectional preferencefeatures taken from WordNet.
As in Schulte im Walde (2000), the selectional preferencesdid not improve the clustering.
In recent work, Stevenson and Joanis (2003) comparedtheir supervised method for verb classification with semisupervised and unsupervisedtechniques.
In these experiments, they enlarged the number of gold standard Englishverb classes to 14 classes related to Levin classes, with a total of 841 verbs.
Low-frequency and ambiguous verbs were excluded from the classes.
They found that asemisupervised approach where the classifier was trained with five seed verbs fromeach verb class outperformed both a manual selection of features and the unsupervised186Schulte im Walde Induction of German Semantic Verb Classesapproach of Dash, Liu, and Yao (1997), which used an entropy measure to organize datainto a multidimensional space.The classification methodology from Merlo and Stevenson (2001) was applied tomultilinguality by Merlo et al (2002) and Tsang, Stevenson, and Merlo (2002).
Merloet al (2002) showed that the classification paradigm is applicable in languages otherthan English by using the same features as defined by Merlo and Stevenson (2001) forthe respective classification of 59 Italian verbs empirically based on the Parole corpus.The resulting accuracy is 86.4%.
In addition, they used the content of Chinese verbfeatures to refine the English verb classification, explained in more detail by Tsang,Stevenson, and Merlo (2002).
The English verbs were manually translated into Chineseand given part-of-speech tag features, passive particles, causative particles, and sublex-ical morphemic properties.
Verb tags and particles in Chinese are overt expressions ofsemantic information that is not expressed as clearly in English, and the multilingualset of features outperformed either set of monolingual features, yielding an accuracyof 83.5%.Pereira, Tishby, and Lee (1993) describe a hierarchical soft clustering method thatclusters words according to their distribution in particular syntactic contexts.
Theyused an application of their method to nouns appearing as direct objects of verbs.The clustering result was a hierarchy of noun clusters, where every noun belongs toevery cluster with a membership probability.
The initial data for the clustering processwere frequencies of verb?noun pairs in a direct object relationship, as extracted fromparsed sentences from the Associated Press news wire corpus.
On the basis of theconditional verb?noun probabilities, the similarity of the distributions was determinedby the Kullback?Leibler divergence.
The EM algorithm (Baum 1972) was used to learnthe hidden cluster membership probabilities, and deterministic annealing performedthe divisive hierarchical clustering.
The resulting class-based model can be utilized forestimating information for unseen events (cf.
Dagan, Lee, and Pereira 1999).Rooth et al (1999) produced soft semantic clusters for English that represent a clas-sification on verbs as well as on nouns.
They gathered distributional data for verb?nounpairs in specific grammatical relations from the British National Corpus.
The extractionwas based on a lexicalized probabilistic context-free grammar (Carroll and Rooth 1998)and contained the subject and object nouns for all intransitive and transitive verbs in theparses?a total of 608,850 verb?noun types.
Conditioning of the verbs and the nouns oneach other was done through hidden classes, and the joint probabilities of classes, verbs,and nouns were trained by the EM algorithm.
The resulting model defined conditionalmembership probabilities for each verb and noun in each class; for example, the class ofcommunicative action contains the most probable verbs ask, nod, think, shape, smile andthe most probable nouns man, Ruth, Corbett, doctor, woman.
The semantic classes wereutilized for the induction of a semantically annotated verb lexicon.6.
Conclusion and OutlookThis article presented a clustering methodology for German verbs whose results agreedwith a manual classification in many respects and should prove useful as automaticbasis for a large-scale clustering.
Without a doubt the cluster analysis needs manualcorrection and completion, but represents a plausible foundation.
Key issues of the clus-tering methodology concern linguistic criteria on the one hand, and technical criteria onthe other hand.Linguistic Criteria: The strategy of utilizing subcategorization frames, prepositionalinformation, and selectional preferences to define the verb features seems promising,187Computational Linguistics Volume 32, Number 2since the experiments illustrated a relation between the induced verb behavior andthe membership of the semantic verb classes.
In addition, each level of representationgenerated a positive effect on the clustering and improved upon the less informativelevel.
The experiments presented evidence for a linguistic limit on the usefulness of theverb features: The meaning of verbs comprises both (1) properties that are general forthe respective verb classes and (2) idiosyncratic properties that distinguish the verbsfrom each other.
As long as we define the verbs by those properties that represent thecommon parts of the verb classes, a clustering can succeed.
But by stepwise refiningthe verb description and including lexical idiosyncrasy, emphasis on the common prop-erties vanishes.
From the theoretical point of view, the distinction between commonand idiosyncratic features is obvious.
But from the practical point of view, featurechoice then depends on the definition of the verb classes, and this definition might varyaccording to the conceptual level and also according to the kind of semantic coherencecaptured by the class.
So far, we have concentrated on synonymy, but the large-scaleexperiment, in particular, discovered additional semantic relations within a verb class,such as participation in a process/script.
However, the investigated feature combinationwithin this article seems to be a useful starting point for verb description.Technical Criteria: We investigated the relationship between clustering idea, cluster-ing parameters, and clustering result in order to develop a clustering methodology thatis suitable for the demands of natural language.
The clustering initialization played animportant role: k-means needed compact, similarly-sized clusters in order to achieve alinguistically meaningful classification.
The linguistically most successful initial clus-ters were therefore based on hierarchical clustering with complete linkage or Ward?smethod, as the resulting clusters are comparable in size and correspond to compactcluster shapes.
The hierarchical clustering achieved more similar clustering outputsthan k-means, which is due to the similarity of the clustering methods with respect tothe common clustering criterion of optimizing the sum of distances between verbs andcluster centroids.
The similarity measure used in the clustering experiments proved tobe of secondary importance, since the differences in clustering due to varying the mea-sure were negligible.
For larger object and feature sets, Kullback?Leibler variants tendedto outperform other measures, confirming language-based results on distributionalsimilarity (Lee 2001).
Both frequencies and probabilities represented a useful basis forthe verb distributions.
The number of clusters played a role concerning the magnitudeof numbers: Inducing fine-grained clusters as given in the manual classification provedto be an ambitious goal because the feature distinction for the classes was also fine-grained.
Inducing coarse clusters provided a coarse classification that was subject toless noise and easier to manually correct.
The ?optimal?
number of clusters is always acompromise and depends on the purpose of the classes, for example, as a fine-grainedlexical resource, or for an NLP application.
In the latter case, the optimal numbershould be determined by automatic means, that is, by trying different magnitudesof cluster numbers, because the level of generalization depends on the purpose forthe abstraction.There are various directions for future research.
(1) The manual definition of theGerman semantic verb classes will be extended in order to include a greater numberand a larger variety of verb classes.
An extended classification would be useful as agold standard for further clustering experiments, and more generally as a resource forNLP applications.
(2) Low-frequency verbs require a specific handling in the clusteringprocedure: Both the small-scale and the large-scale experiments showed that the low-frequency verbs have a negative impact on the cluster coherence.
An alternative modelfor the low-frequency verbs might, for example, first take out of the cluster analysis188Schulte im Walde Induction of German Semantic Verb Classesthose verbs below a certain frequency cutoff, and then assign the left-out verbs to thenearest clusters.
The cluster assignment should also be special, for example, using verbfeatures restricted to the reliable features, that is, above a certain frequency threshold.For example, if we consider the D2 features of the low-frequency verb ekeln ?disgust?
(frequency: 31) with a minimum feature frequency of 2, we get a strong overlap with thedistinguishing features of the verb fu?rchten ?fear?.
Future work will address these issues.
(3) Possible features for describing German verbs will include any kind of informationthat helps to classify the verbs in a semantically appropriate way.
Within this article,we concentrated on defining the verb features with respect to alternation behavior.Other features that are relevant for describing the behavior of verbs are their auxiliaryselection and adverbial combinations.
In addition, if we try to address additional typesof semantic verb relations such as script-based relations, we will need to extend ourfeatures.
For example, Schulte im Walde and Melinger (2005) recently showed thatnouns in co-occurrence windows of verbs contribute to verb descriptions by encodingscene information, rather than intrasentential functions.
They proposed the integrationof window-based approaches into function-based approaches, a combination that hasnot yet been applied.
(4) Variations in the existing feature description are especiallyrelevant for the choice of selectional preferences.
The experiment results demonstratedthat the 15 conceptual GermaNet top levels are not sufficient for all verbs.
For example,the verbs to?ten and unterrichten require a finer version of selectional preferences in orderto be distinguished.
It is worthwhile either to find a more appropriate level of selectionalpreferences in WordNet or to apply a more sophisticated approach towards selectionalpreferences such as that of Li and Abe (1998), in order to determine a more flexiblechoice of selectional preferences.
(5) With respect to a large-scale classification of verbs,it will be interesting to apply classification techniques to the verb data.
This wouldrequire more data manually labeled with classes in order to train a classifier.
But theresulting classifier might abstract better than k-means over the different requirementsof the verb classes with respect to the feature description.
(6) As an extension of theexisting clustering, a soft clustering algorithm will be applied to the German verbs.Soft clustering enables us to assign verbs to multiple clusters and therefore addressthe phenomenon of verb ambiguity.
These clustering outcomes should be even moreuseful for discovering new verb meaning components and semantically related classes,compared with the hard clustering technique.
(7) The verb clusters as resulting fromthe cluster analysis will be used within an NLP application in order to prove theusefulness of the clusters.
For example, replacing verbs in a language model by therespective verb classes might improve the language model?s robustness and accuracy,as the class information provides more stable syntactic and semantic information thanthe individual verbs.Appendix A: Subcategorization Frame TypesThe syntactic part of the German verb behavior is captured by 38 subcategorizationframe types.
The frames comprise maximally three arguments.
Possible argumentsare nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns(r), prepositional phrases (p), expletive es (x), subordinated non-finite clauses (i), sub-ordinated finite clauses (s-2 for verb second clauses, s-dass for dass-clauses, s-ob forob-clauses, s-w for indirect wh-questions), and copula constructions (k).
The resultingframe types are listed in Table A.1, accompanied by annotated verb?second exampleclauses.
The German examples are provided with English glosses; in cases where theglosses are difficult to understand, an English translation is added.189Computational Linguistics Volume 32, Number 2Table A1Subcategorization frame types.Frame Type Examplen NatalienNatalieschwimmt.swimsna HansnHanssiehtseesseine Freundina.his girlfriendnd ErnHeglaubtbelievesden Leutendthe peoplenicht.notnp Die AutofahrernThe driversachtenwatch outbesondersespeciallyauf Kinderp.for childrennad AnnanAnnaversprichtpromisesihrem Vaterdher fatherein tolles Geschenka.a great presentnap Die Verka?uferinnThe salesladyhindertkeepsden Diebathe thiefam Stehlenp.from stealingndp Der ModeratornThe moderatordanktthanksdem Publikumdthe audiencefu?r sein Versta?ndnisp.for their understandingni Mein FreundnMy friendversuchttriesimmer,alwayspu?nktlich zu kommeni.in time to arrive?My friend always tries to arrive in time.
?nai ErnHeho?rthearsseine Mutterahis motherein Lied tra?llerni.a song sing?He hears his mother singing a song.
?ndi HelenenHeleneversprichtpromisesihrem Gro?vaterdher grandfatherihn bald zu besucheni.him soon to visitnr Die KindernThe childrenfu?rchtenare afraidsichr.themselvesnar Der UnternehmernThe businessmanerhoffthopessichrhimselfschnellen Fortschritta.quick progressndr SienSheschlie?tassociatessichrherselfnach 10 Jahrenafter 10 yearswiederagainder Kirchedthe churchan.withnpr Der PastornThe pastorhathassichrhimselfals der Kirche wu?rdigpto the church worthyerwiesen.provennir Die alte FraunThe old womenstelltimaginessichrherselfvor, den Preis zu gewinneni.the price to winx EsxItblitzt.lighteningsxa EsxTheregibtexistviele Bu?chera.many booksxd EsxItgrautterrifiesmird.me190Schulte im Walde Induction of German Semantic Verb ClassesTable A1(cont.
)Frame Type Examplexp EsxItgehtisum einen tollen Preis fu?r mein Sofap.about a great price for my sofaxr EsxItrechnetcalculatessichr.itself?It is worth it.
?xs-dass EsxIthei?t,says,dass Thomas sehr schlau ists?dass.that Thomas very intelligent isns-2 Der ProfessornThe professorhathasgesagt,said,er halte bald einen Vortrags?2.he gives soon a talknas-2 Der ChefnThe chefschnauztbawlsihnahiman,out,er sei ein Idiots?2.he is an idiotnds-2 ErnHesagttellsseiner Freundind,his girlfriend,sie sei zu krank zum Arbeitens?2.she is too ill to worknrs-2 Der KleinenThe boywu?nschtwishessichr,himself,das Ma?dchen bliebe bei ihms?2.the girl stays with himns-dass Der WinternWinterhathasschonalreadyangeku?ndigt,announced,dass er bald kommts?dass.that it soon arrivesnas-dass Der VaternThe fatherfordertrequestsseine Tochterahis daughterauf, dass sie verreists?dass.that she travelsnds-dass ErnHesagttellsseiner Geliebtend,his lover,dass er verheiratet ists?dassthat he married isnrs-dass Der KleinenThe boywu?nschtwishessichr,himself,dass seine Mutter bleibts?dass.that his mother staysns-ob Der ProfessornThe professorhathasgefragt,asked,ob die neue Forscherin interessant seis?obwhether the new researcher interesting isnas-ob AntonnAntonfragtasksseine Fraua,his wife,ob sie ihn liebts?ob.whether she him lovesnds-ob Der NachbarnThe neighborruftshoutsder Fraudthe womanzu, ob sie verreists?obwhether she travelsnrs-ob Der AltenThe old manwirdwillsichrhimselferinnern,remember,ob das Ma?dchen dort wars?ob.whether the girl there wasns-w Der KleinenThe boyhathasgefragt,asked,wann die Tante endlich ankommts?wwhen the aunt finally arrivesnas-w Der MannnThe manfragtasksseine Freundina,his girlfriend,warum sie ihn liebts?w.why she him lovesnds-w Der VaternThe fatherverra?ttellsseiner Tochterdhis daughternicht,not,wer zu Besuch kommts?w.who for a visit comesnrs-w Das Ma?dchennThe girlerinnertrememberssichr,herself,wer zu Besuch kommts?w.who for a visit comesk Der neue NachbarkThe new neighboristiseinaziemlichercompleteIdiot.idiot191Computational Linguistics Volume 32, Number 2AcknowledgmentsThe work reported here was performedwhile the author was a member of theDFG-funded PhD program ?Graduierten-kolleg?
Sprachliche Repra?sentationen und ihreInterpretation at the Institute for NaturalLanguage Processing (IMS), University ofStuttgart, Germany.
Many thanks to HelmutSchmid, Stefan Evert, Frank Keller, ScottMcDonald, Alissa Melinger, Chris Brew,Hinrich Schu?tze, Jonas Kuhn, and the twoanonymous reviewers for their valuablecomments on previous versions of thisarticle.ReferencesBaker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of the 17thInternational Conference on ComputationalLinguistics and the 36th Annual Meeting ofthe Association for Computational Linguistics,pages 86?90, Montreal, Canada.Baum, Leonard E. 1972.
An inequality andassociated maximization technique instatistical estimation for probabilisticfunctions of Markov processes.
Inequalities,III:1?8.Carroll, Glenn and Mats Rooth.
1998.
Valenceinduction with a head-lexicalized PCFG.In Proceedings of the 3rd Conference onEmpirical Methods in Natural LanguageProcessing, Granada, Spain.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
In Proceedings of the 14th NationalConference on Artificial Intelligence,Menlo Park, CA.Chen, Stanley and Joshua Goodman.
1998.An empirical study of smoothingtechniques for language modeling.Technical Report TR-10-98, Center forResearch in Computing Technology,Harvard University.Cover, Thomas M. and Joy A. Thomas.
1991.Elements of Information Theory.Telecommunications.
John Wiley & Sons,New York.Dagan, Ido, Lillian Lee, and FernandoPereira.
1999.
Similarity-based models ofword cooccurrence probabilities.
MachineLearning, 34(1?3):43?69.
Special Issue onNatural Language Learning.Dash, Manoranjan, Hua Liu, and Jun Yao.1997.
Dimensionality reduction forunsupervised data.
In Proceedings of the9th International Conference on Tools withArtificial Intelligence, pages 532?539,Newport Beach, CA.Dorr, Bonnie J.
1997.
Large-scale dictionaryconstruction for foreign language tutoringand interlingual machine translation.Machine Translation, 12(4):271?322.Dorr, Bonnie J. and Doug Jones.
1996.Role of word sense disambiguation inlexical acquisition: Predicting semanticsfrom syntactic cues.
In Proceedingsof the 16th International Conference onComputational Linguistics, pages 322?327,Copenhagen, Denmark.Erk, Katrin, Andrea Kowalski, andManfred Pinkal.
2003.
A corpus resourcefor lexical semantics.
In Proceedingsof the 5th International Workshop onComputational Semantics, Tilburg, TheNetherlands.Fellbaum, Christiane, editor.
1998.WordNet?An Electronic Lexical Database.Language, Speech, and Communication.MIT Press, Cambridge, MA.Fillmore, Charles J.
1977.
Scenes-and-framessemantics.
In Antonio Zampolli, editor,Linguistic Structures Processing, volume 59of Fundamental Studies in Computer Science.North Holland Publishing, Amsterdam.Fillmore, Charles J.
1982.
Frame Semantics.In Linguistics in the Morning Calm,pages 111?137, Hansin, Seoul, Korea.Fontenelle, Thierry, editor.
2003.
FrameNetand Frame Semantics, volume 16(3) ofInternational Journal of Lexicography.
OxfordUniversity Press.Forgy, Edward W. 1965.
Cluster analysis ofmultivariate data: Efficiency vs.interpretability of classifications.Biometrics, 21:768?780.Hamp, Birgit and Helmut Feldweg.1997.
GermaNet?A lexical-semanticNet for German.
In Proceedings of theACL Workshop on Automatic InformationExtraction and Building Lexical SemanticResources for NLP Applications, Madrid,Spain.Harris, Zellig.
1968.
Distributional structure.In Jerold J. Katz, editor, The Philosophyof Linguistics, Oxford Readings inPhilosophy.
Oxford University Press,pages 26?47.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1993.
Towards the automaticidentificaton of adjectival scales:Clustering adjectives according tomeaning.
In Proceedings of the 31st AnnualMeeting of the Associationfor Computational Linguistics,pages 172?182, Columbus.Hubert, Lawrence and Phipps Arabie.
1985.Comparing partitions.
Journal ofClassification, 2:193?218.192Schulte im Walde Induction of German Semantic Verb ClassesJoanis, Eric.
2002.
Automatic verbclassification using a general feature space.Master?s thesis, Department of ComputerScience, University of Toronto.Kaufman, Leonard and Peter J. Rousseeuw.1990.
Finding Groups in Data?AnIntroduction to Cluster Analysis.
Probabilityand Mathematical Statistics.
John Wiley &Sons, Inc., New York.Klavans, Judith L. and Min-Yen Kan. 1998.The role of verbs in document analysis.
InProceedings of the 17th InternationalConference on Computational Linguistics,pages 680?686, Montreal, Canada.Korhonen, Anna.
2002.
SubcategorizationAcquisition.
Ph.D. thesis, University ofCambridge, Computer Laboratory.Technical Report UCAM-CL-TR-530.Korhonen, Anna, Yuval Krymolowski, andZvika Marx.
2003.
Clustering polysemicsubcategorization frame distributionssemantically.
In Proceedings of the 41stAnnual Meeting of the Association forComputational Linguistics, pages 64?71,Sapporo, Japan.Kunze, Claudia.
2000.
Extension and use ofGermaNet, a lexical-semantic database.
InProceedings of the 2nd International Conferenceon Language Resources and Evaluation,pages 999?1002, Athens, Greece.Lapata, Maria.
1999.
Acquiring lexicalgeneralizations from corpora: A case studyfor diathesis alternations.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 397?404,College Park, MD.Lapata, Mirella and Chris Brew.
2004.
Verbclass disambiguation using informativepriors.
Computational Linguistics,30(1):45?73.Lee, Lillian.
2001.
On the effectiveness of theskew divergence for statistical languageanalysis.
In Artificial Intelligence andStatistics, pages 65?72.Levin, Beth.
1993.
English Verb Classes andAlternations.
The University of ChicagoPress.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.McCarthy, Diana.
2001.
Lexical Acquisition atthe Syntax-Semantics Interface: DiathesisAlternations, Subcategorization Frames andSelectional Preferences.
Ph.D. thesis,University of Sussex.Merlo, Paola and Suzanne Stevenson.
2001.Automatic verb classification based onstatistical distributions of argumentstructure.
Computational Linguistics,27(3):373?408.Merlo, Paola, Suzanne Stevenson, VivianTsang, and Gianluca Allaria.
2002.
Amultilingual paradigm for automaticverb classification.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics,pages 207?214, Philadelphia, PA.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1990.
Introduction toWordnet: An on-line lexical database.International Journal of Lexicography,3(4):235?244.Palmer, Martha, Dan Gildea, and PaulKingsbury.
2005.
The Proposition Bank:An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.Pereira, Fernando, Naftali Tishby, and LillianLee.
1993.
Distributional clustering ofEnglish words.
In Proceedings of the 31stAnnual Meeting of the Associationfor Computational Linguistics,pages 183?190, Columbus, OH.Pinker, Steven.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotatedlexicon via EM-based clustering.
InProceedings of the 37th Annual Meetingof the Association for ComputationalLinguistics, Maryland, MD.Saint-Dizier, Patrick.
1998.
Alternationsand verb semantic classes for French:Analysis and class formation.
In PatrickSaint-Dizier, editor, Predicative Formsin Natural Language and in LexicalKnowledge Bases.
Kluwer AcademicPublishers, Dordrecht.Schmid, Helmut.
2000.
LoPar: Design andimplementation.
Arbeitspapiere desSonderforschungsbereichs 340 LinguisticTheory and the Foundations of ComputationalLinguistics 149, Institut fu?r MaschinelleSprachverarbeitung, Universita?t Stuttgart.Schulte im Walde, Sabine.
2000.
Clusteringverbs semantically according to theiralternation behaviour.
In Proceedingsof the 18th International Conferenceon Computational Linguistics,pages 747?753, Saarbru?cken, Germany.Schulte im Walde, Sabine.
2002a.Evaluating verb subcategorisationframes learned by a German statisticalgrammar against manual definitionsin the Duden Dictionary.
In Proceedings193Computational Linguistics Volume 32, Number 2of the 10th EURALEX International Congress,pages 187?197, Copenhagen, Denmark.Schulte im Walde, Sabine.
2002b.
Asubcategorisation lexicon for Germanverbs induced from a lexicalised PCFG.
InProceedings of the 3rd Conference on LanguageResources and Evaluation, volume IV,pages 1351?1357, Las Palmas de GranCanaria, Spain.Schulte im Walde, Sabine.
2003a.
Experimentson the automatic induction of Germansemantic verb classes.
Ph.D. thesis, Institutfu?r Maschinelle Sprachverarbeitung,Universita?t Stuttgart.
Published as AIMSReport 9(2).Schulte im Walde, Sabine.
2003b.Experiments on the choice of features forlearning verb classes.
In Proceedings of the10th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 315?322, Budapest, Hungary.Schulte im Walde, Sabine and Chris Brew.2002.
Inducing German semantic verbclasses from purely syntacticsubcategorisation information.
InProceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics, pages 223?230,Philadelphia, PA.Schulte im Walde, Sabine and AlissaMelinger.
2005.
Identifying semanticrelations and functional properties ofhuman verb associations.
In Proceedings ofthe joint Conference on Human LanguageTechnology and Empirial Methods in NaturalLanguage Processing, pages 612?619,Vancouver, Canada.Schumacher, Helmut.
1986.
Verben in Feldern.de Gruyter, Berlin.Siegel, Eric V. and Kathleen R. McKeown.2000.
Learning methods to combinelinguistic indicators: Improving aspectualclassification and Revealing LinguisticInsights.
Computational Linguistics,26(4):595?628.Stevenson, Suzanne and Eric Joanis.
2003.Semi-supervised verb class discoveryusing noisy features.
In Proceedings of the7th Conference on Natural LanguageLearning, pages 71?78, Edmonton, Canada.Tsang, Vivian, Suzanne Stevenson, and PaolaMerlo.
2002.
Crosslinguistic transfer inautomatic verb classification.
InProceedings of the 19th InternationalConference on Computational Linguistics,pages 1023?1029, Taipei, Taiwan.Va?zquez, Gloria, Ana Ferna?ndez, IreneCastello?n, and Mar?
?a Antonia Mart??.
2000.Clasificacio?n verbal: Alternancias de dia?tesis.Number 3 in Quaderns de Sintagma.Universitat de Lleida.Vossen, Piek.
2004.
EuroWordNet: Amultilingual database of autonomous andlanguage-specific wordnets connected viaan inter-lingual-index.
International Journalof Lexicography, 17(2):161?173.194
