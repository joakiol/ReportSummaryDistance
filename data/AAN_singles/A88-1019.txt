A Stochastic Parts Program and Noun Phrase Parser for Unrestricted TextKenneth Ward ChurchBell Laboratories600 Mountain Ave.Murray Hill, N.J., USA201-582-5325alice!k-wcIt is well-known that part of speech depends oncontext.
The word "table," for example, can bea verb in some contexts (e.g., "He will table themotion") and a noun in others (e.g., "The tableis ready").
A program has been written whichtags each word in an input sentence with themost likely part of speech.
The programproduces the following output for the two"table" sentences just mentioned:?
He/PPS will/lVlD table/VB the/ATmotion/NN ./.?
The/AT table\]NN is/BEZ ready/J/./.
(PPS = subject pronoun; MD = modal; V'B =verb (no inflection); AT = article; NN = noun;BEZ ffi present 3rd sg form of "to be"; Jl =adjective; notation is borrowed from \[Francis andKucera, pp.
6-8\])Part of speech tagging is an important practicalproblem with potential applications in manyareas including speech synthesis, speechrecognition, spelling correction, proof-reading,query answering, machine translation andsearching large text data bases (e.g., patents,newspapers).
The author is particularlyinterested in speech synthesis applications, whereit is clear that pronunciation sometimes dependson part of speech.
Consider the foUowing threeexamples where pronunciation depends on partof speech.
First, there are words like "wind"where the noun has a different vowel than theverb.
That is, the noun "wind" has a shortvowel as in "the wind is strong," whereas theverb "wind" has a long vowel as in "Don'tforget to wind your watch."
Secondly, thepronoun "that" is stressed as in "Did you seeTHAT?"
unlike the complementizer "that," asin "It is a shame that he's leaving."
Thirdly,note the difference between "oily FLUID" and"TRANSMISSION fluid"; as a general rule, anadjective-noun sequence such as "oily FLUID"is typically stressed on the fight whereas anoun-noun sequence such as "TRANSMISSIONfluid" is typically stressed on the left.
These arebut three of the many constructions which wouldsound more natural if the synthesizer had accessto accurate part of speech information.Perhaps the most important application oftagging programs is as a tool for future research.A number of large projects such as \[Cobufld\]have recently been collecting large corpora (10-1000 million words) in order to better describehow language is actually used in practice:"For the first time, a dictionary has beencompiled by the thorough examination ofrepresentative group of English texts, spokenand written, running to many millions ofwords.
This means that in addition to all thetools of the conventional dictionary makers...the dictionary is based on hard, measureableevidence."
\[Cobuild, p. xv\]It is likely that there will be more and moreresearch projects collecting larger and largercorpora.
A reliable parts program might greatlyenhance the value of these corpora to many ofthese researchers.The program uses a linear time dynamicprogramming algorithm to find an assignment ofparts of speech to words that optimizes theproduct of (a) lexical probabilities (probability ofobserving part of speech i given word j), and (b)contextual probabilities (probability of observingpart of speech i given k previous parts ofspeech).
Probability estimates were obtained bytraining on the Tagged Brown Corpus \[Francisand Kucera\], a corpus of approximately1,000,000 words with part of speech tagsassigned laboriously by hand over many years.Program performance is encouraging (95-99%"correct", depending on the definition of"correct").
A small 400 word sample ispresented in the Appendix, and is judged to be99.5% correct.
It is surprising that a local"bottom-up" approach can perform so well.Most errors are attributable to defects in thelexicon; remarkably few errors are related to theinadequacies of the extremely over-simplifiedgrammar (a trigram model).
Apparently, "longdistance" dependences are not very important, at136least most of the time.One might have thought that ngram modelsweren't adequate for the task since it is well-known that they are inadequate for determininggrammaticality:"We find that no finite-state Markov processthat produces ymbols with transition fromstate to state can serve as an Englishgrammar.
Furthermore, the particularsubclass of such processes that produce n-order statistical approximations to English donot come closer, with increasing n, tomatching the output of an English granunar.
"\[Chomsky, p. 113\]Chomsky's conclusion was based on theobservation that constructions such as:?
If St then $2.?
Either $3, or $4.?
The man who said that S s, is arriving today.have long distance dependencies that span acrossany fixed length window n. Thus, ngrammodels are clearly inadequate for many naturallanguage applications.
However, for the taggingapplication, the ngram approximation may be ac-ceptable since long distance dependencies do notseem to be very important.Statistical ngram models were quite popular inthe 1950s, and have been regaining popularityover the past few yeats.
The IBM speech groupis perhaps the strongest advocate of ngrammethods, especially in other applications such asspeech recognition.
Robert Mercer (privatecommunication, 1982) has experimented with thetagging application, using a restricted corpus(laser patents) and small vocabulary (1000words).
Another group of researchers workingin Lancaster around the same time, Leech,Garside and Atwell, also found ngram modelshighly effective; they report 96.7% success inautomatically tagging the LOB Corpus, using abigram model modified with heuristics to copewith more important trigrams.
The present workdeveloped independently from the LOB project.1.
How Hard is Lexical Ambiguity?Many people who have not worked incomputational linguistics have a strong intuitionthat lexical ambiguity is usually not much of aproblem.
It is commonly believed that mostwords have just one part of speech, and that thefew exceptions such as "table" are easilydisambiguated by context in most cases, incontrast, most experts in computational linguistshave found lexical ambiguity to be a majorissue; it is said that practically any content wordcan be used as a noun, verb or adjective, 1 andthat local context is not always adequate todisambiguate.
Introductory texts are full ofambiguous sentences such as?
Time flies like an arrow.?
Flying planes can be dangerous.where no amount of syntactic parsing will help.These examples are generally taken to indicatethat the parser must allow for multiplepossibilities and that grammar formalisms uchas LR(k) are inadequate for natural languagesince these formalisms cannot cope withambiguity.
This argument was behind a large setof objections to Marcus' "LR(k)-like" Deter-ministic Parser.Although it is clear that an expert in compu-tational inguistics can dream up arbitrarily hardsentences, it may be, as Marcus suggested, thatmost texts ate not very hard in practice.
Recallthat Marcus hypothesized most decisions can beresolved by the parser within a small window(i.e., three buffer cells), and there are only a fewproblematic cases where the parser becomesconfuse& He called these confusing cases"garden paths," by analogy with the famousexample:?
The horse raced past the barn fell.With just a few exceptions such as these"garden paths," Marcus assumes, there is almostalways a unique "best" interpretation which Canbe found with very limited resources.
Theproposed stochastic approach is largelycompatible with this; the proposed approach1.
From an information theory point of view, one canquantity ambiguity in bits.
In the case of the BrownTagged Corpus, the lexical entropy, the conditionalentropy of the part of speech given the word is about 0.25bits per part of speech.
This is considerably smaller thanthe contextual entropy, the conditional entropy of the partof speech given the next two parts of speech.
Thisentropy is estimated to be about 2 bits per part of speech.137assumes that it is almost always sufficient toassign each word a unique "best" part of speech(and this can be accomplished with a veryefficient linear time dynamic programmingalgorithm).
After reading introductorydiscussions of "Flying planes can bedangerous," one might have expected thatlexical ambiguity was so pervasive that it wouldbe hopeless to try to assign just one part ofspeech to each word and in just one linear timepass over the input words.2.
Lexical Disambiguation RulesHowever, the proposed stochastic method isconsiderably simpler than what Marcus had inmind.
His thesis parser used considerably moresyntax than the proposed stochastic method.Consider the following pair described in\[Marcus\]:?
Have/VB \[the students who missed the exam\]TAKE the exam today.
(imperative).
Have/AUX \[the students who missed theexam\] TAKEN the exam today?
(question)where it appears that the parser needs to lookpast an arbitrarily long noun phrase in order tocorrectly analyze "have," which could be eithera tenseless main verb (imperative) or a tensedauxiliary verb (question).
Marcus' ratherunusual example can no longer be handled byFidditch, a more recent Marcus-style parser withvery large coverage.
In order to obtain suchlarge coverage, Fidditch has had to take a morerobust/modest view of lexical disambiguation.Whereas Marcus' Parsifal program distinguishedpatterns uch as "have NP tenseless" and "haveNP past-participle," most of Fidditch'sdiagnostic rules axe less ambitious and look onlyfor the start of a noun phrase and do not attemptto look past an arbitrarily long noun phrase.
Forexample, Fidditch has the following lexicaldisambiguation rule:?
(defiule n+prep!"
> \[**n+prep\] !=n \[npstarters\]")which says that a preposition is more likely thana noun before a noun phrase.
More precisely,the rule says that ff a noun/prepositionambiguous word (e.g., "out")  is followed bysomething that starts a noun phrase (e.g., adeterminer), then rule out the noun possibility.This type of lexical diagnostic rule can becaptured with bigram and trigram statistics; itturns out that the sequence ...prepositiondeterminer.., is much more common in theBrown Corpus (43924 observations) than thesequence ...noun determiner... (1135observations).
Most lexical disambiguation rulesin Fidditch can be reformulated in terms ofbigram and trigram statistics in this way.Moreover, it is worth doing so, because bigramand trigram statistics are much easier to obtainthan Fidditch-type disambiguation rules, whichare extremely tedious to program, test anddebug.In addition, the proposed stochastic approach cannaturally take advantage of lexical probabilitiesin a way that is not easy to capture with parsersthat do not make use of frequency information.Consider, for example, the word "see," which isalmost always a verb, but does have an archaicnominal usage as in "the Holy See."
Forpractical purposes, "see"  should not beconsidered noun/verb ambiguous in the samesense as truly ambiguous words like "program,""house" and "wind"; the nominal usage of"see"  is possible, but not likely.If every possibility in the dictionary must begiven equal weight, parsing is very difficult.Dictionaries tend to focus on what is possible,not on what is likely.
Consider the trivialsentence, " I  see a bird."
For all practicalpurposes, every word in the sentence isunambiguous.
According to \[Francis andKucera\], the word 'T '  appears as a pronoun(PPLS) in 5837 out of 5838 observations(-100%), "see" appears as a verb in 771 out of772 observations C100%), "a"  appears as anarticle in 23013 out of 23019 observations('100%) and "bird" appears as a noun in 26 outof 26 observations C100%).
However, accordingto Webster's Seventh New Collegiate Dictionary,every word is ambiguous.
In addition to thedesired assignments of tags, the first thee wordsare listed as nouns and the last as an intransitiveverb.
One might hope that these spuriousassignments could be ruled out by the parser assyntactically ill-formed.
Unfortunately, this isnnlikely to work.
If the parser is going to acceptnoun phrases of the form:?
\[N'P IN city\] IN school\] IN committee\] INmeeting\]\]then it can't rule out138?
\[NP IN I\] IN see\] \[N a\] IN bird\]\]Similarly, the parser probably also has to accept"bird" as an intransitive verb, since there isnothing syntactically wrong with:?
\[S \[NP \[N I\] \[N see\] \[1'4 a\]\] \[V1 a \[V bird\]\]\]These part of speech assignments aren't wrong;they are just extremely improbable.3.
The Proposed MethodConsider once again the sentence, " I  see abird."
The problem is to find an assignment ofparts of speech to words that optimizes bothlexical and contextual probabilities, both ofwhich are estimated from the Tagged BrownCorpus.
The lexical probabilities axe estimatedfrom the following frequencies:WordIseeabirdParts of SpeechPPSS 5837VB 771AT 23013NN 26NP 1UH 1IN (French) 6(PPSS = pronoun; NP = proper noun; VB =verb; UH = interjection; IN = preposition; AT =article; NN = noun)The lexical probabilities are estimated in theobvious way.
For example, the probability that'T '  is a pronoun, Prob(PPSS\['T'), is estimatedas the freq(PPSSl 'T') / freq('T')  or 5837/5838.The probability that "see" is a verb is estimatedto be 771/772.
The other lexical probabilityestimates follow the same pattern.The contextual probability, the probability ofobserving part of speech X given the followingtwo parts of speech Y and Z, is estimated bydividing the trigram frequency XYZ by thebigram frequency YZ.
Thus, for example, theprobability of observing a verb before an articleand a noun is estimated to be the ratio of thefreq(VB, AT, NN) over the freq(AT, NN) or3412/53091 = 0.064.
The probability ofobserving a noun in the same context isestimated as the ratio of freq(NN, AT, NN) over53091 or 629/53091 = 0.01.
The othercontextual probability estimates follow the samepattern.A search is performed in order to find theassignment of part of speech tags to words thatoptimizes the product of the lexical andcontextual probabilities.
Conceptually, thesearch enumerates all possible assignments ofparts of speech to input words.
In this case,there are four input words, three of which aretwo ways ambiguous, producing a set of2*2*2* 1=8 possible assignments of parts ofspeech to input words:I see a birdPPSS VB AT NNPPSS VB IN NNPPSS UH AT NNPPSS UH IN NNNP VB AT NNNP VB IN NNN'P UH AT NNNP UH IN NNEach of the eight sequences are then scored bythe product of the lexical probabilities and thecontextual probabilities, and the best sequence isselected.
In this case, the first sequence is by farthe best.In fact, it is not necessary to enumerate allpossible assignments because the scoringfunction cannot see more than two words away.In other words, in the process of enumeratingpart of speech sequences, it is possible in somecases to know that some sequence cannotpossibly compete with another and can thereforebe abandoned.
Because of this fact, only O(n)paths will be enumerated.
Let us illustrate thisoptimization with an example:Find all assignments of parts of speech to"bird" and score the partial sequence.Henceforth, all scores are to be interpreted as logprobabilities.
(-4.848072 "NN")Find all assignments of parts of speech to "a"and score.
At this point, there are two paths:(-7.4453945 "AT" "NN")(-15.01957 "IN" "NN")Now, find assignments of "see" and score.
Atthis point, the number of paths seem to begrowing exponentially.139(- 10.1914 "VB" "AT" "NN")(-18.54318 "VB" "IN" "NN")(-29.974142 "UIT' "AT" "NN")(-36.53299 "UH" "IN" "NN")Now, find assignments of " I "  and score.
Note,however, that it is no longer necessary tohypothesize that "a"  might be a Frenchpreposition IN because all four paths, PPSS VBIN NN, NN VB IN NN, PPSS UH IN NN andNP UH AT NN score less well than some otherpath and there is no way that any additionalinput could make any difference.
In particular,the path, PPSS VB IN NN scores less well thanthe path PPSS VB AT NN, and additional inputwill not help PPSS VB IN NN because thecontextual scoring function has a limited windowof three parts of speech, and that is not enoughto see past the existing PPSS and VB.
(-12.927581 "PPSS" "VB .... AT" "NN")(-24.177242 "NP" "VB .... AT" "NN")(-35.667458 "PPSS" "UH" "AT" "NN")(-44.33943 "NP" "UH" "AT" "NN")The search continues two more iterations,assuming blank parts of speech for words out ofrange.
(-13.262333 ...... PPSS" "VB" "AT" "NN")(-26.5196 ...... NP" "VB .... AT" "NN")F'mally, the result is: PPSS VB AT NN.
(-12.262333 .......... PPSS .... VB" "AT" "NN")The final result is: I/PPSS see/VB a/AT bird/NN.A slightly more interesting example is: "Canthey can cans.
"cans(-5.456845 "NNS")ca l l(-12.603266 "NN" "NNS")(-15.935471 "VB" "NNS")(-15.946739 "biD" "NNS")they(-18.02618 "PPSS" "biD" "NNS")(- 18.779934 "PPSS" "Vii" "NNS")(-21.411636 "PPSS" "NN" "NNS ")ca l l(-21.766554 "MD" "PPSS" "VB" "NNS")(-26.45485 "NN" "PPSS" "MD" "NNS")(-28.306572 "VB .... PPSS" "MD" "NNS")(-21.932137 ...... MD" "PPSS" "VB .... NNS")(-30.170452 ...... VB" "PPSS" "MD" "NNS")(-31.453785 ...... NN" "PPSS" "MD" "NNS")And the result is: Can/MD they/PPSS can/VBcans/NNS(-20.932137 .......... MD" "PPSS" "V'B" "NNS")4.
Parsing Simple Non-Recursive Noun PhrasesStochasticallySimilar stochastic methods have been applied tolocate simple noun phrases with very highaccuracy.
The program inserts brackets into asequence of parts of speech, producing outputsuch as:\[MAT former/AP top/NN aide/NN\] to/IN\[Attomey/NP General/NP Edwin/NP Meese/NP\]interceded/VBD to/TO extend/VB Jan/ATaircraft/NN company/NN \[business/NN\] with/IN\[a/AT lobbyist/NN\] \[who/WPS\] worked/VBDfor/IN \[the/AT defense/NN contractor/NN\] ,/,according/IN to/IN \[a/AT published/VBN re-port/NN\] .LThe proposed method is a stochastic analog ofprecedence parsing.
Recall that precedenceparsing makes use of a table that says whether toinsert an open or close bracket between any twocategories (terminal or nonterminal).
Theproposed method makes use of a table that givesthe probabilities of an open and close bracketbetween all pairs of parts of speech.
A sampleis shown below for the five parts of speech: AT(article), NN (singular noun), NNS (non-singularnoun), VB (uninflected verb), IN (preposition).The table says, for example, that there is nochance of starting a noun phrases after an article(all five entries on the AT row are O) and thatthere is a large probability of starting a nounphrase between a verb and an noun (the entry in140(vB, AT) is LO).Probability of Starting a Noun PhraseAT NN NNS VB INATNNNNSVBIN0 0 0 0 0.99 .01 0 0 01.O .02 .11 0 01.0 1.0 1.0 0 01.0 1.0 1.0 0 0Probability of Ending a Noun PhraseAT NN NNS VB INATNNNNSVBIN0 0 0 0 01.0 .01 0 0 1.01.0 .02 .l I 1.0 1.00 0 0 0 00 0 0 0 .02These probabilities were estimated from about40,000 words (11,000 noun phrases) of trainingmaterial selected from the Brown Corpus.
Thetraining material was parsed into noun phrasesby laborious semi-automatic means (withconsiderable help from Eva Ejerhed).
It tookabout a man-week to prepare the trainingmaterial.The stochastic parser is given a sequence of partsof speech as input and is asked to insert bracketscorresponding to the beginning and end of nounphrases.
Conceptually, the parser enumerates allpossible parsings of the input and scores each ofthem by the precedence probabilities.
Consider,for example, the input sequence: bin VB.
Thereare 5 possible ways to bracket this sequence(assuming no recursion):.NNVB?
\ [~ \ ]W?
INN VB\ ]?
INN\ ]  \ [VB\ ]?
NN \ [VB\ ]Each of these parsings is scored by multiplying 6precedence probabilities, the probability of anopen/close bracket appearing (or not appearing)in any one of the three positions (before the NN,after the NN or after the VB).
The parsing withthe highest score is returned as output.A small sample of the output is given in theappendix.
The method works remarkably wellconsidering how simple it is.
There is sometendency to underestimate the number ofbrackets and nan two noun phrases together as in\[NP the time Fairchild\].
The proposed methodomitted only 5 of 243 noun phrase brackets inthe appendix.5.
Smoothing IssuesSome of the probabilities are very hard toestimate by direct counting because of ZipFsLaw (frequency is roughly proportional toinverse rank).
Consider, for example, the lexicalprobabilities.
We need to estimate how ofteneach word appears with each part of speech.Unfoaunately, because of ZipFs Law, no matterhow much text we look at, there will always bea large tail of words that appear only a fewtimes.
In the Brown Corpus, for example,40,000 words appear five times or less.
If aword such as "yawn" appears once as a nounand once as a verb, what is the probability that itcan be an adjective?
It is impossible to saywithout more information.
Fortunately,conventional dictionaries can help alleviate thisproblem to some extent.
We add one to thefrequency count of possibilities in the dictionary.For example, "yawn" happens to be listed inour dictionary as noun/verb ambiguous.
Thus,we smooth the frequency counts obtained fromthe Brown Corpus by adding one to bothpossibilities.
In this case, the probabilitiesremain unchanged.
Both before and aftersmoothing, we estimate "yawn" to be a noun50% of the time, and a verb the rest.
There isno chance that "yawn" is an adjective.In some other cases, smoothing makes a bigdifference.
Consider the word "cans."
Thisword appears 5 times as a plural noun and neveras a verb in the Brown Corpus.
The lexicon(and its morphological routines), fortunately,give both possibilities.
Thus, the revisedestimate is that "cans" appears 6/7 times as aplural noun and 1/7 times as a verb.Proper nouns and capitalized words areparticularly problematic; some capitalized wordsare proper nouns and some are not.
Estimatesfrom the Brown Corpus can be misleading.
Forexample, the capitalized word "Acts"  is foundtwice in the Brown Corpus, both times as a141proper noun (in a tide).
It would be a mistake toinfer from this evidence that the word "Acts" isalways a proper noun.
For this reason,capitalized words with small frequency counts (<20) were thrown out of the lexicon.There are other problems with capitalized words.Consider, for example, a sentence beginning withthe capitalized word "Fall"; what is theprobability that it is a proper noun (i.e., asurname)?
Estimates from the Brown Corpusare of little help here since "Fall" never appearsas a capitalized word and it never appears as aproper noun.
Two steps were taken to alleviatethis problem.
First, the frequency estimates for"Fall" are computed from the estimates for"fal l" plus 1 for the proper noun possibility.Thus, "Fal l" has frequency estimates of: ((1 .
"NP") (1 "JJ") (65 "VB") (72 .
"NN"))because "fal l" has the estimates of: ((1 .
"JJ")(65 .
"VB") (72 .
"NN")).
Secondly, a prepasswas introduced which labels words as propernouns if they are "adjacent to" other capitalizedwords (e.g., "White House," "State of theUnion") or if they appear several times in adiscourse and are always capitalized.The lexical probabilities are not the onlyprobabilities that require smoothing.
Contextualfrequencies also seem to follow Zipf's Law.That is, for the set of all sequences of three partsof speech, we have plotted the frequency of thesequence against its rank on log log paper andobserved the classic (approximately) linearrelationship and slope of (almost) -1.
It is clearthat the contextual frequencies requiresmoothing.
Zeros should be avoided.6.
ConclusionA stochastic part of speech program and nounphrase parser has been presented.
Performanceis very encouraging as can be seen from theAppendix.ReferencesChomsky, N., "Three Models for the Descriptionof Language," IRE Transactions on InformationTheory, vol.
IT-2, Proceedings of theSymposium on Information Theory, 1956.
"Collins Cobuild English Language Dictionary,"William Collins Sons & Co Ltd, 1987.Ejerbed, E., "Finding Clauses in UnrestrictedText by Stochastic and F'mitary Methods,"abstracted submitted to this conference.Francis, W., and Kucera, H., "FrequencyAnalysis of English Usage," Houghton Mid'tinCompany, Boston, 1982.Leech, G., Garside, R., Atwell, E., "TheAutomatic Grammatical Tagging of the LOBCorpus," ICAME News 7, 13-33, 1983.Marcus, M., "A  Theory of Syntactic Recognitionfor Natural Language," MIT Press, Cambridge,Massachusetts, 1980.
"Webster's Seventh New CollegiateDictionary," Merriam Company, Springfield,Massachusetts, 1972.Appendix: Sample ResultsThe following story was distributed over the APduring the week of May 26, 1987.
There arejust two tagging errors which are indicated with"***".
There are five missing brackets whichare indicated as "* \ ["  or "*\]".
Words with asecond NP tag were identified as proper nouns ina prepass.\[A/AT former/AP top/NN aide/NN\] to/IN \[At-tomey/NP/NP General/NP/NP Edwin/NP/NPMeese/NP/NP\] interceded/VBD to/I'd extend/VB\[an/AT aircraf*,rNN company/NN 's/$ govem-ment/NN contract/NN\] ,/, then/RB went/VBDinto/IN \[business/NN\] with/IN \[a/AT lobby-ist/NN\] \[who/WPS\] worked/VBD for/IN \[the/ATdefense/NN contractor/NN\] ,/, according/IN to/IN\[a/AT published/VBN report/NN\] ./.\[James/NP/NP E/NP./NP Jenkins/NP/N'P\] ,/,\[a/AT one-time/JJ senior/JJ deputy/NN\] to/IN\[Meese/NP/NP\] ,/, joined/VBD \[the/ATboard/NN\] of/IN \[dimctors/NNS\] of/IN \[Trans-world/NP/NP Group/NP/NP Ltd/NP./NP\] on/IN\[ApriIINP/NP 28/CD\] d, \[198,t/CD\] ,/, \[the/ATChicago/NP/NP Tribune/NP/NP\] reported/VBDin/IN \[its/PP$ Tuesday/NR editions/NNS\] ./.\[The/AT principal/JJ figum/NN\] in/IN \[Trans-world/NP/NP\] was/BEDZ \[Richard/NP/NP Mill-man/NP/NP\] ,/, \[a/AT lobbyist/NN\] for/IN \[Fair-child/NP/NP Industries/NP/NP Inc/NP./NP\] ,/,\[a/AT Virgima/NP/NP defense/NN con-142tractor/NN\] J, \[the/AT Tribune/NP/NP\]said/VBD .\].\[MAT federal/JJ grand\]JJ jury/NN\] is/BEZ in-vestigating/VBG \[the/AT Fairchild/NP/NP trans-action/NN\] and/CC \[other/AP actions/NNS\]of/IN \[Meese/NP/NP\] and/CC \[former/APWhite/NP/NP House/NP/NP aide/NNLyn/NP/NP Nofziger/NP/NP\] in/IN \[connec-tion/NN\] witla/IN \[Wedtech/NP/NPCorp/NP./NP\] ,/, \ [a /AT  New/NP/NPYork/NP/NP defense/NN company/NN\]\[that/WPS\] received/VBD \[$250/CD million/CD\]in/IN \[govemment/NN contracts/NNS\] is-sued/VBN without/IN \[competitive/JJ bid-ding/NN\] during/IN \[the/AT Reagan/NP/NP ad-ministration/NN\] ./.\[Jenkins/N-P/NP\] lefl/VBD \[the/AT White/NP/NPHouse/NP/NP\] in/IN \[1984/CD\] ,/, and/CCjoined/VBD \[Wedtech/NP/NP\] as/CS \[its/PP$director/NNl of/IN \[marketing/NN *\]*\[ two/CDyears/NNS\] later/RBR .L\[Deborah/NP/NP Tucker/NP/NP\] ,/, \[a/ATspokeswoman/NN\] for/IN \[-Fairchild/NP/NP\] ,/,said/VBD \[Friday/N'R\] that/CS \[the/AT com-pany/NN\] had/HVD been/BEN comacted/VBNby/IN \[the/AT office/NN\] offlN \[independem/JJcounsel/NN James/NP/NP McKay/NP/NP\]and/CC \[subpoenas/NNS\] had/HVD beerffBENserved/VBN on/IN \[Fairchild/NP/NP\] ./.\[Tucker/NP/NP\] said/VBD \[the/AT in-vestigation/NN\] involving/IN \[Fairchild/NP/NP\]had/HVD been/BEN going/VBG on/IN \[a/ATnumher/NN\] of/IN \[weeks/NNS\] and/CCpredatesNBZ \[last/AP week/NN 's/$ ex-pamion/NN\] of/IN \[McKay/NP/NP 's/$ in-vestigation/N~ to/TO include/VB\[Meese/NP/NP\] ./.\[The/AT compaay/NN\] is/BEZ coopemling/VBGin/IN \ [ the/AT investigafion/NN\] ,/,\[Tucker/NP/NP\] said/VBD ./.\[MAT source/NN *\] close/NN***\] to/IN\[McKay/NP/NP\] said/VBD \[last/AP week/NN\]that/CS \[Meese/NP/NP\] isn't/BEZ* under/IN\[criminal/JJ investigation/NN\] in/IN \[the/ATFairchild/NP/NP matter/NN\] ,/, but/RB is/BEZ\[a/AT witness/NN\] .L\[The\]NP Tribune//q'P/NP\] said/VBD \[Mi.ll-man/NP/NP\] ,/, acting/VBG as/CS \[a/AT lobby-ist/NN\] for/IN \[the/AT Chamilly/NP/NP\] ,L\[Va/NP.-based/NP company/NN\] ,/, went/VBDto/TO see/VB \[Jenkins/NP/NP\] in/IN \[1982/CD\]and/CC urged/VBD \[him/PPO\] and/CC\[Meese/NP/NP\] to/TO encourage/VB \[the/ATAir/NP/NP Force/NP/NP\] to/TO extend/VB\[the/AT production/NN\] of/IN \[Fairchild/NP/NP's/$ A- 10\]NP bomber/NN\] for/IN \[a/ATyear/NN\] ./.\[MilIman/NP/NP\] said/VBD there/RB was/BEDZ\[a/AT lucrative/JJ market/NN\] in/IN\[TMrd/NP/NP World/NP/NP countries/NNS\] ,/,but/CC that/CS \[Fairchild/NP/NP 's/$chances/NNS\] would/MD be/BE limited/VBNif/CS \[the/AT Air/NP/NP Force/NP/NP\]was/BEDZ not/* producing/VBG \[the/ATplane/NN\] .L\[The/AT Air/NP/NP Force/NP/NP\] had/HVD de-cided/VBN to/TO discontmue/VB \[pro-duction/NN\] of/IN \[the/AT A-10/N'P\] ,/, \[a/AT1960s-era/CD ground-support/NN attack/NNbomber/NN\] ,/, at/IN \[the/AT time/NN *\]*\[ Fair-child\]NP/NP\] was/BEDZ hopmg\]VBG to/TOsell/VB \[A-10s/NP\] abroadfRB J, \[the/ATTribune/NP/NP\] said/VBD ./.\[The/AT newspaper/NN\] said/VBD \[one/CDsource/NN\] reported/VBD that/CS after/CS\[MilIman/NP/NP\] made/VBD \[his/PP$ pitch/NN\]J, \[Meese/NP/NP\] ordered/VBD \[Jen-kins/NP/NP\] to /TO prepare/VB \[a/ATmemo/NN\] on/IN \[behalf/NN\] of/IN \[Fair-child/NP/NP\] ./.\[Memos/NP***\] signed/V'BD by/IN\[Meese/NP/NP\] ,/, stressing/VBG \[the/AT impor-tance/NN\] of/IN \[Fairchild/NP/NP 's/$ ar-ranging/VBG sales/NNS\] in/IN \[Third/NP/NPWorld/NP/NP countries/NNS\] ,/, were/BEDsent/VBN to/IN \[the/AT State/NP/NP Depart-ment/NP/NP\] and/CC \[the/AT Air/NP/NPForce/NP/NP\] ./.\[MilIman/NP/NP\] did/DOD not/* return/VB\[telephone/NN calls/NNS\] to/IN \[his/PP$ of-fice/NN-\] and/CC \[referral/NN numbers/NNS\]\[Monday/NR\] J, \[the/AT Tribune/NP/NP\]said/VBD ./.143
