Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 472?483,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsInvestigations in Exact Inference for Hierarchical TranslationWilker Aziz?, Marc Dymetman?, Sriram Venkatapathy?
?University of Wolverhampton, Wolverhampton, UK?Xerox Research Centre Europe, Grenoble, France?w.aziz@wlv.ac.uk, ?
{first.last}@xrce.xerox.comAbstractWe present a method for inference in hi-erarchical phrase-based translation, whereboth optimisation and sampling are per-formed in a common exact inferenceframework related to adaptive rejectionsampling.
We also present a first imple-mentation of that method along with ex-perimental results shedding light on somefundamental issues.
In hierarchical transla-tion, inference needs to be performed overa high-complexity distribution defined bythe intersection of a translation hypergraphand a target language model.
We replacethis intractable distribution by a sequenceof tractable upper-bounds for which exactoptimisers and samplers are easy to obtain.Our experiments show that exact inferenceis then feasible using only a fraction of thetime and space that would be required bythe full intersection, without recourse topruning techniques that only provide ap-proximate solutions.
While the current im-plementation is limited in the size of inputsit can handle in reasonable time, our exper-iments provide insights towards obtainingfuture speedups, while staying in the samegeneral framework.1 IntroductionIn statistical machine translation (SMT), optimi-sation ?
the task of searching for an optimumtranslation ?
is performed over a high-complexitydistribution defined by the intersection between atranslation hypergraph and a target language model(LM).
This distribution is too complex to be repre-sented exactly and one typically resorts to approx-imation techniques such as beam-search (Koehn etal., 2003) and cube-pruning (Chiang, 2007), wheremaximisation is performed over a pruned represen-tation of the full distribution.Often, rather than finding a single optimum, oneis really interested in obtaining a set of proba-bilistic samples from the distribution.
This is thecase for minimum error rate training (Och, 2003;Watanabe et al 2007), minimum risk training(Smith and Eisner, 2006) and minimum risk de-coding (Kumar and Byrne, 2004).
Due to the ad-ditional computational challenges posed by sam-pling, n-best lists, a by-product of optimisation, aretypically used as approximation to true probabilis-tic samples.
A known issue with n-best lists is thatthey tend to be clustered around only one mode ofthe distribution.
A more direct procedure is to at-tempt to directly draw samples from the underlyingdistribution rather than rely on n-best list approxi-mations (Arun et al 2009; Blunsom and Osborne,2008).OS?
(Dymetman et al 2012a) is a recent ap-proach that stresses a unified view between the twotypes of inference, optimisation and sampling.
Inthis view, rather than resorting to pruning in or-der to cope with the tractability issues, one upper-bounds the complex goal distribution with a sim-pler ?proposal?
distribution for which dynamicprogramming is feasible.
This proposal is incre-mentally refined to be closer to the goal until themaximum is found, or until the sampling perfor-mance exceeds a certain level.This paper applies the OS?
approach to theproblem of inference in hierarchical SMT (Chi-ang, 2007).
In a nutshell, the idea is to replacethe intractable problem of intersecting a context-free grammar with a full language model by thetractable problem of intersecting it with a simpli-fied, optimistic version of this LM which ?forgets?parts of n-gram contexts, and to incrementally addmore context based on evidence of the need to doso.
Evidence is gathered by optimising or samplingfrom the tractable proxy distribution and focussingon the most serious over-optimistic estimates rela-tive to the goal distribution.472Our main contribution is to provide an exact op-timiser/sampler for hierarchical SMT that is effi-cient in exploring only a small fraction of the spaceof n-grams involved in a full intersection.
Al-though at this stage our experiments are limited toshort sentences, they provide insights on the be-havior of the technique and indicate directions to-wards a more efficient implementation within thesame paradigm.The paper is organized as follows: ?2 providesbackground on OS?
and hierarchical translation; ?3describes our approach to exact inference in SMT;in ?4 the experimental setup is presented and find-ings are discussed; ?5 discusses related work, and?6 concludes.2 Background2.1 OS?The OS?
approach (Dymetman et al 2012a;Dymetman et al 2012b) proposes a unified viewof exact inference in sampling and optimisation,where the two modalities are seen as extremes in acontinuum of inference tasks in Lp spaces (Rudin,1987), with sampling associated with the L1 norm,and optimisation with the L?
norm.The objective function p, over which inferenceneeds to be performed, is a complex non-negativefunction over a discrete or continuous space X ,which defines an unnormalised distribution overX .
The goal is to optimise or sample relative top ?
where sampling is interpreted in terms of thenormalised distribution p?(.)
= p(.
)/ ?X p(x)dx.Directly optimising or sampling from p is unfea-sible; however, it is possible to define an (unnor-malized) distribution q of lower complexity thanp, which upper-bounds p everywhere (ie.
p(x) ?q(x), ?x ?
X), and from which it is feasible tooptimise or sample directly.Sampling is performed through rejection sam-pling: first a sample x is drawn from q, and then xis accepted or rejected with probability given by theratio r = p(x)/q(x), which is less than 1 by con-struction.
Accepted x?s can be shown to producean exact sample from p (Robert and Casella, 2004).When the sample x from q is rejected, it is used asa basis for ?refining?
q into a slightly more com-plex q?, where p ?
q?
?
q is still an upper-bound top.
This ?adaptive rejection sampling?
technique in-crementally improves the rate of acceptance, and ispursued until some rate above a given threshold isobtained, at which point one stops refining and usesthe current proposal to obtain further exact samplesfrom p.In the case of optimisation, one finds the maxi-mum x relative to q, and again computes the ratior = p(x)/q(x).
If this ratio equals 1, then it iseasy to show that x is the actual maximum fromp.1 Otherwise we refine the proposal in a similarway to the sampling case, continuing until we finda ratio equal to 1 (or very close to 1 if we are will-ing to accept an approximation to the maximum).For finite spaces X , this optimisation technique isargued to be a generalisation of A?.An application of the OS?
technique to sam-pling/optimisation with High-Order HMM?s is de-scribed in Carter et al(2012) and provides back-ground for this paper.
In that work, while the high-order HMM corresponds to an intractable goal dis-tribution, it can be upper-bounded by a sequenceof tractable distributions for which optimisers andsamplers can be obtained through standard dy-namic programming techniques.2.2 Hierarchical TranslationAn abstract formulation of the decoding processfor hierarchical translation models such as that ofChiang (2007) can be expressed as a sequence ofthree steps.
In a first step, a translation modelG, represented as a weighted synchronous context-free grammar (SCFG) (Chiang, 2005), is applied to(in other words, intersected with) the source sen-tence f to produce a weighted context-free gram-mar G(f) over the target language.2 In a secondstep, G(f) is intersected with a weighted finite-state automaton A representing the target languagemodel, resulting in a weighted context-free gram-mar G?
(f) = G(f) ?
A.
In a final step, a dynamicprogramming procedure (see ?2.4) is applied tofind the maximum derivation x in G?
(f), and thesequence of leaves of yield(x) is the result transla-tion.While this formulation gives the general princi-ple, already mentioned in Chiang (2007), most im-plementations do not exactly follow these steps oruse this terminology.
In practice, the closest ap-proach to this abstract formulation is that of Dyer(2010) and the related system cdec (Dyer et al2010); we follow a similar approach here.1This is because if x?
was such that p(x?)
> p(x), thenq(x?)
?
p(x?)
> p(x) = q(x), and hence x would not be amaximum for q, a contradiction.2G(f) is thus a compact representation of a forest overtarget sequences, and is equivalent to a hypergraph, using dif-ferent terminology.473Whatever the actual implementation chosen, allapproaches face a common problem: the complex-ity of the intersection G?
(f) = G(f)?A increasesrapidly with the order of the language model, andcan become unwieldy for moderate-length inputsentences even with a bigram model.
In order toaddress this problem, most implementations em-ploy variants of a technique called cube-pruning(Chiang, 2007; Huang and Chiang, 2007), wherethe cells constructed during the intersection pro-cess retain only a k-best list of promising candi-dates.
This is an approximation technique, relatedto beam-search, which performs well in practice,but is not guaranteed to find the actual optimum.In the approach presented here ?
described indetail in ?3 ?
we do not prune the search space.While we do construct the full initial grammarG(f), we proceed by incrementally intersectingit with simple automata associated with upper-bounds ofA, for which the intersection is tractable.2.3 Earley IntersectionIn their classical paper Bar-Hillel et al(1961)showed that the intersection of a CFG with a FSA isa CFG, and Billot and Lang (1989) were possiblythe first to notice the connection of this constructwith chart-parsing.
In general, parsing with a CFGcan be seen as a special case of intersection, withthe input sequence represented as a ?flat?
(linearchain) automaton, and this insight allows to gener-alise various parsing algorithms to correspondingintersection algorithms.
One such algorithm, forweighted context-free grammars and automata, in-spired by the CKY parsing algorithm, is presentedin Nederhof and Satta (2008).
The algorithm thatwe are using is different; it is inspired by Earleyparsing, and was introduced in chapter 2 of Dyer(2010).
The advantage of Dyer?s ?Earley Intersec-tion?
algorithm is that it combines top-down pre-dictions with bottom-up completions.
The algo-rithm thus avoids constructing many non-terminalsthat may be justified from the bottom-up perspec-tive, but can never be ?requested?
by a top-downderivation, and would need to be pruned in a sec-ond pass.
Our early experiments showed an impor-tant gain in intermediary storage and in overall timeby using this Earley-based technique as opposed toa CKY-based technique.We do not describe the Earley Intersection algo-rithm in detail here, but refer to Dyer (2010), whichwe follow closely.2.4 Optimisation and Sampling from aWCFGOptimisation in a weighted CFG (WCFG)3, thatis, finding the maximum derivation, is well stud-ied and involves a dynamic programming proce-dure that assigns in turn to each nonterminal, ac-cording to a bottom-up traversal regime, a max-imum derivation along with its weight, up to thepoint where a maximum derivation is found for theinitial nonterminal in the grammar.
This can beseen as working in the max-times semiring, wherethe weight of a derivation is obtained through theproduct of the weights of its sub-derivations, andwhere the weight associated with a nonterminal isobtained by maximising over the different deriva-tions rooted in that nonterminal.The case of sampling can be handled in a verysimilar way, by working in the sum-times insteadof the max-times semiring.
Here, instead of max-imising over the weights of the competing deriva-tions rooted in the same nonterminal, one sumsover these weights.
By proceeding in the samebottom-up way, one ends with an accumulation ofall the weights on the initial nonterminal (this canalso be seen as the partition function associatedwith the grammar).
An efficient exact sampler isthen obtained by starting at the root nonterminal,randomly selecting an expansion proportionally tothe weight of this expansion, and iterating in a top-down way.
This process is described in more detailin section 4 of Johnson et al(2007), for instance.3 ApproachThe complexity of building the full intersectionG(f) ?
A, when A represents a language modelof order n, is related to the fact that the number ofstates of A grows exponentially with n, and thateach nonterminal N in G(f) tends to generate inthe grammar G?
(f) many indexed nonterminals ofthe form (i,N, j), where i, j are states of A andthe nonterminal (i,N, j) can be interpreted as anN connecting an i state to a j state.In our approach, instead of explicitly construct-ing the full intersection G(f) ?
A, which, usingthe notation of ?2.1, is identified with the unnor-malised goal distribution p(x), we incrementallyproduce a sequence of ?proposal?
grammars q(t),which all upper-bound p, where q(0) = G(f) ?A(0), ..., q(t+1) = q(t) ?
A(t), etc.
Here A(0) is3Here the CFG is assumed to be acyclic, which is typicallythe case in translation applications.474an optimistic, low complexity, ?unigram?
versionof the automaton A, and each increment A(t) is asmall automaton that refines q(t) relative to somespecific k-gram context (i.e., sequence of k words)not yet made explicit in the previous increments,where k takes some value between 1 and n. Thisprocess produces a sequence of grammars q(t) suchthat q(0)(.)
?
q(1)(.)
?
q(2)(.)
?
... ?
p(.
).In the limit ?Mt=0A(t) = A for some largeM , sothat we are in principle able to reconstruct the fullintersection p(.)
= q(M) = G(f)?A(0)?...
?A(M)in finite time.
In practice our actual process stopsmuch earlier: in optimisation, when the value ofthe maximum derivation x?t relative to q(t) becomesequal to its value according to the full languagemodel, in sampling when the acceptance rate ofsamples from q(t) exceeds a certain threshold.
Theprocess is detailed in what follows.3.1 OS?
for Hierarchical TranslationOur application of OS?
to hierarchical translation isillustrated in Algorithm 1, with the two modes, op-timisation and sampling, made explicit and shownside-by-side to stress the parallelism.On line 1, we initialise the time step to 0, andfor sampling we also initialise the current accep-tance rate (AR) to 0.
On line 2, we initialise theinitial proposal grammar q(0), where A(0) is de-tailed in ?3.2.
On line 3, we start a loop: in op-timisation we stop when we have found an x thatis accepted, meaning that the maximum has beenfound; in sampling, we stop when the estimatedacceptance rate (AR) of the current proposal q(t)exceeds a certain threshold (e.g.
20%) ?
this ARcan be roughly estimated by observing how manyof the last (say) one hundred samples from the pro-posal have been accepted, and tends to reflect theactual acceptance rate obtained by using q(t) with-out further refinements.
On line 4, in optimisation,we compute the argmax x from the proposal, and insampling we draw a sample x from the proposal.4On line 5, we compute the ratio r = p(x)/q(t)(x);by construction q(t) is an optimistic version of p,thus r ?
1.On line 6, in optimisation we accept x if theratio is equal to 1, in which case we have foundthe maximum, and in sampling we accept x withprobability r, which is a form of adaptive rejec-tion sampling and guarantees that accepted sam-4Following the OS?
approach, taking an argmax is actuallyassimilated to an extreme form of sampling, with an L?
spacetaking the place of an L1 space.ples form exact samples from p; see (Dymetman etal., 2012a).If x was rejected (line 7), we then (lines 8, 9)refine q(t) into a q(t+1) such that p(.)
?
q(t+1)(.)
?q(t)(.)
everywhere.
This is done by defining theincremental automatonA(t+1) on the basis of x andq(t), as will be detailed below, and by intersectingthis automaton with q(t)Finally, on line 11, in optimisation we return thex which has been accepted, namely the maximumof p, and in sampling we return the list of alreadyaccepted x?s, which form an exact sample from p,along with the current q(t), which can be used as asampler to produce further exact samples with anacceptance rate performance above the predefinedthreshold.3.2 Incremental refinementsInitial automatonA(0) This deterministic au-tomaton is an ?optimistic?
version ofA which onlyrecords unigram information.
A(0) has only onestate q0, which is both initial and final.
For eachword a of the target language it has a transition(q0, a, q0) whose weight is denoted by w1(a).
Thisweight is called the ?max-backoff unigram weight?
(Carter et al 2012) and it is defined as:w1(a) ?
maxh plm(a|h),where plm(a|h) is the conditional language modelprobability of a relative to the history h, and wherethe maximum is taken over all possible histories,that is, over all possible sequence of target wordsthat might precede a.Max-backoffs Following Carter et al(2012),for any language model of finite order, the unigrammax-backoff weights w1(a) can be precomputed ina ?Max-ARPA?
table, an extension of the ARPAformat (Jurafsky and Martin, 2000) for the targetlanguage model, which can be precomputed on thebasis of the standard ARPA table.From the Max-ARPA table one can also directlycompute the following ?max-backoff weights?
:w2(a|a?1), w3(a|a?2 a?1), ..., which are definedby:w2(a|a?1) ?
maxh plm(a|h, a?1)w3(a|a?2 a?1) ?
maxh plm(a|h, a?2 a?1)...where the maximum is taken over the part ofthe history which is not explicitely indicated.475Algorithm 1 OS?
for Hierarchical Translation: Optimisation (left) and Sampling (right).1: t?
02: q(0) ?
G(f) ?A(0)3: while not an x has been accepted do4: Find maximum x in q(t)5: r ?
p(x)/q(t)(x)6: Accept-or-Reject x according to r7: if Rejected(x) then8: define A(t+1) based on x and q(t)9: q(t+1) ?
q(t) ?A(t+1)10: t?
t + 111: return x1: t?
0, AR?
02: q(0) ?
G(f) ?A(0)3: while not AR > threshold do4: Sample x ?
q(t)5: r ?
p(x)/q(t)(x)6: Accept-or-Reject x according to r7: if Rejected(x) then8: define A(t+1) based on x and q(t)9: q(t+1) ?
q(t) ?A(t+1)10: t?
t + 111: return already accepted x?s along with q(t)Note that: (i) if the underlying language modelis, say, a trigram model, then w3(a|a?2 a?1)is simply plm(a|a?2 a?1), and similarly for anunderlying model of order k in general, and(ii) w2(a|a?1) = maxa?2 w3(a|a?2 a?1) andw1(a) = maxa?1 w2(a|a?1).Incremental automata A(t) The weightassigned to any target sentence by A(0) is larger orequal to its weight according to A.
Therefore, theinitial grammar q(0) = G(f) ?
A(0) is optimisticrelative to the actual grammar p = G(f) ?
A: forany derivation x in p, we have p(x) ?
q(0)(x).We can then apply the OS?
technique with q(0).In the case of optimisation, this means thatwe find the maximum derivation x from q(0).By construction, with y = yield(x), we haveA(0)(y) ?
A(y).
If the two values are equal, wehave found the maximum,5 otherwise there mustbe a word yi in the sequence ym1 = y for whichplm(yi|yi?11 ) is strictly smaller than w1(yi).
Let ustake among such words the one for which the ratio?
= w2(yi|yi?1)/w1(yi) ?
1 is the smallest, andfor convenience let us rename b = yi?1, a = yi.We then define the (deterministic) automaton A(1)as illustrated in the following figure:b:1 a:?else:1b:1 else:10 1Here the state 0 is both initial and final, and thestate 1 is final; all edges carry a (multiplicative)weight equal to 1, except edge (1, a, 0), which car-ries the weight ?.
We use the abbreviation ?else?to refer to any label other than bwhen starting from0, and other than b or a when starting from 1.5This case is very unlikely with A(0), but helps introducethe general case.It is easy to check that this automaton assigns toany word sequence y a weight equal to ?k, where kis the number of occurrences of b a in y.
In particu-lar, if y is such that yi?1 = b, yi = a, then the tran-sition in (the deterministic automaton) A(0) ?A(1)that consumes yi carries the weight ?
w1(a), inother words, the weight w2(a|b).
Thus the newproposal grammar q(1) = q(0) ?
A(1) has now?incorporated?
knowledge of the bigram a-in-the-context-b, at the cost of some increase in its com-plexity.6The general procedure for choosing A(t+1) fol-lows the same pattern.
We find the max deriva-tion x in q(t) along with its yield y; if p(x) =q(t)(x), we stop and output x; otherwise we findsome subsequence yi?m?1, yi?m, ..., yi such thatthe knowledge of the n-gram yi?m, ..., yi has al-ready been registered in q(t), but not that of then-gram yi?m?1, yi?m, ..., yi, and we define anautomaton A(t+1) which assign to a sequence aweight ?k, where?
= wm+1(yi|yi?m?1, yi?m, ..., yi?1)wm(yi|yi?m, ..., yi?1),and where k is the number of occurrences ofyi?m?1, yi?m, ..., yi in the sequence.7We note that we have p ?
q(t+1) ?
q(t) ev-erywhere, and also that the number of possible re-finement operations is bounded, because at somepoint we would have expanded all contexts to theirmaximum order, at which point we would have re-produced p(.)
on the whole space X of possible6Note that without further increasing q(1)?s complexity onecan incorporate knowledge about all bigrams sharing the pre-fix b.
This is because A(1) does not need additional statesto account for different continuations of the context b, all weneed is to update the weights of the transitions leaving state 1appropriately.
More generally, it is not more costly to accountfor all n-grams prefixed by the same context of n ?
1 wordsthan it is to account for only one of them.7Building A(t+1) is a variant of the standard constructionfor a ?substring-searching?
automaton (Cormen et al 2001)and produces an automaton with n states (the order of the n-gram).
This construction is omitted for the sake of space.476derivations exactly.
However, we typically stopmuch earlier than that, without expanding contextsin the regions of X which are not promising evenon optimistic assessments based on limited con-texts.Following the OS?
methodology, the situationwith sampling is completely parallel to that of op-timisation, the only difference being that, insteadof finding the maximum derivation x from q(t)(.
),we draw a sample x from the distribution asso-ciated with q(t)(.
), then accept it with probabil-ity given by the ratio r = p(x)/q(t)(x) ?
1.
Inthe case of a reject, we identify a subsequenceyi?m?1, yi?m, ..., yi in yield(x) as in the optimi-sation case, and similarly refine q(t) into q(t+1) =q(t) ?
A(t+1).
The acceptance rate gradually in-creases because q(t) comes closer and closer to p.We stop the process at a point where the current ac-ceptance rate, estimated on the basis of, say, the lastone hundred trials, exceeds a predefined threshold,perhaps 20%.3.3 IllustrationIn this section, we present a small running exampleof our approach.
Consider the lowercased Germansource sentence: eine letzte beobachtung .Table 1 shows the translation associated with theoptimum derivation from each proposal q(i).
Then-gram whose cost, if extended by one word to theleft, would be increased by the largest factor is un-derlined.
The extended context selected for refine-ment is highlighted in bold.i Rules Optimum0 311 <s> one last observation .
</s>1 454 <s> one last observation .
</s>2 628 <s> one last observation .
</s>3 839 <s> one final observation .
</s>4 1212 <s> one final observation .
</s>...12 3000 <s> a final observation .
</s>13 3128 <s> one final observation .
</s>Table 1: Optimisation steps showing the iteration(i), the number of rules in the grammar and thetranslation associated to the optimum derivation.Consider the very first iteration (i = 0), at whichpoint only unigram costs have been incorporated.The sequence <s> one last observation .
</s>represents the translation associated to the bestderivation x in q(0).
We proceed by choosing fromit one sequence to be the base for a refinement thatwill lower q(0) bringing it closer to p. Amongst allpossible one-word (to the left) extensions, extend-ing the unigram ?one?
to the bigram ?<s> one?
isthe operation that lowers q(0)(x) the most.
It mightbe helpful to understand it as the bigram ?<s> one?being associated to the largest LM gap observedin x.
Therefore the context ?<s>?
is selected forrefinement, which means that an automaton A(1)is designed to down-weight derivations compatiblewith bigrams prefixed by ?<s>?.
The proposal q(0)is intersected with A(1) producing q(1).
We pro-ceed like this iteratively, always selecting a con-text not yet accounted for until q(i)(x) = p(x) forthe best derivation (13th iteration in our example),when the true optimum is found with a certificateof optimality.Q Q Q Q Q Q Q Q Q Q Q Q Q Q0 2 4 6 8 10 12?2?10123Iteration (i)Score (Q,P, B) ;Delta(C, M) ; #states (R)P P PP P P P P P P P P PPB B B B B B B B B B B B B BC C C C C C C C C C C C C CM M M M M M M M M M M M M MRR R R RRRR RR RRRRQPBCMRQPBestCurrent gapMinimum gapRefinementFigure 1: Certificate of optimality.Figure 1 displays the progression of Q (score ofthe best derivation) and P (that derivation?s truescore).
As guaranteed by construction, Q is alwaysabove P .
B represents the score of the best deriva-tion so far according to the true scoring function,that is, B is a lower-bound on the true optimum8.The optimal solution is achieved when P = Q.Curve B in Figure 1 shows that the best scoringsolution was found quite early in the search (i = 3).However, optimality could only be proven 10 itera-tions later.
Another way of stating the convergencecriterion Q = P is observing a zero gap (in the logdomain) between Q and P (see curve C ?
currentgap), or a zero gap between Q and B (see curve M?
minimum gap).
Observe how M drops quicklyfrom 1 to nearly 0, followed by a long tail whereM8This observation allows for error-safe pruning in optimi-sation: if x is a lower-bound on the true optimum, derivationsin q(i) that score lower than p(x) could be safely removed.We have left that possibility for future work.477decreases much slower.
Note that if we were will-ing to accept an approximate solution, we could al-ready stop the search if B remained unchanged fora predetermined number of iterations or if changesin B were smaller than some threshold, at the costof giving up on the optimality certificate.Finally, curve R shows the number of states inthe automaton A(i) that refines the proposal at it-eration i.
Note how lower order n-grams (2-gramsin fact) are responsible for the largest drop in thefirst iterations and higher-order n-grams (in fact 3-grams) are refined later in the long tail.Figure 2 illustrates the progression of the sam-pler for the same German sentence.
At each iter-ation a batch of 500 samples is drawn from q(i).The rejected samples in the batch are used to col-lect statistics about overoptimistic n-grams and toheuristically choose one context to be refined forthe next iteration, similar to the optimisation mode.We start with a low acceptance rate which growsup to 30% after 15 different contexts were incor-porated.
Note how the L1 norm of q (its partitionfunction) decreases after each refinement, that is,q is gradually brought closer to p, resulting in theincreased number of exact samples and better ac-ceptance rate.Note that, starting from iteration one, all refine-ments here correspond to 2-grams (i.e.
one-wordcontexts).
This can be explained by the fact that,in sampling, lower-order refinements are those thatmostly increase acceptance rate (rationale: high-order n-grams are compatible with fewer grammarrules).Iteration (i)1.01.52.00 5 10ll l l l l l l l l l l l lrefinement0.10.20.3l ll l l ll l l l l l llaccrate01000l l l l ll l ll l ll l lexact910 ll l l l l l l l l l l l lL1Figure 2: L1 norm of q, the number of exact sam-ples drawn, the acceptance rate and the refinementtype at each iteration.4 ExperimentsWe used the Moses toolkit (Koehn et al 2007)to extract a SCFG following Chiang (2005) fromthe 6th version of the Europarl collection (Koehn,2005) (German-English portion).
We trained lan-guage models using lmplz (Heafield et al 2013)and interpolated the models trained on the En-glish monolingual data made available by theWMT (Callison-Burch et al 2012) (i.e.
Eu-roparl, newscommentaries, news-2012 and com-moncrawl).
Tuning was performed via MERT us-ing newstest2010 as development set; test sen-tences were extracted from newstest2011.
Finally,we restricted our SCFGs to having at most 10 tar-get productions for a given source production.Figure 3 shows some properties of the initialgrammar G(f) as a function of the input sentencelength (the quantities are averages over 20 sen-tences for each class of input length).
The numberof unigrams grows linearly with the input length,while the number of unique bigrams compatiblewith strings generated by G(f) appears to growquadratically9 and the size of the grammar in num-ber of rules appears to be cubic ?
a consequenceof having up to two nonterminals on the right-handside of a rule.Figure 4 shows the number of refinement oper-ations until convergence in optimisation and sam-pling, as well as the total duration, as a function ofthe input length.10 The plots will be discussed indetail below.4.1 OptimisationIn optimisation (Figures 4a and 4b), the number ofrefinements up to convergence appears to be lin-ear with the input length, while the total durationgrows much quicker.
These findings are furtherdiscussed in what follows.Table 2 shows some important quantities regard-ing optimisation with OS?
using a 4-gram LM.
Thefirst column shows how many sentences we areconsidering, the second column shows the sentencelength, the third column m is the average num-ber of refinements up to convergence.
Column |A|refers to the refinement type, which is the numberof states in the automaton A, that is, the order of9The number of unique bigrams is an estimate obtained bycombining the terminals at the boundary of nonterminals thatmay be adjacent in a derivation.10The current implementation faces timeouts depending onthe length of the input sentence and the order of the languagemodel, explaining why certain curves are interrupted earlierthan others in Figure 4.4782 4 6 8 1050100150Input lengthunigramslll llll lll(a) Unigrams in G(f)2 4 6 8 100200040006000Input lengthbigramsl ll lllllll(b) Bigrams compatible with G(f)2 4 6 8 10010002000300040005000Input lengthR0l l llllllll(c) Number of rules in G(f)Figure 3: Properties of the initial grammar as function of input lengthS Length m |A| count |Rf ||R0|9 4 45.0 2 20.3 74.6 ?
53.93 19.24 5.410 5 62.3 2 21.9 145.4 ?
162.63 32.94 7.59 6 102.8 2 34.7 535.8 ?
480.03 54.94 13.2Table 2: Optimisation with a 4-gram LM.the n-grams being re-weighted (e.g.
|A| = 2 whenrefining bigrams sharing a one-word context).
Col-umn count refers to the average number of refine-ments that are due to each refinement type.
Finally,the last column compares the number of rules in thefinal proposal to that of the initial one.The first positive result concerns how much con-text OS?
needs to take into account for finding theoptimum derivation.
Table 2 (column m) showsthat OS?
explores a very reduced space of n-gramcontexts up to convergence.
To illustrate that, con-sider the last row in Table 2 (sentences with 6words).
On average, convergence requires incorpo-rating only about 103 contexts of variable order, ofwhich 55 are bigram (2-word) contexts (rememberthat |A| = 3 when accounting for a 2-word con-text).
According to Figure 3b, in sentences with6 words, about 2,000 bigrams are compatible withstrings generated by G(f).
This means that only2.75% of these bigrams (55 out of 2,000) need tobe explicitly accounted for, illustrating how waste-ful a full intersection would be.A problem, however, is that the time until con-vergence grows quickly with the length of the input(Figure 4b).
This can be explained as follows.
Ateach iteration the grammar is refined to account forn-grams sharing a context of (n ?
1) words.
ThatS Input m |A| count |Rf ||R0|10 5 1.0 2 1.0 1.9 ?
1.010 6 6.6 2 6.3 17.6 ?
13.63 0.310 7 14.5 2 12.9 93.8 ?
68.93 1.54 0.1Table 3: Sampling with a 4-gram LM and reachinga 5% acceptance rate.operation typically results in a larger grammar:most rules are preserved, some rules are deleted,but more importantly, some rules are added to ac-count for the portion of the current grammar thatinvolves the selected n-grams.
Enlarging the gram-mar at each iteration means that successive refine-ments become incrementally slower.The histogram of refinement types of Table 2highlights how efficient OS?
is w.r.t.
the space ofn-grams it needs to explore before convergence.The problem is clearly not the number of refine-ments, but rather the relation between the growthof the grammar and the successive intersections.Controlling for this growth and optimising the in-tersection as to partially reuse previously computedcharts may be the key for a more generally tractablesolution.4.2 SamplingFigure 4c shows that sampling is more economi-cal than optimisation in that it explicitly incorpo-rates even fewer contexts.
Note how OS?
con-verges to acceptance rates from 1% to 10% in muchfewer iterations than are necessary to find an opti-mum11.
Although the convergence in sampling is11Currently we use MERT to train the model?s weight vec-tor ?
which is normalised by its L1 norm in the Moses im-plementation.
While optimisation is not sensitive to the scaleof the weights, in sampling the scale determines how flat or4792 2 2 22 2 2 2 2 22 4 6 8 10020406080100Input lengthRefinements3 333333444444234 2?gram LM3?gram LM4?gram LM(a) Optimisation: number of refinements.2 2 2 2 2 2 2 22 22 4 6 8 10050001500025000Input lengthTime (s)3 3 3 33 334 4 4 444234 2?gram LM3?gram LM4?gram LM(b) Optimisation: time for convergence.a a a a a a a a a a a a a a2 4 6 8 10 12 140510152025Input lengthRefinementsb b b b b b b b bb b bb bc c c c c c ccc cc c1 1 1 1 1 1 1112 2 2 2 2 22223 3 3 3 33 3334 4 4 44445 5 5555X X X XXXY Y YYYYabc12345XYLM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%(c) Sampling: number of refinements.a a a a a a a a a a a a aa2 4 6 8 10 12 140200060001000014000Input lengthTime(s)b b b b b b b b b b bbbbc c c c c c c c c ccc1 1 1 1 1 1 1 112 2 2 2 2 2 2 223 3 3 3 3 3 3334 4 4 4 4 445 5 5 5 5 5X X X X X XY Y Y Y YYabc12345XYLM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%(d) Sampling: time for convergence.Figure 4: Convergence for different LM order as function of the input length in optimisation (top) andsampling (bottom).
We show the number of refinements up to convergence on the left, and the convergencetime on the right.
In optimisation we stop when the true optimum is found.
In sampling we stop at differentacceptance rate levels: (a, b and c) use a 2-gram LM to reach 1, 5 and 10% AR; (1-4) use a 3-gram LM toreach 2, 3, 5 and 10% AR; and (X, Y) use a 4-gram LM to reach 5 and 10% AR.faster than in optimisation, the total duration is stillan issue (Figure 4b).Table 3 shows the same quantities as Table 2, butnow for sampling.
It is worth highlighting that eventhough we are using an upper-bound over a 4-gramLM (and aiming at a 5% acceptance rate), very fewcontexts are selected for refinement, most of themlower-order ones (one-word contexts ?
rows with|A| = 2).Observe that an improved acceptance rate al-ways leads to faster acquisition of exact samplesafter we stop refining our proxy distribution.
How-ever, Figure 4d shows for example that movingfrom 5% to 10% acceptance rate using a 4-gramLM (curves X and Y) is time-consuming.
Thusthere is a trade-off between how much time onespends improving the acceptance rate and howmany exact samples one intends do draw.
Figure5 shows the average time to draw batches betweenpeaked the distribution is.
Arun et al(2010) experiment withscaling MERT-trained weights as to maximise BLEU on held-out data, as well as with MBR training.
A more adequatetraining algorithm along similar lines is reserved for futurework.1 1 1 11111e+00 1e+02 1e+04 1e+0620050010002000500010000SamplesTime(s)2 2 2 2 22212 5% AR10% ARFigure 5: Average time to draw 1 to 1 million sam-ples, for input sentences of length 6, using a 4-gramLM at 5% (curve 1) and 10% (curve 2) acceptancerate (including the time to produce the sampler).one and one million samples from two exact sam-plers that were refined up to 5% and 10% accep-tance rate respectively.
The sampler at 5% AR(which is faster to obtain) turns out to be more effi-cient if we aim at producing less than 10K samples.Finally, note that samples are independently480drawn from the final proposal, making the ap-proach an appealing candidate to parallelism in or-der to increase the effective acceptance rate.5 Related WorkRush and Collins (2011) do not consider sampling,but they address exact decoding for hierarchicaltranslation.
They use a Dual Decomposition ap-proach (a special case of Lagrangian Relaxation),where the target CFG (hypergraph in their termi-nology) component and the target language modelcomponent ?trade-off?
their weights so as to ensureagreement on what each component believes to bethe maximum.
In many cases, this technique isable to detect the actual true maximum derivation.When this is not the case, they use a finite-state-based intersection mechanism to ?tighten?
the firstcomponent so that some constraints not satisfied bythe current solution are enforced, and iterate untilthe true maximum is found or a time-out is met,which results in a high proportion of finding thetrue maximum.Arun et al(2009, 2010) address the questionof sampling in a standard phrase-based transla-tion model (Koehn et al 2003).
Contrarily to ouruse of rejection sampling (a Monte-Carlo method),they use a Gibbs sampler (a Markov-Chain Monte-Carlo (MCMC) method).
Samples are obtainedby iteratively re-sampling groups of well-designedvariables in such a way that (i) the sampler does nottend to be trapped locally by high correlations be-tween conditioning and conditioned variables, and(ii) the combinatorial space of possibilities for thenext step is small enough so that conditional prob-abilities can be computed explicitly.
By contrast toour exact approach, the samples obtained by Gibbssampling are not independent, but form a Markovchain that only converges to the target distributionin the limit, with convergence properties difficultto assess.
Also by contrast to us, these papers donot address the question of finding the maximumderivation directly, but only through finding a max-imum among the derivations sampled so far, whichin principle can be quite different.Blunsom and Osborne (2008) address proba-bilistic inference, this time, as we do, in the contextof hierarchical translation, where sampling is usedboth for the purposes of decoding and training themodel.
When decoding in the presence of a lan-guage model, an approximate sampling procedureis performed in two stages.
First, cube-pruning isemployed to construct a WCFG which generatesa subset of all the possible derivations that wouldcorrespond to a full intersection with the target lan-guage model.
In a second step this grammar issampled through the same dynamic programmingprocedure that we have described in ?2.4.
By con-trast to our approach, the paper does not attemptto perform exact inference.
However it does notonly address the question of decoding, but also thatof training the model, which requires, in additionto sampling, an estimate of the model?s partitionfunction.
In common with Arun et al(2010), theauthors stress the fact that a sampler of derivationsis also a sampler of translations as strings, while amaximiser over derivations cannot be used to findthe maximum translation string.6 ConclusionsThe approach we have presented is, to our knowl-edge, the first one to address the problem of ex-act sampling for hierarchical translation and to dothat in a framework that also handles exact opti-misation.
Our experiments show that only a frac-tion of the language model n-grams need to be in-corporated in the target grammar in order to per-form exact inference in this approach.
However,in the current implementation, we experience time-outs for sentences of even moderate length.
We areworking on improving this situation along three di-mensions: (i) our implementation of the Earley In-tersection rebuilds a grammar from scratch at eachintersection, while it could capitalise on the chartsbuilt during the previous steps; (ii) the unigram-level max-backoffs are not as tight as they couldbe if one took into account more precisely the setof contexts in which each word can appear rela-tive to the grammar; (iii) most importantly, whileour refinements are ?local?
in the sense of address-ing one n-gram context at a time, they still affecta large portion of the rules in the current grammar,even rules that have very low probability of beingever sampled by this grammar; by preventing re-finement of such rules during the intersection pro-cess, we may be able to make the intersection morelocal and to produce much smaller grammars, with-out losing the exactness properties of the approach.AcknowledgementsThe first author wishes to thank the PASCAL-2Visit to Industry programme for partially fundinghis visit to Xerox Research Centre Europe last Fall,which initiated this collaboration.481ReferencesAbhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-som, Adam Lopez, and Philipp Koehn.
2009.
Montecarlo inference and maximization for phrase-basedtranslation.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning,CoNLL ?09, pages 102?110, Stroudsburg, PA, USA.Association for Computational Linguistics.Abhishek Arun, Barry Haddow, Philipp Koehn, AdamLopez, Chris Dyer, and Phil Blunsom.
2010.
Montecarlo techniques for phrase-based translation.
Ma-chine Translation, 24(2):103?121, June.Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir.1961.
On formal properties of simple phrase struc-ture grammars.
Zeitschrift fu?r Phonetik, Sprachwis-senschaft und Kommunikationsforschung, (14):143?172.Sylvie Billot and Bernard Lang.
1989.
The structureof shared forests in ambiguous parsing.
In Proceed-ings of the 27th Annual Meeting of the Associationfor Computational Linguistics, pages 143?151, Van-couver, British Columbia, Canada, June.
Associationfor Computational Linguistics.Phil Blunsom and Miles Osborne.
2008.
Probabilis-tic inference for machine translation.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, EMNLP ?08, pages 215?223, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Simon Carter, Marc Dymetman, and GuillaumeBouchard.
2012.
Exact Sampling and Decoding inHigh-Order Hidden Markov Models.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1125?1134, JejuIsland, Korea, July.
Association for ComputationalLinguistics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 263?270, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, 33:201?228.Thomas H. Cormen, Clifford Stein, Ronald L. Rivest,and Charles E. Leiserson.
2001.
Introduction to Al-gorithms.
McGraw-Hill Higher Education, 2nd edi-tion.Chris Dyer, Jonathan Weese, Hendra Setiawan, AdamLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-itkevitch, Phil Blunsom, and Philip Resnik.
2010.cdec: a decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions, ACLDemos ?10, pages 7?12, Stroudsburg, PA,USA.
Association for Computational Linguistics.Christopher Dyer.
2010.
A Formal Model of Ambiguityand its Applications in Machine Translation.
Ph.D.thesis, University of Maryland.M.
Dymetman, G. Bouchard, and S. Carter.
2012a.
TheOS* Algorithm: a Joint Approach to Exact Optimiza-tion and Sampling.
ArXiv e-prints, July.Marc Dymetman, Guillaume Bouchard, and SimonCarter.
2012b.
Optimization and sampling for nlpfrom a unified viewpoint.
In Proceedings of theFirst International Workshop on Optimization Tech-niques for Human Language Technology, pages 79?94, Mumbai, India, December.
The COLING 2012Organizing Committee.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modifiedKneser-Ney language model estimation.
In Proceed-ings of the 51st Annual Meeting of the Association forComputational Linguistics, Sofia, Bulgaria, August.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages139?146, Rochester, New York, April.
Associationfor Computational Linguistics.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Natu-ral Language Processing, Computational Linguisticsand Speech Recognition (Prentice Hall Series in Ar-tificial Intelligence).
Prentice Hall, 1 edition.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: open482source toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedings ofMachine Translation Summit, pages 79?86.Shankar Kumar and William Byrne.
2004.
Mini-mum Bayes Risk Decoding for Statistical MachineTranslation.
In Joint Conference of Human Lan-guage Technologies and the North American chap-ter of the Association for Computational Linguistics(HLT-NAACL 2004).Mark-Jan Nederhof and Giorgio Satta.
2008.
Proba-bilistic parsing.
In M. Dolores Jimnez-Lpez G. Bel-Enguix and C. Martn-Vide, editors, New Develop-ments in Formal Languages and Applications, Stud-ies in Computational Intelligence, volume 113, pages229?258.
Springer.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, volume 1 of ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods (Springer Texts in Statis-tics).
Springer-Verlag New York, Inc., Secaucus, NJ,USA.Walter Rudin.
1987.
Real and Complex Analysis.McGraw-Hill.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 72?82, Stroudsburg, PA,USA.
Association for Computational Linguistics.David A. Smith and Jason Eisner.
2006.
Minimumrisk annealing for training log-linear models.
In Pro-ceedings of the COLING/ACL on Main conferenceposter sessions, COLING-ACL ?06, pages 787?794,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin train-ing for statistical machine translation.
In Proceed-ings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic,June.
Association for Computational Linguistics.483
