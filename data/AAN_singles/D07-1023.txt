Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
218?227, Prague, June 2007. c?2007 Association for Computational LinguisticsUnsupervised Part-of-Speech Acquisition for Resource-Scarce LanguagesSajib Dasgupta and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{sajib,vince}@hlt.utdallas.eduAbstractThis paper proposes a new bootstrappingapproach to unsupervised part-of-speechinduction.
In comparison to previousbootstrapping algorithms developed for thisproblem, our  approach aims to improvethe quality of the seed clusters byemploying seed words that are bothdistributionally and morphologicallyreliable.
In particular, we present a novelmethod for combining morphological anddistributional information for seedselection.
Experimental results demonstratethat our approach works well for Englishand Bengali, thus providing suggestiveevidence that it is applicable to bothmorphologically impoverished languagesand highly inflectional languages.1 IntroductionThe availability of a high-quality lexicon is crucialto the development of fundamental text-processingcomponents such as part-of-speech (POS) taggersand syntactic parsers.
While hand-crafted lexiconsare readily available for resource-rich languagessuch as English, the same is not true for resource-scarce languages.
Unfortunately, manuallyconstructing a lexicon requires a lot of linguisticexpertise, and is practically infeasible for highlyinflectional and agglutinative languages, whichcontain a very large number of lexical items.
Giventhe scarcity of annotated data for acquiring thelexicon in a supervised manner, researchers haveinstead investigated unsupervised POS inductiontechniques for automating the lexicon constructionprocess.
In essence, the goal of unsupervised POSinduction is to learn the set of possible POS tagsfor each lexical item from an unannotated corpus.The most common approach to unsupervisedPOS induction to date has been motivated by Har-ris?s (1954) distributional hypothesis: words withsimilar co-occurrence patterns should have similarsyntactic behavior.
More specifically, unsupervisedPOS induction algorithms typically operate by (1)representing each target word (i.e., a word to betagged with its POS) as a context vector that en-codes its left and right context, (2) clustering dis-tributionally similar words, and (3) manually label-ing each cluster with a POS tag by inspecting themembers of the cluster.This distributional approach works under the as-sumption that the context vector of each word en-codes sufficient information for enabling accurateword clustering.
However, many words are dis-tributionally unreliable: due to data sparseness,they occur infrequently and hence their contextvectors do not capture reliable statistical informa-tion.
To overcome this problem, Clark (2000) pro-poses a bootstrapping approach, in which he (1)clusters the most distributionally reliable words,and then (2) incrementally augments each clusterwith words that are distributionally similar to thosealready in the cluster.The goal of this paper is to propose a new boot-strapping approach to unsupervised POS inductionthat can operate in a resource-scarce setting.
Mostnotably, our approach aims to improve the qualityof the seed clusters by employing seed words thatare both distributionally and morphologically reli-able.
In particular, we present a novel method forcombining morphological and distributional infor-mation for seed selection.
Furthermore, given our218emphasis on resource-scarce languages, our ap-proach does not rely on any language resources.
Inparticular, the morphological information that itexploits is provided by an unsupervised morpho-logical analyzer.It is perhaps not immediately clear why morpho-logical information would play a crucial role in theinduction process, especially since the distribu-tional approach has achieved considerable successfor English POS induction (see Lamb (1961),Sch?tze (1995) and Clark (2000)).
To understandthe role and significance of morphology, it is im-portant to first understand why the distributionalapproach works well for English.
Recall from theabove that the distributional approach assumes thatthe information encoded in the context vector ofeach word, which typically consists of the 250most frequent words of a given language, is suffi-cient for accurately clustering the words.
This ap-proach works well for English because the mostfrequent English words are composed primarily ofclosed-class words such as ?to?
and ?is?, whichprovide strong clues to the POS of the target word.However, this assumption is not necessarily validfor fairly free word order and highly inflectionallanguages such as Bengali.
The reason is that (1)co-occurrence statistics collected from free wordorder languages are not as reliable as those fromfixed word order languages; and (2) many of theclosed-class words that appear in the context vec-tor for English words are realized as inflections inBengali.
The absence of these highly informativewords implies that the context vectors may nolonger capture sufficient information for accuratelyclustering Bengali words, and hence the use ofmorphological information becomes particularlyimportant for unsupervised POS induction forthese inflectional languages.We will focus primarily on labeling open-classwords with their POS tags.
Our decision is moti-vated by the fact that closed-class words generallycomprise a small percentage of the lexical items ofa language.
In Bengali, the percentage of closed-class words is even smaller than that in English: asmentioned before, many closed-class words inEnglish are realized as suffixes in Bengali.Although our attempt to incorporate morpho-logical information into the distributional POS in-duction framework was originally motivated byinflectional languages, experimental results showthat our approach works well for both English andBengali, suggesting its applicability to both mor-phologically impoverished languages and highlyinflectional languages.
Owing to the lack of pub-licly available resources for Bengali, we manuallycreated a 5000-word Bengali lexicon for evaluationpurposes.
Hence, one contribution of our work liesin the creation of an annotated dataset for Bengali.By making this dataset publicly available 1 , wehope to facilitate the comparison of different unsu-pervised POS induction algorithms and to stimu-late interest in Bengali language processing.The rest of the paper is organized as follows.Section 2 discusses related work on unsupervisedPOS induction.
Section 3 describes our tagsets forEnglish and Bengali.
The next three sections de-scribe the three steps of our bootstrapping ap-proach: cluster the words using morphological in-formation (Section 4), remove potentially misla-beled words from each cluster (Section 5), andbootstrap each cluster using a weakly supervisedlearner (Section 6).
Finally, we present evaluationresults in Section 7 and conclusions in Section 8.2 Related WorkSeveral unsupervised POS induction algorithmshave also attempted to incorporate morphologicalinformation into the distributional framework, butour work differs from these in two respects.Computing morphological information.
PreviousPOS induction algorithms have attempted to derivemorphological information from dictionaries (Ha-ji, 2000) and knowledge-based morphologicalanalyzers (Duh and Kirchhoff, 2006).
However,these resources are generally not available for re-source-scarce languages.
Consequently, research-ers have attempted to derive morphological infor-mation heuristically (e.g., Cucerzan and Yarowsky(2000), Clark (2003), Freitag (2004)).
For instance,Cucerzan and Yarowsky (2000) posit a charactersequence x as a suffix if there exists a sufficientnumber of distinct words w in the vocabulary suchthat the concatentations wx are also in the vocabu-lary.
It is conceivable that such heuristically com-puted morphological information can be inaccurate,thus rendering the usefulness of a more accuratemorphological analyzer.
To address this problem,we exploit morphological information provided byan unsupervised word segmentation algorithm.1See http://www.utdallas.edu/~sajib/posDatasets.html.219Tag Description Treebank tagsJJ Adjective JJJJR Adjective, comparative JJRJJS Adjective, superlative JJSNN Singular noun NN, NNPNNS Plural noun NNS, NNPSRB Adverb RBVB Verb, non-3rd ps.
sing.
present VB, VBPVBD Verb, past tense or past participle VBD, VBNVBG Verb, gerund/present participle VBGVBZ Verb, 3rd ps.
sing.
present VBZTable 1: The English tagsetUsing morphological information.
Perhaps due tothe overly simplistic methods employed to com-pute morphological information, morphology hasonly been used as what Biemann (2006) calledadd-on?s in existing POS induction algorithms,which remain primarily distributional in nature.
Incontrast, our approach more tightly integrates mor-phology into the distributional framework.
As wewill see, we train SVM classifiers using both mor-phological and distributional features to select seedwords for our bootstrapping algorithm, effectivelyletting SVM combine these two sources of infor-mation and perform automatic feature weighting.Another appealing feature of our approach is thatwhen labeling each unlabeled word with its POStag, an SVM classifier also returns a numeric valuethat indicates how confident the word is labeled.This opens up the possibility of having a humanimprove our automatically constructed lexicon bymanually checking those entries that are taggedwith low confidence by an SVM classifier.Recently, there have been attempts to perform(mostly) unsupervised POS tagging without rely-ing on a POS lexicon.
Haghighi and Klein?s (2006)prototype-driven approach requires just a few pro-totype examples for each POS tag, exploiting theselabeled words to constrain the labels of their dis-tributionally similar words when training a genera-tive log-linear model for POS tagging.
Smith andEisner (2005) train a log-linear model for POS tag-ging in an unsupervised manner using contrastiveestimation, which seeks to move probability massto a positive example e from its neighbors (i.e.,negative examples created by perturbing e).3 The English and Bengali TagsetsGiven our focus on automatically labeling openclass words, our English and Bengali tagsets aredesigned  to essentially  cover  all of the open-classTag Description ExamplesJJ Adjective vhalo, garam, kharapNN Singular noun kanna, ridoy, shoshonNN2 2nd order inflectional noun dhopake, kalamtikeNN6 6th order inflectional noun gharer, manusherNN7 7th order inflectional noun dhakai, barite, graameNNP Proper noun arjun, ahmmadNNS Plural noun manushgulo, pakhiderNNSH Noun ending with ?sh?
barish, jatrishVB Finite verb kheyechi, krlam, krIVBN Non-finite verb kre, giye, jete, kadteTable 2: The Bengali tagsetwords.
Our English tagset, which is composed often tags, is shown in Table 1.
As we can see, a tagin our tagset can be mapped to more than one PennTreebank tags.
For instance, we use the tag ?NN?for both singular and plural common nouns.
Ourdecision of which Penn Treebank tags to grouptogether is based on that of Sch?tze (1995).Our Bengali tagset, which also consists of tentags, is adapted from the one proposed by Saha etal.
(2004) (see Table 2).
It is worth noting thatunlike English, we assign different tags to Bengaliproper nouns and common nouns.
The reason isthat for English, it is not particularly crucial to dis-tinguish the two types of nouns during POS induc-tion, since they can be distinguished fairly easilyusing heuristics such as initial capitalization.
ForBengali, such simple heuristics do not exist, as theBengali alphabet does not have any upper andlower case letters.
Hence, it is important to distin-guish Bengali proper nouns and common nounsduring POS induction.4 Clustering the Morphologically SimilarWordsAs mentioned before, our approach aims to moretightly integrate morphological information intothe distributional POS induction framework.
Infact, our POS induction algorithm begins by clus-tering the morphologically similar words (i.e.,words that combine with the same set of suffixes).The motivation for clustering morphologicallysimilar words can be attributed to our hypothesisthat words having similar POS should combinewith a similar set of suffixes.
For instance, verbs inEnglish combine with suffixes like ?ing?, ?ed?
and?s?, whereas adjectives combine with suffixes like?er?
and ?est?.
Note, however, that the suffix ?s?can attach to both verbs and nouns in English, andso it is not likely to be a useful feature for identify-220ing the POS of a word.
The question, then, is howto determine which suffixes are useful for the POSidentification task in an unsupervised setting wherewe do not have any prior knowledge of language-specific grammatical constraints.
This section pro-poses a method for identifying the ?useful?
suf-fixes and employing them to cluster the morpho-logically similar words.
As we will see, our clus-tering algorithm not only produces soft clusters,but it also automatically determines the number ofclusters for a particular language.Before we describe how to identify the usefulsuffixes, we need to (1) induce all of the suffixesand (2) morphologically segment the words in ourvocabulary.
2  However, neither of these tasks issimple for a truly resource-scarce language forwhich we do not have a dictionary or a knowledge-based morphological analyzer.
As mentioned in theintroduction, our proposed solution to both tasks isto use an unsupervised morphological analyzer thatcan be built just from an unannotated corpus.
Inparticular, we have implemented an unsupervisedmorphological analyzer that outperforms Gold-smith?s (2001) Linguistica and Creutz and Lagus?s(2005) Morfessor for our English and Bengalidatasets and compares favorably to the best-performing morphological parsers in MorphoChal-lenge 20053 (see Dasgupta and Ng (2007)).Given the segmentation of each word and themost frequent 30 suffixes4 provided by our mor-phological analyzer, our clustering algorithm oper-ates by (1) clustering the similar suffixes and then(2) assigning words to each cluster based on thesuffixes a word combines with.
To cluster similarsuffixes, we need to define the similarity betweentwo suffixes.
Informally, we say that two suffixes xand y are similar if a word that combines with xalso combines with y and vice versa.
In practice,we will rarely posit two suffixes as similar underthis definition unless we assume access to a com-plete vocabulary ?
an assumption that is especiallyunrealistic for resource-scarce languages.
As a re-sult, we relax this definition and consider two suf-fixes x and y similar if P(x | y) > t and P(y | x) > t,where P(x | y) is the probability of a word combin-ing with suffix x given that it combines with suffix2A vocabulary is simply a set of (distinct) words extractedfrom an unannotated corpus.
We extracted our English andBengali vocabulary from WSJ and Prothom Alo, respectively.3http://www.cis.hut.fi/morphochallenge2005/4We found that 30 suffixes are sufficient to cluster the words.y, and t is a threshold that we set to 0.4 in all of ourexperiments.
Note that both probabilities can beestimated from an unannotated corpus.5 Given thisdefinition of similarity, we can cluster the similarsuffixes using the following steps:Creating the initial clusters.
First, we create asuffix graph, in which we have (1) one node foreach of the 30 suffixes, and (2) a directed edgefrom suffix x to suffix y if P(y | x) > 0.4.
We thenidentify the strongly connected components of thisgraph using depth-first search.
These strongly con-nected components define our initial partitioning ofthe 30 suffixes.
We denote the suffixes assigned toa cluster the primary keys of the cluster.Improving the initial clusters.
Recall that weultimately want to cluster the words by assigningeach word w to the cluster in which w combineswith all of its primary keys.
Given this goal, it isconceivable that singleton clusters are notdesirable.
For instance, a cluster that has ?s?
as itsonly primary key is not useful, because although alot of words combine with ?s?, they do notnecessarily have the same POS.
As a result, weimprove each initial cluster by adding moresuffixes to the cluster, in hopes of improving theresulting clustering of the words by placingadditional constraints on each cluster.
Morespecifically, we add a suffix y to a cluster c if, foreach primary key x of c, P(y | x) > 0.4.
If thiscondition is satisfied, then y becomes a secondarykey of c. For each initial cluster c?, we perform thischeck using each of the suffixes x?
not in c?
to seeif x?
can be added to c?.
If, after this expansionstep, we still have a cluster c* defined by a singleprimary key x that also serves as a secondary keyin other clusters, then x is probably ambiguous(i.e., x can probably attach to words belonging todifferent POSs); and consequently, we remove c*.We denote the resulting set of clusters by C.Populating the clusters with words.
Next, foreach word w in our vocabulary, we check whetherw can be assigned to any of the clusters in C. Spe-cifically, we assign w to a cluster c if w can com-bine with each of its primary keys and at least halfof its secondary keys.Labeling and merging the clusters.
After popu-lating each cluster with words, we manually label5For instance, we compute P(x | y) as the ratio of the numberof distinct words that combines with both x and y to the num-ber of distinct words that combine with y only.221each of them with a POS tag from the tagset.
Wefound that all of the clusters are labeled as NN,VB, or JJ.
The reason is that the clustered wordsare mostly root words.
We then merge all the clus-ters labeled with the same POS tag, yielding onlythree ?big?
clusters.
Note that these ?big?
clustersare soft clusters, since a word can belong to morethan one of them.
For instance, ?cool?
can combinewith ?s?
or ?ing?
to form a VB, and it can alsocombine with ?er?
or ?est?
to form a JJ.Generating sub-clusters.
Recall that each ?big?cluster contains a set of suffixes and also a set ofwords that combines with those suffixes.
Now, foreach ?big?
cluster c, we create one sub-cluster cxfor each suffix x that appears in c. Then, for eachword w in c, we use our unsupervised morphologi-cal analyzer to generate w+x and add the surfaceform to the corresponding sub-cluster.Labeling the sub-clusters.
Finally, we manuallylabel each sub-cluster with a POS tag from ourtagset.
For example, all the words ending in ?ing?will be labeled as VBG.
As before, we merge twoclusters if they are labeled with the same POS tag.The resulting clusters are our morphologicallyformed clusters.5 Purifying the Seed SetThe clusters formed thus far cannot be expected tobe perfectly accurate, since (1) our unsupervisedmorphological analyzer is not perfect, and (2)morphology alone is not always sufficient for de-termining the POS of a word.
In fact, we found thatmany adjectives are mislabeled as nouns for bothlanguages.
For instance, ?historic?
is labeled as anoun, since it combines with suffixes like ?al?
and?ally?
that ?accident?
combines with.
In addition,many words are labeled with the POS that does notcorrespond to their most common word sense.
Forinstance, while words like ?chair?, ?crowd?
and?cycle?
are more commonly used as nouns thanverbs, they are labeled as verbs by our clusteringalgorithm.
The reason is that suffixes that typicallyattach to verbs (e.g., ?s?, ?ed?, ?ing?)
also attach tothese words.
Such labelings, though not incorrect,are undesirable, considering the fact that thesewords are to be used as seeds to bootstrap our mor-phologically formed clusters in a distributionalmanner.
For instance, since ?chair?
and ?crowd?are distributionally similar to nouns, their presencein the verb clusters can potentially contaminate theclusters with nouns during the bootstrapping proc-ess.
Hence, for the purpose of effective bootstrap-ping, we also consider these words ?mislabeled?.To identify the words that are potentially misla-beled, we rely on the following assumption: wordsthat are morphologically similar should also bedistributionally similar and vice versa.
Based onthis assumption, we propose a purification methodthat posits a word w as potentially mislabeled (andtherefore should be removed or relabeled) if thePOS of w as predicted using distributional infor-mation differs from that as determined by mor-phology.The question, then, is how to predict the POStag of a word using distributional information?
Ouridea is to use ?supervised?
learning, where we trainand test on the seed set.
Conceptually, we (1) traina multi-class classifier on the morphologically la-beled words, each of which is represented by itscontext vector, and (2) apply the classifier to rela-bel the same set of words.
If the new label of aword w differs from its original label, then mor-phology and context disagree upon the POS of w;and as mentioned above, our method then deter-mines that the word is potentially misclassified.Note, however, that (1) the training instances arenot perfectly labeled and (2) it does not make senseto train a classifier on data that is seriously misla-beled.
Hence, we make the assumption that a largepercentage (> 70%) of the training instances is cor-rectly labeled6, and that our method would workwith a training set labeled at this level of accuracy.In addition, since we are training a classifier basedon distributional features, we train and test on onlydistributionally reliable words, which we define tobe words that appear at least five times in our cor-pus.
Distributionally unreliable words will all beremoved from the morphologically formed clus-ters, since we cannot predict their POS using dis-tributional information.In our implementation of this method, ratherthan train a multi-class classifier, we train a set ofbinary classifiers using SVMlight (Joachims, 1999)together with the distributional features for deter-mining the POS tag of a given word.7 More spe-cifically, we train one classifier for each pair of6An inspection of the morphologically formed clusters revealsthat this assumption is satisfied for both languages.7In this and all subsequent uses of SVMlight, we set al thetraining parameters to their default values.222POS tags.
For instance, since we have ten POStags for English, we will train 45 binary classifi-ers.8 To determine the POS tag of a given Englishword w, we will use these 45 pairwise classifiers toindependently assign a label to w. For instance, theNN-JJ classifier will assign either NN or JJ to w.We then count how many times w is tagged witheach of the ten POS tags.
If there is a POS tag twhose count is nine, it means that all the nine clas-sifiers associated with t have classified w as t, andso our method will label w as t. Otherwise, we re-move w from our seed set, since we cannot confi-dently label it using our classifier ensemble.To create the training set for the NN-JJ classi-fier, for instance, we can possibly use all of thewords labeled with NN and JJ as positive andnegative instances, respectively.
However, to en-sure that we do not have a skewed class distribu-tion, we use the same number of instances fromeach class to train the classifier.
More formally, letINN be the set of instances labeled with NN, and IJJbe the set of instances labeled with JJ.
Without lossof generality, assume that |INN| < |IJJ|, where |X| de-notes the size of the set X.
To avoid class skew-ness, we have to sample from IJJ, since it is the lar-ger set.
Our sampling method is motivated by bag-ging (Breiman, 1996).
More specifically, we create10 training sets from IJJ, each of which has size |INN| and is formed by sampling with replacementfrom IJJ.
We then combine each of these 10 train-ing sets separately with INN, and train 10 SVMclassifiers from the 10 resulting training sets.Given a test instance i, we first apply the 10 classi-fiers independently to i and obtain the signed con-fidence values9 of the predictions provided by theclassifiers.
We then take the average of the 10 con-fidence values, assigning i the positive class if theaverage is at least 0, and negative otherwise.As mentioned above, we use distributional fea-tures to represent an instance created from a wordw.
The distributional features are created based onSch?tze?s (1995) method.
Specifically, the leftcontext and the right context of w are each encodedusing the most frequent 500 words from the vo-cabulary.
A feature in the left (right) context has8We could have trained just one 10-class classifier, but thefairly large number of classes leads us to speculate that thismulti-class classifier will not achieve a high accuracy.9Here, a large positive number indicates that the classifierconfidently labels the instance as NN, and a large negativenumber represents confident prediction for JJ.the value 1 if the corresponding word appears tothe left (right) of w in our corpus, and 0 otherwise.However, we found that using distributional fea-tures alone would erroneously classify words like?car?
and ?cars?
as having the same POS becausethe two words are distributionally similar.
In gen-eral, it is difficult to distinguish words in NN fromthose in NNS by distributional means.
The sameproblem occurs for words in VB and VBD.
To ad-dress this problem, we augment the feature set withsuffixal features.
Specifically, we create one binaryfeature for each of the 30 most frequent suffixesthat we employed in the previous section.
The fea-ture corresponding to suffix x has the value 1 if x isthe suffix of w. Moreover, we create an additionalsuffixal feature whose value is 1 if none of the 30most frequent suffixes is the suffix of w.6 Augmenting the Seed SetAfter purification, we have a set of clusters filledwith distributionally and morphologically reliableseed words that receive the same POS tag whenpredicted independently by morphological featuresand distributional features.
Our goal in this sectionis to augment this seed set.
Since we have a smallseed set (5K words for English and 8K words forBengali) and a large number of unlabeled words,we believe that it is most natural to apply a weaklysupervised learning algorithm to bootstrap the clus-ters.
Specifically, we employ a version of self-training together with SVM as the underlyinglearning algorithm.
10  Below we first present thehigh-level idea of our self-training algorithm andthen discuss the implementation details.Conceptually, our self-training algorithm worksas follows.
We first train a multi-class SVM classi-fier on the seed set for determining the POS tag ofa word using the morphological and distributionalfeatures described in the previous section, and thenapply it to label the unlabeled (i.e., unclustered)words.
Words that are labeled with a confidencevalue that exceeds the current threshold (which isinitially set to 1 and -1 for positively and nega-tively labeled instances, respectively) will be10As a related note, Clark?s (2001) bootstrapping algorithmuses KL-divergence to measure the distributional similaritybetween an unlabeled word and a labeled word, adding to acluster the words that are most similar to its current member.For us, SVM is a more appealing option because it automati-cally combines the morphological and distributional features.223added to the seed set.
In the next iteration, we re-train the classifier on the augmented labeled data,apply it to the unlabeled data, and add to the la-beled data those instances whose predicted confi-dence is above the current threshold.
If none of theinstances has a predicted confidence above the cur-rent threshold, we reduce the threshold by 0.1.
(Forinstance, if the original thresholds are 1 and -1,they will be changed to 0.9 and -0.9.)
We then re-peat the above procedure until the thresholds reach0.5 and -0.5.
11  Finally, we apply the resultingbootstrapped classifier to label all of the unlabeledwords that have a corpus frequency of at least five,using a threshold of 0.In our implementation of the self-training algo-rithm, rather than train a multi-class classifier ineach bootstrapping iteration, we train pairwiseclassifiers (recall that for English, 45 classifiers areformed from 10 POS tags) using the morphologicaland distributional features described in the previ-ous section.
Again, since we employ distributionalfeatures, we apply the 45 pairwise classifiers onlyto the distributionally reliable words (i.e., wordswith corpus frequency at least 5).
To classify anunlabeled word w, we apply the 45 pairwise classi-fiers to independently assign a label to w.12  Wethen count how many times w is tagged with eachof the ten POS tags.
If there is a POS tag whosecount is nine and all of these nine votes are associ-ated with confidence that exceeds the currentthreshold, then we add w to the labeled data to-gether with its assigned tag.7 Evaluation7.1 Experimental SetupCorpora.
Recall that our bootstrapping algorithmassumes as input an unannotated corpus fromwhich we (1) extract our vocabulary (i.e., the set ofwords to be labeled) and (2) collect the statisticsneeded in morphological and distributional cluster-11We decided to stop the bootstrapping procedure at thresh-olds of 0.5 and -0.5, because the more bootstrapping iterationswe use, the lower are the quality of the bootstrapped data aswell as the accuracy of the bootstrapped classifier.12As in purification, each pairwise classifier is implementedas a set of 10 classifiers, each of which is trained on an equalnumber of instances from both classes.
Testing also proceedsas before: the label of an instance is derived from the averageof the confidence values returned by the 10 classifiers, and theconfidence value associated with the label is just the averageof the 10 confidence values.ing.
We use as our English corpus the Wall StreetJournal (WSJ) portion of the Penn Treebank (Mar-cus et al, 1993).
Our Bengali corpus is composedof five years of articles taken from the Bengalinewspaper Prothom Alo.Vocabulary creation.
To extract our English vo-cabulary, we pre-processed each document in theWSJ corpus by first tokenizing them and then re-moving the most frequent 500 words (as they aremostly closed class words), capitalized words,punctuations, numbers, and unwanted charactersequences (e.g., ?***?).
The resulting English vo-cabulary consists of approximately 35K words.
Weapplied similar pre-processing steps to the ProthomAlo articles to generate our Bengali vocabulary,which consists of 80K words.Test set preparation.
Our English test set is com-posed of the 25K words in the vocabulary that ap-pear at least five times in the WSJ corpus.
Thegold-standard POS tags for each word w are de-rived automatically from the parse trees in which wappears.
To create the Bengali test set, we ran-domly chose 5K words from the vocabulary thatappear at least five times in Prothom Alo.
Eachword in the test set was then labeled with its POStags by two of our linguists.Evaluation metric.
Following Sch?tze (1995), wereport performance in terms of recall, precision,and F1.
Recall is the percentage of POS tags cor-rectly proposed, precision is the percentage of POStags proposed that are correct, and F1 is simply theharmonic mean of recall and precision.
To exem-plify, suppose the correct tagset for ?crowd?
is{NN, VB}; if our system outputs {VB, JJ, RB},then recall is 50%, precision is 33%, and F1 is40%.
Importantly, all of our results will be re-ported on word types.
This prevents the frequentlyoccurring words from having a higher influence onthe results than their infrequent counterparts.7.2 Results and DiscussionThe baseline system.
We use as our baseline sys-tem one of the best existing unsupervised POS in-duction algorithms (Clark, 2003).
More specifi-cally, we downloaded from Clark?s website13 thecode that implements a set of POS induction algo-rithms he proposed.
Among these implementa-tions, we chose cluster_neyessenmorph, whichcombines morphological and distributional infor-13http://www.cs.rhul.ac.uk/home/alexc/224mation and achieves the best performance in hispaper.
When running his program, we use WSJ andProthom Alo as the input corpora.
In addition, weset the number of clusters produced to be 128,since this setting yields the best result in his paper.Results of the baseline system for the English andBengali test sets are shown under the ?After Boot-strapping?
column in row 1 of Tables 3 and 4.
Aswe can see, the baseline achieves F1-scores of 59%and 45% for English and Bengali, respectively.The other results in row 1 will be discussed below.Our induction system.
Recall that our unsuper-vised POS induction algorithm operates in threesteps.
To better understand the performance con-tribution of each of these steps, we show in row 2of Tables 3 and 4 the results of our system after we(1) morphologically cluster the words, (2) purifythe seed set, and (3) augment the seed set.
Impor-tantly, the numbers shown for each step are com-puted over the set of words in the test set that arelabeled at the end of that step.
For instance, themorphological clustering algorithm labeled 11KEnglish words and 25K Bengali words, and so re-call, precision and F1-score are computed over thesubset of these labeled words that appear in the testset.
Similarly, after bootstrapping, all the wordsthat appear at least five times in our corpus are la-beled; since our labeled data is now a superset ofour test data, the numbers in the last column arethe results of our algorithm for the entire test set.As we can see, after morphological clustering,our system achieves F1-scores of 79% and 78% forEnglish and Bengali, respectively.
When measuredon exactly the same set of words, the baseline onlyachieves F-scores of 59% and 56%.
In fact, com-paring rows 1 and 2, we outperform the baseline ineach of the three steps of our algorithm.
In particu-lar, our system yields F1-scores of 73% and 77%for the entire English and Bengali test sets, thusoutperforming the baseline by 14% and 18% forEnglish and Bengali, respectively.Two additional points deserve mentioning.
First,for both languages, the highest F1-score isachieved after the purification step.
A closer analy-sis of the labeled words reveals the reason.
ForEnglish, many of the nouns incorrectly labeled asverbs by the morphological clustering algorithmwere subsequently removed during the purificationstep when distributional similarity was used on topof morphological similarity.
For Bengali, manyproper nouns were assigned by the morphologicalclustering algorithm to the clusters dominated bycommon nouns (because the two types of Bengalinouns are morphologically similar), and many ofthese mislabeled proper nouns were subsequentlyremoved during purification.
Second, as expected,precision drops after the seed augmentation step,since the quality of the labeled data deteriorates asbootstrapping progresses.
Nevertheless, with a lotmore words labeled in the bootstrapping step, westill achieve F1-scores of 73% for English and 76%for Bengali.The remaining rows of the Tables 3 and 4 showthe performance of our algorithm for each tag inour two POS tagsets.
Different observations can bemade for the two languages.
For English, the poorresults for VBZ and NNS can be attributed to thefact that it is not easy to distinguish between thesetwo tags: ?s?
is a typical suffix for words that areNNS and words that are the third person singularof a verb.
In addition, results for verbs are betterthan those for nouns, since verbs are easier to iden-tify using only morphological knowledge.For Bengali, results for adjectives are not good,since (1) adjectives and nouns have very similardistributional property in Bengali and (2) there arenot enough suffixes to induce the adjectives mor-phologically.
Moreover, we achieve high precisionbut low recall for proper nouns.
This implies thatmost of the words that our algorithm labels asproper nouns are indeed correct, but there are alsomany proper nouns that are mislabeled.
A closerexamination of the clusters reveals that many ofthese proper nouns are mislabeled as commonnouns, presumably because these two types ofBengali nouns are morphologically and distribu-tionally similar and therefore it is difficult to sepa-rate them.
We will leave the identification of Ben-gali proper nouns as a topic for future research.7.3 Additional ExperimentsLabeling rare words with morphological infor-mation.
Although our discussion thus far has fo-cused on words whose corpus frequency is at leastfive, it would be informative to examine how wellour algorithm performs on rare, distributionallyunreliable words (i.e., words with corpus fre-quency less than five).
Recall that our morphologi-cal clustering algorithm also clusters rare words.
Infact, these rare words comprise 15% of the Englishwords and 18% of the Bengali words in our mor-phological formed clusters.
Perhaps more impor-225After Morphological Clustering After Purification After BootstrappingP R F1 P R F1 P R F1Baseline 84.1 45.3 58.9 84.9 51.4 64.1 75.6 48.0 59.0Ours 85.9 74.0 79.4 89.3 74.4 81.7 80.4 66.8 73.1JJ 88.7 49.1 63.2 91.4 51.9 66.1 57.7 62.9 60.2JJR 91.1 86.2 88.6 92.1 92.0 92.0 62.1 83.1 71.0JJS 100 98.3 99.1 100 100 100 81.3 86.9 83.9NN 91.6 43.7 59.2 94.8 42.8 58.8 95.2 47.1 62.8NNS 90.6 39.2 53.5 93.5 41.3 57.2 96.6 44.7 60.9RB 100 76.1 86.4 100 82.2 90.6 98.8 63.5 77.3VB 74.0 97.7 84.1 79.8 96.0 87.1 65.7 92.8 76.9VBD 96.6 98.9 97.7 97.6 100 98.8 96.7 91.9 93.3VBG 89.9 100 94.7 91.1 100 95.7 90.8 93.5 92.1VBZ 60.9 99.9 74.7 65.1 96.8 77.7 52.8 92.6 67.3Table 3: POS induction results for English based on word typeAfter Morphological Clustering After Purification After BootstrappingP R F1 P R F1 P R F1Baseline 82.1 42.3 55.5 83.1 45.3 58.3 78.1 43.3 49.3Ours 74.1 81.3 77.5 83.4 78.0 80.7 74.1 79.2 76.6JJ 50.0 51.8 50.9 56.1 55.0 55.5 57.5 51.4 54.3NN 63.0 96.8 76.4 67.0 96.0 78.9 62.2 92.2 74.3NN2 96.3 100 98.1 99.0 100 99.5 99.0 99.0 99.0NN6 95.5 89.2 92.2 97.2 90.0 93.9 97.1 91.0 93.9NN7 88.4 94.1 89.7 92.1 99.2 93.1 90.1 78.7 84.1NNP 87.2 37.3 52.3 92.8 43.8 59.4 92.7 51.5 66.1NNS 62.7 93.1 75.0 66.8 93.5 77.9 65.2 94.1 77.1NNSH 91.0 100 95.6 91.0 100 95.7 91.0 100 95.7VB 68.9 93.0 79.2 77.0 94.6 84.9 73.9 91.8 81.9VBN 84.3 49.1 62.1 82.4 50.1 62.9 56.1 46.7 50.1Table 4: POS induction results for Bengali based on word typetantly, when measuring performance on just thesemorphologically clustered rare words, our algo-rithm achieves F1-scores of 81% and 79% for Eng-lish and Bengali, respectively.
These results pro-vide empirical support for the claim that morpho-logical information can be usefully employed tolabel rare words (Clark, 2003).Soft clustering.
Many words have more than onePOS tag.
For instance, ?received?
can be labeled asVBD and JJ.
Although our morphological cluster-ing algorithm can predict some of these ambigui-ties, those are at the ?big?
cluster level.
At the sub-cluster level, the algorithm imposes a hard cluster-ing on the words.
In other words, no word appearsin more than one sub-cluster.Ideally, a POS induction algorithm should pro-duce soft clusters due to lexical ambiguity.
In fact,Jardino and Adda (1994), Sch?tze (1997) andClark (2000) have attempted to address the ambi-guity problem to a certain extent.
We have alsoexperimented with a very simple method for han-dling ambiguity in our bootstrapping algorithm:when augmenting the seed set, instead of labeling aword with a tag that receives 9 votes from the 45pairwise classifiers, we label a word with any tagthat receives at least 8 votes, effectively allowingthe assignment of more than one label to a word.However, our experimental results (not shown dueto space limitations) indicate that the incorporationof this method does not yield better overall per-formance, since many of the additional labels areerroneous and hence their presence deteriorates thequality of the bootstrapped data.8 ConclusionsWe have proposed a new bootstrapping algorithmfor unsupervised POS induction.
In contrast to ex-isting algorithms developed for this problem, ouralgorithm is designed to (1) operate under a re-source-scarce setting in which no language-specific tools or resources are available and (2)more tightly integrate morphological informationwith the distributional POS induction framework.In particular, our algorithm (1) improves the qual-ity of the seed clusters by employing seed words226that are distributionally and morphologically reli-able and (2) uses support vector learning to com-bine morphological and distributional information.Our results show that it outperforms Clark?s algo-rithm for English and Bengali, suggesting that it isapplicable to both morphologically impoverishedand highly inflectional languages.AcknowledgementsWe thank the five anonymous EMNLP-CoNLLreferees for their valuable comments.
We alsothank Zeeshan Abedin and Mahbubur RahmanHaque for creating the Bengali lexicon.ReferencesChris Biemann.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In Pro-ceedings of the COLING/ACL 2006 Student ResearchWorkshop.Leo Breiman.
1996.
Bagging predictors.
MachineLearning 24(2):123-140.Alexander Clark.
2000.
Inducing syntactic categories bycontext distributional clustering.
In Proceedings ofCoNLL, pages 91-94.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the EACL.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using Morfessor 1.0.
In Computerand Information Science, Report A81, Helsinki Uni-versity of Technology.Silviu Cucerzan and David Yarowsky.
2000.
Languageindependent, minimally supervised induction of lexi-cal probabilities.
In Proceedings of the ACL, pages270-277.Sajib Dasgupta and Vincent Ng.
2007.
High-performance, language-independent morphologicalsegmentation.
In Proceedings of NAACL-HLT, pages155-163.Kevin Duh and Katrin Kirchhoff.
2006.
Lexicon acqui-sition for dialectal Arabic using transductive learn-ing.
In Proceedings of EMNLP, pages 399-407.Dayne Freitag.
2004.
Toward unsupervised whole-corpus tagging.
In Proceedings of COLING, pages357-363.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
In ComputationalLinguistics 27(2):153-198.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT-NAACL, pages 320-327.Jan Haji.
2000.
Morphological tagging: Data vs. dic-tionaries.
In Proceedings of the NAACL, pages 94-101.Zellig Harris.
1954.
Distributional structure.
In Word,10(2/3):146-162.Michele Jardino and Gilles Adda.
1994.
Automatic de-termination of a stochastic bi-gram class languagemodel.
In Proceedings of Grammatical Inference andApplications, Second International Colloquium,ICGI-94, pages 57-65.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods ?Support Vector Learning, pages 44-56.
MIT Press.Sydney Lamb.
1961.
On the mechanization of syntacticanalysis.
In Proceedings of the 1961 Conference onMachine Translation of Languages and Applied Lan-guage Analysis, Volume 2, pages 674-685.
HMSO,London.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Bea-trice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313-330.Andrei Mikheev.
1997.
Automatic rule induction forunknown word-guessing.
Computational Linguistics,23(3):405-423.Goutam Kumar Saha, Amiya Baran Saha, and SudiptoDebnath.
2004.
Computer assisted Bangla wordsPOS tagging.
In Proceedings of the InternationalSymposium on Machine Translation NLP and TSS(iTRANS, 2004).Hinrich Sch?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the EACL, pages 141-148.Hinrich Sch?tze.
1997.
Ambiguity Resolution in Lan-guage Learning.
CSLI Publications.Noah Smith and Jason Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.
InProceedings of the ACL, pages 354-362.227
