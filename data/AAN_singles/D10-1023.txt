Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsModeling Organization in Student EssaysIsaac Persing and Alan Davis and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,alan,vince}@hlt.utdallas.eduAbstractAutomated essay scoring is one of the mostimportant educational applications of naturallanguage processing.
Recently, researchershave begun exploring methods of scoring es-says with respect to particular dimensions ofquality such as coherence, technical errors,and relevance to prompt, but there is rela-tively little work on modeling organization.We present a new annotated corpus and pro-pose heuristic-based and learning-based ap-proaches to scoring essays along the organi-zation dimension, utilizing techniques that in-volve sequence alignment, alignment kernels,and string kernels.1 IntroductionAutomated essay scoring, the task of employingcomputer technology to evaluate and score writ-ten text, is one of the most important educationalapplications of natural language processing (NLP)(see Shermis and Burstein (2003) and Shermis et al(2010) for an overview of the state of the art in thistask).
Recent years have seen a surge of interest inthis and other educational applications in the NLPcommunity, as evidenced by the panel discussionon ?Emerging Application Areas in ComputationalLinguistics?
at NAACL 2009, as well as increasedparticipation in the series of workshops on ?Innova-tive Use of NLP for Building Educational Applica-tions?.
Besides its potential commercial value, au-tomated essay scoring brings about a number of rel-atively less-studied but arguably rather challengingdiscourse-level problems that involve the computa-tional modeling of different facets of text structure,such as content, coherence, and organization.A major weakness of many existing essay scor-ing engines such as IntelliMetric (Elliot, 2001) andIntelligent Essay Assessor (Landauer et al, 2003)is that they adopt a holistic scoring scheme, whichsummarizes the quality of an essay with a singlescore and thus provides very limited feedback tothe writer.
In particular, it is not clear which di-mension of an essay (e.g., coherence, relevance)a score should be attributed to.
Recent work ad-dresses this problem by scoring a particular dimen-sion of essay quality such as coherence (Miltsakakiand Kukich, 2004), technical errors, and relevanceto prompt (Higgins et al, 2004).
Automated sys-tems that provide instructional feedback along mul-tiple dimensions of essay quality such as Criterion(Burstein et al, 2004) have also begun to emerge.Nevertheless, there is an essay scoring dimensionfor which few computational models have been de-veloped ?
organization.
Organization refers to thestructure of an essay.
A high score on organizationmeans that writers introduce a topic, state their po-sition on that topic, support their position, and con-clude, often by restating their position (Silva, 1993).A well-organized essay is structured in a way thatlogically develops an argument.
Note that organi-zation is a different facet of text structure than co-herence, which is concerned with the transition ofideas at both the global (e.g., paragraph) and local(e.g., sentence) levels.
While organization is an im-portant dimension of essay quality, state-of-the-artessay scoring software such as e-rater V.2 (Attaliand Burstein, 2006) employs rather simple heuristic-based methods for computing the score of an essayalong this particular dimension.Our goal in this paper is to develop a compu-tational model for the organization of student es-229says.
While many models of text coherence havebeen developed in recent years (e.g., Barzilay andLee (2004), Barzilay and Lapata (2005), Soricut andMarcu (2006), Elsner et al (2007)), the same is nottrue for text organization.
One reason is the avail-ability of training and test data for coherence mod-eling.
Coherence models are typically evaluated onthe sentence ordering task, and hence training andtest data can be generated simply by scrambling theorder of the sentences in a text.
On the other hand, itis not particularly easy to find poorly organized textsfor training and evaluating organization models.
Webelieve that student essays are an ideal source ofwell- and poorly-organized texts.
We evaluate ourorganization model on a data set of 1003 essays an-notated with organization scores.In sum, our contributions in this paper are two-fold.
First, we address a less-studied discourse-leveltask ?
predicting the organization score of an essay?
by developing a computational model of organi-zation, thus establishing a baseline against which fu-ture work on this task can be compared.
Second, weannotate a subset of our student essay corpus withorganization scores and make this data set publiclyavailable.
Since progress in organization modelingis hindered in part by the lack of a publicly anno-tated corpus, we believe that our data set will be avaluable resource to the NLP community.2 Corpus InformationWe use as our corpus the 4.5 million word Interna-tional Corpus of Learner English (ICLE) (Grangeret al, 2009), which consists of more than 6000 es-says written by university undergraduates from 16countries and 16 native languages who are learnersof English as a Foreign Language.
91% of the ICLEtexts are argumentative.
The essays we used varygreatly in length, containing an average of 31.1 sen-tences in 7.5 paragraphs, averaging 4.1 sentences perparagraph.
About one quarter of the essays had fiveor fewer paragraphs, and another quarter containednine or more paragraphs.
Similarly, about one quar-ter of essays contained 24 or fewer sentences and thelongest quarter contained 36 or more sentencesWe selected a subset consisting of 1003 essaysfrom the ICLE to annotate and use for training andtesting of our model of essay organization.
WhileTopic Languages EssaysMost university degrees aretheoretical and do not preparestudents for the real world.They are therefore of very lit-tle value.13 147The prison system is out-dated.
No civilized societyshould punish its criminals: itshould rehabilitate them.11 103In his novel Animal Farm,George Orwell wrote ?Allmen are equal but some aremore equal than others.?
Howtrue is this today?10 82Table 1: Some examples of writing topics.narrative writing asks students to compose descrip-tive stories, argumentative (also known as persua-sive) writing requires students to state their opinionon a topic and to validate that opinion with convinc-ing arguments.
For this reason, we selected only ar-gumentative essays rather than narrative pieces, be-cause they contain the discourse structures and kindof organization we are interested in modeling.To ensure representation across native languagesof the authors, we selected mostly essays writtenin response to topics which are well-represented inmultiple languages.
This avoids many issues thatmay arise when certain vocabulary is used in re-sponse to a particular topic for which essays writtenby authors from only a few languages are available.Table 1 shows three of the twelve topics selected forannotation.
Fifteen native languages are representedin the set of essays selected for annotation.3 Corpus AnnotationTo develop our essay organization model, human an-notators scored 1003 essays using guidelines in anessay annotation rubric.
Annotators evaluated theorganization of each essay using a numerical scorefrom 1 to 4 at half-point increments.
This contrastswith previous work on essay scoring, where the cor-pus is annotated with a binary decision (i.e., good orbad) for a given scoring dimension (e.g., Higgins etal.
(2004)).
Hence, our annotation scheme not onlyprovides a finer-grained distinction of organizationquality (which can be important in practice), but also230makes the prediction task more challenging.The meaning of each integer score was describedand discussed in detail.
Table 2 shows the descrip-tion of each score for the organization dimension.Score Description of Essay Organization4 essay is well structured and is organized ina way that logically develops an argument3 essay is fairly well structured but couldsomewhat benefit from reorganization2 essay is poorly structured and wouldgreatly benefit from reorganization1 essay is completely unstructured and re-quires major reorganizationTable 2: Descriptions of the meaning of each score.Our annotators were selected from over 30 appli-cants who were familiarized with the scoring rubricand given sample essays to score.
The six who weremost consistent with the expected scores were givenadditional essays to annotate.
To ensure consistencyin scoring, we randomly selected a large subset ofour corpus (846 essays) to have graded by two differ-ent annotators.
Analysis of these doubly annotatedessays reveals that, though annotators only exactlyagree on the organization score of an essay 29% ofthe time, the scores they apply are within 0.5 pointsin 71% of essays and within 1.0 point in 93% of es-says.
Additionally, if we treat one annotator?s scoresas a gold standard and the other annotator?s scoresas predictions, the predicted scores have a mean er-ror of 0.54 and a mean squared error of 0.50.
Table 3shows the number of essays that received each of theseven scores for organization.score 1.0 1.5 2.0 2.5 3.0 3.5 4.0essays 24 14 35 146 416 289 79Table 3: Distribution of organization scores.4 Function LabelingAs mentioned before, a high score on organizationmeans that writers introduce a topic, support theirposition, and conclude.
If one or more of these ele-ments are missing or if they appear out of order (e.g.,the conclusion appears before the introduction), theresulting essay will typically be considered poorlyorganized.
Hence, knowing the discourse functionlabel of each paragraph in an essay would be help-ful for predicting its organization score.Two questions naturally arise.
First, how can weobtain the discourse function label of each para-graph?
One way is to automatically acquire suchlabels from a corpus of student essays where eachparagraph is annotated with its discourse functionlabel.
To our knowledge, however, there is no pub-licly available corpus that is annotated with such in-formation.
As a result, we will resort to labeling aparagraph with its function label heuristically.Second, which paragraph function labels wouldbe most useful for scoring the organization of an es-say?
Based on our linguistic intuition, we identifyfour potentially useful paragraph function labels: In-troduction, Body, Rebuttal, and Conclusion.
Table 4gives the descriptions of these labels.Label Name Paragraph TypeI Introduction introduces essay topic andstates author?s position andmain ideasB Body provides reasons, evidence,and examples to support mainideasC Conclusion summarizes and concludes ar-guments made in body para-graphsR Rebuttal considers counter-argumentsto thesis or main ideasTable 4: Descriptions of paragraph function labels.Setting aside for the moment the problem of ex-actly how to predict an essay?s organization scoregiven its paragraph sequence, the problem of ob-taining paragraph labels to use for this task still re-mains.
As mentioned above, we adopt a heuristic ap-proach to paragraph function labeling.
The question,then, is: what kind of knowledge sources should ourheuristics be based on?
We have identified two typesof knowledge sources that are potentially useful forparagraph labeling.
The first of these are positional,dealing with where in the essay a paragraph appears.So for example, the first paragraph in an essay islikely to be an Introduction, while the last is likelyto be a Conclusion.
A paragraph in any other posi-tion, on the other hand, is more likely to be a Bodyor Rebuttal paragraph.231Label Name Sentence FunctionP Prompt restates the prompt given to the author and contains no new material or opinionsT Transition shifts the focus to new topics but contains no meaningful informationH Thesis states the author?s position on the topic for which he/she is arguingM Main Idea asserts reasons and foundational arguments that support the thesisE Elaboration further explains reasons and ideas but contains no evidence or examplesS Support provides evidence and examples to support the claims made in other statementsC Conclusion summarizes and concludes the entire argument or one of the main ideasR Rebuttal considers counter-arguments that contrast with the thesis or main ideasO Solution puts to rest the questions and problems brought up by counter-argumentsU Suggestion proposes solutions the problems brought up by the argumentTable 5: Descriptions of sentence function labels.A second potentially useful knowledge source in-volves the types of sentences appearing in a para-graph.
This idea presupposes that, like paragraphs,sentences too can have discourse function labels in-dicating the logical role they play in an argument.The sentence label schema we propose, which is de-scribed in Table 5, is based on work in discoursestructure by Burstein et al (2003), but features addi-tional sentence labels.To illustrate why these sentence function labelsmay be useful for paragraph labeling, consider aparagraph containing a Thesis sentence.
The pres-ence of a Thesis sentence is a strong indicator thatthe paragraph containing it is either an Introductionor Conclusion.
Similarly, a paragraph containingRebuttal or Solution sentences is more likely to bea Body or Rebuttal paragraph.Hence, to obtain a paragraph?s function label,we need to first label its sentences.
However, weare faced with the same problem: how can we ob-tain the sentence function labels?
One way is tolearn them from a corpus where each sentence ismanually annotated with its sentence function la-bel, which is the approach adopted by Burstein etal.
(2003).
However, this annotated corpus is notpublicly available.
In fact, to our knowledge, thereis no publicly-available corpus that is annotated withsentence function labels.
Consequently, we adopt aheuristic approach to sentence function labeling.Overall, we created a knowledge-lean set ofheuristic rules labeling paragraphs and sentences.Because many of the paragraph labeling heuristicsdepend on the availability of sentence labels, we willdescribe the sentence labeling heuristics first.
Foreach sentence function label x, we identify severalfeatures whose presence increases our confidencethat a given sentence is an example of x.
So forexample, the presence of any of the words ?agree?,?think?, or ?opinion?
increases our confidence thatthe sentence they occur in is a Thesis.
If the sentenceinstead contains words such as ?however?, ?but?,or ?argue?, these increase our confidence that thesentence is a Rebuttal.
The features we examinefor sentence labeling are not limited to words, how-ever.
Each content word the sentence shares withthe essay prompt gives us evidence that the sentenceis a restatement of the prompt.
Having searched asentence for all these clues, we finally assign thesentence the function label having the most supportamong the clues found.The heuristic rules for paragraph labeling are sim-ilar in nature, though they depend heavily on thelabels of a paragraph?s component sentences.
If aparagraph contains Thesis, Prompt, or Backgroundsentences, the paragraph is likely to be an Introduc-tion.
However, if a paragraph contains Main Idea,Support, or Conclusion sentences, it is likely to bea Body paragraph.
Finally, as mentioned previously,some positional information is used in labeling para-graphs.
For example, a paragraph that is the firstparagraph in an essay is likely to be an Introduction,but a paragraph that is neither the first nor the lastis likely to be either a Rebuttal or Body paragraph.After searching a paragraph for all these features,we gather the pieces of evidence in support of eachparagraph label and assign the paragraph the labelhaving the most support.11Space limitations preclude a complete listing of these para-2325 Heuristic-Based Organization ScoringHaving applied labels to each paragraph in an es-say, how can we use these labels to predict the es-say?s score?
Recall that the importance of each para-graph label stems not from the label itself, but fromthe sequence of labels it appears in.
Motivated bythis observation, we exploit a technique that is com-monly used in bioinformatics ?
sequence align-ment.
While sequence alignment has also been usedin text and paraphrase generation (e.g., Barzilay andLee (2002; 2003)), it has not been extensively ap-plied to other areas of language processing, includ-ing essay scoring.
In this section, we will presenttwo heuristic approaches to organization scoring,one based on aligning paragraph sequences and theother on aligning sentence sequences.5.1 Aligning Paragraph SequencesAs mentioned above, our first approach to heuristicorganization scoring involves aligning paragraph se-quences.
Specifically, this approach operates in twosteps.
Given an essay e in the test set, we (1) find thek essays in the training set that are most similar to evia paragraph sequence alignment, and then (2) pre-dict the organization score of e by aggregating thescores of its k nearest neighbors obtained in the firststep.
Below we describe these two steps in detail.First, to obtain the k nearest neighbors of e,we employ the Needleman-Wunsch alignment algo-rithm (Needleman and Wunsch, 1970), which com-putes a similarity score for any pair of essays byfinding an optimal alignment between their para-graph sequences.
To illustrate why we believe se-quence alignment can help us determine which es-says are most similar, consider two example es-says.
One essay, which we will call IBBBC, beginswith an Introductory paragraph, follows it with threeBody paragraphs, and finally ends with a Conclud-ing paragraph.
Another essay CRRRI begins witha paragraph stating its Conclusion, follows it withthree Rebuttal paragraphs, and ends with a para-graph Introducing the essay?s topic.
We can tell bya casual glance at the sequences that any reasonablesimilarity function should tell us that they are notgraph and sentence labeling heuristics.
See our website athttp://www.hlt.utdallas.edu/?alan/ICLE/ forthe complete list of heuristics.very similar.
The Needleman-Wunsch alignment al-gorithm has this effect since the score of the align-ment it produces would be hurt by the facts that (1)there is not much overlap in the sets of paragraphlabels each contains, and (2) the paragraph labelsthey do share (I and C) do not occur in the sameorder.
The resulting alignment would therefore con-tain many mismatches or indels.2If we now consider a third essay whose para-graph sequence could be represented as IBRBC, agood similarity function should tell us that IBBBCand IBRBC are very similar.
The Needleman-Wunsch alignment score between the two paragraphsequences has this property, as the alignment al-gorithm would discover that the two sequences areidentical except for the third paragraph label, whichcould be mismatched for a small penalty.
We wouldtherefore conclude that the IBBBC and IBRBC es-says should receive similar organization scores.To fully specify how to find the k nearest neigh-bors of an essay, we need to define a similarity func-tion between paragraph labels.
In sequence align-ment, similarity function S(i, j) tells us how likelyit is that symbol i (in our case, a paragraph label)will be substituted with another symbol j. Whilewe expect that in an alignment between high-scoringessays, an Introduction paragraph is most likely tobe aligned with another Introduction paragraph, howmuch worse should the alignment score be if an In-troduction paragraph needs to be mismatched witha Rebuttal paragraph or replaced with an indel?
Wesolve this problem by heuristically defining the sim-ilarity function as follows: S(i, j) = 1 when i = j,S(i, j) = ?1 when i 6= j, and also S(i,?)
=S(?, i) = ?1, where ???
is an indel.
In otherwords, the similarity function encourages the align-ment between two identical function labels and dis-courages the alignment between two different func-tion labels, regardless of the type of function labels.After obtaining the k nearest neighbors of e, thenext step is to predict the organization score of eby aggregating the scores of its k nearest neighborsinto one number.
(Note that we know the organiza-2In pairwise sequence alignment, a mismatch occurs whenone symbol has to be substituted for another to make two se-quences match.
An indel indicates that in order to transformone sequence to match another, we must either insert a symbolinto one sequence or delete a symbol from the other sequence.233tion score of each nearest neighbor, since they areall taken from the training set.)
One natural way todo this would be to take the mean, median, or modeof its k nearest neighboring essays from the trainingset.
Hence, our first heuristic method Hp for scoringorganization has three variants.5.2 Aligning Sentence SequencesAn essay?s paragraph sequence captures informationabout its organization at a high level, but ignoresmuch of its lower level structure.
Since we have alsoheuristically labeled sentences, it now makes senseto examine the sequences of sentence function labelswithin an essay?s paragraphs.
The intuition is that atleast some portion of an essay?s organization scorecan be attributed to the organization of the sentencesequences of its component paragraphs.To address this concern, we propose a secondheuristic approach to organization scoring.
Givena test essay e, we first find for each paragraph ine the k paragraphs in the training set that are mostsimilar to it.
Specifically, each paragraph is repre-sented by its sequence of sentence function labels.Given this paragraph representation, we can find thek nearest neighbors of a paragraph by applying theNeedleman-Wunsch algorithm described in the pre-vious subsection to align sentence sequences, usingthe same similarity function we defined above.Next, we score each paragraph pi by aggregatingthe scores of its k nearest neighbors obtained in thefirst step, assuming the score of a nearest neighborparagraph is the same as the organization score ofthe training set essay containing it.
As before, wecan employ the mean, median, or mode to aggregatethe scores of the nearest neighbors of pi.Finally, we predict the organization score of e byaggregating the scores of its paragraphs obtained inthe second step.
Again, we can employ mean, me-dian, or mode to aggregate the scores.
Since we havethree ways of aggregating the scores of a paragraph?snearest neighbors and three ways of aggregating theresulting paragraph scores, this second method Hsfor scoring organization has nine variants.6 Learning-Based Organization ScoringIn the previous section, we proposed two heuris-tic approaches to organization scoring, one basedon aligning paragraph label sequences and the otherbased on aligning sentence label sequences.
In theprocess of constructing these two systems, however,we created a lot of information about the essayswhich might also be useful for organization scoring,but which the heuristic systems are unable to exploit.To remedy the problem, we introduce three learning-based systems which abstract the additional infor-mation we produced in three different ways.
In eachsystem, we use the SVMlight (Joachims, 1999) im-plementation of regression support vector machines(SVMs) (Cortes and Vapnik, 1995) to train a regres-sor because SVMs have been frequently and suc-cessfully applied to a variety of NLP problems.6.1 Linear KernelOwing to the different ways we presented of com-bining the scores of an essay?s nearest neighbors,the paragraph label sequence alignment approachhas three variants, and its sentence label sequencealignment counterpart has nine.
Unfortunately, theseheuristic approaches suffer from two major weak-nesses.
First, it is not intuitively clear which ofthese 12 ways for predicting an essay?s organiza-tion score is clearly better than the others.
Second,it is not clear that the k nearest neighbors of an es-say will always be similar to it with respect to or-ganization score.
While we do expect the alignmentscores between good essays with reasonable para-graph sequences to be high, poorly organized es-says by their nature have more random paragraphsequences.
Hence, we have no intuition about the knearest neighbors of a poor essay, as it may have ashigh an alignment score with another poorly orga-nized essay as with a good essay.Our solution to these problems is to use the orga-nization scores obtained by the 12 heuristic variantsas features in a linear kernel SVM learner.
We be-lieve that using the estimates given by all the 12 vari-ants of the two heuristic approaches rather than onlyone of them addresses the first weakness mentionedabove.
The second weakness, on the other hand, isaddressed by treating the organization score predic-tions obtained by the nearest neighbor methods asfeatures for an SVM learner rather than as estimatesof an essay?s organization score.The approach we have just described, however,does not exploit the full power of linear kernel234SVMs.
One strength of linear kernels is that theymake it easy to incorporate a wide variety of dif-ferent types of features.
In an attempt to furtherenhance the prediction capability of the SVM re-gressor, we will provide it with not only the 12 fea-tures derived from the heuristic-based approaches,but also with two additional types of features.First, to give our learner more direct access tothe information we used to heuristically predict es-say scores, we can extract paragraph label subse-quences3 from each essay and use them as features.To illustrate the intuition behind these features, con-sider two paragraph subsequences: Introduction?Body and Rebuttal?Introduction.
It is fairly typi-cal to see the first subsequence, I?B, at the begin-ning of a good essay, so its occurrence should giveus a small amount of evidence that the essay it oc-curs in is well-organized.
The presence of the sec-ond subsequence, R?I, however, should indicate thatits essay?s organization is poor because, in general, agood essay should not give a Rebuttal before an In-troduction.
Because we can envision subsequencesof various lengths being useful, we create a binarypresence or absence feature in the linear kernel foreach paragraph subsequence of length 1, 2, 3, 4, or5 appearing in the training set.Second, we employ sentence label subsequencesas features in the linear kernel.
Recall that whendescribing our alignment-based nearest neighbororganization score prediction methods, we notedthat an essay?s organization score may be partiallyattributable to how well the sentences within itsparagraphs are organized.
For example, if oneof an essay?s paragraphs contains the sentence la-bel subsequence Main Idea?Elaboration?Support?Conclusion this gives us some evidence that the es-say is overall well-organized since one of its compo-nent paragraphs contains this reasonably-organizedsubsequence.
An essay with a paragraph contain-ing the subsequence Conclusion?Support?Thesis?Rebuttal, however, is likely to be poorly orga-nized because this is a poorly-organized subse-quence.
Since sentence label subsequences of dif-fering lengths may be useful for score prediction, wecreate a binary presence or absence feature for eachsentence label subsequence of length 1, 2, 3, 4, or 53Note that a subsequence is not necessarily contiguous.in the training set.While the number of nearest neighbor features ismanageable, the presence of a large number of fea-tures can sometimes confuse a learner.
For that rea-son, we do feature selection on the two types ofsubsequence features, selecting only 100 featuresfor each type that has the highest information gain(see Yang and Pedersen (1997) for details).
Wecall the system resulting from the use of these threetypes of features Rlnps because it uses Regressionwith linear kernel to predict essay scores, and ituses nearest neighbor, paragraph subsequence, andsentence subsequence features.6.2 String KernelIn a traditional learning setting, the feature set em-ployed by an off-the-shelf learning algorithm typ-ically consists of flat features (i.e., features whosevalues are discrete- or real-valued, as the ones de-scribed in the Linear Kernel subsection).
Advancedmachine learning algorithms such as SVMs, on theother hand, have enabled the use of structured fea-tures (i.e., features whose values are structures suchas parse trees and sequences), owing to their abilityto employ kernels to efficiently compute the similar-ity between two potentially complex structures.Perhaps the most obvious advantage of employ-ing structured features is simplicity.
To understandthis advantage, consider learning in a traditional set-ting.
Recall that we can only employ flat features inthis setting, as we did with the linear kernel.
Hence,if we want to use information from a parse tree asfeatures, we will need to design heuristics to extractthe desired parse-based features from parse trees.For certain tasks, designing a good set of heuris-tics can be time-consuming and sometimes difficult.On the other hand, SVMs enable a parse tree tobe employed directly as a structured feature, obvi-ating the need to design heuristics to extract infor-mation from potentially complex structures.
How-ever, structured features have only been applied to ahandful of NLP tasks such as semantic role labeling(Moschitti, 2004), syntactic parsing and named en-tity identification (Collins and Duffy, 2002), relationextraction (Bunescu and Mooney, 2005), and coref-erence resolution (Versley et al, 2008).
Our goalhere is to explore this rarely-exploited capability ofSVMs for the task of essay scoring.235While the vast majority of previous NLP workon using structured features have involved tree ker-nels, we employ a kernel that is rarely investigated inNLP: string kernels (Lodhi et al, 2002).
Informally,a string kernel aims to efficiently compute the sim-ilarity between two strings (or sequences) of sym-bols based on the similarity of their subsequences.We apply string kernels to essay scoring as follows:we represent each essay using its paragraph functionlabel sequence, and employ a string kernel to com-pute the similarity between two essays based on thisrepresentation.
Typically, a string kernel takes as in-put two parameters: K (which specifies the lengthof the subsequences in the two strings to compare)and ?
(which is a value between 0 and 1 that spec-ifies whether matches between non-contiguous sub-sequences in the two strings should be consideredas important as matches between contiguous subse-quences).
In our experiments, we select values forthese parameters in a somewhat arbitrary manner.
Inparticular, since ?
ranges between 0 and 1, we sim-ply set it to 0.5.
For K , since in the flat features weconsidered all paragraph label sequences of lengthsfrom 1 to 5, we again take the middle value, settingit to 3.
We call the system using this kernel Rs be-cause it uses a Regression SVM with a string kernelto predict essay scores.6.3 Alignment KernelIn general, the purpose of a kernel function is tomeasure the similarity between two examples.
Thestring kernel we described in the previous subsec-tion is just one way of measuring the similarity oftwo essays given their paragraph sequences.
Whilethis may be the most obvious way to use paragraphsequence information from a machine learning per-spective, our earlier use of the Needleman-Wunschalgorithm suggests a more direct way of extractingstructured information from paragraph sequences.More specifically, recall that the Needleman-Wunsch algorithm finds an optimal alignment be-tween two paragraph sequences, where an opti-mal alignment is defined as an alignment havingthe highest possible alignment score.
The optimalalignment score can be viewed as another similar-ity measure between two essays.
As such, withsome slight modifications, the alignment score be-tween two paragraph sequences can be used as thekernel value for an Alignment Kernel.4 We callthe system using this kernel Ra because it uses aRegression SVM with an alignment kernel to pre-dict essay scores.6.4 Combining KernelsRecall that the flat features are computed using a lin-ear kernel, while the two types of structured featuresare computed using string and alignment kernels.
Ifwe want our learner to make use of more than one ofthese types of features, we need to employ a compos-ite kernel to combine them.
Specifically, we defineand employ the following composite kernel:Kc(F1, F2) =1nn?i=1Ki(F1, F2),where F1 and F2 are the full set of features (contain-ing both flat and structured features) that representthe two essays under consideration, Ki is the ith ker-nel we are combining, and n is the number of kernelswe are combining.
To ensure that each kernel underconsideration contributes equally to the compositekernel, each kernel value Ki(F1, F2) is normalizedso that its value falls between 0 and 1.7 Evaluation7.1 Evaluation MetricsWe designed three evaluation metrics to measure theerror of our organization scoring system.
The sim-plest metric, S1, is perhaps the most intuitive.
Itmeasures the frequency at which a system predictsthe wrong score out of the seven possible scores.Hence, a system that predicts the right score only25% of the time would receive an S1 score of 0.75.The S2 metric is slightly less intuitive than S1,but no less reasonable.
It measures the averagedistance between the system?s score and the actualscore.
This metric reflects the idea that a systemthat estimates scores close to the annotator-assignedscores should be preferred over a system whose esti-mations are further off, even if both systems estimatethe correct score at the same frequency.Finally, the S3 evaluation metric measures theaverage square of the distance between a system?s4In particular, we note that for theoretical reasons, a kernelfunction must always return a non-negative value.
The align-ment score function does not have this property, so we increaseall alignment scores until their theoretical minimum value is 0.236organization score estimations and the annotator-assigned scores.
The intuition behind this systemis that not only should we prefer a system whose es-timations are close to the annotator scores, but weshould also prefer one whose estimations are not toofrequently very far away from the annotator scores.These three scores are given by:1N?Ai 6=Ei1, 1NN?i=1|Ai ?
Ei|,1NN?i=1(Ai ?
Ei)2,where Ai and Ei are the annotator assigned and sys-tem estimated scores respectively for essay i, and Nis the number of essays.
Since many of the systemswe have described assign test essays real-valued or-ganization scores, to obtain Ei for system S1 weround the outputs of each system to the nearest ofthe seven scores the human annotators were permit-ted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0).To test our system, we performed 5-fold cross val-idation on our 1003 essay set, micro-averaging ourresults into three scores corresponding to the threescoring metrics described above.7.2 Results and DiscussionThe average baseline.
As mentioned before, thereis no standard baseline for organization modelingagainst which we can compare our systems.
To startwith, we employ a simple ?average?
baseline.
Avgcomputes the average organization score of essaysin the training set and assigns this score to each testset essay.
Results of this baseline are shown in row1 of Table 6.
Though simple, this baseline is by nomeans easy-to-beat, since 41% of the essays have ascore of 3, and 96% of the essays have a score thatis within one point of 3.Heuristic baselines.
Recall that we have 12 ver-sions of the two heuristic approaches to organizationprediction.
Space limitations preclude a discussionof the results of all these versions, so instead, to ob-tain the strongest baseline results, we show only thebest results achieved by the three versions based onaligning paragraph label sequences in row 2 (Hp)and the best results achieved by the nine versionsbased on aligning sentence label sequences in row3 (Hs) of Table 6.
It is clear from the results thatthe Hp systems yielded the best baseline predictionsunder all three scoring metrics, performing signif-icantly better than both the Avg and Hs systemsSystem S1 S2 S31 Avg .585 .412 .3482 Hp .548 .339 .1983 Hs .575 .397 .3294 Rlnps .520 .331 .1865 Rs .577 .369 .2226 Ra .686 .519 .4297 Rlsnps .534 .332 .1878 Rlanps .541 .332 .1789 Rsa .517 .325 .17710 Rlsanps .517 .323 .175Table 6: System Performance(p < 0.01) with respect to the S2 and S3 metrics,but its S1 performance is less significant with re-spect to Avg (p < 0.1) and is indistinguishable ateven the p < 0.1 level from Hs.5 In general, how-ever, it appears to be the case that systems basedon aligning paragraph label sequences achieve betterresults than systems that attempt to align sentencelabel sequences.Learning-based approaches.
Rows 4?6 of Table6 show the results we obtained using each of thethree single-kernel systems.
When compared to thebest baseline, these results suggest that Hp is a prettygood heuristic approach to organization scoring.
Infact, only one of these three learning-based sys-tems (Rlnps) performs better than Hp under the threescoring metrics, and in each case, the difference be-tween the two is not significant even at p < 0.1.
Thissuggests that, even though Rlnps performs slightlybetter than Hp, the only major benefit we have ob-tained by using the linear kernel is that it has madeit unnecessary for us to choose between the 12 pro-posed heuristic systems.Considering that the second best one-kernel sys-tem, Rs, does not have access to any of the near-est neighbor features, which have already provenuseful, its performance seems reasonably good inthat its performance is at least better than the Avgsystem.
This suggests that, even though Rs doesnot perform exceptionally, it is extracting some use-ful information for organization scoring from theheuristically assigned paragraph label sequences.The best one-kernel system, Rlnps, however, is sig-5All significance tests are two-tailed paired t-tests.237nificantly better than Rs with respect to all threescoring metrics, with p < 0.1 for S1 and p < 0.05for S2 and S3.
By contrast, it initially appears thatthe alignment kernel is not extracting any usefulinformation from these paragraph sequences at all,since its S1, S2, and S3 scores are all much worsethan all of the baseline systems.
The second bestone-kernel system Rs performs significantly betterthan Ra at p < 0.01 for all three scoring metrics.Next, we explore the impact of composite kernels,which allow our learners to make use of multipletypes of flat and structured features.
Specifically, theresults shown in rows 7?9 are obtained by combin-ing two kernels at a time.
These experiments revealthe surprising result that the two worst performingsingle-kernel systems, Rs and Ra, when combinedinto Rsa, yield the best two-kernel system results,which are significant with respect to the best one-kernel system results under S3 at p < 0.1.
This re-sult suggests that these two different methods of ex-tracting information from paragraph sequences pro-vide us with different kinds of evidence useful fororganization scoring, although neither method by it-self was exceptionally useful.
Though Rsa doesnot have any access to nearest neighbor informa-tion, it still performs significantly better than Hp atp < 0.05 under S1 and S3.While we have already pointed out that Rsa isthe best composite two-kernel system, it is not clearwhich of Rlsnps and Rlanps is second-best.
Neithersystem consistently performs better than the otherunder all three scoring metrics, and the differencesbetween them are not significant even at p < 0.1.
Itis clear only that Rsa is better than both, as its scoresare statistically significantly better at p < 0.01 withrespect to Rlsnps and Rlanps under at least one ofthe three scoring metrics in each case.Finally, in the last row of Table 6, we combineall three kernels into one SVM learner.
The mostimportant lesson we learn from this experiment isthat each of the three kernels provides the learnerwith a different kind of useful information, so thata composite kernel using all three sources of in-formation performs better than any system usingfewer kernels.
Although the improvements over thebest two-kernel system (Rsa) and one-kernel sys-tem (Rlnps) are small, they are still statistically sig-nificant at p < 0.1 under one of the scoring metrics,S3.
When we compare this combined system to thebest baseline (Hp), we discover the improvementsderived from the three-kernel system are significantimprovements over it at p < 0.05 and p < 0.01 withrespect to S1 and S3 respectively.Feature analysis.
To better understand which ofthe three flat features (nearest neighbors, paragraphlabel sequences, or sentence label sequences) con-tributes the most to the linear kernel portion of thesystems?
performances, we analyze the three fea-ture types on Rlnps using the backward eliminationfeature selection algorithm.
First, we remove eachof the three feature groups independently from theRlnps?s feature set and determine which of the threeremovals yields the best performance according toeach scoring metric.
Next, among the remainingtwo feature groups, we repeat the same step, remov-ing each of the two groups independently from thefeature set to determine which of the two removalsyields the best performance.While space limitations preclude showing the ac-tual numbers, the trend is consistent among all threescoring metrics: the first feature type to removeis paragraph sequences (meaning that they are theleast important) and the last to remove is the near-est neighbor features.
Nevertheless, performance al-ways drops when a feature type is removed, indicat-ing that all three feature types contribute positivelyto overall performance.
The fact that flat paragraphsequence features proved to be least useful high-lights the importance of the structured methods wepresented for using paragraph sequence information.8 ConclusionsWe have investigated the relatively less-studiedproblem of modeling the organization in student es-says.
The contributions of our work include thenovel application of two techniques from bioinfor-matics and machine learning ?
sequence align-ment and string kernels, as well as the introduc-tion of alignment kernels ?
to essay scoring.
Weshowed that each technique makes a significant con-tribution to a scoring system, and we hope that thiswork will increase awareness of these powerful tech-niques among NLP researchers.
Finally, to stimulatework on this problem, we make our corpus of anno-tated essays available to other researchers.238AcknowledgmentsWe thank the three reviewers for their comments.Our six annotators, Andrew Hubbs, Karin Khoo,Jayne Koath, Christopher Maier, Andrew Mallon,and Cory Thornton, all deserve numerous thanks,because without the countless hours they each spentannotating hundreds of essays, none of the researchdescribed in this paper would have been possible.ReferencesYigal Attali and Jill Burstein.
2006.
Automated es-say scoring with e-rater V.2.
Journal of Technology,Learning, and Assessment, 4(3).Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the ACL, pages 141?148.Regina Barzilay and Lillian Lee.
2002.
Bootstrappinglexical choice via multiple-sequence alignment.
InProceedings of EMNLP, pages 164?171.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In HLT-NAACL 2003: Main Pro-ceedings, pages 16?23.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Main Proceedings, pages 113?120.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of HLT/EMNLP, pages 724?731.Jill Burstein, Martin Chodorow, and Claudia Leacock.2004.
Automated essay evaluation: The Criterion on-line writing evaluation service.
AI Magazine, 25(3),27?36.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the write stuff: Automatic identification ofdiscourse structure in student essays.
IEEE IntelligentSystems, 18(1):32?39.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of the ACL, pages 263?270.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.Scott Elliot.
2001.
IntelliMetric: From here to validity.Paper presented at the annual meeting of the AmericanEducational Research Association, Seattle, WA.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for discoursecoherence.
In NAACL HLT 2007: Proceedings of theMain Conference, pages 436?443.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses universitaires deLouvain.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects of co-herence in student essays.
In HLT-NAACL 2004: MainProceedings, pages 185?192.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scholkopf and Alexan-der Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning, pages 44?56.
MIT Press.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz.
2003.
Automated scoring and annotation ofessays with the Intelligent Essay AssessorTM.
In Auto-mated Essay Scoring: A Cross-Disciplinary Perspec-tive, pages 87?112.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Christopher J. C. H. Watkins.
2002.Text classification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring systems.Natural Language Engineering, 10(1):25?55.Alessandro Moschitti.
2004.
A study on convolution ker-nels for shallow statistic parsing.
In Proceedings of theACL, pages 335?342.Saul Ben Needleman and Christian Dennis Wunsch.1970.
A general method applicable to the search forsimilarities in the amino acid sequence of two proteins.Journal of Molecular Biology, 48(3):443?453, March.Mark Shermis and Jill Burstein.
2003.
Automated EssayScoring: A Cross-Disciplinary Perspective.
LawrenceErlbaum Associates, Inc., Mahwah, NJ.Mark Shermis, Jill Burstein, Derrick Higgins, and KlausZechner.
2010.
Automated essay scoring: Writingassessment and instruction.
In International Encyclo-pedia of Education (3rd edition), pages 20?26.Tony Silva.
1993.
Toward an understanding of the dis-tinct nature of L2 writing: The ESL research and itsimplications.
27(4):657?677.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 803?810.Yannick Versley, Alessandro Moschitti, Massimo Poe-sio, and Xiaofeng Yang.
2008.
Coreference systemsbased on kernels methods.
In Proceedings of COL-ING, pages 961?968.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
InProceedings of ICML, pages 412?420.239
