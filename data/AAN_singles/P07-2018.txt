Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69?72,Prague, June 2007. c?2007 Association for Computational LinguisticsRethinking Chinese Word Segmentation: Tokenization, CharacterClassification, or Wordbreak IdentificationChu-Ren HuangInstitute of LinguisticsAcademia Sinica,Taiwanchuren@gate.sinica.edu.twPetr S?imonInstitute of LinguisticsAcademia Sinica,Taiwansim@klubko.netShu-Kai HsiehDoFLALNIU, Taiwanshukai@gmail.comLaurent Pre?votCLLE-ERSS, CNRSUniversite?
de Toulouse, Franceprevot@univ-tlse2.frAbstractThis paper addresses two remaining chal-lenges in Chinese word segmentation.
Thechallenge in HLT is to find a robust seg-mentation method that requires no prior lex-ical knowledge and no extensive training toadapt to new types of data.
The challengein modelling human cognition and acqui-sition it to segment words efficiently with-out using knowledge of wordhood.
We pro-pose a radical method of word segmenta-tion to meet both challenges.
The mostcritical concept that we introduce is thatChinese word segmentation is the classifi-cation of a string of character-boundaries(CB?s) into either word-boundaries (WB?s)and non-word-boundaries.
In Chinese, CB?sare delimited and distributed in between twocharacters.
Hence we can use the distri-butional properties of CB among the back-ground character strings to predict whichCB?s are WB?s.1 Introduction: modeling and theoreticalchallengesThe fact that word segmentation remains a main re-search topic in the field of Chinese language pro-cessing indicates that there maybe unresolved theo-retical and processing issues.
In terms of processing,the fact is that none of exiting algorithms is robustenough to reliably segment unfamiliar types of textsbefore fine-tuning with massive training data.
It istrue that performance of participating teams havesteadily improved since the first SigHAN Chinesesegmentation bakeoff (Sproat and Emerson, 2004).Bakeoff 3 in 2006 produced best f-scores at 95%and higher.
However, these can only be achieved af-ter training with the pre-segmented training dataset.This is still very far away from real-world applica-tion where any varieties of Chinese texts must besuccessfully segmented without prior training forHLT applications.In terms of modeling, all exiting algorithms sufferfrom the same dilemma.
Word segmentation is sup-posed to identify word boundaries in a running text,and words defined by these boundaries are then com-pared with the mental/electronic lexicon for POStagging and meaning assignments.
All existing seg-mentation algorithms, however, presuppose and/orutilize a large lexical databases (e.g.
(Chen and Liu,1992) and many subsequent works), or uses the po-sition of characters in a word as the basis for seg-mentation (Xue, 2003).In terms of processing model, this is a contradic-tion since segmentation should be the pre-requisiteof dictionary lookup and should not presuppose lex-ical information.
In terms of cognitive modeling,such as for acquisition, the model must be able to ac-count for how words can be successfully segmentedand learned by a child/speaker without formal train-ing or a priori knowledge of that word.
All currentmodels assume comprehensive lexical knowledge.2 Previous workTokenization model.
The classical model, de-scribed in (Chen and Liu, 1992) and still adopted inmany recent works, considers text segmentation as a69tokenization.
Segmentation is typically divided intotwo stages: dictionary lookup and out of vocabulary(OOV) word identification.
This approach requirescomparing and matching tens of thousands of dic-tionary entries in addition to guessing thousands ofOOV words.
That is, this is a 104x104 scale map-ping problem with unavoidable data sparseness.More precisely the task consist in findingall sequences of characters Ci, .
.
.
, Cn such that[Ci, .
.
.
Cn] either matches an entry in the lexiconor is guessed to be so by an unknown word resolu-tion algorithm.
One typical kind of the complexitythis model faces is the overlapping ambiguity wheree.g.
a string [Ci ?
1, Ci, Ci + 1] contains multiplesubstrings, such as [Ci ?
1, Ci, ] and [Ci,Ci + 1],which are entries in the dictionary.
The degree ofsuch ambiguities is estimated to fall between 5% to20% (Chiang et al, 1996; Meng and Ip, 1999).2.1 Character classification modelA popular recent innovation addresses the scaleand sparseness problem by modeling segmentationas character classification (Xue, 2003; Gao et al,2004).
This approach observes that by classifyingcharacters as word-initial, word-final, penultimate,etc., word segmentation can be reduced to a simpleclassification problem which involves about 6,000characters and around 10 positional classes.
Hencethe complexity is reduced and the data sparsenessproblem resolved.
It is not surprising then that thecharacter classification approach consistently yieldsbetter results than the tokenization approach.
Thisapproach, however, still leaves two fundamentalquestions unanswered.
In terms of modeling, us-ing character classification to predict segmentationnot only increases the complexity but also necessar-ily creates a lower ceiling of performance In termsof language use, actual distribution of characters isaffected by various factors involving linguistic vari-ation, such as topic, genre, region, etc.
Hence therobustness of the character classification approachis restricted.The character classification model typically clas-sifies all characters present in a string into at leastthree classes: word Initial, Middle or Final po-sitions, with possible additional classification forword-middle characters.
Word boundaries are in-ferred based on the character classes of ?Initial?
or?Final?.This method typically yields better result than thetokenization model.
For instance, Huang and Zhao(2006) claims to have a f-score of around 97% forvarious SIGHAN bakeoff tasks.3 A radical modelWe propose a radical model that returns to thecore issue of word segmentation in Chinese.
Cru-cially, we no longer pre-suppose any lexical knowl-edge.
Any unsegmented text is viewed as a stringof character-breaks (CB?s) which are evenly dis-tributed and delimited by characters.
The charactersare not considered as components of words, instead,they are contextual background providing informa-tion about the likelihood of whether each CB is alsoa wordbreak (WB).
In other words, we model Chi-nese word segmentation as wordbreak (WB) iden-tification which takes all CB?s as candidates andreturns a subset which also serves as wordbreaks.More crucially, this model can be trained efficientlywith a small corpus marked with wordbreaks anddoes not require any lexical database.3.1 General ideaAny Chinese text is envisioned as se-quence of characters and character-boundariesCB0C1CB1C2 .
.
.
CBi?1CiCBi .
.
.
CBn?1CnCBn Thesegmentation task is reduced to finding all CBswhich are also wordbreaks WB.3.2 Modeling character-based informationSince CBs are all the same and do not carry anyinformation, we have to rely on their distributionamong different characters to obtain useful infor-mation for modeling.
In a segmented corpus, eachWB can be differentiated from a non-WB CB by thecharacter string before and after it.
We can assumea reduced model where either one character imme-diately before and after a CB is considered or twocharacters (bigram).
These options correspond toconsider (i) only word-initial and word-final posi-tions (hereafter the 2-CB-model or 2CBM) or (ii) toadd second and penultimate positions (hereafter the4-CB-model or 4CBM).
All these positions are well-attested as morphologically significant.703.3 The nature of segmentationIt is important to note that in this approaches,although characters are recognized, unlike (Xue,2003) and Huang et al (2006), charactes simplyare in the background.
That is, they are the neces-sary delimiter, which allows us to look at the stringof CB?s and obtaining distributional information ofthem.4 Implementation and experimentsIn this section we slightly change our notation toallow for more precise explanation.
As noted be-fore, Chinese text can be formalized as a sequenceof characters and intervals as illustrated in we callthis representation an interval form.c1I1c2I2 .
.
.
cn?1In?1cn.In such a representation, each interval Ik is eitherclassified as a plain character boundary (CB) or asa word boundary (WB).We represent the neighborhood of the characterci as (ci?2, Ii?2, ci?1, Ii?1, ci, Ii, ci+1, Ii+1), whichwe can be simplified as (I?2, I?1, ci, I+1, I+2) byremoving all the neighboring characters and retain-ing only the intervals.4.1 Data collection modelsThis section makes use of the notation introducedabove for presenting several models accounting forcharacter-interval class co-occurrence.Word based model.
In this model, statistical dataabout word boundary frequencies for each characteris retrieved word-wise.
For example, in the case ofa monosyllabic word only two word boundaries areconsidered: one before and one after the characterthat constitutes the monosyllabic word in question.The method consists in mapping all the Chinesecharacters available in the training corpus to a vectorof word boundary frequencies.
These frequenciesare normalized by the total frequency of the char-acter in a corpus and thus represent probability of aword boundary occurring at a specified position withregard to the character.Let us consider for example, a tri-syllabic wordW = c1c2c3, that can be rewritten as the followinginterval form as W I = IB?1c1IN1 c2IN2 c3IB3 .In this interval form, each interval Ik is markedas word boundary B or N for intervals within words.When we consider a particular character c1 in W ,there is a word boundary at index?1 and 3.
We storethis information in a mapping c1 = {?1 : 1, 3 : 1}.For each occurrence of this character in the corpus,we modify the character vector accordingly, eachWB corresponding to an increment of the relevantposition in the vector.
Every character in every wordof the corpus in processed in a similar way.Obviously, each character yields only informationabout positions of word boundaries of a word thisparticular character belongs to.
This means that theindex I?1 and I3 are not necessarily incrementedeverytime (e.g.
for monosyllabic and bi-syllabicwords)Sliding window model.
This model does not op-erate on words, but within a window of a give size(span) sliding through the corpus.
We have exper-imented this method with a window of size 4.
Letus consider a string, s = ?c1c2c3c4?
which is notnecessarily a word and is rewritten into an intervalform as sI = ?c1I1c2I2c3I3c4I4?.
We store theco-occurrence character/word boundaries informa-tion in a fixed size (span) vector.For example, we collect the information forcharacter c3 and thus arrive at a vector c3 =(I1, I2, I3, I4), where 1 is incremented at the respec-tive position ifIk = WB, zero otherwise.This model provides slightly different informa-tion that the previous one.
For example, ifa sequence of four characters is segmented asc1IN1 c2IB2 c3IB3 c4IB4 (a sequence of one bi-syllabicand two monosyllabic words), for c3 we would alsoget probability of I4, i.e.
an interval with index +2.
In other words, this model enables to learn WBprobability across words.4.2 Training corpusIn the next step, we convert our training corpus intoa corpus of interval vectors of specified dimension.Let?s assume we are using dimension span = 4.Each value in such a vector represents the proba-bility of this interval to be a word boundary.
Thisprobability is assigned by character for each positionwith regard to the interval.
For example, we havesegmented corpus C = c1I1c2I2 .
.
.
cn?1In?1cn,where each Ik is labeled as B for word boundaryor N for non-boundary.71In the second step, we move our 4-sized windowthrough the corpus and for each interval we querya character at the corresponding position from theinterval to retrieve the word boundary occurrenceprobability.
This procedure provides us with a vec-tor of 4 probability values for each interval.
Sincewe are creating this training corpus from an alreadysegmented text, a class (B or N ) is assigned to eachinterval.The testing corpus (unsegmented) is encoded in asimilar way, but does not contain the class labels Band N .Finally, we automatically assign probability of 0.5for unseen events.4.3 Predicting word boundary with a classifierThe Sinica corpus contains 6820 types of characters(including Chinese characters, numbers, punctua-tion, Latin alphabet, etc.).
When the Sinica corpus isconverted into our interval vector corpus, it provides14.4 million labeled interval vectors.
In this firststudy we have implement a baseline model, withoutany pre-processing of punctuation, numbers, names.A decision tree classifier (Ruggieri, 2004) hasbeen adopted to overcome the non-linearity issue.The classifier was trained on the whole Sinica cor-pus, i.e.
on 14.4 million interval vectors.
Due tospace limit, actual bakeoff experiment result will bereported in our poster presentation.Our best results is based on the sliding windowmodel, which provides better results.
It has to beemphasized that the test corpora were not processedin any way, i.e.
our method is sufficiently robust toaccount for a large number of ambiguities like nu-merals, foreign words.5 ConclusionIn this paper, we presented a radical and robustmodel of Chinese segmentation which is supportedby initial experiment results.
The model does notpre-suppose any lexical information and it treatscharacter strings as context which provides infor-mation on the possible classification of character-breaks as word-breaks.
We are confident that oncea standard model of pre-segmentation, using tex-tual encoding information to identify WB?s whichinvolves non-Chinese characters, will enable us toachieve even better results.
In addition, we are look-ing at other alternative formalisms and tools to im-plement this model to achieve the optimal results.Other possible extensions including experiments tosimulate acquisition of wordhood knowledge to pro-vide support of cognitive modeling, similar to thesimulation work on categorization in Chinese by(Redington et al, 1995).
Last, but not the least,we will explore the possibility of implementing asharable tool for robust segmentation for all Chinesetexts without training.ReferencesAcademia Sinica Balanced Corpus of Modern Chinese.http://www.sinica.edu.tw/SinicaCorpus/Chen K.J and Liu S.H.
1992.
Word Identification forMandarin Chinese sentences.
Proceedings of the 14thconference on Computational Linguistics, p.101-107,France.Chiang,T.-H., J.-S. Chang, M.-Y.
Lin and K.-Y.
Su.
1996.Statistical Word Segmentation.
In C.-R. Huang, K.-J.Chen and B.K.
T?sou (eds.
): Journal of Chinese Lin-guistics, Monograph Series, Number 9, Readings inChinese Natural Language Processing, pp.
147-173.Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Liand X. Xia and H. Qin.
2004.
Adaptive Chinese WordSegmentation.
In Proceedings of ACL-2004.Meng, H. and C. W. Ip.
1999.
An Analytical Study ofTransformational Tagging for Chinese Text.
In.
Pro-ceedings of ROCLING XII.
101-122.
TaipeiRuggieri S. 2004.
YaDT: Yet another Decision Treebuilder.
Proceedings of the 16th International Con-ference on Tools with Artificial Intelligence (ICTAI2004): 260-265.
IEEE Press, November 2004.Richard Sproat and Thomas Emerson.
2003.
TheFirst International Chinese Word Segmentation Bake-off.
Proceedings of the Second SIGHAN Workshop onChinese Language Processing, Sapporo, Japan, July2003.Xue, N. 2003.
Chinese Word Segmentation as Charac-ter Tagging.
Computational Linguistics and ChineseLanguage Processing.
8(1): 29-48Redington, M. and N. Chater and C. Huang and L. Changand K. Chen.
1995.
The Universality of Simple Dis-tributional Methods: Identifying Syntactic Categoriesin Mandarin Chinese.
Presented at the Proceedings ofthe International Conference on Cognitive Science andNatural Language Processing.
Dublin City University.72
