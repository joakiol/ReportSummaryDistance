Question Terminology and Representation forQuestion Type ClassicationNoriko TomuroDePaul UniversitySchool of Computer Science Telecommunications and Information Systems S Wabash AveChicago IL  USAtomurocsdepauleduAbstractQuestion terminology is a set of terms which appear in keywords idioms and xed expressionscommonly observed in questions This paperinvestigates ways to automatically extract question terminology from a corpus of questions andrepresent them for the purpose of classifying byquestion type Our key interest is to see whetheror not semantic features can enhance the representation of strongly lexical nature of questionsentences We compare two feature sets onewith lexical features only and another with amixture of lexical and semantic features Forevaluation we measure the classication accuracy made by two machine learning algorithmsC and PEBLS by using a procedure calleddomain crossvalidation which eectively measures the domain transferability of features IntroductionIn Information Retrieval 	IR text categorization and clustering documents are usually indexed and represented by domain terminologyterms which are particular to the domaintopicof a document However when documents mustbe retrieved or categorized according to criteriawhich do not correspond to the domains such asgenre 	text styleKessler et al Finn etal or subjectivity 	eg opinion vs factual descriptionWiebe  we must usedierent domainindependent features to indexand represent documents In those tasks selection of the features is in fact one of the mostcritical factors which aect the performance ofa systemQuestion type classication is one of suchtasks where categories are question types 	eghowto why and where In recent yearsquestion type has been successfully used inmany QuestionAnswering 	QAsystems fordetermining the kind of entity or concept being asked and extracting an appropriate answerVoorhees  Harabagiu et al  Hovyet al  Just like genre question typescut across domains for instance we can askhowto questions in the cooking domain thelegal domain etc However features that constitute question types are dierent from those usedfor genre classication 	typically partofspeechor metalingusitic featuresin that features arestrongly lexical due to the large amount of idiosyncrasy 	keywords idioms or syntactic constructionsthat is frequently observed in question sentences For example we can easily thinkof question patterns such as What is the bestway to  and What do I have to do to  Inthis regard terms which identify question typeare considered to form a terminology of theirown which we dene as question terminologyTerms in question terminology have somecharacteristics First they are mostly domainindependent noncontent words Second theyinclude many closedclass words 	such as interrogatives modals and pronouns and someopenclass words 	eg the noun way and theverb do In a way question terminology is acomplement of domain terminologyAutomatic extraction of question terminologyis a rather dicult task since question terms aremixed in with content terms Another complicating factor is paraphrasing  there are manyways to ask the same question For example How can I clean teapots In what way can we clean teapots What is the best way to clean teapots What method is used for cleaning teapots How do I go about cleaning teapotsIn this paper we present the results of ourinvestigation on how to automatically extractquestion terminology from a corpus of questionsand represent them for the purpose of classifying by question type It is an extension ofour previous work 	Tomuro and Lytinen where we compared automatic and manual techniques to select features from questions butonly 	stemmedwords were considered for features The focus of the current work is to investigate the kinds of features rather thanselection techniques which are best suited forrepresenting questions for classication Specifically from a large dataset of questions we automatically extracted two sets of features oneset consisting of terms 	ie lexical featuresonly and another set consisting of a mixture ofterms and semantic concepts 	ie semantic features Our particular interest is to see whetheror not semantic concepts can enhance the representation of strongly lexical nature of questionsentences To this end we apply two machinelearning algorithms 	C 	QuinlanandPEBLS 	Cost and Salzberg and compare the classication accuracy produced for thetwo feature sets The results show that there isno signicant increase by either algorithm bythe addition of semantic featuresThe original motivation behind our work onquestion terminology was to improve the retrieval accuracy of our system called FAQFinderBurke et al Lytinen and Tomuro FAQFinder is a webbased natural languageQA system which uses Usenet FrequentlyAsked Questions 	FAQles to answer usersquestions Figures  and  show an example session with FAQFinder First the user enters aquestion in natural language The system thensearches the FAQ les for questions that aresimilar to the users Based on the results ofthe search FAQFinder displays a maximum of FAQ questions which are ranked the highestby the systems similarity measure CurrentlyFAQFinder incorporates question type as one ofthe four metrics in measuring the similarity between the users question and FAQ questionsIn the present implementation the system usesa small set of manually selected words to determine the type of a question The goal of ourwork here is to derive optimal features whichwould produce improved classication accuracyThe other three metrics are vector similarity semantic similarity and coverage Lytinen and Tomuro Figure  User question entered as a naturallanguage query to FAQFinderFigure  The  bestmatching FAQ questions Question TypesIn our work we dened  question types below DEF definition  PRC procedure REF reference  MNR manner TME time  DEG degree LOC location 	 ATR atrans ENT entity  INT interval RSN reason  YNQ yesnoDescriptive denitions of these types arefound in 	Tomuro and Lytinen  Tableshows example FAQ questions which we hadused to develop the question types Note thatour question types are general question categories They are aimed to cover a wide varietyof questions entered by the FAQFinder users Selection of Feature SetsIn our current work we utilized two feature setsone set consisting of lexical features only 	LEXand another set consisting of a mixture of lexical features and semantic concepts 	LEXSEMObviously there are many known keywords idioms and xed expressions commonly observedin question sentences However categorizationof some of our  question types seem to depend on openclass words for instance Whatdoes mpg mean 	DEFand What does Belgium import and export 	REF To distinguish those types semantic features seem eective Semantic features could also be useful asbacko features since they allow for generalization For example in WordNet 	Millerthe noun knowhow is encoded as a hypernymof method methodology solution andtechnique By selecting such abstract concepts as semantic features we can cover a variety of paraphrases even for xed expressionsand supplement the coverage of lexical featuresWe selected the two feature sets in the following two steps In the rst step using a datasetof  example questions taken from  FAQlesdomains we rst manually tagged eachquestion by question type and then automatically derived the initial lexical set and initialsemantic set Then in the second step we rened those initial sets by pruning irrelevant features and derived two subsets LEX from theinitial lexical set and LEXSEM from the unionof lexical and semantic setsTo evaluate various subsets tried duringthe selection steps we applied two machinelearning algorithms C 	the commercialversion of C 	Quinlan availableat httpwwwrulequestcom a decision treeclassier and PEBLS 	Cost and Salzberg a knearest neighbor algorithmWealso measured the classication accuracy bya procedure we call domain crossvalidationDCV DCV is a variation of the standardcrossvalidation 	CVwhere the data is partitioned according to domains instead of randomWe used k  	 and majority voting scheme for allexperiments in our current workchoice To do a kfold DCV on a set of examples from n domains the set is rst brokeninto k nonoverlapping blocks where each blockcontains examples exactly from m nkdomains Then in each fold a classier is trainedwith 	k  m domains and tested on examples from m unseen domains Thus by observing the classication accuracy of the targetcategories using DCV we can measure the domain transferability how well the features extracted from some domains transfer to other domains Since question terminology is essentiallydomainindependent DCV is a better evaluation measure than CV for our purpose Initial Lexical SetThe initial lexical set was obtained by orderingthe words in the dataset by their Gain Ratioscores then selecting the subset which producedthe best classication accuracy by C and PEBLS Gain Ratio 	GRis a metric often usedin classication systems 	notably in Cformeasuring how well a feature predicts the categories of the examples GR is a normalized version of another metric called Information GainIG which measures the informativeness of afeature by the number of bits required to encode the examples if they are partitioned intotwo sets based on the presence or absence ofthe featureLet C denote the set of categories c  cmfor which the examples are classied 	ie target categories Given a collection of examplesS the Gain Ratio of a feature A GR	SA isdened asGR	SAIG	SASI	SAwhere IG	SAis the Information Gain denedto beIGSA PmiPrcilogPrciPrAPmiPrcijAlogPrcijAPrAPmiPrcijAlogPrcijAand SI	SAis the Splitting Information dened to beSISA PrAlogPrA PrAlogPrAThe description of Information Gain here is for binary partitioning Information Gain can also be generalized to mway partitioning for all m  Table  Example FAQ questionsQuestion Type QuestionDEF What does reactivity of emissions meanREF What do mutual funds invest inTME What dates are important when investing in mutual fundsENT Who invented Octane RatingsRSN Why does the Moon always show the same face to the EarthPRC How can I get rid of a caeine habitMNR How did the solar system formATR Where can I get British tea in the United StatesINT When will the sun dieYNQ Is the Moon moving away from the EarthThen features which yield high GR values aregood predictors In previous work in text categorization GR 	or IGhas been shown to beone of the most eective methods for reducingdimensions 	ie words to represent each textYang and PedersenHere in applying GR there was one issuewe had to consider how to distinguish content words from noncontent words This issuearose from the uneven distribution of the question types in the dataset Since not all questiontypes were represented in every domain if wechose question type as the target category features which yield high GR values might includesome domainspecic words In eect good predictors for our purpose are words which predictquestion types very well but do not predict domains Therefore we dened the GR score of aword to be the combination of two values theGR value when the target category was question type minus the GR value when the targetcategory was domainWe computed the 	modiedGR score for words which appeared more than twice inthe dataset and applied C and PEBLS Thenwe gradually reduced the set by taking the top nwords according to the GR scores and observedchanges in the classication accuracy Figure shows the result The evaluation was done byusing the fold DCV and the accuracy percentages indicated in the gure were an average of runs The best accuracy was achieved by thetop  words by both algorithms the remaining words seemed to have caused overtting asthe accuracy showed slight decline Thus wetook the top  words as the initial lexical feature set01020304050607080900 200 400 600 800 1000 1200 1400# featuresAccuracy(%)C5.0PEBLSFigure  Classication Accuracyon thetraining data measured by Domain Cross Validation 	DCV Initial Semantic SetThe initial semantic set was obtained by automatically selecting some nodes in the WordNet 	Millernoun and verb trees Foreach question type we chose questions of certain structures and applied a shallow parser toextract nouns andor verbs which appeared ata specic position For example for all question types 	except for YNQ we extracted thehead noun from questions of the form Whatis NP  Those nouns are essentially thedenominalization of the question type Thenouns extracted included way methodprocedure process for the type PRC reason advantage for RSN and organizationrestaurant for ENT For the types DEF andMNR we also extracted the main verb fromquestions of the form HowWhat does NP V Such verbs included work mean forDEF and aect and form for MNRThen for the nouns and verbs extracted foreach question type we applied the sense disambiguation algorithm used in 	Resnikand derived semantic classes 	or nodes in theWordNet treeswhich were their abstract generalization For each word in a set we traversedthe WordNet tree upward through the hypernym links from the nodes which correspondedto the rst two senses of the word and assignedeach ancestor a value which equaled to the inverse of the distance 	ie the number of linkstraversedfrom the original node Then weaccumulated the values for all ancestors andselected ones 	excluding the top nodeswhosevalue was above a threshold For examplethe set of nouns extracted for the type PRCwere knowhow 	an ancestor of way andmethodand activity 	an ancestor of procedure and processBy applying the procedure above for all question types we obtained a total of  semanticclasses This constitutes the initial semantic set RenementThe nal feature sets LEX and LEXSEM werederived by further rening the initial sets Themain purpose of renement was to reduce theunion of initial lexical and semantic sets 	a total of      featuresand deriveLEXSEM It was done by taking the featureswhich appeared in more than half of the decision trees induced by C during the iterationsof DCVThen we applied the same procedureto the initial lexical set 	 featuresand derived LEX Now both sets were 	suboptimalsubsets with which we could make a fair comparison There were  featureswords and features selected for LEX and LEXSEM respectivelyOur renement method is similar to 	Cardiein that it selects features by removingones that did not appear in a decision treeThe dierence is that in our method each decision tree is induced from a strict subset ofthe domains of the dataset Therefore by taking the intersection of multiple such trees wecan eectively extract features that are domainindependent thus transferable to other unseendomains Our method is also computationallyWe have in fact experimented various threshold values It turned out that produced the best accuracyTable  Classication accuracyon thetraining set by using reduced feature setsFeature set  features C	 PEBLSInitial lex  LEX reduced  Initial lex  sem   LEXSEM reduced  less expensive and feasible given the number offeatures expected to be in the reduced set 	overa hundred by our intuition than other feature subset selection techniques most of whichrequire expensive search through model spacesuch as wrapper approach 	John et alTable  shows the classication accuracy measured by DCV for the training set The increaseof the accuracy after the renement was minimal using C 	from  to  for LEX from to  for LEXSEM as expected Butthe increase using PEBLS was rather signicantfrom  to  for LEX from  to  forLEXSEM This result agreed with the ndingsin 	Cardie and conrmed that LEX andLEXSEM were indeed 	suboptimal Howeverthe dierence between LEX and LEXSEM wasnot statistically signicant by either algorithmfrom  to  by C from  to by PEBLS pvalues were  and  respectively This means the semantic features didnot help improve the classication accuracyAs we inspected the results we discoveredthat out of the  features in LEXSEM were semantic features and they did occur in of the training examples 	  However in most of those examples keyterms were already represented by lexical features thus semantic features did not add anymore information to help determine the question type As an example a sentence Whatare the dates of the upcoming Jewish holidays was represented by lexical featureswhat be of and date and a semantic feature timeunit 	an ancestor of dateThe  words in LEX are listed in the Appendix at the end of this paperPvalues were obtained by applying the ttest onthe accuracy produced by all iterations of DCV witha null hypothesis that the mean accuracy of LEXSEMwas higher than that of LEXTable  Classication accuracyon the testsetsFeature set  FAQFinder AskJeevesfeatures C PEBLS C PEBLSLEX     LEXSEM      External TestsetsTo further investigate the eect of semantic features we tested LEX and LEXSEM with twoexternal testsets one set consisting of  questions taken from FAQFinder user log and another set consisting of  questions taken fromthe AskJeeves 	httpwwwaskjeevescomuserlog Both datasets contained questions from awide range of domains therefore served as anexcellent indicator of the domain transferabilityfor our two feature setsTable  shows the results For the FAQFinderdata LEX and LEXSEM produced comparable accuracy using both C and PEBLS Butfor the AskJeeves data LEXSEM did worsethan LEX consistently by both classiers Thismeans the additional semantic features were interacting with lexical featuresWe speculate the reason to be the following Compared to the FAQFinder data theAskJeeves data was gathered from a much wideraudience and the questions spanned a broadrange of domains Many terms in the questionswere from vocabulary considerably larger thanthat of our training set Therefore the datacontained quite a few words whose hypernymlinks lead to a semantic feature in LEXSEMbut did not fall into the question type keyedby the feature For instance a question inAskJeeves What does Hanukah mean wasmisclassied as type TME by using LEXSEMThis was because Hanukah in WordNet wasencoded as a hyponym of time period On theother hand LEX did not include Hanukahthus correctly classied the question as typeDEF Related WorkRecently with a need to incorporate user preferences in information retrieval several work hasbeen done which classies documents by genreFor instance 	Finn et al used machinelearning techniques to identify subjective 	opiniondocuments from newspaper articles To determine what feature adapts well to unseen domains they compared three kinds of featureswords partofspeech statistics and manuallyselected metalinguistic features They concluded that the partofspeech performed thebest with regard to domain transfer Howevernot only were their feature sets predeterminedtheir features were distinct from words in thedocuments 	or features were the entire wordsthemselves thus no feature subset selectionwas performedWiebe also used machine learningtechniques to identify subjective sentences Shefocused on adjectives as an indicator of subjectivity and used corpus statistics and lexicalsemantic information to derive adjectives thatyielded high precision Conclusions and Future WorkIn this paper we showed that semantic featuresdid not enhance lexical features in the representation of questions for the purpose of questiontype classication While semantic features allow for generalization they also seemed to domore harm than good in some cases by interacting with lexical features This indicates thatquestion terminology is strongly lexical indeedand suggests that enumeration of words whichappear in typical idiomatic question phraseswould be more eective than semanticsFor future work we are planning to experiment with synonyms The use of synonymsis another way of increasing the coverage ofquestion terminology while semantic featurestry to achieve it by generalization synonymsdo it by lexical expansion Our plan is to usethe synonyms obtained from very large corpora reported in 	Lin We are alsoplanning to compare the 	lexical and semanticfeatures we derived automatically in thiswork with manually selected features In ourprevious work manually selected 	lexicalfeatures showed slightly better performance for thetraining data but no signicant dierence forthe test data We plan to manually pick out semantic as well as lexical features and apply tothe current dataReferencesR Burke K Hammond V Kulyukin S Lytinen N Tomuro and S SchoenbergQuestion answering from frequently askedquestion les Experiences with the faqndersystem AI Magazine 	C Cardie Using decision trees to improve casebased learning In Proceedings ofthe th International Conference on Machine Learning ICMLS Cost and S Salzberg A weighted nearest neighbor algorithm for learning with symbolic features Machine Learning A Finn N Kushmerick and B Smyth Genre classication and domain transfer forinformation ltering In Proceedings of theEuropean Colloquium on Information Retrieval Research GlasgowS Harabagiu D Moldovan M Pasca R Mihalcea M Surdeanu R Bunescu R GirjuV Rus and P Morarescu  FalconBoosting knowledge for answer engines InProceedings of TRECE Hovy L Gerber U Hermjakob C Lin andD Ravichandran  Toward semanticsbased answer pinpointing In Proceedings ofthe DARPA Human Language TechnologiesHLTG John R Kohavi and K P eger Irrelevant features and the subset selection problem In Proceedings of the th InternationalConference on Machine Learning ICML	K Kessler G Nunberg and H SchutzeAutomatic detection of text genre In Proceedings of the th Annual Meeting of theAssociation for Computational LinguisticsACLD Lin Automatic retrieval and clustering of similar words In Proceedings of theth Annual Meeting of the Association forComputational Linguistics ACLS Lytinen and N Tomuro  The useof question types to match questions infaqnder In Papers from the  AAAISpring Symposium on Mining Answers fromTexts and Knowledge BasesG Miller Wordnet An online lexicaldatabase International Journal of Lexicography 	R Quinlan C	 Programs for MachineLearning Morgan KaufmanP Resnik Selectional preference andsense disambiguation In Proceedings of theACL SIGLEX Workshop on Tagging Textwith Lexical Semantics Washington DCN Tomuro and S Lytinen  Selectingfeatures for paraphrasing question sentencesIn Proceedings of the workshop on Automatic Paraphrasing at NLP Pacic Rim NLPRS Tokyo JapanE Voorhees  The trecquestion answering track report In Proceedings of TRECJ Wiebe  Learning subjective adjectivesfrom corpora In Proceedings of the th National Conference on Articial IntelligenceAAAI Austin TexasY Yang and J Pedersen A comparativestudy on feature selection in text categorization In Proceedings of the 	th InternationalConference on Machine Learning ICMLAppendix The LEX Setabout address advantage aect andany archive available bag be beginbenet better buy can cause cleancome company compare contact contagious copy cost create date day dealdier dierence do eect emission evaporative expense fast nd for get gogood handle happen have history howif in internet keep know learn longmake many mean milk much myname number obtain of often old onone or organization origin people percentage place planet price procedure pronounce purpose reason relate relationshipshall shuttle site size sky so solarsome start store sun symptom taketank tax that there time to us wayweb what when where which whowhy will with work world wide webwrong www year you
