Shallow Semantic Parsing using Support Vector Machines?Sameer Pradhan, Wayne Ward,Kadri Hacioglu, James H. MartinCenter for Spoken Language Research,University of Colorado, Boulder, CO 80303{spradhan,whw,hacioglu,martin}@cslr.colorado.eduDan JurafskyDepartment of LinguisticsStanford UniversityStanford, CA 94305jurafsky@stanford.eduAbstractIn this paper, we propose a machine learning al-gorithm for shallow semantic parsing, extend-ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.
Our al-gorithm is based on Support Vector Machineswhich we show give an improvement in perfor-mance over earlier classifiers.
We show perfor-mance improvements through a number of newfeatures and measure their ability to general-ize to a new test set drawn from the AQUAINTcorpus.1 IntroductionAutomatic, accurate and wide-coverage techniques thatcan annotate naturally occurring text with semantic argu-ment structure can play a key role in NLP applicationssuch as Information Extraction, Question Answering andSummarization.
Shallow semantic parsing ?
the processof assigning a simple WHO did WHAT to WHOM, WHEN,WHERE, WHY, HOW, etc.
structure to sentences in text,is the process of producing such a markup.
When pre-sented with a sentence, a parser should, for each predicatein the sentence, identify and label the predicate?s seman-tic arguments.
This process entails identifying groups ofwords in a sentence that represent these semantic argu-ments and assigning specific labels to them.In recent work, a number of researchers have cast thisproblem as a tagging problem and have applied vari-ous supervised machine learning techniques to it (Gildeaand Jurafsky (2000, 2002); Blaheta and Charniak (2000);Gildea and Palmer (2002); Surdeanu et al (2003); Gildeaand Hockenmaier (2003); Chen and Rambow (2003);Fleischman and Hovy (2003); Hacioglu and Ward (2003);Thompson et al (2003); Pradhan et al (2003)).
In this?This research was partially supported by the ARDAAQUAINT program via contract OCG4423B and by the NSFvia grant IS-9978025paper, we report on a series of experiments exploring thisapproach.For the initial experiments, we adopted the approachdescribed by Gildea and Jurafsky (2002) (G&J) and eval-uated a series of modifications to improve its perfor-mance.
In the experiments reported here, we first re-placed their statistical classification algorithm with onethat uses Support Vector Machines and then added to theexisting feature set.
We evaluate results using both hand-corrected TreeBank syntactic parses, and actual parsesfrom the Charniak parser.2 Semantic Annotation and CorporaWe will be reporting on results using PropBank1 (Kings-bury et al, 2002), a 300k-word corpus in which predi-cate argument relations are marked for part of the verbsin the Wall Street Journal (WSJ) part of the Penn Tree-Bank (Marcus et al, 1994).
The arguments of a verb arelabeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a transitive verb) ARG1is the PROTO-PATIENT (usually its direct object), etc.PropBank attempts to treat semantically related verbsconsistently.
In addition to these CORE ARGUMENTS,additional ADJUNCTIVE ARGUMENTS, referred to asARGMs are also marked.
Some examples are ARGM-LOC, for locatives, and ARGM-TMP, for temporals.
Fig-ure 1 shows the syntax tree representation along with theargument labels for an example structure extracted fromthe PropBank corpus.Most of the experiments in this paper, unless speci-fied otherwise, are performed on the July 2002 releaseof PropBank.
A larger, cleaner, completely adjudicatedversion of PropBank was made available in Feb 2004.We will also report some final best performance numberson this corpus.
PropBank was constructed by assigningsemantic arguments to constituents of the hand-correctedTreeBank parses.
The data comprise several sections ofthe WSJ, and we follow the standard convention of using1http://www.cis.upenn.edu/?ace/Section-23 data as the test set.
Section-02 to Section-21 were used for training.
In the July 2002 release, thetraining set comprises about 51,000 sentences, instantiat-ing about 132,000 arguments, and the test set comprises2,700 sentences instantiating about 7,000 arguments.
TheFeb 2004 release training set comprises about 85,000 sen-tences instantiating about 250,000 arguments and the testset comprises 5,000 sentences instantiating about 12,000arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about20 minutes].Shhhh((((NPPRPHeARG0VPhhhh((((VBDtalkedpredicatePPhhh(((INforNULLNPhhhhh(((((about 20 minutesARGM ?
TMPFigure 1: Syntax tree for a sentence illustrating the Prop-Bank tags.3 Problem DescriptionThe problem of shallow semantic parsing can be viewedas three different tasks.Argument Identification ?
This is the process of identi-fying parsed constituents in the sentence that representsemantic arguments of a given predicate.Argument Classification ?
Given constituents known torepresent arguments of a predicate, assign the appropri-ate argument labels to them.Argument Identification and Classification ?
A combina-tion of the above two tasks.Each node in the parse tree can be classified as eitherone that represents a semantic argument (i.e., a NON-NULL node) or one that does not represent any seman-tic argument (i.e., a NULL node).
The NON-NULL nodescan then be further classified into the set of argument la-bels.
For example, in the tree of Figure 1, the node INthat encompasses ?for?
is a NULL node because it doesnot correspond to a semantic argument.
The node NPthat encompasses ?about 20 minutes?
is a NON-NULLnode, since it does correspond to a semantic argument?
ARGM-TMP.4 Baseline FeaturesOur baseline system uses the same set of features in-troduced by G&J.
Some of the features, viz., predicate,voice and verb sub-categorization are shared by all thenodes in the tree.
All the others change with the con-stituent under consideration.?
Predicate ?
The predicate itself is used as a feature.?
Path ?
The syntactic path through the parse treefrom the parse constituent to the predicate beingclassified.
For example, in Figure 1, the path fromARG0 ?
?He?
to the predicate talked, is representedwith the string NP?S?VP?VBD.
?
and ?
representupward and downward movement in the tree respec-tively.?
Phrase Type ?
This is the syntactic category (NP,PP, S, etc.)
of the phrase/constituent correspondingto the semantic argument.?
Position ?
This is a binary feature identifyingwhether the phrase is before or after the predicate.?
Voice ?
Whether the predicate is realized as an ac-tive or passive construction.?
Head Word ?
The syntactic head of the phrase.
Thisis calculated using a head word table described by(Magerman, 1994) and modified by (Collins, 1999,Appendix.
A).?
Sub-categorization ?
This is the phrase struc-ture rule expanding the predicate?s parent nodein the parse tree.
For example, in Figure 1, thesub-categorization for the predicate talked isVP?VBD-PP.5 Classifier and ImplementationWe formulate the parsing problem as a multi-class clas-sification problem and use a Support Vector Machine(SVM) classifier (Hacioglu et al, 2003; Pradhan et al2003).
Since SVMs are binary classifiers, we have to con-vert the multi-class problem into a number of binary-classproblems.
We use the ONE vs ALL (OVA) formalism,which involves training n binary classifiers for a n-classproblem.Since the training time taken by SVMs scales exponen-tially with the number of examples, and about 80% of thenodes in a syntactic tree have NULL argument labels, wefound it efficient to divide the training process into twostages, while maintaining the same accuracy:1.
Filter out the nodes that have a very high probabil-ity of being NULL.
A binary NULL vs NON-NULLclassifier is trained on the entire dataset.
A sigmoidfunction is fitted to the raw scores to convert thescores to probabilities as described by (Platt, 2000).2.
The remaining training data is used to train OVAclassifiers, one of which is the NULL-NON-NULLclassifier.With this strategy only one classifier (NULL vs NON-NULL) has to be trained on all of the data.
The remainingOVA classifiers are trained on the nodes passed by thefilter (approximately 20% of the total), resulting in a con-siderable savings in training time.In the testing stage, we do not perform any filteringof NULL nodes.
All the nodes are classified directlyas NULL or one of the arguments using the classifiertrained in step 2 above.
We observe no significant per-formance improvement even if we filter the most likelyNULL nodes in a first pass.For our experiments, we used TinySVM2 along withYamCha3 (Kudo and Matsumoto, 2000)(Kudo and Matsumoto, 2001) as the SVM training andtest software.
The system uses a polynomial kernel withdegree 2; the cost per unit violation of the margin, C=1;and, tolerance of the termination criterion, e=0.001.6 Baseline System PerformanceTable 1 shows the baseline performance numbers on thethree tasks mentioned earlier; these results are based onsyntactic features computed from hand-corrected Tree-Bank (hence LDC hand-corrected) parses.For the argument identification and the combined iden-tification and classification tasks, we report the precision(P), recall (R) and the F14 scores, and for the argumentclassification task we report the classification accuracy(A).
This test set and all test sets, unless noted otherwiseare Section-23 of PropBank.Classes Task P R F1 A(%) (%) (%)ALL Id.
90.9 89.8 90.4ARGs Classification - - - 87.9Id.
+ Classification 83.3 78.5 80.8CORE Id.
94.7 90.1 92.3ARGs Classification - - - 91.4Id.
+ Classification 88.4 84.1 86.2Table 1: Baseline performance on all three tasks usinghand-corrected parses.7 System Improvements7.1 Disallowing OverlapsThe system as described above might label two con-stituents NON-NULL even if they overlap in words.
Thisis a problem since overlapping arguments are not allowedin PropBank.
Among the overlapping constituents we re-tain the one for which the SVM has the highest confi-dence, and label the others NULL.
The probabilities ob-tained by applying the sigmoid function to the raw SVMscores are used as the measure of confidence.
Table 2shows the performance of the parser on the task of iden-tifying and labeling semantic arguments using the hand-corrected parses.
On all the system improvements, weperform a ?2 test of significance at p = 0.05, and all the2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/4F1 = 2PRP+Rsignificant improvements are marked with an ?.
In thissystem, the overlap-removal decisions are taken indepen-dently of each other.P R F1(%) (%)Baseline 83.3 78.5 80.8No Overlaps 85.4 78.1 ?81.6Table 2: Improvements on the task of argument identi-fication and classification after disallowing overlappingconstituents.7.2 New FeaturesWe tested several new features.
Two were obtained fromthe literature ?
named entities in constituents and headword part of speech.
Other are novel features.1.
Named Entities in Constituents ?
FollowingSurdeanu et al (2003), we tagged 7 named en-tities (PERSON, ORGANIZATION, LOCATION,PERCENT, MONEY, TIME, DATE) using Identi-Finder (Bikel et al, 1999) and added them as 7binary features.2.
Head Word POS ?
Surdeanu et al (2003) showedthat using the part of speech (POS) of the head wordgave a significant performance boost to their system.Following that, we experimented with the additionof this feature to our system.3.
Verb Clustering ?
Since our training data is rel-atively limited, any real world test set will con-tain predicates that have not been seen in training.In these cases, we can benefit from some informa-tion about the predicate by using predicate clus-ter as a feature.
The verbs were clustered into 64classes using the probabilistic co-occurrence modelof Hofmann and Puzicha (1998).
The clustering al-gorithm uses a database of verb-direct-object rela-tions extracted by Lin (1998).
We then use the verbclass of the current predicate as a feature.4.
Partial Path ?
For the argument identification task,path is the most salient feature.
However, it is alsothe most data sparse feature.
To overcome this prob-lem, we tried generalizing the path by adding a newfeature that contains only the part of the path fromthe constituent to the lowest common ancestor of thepredicate and the constituent, which we call ?Partial-Path?.5.
Verb Sense Information ?
The arguments that apredicate can take depend on the word sense of thepredicate.
Each predicate tagged in the PropBankcorpus is assigned a separate set of arguments de-pending on the sense in which it is used.
Table 3illustrates the argument sets for the predicate talk.Depending on the sense of the predicate talk, eitherARG1 or ARG2 can identify the hearer.
Absence ofthis information can be potentially confusing to thelearning mechanism.Talk sense 1: speak sense 2: persuade/dissuadeTag Description Tag DescriptionARG0 Talker ARG0 TalkerARG1 Subject ARG1 Talked toARG2 Hearer ARG2 Secondary actionTable 3: Argument labels associated with the two sensesof predicate talk in PropBank corpus.We added the oracle sense information extractedfrom PropBank, to our features by treating eachsense of a predicate as a distinct predicate.6.
Head Word of Prepositional Phrases ?
Many ad-junctive arguments, such as temporals and locatives,occur as prepositional phrases in a sentence, andit is often the case that the head words of thosephrases, which are always prepositions, are not verydiscriminative, eg., ?in the city?, ?in a few minutes?,both share the same head word ?in?
and neithercontain a named entity, but the former is ARGM-LOC, whereas the latter is ARGM-TMP.
Therefore,we tried replacing the head word of a prepositionalphrase, with that of the first noun phrase inside theprepositional phrase.
We retained the preposition in-formation by appending it to the phrase type, eg.,?PP-in?
instead of ?PP?.7.
First and Last Word/POS in Constituent ?
Somearguments tend to contain discriminative first andlast words so we tried using them along with theirpart of speech as four new features.8.
Ordinal constituent position ?
In order to avoidfalse positives of the type where constituents faraway from the predicate are spuriously identified asarguments, we added this feature which is a concate-nation of the constituent type and its ordinal positionfrom the predicate.9.
Constituent tree distance ?
This is a finer way ofspecifying the already present position feature.10.
Constituent relative features ?
These are nine fea-tures representing the phrase type, head word andhead word part of speech of the parent, and left andright siblings of the constituent in focus.
These wereadded on the intuition that encoding the tree contextthis way might add robustness and improve general-ization.11.
Temporal cue words ?
There are several temporalcue words that are not captured by the named entitytagger and were considered for addition as a binaryfeature indicating their presence.12.
Dynamic class context ?
In the task of argumentclassification, these are dynamic features that repre-sent the hypotheses of at most previous two nodesbelonging to the same tree as the node being classi-fied.8 Feature PerformanceTable 4 shows the effect each feature has on the ar-gument classification and argument identification tasks,when added individually to the baseline.
Addition ofnamed entities improves the F1 score for adjunctive ar-guments ARGM-LOC from 59% to ?68% and ARGM-TMP from 78.8% to ?83.4%.
But, since these argumentsare small in number compared to the core arguments, theoverall accuracy does not show a significant improve-ment.
We found that adding this feature to the NULL vsNON-NULL classifier degraded its performance.
It alsoshows the contribution of replacing the head word and thehead word POS separately in the feature where the headof a prepositional phrase is replaced by the head wordof the noun phrase inside it.
Apparently, a combinationof relative features seem to have a significant improve-ment on either or both the classification and identificationtasks, and so do the first and last words in the constituent.Features Class ARGUMENT IDAcc.P R F1Baseline 87.9 93.7 88.9 91.3+ Named entities 88.1 - - -+ Head POS ?88.6 94.4 90.1 ?92.2+ Verb cluster 88.1 94.1 89.0 91.5+ Partial path 88.2 93.3 88.9 91.1+ Verb sense 88.1 93.7 89.5 91.5+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2+ Noun head PP (only head) ?89.8 94.0 89.4 91.7+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6+ First word in constituent ?89.0 94.4 91.1 ?92.7+ Last word in constituent ?89.4 93.8 89.4 91.6+ First POS in constituent 88.4 94.4 90.6 ?92.5+ Last POS in constituent 88.3 93.6 89.1 91.3+ Ordinal const.
pos.
concat.
87.7 93.7 89.2 91.4+ Const.
tree distance 88.0 93.7 89.5 91.5+ Parent constituent 87.9 94.2 90.2 ?92.2+ Parent head 85.8 94.2 90.5 ?92.3+ Parent head POS ?88.5 94.3 90.3 ?92.3+ Right sibling constituent 87.9 94.0 89.9 91.9+ Right sibling head 87.9 94.4 89.9 ?92.1+ Right sibling head POS 88.1 94.1 89.9 92.0+ Left sibling constituent ?88.6 93.6 89.6 91.6+ Left sibling head 86.9 93.9 86.1 89.9+ Left sibling head POS ?88.8 93.5 89.3 91.4+ Temporal cue words ?88.6 - - -+ Dynamic class context 88.4 - - -Table 4: Effect of each feature on the argument identifi-cation and classification tasks when added to the baselinesystem.We tried two other ways of generalizing the head word:i) adding the head word cluster as a feature, and ii) replac-ing the head word with a named entity if it belonged toany of the seven named entities mentioned earlier.
Nei-ther method showed any improvement.
We also tried gen-eralizing the path feature by i) compressing sequences ofidentical labels, and ii) removing the direction in the path,but none showed any improvement on the baseline.8.1 Argument Sequence InformationIn order to improve the performance of their statistical ar-gument tagger, G&J used the fact that a predicate is likelyto instantiate a certain set of arguments.
We use a similarstrategy, with some additional constraints: i) argumentordering information is retained, and ii) the predicate isconsidered as an argument and is part of the sequence.We achieve this by training a trigram language model onthe argument sequences, so unlike G&J, we can also es-timate the probability of argument sets not seen in thetraining data.
We first convert the raw SVM scores toprobabilities using a sigmoid function.
Then, for eachsentence being parsed, we generate an argument latticeusing the n-best hypotheses for each node in the syn-tax tree.
We then perform a Viterbi search through thelattice using the probabilities assigned by the sigmoidas the observation probabilities, along with the languagemodel probabilities, to find the maximum likelihood paththrough the lattice, such that each node is either assigneda value belonging to the PROPBANK ARGUMENTs, orNULL.CORE ARGs/ P R F1Hand-corrected parses (%) (%)Baseline w/o overlaps 90.0 86.1 88.0Common predicate 90.8 86.3 88.5Specific predicate lemma 90.5 87.4 ?88.9Table 5: Improvements on the task of argument identifi-cation and tagging after performing a search through theargument lattice.The search is constrained in such a way that no twoNON-NULL nodes overlap with each other.
To simplifythe search, we allowed only NULL assignments to nodeshaving a NULL likelihood above a threshold.
While train-ing the language model, we can either use the actual pred-icate to estimate the transition probabilities in and outof the predicate, or we can perform a joint estimationover all the predicates.
We implemented both cases con-sidering two best hypotheses, which always includes aNULL (we add NULL to the list if it is not among thetop two).
On performing the search, we found that theoverall performance improvement was not much differ-ent than that obtained by resolving overlaps as mentionedearlier.
However, we found that there was an improve-ment in the CORE ARGUMENT accuracy on the combinedtask of identifying and assigning semantic arguments,given hand-corrected parses, whereas the accuracy of theADJUNCTIVE ARGUMENTS slightly deteriorated.
Thisseems to be logical considering the fact that the ADJUNC-TIVE ARGUMENTS are not linguistically constrained inany way as to their position in the sequence of argu-ments, or even the quantity.
We therefore decided touse this strategy only for the CORE ARGUMENTS.
Al-though, there was an increase in F1 score when the lan-guage model probabilities were jointly estimated over allthe predicates, this improvement is not statistically signif-icant.
However, estimating the same using specific predi-cate lemmas, showed a significant improvement in accu-racy.
The performance improvement is shown in Table 5.9 Best System PerformanceThe best system is trained by first filtering the mostlikely nulls using the best NULL vs NON-NULL classi-fier trained using all the features whose argument identi-fication F1 score is marked in bold in Table 4, and thentraining a ONE vs ALL classifier using the data remain-ing after performing the filtering and using the featuresthat contribute positively to the classification task ?
oneswhose accuracies are marked in bold in Table 4.
Table 6shows the performance of this system.Classes Task Hand-corrected parsesP R F1 A(%) (%) (%)ALL Id.
95.2 92.5 93.8ARGs Classification - - - 91.0Id.
+ Classification 88.9 84.6 86.7CORE Id.
96.2 93.0 94.6ARGs Classification - - - 93.9Id.
+ Classification 90.5 87.4 88.9Table 6: Best system performance on all tasks usinghand-corrected parses.10 Using Automatic ParsesThus far, we have reported results using hand-correctedparses.
In real-word applications, the system will haveto extract features from an automatically generatedparse.
To evaluate this scenario, we used the Charniakparser (Chaniak, 2001) to generate parses for PropBanktraining and test data.
We lemmatized the predicate usingthe XTAG morphology database5 (Daniel et al, 1992).Table 7 shows the performance degradation whenautomatically generated parses are used.11 Using Latest PropBank DataOwing to the Feb 2004 release of much more and com-pletely adjudicated PropBank data, we have a chance to5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-1.5.tar.gzClasses Task Automatic parsesP R F1 A(%) (%) (%)ALL Id.
89.3 82.9 86.0ARGs Classification - - - 90.0Id.
+ Classification 84.0 75.3 79.4CORE Id.
92.0 83.3 87.4ARGs Classification - - - 90.5Id.
+ Classification 86.4 78.4 82.2Table 7: Performance degradation when using automaticparses instead of hand-corrected ones.report our performance numbers on this data set.
Table 8shows the same information as in previous Tables 6 and7, but generated using the new data.
Owing to time limi-tations, we could not get the results on the argument iden-tification task and the combined argument identificationand classification task using automatic parses.ALL ARGs Task P R F1 A(%) (%) (%)HAND Id.
96.2 95.8 96.0Classification - - - 93.0Id.
+ Classification 89.9 89.0 89.4AUTOMATIC Classification - - - 90.1Table 8: Best system performance on all tasks usinghand-corrected parses using the latest PropBank data.12 Feature AnalysisIn analyzing the performance of the system, it is usefulto estimate the relative contribution of the various featuresets used.
Table 9 shows the argument classification ac-curacies for combinations of features on the training andtest data, using hand-corrected parses, for all PropBankarguments.Features Accuracy(%)All 91.0All except Path 90.8All except Phrase Type 90.8All except HW and HW -POS 90.7All except All Phrases ?83.6All except Predicate ?82.4All except HW and FW and LW -POS ?75.1Path, Predicate 74.4Path, Phrase Type 47.2Head Word 37.7Path 28.0Table 9: Performance of various feature combinations onthe task of argument classification.In the upper part of Table 9 we see the degradation inperformance by leaving out one feature or a feature fam-ily at a time.
After the addition of all the new features,it is the case that removal of no individual feature exceptpredicate degrades the classification performance signifi-cantly, as there are some other features that provide com-plimentary information.
However, removal of predicateinformation hurts performance significantly, so does theremoval of a family of features, eg., all phrase types, orthe head word (HW), first word (FW) and last word (LW)information.
The lower part of the table shows the per-formance of some feature combinations by themselves.Table 10 shows the feature salience on the task of ar-gument identification.
One important observation we canmake here is that the path feature is the most salient fea-ture in the task of argument identification, whereas it isthe least salient in the task of argument classification.
Wecould not provide the numbers for argument identifica-tion performance upon removal of the path feature sincethat made the SVM training prohibitively slow, indicatingthat the SVM had a very hard time separating the NULLclass from the NON-NULL class.Features P R F1(%) (%)All 95.2 92.5 93.8All except HW 95.1 92.3 93.7All except Predicate 94.5 91.9 93.2Table 10: Performance of various feature combinationson the task of argument identification13 Comparing Performance with OtherSystemsWe compare our system against 4 other shallow semanticparsers in the literature.
In comparing systems, results arereported for all the three types of tasks mentioned earlier.13.1 Description of the SystemsThe Gildea and Palmer (G&P) System.The Gildea and Palmer (2002) system uses the samefeatures and the same classification mechanism used byG&J.
These results are reported on the December 2001release of PropBank.The Surdeanu et al System.Surdeanu et al (2003) report results on two systemsusing a decision tree classifier.
One that uses exactly thesame features as the G&J system.
We call this ?SurdeanuSystem I.?
They then show improved performance of an-other system ?
?Surdeanu System II,?
which uses someadditional features.
These results are are reported on theJuly 2002 release of PropBank.The Gildea and Hockenmaier (G&H) SystemThe Gildea and Hockenmaier (2003) system uses fea-tures extracted from Combinatory Categorial Grammar(CCG) corresponding to the features that were used byG&J and G&P systems.
CCG is a form of dependencygrammar and is hoped to capture long distance relation-ships better than a phrase structure grammar.
The fea-tures are combined using the same algorithm as in G&Jand G&P.
They use a slightly newer ?
November 2002 re-lease of PropBank.
We will refer to this as ?G&H SystemI?.The Chen and Rambow (C&R) SystemChen and Rambow report on two different systems,also using a decision tree classifier.
The first ?C&R Sys-tem I?
uses surface syntactic features much like the G&Psystem.
The second ?C&R System II?
uses additionalsyntactic and semantic representations that are extractedfrom a Tree Adjoining Grammar (TAG) ?
another gram-mar formalism that better captures the syntactic proper-ties of natural languages.Classifier Accuracy(%)SVM 88Decision Tree (Surdeanu et al, 2003) 79Gildea and Palmer (2002) 77Table 11: Argument classification using same featuresbut different classifiers.13.2 Comparing ClassifiersSince two systems, in addition to ours, report results us-ing the same set of features on the same data, we candirectly assess the influence of the classifiers.
G&P sys-tem estimates the posterior probabilities using several dif-ferent feature sets and interpolate the estimates, whileSurdeanu et al (2003) use a decision tree classifier.
Ta-ble 11 shows a comparison between the three systems forthe task of argument classification.13.3 Argument Identification (NULL vs NON-NULL)Table 12 compares the results of the task of identify-ing the parse constituents that represent semantic argu-ments.
As expected, the performance degrades consider-ably when we extract features from an automatic parse asopposed to a hand-corrected parse.
This indicates that thesyntactic parser performance directly influences the argu-ment boundary identification performance.
This could beattributed to the fact that the two features, viz., Path andHead Word that have been seen to be good discriminatorsof the semantically salient nodes in the syntax tree, arederived from the syntax tree.Classes System Hand AutomaticP R F1 P R F1ALL SVM 95 92 94 89 83 86ARGs Surdeanu System II - - 89 - - -Surdeanu System I 85 84 85 - - -Table 12: Argument identification13.4 Argument ClassificationTable 13 compares the argument classification accuraciesof various systems, and at various levels of classificationgranularity, and parse accuracy.
It can be seen that theSVM System performs significantly better than all theother systems on all PropBank arguments.Classes System Hand AutomaticAccuracy AccuracyALL SVM 91 90ARGs G&P 77 74Surdeanu System II 84 -Surdeanu System I 79 -CORE SVM 93.9 90.5ARGs C&R System II 93.5 -C&R System I 92.4 -Table 13: Argument classification13.5 Argument Identification and ClassificationTable 14 shows the results for the task where the systemfirst identifies candidate argument boundaries and thenlabels them with the most likely argument.
This is thehardest of the three tasks outlined earlier.
SVM does avery good job of generalizing in both stages of process-ing.Classes System Hand AutomaticP R F1 P R F1ALL SVM 89 85 87 84 75 79ARGs G&H System I 76 68 72 71 63 67G&P 71 64 67 58 50 54CORE SVM System 90 87 89 86 78 82ARGs G&H System I 82 79 80 76 73 75C&R System II - - - 65 75 70Table 14: Identification and classification14 Generalization to a New Text SourceThus far, in all experiments our unseen test data wasselected from the same source as the training data.In order to see how well the features generalize totexts drawn from a similar source, we used the classifiertrained on PropBank training data to test data drawn fromthe AQUAINT corpus (LDC, 2002).
We annotated 400sentences from the AQUAINT corpus with PropBankarguments.
This is a collection of text from the NewYork Times Inc., Associated Press Inc., and XinhuaNews Service (PropBank by comparison is drawn fromWall Street Journal).
The results are shown in Table 15.Task P R F1 A(%) (%) (%)ALL Id.
75.8 71.4 73.5 -ARGs Classification - - - 83.8Id.
+ Classification 65.2 61.5 63.3 -CORE Id.
88.4 74.4 80.8 -ARGs Classification - - - 84.0Id.
+ Classification 75.2 63.3 68.7 -Table 15: Performance on the AQUAINT test set.There is a significant drop in the precision and recallnumbers for the AQUAINT test set (compared to the pre-cision and recall numbers for the PropBank test set whichwere 84% and 75% respectively).
One possible reasonfor the drop in performance is relative coverage of thefeatures on the two test sets.
The head word, path andpredicate features all have a large number of possible val-ues and could contribute to lower coverage when movingfrom one domain to another.
Also, being more specificthey might not transfer well across domains.Features Arguments non-Arguments(%) (%)Predicate, Path 87.60 2.91Predicate, Head Word 48.90 26.55Cluster, Path 96.31 4.99Cluster, Head Word 83.85 60.14Path 99.13 15.15Head Word 93.02 90.59Table 16: Feature Coverage on PropBank test set usingparser trained on PropBank training set.Features Arguments non-Arguments(%) (%)Predicate, Path 62.11 4.66Predicate, Head Word 30.26 17.41Cluster, Path 87.19 10.68Cluster, Head Word 65.82 45.43Path 96.50 29.26Head Word 84.65 83.54Table 17: Coverage of features on AQUAINT test set us-ing parser trained on PropBank training set.Table 16 shows the coverage for features on the hand-corrected PropBank test set.
The tables show featurecoverage for constituents that were Arguments and con-stituents that were NULL.
About 99% of the predicates inthe AQUAINT test set were seen in the PropBank train-ing set.
Table 17 shows coverage for the same features onthe AQUAINT test set.
We believe that the drop in cover-age of the more predictive feature combinations explainspart of the drop in performance.15 ConclusionsWe have described an algorithm which significantly im-proves the state-of-the-art in shallow semantic parsing.Like previous work, our parser is based on a supervisedmachine learning approach.
Key aspects of our resultsinclude significant improvement via an SVM classifier,improvement from new features and a series of analyticexperiments on the contributions of the features.
Addingfeatures that are generalizations of the more specific fea-tures seemed to help.
These features were named enti-ties, head word part of speech and verb clusters.
We alsoanalyzed the transferability of the features to a new textsource.We would like to thank Ralph Weischedel and Scott Miller ofBBN Inc. for letting us use their named entity tagger ?
Iden-tiFinder; Martha Palmer for providing us with the PropBankdata, Valerie Krugler for tagging the AQUAINT test set withPropBank arguments, and all the anonymous reviewers for theirhelpful comments.References[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.1999.
An algorithm that learns what?s in a name.
Machine Learning, 34:211?231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak.
2000.
Assigningfunction tags to parsed text.
In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak.
2001.
Immediate-head parsing for languagemodels.
In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow.
2003.
Use of deeplinguistics features for the recognition and labeling of semantic arguments.EMNLP-03.
[Collins1999] Michael John Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania,Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi.
1992.
A freelyavailable wide coverage morphological analyzer for English.
In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy.
2003.
Amaximum entropy approach to framenet tagging.
In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier.
2003.
Identi-fying semantic roles using combinatory categorial grammar.
In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000.
Automaticlabeling of semantic roles.
In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002.
The necessityof syntactic parsing for predicate argument recognition.
In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003.
Target worddetection and semantic role chunking using support vector machines.
In HLT-03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-tin, and Dan Jurafsky.
2003.
Shallow semantic parsing using support vectormachines.
Technical Report TR-CSLR-2003-1, Center for Spoken LanguageResearch, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha.
1998.
Statisticalmodels for co-occurrence data.
Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002.Adding semantic annotation to the Penn Treebank.
In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000.
Use of sup-port vector learning for chunk identification.
In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In NAACL-01.
[LDC2002] LDC.
2002.
The AQUAINT Corpus of English News Text, Catalogno.
LDC2002t31.
[Lin1998] Dekang Lin.
1998.
Automatic retrieval and clustering of similar words.In COLING-98.
[Magerman1994] David Magerman.
1994.
Natural Language Parsing as Statisti-cal Pattern Recognition.
Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt.
2000.
Probabilities for support vector machines.
InA.
Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances inLarge Margin Classifiers.
MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-tin, and Dan Jurafsky.
2003.
Semantic role parsing: Adding semantic struc-ture to unstructured text.
In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and PaulAarseth.
2003.
Using predicate-argument structures for information extrac-tion.
In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.Manning.
2003.
A generative model for semantic role labeling.
In ECML-03.
