Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsNonlinear Evidence Fusion and Propagationfor Hyponymy Relation MiningFan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin11Microsoft Research Asia2Nankai University, China3Harbin Institute of Technology, China{shumings, cyl}@microsoft.comAbstractThis paper focuses on mining the hypon-ymy (or is-a) relation from large-scale,open-domain web documents.
A nonlinearprobabilistic model is exploited to modelthe correlation between sentences in theaggregation of pattern matching results.Based on the model, we design a set of ev-idence combination and propagation algo-rithms.
These significantly improve theresult quality of existing approaches.
Ex-perimental results conducted on 500 mil-lion web pages and hypernym labels for300 terms show over 20% performanceimprovement in terms of P@5, MAP andR-Precision.1 Introduction1An important task in text mining is the automaticextraction of entities and their lexical relations; thishas wide applications in natural language pro-cessing and web search.
This paper focuses onmining the hyponymy (or is-a) relation from large-scale, open-domain web documents.
From theviewpoint of entity classification, the problem is toautomatically assign fine-grained class labels toterms.There have been a number of approaches(Hearst 1992; Pantel & Ravichandran 2004; Snowet al, 2005; Durme & Pasca, 2008; Talukdar et al,2008) to address the problem.
These methods typi-cally exploited manually-designed or automatical-* This work was performed when Fan Zhang and Shuqi Sunwere interns at Microsoft Research Asialy-learned patterns (e.g., ?NP such as NP?, ?NPlike NP?, ?NP is a NP?).
Although some degree ofsuccess has been achieved with these efforts, theresults are still far from perfect, in terms of bothrecall and precision.
As will be demonstrated inthis paper, even by processing a large corpus of500 million web pages with the most popular pat-terns, we are not able to extract correct labels formany (especially rare) entities.
Even for popularterms, incorrect results often appear in their labellists.The basic philosophy in existing hyponymy ex-traction approaches (and also many other text-mining methods) is counting: count the number ofsupporting sentences.
Here a supporting sentenceof a term-label pair is a sentence from which thepair can be extracted via an extraction pattern.
Wedemonstrate that the specific way of counting has agreat impact on result quality, and that the state-of-the-art counting methods are not optimal.
Specifi-cally, we examine the problem from the viewpointof probabilistic evidence combination and find thatthe probabilistic assumption behind simple count-ing is the statistical independence between the ob-servations of supporting sentences.
By assuming apositive correlation between supporting sentenceobservations and adopting properly designed non-linear combination functions, the results precisioncan be improved.It is hard to extract correct labels for rare termsfrom a web corpus due to the data sparseness prob-lem.
To address this issue, we propose an evidencepropagation algorithm motivated by the observa-tion that similar terms tend to share common hy-pernyms.
For example, if we already know that 1)Helsinki and Tampere are cities, and 2) Porvoo issimilar to Helsinki and Tampere, then Porvoo is1159very likely also a city.
This intuition, however,does not mean that the labels of a term can alwaysbe transferred to its similar terms.
For example,Mount Vesuvius and Kilimanjaro are volcanoesand Lhotse is similar to them, but Lhotse is not avolcano.
Therefore we should be very conservativeand careful in hypernym propagation.
In our prop-agation algorithm, we first construct some pseudosupporting sentences for a term from the support-ing sentences of its similar terms.
Then we calcu-late label scores for terms by performing nonlinearevidence combination based on the (pseudo andreal) supporting sentences.
Such a nonlinear prop-agation algorithm is demonstrated to perform bet-ter than linear propagation.Experimental results on a publicly available col-lection of 500 million web pages with hypernymlabels annotated for 300 terms show that our non-linear evidence fusion and propagation significant-ly improve the precision and coverage of theextracted hyponymy data.
This is one of the tech-nologies adopted in our semantic search and min-ing system NeedleSeek2.In the next section, we discuss major related ef-forts and how they differ from our work.
Section 3is a brief description of the baseline approach.
Theprobabilistic evidence combination model that weexploited is introduced in Section 4.
Our main ap-proach is illustrated in Section 5.
Section 6 showsour experimental settings and results.
Finally, Sec-tion 7 concludes this paper.2 Related WorkExisting efforts for hyponymy relation extractionhave been conducted upon various types of datasources, including plain-text corpora (Hearst 1992;Pantel & Ravichandran, 2004; Snow et al, 2005;Snow et al, 2006; Banko, et al, 2007; Durme &Pasca, 2008; Talukdar et al, 2008), semi-structured web pages (Cafarella  et al, 2008; Shin-zato & Torisawa, 2004), web search results (Geraciet al, 2006; Kozareva et al, 2008; Wang & Cohen,2009), and query logs (Pasca 2010).
Our target foroptimization in this paper is the approaches thatuse lexico-syntactic patterns to extract hyponymyrelations from plain-text corpora.
Our future workwill study the application of the proposed algo-rithms on other types of approaches.2 http://research.microsoft.com/en-us/projects/needleseek/ orhttp://needleseek.msra.cn/The probabilistic evidence combination modelthat we exploit here was first proposed in (Shi etal., 2009), for combining the page in-link evidencein building a nonlinear static-rank computationalgorithm.
We applied it to the hyponymy extrac-tion problem because the model takes the depend-ency between supporting sentences intoconsideration and the resultant evidence fusionformulas are quite simple.
In (Snow et al, 2006), aprobabilistic model was adopted to combine evi-dence from heterogeneous relationships to jointlyoptimize the relationships.
The independence ofevidence was assumed in their model.
In compari-son, we show that better results will be obtained ifthe evidence correlation is modeled appropriately.Our evidence propagation is basically about us-ing term similarity information to help instancelabeling.
There have been several approacheswhich improve hyponymy extraction with instanceclusters built by distributional similarity.
In (Pantel& Ravichandran, 2004), labels were assigned tothe committee (i.e., representative members) of asemantic class and used as the hypernyms of thewhole class.
Labels generated by their approachtend to be rather coarse-grained, excluding the pos-sibility of a term having its private labels (consid-ering the case that one meaning of a term is notcovered by the input semantic classes).
In contrastto their method, our label scoring and ranking ap-proach is applied to every single term rather than asemantic class.
In addition, we also compute labelscores in a nonlinear way, which improves resultsquality.
In Snow et al (2005), a supervised ap-proach was proposed to improve hypernym classi-fication using coordinate terms.
In comparison, ourapproach is unsupervised.
Durme & Pasca (2008)cleaned the set of instance-label pairs with aTF*IDF like method, by exploiting clusters of se-mantically related phrases.
The core idea is to keepa term-label pair (T, L) only if the number of termshaving the label L in the term T?s cluster is above athreshold and if L is not the label of too many clus-ters (otherwise the pair will be discarded).
In con-trast, we are able to add new (high-quality) labelsfor a term with our evidence propagation method.On the other hand, low quality labels get smallerscore gains via propagation and are ranked lower.Label propagation is performed in (Talukdar etal., 2008; Talukdar & Pereira, 2010) based on mul-tiple instance-label graphs.
Term similarity infor-mation was not used in their approach.1160Most existing work tends to utilize small-scaleor private corpora, whereas the corpus that we usedis publicly available and much larger than most ofthe existing work.
We published our term sets (re-fer to Section 6.1) and their corresponding userjudgments so researchers working on similar topicscan reproduce our results.Type PatternHearst-I NPL {,} (such as) {NP,}* {and|or} NPHearst-IINPL {,} (include(s) | including) {NP,}*{and|or} NPHearst-III NPL {,} (e.g.|e.g) {NP,}* {and|or} NPIsA-I NP (is|are|was|were|being) (a|an) NPLIsA-II NP (is|are|was|were|being) {the, those} NPLIsA-III NP (is|are|was|were|being) {another, any} NPLTable 1.
Patterns adopted in this paper (NP: namedphrase representing an entity; NPL: label)3 PreliminariesThe problem addressed in this paper is corpus-based is-a relation mining: extracting hypernyms(as labels) for entities from a large-scale, open-domain document corpus.
The desired output is amapping from terms to their corresponding hyper-nyms, which can naturally be represented as aweighted bipartite graph (term-label graph).
Typi-cally we are only interested in top labels of a termin the graph.Following existing efforts, we adopt pattern-matching as a basic way of extracting hyper-nymy/hyponymy relations.
Two types of patterns(refer to Table 1) are employed, including the pop-ular ?Hearst patterns?
(Hearst, 1992) and the IsApatterns which are exploited less frequently in ex-isting hyponym mining efforts.
One or more term-label pairs can be extracted if a pattern matches asentence.
In the baseline approach, the weight ofan edge T?L (from term T to hypernym label L) inthe term-label graph is computed as,w(T?L)      ( )( )(3.1)where m is the number of times the pair (T, L) isextracted from the corpus, DF(L) is the number ofin-links of L in the graph, N is total number ofterms in the graph, and IDF means the ?inversedocument frequency?.A term can only keep its top-k neighbors (ac-cording to the edge weight) in the graph as its finallabels.Our pattern matching algorithm implemented inthis paper uses part-of-speech (POS) tagging in-formation, without adopting a parser or a chunker.The noun phrase boundaries (for terms and labels)are determined by a manually designed POS taglist.4 Probabilistic Label-Scoring ModelHere we model the hyponymy extraction problemfrom the probability theory point of view, aimingat estimating the score of a term-label pair (i.e., thescore of a label w.r.t.
a term) with probabilisticevidence combination.
The model was studied in(Shi et al, 2009) to combine the page in-link evi-dence in building a nonlinear static-rank computa-tion algorithm.We represent the score of a term-label pair bythe probability of the label being a correct hyper-nym of the term, and define the following events,AT,L: Label L is a hypernym of term T (the ab-breviated form A is used in this paper unless it isambiguous).Ei: The observation that (T, L) is extracted froma sentence Si via pattern matching (i.e., Si is a sup-porting sentence of the pair).Assuming that we already know m supportingsentences (S1~Sm), our problem is to computeP(A|E1,E2,..,Em), the posterior probability that L isa hypernym of term T, given evidence E1~Em.Formally, we need to find a function f to satisfy,P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1)For simplicity, we first consider the case ofm=2.
The case of m>2 is quite similar.We start from the simple case of independentsupporting sentences.
That is,(     )   (  )   (  ) (4.2)(       )   (    )   (    ) (4.3)By applying Bayes rule, we get,(       )(       )   ( )(     )(    )   ( )(  )(    )   ( )(  )( )(    )   (    )( )(4.4)Then define(   )(   )( )( (   ))     ( ( ))1161Here G(A|E) represents the log-probability-gainof A given E, with the meaning of the gain in thelog-probability value of A after the evidence E isobserved (or known).
It is a measure of the impactof evidence E to the probability of event A. Withthe definition of G(A|E), Formula 4.4 can be trans-formed to,(       )   (    )   (    ) (4.5)Therefore, if E1 and E2 are independent, the log-probability-gain of A given both pieces of evidencewill exactly be the sum of the gains of A given eve-ry single piece of evidence respectively.
It is easyto prove (by following a similar procedure) that theabove Formula holds for the case of m>2, as longas the pieces of evidence are mutually independent.Therefore for a term-label pair with m mutuallyindependent supporting sentences, if we set everygain G(A|Ei) to be a constant value g, the posteriorgain score of the pair will be ?
.
If thevalue g is the IDF of label L, the posterior gain willbe,G(AT,L|E1?,Em) ?
( )( ) (4.6)This is exactly the Formula 3.1.
By this way, weprovide a probabilistic explanation of scoring thecandidate labels for a term via simple counting.Hearst-I IsA-IE1: Hearst-IE2: IsA-IRA:(      )(    ) (    )66.87 17.30 24.38R:(    )(  ) (  )5997 1711 802.7RA/R 0.011 0.010 0.030Table 2.
Evidence dependency estimation for intra-pattern and inter-pattern supporting sentencesIn the above analysis, we assume the statisticalindependence of the supporting sentence observa-tions, which may not hold in reality.
Intuitively, ifwe already know one supporting sentence S1 for aterm-label pair (T, L), then we have more chance tofind another supporting sentence than if we do notknow S1.
The reason is that, before we find S1, wehave to estimate the probability with the chance ofdiscovering a supporting sentence for a randomterm-label pair.
The probability is quite low be-cause most term-label pairs do not have hyponymyrelations.
Once we have observed S1, however, thechance of (T, L) having a hyponymy relation in-creases.
Therefore the chance of observing anothersupporting sentence becomes larger than before.Table 2 shows the rough estimation of(      )(    ) (    )(denoted as RA),(    )(  ) (  )(denotedas R), and their ratios.
The statistics are obtainedby performing maximal likelihood estimation(MLE) upon our corpus and a random selection ofterm-label pairs from our term sets (see Section6.1) together with their top labels3.
The data veri-fies our analysis about the correlation between E1and E2 (note that R=1 means independent).
In addi-tion, it can be seen that the conditional independ-ence assumption of Formula 4.3 does not hold(because RA>1).
It is hence necessary to considerthe correlation between supporting sentences in themodel.
The estimation of Table 2 also indicatesthat,(     )(  ) (  )(       )(    ) (    )(4.7)By following a similar procedure as above, withFormulas 4.2 and 4.3 replaced by 4.7, we have,(       )   (    )   (    ) (4.8)This formula indicates that when the supportingsentences are positively correlated, the posteriorscore of label L w.r.t.
term T (given both the sen-tences) is smaller than the sum of the gains causedby one sentence only.
In the extreme case that sen-tence S2 fully depends on E1 (i.e.
P(E2|E1)=1), it iseasy to prove that(       )   (    )It is reasonable, since event E2 does not bring inmore information than E1.Formula 4.8 cannot be used directly for compu-ting the posterior gain.
What we really need is afunction h satisfying(         )   ( (    )    (    )) (4.9)and(      )  ?
(4.10)Shi et al (2009) discussed other constraints to hand suggested the following nonlinear functions,(      )    (  ?
()    )  (4.11)3 RA is estimated from the labels judged as ?Good?
; whereasthe estimation of R is from all judged labels.1162(      )  ??
(p>1) (4.12)In the next section, we use the above two h func-tions as basic building blocks to compute labelscores for terms.5 Our ApproachMultiple types of patterns (Table 1) can be adoptedto extract term-label pairs.
For two supporting sen-tences the correlation between them may dependon whether they correspond to the same pattern.
InSection 5.1, our nonlinear evidence fusion formu-las are constructed by making specific assumptionsabout the correlation between intra-pattern sup-porting sentences and inter-pattern ones.Then in Section 5.2, we introduce our evidencepropagation technique in which the evidence of a(T, L) pair is propagated to the terms similar to T.5.1 Nonlinear evidence fusionFor a term-label pair (T, L), assuming K patternsare used for hyponymy extraction and the support-ing sentences discovered with pattern i are,(5.1)where mi is the number of supporting sentencescorresponding to pattern i.
Also assume the gainscore of Si,j is xi,j, i.e., xi,j=G(A|Si,j).Generally speaking, supporting sentences corre-sponding to the same pattern typically have a high-er correlation than the sentences corresponding todifferent patterns.
This can be verified by the datain Table-2.
By ignoring the inter-pattern correla-tions, we make the following simplified assump-tion:Assumption: Supporting sentences correspond-ing to the same pattern are correlated, while thoseof different patterns are independent.According to this assumption, our label-scoringfunction is,(   )  ?
(               )(5.2)In the simple case that         ( ) , if the hfunction of Formula 4.12 is adopted, then,(   )  (?
?)
( ) (5.3)We use an example to illustrate the above for-mula.Example: For term T and label L1, assume thenumbers of the supporting sentences correspondingto the six pattern types in Table 1 are (4, 4, 4, 4, 4,4), which means the number of supporting sen-tences discovered by each pattern type is 4.
Alsoassume the supporting-sentence-count vector oflabel L2 is (25, 0, 0, 0, 0, 0).
If we use Formula 5.3to compute the scores of L1 and L2, we can havethe following (ignoring IDF for simplicity),Score(L1)   ?
; Score(L2) ?One the other hand, if we simply count the totalnumber of supporting sentences, the score of L2will be larger.The rationale implied in the formula is: For agiven term T, the labels supported by multipletypes of patterns tend to be more reliable thanthose supported by a single pattern type, if theyhave the same number of supporting sentences.5.2 Evidence propagationAccording to the evidence fusion algorithm de-scribed above, in order to extract term labels relia-bly, it is desirable to have many supportingsentences of different types.
This is a big challengefor rare terms, due to their low frequency in sen-tences (and even lower frequency in supportingsentences because not all occurrences can be cov-ered by patterns).
With evidence propagation, weaim at discovering more supporting sentences forterms (especially rare terms).
Evidence propaga-tion is motivated by the following two observa-tions:(I) Similar entities or coordinate terms tend toshare some common hypernyms.
(II) Large term similarity graphs are able to bebuilt efficiently with state-of-the-art techniques(Agirre et al, 2009; Pantel et al, 2009; Shi et al,2010).
With the graphs, we can obtain the similari-ty between two terms without their hypernyms be-ing available.The first observation motivates us to ?borrow?the supporting sentences from other terms as auxil-iary evidence of the term.
The second observationmeans that new information is brought with thestate-of-the-art term similarity graphs (in additionto the term-label information discovered with thepatterns of Table 1).1163Our evidence propagation algorithm containstwo phases.
In phase I, some pseudo supportingsentences are constructed for a term from the sup-porting sentences of its neighbors in the similaritygraph.
Then we calculate the label scores for termsbased on their (pseudo and real) supporting sen-tences.Phase I: For every supporting sentence S andevery similar term T1 of the term T, add a pseudosupporting sentence S1 for T1, with the gain score,(         )       (    )   (      ) (5.5)where         is the propagation factor, and(   ) is the term similarity function taking val-ues in [0, 1].
The formula reasonably assumes thatthe gain score of the pseudo supporting sentencedepends on the gain score of the original real sup-porting sentence, the similarity between the twoterms, and the propagation factor.Phase II: The nonlinear evidence combinationformulas in the previous subsection are adopted tocombine the evidence of pseudo supporting sen-tences.Term similarity graphs can be obtained by dis-tributional similarity or patterns (Agirre et al,2009; Pantel et al, 2009; Shi et al, 2010).
We callthe first type of graph DS and the second type PB.DS approaches are based on the distributional hy-pothesis (Harris, 1985), which says that terms ap-pearing in analogous contexts tend to be similar.
Ina DS approach, a term is represented by a featurevector, with each feature corresponding to a con-text in which the term appears.
The similarity be-tween two terms is computed as the similaritybetween their corresponding feature vectors.
In PBapproaches, a list of carefully-designed (or auto-matically learned) patterns is exploited and appliedto a text collection, with the hypothesis that theterms extracted by applying each of the patterns toa specific piece of text tend to be similar.
Two cat-egories of patterns have been studied in the litera-ture (Heast 1992; Pasca 2004; Kozareva et al,2008; Zhang et al, 2009): sentence lexical patterns,and HTML tag patterns.
An example of sentencelexical patterns is ?T {, T}*{,} (and|or) T?.
HTMLtag patterns include HTML tables, drop-down lists,and other tag repeat patterns.
In this paper, wegenerate the DS and PB graphs by adopting thebest-performed methods studied in (Shi et al,2010).
We will compare, by experiments, the prop-agation performance of utilizing the two categoriesof graphs, and also investigate the performance ofutilizing both graphs for evidence propagation.6 Experiments6.1 Experimental setupCorpus We adopt a publicly available dataset inour experiments: ClueWeb094.
This is a very largedataset collected by Carnegie Mellon University inearly 2009 and has been used by several tracks ofthe Text Retrieval Conference (TREC)5.
The wholedataset consists of 1.04 billion web pages in tenlanguages while only those in English, about 500million pages, are used in our experiments.
Thereason for selecting such a dataset is twofold: First,it is a corpus large enough for conducting web-scale experiments and getting meaningful results.Second, since it is publicly available, it is possiblefor other researchers to reproduce the experimentsin this paper.Term sets Approaches are evaluated by usingtwo sets of selected terms: Wiki200, and Ext100.For every term in the term sets, each approachgenerates a list of hypernym labels, which aremanually judged by human annotators.
Wiki200 isconstructed by first randomly selecting 400 Wik-ipedia6 titles as our candidate terms, with the prob-ability of a title T being selected to be     (( )), where F(T) is the frequency of T in our datacorpus.
The reason of adopting such a probabilityformula is to balance popular terms and rare onesin our term set.
Then 200 terms are manually se-lected from the 400 candidate terms, with the prin-ciple of maximizing the diversity of terms in termsof length (i.e., number of words) and type (person,location, organization, software, movie, song, ani-mal, plant, etc.).
Wiki200 is further divided intotwo subsets: Wiki100H and Wiki100L, containingrespectively the 100 high-frequency and low-frequency terms.
Ext100 is built by first selecting200 non-Wikipedia-title terms at random from theterm-label graph generated by the baseline ap-proach (Formula 3.1), then manually selecting 100terms.Some sample terms in the term sets are listed inTable 3.4 http://boston.lti.cs.cmu.edu/Data/clueweb09/5 http://trec.nist.gov/6 http://www.wikipedia.org/1164TermSetSample TermsWiki200Canon EOS 400D, Disease management, El Sal-vador, Excellus Blue Cross Blue Shield, F33,Glasstron, Indium, Khandala, Kung Fu, LakeGreenwood, Le Gris, Liriope, Lionel Barrymore,Milk, Mount Alto, Northern Wei, Pink Lady,Shawshank, The Dog Island, White flight, WorldWar II?Ext100A2B, Antique gold, GPTEngine, Jinjiang Inn,Moyea SWF to Apple TV Converter, Nanny ser-vice, Outdoor living, Plasmid DNA, Popon, Spamdetection, Taylor Ho Bynum, Villa Michelle?Table 3.
Sample terms in our term setsAnnotation For each term in the term set, thetop-5 results (i.e., hypernym labels) of variousmethods are mixed and judged by human annota-tors.
Each annotator assigns each result item ajudgment of ?Good?, ?Fair?
or ?Bad?.
The annota-tors do not know the method by which a result itemis generated.
Six annotators participated in the la-beling with a rough speed of 15 minutes per term.We also encourage the annotators to add new goodresults which are not discovered by any method.The term sets and their corresponding user anno-tations are available for download at the followinglinks (dataset ID=data.queryset.semcat01):http://research.microsoft.com/en-us/projects/needleseek/http://needleseek.msra.cn/datasets/Evaluation We adopt the following metrics toevaluate the hypernym list of a term generated byeach method.
The evaluation score on a term set isthe average over all the terms.Precision@k: The percentage of relevant (goodor fair) labels in the top-k results (labels judged as?Fair?
are counted as 0.5)Recall@k: The ratio of relevant labels in the top-k results to the total number of relevant labelsR-Precision: Precision@R where R is the totalnumber of labels judged as ?Good?Mean average precision (MAP): The average ofprecision values at the positions of all good or fairresultsBefore annotation and evaluation, the hypernymlist generated by each method for each term is pre-processed to remove duplicate items.
Two hyper-nyms are called duplicate items if they share thesame head word (e.g., ?military conflict?
and ?con-flict?).
For duplicate hypernyms, only the first (i.e.,the highest ranked one) in the list is kept.
The goalwith such a preprocessing step is to partially con-sider results diversity in evaluation and to make amore meaningful comparison among differentmethods.
Consider two hypernym lists for ?sub-way?
:List-1: restaurant; chain restaurant; worldwide chainrestaurant; franchise; restaurant franchise?List-2: restaurant; franchise; transportation; company;fast food?There are more detailed hypernyms in the firstlist about ?subway?
as a restaurant or a franchise;while the second list covers a broader range ofmeanings for the term.
It is hard to say which isbetter (without considering the upper-layer appli-cations).
With this preprocessing step, we keep ourfocus on short hypernyms rather than detailed ones.Term Set Method MAP R-Prec P@1 P@5Wiki200Linear 0.357 0.376 0.783 0.547Log0.3713.92%0.3842.13%0.8032.55%0.5612.56%PNorm0.3724.20%0.3842.13%0.8002.17%0.5622.74%Wiki100HLinear 0.363 0.382 0.805 0.627Log0.3938.26%0.4025.24%0.8454.97%0.6605.26%PNorm0.3958.82%0.4035.50%0.8404.35%0.6625.28%Table 4.
Performance comparison among variousevidence fusion methods (Term sets: Wiki200 andWiki100H; p=2 for PNorm)6.2 Experimental resultsWe first compare the evaluation results of differentevidence fusion methods mentioned in Section 4.1.In Table 4, Linear means that Formula 3.1 is usedto calculate label scores, whereas Log and PNormrepresent our nonlinear approach with Formulas4.11 and 4.12 being utilized.
The performance im-provement numbers shown in the table are basedon the linear version; and the upward pointing ar-rows indicate relative percentage improvementover the baseline.
From the table, we can see thatthe nonlinear methods outperform the linear oneson the Wiki200 term set.
It is interesting to notethat the performance improvement is more signifi-cant on Wiki100H, the set of high frequency terms.By examining the labels and supporting sentencesfor the terms in each term set, we find that formany low-frequency terms (in Wiki100L), thereare only a few supporting sentences (corresponding1165to one or two patterns).
So the scores computed byvarious fusion algorithms tend to be similar.
Incontrast, more supporting sentences can be discov-ered for high-frequency terms.
Much informationis contained in the sentences about the hypernymsof the high-frequency terms, but the linear functionof Formula 3.1 fails to make effective use of it.The two nonlinear methods achieve better perfor-mance by appropriately modeling the dependencybetween supporting sentences and computing thelog-probability gain in a better way.The comparison of the linear and nonlinearmethods on the Ext100 term set is shown in Table5.
Please note that the terms in Ext100 do not ap-pear in Wikipedia titles.
Thanks to the scale of thedata corpus we are using, even the baseline ap-proach achieves reasonably good performance.Please note that the terms (refer to Table 3) we areusing are ?harder?
than those adopted for evalua-tion in many existing papers.
Again, the resultsquality is improved with the nonlinear methods,although the performance improvement is not bigdue to the reason that most terms in Ext100 arerare.
Please note that the recall (R@1, R@5) in thispaper is pseudo-recall, i.e., we treat the number ofknown relevant (Good or Fair) results as the totalnumber of relevant ones.Method MAP R-Prec P@1 P@5 R@1 R@5Linear 0.384 0.429 0.665 0.472 0.116 0.385Log0.395 0.429 0.715 0.472 0.125 0.3852.86%  0%  7.52%  0%  7.76%  0%PNorm0.390 0.429 0.700 0.472 0.120 0.3851.56%  0%   5.26%  0%  3.45%  0%Table 5.
Performance comparison among variousevidence fusion methods (Term set: Ext100; p=2for PNorm)The parameter p in the PNorm method is relatedto the degree of correlations among supportingsentences.
The linear method of Formula 3.1 corre-sponds to the special case of p=1; while p=  rep-resents the case that other supporting sentences arefully correlated to the supporting sentence with themaximal log-probability gain.
Figure 1 shows that,for most of the term sets, the best performance isobtained for   [2.0, 4.0].
The reason may be thatthe sentence correlations are better estimated withp values in this range.Figure 1.
Performance curves of PNorm with dif-ferent parameter values (Measure: MAP)The experimental results of evidence propaga-tion are shown in Table 6.
The methods for com-parison are,Base: The linear function without propagation.NL: Nonlinear evidence fusion (PNorm withp=2) without propagation.LP: Linear propagation, i.e., the linear functionis used to combine the evidence of pseudo support-ing sentences.NLP: Nonlinear propagation where PNorm(p=2) is used to combine the pseudo supportingsentences.NL+NLP: The nonlinear method is used tocombine both supporting sentences and pseudosupporting sentences.Method MAP R-Prec P@1 P@5 R@5Base 0.357 0.376 0.783 0.547 0.317NL0.372 0.384 0.800 0.562 0.3254.20%  2.13%  2.17%  2.74%  2.52%LP0.357 0.376 0.783 0.547 0.3170%  0%  0%  0%  0%NLP0.396 0.418 0.785 0.605 0.35710.9%  11.2%  0.26%  10.6%  12.6%NL+NLP0.447 0.461 0.840 0.667 0.40425.2%  22.6%  7.28%  21.9%  27.4%Table 6.
Evidence propagation results (Term set:Wiki200; Similarity graph: PB; Nonlinear formula:PNorm)In this paper, we generate the DS (distributionalsimilarity) and PB (pattern-based) graphs by adopt-ing the best-performed methods studied in (Shi etal., 2010).
The performance improvement numbers(indicated by the upward pointing arrows) shownin tables 6~9 are relative percentage improvement1166over the base approach (i.e., linear function with-out propagation).
The values of parameter   are setto maximize the MAP values.Several observations can be made from Table 6.First, no performance improvement can be ob-tained with the linear propagation method (LP),while the nonlinear propagation algorithm (NLP)works quite well in improving both precision andrecall.
The results demonstrate the high correlationbetween pseudo supporting sentences and the greatpotential of using term similarity to improve hy-pernymy extraction.
The second observation is thatthe NL+NLP approach achieves a much larger per-formance improvement than NL and NLP.
Similarresults (omitted due to space limitation) can beobserved on the Ext100 term set.Method MAP R-Prec P@1 P@5 R@5Base 0.357 0.376 0.783 0.547 0.317NL+NLP(PB)0.415 0.439 0.830 0.633 0.37916.2%  16.8%  6.00%  15.7%  19.6%NL+NLP(DS)0.456 0.469 0.843 0.673 0.40627.7%  24.7%  7.66%  23.0%  28.1%NL+NLP(PB+DS)0.473 0.487 0.860 0.700 0.43432.5%  29.5%  9.83%  28.0%  36.9%Table 7.
Combination of PB and DS graphs forevidence propagation (Term set: Wiki200; Nonlin-ear formula: Log)Method MAP R-Prec P@1 P@5 R@5Base 0.351 0.370 0.760 0.467 0.317NL+NLP(PB)0.411 0.448 0.770 0.564 0.401?17.1% ?21.1% ?1.32% ?20.8% ?26.5%NL+NLP(DS)0.469 0.490 0.815 0.622 0.43833.6%  32.4%  7.24%  33.2%  38.2%NL+NLP(PB+DS)0.491 0.513 0.860 0.654 0.47939.9%  38.6%  13.2%  40.0%  51.1%Table 8.
Combination of PB and DS graphs forevidence propagation (Term set: Wiki100L)Now let us study whether it is possible to com-bine the PB and DS graphs to obtain better results.As shown in Tables 7, 8, and 9 (for term setsWiki200, Wiki100L, and Ext100 respectively, us-ing the Log formula for fusion and propagation),utilizing both graphs really yields additional per-formance gains.
We explain this by the fact that theinformation in the two term similarity graphs tendsto be complimentary.
The performance improve-ment over Wiki100L is especially remarkable.
Thisis reasonable because rare terms do not have ade-quate information in their supporting sentences dueto data sparseness.
As a result, they benefit themost from the pseudo supporting sentences propa-gated with the similarity graphs.Method MAP R-Prec P@1 P@5 R@5Base 0.384 0.429 0.665 0.472 0.385NL+NLP(PB)0.454 0.479 0.745 0.550 0.45618.3%  11.7%  12.0%  16.5%  18.4%NL+NLP(DS)0.404 0.441 0.720 0.486 0.4025.18%  2.66%  8.27%  2.97%  4.37%NL+NLP(PB+DS)0.483 0.518 0.760 0.586 0.49226.0%  20.6%  14.3%  24.2%  27.6%Table 9.
Combination of PB and DS graphs forevidence propagation (Term set: Ext100)7 ConclusionWe demonstrated that the way of aggregating sup-porting sentences has considerable impact on re-sults quality of the hyponym extraction task usinglexico-syntactic patterns, and the widely-usedcounting method is not optimal.
We applied a se-ries of nonlinear evidence fusion formulas to theproblem and saw noticeable performance im-provement.
The data quality is improved furtherwith the combination of nonlinear evidence fusionand evidence propagation.
We also introduced anew evaluation corpus with annotated hypernymlabels for 300 terms, which were shared with theresearch community.AcknowledgmentsWe would like to thank Matt Callcut for readingthrough the paper.
Thanks to the annotators fortheir efforts in judging the hypernym labels.Thanks to Yueguo Chen, Siyu Lei, and the anony-mous reviewers for their helpful comments andsuggestions.
The first author is partially supportedby the NSF of China (60903028,61070014), andKey Projects in the Tianjin Science and Technolo-gy Pillar Program.1167ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-ca, and A. Soroa.
2009.
A Study on Similarity andRelatedness Using Distributional and WordNet-basedApproaches.
In Proc.
of NAACL-HLT?2009.M.
Banko, M.J. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open Information Extractionfrom the Web.
In Proc.
of IJCAI?2007.M.
Cafarella, A. Halevy, D. Wang, E. Wu, and Y.Zhang.
2008.
WebTables: Exploring the Power ofTables on the Web.
In Proceedings of the 34th Con-ference on Very Large Data Bases (VLDB?2008),pages 538?549, Auckland, New Zealand.B.
Van Durme and M. Pasca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition oflabeled instances for open-domain information ex-traction.
Twenty-Third AAAI Conference on Artifi-cial Intelligence.F.
Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani.2006.
Cluster Generation and Cluster Labelling forWeb Snippets: A Fast and Accurate Hierarchical So-lution.
In Proceedings of the 13th Conference onString Processing and Information Retrieval(SPIRE?2006), pages 25?36, Glasgow, Scotland.Z.
S. Harris.
1985.
Distributional Structure.
The Philos-ophy of Linguistics.
New York: Oxford UniversityPress.M.
Hearst.
1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
In Fourteenth InternationalConference on Computational Linguistics, Nantes,France.Z.
Kozareva, E. Riloff, E.H. Hovy.
2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.
In Proc.
of ACL'2008.P.
Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu andV.
Vyas.
2009.
Web-Scale Distributional Similarityand Entity Set Expansion.
EMNLP?2009.
Singapore.P.
Pantel and D. Ravichandran.
2004.
AutomaticallyLabeling Semantic Classes.
In Proc.
of the 2004 Hu-man Language Technology Conference (HLT-NAACL?2004), 321?328.M.
Pasca.
2004.
Acquisition of Categorized NamedEntities for Web Search.
In Proc.
of CIKM?2004.M.
Pasca.
2010.
The Role of Queries in Ranking La-beled Instances Extracted from Text.
In Proc.
ofCOLING?2010, Beijing, China.S.
Shi, B. Lu, Y. Ma, and J.-R. Wen.
2009.
NonlinearStatic-Rank Computation.
In Proc.
of CIKM?2009,Kong Kong.S.
Shi, H. Zhang, X. Yuan, J.-R. Wen.
2010.
Corpus-based Semantic Class Mining: Distributional vs. Pat-tern-Based Approaches.
In Proc.
of COLING?2010,Beijing, China.K.
Shinzato and K. Torisawa.
2004.
Acquiring Hypon-ymy Relations from Web Documents.
In Proc.
of the2004 Human Language Technology Conference(HLT-NAACL?2004).R.
Snow, D. Jurafsky, and A. Y. Ng.
2005.
LearningSyntactic Patterns for Automatic Hypernym Discov-ery.
In Proceedings of the 19th Conference on NeuralInformation Processing Systems.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
SemanticTaxonomy Induction from Heterogenous Evidence.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING-ACL-06), 801?808.P.
P. Talukdar and F. Pereira.
2010.
Experiments inGraph-based Semi-Supervised Learning Methods forClass-Instance Acquisition.
In 48th Annual Meetingof the Association for Computational Linguistics(ACL?2010).P.
P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran,R.
Bhagat, and F. Pereira.
2008.
Weakly-SupervisedAcquisition of Labeled Class Instances using GraphRandom Walks.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP?2008), pages 581?589.R.C.
Wang.
W.W. Cohen.
Automatic Set Instance Ex-traction using the Web.
In Proc.
of the 47th AnnualMeeting of the Association for Computational Lin-guistics (ACL-IJCNLP?2009), pages 441?449, Sin-gapore.H.
Zhang, M. Zhu, S. Shi, and J.-R. Wen.
2009.
Em-ploying Topic Models for Pattern-based SemanticClass Discovery.
In Proc.
of the 47th Annual Meet-ing of the Association for Computational Linguistics(ACL-IJCNLP?2009), pages 441?449, Singapore.1168
