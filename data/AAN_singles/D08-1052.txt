Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495?504,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLTAG Dependency Parsing with Bidirectional Incremental ConstructionLibin ShenBBN Technologieslshen@bbn.comAravind K. JoshiUniversity of Pennsylvaniajoshi@cis.upenn.eduAbstractIn this paper, we first introduce a new archi-tecture for parsing, bidirectional incrementalparsing.
We propose a novel algorithm for in-cremental construction, which can be appliedto many structure learning problems in NLP.We apply this algorithm to LTAG dependencyparsing, and achieve significant improvementon accuracy over the previous best result onthe same data set.1 IntroductionThe phrase ?Bidirectional Incremental?
may appearself-contradictory at first sight, since incrementalparsing usually means left-to-right parsing in thecontext of conventional parsing.
In this paper, wewill extend the meaning of incremental parsing.The idea of bidirectional parsing is related tothe bidirectional sequential classification method de-scribed in (Shen et al, 2007).
In that paper, a taggerassigns labels to words of highest confidence first,and then these labels in turn serve as the context oflater labelling operations.
The bidirectional taggerobtained the best results in literature on POS taggingon the standard PTB dataset.We extend this method from labelling to structurelearning, The search space of structure learning ismuch larger, so that it is appropriate to exploit con-fidence scores in search.In this paper, we are interested in LTAG depen-dency parsing because TAG parsing is a well knownproblem of high computational complexity in reg-ular parsing.
In order to get a focus for the learn-ing algorithm, we work on a variant of LTAG basedparsing in which we learn the word dependency re-lations encoded in LTAG derivations instead of thefull-fledged trees.1.1 ParsingTwo types of parsing strategies are popular in nat-ural language parsing, which are chart parsing andincremental parsing.Suppose the input sentence is w1w2...wn.
Let cell[i, j] represent wiwi+1...wj , a substring of the sen-tence.
As far as CFG parsing is concerned, a chartparser computes the possible structures over all pos-sible cells [i, j], where 1 ?
i ?
j ?
n. The orderof computing on these n(n + 1)/2 cells is based onsome partial order , such that [p1, p2]  [q1, q2] ifq1 ?
p1 ?
p2 ?
q2.
In order to employ dynamicprogramming, one can only use a fragment of a hy-pothesis to represent the whole hypothesis, whichis assumed to satisfy conditional independence as-sumption.
It is well known that richer context rep-resentation gives rise to better parsing performance(Johnson, 1998).
However, the need for tractabilitydoes not allow much internal information to be usedto represent a hypothesis.
The designs of hypothe-ses in (Collins, 1999; Charniak, 2000) show a del-icate balance between expressiveness and tractabil-ity, which play an important role in natural languageparsing.Some recent work on incremental parsing(Collins and Roark, 2004; Shen and Joshi, 2005)showed another way to handle this problem.
Inthese incremental parsers, tree structures are usedto represent the left context.
In this way, one canaccess the whole tree to collect rich context in-formation at the expense of being limited to beamsearch, which only maintains k-best results at each495step.
Compared to chart parsing, incremental pars-ing searches for the analyses for only 2n ?
1 cells,[1, 1], [2, 2], [1, 2], .., [i, i], [1, i], .., [1, n], incremen-tally, while complex structures are used for the anal-yses for each cell, which satisfy conditional inde-pendence under a much weaker assumption.In this paper, we call this particular approachleft-to-right incremental parsing, since one can alsosearch from right to left incrementally in a similarway.
A major problem of the left-to-right approachis that one can only utilize the structural informationon the left side but not the right side.1.2 Parsing as Bidirectional ConstructionA natural way to handle this problem is to employbidirectional search, which means we can dynami-cally search the space in two directions.
So we ex-pand the idea of incremental parsing by introducinggreedy search.
Specifically, we look for the hypothe-ses over the cell [1, n] by building analyses over2n?
1 cells [ai,1, ai,2], i = 1, .., 2n?
1 step by step,where [a2n?1,1, a2n?1,2] = [1, n].
Furthermore, forany [ai,1, ai,2]?
ai,1 = ai,2, or?
?j, k, such that [ai,1, ai,2] = [aj,1, ak,2], wherej < i, k < i and aj,2 + 1 = ak,1.It is easy to show that the set {[ai,1, ai,2] | 1 ?i ?
2n?
1} forms a tree relation, which means thateach cell except the last one will be used to build an-other cell just once.
In this framework, we can beginwith several starting points in a sentence and searchin any direction.
So left-to-right parsing is only aspecial case of incremental parsing defined in thisway.
We still use complex structures to representthe partial analyses, so as to employ both top-downand bottom-up information as in (Collins and Roark,2004; Shen and Joshi, 2005).
Furthermore, we canutilize the rich context on both sides of the partialresults.Similar to bidirectional labelling in (Shen et al,2007), there are two learning tasking in this model.First, we need to learn which cell we should choose.At each step, we can select only one path.
Sec-ondly, we need to learn which operation we shouldtake for a given cell.
We maintain k-best candidatesfor each cell instead of only one, which differenti-ates this model from normal greedy search.
So ourmodel is more robust.
Furthermore, we need to findan effective way to iterate between these two tasks.Instead of giving an algorithm specially designedfor parsing, we generalize the problem for graphs.
Asentence can be viewed as a graph in which wordsare viewed as vertices and neighboring words areconnected with an arc.
In Sections 2 and 3, wewill propose decoding and training algorithms re-spectively for graph-based incremental construction,which can be applied to many structure learningproblems in NLP.We will apply this algorithm to dependency pars-ing of Lexicalized Tree Adjoining Grammar (Joshiand Schabes, 1997).
Specifically, we will train andevaluate an LTAG dependency parser over the LTAGtreebank described in Shen et al (2008).
We reportthe experimental results on PTB section 23 of theLTAG treebank.
The accuracy on LTAG dependencyis 90.5%, which is 1.2 points over 89.3%, the previ-ous best result (Shen and Joshi, 2005) on the samedata set.It should be noted that PTB-based bracketed la-belling is not an appropriate evaluation metric here,since the experiments are on an LTAG treebank.The derived trees in the LTAG treebank are differentfrom the CFG trees in PTB.
Hence, we do not usemetrics such as labeled precision and labeled recallfor evaluation.2 Graph-based Incremental Construction2.1 Idea and Data StructuresNow we define the problem formally.
We will usedependency parsing as an example to illustrate theidea.We are given a connected graph G(V,E) whosehidden structure is U , where V = {vi}, E ?V ?
V is a symmetric relation, and U = {uk} iscomposed of a set of elements that vary with ap-plications.
As far as dependency parsing is con-cerned, the input graph is simply a chain of ver-tices, where E(vi?1, vi), and its hidden structure is{uk = (vsk , vek , bk)}, where vertex vek depends onvertex vsk with label bk.A graph-based incremental construction algo-rithm looks for the hidden structure in a bottom-up496style.Let xi and xj be two sets of connected vertexesin V , where xi ?
xj = ?
and they are directly con-nected via an edge in E. Let yxi be a hypothesizedhidden structure of xi, and yxj a hypothesized hid-den structure of xj .Suppose we choose to combine yxi and yxj withan operation r to build a hypothesized hidden struc-ture for xk = xi ?
xj .
We say the process of con-struction is incremental if the output of the opera-tion, yxk = r(xi, xj, yxi, yxj) ?
yxi ?
yxj for allthe possible xi, xj , yxi, yxj and operation r. As faras dependency parsing is concerned, incrementalitymeans that we cannot remove any links coming fromthe substructures.Once yxk is built, we can no longer use yxi oryxj as a building block.
It is easy to see that leftto right incremental construction is a special case ofour approach.
So the question is how we decide theorder of construction as well as the type of operationr.
For example, in the very first step of dependencyparsing, we need to decide which two words are tobe combined as well as the dependency label to beused.This problem is solved statistically, based on thefeatures defined on the substructures involved in theoperation and their context.
Suppose we are giventhe weights of these features, we will show in thenext section how these parameters guide us to builda set of hypothesized hidden structures with beamsearch.
In Section 3, we will present a Perceptronlike algorithm (Collins, 2002; Daume?
III and Marcu,2005) to obtain the parameters.Now we introduce the data structure to be used inour algorithms.A fragment is a connected sub-graph of G(V,E).Each fragment x is associated with a set of hypothe-sized hidden structures, or fragment hypotheses forshort: Y x = {yx1 , ..., yxk}.
Each yx is a possible frag-ment hypothesis of x.It is easy to see that an operation to combine twofragments may depend on the fragments in the con-text, i.e.
fragments directly connected to one of theoperands.
So we introduce the dependency relationover fragments.
Suppose there is a dependency re-lation D ?
F ?
F , where F ?
2V is the set of allfragments in graph G. D(xi, xj) means that any op-eration on a fragment hypothesis of xi depends onthe features in the fragment hypothesis of xj , andvice versa.We are especially interested in the following twodependency relations.?
level-0 dependency: D0(xi, xj) ??
i = j.?
level-1 dependency: D1(xi, xj) ??
xi andxj are directly connected in G.Level-0 dependency means that the features ofa hypothesis for a vertex xi do not depend on thehypotheses for other vertices.
Level-1 dependencymeans that the features depend on the hypotheses ofnearby vertices only.The learning algorithm for level-0 dependency issimilar to the guided learning algorithm for labellingas described in (Shen et al, 2007).
Level-1 depen-dency requires more data structures to maintain thehypotheses with dependency relations among them.However, we do not get into the details of level-1formalism in this papers for two reasons.
One is thelimit of page space and depth of a conference pa-per.
On the other hand, our experiments show thatthe parsing performance with level-1 dependency isclose to what level-0 dependency could provides.Interested readers could refer to (Shen, 2006) fordetailed description of the learning algorithms forlevel-1 dependency.2.2 AlgorithmsAlgorithm 1 shows the procedure of building hy-potheses incrementally on a given graph G(V,E).Parameter k is used to set the beam width of search.Weight vector w is used to compute score of an op-eration.We have two sets, H and Q, to maintain hypothe-ses.
Hypotheses in H are selected in beam search,and hypotheses in Q are candidate hypotheses forthe next step of search in various directions.We first initiate the hypotheses for each vertex,and put them into set H .
For example, in depen-dency parsing, the initial value is a set of possiblePOS tags for each single word.
Then we use a queueQ to collect all the possible hypotheses over the ini-tial hypotheses H .Whenever Q is not empty, we search for the hy-pothesis with the highest score according to a givenweight vector w. Suppose we find (x, y).
We select497Algorithm 1 Incremental ConstructionRequire: graph G(V,E);Require: beam width k;Require: weight vector w;1: H ?
initH();2: Q?
initQ(H);3: repeat4: (x?, y?)?
arg max(x,y)?Q score(y);5: H ?
updateH(H,x?
);6: Q?
updateQ(Q,H, x?
);7: until (Q = ?
)DT MD NNVB CDNNNN NNstudent will take four coursestheFigure 1: After initializationtop k-best hypotheses for segment x from Q and usethem to update H .
Then we remove from Q all thehypotheses for segments that have overlap with seg-ment x.
In the end, we build new candidate hypothe-ses with the updated selected hypothesis set H , andadd them to Q.2.3 An ExampleWe use an example of dependency parsing to illus-trate the incremental construction algorithm first.Suppose the input sentence is the student will takefour courses.
We are also given the candidate POStags for each word.
So the graph is just a linear struc-ture in this case.
We use level-0 dependency and setbeam width to two.We use boxes to represent fragments.
The depen-dency links are from the parent to the child.Figure 1 shows the result after initialization.
Fig-ure 2 shows the result after the first step, combiningthe fragments of four and courses.
Figure 3 showsthe result after the second step, combining the andstudent, and figure 4 shows the result after the thirdstep, combining take and four courses.
Due to lim-ited space, we skip the rest operations.2.4 DescriptionNow we will explain the functions in Algorithm 1one by one.DT NN VBMD CD NNNN NN CD NNstudent will take four coursestheFigure 2: Step 1DT NN VBMD CD NNDT NN NN NN CD NNstudent will take four coursestheFigure 3: Step 2?
initH() initiates hypotheses for each vertex.Here we set the initial fragment hypotheses,Y xi = {yxi1 , ..., yxik }, where xi = {vi} con-tains only one vertex.?
initQ(H) initiates the queue of candidate op-erations over the current hypotheses H .
Sup-posed there exist segments xi and xj which aredirectly connected in G. We apply all possi-ble operations to all fragment hypotheses for xjand xj , and add the result hypotheses in Q. Forexample, we generate (x, y) with some opera-tion r, where segment x is xi ?
xj .All the candidate operations are organized withrespect to the segments.
For each segment, wemaintain top k candidates according to theirscores.?
updateH(H,x) is used to update hypotheses inH .
First, we remove from H all the hypotheseswhose corresponding segment is a sub-set of x.Then, we add into H the top k hypotheses forsegment x.?
updateQ(Q,H, x) is also designed to completetwo tasks.
First, we remove from Q all thehypotheses whose corresponding segment hasoverlap with segment x.
Then, we add newcandidate hypotheses depending on x in a way498DT NN VBMD CD NNDT NN NN CD NNstudent will take four coursesMDtheFigure 4: Step 3Algorithm 2 Parameter Optimization1: w?
0;2: for (round r = 0; r < R; r++) do3: load graph Gr(V,E), gold standard Hr;4: initiate H and Q;5: repeat6: (x?, y?)?
arg max(x,y)?Q score(y);7: if (y?
is compatible with Hr) then8: update H and Q;9: else10: y?
?
positive(Q,x?
);11: promote(w, y?
);12: demote(w, y?
);13: update Q with w;14: end if15: until (Q = ?
)16: end forsimilar to the initQ(H) function.
For each seg-ment, we maintain the top k candidates for eachsegment.3 Parameter OptimizationIn the previous section, we described an algorithmfor graph-based incremental construction for a givenweight vector w. In Algorithm 2, we present a Per-ceptron like algorithm to obtain the weight vectorfor the training data.For each given training sample (Gr,Hr), whereHr is the gold standard hidden structure of graphGr, we first initiate cut T , hypotheses HT and can-didate queue Q by calling initH and initQ as in Al-gorithm 1.Then we use the gold standard Hr to guide thesearch.
We select candidate (x?, y?)
which has thehighest operation score in Q.
If y?
is compatible withHr, we update H and Q by calling updateH andupdateQ as in Algorithm 1.
If y?
is incompatiblewith Hr, we treat y?
as a negative sample, and searchfor a positive sample y?
in Q with positive(Q,x?
).If there exists a hypothesis y?x?
for fragment x?which is compatible with Hr, then positive(Q,x?
)returns y?x?
.
Otherwise positive(Q,x?)
returns thecandidate hypothesis which is compatible with Hrand has the highest operation score in Q.Then we update the weight vector w with y?
andy?.
At the end, we update the candidate Q by usingthe new weights w.In order to improve the performance, we use Per-ceptron with margin in the training (Krauth andMe?zard, 1987).
The margin is proportional to theloss of the hypothesis.
Furthermore, we use aver-aged weights (Collins, 2002; Freund and Schapire,1999) in Algorithm 1.4 LTAG Dependency ParsingWe apply the new algorithm to LTAG dependencyparsing on an LTAG Treebank (Shen et al, 2008)extracted from Penn Treebank (Marcus et al, 1994)and Proposition Bank (Palmer et al, 2005).
PennTreebank was previously used to train and evalu-ate various dependency parsers (Yamada and Mat-sumoto, 2003; McDonald et al, 2005).
In theseworks, Magerman?s rules are used to pick the headat each level according to the syntactic labels in alocal context.The dependency relation encoded in the LTAGTreebank reveals deeper information for the follow-ing two reasons.
First, the LTAG architecture itselfreveals deeper dependency.
Furthermore, the PTBwas reconciled with the Propbank in the LTAG Tree-bank extraction (Shen et al, 2008).We are especially interested in the two types ofstructures in the LTAG Treebank, predicate adjunc-tion and predicate coordination.
They are used toencode dependency relations which are unavailablein other approaches.
On the other hand, these struc-tures turn out to be a big problem for the general rep-resentation of dependency relations, including ad-junction and coordination.
We will show that thealgorithm proposed here provides a nice solution forthis problem.499hassays nowheattachattachpackagesunionadjoinattachattachFigure 5: Predicate Adjunction4.1 Representation of the LTAG TreebankIn the LTAG Treebank (Shen et al, 2008), each wordis associated with a spinal template, which repre-sents the projection from the lexical item to the root.Templates are linked together to form a derivationtree.
The topology of the derivation tree shows atype of dependency relation, which we call LTAGdependency here.There are three types of operations in the LTAGTreebank, which are attachment, adjunction, and co-ordination.
Attachment is used to represent bothsubstitution and sister adjunction in the traditionalLTAG.
So it is similar to the dependency relation inother approaches.The LTAG dependency can be a non-projectiverelation thanks to the operation of adjunction.
Inthe LTAG Treebank, raising verbs and passive ECMverbs are represented as auxiliary trees to be ad-joined.
In addition, adjunction is used to handlemany cases of discontinuous arguments in Prop-bank.
For example, in the following sentence,ARG1 of says in Propbank is discontinuous, whichis First Union now has packages for seven customergroups.?
First Union, he says, now has packages forseven customer groups.In the LTAG Treebank, the subtree for he says ad-joins onto the node of has, which is the root of thederivation tree, as shown in Figure 5.Another special aspect of the LTAG Treebank isthe representation of predicate coordination.
Figure6 is the representation of the following sentence.?
I couldn?t resist rearing up on my soggy loafersand saluting.The coordination between rearing and saluting isrepresented explicitly with a coord-structure, andresistrearing salutingandIattachattach attachcoordinationFigure 6: Predicate CoordinationcontinuedstockpoundedamidattachadjoinattachFigure 7: Non-projective Adjunctionthis coord-structure attaches to resist.
It is shownin (Shen et al, 2008) that coord-structures could en-code the ambiguity of argument sharing, which canbe non-projective also.4.2 Incremental ConstructionWe build LTAG derivation trees incrementally.
Ahypothesis of a fragment is represented with a par-tial derivation tree.
When the fragment hypothesesof two nearby fragments combine, the partial deriva-tion trees are combined into one.It is trivial to combine two partial derivation treeswith attachment.
We simply attach the root of onetree to some node on the other tree which is visible tothis root node.
Adjunction is similar to attachment,except that an adjoined subtree may be visible fromthe other side of the derivation tree.
For example, insentence?
The stock of UAL Corp. continued to bepounded amid signs that British Airways ...continued adjoins onto pounded, and amid attachesto continued from the other side of the derivationtree (pounded is between continued and amid), asshown in Figure 7.The predicate coordination is decomposed into aset of operations to meet the need for incremen-tal processing.
Suppose a coordinated structure at-taches to the parent node on the left side.
We buildthis structure incrementally by attaching the first500resistrearing salutingandIattachattachattachconjoinFigure 8: Conjunction?$?+  @@R.................flQQs@@R @@Rm1.1m1.1.1s1 s2m1m1.2mmrm2 sattachFigure 9: Representation of nodesconjunct to the parent and conjoining other con-juncts to first one.
In this way, we do not need toforce the coordination to be built before the attach-ment.
Either can be executed first.
A sample isshown in Figure 8.4.3 FeaturesIn this section, we will describe the features used inLTAG dependency parsing.
An operation is repre-sented by a 4-tuple?
op = (type, dir, posleft, posright),where type ?
{attach, adjoin, conjoin} and diris used to represent the direction of the operation.posleft and posright are the POS tags of the twooperands.Features are defined on POS tags and lexical itemsof the nodes in the context.
In order to represent thefeatures, we use m for the main-node of the oper-ation, s for the sub-node, mr for the parent of themain-node, m1..mi for the children of m, and s1..sjfor the children of s, as shown in Figure 9.
The in-dex always starts from the side where the operationtakes place.
We use the Gorn addresses to representthe nodes in the subtrees rooted on m and s.Furthermore, we use lk and rk to represent thenodes in the left and right context of the flat sen-tence.
We use hl and hr to represent the head of thehypothesis trees on the left and right context respec-tively.
Let x be a node.
We use x.p to represent thePOS tag of node x, and x.w to represent the lexicalitem of node x.Table 1 show the features used in LTAG depen-dency parsing.
There are seven classes of features.The first three classes of features are those definedon only one operand, on both operands, and on thesiblings respectively.
If gold standard POS tags areused as input, we define features on the POS tags inthe context.
If level-1 dependency is used, we definefeatures on the root node of the hypothesis partialderivation trees in the neighborhood.Half check and full check features are designedfor grammatical check.
For example, in Figure 9,node s attaches onto node m from left.
Then nothingcan attach onto s from the right side.
The children ofthe right side of s are fixed, so we use the half checkfeatures to check the completeness of the childrenof the right half for s. Furthermore, we notice thatall the rightmost descendants of s and the leftmostdescendants of m at each level become unavailablefor any further operation.
So their children are fixedafter this operation.
All these nodes are in the formof m1.1...1 or s1.1...1.
We use full check features tocheck the children from both sides for these nodes.In the discussion above, we ignored adjunctionand conjunction.
We need to slightly refine the con-ditions of checking.
Due to the limit of space, weskip these cases.5 ExperimentsWe use the same data set as in (Shen and Joshi,2005).
We use Sec.
2-21 of the LTAG Treebank fortraining, Sec.
22 for feature selection, and Sec.
23for test.
Table 2 shows the comparison of differentmodels.
Beam size is set to five in our experiments.With level-0 dependency, our system achieves an ac-curacy of 90.3% at the speed of 4.25 sentences a sec-ond on a Xeon 3G Hz processor with JDK 1.5.
Withlevel-1 dependency, the parser achieves 90.5% at3.59 sentences a second.
Level-1 dependency doesnot provide much improvement due to the fact thatlevel-0 features provide most of the useful informa-tion for this specific application.It is interesting to compare our system with otherdependency parsers.
The accuracy on LTAG depen-501category description templatesone operand Features defined on only one operand.
For eachtemplate tp, [type, dir, tp] is used as a feature.
(m.p), (m.w), (m.p,m.w), (s.p),(s.w), (s.p, s.w)two operands Features defined on both operands.
For each tem-plate tp, [op, tp] is used as a feature.
In addition,[op] is also used as a feature.
(m.w), (s.w), (m.w, s.w)siblings Features defined on the children of themain nodes.
For each template tp,[op, tp], [op,m.w, tp], [op,mr.p, tp] and[op,mr.p,m.w, tp] are used as features.
(m1.p), (m1.p,m2.p), ..,(m1.p,m2.p, ..,mi.p)POS context In the case that gold standard POS tags are usedas input, features are defined on the POS tags ofthe context.
For each template tp, [op, tp] is usedas a feature.
(l2.p), (l1.p), (r1.p), (r2.p),(l2.p, l1.p), (l1.p, r1.p),(r1.p, r2.p)tree context In the case that level-1 dependency is employed,features are defined on the trees in the context.For each template tp, [op, tp] is used as a feature.
(hl.p), (hr.p)half check Suppose s1, ..., sk are all the children of s whichare between s and m in the flat sentence.
Foreach template tp, [tp] is used as a feature.
(s.p, s1.p, s2.p, .., sk.p),(m.p, s.p, s1.p, s2.p, .., sk.p)and (s.w, s.p, s1.p, s2.p, .., sk.p),(s.w,m.p, s.p, s1.p, s2.p, .., sk.p)if s.w is a verbfull check Let x1, x2, .., xk be the children of x, and xrthe parent of x.
For any x = m1.1...1 or s1.1...1,template tp, [tp(x)] is used as a feature.
(x.p, x1.p, x2.p, .., xk.p),(xr.p, x.p, x1.p, x2.p, .., xk.p) and(x.w, x.p, x1.p, x2.p, .., xk.p),(x.w, xr.p, x.p, x1.p, x2.p, .., xk.p)if x.w is a verbTable 1: Features defined on the context of operationmodel accuracy%Shen and Joshi, 2005 89.3level-0 dependency 90.3level-1 dependency 90.5Table 2: Experiments on Sec.
23 of the LTAG Treebankdency is comparable to the numbers of the previ-ous best systems on dependency extracted from PTBwith Magerman?s rules, for example, 90.3% in (Ya-mada and Matsumoto, 2003) and 90.9% in (McDon-ald et al, 2005).
However, their experiments are onthe PTB, while ours is on the LTAG corpus.It should be noted that it is more difficult to learnLTAG dependencies.
Theoretically, the LTAG de-pendencies reveal deeper relations.
Adjunction canlead to non-projective dependencies, and the depen-dencies defined on predicate adjunction are linguis-tically more motivated, as shown in the examples inFigure 5 and 7.
The explicit representation of predi-cate coordination also provides deeper relations.
Forexample, in Figure 6, the LTAG dependency con-tains resist ?
rearing and resist ?
saluting,while the Magerman?s dependency only containsresist ?
rearing.
The explicit representation ofpredicate coordination will help to solve for the de-pendencies for shared arguments.6 DiscussionIn our approach, each fragment in the graph is asso-ciated with a hidden structure, which means that wecannot reduce it to a labelling task.
Therefore, theproblem of interest to us is different from previous502work on graphical models, such as CRF (Lafferty etal., 2001) and MMMN (Taskar et al, 2003).McAllester et al (2004) introduced Case-FactorDiagram (CFD) to transform a graph based con-struction problem to a labeling problem.
However,adjunction, prediction coordination, and long dis-tance dependencies in LTAG dependency parsingmake it difficult to implement.
Our approach pro-vides a novel alternative to CFD.Our learning algorithm stems from Perceptrontraining in (Collins, 2002).
Variants of this methodhave been successfully used in many NLP tasks, likeshallow processing (Daume?
III and Marcu, 2005),parsing (Collins and Roark, 2004; Shen and Joshi,2005) and word alignment (Moore, 2005).
Theoret-ical justification for those algorithms can be appliedto our training algorithm in a similar way.In our algorithm, dependency is defined on com-plicated hidden structures instead of on a graph.Thus long distance dependency in a graph becomeslocal in hidden structures, which is desirable fromlinguistic considerations.The search strategy of our bidirectional depen-dency parser is similar to that of the bidirectionalCFG parser in (Satta and Stock, 1994; Ageno andRodrguez, 2001; Kay, 1989).
A unique contribu-tion of this paper is that selection of path and deci-sions about action are trained simultaneously withdiscriminative learning.
In this way, we can employcontext information more effectively.7 ConclusionIn this paper, we introduced bidirectional incremen-tal parsing, a new architecture of parsing.
We pro-posed a novel algorithm for graph-based incremen-tal construction, and applied this algorithm to LTAGdependency parsing, revealing deep relations, whichare unavailable in other approaches and difficult tolearn.
We evaluated the parser on an LTAG Tree-bank.
Experimental results showed significant im-provement over the previous best system.
Incre-mental construction can be applied to other structurelearning problems of high computational complex-ity, for example, such as machine translation and se-mantic parsing.ReferencesA.
Ageno and H. Rodrguez.
2001.
Probabilistic mod-elling of island-driven parsing.
In International Work-shop on Parsing Technologies.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Meeting of the North Ameri-can Chapter of the Association for Computational Lin-guistics.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics (ACL).M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proceedings of the 2002Conference of Empirical Methods in Natural Lan-guage Processing.H.
Daume?
III and D. Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In Proceedings of the 22nd In-ternational Conference on Machine Learning.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.M.
Johnson.
1998.
PCFG Models of Linguistic TreeRepresentations.
Computational Linguistics, 24(4).A.
K. Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, volume 3, pages 69?
124.
Springer-Verlag.M.
Kay.
1989.
Head-driven parsing.
In Proceedings ofWorkshop on Parsing Technologies.W.
Krauth and M. Me?zard.
1987.
Learning algorithmswith optimal stability in neural networks.
Journal ofPhysics A, 20:745?752.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional random fields: Probabilistic models for segmen-tation and labeling sequence data.
In Proceedings ofthe 18th International Conference on Machine Learn-ing.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.D.
McAllester, M. Collins, and F. Pereira.
2004.
Case-factor diagrams for structured probabilistic modeling.In UAI 2004.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Pro-ceedings of the 43th Annual Meeting of the Associationfor Computational Linguistics (ACL).503R.
Moore.
2005.
A discriminative framework for bilin-gual word alignment.
In Proceedings of Human Lan-guage Technology Conference and Conference on Em-pirical Methods in Natural Language Processing.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).G.
Satta and O.
Stock.
1994.
Bi-Directional Context-Free Grammar Parsing for Natural Language Process-ing.
Artificial Intelligence, 69(1-2).L.
Shen and A. K. Joshi.
2005.
Incremental LTAG Pars-ing.
In Proceedings of Human Language TechnologyConference and Conference on Empirical Methods inNatural Language Processing.L.
Shen, G. Satta, and A. K. Joshi.
2007.
Guided Learn-ing for Bidirectional Sequence Classification.
In Pro-ceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL).L.
Shen, L. Champollion, and A. K. Joshi.
2008.
LTAG-spinal and the Treebank: a new resource for incremen-tal, dependency and semantic parsing.
Language Re-sources and Evaluation, 42(1):1?19.L.
Shen.
2006.
Statistical LTAG Parsing.
Ph.D. thesis,University of Pennsylvania.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginmarkov networks.
In Proceedings of the 17th AnnualConference Neural Information Processing Systems.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with Support Vector Machines.
InIWPT 2003.504
