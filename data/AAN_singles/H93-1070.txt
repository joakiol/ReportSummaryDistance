The Importance of Proper Weighting MethodsChris BuckleyDepar tment  of Computer  ScienceCorne l l  Un ivers i tyI thaca ,  NY  14853ABSTRACTThe importance of good weighting methods in informationretrieval - -  methods that stress the most useful features of adocument or query representative - -  is examined.
Evidenceis presented that good weighting methods are more importantthan the feature selection process and it is suggested that thetwo need to go hand-in-hand in order to be effective.
Thepaper concludes with a method for learning a good weightfor a term based upon the characteristics of that term.1.
INTRODUCTIONOther than experimental results, the first part of thispaper contains little new material.
Instead, it's an at-tempt to demonstrate the relative importance and dif-ficulties involved in the common information retrievaltask of forming documents and query representatives andweighting features.
This is the sort of thing that tendsto get passed by word of mouth if at all, and never getspublished.
However, there is a tremendous revival of in-terest in information retrieval; thus this attempt o helpall those new people just starting in experimental infor-mation retrieval.A common approach in many areas of natural anguageprocessing is to1.
Find "features" of a natural anguage xcerpt2.
Determine the relative importance of those featureswithin the excerpt3.
Submit the weighted features to some task-appropriate decision procedureThis presentation focuses on the second sub:task above:the process of weighting features of a natural anguagerepresentation.
Features here could be things like sin-gle word occurrences, phrase occurrences, other relation-ships between words, occurrence of a word in a title,part-of-speech of a word, automatically or manually as-signed categories of a document, citations of a document,and so on.
The particular overall task addressed here isthat of information retrieval - finding textual documents(from a large set of documents) that are relevant o auser's information eed.
Weighting features is somethingthat many information retrieval systems eem to regardas being of minor importance as compared fo finding thefeatures in the first place; but the experiments describedhere suggest hat weighting is considerably more impor-tant than additional feature selection.This is not an argument hat feature selection is unim-portant, but that development of feature selection andmethods of weighting those features need to proceedhand-in-hand if there is to be hope of improving perfor-mance.
There have been many papers (and innumerableunpublished negative result experiments) where authorshave devoted tremendous resources and intellectual in-sights into finding good features to help represent a doc-ument, but then weighted those features in a haphaz-ard fashion and ended up with little or no improvement.This makes it extremely difficult for a reader to judgethe worthiness of a feature approach, especially sincethe weighting methods are very often not described indetail.Long term, the best weighting methods will obviouslybe those that can adapt weights as more information be-comes available.
Unfortunately, in information retrievalit is very difficult to learn anything useful from one querythat will be applicable to the next.
In the routing or rele-vance feedback environments, weights can be learned fora query and then applied to that same query.
But in gen-eral there is not enough overlap in vocabulary (and usesof vocabulary) between queries to learn much about theusefulness of particular words.
The second half of thispaper discusses an approach that learns the importantcharacteristics of a good term.
Those characteristics anthen be used to properly weight all terms.Several sets of experiments are described, with each setusing different types of information to determine theweights of features.
All experiments were done withthe SMART information retrieval system, most using theTREC/T IPSTER collections of documents, queries, andrelevance judgements.
Each run is evaluated using the"l l -point recall-precision average" evaluation methodthat was standard at the TREC 1 conference.349The basic SMART approach is a completely automaticindexing of the full text of both queries and documents.Common meaningless words (like 'the' or 'about') are re-moved, and all remaining words are stemmed to a rootform.
Term weights are assigned to each unique word(or other feature) in a vector by the statistical/learningprocesses described below.
The final form of a represen-tative for a document (or query) is a vectorD~ = (w~,l, w~,2,..., wi,~)where D~ represents a document (or query) text and w~,kis a term weight of term Tk attached to document Di.The similarity between a query and document is set tothe inner-product of the query vector and document vec-tor; the information retrieval system as a whole will re-turn those documents with the highest similarity to thequery.2.
AD-HOC WEIGHTSDocument or query weights can be based on any num-ber of factors; two would be statistical occurrence infor-mation and a history of how well this feature (or othersimilar features) have performed in the past.
In manysituations, it's impossible to obtain history informationand thus initial weights are often based purely on sta-tistical information.
A major class of statistical weight-ing schemes is examined below, showing that there is anenormous performance range within the class.
Then theprocess of adding additional features to a document orquery representative is examined in the context of theseweighting schemes.
These are issues that are somewhatsubtle and are often overlooked.2.1.
T f  * Id f  We ightsOver the past 25 years, one class of term weights hasproven itself to be useful over a wide variety of collec-tions.
This is the class of tf*idf (term frequency times in-verse document frequency) weights \[1, 6, 7\], that assignsweight wik to term Tk in document / ) i  in proportion tothe frequency of occurrence of the term in D~, and ininverse proportion to the number of documents to whichthe term is assigned.
The weights in the document arethen normalized by the length of the document, so thatlong documents are not automatically favored over shortdocuments.
While there have been some post-facto the-oretical justifications for some of the tf*idf weight vari-ants, the fact remains that they are used because theywork well, rather than any theoretical reason.Table 1 presents the evaluation results of running a num-ber of tf*idf variants for query weighting against a num-ber of variants for document weighting (the runs pre-sented here are only a small subset of the variants ac-tually run).
All of these runs use the same set of fea-tures (single terms), the only differences are in the termweights.
The exact variants used aren't important; whatis important is the range of results.
Disregarding one ex-tremely poor document weighting, the range of resultsis from 0.1057 to 0.2249.
Thus a good choice of weightsmay gain a system over 100%.
As points of compari-son, the best official TREC run was 0.2171 (a systemincorporating a very large amount of user knowledge todetermine features) and the median TREC run in thiscategory was 0.1595.
The best run (DOCWT = lnc,QWT = ltc), is about 24% better than the most gener-ally used tf*idf run (DOCWT = QWT = ntc).24%is a substantial difference in performance, in a fieldwhere historically an improvement of 10% is consideredquite good.
The magnitude of performance improvementdue to considering additional features uch as syntacticphrases, titles and parts of speech is generally quite small(0 - 10%).
Adding features and using good weights canof course be done at the same time; but the fact thatsomewhat subtle differences in weighting strategy canoverwhelm the effect due to additional features is worri-some.
This means the experimenter must be very carefulwhen adding features that they do not change the ap-propriateness of the weighting strategy.2.2.
Add ing  New FeaturesSuppose an experimenter has determined a good weight-ing strategy for a basic set of features used to describe aquery or document and now wishes to extend the set offeatures.
In the standard tf*idf, cosine-normalized classof weights, it is not as simple as it may first appear.
Theobvious first step, making sure the weights before nor-malization of the new set of features and the old set arecommensurate, is normally straightforward.
But thenproblems occur because of the cosine normalization.
Forexample, suppose there were two documents in a collec-tion, one of them much longer then the other:?
Di = (w1,1, wl,2, wl,3)?
D2 = (w2,1,w~,2,...w2,100)Now suppose the new approach adds a reasonably con-stant five features onto each document representative.
(Examples of such features might be title words, or cat-egories the document is in.)
If the new features are justadded on to the list of old features, and then the weightsof the features are normalized by the total length of thedocument, then there are definite problems.
Not onlydoes the weight of the added features vary accordingto the length of the document (that could very well bewhat is wanted), but the weight of the old features have350changed.
A query that does not take advantage of thenew features will suddenly find it much more difficultto retrieve short documents like D1.
D1 is now muchlonger than it was, and therefore the values of Wl,k haveall decreased because of normalization.Similarly, if the number of new added features tends tobe much more for longer documents than short (for ex-ample, a very loose definition of phrase), a query com-posed of only old features will tend to favor short doc-uments more than long (at least, more than it did orig-inally).
Since the original weighting scheme was a sup-posedly good one, these added features will hurt perfor-mance on the original feature portion of the similarity.The similarity on the added feature portion might help,but it will be difficult to judge how much.These normalization effects can be very major effects.Using a loose definition of phrase on CACM (a smalltest collection), adding phrases in the natural fashionabove will hurt performance by 12~0.
However, if thephrases are added in such a way that the weights of theoriginal single terms are not affected by normalization,then the addition of phrases improves performance by9%.One standard approach when investigating the useful-ness of adding features is to ensure that the weights ofthe old features remain unchanged throughout the inves-tigation.
In this way, the contribution of the new featurescan be isolated and studied separately at the similaritylevel.
\[Note that if this is done, the addition of new fea-tures may mean the re-addition of old features, if theweights of some old features are supposed to be modi-fied.\] This is the approach we've taken, for instance withthe weighting of phrases in TREC.
The single term in-formation and the phrase information are kept separatewithin a document vector.
Each of the separate sub-vectors is normalized by the length of the single termsub-vector.
In this way, the weights of all terms are keptcommensurate with each other, and the similarity dueto the original single terms is kept unchanged.The investigation of weighting strategies for additionalfeatures is not a simple task, even if separation of oldfeatures and new features is done.
For example, JoelFagan in his excellent study of syntactic and statisti-cal phrases\[2\], spent over 8 months looking at weightingstrategies.
But if it's not designed into the experimentfrom the beginning, it will be almost impossible.2.3.
Re levance  FeedbackOne opportunity for good term weighting occurs in therouting environment.
Here, a query is assumed to repre-sent a continuing information eed, and there have beena number of documents already seen for each query, somesubset of which has been judged relevant.
With thiswealth of document features and information available,the official TREC routing run that proved to be the mosteffective was one that took the original query terms andassigned weights based on probability of occurrence inrelevant and non-relevant documents\[3, 51.
Once again,weighting, rather than feature selection, worked verywell.
(However, in this case the feature selection processdid not directly adversely affect the weighting process.Instead, it was mostly the case that the additional fea-tures from relevant documents were simply not chosenor weighted optimally.
)In this run, using the RPI feedback model developedby Fuhr\[3\], relevance feedback information was used forcomputing the feedback query term weight q~ of a termas p~(1 -ri)/\[ri(1 -Pi)\] - 1 Here Pi is the average docu-ment term weight for relevant documents, and ri is thecorresponding factor for nonrelevant i ems.
Only theterms occurring in the query were considered here, sono query expansion took place.
Having derived thesequery term weights, the query was run against he docu-ment set.
Let di denote the document term weight, thenthe similarity of a query to a document is computed byS(q, d) = ~\](log(qi * di + 1))3.
LEARNING WEIGHTS BY  TERMFEATURESThe ad-hoc tf*idf weights above use only collectionstatistics to determine weights.
However, if previousqueries have been run on this collection, the resultsfrom those queries can be used to determine what termweighting factors are important for this collection.
Thefinal term weight is set to a linear combination ofterm weight factors, where the coemcient of each fac-tor is set to minimize the squared error for the previousqueries\[4, 5\].
The offcial TREC runs using this approachwere nearly the top results; which was somewhat sur-prising given the very limited and inaccurate traininginformation which was available.This approach to learning solves the major problem oflearning in an ad-hoc environment: he fact that there isinsufficient information about individual terms to learnreasonable weights.
Most document terms have not oc-curred in previous queries, and therefore there is no ev-idence that can be directly applied.
Instead, the knownrelevance information determines the importance of fea-tures of each term.
The particular features used inTREC 1 were combinations of the following term fac-tors:t f: within-document frequency of the term351logidf: log ( (N+l ) /n ) ,  where N is the number of doc-uments in the collection and n is the numberof documents containing the termlognumterms:  log (number of different terms of thedocument)imaxt f :  1 / (maximum within-document frequency ofa term in the document)After using the relevance information, the final weightfor a term in a TREC 1 document wasW(t,) = 0.00042293 +0.00150083 * tf* logidf* imaxtf +-0.00150665 ?
tf* imaxtf +0.00010465 * logidf +-0.00122627 * lognumterms ?
imaxtf.There is no reason why the choice of factors used inTREC 1 is optimal; slight variations had been used foran earlier experiment.
Experimentation is progressingon the choice of factors, especially when dealing withboth single terms and phrases.
However, even so, theTREC 1 evaluation results were very good.
If the mini-mal learning information used by this approach is avail-able, the results suggest it should be preferred to thead-hoc weighting schemes discussed earlier.4.
CONCLUSIONTile sets of experiments described above focus on featureweighting and emphasize that feature weighting seems tobe more important han feature selection.
This is not tosay that good feature selection is not needed for optimalperformance, but these experiments suggest that goodweighting is of equal importance.
Feature selection issexy, and weighting isn't, but optimal performance seemsto demand that weighting schemes and feature selectionneed to be developed simultaneously.Re ferences1.
Buckley, C. and Salton, G. and Allan, J., "AutomaticRetrieval With Locality Information Using SMART.
"Proceedings of the First TREC Conference, 1993.2.
Fagan, J., Experiments in Automatic Phrase Indexingfor Document Retrieval: A Comparison of Syntactic andNonsyntactic Methods, Doctoral Dissertation, CornellUniversity, Report TR 87-868, Department of ComputerScience, Ithaca, NY, 1987.3.
Fuhr, N., "Models for Retrieval with Probabilistic Index-ing."
Information Processing and Management 25(1),1989, pp.
55-72.4.
Fuhr, N. and Buckley, C., "A Probabilistic Learning Ap-proach for Document Indexing."
ACM Transactions onInformation Systems 9(3), 1991, pages 223-248.3525.
Fuhr, N. and Buckley, C., "Optimizing Document In-dexing and Search Term Weighting Based on Probabilis-tic Models" Proceedings of the First TREC Conference,"1993.6.
Salton, G. and Buckley, C., "Term Weighting Ap-proaches in Automatic Text Retrieval."
InformationProcessing and Management 24(5), 1988, pages 513-523.7.
Salton, G. and Yang, C.S., "On the Specification of TermValues in Automatic Indexing."
Journal of Documenta-tion 29(4), 1973, pages 351-372.Query "a ntc nnc atc btc ltc lnc.P_9-?ntc 1813 1594 1834 1540 1908 1738nnc 1818 1453 1916 1595 1993 1607atc 1558 1473 1682 1437 1757 1499anc 1892 1467 1908 1645 2000 1396btc 1241 1179 1454 1231 1493 1237bnc 1569 1130 1577 1421 1689 1057ltc 1909 1815 1986 1726 2061 1843lnc 2221 1857 2126 1887 2249 1716nnn 0062 0051 0059 0067 0061 0050Table 1: Comparison of tf * idf variants.All weights expressed as triplets:{tf contribution} {idf contribution} {normalization}?
tf:- n : Normal tf (ie, number of times term occursin vector)- 1 : Log.
1.0 + In (tf).- a : Augmented.
normalized between 0.5 and1.0 in each vector.
0.5 + 0.5. tf /MaxTflnVector- b : Binary (ie, always 1)?
idf:- n : None (ie, always 1)- t : Traditional (log ( (N+l ) /n ) )  where N isnumber of documents in collection and n isnumber of documents?
normalization:- n : None- c :  Cosine.
