Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25?32,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsTransductive learning for statistical machine translationNicola UeffingNational Research Council CanadaGatineau, QC, Canadanicola.ueffing@nrc.gc.caGholamreza Haffari and Anoop SarkarSimon Fraser UniversityBurnaby, BC, Canada{ghaffar1,anoop}@cs.sfu.caAbstractStatistical machine translation systems areusually trained on large amounts of bilin-gual text and monolingual text in the tar-get language.
In this paper we explore theuse of transductive semi-supervised meth-ods for the effective use of monolingual datafrom the source language in order to im-prove translation quality.
We propose sev-eral algorithms with this aim, and present thestrengths and weaknesses of each one.
Wepresent detailed experimental evaluations onthe French?English EuroParl data set and ondata from the NIST Chinese?English large-data track.
We show a significant improve-ment in translation quality on both tasks.1 IntroductionIn statistical machine translation (SMT), translationis modeled as a decision process.
The goal is to findthe translation t of source sentence s which maxi-mizes the posterior probability:argmaxtp(t | s) = argmaxtp(s | t) ?
p(t) (1)This decomposition of the probability yields two dif-ferent statistical models which can be trained in-dependently of each other: the translation modelp(s | t) and the target language model p(t).State-of-the-art SMT systems are trained on largecollections of text which consist of bilingual corpora(to learn the parameters of p(s | t)), and of monolin-gual target language corpora (for p(t)).
It has beenshown that adding large amounts of target languagetext improves translation quality considerably.
How-ever, the availability of monolingual corpora in thesource language does not help improve the system?sperformance.
We will show how such corpora canbe used to achieve higher translation quality.Even if large amounts of bilingual text are given,the training of the statistical models usually suffersfrom sparse data.
The number of possible events,i.e.
phrase pairs or pairs of subtrees in the two lan-guages, is too big to reliably estimate a probabil-ity distribution over such pairs.
Another problem isthat for many language pairs the amount of availablebilingual text is very limited.
In this work, we willaddress this problem and propose a general frame-work to solve it.
Our hypothesis is that adding infor-mation from source language text can also provideimprovements.
Unlike adding target language text,this hypothesis is a natural semi-supervised learn-ing problem.
To tackle this problem, we proposealgorithms for transductive semi-supervised learn-ing.
By transductive, we mean that we repeatedlytranslate sentences from the development set or testset and use the generated translations to improve theperformance of the SMT system.
Note that the eval-uation step is still done just once at the end of ourlearning process.
In this paper, we show that suchan approach can lead to better translations despitethe fact that the development and test data are typi-cally much smaller in size than typical training datafor SMT systems.Transductive learning can be seen as a means toadapt the SMT system to a new type of text.
Say asystem trained on newswire is used to translate we-blog texts.
The proposed method adapts the trainedmodels to the style and domain of the new input.2 Baseline MT SystemThe SMT system we applied in our experiments isPORTAGE.
This is a state-of-the-art phrase-basedtranslation system which has been made available25to Canadian universities for research and educationpurposes.
We provide a basic description here; for adetailed description see (Ueffing et al, 2007).The models (or features) which are employed bythe decoder are: (a) one or several phrase table(s),which model the translation direction p(s | t), (b) oneor several n-gram language model(s) trained withthe SRILM toolkit (Stolcke, 2002); in the experi-ments reported here, we used 4-gram models on theNIST data, and a trigram model on EuroParl, (c)a distortion model which assigns a penalty basedon the number of source words which are skippedwhen generating a new target phrase, and (d) a wordpenalty.
These different models are combined log-linearly.
Their weights are optimized w.r.t.
BLEUscore using the algorithm described in (Och, 2003).This is done on a development corpus which we willcall dev1 in this paper.
The search algorithm imple-mented in the decoder is a dynamic-programmingbeam-search algorithm.After the main decoding step, rescoring with ad-ditional models is performed.
The system generatesa 5,000-best list of alternative translations for eachsource sentence.
These lists are rescored with thefollowing models: (a) the different models used inthe decoder which are described above, (b) two dif-ferent features based on IBM Model 1 (Brown et al,1993), (c) posterior probabilities for words, phrases,n-grams, and sentence length (Zens and Ney, 2006;Ueffing and Ney, 2007), all calculated over the N -best list and using the sentence probabilities whichthe baseline system assigns to the translation hy-potheses.
The weights of these additional modelsand of the decoder models are again optimized tomaximize BLEU score.
This is performed on a sec-ond development corpus, dev2.3 The Framework3.1 The AlgorithmOur transductive learning algorithm, Algorithm 1,is inspired by the Yarowsky algorithm (Yarowsky,1995; Abney, 2004).
The algorithm works as fol-lows: First, the translation model is estimated basedon the sentence pairs in the bilingual training data L.Then, a set of source language sentences, U , is trans-lated based on the current model.
A subset of goodtranslations and their sources, Ti, is selected in eachiteration and added to the training data.
These se-lected sentence pairs are replaced in each iteration,and only the original bilingual training data, L, iskept fixed throughout the algorithm.
The processof generating sentence pairs, selecting a subset ofgood sentence pairs, and updating the model is con-tinued until a stopping condition is met.
Note thatwe run this algorithm in a transductive setting whichmeans that the set of sentences U is drawn eitherfrom a development set or the test set that will beused eventually to evaluate the SMT system or fromadditional data which is relevant to the developmentor test set.
In Algorithm 1, changing the definitionof Estimate, Score and Select will give us the dif-ferent semi-supervised learning algorithms we willdiscuss in this paper.Given the probability model p(t | s), consider thedistribution over all possible valid translations t fora particular input sentence s. We can initializethis probability distribution to the uniform distribu-tion for each sentence s in the unlabeled data U .Thus, this distribution over translations of sentencesfrom U will have the maximum entropy.
Undercertain precise conditions, as described in (Abney,2004), we can analyze Algorithm 1 as minimizingthe entropy of the distribution over translations of U .However, this is true only when the functions Esti-mate, Score and Select have very prescribed defini-tions.
In this paper, rather than analyze the conver-gence of Algorithm 1 we run it for a fixed numberof iterations and instead focus on finding useful def-initions for Estimate, Score and Select that can beexperimentally shown to improve MT performance.3.2 The Estimate FunctionWe consider the following different definitions forEstimate in Algorithm 1:Full Re-training (of all translation models): IfEstimate(L, T ) estimates the model parametersbased on L ?
T , then we have a semi-supervised al-gorithm that re-trains a model on the original train-ing data L plus the sentences decoded in the last it-eration.
The size of L can be controlled by filteringthe training data (see Section 3.5).Additional Phrase Table: If, on the other hand, anew phrase translation table is learned on T onlyand then added as a new component in the log-linearmodel, we have an alternative to the full re-training26Algorithm 1 Transductive learning algorithm for statistical machine translation1: Input: training set L of parallel sentence pairs.
// Bilingual training data.2: Input: unlabeled set U of source text.
// Monolingual source language data.3: Input: number of iterations R, and size of n-best list N .4: T?1 := {}.
// Additional bilingual training data.5: i := 0.
// Iteration counter.6: repeat7: Training step: pi(i) := Estimate(L, Ti?1).8: Xi := {}.
// The set of generated translations for this iteration.9: for sentence s ?
U do10: Labeling step: Decode s using pi(i) to obtain N best sentence pairs with their scores11: Xi := Xi ?
{(tn, s, pi(i)(tn | s))Nn=1}12: end for13: Scoring step: Si := Score(Xi) // Assign a score to sentence pairs (t, s) from X .14: Selection step: Ti := Select(Xi, Si) // Choose a subset of good sentence pairs (t, s) from X .15: i := i+ 1.16: until i > Rof the model on labeled and unlabeled data whichcan be very expensive if L is very large (as on theChinese?English data set).
This additional phrasetable is small and specific to the development ortest set it is trained on.
It overlaps with the origi-nal phrase tables, but also contains many new phrasepairs (Ueffing, 2006).Mixture Model: Another alternative for Estimateis to create a mixture model of the phrase table prob-abilities with new phrase table probabilitiesp(s | t) = ?
?
Lp(s | t) + (1?
?)
?
Tp(s | t) (2)where Lp and Tp are phrase table probabilities esti-mated on L and T , respectively.
In cases where newphrase pairs are learned from T , they get added intothe merged phrase table.3.3 The Scoring FunctionIn Algorithm 1, the Score function assigns a score toeach translation hypothesis t. We used the followingscoring functions in our experiments:Length-normalized Score: Each translated sen-tence pair (t, s) is scored according to the modelprobability p(t | s) normalized by the length |t| of thetarget sentence:Score(t, s) = p(t | s) 1|t| (3)Confidence Estimation: The confidence estimationwhich we implemented follows the approaches sug-gested in (Blatz et al, 2003; Ueffing and Ney, 2007):The confidence score of a target sentence t is cal-culated as a log-linear combination of phrase pos-terior probabilities, Levenshtein-based word poste-rior probabilities, and a target language model score.The weights of the different scores are optimizedw.r.t.
classification error rate (CER).The phrase posterior probabilities are determinedby summing the sentence probabilities of all trans-lation hypotheses in the N -best list which containthis phrase pair.
The segmentation of the sentenceinto phrases is provided by the decoder.
This sumis then normalized by the total probability mass ofthe N -best list.
To obtain a score for the whole tar-get sentence, the posterior probabilities of all targetphrases are multiplied.
The word posterior proba-bilities are calculated on basis of the Levenshteinalignment between the hypothesis under consider-ation and all other translations contained in the N -best list.
For details, see (Ueffing and Ney, 2007).Again, the single values are multiplied to obtain ascore for the whole sentence.
For NIST, the lan-guage model score is determined using a 5-grammodel trained on the English Gigaword corpus, andon French?English, we use the trigram model whichwas provided for the NAACL 2006 shared task.3.4 The Selection FunctionThe Select function in Algorithm 1 is used to createthe additional training data Ti which will be used in27the next iteration i + 1 by Estimate to augment theoriginal bilingual training data.
We use the follow-ing selection functions:Importance Sampling: For each sentence s in theset of unlabeled sentences U , the Labeling step inAlgorithm 1 generates an N -best list of translations,and the subsequent Scoring step assigns a score foreach translation t in this list.
The set of generatedtranslations for all sentences in U is the event spaceand the scores are used to put a probability distri-bution over this space, simply by renormalizing thescores described in Section 3.3.
We use importancesampling to select K translations from this distri-bution.
Sampling is done with replacement whichmeans that the same translation may be chosen sev-eral times.
These K sampled translations and theirassociated source sentences make up the additionaltraining data Ti.Selection using a Threshold: This method com-pares the score of each single-best translation to athreshold.
The translation is considered reliable andadded to the set Ti if its score exceeds the thresh-old.
Else it is discarded and not used in the addi-tional training data.
The threshold is optimized onthe development beforehand.
Since the scores of thetranslations change in each iteration, the size of Tialso changes.Keep All: This method does not perform any fil-tering at all.
It is simply assumed that all transla-tions in the set Xi are reliable, and none of them arediscarded.
Thus, in each iteration, the result of theselection step will be Ti = Xi.
This method wasimplemented mainly for comparison with other se-lection methods.3.5 Filtering the Training DataIn general, having more training data improves thequality of the trained models.
However, when itcomes to the translation of a particular test set, thequestion is whether all of the available training dataare relevant to the translation task or not.
Moreover,working with large amounts of training data requiresmore computational power.
So if we can identify asubset of training data which are relevant to the cur-rent task and use only this to re-train the models, wecan reduce computational complexity significantly.We propose to Filter the training data, eitherbilingual or monolingual text, to identify the partscorpus use sentencesEuroParl phrase table+LM 688Ktrain100k phrase table 100Ktrain150k phrase table 150Kdev06 dev1 2,000test06 test 3,064Table 1: French?English corporacorpus use sentencesnon-UN phrase table+LM 3.2MUN phrase table+LM 5.0MEnglish Gigaword LM 11.7Mmulti-p3 dev1 935multi-p4 dev2 919eval-04 test 1,788eval-06 test 3,940Table 2: NIST Chinese?English corporawhich are relevant w.r.t.
the test set.
This filteringis based on n-gram coverage.
For a source sentences in the training data, its n-gram coverage over thesentences in the test set is computed.
The averageover several n-gram lengths is used as a measureof relevance of this training sentence w.r.t.
the testcorpus.
Based on this, we select the top K sourcesentences or sentence pairs.4 Experimental Results4.1 SettingWe ran experiments on two different corpora: oneis the French?English translation task from the Eu-roParl corpus, and the other one is Chinese?Englishtranslation as performed in the NIST MT evaluation(www.nist.gov/speech/tests/mt).For the French?English translation task, we usedthe EuroParl corpus as distributed for the shared taskin the NAACL 2006 workshop on statistical ma-chine translation.
The corpus statistics are shownin Table 1.
Furthermore we filtered the EuroParlcorpus, as explained in Section 3.5, to create twosmaller bilingual corpora (train100k and train150kin Table 1).
The development set is used to optimizethe model weights in the decoder, and the evaluationis done on the test set provided for the NAACL 2006shared task.For the Chinese?English translation task, we usedthe corpora distributed for the large-data track in the28setting EuroParl NISTfull re-training w/ filtering ?
?
?full re-training ??
?mixture model ?
?new phrase table ff:keep all ??
?imp.
sampling norm.
??
?conf.
??
?threshold norm.
??
?conf.
??
?Table 3: Feasibility of settings for Algorithm 12006 NIST evaluation (see Table 2).
We used theLDC segmenter for Chinese.
The multiple transla-tion corpora multi-p3 and multi-p4 were used as de-velopment corpora.
Evaluation was performed onthe 2004 and 2006 test sets.
Note that the train-ing data consists mainly of written text, whereas thetest sets comprise three and four different genres:editorials, newswire and political speeches in the2004 test set, and broadcast conversations, broad-cast news, newsgroups and newswire in the 2006test set.
Most of these domains have characteristicswhich are different from those of the training data,e.g., broadcast conversations have characteristics ofspontaneous speech, and the newsgroup data is com-paratively unstructured.Given the particular data sets described above, Ta-ble 3 shows the various options for the Estimate,Score and Select functions (see Section 3).
The ta-ble provides a quick guide to the experiments wepresent in this paper vs. those we did not attempt dueto computational infeasibility.
We ran experimentscorresponding to all entries marked with ?
(see Sec-tion 4.2).
For those marked ??
the experiments pro-duced only minimal improvement over the baselineand so we do not discuss them in this paper.
The en-tries marked as ?
were not attempted because theyare not feasible (e.g.
full re-training on the NISTdata).
However, these were run on the smaller Eu-roParl corpus.Evaluation MetricsWe evaluated the generated translations usingthree different evaluation metrics: BLEU score (Pa-pineni et al, 2002), mWER (multi-reference worderror rate), and mPER (multi-reference position-independent word error rate) (Nie?en et al, 2000).Note that BLEU score measures quality, whereasmWER and mPER measure translation errors.
Wewill present 95%-confidence intervals for the base-line system which are calculated using bootstrap re-sampling.
The metrics are calculated w.r.t.
one andfour English references: the EuroParl data comeswith one reference, the NIST 2004 evaluation setand the NIST section of the 2006 evaluation setare provided with four references each, whereas theGALE section of the 2006 evaluation set comeswith one reference only.
This results in much lowerBLEU scores and higher error rates for the transla-tions of the GALE set (see Section 4.2).
Note thatthese values do not indicate lower translation qual-ity, but are simply a result of using only one refer-ence.4.2 ResultsEuroParlWe ran our initial experiments on EuroParl to ex-plore the behavior of the transductive learning algo-rithm.
In all experiments reported in this subsec-tion, the test set was used as unlabeled data.
Theselection and scoring was carried out using impor-tance sampling with normalized scores.
In one setof experiments, we used the 100K and 150K train-ing sentences filtered according to n-gram coverageover the test set.
We fully re-trained the phrase ta-bles on these data and 8,000 test sentence pairs sam-pled from 20-best lists in each iteration.
The resultson the test set can be seen in Figure 1.
The BLEUscore increases, although with slight variation, overthe iterations.
In total, it increases from 24.1 to 24.4for the 100K filtered corpus, and from 24.5 to 24.8for 150K, respectively.
Moreover, we see that theBLEU score of the system using 100K training sen-tence pairs and transductive learning is the same asthat of the one trained on 150K sentence pairs.
Sothe information extracted from untranslated test sen-tences is equivalent to having an additional 50K sen-tence pairs.In a second set of experiments, we used the wholeEuroParl corpus and the sampled sentences for fullyre-training the phrase tables in each iteration.
Weran the algorithm for three iterations and the BLEUscore increased from 25.3 to 25.6.
Even though this290 2 4 6 8 10 12 14 16 1824.0524.124.1524.224.2524.324.3524.424.45IterationBleuscore0 2 4 6 8 10 12 14 1624.4524.524.5524.624.6524.724.7524.824.85IterationBleuscoreFigure 1: Translation quality for importance sampling with full re-training on train100k (left) and train150k(right).
EuroParl French?English task.is a small increase, it shows that the unlabeled datacontains some information which can be explored intransductive learning.In a third experiment, we applied the mixturemodel idea as explained in Section 3.2.
The initiallylearned phrase table was merged with the learnedphrase table in each iteration with a weight of ?
=0.1.
This value for ?
was found based on cross val-idation on a development set.
We ran the algorithmfor 20 iterations and BLEU score increased from25.3 to 25.7.
Since this is very similar to the re-sult obtained with the previous method, but with anadditional parameter ?
to optimize, we did not usemixture models on NIST.Note that the single improvements achieved hereare slightly below the 95%-significance level.
How-ever, we observe them consistently in all settings.NISTTable 4 presents translation results on NIST withdifferent versions of the scoring and selection meth-ods introduced in Section 3.
In these experiments,the unlabeled data U for Algorithm 1 is the develop-ment or test corpus.
For this corpus U , 5,000-bestlists were generated using the baseline SMT system.Since re-training the full phrase tables is not feasi-ble here, a (small) additional phrase table, specific toU , was trained and plugged into the SMT system asan additional model.
The decoder weights thus hadto be optimized again to determine the appropriateweight for this new phrase table.
This was done onthe dev1 corpus, using the phrase table specific todev1.
Every time a new corpus is to be translated,an adapted phrase table is created using transductivelearning and used with the weight which has beenlearned on dev1.
In the first experiment presentedin Table 4, all of the generated 1-best translationswere kept and used for training the adapted phrasetables.
This method yields slightly higher transla-tion quality than the baseline system.
The secondapproach we studied is the use of importance sam-pling (IS) over 20-best lists, based either on length-normalized sentence scores (norm.)
or confidencescores (conf.).
As the results in Table 4 show, bothvariants outperform the first method, with a consis-tent improvement over the baseline across all testcorpora and evaluation metrics.
The third methoduses a threshold-based selection method.
Combinedwith confidence estimation as scoring method, thisyields the best results.
All improvements over thebaseline are significant at the 95%-level.Table 5 shows the translation quality achieved onthe NIST test sets when additional source languagedata from the Chinese Gigaword corpus compris-ing newswire text is used for transductive learning.These Chinese sentences were sorted according totheir n-gram overlap (see Section 3.5) with the de-velopment corpus, and the top 5,000 Chinese sen-tences were used.
The selection and scoring in Al-gorithm 1 were performed using confidence estima-tion with a threshold.
Again, a new phrase table wastrained on these data.
As can be seen in Table 5, this30select score BLEU[%] mWER[%] mPER[%]eval-04 (4 refs.
)baseline 31.8?0.7 66.8?0.7 41.5?0.5keep all 33.1 66.0 41.3IS norm.
33.5 65.8 40.9conf.
33.2 65.6 40.4thr norm.
33.5 65.9 40.8conf.
33.5 65.3 40.8eval-06 GALE (1 ref.
)baseline 12.7?0.5 75.8?0.6 54.6?0.6keep all 12.9 75.7 55.0IS norm.
13.2 74.7 54.1conf.
12.9 74.4 53.5thr norm.
12.7 75.2 54.2conf.
13.6 73.4 53.2eval-06 NIST (4 refs.
)baseline 27.9?0.7 67.2?0.6 44.0?0.5keep all 28.1 66.5 44.2IS norm.
28.7 66.1 43.6conf.
28.4 65.8 43.2thr norm.
28.3 66.1 43.5conf.
29.3 65.6 43.2Table 4: Translation quality using an additionaladapted phrase table trained on the dev/test sets.Different selection and scoring methods.
NISTChinese?English, best results printed in boldface.system outperforms the baseline system on all testcorpora.
The error rates are significantly reduced inall three settings, and BLEU score increases in allcases.
A comparison with Table 4 shows that trans-ductive learning on the development set and test cor-pora, adapting the system to their domain and style,is more effective in improving the SMT system thanthe use of additional source language data.In all experiments on NIST, Algorithm 1 was runfor one iteration.
We also investigated the use of aniterative procedure here, but this did not yield anyimprovement in translation quality.5 Previous WorkSemi-supervised learning has been previously ap-plied to improve word alignments.
In (Callison-Burch et al, 2004), a generative model for wordalignment is trained using unsupervised learning onparallel text.
In addition, another model is trained ona small amount of hand-annotated word alignmentdata.
A mixture model provides a probability forsystem BLEU[%] mWER[%] mPER[%]eval-04 (4 refs.
)baseline 31.8?0.7 66.8?0.7 41.5?0.5add Chin.
data 32.8 65.7 40.9eval-06 GALE (1 ref.
)baseline 12.7?0.5 75.8?0.6 54.6?0.6add Chin.
data 13.1 73.9 53.5eval-06 NIST (4 refs.
)baseline 27.9?0.7 67.2?0.6 44.0?0.5add Chin.
data 28.1 65.8 43.2Table 5: Translation quality using an additionalphrase table trained on monolingual Chinese newsdata.
Selection step using threshold on confidencescores.
NIST Chinese?English.word alignment.
Experiments showed that putting alarge weight on the model trained on labeled dataperforms best.
Along similar lines, (Fraser andMarcu, 2006) combine a generative model of wordalignment with a log-linear discriminative modeltrained on a small set of hand aligned sentences.
Theword alignments are used to train a standard phrase-based SMT system, resulting in increased translationquality .In (Callison-Burch, 2002) co-training is appliedto MT.
This approach requires several source lan-guages which are sentence-aligned with each otherand all translate into the same target language.
Onelanguage pair creates data for another language pairand can be naturally used in a (Blum and Mitchell,1998)-style co-training algorithm.
Experiments onthe EuroParl corpus show a decrease in WER.
How-ever, the selection algorithm applied there is actuallysupervised because it takes the reference translationinto account.
Moreover, when the algorithm is runlong enough, large amounts of co-trained data in-jected too much noise and performance degraded.Self-training for SMT was proposed in (Ueffing,2006).
An existing SMT system is used to translatethe development or test corpus.
Among the gener-ated machine translations, the reliable ones are au-tomatically identified using thresholding on confi-dence scores.
The work which we presented herediffers from (Ueffing, 2006) as follows:?
We investigated different ways of scoring andselecting the reliable translations and comparedour method to this work.
In addition to the con-31fidence estimation used there, we applied im-portance sampling and combined it with confi-dence estimation for transductive learning.?
We studied additional ways of exploring thenewly created bilingual data, namely re-training the full phrase translation model or cre-ating a mixture model.?
We proposed an iterative procedure whichtranslates the monolingual source languagedata anew in each iteration and then re-trainsthe phrase translation model.?
We showed how additional monolingualsource-language data can be used in transduc-tive learning to improve the SMT system.6 DiscussionIt is not intuitively clear why the SMT system canlearn something from its own output and is improvedthrough semi-supervised learning.
There are twomain reasons for this improvement: Firstly, the se-lection step provides important feedback for the sys-tem.
The confidence estimation, for example, dis-cards translations with low language model scores orposterior probabilities.
The selection step discardsbad machine translations and reinforces phrases ofhigh quality.
As a result, the probabilities of low-quality phrase pairs, such as noise in the table oroverly confident singletons, degrade.
Our experi-ments comparing the various settings for transduc-tive learning shows that selection clearly outper-forms the method which keeps all generated transla-tions as additional training data.
The selection meth-ods investigated here have been shown to be well-suited to boost the performance of semi-supervisedlearning for SMT.Secondly, our algorithm constitutes a way ofadapting the SMT system to a new domain or stylewithout requiring bilingual training or developmentdata.
Those phrases in the existing phrase tableswhich are relevant for translating the new data arereinforced.
The probability distribution over thephrase pairs thus gets more focused on the (reliable)parts which are relevant for the test data.
For an anal-ysis of the self-trained phrase tables, examples oftranslated sentences, and the phrases used in trans-lation, see (Ueffing, 2006).ReferencesS.
Abney.
2004.
Understanding the Yarowsky Algo-rithm.
Comput.
Ling., 30(3).J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2003.
Confidence estimation for machine transla-tion.
Final report, JHU/CLSP Summer Workshop.www.clsp.jhu.edu/ws2003/groups/estimate/.A.
Blum and T. Mitchell.
1998.
Combining Labeled andUnlabeled Data with Co-Training.
In Proc.
Computa-tional Learning Theory.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The Mathematics of StatisticalMachine Translation: Parameter Estimation.
Compu-tational Linguistics, 19(2).C.
Callison-Burch, D. Talbot, and M. Osborne.2004.
Statistical machine translation with word- andsentence-aligned parallel corpora.
In Proc.
ACL.C.
Callison-Burch.
2002.
Co-training for statistical ma-chine translation.
Master?s thesis, School of Informat-ics, University of Edinburgh.A.
Fraser and D. Marcu.
2006.
Semi-supervised trainingfor statistical word alignment.
In Proc.
ACL.S.
Nie?en, F. J. Och, G. Leusch, and H. Ney.
2000.
Anevaluation tool for machine translation: Fast evalua-tion for MT research.
In Proc.
LREC.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
ACL.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
ICSLP.N.
Ueffing and H. Ney.
2007.
Word-level confidence es-timation for machine translation.
Computational Lin-guistics, 33(1):9?40.N.
Ueffing, M. Simard, S. Larkin, and J. H. Johnson.2007.
NRC?s Portage system for WMT 2007.
InProc.
ACL Workshop on SMT.N.
Ueffing.
2006.
Using monolingual source-languagedata to improve MT performance.
In Proc.
IWSLT.D.
Yarowsky.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProc.
ACL.R.
Zens and H. Ney.
2006.
N-gram posteriorprobabilities for statistical machine translation.
InProc.
HLT/NAACL Workshop on SMT.32
