Structure Learning in Weighted LanguagesAndra?s Kornai, Attila Zse?der, Ga?bor RecskiHAS Computer and Automation Research InstituteH-1111 Kende u 13-17, Budapest{kornai,zseder,recski}@sztaki.mta.huAbstractWe present Minimum Description Lengthtechniques for learning the structure ofweighted languages.
MDL is alreadywidely used both for segmentation andclassification tasks, and here we show itcan be used to formalize further importanttools in the descriptive linguists?
toolbox,including the distinction between acciden-tal and systematic gaps in the data, the de-tection of ambiguity, the selective discard-ing of data, and the merging of categories.IntroductionThe Minimum Description Length (MDL, see Ris-sanen 1978) framework is primarily about datacompression: if we are given some data D, ourgoal is to find a modelM, and a correction termE , such that the model output and the correctionterm together describe the data, and transmittingM and E takes fewer bits than transmitting anycompetingM?
and E ?.From the very beginning, starting with Pa?n.
ini,linguists have put a premium on brevity.
The hopeis that the shortest theory is the best theory (seeVitanyi and Li 2000), at least if we are willingto posit a theory of Universal Grammar (UG) thatwill let us specifyM briefly, since we can assumeUG to be amortized over many languages.In this paper we study the problem of com-pressing weighted languages by presenting themvia weighted finite state automata (WFSA).
Thetheoretical approach we discuss here has a longhistory: the founding paper of Kolmogorov com-plexity, Solomonoff (1964), already studied theproblem of inferring a grammar from data, andGru?nwald (1996) uses MDL to infer CFGs fromcorpora, there conceived of as long strings over afinite alphabet.
It is fair to say that this theory hasnot had much impact on computational practice,where grammatical inference is dominated by thestandard n-gram based language modeling meth-ods, see Jelinek (1997) for an excellent summaryof the basic ideas and techniques, most of whichare still in wide use.While the two approaches may coincide in cer-tain cases (see Gru?nwald 1996), and in theory n-gram models are just a special case of the generalWFSA, in practice they are divided by a funda-mental difference in modeling unseen data.
Froman engineering standpoint, Church et al2007) areentirely right in saying:No matter how much data we have, wenever have enough.
Nothing has zeroprobability.Linguists, starting perhaps with Chomsky (1965),draw a bright line between accidentally and sys-tematically missing data, and would prefer to re-strict backoff techniques to the accidental gaps.The distinction is often lost in applied work, be-cause the models need to be built in a noisy envi-ronment, where frequent typos like *teh and simi-lar performance errors can easily overwhelm gen-uine items like boisterous or mopeds by an orderof magnitude or even more.
In the eyes of manylinguists, this observation alone is sufficient to robprobabilistic models of grammatical content, sincethis makes it impossible to define a single thresh-old g such that all and only strings with weightgreater than g are grammatical.Aside from this subtle but important distinc-tion between accidental and systematic gaps, bothkinds of language modeling can be cast in thesame formal terms: we fit a model M that min-imizes some function E (typically, the squaredsum) of the error E .
Obviously, the more pa-rameters M has, the better fit we can obtain.Much of contemporary computational linguisticsfollows the route of training simple models suchas Hidden Markov Models (HMMs) and proba-bilistic context-free grammars (PCFGs) with verymany parameters, and stops adding more onlywhen compressing the memory footprint is ofparamount importance.
As (Church et al 2007)notes, applications like the contextual speller ofMicrosoft Office simply could not ship withoutkeeping the language model within reasonablesize limits.
In such cases, we are quite willing totrade in E for gains in the size M ofM, and con-siderations of optimizing the sum of the two aresimply irrelevant.In contrast, our strategy is to search for modelwhich measures both M and E in bits, and opti-mizes the sum M + E, not because we put sucha premium on data compression, but rather be-cause we follow in Pa?n.
ini?s footsteps.
Our goal isfinding structural models capable of distinguish-ing structurally excluded (ungrammatical) stringslike furiously sleep ideas green colorless from lowprobability but grammatical strings like colorlessgreen ideas sleep furiously (Pereira, 2000).
Forthis more ambitious goal comparing models withdifferent number of parameters is a key issue, andthis is precisely where MDL is helpful.The rest of this Introduction provides the basicdefinitions, notation, and terminology, all fairlystandard except for the use of Moore rather thanMealy machines ?
the significance of this choicewill be discussed in Section 2.
In Section 1we bring a fundamental idea of signal process-ing, quantization error, to bear on the problemof model selection, illustrating the issue on a realexample, the proquant system of Hungarian.
InSection 2 we show how one of the most power-ful tools at disposal of the linguist, ambiguity, canbe detected by MDL, bringing another standardidea, signal to noise ratio to bear.
In Section 3we discuss another real example, Hungarian mor-photactics, and show that two methods widely (butshamefacedly) used in practice, discarding dataand merging descriptive categories, can be usedon a principled basis within MDL.
Our goal isto show that by consistent application of MDLprinciples we can automatically set up the kindof models that linguists would set up.
Ultimately,both man and machine work toward the same goal,optimization of grammar elegance or, what is thesame, brevity.Definition 1.
Given some finite alphabet ?, aweighted language p over this alphabet is definedas a mapping p : ??
?
R taking non-negative val-ues such that?????
p(?)
= 1.
This is less gen-eral than the standard notion of noncommutativepower series with weights taken in arbitrary semir-ings (Eilenberg 1974, Salomaa 1978) but will suf-fice here.
The stringset {?|p(?)
> 0} is called thesupport of p and will be denoted by S(p).Definition 2.
Given two weighted languagesp and q, we say the Kullback-Leibler (KL)approximation error Q of q relative to p is??
?S(q) p(?)
log(p(?)/q(?)).
The entropy of pis defined as ???
?S(p) p(?)
log(p(?
)).Definition 3.
A WFSAM is defined by a squaretransition matrix M whose element mij give theprobability of transition from state i to state j, anemission list h that gives a string hi ?
??
for eachi 6= 0, and an acceptance vector ~awhose i-th com-ponent is 1 if i is an accepting state and 0 other-wise.
There is a unique initial state which startsthe state numbering at 0, and we permit states withempty outputs.
Rows of M must sum to 1.
Thuswe have defined WFSA as normalized probability-weighted nondeterministic Moore machines.Definition 4.
The weight a WFSA assigns to ageneration path is the product of the weights onthe edges traversed, and the weight it assigns to astring ?
is the sum of the weights assigned to allpaths that generate ?.1 Quantization errorThe notions of quantization error and quantiza-tion noise, while well known in the signal pro-cessing literature (for a monographic treatment,see Widrow and Kolla?r 2008), and widely usedin speech processing (Makhoul et al 1985), havehad little impact on language processing.
YetMDL description of even the simplest weightedlanguage brings up a significant problem that can-not be addressed without approximation.Let p be a non-computable real number between0 and 1, and let us define the language A as con-taining only two strings, a and b, with probabilityp and 1 ?
p respectively.
Since p is incompress-ible, the only way Alice can send A to Bob is bysending all bits of p. By Alice sending only thefirst n bits, Bob obtains a language An that ap-proximatesA with error of 2?n.
Since sending thestrings {a, b} has only a small constant cost, theoverall MDL cost is dominated by the error termE, which is just as incompressible as the originalp was.As long as the weights themselves are treatedas information objects of arbitrary capacity, thereis no way out of this conundrum (de Leeuw 1956).On the other hand, the weighted languages we en-counter in practice are generally abstracted fromgigaword or smaller corpora, and as such their in-herent precision is less than 32 bits.
For weightedlanguages with finite support (corpora and lan-guage models without smoothing) p is simply alist containing strings and probabilities.
The costof transmitting this list comes from two sources:the cost of transmitting the probabilities, and thecost of transmitting the strings.
As a first approx-imation, let us assume the two are independent, amatter we shall return to in Section 2.We begin by investigating the inherentcost/error tradeoff of transmitting a discreteprobability distribution {pj |1 ?
j ?
k} byuniform quantization to b bits.
We divide the unitinterval in n = 2b equal parts.
For our theoremswe will use a value b large enough so that wehave pj ?
2?
(b?2) for all j, leaving at least thefirst 4 bins empty.
Usually 32 bits suffice forthis, and as we shall see shortly, often a lot fewerare truly needed, though standard modeling toolslike SRILM often use 64-bit quantities.
For eachprobability, Alice sends b bits (the bin number).Bob, who knows b, reconstructs a value based onthe center of the bin.Since this process does not guarantee that thereconstructed values sum to 1, Bob takes the ad-ditional step of renormalizing these values: if?qi = r, he will use q = qi/r instead of theqi that were transmitted by Alice.
When b is large,the pi will be distributed uniformly mod 2?b.
Inthis case, the expected values E(pi ?
qi) are zerofor all i, so E(?qi) =?E(qi) =?E(pi) =E(?pi) = 1 or, in other words, E(r) = 1.
SinceVar(r ?
1) =?i Var(pi ?
qi) = k/12n2 is onthe order 1/n2, in the following estimate we cansafely ignore the effects of renormalization.
ByDefinition 2, the KL approximation error isQ =n?1?i=0?i/n?pj?(i+1)/npj?
(pij) (1)where ?
(pij) = log(2npj/(2i + 1)) is the dif-ference between the logarithms of the actual pjand the centerpoint of the interval [i/n, (i+ 1)/n)where pj falls.
In absolute value, this is maximalwhen pj is at the lower end of this interval, where?
(pij) is log(2i+12i ).
Using the standard estimatelog(1 + x) ?
x this will be less than 12i ?
1/8since i ?
4 .
Since the ?
(pij) are now estimateduniformly, and the pj sum to 1, we obtainTheorem 1.
The approximation error Qn of uni-form quantization into n bins [i/n, (i+1)/n) suchthat the first 4 bins are empty satisfiesQn ?18 log 2?
0.18 (2)bits independent of n (the computation was in basee rather than base 2, hence the factor log 2).
Withgrowing n the number of bins that remain emptywill grow, and the estimate 12i of ?
can be im-proved accordingly.Theorem 1 of course gives just an upper bound,and a rather crude one, the expected value of Qnis considerably less.
Instead of using the maxvalue ?
(pij) we can consider the expected abso-lute value, which is log(1 + 12i)/n, so equation (2)could be reformulated asE(Qn) ?18n log 2(3)It is evident from the foregoing that the crux ofthe matter are the small pi values, and at any rate,there can only be a handful of relatively large val-ues, since the sum is 1.
Experience shows thatprobabilities obtained from corpora span many or-ders of magnitude, which justifies the use of a logscale.
Instead of the simple uniform quantizationof Theorem 1, we will use a two-parameter quan-tization scheme, whereby first log pj are cut off at?C, and the rest, which are on the (?C, 0) inter-val, are sorted in n = 2b bins ?b-bit quantization?.In effect, all probabilities below e?C are as-sumed to be below measurement precision, andthe log of the rest are uniformly quantized.
Weexperimented with two simple techniques: repre-senting the class (?
?,?C) with a very low fixedvalue (10?50) or with one set to e?2C based onthe parameter C of the encoding.
As there wasno appreciable difference (which is not surprisinggiven limx?0 x log x = 0), from here on we sim-ply speak of zero weights for weights below e?C .We emphasize that ?being below measurementprecision?
is not the same as ?being zero?
in theabove sense.
First, in any corpus of size N thesmallest number we can measure is 1/N , yet weknow that further strings that were not in oursample are not necessarily probability zero.
Itis therefore common to reserve a small fraction,?
a aka?r ba?r egyvala ma?s ma?svala minden se valaha?ny 72383 9502 2432 55 21 4584hogy 7781539 213687 3173 1839 4570 123 4138 31873hol 117231 399052 1037 9845 16066 16009 20521 34081honnan 24777 18628 296 1205 2482 1321 627 4274honne?t 1598 1197 12 25 78 33 23 236hova?
17589 21073 486 1753 1 5073 1 1859 2249 3966hova 17360 10591 309 1166 1788 1381 2105 3036ki 1309618 1464744 3933 60923 884 814 308508 165230 221175meddig 11879 8171 189 225 74 252mely 761277 1586913 166 74262 3 4 40601melyik 68051 47564 1996 34477 2 939 48274mennyi 76429 25805 657 1415 517 96184mi 1626013 1303820 6500 52480 1337 161 275773 355690mie?rt 251120 20672 58 205 4 1810 13552mikor 173652 555325 679 33516 15892 11288 206 18235milyen 343643 38921 8217 68033 1618 1 55603 81155Table 1: Frequencies of proquants in the Hungarian Webcorpusgenerally 1-5% of the probability mass, to un-seen events, and use calculated numbers, insteadof measured values, to smooth the distribution.Unfortunately, the engineering philosophy behindthe various backoff schemes (which often utilizeMDL stopping criteria both in speech and lan-guage modeling, see e.g.
Shinoda and Watanabe2000, Seymore and Rosenfeld 1996) is diametri-cally opposed to the the method of inquiry pre-ferred by linguists, whose primary interest is withgeneralization, i.e.
with models that make falsifi-able predictions, rather than furnishing descriptivestatistics.
In particular, negative generalizations,that something is forbidden by some rule of gram-mar, are just as interesting from their standpoint aspositive generalizations.
But how do we express anegative generalization?Definition 5.
A string will be deemed ungrammat-ical or structurally excluded iff every generationpath includes at least one zero weight in the abovesense.If scores from different sources are multipliedtogether, the use of zero weights as markers ofungrammaticality is implicit in the semantics ofWFSA.1 Still, there are significant difficulties inimplementing the idea.
If we want to maintainthe commonsensical assumption that *teh is nota word (has zero unigram weight) and also ac-count for the data that makes it the 34,174th mostcommon string in English text, we will need to1We owe this observation to an anonymous MOL referee.model typos.
Once we learn that the log price ofthe /the/teh/ substitution is about -9.8, we can pre-dict not just the frequency of teh, but also thoseof weatehr, otehr, tehy, tehre, tehft, and so forth,without adding these to the lexicon.
Since such amodel is based on computed frequencies of lettersubstitution and exchange rather than on the typosdirectly, the engineer has to give up the enterpriseof building the entire language model in a singlesweep directly on the data.At the same time, the linguist has to give upthe attractive simplicity of ?zero weight iff un-grammatical?
: the misspelling model will assigna low but nonzero weight to everything, and ifthis model is compiled together with a unigrammodel that contains only grammatical words, thesimple world-view of Definition 5 will no longerwork.
Rather, we will have to say that it is zeroweight in the grammatical subautomaton (visibleonly prior to getting compiled together with the se-mantic, spelling, stylistic, and possibly other sub-automata) that defines grammaticality.
We haveto build an explicit noise model to make sense ofthe raw data, but this is not particularly surprisingfrom the perspective of other sciences like astron-omy where noise reduction is common practice.The specific contribution of the MDL approachis that zero weights in the model are a lotcheaper than using low probabilities would be: theparadigm encourages both sparseness and struc-tural decomposition.
But before we can establishthese points in Sections 2 and 3, we need to as-similate another piece of computational practice,the use of log probabilities.
When quantization isuniform on the log scale, the expected value of thebinning error is no longer zero, given our assump-tion of uniformity on the linear scale, but rather?C log in?
?C log i+1nex ?
e?Ci+0.5n dx ?
C3/8n3 (4)which yields an expected r ?
kC3/8n3, still neg-ligible compared to the bound given in Theorem 2,which is obtained by methods similar to those usedabove.Theorem 2.
For C, n sufficiently large for thefirst 4 bins to remain empty, the approximationerror LCn of log-uniform quantization with cutoff?C into n = 2b bins [?C(i + 1)/n,?Ci/n) isbounded byLCn ?C2n log 2(5)and the expected value E(LCn ) is bounded byC2/4n2 log 2.Figure 1: Error of log-scale uniform quantizationLet us see on an example how these errorbounds compare to values obtained numerically.Our first example will explore what we will call,for want of a better name, the proquant systemof Hungarian that covers both pro-forms (pro-nouns, proadjectives, proadverbials) and quan-tifiers.
Given the prefixes a-, minden-, vala-,egyvala-, ma?svala-, se-, aka?r-, ma?s-, ba?r- andzero, and suffixes -ki, -mi, -hol, -hogy, -hova,etc.
we can create forms such as valaki ?some-one?, valami ?something?, aka?rki ?anyone?, sehol?nowhere?
and so on.
Clearly, many of what wecall prefixes and suffixes could be analyzed fur-ther, e.g.
ma?svala as ma?s+vala, but we don?t wantto prejudge the issue by presenting a maximallydetailed analysis.In a corpus of over 40 million sentences (Hun-garian Webcorpus, Hala?csy et al2004) we ob-served the frequencies in Table 1.
Many of theseproquant forms take inflectional suffixes (case,number, etc.
), and the numbers presented here al-ready include these, so that the 814 occurrences ofma?svalaki include forms like ma?svalakivel ?withsomeone else?, ma?svalakinek ?to someone else?etc.
If we think of the (stemmed) Hungarian vo-cabulary as a weighted language h, the set ofprefixes (suffixes) as an unweighted language Pre(resp.
Suff), the data is a sample from S(h)?Pre?Suff with the weights renormalized.
Alto-gether, we have 121 nonzero values plus 39 ze-ros, the entropy of the distribution is H = 3.677.Figure 1 plots the log of the observed quantiza-tion noise as a function of the number of bits b andthe cutoff ?C.
Notice that once C is sufficientlylarge, no further gains are made by increasing itfurther.
As expected from Theorem 2, the log ofthe error is roughly linear in b = log2 n (the ob-served values are of course better than the bounds).Definition 6.
The inherent noise of a dataset Dis the KL approximation error between a randomsubsample and its complement.Ideally, we would want to compare another sam-ple D?
to D, but in many cases launching a com-parable data collection effort is simply not feasi-ble, and we must content ourselves with the sim-ple procedure suggested by this definition.
By ran-domly cutting the 40m sentence corpus on whichthe proquant dataset is based in 10m sentence partsand computing the KL divergence between anytwo, we obtain numbers in the 7-8?10?5 range,which means it makes little sense to approximateD with better precision than 10?5.
How to handlethe singular cases when some qj becomes 0 (ashappens with half of the hapaxes when we cut thesample in two) is an issue we defer to Section 3.Since the smallest pj in this data is about5?10?8, by taking C = 20 we guarantee that nolog probability is less than the cutoff point ?C.Trivial ?list?
automata consisting of an initial state,a final state, and a separate Mealy arc (or Moorestate) for each of the 121 nonzero observations al-ready generate a weighted language within the in-herent noise of the data at 10 bits, where the KLdivergence is at 8 ?10?6.
At 12 bits, the divergenceis below 1.4 ?
10?6, and at 16 bits, below 5 ?
10?9.As we shall see in the next Section, the MDL sizeof these models, between 2k and 7k bits, is domi-nated by factors unrelated to the precision b of theencoding.Figure 2: Model fit to observed probabilitiesFigure 2 shows the 80 largest observed proquantprobabilities (in black) in descending order, andthe probabilities of the same strings as computedfrom several models.
The 10 and 12-bit list au-tomata are not plotted, as the computed values aregraphically indistinguishable from the observedvalues, the rest will be discussed in the text.2 Detecting ambiguityBefore turning to the actual MDL learning pro-cess, let us summarize what we have for the Hun-garian proquant system so far.
We have a weightedlanguage of about 120 strings.
When transmit-ting a weighted automaton, Alice is sending notstrings and weights, but rather weight-labeled arcsand string-labeled states of a WFSA.
In Defini-tion 3 we used Moore machines, but in the liter-ature Mealy automata, where inputs/outputs andweights are both tied to arcs are more common(see e.g.
Mohri 2009).
The rationale for preferringMoore over Mealy in the MDL context is that nogains can be obtained from joint compression ofstrings and probabilities (even though Mealy ma-chines couple the two), while sharing of stringshas very significant impact on MDL length, as weshall see shortly.
For the simple ?list?
automatathis means adding extra states in the middle of aMealy arc, and we need to take some precautionsto guarantee that the representation is just as com-pact as it would be for a Mealy machine.Let us now see in some detail how compactthese encodings can get.
With s states, and b bitsfor probability, an arc requires 2 log2 s + b bits.However, Bob can reasonably assume that Aliceis only sending trimmed machines, with states thatcannot be reached from the initial state or with nopath to an accepting state already removed.
There-fore, if Bob sees a state with no outbound pathhe supplies an outgoing arc, with probability 1, tothe final state ?
such arcs need not be sent by Al-ice to begin with.
Similarly, Bob can assume thatall states except for the last one are non-accepting,and Alice will transmit information only to over-ride this default when needed.As for emissions, in a Moore machine each stateemits a string (but no guarantees that differentstates emit different strings), so Alice needs to en-code the strings somehow.
If we assume that thereis a character table shared between Alice and Bob,e.g.
the character frequencies of Hungarian, withentropy H , encoding a string ?
costs simply |?|Hbits.
(We could take this also to be a case of trans-mitting a weighted language, but we assume thatthe cost of transmitting this language can be amor-tized over many WFSA that deal with Hungarian.
)b l M cs ca KL Hq1 121 5210 4306 904 2.1883 6.8332 121 5386 4306 1080 1.1207 3.4873 121 5507 4306 1201 0.268 2.8894 121 5628 4306 1322 0.041436 4.0445 121 5749 4306 1443 0.016117 3.4246 121 5870 4306 1564 0.002409 3.6677 121 5991 4306 1685 0.000676 3.6538 121 6112 4306 1806 0.000288 3.6479 121 6233 4306 1927 5.905e-5 3.68110 121 6354 4306 2048 8.003e-6 3.67811 121 6475 4306 2169 3.999e-6 3.67812 121 6596 4306 2290 1.387e-6 3.67816 121 7080 4306 2774 4.660e-9 3.676Table 2: List models with character-based stringencodingTable 2 summarizes the relevant values for the triv-ial models where each weight gets its own train-able parameter.
b is the number of bits, l is thenumber of trainable parameters (weights associ-ated to arcs), ca is the cost of transmitting the arcs.Note that this is less than l(b+2 log2 s), because ofthe redundancy assumptions shared by Alice andBob.
cs is the cost of transmitting the emissions,and the total model cost is M = ca + cs.
Wewould, ideally, also need to add to M a dozen bitsor so to encode the major parameters of the cod-ing scheme itself, such as the values b = 10 andC = 20, but these turn out to be negligible com-pared to the basic cost.
Also, these major param-eters are shared across the alternatives we com-pare, so whatever we do to minimize M will notbe affected by uniformly adding (or uniformly ig-noring) this constant cost.
KL gives the KL di-vergence between the model and the training data.This measures the expected extra message lengthper arc weight, so that the error residual E is ktimes this value, where k is the number of valuesbeing modeled.
We emphasize that k = l onlyin the listing format, where all values are treatedas independent ?
in the ?hub?
model we shall dis-cuss shortly l is only 26 (10 prefix and 16 suffixweights) but k is still 121.The main components of the total MDL cost,M , l ?KL, l(KL+Hq), and the totalM+l(KL+Hq) are plotted on Figure 3.Figure 3: MDL cost componentsAll models with b ?
10 are within the inter-nal noise of the data, and it takes over 6kb to de-scribe such a model.
However, the bulk of thesebits come from encoding the output strings char-acter by character ?
if we assume that Alice andBob share a morpheme table, the results improvea great deal, by over 3,600 bits.
If the system rec-ognizes what we already anticipated in Table 1,that each string can be expressed as the concate-nation of a prefix and suffix, encoding the stringsbecomes drastically cheaper.
Using MDL for seg-mentation is a well-explored area (see in particularGoldsmith 2001, Creutz and Lagus 2002, 2005),and we are satisfied by pointing out that usingthe morphemes in the first row and column ofTable 1 we drastically reduce cs, to about 708bits, below the cost ca of encoding the probabil-ities.
The 3-bit list model providing the MDL op-timum (dark blue in Figure 2) requires 1,900 bitswith this string encoding, and is noticeably betterthan the SRILM bigram/trigram (turquoise) whichtakes around 12kb.By encoding the emissions in a more cleverfashion, we have not changed the structure of themodel: the same states are still linked by thesame arcs carrying the same probabilities, it is justthe state labels that are now encoded differently.When expressed as a Mealy automaton, a listingof probabilities corresponds to a two-state WFSAwith as many arcs as we have list elements (in ourcase, 121), while the arrangement of Table 1 issuggestive of a different model, one with 10 prefixarcs from the initial state to a central ?hub?
state,and 16 suffix arcs from this hub to the final state.We have trained such ?hub?
models using KL,Euclidean (L2), and absolute value (L1) minimiza-tion techniques.
Of these, direct minimization ofKL divergence works best, obtaining 0.325 bits atb = 10, and 0.298 at b = 12 (red and green in Fig-ure 2).
While the difference, about 0.027 bits, isstill perceptible compared to the noise level, witha signal to noise ratio (SNR) of 8 dB, it simplydoes not amortize over the 26 model probabilitieswe need to encode.
Adding 2 bits for encoding onevalue requires a total of adding 52 bits to our spec-ification ofM, while the gain of the error residualE, computed over the 121 observed values, is just2.074 bits.
In short, there is not much to be gainedby going from 10 to 12 bits, and we need to lookelsewhere for further compression.Definition 7.
For a weighted language p a modeltransform X is learnable in principle (LIP) if (i)both M and X(M) are part of the hypothesisspace and (ii) the total MDL cost of describing pby X(M) is significantly below that of describingp byM.In a critical sense, LIP is weaker than MDL learn-ability, since the space itself can be very large, andtesting all hypothetical transforms X that fit thebill may not be feasible.
The difference betweenLIP and practical MDL learnability is precisely thedifference between existence proofs and construc-tive proofs.
Our interest here is with the former:our goal is to demonstrate that structurally soundmodels are LIP.
So far, we have seen that struc-turally valid segmentations can be effectively ob-tained by MDL.
Our next task is to show that am-biguity is LIP.As linguists, we know that the weakest point ofthe hub model is that hogy, accounting for almost40% of the data, is not just a proquant ?how?
butalso a subordinating conjunction ?that?.
To encodethis ambiguity, we add another arc emitting hogydirectly.
Table 3 compares list models (lines 1-3, emissions encoded over morphemes rather thancharacters), simple hub models (lines 4-6), andhub models with this extra arc (lines 7-9).b l M cs ca KLe5 M+E3 121 1907 705 1202 26800 228910 121 2754 705 2049 0.8 319912 121 2999 705 2290 0.14 34413 26 473 81 392 42343 130510 26 662 81 581 32593 120112 26 716 81 635 29827 12493 27 480 81 400 23094 105210 27 676 81 596 11268 116112 27 733 81 652 10022 1198Table 3: Hub models with/out ambiguous hogyAs can be seen from the table, the best modelagain takes only 3 bits, but must include the ex-tra parameter for handling the ambiguity of hogy.To learn this, at least in principle, without relyingon the human knowledge that drove the heuristicsearch, consider the leading terms of the KL error.Arranging the pi log(pi/qi) in order of decreasingabsolute value we obtain mi 0.0192; minden+ki0.0175; a+mely 0.0169; mely -0.0147; a+mikor0.0135; hogy -0.0128; and so forth.
Of all the 121strings we may consider for direct emission, onlyhogy is worth adding a separate arc for.
Further, ifwe repeat the process, adding a second direct arcnever results in sufficient entropy gain comparedto adding hogy alone.To summarize, list models can approximate theoriginal data within its inherent noise level, butincur a very significant MDL cost, even if theyuse an efficient string encoding because they keepmany parameters, see the first three lines of Ta-ble 3 above.
The hub models, which build struc-ture similar to the one used in the string encoding,recognizing prefixes and suffixes for what they are,are far more compact, at 470-730 bits, even thoughthey have a KL error of about .1-.4 bits.
Finally,the hub+ambiguity model, with 27 parameters, re-duces the total MDL cost to 1052 bits, less thanhalf of the best list model.Currently we lack the kind of detailed under-standing of the description length surface overthe WFSA?stringencoding space that would letus say with absolute certainty that e.g.
the hubmodel with ambiguous hogy is the global mini-mum, and we cannot muster the requisite com-putational power to exhaustively search the spaceof all WFSA with 27 arcs or less.
Further gainscould quite possibly made with even cruder quan-tization, e.g.
to n = 6 levels (powers of 2 areconvenient, but not essential for the model), or bybringing in non-uniform quantization.On the one hand, we are virtually certain thatthe only encoding of emissions worth studyingis the morpheme-based one, since the economybrought by this is tremendous, 3,600 bits over theproquants alone, and no doubt further gains else-where, as we extend the scope to other words thatcontain the same morphemes ?
in this regard, ourfindings simply confirm what Goldsmith, Creutz,Lagus, and others have already demonstrated.
Onthe other hand, finding the right segmentation isonly the first step, we also need a good model ofthe tactics.
As we said at the beginning, the en-coding of arcs and probabilities can to a signifi-cant extent be independent of the encoding of theemissions.
Here the remarkable fact is that a bet-ter emission model could to a large extent drive thesearch for structuring the WFSA itself.Given a segmentation of a string ?
= ?1?2, thehypothesis space includes both a single arc fromsome r to some t where we emit ?, or the con-catenation of two arcs r ?
s and s ?
t with sand t emitting ?1 and ?2 respectively.
This bringsin a bit of ambiguity in regards to the distributionof the probabilities, for if ?
had weight p the newarcs could be assigned any values p1, p2 as long asp1p2 = p, at least if the sum of outgoing proba-bilities from s remains 1.
If s has no other arcsoutgoing than s ?
t this forces p1 = p, but ifwe collapse the intermediate states from severalbimorphemic words, there is room for joint opti-mization.
In our example, collapsing all interme-diate states in a single ?hub?
halves the MDL cost.3 DecompositionFor our next example we consider Hungarianstem-internal morphotactics.
The Analytic Dic-tionary of Hungarian (Kiss et al011) provides,for each stem like beleilleszt ?fit in?
an analysislike preverb+root+suffix wherein bele is one of aclosed set of Hungarian preverbal particles, ill isthe root, and eszt is a verb-forming suffix.
Thereare six analytic categories: Stem S; sUffix U ;Preverb P ; root E; Modified M ; and foreIgn I;so that each stem gets mapped on a string over?
= {S,U, P,E,M, I}.
We have two weightedlanguages: the tYpe-weighted language Y whereeach string is counted as many times as there areword types corresponding to it (so that e.g.
forSUU we have 3,739 stems from a?bra?ndozik ?day-dream?
to zuhanyozo?
?shower stall?, and the tOken-weighted language O where the same pattern hasweight 18,739,068 because these words togetherappeared that many times in the Hungarian Web-corpus (Hala?csy et al 2004).Since the inherent noise of O is about 0.0474bits, we are interested in automata that approxi-mate it within this limit.
This is easily achievedwith HMM-like WFSA that have arcs between anytwo states, using b = 11 bits or more, the smallestrequiring only 781 bits.
For Y the inherent noise isless, 0.011 bits, and the complete graph architec-ture, which only has 49 parameters (6 states, plusarcs from an initial state and arcs to a final state) isnot capable of getting this close to the data, withthe best models, from b = 11 onwards, remain-ing at KL distance 0.3.
The two languages differquite markedly in other respects as well, as can beseen from the fact that the character entropy of Ois 0.933, that of Y is 1.567.
Type frequency is nota good predictor of token frequency: the KL ap-proximation error of O relative to Y is 2.11 bits.An important aspect of the MDL calculus is thetreatment of the singularities which arise when-ever some of the qi in Definition 2 are 0.
In thecase at hand, we find both types that are not at-tested in the corpus, and tokens whose type wasnot listed in the Analytic Dictionary, a situationthat would theoretically render it impossible tocompute the KL divergence in either direction.
Inpractice, tokens with no dictionary type are eithercollected in a single ?unknown?
type or are silentlydiscarded.
Both techniques have merit.
The catch-all ?unknown?
type can simply be assumed to fol-low the distribution of the known types, so a modelthat captures the data enshrined in the dictionaryshould, at least in principle, be also ideal for thedata not seen by the lexicographer.
Surprises mayof course lurk in the unseen data, but as long ascoverage is high, say P (unseen)?
0.05, surpriseswill really be restricted to this 5% of the unseen,or what is the same, will be at order P 2.
In generalwe may consider two distributions {pi} and {qi}as in Definition 2, and compute P =?qi=0pi,the proportion of q-singular data in p.Theorem 3.
The total cost L of transmitting anitem from the p-distribution is bound byL ?
(1?P )(KL(p, q)+Hq)+P (1+log2 n) (6)Proof We use, with probability (1 ?
P ), the q-based codebook: this will have cost Hq plus themodeling loss KL(p, q).
In the remaining cases(probability P ) we should use a codebook basedon the q-singular portion of p, but we resort to uni-form coding at cost log2 n, where n is the numberof singular cases.
We need to transmit some infor-mation as to which codebook is used: this requiresan extra H(P, 1 ?
P ) ?
P bits ?
collecting theseterms gives (6).
2Theorem 3 gives a principled basis for discard-ing data within the MDL framework: when P issmall, the second term of (6) can be absorbed inthe noise.
To give an example, consider the uni-gram frequencies listed in columns fO and fY ofTable 4.
Some letters are quite rare, in particu-lar, I makes up less than 0.06% of O and 0.013%of Y .
Columns KLO and KLY show the KL di-vergence of O and Y from models obtained by bydiscarding words containing the letter in question,columns PO and PY show the weight of the stringsthat are getting discarded.fO PO KLO fY PY KLYS .7967 .9638 4.7887 .5342 .9122 3.5092U .1638 .1464 0.2284 .3443 .5699 1.2174M .0083 .0103 0.0149 .0255 .0623 0.0928E .0114 .0141 0.0205 .0331 .0804 0.1209P .0198 .0248 0.0362 .0623 .1531 0.2397I .0001 .0001 0.0002 .0006 .0010 0.0006Table 4: Divergence caused by discarding dataIn the case of Y , only I can be discarded whilekeeping below the inherent noise of the data, butfor O we have three other symbols M, E, and P,that could be removed.
Further, removing both let-ters M,E only produces a KL loss of 0.036 bits;removing M, I a loss of 0.015 bits; E, I 0.021bits; P, I 0.036 bits; and even removing all threeof M,E, I only 0.036 bits.Figure 4: MS merge-split modelIn the case of I , again as linguists we under-stand quite well what discarding this data means:we are excluding foreign stems.
This is quite jus-tified, not because foreign words like paperback,pacemaker or baseball are in any way inferior, butbecause their internal analysis is not transparent tothe Hungarian reader (it is telling that the editorsof the Analytic Dictionary coded the stem bound-ary in paper+back but not in base+ball).Discarding M , a category that differs from Sonly in that the stem undergoes some automaticmorphophonological change such as vowel eli-sion, is also a sensible step in that the fundamen-tal morphotactics are not at all affected by thesechanges, but how is this learnable, even in princi-ple?
Here we introduce another model transformcalled XY merge-split composed of two steps: firstwe replace all letters (or strings) X by Y and traina model, and next we split up the emission statesof Y in the merged model to X and Y -emissionsaccording to the relative proportions of X and Yin the original data.For LIP, the key observation is that models con-structed by XY merge-split have a transmissioncost composed of two parts, the length of thesmaller merged model (given in black in Figure 2),plus transmitting the pairX,Y and the probabilityof the split, which is exactly the cost of a singlearc, even though the actual split model will havemany more arcs (given in red in Figure 4).
Oncethis is taken into account, we can systematicallyinvestigate all 6?5 merge-split possibilities.
Theresults confirm the educated linguistic guess quiteremarkably.
The best compression rates are ob-tained by merging I with any of the minor cate-gories or, if I is already discarded or merged in,merging M into S. The smallest O model beforethese steps took 781 bits, this is now reduced to502 bits.
If we start by discarding I , and mergingM to S afterwards, this can be reduced to 349 bits.In the end we merge the morphophonologically af-fected forms with the ones not so affected not be-cause our training as linguists tells us we shoulddo this, but because that is what brevity demands.4 ConclusionsIn this paper we have developed an MDL-basedframework for structure detection based on simplenotions mostly borrowed from signal processing:quantization noise, inherent noise level, and cut-offs.
Standard n-gram models fare rather poorlycompared either in size or in model accuracy tothe WFSA results obtained here: for example onthe morphotactics data a straight SRILM trigrammodel has over 200 parameters and has KL diver-gence 1.09 bits.
Most of the 64 bits per n-gramparameter are wasted (if we assume only 12 bitsper parameter, the WFSA we use requires only49 parameters and gets within 0.03 bits of the ob-served data) and further, the general-purpose back-off scheme built into SRILM just makes mattersworse.Similarly, on the proquant data an SRILM bi-gram model has 175 parameters (including the26 unigram weights but excluding the backoffweights), yet it is farther from the data at 64 bitsresolution than our best 27-parameter model at 3bits.
More important, the bigram structure of theproquant data has to be hand-fed into the standardmodel, while the MDL approach can discover this,together with other linguistically relevant observa-tions such that hogy was ambiguous.This is not to say that n-gram models are nolonger competitive, for our current MDL meth-ods, based on a simulated annealing learner, usetoo much CPU and will not scale to the gigawordregime without much further work.
Yet if for-mal grammar and information theory are to get to-gether again, as (Pereira, 2000) suggests, we mustdirect effort towards recapitulating linguistic prac-tice, including the ?dirty?
parts such as discardingdata strategically.
The main thrust of the work pre-sented here is that the data manipulation methodsthat are the stock in trade of the descriptive linguistare LIP, and Universal Grammar is simply a shortlist of the permissible model transformations in-cluding path duplication for ambiguity, state merg-ing for position class effects, and merge-split forcollapsing categories.AcknowledgmentsWe thank Da?niel Varga (Prezi) and Viktor Nagy(Prezi) for the first version of the simulated an-nealing WFSA learner.
Zse?der wrote the versionused in this study, Recski collected the data andran the HMM baseline, Kornai advised.
The cur-rent version benefited greatly from the remarks ofanonymous referees.
Work supported by OTKAgrants #77476 and #82333.ReferencesNoam Chomsky and Morris Halle.
1965a.
Some con-troversial questions in phonological theory.
Journalof Linguistics, 1:97?138.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with Golombcoding.
In Proceedings of the Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 199?207, Prague.
ACL.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Proc.
6th SIGPHON,pages 21?30.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using Morfessor 1.0.
TechnicalReport A81, Helsinki University of Technology.Karel de Leeuw, Edward F. Moore, Claude E. Shannon,and N. Shapiro.
1956.
Computability by probabilis-tic machines.
In C.E.
Shannon and J. McCarthy, ed-itors, Automata studies, pages 185?212.
PrincetonUniversity Press.Samuel Eilenberg.
1974.
Automata, Languages, andMachines, volume A.
Academic Press.John A. Goldsmith.
2001.
Unsupervised learning ofthe morphology of a natural language.
Computa-tional Linguistics, 27(2):153?198.Peter Gru?nwald.
1996.
A minimum descriptionlength approach to grammar inference.
In StefanWermter, Ellen Riloff, and Gabriele Scheler, editors,Conectionist, statistical, and symbolic approachesto learning for natural language processing, LNCS1040, pages 203?216.
Springer.Pe?ter Hala?csy, Andra?s Kornai, La?szlo?
Ne?meth, Andra?sRung, Istva?n Szakada?t, and Viktor Tro?n.
2004.
Cre-ating open language resources for Hungarian.
InProceedings of the 4th international conference onLanguage Resources and Evaluation (LREC2004),pages 203?210.Frederick Jelinek.
1997.
Statistical Methods forSpeech Recognition.
MIT Press.Ga?bor Kiss, Ma?rton Kiss, Bala?zs Sa?fra?ny-Kovalik,and Dorottya To?th.
2011.
A magyar szo?elemta?rmegalkota?sa e?s a magyar gyo?kszo?ta?r elo?ke?szt?o?munka?latai.
In A. Tana?cs and V. Vincze, editors,MSZNY 2012, pages 102 ?
112.John Makhoul, Salim Roucos, and Herbert Gish.
1985.Vector quantization in speech coding.
Proceedingsof the IEEE, 73(11):1551?1588.Mehryar Mohri.
2009.
Weighted automata algo-rithms.
In Manfred Droste, Werner Kuich, andHeiko Vogler, editors, Handbook of Weighted Au-tomata, Monographs in Theoretical Computer Sci-ence, pages 213?254.
Springer.Fernando Pereira.
2000.
Formal grammar and in-formation theory: Together again?
Philosophi-cal Transactions of the Royal Society, A 358:1239?1253.Jorma Rissanen.
1978.
Modeling by the shortest datadescription.
Automatica, 14:465?471.Arto Salomaa and Matti Soittola.
1978.
Automata-Theoretic Aspects of Formal Power Series.
Springer,Texts and Monographs in Computer Science.Kristie Seymore and Ronald Rosenfeld.
1996.
Scal-able backoff language models.
In Spoken Language,1996.
ICSLP 96.
Proceedings., Fourth InternationalConference on, volume 1, pages 232?235.
IEEE.Koichi Shinoda and Takao Watanabe.
2000.
MDL-based context-dependent subword modeling forspeech recognition.
Journal of the Acoustical So-ciety of Japan (Eenglish edition), 21(2):79?86.Ray J. Solomonoff.
1964.
A formal theory of inductiveinference.
Information and Control, 7:1?22, 224?254.Paul M. B. Vitanyi and Ming Li.
2000.
Minimumdescription length induction, Bayesianism, and Kol-mogorov complexity.
IEEE Transactions on Infor-mation Theory, 46(2):446?464.Bernard Widrow and Istva?n Kolla?r.
2008.
Quantiza-tion Noise: Roundoff Error in Digital Computation,Signal Processing, Control, and Communications.Cambridge University Press.
