Parsing with the Shortest DerivationRens Bodhtformatics P.cseareh Institute, University of Leeds, Leeds LS2 9JT, &Institute for Logic, Language and Computation, University of Amsterdamtens @ scs.lecd s.ac.ukAbstractCommon wisdom has it that tile bias of stochasticgrammars in favor of shorter deriwttions of a sentenceis hamfful and should be redressed.
We show that thecommon wisdom is wrong for stochastic grammarsthat use elementary trees instead o1' conlext-l 'reerules, such as Stochastic Tree-Substitution Grammarsused by Data-Oriented Parsing models.
For suchgrammars a non-probabi l ist ic metric based on tileshortest derivation outperforms a probabilistic metricon the ATIS and OVIS corpora, while it obtainscompetitive results on the Wall Street Journal (WSJ)corpus.
This paper also contains the first publislmdexperiments with DOP on the WSJ.1.
IntroductionA well-known property of stochastic grammars is theirprope,lsity to assign highe, probabil it ies to shorterderivations o1' a sentence (cf.
Chitrao & Grishman1990; Magerman & Marcus 1991; Briscoe & Carroll1993; Charniak 1996).
This propensity is due to theprobabil ity o1' a derivation being computed as tileproduct of the rule probabil it ies, and thus shorterderivations involving fewer rules tend to have higherprobabilities, ahnost regardless of the training data.While this bias may seem interesting in the light ofthe principle of cognitive economy, shorter derivat-ions generate smaller parse h'ees (consisting of fewernodes) whiclt are not warranted by tile correct parsesof sentences.
Most systems therefore redress this bias,for instance by normalizillg the derivation probability(see Caraballo & Charniak 1998).However, for stochastic grammars lhat useelementary trees instead o1' context-l'ree rules, thepropensity to assign higher probabil it ies to shorterderivations does not necessarily lead to a bias infavor of smaller parse trees, because lementary treesmay differ in size and lexicalization.
For StochasticTree-Substitution Grammars (STSG) used by Data-Oriented Parsing (DOP) models, it has been observedlhat the shortest derivation of a sentence consists ofthe largest subtrees een in a treebank thai generatethat sentence (of.
Bed 1992, 98).
We may thereforewonder whether for STSG lhe bias in favor of shorterderivations is perhaps beneficial rather than llarmful.To investigate this question we created a newSTSG-DOP model which uses this bias as a feature.This non-probabi l i s t i c  DOP model parses eachsentence by returning its shortest der ivat ion(consisting of tile fewest subtrees een in ttle corpus).Only if there is more than one shortest derivation themodel backs off to a frequency ordering o1' the corpus-subtrees and chooses the shortest deriwttion with mosthighest ranked subtrees.
We compared this non-probabil ist ic DOP model against tile probabil ist icDOP model (which estimales the most probable parsefor each sentence) on three different domains: tbePenn ATIS treebank (Marcus et al 1993), the DutchOVIS treebank (Bonnema el al.
1997) and tile PennWall Street Journal (WSJ) treebank (Marcus el al.1993).
Stwprisingly, the non-probabilistic DOP modeloult~erforms the probabilistic I)OP model on both lheATIS and OVIS treebanks, while it obtains competit-ive resuhs on the WSJ trcebank.
We conjectu,c thaiany stochastic granlnlar which uses units of flexiblesize can be turned into an accurate non-probabilisticversion.Tile rest of this paper is organized as follows:we first explain botll the probabil ist ic and non-prolmbil istic DOP model.
Next, we go into tilecomputational aspccls of these models, and finallywe compare lhe performance of the models on thethree treebanks.2.
Probabilistic vs. Non-ProbabilisticData-Oriented ParsingBoth probabil istic and non-probabil ist ic DOP arebased on the DOP model in Bod (1992) whichextracts a Stochastic Tree-Substitution Grammar fi'oma treebank ("STSG-DOP").
I STSG-DOP uses subtreesJ Note that the l)OP-approach of extracting rammars f,omcorpora has been applied to a wide variety of othergrammatical fimncworks, including Tree-lnsertio,~ Grmnmar69from parse trees in a corpus as elementary trees, andleftmost-substitution t  combine subtrees into newtrees.
As at\] example, consider a very simple corpusconsisting of only two trees (we leave out somesubcategorizations to keel) the example simt}le):S s / x  /XNP VP \] ~ NP VPshe V NP \]\] ~ she VI' 1'1'....... ted A ANP PP V NP P NP /N /N  IAsaw the dog wilh Ihe telescopetile dress i } A{}I1 Ihe rackFigure 1.
A simple corpus o1' two trees.A new sentence such as She saw the dress with thete lescope can be parsed by combining subtrees fromthis corpus by means of lel 'tmost-substitution(indicated as ?
):S o X o = ,S / x  & /xPi" v,.
..... IA  i P /x..A with the telescopeshe VP PP s e VI'A A 7 ,SaW SaW I\]IC dl'CSs whh the telescopeFigure 2.
Derivation and parse tree for the sentence She sawlhe dress with the telescopeNote that other derivations, involving differentsubtrees, may yield the same parse tree; for instance:S o NPNP VP the dresssl!e VP PPA AV NP Pi IAsaw with file telescopeSNP VPVP PPA AV NP P NPsaw the dB:ss with the telescopeFigure 3. l)ifferent derivation yielding the same parse truelbr She saw tile &'ess with tile telescopeNote also that, given this example corpus, thesentence we considered is ambiguous; by combining(Hoogweg 2000), Tree-Adjoining Grammar (Neumalm1998), Lexical-Functional Grammar (Bed & Kaplan 1998;Way 1999; Bed 2000a), Head-driven Phrase StructureGrammar (Neumann & Flickinger 1999), and MontagueGrammar (van den Berg et al 1994; Bed 1998).
For therelation between DOP and Memory-Based Learning, seeDaelemans (1999).other subtrees, a dilTerent parse may be derived,which is analogous to the first rather than the secondcorpus  Sel l lOl lCe:S o V o pp SA I A A NP VP NP VP saw I} NPIA  i /X  IAshe V NP with Ih?
telescol}e she V NPNP PP NP PPP NPIhe dless the dress I Awith tile telese:}peFigure 4.
Different derivation yielding a different parse treelbr Site saw the dress with the telescopeThe probabilistic and non-probabilistie DOP modelsdiffer in the way they define the best parse tree of asentence.
We now discuss these models separately.2.1 The probabil ist ic DOP modelThe probabilistic DOP model introduced in Bed(1992, 93) computes the most probable parse tree of asentence from the normalized subtree l'requencies inthe corpus.
The probability of a subtree t is estimatedas the nunlber of occurrences of t seen in the corpus,divided by the total number of occurrences of corpus-subtrees that have the same root label as t. Let It Irett, rn the number of occurrences of t in the corpusand let r ( t )  return the root label of t then:l'(t)=ltl/Zt':r(r)=,.
(t)lt' \[.2 Tim probability of aderivation is computed as the product of theprobabilities of the subtrees involved in it.
Theprobability of a parse tree is computed as the sum ofthe probabil it ies ol' all distinct derivations thatproduce that tree.
The parse tree with the highest2 It should be stressed that there may be several other waysto estimate subtree probabilities in I)OP.
For example,Bonnema et al (1999) estimate the probability era subtreeas the probability that it has been involved in the derivationof a corpus tree.
It is not yet known whether this alternativeprobability model outperforms the model in Bed (1993).Johnson (1998) pointed out that the subtree estimator inBed (1993) yields a statistically inconsistent model.
Thismeans that as the training corpus increases thecorresponding sequences of probability distributions do notconverge to the true distribution that generated the trainingdata.
Experiments with a consistent maximum likelihoodestimator (based on the inside-outside algorithln in Lari andYot, ng 1990), leads however to a significant decrease inparse accuracy on the ATIS and OVIS corpora.
Thisindicates that statistically consistency does not necessarilylead to better performance.70probalfil ity is defined as the best parse tree of aSel ltence.The fn'obabilistie DOP model thus considerscounts of subtrees of a wide range o1' sizes incomputing the probability of a tree: everything fromcounts ?51' single-level rules to cotmts of entire trees.2.2 The noi l -probabi l ist ic I )OP modelTim non-prolmlfi l istic I)OP model uses a ratherdifferent definition of the best parse tree.
instead ofconqmting the most probable patse of a sentence, itcomputes the parse tree which can be generated bythe fewest eorpus-stibtrees, i.e., by the shortestderiwltion independent of the subtree prolmbilities.Since stlblrees are allowed to be of arbitrary size, tileshortest derivation typically corresponds 1(5 the pa.rsetree which consists of largest possible corpus-subtrees, thus maximizing syntaclic context.
Forcxmnple, given the corpus in Figure 1, the best parsetree for She saw the dress with the telescope is givenin Figure 3, since that parse tree can be generated bya derivation of only two corpus-sublrees, while tileparse tree in Figure 4 needs at least three corpus-sublrees to be generated.
(Interestingly, the parse lreewith the sho,'test derivation in Figure 3 is also tilemost probable parse tree according to pl'obalfilistic1)O1 ) for this corpus, but this need not always be so.As mentioned, the probabil ist ic 1)O1' lnodel hasah'eady a bias l(5 assign higher probabilities to parsetrees that can be generaled 153, shorter deriwtlions.
Thenon-pvobabi l ist ic  I)OP model makes this biasabsolute.
)The shortest deriwttion may not t)e unique: itmay happen that different parses of a sentence aregenerated by tile same mininml nmnl)er of corpus-subtrees.
In lhat ease the model backs off to al'requency ordering (51' the subtrees.
That is, allsubtrees of each root label arc assigned a rankaccording to their frequency ill the co,pus: the mostfrequent sublree (or subtrees) (51" each root label getrank 1, the second most frequent subtree gels rank 2,etc.
Next, the rank of each (shortest) derivation iscomputed its the sum of the ranks (51" tile subtrecsinvolved.
The deriwttion with the smallest sum, orhighest rank, is taken as the best derivation producingthe best parse tree.The way we compute the rank of a de,'ivalionby surmrdng up lhe ranks of its subtrees may seemrather ad hoc.
However, it is possible to provide aninformation-theoretical ,notivation for this model.According to Zipl"s law, rank is roughly prolxsrtionalto tile negative logaritlun of frequency (Zipf 1935).
InSlmnnon's Information Theory (Shannon 194~,), tilenegative logaritlun (of base 2) of the probability of anevent is belter known as the information (51' that event.Thus, tile rank of a subtree is roughly proportional toits information.
It follows that minimizing the sum ofthe sublrce ranks in a derivation corresponds tominimizing the (self-)information of a derivation.3.
Computational Aspects3.1 Computing the most probable  parseBed (1993) showed how standard chart parsingtechniques can be applied to probabilistic DOP.
Eachcorpus-subtree t is converted into a context-free rule rwhere the lefthand side <51" r corresponds to tile rootlabel of t and tile righthand side of r corresponds tothe fronlier labels of t. Indices link the rules to theoriginal subtrees so as to maintain the sublree'sinternal structure and probability.
These rules are usedlO Cl'e~:lte il.
der ivat ion  forest for a sentenc/2, illld themost p,obable parse is computed by sampling asufficiently large number of random deriwttions fromthe forest ("Monte Carlo disamt~iguation", see Bed1998; Chappel ier  & Rajman 2000).
Whi le thistechnique has been sttccessfully applied to parsinglhe ATIS portion in the Penn Treebank (Marcus et al1993), it is extremely time consuming.
This is mainlybecause lhe nun/bcr of random derivations thai shouldbe sampled to reliably estimate tile most prolmbleparse increases exponential ly with the sentencelength (see Goodman 1998).
It is therefore question-able whether Bed's slunpling teclmique can be scaledto larger corpora such as tile OVIS and the WSJcorpora.Goodman (199g) showed how tile probabil-istic I)OP model can be reduced to a compactstochastic context-free grammar (SCFG) whichcontains exactly eight SCFG rules for each node inthe training set trues.
Although Goodman's rcductkmmethod does still not al low for an eff ic ientcomputation {51 tile most probable parse in DOP (illfact, the prol~lem of computing the most prolmbleparse is NP-hard -- sue Sima'an 1996), his methoddoes al low for an eff icient computation o1' the"nmximun~ constituents parse", i.e., the parse tree thatis most likely to have the largest number of correctconstitueuts (also called the "labeled recall parse").Goodman has shown on tile ATIS corpus that thenla.xinltllll constituents parse perfor,ns at least as wellas lhe most probable parse if all subtl'ees are used.Unfortunately, Goodman's reduction method remains71beneficial only if indeed all treebank subtrces arcused (see Sima'an 1999: 108), while maximum parseaccuracy is typical ly obtained with a snbtree setwhich is smalle," than the total set of subtrees (this isprobably due to data-sparseness effects -- seeBonnema et al 1997; Bod 1998; Sima'an 1999).In this paper we will use Bod's subtree-to-ruleconversion method for studying the behavior ofprobabi l ist ic  against non-probabi l ist ic  DOP fordifferent maximtnn subtree sizes.
However, we willnot use Bod's Monte Carlo sampling technique fromcomplete derivation forests, as this turns out to becomputationally impractical for our larger corpora.Instead, we use a Viterbi n-best search and estimatethe most probable parse fi'mn the 1,000 most probablederiwltions, summing up tile probabilities hi' derivat-ions that generate the same tree.
Tile algorithm forcomputing n most probable deriwttions fol lowsstraightforwardly from the algorithm which computesthe most probable derivation by means of Viterbioptimization (see Sima'an 1995, 1999).3.2 Comput ing the shortest der ivat ionAs with the probabilistic DOP model, we first convertthe corpus-subtrees into rewrite rules.
Next, theshortest derivation can be computed in the same wayas the most probable deriwltion (by Viterbi) if wegive all rules equal probabilities, in which case tileshortest derivation is equal to the most probablederiwltion.
This can be seen as follows: if each rulehas a probability p then the probability of a derivationinvolving n rules is equal to pn, and since 0<p<l thederivation with the fewest rules has the greatestprobability.
In out" experiments, we gave each rule aprobability mass equal to I/R, where R is the ntunbcrof distinct rules derived by Bod's method.As mentioned above, the shortest derivationmay not be unique.
In that case we compute allshortest derivations of a sentence and then apply out"ranking scheme to these derivations.
Note that thisranking scheme does distinguish between snbtrees ordifferent root labels, as it ranks the subtrecs giventheir root label.
The ranks of the shortest derivationsare computed by summing up the ranks of thesubtrees they involve.
The shortest derivation with thesmallest stun o1' subtree ranks is taken to produce tilebest parse tree.
33 It may happcn that different shortest derivations generatethe same tree.
We will not distinguish between these cases,however, and co,npt, te only the shortest derivation with thehighest rank.4.
Experimental Comparison4.1 Experiments on the ATIS corpusFor our first comparison, we used I0 splits from thePenn ATIS corpus (Marcus et al 1993) into trainingsets of 675 sentences and test sets of 75 sentences.These splits were random except for one constraint:tbat all words in the test set actually occurred in thetraining set.
As in Bod (1998), we el iminated allepsilon productions and all "pseudo-attachments".
Asaccuracy metric we used the exact match defined asthe percentage of the best parse trees that areidentical to the test set parses.
Since the Penn ATISportion is relatively small, we were able to computethe most probable parse both by means of MonteCarlo sampling and by means of Viterbi n-best.
Table1 shows the means o1' tile exact match accuracies forincreasing maximum subtrec depths (up to depth 6).Depth of Probabilistic DOP Non-probabilisticsubtrees Monte Carlo Viterbi n-best l)OPl 46.7 46.7 24.8<2 67.5 67.5 40.3__.<3 78. l 78.2 57.1__<4 83.6 83.0 81.5-<5 83.9 83.4 83.6-<6 84.
I 84.0 85.6Tablc 1.
Exact match accuracies/'or the ATIS corpusTile table shows that tile two methods for probabilisticDOP score roughly tile same: at dcpfll _< 6, the MonteCarlo method obtains 84.1% while the Viterbi n-bestmethod obtains 84.0%.
These differences are notstatistically significant.
The table also shows that forsmall subtree depths the non-probabilistic DOP modelperforms considerably worse than the probabil isticmodel.
This may not be surprising since for smallsubtrecs tile shortest derivation corresponds to tilesmallest parse tree which is known to be a badprediction of the correct parse tree.
Only il' thesubtrees are larger than depth 4, the non-probabilisticDOP model  scores roughly the same as itsprobabil istic ounterpart.
At subtree depth < 6, thenon-probabilistic DOP model scores 1.5% better thanthe best score of the probabilistic DOP model, whichis statistically significant according to paired t-tests.4.2 Experiments on tile OVIS corpusFor out" comparison on tile OVIS corpus (Bonnema ctal.
1997; Bod 1998) we again used 10 random splitsunder tile condition that all words in tile test setoccurred in the training set (9000 sentences for72training, 1000 sentences for testing).
The ()VIS treescontain both syntactic and se,nantic annotations, butno epsilon productions.
As in Bod (1998), we lreatedthe syntactic and semantic annotations of each nodeas one label.
Consequently, the labels are veryrestrictive and col lecting statistics over them isdifficult.
Bonncma et al (1997) and Sima'an (1999)report that (probal)ilislic) I)OP sulTers considerablyfrom data-sparseness on OVIS, yielding a decrease inparse accuracy if subtrees larger lh'an depth 4 areincluded.
Thus it is interesting to investigate how non-probabil istic DOP behaves on this corpus.
Table 2shows the means of the exact match accuracies forincreasing subtree depths.l)epth of I'rolmbilistic Non-probabilisticsubtrecs 1)OP D()P1 83.1 70.4~2 87.6 85.1_<3 89.6 89.5_<4 90.0 90.9_<5 89.7 91.5_<6 88.8 92.2Table 2.
Exact match accuracies for the OV1S corpusWe again sue that the non-pl'olmlfilistic l)()P modelperforms badly fOl small subtree depths while itoutperforms the probabi l is l ic DOP model if thesublrees gel larger (in this case for depth > 3).
Bulwhile lhe accuracy of probabilislic l)()P deterioratesafter depth 4, the accuracy of non-prolmbilistic 1)O1 +contintms to grow.
Thus non-prolmlfilistic \])()P seemsrelatively insensitive to tile low frequency of largersubtrees.
This properly may be especially useful if nomeaningful  stat ist ics can be co l lected whilesentences can still be parsed by large chunks.
Atdepth ___ 6, non-probabilislic DOP scores 3.4% betterthan probalfilistic DOP, which is statistically signifi-cant using paired t-tests.4.3 Exper iments on the WSJ  corpusBoth the ATIS and OVIS corpus represent restricteddomains.
In order to extend ()tit" results to a broad-coverage domain, we tested tile two models also ontile Wall Street Journal portion in the Penn Treebank(Marcus et ill. 1993).To make our results comparable to ()tilers, wedid not test on different random splits but used thenow slandard division of the WSJ with seclions 2-21for training (approx.
40,000 sentences) and section 23for testing (see Collins 1997, 1999; Charniak 1997,2000; l~,atnalmrkhi 1999); we only tested on sentences_< 40 words (2245 sentences).
All trees were strippedoff their Selllalltic lags, co-reference information andquotation marks.
We used all training set sublrees o1'depth 1, but due to memory limitations we used asubset of the subtrees larger than depth l by taking foreach depth a random sample o1' 400,000 subtrecs.
Nosubtrces larger than depth 14 were used.
This resultedinto a total set of 5,217,529 subtrees which weresmoothed by Good-Turing (see Bod 1996).
We did notemploy a separate part-of-speech tagger: tile testsentences were directly parsed by the training setsubtrees.
For words that were unknown in tile trainingset, we guessed their categories by means of themethod described in Weischedel et al (1993) whichuses statistics on word-endings, hyphenation andcapital izat ion.
The guessed category for eachllllklloWn Wol'd was converted into a depth-I subtreeand assigned a probabil ity (or frequency for non-probabilistic I)OP) by means of simple Good-Turing.As accuracy metric we used the standardPAP, SEVAI, scores (Black et al 1991) to compare aproposed parse 1' with tile corresponding correcttreebank parse 7' as follows:# correct constituents in Pl.abcled Precision -# constilucnts in 1'# COI'I'CCI.
COllstittlcnts ill l ~Labeled Recall =# constituents in TA constituent in P is "correct" if there exisls aconstituent in 7' of tile sanle label that spans the samewords.
As in other work, we collapsed AI)VP andPl?Jl" to the same label when calculating these scores(see Collins 1997; I~,atnaparkhi 1999; Charniak 1997).Table 3 shows the labeled precision (LP) andlabeled recall (LR) scores for probabilistic and non-probabilistic DOP for six different maximum subtreedepths.Depth of l'robabilistic I)OP Non-probabilislicsubtrecs l)OP1,1 ~ LR LP LR<_4 84.7 84.1 81.6 80.1<_6 86.2 86.0 85.0 84.7_<8 87.9 87.1 87.2 87.0_< 10 88.6 88.0 86.8 86.5_<12 89.1 88.8 87.1 86.9_< 14 89.5 89.3 87.2 86.9Table 3.
Scores on tile WSJ corpus (sentences _<40 words)73The table shows that probabil istic DOP outperl'ormsnon-probabilistic DOP for maximum subtree depths 4and 6, while the models yield rather similar results formaximum subtree depth 8.
Surprisingly, the scores ofnonq~robabilistic DOP deteriorate if the subtrees arefurther enlarged, while tile scores of probabil isticDOP continue to grow, up to 89.5% LP and 89.3%LR.
These scores are higher than those of severalother parsers (e.g.
Collins 1997, 99; Charniak 1997),but remain behind tim scores of Charniak (2000) whoobtains 90.1% LP and 90.1% LR for sentences _< 40words.
However, in Bod (2000b) we show that evenhigher scores can be obtained with probabilistic DOPby restricting tile number of words in the subtreefrontiers to 12 and restricting the depth of unlexical-ized subtrees to 6; with these restrictions an LP of90.8% and an LR of 90.6% is achieved.We may raise the question as to whether weactually need these extremely large subtrees to obtainour best results.
One could argue that DOP's gain inparse accuracy with increasing subtree depth is due totile model becoming sensitive to the int'luence o1'lexical heads higher in tile lree, and that this gaincould also be achieved by a more compact depth-1DOP model (i.e.
an SCFG) which annotates thenonterminals with headwords.
However, such a head-lexical ized stochastic grammar does not capturedependencies between nonheadwords (such as moreaud than in tile W,qJ construction carry more peoplethan cargo where neither more nor  th\[lll are headwordsol' tile NP-constitucnt lllore people than cargo)),whe,eas a frontier-lexicalized DOP model using largesubtrecs does capture these dependencies ince itincludes subtrees in which e.g.
more and than are theonly frontier words.
In order to isolate tile contributionof nonheadword dependencies, we el iminated allsubtrees containing two or more nonheadwnrds (wherea nonheadword of a subtl'ec is a word which is not aheadword of the subtree's root nonterminal -- althoughsuch a nonheadword may be a headword of one of thesubtree's internal nodes).
On the WSJ this led to adecrease in LP/LR of 1.2%/1.0% for probabil isticDOP.
Thus nonheadword ependencies contribute tohigher parse accuracy, and should not be discarded.This goes against common wisdom that the relevantlexical dependencies can be restricted to the localityof beadwords of constituents (as advocated in Collins1999).
It also shows that DOP's frontier lexicalizationis a viable alternative to constituent lexicalization(as proposed in Charniak 1997; Collins 1997, 99;Eisner 1997).
Moreover, DOP's use of large subtreesmakes tim model not only more lexically but alsomore structurally sensitive.5.
ConclusionCommou wisdom has it that tile bias o1' stochasticgrammars ill favor of shorter derivations is harnffuland should be redressed.
We have shown that thecommon wisdom is wrong for stochastic trek-substitution grammars that use elementary trees (11'flexible size.
For such grammars, a non-probabilisticmetric based on the shortest derivation outperforms aprobabilistic metric on tile ATIS and OVIS corpora,while it obtains competit ive results on tile WallStreet Journal corpus.
We have seen that a non-probabil istic version o1' DOP performed especiallywell on corpora for which collecting sublree statisticsis difficult, while sentences can still be parsed byrelatively large chunks.
We have also seen thatprobabilistic DOP obtains very competitive results onthe WSJ corpus.
Final ly, we conjecture thai anystochastic grammar which uses elementary treksrather than context-free rules can be turned into anaccurate non-probabilistic version (e.g.
Tree-InsertionGrammar and Tree-At\[joining Grammar).AcknowledgementsThanks to Khali l  Sima'an and three anonymousreviewers for useful suggestions.ReferencesBerg, M. van den, R. Bod and R. Scha, 1994.
"A Corpus-Based Approach to Semantic Interpretation",l'roceedings Ninth Amsterdam Colloquium,Amsterdam, The Netherlands.Black, E. et al 1991, A Procedure for QuantitativelyComparing the Syntactic Coverage of English,Proceedings DARPA Speech and Natural LanguageWorkshop, Pacific Grove, Morgan Kattfinann.Bod, P,.
1992.
"l)ata-Oriented Parsing (I)OP)", ProceedingsCOLING-92, Nantes, France.Bod, R. 1993.
"Using an Annotated Corpus as a StochasticGrammar", l'roceedings European ChcqJter q/" theACL'93, Utrecht, The Netherlands.Bod, R. 1996.
"Two Questions about l)ata OrientedParsing", l'roceedings Fourth Workshop on Very LargeCorpora, Copenhagen, 1)cnmark.Bod, R. 1998.
Beyond Grammar: An EaT,erience-l~asedTheot3, of Language, CSLI Publications, CambridgeUniversity Press.Bod, R. 2000a.
"An Empirical Evaluation of LI~G-I)OP '',Proceedings COLING-2000, Saarbriickcn, Germany.74Bed, R. 200Oh.
"Redundancy and Minimality in StatisticalParsing with Ihe I)OP Model", submitted forpublicalion.Bed, P,.
and 1(.
l(al)lan, 1998,.
"A Probal)ilistic Corpus-l)rivcn Model lkw l,cxical Ftmctional Analysis",l't'oceedings COIJNf;-ACI,'9& Montreal, CanadzLllonncma, R., R. Bed and R. Scha, 1997.
'% 1)OI ) Model Ik)lSemantic Inlerpretation", Proceedings ACI/l';ACI.-97,Madrid, Spain.Bonncma, R., t'.
Buying and P,.
Scha, 1999.
"A NewI'robalfilily Model for I)ata-()ricntcd Parsing",lb-oceedings o.f the Amsterdam Colloqttittm 1999,Amsterdam, The Nethcrhmds.Briscoe, T. and J. Carroll, 1993.
"Generalized I'robabilisticIA~.
I'arsing of Natural l,anguage (Corpora) withUni ficalion-I~ased Gramnlars",  ComlnaatioualLinq, uistics 19(1 ), 25-59.
('araballo, S. and E. Charniak, 1998.
"New Figures of Meritfor Best-First Probabil istic Chart Parsing",Comlmtational Lit~gttistics 2d, 275-298.Chappclicr, J. and M. l,'ajman, 2000.
"Monte CarloSampling Ior Nl'-hard Maximization I'roblems in thel"ramework of  Wc ghtcd Parsing", iu Natmcdl.altguag<!
l)roce.ssing - -  NI.I' 2000, l.eclure Not,:!.,; i,qArttl/h:ial nlelligence 1835, 1).
Chrislodoulakis (ed.
),2000, 106-117.Charniak, 1';.
1996.
"Tree-bank Grammars", ProceedingsAAAI-96, Menlo Park, Ca.Charniak, 1!.
1997.
"Statistical Parsing wilh a Contcxl-l:rccGrammar and Word Statistics", l'roceedinw AAAI-97.Menlo Park.
Ca.Charniak, I:_ 2000.
"A Maximtm>lintropy-hlspircd Parser",Proceedings ANI,I~-NAACI,'2000, Scnttlc, Washington.Chitrao, M. and P,.
Grishman, 1990.
"Stalislical Parsing ofMessages", Proceedings I)ARPA Speech a/ul l,angtta,~,,eWorkshol~ 1990.Collins, M. 1997.
"Three generalive lexicaliscd models tbrstatistical parsing", l'roceedittgs EACI/ACL'97,Madrid, Spain.Collins, M. 1999. llead-l)riven Statistical Models./br NaturalI,anguagc I'arsing, Phl)-thesis, University ofPennsylvania, PA.I)aclcmans, W.
(ed.)
1999.
Memmy-Ilased LanguageProcessing, Journal./br l';.vperimental and TheoreticalArti/i'cial Inlelligence, 11(3).Eisner, J.
1997.
"Bilexical Granlnlars and a Cubic-TimeProbabilistic Parser", I'mceedings l,'\[/?h InternationalWorkshop on l'alwing 7?clmologies, I{oslon, Mass.Goodman, J.
1998.
Palwing lnsMe-Out, Ph.l).
thesis, l larwu'dUniversity, Mass.l loogweg, 1.. 2000.
Extending DOPI with the InsertionOperation, Maslel"S thesis, University of Amsterdam,The Netherhmds.Johnson, M. 1998.
"The I)OP Estimatkm Method is Biasedand Inconsistent", squib.l,ari, K. and S. Young 1990.
"The Estimation of StochasticContext-Free Granunars Using the lnside-()utsidcAlgorithm", ()mtl~ltlel" ,~'1~(3cc11 alld l,angttctge, d, 35-56.Magerman, l).
and M. Marcus, 1991.
"Pearl: A ProbabilislicChart Parser", Proceedings I'ACL'91, Berlin,(iermany.Marcus, M., B. Santorini and M. Marcinkiewicz, 1993.
"lhtikling a l'u-e,~ ~ Annotaled Corpus of Fm~lish:~ thePcnt~ Trcebank", Comlmtatioual l,ingttistics 19(2).Nculnann, (~.
1998.
"Automatic Extraction of SlochasticImxicalizcd Tree C;r~.lllllllals flonl Treebanks",Proceedings of the 4th Workshop on 7)'ee-Adjoining(;rammcuw and Related I,)ameworl<s, Philadclplaia, PA.Neumann, (3. and I).
Flickinger, 1999.
"l~earning Stochasticl,exicalizcd Tree (\]lanllnars frolll IIPSG", I)FKITechnical (epoit, Saartwiicken, Germany.Ilalnaparkhi, A.
1999.
"l,carning to Parse Natural 14mguagewith Maximum linlropy Models", Machit~e /.earning34.
151-176.S\[laIlllOll, C. 1948.
A Mathematical Theory of Communic-ation.
IMII System Technical .Iota'hal.
27, 379-423, 623-656.Sima'an, K. 1995.
"An optimized algorilhin for l)ataOriented Parsing", in: R. Milkov and N, Nicolov(eds.
), Recent Advam:es in Natttral l,alsguagel'rocessing 1995, w>lume 136 of Current lsstws inLinguistic T/woO'.
JohI1 Bcnjalll.ins, Aiilslerdali1.Sima'an, K. 1996.
"Comlmtational Complcxity ofPlobabilistic l)isambiguation hy means of Tree(;l'\[lllllll\[ll'S", l'rocc('dings C()I.IN(;-9(~, Copenhagen,I)cnmark.Sima'an, K. 1999. l.earni/tg l:i/.\]icient l)isambigua/i:m. 113,CDissertation SOlits 199%02, l.Jtrcdlt University /University of Amsterdam, March 1999, TheNclherlands.Wcischedel, R., M. Mercer, R, Schwarz, L. Ramshaw andJ.
Pahnucci, 1993.
"Coping with Ambiguity andUnknown Words through Probabilistic Models",Comlmtational Linguistics, 19(2), 359-382.Way, A.
1999.
"A t lybrid Architecture for I~.obus!
MT usingI,t"G-I)OP", .hmrnal o/" l'2xlwrimental m/ TheoreticalArti/}'cial Intelligence I1 (Special Issue on Memory-Based l~anguage Processing).Zipf, G. 1935.
The l'sycho-Biology of Language, l loughtonMifflin.75
