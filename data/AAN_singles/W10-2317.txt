Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 103?107,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsComputing Word Senses by Semantic Mirroring and Spectral GraphPartitioningMartin FagerlundLinko?ping UniversityLinko?ping, Swedenmarfa229@student.liu.seLars Elde?nLinko?ping UniversityLinko?ping, Swedenlars.elden@liu.seMagnus MerkelLinko?ping UniversityLinko?ping, Swedenmagnus.merkel@liu.seLars AhrenbergLinko?ping UniversityLinko?ping, Swedenlars.ahrenberg@liu.seAbstractUsing the technique of ?semantic mirror-ing?
a graph is obtained that representswords and their translations from a paral-lel corpus or a bilingual lexicon.
The con-nectedness of the graph holds informationabout the different meanings of words thatoccur in the translations.
Spectral graphtheory is used to partition the graph, whichleads to a grouping of the words accordingto different senses.
We also report resultsfrom an evaluation using a small sample ofseed words from a lexicon of Swedish andEnglish adjectives.1 IntroductionA great deal of linguistic knowledge is encodedimplicitly in bilingual resources such as par-allel texts and bilingual dictionaries.
Dyvik(1998, 2005) has provided a knowledge discov-ery method based on the semantic relationship be-tween words in a source language and words ina target language, as manifested in parallel texts.His method is called Semantic mirroring and theapproach utilizes the way that different languagesencode lexical meaning by mirroring source wordsand target words back and forth, in order to es-tablish semantic relations like synonymy and hy-ponymy.
Work in this area is strongly related towork within Word Sense Disambiguation (WSD)and the observation that translations are a goodsource for detecting such distinctions (Resnik &Yarowsky 1999, Ide 2000, Diab & Resnik 2002).A word that has multiple meanings in one lan-guage is likely to have different translations inother languages.
This means that translationsserve as sense indicators for a particular sourceword, and make it possible to divide a given wordinto different senses.In this paper we propose a new graph-based ap-proach to the analysis of semantic mirrors.
Theobjective is to find a viable way to discover syn-onyms and group them into different senses.
Themethod has been applied to a bilingual dictionaryof English and Swedish adjectives.2 Preparations2.1 The Translation MatrixIn these experiments we have worked with aEnglish-Swedish lexicon consisting of 14850 En-glish adjectives, and their corresponding Swedishtranslations.
Out of the lexicon was created atranslation matrix B, and two lists with all thewords, one for English and one for Swedish.
Bis defined asB(i, j) ={1, if i ?
j,0, otherwise.The relation i ?
j means that word i translatesto word j.2.2 TranslationTranslation is performed as follows.
From theword i to be translated, we create a vector e?i, witha one in position i, and zeros everywhere else.Then perform the matrix multiplication Be?i if itis a Swedish word to be translated, or BT e?i if it isan English word to be translated.
e?i has the samelength as the list in which the word i can be found.3 Semantic MirroringWe start with an English word, called eng11.
Welook up its Swedish translations.
Then we look up1Short for english1.
We will use swe for Swedish words.103the English translations of each of those Swedishwords.
We have now performed one ?mirror-operation?.
In mathematical notation:f = BBT e?eng1.The non-zero elements in the vector f representEnglish words that are semantically related toeng1.
Dyvik (1998) calls the set of words that weget after two translations the inverse t-image.
Butthere is one problem.
The original word should notbe here.
Therefore, in the last translation, we mod-ify the matrix B, by replacing the row in B corre-sponding to eng1, with an all-zero row.
Call thisnew modified matrixBmod1.
So instead of the ma-trix multiplication performed above, we start overwith the following one:Bmod1BT e?eng1.
(1)To make it clearer from a linguistic perspective,consider the following figure2.eng2swe133ffffffffff eng3eng1++XXXXXXXXXX//33ffffffffff swe233ffffffffff++XXXXXXXXXXeng1swe3 //++XXXXXXXXXXeng4eng5The words to the right in the picture above(eng2,...,eng5) are the words we want to divideinto senses.
To do this, we need some kind ofrelation between the words.
Therefore we con-tinue to translate, and perform a second ?mirroroperation?.
To keep track of what each word inthe inverse t-image translates to, we must firstmake a small modification.
We have so far donethe operation (1), which gave us a vector, call ite ?
R14850?1.
The vector e consists of nonzero in-tegers in the positions corresponding to the wordsin the invers t-image, and zeros everywhere else.We make a new matrix E, with the same numberof rows as e, and the same number of columns asthere are nonzeros in e. Now go through every el-ement in e, and when finding a nonzero elementin row i, and if it is the j:th nonzero element, thenput a one in position (i, j) in E. The procedure isillustrated in (2).2The arrows indicate translation.????????102103?????????????????
?1 0 0 00 0 0 00 1 0 00 0 1 00 0 0 00 0 0 1????????
(2)When doing our second ?mirror operation?, we donot want to translate through the Swedish wordsswe1,...,swe3.
We once again modify the matrixB, this time replacing the columns of B corre-sponding to the Swedish words swe1,...,swe3, withzeros.
Call this second modified matrix Bmod2.With the matrix E from (2), we now get:Bmod2BTmod2E (3)We illustrate the operation (3):swe4 //))SSSSeng6eng255kkkk// swe5 //))SSSSeng2swe155kkkkeng355kkkkswe1 eng3eng1))SSSS//55kkkkswe255kkkk))SSSSeng1 swe2 eng1swe3 //))SSSSeng4))SSSSswe3 eng4eng5 //))SSSSswe655kkkk//))SSSSeng5swe755kkkk// eng7Now we have got the desired relation betweeneng2,...eng5.
In (3) we keep only the rows corre-sponding to eng2,...eng5, and get a symmetric ma-trix A, which can be considered as the adjacencymatrix of a graph.
The adjacency matrix and thegraph of our example are illustrated below.A =???
?2 1 0 01 1 0 00 0 1 10 0 1 2????
(4)eng2eng3eng4eng5Figure 1: The graph to the matrix in (4).The adjacency matrix should be interpreted in thefollowing way.
The rows and the columns corre-spond to the words in the inverse t-image.
Follow-ing our example, eng2 corresponds to row 1 and104column 1, eng3 corresponds to row 2 and column2, and so on.
The elements on position (i, i) inA are the vertex weights.
The vertex weight as-sociated with a word, describes how many transla-tions that word has in the other language, e.g.
eng2translates to swe4 and swe5 that is translated backto eng2.
So the vertex weight for eng2 is 2, as alsocan be seen in position (1, 1) in (4).
A high vertexweight tells us that the word has a high number oftranslations, and therefore probably a wide mean-ing.The elements in the adjacency matrix on posi-tion (i, j), i ?= j are the edge weights.
Theseweights are associated with two words, and de-scribe how many words in the other language thatboth word i and j are translated to.
E.g.
eng5and eng4 are both translated to swe6, and it fol-lows that the weight, w(eng4,eng5) = 1.
If we in-stead would take eng5 and eng7, we see that theyboth translate to swe6 and swe7, so the weight be-tween those words, w(eng5,eng7) = 2.
(But this isnot shown in the adjacency matrix, since eng7 isnot a word in the inverse t-image).
A high edgeweight between two words tells us that they sharea high number of translations, and therefore prob-ably have the same meanings.4 Graph PartitioningThe example illustrated in Figure 1 gave as a re-sult two graphs that are not connected.
Dyvik ar-gues that in such a case the graphs represent twogroups of words of different senses.
In a largerand more realistic example one is likely to obtaina graph that is connected, but which can be parti-tioned into two subgraphs without breaking morethan a small number of edges.
Then it is reason-able to ask whether such a partitioning has a sim-ilar effect in that it represents a partitioning of thewords into different senses.We describe the mathematical procedure of par-titioning a graph into subgraphs, using spectralgraph theory (Chung, 1997).
First, define the de-gree d(i) of a vertex i to bed(i) =?jA(i, j).Let D be the diagonal matrix defined byD(i, j) ={d(i), if i = j,0, otherwise.The Laplacian L is defined asL = D ?A.We define the normalised Laplacian L to beL = D?12LD?12 .Now calculate the eigenvalues ?0, .
.
.
, ?n?1, andthe eigenvectors of L. The smallest eigen-value, ?0, is always equal to zero, as shown byChung (1997).
The multiplicity of zero amongthe eigenvalues is equal to the number of con-nected components in the graph, as shown bySpielman (2009).
We will look at the eigenvectorbelonging to the second smallest eigenvalue, ?1.This eigenpair is often referred to as the Fiedlervalue and the Fiedler vector.
The entries in theFiedler vector corresponds to the vertices in thegraph.
(We will assume that there is only onecomponent in the graph.
If not, chose the com-ponent with the largest number of vertices).
Sortthe Fiedler vector, and thus sorting the vertices inthe graph.
Then make n?
1 cuts along the Fiedlervector, dividing the elements of the vector into twosets, and for each cut compute the conductance,?
(S), defined as?
(S) = d(V ) |?
(S, S?)
|d(S)d(S?
), (5)where d(S) =?i?S d(i).
| ?
(S, S?)
| is the totalweight of the edges with one end in S and one endin S?, and V = S + S?
is the set of all vertices inthe graph.
Another measure used is the sparsity,sp(S), defined assp(S) = |?
(S, S?)
|min(d(S), d(S?
))(6)For details, see (Spielman, 2009).
Choose the cutwith the smallest conductance, and in the graph,delete the edges with one end in S and the otherend in S?.
The procedure is then carried out untilthe conductance, ?
(S), reaches a tolerance.
Thetolerance is decided by human evaluators, per-forming experiments on test data.5 ExampleWe start with the word slithery, and after the mir-roring operation (3) we get three groups of wordsin the inverse t-image, shown in Table 1.
Aftertwo partitionings of the graph to slithery, using themethod described in section 4, we get five sensegroups, shown in Table 2.105smooth slimy saponaceousslick smooth-facedlubricious oleaginousslippery oily slippyglib greasysleekTable 1: The three groups of words after the mir-roring operation.slimy glib oleaginoussmooth-faced slippery oilysmooth lubricious greasysleek slicksaponaceous slippyTable 2: The five sense groups of slithery after twopartitionings.6 EvaluationA small evaluation was performed using a ran-dom sample of 10 Swedish adjectives.
We gen-erated sets under four different conditions.
For thefirst, using conductance (5).
For the second, usingsparsity (6).
For the third and fourth, we set thediagonal entries in the adjacency matrix to zero.These entries tell us very little of how the wordsare connected to each other, but they may effecthow the partitioning is made.
So for the third, weused conductance and no vertex weights, and forthe fourth we used sparsity and no vertex weights.There were only small differences in results due tothe conditions, so we report results only for one ofthem, the one using vertex weights and sparsity.Generated sets, with singletons removed, wereevaluated from two perspectives: consistency andsynonymy with the seed word.
For consistency athree-valued scheme was used: (i) the set forms asingle synset, (ii) at least two thirds of the wordsform a single synset, and (iii) none of these.
Syn-onymy with the seed word was judged as eitheryes or no.Two evaluators first judged all sets indepen-dently and then coordinated their judgements.
Thecriterion for consistency was that at least one do-main, such as personality, taste, manner, can befound where all adjectives in the set are inter-changeable.
Results are shown in Table 3.Depending on howwe count partially consistentgroups this gives a precision in the range 0.57 to0.78.
We have made no attempt to measure recall.Count Average PercentageAll groups 58 5.8 100Consistentgroups33 3.3 572/3 consistency 12 1.2 21Synonymywith seed word14 1.4 24Table 3: Classified output with frequencies fromone type of partitionIt may be noted that group size varies.
There areoften several small groups with just 2 or 3 words,but sometimes as many as 10-15 words make up agroup.
For large groups, even though they are notfully consistent, the words tend to be drawn fromtwo or three synsets.7 ConclusionSo far we have performed a relatively limited num-ber of tests of the method.
Those tests indi-cate that semantic mirroring coupled with spectralgraph partitioning is a useful method for comput-ing word senses, which can be developed furtherusing refined graph theoretic and linguistic tech-niques in conjunction.8 Future workThere is room for many more investigations of theapproach outlined in this paper.
We would liketo explore the possibility to have a vertex (word)belong to multiple synsets, instead of having dis-crete cuts between synsets.
In the present solu-tion a vertex belongs to only one partition of agraph, making it impossible to having the sameword belong to several synsets.
We would alsolike to investigate the properties of graphs to seewhether it is possible to automatically measurehow close a seed word is to a particular synset.Furthermore, more thorough evaluations of largerdata sets would give us more information on howto combine similar synsets which were generatedfrom distinct seed words and explore more com-plex semantic fields.
In our future research we willtest the method also on other lexica, and performexperiments with the different tolerances involved.We will also perform extensive tests assessing theresults using a panel of human evaluators.106ReferencesDaniel A. Spielman.
2009.
Spectral Graph theory.Lecture notes.Daniel A. Spielman, S. -H. Teng.
2006.
Spectral par-titioning works: Planar graphs and finite elementmeshes.
Elsevier Inc.Diab, M. Resnik, P. 2002.
An Unsupervised Methodfor Word Sense Tagging using Parallel Corpora.Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics.
255-262.Fan R. K. Chung.
1997.
Spectral Graph Theory.American Mathematical Society, Providence, RhodeIsland.H.
Dyvik.
1998.
A Translational Basis for Semantics.In: Stig Johansson and Signe Oksefjell (eds.
): Cor-pora and Crosslinguistic Research: Theory, Methodand Case Studies, pp.
51-86.
Rodopi.H.
Dyvik.
2005.
Translations as a Semantic Knowl-edge Source.
Proceedings of the Second Baltic Con-ference on Human Language Technologies, Tallinn.Nancy Ide.
2000.
Cross-lingual sense determination:Can it work?
Computers and the Humanities: Spe-cial issue on SENSEVAL, 34:223?234.Philip Resnik , David Yarowsky.
Distinguishing sys-tems and distinguishing senses: new evaluationmethods for Word Sense Disambiguation Natu-ral Language Engineering, v.5 n.2, p.113-133, June1999107
