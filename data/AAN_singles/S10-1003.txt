Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15?20,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsSemEval-2010 Task 3: Cross-Lingual Word SenseDisambiguationEls Lefever1 ,2and Veronique Hoste1 ,21LT3, Language and Translation Technology Team, University College Ghent, Belgium2Department of Applied Mathematics and Computer Science, Ghent University, Belgium{Els.Lefever,Veronique.Hoste}@hogent.beAbstractThe goal of this task is to evaluatethe feasibility of multilingual WSD ona newly developed multilingual lexi-cal sample data set.
Participants wereasked to automatically determine thecontextually appropriate translation ofa given English noun in five languages,viz.
Dutch, German, Italian, Spanishand French.
This paper reports on thesixteen submissions from the five dif-ferent participating teams.1 IntroductionWord Sense Disambiguation, the task of se-lecting the correct sense of an ambiguous wordin a given context, is a well-researched NLPproblem (see for example Agirre and Edmonds(2006) and Navigli (2009)), largely boostedby the various Senseval and SemEval editions.The SemEval-2010 Cross-lingual Word SenseDisambiguation task focuses on two bottle-necks in current WSD research, namely thescarcity of sense inventories and sense-taggedcorpora (especially for languages other thanEnglish) and the growing tendency to eval-uate the performance of WSD systems in areal application such as machine translationand cross-language information retrieval (seefor example Agirre et al (2007)).The Cross-lingual WSD task aims at the de-velopment of a multilingual data set to test thefeasibility of multilingual WSD.
Many studieshave already shown the validity of this cross-lingual evidence idea (Gale et al, 1993; Ide etal., 2002; Ng et al, 2003; Apidianaki, 2009),but until now no benchmark data sets havebeen available.
For the SemEval-2010 compe-tition we developed (i) a sense inventory inwhich the sense distinctions were extractedfrom the multilingual corpus Europarl1and(ii) a data set in which the ambiguous wordswere annotated with the senses from the mul-tilingual sense inventory.
The Cross-LingualWSD task is a lexical sample task for Englishnouns, in which the word senses are made up ofthe translations in five languages, viz.
Dutch,French, Italian, Spanish and German.
Boththe sense inventory and the annotated dataset were constructed for a sample of 25 nouns.The data set was divided into a trial set of 5ambiguous nouns and a test set of 20 nouns.The participants had to automatically deter-mine the contextually appropriate translationfor a given English noun in each or a subsetof the five target languages.
Only translationspresent in Europarl were considered as validtranslations.The remainder of this article is organized asfollows.
Section 2 focuses on the task descrip-tion and gives a short overview of the construc-tion of the sense inventory and the annotationof the benchmark data set with the senses fromthe multilingual sense inventory.
Section 3clarifies the scoring metrics and presents twofrequency-based baselines.
The participatingsystems are presented in Section 4, while theresults of the task are discussed in Section 5.Section 6 concludes this paper.2 Task setup2.1 Data setsTwo types of data sets were used in theCross-lingual WSD task: (a) a parallel corpuson the basis of which the gold standard senseinventory was created and (b) a collection ofEnglish sentences containing the lexical sam-ple words annotated with their contextuallyappropriate translations in five languages.1http://www.statmt.org/europarl/15Below, we provide a short summary of thecomplete data construction process.
For amore detailed description, we refer to Lefeverand Hoste (2009; 2010).The gold standard sense inventory wasderived from the Europarl parallel corpus2,which is extracted from the proceedings of theEuropean Parliament (Koehn, 2005).
We se-lected 6 languages from the 11 European lan-guages represented in the corpus, viz.
English(our target language), Dutch, French, Ger-man, Italian and Spanish.
All data were al-ready sentence-aligned using a tool based onthe Gale and Church (1991) algorithm, whichwas part of the Europarl corpus.
We only con-sidered the 1-1 sentence alignments betweenEnglish and the five other languages.
Thesesentence alignments were made available tothe task participants for the five trial words.The sense inventory extracted from the paral-lel data set (Section 2.2) was used to annotatethe sentences in the trial set and the test set,which were extracted from the JRC-ACQUISMultilingual Parallel Corpus3and BNC4.2.2 Creation of the sense inventoryTwo steps were taken to obtain a multilingualsense inventory: (1) word alignment on thesentences to find the set of possible transla-tions for the set of ambiguous nouns and (2)clustering by meaning (per target word) of theresulting translations.GIZA++ (Och and Ney, 2003) was used togenerate the initial word alignments, whichwere manually verified by certified translatorsin all six involved languages.
The human an-notators were asked to assign a ?NULL?
linkto words for which no valid translation couldbe identified.
Furthermore, they were alsoasked to provide extra information on com-pound translations (e.g.
the Dutch word In-vesteringsbank as a translation of the Englishmultiword Investment Bank), fuzzy links, ortarget words with a different PoS (e.g.
the verbto bank).The manually verified translations wereclustered by meaning by one annotator.
Inorder to do so, the translations were linked2http://www.statmt.org/europarl/3http://wt.jrc.it/lt/Acquis/4http://www.natcorp.ox.ac.uk/across languages on the basis of uniquesentence IDs.
After the selection of allunique translation combinations, the transla-tions were grouped into clusters.
The clus-ters were organized in two levels, in whichthe top level reflects the main sense categories(e.g.
for the word coach we have (1) (sports)manager, (2) bus, (3) carriage and (4) partof a train), and the subclusters represent thefiner sense distinctions.
Translations that cor-respond to English multiword units were iden-tified and in case of non-apparent compounds,i.e.
compounds which are not marked with a?-?, the different compound parts were sepa-rated by ??
in the clustering file (e.g.
the Ger-man Post??kutsche).
All clustered translationswere also manually lemmatized.2.3 Sense annotation of the test dataThe resulting sense inventory was used to an-notate the sentences in the trial set (20 sen-tences per ambiguous word) and the test set(50 sentences per ambiguous word).
In total,1100 sentences were annotated.
The annota-tors were asked to (a) pick the contextually ap-propriate sense cluster and to (b) choose theirthree preferred translations from this cluster.In case they were not able to find three ap-propriate translations, they were also allowedto provide fewer.
These potentially differ-ent translations were used to assign frequencyweights (shown in example (2)) to the goldstandard translations per sentence.
The ex-ample (1) below shows the annotation result inboth German and Dutch for an English sourcesentence containing coach.
(1) SENTENCE 12.
STRANGELY , the na-tional coach of the Irish teams down theyears has had little direct contact with thefour provincial coaches .German 1: NationaltrainerGerman 2: TrainerGerman 3: CoachDutch 1: trainerDutch 2: coachDutch 3: voetbaltrainerFor each instance, the gold standard thatresults from the manual annotation containsa set of translations that are enriched with16frequency information.
The format of boththe input file and gold standard is similar tothe format that will be used for the Sem-Eval Cross-Lingual Lexical Substitution task(Sinha and Mihalcea, 2009).
The followingexample illustrates the six-language gold stan-dard format for the trial sentence in (1).
Thefirst field contains the target word, PoS-tagand language code, the second field containsthe sentence ID and the third field contains thegold standard translations in the target lan-guage, enriched with their frequency weight:(2) coach.n.nl 12 :: coach 3; speler-trainer 1;trainer 3; voetbaltrainer 1;coach.n.fr 12 :: capitaine 1; entra?
?neur 3;coach.n.de 12 :: Coach 1; Fu?baltrainer 1;Nationaltrainer 2; Trainer 3;coach.n.it 12 :: allenatore 3;coach.n.es 12 :: entrenador 3;3 Evaluation3.1 ScoringTo score the participating systems, we use anevaluation scheme which is inspired by theEnglish lexical substitution task in SemEval2007 (McCarthy and Navigli, 2007).
We per-form both a best result evaluation and a morerelaxed evaluation for the top five results.
Theevaluation is performed using precision and re-call (Prec and Rec in the equations below),and Mode precision (MP) and Mode recall(MR), where we calculate precision and re-call against the translation that is preferred bythe majority of annotators, provided that onetranslation is more frequent than the others.For the precision and recall formula we usethe following variables.
Let H be the set ofannotators, T the set of test items and hitheset of responses for an item i ?
T for annota-tor h ?
H. For each i ?
T we calculate themode (mi) which corresponds to the transla-tion with the highest frequency weight.
Fora detailed overview of the MPand MRcal-culations, we refer to McCarthy and Navigli(2007).
Let A be the set of items from T (andTM) where the system provides at least oneanswer and ai: i ?
A the set of guesses fromthe system for item i.
For each i, we calculatethe multiset union (Hi) for all hifor all h ?
Hand for each unique type (res) in Hithat hasan associated frequency (freqres).
In order toassign frequency weights to our gold standardtranslations, we asked our human annotatorsto indicate their top 3 translations, which en-ables us to also obtain meaningful associatedfrequencies (freqres) viz.
?1?
in case a transla-tion is picked by 1 annotator, ?2?
if picked bytwo annotators and ?3?
if chosen by all threeannotators.Best result evaluation For the best re-sult evaluation, systems can propose as manyguesses as the system believes are correct, butthe resulting score is divided by the number ofguesses.
In this way, systems that output a lotof guesses are not favoured.Prec =?ai:i?A?res?aifreqres|ai||Hi||A|(1)Rec =?ai:i?T?res?aifreqres|ai||Hi||T |(2)Out-of-five (Oof) evaluation For themore relaxed evaluation, systems can proposeup to five guesses.
For this evaluation, theresulting score is not divided by the numberof guesses.Prec =?ai:i?A?res?aifreqres|Hi||A|(3)Rec =?ai:i?T?res?aifreqres|Hi||T |(4)3.2 BaselinesWe produced two frequency-based baselines:1.
For the Best result evaluation, we selectthe most frequent lemmatized translationthat results from the automated wordalignment process (GIZA++).2.
For the Out-of-five or more relaxed eval-uation, we select the five most fre-quent (lemmatized) translations that re-sult from the GIZA++ alignment.Table 1 shows the baselines for the Bestevaluation, while Table 2 gives an overviewper language of the baselines for the Out-of-five evaluation.17Prec Rec MPMRSpanish 18.36 18.36 23.38 23.38French 20.71 20.71 15.21 15.21Italian 14.03 14.03 11.23 11.23Dutch 15.69 15.69 8.71 8.71German 13.16 13.16 6.95 6.95Table 1: Best BaselinesPrec Rec MPMRSpanish 48.41 48.41 42.62 42.62French 45.99 45.99 36.45 36.45Italian 34.51 34.51 29.70 29.70Dutch 37.43 37.43 24.58 24.58German 32.89 32.89 29.80 29.80Table 2: Out-of-five Baselines4 SystemsWe received sixteen submissions from five dif-ferent participating teams.
One group tack-led all five target languages, whereas the othergroups focused on four (one team), two (oneteam) or one (two teams) target language(s).For both the best and the Out-of-five evalua-tion tasks, there were between three and sevenparticipating systems per language.The OWNS system identifies the nearestneighbors of the test instances from the train-ing data using a pairwise similarity measure(weighted sum of the word overlap and se-mantic overlap between two sentences).
Theyuse WordNet similarity measures as an ad-ditional information source, while the otherteams merely rely on parallel corpora to ex-tract all lexical information.
The UvT-WSDsystems use a k-nearest neighbour classifierin the form of one word expert per lemma?Part-of-Speech pair to be disambiguated.
Theclassifier takes as input a variety of localand global context features.
Both the FCC-WSD and T3-COLEUR systems use bilingualtranslation probability tables that are derivedfrom the Europarl corpus.
The FCC-WSDsystem uses a Naive Bayes classifier, whilethe T3-COLEUR system uses an unsupervisedgraph-based method.
Finally, the UHD sys-tems build for each target word a multilin-gual co-occurrence graph based on the targetword?s aligned contexts found in parallel cor-pora.
The cross-lingual nodes are first linkedby translation edges, that are labeled with thetranslations of the target word in the corre-sponding contexts.
The graph is transformedinto a minimum spanning tree which is usedto select the most relevant words in context todisambiguate a given test instance.5 ResultsFor the system evaluation results, we showprecision (Prec), recall (Rec), Mode precision(MP) and Mode recall (MR).
We ranked allsystem results according to recall, as was donefor the Lexical Substitution task.
Table 3shows the system ranking on the best task,while Table 4 shows the results for the Ooftask.Prec Rec MPMRSpanishUvT-v 23.42 24.98 24.98 24.98UvT-g 19.92 19.92 24.17 24.17T3-COLEUR 19.78 19.59 24.59 24.59UHD-1 20.48 16.33 28.48 22.19UHD-2 20.2 16.09 28.18 22.65FCC-WSD1 15.09 15.09 14.31 14.31FCC-WSD3 14.43 14.43 13.41 13.41FrenchT3-COLEUR 21.96 21.73 16.15 15.93UHD-2 20.93 16.65 17.78 14.15UHD-1 20.22 16.21 17.59 14.56OWNS2 16.05 16.05 14.21 14.21OWNS1 16.05 16.05 14.21 14.21OWNS3 12.53 12.53 14.21 14.21OWNS4 10.49 10.49 14.21 14.21ItalianT3-COLEUR 15.55 15.4 10.2 10.12UHD-2 16.28 13.03 14.89 9.46UHD-1 15.94 12.78 12.34 8.48DutchUvT-v 17.7 17.7 12.05 12.05UvT-g 15.93 15.93 10.54 10.54T3-COLEUR 10.71 10.56 6.18 6.16GermanT3-COLEUR 13.79 13.63 8.1 8.1UHD-1 12.2 9.32 11.05 7.78UHD-2 12.03 9.23 12.91 9.22Table 3: Best System ResultsBeating the baseline seems to be quite chal-lenging for this WSD task.
While the best sys-tems outperform the baseline for the best task,18Prec Rec MPMRSpanishUvT-g 43.12 43.12 43.94 43.94UvT-v 42.17 42.17 40.62 40.62FCC-WSD2 40.76 40.76 44.84 44.84FCC-WSD4 38.46 38.46 39.49 39.49T3-COLEUR 35.84 35.46 39.01 38.78UHD-1 38.78 31.81 40.68 32.38UHD-2 37.74 31.3 39.09 32.05FrenchT3-COLEUR 49.44 48.96 42.13 41.77OWNS1 43.11 43.11 38.29 38.29OWNS2 38.74 38.74 37.73 37.73UHD-1 39.06 32 37.00 26.79UHD-2 37.92 31.38 37.66 27.08ItalianT3-COLEUR 40.7 40.34 38.99 38.70UHD-1 33.72 27.49 27.54 21.81UHD-2 32.68 27.42 29.82 23.20DutchUvT-v 34.95 34.95 24.62 24.62UvT-g 34.92 34.92 19.72 19.72T3-COLEUR 21.47 21.27 12.05 12.03GermanT3-COLEUR 33.21 32.82 33.60 33.56UHD-1 27.62 22.82 25.68 21.16UHD-2 27.24 22.55 27.19 22.30Table 4: Out-of-five System Resultsthis is not always the case for the Out-of-fivetask.
This is not surprising though, as the Oofbaseline contains the five most frequent Eu-roparl translations.
As a consequence, thesetranslations usually contain the most frequenttranslations from different sense clusters, andin addition they also contain the most generictranslation that often covers multiple senses ofthe target word.The best results are achieved by the UvT-WSD (Spanish, Dutch) and ColEur (French,Italian and German) systems.
An interest-ing feature that these systems have in com-mon, is that they extract all lexical informa-tion from the parallel corpus at hand, and donot need any additional data sources.
As aconsequence, the systems can easily be appliedto other languages as well.
This is clearly il-lustrated by the ColEur system, that partici-pated for all supported languages, and outper-formed the other systems for three of the fivelanguages.In general, we notice that Spanish andFrench have the highest scores, followed byItalian, whereas Dutch and German seem to bemore challenging.
The same observation canbe made for both the Oof and Best results,except for Italian that performs worse thanDutch for the latter.
However, given the lowparticipation rate for Italian, we do not havesufficient information to explain this differentbehaviour on the two tasks.
The discrepancybetween the performance figures for Spanishand French on the one hand, and German andDutch on the other hand, seems more readilyexplicable.
A likely explanation could be thenumber of classes (or translations) the systemshave to choose from.
As both Dutch and Ger-man are characterized by a rich compound-ing system, these compound translations alsoresult in a higher number of different trans-lations.
Figure 1 illustrates this by listingthe number of different translations (or classesin the context of WSD) for all trial and testwords.
As a result, the broader set of trans-lations makes the WSD task, that consistsin choosing the most appropriate translationfrom all possible translations for a given in-stance, more complicated for Dutch and Ger-man.6 Concluding remarksWe believe that the Cross-lingual Word SenseDisambiguation task is an interesting contri-bution to the domain, as it attempts to ad-dress two WSD problems which have receiveda lot of attention lately, namely (1) the scarcityof hand-crafted sense inventories and sense-tagged corpora and (2) the need to make WSDmore suited for practical applications.The system results lead to the following ob-servations.
Firstly, languages which make ex-tensive use of single word compounds seemharder to tackle, which is also reflected in thebaseline scores.
A possible explanation forthis phenomenon could lie in the number oftranslations the systems have to choose from.Secondly, it is striking that the systems withthe highest performance solely rely on paral-lel corpora as a source of information.
Thiswould seem very promising for future multi-lingual WSD research; by eliminating the need19Figure 1: Number of different translations per word for Dutch, French, Spanish, Italian andGerman.for external information sources, these sys-tems present a more flexible and language-independent approach to WSD.ReferencesE.
Agirre and P. Edmonds, editors.
2006.
WordSense Disambiguation.
Text, Speech and Lan-guage Technology.
Springer, Dordrecht.E.
Agirre, B. Magnini, O. Lopez de Lacalle,A.
Otegi, G. Rigau, and P. Vossen.
2007.Semeval-2007 task01: Evaluating wsd on cross-language information retrieval.
In Proceedingsof CLEF 2007 Workshop, pp.
908 - 917.
ISSN:1818-8044.
ISBN: 2-912335-31-0.M.
Apidianaki.
2009.
Data-driven semantic anal-ysis for multilingual wsd and lexical selection intranslation.
In Proceedings of the 12th Confer-ence of the European Chapter of the Associationfor Computational Linguistics (EACL), Athens,Greece.W.A.
Gale and K.W.
Church.
1991.
A programfor aligning sentences in bilingual corpora.
InComputational Linguistics, pages 177?184.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1993.A method for disambiguating word senses in alarge corpus.
In Computers and the Humanities,volume 26, pages 415?439.N.
Ide, T. Erjavec, and D. Tufis.
2002.
Sense dis-crimination with parallel corpora.
In Proceed-ings of ACL Workshop on Word Sense Disam-biguation: Recent Successes and Future Direc-tions, pages 54?60.P.
Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedingsof the MT Summit.E.
Lefever and V. Hoste.
2009.
Semeval-2010task 3: Cross-lingual word sense disambigua-tion.
In Proceedings of the NAACL-HLT 2009Workshop: SEW-2009 - Semantic Evaluations,pages 82?87, Boulder, Colorado.E.
Lefever and V. Hoste.
2010.
Construction of abenchmark data set for cross-lingual word sensedisambiguation.
In Proceedings of the seventhinternational conference on Language Resourcesand Evaluation., Malta.D.
McCarthy and R. Navigli.
2007.
Semeval-2007task 10: English lexical substitution task.
InProceedings of the 4th International Workshopon Semantic Evaluations (SemEval-2007), pages48?53, Prague, Czech Republic.R.
Navigli.
2009.
Word sense disambiguation: asurvey.
In ACM Computing Surveys, volume 41,pages 1?69.H.T.
Ng, B. Wang, and Y.S.
Chan.
2003.
Exploit-ing parallel texts for word sense disambiguation:An empirical study.
In Proceedings of the 41stAnnual Meeting of the Association for Compu-tational Linguistics, pages 455?462, Santa Cruz.F.J.
Och and H. Ney.
2003.
A systematic com-parison of various statistical alignment models.Computational Linguistics, 29(1):19?51.McCarthy D. Sinha, R. D. and R. Mihalcea.
2009.Semeval-2010 task 2: Cross-lingual lexical sub-stitution.
In Proceedings of the NAACL-HLT2009 Workshop: SEW-2009 - Semantic Evalua-tions, Boulder, Colorado.20
