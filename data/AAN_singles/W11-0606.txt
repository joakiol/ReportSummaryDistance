Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 49?57,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsExploring the relationship between learnability and linguistic universalsAnna N. Rafferty (rafferty@cs.berkeley.edu)Computer Science Division, University of California, Berkeley, CA 94720 USAThomas L. Griffiths (tom griffiths@berkeley.edu)Department of Psychology, University of California, Berkeley, CA 94720 USAMarc Ettlinger (marc@northwestern.edu)Department of Communication Sciences and DisordersNorthwestern University, Evanston, IL 60208 USAAbstractGreater learnability has been offered as an ex-planation as to why certain properties appearin human languages more frequently than oth-ers.
Languages with greater learnability aremore likely to be accurately transmitted fromone generation of learners to the next.
We ex-plore whether such a learnability bias is suffi-cient to result in a property becoming preva-lent across languages by formalizing languagetransmission using a linear model.
We thenexamine the outcome of repeated transmissionof languages using a mathematical analysis, acomputer simulation, and an experiment withhuman participants, and show several ways inwhich greater learnability may not result in aproperty becoming prevalent.
Both the waysin which transmission failures occur and therelative number of languages with and with-out a property can affect whether the rela-tionship between learnability and prevalenceholds.
Our results show that simply findinga learnability bias is not sufficient to explainwhy a particular property is a linguistic univer-sal, or even frequent among human languages.1 IntroductionA comparison of languages around the world revealsthat certain properties are far more frequent thanothers, which are taken to reflect linguistic univer-sals (Greenberg, 1963; Comrie, 1981; Croft, 2002).Understanding the origins of linguistic universals isan important project for linguistics, and understand-ing how they relate to human cognitive processesis an important project for cognitive science.
Oneprominent explanation for the existence of these pat-terns is the presence of cognitive biases that makecertain properties of language more easily learnedthan others (Slobin, 1973; Wilson, 2003; Finley &Badecker, 2007; Wilson, 2006).
Under this hypothe-sis, certain properties are common across languagesbecause they are more easily learned than others (alearnability bias) and are therefore more likely to bemaintained when a language is passed from one gen-eration to the next.
These universals generally reflecttendencies, rather than properties that are present ineach and every language (Croft, 2002).Recent work in psycholinguistics has providedsupport for a relationship between learnability bi-ases and the properties that are prevalent in humanlanguages.
A number of studies have shown that cer-tain common phonological patterns, such as vowelharmony, voicing agreement and final devoicing are,indeed, more learnable than other unattested patterns(Finley & Badecker, 2007; Moreton, 2008; Becker,Ketrez, & Nevins, 2011).
Based on these findings,it is tempting to argue that learnability biases alonemight account for the prevalence of these proper-ties in human languages.
However, this argumentassumes that more accurate learning of a languagewith a certain property is sufficient for that propertyto become widespread across languages and doesnot account for why a property might be prevalentbut not universal across languages.In this paper, we examine the assumption thatgreater learnability is sufficient for a property to be-come prevalent.
We formalize language transmis-sion using a simple linear model, and then show twobasic scenarios in which greater learnability for aparticular language does not result in that languagebecoming prevalent.
We first perform a mathemat-ical analysis to show that one way this can occuris for errors in transmission to favor particular lan-49guages over others.
We next use a simulation toshow another scenario in which greater learnabil-ity can fail to result in a dominant pattern: whenthe number of alternative languages is large.
Weconduct two experiments with human participants toillustrate the occurrence of this second scenario inthe case of a particular property of human language,vowel harmony.2 Linking Learnability and TransmissionLanguages change over time due to transmissionfrom generation to generation (e.g., Labov, 2001).Our goal is to understand how long-term trends oflanguage change are related to cognitive, perceptual,and production biases observed in a single instanceof transmission.
We begin by formalizing transmis-sion using a general mathematical model in orderto uncover what long term trends emerge given thatcertain languages are more likely to be accuratelytransmitted than others.We use a linear model of cultural transmission,in which it is assumed that each person learns a lan-guage from utterances produced by one person in theprevious generation.
This linear model of transmis-sion has many specific instantiations in the literatureon language evolution, such as the iterated learn-ing model (Kirby, 2001; Griffiths & Kalish, 2007)or the replicator dynamics (Schuster & Sigmund,1983; Komarova & Nowak, 2003).
To specify thismodel, we first define the set of possible languages,denoted H. Each element h ?
H is one possible lan-guage.
Transmission occurs when a new member ofthe population receives linguistic data (a set of utter-ances) from another member of the population andlearns a language h ?
H. We assume transmissionoccurs only from one person to another person, andthat each person learns only one language.
For ex-ample, someone who knows language j might speakto another member of the population, and based onhearing those utterances, the learner might also learnthe language j. Alternatively, the learner might learnanother language: The learner might not have heardenough language to fully specify j as the languageor might have misheard something, and thus simplyinfers another language i that is consistent with thedata she or he heard.
More generally, we assumethat for all i, j ?
H, qi j is the probability that some-Q =?
?0.90 0.05 0.070.04 0.76 0.080.06 0.19 0.85??
(a)(b) (c)?
=??0.390.200.41?
?This process may continue indefinitely, with the tth learner re-ceiving the output of the (t ?1)th learner.
The iterated learn-ing models we analyze make the simplifying assumptionsthat language evolution occurs in only one direction (previ-ous generations do not change their hypotheses based on thedata produced by future generations) and that each learner re-ceives input from only one previous learner.
We first charac-terize how learning occurs, independent of specific represen-tation, and then give a more detailed d scription of the formof these hypotheses and data.Our models assume that learners represent (or act as if theyrepresent) the degree to which const ints predispose them tocertain hypotheses about language through a probability dis-tribution over hypotheses, and that they combine these pre-dispositions with information from the dat using Bayesianinference.
Starting with a prior distribution over hypothesesp(h) for all hypotheses h in a hypothesis space H, the pos-terior distribution over hypotheses given data d is giv n byBayes?
rule,p(h|d) =p(d|h)p(h)?h?
?H p(d|h?)p(h?
)(1)where the likelihood p(d|h) indicates the probability of see-ing d under hypothesis h. The learners thus shape the lan-guage they are learning through their own bias s in the formof the prior probabilities: the prior p(h) incorporates the hu-man learning constraints.
These probabilities might, for ex-ample, tend to favor lword forms wi h alternating c so ant-vowel phonemes.
We assume that learners?
expectationsabout the distribution of the data given the hypothesis areconsistent with the actual distribution (i.e.
that the probabil-ity of the previous learner generating data d from hypothesish matches the likelihood function p(d|h)).
Finally, we as-sume that learners choose a hypothesis by sampling from theposterior distribution (although we consider other ways of se-lecting hypotheses in the Discussion section).1The analyses we present in this paper are based on the ob-servation that iterated learning defines a Markov chain.
AMarkov chain is a sequence of random variables Xt such thateach Xt is independent of all preceding variables when condi-tioned on the immediately preceding variable, Xt?1.
Thus,p(xt |x1, .
.
.
,xt?1) = p(xt |xt?1).
There are several ways ofreducing iterated learning to a Markov chain (Griffiths &Kalish, 2007).
We will focus on the Markov chain on hy-potheses, where transitions from one state to another occureach generation: the tth learner assumes the data were gen-erated by ht , where these data are dependent only on thehypothesis ht?1 chosen by the previous learner.
The transi-tion probabilities for this Markov chain are obtained by sum-ming over the data from the previous time step di?1, withp(ht |ht?1) = ?di?1 p(ht |di?1)p(di?1|ht?1) (see Figure 1).Identifying iterated learning as a Markov chain allows us todraw on mathematical results concerning the convergence of1Note that these various probabilities form our model of thelearners.
Learners need not actually hold them explicitly, nor per-form the exact computations, provided that they act as if they do.data ...(a)(c)...(b)...datahypothesis hypothesisdata?d p(h|d)p(d|h)d0 h2d1h1p(h|d) p(d|h) p(h|d) p(d|h)d2h2h1?d p(h|d)p(d|h)Figure 1: Language evolution by iterated learning.
(a) Eachlearner sees data, forms a hypothesis, and generates the dataprovided to the next learner.
(b) The underlying stochasticprocess, with dt and ht being the data generated by the tthlearner and the hypothesis selected by that learner respec-tively.
(c) We consider the Markov chain over hypothesesformed by summing over the data variables.
All learnersshare the same prior p(h), and each learner assumes the inputdata were created using the same p(d|h).Markov chains.
In particular, Markov chains can converge toa stationary distribution, meaning that after some number ofgenerations t, the marginal probability that a variable Xt takesvalue xt becomes fixed and independent of the value of thefirst variable in the chain (Norris, 1997).
Intuitively, the sta-tionary distribution is a distribution over states in which theprobability of each state is not affected by further iterationsof the Markov chain; in our case, the probability that a learnerlearns a specific grammar at time t is equal to the probabilityof any future learner learning that grammar.
The stationarydistribution is thus an equilibrium state that iterated learn-ing will eventually reach, regardless of the hypothesis of thefirst ancestral learner, provided simple technical conditionsare satisfied (see Griffiths & Kalish, 2007, for details).Previous work has shown that the stationary distributionof the Markov chain defined by Bayesian learners samplingfrom the posterior is the learners?
prior distribution over hy-potheses, p(h) (Griffiths & Kalish, 2007).
These results illus-trate how constraints on learning can influence the languagesthat people come to speak, indicating that it is possible foriterated learning to converge to an equilibrium that is deter-mined by these constraints and independent of the languagespoken by the first learner in the chain.However, characterizing the stationary distribution of iter-ated learning still leaves open the question of whether enoughgenerations of learning have occurred for convergence to thisdistribution to have taken place in human languages.
To un-derstand the degree to which linguistic universals reflect con-straints on learning rather than descent from a common ances-tor, it is necessary to establish bounds on convergence time.Previous work has identified factors influencing the rate ofconvergence in very simple settings (e.g., Griffiths & Kalish,2007).
Our contribution is to provide analytic upper boundson the convergence time of iterated learning with relativelycomplex representations of the structure of a language thatare consistent with linguistic theories.Qlanguage languagetFigure 1: (a) A general model of the cultural transmis-sion of languages.
A language is passed from one learnerto another, and the matrix Q encodes the probability alearner will learn a particular language i from someonewho knows language j.
(b) An example transition matrixQ with three states.
(c) The solution to the eigenvectorequation Qpi = pi for this transition matrix.
pi gives theequilibrium probability that a learner will learn a particu-lar language when languages are transmitted via a processthat has transition matrix Q.one will learn language i from someone who knowslanguage j.
These can be encoded in a transitionmatrix Q where the (i, j)th entry of the matrix cor-responds to qi j (see Figure 1).Using this framework, we can formally definelearnability biases and determine whether learn-ability bias for some property necessarily impliesthat this property will be present in the majorityof languages.
As mentioned previously, we definelearnability bias to mean that one type of languageis more likely to be transmitted accurately to the nextgeneration than another; this is similar to the notionof ?cognitive bias?
discussed in Wilson (2003) andis what is tested in experiments.
Formally, a learn-ability bias for some language i over some otherlanguage j means that qii > q j j.
For example, onemight expose one group of learners to language i andanother group to language j.
If more le rners in thefirst group accurately learned the language they wereexposed to, this would indicate a learnability bias forlanguage i over language j.We can extend the idea of a learnability bias toa property of a language, rather than a specific lan-guage, by applying a similar definition to sets of lan-guages.
Imagine there are two sets of languages, H1and H2.
These sets might be defined by classifying50all languages with a particular property in H1 andall languages without the property in H2.
One wayof defining a learnability bias that favors a particularproperty is for each language with that property to bemore likely to be transmitted successfully than eachlanguage without that property.
That is, for all pos-sible pairs i ?
H1 and j ?
H2, qii > q j j.
This wouldindicate a general learnability bias for languages inH1 over languages in H2.Using this definition of a learnability bias, we candetermine whether such a bias is sufficient to estab-lish that the property will be present in the majorityof languages.
That is, if H1 denotes the languageswith the property of interest, we want to determinewhether a learnability bias for languages in H1 im-plies that after many generations, the majority of thelanguages in the population will be in H1 and notin H2.
We can determine the consequences of manyinstances of language transmission in this model byappealing to existing results on the equilibrium ofthis linear dynamical system.
As mentioned above,this linear transmission model is related to two kindsof models that have been used to study languageevolution: If we assume that learners are organizedin a chain, this linear model is called iterated learn-ing (Kirby, 2001); alternatively, if we assume thatthere are an infinite number of learners in the pop-ulation, the model is called the replicator dynam-ics (Schuster & Sigmund, 1983).
In either case, theprobability that a learner will learn language h, as-suming the population has reached equilibrium, isgiven by the solution to the eigenvector equationQpi = pi, normalized such that ?ni=1pii = 1 (for de-tails, see Griffiths & Kalish, 2007).
For languages inH1 to occur the majority of the time, it thus must bethe case that ?h?H1 pih > ?h?H2 pih.We can now identify one context in which a learn-ability bias is not sufficient to ensure that a propertywill appear in the majority of languages.
Considerthe example transition matrix Q shown in Figure 1(b).
Let H1 = {s1} and H2 = {s2,s3}, where eachstate si represents a distinct language.
We have thatq11 > qii for all i ?
H2: each state in H2 has a lowerself transition probability than state s1, the only statein H1.
Thus, we have a learnability bias for states1 over all states in H2.
However, the eigenvectorpi shown in Figure 1 (c) indicates that the equilib-rium of this system, which will be reached after lan-guages are transmitted from person to person manytimes, favors state s3 over the other states.
Overall,?h?H1 pih = 0.39 while ?h?H2 pih = 0.61: most of thelearners will learn a language in H2.1Intuitively, this result comes from the fact thattransmission failures tend to favor languages in H2.A learner who learns from someone who speaks alanguage i in H2 will rarely learn the language inH1, although she may learn a different language thani in H2.
This pattern of transmission failures over-whelms the learnability bias that the language in H1has over the languages in H2.
Note that this patternholds even given that q1i > qi1 for all i?H2, anothercommon criterion for a learnability bias.This result implies that if the linear transmissionmodel is an accurate model for understanding hu-man language evolution, then it is not sufficient tocompare how accurately languages are maintainedover a single generation in order to predict whattrends will emerge after many generations.
Instead,one must also look at what happens when languagesare not maintained accurately.
The ways in whichmutations occur may be as important as the relativefidelities of transmission in determining long termtrends.
When one only looks for a learnability bias,the rate of different mutations is not accounted for,leaving open the possibility that predictions aboutlong term trends will be incorrect.3 Simulating Language TransmissionIn the previous section, we used a simple lineartransmission model to identify one context in whicha learnability bias is not sufficient for languages witha certain property to become prevalent.
We now ex-plore a second context in which a learnability biasis not sufficient to guarantee that languages with aparticular property become prevalent, using a sim-ulation of language transmission.
We use an iter-ated learning model in which our representation oflanguage is inspired by the principles and parame-ters approach (Chomsky & Lasnik, 1993).
Rafferty,Griffiths, and Klein (2009) present a model similarto the one we consider here and show that compa-1While one might try to resolve this issue by collapsing alllanguages in H2 into a single state in the Markov chain, sucha transformation is possible only in cases where qi j = qik forall languages j,k ?
H2 and i /?
H2 (Burke & Rosenblatt, 1958;Kemeny & Snell, 1960).510 200 400 600 800 100005121024LanguageSamples From Transition Matrix0 256 512 768 1024035007000FrequencyLanguage FrequencyOther Target0500010000FrequencyRelative Frequency of Target Language0 200 400 600 800 100005121024Sample IterationLanguage0 256 512 768 1024350007000LanguageFrequencyOther Target0500010000Frequency?
= 0.6?
= 0.1Figure 2: Model results for the frequency of the target language based on adjusting the bias towards that hypothesis.The rows in the above figure correspond to two possible values of ?
; larger ?
results in a higher prior probability onthe target language.
The leftmost column shows 1,000 samples from the transition matrix, with black x marks corre-sponding to occurrences of the target language.
The middle column corresponds to the frequency of each language inthe full 10,000 samples; the rightmost bar in each figure corresponds to the target language.
The rightmost columnshows the frequency of the target language versus all other languages for the same 10,000 samples.rable results hold using other representations of lan-guage, such as those based on optimality theory.In order to define the transition matrix Q, we needto specify the process by which learners select a lan-guage.
We assume that learners are Bayesian, mean-ing that they infer a language h based on the data dthat they receive according to Bayes?
rule.
The pos-terior probability assigned to h after observing d isp(h|d) ?
p(d|h)p(h), where p(d|h) (the likelihood)indicates the probability of d being generated fromh, and p(h) (the prior) indicates the extent to whichthe learner was biased towards h before observing d.If we assume learners select hypotheses with proba-bility equal to their posterior probability, we obtaina transition matrix Q with entriesqi j = p(h(t+1) = i|h(t) = j)=?dp(h(t+1) = i|d)p(d|h(t) = j)where h(t) and h(t+1) are the languages of learners atiterations t and t +1 respectively.To represent languages, we use binary vectors oflength N. Each place corresponds to the setting fora particular parameter.
We consider one particularsetting of the parameters to be the target languageand include a learnability bias for this language inthe model; we then look at whether this languageis more prevalent than other languages after manytransmissions.
In the iterated learning model that weuse, learners are organized into a chain, with eachlearner learning from data generated by the previouslearner (Kirby, 2001).
The previous learner gener-ates k pieces of data that match her or his language.These pieces of data each specify the correct param-eter setting for one of the properties represented bythe binary vector.
The other N?k properties are leftunspecified in the data given to the next learner.In order to define the transition probability be-tween languages, we need to define the two termsin Bayes rule: the prior p(h) and the likelihoodp(d|h).
Intuitively, the prior probability distributionover languages corresponds to how much evidenceis required for the learner to learn each hypothesis.If one hypothesis has a very high prior probability,only a small amount of evidence will be required toconvince the learner that that hypothesis is the cor-rect one.
By controlling the prior probability of thetarget language versus the other languages, we canmanipulate the learnability bias for the target lan-guage.
We thus set the prior probability of the targetlanguage to ?
and then divide the remaining proba-bility mass of 1??
uniformly across all of the lan-guages (including the target language).
The param-eter ?
thus controls the strength of the learnabilitybias for the target language, but this language is al-ways favored for any ?
greater than 0.The likelihood p(d|h) reflects the probability thata given hypothesis h would produce data d. We as-sume d is a string of length N that contains 0s, 1s,and ?s.
A ???
in the ith position means that no in-formation was given about the ith property.
We alsoassume there is a probability ?
that the chosen lan-guage will not match the data at each position; thatis, with probability ?, the language chosen by the52learner will have a 1 in the ith spot if the data had a0 in that spot.
This gives:p(d|h) = ?Ni=1,di 6=?
?I(h`di)(1?
?
)I(h0di)where h ` di means that h has the same setting of theith property as di.Given these specifications for the prior and thelikelihood, we can calculate the 2N ?
2N transitionmatrix and sample from this matrix to simulate asequence of learners each learning a language fromthe utterances produced by the previous learner.
Welet N = 10 and k = 5.
As shown in Griffiths andKalish (2007), in this model ?
iterated learning withBayesian learners ?
the equilibrium pi is simply theprior distribution p(h).
The distribution over lan-guages is thus unaffected by the error parameter ?
;this parameter only affects the time to reach equilib-rium (Rafferty et al, 2009).
We present results using?
= 0.25.
Figure 2 shows how relative frequency ofthe target language is affected by changing the pa-rameter ?, using ?
= 0.6 and ?
= 0.1.
Frequenciesare based on taking 11,000 samples from the ma-trix and discarding the first 1,000 to ensure that thepopulation had reached equilibrium.The middle column of Figure 2 shows that thefrequency with which learners chose the target lan-guage was greater than that of the other languagesfor both values of ?.
This is consistent with thetarget language having a higher prior probabilitythan other languages.
However, depending on thestrength of the bias, this language may still not bechosen the majority of the time, as shown in therightmost column of Figure 2.
When ?
is large,its probability overwhelms that of its competitors.However, if ?
is relatively small, the combined fre-quencies of all other languages exceed that of thetarget language.
Thus, despite being favored by alearnability bias, the target language is not chosen bythe majority of learners.
Like the previous example,this simulation demonstrates that learnability biasesmay not always lead to accurate prediction of longterm trends.
More specifically, it highlights that onemust consider the size of the comparison set: If thereare many alternate possible languages, learners maytend to learn one of these languages even if someparticular language with a learnability bias is morefrequent than any other given individual language.4 Language Transmission in the LabWhile we have shown two scenarios in which a sim-ple linear transmission model does not predict thatlearnability biases will necessarily lead to linguisticuniversals, human learners are not necessarily con-sistent with this model and could follow a differ-ent pattern.
Thus, we conducted two experimentsto determine if the same dissociation between in-dividual bias and long-term change can be shownwhen teaching human learners an artificial gram-mar.
In Experiment 1, we establish a learnabilitybias for a linguistic pattern that is common in theworld?s languages over an arbitrary pattern.
In Ex-periment 2, we explore what happens when a lan-guage with the common pattern is transmitted mul-tiple times among learners in the lab.
Each learnerlearns a language and then produces data from thislanguage to teach the next learner.
By examining thelanguages that emerge after several transmissions,we will show that the learnability bias in Experi-ment 1 does not translate to the pattern becomingwidespread across the learned languages in Experi-ment 2.
This pattern is an instance of the scenarioin which the many alternative languages overwhelmthe language with the learnability bias.In our experiments, we use the property of vowelharmony.
Relatively common across the world?slanguages (van der Hulst & van de Weijer, 1995),vowel harmony is a linguistic pattern wherein thevowels in words in a language must share somephonological feature.
For example, in Turkish, theplural suffix is -lar in bash-lar ?heads?, but -ler inbebek-ler ?babies?
so as to adhere to the requirementthat words are front-back harmonic.
In the former,both vowels are back vowels and in the latter, bothvowels are front vowels.
Harmony is well-suited foruse in this case because English speakers have no fa-miliarity with vowel harmony from their native lan-guage input and because previous work has shownthat typologically attested vowel harmony patternsare generally more easily learned (Moreton, 2008;Finley & Badecker, 2009).5 Experiment 1: Establishing a Bias5.1 MethodsParticipants.
There were 40 participants whoreceived either monetary compensation or course53credit for their participation.
All were native speak-ers of English.Stimuli.
A native speaker of English was recordedsaying 160 CVCVC words.
Each word began withone of 80 CVC stems, twenty each with the vow-els /i/, /e/, /u/ and /o/ and random consonants.Each stem was recorded with both variants, or al-lomorphs, of a suffix, [it] and [ut].
Thus, half thewords were front-harmonic (e.g., pel-it, bis-it) andhalf were front-disharmonic (e.g., pel-ut, bis-ut).Procedure.
The procedure followed a modified arti-ficial grammar paradigm.
Participants were assignedto one of two conditions: the harmonic conditionor the height-front dependency condition, which isunattested.
In both conditions, participants were ex-posed in training to 40 words from the languagethey were learning.
In the harmonic condition, 40harmonic words were selected.
In the height-frontdependency condition, words were selected suchthat mid-vowel stems received the front vowel suffix(e.g., pel-it, bod-it) and high-vowel stems receivedthe back-vowel suffix (e.g., bis-ut, tug-ut).
This rulewas chosen arbitrarily from the space of possiblelanguages to test the hypothesis that vowel harmonywould have a learnability bias over other patterns.Participants were familiarized with the words inthe same way regardless of condition.
They weregiven alternating blocks of passive listening andblocks in which for each trial, two words wereplayed and they were required to choose which wordthey had previously heard.
In the forced choice tri-als, the choice was between a word that had beenplayed in the passive listening section and a wordwith the same prefix and the alternate allomorph.
Atotal of five blocks of 40 trials each were included intraining: three passive listening blocks with a forcedchoice block in between each.Following the training trials, participants com-pleted one block of 80 test trials.
On each testtrial, participants were asked to choose which oftwo words they thought was from the language theyhad learned in the training trials.
In each trial, thetwo words both had the same stem and differed inthe suffix.
40 of the test trials included words fromtraining, and 40 were generalization trials involvingnovel words.Height?Frontness Harmony00.51Proportion of GeneralizationsProportion of Generalizations Following Training Set RuleHeight?Frontness Harmony00.51Test AccuracyTest Accuracy by Training Set RuleFigure 3: Results for harmonic versus height-frontnessrule conditions.
By condition, there are significant dif-ferences in the proportion of generalizations followingthe rule (0.70 for harmony rule versus 0.57 for height-frontness rule, t(38) = 2.05, p < 0.05; left) and in testaccuracy (0.80 for harmony rule versus 0.68 for height-frontness rule, t(38) = 2.23, p< 0.05; right).5.2 ResultsAs shown in Figure 3, we found a learnability biasfor the harmonic language.
Learners had signifi-cantly greater accuracy in test when they learned thevowel harmonic language than when they learnedthe height-front dependency language (80% correctfor learners of the harmony rule versus 68% cor-rect for the height-frontness rule, t(38) = 2.23, p <0.05).
Additionally, 70% of generalizations madeby learners in the harmony rule condition followedthe harmonic rule while only 57% of generaliza-tions made by learners in the height-front depen-dency condition followed the height-frontness rule(t(38) = 2.05, p < 0.05).2 The result of these twophenomena was that the final languages produced bythe learners in the harmony condition had a greaterprevalence of harmonic words than the final lan-guages of learners in the height-frontness depen-dency had of adhering words.These results establish that the probability of tran-sitioning from a harmonic language to another lan-guage with a high proportion of harmonic wordsis higher than the probability of transitioning froma height-front dependency language to another lan-guage with a high proportion of adhering words.
In2For the second experiment, participants who had low ac-curacy (< 62.5% of previously heard words chosen in test as?from the language?)
were excluded.
Performing this exclusionin this experiment preserves the same results: Mean accuracyof 87% for the harmonic condition versus 73% for the height-front dependency condition (t(28) = 2.74, p< 0.025), and 77%mean proportion of generalizations following the rule for theharmonic condition versus 58% for the height-front dependencycondition (t(28) = 2.43, p< 0.025).
This exclusion criterion re-sulted in removing five participants from each condition.54terms of the transition matrix, this corresponds toq`harm,`harm > q`h-f,`h-f , where `harm is the set of lan-guages with a high proportion of harmonic wordsand `h-f is the set of languages with a high propor-tion of words that follow the height-frontness rule.In other words, the harmonic language is easier tolearn than the height-front dependency language.6 Experiment 2: Language Transmission6.1 MethodsParticipants.
There were a total of 104 partici-pants who received either monetary compensation orcourse credit for their participation.
All were nativespeakers of English.Stimuli.
The same stimuli were used as in Experi-ment 1.Procedure.
The procedure for this experiment wassimilar to Experiment 1, but the way that words werechosen for training differed.
For the first subject ineach chain, a total of 40 prefixes were selected atrandom, and based on the starting condition of thechain, the allophone for each prefix was selected.For example, for the 50% harmonic starting con-dition, 40 prefixes were chosen and of those pre-fixes, half were chosen to have the appropriate al-lophone to make the word harmonic and half werechosen to have the allophone to make the word non-harmonic.
For subsequent subjects in each chain,40 words were chosen at random from those wordswhich the previous subject had said was in the lan-guage.
In order to exclude subjects who had not ac-tually learned the language in training, subjects werenot included in the chain if their accuracy in test onpreviously seen words was below 62.5%; this is thelowest level of accuracy that is significantly differ-ent (binomial test, p < 0.05) from chance guessing.Chains were started at 100%, 75%, 50%, 25%, and0% harmonic.
One chain with 10 subjects was runfor each starting point except for 100%.
Four chainsof 10 subjects each were run at this starting point asthis is the point of most interest: given a learnabil-ity bias, does the percentage of harmonic words in alanguage remain consistently large?6.2 ResultsWhile Experiment 1 showed a learnability bias forthe harmonic language over an arbitrarily chosenlanguage, the iterated learning chains in Experiment2 did not favor the harmonic language.
As shownin Figure 4, all chains tended toward languages withapproximately 50% harmonic words, and after sev-eral generations, the chains that began with 100%harmonic words did not differ significantly from theother chains.
There is also no difference in accuracyon the harmonic items over time, as shown in Figure5.
This is empirical evidence that the pattern shownin simulation can also occur with human learners:One language is more accurately transmitted thanothers, but due to the large number of other possi-ble languages, this language does not predominateafter many transmissions.7 General DiscussionIn this paper, we formalized language transmissionusing a linear model in order to examine whethera learnability bias for some property of language issufficient for that property to become prevalent inhuman languages.
We showed two ways in whicha learnability bias for a property can exist but notcause that property to become prevalent.
First, usinga mathematical analysis, we showed that this can oc-cur when transmission failures favor languages otherthan those that have greater learnability.
This illus-trates the importance of considering the entire trans-mission matrix, not just the probabilities of accuratetransmissions that are considered when establishinga learnability bias.Second, we showed that it is possible for the sheernumber of other possible languages to overwhelmgreater learnability for a particular language.
Wethen illustrated that this second scenario might leadto incorrect predictions in an experimental context.In artificial language experiments, greater learnabil-ity is often established by comparing the accuracyof transmission for a language with the property ofinterest to an arbitrary language.
However, in ourexperiment, we established such a learnability biasfor vowel harmony, but this did not result in vowelharmony being maintained after many instances oftransmission.
This result seems to be due to the factthat numerous languages other than harmonic lan-guages were possible, so learners tended to learn oneof these many other languages.One limitation of our analysis is the use of the551 2 3 4 5 6 7 8 9 100.20.40.60.81.0Proportion Harmonic Proportion of Harmonic Words Chosen in Test100%75%50%25%0%1 2 3 4 5 6 7 8 9 100.20.40.60.81.0 Proportion of Generalizations in Test that were HarmonicIteration in ChainProportionHarmonic100%75%50%25%0%Figure 4: Iterated learning chain results.
Dotted lines show the two-tailed 95% confidence interval for chance re-sponding; confidence intervals differ between the two graphs because there are 40 opportunities to generalize versus80 opportunities to choose harmonic words.1 2 3 4 5 6 7 8 9 100.20.40.60.81.0AccuracyAccuracy for Harmonic Words1 2 3 4 5 6 7 8 9 100.20.40.60.81.0Iteration in ChainAccuracyAccuracy for Non?Harmonic Words100%75%50%25%0%100%75%50%25%0%Figure 5: Accuracy on harmonic versus non-harmonic words by iteration.
Overall, there is no difference in accuracy.simple linear transmission model, in which eachlearner learns from one member of the previous gen-eration.
It is easy to imagine variants on this modelthat make more realistic assumptions about culturaltransmission of languages.
However, we suspectthat these more complex models would not alter theconclusions that we have drawn here.
For exam-ple, learning from multiple members of the previousgeneration tends to dilute the effects of learnabilityon the languages produced by a population (Smith,2009; Burkett & Griffiths, 2010).Overall, the result of a more complicated relation-ship between learnability biases and linguistic uni-versals is congruent with the evidence that all lan-guages do not exhibit all properties for which learn-ability biases have been found.
Indeed, in histori-cal linguistics, the general principle is one of lan-guage divergence, rather than convergence on someuniversal language (e.g., Greenberg, 1971).
Giventhis relationship, one must rethink using experimen-tal evidence for particular learnability biases to ex-plain linguistic tendencies.
Instead, one must eitherestimate all of the values in the transmission matrix,or actually simulate the process of multiple trans-missions in the lab to establish whether a particu-lar property with a learnability bias is actually main-tained over many generations.
While this process isdependent on assuming a particular model of howtransmission occurs in populations, such as the lin-ear iterated learning paradigm we used in our exper-iments, it provides a way of understanding what mu-tations are likely to occur and of exploring the longterm trends that result from particular learnability bi-ases.
As we showed for vowel harmony, long termtrends may not match what one predicted based on alearnability bias.
Given such a result, one must lookto factors other than the learnability bias to explainwhy a property is common across languages.Acknowledgements.
This work was supported by anNSF Graduate Research Fellowship to ANR, grant num-ber BCS-0704034 from the NSF to TLG, and grant num-ber T32 NS047987 from the NIH to ME.56ReferencesBecker, M., Ketrez, N., & Nevins, A.
(2011).
The surfeitof the stimulus: Analytic biases filter lexical statisticsin turkish laryngeal alternations.
Language, 87(1), 84-125.Burke, C. J., & Rosenblatt, M. (1958).
A Markovianfunction of a Markov chain.
The Annals of Mathemat-ical Statistics, 29(4), 1112?1122.Burkett, D., & Griffiths, T. (2010).
Iterated learningof multiple languages from multiple teachers.
In TheEvolution of Language: Proceedings of the 8th Inter-national Conference (EVOLANG8).Chomsky, N., & Lasnik, H. (1993).
The theory of prin-ciples and parameters.
In J. Jacobs, A. von Stechow,W.
Sternefeld, & T. Vannemann (Eds.
), Syntax: An in-ternational handbook of contemporary research (pp.506?569).
Berlin: Walter de Gruyter.Comrie, B.
(1981).
Language universals and linguistictypology.
Chicago: University of Chicago Press.Croft, W. (2002).
Typology and universals.
CambridgeUniversity Press.Finley, S., & Badecker, W. (2007).
Towards a substan-tively biased theory of learning.
Berkeley LinguisticsSociety, 33.Finley, S., & Badecker, W. (2009).
Artificial languagelearning and feature-based generalization.
Journal ofMemory and Language, 61, 423?437.Greenberg, J.
(Ed.).
(1963).
Universals of language.Cambridge, MA: MIT Press.Greenberg, J.
(1971).
Language, culture, and communi-cation.
Stanford: Stanford University Press.Griffiths, T. L., & Kalish, M. L. (2007).
A Bayesian viewof language evolution by iterated learning.
CognitiveScience, 31, 441-480.Kemeny, J., & Snell, J.
(1960).
Finite markov chains.Princeton, NJ: van Nostrand.Kirby, S. (2001).
Spontaneous evolution of linguisticstructure: An iterated learning model of the emergenceof regularity and irregularity.
IEEE Journal of Evolu-tionary Computation, 5, 102-110.Komarova, N. L., & Nowak, M. A.
(2003).
Languagedynamics in finite populations.
Journal of TheoreticalBiology, 221, 445-457.Labov, W. (2001).
Principles of linguistic change.
Vol-ume II: Social Factors.
Blackwell.Moreton, E. (2008).
Analytic bias and phonological ty-pology.
Phonology, 25(1), 83?127.Rafferty, A. N., Griffiths, T. L., & Klein, D. (2009).Convergence bounds for language evolution by iteratedlearning.
Proceedings of the Thirty-First Annual Con-ference of the Cognitive Science Society.Schuster, P., & Sigmund, K. (1983).
Replicator dynam-ics.
Journal of Theoretical Biology, 100(3), 533 - 538.Slobin, D. (1973).
Cognitive prerequisites for the acqui-sition of grammar.
In C. Ferguson & D. Slobin (Eds.
),Studies of child language development (pp.
173?208).Smith, K. (2009).
Iterated learning in populations ofBayesian agents.
In Proceedings of the 31st AnnualConference of the Cognitive Science Society.van der Hulst, H., & van de Weijer, J.
(1995).
Vowel har-mony.
In J. Goldsmith (Ed.
), The Handbook of Phono-logical Theory (pp.
495?534).
Blackwell.Wilson, C. (2003).
Experimental investigation of phono-logical naturalness.
Proceedings of the 22nd WestCoast Conference on Formal Linguistics.Wilson, C. (2006).
Learning phonology with substan-tive bias: An experimental and computational study ofvelar palatalization.
Cognitive Science, 30, 945?982.57
