Chinese Named Entity Identification Using Class-basedLanguage Model1Jian Sun*, Jianfeng Gao, Lei Zhang**, Ming Zhou, Changning Huang* Beijing University of Posts & Telecommunications, China, jiansun_china@hotmail.com#Microsoft Research Asia, {jfgao, mingzhou, cnhuang}@microsoft.comTsinghua University, China1 This work was done while the author was visiting Microsoft Research Asia$EVWUDFWWe consider here the problem of Chinesenamed entity (NE) identification usingstatistical language model(LM).
In thisresearch, word segmentation and NEidentification have been integrated into aunified framework that consists of severalclass-based language models.
We also adopt ahierarchical structure for one of the LMs sothat the nested entities in organization namescan be identified.
The evaluation on a large testset shows consistent improvements.
Ourexperiments further demonstrate theimprovement after seamlessly integrating withlinguistic heuristic information, cache-basedmodel and NE abbreviation identification. ,QWURGXFWLRQ1(LGHQWLILFDWLRQ is the key technique in manyapplications such as information extraction,question answering, machine translation and soon.
English NE identification has achieved agreat success.
However, for Chinese, NEidentification is very different.
There is nospace to mark the word boundary and nostandard definition of words in Chinese.
TheChinese NE identification and wordsegmentation are interactional in nature.This paper presents a unified approachthat integrates these two steps together using aclass-based LM, and apply Viterbi search toselect the global optimal solution.
Theclass-based LM consists of two sub-models,namely the context model and the entity model.The context model estimates the probability ofgenerating a NE given a certain context, andthe entity model estimates the probability of asequence of Chinese characters given a certainkind of NE.
In this study, we are interested inthree kinds of Chinese NE that are mostcommonly used, namely person name (PER),location name (LOC) and organization name(ORG).
We have also adopted a variety ofapproaches to improving the LM.
In addition, ahierarchical structure for organization LM isemployed so that the nested PER, LOC inORG can be identified.The evaluation is conducted on a large testset in which NEs have been manually tagged.The experiment result shows consistentimprovements over existing methods.
Ourexperiments further demonstrate theimprovement after integrating with linguisticheuristic information, cache-based model andNE abbreviation identification.
The precisionof PER, LOC, ORG on the test set is 79.86%,80.88%, 76.63%, respectively; and the recall is87.29%, 82.46%, 56.54%, respectively. 5HODWHG:RUNRecently, research on English NEidentification has been focused on themachine-learning approaches, includinghidden Markov model (HMM), maximumentropy model, decision tree andtransformation-based learning, etc.
(Bikel et al1997; Borthwick et al 1999; Sekine et al1998).
Some systems have been applied to realapplication.Research on Chinese NE identification is,however, still at its early stage.
Someresearches apply methods of English NEidentification to Chinese.
Yu et al(1997)applied the HMM approach where the NEidentification is formulated as a taggingproblem using Viterbi algorithm.
In general,current approaches to NE identification (e.g.Chen, 1997) usually contain two separate steps:word segmentation and NE identification.
Theword segmentation error will definitely lead toerrors in the NE identification results.
Zhang(2001) put forward class-based LM forChinese NE identification.
We further developthis idea with some new features, which leadsto a new framework.
In this framework, weintegrate Chinese word segmentation and NEidentification into a unified framework using aclass-based language model (LM). &ODVVEDVHG/0 IRU1(,GHQWLILFDWLRQThe n-gram LM is a stochastic model whichpredicts the next word given the previous n-1words by estimating the conditional probabilityP(wn|w1?wn-1).
In practice, trigramapproximation P(wi|wi-2wi-1) is widely used,assuming that the word wi depends only on twopreceding words wi-2 and wi-1.
Brown et al(1992)put forward and discussed n-gram modelsbased on classes of words.
In this section, wewill describe how to use class-based trigrammodel for NE identification.
Each kind of NE(including PER, LOC and ORG) is defined as aclass in the model.
In addition, we differentiatethe transliterated person name (FN) from theChinese person name since they have differentconstitution patterns.
The four classes of NEused in our model are shown in Table 1.
Allother words are also defined as individualclasses themselves (i.e.
one word as one class).Consequently, there are _9_+4 classes in ourmodel, where _9_ is the size of vocabulary.7DEOH : Classes defined in class-based model7DJ 'HVFULSWLRQPN Chinese person nameFN Transliterated person nameLN Location nameON Organization name 7KH/DQJXDJH0RGHOLQJ )RUPXODWLRQGiven a Chinese character sequence 6=V?VQ,the task of Chinese NE identification is to findthe optimal class sequence &=F?FP (P<=Q)that maximizes the probability 3&_6.
It can beexpressed in the equation (1) and we call itclass-based model.The class-based model consists of twosub-models: the context model 3& and theentity model P (S|C).
The context modelindicates the probability of generating a NEclass given a (previous) context.
P(C) is apriori probability, which is computedaccording to Equation (2):?=--@PL LLL FFF3&3 1 12 )|()( (2)P(C) can be estimated using a NE labeledcorpus.
The entity model can be parameterizedby Equation (3):?=----@@=PM MHQGFVWDUWFPQVWDUWFHQGFPQFVV3FFVVVV3 FFVV3&63MMP11111)|]...([)...|]...]...[...([)...|...()|(1 (3)The entity model estimates the generativeprobability of the Chinese character sequence insquare bracket pair (i.e.
starting from FMVWDUW to FMHQG) given the specific NE class.For different class, we define the differententity model.For the class of PER (including PN andFN), the entity model is a FKDUDFWHUEDVHGtrigram model as shown in Equation (4).
?--=----===HQGFVWDUWFN MNNNMHQGFVWDUWFMMMM3(5FVVV33(5FVV3),,|()|]...([12(4)where s can be any characters occurred in aperson name.
For example, the generativeprobability of character sequence "?# (LiDapeng) is much larger than that of ?
?H(many years) given the PER since " is acommonly used family name, and ?
and # arecommonly used first names.
The probabilitiescan be estimated with the person name list.For the class of LOC, the entity model is a ZRUGEDVHG trigram model as shown inEquation (5).
)|(maxarg* 6&3& &=)|()(maxarg &63&3& ?= (1))|]...([ /2&FVV3 MHQGFVWDUWF MM =--@/2&FZZ_Z3>PD[/2&F_ZZ3PD[ON MNNN:MO:?=-- ===?
(5)where W = w1?wl is possible segmentationresult of  character sequence HQGFVWDUWF MM VV -- ... .For the class of ORG, the construction ismuch more complicated because an ORG oftencontain PER and/or LOC.
For example, theORG ????N@?
?
(Air ChinaCorporation) contains the LOC ???
(China).It is beneficial to such applications as questionanswering, information extraction and so on ifnested NE can be identified as well .
In order toidentify the nested PER, LOC in ORG 2, weadopted class-based LMs for ORG further, inwhich there are three sub models, one is theclass generative model, and the others are entitymodel: person name model and location namemodel in ORG.
Therefore, the entity model ofORG is shown in Equation (6) which is almostsame as Equation (1).)|]...
([ 25*FVV3 MHQGFVWDUWF MM =--??????????????=?=@????????=?===@?
?=--=------NL MLHQGFVWDUWFNL MLLL&MNHQGFVWDUWFMN&MHQGFVWDUWFM&25*FFVV325*FFF_3F25*FFFVV3 25*FFF325*F&VV3F&3LLMMMM1'''11''),'|]...([max),'...'|]...([)|'...'(max)],'|]...([)|'([max(6)where '...'1' NFF& = is the sequence of classcorresponding to the Chinese charactersequence.In addition, if MF is a normal word,1)|]...([ =-- MHQGFVWDUWF FVV3 MM .
(7)Based on the context model and entitymodels, we can compute the probability 3&_62 For simplification, only nested person, locationnames are identified in organization.
The nestedperson in location is not identified because of lowfrequencyand can get the optimal class sequence TheChinese PER and transliterated PER share thesame context class model when computing theprobability. 0RGHOV(VWLPDWLRQAs discussed in 3.1.1, there are two kinds ofprobabilities to be estimated: P(C) and P(S|C) .Both probabilities are estimated usingMaximum Likelihood Estimation (MLE) withthe annotated training corpus.The parser NLPWin3 was used to tag thetraining corpus.
As a result, the corpus wasannotated with NE marks.
Four lists wereextracted from the annotated corpus and eachlist corresponds one NE class.
The contextmodel 3& was trained with the annotatedcorpus and the four entity models were trainedwith corresponding NE lists.
The Figure 1shows the training process.
(Begin of sentence(BOS) and end of sentence (EOS) is added)NLPWinTaggedSentence<LOC>b?</LOC>?<PER>??</PER>,$<ORG>???N@?</ORG>X?
??<LOC>?<LOC>ContextClassBOS LN ?
PN ,$ ON X???
LN  EOSLN list b??FN list ?
?ON list ?
??N@?ON Class list LN ??N@?CorrespondingEnglishSentence<LOC>U.S.</LOC>president<PER>Bush</PER> arrived in<LOC> P.R.
China </LOC> by flightNo.1 of <ORG>Air ChinaCorp.</ORG>Figure 1:  Example of  Training Process 'HFRGHUGiven a sequence of Chinese characters, thedecoding process consists of the followingthree steps:6WHS  All possible word segmentations aregenerated using a Chinese lexiconcontaining 120,050 entries.
The lexicon isonly used for segmentation and there is noNE tag in it even if one word is PER, LOC or3 NLPWin system is a natural language processingsystem developed by Microsoft Research.ORG.
For example, ??
(Beijing) is nottagged as LOC.6WHS NE candidates are generated from anyone or more segmented character strings andthe corresponding generative probability foreach candidate is computed using entitymodels described in Equation (4)?(7).
6WHS  Viterbi search is used to selecthypothesis with the highest probability asthe best output.
Furthermore, in order toidentify nested named entities, two-passViterbi search is adopted.
The inner Viterbisearch is corresponding to Equation (6) andthe outer one corresponding to Equation (1).After the two-pass searches, the wordsegmentation and the named entities(including nested ones) can be obtained. ,PSURYHPHQWThere are some problems with the frameworkof NE identification using the class-based LM.First, redundant candidates NEs are generatedin the decoding process, which results in verylarge search space.
The second problem is thatdata sparseness will seriously influence theperformance.
Finally, the abbreviation of NEscannot be handled effectively.
In the followingthree subsections, we provide solutions to thethree problems mentioned above. +HXULVWLF,QIRUPDWLRQIn order to overcome the redundant candidategeneration problem, the heuristic informationis introduced into the class-based LM.
Thefollowing resources were used: (1) Chinesefamily name list, containing 373 entries (e.g.
?
(Zhang), _ (Wang)); (2) transliterated namecharacter list, containing 618 characters (e.g.?
(shi), S (dun)); and (3) ORG keyword list,containing 1,355 entries (e.g.
?
: (university),@?
(corporation)).The heuristic information is used toconstrain the generation of NE candidates.
ForPER (PN), only PER candidates beginning withthe family name is considered.
For PER (FN), acandidate is generated only if all its composingcharacter belongs to the transliterated namecharacter list.
For ORG, a candidate is excludedif it does not contain one ORG keyword.Here, we do not utilize the LOC keyword togenerate LOC candidate because of the fact thatmany LOC do not end with keywords. &DFKH0RGHOThe cache entity model can address the datasparseness problem by adjusting the parameterscontinually as NE identification proceeds.
Thebasic idea is to accumulate Chinese character orword n-gram so far appeared in the documentand use them to create a local dynamic entitymodel such as )|( 1-LLELFDFKH ZZ3 and)( LXQLFDFKH Z3 .
We can interpolate the cacheentity model with the static entityLM )...|( 121 -- LLLVWDWLF ZZZZ3 :ZZZ_Z3 LLLFDFKH -- (8))....|()1()|()(1121121----++=LLVWDWLFLLELFDFKHLXQLFDFKH ZZZ3 ZZ3Z3 OO OOwhere ]1,0[, 21 ?OO are interpolation weightthat is determined on the held-out data set. 'HDOLQJZLWK$EEUHYLDWLRQWe found that many errors result from theoccurrence of abbreviation of person, location,and organization.
Therefore, differentstrategies are adopted to deal withabbreviations for different kinds of NEs.
ForPER, if Chinese surname is followed by thetitle, then this surname is tagged as PER.
Forexample, ?
?S (President Zuo) is tagged as<PER>?</PER> ?S.
For LOC, if at leasttwo location abbreviations occur consecutive,the individual location abbreviation is tagged asLOC.
For example,?G?
(Sino-Japanrelation) is tagged as <LOC> </LOC><LOC>?</LOC> G?.
For ORG, iforganization abbreviation is followed by LOC,which is again followed by organizationkeyword, the three units are tagged as one ORG.For example,  E ?
?
?
?
(ChineseCommunist Party Committee of Beijing) i stagged as <ORG>E<LOC>?
?</LOC> ??
</ORG>.
At present, we collected 112organization abbreviations and 18 locationabbreviations. ([SHULPHQWV (YDOXDWLRQ0HWULFWe conduct evaluations in terms of precision (P)and recall (R).1(LGHQWLILHGRIQXPEHU 1(LGHQWLILHGFRUUHFWO\RIQXPEHU3 = (9)1(DOORIQXPEHU 1(LGHQWLILHGFRUUHFWRIQXPEHU5 = (10)We also used the F-measure, which is definedas a weighted combination of precision andrecall as Equation (11):53 53)  +?
?
?+= EE (11)where E is the relative weight of precision andrecall.There are two differences between METevaluation and ours.
First, we include nestedNE in our evaluation whereas MET does not.Second, in our evaluation, only NEs withcorrect boundary and type label are consideredthe correct identifications.
In MET, theevaluation is somewhat flexible.
For example, aNE may be identified partially correctly if thelabel is correct but the boundary is wronglydetected. 'DWD6HWVThe training text corpus contains data fromPeople?s Daily (Jan.-Jun.1998).
It contains357,544 sentences (about 9,200,000 Chinesecharacters).
This corpus includes 104,487Chinese PER, 51,708 transliterated PER,218,904 LOC, and 87,391 ORG.
These datawas obtained after this corpus was parsed withNLPWin.We built the wide coverage test dataaccording to the guidelines4 that are just sameas those of 1999 IEER.
The test set (as shown inTable 2) contains half a million Chinesecharacters; it is a balanced test set covering 11domains.
The test set contains 11,844 sentences,49.84% of the sentences contain at least one NE.The number of characters in NE accounts for8.448% in all Chinese characters.We can see that the test data is much largerthan the MET test data and IEER data4 The difference between IEER?s guidelines andours is that the nested person and location name inorganization are tagged in our guidelines.7DEOH: Statistics of Open-TestNumber of NE TokensID DomainPER LOC ORGSize(byte)1 Army 65 202 25 19k2 Computer 75 156 171 59k3 Culture 548 639 85 138k4 Economy 160 824 363 108k5 Entertainment 672 575 139 104k6 Literature 464 707 122 96k7 Nation 448 1193 250 101k8 People 1147 912 403 116k9 Politics 525 1148 218 122k10 Science 155 204 87 60k11 Sports 743 1198 628 114kTotal 5002 7758 2491 1037k 7UDLQLQJ'DWD3UHSDUDWLRQThe training data produced by NLPWin hassome noise due to two reasons.
First, the NEguideline used by NLPWin is different fromthe one we used.
For example, in NLPWin, ???
(Beijing City) is tagged as <LOC>?
?</LOC> ?, whereas ???
should be LOCin our definition.
Second, there are some errorsin NLPWin results.
We utilized 18 rules tocorrect the frequent errors.
The followingshows some examples.The Table 4 shows the quality of our trainingcorpus.Table 4   Quality of Training CorpusNE P (%) R (%) F (%)PER 61.05 75.26 67.42LOC 78.14 71.57 74.71ORG 68.29 31.50 43.11Total 70.07 66.08 68.02 ([SHULPHQWVWe conduct incrementally the following fourexperiments:(1) Class-based LM, we view the results asbaseline performance;(2) Integrating heuristic information into (1);(3) Integrating Cache-based LM with (2);(4) Integrating NE abbreviation processingwith (3)./1/RFDWLRQ.H\ : /1/1O /1Z : 21_b_?_??
??
: /1? &ODVVEDVHG/0%DVHOLQHBased on the basic class-based modelsestimated with the training data, we can get thebaseline performance, as is shown in Table 5.Comparing Table 4 and Table 5, we found thatthe performance of baseline is better than thequality of training data.Table 5    Baseline PerformanceNE P (%) R (%) F (%)PER 65.70 84.37 73.87LOC 82.73 76.03 79.24ORG 56.55 38.56 45.86Total 72.61 72.44 72.53 ,QWHJUDWLQJ+HXULVWLF,QIRUPDWLRQIn this part, we want to see the effects of usingheuristic information.
The results are shown inTable 6.
In experiments, we found that byintegrating the heuristic information, we notonly achieved more efficient decoding, but alsoobtained higher NE identification precision.
Forexample, the precision of PER increases from65.70% to 77.63%, and precision of ORGincreases from 56.55% to 81.23%.
The reasonis that adopting heuristic information reducesthe noise influence.However, we noticed that the recall of PERand LOC decreased a bit.
There are two reasons.First, organization names without organizationending keywords were not marked as ORG.Second, Chinese names without surnames werealso missed.Table 6 Results of Heuristic Information Integratedinto the Class-based LMNE P (%) R (%) F (%)PER 77.63 80.89 79.23LOC 80.05 80.80 80.42ORG 81.23 36.65 50.51Total 79.26 73.41 76.23 ,QWHJUDWLQJ&DFKHEDVHG/0Table 7 shows the evaluation results aftercache-based LM was integrated.
From Table 6and Table 7, we found that almost all theprecision and recall of PER, LOC, ORG haveobtained slight improvements.Table 7   Results of our systemNE P (%) R (%) F (%)PER 79.12 82.06 80.57LOC 80.11 81.27 80.69ORG 79.71 39.89 53.17Total 79.72 74.58 77.06 ,QWHJUDWLQJ ZLWK 1( $EEUHYLDWLRQ3URFHVVLQJIn this experiment, we integrated with NEabbreviation processing.
As shown in Table 8,the experiment result indicates that the recall ofPER, LOC, ORG increased from 82.06%,81.27%, 36.65% to 87.29%, 82.46%, 56.54%,respectively.Table 8   Results of our systemNE P (%) R (%) F (%)PER 79.86 87.29 83.41LOC 80.88 82.46 81.66ORG 76.63 56.54 65.07Total 79.99 79.68 79.83 6XPPDU\From above data, we observed that (1) the classbased SLM performs better than the trainingdata automatically produced with the parser; (2)the distinct improvements is achieved by usingheuristic information; (3) Furthermore, ourmethod of dealing with abbreviation increasesthe recall of NEs.In addition, the cache-based LM increasesthe performance not so much.
The reason is asfollows: The cache-based LM is based on thehypothesis that a word used in the recent past ismuch likely either to be used soon than itsoverall frequency in the language or a 3 -grammodel would suggest (Kuhn, 1990).
However,we found that the same NE often vari es itsmorpheme in the same document.
For example,the same NE  E ?
?
?
?
(ChineseCommunist Party Committee of Beijing),???
?
(Committee of Beijing City), ?
?
(Committee) occur in order.Furthermore, we notice that thesegmentation dictionary has an importantimpact on the performance of NEidentification.
We do not think it is better ifmore words are added into dictionary.
Forexample, because ??
(Chinese) is in ourdictionary, there is much possibility that ?
(China) in ??
is missed identified. (YDOXDWLRQZLWK0(7DQG,((57HVW'DWDWe also evaluated on the MET2 test data andIEER test data.
The results are shown in Table9.
The results on MET2 are lower than thehighest report of MUC7 (PER: Precision 66%,Recall 92%; LOC: Precision 89%, Recall 91%;ORG: Precision 89%, Recall 88%,http://www.itl.nist.gov).
We speculate thereasons for this in the following.
The mainreason is that our class-based LM wasestimated with a general domain corpus, whichis quite different from the domain of MUCdata.
Moreover, we didn?t use a NE dictionary.Another reason is that our NE definitions areslightly different from MET2.Table 9 Results on MET2 and IEERMET2 Data IEER DataNEP(%)R(%)F(%)P(%)R(%)F(%)PER 65.86 94.25 77.54 79.38 84.43 81.83LOC 77.42 89.60 83.07 79.09 80.18 79.63ORG 88.47 75.33 81.38 88.03 62.30 72.96Total 77.89 86.09 81.79 80.82 76.78 78.75 &RQFOXVLRQV 	)XWXUHZRUNIn this research, Chinese word segmentationand NE identification has been integrated intoa framework using class-based languagemodels (LM).
We adopted a hierarchicalstructure in ORG model so that the nestedentities in organization names can be identified.Another characteristic is that our NEidentification do not utilize NE dictionarywhen decoding.The evaluation on a large test set showsconsistent improvements.
The integration ofheuristic information improves the precisionand recall of our system.
The cache-based LMincreases the recall of NE identification tosome extent.
Moreover, some rules dealingwith abbreviations of NEs have increaseddramatically the performance.
The precision ofPER, LOC, ORG on the test set is 79.86%,80.88%, 76.63%, respectively; and the recall is87.29%, 82.46%, 56.54%, respectively.In our future work, we will be focusingmore on NE coreference using language model.Second, we intend to extend our model toinclude the part-of-speech tagging model toimprove the performance.
At present, theclass-based LM is based on the general domainand we may need to fine-tune the model for aspecific domain.ACKNOWLEDGEMENTI would like to thank Ming Zhou, JianfengGao, Changning Huang, Andi Wu, Hang Liand other colleagues from Microsoft Researchfor their help.
And I want to thank especiallyLei Zhang from Tsinghua University for hishelp in developing the ideas.5HIHUHQFHVBorthwick.
A.
(1999) A Maximum EntropyApproach to Named Entity Recognition.
PhDDissertationBikel D., Schwarta R., Weischedel.
R. (1997) Analgorithm that learns what?s in a name.
MachineLearning 34, pp.
211-231Brown, P. F., DellaPietra, V. J., deSouza, P. V., Lai,J.
C., and Mercer, R. L. (1992).
Class-basedn-gram models of natural language.
ComputationalLinguistics, 18(4):468--479.Chinchor.
N. (1997) MUC-7 Named Entity TaskDefinition Version 3.5.
Available by fromftp.muc.saic.com/pub/MUC/MUC7-guidelinesChen H.H., Ding Y.W., Tsai S.C. and Bian G.W.
(1997) Description of the NTU System Used forMET2Gao J.F., Goodman J., Li M.J., Lee K.F.
(2001)Toward a unified Approach to Statistical LanguageModeling for Chinese.
To appear in ACMTransaction on Asian Language ProcessingKuhn R., Mori.
R.D.
(1990) A Cache-BasedNatural Language Model for Speech Recognition.IEEE Transaction on Pattern Analysis and MachineIntelligence.Vol.12.
No.
6. pp 570-583Mikheev A., Grover C. and Moens M. (1997)Description of the LTG System Used for MUC-7Sekine S., Grishman R. and Shinou H. (1998), ?Adecision tree method for finding and classifyingnames in Japanese texts?, Proceedings of the SixthWorkshop on Very Large Corpora, CanadaYu S.H., Bai S.H.
and Wu P. (1997) Description ofthe Kent Ridge Digital Labs System Used forMUC-7Zhang L. (2001) Study on Chinese ProofreadingOriented Language Modeling, PhD Dissertation
