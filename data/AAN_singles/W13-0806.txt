Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48?57,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsCombining Top-down and Bottom-up Searchfor Unsupervised Induction of Transduction GrammarsMarkus SAERS and Karteek ADDANKI and Dekai WUHuman Language Technology CenterDept.
of Computer Science and EngineeringHong Kong University of Science and Technology{masaers|vskaddanki|dekai}@cs.ust.hkAbstractWe show that combining both bottom-up rulechunking and top-down rule segmentationsearch strategies in purely unsupervised learn-ing of phrasal inversion transduction gram-mars yields significantly better translation ac-curacy than either strategy alone.
Previous ap-proaches have relied on incrementally buildinglarger rules by chunking smaller rules bottom-up; we introduce a complementary top-downmodel that incrementally builds shorter rulesby segmenting larger rules.
Specifically, wecombine iteratively chunked rules from Saerset al(2012) with our new iteratively seg-mented rules.
These integrate seamlessly be-cause both stay strictly within a pure trans-duction grammar framework inducing undermatching models during both training andtesting?instead of decoding under a com-pletely different model architecture than whatis assumed during the training phases, whichviolates an elementary principle of machinelearning and statistics.
To be able to drive in-duction top-down, we introduce a minimumdescription length objective that trades offmaximum likelihood against model size.
Weshow empirically that combining the more lib-eral rule chunking model with a more conser-vative rule segmentation model results in sig-nificantly better translations than either strat-egy in isolation.1 IntroductionIn this paper we combine both bottom-up chunkingand top-down segmentation as search directions inthe unsupervised pursuit of an inversion transduc-tion grammar (ITG); we also show that the combi-nation of the resulting grammars is superior to ei-ther of them in isolation.
For the bottom-up chunk-ing approach we use the method reported in Saerset al(2012), and for the top-down segmentation ap-proach, we introduce a minimum description length(MDL) learning objective.
The new learning objec-tive is similar to the Bayesian maximum a poste-riori objective, and makes it possible to learn top-down, which is impossible using maximum likeli-hood, as the initial grammar that rewrites the startsymbol to all sentence pairs in the training data al-ready maximizes the likelihood of the training data.Since both approaches result in stochastic ITGs, theycan be easily combined into a single stochastic ITGwhich allows for seamless combination.
The pointof our present work is that the two different searchstrategies result in very different grammars so thatthe combination of them is superior in terms of trans-lation accuracy to either of them in isolation.The transduction grammar approach has the ad-vantage that induction, tuning and testing are op-timized on the exact same underlying model?thisused to be a given in machine learning and statisticalprediction, but has been largely ignored in the statis-tical machine translation (SMT) community, wheremost current SMT approaches to learning phrasetranslations that (a) require enormous amounts ofrun-time memory, and (b) contain a high degree ofredundancy.
In particular, phrase-based SMT mod-els such as Koehn et al(2003) and Chiang (2007)often search for candidate translation segments andtransduction rules by committing to a word align-ment that is completely alien to the grammar, as itis learned with very different models (Brown et al(1993), Vogel et al(1996)), whose output is thencombined heuristically to form the alignment actu-ally used to extract lexical segment translations (Och48and Ney, 2003).
The fact that it is even possibleto improve the performance of a phrase-based di-rect translation system by tossing away most of thelearned segmental translations (Johnson et al 2007)illustrates the above points well.Transduction grammars can also be induced fromtreebanks instead of unannotated corpora, which cutsdown the vast search space by enforcing additional,external constraints.
This approach was pioneeredby Galley et al(2006), and there has been a lot of re-search since, usually referred to as tree-to-tree, tree-to-string and string-to-tree, depending on wherethe analyses are found in the training data.
This com-plicates the learning process by adding external con-straints that are bound to match the translation modelpoorly; grammarians of English should not be ex-pected to care about its relationship to Chinese.
Itdoes, however, constitute a way to borrow nonter-minal categories that help the translation model.It is also possible for the word alignments leadingto phrase-based SMT models to be learned throughtransduction grammars (see for example Cherry andLin (2007), Zhang et al(2008), Blunsom et al(2008), Saers andWu (2009), Haghighi et al(2009),Blunsom et al(2009), Saers et al(2010), Blunsomand Cohn (2010), Saers and Wu (2011), Neubig etal.
(2011), Neubig et al(2012)).
Even when theSMT model is hierarchical, most of the informationencoded in the grammar is tossed away, when thelearned model is reduced to a word alignment.
Aword alignment can only encode the lexical relation-ships that exist between a sentence pair according toa single parse tree, which means that the rest of themodel: the alternative parses and the syntactic struc-ture, is ignored.Theminimumdescription length (MDL) objectivethat we will be using to drive the learning will pro-vide a way to escape the maximum-likelihood-of-the-data-given-the-model optimum that we start outwith.
However, going only by MDL will also lead toa degenerate case, where the size of the grammar isallowed to shrink regardless of how unlikely the cor-pus becomes.
Instead, we will balance the length ofthe grammar with the probability of the corpus giventhe grammar.
This has a natural Bayesian interpreta-tion where the length of the grammar acts as a priorover the structure of the grammar.Similar approaches have been used before, but toinduce monolingual grammars.
Stolcke and Omo-hundro (1994) use a method similar to MDL calledBayesianmodel merging to learn the structure of hid-den Markov models as well as stochastic context-free grammars.
The SCFGs are induced by allowingsequences of nonterminals to be replaced with a sin-gle nonterminal (chunking) as well as allowing twononterminals to merge into one.
Gr?nwald (1996)uses it to learn nonterminal categories in a context-free grammar.
It has also been used to interpret vi-sual scenes by classifying the activity that goes on ina video sequences (Si et al 2011).
Our work in thispaper is markedly different to even the previous NLPwork in that (a) we induce an inversion transduc-tion grammar (Wu, 1997) rather than a monolingualgrammar, and (b) we focus on learning the terminalsegments rather than the nonterminal categories.The similar Bayesian approaches to finding themodel structure of ITGs have been tried before, butonly to generate alignments that mismatched trans-lation models are then trained on, rather than usingthe ITG directly as translation model, which we do.Zhang et al(2008) use variational Bayes with a spar-sity prior over the parameters to prevent the size ofthe grammar to explode when allowing for adjacentterminals in the Viterbi biparses to chunk together.Blunsom et al(2008), Blunsom et al(2009) andBlunsom and Cohn (2010) use Gibbs sampling tofind good phrasal translations.
Neubig et al(2011)and Neubig et al(2012) use a method more similarto ours, but with a Pitman-Yor process as prior overthe structures.The idea of iteratively segmenting the existingsentence pairs to find good phrasal translations hasalso been tried before; Vilar and Vidal (2005) intro-duces the Recursive Alignment Model, which recur-sively determines whether a bispan is a good enoughtranslation on its own (using IBM model 1), or if itshould be split into two bispans (either in straight orinverted order).
The model uses length of the inputsentence to determine whether to split or not, anduses very limited local information about the splitpoint to determine where to split.
Training the pa-rameters is done with a maximum likelihood objec-tive.
In contrast, our model is one single genera-tive model (as opposed to an ad hoc model), trainedwith a minimum description length objective (ratherthan trying to maximize the probability of the train-49ing data).The rest of the paper is structured so that we firsttake a closer look at the minimum description lengthprinciple that will be used to drive the top-downsearch (Section 2).
We then show how the top-downgrammar is learned (Sections 3 and 4), before show-ing how we combine the new grammar with that ofSaers et al(2012) (Section 5).
We then detail theexperimental setup that will substantiate our claimsempirically (Section 6) before interpreting the resultsof those experiments (Section 7).
Finally, we offersome conclusions (Section 8).2 Minimum description lengthThe minimum description length principle is aboutfinding the optimal balance between the size of amodel and the size of some data given the model(Solomonoff (1959), Rissanen (1983)).
Consider theinformation theoretical problem of encoding somedatawith amodel, and then sending both the encodeddata and the information needed to decode the data(the model) over a channel; the minimum descrip-tion length would be the minimum number of bitssent over the channel.
The encoded data can be inter-preted as carrying the information necessary to dis-ambiguate the ambiguities or uncertainties that themodel has about the data.
Theoretically, the modelcan grow in size and become more certain about thedata, and it can shrink in size and become more un-certain about the data.
An intuitive interpretation ofthis is that the exceptions, which are a part of the en-coded data, can be moved into the model itself.
Bydoing so, the size of the model increases, but thereis no longer an exception that needs to be conveyedabout the data.
Some ?exceptions?
occur frequentlyenough that it is a good idea to incorporate them intothe model, and some do not; finding the optimal bal-ance minimizes the total description length.Formally, the description length (DL) is:DL (M,D) = DL (D|M) + DL (M) (1)Where M is the model and D is the data.
Note theclear parallel to probabilities that have been movedinto the logarithmic domain.In natural language processing, we never havecomplete data to train on, so we need our models togeneralize to unseen data.
A model that is very cer-tain about the training data runs the risk of not beingable to generalize to new data: it is over-fitting.
Itis bad enough when estimating the parameters of atransduction grammar, and catastrophic when induc-ing the structure of the grammar.
The key conceptthat we want to capture when learning the structureof a transduction grammar is generalization.
This isthe property that allow it to translate new, unseen,input.
The challenge is to pin down what general-ization actually is, and how to measure it.One property of generalization for grammars isthat it will lower the probability of the training data.This may seem counterintuitive, but can be under-stood as moving some of the probability mass awayfrom the training data and putting it in unseen data.A second property is that rules that are specific tothe training data can be eliminated from the gram-mar (or replaced with less specific rules that generatethe same thing).
The second property would shortenthe description of the grammar, and the first wouldmake the description of the corpus given the gram-mar longer.
That is: generalization raises the firstterm and lowers the second in Equation 1.
A goodgeneralization will lower the total MDL, whereas apoor onewill raise it; a good generalizationwill tradea little data certainty for more model parsimony.2.1 Measuring the length of a corpusThe information-theoretic view of the problem alsogives a hint at the operationalization of length.
Shan-non (1948) stipulates that the number of bits it takesto encode that a probabilistic variable has taken a cer-tain value can be encoded using as little as the nega-tive logarithmic probability of that outcome.Following this, the parallel corpus given the trans-duction grammar gives the number of bits requiredto encode it: DL (C|G) = ?log2 (P (C|G)), whereC is the corpus and G is the grammar.2.2 Measuring the length of an ITGSince information theory deals with encoding se-quences of symbols, we need some way to serializean inversion transduction grammar (ITG) into a mes-sage whose length can be measured.To serialize an ITG, we first need to determinethe alphabet that the message will be written in.
Weneed one symbol for every nonterminal, L0-terminaland L1-terminal.
We will also make the assump-tion that all these symbols are used in at least one50rule, so that it is sufficient to serialize the rules inorder to express the entire grammar.
To serializethe rules, we need some kind of delimiter to knowwhere one rule starts and the next ends; we will ex-ploit the fact that we also need to specify whether therule is straight or inverted (unary rules are assumedto be straight), and merge these two functions intoone symbol.
This gives the union of the symbols ofthe grammar and the set {[], ??
}, where [] signals thebeginning of a straight rule, and ??
signals the be-ginning of an inverted rule.
The serialized formatof a rule will be: rule type/start marker, followed bythe left-hand side nonterminal, followed by all right-hand side symbols.
The symbols on the right-handsides are either nonterminals or biterminals?pairsofL0-terminals andL1-terminals that model transla-tion equivalences.
The serialized form of a grammaris the serialized form of all rules concatenated.Consider the following toy grammar:S ?
A, A ?
?AA?, A ?
[AA] ,A ?
have/?, A ?
yes/?, A ?
yes/?Its serialized form would be:[]SA??AAA[]AAA[]Ahave?[]Ayes?
[]Ayes?Now we can, again turn to information theory to ar-rive at an encoding for this message.
Assuming auniform distribution over the symbols, each symbolwill require ?log2(1N)bits to encode (where N isthe number of different symbols?the type count).The above example has 8 symbols, meaning thateach symbol requires 3 bits.
The entire message is23 symbols long, which means that we need 69 bitsto encode it.3 Model initializationRather than starting out with a general transductiongrammar and fitting it to the training data, we do theexact opposite: we start with a transduction gram-mar that fits the training data as well as possible, andgeneralize from there.
The transduction grammarthat fits the training data the best is the one wherethe start symbol rewrites to the full sentence pairsthat it has to generate.
It is also possible to add anynumber of nonterminal symbols in the layer betweenthe start symbol and the bisentences without alteringthe probability of the training data.
We take advan-tage of this by allowing for one intermediate sym-bol so that the start symbol conforms to the normalform and always rewrites to precisely one nontermi-nal symbol.
This violate the MDL principle, as theintroduction of new symbols, by definition, makesthe description of the model longer, but conformingto the normal form of ITGs was deemedmore impor-tant than strictly minimizing the description length.Our initial grammar thus looks like this:S ?
A,A ?
e0..T0/f0..V0 ,A ?
e0..T1/f0..V1 ,...,A ?
e0..TN /f0..VNWhere S is the start symbol, A is the nonterminal,N is the number of sentence pairs in the training cor-pus, Ti is the length of the ith output sentence (whichmakes e0..Ti the ith output sentence), and Vi is thelength of the ith input sentence (which makes f0..Vithe ith input sentence).4 Model generalizationTo generalize the initial inversion transduction gram-mar we need to identify parts of the existing biter-minals that could be validly used in isolation, andallow them to combine with other segments.
Thisis the very feature that allows a finite transductiongrammar to generate an infinite set of sentence pairs.Doing this moves some of the probability mass,which was concentrated in the training data, to un-seen data?the very definition of generalization.
Ourgeneral strategy is to propose a number of sets ofbiterminal rules and a place to segment them, eval-uate how the description length would change if wewere to apply one of these sets of segmentations tothe grammar, and commit to the best set.
That is:we do a greedy search over the power set of possi-ble segmentations of the rule set.
As we will see, thisintractable problem can be reasonable efficiently ap-proximated, which is what we have implemented andtested.The key component in the approach is the abilityto evaluate how the description length would changeif a specific segmentation was made in the grammar.51This can then be extended to a set of segmentations,which only leaves the problem of generating suitablesets of segmentations.The key to a successful segmentation is to maxi-mize the potential for reuse.
Any segment that canbe reused saves model size.
Consider the terminalrule:A ?
five thousand yen is my limit/????????
(Chinese gloss: ?w?
z?i d?o ch?
w?
q?an r?
y?an?
).This rule can be split into three rules:A ?
?AA?,A ?
five thousand yen/???
?,A ?
is my limit/???
?Note that the original rule consists of 16 symbols (inour encoding scheme), whereas the new three rulesconsists of 4 + 9 + 9 = 22 symbols.
It is reason-able to believe that the bracketing inverted rule is inthe grammar already, but this still leaves 18 symbols,which is decidedly longer than 16 symbols?and weneed to get the length to be shorter if we want to seea net gain, since the length of the corpus given thegrammar is likely to be longer with the segmentedrules.
What we really need to do is find a way toreuse the lexical rules that came out of the segmen-tation.
Now suppose the grammar also contained thisterminal rule:A ?
the total fare is five thousand yen/??????????
(Chinese gloss: ?z?ng g?ng de f?i y?ng sh?
w?
q?anr?
y?an?).
This rule can also be split into three rules:A ?
[AA] ,A ?
the total fare is/?????
?,A ?
five thousand yen/???
?Again, we will assume that the structural rule is al-ready present in the grammar, the old rule was 19symbols long, and the two new terminal rules are12+9 = 21 symbols long.
Again we are out of luck,as the new rules are longer than the old one, and threerules are likely to be less probable than one rule dur-ing parsing.
The way to make this work is to realizethat the two existing rules share a bilingual affix?abiaffix: ?five thousand dollars?
translating into ??????.
If we make the two changes at the sametime, we get rid of 16 + 19 = 35 symbols worth ofrules, and introduce a mere 9 + 9 + 12 = 30 sym-bols worth of rules (assuming the structural rules arealready in the grammar).
Making these two changesat the same time is essential, as the length of the fivesaved symbols can be used to offset the likely in-crease in the length of the corpus given the data.
Andof course: the more rules we can find with shared bi-affixes, the more likely we are to find a good set ofsegmentations.Our algorithm takes advantage of the above obser-vation by focusing on the biaffixes found in the train-ing data.
Each biaffix defines a set of lexical rulespaired up with a possible segmentation.
We evaluatethe biaffixes by estimating the change in descriptionlength associated with committing to all the segmen-tations defined by a biaffix.
This allows us to findthe best set of segmentations, but rather than com-mitting only to the one best set of segmentations, wewill collect all sets which would improve descrip-tion length, and try to commit to as many of themas possible.
The pseudocode for our algorithm is asfollows:G // The grammarbiaffixes_to_rules // Maps biaffixes to the// rules they occur inbiaffixes_delta = [] // A list of biaffixes and// their DL impact on Gfor each biaffix b :delta = eval_dl(b, biaffixes_to_rules[b], G)if (delta < 0)biaffixes_delta.push(b, delta)sort_by_delta(biaffixes_delta)for each b:delta pair in biaffixes_delta :real_delta = eval_dl(b, biaffixes_to_rules[b], G)if (real_delta < 0)G = make_segmentations(b, biaffixes_to_rules[b], G)The methods eval_dl, sort_by_delta andmake_segmentations evaluates the impact on de-scription length that committing to a biaffix wouldcause, sorts a list of biaffixes according to this delta,and applies all the changes associated with a biaffixto the grammar, respectively.Evaluating the impact on description lengthbreaks down into two parts: the difference in de-scription length of the grammar DL (G?)
?
DL (G)(where G?
is the grammar that results from applyingall the changes that committing to a biaffix dictates),52and the difference in description length of the corpusgiven the grammar DL (C|G?)
?
DL (C|G).
Thesetwo quantities are simply added up to get the totalchange in description length.The difference in grammar length is calculatedas described in Section 2.2.
The difference in de-scription length of the corpus given the grammarcan be calculated by biparsing the corpus, sinceDL (C|G?)
= ?log2 (P (C|p?))
and DL (C|G) =?log2 (P (C|p)) where p?
and p are the rule prob-ability functions of G?
and G respectively.
Bipars-ing is, however, a very costly process that we do notwant to have inside a loop.
Instead, we assume thatwe have the original corpus probability (through bi-parsing outside the loop), and estimate the new cor-pus probability from it (in closed form).
Given thatwe are splitting the rule r0 into the three rules r1,r2 and r3, and that the probability mass of r0 is dis-tributed uniformly over the new rules, the new ruleprobability function p?
will be identical to p, exceptthat:p?
(r0) = 0,p?
(r1) = p (r1) +13p (r0) ,p?
(r2) = p (r2) +13p (r0) ,p?
(r3) = p (r3) +13p (r0)Since we have eliminated all the occurrences of r0and replaced them with combinations of r1, r2 andr3, the probability of the corpus given this new ruleprobability function will be:P(C|p?
)= P (C|p) p?
(r1) p?
(r2) p?
(r3)p (r0)To make this into a description length, we need totake the negative logarithm of the above, which re-sults in:DL(C|G?
)=DL (C|G) ?
log2(p?
(r1) p?
(r2) p?
(r3)p (r0))The difference in description length of the corpusgiven the grammar can now be expressed as:DL (C|G?)
?
DL (C|G) =?log2(p?(r1)p?(r2)p?
(r3)p(r0))To calculate the impact of a set of segmentations, weneed to take all the changes into account in one go.We do this in a two-pass fashion, first calculatingthe new probability function (p?)
and the change ingrammar description length (taking care not to countthe same rule twice), and then, in the second pass,calculating the change in corpus description length.5 Model combinationThemodel we learn by iteratively subsegmenting thetraining data is guaranteed to be parsimonious whileretaining a decent fit to the training data; these aredesirable qualities, but there is a real risk that wefailed to make some generalization that we shouldhave made; to counter this risk, we can use a modeltrained under more liberal conditions.
We chose theapproach taken by Saers et al(2012) for two rea-sons: (a) the model has the same form as our model,which means that we can integrate it seamlessly, and(b) their aims are similar to ours but their methoddiffers significantly; specifically, they let the modelgrow in size as long as the data reduces in size.
Boththese qualities make it a suitable complement for ourmodel.Assuming we have two grammars (Ga and Gb)that we want to combine, the interpolation param-eter ?
will determine the probability function of thecombined grammar such that:pa+b (r) = ?pa (r) + (1 ?
?
)pb (r)for all rules r in the union of the two rule sets, andwhere pa+b is the rule probability function of thecombined grammar and pa and pb are the rule prob-ability functions of Ga and Gb respectively.
Someinitial experiments indicated that an ?
value of about0.4 was reasonable (when Ga was the grammar ob-tained through the training scheme outlined above,andGb was the grammar obtained through the train-ing scheme outlined in Saers et al(2012)), so weused 0.4 in this paper.6 Experimental setupWe have made the claim that iterative top-down seg-mentation guided by the objective of minimizing thedescription length gives a better precision grammarthan iterative bottom-up chunking, and that the com-bination of the two gives superior results to either53024681012140  1  2  3  4  5  6  7Probabilityinlogdomain (Mbit)IterationsFigure 1: Description length in bits over the different it-erations of top-down search.
The lower portion representsDL (G) and the upper portion represents DL (C|G).approach in isolation.
We have outlined how thiscan be done in practice, and we now substantiate thatclaim empirically.We will initialize a stochastic bracketing inver-sion transduction grammar (BITG) to rewrite it?sone nonterminal symbol directly into all the sen-tence pairs of the training data (iteration 0).
We willthen segment the grammar iteratively a total of seventimes (iterations 1?7).
For each iteration we willrecord the change in description length and test thegrammar.
Each iteration requires us to biparse thetraining data, which we do with the cubic time algo-rithm described in Saers et al(2009), with a beamwidth of 100.As training data, we use the IWSLT07 Chinese?English data set (Fordyce, 2007), which contains46,867 sentence pairs of training data, 506 Chinesesentences of development data with 16 English ref-erence translations, and 489 Chinese sentences with6 English reference translations each as test data; allthe sentences are taken from the traveling domain.Since the Chinese is written without whitespace, weuse a tool that tries to clump characters together intomore ?word like?
sequences (Wu, 1999).As the bottom-up grammar, we will reuse thegrammar learned in Saers et al(2012), specifically,we will use the BITG that was bootstrapped froma bracketing finite-state transduction grammar (BF-STG) that has been chunked twice, giving bitermi-nals where the monolingual segments are 0?4 tokenslong.
The bottom-up grammar is trained on the same01020304050600  1  2  3  4  5  6  7Numberofrules (thousands)IterationsFigure 2: Number of rules learned during top-downsearch over the different iterations.data as our model.To test the learned grammars as translation mod-els, we first tune the grammar parameters to the train-ing data using expectation maximization (Dempsteret al 1977) and parse forests acquired with theabove mentioned biparser, again with a beam widthof 100.
To do the actual decoding, we use ourin-house ITG decoder.
The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965;Younger, 1967) and cube pruning (Chiang, 2007) tointegrate the language model scores.
The decoderbuilds an efficient hypergraph structure which is thenscored using both the induced grammar and the lan-guage model.
The weights for the language modeland the grammar, are tuned towards BLEU (Papineniet al 2002) using MERT (Och, 2003).
We use theZMERT (Zaidan, 2009) implementation ofMERT asit is a robust and flexible implementation of MERT,while being loosely coupled with the decoder.
Weuse SRILM (Stolcke, 2002) for training a trigramlanguage model on the English side of the trainingdata.
To evaluate the quality of the resulting transla-tions, we use BLEU, and NIST (Doddington, 2002).7 Experimental resultsThe results from running the experiments detailedin the previous section can be summarized in fourgraphs.
Figures 1 and 2 show the size of our new,segmenting model during induction, in terms of de-scription length and in terms of rule count.
The ini-tial ITG is at iteration 0, where the vast majority540.000.050.100.150.200  1  2  3  4  5  6  7BLEUIterationsFigure 3: Variations in BLEU score over different iter-ations.
The thin line represents the baseline bottom-upsearch (Saers et al 2012), the dotted line represents thetop-down search, and the thick line represents the com-bined results.of the size is taken up by the model (DL (G)), andvery little by the data (DL (C|G))?just as we pre-dicted.
The trend over the induction phase is a sharpdecrease in model size, and a moderate increase indata size, with the overall size constantly decreas-ing.
Note that, although the number of rules rises,the total description length decreases.
Again, this isprecisely what we expected.
The size of the modellearned according to Saers et al(2012) is close to 30Mbits?far off the chart.
This shows that our newtop-down approach is indeed learning a more parsi-monious grammar than the bottom-up approach.Figures 3 and 4 shows the translation quality ofthe learned model.
The thin flat lines show the qual-ity of the bottom-up approach (Saers et al 2012),whereas the thick curves shows the quality of thenew, top-down model presented in this paper with-out (dotted line), and without the bottom-up model(solid line).
Although the MDL-based model is bet-ter than the old model, the combination of the twois still superior.
It is particularly encouraging to seethat the over-fitting that seems to take place after iter-ation 3 with the MDL-based approach is amelioratedwith the bottom-up model.8 ConclusionsWe have introduced a purely unsupervised learningscheme for phrasal stochastic inversion transductiongrammars that is the first to combine two oppos-0.01.02.03.04.05.00  1  2  3  4  5  6  7NISTIterationsFigure 4: Variations in NIST score over different iter-ations.
The thin line represents the baseline bottom-upsearch (Saers et al 2012), the dotted line represents thetop-down search, and the thick line represents the com-bined results.ing ways of searching for the phrasal translations: abottom-up rule chunking approach driven by a maxi-mum likelihood (ML) objective and a top-down rulesegmenting approach driven by a minimum descrip-tion length (MDL) objective.
The combination ap-proach takes advantage of the fact that the conser-vative top-down MDL-driven rule segmenting ap-proach learns a very parsimonious, yet competitive,model when compared to a liberal bottom-up ML-driven approach.
Results show that the combinationof the two opposing approaches is significantly su-perior to either of them in isolation.9 AcknowledgementsThis material is based upon work supported in partby the Defense Advanced Research Projects Agency(DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by the EuropeanUnion under the FP7 grant agreement no.
287658;and by the Hong Kong Research Grants Council(RGC) research grants GRF620811, GRF621008,and GRF612806.
Any opinions, findings and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of DARPA, the EU, or RGC.55ReferencesP.
Blunsom and T. Cohn.
Inducing syn-chronous grammars with slice sampling.
InHLT/NAACL2010, pages 238?241, Los Angeles,California, June 2010.P.
Blunsom, T. Cohn, and M. Osborne.
Bayesiansynchronous grammar induction.
In Proceedingsof NIPS 21, Vancouver, Canada, December 2008.P.
Blunsom, T. Cohn, C. Dyer, and M. Osborne.
Agibbs sampler for phrasal synchronous grammarinduction.
In Proceedings of ACL/IJCNLP, pages782?790, Suntec, Singapore, August 2009.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L.Mercer.
TheMathematics ofMachine Trans-lation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311, 1993.C.
Cherry and D. Lin.
Inversion transduction gram-mar for joint phrasal translation modeling.
In Pro-ceedings of SSST, pages 17?24, Rochester, NewYork, April 2007.D.
Chiang.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228, 2007.J.
Cocke.
Programming languages and their compil-ers: Preliminary notes.
Courant Institute ofMath-ematical Sciences, New York University, 1969.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
Max-imum likelihood from incomplete data via the emalgorithm.
Journal of the Royal Statistical Soci-ety.
Series B (Methodological), 39(1):1?38, 1977.G.
Doddington.
Automatic evaluation of machinetranslation quality using n-gram co-occurrencestatistics.
In Proceedings of the 2nd InternationalConference on Human Language Technology Re-search, pages 138?145, San Diego, California,2002.C.
S. Fordyce.
Overview of the IWSLT 2007 evalu-ation campaign.
In Proceedings of IWSLT, pages1?12, 2007.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang, and I. Thayer.
Scalable infer-ence and training of context-rich syntactic trans-lation models.
In Proceedings of COLING/ACL-2006, pages 961?968, Sydney, Australia, July2006.Peter Gr?nwald.
A minimum description length ap-proach to grammar inference in symbolic.
LectureNotes in Artificial Intelligence, (1040):203?216,1996.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.Better word alignments with supervised itg mod-els.
In Proceedings of ACL/IJCNLP-2009, pages923?931, Suntec, Singapore, August 2009.H.
Johnson, J. Martin, G. Foster, and R. Kuhn.Improving translation quality by discarding mostof the phrasetable.
In Proceedings of EMNLP-CoNLL-2007, pages 967?975, Prague, Czech Re-public, June 2007.T.
Kasami.
An efficient recognition and syntax anal-ysis algorithm for context-free languages.
Tech-nical Report AFCRL-65-00143, Air Force Cam-bridge Research Laboratory, 1965.P.
Koehn, F. J. Och, and D. Marcu.
StatisticalPhrase-Based Translation.
In Proceedings ofHLT/NAACL-2003, volume 1, pages 48?54, Ed-monton, Canada, May/June 2003.G.
Neubig, T. Watanabe, E. Sumita, S. Mori, andT.
Kawahara.
An unsupervised model for jointphrase alignment and extraction.
In Proceedingsof ACL/HLT-2011, pages 632?641, Portland, Ore-gon, June 2011.G.
Neubig, T. Watanabe, S. Mori, and T. Kawahara.Machine translation without words through sub-string alignment.
In Proceedings of ACL-2012,pages 165?174, Jeju Island, Korea, July 2012.F.
J. Och and H. Ney.
A Systematic Comparison ofVarious Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51, 2003.F.
J. Och.
Minimum error rate training in statisticalmachine translation.
InProceedings of ACL-2003,pages 160?167, Sapporo, Japan, July 2003.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of ACL-2002,pages 311?318, Philadelphia, Pennsylvania, July2002.J.
Rissanen.
A universal prior for integers and esti-mation by minimum description length.
The An-nals of Statistics, 11(2):416?431, June 1983.56M.
Saers and D. Wu.
Improving phrase-basedtranslation via word alignments from StochasticInversion Transduction Grammars.
In Proceed-ings of SSST-3, pages 28?36, Boulder, Colorado,June 2009.M.
Saers and D. Wu.
Principled induction of phrasalbilexica.
In Proceedings of EAMT-2011, pages313?320, Leuven, Belgium, May 2011.M.
Saers, J. Nivre, and D. Wu.
Learning stochasticbracketing inversion transduction grammars witha cubic time biparsing algorithm.
In Proceedingsof IWPT?09, pages 29?32, Paris, France, October2009.M.
Saers, J. Nivre, and D. Wu.
Word alignment withstochastic bracketing linear inversion transduc-tion grammar.
In Proceedings of HLT/NAACL-2010, pages 341?344, Los Angeles, California,June 2010.M.
Saers, K. Addanki, and D. Wu.
From finite-stateto inversion transductions: Toward unsupervisedbilingual grammar induction.
In Proceedings ofCOLING 2012: Technical Papers, pages 2325?2340, Mumbai, India, December 2012.C.
E. Shannon.
A mathematical theory of com-munication.
The Bell System Technical Journal,27:379?423, 623?, July, October 1948.Z.
Si, M. Pei, B. Yao, and S. Zhu.
Unsuper-vised learning of event and-or grammar and se-mantics from video.
In Proceedings of the 2011IEEE International Conference on Computer Vi-sion (ICCV), pages 41?48, November 2011.R.
J. Solomonoff.
A new method for discovering thegrammars of phrase structure languages.
In IFIPCongress, pages 285?289, 1959.A.
Stolcke and S. Omohundro.
Inducing proba-bilistic grammars by bayesian model merging.
InR.
C. Carrasco and J. Oncina, editors, Grammat-ical Inference and Applications, pages 106?118.Springer, 1994.A.
Stolcke.
SRILM ?
an extensible language model-ing toolkit.
In Proceedings of ICSLP-2002, pages901?904, Denver, Colorado, September 2002.J.
M. Vilar and E. Vidal.
A recursive statistical trans-lation model.
In ACL-2005 Workshop on BuildingandUsing Parallel Texts, pages 199?207, AnnAr-bor, Jun 2005.S.
Vogel, H. Ney, and C. Tillmann.
HMM-basedWord Alignment in Statistical Translation.
In Pro-ceedings of COLING-96, volume 2, pages 836?841, 1996.D.
Wu.
Stochastic Inversion Transduction Gram-mars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377?403, 1997.Z.
Wu.
LDC Chinese segmenter, 1999.D.
H. Younger.
Recognition and parsing of context-free languages in time n3.
Information and Con-trol, 10(2):189?208, 1967.O.
F. Zaidan.
Z-MERT: A Fully Configurable OpenSource Tool for Minimum Error Rate Trainingof Machine Translation Systems.
The PragueBulletin of Mathematical Linguistics, 91:79?88,2009.H.
Zhang, C. Quirk, R. C. Moore, and D. Gildea.Bayesian learning of non-compositional phraseswith synchronous parsing.
In Proceedings ofACL-08: HLT, pages 97?105, Columbus, Ohio,June 2008.57
