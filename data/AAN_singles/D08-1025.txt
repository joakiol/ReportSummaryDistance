Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 234?243,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA noisy-channel model of rational human sentence comprehension underuncertain inputRoger LevyDepartment of LinguisticsUniversity of California ?
San Diego9500 Gilman Drive #0108La Jolla, CA 92093-0108rlevy@ling.ucsd.edu ?AbstractLanguage comprehension, as with all othercases of the extraction of meaningful struc-ture from perceptual input, takes places un-der noisy conditions.
If human languagecomprehension is a rational process in thesense of making use of all available infor-mation sources, then we might expect uncer-tainty at the level of word-level input to af-fect sentence-level comprehension.
However,nearly all contemporary models of sentencecomprehension assume clean input?that is,that the input to the sentence-level com-prehension mechanism is a perfectly-formed,completely certain sequence of input tokens(words).
This article presents a simple modelof rational human sentence comprehensionunder noisy input, and uses the model to in-vestigate some outstanding problems in thepsycholinguistic literature for theories of ra-tional human sentence comprehension.
Weargue that by explicitly accounting for input-level noise in sentence processing, our modelprovides solutions for these outstanding prob-lems and broadens the scope of theories of hu-man sentence comprehension as rational prob-abilistic inference.
?Part of this work has benefited from presentation at the21st annual meeting of the CUNY Sentence Processing Confer-ence in Chapel Hill, NC, 14 March 2008, and at a seminar at theCenter for Research on Language, UC San Diego.
I am gratefulto Klinton Bicknell, Andy Kehler, and three anonymous review-ers for comments and suggestions, Cyril Allauzen for guidanceregarding the OpenFST library, and to Mark Johnson, Mark-Jan Nederhof, and Noah Smith for discussion of renormalizingweighted CFGs.1 IntroductionConsidering the adversity of the conditions underwhich linguistic communication takes place in ev-eryday life?ambiguity of the signal, environmentalcompetition for our attention, speaker error, and soforth?it is perhaps remarkable that we are as suc-cessful at it as we are.
Perhaps the leading expla-nation of this success is that (a) the linguistic sig-nal is redundant, and (b) diverse information sourcesare generally available that can help us obtain inferthe intended message (or something close enough)when comprehending an utterance (Tanenhaus et al,1995; Altmann and Kamide, 1999; Genzel and Char-niak, 2002, 2003; Aylett and Turk, 2004; Keller,2004; Levy and Jaeger, 2007).
Given the difficultyof this task coupled with the availability of redun-dancy and useful information sources, it would seemrational for all available information to be used toits fullest in sentence comprehension.
This idea iseither implicit or explicit in several interactivist the-ories of probabilistic language comprehension (Ju-rafsky, 1996; Hale, 2001; Narayanan and Jurafsky,2002; Levy, 2008).
However, these theories haveimplicitly assumed a partitioning of interactivity thatdistinguishes the word as a fundamental level oflinguistic information processing: word recognitionis an evidential process whose output is nonethe-less a specific ?winner-takes-all?
sequence of words,which is in turn the input to an evidential sentence-comprehension process.
It is theoretically possiblethat this partition is real and is an optimal solutionto the problem of language comprehension undergross architectural constraints that favor modularity234(Fodor, 1983).
On the other hand, it is also possiblethat this partition has been a theoretical conveniencebut that, in fact, evidence at the sub-word level playsan important role in sentence processing, and thatsentence-level information can in turn affect wordrecognition.
If the latter is the case, then the ques-tion arises of how we might model this type of infor-mation flow, and what consequences it might havefor our understanding of human language compre-hension.
This article employs the well-understoodformalisms of probabilistic context-free grammars(PCFGs) and weighted finite-state automata (wF-SAs) to propose a novel yet simple noisy-channelprobabilistic model of sentence comprehension un-der circumstances where there is uncertainty aboutword-level representations.
Section 2 introduces thismodel.
We use this new model to investigate twooutstanding problems for the theory of rational sen-tence comprehension: one involving global infer-ence?the beliefs that a human comprehender ar-rives at regarding the meaning of a sentence afterreading it in its entirety (Section 3)?and one involv-ing incremental inference?the beliefs that a com-prehender forms and updates moment by momentwhile reading each part of it (Section 4).
The com-mon challenge posed by each of these problems isan apparent failure on the part of the comprehenderto use information made available in one part of asentence to rule out an interpretation of another partof the sentence that is inconsistent with this informa-tion.
In each case, we will see that the introductionof uncertainty into the input representation, coupledwith noisy-channel inference, provides a unified so-lution within a theory of rational comprehension.2 Sentence comprehension underuncertain inputThe use of generative probabilistic grammars forparsing is well understood (e.g., Charniak, 1997;Collins, 1999).
The problem of using a probabilisticgrammar G to find the ?best parse?
T for a knowninput string w is formulated as11By assumption, G is defined such that its complete pro-ductions T completely specify the string, such that P (w|T ) isnon-zero for only one value of w.argmaxTPG(T |w) (I)but a generative grammar directly defines the jointdistribution PG(T,w) rather than the conditionaldistribution.
In this case, Bayes?
rule is used to findthe posterior:PG(T |w) =P (T,w)P (w) (II)?
P (T,w) (III)If the input string is unknown, the problemchanges.
Suppose we have some noisy evidence Ithat determines a probability distribution over inputstrings P (w|I).
We can still use Bayes?
rule to ob-tain the posterior:PG(T |I) =P (T, I)P (I) (IV)?
?wP (I|T,w)P (w|T )P (T ) (V)Likewise, if we are focused on inferring whichwords were seen given an uncertain input, we havePG(w|I) ?
?TP (I|T,w)P (w|T )P (T ) (VI)2.1 Uncertainty for a Known InputThis paper considers situations such as controlledpsycholinguistic experiments where we (the re-searchers) know the sentence w?
presented to acomprehender, but do not know the specific input Ithat the comprehender obtains.
In this case, if weare, for example, interested in the expected infer-ences of a rational comprehender about what wordstring she was exposed to, the probability distribu-tion of interest isP (w|w?)
=?IPC(w|I,w?
)PT (I|w?)
dI (VII)where PC is the probability distribution used by thecomprehender to process perceived input, and PTis the ?true?
probability distribution over the inputs235that might actually be perceived given the true sen-tence.
Since the comprehender does not observe w?we must have conditional independence between wand w?
given I .
We can then apply Bayes?
rule to(VII) to obtainP (w|w?)
=?IPC(I|w)PC(w)PC(I)PT (I|w?)
dI(VIII)= PC(w)?IPC(I|w)PT (I|w?)PC(I)dI(IX)?
PC(w)Q(w,w?)
(X)where Q(w,w?)
is proportional to the integral termin Equation (IX).
The term PC(w) correspondsto the comprehender?s prior beliefs; the integralterm is the effect of input uncertainty.
If com-prehenders model noise rationally, then we shouldhave PC(I|w) = PT (I|w), and thus Q(w,w?
)becomes a symmetric, non-negative function of wand w?
; hence the effect of input uncertainty canbe modeled by a kernel function on input stringpairs.
(Similar conclusions result when the poste-rior distribution of interest is over structures T .)
Itis an open question which kernel functions mightbest model the inferences made in human sentencecomprehension.
Most obviously the kernel func-tion should account for noise (environmental, per-ceptual, and attentional) introduced into the signalen route to the neural stage of abstract sentenceprocessing.
In addition, this kernel function mightalso be a natural means of accounting for modelingerror such as disfluencies (Johnson and Charniak,2004), word/phrase swaps, and even well-formed ut-terances that the speaker did not intend.
For pur-poses of this paper, we limit ourselves to a simplekernel based on the Levenshtein distance LD(w,w?
)between words and constructed in the form of aweighted finite-state automaton (Mohri, 1997).2.2 The Levenshtein-distance kernelSuppose that the input word string w?
consists ofwords w1...n. We define the Levenshtein-distancekernel as follows.
Start with a weighted finite-stateautomaton in the log semiring over the vocabulary?
with states 0 .
.
.
n, state 0 being the start state0a/1cat/3sat/31<eps>/1acat/2sat/2a/1cat/3sat/32<eps>/3a/2catsat/1a/1cat/3sat/33<eps>/3a/2cat/1sata/1cat/3sat/3Figure 1: The Levenshtein-distance kernel for multi-word string edits.
KLD(w?)
is shown for ?
={cat,sat,a}, w?
= (a cat sat), and ?
= 1.
State 0is the start state, and State 3 is the lone (zero-cost)final state.and n the (zero-cost) final state.
We add two typesof arcs to this automaton: (a) substitution/deletionarcs (i ?
1, w?)
?
i, i ?
1, .
.
.
, n, each with cost?LD(wi, w?
), for all w?
?
?
?
{?
}; and (b) in-sertion loop arcs (j, w?)
?
j, j ?
0, .
.
.
, n, eachwith cost ?LD(?, w?
), for all w?
?
?.2 The result-ing wFSA KLD(w?)
defines a function over w suchthat the summed weight of paths through the wFSAaccepting w is logQ(w,w?).
This kernel allows forthe possibility of word substitutions (represented bythe transition arcs with labels that are neither wi nor?
), word deletions (represented by the transition arcswith ?
labels), and even word insertions (representedby the loop arcs).
The unnormalized probability ofeach type of operation is exponential in the Leven-shtein distance of the change induced by the oper-ation.
The term ?
is a free parameter, with smallervalues corresponding to noisier input.
Figure 1 givesan example of the Levenshtein-distance kernel for asimple vocabulary and sentence.32For purposes of computing the Levenshtein distance be-tween words, the epsilon label ?
is considered to be a zero-length letter string.3The Levenshtein-distance kernel can be seen to be sym-metric in w,w?
as follows.
Any path accepting w in thewFSA generated from w?
involves the following non-zero-cost transitions: insertions w?I1...i, deletions wD1...j , and substi-tutions (w ?
w?)S1...k.
For each such path P , there will beexactly one path P ?
in the wFSA generated from w that ac-cepts w?
with insertions wD1...j , deletions w?I1...i, and substitu-tions (w?
?
w)S1...k. Due to the symmetry of the Levenshteindistance, the paths P and P ?
will have identical costs.
There-fore the kernel is indeed symmetric.2362.3 Efficient computation of posterior beliefsThe problem of finding structures or strings withhigh posterior probability given a particular inputstring w?
is quite similar to the problem faced inthe parsing of speech, where the acoustic input I toa parser can be represented as a lattice of possibleword sequences, and the edges of the lattice haveweights determined by a model of acoustic realiza-tion of words, P (I|w) (Collins et al, 2004; Halland Johnson, 2003, 2004).
The two major differ-ences between lattice parsing and our problem are(a) we have integrated out the expected effect ofnoise, which is thus implicit in our choice of kernel;and (b) the loops in the Levenshtein-distance kernelmean that the input to parsing is no longer a lattice.This latter difference means that some of the tech-niques applicable to string parsing and lattice pars-ing ?
notably the computation of inside probabilities?
are no longer possible using exact methods.
Wereturn to this difference in Sections 3 and 4.3 Global inferenceOne clear prediction of the uncertain-input model of(VII)?
(X) is that under appropriate circumstances,the prior expectations PC(w) of the comprehen-der should in principle be able to override the lin-guistic input actually presented, so that a sentenceis interpreted as meaning?and perhaps even be-ing?something other than it actually meant or was.At one level, it is totally clear that comprehendersdo this on a regular basis: the ability to do thisis required for someone to act as a copy editor?that is, to notice and (crucially) correct mistakeson the printed page.
In many cases, these typesof correction happen at a level that may be belowconsciousness?thus we sometimes miss a typo butinterpret the sentences as it was intended, or ignorethe disfluency of a speaker.
What has not been pre-viously proposed in a formal model, however, is thatthis can happen even when an input is a completelygrammatical sentence.
Here, we argue that an ef-fect demonstrated by Christianson et al (2001) (seealso Ferreira et al, 2002) is an example of expec-tations overriding input.
When presented sentencesof the forms in (1) using methods that did not per-mit rereading, and asked questions of the type Didthe man hunt the deer?, experimental participantsgave affirmative responses significantly more oftenfor sentences of type (1a), in which the substring theman hunted the deer appears, than for either (1b) or(1c).
(1) a.
While the man hunted the deer ran intothe woods.
(GARDENPATH)b.
While the man hunted the pheasant thedeer ran into the woods.
(TRANSITIVE)c. While the man hunted, the deer ran intothe woods.
(COMMA)This result was interpreted by Christianson et al(2001) and Ferreira et al (2002) as reflecting (i)the fact that there is a syntactic garden path in(1a)?after reading the first six words of the sen-tence, the preferred interpretation of the substringthe man hunted the deer is as a simple clause in-dicating that the deer was hunted by the man?and(ii) that readers were not always successful at revis-ing away this interpretation when they saw the dis-ambiguating verb ran, which signals that the deeris actually the subject of the main clause, and thathunted must therefore be intransitive.
Furthermore(and crucially), for (1a) participants also respondedaffirmatively most of the time to questions of thetype Did the deer run into the woods?
This resultis a puzzle for existing models of sentence compre-hension because no grammatical analysis exists ofany substring of (1a) for which the deer is both theobject of hunted and the subject of ran.
In fact, noformal model has yet been proposed to account forthis effect.The uncertain-input model gives us a means ofaccounting for these results, because there are nearneighbors of (1a) for which there is a global gram-matical analysis in which either the deer or a coref-erent NP is in fact the object of the subordinate-clause verb hunted.
In particular, inserting the wordit either before or after the deer creates such a nearneighbor:(2) a.
While the man hunted the deer it raninto the woods.b.
While the man hunted it the deer raninto the woods.We formalize this intuition within our model by us-ing the wFSA representation of the Levenshtein-237ROOT ?
S PUNCT.
0.0S ?
SBAR S 6.3S ?
SBAR PUNCT S 4.6PUNCT S ?
, S 0.0S ?
NP VP 0.1SBAR ?
IN S 0.0NP ?
DT NN 1.9NP ?
NNS 4.4NP ?
NNP 3.3NP ?
DT NNS 4.5NP ?
PRP 1.3NP ?
NN 3.1VP ?
VBD RB 9.7VP ?
VBD PP 2.2VP ?
VBD NP 1.2VP ?
VBD RP 8.3VP ?
VBD 2.0VP ?
VBD JJ 3.4PP ?
IN NP 0.0Figure 2: The PCFG used in the global-inferencestudy of Section 3.
Rule weights given as negativelog-probabilities in bits.distance kernel.
A probabilistic context-free gram-mar (PCFG) representing the comprehender?s gram-matical knowledge can be intersected with thatwFSA using well-understood techniques, generatinga new weighted CFG (Bar-Hillel et al, 1964; Neder-hof and Satta, 2003).
This intersection thus repre-sents the unnormalized posterior PC(T,w|w?).
Be-cause there are loops in the wFSA generated by theLevenshtein-distance kernel, exact normalization ofthe posterior is not tractable (though see Neder-hof and Satta, 2003; Chi, 1999; Smith and John-son, 2007 for possible approaches to approximat-ing the normalization constant).
We can, however,use the lazy k-best algorithm of Huang and Chiang(2005; Algorithm 3) to obtain the word-string/parse-tree pairs with highest posterior probability.3.1 Experimental VerificationTo test our account of the rational noisy-channel in-terpretation of sentences such as (1), we defined asmall PCFG using the phrasal rules listed in Figure2, with rule probabilities estimated from the parsedBrown corpus.4 Lexical rewrite probabilities weredetermined using relative-frequency estimation overthe entire parsed Brown corpus.
For each of the sen-tence sets like (1) used in Experiments 1a, 1b, and 2of Christianson et al (2001) that have complete lex-ical coverage in the parsed Brown corpus (22 setsin total), a noisy-input wFSA was constructed us-ing KLD, permitting all words occurring more than2500 times in the parsed Brown corpus as possi-ble edit/insertion targets.5 Figure 3 shows the av-erage proportion of parse trees among the 100 bestparses in the intersection between this PCFG and thewFSA for each sentence for which an interpretationis available such that the deer or a coreferent NP isthe direct object of hunted.6 The Levenshtein dis-tance penalty ?
is a free parameter in the model, butthe results are consistent for a wide range of ?
: in-terpretations of type (2) are more prevalent both interms of number mass for (1a) than for either (1b)or (1c).
Furthermore, across 9 noise values for 22sentence sets, there were never more interpretationsof type (2) for COMMA sentences than for the cor-responding GARDENPATH sentences, and in onlyone case were there more such interpretations fora TRANSITIVE sentence than for the correspondingGARDENPATH sentence.4 Incremental comprehension and erroridentificationWe begin taking up the role of input uncertainty forincremental comprehension by posing a question:4Counts of these rules were obtained usingtgrep2/Tregex tree-matching patterns (Rohde,2005; Levy and Andrew, 2006), available online athttp://idiom.ucsd.edu/?rlevy/papers/emnlp2008/tregex_patterns.
We have also in-vestigated the use of broad-coverage PCFGs estimated usingstandard treebank-based techniques, but found that the compu-tational cost of inference with treebank-sized grammars wasprohibitive.5The word-frequency cutoff was introduced for computa-tional speed; we have obtained qualitatively similar results withlower word-frequency cutoffs.6We took a parse tree to satisfy this criterion if the NPthe deer appeared either as the matrix-clause subject or theembedded-clause object, and a pronoun appeared in the otherposition.
In a finer-grained grammatical model, number/genderagreement would be enforced between such a pronoun and theNP in the posterior, so that the probability mass for these parseswould be concentrated on cases where the pronoun is it.2380 5 10 15 200510152025Levenshtein edit distance penalty (bits)#Misleadingparsesintop100GardenPathCommaTransitiveFigure 3: Results for 100-best global inference, asa function of the Levenshtein distance penalty ?
(inbits).what is the optimal way to read a sentence on a page(Legge et al, 1997)?
Presumably, the goal of read-ing is to find a good compromise between scanningthe contents of the sentence as quickly as possiblewhile achieving an accurate understanding of thesentence?s meaning.
To a first approximation, hu-mans solve this problem by reading each sentence ina document from beginning to end, regardless of theactual layout; whether this general solution is bestunderstood in terms of optimality or rather as para-sitic on spoken language comprehension is an openquestion beyond the immediate scope of the presentpaper.
However, about 10?15% of eye movements inreading are regressive (Rayner, 1998), and we mayusefully refine our question to when a regressive eyemovement might be a good decision.
In traditionalmodels of sentence comprehension, the optimal an-swer would certainly be ?never?, since past observa-tions are known with certainty.
But once uncertaintyabout the past is accounted for, it is clear that theremay in principle be situations in which regressivesaccades may be the best choice.What are these situations?
One possible answerwould be: when the uncertainty (e.g., measured byentropy) about an earlier part of the sentence is high.There are some cases in which this is probably thecorrect answer: many regressive eye movements arevery small and the consensus in the eye-movementliterature is that they represent corrections for motorerror at the saccadic level.
That is, the eyes over-shoot the intended target and regress to obtain in-formation about what was missed.
However, mo-tor error can account only for short, isolated regres-sions, and about one-sixth of regressions are part ofa longer series back into the sentence, into a muchearlier part of the text which has already been read.We propose that these regressive saccades might bethe best choice when the most recent observed in-put significantly changes the comprehender?s beliefsabout the earlier parts of the sentence.
To make thediscussion more concrete, we turn to another recentresult in the psycholinguistic literature that has beenargued to be problematic for rational theories of sen-tence comprehension.It has been shown (Tabor et al, 2004) that sen-tences such as (3) below induce considerable pro-cessing difficulty at the word tossed, as measured inword-by-word reading times:(3) The coach smiled at the player tossed a fris-bee.
(LOCALLY COHERENT)Both intuition and controlled experiments reveal thatthis difficulty seems due at least in part to the cat-egory ambiguity of the word tossed, which is oc-casionally used as a participial verb but is muchmore frequently used as a simple-past verb.
Al-though tossed in (3) is actually a participial verb in-troducing a reduced relative clause (and the playeris hence its recipient), most native English speakersfind it extremely difficult not to interpret tossed as amain verb and the player as its agent?far more dif-ficult than for corresponding sentences in which thecritical participial verb is morphologically distinctfrom the simple past form ((4a), (4c); c.f.
threw) orin which the relative clause is unreduced and thusclearly marked ((4b), (4c)).
(4) a.
The coach smiled at the player thrown afrisbee.
(LOCALLY INCOHERENT)b.
The coach smiled at the player who wastossed a frisbee.c.
The coach smiled at the player who wasthrown a frisbee.The puzzle here for rational approaches to sentencecomprehension is that the preceding top-down con-text provided by The coach smiled at.
.
.
should com-pletely rule out the possibility of seeing a mainverb immediately after player, hence a rational com-239prehender should not be distracted by the part-of-speech ambiguity.74.1 An uncertain-input solutionThe solution we pursue to this puzzle lies in the factthat (3) has many near-neighbor sentences in whichthe word tossed is in fact a simple-past tense verb.Several possibilities are listed below in (5):(5) a.
The coach who smiled at the playertossed a frisbee.b.
The coach smiled as the player tossed afrisbee.c.
The coach smiled and the player tosseda frisbee.d.
The coach smiled at the player whotossed a frisbee.e.
The coach smiled at the player thattossed a frisbee.f.
The coach smiled at the player andtossed a frisbee.The basic intuition we follow is that simple-pastverb tossed is much more probable where it appearsin any of (5a)-(5f) than participial tossed is in (3).Therefore, seeing this word causes the comprehen-der to shift her probability distribution about the ear-lier part of the sentence away from (3), where it hadbeen peaked, toward its near neighbors such as theexamples in (5).
This change in beliefs about thepast is treated as an error identification signal (EIS).In reading, a sensible response to an EIS would bea slowdown or a regressive saccade; in spoken lan-guage comprehension, a sensible response would beto allocate more working memory resources to thecomprehension task.4.2 Quantifying the Error Identification SignalWe quantify our proposed error identification sig-nal as follows.
Consider the probability distributionover the input up to, but not including, a position jin a sentence w:7This preceding context sharply distinguishes (3) frombetter-known, traditional garden-path sentences such as Thehorse raced past the barn fell, in which preceding context can-not be used to correctly disambiguate the part of speech of theambiguous verb raced.P (w[0,j)) (XI)We use the subscripting [0, j) to illustrate that thisinterval is ?closed?
through to include the beginningof the string, but ?open?
at position j?that is, it in-cludes all material before position j but does not in-clude anything at that position or beyond.
Let usthen define the posterior distribution after seeing allinput up through and including word i as Pi(w[0,j)).We define the EIS induced by reading a word wi asfollows:D(Pi(w[0,i))||Pi?1(w[0,i))) (XII)??w?
{w[0,i)}Pi (w) logPi (w)Pi?1 (w)(XIII)where D(q||p) is the Kullback-Leibler divergence,or relative entropy, from p to q, a natural way ofquantifying the distance between probability distri-butions (Cover and Thomas, 1991) which has alsobeen argued for previously in modeling attention andsurprise in both visual and linguistic cognition (Ittiand Baldi, 2005; Levy, 2008).4.3 Experimental VerificationAs in Section 3, we use a small probabilistic gram-mar covering the relevant structures in the problemdomain to represent the comprehender?s knowledge,and a wFSA based on the Levenshtein-distance ker-nel to represent noisy input.
We are interested incomparing the EIS at the word tossed in (3) versusthe EIS at the word thrown in (4a).
In this case,the interval w[0,j) contains all the material that couldpossibly have come before the word tossed/thrown,but does not contain material at or after the positionintroduced by the word itself.
Loops in the prob-abilistic grammar and the Levenshtein-distance ker-nel pose a challenge, however, to evaluating the EIS,because the normalization constant of the resultinggrammar/input intersection is essential to evaluat-ing Equation (XIII).
To circumvent this problem,we eliminate loops from the kernel by allowing onlyone insertion per inter-word space.8 (See Section 5for a possible alternative).8Technically, this involves the following transformation ofa Levenshtein-distance wFSA.
First, eliminate all loop arcs.240ROOT ?
S 0.00S ?
S-base CC S-base 7.3S ?
S-base 0.01S-base ?
NP-base VP 0NP ?
NP-base RC 4.1NP ?
NP-base 0.5NP ?
NP-base PP 2.0NP-base ?
DT N N 4.7NP-base ?
DT N 1.9NP-base ?
DT JJ N 3.8NP-base ?
PRP 1.0NP-base ?
NNP 3.1VP/NP ?
V NP 4.0VP/NP ?
V 0.1VP ?
V PP 2.0VP ?
V NP 0.7VP ?
V 2.9RC ?
WP S/NP 0.5RC ?
VP-pass/NP 2.0RC ?
WP FinCop VP-pass/NP 4.9PP ?
IN NP 0S/NP ?
VP 0.7S/NP ?
NP-base VP/NP 1.3VP-pass/NP ?
VBN NP 2.2VP-pass/NP ?
VBN 0.4Figure 4: The grammar used for the incremental-inference experiment of Section 4.
Rule weightsgiven as negative log-probabilities in bits.Figure 4 shows the (finite-state) probabilisticgrammar used for the study, with rule probabilitiesonce again determined from the parsed Brown cor-pus using relative frequency estimation.
To calcu-late the distribution over strings after exposure tothe i-th word in the sentence, we ?cut?
the inputwFSA such that all transitions and arcs after state2i+2 were removed and replaced with a sequence ofstates j = 2i + 3, .
.
.
,m, with zero-cost transitions(j?1, w?)
?
j for all w?
?
??{?
}, and each new jNext, map every state i onto a state pair in a new wFSA(2i, 2i+1), with all incoming arcs in i being incoming into 2i,all outgoing arcs from i being outgoing from 2i + 1, and newtransition arcs (2i, w?)
?
2i + 1 for each w?
?
?
?
{?}
withcost LD(?, w?).
Finally, add initial state 0 to the new wFSAwith transition arcs to state 1 for all w?
?
?
?
{?}
with costLD(?, w?).
A final state i in the old wFSA corresponds to afinal state 2i + 1 in the new wFSA.0 1 2 3 4 5 6 70.00.10.20.30.4Levenshtein edit distance penalty (bits)EISLocally coherentLocally incoherentFigure 5: The Error Identification Signal (EIS) for(3) and (4a), as a function of the Levenshtein dis-tance penalty ?
(in bits)being a zero-cost final state.9 Because the intersec-tion between this ?cut?
wFSA and the probabilisticgrammar is loop-free, it can be renormalized, andthe EIS can be calculated without difficulty.
All thecomputations in this section were carried out usingthe OpenFST library (Allauzen et al, 2007).Figure 5 shows the average magnitude of the EISfor sentences (3) versus (4a) at the critical word po-sition tossed/thrown.
Once again, the Levenshtein-distance penalty ?
is a free parameter in the model,so we show model behavior as a function of ?, forthe eight sentence pairs in Experiment 1 of Taboret al with complete lexical and syntactic coveragefor the grammar of Figure 4.
For values of ?
wherethe EIS is non-negligible, it is consistently larger atthe critical word (tossed in (3), thrown in (4a)) inthe COHERENT condition than in the INCOHERENTcondition.
Across a range of eight noise levels, 67%of sentence pairs had a higher EIS in the COHERENTcondition than in the INCOHERENT condition.
Fur-thermore, the cases where the INCOHERENT condi-tion had a larger EIS occurred only for noise levelsbelow 1.1 and above 3.6, and the maximum such EISwas quite small, at 0.067.
Overall, the model?s be-havior is consistent with the experimental results ofTabor et al (2004), and can be explained through theintuition described at the end of Section 4.1.9The number of states added had little effect on results, solong as at least as many states were added as words remained inthe sentence.2415 ConclusionIn this paper we have outlined a simple model of ra-tional sentence comprehension under uncertain in-put and explored some of the consequences for out-standing problems in the psycholinguistic literature.The model proposed here will require further em-pirical investigation in order to distinguish it fromother proposals that have been made in the liter-ature, but if our proposal turns out to be correctit has important consequences for both the theoryof language processing and cognition more gener-ally.
Most notably, it furthers the case for ratio-nality in sentence processing; and it eliminates oneof the longest-standing modularity hypotheses im-plicit in work on the cognitive science of language:a partition between systems of word recognitionand sentence comprehension (Fodor, 1983).
Unlikethe pessimistic picture originally painted by Fodor,however, the interactivist picture resulting from ourmodel?s joint inference over possible word stringsand structures points to many rich details that stillneed to be filled in.
These include questions such aswhat kernel functions best account for human com-prehenders?
modeling of noise in linguistic input,and what kinds of algorithms might allow represen-tations with uncertain input to be computed incre-mentally.The present work could also be extended in sev-eral more technical directions.
Perhaps most notableis the problem of the normalization constant for theposterior distribution over word strings and struc-tures; this problem was circumvented via a k-bestapproach in Section 3 and by removing loops fromthe Levenshtein-distance kernel in Section 4.
Webelieve, however, that a more satisfactory solutionmay exist via sampling from the posterior distribu-tion over trees and strings.
This may be possibleeither by estimating normalizing constants for theposterior grammar using iterative weight propaga-tion and using them to obtain proper production ruleprobabilities (Chi, 1999; Smith and Johnson, 2007),or by using reversible-jump Markov-chain MonteCarlo (MCMC) techniques to sample from the pos-terior (Green, 1995), and estimating the normaliz-ing constant with annealing-based techniques (Gel-man and Meng, 1998) or nested sampling (Skilling,2004).
Scaling the model up for use with treebank-size grammars is another area for technical improve-ment.Finally, we note that the model here could poten-tially find practical application in grammar correc-tion.
Although the noisy channel has been in use formany years in spelling correction, our model couldbe used more generally for grammar corrections, in-cluding insertions, deletions, and (with new noisefunctions) potentially changes in word order.ReferencesAllauzen, C., Riley, M., Schalkwyk, J., Skut, W.,and Mohri, M. (2007).
OpenFst: A generaland efficient weighted finite-state transducer library.In Proceedings of the Ninth International Confer-ence on Implementation and Application of Au-tomata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Altmann, G. T. and Kamide, Y.
(1999).
Incremental in-terpretation at verbs: restricting the domain of subse-quent reference.
Cognition, 73(3):247?264.Aylett, M. and Turk, A.
(2004).
The Smooth Sig-nal Redundancy Hypothesis: A functional explanationfor relationships between redundancy, prosodic promi-nence, and duration in spontaneous speech.
Languageand Speech, 47(1):31?56.Bar-Hillel, Y., Perles, M., and Shamir, E. (1964).
On for-mal properties of simple phrase structure grammars.
InLanguage and Information: Selected Essays on theirTheory and Application.
Addison-Wesley.Charniak, E. (1997).
Statistical parsing with a context-free grammar and word statistics.
In Proceedings ofAAAI, pages 598?603.Chi, Z.
(1999).
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1):131?160.Christianson, K., Hollingworth, A., Halliwell, J. F., andFerreira, F. (2001).
Thematic roles assigned along thegarden path linger.
Cognitive Psychology, 42:368?407.Collins, C., Carpenter, B., and Penn, G. (2004).
Head-driven parsing for word lattices.
In Proceedings ofACL.Collins, M. (1999).
Head-Driven Statistical Models forNatural Language Parsing.
PhD thesis, University ofPennsylvania.Cover, T. and Thomas, J.
(1991).
Elements of InformationTheory.
John Wiley.Ferreira, F., Ferraro, V., and Bailey, K. G. D. (2002).Good-enough representations in language comprehen-sion.
Current Directions in Psychological Science,11:11?15.242Fodor, J.
A.
(1983).
The Modularity of Mind.
MIT Press.Gelman, A. and Meng, X.-L. (1998).
Simulating nor-malizing constants: from importance sampling tobridge sampling to path sampling.
Statistical Science,13(2):163?185.Genzel, D. and Charniak, E. (2002).
Entropy rate con-stancy in text.
In Proceedings of ACL.Genzel, D. and Charniak, E. (2003).
Variation of entropyand parse trees of sentences as a function of the sen-tence number.
In Empirical Methods in Natural Lan-guage Processing, volume 10.Green, P. J.
(1995).
Reversible jump Markov chain MonteCarlo and Bayesian model determination.
Biometrika,82:711?732.Hale, J.
(2001).
A probabilistic Earley parser as a psy-cholinguistic model.
In Proceedings of NAACL, vol-ume 2, pages 159?166.Hall, K. and Johnson, M. (2003).
Language modelingusing efficient best-first bottom-up parsing.
In Pro-ceedings of the IEEE workshop on Automatic SpeechRecognition and Understanding.Hall, K. and Johnson, M. (2004).
Attention shifting forparsing speech.
In Proceedings of ACL.Huang, L. and Chiang, D. (2005).
Better k-best parsing.In Proceedings of the International Workshop on Pars-ing Technologies.Itti, L. and Baldi, P. (2005).
Bayesian surprise attractshuman attention.
In Advances in Neural InformationProcessing Systems.Johnson, M. and Charniak, E. (2004).
A TAG-basednoisy channel model of speech repairs.
In Proceed-ings of ACL.Jurafsky, D. (1996).
A probabilistic model of lexical andsyntactic access and disambiguation.
Cognitive Sci-ence, 20(2):137?194.Keller, F. (2004).
The entropy rate principle as a pre-dictor of processing effort: An evaluation against eye-tracking data.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 317?324, Barcelona.Legge, G. E., Klitz, T. S., and Tjan, B. S. (1997).
Mr.Chips: An ideal-observer model of reading.
Psycho-logical Review, 104(3):524?553.Levy, R. (2008).
Expectation-based syntactic compre-hension.
Cognition, 106:1126?1177.Levy, R. and Andrew, G. (2006).
Tregex and Tsurgeon:tools for querying and manipulating tree data struc-tures.
In Proceedings of the 2006 conference on Lan-guage Resources and Evaluation.Levy, R. and Jaeger, T. F. (2007).
Speakers optimize in-formation density through syntactic reduction.
In Ad-vances in Neural Information Processing Systems.Mohri, M. (1997).
Finite-state transducers in languageand speech processing.
Computational Linguistics,23(2):269?311.Narayanan, S. and Jurafsky, D. (2002).
A Bayesian modelpredicts human parse preference and reading time insentence processing.
In Advances in Neural Informa-tion Processing Systems, volume 14, pages 59?65.Nederhof, M.-J.
and Satta, G. (2003).
Probabilistic pars-ing as intersection.
In Proceedings of the InternationalWorkshop on Parsing Technologies.Rayner, K. (1998).
Eye movements in reading and infor-mation processing: 20 years of research.
Psychologi-cal Bulletin, 124(3):372?422.Rohde, D. (2005).
TGrep2 User Manual, version 1.15edition.Skilling, J.
(2004).
Nested sampling.
In Fischer, R.,Preuss, R., and von Toussaint, U., editors, Bayesian in-ference and maximum entropy methods in science andengineering, number 735 in AIP Conference Proceed-ings, pages 395?405.Smith, N. A. and Johnson, M. (2007).
Weighted andprobabilistic context-free grammars are equally ex-pressive.
Computational Linguistics, 33(4):477?491.Tabor, W., Galantucci, B., and Richardson, D. (2004).Effects of merely local syntactic coherence on sen-tence processing.
Journal of Memory and Language,50(4):355?370.Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.,and Sedivy, J. C. (1995).
Integration of visual andlinguistic information in spoken language comprehen-sion.
Science, 268:1632?1634.243
