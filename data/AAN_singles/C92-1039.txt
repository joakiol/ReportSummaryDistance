SYNTACTIC PREFERENCES FOR ROBUST I)ARSING WIT\[ t  SEMANTIC PREFERENCESJIN WANGComputing Research LaboratoryNew Mexico State UniversityLas Cruces, NM 88003E-mail: wjin@nmsu.eduAbstractUsing constraints in robust parsing seems to havewhat we call "robust parsing paradox".
PreferenceSem,'ultics and Coimecfionisln both offered a promis-ing approach to fins problem.
However, PrefelenceSemantics has not addressed the problem of how tomake flfll use of syntactic onstraints, and Connec-tiouism has some inherent difficulties of its OWllwhich prevent i producing a practical system.
In thispaper we are proposing a method to add syntacticpreferences to the Preferealce Semantics paradigmwhile nmiutaining its fundamental philosophy.
It willbe shown ttmt syntactic preferences can be coded asa set of weights associated with the set of symboli-cally manipulatable rules of a new grammar formal-ism.
~121e syntactic preferences uch codezt can lyeeasily used to compute with semantic preferences.With the help of some tectmiques borrowed fromConnectioeisln, these weights can be adjustedthrough tralrting.1.
IntroductionRobust parsing faces what seems to be a "para-dox".
On one hand, it needs to tolerate inputsentences which more or less break syntacticand semantic onstraints; that is, given m~ ill-formed sentence, the parser should attempt ointerpret it somehow rather thml simply reject itbecause it violates some constraints.
In order todo this it allows syntactic and semantic con-stralnts to be relaxed.
On the other hand, robustparsing still need to be able to disambiguate.
Forthe sake of disambiguation, stronger syntacticand ,semantic oilstraints are required, since thestronger these constraints are, the greater thenumber of unlikely readings that can be rejected,and the more likely the correct reading isselected.A lot of work related to this problem has beendone.
The most commonly used snategy is touse the strongest constraints first to parse theinput.
If the parser fails, the violated constraintsare relaxed in order to recover the failed processm~d arrive at a successful parse (CarboneU &Hayes, 1983) (Huang, 1988) (Kwasny & Son-dheimer, 1981) (Weischedel & Sondheimer,1983).
The major problem with this approach isthat it is difficult to tell which constraint is actu-ally being violated and, therefore, needs to berelaxed when the parser fails.
Tiffs l)roblem ismore serious with the parser using backtracking.Another strategy is based on Preference Seman-tics (Wilks, 1975, 1978) (Fass & Wilks, 1983).The idea is that all the possible sentence read-ings can (though not necessarily) be produced.All the "readings are scored according to howmm~y preference satisfactions they contmn", and"the best reading (that is, the one with the mostpreference satisfactions) is taken, even if it con-tains preference violations."
Selfridge (1986)and Slator (1988) 'also use this strategy.
Oneimportant advantage of this approach is thatwithin a uniform mechanism, the semantic on-straints can both be used maximally for the sakeof disambiguation m~d be gracefully relaxedwhen nece,ssmy for the sake of robusmess.
How-ever, how to extend the preference philosophy tosyntactic constraints have not been addressed.There are two li-equently used approaches toincorporate syntactic constraints in systemsusing semantic preferences.
The first ,~s in (Slaotot, 1988), is to use a weak version of a rathertypical syntactic module.
The problem with thisapproach is that the syntactic constraints herestill suffer from the problem of "robust parsingparadox".
Another problem with this approach isthat it shifts more bin-dens of disambiguation tosemm~tic preferences because the syntactic on-stralnts need to be weak enough in order not toreject too many noisy inputs.
The secondapproach, as in (Selfridge, 1986), is to try all thepossible combinations without any structuralpreference.
Tfie problein hexe is computationalcomplexity especially with long sentences whichhave conlplex or recnrsive slJllctares in them.Cormectionism has also shown some appealingresults (Dyer.
1991) (Waltz & Pollack.
1985)(l,ehnelt, 1991) on robusmess.
However, thereare some very difficult problems which theypose, such as the difficulty with complex struc-tures (especially recursive structures), thedifficulty with variables, variable fiindings andratification, and the difficulty with schemes andtheir instantiations.
These problems need to besolved before such an approach can be used.
inpractical natural language processing systemsACRES DE COLING-92, NA~rt~s, 23-28 AO~r 1992 2 3 9 Pl~oc.
ot: COLING-92, NAbrrEs, AuG. 23-28.
1992especially those which involve syntactic pro-cessing (Dyer, 1991).In this paper we will propose a framework ofrepresenting syntactic preferences 1 that willkeep all the virtues of robustness PreferenceSemantics suggests.
Furthermore, such prefer-ences can be learned.2.
Sketch of Our ApproachAs we seen the idea of Preference Semanticsabout robustness is to enable the system to 1)generate (potentially) all the possible readingsfor all the possible inputs, 2) find out among allthe possible readings which one is the mostappropriate one and generate it first.
To meetthis goal for syntactic constraints we want toaccomplish the following:1.
A formalism with a practically feasibleamount of symbolic mampulatable rules whichwill have enough generative power to accept anyinput and produce all the possible structures ofit (section 3.1).2.
A scheme to associate weights with each ofthese rules such that the weights of all the rulesthat are used to produce a structure will reflecthow preferable (syntactically speaking) thewhole structure is (section 2.1).3.
A algorithm that will incorporate these rulesand weights with semantic preferences o thatthe overall best reading will be generated first.
(section 2.2).4.
A method to train the weights (section 2.1and 3.3).2.1.
Coding Syntactic ConstraintsThe most popular method to encode syntacticconstraints is probably phrase structure gram-mar.
One way to make it robust is to assign forall the grammar rules a cost.
So, a rule designedfor well formed inputs will have less cost thanthe rules designed for less well formed inputs;that is to say that a higher cost grammar ulewill be less preferred than a lower cost grammarrule.
However there are two serious problemswith this naive method.
First, i f  all the ill-formedness will have a set of special grammarrules designed for them, to capture a reasonableI We u~ syntactic preferences in a broader sensewhich not only includes yntactic feature agreementsbut also the order of the constituents in the sentenceand the way in which different constituents are com-bined.
On the other hand, is yOU will ram, the absenceof nonterminals and syntactic derivation tree~ willalso di.staoce us from a strong syntax theory.variety of ill-fornledness, we need an unreason-ably amount of grammar ules.
Second, it is notclear how we can find the costs for all theserules.
Next we will show how we will solvethese two problems.To solve the first problem, our approach aban-dons the use of phrase structure grammar, andinstead a new grammar formalism is used.
Theidea of this new formalism is to reduce syntacticconstructions to a relative small set of moreprimitive rides (P-rules), so that each syntacticconstruction can be produced by the applicationof a particular sequence of these P-rules.
Section3.1 will give more details about this formalism,and we win just list some of its relevant charac-teristics here: 1.
There are no nonterminals inthis formalism.
2.
Each P-ntle take only threeparameters to specify a pattern on which itsaction will be fired.
All these three parameterswill be one of the parts of speech (or wordtypes), such as noun, verb, adverb, etc.
Forexample: (noun, verb, noun) ~> action3 is aP-rule.
3.
Each rule has one action.
There areonly three possible actions.Since the number of parts of speech is generallylimited 2, the total rule space (the number of allthe possible P-rules) is rather small 3.
So, it ispossible for us to keep all of the possible P-rulesin the grammar rule base of the parser.
The out-put of the parser is a parsing tree in which eachnode corresponds to a word in the input sen-tence and the edge represents he dependency orthe attachment relation between the words in thesentence.
One important property of this formal-ism is that given any input sentence, for all ofthe normal parsing trees that can be built on it(the definition of normal parsing tree is given innext section), there exists a sequence of P-rules,such that the tree will be derived by applyingthis rule sequence.
This means that we can nowuse this small P-rule set to guide the parsing 4without pre-excluding any kinds of ill-formedinputs for which a normal parsing tree doesexist.2 For example in Longnmn Dictionary of Contem-porary English (1978) there are only 14 different partsof speech.
Given that interjection can be taken care byJt preprocessor and the complex parts like v adv, v advprep, v adv;prep, v prep can be represented as v plusanme syntactic feature.s, this number can be further m-duce, d to 9.3 If there re 10 different parts of speech, the totalP-rule sl~ee is smaller than lOxlOxlOx3 = 3000.4 Because of the searching algorithm we used, alot of searching paths anctioned by p-rules will bepruned by semantic prefurences.
In this sense theAcrEs DE COLING-92, NANTES, 23-28 AOt3X 1992 2 4 0 PRec, OF COLING-92, NANTES, AUG. 23-28, 1992To solve the second problem, we will use sometechniques bon'owed from the connectionistcmnp.
As we have mentioned before each ruletakes three parameters to specify a pattern.Each of these parameters will associate with avector of syntactic feature 5 roles.
So, every P-rule will have a feature rote vector:(FFlt ...... Fln, F21 ...... F2n, F31 ......Each feature role is in turn associated with aweight.
So each P-rule also has a vector ofweights:(W W W 11 ...... W .
ln'  W21 ...... W2n'31 ...... 3n )If a rule is applicable at a certain situation, thefeature roles will be filled by the correspondingfeature values given by the language objectsmatched out by the pattern of the rule.
Thevalue of each feature can be either 1 or 0, LetF' i .
denotes the value filling role F i ?
at one par-t ic~ar application.
The cost of al~plying thatrule at that situation will be:-Y. Wij ?
F'i j  (*)The weights can be trained by a training algo-rithm similar to that used in the single layer per-ceptron network.To summarize, the syntactic constraints areencoded partly as P-rules and partly as theweights associated with each p-ride.
The P-rnlesare used to guide the parsing and tell which con-stituent should combined with which co~tsti-tuent.
There are two types of syntactic prefer-ences encoded in the weights: the preferences ofthe language to different P-rules and the prefer-ences of each P-rule to different syntacticfeatures.
"l~e P-rule is still symbolic and work-ing on a stack.
So, the ability of recursion iskept.2.2.
Organizat ion of the ParserAt the topmost level, the parser is a standardsearching algorithm.
Due to its efficiency, A*algorithm (Nilsson, 1980) is used.
However, it isworth noting that any other standard searchingalgorithm can also be used.
For any searchingparsing is actually guided by both syntactic prefer-enccs and semantic preferences.5 The use of term "syntactic feature" is only tomake it distinct from semantic preference and seman-tic type .
We, by no means, try to exclude th~ use ofthose features which are generally considered as so-mantle feantres but which will help to make the rightsyntactic choice.algorithm, the following things need to bespecilied: 1. what the states hi the searchingspace are: 2. what the initial state is; 3. what theaction roles are aed taking one state how theycreate new states; 4. what the cost tff creating anew node is (please note that the cost is actuallycharged to the edge of the searching raph); 5.what the final states are.In the pmser, tile searching states are pairs like:(<partial resutt>,<unparsed sen-tence>)The pml:ial re.suit is represented a.s one or moreparsing trees which arc kept on a working stack.Details of this representation are given in nextsection.
The initial state is a pair of an emptystack and a sentence to be parsed.
The action ofthe searching process is to look at the nextread-in word and the roots of the top two taeeson the wolking stack, which represent the twomost recent /ound active constituents.
Thesearching will then search for all the applicableP-rules based on what it sees.
All P-rules it hasfound will then be fired.
The action part of theseP-rules will decide bow to manipulate the treeson the stack and the current read-in.
At will alsodecide whether it needs to read in next word.Therefore, for each of these P-rules being fired,a new state is created.
The cost of creating thisnew state is following: the cost of applying theP-rule on its father state; tile degree of violationof any new local, preference just being found inthe newly built trees; the cost of contextualpreference 6 associated with file new consumedread in sense.
All these costs are tmrmalized andadded to yield the total cost of creating this newstate.
The reason for normalization is that, forexample, the cost of applying P-rule (see section2.1) needs to be norntalized to be positive as itis required by A* algorithm.
Tile relative magni-tudes of different types of costs need also bebalanced, so that one kind of cosls will notoverwhelm the others.
The final states of thesearching are the states where the unparsed sen-tence is nil and the working stack has only onecomplete parsing tree on it.
Obviously the out-put of this searching algorithm is the reading(represented as the n'ee) of dte input sentencewhich violates the least amount of Sylltactic andsemantic preferences.So far the heuristics used in the A* searching issimply taken to be:6 For the idea of contextual prefe.rence~ see (Slator,1988).AcrEs DE COLING-92, NANTES, 23-28 AOl'Yr 1992 2 4 1 t'l~nc.
Ov COIANG-92, NANTES, AUG. 23-28, 1992tX/Cwhere ct is a constant.
I is the length of theunparsed sentence, c is the average cost for pars-ing each word.It is needed to mention that the input sentencewe talked above is actually a list of lexicalframes.
Each lexical flame contains the follow-ing information about he word it stands for: thepart of speech, syntactic features, local prefer-ences, contextual preferences, etc.
Since wordscan have more than one senses and one lexicalframe will only represent one sense, for eachread-in word, the parser will have to work foreach of its senses and produce all of their chil-dren.3.
Some Details3.1.
P-rules, Parsing Trees, and P-parsers.Given an input string a on a vocabulary E 7, aparsing tree of the input is a trees with all itsnodes corresponding to one of the words in theinput string.
For example, one of the parsingtrees for input abcde is:(e (a) (c (b) (d)))Here the head of each list is the root of the treeor the subtree, and the tail of the list are its chil-dren.
Intuitively the parenthood relation reflectsthe attachment or dependency relation in theinput expression.
For example, given an inputexpression a small dog, the dog will be thefather of both a and small.
The parsing tree ismore formally defined as follows:Definition (Parsing Tree): Given an input stringa, the parsing tree on a is recursively defined asfollowing:I.
(a) is a parsing tree on a, if a is in2.
(a T 1 T 2 ...... Ti) is a parsing treeon a, if a is in a and T k (l~..<i) isalso a parsing tree on a.Definition (Complete Parsing Tree): Suppose Tis a parsing tree on a.
If for all the a in a, a isalso in T, then T is a complete parsing tree onTo reduce the computational complexity, wewill limit our attentions to only one special typeof parsing trees, namely the normal parsing tree.7 in  tho actual pars~ I~ is tho ~t  of all tho parts ofIpeech.S The order of children for any node is significanthera, and wo will usa LIsP convention torapre.lasntthe par~ing ~a?
in this paper.It should not be difficult to see and prove fromthe following definition that normal parsingtrees are simply the trees which do not have"crossing dependencies" (Maling & Zaenen,1982).Definition: (Normal Parsing Tree): Suppose T isa parsing tree on a. T is a normal parsing treeon a iff for all the nodes in T, if they have chil-dren, say T .
.
T, then all the nodes in T1 ' "  "" ' .
.
I must be appeared ~fore all nodes m T jm theinput stnng a, where l<i<j~,k.The P-parser has an input tape, and the readinghead is always moving forward.
The P-parseralso keeps a stack of parsing trees which willrepresent the parsing result of the part of theinput which has already been read.
Besides thereis a working register in the P-parser to store thecurrent read in word.
As you will see later, theread-in word is actually transformed into a U'eebefore it being stored in the working register.The configurauon of the P-parser is thus definedas a triple \[<the stack>,<content of workingregister>,<unparsed input>\].
The P-rule is usedto specify how the P-parser works.
If the mputis of a vocabulary E, the P-rules on it can bedefined as follows:Definition (P-rule): A P-rule on E is a memberof set E' xE '  xE '  xA, where E' isE to \[nil}and A is the set of actions defined below.Definition (Action): Actions are defined as func-tions of type E ~ E, where E is the set ofconfigurations.
There are total three differentacUons used in P-rules, and they are listedbelow:5~\[S,C,R\]= \[(cons C S), (list (carR)), (cAr R)\]5?\[S,C,R\]= \[(cdr S), (cons (car C)(Cons (car S) (cdr C))), R\]5~s\[S,C,R\]= \[(cons (append (car S)t (cadr S))) (cddr S)), C, R\]Action 1 simply pushes the current read-in onthe stack and then reads the next word from theinput.
Action 2 attaches top of the stack as thefirst child of the current read-in stored in theworking register.
Action 3 pops the top of thestack first and then attaches it to the second topof the stack as its last child.The initial configuration for the P-parser is \[nil,(list (car a)), (cdr ct)\], where the a is the inputstring to be parsed.
There is a set of P-rules ineach P-parser to specify its behavior.
The P-parser will work non-deterministically.
A P-rulecan be fired iff its three parameters march theroots of the top two parsing trees on the stackand the root of the tree in the working registerACRES DE COLING-92, NANTES, 23-28 Aotrr 1992 2 4 2 PROC.
O1: COLING-92, NANTES, AUG. 23-28, 1992respectively.
Note the P-rule does not care aboutthe unparsed input part in the configuration tri-ple.
A cmtfiguratiml is a final configuration iffthe unparsed input string and the working regis-ter are all empty mid the stack only has oneparsing tree on it.
A input is grammatical if andonly if the P-parser can reach from the initialconfiguration to a final configuration by apply-ing a sequence of P-rules taken from its gram-mar set.
If there are more than one possible finalstates for a given input sWing and the parsingtxees produced at these states are different, thenwe say that the input swing is ambiguous.
Hereis a simple example to show how the P-parserworks.
You may need a pen and a piece ofpaper to work it out.
Given input abcde (a goodfriend of mine, for example), the parsing tree (c(a) (b) (d (e))) can be constructed fly applyingthe following sequence of rules:(nil,nil,a)=> 1 (a,nil,b)=> 1 (b,a,c)=>2(a,nil,c)=>2 (nil,nil,c)=>l (c,nil,d)=>l(d,e,c)=>l (c,d,nil)~.>3 (d,c,nll)=>3Most properties of this formalism will not be ofthe interests of this paper, except his theorem:Theorem: A P-parser with all the P-rules on Eas its grammar set cm~ produce all the completenormal parsing trees for any input sWing on Z.PROOF: It is easy to prove this by induction onthe length of the input string, r2The theorem tells that a P-parser with all thepossible P-rules as its rule set can produce forany input string all of its possible parsing treeswhich have no crossing dependencies.
Thisalone may not be very useful since this P-parserwill also accept all the strings over E. However,as we have shown in the last section, with aproper weighting scheme, this P-parser offers asuitable framework for coding syntactic prefer-ences.3.2.
Syntactic Preferences in the Weights ofP-rulesEach lexical item will have a vector (of a fixedlength) containing syntactic features.
The valueof the each feature is either 1 or 0.
Each of thethree parameters in the P-rule is associated witha vector (of the same length) of syntactic featureroles.
Each of these syntactic feature roles isassociated with a weighL Each weight thusencodes a preference of the P-rule toward theparticular syntactic feature it corresponds to.
Ahigher weight means the P-rule will be moresensitive to the appearance of this syntacticfeature, and the result of applying this ruletherefore will be more competitive when it doesappeaa'.
The preferences of the language to dilCferent P-rules arc 'also ~ellected in weight,s.Instead of being reflected in each individualweight, they are reflected in the distribution ofweights in P-rules.
A P-rule with a higher aver-age weight is generally more favored than a P-ntle with a lower average weight, since thehigher tile average weight is, tile lower the costof applying the P-rule tends to be.
It is alsonecessary to emphasize that these two types ofpreferences are closely integrated.3.3.
Weight LearningThe weights of ,all the P-rules will be trained.The training is supervised.
Tllere are two dif-ferent methods to train the weights, The firstmethod takes an input string and a correct pars~ing tree for that stnng.
We will use an algoritlmtto computer a sequence of P-rules that will pro-duce the given parsing tree from the given inputstring.
"\[qfis algorithm is not difficult to design,and due to the space limits we will not present ihere.
After the P-rule sequence is produced,each of P-rule in the sexluence will get a bonus5.
The bonus will farther be distributed to theweights in the P-rule in the same fashion as thatin the single layer perceptron etwork, that is:AWij = ~llSF'ijwhere rI is the learning factor.The second method requires less interventions.The parser is let to work on its own.
'llae usertufty need to tell the parser whether the output itgives is correct.
If the result is correct, all theP-rules used to construct his outpnt will get abonus and all the rules used during the parsingwhich did not contribute to tim output will get apunishinent.
If the answer is wrong, all the Porules used to construct his artswer will get apunishment, and the parser will continue tosearch for a second best guess.
The Ixmus andpunishment will be diswibuted to the weights ofthe P-rule in the san~e maimer as that in the firstmethod.The first method is more appropriate at the earlystage of the training when most of the ruleshave about he stone weights.
It will take muchlonger for the parser to find the correct result onits own at this time.
On the other hand, thesecond method needs less interventions.
So, itwill be more appropriate to be used whenever itcan work reasonably well.It is well known that the single layea" perceptionnet can not be trained to deal with exclusive-or.The same situation will also happea here.
Sinceexclusive-or relations do exist between syntacticAcrEs DE COLING-92, NAm'.ES, 23-28 AO~" 1992 2 4 3 I'ROC.
OF COLING-92, NANTES, AU(L 23-28, 1992features, we need to solve this problem.
Thesimplest solution is to make multiple copies forthe P-role, and, hopefully, each copy will con-verge to each side of the exclusive-or relation.3.4.
Measuring Preference ViolationsThe syntactic preference violation is measuredby formula (*) in section 2.
Both action 2 andaction 3 of P-rule actions make an aaachrnent,and some semantic preferences may be eitherviolated or satisfied by the attachment.
So, aftereach application of such p-ride actions, theparser needs to check whether there are somenew preferences being violated or satisfied.
Ifthere are, it will computer the cost of these vio-lations and report it to the searching process.Similarly, eacli action 1 will cause a new con-textual preference being reported.
The measure-ment for the violation degree of both localpreferences and contextual preferences i  basi-cally taken from PREMO (Slator, 1988), whichwe will not repeat here.4.
Conclusion and ComparisonsPreference Semantics offers an excellent frame-work for robust parsing.
However, how to makefull use of syntactic constraints has not beenaddressed.
Using weights to code syntactic on-straints on a relatively small set of P-rules (fromwhich all the possible syntactic structures can bederived) enables us to expand the philosophy ofPreference Semantics from semantic onstraintsto syntactic onstraints.
The weighting systemnot only reflects the preferences of the language,say English, to different P-rnles but also reflectsthe preferences of each P-rule to each syntacticfeature.
Besides of this, it also offers a niceinterface so that we can integrate the applicationof these syntactic preferences nicely with theapplication of both local semantic preferencesand contextual preferences by using a highlyefficient searching algorithm.This project has also shown that some of thetechniques commonly associated with the con-nectionism, such as coding information asweights, training the weights, and so on, canalso be used to benefit symbolic omputing.
Theresult is gaining the robustness and adaptabilitywhile not losing the advantages of symboliccomputing such as recursion, variable binding,etc.The notion 'syntactic preference' has been usedin (Pereira, 1985) (Frazier & Fodor, 1978)(Kimball.
1973) to describe the preferencebetween Right Association and Minimal Attach-ment.
Our approach shares some similaritieswith (Pereira, 1985), in that MA and RA simply"corresponds to two precise roles on how tochoose between alternative parsing actions" atcertain parsing configurations.
However, he didnot offer a framework for how one of them willbe preferred.
According to our model the prefer-ence between them will be based on the weightsassociated with the two rules, the syntacticfeatures of the words involved and the semanticpreferences found between these words.
Besides,the idea of syntactic preferences in this paper ismore general than the one used in their work,since it includes not only the preference betweenMA or RA but other kinds of syntactic prefer-ences as well.Wilks, Huang and Fass (1985) showed thatprepositmnal phrase attachments are possiblewith only semantic information.
In theirapproach syntactical preferences are limited tothe order of matchings and the default attaching.Their atxaching algorithm can be seen as a spe-cial case of the model we proposed here, in that,if they are correct, the preferences between thesequences of rules used for RA and MA wouldturn out to be very similar so that the semanticpreferences involved would generally over sha-dow their effects.There are some significant differences betweenour approach and some hybrid systems (Kwasny& FaJsal, 1989) (Simmons & Yu, 1990).
First,our approach is not a hybrid approach.
Every-thing in our approach is still symbolic comput-ing.
Second, in our approach the costs of theapplications of syntactic constraints are passedto a global search process.
The search processwill consider these costs along with the costsgiven by other types of constraints and make adecision globally.
In a hybrid system, the syn-tactic decision is made by the network locally,and there is no intervention from the semanticprocessing.
Third, our parser is non-deterministic while the parsers in hybrid systemsare deterministic since there is no easy way todo backtracking.
It is also easy to see thatwithout he intervention of semantic processing,lacking the ability of backtracking is hardly anacceptable s~ategy for a practical naturallanguage parser.
Finally, in our approach eachP-rule has its own set of weights, while in thehybrid systems all the grammar rules share acommon etwork, and it is quite likely that thisnet will be overloaded with information when areasonably large grammar is used.ACTES DE COIANG-92, NANTES, 23-28 ^Ot~T 1992 2 4 4 PROC.
OF COLING-92, NANTES, Aun.
23-28, 1992AcknowledgmentAll the experiments for this project are carriedon the platform given by PREMO which wasdesigned and implemented by Brian Slator whenhe was here in CRL.
The author "also wants tothank Dr Yorick WiNs, Dr David Farwell, DrJohn Barnden and one referee for their com-ments.
Of course, the author is solely responsi~ble for all the mistakes in the paper.References1.
J .G.
Carbonell and P. J. Hayes, "RecoveryStrategies for Parsing ExtragrammticalLanguage," American Journal of Computa-tional Linguistics, vol.
9(3-4), pp.
123-146,1983.2, D. Fass and Y. Wilks, "Preference Semantics,Ill-Formedness, and Metaphor," AmericanJournal of Computational Linguistics, voL9(3-4), pp.
178-187, 1983.3.
M.G.
Dyer, "Symbolic NeuroEngmeelmg dnatural language processing: a multilevelresearch approach.," in Advances in Conaec-tionist and Neural Computation Theory, Vol.1., ed.
J.A.
Bmnden and J.B. Pollack, pp.
32--86, Ablex Pnblishing Corp. , Norwood, N.J.,1991.4.
L. Frazter and J. D. Fodor, "The SausageMachine: A New Two-Stage Parsing Model,"Cognition, vol.
6, pp.
291-325, 1978.5.
X. Huang, "XTRA: 'Ihe design and Implemen-tation of A Fully Automatic Machine Transla-tton System (Doctoral Dissertation),"Memoranda in Computer and Cognitive Sci-ence, vol.
MCCS-88-121, Computing ResearchLaboratory, New Mexico State University, LasCruces, NM 88003, USA, 1988.J.
Kimball, "Seven Principles of SurfaceStructure Parsing in Natural Language," Cog-nition, vol.
2, pp.
15-47, 1973.S.
C. Kwasny and N. K. Sondheimer, "Relaxa-tion theories for parsing ill-forared input,"American Journal of Computational Linguis-tics, vol.
7, no.
2, pp.
99-108, 1981.S.
C. Kwasny aruJ K. A. Faisal, "Competitionand Learmng in a Coanectiohist DeterministicParser," Procs.
11 th Annual Conf.
of the Cog-nitive Science Society, pp.
635-642, Lawrea~ceErlbaum, Hillsdale, N.J., 1989.W.
G. Lehnert, "Symbolic/Sabsymbolic Sen-tence Analysis: Exploiting the best of twoworlds.," in Advances in Com~ectionist andNeural Computation Theory, Vol.
1., ed.
J.A.Banlden and J.B. Pollack, pp.
32--86, AblexPublishing Corp. , Norwood, N.J., 1991.10.
J. Maling and A. Zaeslen, "A Phrase StructureAccount of Scandinavian Extraction6.7.8.Phenomeala," in The Nature of SyntacticalRepresentation, ed.P.
Jacobsun and G. K. Pul-lure, pp.
229--282, Reidel Publishing Com-pany, Holland, 1982.11.
N. Nilsson, Principles of A/, Tioga pulishingco., Mtaflo Park, 1980.12.
F .C .N.
Pereira, "A New Characterization fAttachment Preference," in Natural LanguagePursing, exl.
D. R. Dowry, L. Kalltunen & A.M. Zwicky, pp.
307-319, Cmnhiidge Univer-sity Press, C,'unb\[idge, 1985.13.
M. Selfridge, "Integrated Pr~essing ProducesRobust Understanding," ComputationalLinguistics, wfl.
12(2), pp.
89-106, 1986.14.
R. 17 .
Sinanons mad Y. II.
Yu, "Training aNeural Network to be a Context SensitiveGrmnmar," Proceedings of the 5th RockyMoant<?in Co@rence on AI, pp.
25t-256, LasCruces, NM., 1990.15.
B.M.
Slator, "Lexical Semantics and Prefer-ence Semantics Atmlysis (Doctoral Disserta-tion)," Memorandn i Computer and Cogni-tive Science, vol MCCS-88-1, CompntingResearch Laboratory, New Mexico StateUniversity, L~s Cruces, NM 88003, USA,1988.16.
D.L.
Waltz and J.
13.
Pollack, "Massive Paral-lel Prosing: A Strongly h~teractive Model ofNatural Language hlterpretation," CognitiveScience, vol.
9, pp.
51-74, 1985.17.
R. M. Weischedel and N. K. Soodheitner,"Meta-rules as a basis for processing ill-formed output," Americcul Journal of Compu-tational Linguistics, vol.
9, no.
3-4, pp.
161-177, 1983.18.
Y. Wilks, "A Prefereaaial Pattern-SeekingSemamtcs for Natural Language lnferalce,"Artificial Intelligence, vol.
6, pp.
53-74, 1975.19.
Y. Wi|ks, "Making Preferences More Active,"Artificial Intelligence, vol.
11, pp.
197-223,1978.20.
Y.A.
Wilks, X Huang and D. Fass, "Syntax,plefercalcc and nght artaclmmnt," IJCA\[-85,pp.
635-642.
1985Actas DE COLING-92, NANTES, 23-28 AoOr 1992 2 4 5 I'R(){:.
OF COLING.92, NAN-rES, AUU.
23-28, 1992
