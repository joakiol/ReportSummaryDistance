Learning Named Entity Hyponyms for Question AnsweringPaul McNameeJHU Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723-6099, USApaul.mcnamee@jhuapl.eduRion SnowStanford AI LaboratoryStanford UniversityStanford, CA 94305, USArion@cs.stanford.eduPatrick SchoneDepartment of DefenseFort George G. Meade, MD 20755-6000pjschon@tycho.ncsc.milJames MayfieldJHU Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723-6099, USAjames.mayfield@jhuapl.eduAbstractLexical mismatch is a problem that con-founds automatic question answering sys-tems.
While existing lexical ontologies suchas WordNet have been successfully used tomatch verbal synonyms (e.g., beat and de-feat) and common nouns (tennis is-a sport),their coverage of proper nouns is less ex-tensive.
Question answering depends sub-stantially on processing named entities, andthus it would be of significant benefit iflexical ontologies could be enhanced withadditional hypernymic (i.e., is-a) relationsthat include proper nouns, such as EdwardTeach is-a pirate.
We demonstrate how a re-cently developed statistical approach to min-ing such relations can be tailored to iden-tify named entity hyponyms, and how as aresult, superior question answering perfor-mance can be obtained.
We ranked candi-date hyponyms on 75 categories of namedentities and attained 53% mean average pre-cision.
On TREC QA data our method pro-duces a 9% improvement in performance.1 IntroductionTo correctly extract answers, modern question an-swering systems depend on matching words be-tween questions and retrieved passages containinganswers.
We are interested in learning hypernymic(i.e., is-a) relations involving named entities becausewe believe these can be exploited to improve a sig-nificant class of questions.For example, consider the following questions:?
What island produces Blue Mountain coffee??
In which game show do participants competebased on their knowledge of consumer prices??
What villain is the nemesis of Dudley Do-Right?Knowledge that Jamaica is an island, that The Priceis Right is a game show, and that Snidely Whiplashis a villain, is crucial to answering these questions.Sometimes these relations are evident in the samecontext as answers to questions, for example, in?The island of Jamaica is the only producer of BlueMountain coffee?
; however, ?Jamaica is the onlyproducer of Blue Mountain coffee?
should be suf-ficient, despite the fact that Jamaica is an island isnot observable from the sentence.The dynamic nature of named entities (NEs)makes it difficult to enumerate all of their evolv-ing properties; thus manual creation and curationof this information in a lexical resource such asWordNet (Fellbaum, 1998) is problematic.
Pascaand Harabagiu discuss how insufficient coverage ofnamed entities impairs QA (2001).
They write:?Because WordNet was not designedas an encyclopedia, the hyponyms of con-cepts such as composer or poet are illus-trations rather than an exhaustive list ofinstances.
For example, only twelve com-poser names specialize the concept com-poser ... Consequently, the enhancementof WordNet with NE information couldhelp QA.
?799The chief contribution of this study is demonstrat-ing that an automatically mined knowledge base,which naturally contains errors as well as correctlydistilled knowledge, can be used to improve QA per-formance.
In Section 2 we discuss prior work inidentifying hypernymic relations.
We then explainour methods for improved NE hyponym learningand its evaluation (Section 3) and apply the relationsthat are discovered to enhance question answering(Section 4).
Finally we discuss our results (Section5) and present our conclusions (Section 6).2 Hyponym InductionWe review several approaches to learning is-a rela-tions.2.1 Hearst PatternsThe seminal work in the field of hypernym learn-ing was done by Hearst (1992).
Her approach wasto identify discriminating lexico-syntactic patternsthat suggest hypernymic relations.
For example, ?X,such as Y?, as in ?elements, such as chlorine andfluorine?.2.2 KnowItAllEtzioni et al developed a system, KnowItAll, thatdoes not require training examples and is broadlyapplicable to a variety of classes (2005).
Startingwith seed examples generated from high precisiongeneric patterns, the system identifies class-specificlexical and part-of-speech patterns and builds aBayesian classifier for each category.
KnowItAllwas used to learn hundreds of thousands of classinstances and clearly has potential for improvingQA; however, it would be difficult to reproduce theapproach because of information required for eachclass (i.e., specifying synonyms such as town andvillage for city) and because it relies on submitting alarge number of queries to a web search engine.2.3 Query LogsPasca and Van Durme looked at learning entity classmembership for five high frequency classes (com-pany, country, city, drug, and painter), using searchengine query logs (2007).
They reported precisionat 50 instances between 0.50 and 0.82.2.4 Dependency PatternsSnow et al have described an approach with severaldesirable properties: (1) it is weakly-supervised andonly requires examples of hypernym/hyponym rela-tions and unannotated text; (2) the method is suit-able for both common and rare categories; and, (3)it achieves good performance without post filteringusing the Web (2005; 2006).
Their method relieson dependency parsing, a form of shallow parsingwhere each word modifies a single parent word.Hypernym/hyponym word pairs where the words1belong to a single WordNet synset were identifiedand served to generate training data in the follow-ing way: making the assumption that when the twowords co-occur, evidence for the is-a relation ispresent, sentences containing both terms were ex-tracted from unlabeled text.
The sentences wereparsed and paths between the nouns in the depen-dency trees were calculated and used as features in asupervised classifier for hypernymy.3 Learning Named Entity HyponymsThe present work follows the technique describedby Snow et al; however, we tailor the approach inseveral ways.
First, we replace the logistic regres-sion model with a support vector machine (SVM-Light).
Second, we significantly increase the sizeof training corpora to increase coverage.
This ben-eficially increases the density of training and testvectors.
Third, we include additional features notbased on dependency parses (e.g., morphology andcapitalization).
Fourth, because we are specificallyinterested in hypernymic relations involving namedentities, we use a bootstrapping phase where train-ing data consisting primarily of common nouns areused to make predictions and we then manually ex-tract named entity hyponyms to augment the train-ing data.
A second learner is then trained using theentity-enriched data.3.1 DataWe rely on large amounts of text; in all our exper-iments we worked with a corpus from the sourcesgiven in Table 1.
Sentences that presented difficul-ties in parsing were removed and those remaining1Throughout the paper, use of the term word is intended toinclude named entities and other multiword expressions.800Table 1: Sources used for training and learning.Size Sentences GenreTREC Disks 4,5 81 MB 0.70 M NewswireAQUAINT 1464 MB 12.17 M NewswireWikipedia (4/04) 357 MB 3.27 M EncyclopediaTable 2: Characteristics of training sets.Pos.
Pairs Neg.
Pairs Total FeaturesBaseline 7975 63093 162528+NE 9331 63093 164298+Feat 7975 63093 162804were parsed with MINIPAR (Lin, 1998).
We ex-tracted 17.3 million noun pairs that co-occurred inat least one sentence.
All pairs were viewed as po-tential hyper/hyponyms.Our three experimental conditions are summa-rized in Table 2.
The baseline model used 71068pairs as training data; it is comparable to theweakly-supervised hypernym classifier of Snow etal.
(2005), which used only dependency parse fea-tures, although here the corpus is larger.
The entity-enriched data extended the baseline training set byadding positive examples.
The +Feat model uses ad-ditional features besides dependency paths.3.2 BootstrappingOur synthetic data relies on hyper/hyponym pairsdrawn from WordNet, which is generally rich incommon nouns and lacking in proper nouns.
Butcertain lexical and syntactic features are more likelyto be predictive for NE hyponyms.
For example, itis uncommon to precede a named entity with an in-definite article, and certain superlative adjectives aremore likely to be used to modify classes of entities(e.g., ?the youngest coach?, ?the highest peak?).
Ac-cordingly we wanted to enrich our training data withNE exemplars.By manually reviewing highly ranked predictionsof the baseline system, we identified 1356 additionalpairs to augment the training data.
This annotationtook about a person-day.
We then rescanned the cor-pus to build training vectors for these co-occurringnouns to produce the +NE model vectors.Table 3: Features considered for +Feat model.Feature CommentHypernym con-tained in hyponymSands Hotel is-a hotelLength in chars /wordsChars: 1-4, 5-8, 9-16, 17+Words: 1, 2, 3, 4, 5, 6, 7+Has preposition Treaty of Paris; Statue of LibertyCommon suffixes -ation, -ment, -ology, etc...Figurative term Such as goal, basis, or problemAbstract category Like person, location, amountContains digits Usually not a good hyponymDay of week;month of yearIndiscriminately co-occurs withmany nouns.Presence and depthin WordNet graphShallow hypernyms are unlikely tohave entity hyponyms.
Presence inWN suggests word is not an entity.Lexname of 1stsynset in WordNetRoot classes like person, location,quantity, and process.Capitalization Helps identify entities.Binned documentfrequencyPartitioned by base 10 logs3.3 Additional FeaturesThe +Feat model incorporated an additional 276 bi-nary features which are listed in Table 3.
We consid-ered other features such as the frequency of patternson the Web, but with over 17 million noun pairs thiswas computationally infeasible.3.4 EvaluationTo compare our different models we created a testset of 75 categories.
The classes are diverse andinclude personal, corporate, geographic, political,artistic, abstract, and consumer product entities.From the top 100 responses of the different learn-ers, a pool of candidate hyponyms was created, ran-domly reordered, and judged by one of the authors.To assess the quality of purported hyponyms weused average precision, a measure in ranked infor-mation retrieval evaluation, which combines preci-sion and recall.Table 4 gives average precision values for thethree models on 15 classes of mixed difficulty2.
Per-formance varies considerably based on the hyper-nym category, and for a given category, by classifier.N is the number of known correct instances found inthe pool that belong to a given category.Aggregate performance, as mean average preci-sion, was computed over all 75 categories and is2These are not the highest performing classes801Table 4: Average precision on 15 categories.N Baseline +NE +Featchemical element 78 0.9096 0.9781 0.8057african country 48 0.8581 0.8521 0.4294prep school 26 0.6990 0.7098 0.7924oil company 132 0.6406 0.6342 0.7808boxer 109 0.6249 0.6487 0.6773sculptor 95 0.6108 0.6375 0.8634cartoonist 58 0.5988 0.6109 0.7097volcano 119 0.5687 0.5516 0.7722horse race 23 0.4837 0.4962 0.7322musical 80 0.4827 0.4270 0.3690astronaut 114 0.4723 0.5912 0.5738word processor 26 0.4437 0.4426 0.6207chief justice 115 0.4029 0.4630 0.5955perfume 43 0.2482 0.2400 0.5231pirate 10 0.1885 0.3070 0.2282Table 5: Mean average precision over 75 categories.Baseline +NE +FeatMAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)given in Table 5.
Both the +NE and +Feat modelsyielded improvements that were statistically signif-icant at a 99% confidence level.
The +Feat modelgained 11% over the baseline condition.
The maxi-mum F-score for +Feat is 0.55 at 70% recall.Mean average precision emphasizes precision atlow ranks, so to capture the error characteristics atmultiple operating points we present a precision-recall graph in Figure 1.
The +NE and +Feat modelsboth attain superior performance at all but the lowestrecall levels.
For question answering this is impor-tant because it is not known which entities will bethe focus of a question, so the ability to deeply minevarious entity classes is important.Table 6 lists top responses for four categories.3.5 Discussion53% mean average precision seems good, but is itgood enough?
For automated taxonomy construc-tion precision of extracted hyponyms is criticallyimportant; however, because we want to improvequestion answering we prefer high recall and cantolerate some mistakes.
This is because only a smallset of passages that are likely to contain an answerare examined in detail, and only from this subsetof passages do we need to reason about potential0.000.100.200.300.400.500.600.700.800.901.000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recall LevelPrecisionFeatEntBaselineFigure 1: Precision-recall graph for three classifiers.hyponyms.
In the next section we describe an ex-periment which confirms that our learned entity hy-ponyms are beneficial.4 QA Experiments4.1 QACTISTo evaluate the usefulness of our learned NE hy-ponyms for question answering, we used the QAC-TIS system (Schone et al, 2005).
QACTIS wasfielded at the 2004-2006 TREC QA evaluations andplaced fifth at the 2005 workshop.
We worked witha version of the software from July 2005.QACTIS uses WordNet to improve matching ofquestion and document words, and a resource, theSemantic Forest Dictionary (SFD), which containsmany hypernym/hyponym pairs.
The SFD was pop-ulated through both automatic and manual means(Schone et al, 2005), and was updated based onquestions asked in TREC evaluations through 2004.4.2 Experimental SetupWe used factoid questions from the TREC 2005-2006 QA evaluations (Voorhees and Dang, 2005)and measured performance with mean reciprocalrank (MRR) and percent correct at rank 1.All runs made use of WordNet 2.0, and we ex-amined several other sources of hypernym knowl-802Table 6: Top responses for four categories using the +Feat model.
Starred entries were judged incorrect.Sculptor Horse Race Astronaut Perfume1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon3 Phidias Cox Plate George D Nelson Poeme4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro10 Cildo Meireles Palio * Frank Poole JickyTable 7: Additional knowledge sources by size.Classes Class InstancesBaseline 76 11,066SFD 1,140 75,647SWN 7,327 458,370+Feat 44,703 1,868,393edge.
The baseline condition added a small subsetof the Semantic Forest Dictionary consisting of 76classes seen in earlier TREC test sets (e.g., nation-alities, occupations, presidents).
We also tested: (1)the full SFD; (2) a database from the Stanford Word-net (SWN) project (Snow et al, 2006); and, (3) the+Feat model discussed in Section 3.
The number ofclasses and entries of each is given in Table 7.4.3 ResultsWe observed that each source of knowledge benefit-ted questions that were incorrectly answered in thebaseline condition.
Examples include learning a me-teorite (Q84.1), a university (Q93.3), a chief oper-ating officer (Q108.3), a political party (Q183.3), apyramid (Q186.4), and a movie (Q211.5).In Table 8 we compare performance on questionsfrom the 2005 and 2006 test sets.
We assessedperformance primarily on test questions that weredeemed likely to benefit from hyponym knowledge?
questions that had a readily discernible category(e.g., ?What film ...?, ?In what country ...?)
?
but wealso give results on the entire test set.The WordNet-only run suffers a large decreasecompared to the baseline.
This is expected becauseWordNet lacks coverage of entities and the baselinecondition specifically populates common categoriesof entities that have been observed in prior TRECevaluations.
Nonetheless, WordNet is useful to thesystem because it addresses lexical mismatch thatdoes not involve entities.The full SFD, the SWN, and the +Feat modelachieved 17%, 2%, and 9% improvements in answercorrectness, respectively.
While no model had ex-posure to the 2005-2006 TREC questions, the SFDdatabase was manually updated based on trainingon the TREC-8 through TREC-2004 data sets.
Itapproximates an upper bound on gains attributableto addition of hyponym knowledge: it has an un-fair advantage over the other models because recentquestion sets use similar categories to those in ear-lier TRECs.
Our +Feat model, which has no biastowards TREC questions, realizes larger gains thanthe SWN.
This is probably at least in part because itproduced a more diverse set of classes and a signif-icantly larger number of class instances.
Comparedto the baseline condition the +Feat model sees a 7%improvement in mean reciprocal rank and a 9% im-provement in correct first answers; both results rep-resent a doubling of performance compared to theuse of WordNet alne.
We believe that these resultsillustrate clear improvement attributable to automat-ically learned hyponyms.The rightmost columns in Table 8 reveal that themagnitude of improvements, when measured overall questions, is less.
But the drop off is consistentwith the fact that only one third of questions haveclear need for entity knowledge.5 DiscussionAlthough there is a significant body of work in auto-mated ontology construction, few researchers haveexamined the relationship between their methods803Table 8: QA Performance on TREC 2005 & 2006 DataHyponym-Relevant Subset (242) All Questions (734)MRR % Correct MRR % CorrectWN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)Baseline 0.348 26.4 0.342 26.4SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)for knowledge discovery and improved question-answering performance.
One notable study was con-ducted by Mann (2002).
Our work differs in twoways: (1) his method for identifying hyponyms wasbased on a single syntactic pattern, and (2) he lookedat a comparatively simple task ?
given a questionand one answer sentence containing the answer, ex-tract the correct named entity answer.Other attempts to deal with lexical mismatch inautomated QA include rescoring based on syntacticvariation (Cui et al, 2005) and identification of ver-bal paraphrases (Lin and Pantel, 2001).The main contribution of this paper is showingthat large-scale, weakly-supervised hyponym learn-ing is capable of producing improvements in an end-to-end QA system.
In contrast, previous studies havegenerally presented algorithmic advances and show-cased sample results, but failed to demonstrate gainsin a realistic application.
While the hypothesis thatdiscovering is-a relations for entities would improvefactoid QA is intuitive, we believe these experimentsare important because they show that automaticallydistilled knowledge, even when containing errorsthat would not be introduced by human ontologists,is effective in question answering systems.6 ConclusionWe have shown that highly accurate statistical learn-ing of named entity hyponyms is feasible and thatbootstrapping and feature augmentation can signif-icantly improve classifier accuracy.
Mean aver-age precision of 53% was attained on a set of 75categories that included many fine-grained entityclasses.
We also demonstrated that mining knowl-edge about entities can be directly applied to ques-tion answering, and we measured the benefit onTREC QA data.
On a subset of questions forwhich NE hyponyms are likely to help we found thatlearned hyponyms generated a 9% improvement inperformance compared to a strong baseline.ReferencesHang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-SengChua.
2005.
Question answering passage retrieval usingdependency relations.
In SIGIR 2005, pages 400?407.Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,and Alexander Yates.
2005.
Unsupervised Named-EntityExtraction from the Web: An Experimental Study.
ArtificialIntelligence, 165(1):191?134.Christine Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Marti A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In ACL 1992, pages 539?545.Dekang Lin and Patrick Pantel.
2001.
Discovery of inferencerules for question-answering.
Natural Language Engineer-ing, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation of minipar.In Workshop on the Evaluation of Parsing Systems.Gideon S. Mann.
2002.
Fine-grained proper noun ontolo-gies for question answering.
In COLING-02 on SEMANET,pages 1?7.Marius Pasca and Benjamin Van Durme.
2007.
What you seekis what you get: Extraction of class attributes from querylogs.
In IJCAI-07, pages 2832?2837.Marius Pasca and Sanda M. Harabagiu.
2001.
The informa-tive role of wordnet in open-domain question answering.
InProceedings of the NAACL 2001 Workshop on WordNet andOther Lexical Resources.Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,and Thomas Smith.
2005.
QACTIS-based Question An-swering at TREC 2005.
In Proceedings of the 14th Text RE-trieval Conference.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.
Learn-ing syntactic patterns for automatic hypernym discovery.
InNIPS 17.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.
Seman-tic taxonomy induction from heterogenous evidence.
In ACL2006, pages 801?808.804
