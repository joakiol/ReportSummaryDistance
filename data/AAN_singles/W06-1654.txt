Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457?464,Sydney, July 2006. c?2006 Association for Computational LinguisticsRandom Indexing using Statistical Weight FunctionsJames Gorman and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{jgorman2,james}@it.usyd.edu.auAbstractRandom Indexing is a vector space tech-nique that provides an efficient and scal-able approximation to distributional simi-larity problems.
We present experimentsshowing Random Indexing to be poor athandling large volumes of data and evalu-ate the use of weighting functions for im-proving the performance of Random In-dexing.
We find that Random Index is ro-bust for small data sets, but performancedegrades because of the influence high fre-quency attributes in large data sets.
Theuse of appropriate weight functions im-proves this significantly.1 IntroductionSynonymy relations between words have beenused to inform many Natural Language Processing(NLP) tasks.
While these relations can be extractedfrom manually created resources such as thesauri(e.g.
Roget?s Thesaurus) and lexical databases(e.g.
WordNet, Fellbaum, 1998), it is often ben-eficial to extract these relationships from a corpusrepresentative of the task.Manually created resources are expensive andtime-consuming to create, and tend to suffer fromproblems of bias, inconsistency, and limited cover-age.
These problems may result in an inappropri-ate vocabulary, where some terms are not presentor an unbalanced set of synonyms.
In a medicalcontext it is more likely that administration will re-fer to the giving of medicine than to paper work,whereas in a business context the converse is morelikely.The most common method for automaticallycreating these resources uses distributional simi-larity and is based on the distributional hypoth-esis that similar words appear in similar con-texts.
Terms are described by collating informa-tion about their occurrence in a corpus into vec-tors.
These context vectors are then compared forsimilarity.
Existing approaches differ primarily intheir definition of context, e.g.
the surroundingwords or the entire document, and their choice ofdistance metric for calculating similarity betweenthe context vectors representing each term.In this paper, we analyse the use of Random In-dexing (Kanerva et al, 2000) for semantic similar-ity measurement.
Random Indexing is an approxi-mation technique proposed as an alternative to La-tent Semantic Analysis (LSA, Landauer and Du-mais, 1997).
Random Indexing is more scalableand allows for the incremental learning of contextinformation.Curran and Moens (2002) found that dramati-cally increasing the volume of raw input data fordistributional similarity tasks increases the accu-racy of synonyms extracted.
Random Indexingperforms poorly on these volumes of data.
Notingthat in many NLP tasks, including distributionalsimilarity, statistical weighting is used to improveperformance, we modify the Random Indexing al-gorithm to allow for weighted contexts.We test the performance of the original and ourmodified system using existing evaluation metrics.We further evaluate against bilingual lexicon ex-traction using distributional similarity (Sahlgrenand Karlgren, 2005).
The paper concludes witha more detailed analysis of Random Indexing interms of both task and corpus composition.
Wefind that Random Index is robust for small cor-pora, but larger corpora require that the contextsbe weighted to maintain accuracy.4572 Random IndexingRandom Indexing is an approximating techniqueproposed by Kanerva et al (2000) as an alternativeto Singular Value Decomposition (SVD) for LatentSemantic Analysis (LSA, Landauer and Dumais,1997).
In LSA, it is assumed that there is someunderlying dimensionality in the data, so that theattributes of two or more terms that have similarmeanings can be folded onto a single axis.Sahlgren (2005) criticise LSA for being bothcomputationally inefficient and requiring the for-mation of a full co-occurrence matrix and its de-composition before any similarity measurementscan be made.
Random Indexing avoids both theseby creating a short index vector for each uniquecontext, and producing the context vector for eachterm by summing index vectors for each contextas it is read, allowing an incremental building ofthe context space.Hecht-Nielsen (1994) observed that there aremany more nearly orthogonal directions in high-dimensional space than there are truly orthogo-nal directions.
The random index vectors arenearly-orthogonal, resulting in an approximatedescription of the context space.
The approx-imation comes from the Johnson-Lindenstrausslemma (Johnson and Lindenstrauss, 1984), whichstates that if we project points in a vector spaceinto a randomly selected subspace of sufficientlyhigh dimensionality, the distances between thepoints are approximately preserved.
Random Pro-jection (Papadimitriou et al, 1998) and RandomMapping (Kaski, 1998) are similar techniques thatuse this lemma.
Achlioptas (2001) showed thatmost zero-mean distributions with unit variance,including very simple ones like that used in Ran-dom Indexing, produce a mapping that satisfiesthe lemma.
The following description of Ran-dom Indexing is taken from Sahlgren (2005) andSahlgren and Karlgren (2005).We allocate a d length index vector to eachunique context as is it found.
These vectors con-sist of a large number of 0s and a small number(?)
of ?1s.
Each element is allocated one of thesevalues with the following probability:????
?+1 with probability ?/2d0 with probability d?
?d?1 with probability ?/2dContext vectors are generated on-the-fly.
As thecorpus is scanned, for each term encountered, itscontexts are extracted.
For each new context, anindex vector is produced for it as above.
The con-text vector is the sum of the index vectors of allthe contexts in which the term appears.The context vector for a term t appearing in oneeach in the contexts c1 = [1, 0, 0,?1] and c2 =[0, 1, 0,?1] would be [1, 1, 0,?2].
If the contextc1 encountered again, no new index vector wouldbe generated and the existing index vector for c1would be added to the existing context vector toproduce a new context vector for t of [2, 1, 0,?3].The distance between these context vectors canthen be measured using any vector space distancemeasure.
Sahlgren and Karlgren (2005) use thecosine measure:cos(?
(u, v)) = ~u ?
~v|~u| |~v| =?di=1 ~ui~vi?
?di=1 ~u2i?
?di=1 ~v2iRandom Indexing allows for incremental sam-pling.
This means that the entire data set need notbe sampled before similarity between terms can bemeasured.
It also means that additional contextinformation can be added at any time without in-validating the information already produced.
Thisis not feasible with most other word-space mod-els.
The approach used by Grefenstette (1994) andCurran (2004) requires the re-computation of allnon-linear weights if new data is added, althoughsome of these weights can be approximated whenadding new data incrementally.
Similarly, newdata can be folded into a reduced LSA space, butthere is no guarantee that the original smoothingwill apply correctly to the new data (Sahlgren,2005).3 WeightsOur initial experiments using Random Indexingto extract synonymy relations produced worse re-sults than those using full vector measures, such asJACCARD (Curran, 2004), when the full vector isweighted.
We experiment using weight functionswith Random Indexing.Only a linear weighting scheme can be appliedwhile maintaining incremental sampling.
Whileincremental sampling is part of the rationale be-hind its development, it is not required for Ran-dom Indexing to work as a dimensionality reduc-tion technique.To this end, we revise Random Indexing to en-able us to use weight functions.
For each unique458IDENTITY 1.0 FREQ f(w, r, w?
)RELFREQ f(w,r,w?)f(w,?,?)
TF-IDFf(w,r,w?)n(?,r,w?)TF-IDF?
log2(f(w,r,w?)+1)log2(1+N(r,w?)n(?,r,w?)
)MI log( p(w,r,w?)p(w,?,?)p(?,r,w?)
)TTEST p(w,r,w?)?p(?,r,w?)p(w,?,?)?p(?,r,w?)p(w,?,?
)GREF94 log2(f(w,r,w?)+1)log2(n(?,r,w?
)+1)LIN98A log( f(w,r,w?)f(?,r?)f(?,r,w?)f(w,r,?)
) LIN98B ?
log(n(?,r,w?
)Nw )CHI2 cf.
Manning and Schu?tze (1999) LR cf.
Manning and Schu?tze (1999)DICE 2p(w,r,w?)p(w,?,?)+p(?,r,w?
)Table 1: Weight Functions Evaluatedcontext attribute, a d length index vector will begenerated.
The context vector of a term w is thencreated by the weighted sum of each of its at-tributes.
The results of the original Random In-dexing algorithm are reproduced using frequencyweighting (FREQ).Weights are generated using the frequency dis-tribution of each term and its contexts.
This in-creases the overhead, as we must store the contextattributes for each term.
Rather than the contextvector being generated by adding each individualcontext, it is generated by adding each the indexvector for each unique context multiplied by itsweight.The time to calculate the weight of all attributesof all terms is negligible.
The original techniquescales to O(dnm) in construction, for n terms andm unique attributes.
Our new technique scales toO(d(a + nm)) for a non-zero context attributesper term, which since a  m is also O(dnm).Following the notation of Curran (2004), a con-text relation is defined as a tuple (w, r, w?)
wherew is a term, which occurs in some grammatical re-lation r with another word w?
in some sentence.We refer to the tuple (r, w?)
as an attribute of w.For example, (dog, direct-obj, walk) indicates thatdog was the direct object of walk in a sentence.An asterisk indicates the set of all existing val-ues of that component in the tuple.
(w, ?, ?)
?
{(r, w?)|?
(w, r, w?
)}The frequency of a tuple, that is the number oftimes a word appears in a context is f(w, r, w?
).f(w, ?, ?)
is the instance or token frequency of thecontexts in which w appears.
n(w, ?, ?)
is the typefrequency.
This is the number of attributes of w.f(w, ?, ?)
??(r,w?)?(w,?,?)
f(w, r, w?
)p(w, ?, ?)
?
f(w,?,?)f(?,?,?
)n(w, ?, ?)
?
|(w, ?, ?
)|Nw ?
|{w|n(w, ?, ?)
> 0}|Most experiments limited weights to the positiverange; those evaluated with an unrestricted rangeare marked with a ?
suffix.
Some weights werealso evaluated with an extra log2(f(w, r, w?)
+1) factor to promote the influence of higher fre-quency attributes, indicated by a LOG suffix.
Al-ternative functions are marked with a dagger.The context vector of each term w is thus:w?
=?(r,w?)?(w,?,?
)~(r, w?)
wgt(w, r, w?
)where ~(r, w?)
is the index vector of the context(r, w?).
The weights functions we evaluate arethose from Curran (2004) and are given in Table 1.4 Semantic SimilarityThe first use of Random Indexing was to measuresemantic similarity using distributional similarity.Kanerva et al (2000) used Random Indexing tofind the best synonym match in Test of Englishas a Foreign Language (TOEFL).
TOEFL was usedby Landauer and Dumais (1997), who reported anaccuracy 36% using un-normalised vectors, whichwas improved to 64% using LSA.
Kanerva et al(2000) produced an accuracy of 48?51% using thesame type of document based contexts and Ran-dom Indexing, which improved to 62?70% usingnarrow context windows.
Karlgren and Sahlgren(2001) improved this to 72% using lemmatisationand POS tagging.4594.1 Distributional SimilarityMeasuring distributional similarity first requiresthe extraction of context information for each ofthe vocabulary terms from raw text.
The contextsfor each term are collected together and counted,producing a vector of context attributes and theirfrequencies in the corpus.
These terms are thencompared for similarity using a nearest-neighboursearch based on distance calculations between thestatistical descriptions of their contexts.The simplest algorithm for finding synonyms isa k-nearest-neighbour search, which involves pair-wise vector comparison of the context vector ofthe target term with the context vector of everyother term in the vocabulary.We use two types of context extraction to pro-duce both high and low quality context descrip-tions.
The high quality contexts were extractedfrom grammatical relations extracted using theSEXTANT relation extractor (Grefenstette, 1994)and are lemmatised.
This is the same data used inCurran (2004).The low quality contexts were extracted takinga window of one word to the left and right of thetarget term.
The context is marked as to whetherit preceded or followed the term.
Curran (2004)found this extraction technique to provided rea-sonable results on the non-speech portion of theBNC when the data was lemmatised.
We do notlemmatise, which produces noisier data.4.2 Bilingual Lexicon AcquisitionA variation on the extraction of synonymy rela-tions, is the extraction of bilingual lexicons.
Thisis the task of finding for a word in one languagewords of a similar meaning in a second language.The results of this can be used to aid manual con-struction of resources or directly aid translation.This task was first approached as a distribu-tional similarity-like problem by Brown et al(1988).
Their approach uses aligned corpora intwo or more languages: the source language, fromwhich we are translating, and the target language,to which we are translating.
For a each alignedsegment, they measure co-occurrence scores be-tween each word in the source segment and eachword in the target segment.
These co-occurrencescores are used to measure the similarity betweensource and target language termsSahlgren and Karlgren?s approach models theproblem as a distributional similarity problem us-Source Context TargetLanguage Languageaaabbc I xxyzzzbcc II wxyaab III xzzTable 2: Paragraph Aligned Corporaing the paragraph as context.
In Table 2, the sourcelanguage is limited to the words a, b and c and thetarget language to the words x, y and z.
Three para-graphs in each of these languages are presented aspairs of translations labelled as a context: aaabbcis translated as xxyzzz and labelled context I. Thefrequency weighted context vector for a is {I:3,III:2} and for x is {I:2, II:1, III:1}.A translation candidate for a term in the sourcelanguage is found by measuring the similarity be-tween its context vector and the context vectors ofeach of the terms in the target language.
The mostsimilar target language term is the most likelytranslation candidate.Sahlgren and Karlgren (2005) use Random In-dexing to produce the context vectors for thesource and target languages.
We re-implementtheir system and apply weighting functions in anattempt to achieve improved results.5 ExperimentsFor the experiments extracting synonymy rela-tions, high quality contexts were extracted fromthe non-speech portion of the British NationalCorpus (BNC) as described above.
This represents90% of the BNC, or 90 million words.Comparisons between low frequency terms areless accurate than between high frequency termsas there is less evidence describing them (Cur-ran and Moens, 2002).
This is compounded inrandomised vector techniques because the ran-domised nature of the representation means thata low frequency term may have a similar contextvector to a high frequency term while not sharingmany contexts.
A frequency cut-off of 100 wasfound to balance this inaccuracy with the reduc-tion in vocabulary size.
This reduces the original246,046 word vocabulary to 14,862 words.
Exper-iments showed d = 1000 and ?
= 10 to provide abalance between speed and accuracy.Low quality contexts were extracted from por-tions of the entire of the BNC.
These formed cor-pora of 100,000, 500,000, 1 million, 5 million, 10460million, 50 million and 100 million words, cho-sen from random documents.
This allowed us testthe effect of both corpus size and context qual-ity.
This produced vocabularies of between 10,380and 522,163 words in size.
Because of the sizeof the smallest corpora meant that a high cutoffwould remove to many terms for a fair test, a cut-off of 5 was applied.
The values d = 1000 and?
= 6 were used.For our experiments in bilingual lexicon acqui-sition we follow Sahlgren and Karlgren (2005).We use the Spanish-Swedish and the English-German portions of the Europarl corpora (Koehn,2005).1 These consist of 37,379 aligned para-graphs in Spanish?Swedish and 45,556 in English-German.
The text was lemmatised using Con-nexor Machinese (Tapanainen and Ja?vinen, 1997)2producing vocabularies of 42,671 terms of Span-ish, 100,891 terms of Swedish, 40,181 terms ofEnglish and 70,384 terms of German.
We used = 600 and ?
= 6 and apply a frequency cut-off of 100.6 Evaluation MeasuresThe simplest method for evaluation is the directcomparison of extracted synonyms with a man-ually created gold standard (Grefenstette, 1994).To reduce the problem of limited coverage, ourevaluation of the extraction of synonyms combinesthree electronic thesauri: the Macquarie, Roget?sand Moby thesauri.We follow Curran (2004) and use two perfor-mance measures: direct matches (DIRECT) andinverse rank (INVR).
DIRECT is the number ofreturned synonyms found in the gold standard.INVR is the sum of the inverse rank of each match-ing synonym, e.g.
matches at ranks 3, 5 and 28give an inverse rank score of 13 + 15 + 128 .
Withat most 100 matching synonyms, the maximumINVR is 5.187.
This more fine grained as it incor-porates the both the number of matches and theirranking.The same 300 single word nouns were used forevaluation as used by Curran (2004) for his largescale evaluation.
These were chosen randomlyfrom WordNet such that they covered a range overthe following properties: frequency, number ofsenses, specificity and concreteness.
On averageeach evaluation term had 301 gold-standard syn-1http://www.statmt.org/europarl/2http://www.connexor.com/Weight DIRECT INVRFREQ 2.87 0.94IDENTITY 3.18 0.95RELFREQ 2.87 0.94TF-IDF 0.30 0.07TF-IDF?
3.92 1.39MI 1.52 0.54MILOG 3.38 1.39MI?
1.87 0.65MILOG?
3.49 1.41TTEST 1.06 0.52TTESTLOG 1.53 0.62TTEST?
1.06 0.52TTESTLOG?
1.52 0.61GREF94 2.82 0.86LIN98A 1.52 0.50LIN98B 2.95 0.84CHI2 0.46 0.25DICE 3.32 1.11DICELOG 2.56 0.81LR 1.96 0.58Table 3: Evaluation of synonym extractiononyms.
For each of these terms, the closest 100terms and their similarity scores were extracted.For the evaluation of bilingual lexicon acqui-sition we use two online lexical resources usedby Sahlgren and Karlgren (2005) as gold stan-dards: Lexin?s online Swedish-Spanish lexicon3and TU Chemnitz?
online English-German dic-tionary.4 Each of the elements in a compoundor multi-word expression is treated as a poten-tial translation.
The German abblendlicht (low beamlight) is treated as a translation candidate for low,beam and light separately.Low coverage is more of problem than in ourthesaurus task as we have not used combined re-sources.
There are an average of 19 translationsfor each of the 3,403 Spanish terms and 197 trans-lations for each of the 4,468 English terms.
TheEnglish-German translation count is skewed bythe presence of connectives in multi-word expres-sions, such as of and on, producing mistranslations.Sahlgren and Karlgren (2005) provide good com-mentary on the evaluation of this task.Spanish and English are used as the source lan-guages.
The 200 closest terms in the target lan-guage are found for all terms in both the sourcevocabulary and the gold-standards.We measure the DIRECT score and INVR asabove.
In addition we measure the precision of theclosest translation candidate, as used in Sahlgrenand Karlgren (2005).3http://lexin.nada.kth.se/sve-spa.shtml4http://dict.tu-chemnitz.de/461Weight English-German Spanish-SwedishDIRECT Precision INVR DIRECT Precision INVRFREQ 6.1 58% 0.97 0.8 47% 0.53IDENTITY 6.0 58% 0.91 0.8 47% 0.53RELFREQ 6.1 58% 0.97 0.8 47% 0.53TF-IDF 4.9 53% 0.84 0.8 43% 0.50TF-IDF?
6.3 58% 0.94 0.8 47% 0.53MI 2.3 58% 0.76 0.8 48% 0.56MILOG 2.1 58% 0.76 0.8 49% 0.56MI?
4.6 57% 0.86 0.8 46% 0.53MILOG?
4.6 57% 0.87 0.8 47% 0.54TTEST 2.1 57% 0.75 0.8 48% 0.56TTESTLOG 1.9 56% 0.72 0.8 46% 0.54TTEST?
4.3 57% 0.85 0.8 45% 0.53TTESTLOG?
4.0 56% 0.80 0.8 46% 0.53GREF94 6.1 58% 0.95 0.8 48% 0.54LIN98A 4.0 59% 0.82 0.8 48% 0.56LIN98B 5.9 58% 0.91 0.8 48% 0.54CHI2 3.1 50% 0.71 0.7 41% 0.48DICE 5.7 58% 0.95 0.8 47% 0.53DICELOG 4.7 57% 0.90 0.8 46% 0.52LR 4.5 57% 0.86 0.8 47% 0.54Table 4: Evaluation of bilingual lexicon extraction7 ResultsTable 3 shows the results for the experiments ex-tracting synonymy.
The basic Random Indexingalgorithm (FREQ) produces a DIRECT score of2.87, and an INVR of 0.94.
It is interesting thatthe only other linear weight, IDENTITY, producesmore accurate results.
This shows high frequency,low information contexts reduce the accuracy ofRandom Indexing.
IDENTITY removes this effectby ignoring frequency, but does not address theinformation aspect.
A more accurate weight willconsider the information provided by a context inits weighting.There was a large variance in the effective-ness of the other weights and most proved to bedetrimental to Random Indexing.
TF-IDF was theworst, reducing the DIRECT score to 0.30 and theINVR to 0.07.
TF-IDF?, which is a log-weightedalternative to TF-IDF, produced very good results.With the exception of DICELOG, adding anadditional log factor improved performance (TF-IDF?, MILOG and TTESTLOG).
Unrestrictedranges improved the MI family, but made no dif-ference to TTEST.
Grefenstette?s variation onTF-IDF (GREF94) does not perform as well asTF-IDF?, and Lin?s variations on MI?
(LIN98A,LIN98B) do not perform as well as MILOG?.MILOG?
had a higher INVR than TF-IDF?, buta lower DIRECT score, indicating that it forcesmore correct results to the top of the results list,but also forces some correct results further downso that they no longer appear in the top 100.Weight BNC LARGEDIRECT INVR DIRECT INVRFREQ 8.9 0.93 7.2 0.85TF-IDF?
11.8 1.39 12.5 1.50MILOG?
10.5 1.41 13.8 1.75Table 5: Evaluation of Random Indexing using avery large corpusThe effect of high frequency contexts is in-creased further as we increase the size of the cor-pus.
Table 5 presents results using the 2 billionword corpus used by Curran (2004).
This consistsof the non-speech portion of the BNC, the Reuter?sCorpus Volume 1 and most of the English newsholdings of the LDC in 2003.
Contexts were ex-tracted as presented in Section 4.
A frequency cut-off of 100 was applied and the values d = 1000and ?
= 5 for FREQ and ?
= 10 for the improvedweights were used.We see that the very large corpus has reducedthe accuracy of frequency weighted Random In-dexing.
In contrast, our two top performers haveboth substantially increased in accuracy, present-ing a 75?100% improvment in performance overFREQ.
MILOG?
is more accurate than TF-IDF?for both measures of accuracy now, indicating it isa better weight function for very large data sets.7.1 Bilingual Lexicon AcquisitionWhen the same function were applied to the bilin-gual lexicon acquisition task we see substantiallydifferent results: neither the improvement nor theextremely poor results are found (Table 4).4620.10.20.30.40.50.60.70.80.911.10  20  40  60  80  100INVRCorpus Size (millions of words)FREQIDENTITYTF-IDF?MILOGJACCARDFigure 1: Random Indexing using window-basedcontextIn the English-German corpora we replicateSahlgren and Karlgren?s (2005) results, with a pre-cision of 58%.
This has a DIRECT score of 6.1 andan INVR of 0.97.
The only weight to make an im-provement is TF-IDF?, which has a DIRECT scoreof 6.3, but a lower INVR and all weights performworse in at least one measure.Our results for the Spanish-Swedish corporashow similar results.
Our accuracy is down fromthat in Sahlgren and Karlgren (2005).
This is ex-plained by our application of the frequency cut-offto both the source and target languages.
There aremore weights with higher accuracies, and fewerwith significantly lower accuracies.7.2 Smaller CorporaThe absence of a substantial improvement in bilin-gual lexicon acquisition requires further investiga-tion.
Three main factors differ between our mono-lingual and bilingual experiments: that we aresmoothing a homogeneous data set in our mono-lingual experiments and a heterogeneous data setin our bilingual experiments; we are using localgrammatical contexts in our monolingual experi-ments and paragraph contexts in our bilingual ex-periments; and, the volume of raw data used in ourmonolingual experiments is many times that usedin our bilingual experiments.Figure 1 presents results for corpora extractedfrom the BNC using the window-based context.Results are shown for the original Random Index-ing (FREQ) and using IDENTITY, MILOG?
andTF-IDF?, as well as for the full vector measure-ment using JACCARD measure and the TTEST?weight (Curran, 2004).
Of the Random Index-ing results FREQ produces the lowest overall re-sults.
It performs better than MILOG?
for verysmall corpora, but produces near constant resultsfor greater corpus sizes.
Curran and Moens (2002)found that increasing the volume of input data in-creased the accuracy of results generated using afull vector space model.
Without weighting, Ran-dom Indexing fails this, but after weighting is ap-plied Curran and Moens?
results are confirmed.The quality of context extracted influences howweights perform individually, but Random In-dexing using weights still outperforms not usingweights.
The relative performance of MILOG?has been reduced when compared with TF-IDF?,but is still greater then FREQ.Gorman and Curran (2006) showed Random In-dexing to be much faster than full vector spacetechniques, but with a 46?56% reduction in accu-racy compared to using JACCARD and TTEST?
.Using the MI?
weight kept the improvement inspeed but with only a 10?18% reduction in accu-racy.
When JACCARD and TTEST?
are used withour low quality contexts they perform consistentlyworse that Random Indexing.
This indicates Ran-dom Indexing is stable in the presence of noisydata.
It would be interesting to further comparethese results to those produced by LSA.The results we have presented have shown thatapplying weights to Random Indexing can im-prove its performance for thesaurus extractiontasks.
This improvement is dependent on the vol-ume of raw data used to generate the context in-formation.
It is less dependent on the quality ofcontexts extracted.What we have not shown is whether this extendsto the extraction of bilingual lexicons.
The bilin-gual corpora have 12-16 million words per lan-guage, and for this sized corpora we already seesubstantial improvement with corpora as small as5 million words (Figure 1).
It may be that ex-tracting paragraph-level contexts is not well suitedto weighting, or that the heterogeneous nature ofthe aligned corpora reduces the meaningfulness ofweighting.
There is also the question as to whetherit can be applied to all languages.
There is a lack offreely available large-scale multi-lingual resourcesthat makes this difficult to examine.8 ConclusionWe have applied weighting functions to the vec-tor space approximation Random Indexing.
Forlarge data sets we found a significant improvement463when weights were applied.
For smaller data setswe found that Random Indexing was sufficientlyrobust that weighting had at most a minor effect.Our weighting schemes removed the possibil-ity of incremental learning of the term space.
Aninteresting direction would be the development ofalgorithms that allowed the incremental applica-tion of weights, perhaps by re-weighting vectorswhen a new context is learned.Other areas left open for investigation are the in-teraction between Random Indexing, weights andthe type of context extracted, the use of large-scale bilingual corpora, the acquisition of lexi-cons for non-Indo-European languages and acrosslanguage family boundaries, and the difference ineffect term and paragraph/document contexts forthesaurus extraction.We have demonstrated that the accuracy of Ran-dom Indexing can be improved by applying weightfunctions, increasing accuracy by up to 50% on theBNC and 100% on a 2 billion word corpus.AcknowledgementsWe would like to thank Magnus Sahlgren for gen-erously supplying his training and evaluation dataand our reviewers for their helpful feedback andcorrections.
This work has been supported bythe Australian Research Council under DiscoveryProject DP0453131.ReferencesDimitris Achlioptas.
2001.
Database-friendly random pro-jections.
In Symposium on Principles of Database Sys-tems, pages 274?281, Santa Barbara, CA, USA, 21?23May.Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vin-cent J. Della Pietra, Frederick Jelinek, Robert L. Mer-cer, and Paul S. Roossin.
1988.
A statistical approachto language translation.
In Proceedings of the 12th Con-ference on Computational linguistics, pages 71?76, Bu-dapest, Hungry, 22?27 August.James R. Curran and Marc Moens.
2002.
Scaling con-text space.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics, pages231?238, Philadelphia, PA, USA, 7?12 July.James R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Christiane Fellbaum, editor.
1998.
WordNet: an electroniclexical database.
The MIT Press, Cambridge, MA, USA.James Gorman and James R. Curran.
2006.
Scaling distri-butional similarity to large corpora.
In Proceedings of the44th Annual Meeting of the Association for ComputationalLinguistics, Sydney, Australia, 17?21 July.
To appear.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, Boston.Robert Hecht-Nielsen.
1994.
Context vectors: general pur-pose approximate meaning representations self-organizedfrom raw data.
Computational Intelligence: ImitatingLife, pages 43?56.William B. Johnson and Joram Lindenstrauss.
1984.
Exten-sions to Lipshitz mapping into Hilbert space.
Contempo-rary mathematics, 26:189?206.Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000.Random indexing of text samples for latent semantic anal-ysis.
In Proceedings of the 22nd Annual Conference of theCognitive Science Society, page 1036, Philadelphia, PA,USA, 13?15 August.Jussi Karlgren and Magnus Sahlgren.
2001.
From words tounderstanding.
In Y. Uesaka, P. Kanerva, and H Asoh, ed-itors, Foundations of Real-World Intelligence, pages 294?308.
CSLI Publications, Stanford, CA, USA.Samuel Kaski.
1998.
Dimensionality reduction by randommapping: Fast similarity computation for clustering.
InProceedings of the International Joint Conference on Neu-ral Networks, pages 413?418.
Piscataway, NJ, USA, 31July?4 August.Philipp Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit X, Phuket, Thai-land, 12?16 September.Thomas K. Landauer and Susan T. Dumais.
1997.
A solutionto plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240, April.Christopher D. Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.
TheMIT Press, Cambridge, MA, USA.Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Ragha-van, and Santosh Vempala.
1998.
Latent semantic index-ing: A probabilistic analysis.
In Proceedings of the 17thACM Symposium on the Principle of Database Systems,pages 159?168, Seattle, WA, USA, 2?4 June.Magnus Sahlgren and Jussi Karlgren.
2005.
Automatic bilin-gual lexicon acquisition using random indexing of parallelcorpora.
Journal of Natural Language Engineering, Spe-cial Issue on Parallel Texts, 11(3), June.Magnus Sahlgren.
2005.
An introduction to random index-ing.
In Methods and Applications of Semantic IndexingWorkshop at the 7th International Conference on Termi-nology and Knowledge Engineering, Copenhagen, Den-mark, 16 August.Pasi Tapanainen and Timo Ja?vinen.
1997.
A non-projectivedependency parser.
In Proceedings of the 5th Conferenceon Applied Natural Language Processing, pages 64?71,31 March?3 April.464
