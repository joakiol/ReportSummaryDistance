Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 88?96,Baltimore, Maryland USA, June 27, 2014.c?2014 Association for Computational LinguisticsLinguistic and Acoustic Features for Automatic Identification of AutismSpectrum Disorders in Children?s NarrativeHiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi NakamuraGraduate School of Information Science, Nara Institute of Science and Technology{hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jpAbstractAutism spectrum disorders are develop-mental disorders characterised as deficitsin social and communication skills, andthey affect both verbal and non-verbalcommunication.
Previous works measureddifferences in children with and withoutautism spectrum disorders in terms oflinguistic and acoustic features, althoughthey do not mention automatic identifi-cation using integration of these features.In this paper, we perform an exploratorystudy of several language and speech fea-tures of both single utterances and full nar-ratives.
We find that there are charac-teristic differences between children withautism spectrum disorders and typical de-velopment with respect to word categories,prosody, and voice quality, and that thesedifferences can be used in automatic clas-sifiers.
We also examine the differencesbetween American and Japanese childrenand find significant differences with re-gards to pauses before new turns and lin-guistic cues.1 IntroductionAutism spectrum disorders (ASD) are develop-mental disorders, first described by Kanner andAsperger in 1943 and 1944 respectively (Kanner,1943; Asperger, 1944).
The American Psychi-atric Association defines the two characteristics ofASD as: 1) persistent deficits in social communi-cation and social interaction across multiple con-texts, and 2) restricted, repetitive patterns of be-havior, interests, or activities (American Psychi-atric Association, 2013).
In particular, the formerdeficits in social communication are viewed as themost central characteristic of ASD.
Thus, quanti-fying the degree of social communication skills isa necessary component of understanding the na-ture of ASD, creating systems for automatic ASDscreening, and early intervention methods such associal skills training and applied behaviour analy-sis (Wallace et al., 1980; Lovaas et al., 1973).There are a number of studies finding differ-ences between people with ASD and people withtypical development (TD).
In terms of deficits insocial communication, there have been reports de-scribing atypical usage of gestures (Ashley andInge-Marie, 2010), frequency of eye-contact andlaughter (Geraldine et al., 1990), prosody (Mc-Cann and Peppe, 2003; Rhea et al., 2005), voicequality (Asgari et al., 2013), delay responses(Heeman et al., 2010), and unexpected words(Rouhizadeh et al., 2013).
In this paper, we par-ticularly focus on the cues of ASD that appear inchildren?s language and speechIn the case of language, Newton et al.
(2009)analyze blogs of people with ASD and TD, andfound that people with ASD have larger variationof usage of words describing social processes, al-though there are no significant differences in otherword categories.
In the case of speech, people withASD tend to have prosody that differs from thatof their peers (Kanner, 1943), although McCannand Peppe (2003) note that prosody in ASD is anunder-researched area and that where research hasbeen undertaken, findings often conflict.
Sincethen, there have been various studies analyzingand modeling prosody in people with ASD (Danielet al., 2012; Kiss et al., 2013; Santen et al., 2013;Van et al., 2010).
For example, Kiss et al.
(2012)find several significant differences in the pitchcharacteristics of ASD, and report that automaticclassification utilizing these features achieves ac-curacy well above chance level.
To our knowl-edge, there is no previous work integrating bothlanguage and speech features to identify differ-ences between people with ASD and TD.
How-ever, it has been noted that differences in person-88ality traits including introversion/extroversion canbe identified using these features (Mairesse et al.,2007).In this paper, we perform a comprehensive anal-ysis of language and speech features mentioned inprevious works, as well as novel features specificto this work.
In addition, while previous works an-alyzed differences between people with ASD andTD, we additionally investigate whether it is possi-ble to automatically distinguish between childrenwith ASD or TD using both language and speechfeatures and a number of classification methods.We focus on narratives, where the children servingas our subjects tell a memorable story to their par-ent (Davis et al., 2004).
Here, the use of narrativeallows us to consider not only single-sentence fea-tures, but also features considering interaction as-pects between the child and parent such as pausesbefore new turns and overall narrative-specific fea-tures such as words per minute and usage of un-expected words.
Given this setting, we performa pilot study examining differences between chil-dren with ASD and TD, the possibilities of auto-matic classification between ASD and TD, and thedifferences between American and Japanese chil-dren.2 Data DescriptionAs a target for our analysis, we first collected adata set of interactions between Japanese childrenand their parents.
In collecting the data, we fol-lowed the procedure used in the creation of theUSC Rachel corpus (Mower et al., 2011).
The dataconsists of four sessions: doh (free play), jenga (agame), narrative, and natural conversation.
Thefirst child-parent interaction is free play with theparent.
The child and parent are given play doh,Mr.
Potato Head, and blocks.
The second child-parent interaction is a jenga game.
Jenga is a gamein which the participants must remove blocks, oneat a time, from a tower.
The game ends when thetower falls.
The third child-parent interaction is anarrative task.
The child and parent are asked toexplain stories in which they experienced a mem-orable emotion.
The final child-parent interactionis a natural conversation without a task.
Thesechild-parent interactions are recorded and will en-able comparison of the child?s interaction style andcommunication with their parent.
Each sessioncontinues for 10 minutes.
During interaction, a pinmicrophone and video camera record the speechand video of the child and the parent.In this paper, we use narrative data of four chil-dren with ASD (male: 3, female: 1) and twochildren with TD (male: 1, female: 1) as an ex-ploratory study.
The intelligence quotient (IQ) forall subjects is above 70, which is often used asa threshold for diagnosis of intellectual disabil-ity.
Each subject?s age and diagnosis as ASD/TDis provided in Table 1.
In the narrative session,each child and parent speaks ?a memorable story?for 5 minutes in turn, and the listener responds tothe speaker?s story by asking questions.
After 5minutes, the experimenter provides directions tochange the turn.Table 1: Subjects?
age and diagnosisSubject A1 A2 A3 A4 T1 T2Age 10 10 10 13 10 12Diagnosis ASD ASD ASD ASD TD TDIn this paper, we analyze the child-speaking turnof the narrative session in which the parent re-sponds to the child?s utterances.
All utterances aretranscribed based on USC Rachel corpus manual(Mower et al., 2011) to facilitate comparison withthis existing corpus.
In the transcription manual, ifthe speaker pauses for more than one second, thespeech is transcribed as separate utterances.
In thispaper, we examine two segment levels, the firsttreating each speech segment independently, andthe second handling a whole narrative as the tar-get.
When handling each segment independently,we use a total of 116 utterances for both childrenwith ASD and TD.3 Single Utterance LevelIn this section, we describe language and speechfeatures and analysis of these characteristics to-wards automatic classification of utterances basedon whether they were spoken by children withASD or TD.
We hypothesize that based on the fea-tures extracted from the speech signal we are ca-pable to classify children with ASD and TD on aspeech segment level, as well as on narrative levelafter temporally combining all the segment-baseddecisions.3.1 Feature ExtractionWe extract language and speech features basedon those proposed by (Mairesse et al., 2007) and89(Hanson, 1995).
Extracted features are summa-rized in Table 2.
We also add one feature not cov-ered in previous work counting the number of oc-currences of laughter.Table 2: Description of language and speech fea-tures.Language FeaturesWords per sentence (WPS)General descriptor Words with more than 6 lettersOccurrences of laughterSentence structurePercentage of pronouns, conjunctions,negations, quantifiers, numbersPsychological proc.Percentage of words describing social,affect, cognitive, perceptual,and biologicalPersonal concernsPercentage of words describing work,achievement, leisure, and homeParalinguisticPercentage of assent,disfluencies, and fillersSpeech FeaturesPitch Statistics of sd and covIntensity Statistics of sd and covSpeech rate Words per voiced secondAmplitude of a3Voice quality Difference of the h1 and the h2Difference of the h1 and the a33.1.1 Language FeaturesWe use the linguistic inquiry and word count(LIWC) (Pennebaker et al., 2007), which is a toolto categorize words, to extract language features.Because a Japanese version of LIWC is not avail-able and there is no existing similar resource forJapanese, we implement the following proceduresto automatically establish correspondences be-tween LIWC categories and transcribed Japaneseutterances.
First, we use Mecab1for part-of-speech tagging in Japanese utterances, translateeach word into English using the WWWJDIC2dictionary, and finally determine the LIWC cate-gory corresponding to the English word.
Amongthe language features described in Table 2, wecalculate sentence structures, psychological pro-cesses, and personal concerns using LIWC, andother features using Mecab.
Here, we do notconsider language-dependent features and subcat-egories of LIWC.1https://code.google.com/p/mecab/2http://www.edrdg.org/cgi-bin/wwwjdic/wwwjdic?1C3.1.2 Speech FeaturesFor speech feature extraction, we use the Snacksound toolkit3.
Here, we consider fundamentalfrequency, power, and voice quality, which are ef-fective features according to previous works (Mc-Cann and Peppe, 2003; Hanson, 1995).
We donot extract mean values of fundamental frequencyand power because those features are strongly re-lated to individuality.
Thus, we extract statisticsof standard deviation (fsd, psd) and coefficient ofvariation (fcov, pcov) for fundamental frequencyand power.
We calculate speech rate, which is afeature dividing the number of words by the num-ber of voiced seconds.
Voice quality is also com-puted using: the amplitude of the third formant(a3), the difference between the first harmonic andthe second harmonic (h1h2), and the differencebetween the first harmonic and the third formant(h1a3) (Hanson, 1995).3.1.3 Projection NormalizationFor normalization, we simply project all featurevalues to a range of [0, 1], where 0 correspondsto the smallest observed value and 1 to the largestobserved value across all utterances.
For utterancei, we define the value of the jth feature as vijanddefine pij=vij?minjmaxj?minj, where pijis the featurevalue after normalisation.3.2 Characteristics of Language and SpeechFeaturesIn this section, we report the result of a t-test, prin-cipal component analysis, factor analysis, and de-cision tree using the normalised features.
We useR4for statistical analysis.Table 3 shows whether utterances of childrenwith ASD or TD have a greater mean on the cor-responding feature.
The results indicate that thechildren with ASD more frequently use wordswith more than 6 letters (e.g.
complicated words),assent (e.g.
?uh-huh,?
or ?un?
in Japanese), andfillers (e.g.
?umm,?
or ?eh?
in Japanese) signif-icantly more than the children with TD.
In con-trast, the children with TD more frequently use thewords words categorized as social (e.g.
friend), af-fect (e.g.
enjoy), and cognitive (e.g.
understand)significantly more than the children with ASD.
Inaddition, there are differences in terms of funda-mental frequency variations and voice quality (e.g.3http://www.speech.kth.se/snack/4http://www.r-project.org90Table 3: Difference of mean values between ASD and TD based on language and speech features fromchildren?s utterances.
Each table cell notes which of the two classes has the greater mean on the corre-sponding feature (*: p < 0.01, **: p < 0.005).WPS 6 let.
laughter adverb pronoun conjunctions negations quantifiers numbers social- ASD* - - - - - - - TD**affect cognitive perceptual biological relativity work achievement leisure home assentTD** TD* - - - - - - - ASD**nonfluent fillers fsd fcov psd pcov speech rate a3 h1h2 h1a3- ASD* TD** TD* - - - - - ASD**h1a3).
In particular, we observe that the childrenwith ASD tend to use monotonous intonation asreported in (Kanner, 1943).
We do not confirm asignificant differences in other features.Next, we use principal component analysis andfactor analysis to find features that have a largecontribution based on large variance values.
Asa result of principal component analysis, featuresabout fundamental frequency, power, and h1a3have large variance in the first component, and thefeature counting perceptual words also has largevalue in the second component.
To analyze a dif-ferent aspect of principal component analysis withrotated axes, we use factor analysis with the vari-max rotation method.
Figure 1 shows the result offactor analysis indicating that features regardingfundamental frequency and power have large vari-ance.
In addition, other features such as speechrate, a3, and h1a3 also have large variance.
Here,we can see that for features such as statistics offundamental frequency (fsd and fcov) and power(psd and pcov), the correlation coefficient betweenthese features are over 80% (p < 0.01).
For cor-related features, we use only standard deviation inthe following sections.We also analyze important features to distin-guish between children with ASD and TD by us-ing a decision tree.
Figure 2 shows the result of adecision tree with 10 leaves indicating that speechfeatures fill almost all of the leaves (e.g.
fsd is amost useful feature to distinguish between ASDand TD).
In terms of the language features, weconfirm that WPS and perceptual words are im-portant for classification.3.3 ClassificationIn this section, we examine the possibility of au-tomatic identification of whether an utterance be-longs to a speaker with ASD or TD.
Based onthe previous analysis, we prepare the followingFigure 1: Factor analysis with varimax rotationmethod.
First and second factors are indicated.feature sets: 1) language features (Language), 2)speech features (Speech), 3) all features (All), 4)important features according to the t-test, princi-pal component analysis, factor analysis, and de-cision tree (Selected), 5) important features ac-cording to the t-test that are not highly correlated(T-Uncor).
The feature set of T-Uncor is as fol-lows: 6 let., social, affect, cognitive, fillers, as-sent, fed, and h1a3.
We also show the chancerate, which is a baseline of 50% because the num-ber of utterances in each group is the same, andmeasure accuracy with 10-fold cross-validationand leave-one-speaker-out cross-validation usingnaive Bayes (NB) and support vector machineswith a linear kernel (SVM).
In the case of leave-one-speaker-out cross-validation, we use T-Uncorbecause the number of utterances without onespeaker is too small to train using high dimen-sional feature sets.Table 4 shows the result indicating that accu-91racies with almost all feature sets and classifiersare over 65%.
The SVM with Selected achievesthe best performance for the task of 10-fold cross-validation, and The SVM with T-Uncor achieves66.7% for the task of leave-one-speaker-out.
Theaccuracy for the task of leave-one-speaker-out oneach speaker A1 to T2 is as follows: 78%, 60%,53%, 51%, 82%, and 78%.Table 4: Accuracy using Naive Bayes and SVMclassifiers.
The p-value of the t-test is measuredcompared to baseline (chance rate) (?
: p < 0.1, *:p < 0.01)Feature set Accuracy [%]Baseline NB SVMLanguage 62.2?
70.3*Speech 57.6 67.6*All 50.0 65.0?
68.8*Selected 67.4* 71.9*T-Uncor 67.8?
68.1?Per-Speaker 50.0 65.5?
66.7?4 Narrative LevelIn this section, we focus on the features of en-tire narratives, which allows us to examine otherfeatures of child-parent interaction for a better un-derstanding of ASD and classification in childrenwith ASD and TD.
Each following subsection de-scribes the procedure of feature extraction andanalysis of characteristics at the narrative level.We consider pauses before new turns and unex-pected words, which are mentioned in previousworks, as well as words per minute.4.1 Pauses Before New TurnsHeeman et al., (2010) reported that children withASD tend to delay responses to their parent morethan children with TD in natural conversation.
Inthis paper, we examine whether a similar result isfound in interactive narrative.
We denote valuesof pauses before new turns as time between theend of the parent?s utterance and the start of thechild?s utterance.
We do not consider overlap ofutterances.
We test goodness of fit of pauses to agamma and an exponential distribution based on(Theodora et al., 2013), because the later is a spe-cial case of gamma with a unity shape parameter,using the Kolmogorov-Smirnov test.Figure 3 shows a fitting of pauses to gammaor exponential distributions, and we select a bet-2 4 6 8 100.00.10.20.30.40.5Pauses before new turns (sec)Exponential/Gammaprobabilityvalues0.00.10.20.30.40.50.00.10.20.30.40.50.00.10.20.30.40.50.00.10.20.30.40.50.00.10.20.30.40.5TDASDFigure 3: Gamma/Exponential pause distributionswith parameters computed using Maximum Like-lihood Estimation (MLE) for children with ASDand TD.ter fitted distribution.
All subjects significantly fit(p > 0.6).
As shown in Figure 3, we confirm thatchildren with ASD tend to delay responses to theirparent compared with children with TD.
To reflectthis information in our following experiments inautomatic identification of ASD in narrative, weextract the expectation value of the exponentialdistributionHeeman et al., (2010) also reported the rela-tionship of the parent?s previous utterance?s type(question or non-question) and the child?s pauses.We examine the relationship between the parent?sprevious question?s type and pauses before newturns.
For each of the children?s utterances, welabel the parent?s utterance that directly precedesas either ?open question,?
?closed question,?
or?non-question?, and we calculate pause latency.Closed-questions are those which can be answeredby a simple ?yes?
or ?no,?
while open-questionsare those which require more thought and morethan a simple one-word answer.
As shown in Table5, children with ASD tend to delay responses totheir parent to a greater extent than children withTD.
We found no difference between open andclosed questions, although a difference betweenquestions and non-questions is observed.
Theseresults are consistent with those of previous work(Heeman et al., 2010) in terms of differences be-tween questions and non-questions.92|fsd < 0.366375fcov < 0.308899WPS < 0.0543478fcov < 0.204553psd < 0.306304pcov < 0.46429fsd < 0.513756perceptual < 0.07pcov < 0.230634atta atataaFigure 2: Decision tree with 10 leaves (a: ASD, t: TD).Table 5: Relationship of pauses before new turnsand parents?
question types.
The mean value andstandard deviation are shown.Question type TD ASDClosed-question 0.47 (0.46) 1.61 (1.87)Open-question 0.43 (0.34) 1.76 (1.51)Non-question 0.95 (1.18) 2.60 (3.64)4.2 Words Per MinuteWe analyze words per minute (WPM) in childrenwith ASD and TD to clarify the relationship be-tween ASD and frequency of speech.
We use atotal of 5 minutes of data in each narrative, andthus the total number of words are divided by 5 tocalculate WPM.
Table 6 shows the result.
The datain this table indicates that some children with ASDhave a significantly lower speaking rate than oth-ers with TD, but it is not necessarily the case thatASD will result in a low speaking rate such as thecase of Asperger?s syndrome (Asperger, 1944).4.3 Unexpected WordsCharacteristics of ASD include deficits in socialcommunication, and these deficits affect inappro-Table 6: Mean value of words per minute.Subj.
Averaged WPMA1 18.25A2 86.75A3 23.75A4 115.5T1 99.25T2 103.5priate usage of words (Rouhizadeh et al., 2013).We evaluate these unexpected words using twomeasures, term frequency-inverse document fre-quency (TF-IDF) and log odds ratio.
We usethe following formulation to calculate TF-IDF foreach child?s narrative i and each word in that nar-rative j, where cijis the count of word j in narra-tive i. fjis the number of narratives from the fulldata of child narratives containing that word j, andD is the total number of narratives (Rouhizadeh etal., 2013).tf ?
idfij= (1 + log cij) logDfjThe log odds ratio, another measure used in in-93formation retrieval and extraction tasks, is the ratiobetween the odds of a particular word, j, appear-ing in a child?s narrative, i.
Letting the probabil-ity of a word appearing in a narrative be p1andthe probability of that word appearing in all othernarratives be p2, we can express the odds ratio asfollows:odds ratio =odds(p1)odds(p2)=p1/(1?
p1)p2/(1?
p2)A large TF-IDF and log odds score indicatesthat the word j is very specific to the narrativei, which in turn suggests that the word might beunexpected or inappropriate.
In addition, becausethe overall amount of data included in the narra-tives is too small to robustly analyze these statis-tics for all words, we also check for the presenceof each word in Japanese WordNet5and deter-mine that if it exists in WordNet it is likely a com-mon (expected) word.
Table 7 shows the resultof TF-IDF, log odds ratio, and their summation,and we confirm that there is no difference betweenchildren with ASD and TD.
This result is differ-ent from that of previous work (Rouhizadeh et al.,2013).
The children in that study were all tellingthe same story, and one possible explanation forthis is due to the fact that in this work we donot use language-constricted data such as narrativeretelling, and thus differences due to individualityare more prevalent.Table 7: TF-IDF, log odds ratio, and their summa-tion.Subj.
TF-IDF Log-odds T+LA1 0.50 1.01 1.52A2 0.58 0.49 1.08A3 0.66 1.23 1.89A4 0.66 0.31 0.96T1 0.74 0.49 1.23T2 0.62 0.44 1.064.4 ClassificationIn this section, we examine the possibility of auto-matic classification of whether an interactive nar-rative belongs to children with ASD or TD.
Be-cause of the total number of subjects is small (n=4for ASD, n=2 for TD), we perform classification5http://www.omomimi.com/wnjpn/with a K-NN classifier with K=1 nearest neigh-bour.
As features, we compute the features men-tioned in Section 3.1, and use the average over allutterances as the features for the entire narrative.Finally, we use pauses before new turns (expecta-tion value of the exponential distribution), WPM,TF-IDF, log odds ratio, 6 let., social, affect, cogni-tive, assent, fillers, fsd, h1a3, and calculate accu-racy with leave-one-speaker-out cross-validation.As a result, we achieved an accuracy of 100%in classification between ASD and TD on the full-narrative level, which shows that these featuresare effective to some extent to distinguish childrenwith ASD and TD.
However, with only a total of 6children, our sample size is somewhat small, andthus experiments with a larger data set will be nec-essary to draw more firm conclusions.5 Data ComparisonAs all our preceding experiments have been per-formed on data for Japanese child-parent pairs, itis also of interest to compare these results withdata of children and parents from other cultures.In particular, we refer to the USC Rachel corpus(Mower et al., 2011) (the subjects are nine chil-dren with ASD) for comparison.
Using the USCRachel corpus, there is a report mentioning the re-lationship of parent?s and child?s linguistic infor-mation and pauses before new turns (Theodora etal., 2013).
In this paper, we follow this work us-ing Japanese data.
The USC Rachel corpus in-cludes a session of child-parent interaction, andthe same transcription standard is used.
We ex-tract pauses before new turns, and short and longpauses are differentiated based on the 70th per-centile of latency values for each child individu-ally.
We investigate the relationship between theparent and child?s language information based onfeatures used in Section 3.1, and short and longpauses.Table 8 and 9 show significantly greater meanvalues performed using bootstrap significancetesting on the means of the two pause types.
Byobserving the values in the table, we can seethat the trends are similar for both American andJapanese children.
However, in terms of WPS,there is a difference.
The American ASD chil-dren have greater means for WPS in the case oflong pauses, while Japanese children have greatermeans for WPS in the case of short pauses.
Weanalyze these differences in detail.94Table 8: In the case of USC Rachel corpus, boot-strap on difference of means between short (S) andlong (L) pauses based on linguistic features fromchild?s and parent?s utterances (?
: p < 0.1, *: p <0.01).
Each table cell notes which of the two typesof pauses has greater mean on the correspondingfeature.Subj.Child ParentWPS conj.
affect nonflu.
adverb cogn.
percept.S1 L* L* S* - L* L* L*S2 L* L* S?
L* L* L* L*S3 L* L?
- S?
L* L* L*S4 - - - L* L* L* L*S5 L?
- - - L* L* L*S6 L* - S* - L* L* -S7 L?
- S?
- L?
- -S8 L* - - - L* L* L*S9 - - - S?
L* L* L*Table 9: Bootstrap for pause differences in theJapanese corpus.Subj.Child ParentWPS conj.
affect nonflu.
adverb cogn.
percept.A1 S* - - - S* L* -A2 S?
- S* - L* L* L*A3 S?
- - - L* L* L*A4 S* - - - - - -In the Japanese corpus, we observe that WPS islarger in the case of short pauses.
As we noticedthat the child often utters only a single word forresponses that follow a long pause, we analyzedthe content of these single word utterances.
Asshown in Figure 4, for example, A1 tends to usea word related to assent when latency is long, andA4 tends to use a word related to filler, assent orothers when latency is long.
Though there are in-dividual differences, we confirm that the Japanesechildren with ASD examined in this study tendto delay their responses before uttering one word.These characteristics may be related to the parent?squestion types and the child?s cognitive process,and thus we need to examine these possibilities indetail.6 ConclusionIn this work, we focused on differentiation of chil-dren with ASD and TD in terms of social com-munication, particularly focusing on language andspeech features.
Using narrative data, we exam-ined several features on both the single utteranceA1 A2 A3 A4OthersLaughFillerAssentSubjectPercentageofone?wordresponces0.00.20.40.60.81.0Figure 4: The language category of one-word re-sponses in the case of a long pause.level and the narrative level.
We examined fea-tures mentioned in a number of previous works, aswell as a few novel features.
We confirmed about70% accuracy in an evaluation over single utter-ances, and some narrative features also proved tohave a correlation with ASD.For future directions, we plan to perform largerscale experiments to examine the potential of thesefeatures for automated ASD screening.
Given theresults of this, we plan to move to applications in-cluding the development of dialogue systems forautomatic ASD screening and social skills train-ing.AcknowledgmentsWe would like to thank the participants, childrenand their parents, in this study.
We also thankDr.
Hidemi Iwasaka for his advice and support asclinician in pediatrics.
A part of this study wasconducted in Signal Analysis and InterpretationLaboratory (SAIL), University of Southern Cali-fornia.
This study is supported by JSPS KAKEN24240032.ReferencesAmerican Psychiatric Association.
2013.
The Diag-nostic and Statistical Manual of Mental Disorders:DSM 5.Asgari, Meysam, Alireza Bayestehtashk, and IzhakShafran.
2013.
Robust and Accurate Featuers forDetecting and Diagnosing Autism Spectrum Disor-ders.
Proceedings of Interspeech, 191?194.95Asperger, H.. 1944.
Die ,,Autistischen Psychopathen?im Kindesalter.
European Archives of Psychiatryand Clinical Neuroscience, 117: 76?136.Bone, D., Black, M. P., Lee, C. C., Williams, M.E., Levitt, P., Lee, S., and Narayanan, S.. 2012.Spontaneous-Speech Acoustic-Prosodic Features ofChildren with Autism and the Interacting Psycholo-gist.
Proceedings of Interspeech.Chaspari, T., Gibson, D. B., Lee, C.-C., and Narayanan,S.
S. 2013.
Using physiology and language cues formodeling verbal response latencies of children withASD.
Proceedings of ICASSP, 3702?3706.Davis, Megan, Kerstin Dautenhahn, CL Nehaniv, andSD Powell.
2004.
Towards an Interactive Sys-tem Facilitating Therapeutic Narrative Elicitation inAutism.
Proceedings of NILE.Dawson, Geraldine, Deborah Hill, Art Spencer, LarryGalpert, and Linda Watson.. 1990.
Affective ex-changes between young autistic children and theirmothers.
Journal of Abnormal Child Psychology,18: 335?345.de Marchena, A. and Inge-Marie E.. 2010.
Conversa-tional gestures in autism spectrum disorders: asyn-chrony but not decreased frequency.
Autism Re-search, 3: 311?322.Hanson M. H.. 1995.
Glottal characteristics of femalespeakers.
Harvard University, Ph.D. dissertation.Heeman, P. A., Lunsford, R., Selfridge, E., Black, L.,and Van Santen, J.. 2010.
Autism and interactionalaspects of dialogue.
Proceedings of SIGDIAL, 249?252.Kanner, L.. 1943.
Autistic disturbances of affectivecontact.
Nervous Child, 2: 217?250.Kiss, G. and van Santen, J. P. H.. 2013.
EstimatingSpeaker-Specific Intonation Patterns Using the Lin-ear Alignment Model.
Proceedings of Interspeech354?358.Kiss, G., van Santen, J. P. H., Prud?hommeaux, E. T.,and Black, L. M.. 2012.
Quantitative Analysis ofPitch in Speech of Children with Neurodevelopmen-tal Disorders.
Proceedings of Interspeech.Lovaas, O Ivar, Robert Koegel, James Q Simmons, andJudith Stevens Long.
1973.
Some generalisationand follow-up measures on autistic children in be-haviour therapy.
Journal of Applied Behavior Anal-ysis, 6: 131?166.Mairesse, Francois, Marilyn A Walker, Matthias RMehl, and Roger K Moore.
2007.
Using Linguis-tic cues for the automatic recognition of personalityin conversation and text.
Journal of Artificial Intel-ligence Research, 30: 457?500.McCann, J. and Sue, P.. 2003.
Prosody in autismspectrum disorders: a critical review.
InternationalJournal of Language & Communication Disorders,38(4): 325?350.Mower, E., Black, M. P., Flores, E., Williams, M., andNarayanan, S.. 2011.
Rachel: Design of an emo-tionally targeted interactive agent for children withautism.
Proceedings of IEEE ICME, 1?6.Newton, A. T., Kramer, A. D. I., and McIntosh, D. N..2009.
Autism online: a comparison of word usagein bloggers with and without autism spectrum disor-ders.
Proceedings of SIGCHI, 463?466.Paul, Rhea, Amy Augustyn, Ami Klin, and Fred RVolkmar.
2005.
Perception and production ofprosody by speakers with autism spectrum disor-ders.
Journal of Autism and Developmental Disor-ders, 35: 205?220.Pennebaker, James W, Martha E Francis, and Roger JBooth.
2005.
Linguistic inquiry and word count:LIWC [Computer software] Austin, TX: liwc.
net.Rouhizadeh Masoud, Prud?hommeaux Emily, RoarkBrian, and van Santen Jan. 2013.
Distributional se-mantic models for the evaluation of disordered lan-guage.
Proceedings of NAACL-HLT, 709?714.Santen, Jan PH, Richard W Sproat, and Alison Pres-manes Hill.
2013.
Quantifying repetitive speechin autism spectrum disorders and language impair-ment.
Autism Research, 6: 372?383.Sharda, Megha, T Padma Subhadra, Sanchita Sahay,Chetan Nagaraja, Latika Singh, Ramesh Mishra,Amit Sen, Nidhi Singhal, Donna Erickson, and Nan-dini C Singh.
2010.
Sounds of melody?Pitch pat-terns of speech in autism.
Neuroscience letters, 478:42?45.Van Santen, Jan PH, Emily T Prud?hommeaux, LoisM Black, and Margaret Mitchell.
2010.
Compu-tational prosodic markers for autism.
Autism, 14:215?236.Wallace, Charles J, Connie J Nelson, Robert PaulLiberman, Robert A Aitchison, David Lukoff, JohnP Elder, and Chris Ferris.
1980.
A review and cri-tique of social skills training with schizophrenic pa-tients.
Schizophrenia Bulletin, 6:42?63.96
