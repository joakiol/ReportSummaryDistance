Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 133?142,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsPhilosophers are Mortal: Inferring the Truth of Unseen FactsGabor AngeliStanford UniversityStanford, CA 94305angeli@stanford.eduChristopher D. ManningStanford UniversityStanford, CA 94305manning@stanford.eduAbstractLarge databases of facts are prevalent inmany applications.
Such databases areaccurate, but as they broaden their scopethey become increasingly incomplete.
Incontrast to extending such a database, wepresent a system to query whether it con-tains an arbitrary fact.
This work can bethought of as re-casting open domain in-formation extraction: rather than growinga database of known facts, we smooth thisdata into a database in which any possi-ble fact has membership with some confi-dence.
We evaluate our system predictingheld out facts, achieving 74.2% accuracyand outperforming multiple baselines.
Wealso evaluate the system as a common-sense filter for the ReVerb Open IE sys-tem, and as a method for answer validationin a Question Answering task.1 IntroductionDatabases of facts, such as Freebase (Bollackeret al 2008) or Open Information Extraction(Open IE) extractions, are useful for a range ofNLP applications from semantic parsing to infor-mation extraction.
However, as the domain of adatabase grows, it becomes increasingly impracti-cal to collect completely, and increasingly unlikelythat all the elements intended for the database areexplicitly mentioned in the source corpus.
In par-ticular, common-sense facts are rarely explicitlymentioned, despite their abundance.
It would beuseful to infer the truth of such unseen facts ratherthan assuming them to be implicitly false.A growing body of work has focused on auto-matically extending large databases with a finiteset of additional facts.
In contrast, we proposea system to generate the (possibly infinite) com-pletion of such a database, with a degree of con-fidence for each unseen fact.
This task can becast as querying whether an arbitrary element isa member of the database, with an informative de-gree of confidence.
Since often the facts in thesedatabases are devoid of context, we refine our no-tion of truth to reflect whether we would assumea fact to be true without evidence to the contrary.In this vein, we can further refine our task as de-termining whether an arbitrary fact is plausible ?true in the absence contradictory evidence.In addition to general applications of such largedatabases, our approach can further be integratedinto systems which can make use of probabilis-tic membership.
For example, certain machinetranslation errors could be fixed by determiningthat the target translation expresses an implausiblefact.
Similarly, the system can be used as a softfeature for semantic compatibility in coreference;e.g., the types of phenomena expressed in Hobbs?selectional constraints (Hobbs, 1978).
Lastly, it isuseful as a common-sense filter; we evaluate thesystem in this role by filtering implausible factsfrom Open IE extractions, and filtering incorrectresponses for a question answering system.Our approach generalizes word similarity met-rics to a notion of fact similarity, and judges themembership of an unseen fact based on the aggre-gate similarity between it and existing membersof the database.
For instance, if we have not seenthe fact that philosophers are mortal1 but we knowthat Greeks are mortal, and that philosophers andGreeks are similar, we would like to infer that thefact is nonetheless plausible.We implement our approach on both a largeopen-domain database of facts extracted from theOpen IE system ReVerb (Fader et al 2011), andConceptNet (Liu and Singh, 2004), a hand curateddatabase of common sense facts.1This is an unseen fact in http://openie.cs.washington.edu.1332 Related WorkMany NLP applications make use of a knowl-edge base of facts.
These include semantic pars-ing (Zelle and Mooney, 1996; Zettlemoyer andCollins, 2005; Kate et al 2005; Zettlemoyerand Collins, 2007) question answering (Voorhees,2001), information extraction (Hoffmann et al2011; Surdeanu et al 2012), and recognizing tex-tual entailment (Schoenmackers et al 2010; Be-rant et al 2011).A large body of work has been devoted to creat-ing such knowledge bases.
In particular, Open IEsystems such as TextRunner (Yates et al 2007),ReVerb (Fader et al 2011), Ollie (Mausam et al2012), and NELL (Carlson et al 2010) have tack-led the task of compiling an open-domain knowl-edge base.
Similarly, the MIT Media Lab?s Con-ceptNet project (Liu and Singh, 2004) has beenworking on creating a large database of commonsense facts.There have been a number of systems aimed atautomatically extending these databases.
That is,given an existing database, they propose new re-lations to be added.
Snow et al(2006) presentan approach to enriching the WordNet taxonomy;Tandon et al(2011) extend ConceptNet with newfacts; Soderland et al(2010) use ReVerb extrac-tions to enrich a domain-specific ontology.
Wediffer from these approaches in that we aim to pro-vide an exhaustive completion of the database; wewould like to respond to a query with either mem-bership or lack of membership, rather than extend-ing the set of elements which are members.Yao et al(2012) and Riedel et al(2013) presenta similar task of predicting novel relations be-tween Freebase entities by appealing to a large col-lection of Open IE extractions.
Our work focuseson arguments which are not necessarily namedentities, at the expense of leveraging less entity-specific information.Work in classical artificial intelligence has tack-led the related task of loosening the closed worldassumption and monotonicity of logical reason-ing, allowing for modeling of unseen propositions.Reiter (1980) presents an approach to leveragingdefault propositions in the absence of contradic-tory evidence; McCarthy (1980) defines a meansof overriding the truth of a proposition in abnor-mal cases.
Perhaps most similar to this workis Pearl (1989), who proposes approaching non-monotonicity in a probabilistic framework, and inparticular presents a framework for making infer-ences which are not strictly entailed but can bereasonably assumed.
Unlike these works, our ap-proach places a greater emphasis on working withlarge corpora of open-domain predicates.3 ApproachAt a high level, we are provided with a largedatabase of facts which we believe to be true, anda query fact not in the database.
The task is tooutput a judgment on whether the fact is plausible(true unless we have reason to believe otherwise),with an associated confidence.
Although our ap-proach is robust to unary relations, we evaluateonly against binary relations.We decompose this decision into three parts, asillustrated in Figure 1: (i) we find candidate factsthat are similar to our query, (ii) we define a notionof similarity between these facts and our query,and (iii) we define a method for aggregating a col-lection of these similarity values into a single judg-ment.
The first of these parts can be viewed as aninformation retrieval component.
The second partcan be viewed as an extension of word similarityto fact similarity.
The third part is cast as a classifi-cation task, where the input is a set of similar facts,and the decision is the confidence of the query be-ing plausible.We define a fact as a triple of two argumentsand a relation.
We denote a fact in our databaseas f = (a1, r, a2).
A fact which we are queryingis denoted by fq ?
as our focus is on unseen facts,this query is generally not in the database.3.1 Finding Candidate FactsNa?
?vely, when determining the correctness of aquery fact, it would be optimal to compare it tothe entire database of known facts.
However, thisapproach poses significant problems:1.
The computational cost becomes unreason-able with a large database, and only a smallportion of the database is likely to be relevant.2.
The more candidates we consider the moreopportunities we create for false positivesin finding similar facts.
For a sufficientlylarge database, even a small false positive ratecould hurt performance.To address these two problems, we consideronly facts which match the query fact in twoof their three terms.
Formally, we define134database candidates similarity aggregatef1f2.
.
.fn(f1, s1).
.
.
(f1, sn).
.
.
(fn, sn)Figure 1: An overview of our approach.
A large database of facts is queried for candidate entries thatmay be similar to the query fact (see Section 3.1); the similarity of each of these facts to the query fact iscomputed using a number of word similarity metrics (see Section 3.2); finally, these similarity judgmentsare aggregated into a single judgment per metric, and then a single overall judgment (see Section 3.3).functions: cand(fq, fi; a1), cand(fq, fi; r), andcand(fq, fi; a2) for whether the query fq matchesa fact in our database fi on all but one of the argu-ments (or relation).
For efficiency, the total num-ber of candidates returned by each of these threefunctions was limited to 100, creating up to 300similar facts overall.The simplest implementation of this candfunction would be exact match (candexact);however, this is liable to return few re-sults.
As an example, suppose our queryis (private land, be sold to, government).
Wewould like to consider a fact in our database(his land, be sold to, United States) as similar ex-cept for second argument (government versusUnited States), despite the first argument notmatching exactly.
To account for this, we definea class of functions which match the head wordof the two phrases, and as many of the follow-ing stricter criteria as possible while maintainingat least 40 candidate facts:2candhead Match the head word of the twophrases only.
Head words were extracted using theStanford Parser (Klein and Manning, 2003), treat-ing each argument and relation as a sentence.candvn Match all verbs and nouns in thetwo phrases; This prunes candidates such as(land of our ancestors, be sold to, Prussia).Tagging was done with the Stanford Tagger(Toutanova et al 2003).candopen Match the open-class words be-tween the two phrases.
More precisely, itmatches every word which is not a pro-noun, determiner, preposition, or form of the2This threshold is chosen in conjunction with the aggre-gation threshold in Section 3.3, to allow for at least two factsin the 95% threshold.verb be.
This prunes candidates such as(worthless land, be sold to, gullible investors).We proceed to describe our notion of similaritybetween facts, which will be applied to the set ofcandidate similar facts retrieved.3.2 Similarity Between FactsDetermining the similarity between two facts isin general difficult.
For sufficiently complicatedfacts, it can be has hard as recognizing textual en-tailment (RTE); for instance, determining that ev-ery philosopher is mortal and Socrates is mortalare similar requires fairly sophisticated inference.We choose a simple approach, in order to avoid fit-ting to a particular corpus or weakening our abilityto generalize to arbitrary phrases.Our approach casts fact similarity in terms of as-sessing word similarity.
The candidate facts fromSection 3.1 differ from the query fact by a singlephrase; we define the similarity between the can-didate and query fact to be the similarity betweenthe differing term.The word similarity metrics are summarizedin Table 1.
They fall into two broad classes:information-theoretic thesaurus based metrics,and distributional similarity metrics.Thesaurus Based Metrics We adopt many ofthe thesaurus based similarity metrics describedin Budanitsky and Hirst (2006).
For each metric,we use the WordNet ontology (Miller, 1995) com-bined with n-gram counts retrieved from Googlen-grams (Brants and Franz, 2006).
Every wordform was assigned a minimum count of 1; 2265entries had no counts and were assigned this min-imum (1.5%).
167 of these were longer than 5words; the remaining did not appear in the corpus.Since WordNet is a relatively sparse resource,135if a query phrase is not found a number of simplevariants are also tried.
These are, in order of pref-erence: a lemmatized version of the phrase, thehead word of the phrase, and the head lemma ofthe phrase.
If none of these are found, then thenamed entities in the sentence were replaced withtheir types.
If that fails as well, acronyms3 wereexpanded.
For words with multiple sense, themaximum similarity for any pair of word senseswas used.Distributional Similarity Based Metrics Wedefine a number of similarity metrics on the 50dimensional word vectors of Huang et al(2012).These cover a vocabulary of 100,231 words; a spe-cial vector is defined for unknown words.Compound phrases are queried by treating thephrase as a bag of words and averaging the wordvectors of each word in the phrase, pruning outunknown words.
If the phrase contains no knownwords, the same relaxation steps are tried as thethesaurus based metrics.3.3 Aggregating SimilarityAt this stage, we are presented with a set of candi-date facts which may be similar to our query, anda set of similarity judgments for each of these can-didate facts.
Intuitively, we would like to mark afact as plausible if it has enough sufficiently simi-lar candidate facts based on a large number of met-rics.
This is a two-dimensional aggregation task:(i) we aggregate judgments for a single similaritymetric, and (ii) we aggregate these aggregate judg-ments across similarity metrics.
We accomplishthe first half with a thresholded average similarity;the second half we accomplish by using the aggre-gate similarity judgments as features for a logisticregression model.Thresholded Average Similarity Given a setof similarity values, we average the top 5% ofthe values and use this as the aggregate similarityjudgment.
This approach incorporates the benefitof two simpler aggregation techniques: averagingand taking the maximum similarity.Averaging similarity values has the advantageof robustness ?
given a set of candidate facts, wewould like as many of those facts to be as similarto the query as possible.
To illustrate, we shouldbe more certain that (philosophers, are, mortal)36053 acronyms and initialisms were scraped fromhttp://en.wikipedia.org/wiki/List_of_acronyms_and_initialismsName FormulaThesaurusBased Path ?
log len(w1, lcs, w2)Resnik ?
logP (lcs)Lin log(P (lcs)2)log(P (w1)?P (w2))Jiang-Conrath log(P (lcs)2P (w1)?P (w2))?1Wu-Palmer 2?depth(lcs)2?depth(lcs)+len(w1,lcs,w2)Distributional Cosinew1?w2?w1?
?w2?Angle arccos(w1?w2?w1??w2?
))Jensen-Shannon (KL(p1?p2)+KL(p2?p1))2Hellinger 1?2?
?p1 ?
?p2?Jaccard ?min(w1,w2)?1?max(w1,w2)?1Dice ?min(w1,w2)?112?w1+w2?1Table 1: A summary of similarity metrics used tocalculate fact similarity.
For the thesaurus basedmetrics, the two synsets being compared are de-noted by w1 and w2; the lowest common subsumeris denoted as lcs.
For distributional similarity met-rics, the two word vectors are denoted by w1 andw2.
For metrics which require a probability distri-bution, we pass the vectors through a sigmoid toobtain pi = 11+e?wi .if we know both that (Greeks, are, mortal) and(men, are, mortal).
However, since the number ofsimilar facts is likely to be small relative the num-ber of candidate facts considered, this approachhas the risk of losing the signal in the noise of un-informative candidates.
Taking the maximum sim-ilarity judgment alleviates this concern, but con-strains the use of only one element in our aggre-gate judgment.If fewer than 20 candidates are returned, ourcombination approach reduces to taking the max-imum similarity value.
Note also that the 40 factthreshold in the candidate selection phase is cho-sen to provide at least two similarity values to beaveraged together.
The threshold was chosen em-pirically, although varying it does not have a sig-nificant effect on performance.Aggregate Similarity Values At this point, wehave a number of distinct notions of similarity:for each metric, for each differing term, we havea judgment for whether the query fact is similarto the list of candidates.
We combine these using136a simple logistic regression model, treating eachjudgment over different metrics and terms as a fea-ture with weight given by the judgment.
For ex-ample, cosine similarity may judge candidate factsdiffering on their first argument to have a similar-ity of 0.2.
As a result, a feature would be createdwith weight 0.2 for the pair (cosine, argument 1).In addition, features are created which are agnosticto which term differs (e.g., the cosine similarity onwhichever term differs), bringing the total featurecount to 44 for 11 similarity metrics.Lastly, we define 3 auxiliary feature classes:?
Argument Similarity: We define a featurefor the similarity between the two argumentsin the query fact.
Similarity metrics (partic-ularly distributional similarity metrics) oftencapture a notion more akin to relatedness thansimilarity (Budanitsky and Hirst, 2006); thesubject and object of a relation are, in manycases, related in this sense.?
Bias: A single bias feature is included to ac-count for similarity metrics which do not cen-ter on zero.?
No Support Bias: A feature is included forexamples which have no candidate facts inthe knowledge base.4 DataOur approach is implemented using two datasets.The first, described in Section 4.1, is built us-ing facts retrieved from running the University ofWashington?s ReVerb system run over web text.To showcase the system within a cleaner environ-ment, we also build a knowledge base from theMIT Media Lab?s ConceptNet.4.1 ReVerbWe created a knowledge base of facts by runningReVerb over ClueWeb09 (Callan et al 2009).
Ex-tractions rated with a confidence under 0.5 werediscarded; the first billion undiscarded extractionswere used in the final knowledge base.
This re-sulted in approximately 500 million unique facts.Some examples of facts extracted with ReVerbare given in Table 2.
Note that our notion of plau-sibility is far more unclear than in the ConceptNetdata; many facts extracted from the internet are ex-plicitly false, and others are true only in specificcontexts, or are otherwise underspecified.Argument 1 Relation Argument 2cat Desires tuna fishair CapableOf move throughtiny holesneeze HasA allergyperson who IsA not wage-slavesget more sleepTable 3: Example ConceptNet extractions.
Thetop rows correspond to characteristic correct ex-tractions; the bottom rows characterize the typesof noise in the data.4.2 ConceptNetWe also created a dataset using a subset of Con-ceptNet.
ConceptNet is a hand-curated commonsense database, taking information from multi-ple sources (including ReVerb) and consolidatingthem in a consistent format.
We focus on the man-ually created portion of the database, extractedfrom sources such as the Open Mind CommonSense4 (Singh et al 2002).The knowledge base consists of 597,775 facts,each expressing one of 34 relations.
Examples offacts in the ConceptNet database are given in Ta-ble 3.
While the arguments are generally cleanerthan the ReVerb corpus, there are nonetheless in-stances of fairly complex facts.4.3 Training DataOur training data consists of a set of tuples, eachconsisting of a fact f and a database d whichdoes not contain f .
We create artificial negativetraining instances in order to leverage the stan-dard classification framework.
We would like neg-ative examples which are likely to be implausi-ble, but which are close enough to known factsthat we can learn a reasonable boundary for dis-criminating between the two.
To this end, wesample negative instances by modifying a sin-gle argument (or the relation) of a correspond-ing positive training instance.
In more detail: wetake a positive training instance (a1, r, a2) and afact from our database (a?1, r?, a?2), and computethe cosine similarity simcos(a1, a?1), simcos(r, r?
),and simcos(a2, a?2).
Our negative instance will beone of (a?1, r, a2), (a1, r?, a2), or (a1, r, a?2) cor-responding to the entry whose similarity was thelargest.
Negative facts which happen to be in thedatabase are ignored.4http://openmind.media.mit.edu/137Argument 1 Relation Argument 2officials contacted studentsfood riots have recently taken place in many countriesturn left on Front Streetanimals have not been performed to evaluate the carcinogenic potential of adenosineTable 2: Example ReVerb extractions.
The top rows correspond to characteristic correct extractions; thebottom rows shows examples of the types of noise in the data.
Note that in general, both the argumentsand the predicate can be largely unconstrained text.To simulate unseen facts, we construct traininginstances by predicting the plausibility of a factheld out from the database.
That is, if our databaseconsists of d = {f0, f1, .
.
.
fn}we construct train-ing instances (fi, d\{fi}).
Negative examples arelikewise constrained to not occur in the database,as are the facts used in their construction.5 ResultsWe evaluate our system with three experiments.The first, described in Section 5.2, evaluates thesystem?s ability to discriminate plausible factsfrom sampled implausible facts, mirroring thetraining regime.
The second evaluates the systemas a semantic filter for ReVerb extractions, testedagainst human evaluations.
The third uses our sys-tem for validating question answering responses.5.1 BaselinesWe define a number of baselines to compareagainst.
Many of these are subsets of our system,to justify the inclusion of additional complexity.Similar Fact Count This baseline judges thetruth of a fact by tuning a threshold on the totalnumber of similar facts in the database.
This base-line would perform well if our negative facts werenoticeably disconnected from our database.Argument Similarity A key discriminating fea-ture may be the similarity between a1 and a2 intrue versus false facts.
This baseline thresholds thecosine similarity between arguments, tuned on thetraining data to maximize classification accuracy.Cosine Similarity At its core, our model judgesthe truth of a fact based on its similarity to factsin the database; we create a baseline to capturethis intuition.
For every candidate fact (differingin either an argument or the relation), we computethe cosine similarity between the query and thecandidate, evaluated on the differing terms.
ThisSystem ReVerb ConceptNetTrain Test Train Testrandom 50.0 50.0 50.0 50.0count 51.9 52.3 51.0 51.6argsim 52.0 52.6 62.1 60.0cos 71.4 70.6 71.9 70.5system 74.3 74.2 76.5 74.3Table 4: Classification accuracy for ReVerb andConceptNet data.
The three baselines are de-scribed above the line as described in Section 5.1;random chance would get an accuracy of 50%.baseline outputs the maximum similarity betweena query and any candidate; a threshold on this sim-ilarity is tuned on the training data to maximizeclassification accuracy.5.2 Automatic EvaluationA natural way to evaluate our system is to use thesame regime as our training, evaluating on heldout facts.
For both domains we train on a balanceddataset of 20,000 training and 10,000 test exam-ples.
Performance is measured in terms of classi-fication accuracy, with a random baseline of 50%.Table 4 summarizes our results.
The similar factcount baseline performs nearly at random chance,suggesting that our sampled negative facts cannotbe predicted solely on the basis of connectednesswith the rest of the database.
Furthermore, we out-perform the cosine baseline, supporting the intu-ition that aggregating similarity metrics is useful.To evaluate the informativeness of the confi-dence our system produces, we can allow our sys-tem to abstain from unsure judgments.
Recallrefers to the percentage of facts the system choosesto make a guess on; precision is the percentage ofthose facts which are classified correctly.
Fromthis, we can create a precision/recall curve ?
pre-sented in Figure 2 for ReVerb and Figure 3 forConceptNet.
Our system achieves an area under1380.50.60.70.80.910  0.2  0.4  0.6  0.8  1PrecisionRecallsystemcosFigure 2: Accuracy of ReVerb classification, as afunction of the percent of facts answered.
The yaxis begins at random chance (50%).0.50.60.70.80.910  0.2  0.4  0.6  0.8  1PrecisionRecallsystemcosFigure 3: Accuracy of ConceptNet classification,as a function of the percent of facts answered.
They axis begins at random chance (50%).the curve of 0.827 on ConceptNet (compared tothe cosine baseline of 0.751).
For ReVerb, we ob-tain an area of 0.860 (compared to 0.768 for thecosine baseline).55.3 ReVerb FilteringIn order to provide a grounded evaluation metricwe evaluate our system as a confidence estima-tor for ReVerb extractions.
Many ReVerb extrac-tions are semantically implausible, or clash withcommon-sense intuition.
We annotate a numberof extractions on Mechanical Turk, and attempt topredict the extractions?
feasibility.This task is significantly more difficult than theintrinsic evaluations.
Part of the difficulty stems5Curves begin at the recall value given a system confi-dence of 1.0.
For area under the curve calculations, this valueis extended through to recall 0.0.650.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallsystemcosFigure 4: PR curve for ReVerb confidence estima-tion.
The y axis of the graph is truncated at 65% ?this corresponds to the majority class baseline.from our database itself (and therefore our can-didate similar facts) being unfiltered ?
our queryfacts empirically were and therefore in a senseshould be in the database.
Another part stems fromthese facts already having been filtered once byReVerb?s confidence estimator.To collect training and test data, we asked work-ers on Amazon Mechanical Turk to rate facts ascorrect, plausible, or implausible.
They were in-structed that they need not research the facts, andthat correct facts may be underspecified.
Workerswere given the following descriptions of the threepossible responses:?
Correct: You would accept this fact if youread it in a reputable source (e.g., Wikipedia)in an appropriate context.?
Plausible: You would accept this fact if youread it in a storybook.?
Implausible: The fact is either dubious, orotherwise nonsense.Below this, five examples were shown along-side one control (e.g., (rock, float on, water)).Workers who answered more than 20% of the con-trols incorrectly were discarded.
In total, 9 work-ers and 117 of 1200 HITs were discarded.Each example was shown to three separateworkers; a final judgment was made by taking themajority vote between correct (corresponding toour notion of plausibility) and implausible, ignor-ing votes of plausible.
In cases where all the voteswere made for plausible, or there was a tie, theexample was discarded.The experiment was run twice on 2000 ReVerbextractions to collect training and test data.
The139training corpus consists of 1256 positive and 540negative examples (1796 total; 70% positive).
Thetest corpus consists of 1286 positive and 689 neg-ative examples (1975 total; 65% positive)Our system was retrained with the human eval-uated training data; to account for class bias, oursystem?s classification threshold was then tunedon the training data, optimizing for area under theprecision/recall curve.
Figure 4 illustrates our re-sults, bounded below by majority choice.
Our sys-tem achieves an area under the curve of 0.721; thecosine baseline has an area of 0.696.Our system offers a viable trade-off of recall infavor of precision.
For example, keeping only athird of the data can reduce the error rate by 25%?
this can be appealing for large corpora wherefiltering is frequent anyways.5.4 Answer Validation ExerciseThe Answer Validation Exercise, organized as atrack at CLEF between 2006 and 2008, focuses onfiltering candidate answers from question answer-ing systems (Pen?as et al 2007; Pen?as et al 2008;Rodrigo et al 2009).
Systems are presented witha question, and a set of answers along with theirjustification.
The answers are either validated, re-jected, or given an annotation of unknown and ig-nored during scoring.
Since the proportion of cor-rect answers is small (around 10%), the evaluationmeasures precision and recall over true answerspredicted by each system.Many answers in the task are incorrect be-cause they violate common-sense intuition ?
forinstance, one answer to What is leprosy?
wasAfrica clinic.
While any such specific mistake iseasy to fix, our approach can be a means of han-dling a wide range of such mistakes elegantly.To adapt our system to the task, we first heuris-tically converted the question into a query fact us-ing the subject and object Stanford Dependencylabels (de Marneffe and Manning, 2008).
If ei-ther the subject or object specifies a type (e.g.,Which party does Bill Clinton belong to?
), thescore of the fact encoding this relationship (e.g.,(Democrat, be, party)) is averaged with the mainquery.
Next, answers with very little n-gram over-lap between the justification and either the ques-tion or answer are filtered; this filters answerswhich may be correct, but were not properly justi-fied.
Lastly, our system trained on Turk data (seeSection 5.3), predicts an answer to be correct if itSystem 2007 2008P R F1 P R F1all validated 11 100 19 8 100 14filter only 16 95 27 14 100 24median ?
?
35 ?
?
20best ?
?
55 ?
?
64system 31 62 41 16 43 23Table 5: Classification accuracy for the AnswerValidation Exercise task.
The baseline is accept-ing all answers as correct (all validated); a secondbaseline (filter only) incorporates only the n-gramoverlap threshold.
The median and top performingscores for both years are provided for comparison.scores above the 65th percentile of candidate re-sponse scores.
Lastly, as our system has no princi-pled way of handling numbers, any answer whichis entirely numeric is considered invalid.Results are shown in Table 5.
We evaluate onthe 2007 and 2008 datasets, outperforming the me-dian score both years.
Our system would placethird out of the eight systems that competed inboth the 2007 and 2008 tasks.
As we are evaluat-ing our system as a single component not trainedon the task, we understandably fall well underthe top performing systems; however, our perfor-mance is nonetheless an indication that the systemprovides a valuable signal for the task.6 ConclusionWe have created a simple yet effective system todetermine the plausibility of an arbitrary fact, bothin terms of an intrinsic measure, and in down-stream applications.
Furthermore we have shownthat the confidences returned by our system are in-formative, and that high-precision judgments canbe obtained even at reasonable recall.
We hopeto devote future work to enriching the notion offact similarity, and better handling the noise in thetraining data.Acknowledgements We gratefully acknowledge the sup-port of the Defense Advanced Research Projects Agency(DARPA) Deep Exploration and Filtering of Text (DEFT)Program under Air Force Research Laboratory (AFRL) con-tract no.
FA8750-13-2-0040.
Any opinions, findings, andconclusion or recommendations expressed in this material arethose of the authors and do not necessarily reflect the view ofthe DARPA, AFRL, or the US government.140ReferencesJonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of ACL, Portland, OR.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram version 1.
Linguistic Data Consortium.A.
Budanitsky and G. Hirst.
2006.
EvaluatingWordNet-based measures of lexical semantic relat-edness.
Computational Linguistics, pages 13?47.Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.2009.
The ClueWeb09 data set.Andrew Carlson, Justin Betteridge, Bryan Kisiel,Burr Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In AAAI, pages 3?3.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In EMNLP, pages 1535?1545.Jerry R Hobbs.
1978.
Resolving pronoun references.Lingua, pages 311?338.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In ACL-HLT, pages 541?550.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
ACL.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural toformal languages.
In AAAI, pages 1062?1068, Pitts-burgh, PA.Dan Klein and Christopher D Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL, pages 423?430.Hugo Liu and Push Singh.
2004.
ConceptNet: a prac-tical commonsense reasoning toolkit.
BT technologyjournal, pages 211?226.Mausam, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open languagelearning for information extraction.
In EMNLP.John McCarthy.
1980.
Circumscription?a form ofnon-monotonic reasoning.
Artificial intelligence,pages 27?39.George A. Miller.
1995.
WordNet: a lexical databasefor English.
Communications of the ACM, pages39?41.Judea Pearl.
1989.
Probabilistic semantics for non-monotonic reasoning: A survey.
Knowledge Repre-sentation and Reasoning.Anselmo Pen?as, A?lvaro Rodrigo, Valent?
?n Sama, andFelisa Verdejo.
2007.
Overview of the answer vali-dation exercise 2006.
In Evaluation of Multilingualand Multi-modal Information Retrieval, pages 257?264.Anselmo Pen?as, A?lvaro Rodrigo, and Felisa Verdejo.2008.
Overview of the answer validation exercise2007.
In Advances in Multilingual and MultimodalInformation Retrieval, pages 237?248.Raymond Reiter.
1980.
A logic for default reasoning.Artificial intelligence, pages 81?132.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InNAACL-HLT, pages 74?84.A?lvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.2009.
Overview of the answer validation exercise2008.
In Evaluating Systems for Multilingual andMultimodal Information Access, pages 296?313.Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,and Jesse Davis.
2010.
Learning first-order hornclauses from web text.
In EMNLP, pages 1088?1098.Push Singh, Thomas Lin, Erik Mueller, Grace Lim,Travell Perkins, and Wan Li Zhu.
2002.
Openmind common sense: Knowledge acquisition fromthe general public.
On the Move to Meaningful In-ternet Systems 2002: CoopIS, DOA, and ODBASE,pages 1223?1237.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In ACL, pages 801?808.Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,Oren Etzioni, et al2010.
Adapting open infor-mation extraction to domain-specific relations.
AIMagazine, pages 93?102.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-ati, and Christopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.In EMNLP.Niket Tandon, Gerard de Melo, and Gerhard Weikum.2011.
Deriving a web-scale common sense factdatabase.
In AAAI.141Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In NAACL-HLT, pages 173?180.Ellen M Voorhees.
2001.
Question answering inTREC.
In Proceedings of the tenth internationalconference on Information and knowledge manage-ment, pages 535?537.Limin Yao, Sebastian Riedel, and Andrew McCal-lum.
2012.
Probabilistic databases of universalschema.
In Proceedings of the Joint Workshop onAutomatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 116?121.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
TextRunner: Open informationextraction on the web.
In ACL-HLT, pages 25?26.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In AAAI/IAAI, pages 1050?1055,Portland, OR.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI, pages 658?666.
AUAI Press.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In EMNLP-CoNLL, pages 678?687.142
