A New Statistical Parser Based on Bigram Lexical Dependenc iesMichae l  J ohn  Co l l ins*Dept .
of Computer  and In format ion  Sc ienceUn ivers i ty  of Pennsy lvan iaPh i lade lph ia ,  PA,  19104, U.S.A.mcollins@gradient, cis.
upenn, eduAbstractThis paper describes a new statisticalparser which is based on probabilities ofdependencies between head-words in theparse tree.
Standard bigram probability es-timation techniques are extended to calcu-late probabilities of dependencies betweenpairs of words.
Tests using Wall StreetJournal data show that the method per-forms at least as well as SPATTER (Mager-man 95; Jelinek et al 94), which hasthe best published results for a statisticalparser on this task.
The simplicity of theapproach means the model trains on 40,000sentences in under 15 minutes.
With abeam search strategy parsing speed can beimproved to over 200 sentences a minutewith negligible loss in accuracy.1 In t roduct ionLexical information has been shown to be crucial formany parsing decisions, such as prepositional-phraseattachment (for example (Hindle and Rooth 93)).However, early approaches to probabilistic parsing(Pereira and Schabes 92; Magerman and Marcus 91;Briscoe and Carroll 93) conditioned probabilities onnon-terminal labels and part of speech tags alone.The SPATTER parser (Magerman 95; 3elinek et ah94) does use lexical information, and recovers labeledconstituents in Wall Street Journal text with above84% accuracy - as far as we know the best publishedresults on this task.This paper describes a new parser which is muchsimpler than SPATTER, yet performs at least as wellwhen trained and tested on the same Wall StreetJournal data.
The method uses lexical informa-tion directly by modeling head-modifier 1 relationsbetween pairs of words.
In this way it is similar to*This research was supported by ARPA GrantN6600194-C6043.1By 'modifier' we mean the linguistic notion of eitheran argument or adjunct.Link grammars (Lafferty et al 92), and dependencygrammars in general.2 The  Stat is t ica l  Mode lThe aim of a parser is to take a tagged sentenceas input (for example Figure l(a)) and produce aphrase-structure tree as output (Figure l(b)).
Astatistical approach to this problem consists of twocomponents.
First, the statistical model assigns aprobability to every candidate parse tree for a sen-tence.
Formally, given a sentence S and a tree T, themodel estimates the conditional probability P(T\[S).The most likely parse under the model is then:Tb~,, -- argmaxT P(T IS  ) (1)Second, the parser is a method for finding Tbest.This section describes the statistical model, whilesection 3 describes the parser.The key to the statistical model is that any treesuch as Figure l(b) can be represented as a set ofbaseNPs  2 and a set of dependenc ies  as in Fig-ure l(c).
We call the set of baseNPs B, and theset of dependencies D; Figure l(d) shows B and Dfor this example.
For the purposes of our model,T = (B, D), and:P(T IS  ) = P(B ,D\ ]S)  = P(B\[S) x P(D\ ]S ,B)  (2)S is the sentence with words tagged for part ofspeech.
That is, S =< (wl,t l ) ,  (w2,t2)..
.
(w~,t,)  >.For POS tagging we use a maximum-entropy tag-ger described in (Ratnaparkhi 96).
The tagger per-forms at around 97% accuracy on Wall Street Jour-nal Text, and is trained on the first 40,000 sentencesof the Penn Treebank (Marcus et al 93).Given S and B, the reduced sentence  :~ is de-fined as the subsequence of S which is formed byremoving punctuation and reducing all baseNPs totheir head-word alone.~A baseNP or 'minimal' NP is a non-recursive NP,i.e.
none of its child constituents are NPs.
The termwas first used in (l:tamshaw and Marcus 95).184(a)John/NNP Smith/NNP, the/DT president/NN of/IN IBM/NNP, announced/VBD his/PR, P$ res-ignation/NN yesterday/NN .
(b)SNPJ ~NP NPNP PPA AIN NPNNP NNP DT NN I aI I I I \] NNP IJohn Smith the president of IBMVPVBD NP NPPRP$ NN NNI I Iannounced his resignation yesterday(c)\[JohnNP S VP VBDSmith\] \[the president\] of \[ IBM\] announced \[hisVP NPvp NP IIresignation \] \[yesterday \](d)B={ \[John Smith\], \[the president\], \[IBM\], \[his resignation\], \[yesterday\] }NP S VP NP NP NP NPNPPP INPPNP VBD vP NPD=\[ Smith announced, Smith president, president of, of IBM, announced resignationVBD VP NPannounced yesterday }Figure 1: An overview of the representation used by the model.
(a) The tagged sentence; (b) A candidateparse-tree (the correct one); (c) A dependency representation of (b).
Square brackets enclose baseNPs(heads of baseNPs are marked in bold).
Arrows show modifier --* head dependencies.
Section 2.1 describeshow arrows are labeled with non-terminal triples from the parse-tree.
Non-head words within baseNPs areexcluded from the dependency structure; (d) B, the set of baseNPs, and D, the set of dependencies, areextracted from (c).Thus the reduced sentence is an array of word/tagpairs, S=< (t~l,tl),(@2,f2)...(@r~,f,~)>, wherem _~ n. For example for Figure l(a)Example  1 S =< (Smith, ggP), (president, NN), (of, IN),(IBM, NNP), (announced, VBD),(resignation, N N), (yesterday, N g) >Sections 2.1 to 2.4 describe the dependency model.Section 2.5 then describes the baseNP model, whichuses bigram tagging techniques similar to (Ramshawand Marcus 95; Church 88).2.1 The  Mapp ing  f rom Trees to Sets ofDependenc iesThe dependency model is limited to relationshipsbetween words in reduced sentences uch as Ex-ample 1.
The mapping from trees to dependencystructures is central to the dependency model.
It isdefined in two steps:1.
For each constituent P --.< C1...Cn > in theparse tree a simple set of rules 3 identifies whichof the children Ci is the 'head-child' of P. Forexample, NN would be identified as the head-childof NP ~ <DET JJ 33 NN>, VP would be identifiedas the head-child of $ -* <NP VP>.
Head-wordspropagate up through the tree, each parent receiv-ing its head-word from its head-child.
For example,in S --~ </~P VP>, S gets its head-word, announced,3The rules are essentially the same as in (Magerman95; Jelinek et al 94).
These rules are also used to findthe head-word of baseNPs, enabling the mapping fromS and B to S.185from its head-child, the VP.S ( ~ )NP(Sml*h) VP(announcedlIq~smah) NPLmu~=nt)J~(presidmt) PP(of) VBD(annoumzdI NP(fesignatian) NP(yeuaerday)NN T ~P I NN NNSmith l~sid~t of IBM ~mounced rmign~ioe ~ yFigure 2: Parse tree for the reduced sentence inExample 1.
The head-child of each constituent isshown in bold.
The head-word for each constituentis shown in parentheses.2.
Head-modifier relationships are now extractedfrom the tree in Figure 2.
Figure 3 illustrates howeach constituent contributes a set of dependency re-lationships.
VBD is identified as the head-child ofVP ---," <VBD NP NP>.
The head-words of the twoNPs, resignation and yesterday, both modify thehead-word of the VBD, announced.
Dependencies arelabeled by the modifier non-terminal, lip in both ofthese cases, the parent non-terminal, VP, and finallythe head-child non-terminal, VBD.
The triple of non-terminals at the start, middle and end of the arrowspecify the nature of the dependency relationship -<liP,S,VP> represents a subject-verb dependency,<PP ,liP ,liP> denotes prepositional phrase modifi-cation of an liP, and so on 4.v ~7Figure 3: Each constituent with n children (in thiscase n = 3) contributes n - 1 dependencies.Each word in the reduced sentence, with the ex-ception of the sentential head 'announced', modifiesexactly one other word.
We use the notationAF(j) = (hi, Rj) (3)to state that the j th  word in the reduced sentenceis a modifier to the hjth word, with relationshipRj 5.
AF stands for 'arrow from'.
Rj is the tripleof labels at the start, middle and end of the ar-row.
For example, wl = Smith in this sentence,4The triple can also be viewed as representing a se-mantic predicate-argument relationship, with the threeelements being the type of the argument, result and func-tot respectively.
This is particularly apparent in Cat-egorial Grammar formalisms (Wood 93), which makean explicit link between dependencies and functionalapplication.5For the head-word of the entire sentence hj = 0, withRj=<Label of the root of the parse tree >.
So in thiscase, AF(5) = (0, < S >).and ~5 = announced, so AF(1) = (5, <NP,S,VP>).D is now defined as the m-tuple of dependen-cies: n = {(AF(1),AF(2)...AF(m)}.
The modelassumes that the dependencies are independent, sothat:P(DIS, B) = 11 P(AF(j)IS' B) (4)j=l2.2 Ca lcu la t ing  Dependency  Probab i l i t iesThis section describes the way P(AF(j)\]S, B) is es-timated.
The same sentence is very unlikely to ap-pear both in training and test data, so we need toback-offfrom the entire sentence context.
We believethat lexical information is crucial to attachment de-cisions, so it is natural to condition on the words andtags.
Let 1) be the vocabulary of all words seen intraining data, T be the set of all part-of-speech tags,and TTCAZA f be the training set, a set of reducedsentences.
We define the following functions:?
C ( (a, b/, (c, d / ) for a, c c l\], and b, d c 7- is thenumber of times (a,b I and (c,d) are seen in thesame reduced sentence in training data.
6 Formally,C((a,b>, <c,d>)=Z h = <a, b), : <e, d))?
~ ?
T 'R , ,AZ~/"k,Z=l..I;I, z#kwhere h(m) is an indicator function which is 1 if m istrue, 0 if x is false.?
C (R, (a, b), (c, d) ) is the number of times (a, b /and (c, d) are seen in the same reduced sentence intraining data, and {a, b) modifies (c,d) with rela-tionship R. Formally,C (R, <a, b), <e, d) ) =Z h(S\[k\] = (a,b), SIll = (c,d), AF(k) = (l,R))-?
c T'R~gZ2q"k3_-1..1~1, l?:k(6)?
F(RI(a, b), (c, d) ) is the probability that (a, b)modifies (c, d) with relationship R, given that (a, b)and (e, d) appear in the same reduced sentence.
Themaximum-likelihood estimate of F(RI (a, b), (c, d) )is:C(R, (a, b), (c, d) ) (7)fi'(Rl<a ,b), <c,d) = C( (a,b), (c,d) )We can now make the following approximation:P(AF(j) = (hi, Rj) IS, B)P(R  I (S)Ek=l P(P IeNote that we count multiple co-occurrences in asingle sentence, e.g.
if 3=(<a,b>,<c ,d>,<c ,d>)then C(< a,b >,< c,d >) = C(< c,d >,< a,b >) = 2.186where 79 is the set of all triples of non-terminals.
Thedenominator is a normalising factor which ensuresthatE P(AF(j) = (k,p) l S, B) = 1k=l..rn,k~j,pe'PFrom (4) and (8):P(DIS, B) ~ (9)YTThe denominator of (9) is constant, so maximisingP(D\[S, B) over D for fixed S, B is equivalent to max-imising the product of the numerators, Af(DIS, B).
(This considerably simplifies the parsing process):mN(DIS, B) = I-\[ 6), Zh ) ) (10)j= l2.3 The Dis tance MeasureAn estimate based on the identities of the two tokensalone is problematic.
Additional context, in partic-ular the relative order of the two words and the dis-tance between them, will also strongly influence thelikelihood of one word modifying the other.
For ex-ample consider the relationship between 'sales' andthe three tokens of 'of':Example  2 Shaw, based in Dalton, Ga., has an-nual sales of about $1.18 billion, and has economiesof scale and lower raw-material costs that are ex-pected to boost the profitability of  Armstrong'sbrands, sold under the Armstrong and Evans-Blacknames .In this sentence 'sales' and 'of' co-occur threetimes.
The parse tree in training data indicates arelationship in only one of these cases, so this sen-tence would contribute an estimate of ?
that thetwo words are related.
This seems unreasonably lowgiven that 'sales of' is a strong collocation.
The lat-ter two instances of 'of' are so distant from 'sales'that it is unlikely that there will be a dependency.This suggests that distance is a crucial variablewhen deciding whether two words are related.
It isincluded in the model by defining an extra 'distance'variable, A, and extending C, F and /~ to includethis variable.
For example, C( (a, b), (c, d), A) isthe number of times (a, b) and (c, d) appear in thesame sentence at a distance A apart.
(11) is thenmaximised instead of (10):rnAt(DIS, B) = 1-I P(Rj I ((vj, tj), (~hj, \[hj), Aj,ni)j=l(11)A simple example of Aj,hj would be Aj,hj = hj - j .However, other features of a sentence, such as punc-tuation, are also useful when deciding if two wordsare related.
We have developed a heuristic 'dis-tance' measure which takes several such features intoaccount The current distance measure Aj,h~ is thecombination of 6 features, or questions (we motivatethe choice of these questions qualitatively - section 4gives quantitative r sults showing their merit):Quest ion 1 Does the hjth word precede or followthe j th word?
English is a language with strongword order, so the order of the two words in surfacetext will clearly affect their dependency statistics.Quest ion 2 Are the hjth word and the j th wordadjacent?
English is largely right-branching andhead-initial, which leads to a large proportion of de-pendencies being between adjacent words 7.
Table 1shows just how local most dependencies are.Distance 1 < 2 < 5 < 10Percentage 74.2 86.3 95.6 99.0Table 1: Percentage of dependencies v .
distance be-tween the head words involved.
These figures countbaseNPs as a single word, and are taken from WSJtraining data.Number of verbs 0 <=1 <=2Percentage 94.1 98.1 99.3Table 2: Percentage of dependencies v .
number ofverbs between the head words involved.Quest ion 3 Is there a verb between the hjth wordand the j th word?
Conditioning on the exact dis-tance between two words by making Aj,hj = hj - jleads to severe sparse data problems.
But Table 1shows the need to make finer distance distinctionsthan just whether two words are adjacent.
Considerthe prepositions 'to', 'in' and 'of' in the followingsentence:Example  3 Oil stocks escaped the brunt of Fri-day's selling and several were able to post gains ,including Chevron , which rose 5/8 to 66 3//8 inBig Board composite trading of 2.4 million shares.The prepositions' main candidates for attachmentwould appear to be the previous verb, 'rose', andthe baseNP heads between each preposition and thisverb.
They are less likely to modify a more distantverb such as 'escaped'.
Question 3 allows the parserto prefer modification of the most recent verb - effec-tively another, weaker preference for right-branchingstructures.
Table 2 shows that 94% of dependenciesdo not cross a verb, giving empirical evidence thatquestion 3 is useful.ZFor example in '(John (likes (to (go (to (University(of Pennsylvania)))))))' all dependencies are between ad-jacent words.187Quest ions  4, 5 and  6?
Are there 0, 1, 2, or more than 2 'commas' be-tween the hith word and the j th  word?
(Allsymbols tagged as a ',' or ':' are considered tobe 'commas').?
Is there a 'comma' immediately following thefirst of the hjth word and the j th  word??
Is there a 'comma' immediately preceding thesecond of the hjth word and the j th  word?People find that punctuation is extremely usefulfor identifying phrase structure, and the parser de-scribed here also relies on it heavily.
Commas arenot considered to be words or modifiers in the de-pendency model - but they do give strong indica-tions about the parse structure.
Questions 4, 5 and6 allow the parser to use this information.2.4 Sparse  DataThe maximum likelihood estimator in (7) islikely to be plagued by sparse data problems -C( (,.~j, {j), (wa~,{h,), Aj,h i) may be too low to givea reliable estimate, or worse still it may be zero leav-ing the estimate undefined.
(Collins 95) describeshow a backed-off estimation strategy is used for mak-ing prepositional phrase attachment decisions.
Theidea is to back-off to estimates based on less context.In this case, less context means looking at the POStags rather than the specific words.There are four estimates, El, E2, Ea and E4,based respectively on: 1) both words and both tags;2) ~j and the two POS tags; 3) ~hj and the twoPOS tags; 4) the two POS tags alone.E1 =where 861 =62 =6a =64 =7\]2 _7_773 =E2-  ~ Ea= ~ E4= ~- (12) 6a 6~c( (~,/~),  (~.,,/,,, ), as,h~)c( (/-~), <~h~, ~-,,,), ~,~,)C(R~, (~,~~), (/),~), ?~,h~)C(Ro, (~), (~,~.)
,  A~,.,)C(~, (~), ?.j),,~,.~) (13)c( (~,~, ~j), (~-,.j), Aj,,.j ) = ~ C( (~,j, {j), (=, ~-,.~), Aj,,,j )xCVc((~),  <%), %,,,~) = ~ ~ c( <~, ~), (y, ~,,j), A~,,,,)xelJ y~/~where Y is the set of all words seen in training data: theother definitions of C follow similarly.Estimates 2 and 3 compete - for a given pair ofwords in test data both estimates may exist andthey are equally 'specific' to the test case example.
(Collins 95) suggests the following way of combiningthem, which favours the estimate appearing moreoften in training data:E2a - '12 + '~a (14)62 + 63This gives three estimates: El ,  E2a and E4, asimilar situation to trigram language modeling forspeech recognition (Jelinek 90), where there are tri-gram, bigram and unigram estimates.
(Jelinek 90)describes a deleted interpolation method which com-bines these estimates to give a 'smooth' estimate,and the model uses a variation of this idea:I f  E1 exists,  i.e.
61 > 0~(Rj I (~J,~J), (~h~,ih~), A~,h~) :A1 x El + ( i -  At) x E23 (15)Else I f  Eus exists,  i.e.
62 + 63 > 0A2 x E23 + (1 - A2) x E4 (16)Else~'(R~I(~.~,~)), (?hj,t),j),Aj,hj) = E4 (17)(Jelinek 90) describes how to find A valuesin (15) and (16) which maximise the likelihood ofheld-out data.
We have taken a simpler approach,namely:61A1 --61+162 + 6aA2 - (18)62 + 6a + 1These A vMues have the desired property of increas-ing as the denominator f the more 'specific' esti-mator increases.
We think that a proper implemen-tation of deleted interpolation is likely to improveresults, although basing estimates on co-occurrencecounts alone has the advantage of reduced trainingtimes.2.5 The  BaseNP Mode lThe overall model would be simpler if we could dowithout the baseNP model and frame everything interms of dependencies.
However the baseNP modelis needed for two reasons.
First, while adjacency be-tween words is a good indicator of whether thereis some relationship between them, this indicatoris made substantially stronger if baseNPs are re-duced to a single word.
Second, it means thatwords internal to baseNPs are not included in theco-occurrence counts in training data.
Otherwise,188in a phrase like 'The Securities and Exchange Com-mission closed yesterday', pre-modifying nouns like'Securities' and 'Exchange' would be included in co-occurrence counts, when in practice there is no waythat they can modify words outside their baseNP.The baseNP model can be viewed as taggingthe gaps between words with S(tart), C(ontinue),E(nd), B(etween) or N(ull) symbols, respectivelymeaning that the gap is at the start of a BaseNP,continues a BaseNP, is at the end of a BaseNP, isbetween two adjacent baseNPs, or is between twowords which are both not in BaseNPs.
We call thegap before the ith word Gi (a sentence with n wordshas n - 1 gaps).
For example,\[ 3ohn Smith \] \[ the president \] of \[ IBM \] has an-nounced \[ his resignation \]\[ yesterday \]=~John C Smith B the C president E of S IBM E hasN announced S his C resignation B yesterdayThe baseNP model considers the words directly tothe left and right of each gap, and whether there isa comma between the two words (we write ci = 1if there is a comma, ci = 0 otherwise).
Probabilityestimates are based on counts of consecutive pairs ofwords in unreduced training data sentences, wherebaseNP boundaries define whether gaps fall into theS, C, E, B or N categories.
The probability ofa baseNP sequence in an unreduced sentence S isthen:1-I P(G, I ~,,_,,ti_l, wi,t,,c,) (19)i=2 .
.
.nThe estimation method is analogous to that de-scribed in the sparse data section of this paper.
Themethod is similar to that described in (Ramshaw andMarcus 95; Church 88), where baseNP detection isalso framed as a tagging problem.2.6 Summary  of  the  Mode lThe probability of a parse tree T, given a sentenceS, is:P(T\[S) = P(B, DIS) = P(BIS ) x P(D\[S, B)The denominator in Equation (9) is not actu-ally constant for different baseNP sequences, hut wemake this approximation for the sake of efficiencyand simplicity.
In practice this is a good approxima-tion because most baseNP boundaries are very welldefined, so parses which have high enough P(BIS )to be among the highest scoring parses for a sen-tence tend to have identical or very similar baseNPs.Parses are ranked by the following quantityg:P(BIS ) x AZ(DIS, B) (20)Equations (19) and (11) define P(B\]S) andAf(DIS, B).
The parser finds the tree which max-imises (20) subject to the hard constraint hat de-pendencies cannot cross.9in fact we also model the set of unary productions,U, in the tree, which are of the form P -~< Ca >.
Thisintroduces an additional term, P(UIB , S), into (20).2.7 Some Fur ther  Improvements  to  theMode lThis section describes two modifications which im-prove the model's performance.?
In addition to conditioning on whether depen-dencies cross commas, a single constraint concerningpunctuation is introduced.
If for any constituent Zin the chart Z --+ <..  X ?
.
.
> two of its childrenX and ?
are separated by a comma, then the lastword in ?
must be directly followed by a comma, ormust be the last word in the sentence.
In trainingdata 96% of commas follow this rule.
The rule alsohas the benefit of improving efficiency by reducingthe number of constituents in the chart.?
The model we have described thus far takes thesingle best sequence of tags from the tagger, andit is clear that there is potential for better integra-tion of the tagger and parser.
We have tried twomodifications.
First, the current estimation meth-ods treat occurrences of the same word with differ-ent POS tags as effectively distinct types.
Tags canbe ignored when lexical information is available bydefiningC(a,c)= E C((a,b>, (c,d>) (21)b,deTwhere 7" is the set of all tags.
Hence C (a, c) is thenumber of times that the words a and c occur inthe same sentence, ignoring their tags.
The otherdefinitions in (13) are similarly redefined, with POStags only being used when backing off from lexicalinformation.
This makes the parser less sensitive totagging errors.Second, for each word wi the tagger can providethe distribution of tag probabilities P(tiIS) (giventhe previous two words are tagged as in the bestoverall sequence of tags) rather than just the firstbest tag.
The score for a parse in equation (20) thenhas an additional term, 1-\[,'=l P(ti IS), the product ofprobabilities of the tags which it contains.Ideally we would like to integrate POS tagginginto the parsing model rather than treating it as aseparate stage.
This is an area for future research.3 The  Pars ing  A lgor i thmThe parsing algorithm is a simple bottom-up chartparser.
There is no grammar as such, althoughin practice any dependency with a triple of non-terminals which has not been seen in trainingdata will get zero probability.
Thus the parsersearches through the space of all trees with non-terminal triples seen in training data.
Probabilitiesof baseNPs in the chart are calculated using (19),while probabilities for other constituents are derivedfrom the dependencies and baseNPs that they con-tain.
A dynamic programming algorithm is used:if two proposed constituents span the same set ofwords, have the same label, head, and distance from189MODEL ~ 40 Words (2245 sentences) < 100 Words (2416 sentences) s(1) 84.9% 84.9% 1.32 57.2% 80.8% 84.3% 84.3% 1.53 54.7% 77.8%(2) 85.4% 85.5% 1.21 58.4% 82.4% 84.8% 84.8% 1.41 55.9% 79.4%(3) 85.5% 85.7% 1.19 59.5% 82.6% 85.0% 85.1% 1.39 56.8% 7.9.6%(4) 85.8% 86.3% 1.14 59.9% 83.6% 85.3% 85.7% 1.32 57.2% 80.8%SPATTER 84.6% 84.9% 1.26 56.6% 81.4% 84.0% 84.3% 1.46 54.0% 78.8%Table 3: Results on Section 23 of the WSJ Treebank.
(1) is the basic model; (2) is the basic modelwith the punctuation rule described in section 2.7; (3) is model (2) with POS tags ignored when lexicalinformation is present; (4) is model (3) with probability distributions from the POS tagger.
L I : t /LP  =labeled recall/precision.
CBs is the average number of crossing brackets per sentence.
0 CBs,  ~ 2 CBsare the percentage of sentences with 0 or < 2 crossing brackets respectively.VBD NPannounced his resignationScorc=Sl Score=S2vPVBD NPannounced his resignationScore = S1 * $2 *P(Gap--S I announced, his) *P(<np,vp,vbd> I resignation, a nounced)DistanceMeasureYes YesYes NoNo YesLexicalinformationl  LR I LP \] CBs85.0% 85.1% 1.3976.1% 76.6% 2.2680.9% 83.6% 1.51Figure 4: Diagram showing how two constituentsjoin to form a new constituent.
Each operation givestwo new probability terms: one for the baseNP gaptag between the two constituents, and the other forthe dependency between the head words of the twoconstituents.the head to the left and right end of the constituent,then the lower probability constituent can be safelydiscarded.
Figure 4 shows how constituents in thechart combine in a bottom-up manner.4 Resu l tsThe parser was trained on sections 02 - 21 of the WallStreet Journal portion of the Penn Treebank (Mar-cus et al 93) (approximately 40,000 sentences), andtested on section 23 (2,416 sentences).
For compari-son SPATTER (Magerman 95; Jelinek et al 94) wasalso tested on section 23.
We use the PARSEVALmeasures (Black et al 91) to compare performance:Labe led  Prec is ion  --number of correct constituents inproposed parsenumber of constituents in proposed parseLabe led  Recal l  =number of correct constituents in proposed parsenumber of constituents in treebank parseCross ing Brackets  = numberof constituents which violate constituent bound-aries with a constituent in the treebank parse.For a constituent to be 'correct' it must span thesame set of words (ignoring punctuation, i.e.
all to-kens tagged as commas, colons or quotes) and havethe same label ?
as a constituent in the treebank1?SPATTER collapses ADVP and PRT to the same label,for comparison we also removed this distinction whenTable 4: The contribution of various components ofthe model.
The results are for all sentences of < 100words in section 23 using model (3).
For 'no lexi-cal information' all estimates are based on POS tagsalone.
For 'no distance measure' the distance mea-sure is Question 1 alone (i.e.
whether zbj precedesor follows ~hj).parse.
Four configurations of the parser were tested:(1) The basic model; (2) The basic model with thepunctuation rule described in section 2.7; (3) Model(2) with tags ignored when lexical information ispresent, as described in 2.7; and (4) Model (3) alsousing the full probability distributions for POS tags.We should emphasise that test data outside of sec-tion 23 was used for all development of the model,avoiding the danger of implicit training on section23.
Table 3 shows the results of the tests.
Table 4shows results which indicate how different parts ofthe system contribute to performance.4.1 Per fo rmance  IssuesAll tests were made on a Sun SPARCServer 1000E,using 100% of a 60Mhz SuperSPARC processor.
Theparser uses around 180 megabytes of memory, andtraining on 40,000 sentences (essentially extractingthe co-occurrence counts from the corpus) takes un-der 15 minutes.
Loading the hash table of bigramcounts into memory takes approximately 8 minutes.Two strategies are employed to improve parsingefficiency.
First, a constant probability threshold isused while building the chart - any constituents withlower probability than this threshold are discarded.If a parse is found, it must be the highest rankedparse by the model (as all constituents discardedhave lower probabilities than this parse and could190calculating scores.not, therefore, be part of a higher probability parse).If no parse is found, the threshold is lowered andparsing is attempted again.
The process continuesuntil a parse is found.Second, a beam search strategy is used.
For eachspan of words in the sentence the probability, Ph, ofthe highest probability constituent is recorded.
Allother constituents spanning the same words musthave probability greater than ~-~ for some constantbeam size /3 - constituents which fall out of thisbeam are discarded.
The method risks introduc-ing search-errors, but in practice efficiency can begreatly improved with virtually no loss of accuracy.Table 5 shows the trade-off between speed and ac-curacy as the beam is narrowed.I Beam \[ Speed\[ Sizefl ~ Sentences/minute118166217261283289Table 5: The trade-off between speed and accuracyas the beam-size is varied.
Model (3) was used forthis test on all sentences < 100 words in section 23.5 Conc lus ions  and  Future  WorkWe have shown that a simple statistical modelbased on dependencies between words can parseWall Street Journal news text with high accuracy.The method is equally applicable to tree or depen-dency representations of syntactic structures.There are many possibilities for improvement,which is encouraging.
More sophisticated stimationtechniques such as deleted interpolation should betried.
Estimates based on relaxing the distance mea-sure could also be used for smoothing- at present weonly back-off on words.
The distance measure couldbe extended to capture more context, such as otherwords or tags in the sentence.
Finally, the modelmakes no account of valency.AcknowledgementsI would like to thank Mitch Marcus, Jason Eisner,Dan Melamed and Adwait Ratnaparkhi for manyuseful discussions, and for comments on earlier ver-sions of this paper.
I would also like to thank DavidMagerman for his help with testing SPATTER.ReferencesE.
Black et al 1991.
A Procedure for Quantita-tively Comparing the Syntactic Coverage of En-glish Grammars.
Proceedings of the February 1991DARPA Speech and Natural Language Workshop.T.
Briscoe and J. Carroll.
1993.
GeneralizedLR Parsing of Natural Language (Corpora)with Unification-Based Grammars.
Computa-tional Linguistics, 19(1):25-60.K.
Church.
1988.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
SecondConference on Applied Natural Language Process-ing, A CL.M.
Collins and J. Brooks.
1995.
Prepositional PhraseAttachment through aBacked-off Model.
Proceed-ings of the Third Workshop on Very Large Cor-pora, pages 27-38.D.
Hindle and M. Rooth.
1993.
Structural Ambigu-ity and Lexical Relations.
Computational Linguis-tics, 19(1):103-120.F.
Jelinek.
1990.
Self-organized Language Model-ing for Speech Recognition.
In Readings in SpeechRecognition.
Edited by Waibel and Lee.
MorganKaufmann Publishers.F.
Jelinek, J. Lafferty, D. Magerman, R. Mercer, A.Ratnaparkhi, S.Roukos.
1994.
Decision Tree Pars-ing using a Hidden Derivation Model.
Proceedingsof the 1994 Human Language Technology Work-shop, pages 272-277.J.
Lafferty, D. Sleator and, D. Temperley.
1992.Grammatical Trigrams: A Probabilistic Model ofLink Grammar.
Proceedings of the 1992 AAAIFall Symposium on Probabilistic Approaches toNatural Language.D.
Magerman.
1995.
Statistical Decision-Tree Mod-els for Parsing.
Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, pages 276-283.D.
Magerman and M. Marcus.
1991.
Pearl: A Prob-abilistic Chart Parser.
Proceedings of the 1991 Eu-ropean A CL Conference, Berlin, Germany.M.
Marcus, B. Santorini and M. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: the Penn Treebank.
Computational Linguis-tics, 19(2):313-330.F.
Pereira and Y. Schabes.
1992.
Inside-OutsideReestimation from Partially Bracketed Corpora.Proceedings of the 30th Annual Meeting of theAssociation for Computational Linguistics, pages128-135.L.
Ramshaw and M. Marcus.
1995.
Text Chunk-ing using Transformation-Based Learning.
Pro-ceedings of the Third Workshop on Very LargeCorpora, pages 82-94.A.
Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-Of-Speech Tagging.
Conference on Em-pirical Methods in Natural Language Processing,May 1996.M.
M. Wood.
1993.
Categorial Grammars, Rout-ledge.191
