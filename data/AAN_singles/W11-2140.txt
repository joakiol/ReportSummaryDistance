Proceedings of the 6th Workshop on Statistical Machine Translation, pages 344?350,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsNoisy SMS Machine Translation in Low-Density LanguagesVladimir Eidelman?, Kristy Hollingshead?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing?Department of LinguisticsUniversity of Maryland, College Park{vlad,hollingk,resnik}@umiacs.umd.eduAbstractThis paper presents the system we developedfor the 2011 WMT Haitian Creole?EnglishSMS featured translation task.
Applying stan-dard statistical machine translation methods tonoisy real-world SMS data in a low-densitylanguage setting such as Haitian Creole posesa unique set of challenges, which we attemptto address in this work.
Along with techniquesto better exploit the limited available train-ing data, we explore the benefits of severalmethods for alleviating the additional noiseinherent in the SMS and transforming it tobetter suite the assumptions of our hierarchi-cal phrase-based model system.
We showthat these methods lead to significant improve-ments in BLEU score over the baseline.1 IntroductionFor the featured translation task of the Sixth Work-shop on Statistical Machine Translation, we devel-oped a system for translating Haitian Creole Emer-gency SMS messages.
Given the nature of the task,translating text messages that were sent during theJanuary 2010 earthquake in Haiti to an emergencyresponse service called Mission 4636, we were notonly faced with the problem of dealing with a low-density language, but additionally, with noisy, real-world data in a domain which has thus far receivedrelatively little attention in statistical machine trans-lation.
We were especially interested in this task be-cause of the unique set of challenges that it posesfor existing translation systems.
We focused our re-search effort on techniques to better utilize the lim-ited available training resources, as well as ways inwhich we could automatically alleviate and trans-form the noisy data to our advantage through theuse of automatic punctuation prediction, finite-stateraw-to-clean transduction, and grammar extraction.All these techniques contributed to improving trans-lation quality as measured by BLEU score over ourbaseline system.The rest of this paper is structured as follows.First, we provide a brief overview of our baselinesystem in Section 2, followed by an examination ofissues posed by this task and the steps we have takento address them in Section 3, and finally we con-clude with experimental results and additional anal-ysis.2 System OverviewOur baseline system is based on a hierarchicalphrase-based translation model, which can formallybe described as a synchronous context-free gram-mar (SCFG) (Chiang, 2007).
Our system is imple-mented in cdec, an open source framework for align-ing, training, and decoding with a number of differ-ent translation models, including SCFGs.
(Dyer etal., 2010).
1 SCFG grammars contain pairs of CFGrules with aligned nonterminals, where by introduc-ing these nonterminals into the grammar, such a sys-tem is able to utilize both word and phrase level re-ordering to capture the hierarchical structure of lan-guage.
SCFG translation models have been shownto produce state-of-the-art translation for most lan-guage pairs, as they are capable of both exploit-ing lexical information for and efficiently comput-ing all possible reorderings using a CKY-based de-coder (Dyer et al, 2009).1http://cdec-decoder.org344One benefit of cdec is the flexibility allowed withregard to the input format, as it expects either astring, lattice, or context-free forest, and subse-quently generates a hypergraph representing the fulltranslation forest without any pruning.
This forestcan now be rescored, by intersecting it with a lan-guage model for instance, to obtain output transla-tions.
These capabilities of cdec allow us to performthe experiments described below, which may haveotherwise proven to be quite impractical to carry outin another system.The set of features used in our model were therule translation relative frequency P (e|f), a targetn-gram language model P (e), lexical translationprobabilities Plex(e|f) and Plex(f |e), a count of thetotal number of rules used, a target word penalty,and a count of the number of times the glue ruleis used.
The number of non-terminals allowed ina synchronous grammar rule was restricted to two,and the non-terminal span limit was 12 for non-gluegrammars.
The hierarchical phrase-based transla-tion grammar was extracted using a suffix array ruleextractor (Lopez, 2007).To optimize the feature weights for our model, weused an implementation of the hypergraph minimumerror rate training (MERT) algorithm (Dyer et al,2010; Och, 2003) for training with an arbitrary lossfunction.
The error function we used was BLEU (Pa-pineni et al, 2002), and the decoder was configuredto use cube pruning (Huang and Chiang, 2007) witha limit of 100 candidates at each node.2.1 Data PreparationThe SMS messages were originally translated byEnglish speaking volunteers for the purpose of pro-viding first responders with information and loca-tions requiring their assistance.
As such, in order tocreate a suitable parallel training corpus from whichto extract a translation grammar, a number of stepshad to be taken in addition to lowercasing and tok-enizing both sides of training data.
Many of the En-glish translations had additional notes sections thatwere added by the translator to the messages witheither personal notes or further informative remarks.As these sections do not correspond to any text onthe source side, and would therefore degrade thealignment process, these had to be identified and re-moved.
Furthermore, the anonymization of the dataresulted in tokens such as firstname and phonenum-ber which were prevalent and had to be preservedas they were.
Since the total amount of Haitian-English parallel data provided is quite limited, wefound additional data and augmented the availableset with data gathered by the CrisisCommons groupand made it available to other WMT participants.The combined training corpus from which we ex-tracted our grammar consisted of 123,609 sentencepairs, which was then filtered for length and alignedusing the GIZA++ implementation of IBM Model4 (Och and Ney, 2003) to obtain one-to-many align-ments in either direction and symmetrized using thegrow-diag-final-and method (Koehn et al, 2003).We trained a 5-gram language model using theSRI language modeling toolkit (Stolcke, 2002) fromthe English monolingual News Commentary andNews Crawl language modeling training data pro-vided for the shared task and the English portion ofthe parallel data with modified Kneser-Ney smooth-ing (Chen and Goodman, 1996).
We have previ-ously found that since the beginnings and ends ofsentences often display unique characteristics thatare not easily captured within the context of themodel, explicitly annotating beginning and end ofsentence markers as part of our translation processleads to significantly improved performance (Dyeret al, 2009).A further difficulty of the task stems from the factthat there are two versions of the SMS test set, a rawversion, which contains the original messages, and aclean version which was post-edited by humans.
Asthe evaluation of the task will consist of translatingthese two versions of the test set, our baseline sys-tem consisted of two systems, one built on the cleandata using the 900 sentences in SMS dev clean totune our feature weights, and evaluated using SMSdevtest clean, and one built analogously for the rawdata tuned on the 900 sentences in SMS dev raw andevaluated on SMS devtest raw.
We report results onthese sets as well as the 1274 sentences in the SMStest set.3 Experimental VariationThe results produced by the baseline systems arepresented in Table 1.
As can be seen, the clean ver-sion performs on par with the French-English trans-345BASELINEVersion Set BLEU TERcleandev 30.36 56.04devtest 28.15 57.45test 27.97 59.19rawdev 25.62 63.27devtest 24.09 63.82test 23.33 65.93Table 1: Baseline system BLEU and TER scoreslation quality in the 2011 WMT shared translationtask,2 and significantly outperforms the raw version,despite the content of the messages being identical.This serves to underscore the importance of properpost-processing of the raw data in order to attempt toclose the performance gap between the two versions.Through analysis of the raw and clean data we iden-tified several factors which we believe greatly con-tribute to the difference in translation output.
Weexamine punctuation in Section 3.2, grammar post-processing in Section 3.3, and morphological differ-ences in Sections 3.4 and 3.5.3.1 Automatic Resource Confidence WeightingA practical technique when working with a low-density language with limited resources is to du-plicate the same trusted resource multiple times inthe parallel training corpus in order for the transla-tion probabilities of the duplicated items to be aug-mented.
For instance, if we have confidence in theentries of the glossary and dictionary, we can dupli-cate them 10 times in our training data to increasethe associated probabilities.
The aim of this strat-egy is to take advantage of the limited resources andexploit the reliable ones.However, what happens if some resources aremore reliable than others?
Looking at the providedresources, we saw that in the Haitisurf dictionary,the entry for paske is matched with for, while inglossary-all-fix, paske is matched with because.
Ifwe then consider the training data, we see that inmost cases, paske is in fact translated as because.Motivated by this type of phenomenon, we em-ployed an alternative strategy to simple duplicationwhich allows us to further exploit our prior knowl-edge.2http://matrix.statmt.org/matrixFirst, we take the previously word-aligned base-line training corpus and for each sentence pair andword ei compute the alignment link count c(ei, fj)over the positions j that ei is aligned with, repeatingfor c(fi, ej) in the other direction.
Then, we pro-cess each resource we are considering duplicating,and augment its score by c(ei, fj) for every pair ofwords which was observed in the training data andis present in the resource.
This score is then normal-ized by the size of the resource, and averaged overboth directions.
The outcome of this process is ascore for each resource.
Taking these scores on alog scale and pinning the top score to associate with20 duplications, the result is a decreasing number ofduplications for each subsequent resources, based onour confidence in its entries.
Thus, every entry in theresource receives credit, as long as there is evidencethat the entries we have observed are reliable.
Onour set of resources, the process produces a score of17 for the Haitisurf dictionary and 183 for the glos-sary, which is in line with what we would expect.It may be that the resources may have entries whichoccur in the test set but not in the training data, andthus we may inadvertently skew our distribution ina way which negatively impacts our performance,however, overall we believe it is a sound assumptionthat we should bias ourselves toward the more com-mon occurrences based on the training data, as thisshould provide us with a higher translation probabil-ity from the good resources since the entries are re-peated more often.
Once we obtain a proper weight-ing scheme for the resources, we construct a newtraining corpus, and proceed forward from the align-ment process.Table 2 presents the BLEU and TER results of thestandard strategy of duplication against the confi-dence weighting scheme outlined above.
As can beCONF.
WT.
X10Version Set BLEU TER BLEU TERcleandev 30.79 55.71 30.61 55.31devtest 27.92 57.66 28.22 57.06test 27.97 59.65 27.74 59.34rawdev 26.11 62.64 25.72 62.99devtest 24.16 63.71 24.18 63.71test 23.66 65.69 23.06 66.78Table 2: Confidence weighting versus x10 duplication346seen, the confidence weighting scheme substantiallyoutperforms the duplication for the dev set of bothversions, but these improvements do not carry overto the clean devtest set.
Therefore, for the rest of theexperiments presented in the paper, we will use theconfidence weighting scheme for the raw version,and the standard duplication for the clean version.3.2 Automatic Punctuation PredictionPunctuation does not usually cause a problem intext-based machine translation, but this changeswhen venturing into the domain of SMS.
Punctua-tion is very informative to the translation process,providing essential contextual information, muchas the aforementioned sentence boundary markers.When this information is lacking, mistakes whichwould have otherwise been avoided can be made.Examining the data, we see there is substantiallymore punctuation in the clean set than in the raw.For example, there are 50% more comma?s in theclean dev set than in the raw.
A problem of lack ofpunctuation has been studied in the context of spo-ken language translation, where punctuation predic-tion on the source language prior to translation hasbeen shown to improve performance (Dyer, 2007).We take an analogous approach here, and train a hid-den 5-gram model using SRILM on the punctuatedportion of the Haitian side of the parallel data.
Wethen applied the model to punctuate the raw dev set,and tuned a system on this punctuated set.
How-ever, the translation performance did not improve.This may have been do to several factors, includingthe limited size of the training set, and the lack ofin-domain punctuated training data.
Thus, we ap-plied a self-training approach.
We applied the punc-tuation model to the SMS training data, which isonly available in the raw format.
Once punctuated,we re-trained our punctuation prediction model, nowincluding the automatically punctuated SMS dataAUTO-PUNCVersion Set BLEU TERrawdev 26.09 62.84devtest 24.38 64.26test 23.59 65.91Table 3: Automatic punctuation prediction resultsas part of the punctuation language model trainingdata.
We use this second punctuation predictionmodel to predict punctuation for the tuning and eval-uation sets.
We continue by creating a new paralleltraining corpus which substitutes the original SMStraining data with the punctuated version, and builda new translation system from it.
The results fromusing the self-trained punctuation method are pre-sented in Table 3.
Future experiments on the rawversion are performed using this punctuation.3.3 Grammar FilteringAlthough the grammars of a SCFG model per-mit high-quality translation, the grammar extractionprocedure extracts many rules which are formally li-censed by the model, but are otherwise incapable ofhelping us produce a good translation.
For example,in this task we know that the token firstname must al-ways translate as firstname, and never as phonenum-ber.
This refreshing lack of ambiguity allows us tofilter the grammar after extracting it from the train-ing corpus, removing any grammar rule where theseconditions are not met, prior to decoding.
Filteringremoved approximately 5% of the grammar rules.3Table 4 shows the results of applying grammar fil-tering to the raw and clean version.GRAMMARVersion Set BLEU TERcleandev 30.88 54.53devtest 28.69 56.21test 28.29 58.78rawdev 26.41 62.47devtest 24.47 63.26test 23.96 65.82Table 4: Results of filtering the grammar in a post-processing step before decoding3.4 Raw-Clean Segmentation LatticeAs noted above, a major cause of the performancedegradation from the clean to the raw version is re-lated to the morphological errors in the messages.Figure 1 presents a segmentation lattice with twoversions of the same sentence; the first being from3We experimented with more aggressive filtering basedon punctuation and numbers, but translation quality degradedrapidly.347the raw version, and the second from the clean.
Wecan see that that Ilavach has been broken into twosegments, while ki sou has been combined into one.Since we do not necessarily know in advancewhich segmentation is the correct one for a betterquality translation, it may be of use to be able toutilize both segmentations and allow the decoder tolearn the appropriate one.
In previous work, wordsegmentation lattices have been used to address theproblem of productive compounding in morphologi-cally rich languages, such as German, where mor-phemes are combined to make words but the or-thography does not delineate the morpheme bound-aries.
These lattices encode alternative ways of seg-menting compound words, and allow the decoderto automatically choose which segmentation is bestfor translation, leading to significantly improved re-sults (Dyer, 2009).
As opposed to building wordsegmentation lattices from a linguistic morphologi-cal analysis of a compound word, we propose to uti-lize the lattice to encode all alternative ways of seg-menting a word as presented to us in either the cleanor raw versions of a sentence.
As the task requiresus to produce separate clean and raw output on thetest set, we tune one system on a lattice built fromthe clean and raw dev set, and use the single systemto decode both the clean and raw test set separately.Table 5 presents the results of using segmentationlattices.3.5 Raw-to-Clean Transformation LatticeAs can be seen in Tables 1, 2, and 3, system per-formance on clean text greatly outperforms systemperformance on raw text, with a difference of almost5 BLEU points.
Thus, we explored the possibility ofautomatically transforming raw text into clean text,based on the ?parallel?
raw and clean texts that wereprovided as part of the task.One standard approach might have been to trainSEG-LATTICEVersion Set BLEU TERrawdev 26.17 61.88devtest 24.64 62.53test 23.89 65.27Table 5: Raw-Clean segmentation lattice tuning resultsFST-LATTICEVersion Set BLEU TERrawdev 26.20 62.15devtest 24.21 63.45test 22.56 67.79Table 6: Raw-to-clean transformation lattice resultsa Haitian-to-Haitian MT system to ?translate?
fromraw text to clean text.
However, since the training setwas only available as raw text, and only the dev anddevtest datasets had been cleaned, we clearly did nothave enough data to train a raw-to-clean translationsystem.
Thus, we created a finite-state transducer(FST) by aligning the raw dev text to the clean devtext, on a sentence-by-sentence basis.
These raw-to-clean alignments were created using a simple mini-mum edit distance algorithm; substitution costs werecalculated according to orthographic match.One option would be to use the resulting raw-to-clean transducer to greedily replace each word (orphrase) in the raw input with the predicted transfor-mation into clean text.
However, such a destructivereplacement method could easily introduce cascad-ing errors by removing text that might have beentranslated correctly.
Fortunately, as mentioned inSection 2, and utilized in the previous section, thecdec decoder accepts lattices as input.
Rather thanreplacing raw text with the predicted transformationinto ?clean?
text, we add a path to the input lat-tice for each possible transform, for each word andphrase in the input.
We tune a system on a latticebuilt from this approach on the dev set, and use theFST developed from the dev set in order to createlattices for decoding the devtest and test sets.
Anexample is shown in Figure 3.4.
Note that in thisexample, the transformation technique correctly in-serted new paths for ilavach and ki sou, correctlyretained the single path for zile, but overgeneratedmany (incorrect) options for nan.
Note, though, thatthe original path for nan remains in the lattice, de-laying the ambiguity resolution until later in the de-coding process.
Results from creating raw-to-cleantransformation lattices are presented in Table 6.By comparing the results in Table 6 to those inTable 5, we can see that the noise introduced by thefinite-state transformation process outweighed the3481 2Eske?ske 3nou ap kite nou mouri nan zile4ila5Ilavachvach6la 7kisou9ki 8okay 10lasouFigure 1: Partial segmentation lattice combining the raw and clean versions of the sentence:Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes.1516ampatel17annanlannan18nanak19zile20tant21ila22ilavachnan vach23e24salalanan 25ki26kisousouFigure 2: Partial input lattice for sentence in Figure 3.4, generated using the raw-to-clean transform techniquedescribed in Section 3.5.gains of adding new phrases for tuning.4 System ComparisonTable 7 shows the performance on the devtest setof each of the system variations that we have pre-sented in this paper.
From this table, we can seethat our best-performing system on clean data wasthe GRAMMAR system, where the training data wasmultiplied by ten as described in Section 3.1, thenthe grammar was filtered as described in Section 3.3.Our performance on clean test data, using this sys-tem, was 28.29 BLEU and 58.78 TER.
Table 7 alsodemonstrates that our best-performing system onraw data was the SEG-LATTICE system, where thetraining data was confidence-weighted (Section 3.1),the grammar was filtered (Section 3.3), punctuationwas automatically added to the raw data as describedin Section 3.2, and the system was tuned on a latticecreated from the raw and clean dev dataset.
Our per-formance on raw test data, using this system, was23.89 BLEU and 65.27 TER.5 ConclusionIn this paper we presented our system for the 2011WMT featured Haitian Creole?English translationtask.
In order to improve translation quality of low-density noisy SMS data, we experimented with anumber of methods that improve performance onboth the clean and raw versions of the data, and helpclean rawSystem BLEU TER BLEU TERBASELINE 28.15 57.45 24.09 63.82CONF.
WT.
27.92 57.66 24.16 63.71X10 28.22 57.06 24.18 63.71GRAMMAR 28.69 56.21 24.47 63.26AUTO-PUNC ?
?
24.38 64.26SEG-LATTICE ?
?
24.64 62.53FST-LATTICE ?
?
24.21 63.45Table 7: Comparison of all systems?
performance ondevtest setclose the gap between the post-edited and real-worlddata according to BLEU and TER evaluation.
Themethods employed were developed to specificallyaddress shortcomings we observed in the data, suchas segmentation lattices for morphological ambigu-ity, confidence weighting for resource utilization,and punctuation prediction for lack thereof.
Overall,this work emphasizes the feasibility of adapting ex-isting translation technology to as-yet underexploreddomains, as well as the shortcomings that need to beaddressed in future work in real-world data.6 AcknowledgmentsThe authors gratefully acknowledge partial supportfrom the DARPA GALE program, No.
HR0011-06-2-001.
In addition, the first author was supported bythe NDSEG Fellowship.
Any opinions or findingsdo not necessarily reflect the view of the sponsors.349ReferencesStanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th Annual Meeting ofthe Association for Computational Linguistics, pages310?318.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
In Computational Linguistics, volume 33(2),pages 201?228.Chris Dyer, Hendra Setiawan, Yuval Marton, and PhilipResnik.
2009.
The University of Maryland statisticalmachine translation system for the Fourth Workshopon Machine Translation.
In Proceedings of the EACL-2009 Workshop on Statistical Machine Translation.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of ACL System Demonstrations.Chris Dyer.
2007.
The University of Maryland Trans-lation system for IWSLT 2007.
In Proceedings ofIWSLT.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In Proceedings ofNAACL-HLT.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 144?151.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP,pages 976?985.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
InComputational Linguistics, volume 29(21), pages 19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Intl.
Conf.
on Spoken LanguageProcessing.350
