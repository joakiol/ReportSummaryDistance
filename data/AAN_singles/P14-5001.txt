Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6,Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational LinguisticsCross-Lingual Information to the Rescue in Keyword Extraction1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang1,2,3Language Technologies Institute, CMU, United States4Institute of Information Science, Academia Sinica, Taiwan5Institute for Information Industry, Taipei, Taiwan{1u901571,4lunwei.jennifer.ku}@gmail.com{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.twAbstractWe introduce a method that extracts keywordsin a language with the help of the other.
In ourapproach, we bridge and fuse conventionallyirrelevant word statistics in languages.
Themethod involves estimating preferences forkeywords w.r.t.
domain topics and generatingcross-lingual bridges for word statisticsintegration.
At run-time, we transform parallelarticles into word graphs, build cross-lingualedges, and exploit PageRank with wordkeyness information for keyword extraction.We present the system, BiKEA, that appliesthe method to keyword analysis.
Experimentsshow that keyword extraction benefits fromPageRank, globally learned keywordpreferences, and cross-lingual word statisticsinteraction which respects language diversity.1 IntroductionRecently, an increasing number of Web servicestarget extracting keywords in articles for contentunderstanding, event tracking, or opinion mining.Existing keyword extraction algorithm (KEA)typically looks at articles monolingually andcalculate word significance in certain language.However, the calculation in another languagemay tell the story differently since languagesdiffer in grammar, phrase structure, and wordusage, thus word statistics on keyword analysis.Consider the English article in Figure 1.
Basedon the English content alone, monolingual KEAmay not derive the best keyword set.
A better setmight be obtained by referring to the article andits counterpart in another language (e.g.,Chinese).
Different word statistics in articles ofdifferent languages may help, due to languagedivergence such as phrasal structure (i.e., wordorder) and word usage and repetition (resultingfrom word translation or word sense) and so on.For example, bilingual phrases ?socialreintegration?
and ??????
in Figure 1 haveinverse word orders (?social?
translates into ???
?
and ?reintegration?
into ?
?
?
?
), both?prosthesis?
and ?artificial limbs?
translate into???
?, and ?physical?
can be associated with ???
?
and ???
?
in ?physical therapist?
and?physical rehabilitation?
respectively.
Intuitively,using cross-lingual statistics (implicitlyleveraging language divergence) can help look atarticles from different perspectives and extractkeywords more accurately.We present a system, BiKEA, that learns toidentify keywords in a language with the help ofthe other.
The cross-language information isexpected to reinforce language similarities andvalue language dissimilarities, and betterunderstand articles in terms of keywords.
Anexample keyword analysis of an English articleis shown in Figure 1.
BiKEA has aligned theparallel articles at word level and determined thescores of topical keyword preferences for words.BiKEA learns these topic-related scores duringtraining by analyzing a collection of articles.
Wewill describe the BiKEA training process in moredetail in Section 3.At run-time, BiKEA transforms an article in alanguage (e.g., English) into PageRank wordgraph where vertices are words in the article andedges between vertices indicate the words?
co-occurrences.
To hear another side of the story,BiKEA also constructs graph from its counterpartin another language (e.g., Chinese).
These twoindependent graphs are then bridged over nodes1Figure 1.
An example BiKEA keyword analysis for an article.that are bilingually equivalent or aligned.
Thebridging is to take language divergence intoaccount and to allow for language-wiseinteraction over word statistics.
BiKEA, then inbilingual context, iterates with learned wordkeyness scores to find keywords.
In ourprototype, BiKEA returns keyword candidates ofthe article for keyword evaluation (see Figure 1);alternatively, the keywords returned by BiKEAcan be used as candidates for social tagging thearticle or used as input to an articlerecommendation system.2 Related WorkKeyword extraction has been an area of activeresearch and applied to NLP tasks such asdocument categorization (Manning and Schutze,2000), indexing (Li et al., 2004), and text miningon social networking services ((Li et al., 2010);(Zhao et al., 2011); (Wu et al., 2010)).The body of KEA focuses on learning wordstatistics in document collection.
Approachessuch as tfidf and entropy, using local documentand/or across-document information, pose strongbaselines.
On the other hand, Mihalcea andTarau (2004) apply PageRank, connecting wordslocally, to extract essential words.
In our work,we leverage globally learned keywordpreferences in PageRank to identify keywords.Recent work has been done on incorporatingsemantics into PageRank.
For example, Liu et al.
(2010) construct PageRank synonym graph toaccommodate words with similar meaning.
AndHuang and Ku (2013) weigh PageRank edgesbased on nodes?
degrees of reference.
In contrast,we bridge PageRank graphs of parallel articles tofacilitate statistics re-distribution or interactionbetween the involved languages.In studies more closely related to our work,Liu et al.
(2010) and Zhao et al.
(2011) presentPageRank algorithms leveraging article topicinformation for keyword identification.
The maindifferences from our current work are that thearticle topics we exploit are specified by humansnot by automated systems, and that ourPageRank graphs are built and connectedbilingually.In contrast to the previous research in keywordextraction, we present a system thatautomatically learns topical keyword preferencesand constructs and inter-connects PageRankgraphs in bilingual context, expected to yieldbetter and more accurate keyword lists forarticles.
To the best of our knowledge, we are thefirst to exploit cross-lingual information and takeadvantage of language divergence in keywordextraction.3 The BiKEA SystemSubmitting natural language articles to keywordextraction systems may not work very well.Keyword extractors typically look at articlesfrom monolingual points of view.
Unfortunately,word statistics derived based on a language mayThe English Article:I've been in Afghanistan for 21 years.
I work for the Red Cross and I'm a physical therapist.
My job is tomake arms and legs -- well it's not completely true.
We do more than that.
We provide the patients, theAfghan disabled, first with the physical rehabilitation then with the social reintegration.
It's a very logicalplan, but it was not always like this.
For many years, we were just providing them with artificial limbs.
Ittook quite many years for the program to become what it is now.
?Its Chinese Counterpart: ???????
21 ??
?????????
??????????
???????????
-- ??????????
??????????
????????
????????
??????
?, ???????
????????????
??????????
???????????
?????
????????
?????????????
?Word Alignment Information:physical (??
), therapist (???
), social (??
), reintegration (??
), physical (??
), rehabilitation  (??
), prosthesis (??
), ?Scores of Topical Keyword Preferences for Words:(English)    prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ?
(Chinese)   ??
: 0.41; ?????
: 0.15; ??
:0.10; ???
: 0.08, ?English Keywords from Bilingual Perspectives:prosthesis, artificial, leg, rehabilitation, orthopedic, ?2be biased due to the language?s grammar, phrasestructure, word usage and repetition and so on.To identify keyword lists from natural languagearticles, a promising approach is to automaticallybridge the original monolingual framework withbilingual parallel information expected to respectlanguage similarities and diversities at the sametime.3.1 Problem StatementWe focus on the first step of the articlerecommendation process: identifying a set ofwords likely to be essential to a given article.These keyword candidates are then returned asthe output of the system.
The returned keywordlist can be examined by human users directly, orpassed on to article recommendation systems forarticle retrieval (in terms of the extractedkeywords).
Thus, it is crucial that keywords bepresent in the candidate list and that the list notbe too large to overwhelm users or thesubsequent (typically computationally expensive)article recommendation systems.
Therefore, ourgoal is to return reasonable-sized set of keywordcandidates that, at the same time, must containessential terms in the article.
We now formallystate the problem that we are addressing.Problem Statement: We are given a bilingualparallel article collection of various topics fromsocial media (e.g., TED), an article ARTe inlanguage e, and its counterpart ARTc in languagec.
Our goal is to determine a set of words that arelikely to contain important words of ARTe.
Forthis, we bridge language-specific statistics ofARTe and ARTc via bilingual information (e.g.,word alignments) and consider word keynessw.r.t.
ARTe?s topic such that cross-lingualdiversities are valued in extracting keywords in e.In the rest of this section, we describe oursolution to this problem.
First, we definestrategies for estimating keyword preferences forwords under different article topics (Section 3.2).These strategies rely on a set of article-topicpairs collected from the Web (Section 4.1), andare monolingual, language-dependentestimations.
Finally, we show how BiKEAgenerates keyword lists for articles leveragingPageRank algorithm with word keyness andcross-lingual information (Section 3.3).3.2 Topical Keyword PreferencesWe attempt to estimate keyword preferenceswith respect to a wide range of article topics.Basically, the estimation is to calculate wordsignificance in a domain topic.
Our learningprocess is shown in Figure 2.Figure 2.
Outline of the process usedto train BiKEA.In the first two stages of the learning process, wegenerate two sets of article and word information.The input to these stages is a set of articles andtheir domain topics.
The output is a set of pairsof article ID and word in the article, e.g.,(ARTe=1, we=?prosthesis?)
in language e or(ARTc=1, wc=????)
in language c, and a set ofpairs of article topic and word in the article, e.g.,(tpe=?disability?, we=?prosthesis?)
in e and(tpe=?disability?, wc=????)
in c. Note that thetopic information is shared between the involvedlanguages, and that we confine the calculation ofsuch word statistics in their specific language torespect language diversities and the language-specific word statistics will later interact inPageRank at run-time (See Section 3.3).The third stage estimates keyword preferencesfor words across articles and domain topics usingaforementioned (ART,w) and (tp,w) sets.
In ourpaper, two popular estimation strategies inInformation Retrieval are explored.
They are asfollows.tfidf.
tfidf(w)=freq(ART,w)/appr(ART?,w) whereterm frequency in an article is divided by itsappearance in the article collection to distinguishimportant words from common words.ent.
entropy(w)= -?tp?Pr(tp?|w)?log(Pr(tp?|w))where  a word?s uncertainty in topics is used toestimate its associations with domain topics.These strategies take global information (i.e.,article collection) into account, and will be usedas keyword preference models, bilinguallyintertwined, in PageRank at run-time whichlocally connects words (i.e., within articles).3.3 Run-Time Keyword ExtractionOnce language-specific keyword preferencescores for words are automatically learned, theyare stored for run-time reference.
BiKEA thenuses the procedure in Figure 3 to fuse theoriginally language-independent word statistics(1) Generate article-word pairs in training data(2) Generate topic-word pairs in training data(3) Estimate keyword preferences for words w.r.t.article topic based on various strategies(4) Output word-and-keyword-preference-scorepairs for various strategies3to determine keyword list for a given article.
Inthis procedure a machine translation technique(i.e., IBM word aligner) is exploited to gluestatistics in the involved languages and makebilingually motivated random-walk algorithm(i.e., PageRank) possible.Figure 3.
Extracting keywords at run-time.Once language-specific keyword preferencescores for words are automatically learned, theyare stored for run-time reference.
BiKEA thenuses the procedure in Figure 3 to fuse theoriginally language-independent word statisticsto determine keyword list for a given article.
Inthis procedure a machine translation technique(i.e., IBM word aligner) is exploited to gluestatistics in the involved languages and makebilingually motivated random-walk algorithm(i.e., PageRank) possible.In Steps (1) and (2) we construct PageRankword graphs for the article ARTe in language eand its counterpart ARTc in language c. They arebuilt individually to respect language properties(such as subject-verb-object or subject-object-verb structure).
Figure 4 shows the algorithm.
Inthis algorithm, EW stores normalized edgeweights for word wi and wj (Step (2)).
And EWis a v by v matrix where v is the vocabulary sizeof ARTe and ARTc.
Note that the graph is directed(from words to words that follow) and edgeweights are words?
co-occurrences withinwindow size WS.
Additionally we incorporateedge weight multiplier m>1 to propagate morePageRank scores to content words, with theintuition that content words are more likely to bekeywords (Step (2)).Figure 4.
Constructing PageRank word graph.Step (3) in Figure 3 linearly combines wordgraphs EWe and EWc using ?.
We use ?
tobalance language properties or statistics, andBiKEA backs off to monolingual KEA if ?
is one.In Step (4) of Figure 3 for each wordalignment (wic, wje), we construct a link betweenthe word nodes with the weight BiWeight.
Theinter-language link is to reinforce languagesimilarities and respect language divergencewhile the weight aims to elevate the cross-language statistics interaction.
Word alignmentsare derived using IBM models 1-5 (Och and Ney,2003).
The inter-language link is directed fromwicto wje, basically from language c to e based onthe directional word-aligning entry (wic, wje).
Thebridging is expected to help keyword extractionin language e with the statistics in language c.Although alternative approach can be used forbridging, our approach is intuitive, and mostimportantly in compliance with the directionalspirit of PageRank.Step (6) sets KP of keyword preference modelusing topical preference scores learned fromSection 3.2, while Step (7) initializes KN ofPageRank scores or, in our case, word keynessscores.
Then we distribute keyness scores untilthe number of iteration or the average scoredifferences of two consecutive iterations reachtheir respective limits.
In each iteration, a word?skeyness score is the linear combination of itskeyword preference score and the sum of thepropagation of its inbound words?
previousPageRank scores.
For the word wje in ARTe, anyedge (wie,wje) in ARTe, and any edge (wkc,wje) inWA, its new PageRank score is computed asbelow.procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,?,N)//Construct language-specific word graph for PageRank(1)  EWe=constructPRwordGraph(ARTe)(2)  EWc=constructPRwordGraph(ARTc)//Construct inter-language bridges(3)  EW=??
EWe+(1-?)
?
EWcfor each word alignment (wic, wje) in WAif IsContWord(wic) and IsContWord(wje)(4a)      EW[i,j]+=1?
BiWeightcontelse(4b)      EW[i,j]+=1?
BiWeightnoncont(5)  normalize each row of EW to sum to 1//Iterate for PageRank(6)  set KP1 ?v to[KeyPrefs(w1), KeyPrefs(w2), ?,KeyPrefs(wv)](7)  initialize KN1 ?v to [1/v,1/ v, ?,1/v]repeat(8a)  KN?=??
KN?
EW+(1-?)
?
KP(8b)  normalize KN?
to sum to 1(8c)  update KN with KN?
after the check of KN and KN?until maxIter or avgDifference(KN,KN?)
?
smallDiff(9)  rankedKeywords=Sort words in decreasing order of KNreturn the N rankedKeywords in e with highestscoresprocedure constructPRwordGraph(ART)(1) EWv ?v=0v ?vfor each sentence st in ARTfor each word wi in stfor each word wj in st where i<j and j-i ?
WSif not IsContWord(wi) and IsContWord(wj)(2a)            EW[i,j]+=1?
melif not IsContWord(wi) and not IsContWord(wj)(2b)            EW[i,j]+=1 ?
(1/m)elif IsContWord(wi) and not IsContWord(wj)(2c)            EW[i,j]+=1?
(1/m)elif IsContWord(wi) and IsContWord(wj)(2d)            EW[i,j]+=1 ?
mreturn EW4???
[1, ?]
=?
????
?
????
[1, ?]
?
???
[?, ?]
+???
(1 ?
?)
????
[1, ?]
?
??
[?, ?]???
??
?+ (1 ??)
?
??
[1, ?
]Once the iterative process stops, we rankwords according to their final keyness scores andreturn top N ranked words in language e askeyword candidates of the given article ARTe.
Anexample keyword analysis for an English articleon our working prototype is shown in Figure 1.Note that language similarities and dissimilaritieslead to different word statistics in articles ofdifference languages, and combining such wordstatistics helps to generate more promisingkeyword lists.4 ExperimentsBiKEA was designed to identify words ofimportance in an article that are likely to coverthe keywords of the article.
As such, BiKEA willbe trained and evaluated over articles.Furthermore, since the goal of BiKEA is todetermine a good (representative) set ofkeywords with the help of cross-lingualinformation, we evaluate BiKEA on bilingualparallel articles.
In this section, we first presentthe data sets for training BiKEA (Section 4.1).Then, Section 4.2 reports the experimentalresults under different system settings.4.1 Data SetsWe collected approximately 1,500 Englishtranscripts (3.8M word tokens and 63K wordtypes) along with their Chinese counterparts(3.4M and 73K) from TED (www.ted.com) forour experiments.
The GENIA tagger (Tsuruokaand Tsujii, 2005) was used to lemmatize andpart-of-speech tag the English transcripts whilethe CKIP segmenter (Ma and Chen, 2003)segment the Chinese.30 parallel articles were randomly chosen andmanually annotated for keywords on the Englishside to examine the effectiveness of BiKEA inEnglish keyword extraction with the help ofChinese.4.2 Experimental ResultsTable 1 summarizes the performance of thebaseline tfidf and our best systems on the test set.The evaluation metrics are nDCG (Jarvelin andKekalainen, 2002), precision, and meanreciprocal rank.
(a) @N=5 nDCG P MRRtfidf .509 .213 .469PR+tfidf .676 .400 .621BiKEA+tfidf .703 .406 .655(b) @N=7 nDCG P MRRtfidf .517 .180 .475PR+tfidf .688 .323 .626BiKEA+tfidf .720 .338 .660(c) @N=10 nDCG P MRRtfidf .527 .133 .479PR+tfidf .686 .273 .626BiKEA+tfidf .717 .304 .663Table 1.
System performance at(a) N=5 (b) N=7 (c) N=10.As we can see, monolingual PageRank (i.e.,PR) and bilingual PageRank (BiKEA), usingglobal information tfidf, outperform tfidf.
Theyrelatively boost nDCG by 32% and P by 87%.The MRR scores also indicate their superiority:their top-two candidates are often keywords vs.the 2nd place candidates from tfidf.Encouragingly, BiKEA+tfidf achieves betterperformance than the strong monolingualPR+tfidf across N?s.
Specifically, it furtherimproves nDCG relatively by 4.6% and MRRrelatively by 5.4%.Overall, the topical keyword preferences, andthe inter-language bridging and the bilingualscore propagation in PageRank are simple yeteffective.
And respecting language statistics andproperties helps keyword extraction.5 SummaryWe have introduced a method for extractingkeywords in bilingual context.
The methodinvolves estimating keyword preferences, word-aligning parallel articles, and bridging language-specific word statistics using PageRank.Evaluation has shown that the method canidentify more keywords and rank them higher inthe candidate list than monolingual KEAs.
As forfuture work, we would like to explore thepossibility of incorporating the articles?
readerfeedback into keyword extraction.
We wouldalso like to examine the proposed methodologyin a multi-lingual setting.5AcknowledgementThis study is conducted under the ?Online andOffline integrated Smart Commerce Platform(1/4)?
of the Institute for Information Industrywhich is subsidized by the Ministry of EconomyAffairs of the Republic of China.ReferencesScott A. Golder and Bernardo A. Huberman.2006.
Usage patterns of collaborative taggingsystems.
Information Science, 32(2): 198-208.Harry Halpin, Valentin Robu, and HanaShepherd.
2007.
The complex dynamics ofcollaborative tagging.
In Proceedings of theWWW, pages 211-220.Chung-chi Huang and Lun-wei Ku.
2013.Interest analysis using semantic PageRank andsocial interaction content.
In Proceedings ofthe ICDM Workshop on Sentiment Elicitationfrom Natural Text for Information Retrievaland Extraction, pages 929-936.Kalervo Jarvelin and Jaana Kekalainen.
2002.Cumulated gain-based evaluation of IRtechnologies.
ACM Transactions onInformation Systems, 20(4): 422-446.Philipp Koehn, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 48-54.Quanzhi Li, Yi-Fang Wu, Razvan Bot, and XinChen.
2004.
Incorporating documentkeyphrases in search results.
In Proceedings ofthe Americas Conference on InformationSystems.Zhenhui Li, Ging Zhou, Yun-Fang Juan, andJiawei Han.
2010.
Keyword extraction forsocial snippets.
In Proceedings of the WWW,pages 1143-1144.Marina Litvak and Mark Last.
2008.
Graph-based keyword extraction for single-documentsummarization.
In Proceedings of the ACLWorkshop on Multi-Source MultilingualInformation Extraction and Summarization,pages 17-24.Zhengyang Liu, Jianyi Liu, Wenbin Yao, CongWang.
2010.
Keyword extraction usingPageRank on synonym networks.
InProceedings of the ICEEE, pages 1-4.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2010.
Automatic keyphraseextraction via topic decomposition.
InProceedings of the EMNLP, pages 366-376.Wei-Yun Ma and Keh-Jiann Chen.
2003.Introduction to CKIP Chinese wordsegmentation system for the first internationalChinese word segmentation bakeoff.
InProceedings of the ACL Workshop on ChineseLanguage Processing.Chris D. Manning and Hinrich Schutze.
2000.Foundations of statistical natural languageprocessing.
MIT Press.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing orders into texts.
In Proceedings ofthe EMNLP, pages 404-411.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational Linguistics,29(1): 19-51.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.Bidirectional inference with the easiest-firststrategy for tagging sequence data.
InProceedings of the EMNLP, pages 467-474.Peter D. Turney.
2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval,2(4): 303-336.Wei Wu, Bin Zhang, and Mari Ostendorf.
2010.Automatic generation of personalizedannotation tags for Twitter users.
InProceedings of the NAACL, pages 689-692.Wayne Xin Zhao, Jing Jiang, Jing He, YangSong, Palakorn Achananuparp, Ee-Peng Lim,and Xiaoming Li.
2011.
Topical keywordextraction from Twitter.
In Proceedings of theACL, pages 379-388.6
