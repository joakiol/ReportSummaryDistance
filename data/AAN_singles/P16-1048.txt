Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 505?515,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCSE: Conceptual Sentence Embeddings based on Attention ModelYashen Wang, Heyan Huang?, Chong Feng, Qiang Zhou, Jiahui Gu and Xiong GaoBeijing Engineering Research Center of High Volume Language Information Processingand Cloud Computing Applications, School of Computer,Beijing Institute of Technology, Beijing, P. R. China{yswang,hhy63,fengchong,qzhou,gujh,gaoxiong}@bit.edu.cnAbstractMost sentence embedding models typical-ly represent each sentence only using wordsurface, which makes these models indis-criminative for ubiquitous homonymy andpolysemy.
In order to enhance represen-tation capability of sentence, we employconceptualization model to assign associ-ated concepts for each sentence in the tex-t corpus, and then learn conceptual sen-tence embedding (CSE).
Hence, this se-mantic representation is more expressivethan some widely-used text representationmodels such as latent topic model, espe-cially for short-text.
Moreover, we fur-ther extend CSE models by utilizing a lo-cal attention-based model that select rel-evant words within the context to makemore efficient prediction.
In the experi-ments, we evaluate the CSE models on twotasks, text classification and informationretrieval.
The experimental results showthat the proposed models outperform typi-cal sentence embed-ding models.1 IntroductionMany natural language processing applications re-quire the input text to be represented as a fixed-length feature, of which sentence representation isvery important.
Perhaps the most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams (Harris, 1970).
Howev-er, they suffer severely from data sparsity and highdimensionality, and have very little sense aboutthe semantics of words or the distances betweenthe words.
Recently, in sentence representationand classification, deep neural network (DNN) ap-proaches have achieved state-of-the-art results (Le?The contact author.and Mikolov, 2014; Liu et al, 2015; Palangi et al,2015; Wieting et al, 2015).
Despite of their use-fulness, recent sentence embeddings face severalchallenges: (i) Most sentence embedding modelsrepresent each sentence only using word surface,which makes these models indiscriminative for u-biquitous polysemy; (ii) For short-text, however,neither parsing nor topic modeling works well be-cause there are simply not enough signals in theinput; (iii) Setting window size of context word-s is very difficult.
To solve these problems, wemust derive more semantic signals from the inputsentence, e.g., concepts.
Besides, we should as-signed different attention for different contextualword, to enhance the influence of words that arerelevant for each prediction.This paper proposed Conceptual Sentence Em-bedding (CSE), an unsupervised framework thatlearns continuous distributed vector representa-tions for sentence.
Specially, by innovatively in-troducing concept information, this concept-levelvector representations of sentence are learned topredict the surrounding words or target word incontexts.
Our research is inspired by the recentwork in learning vector representations of word-s using deep learning strategy (Mikolov et al,2013a; Le and Mikolov, 2014).
More precisely,we first obtain concept distribution of the sentence,and generate corresponding concept vector.
Thenwe concatenate or average the sentence vector,contextual word vectors with concept vector of thesentence, and predict the target word in the givencontext.
All of the sentence vectors and word vec-tors are trained by the stochastic gradient descen-t and backpropagation (Rumelhart et al, 1986).At prediction time, sentence vectors are inferredby fixing the word vectors and observed sentencevectors, and training the new sentence vector untilconvergence.In parallel, the concept of attention has gained505popularity recently in neural natural languageprocessing researches, which allowing modelsto learn alignments between different modalities(Bahdanau et al, 2014; Bansal et al, 2014; Rushet al, 2015).
In this work, we further propose theextensions to CSE, which adds an attention mod-el that considers contextual words differently de-pending on the word type and its relative positionto the predicted word.
The main intuition behindthe extended model is that prediction of a word ismainly dependent on certain words surrounding it.In summary, the basic idea of CSE is that, we al-low each word to have different embeddings underdifferent concepts.
Taking word apple into consid-eration, it may indicate a fruit under the conceptfood, and indicate an IT company under the con-cept information technology.
Hence, concept in-formation significantly contributes to the discrimi-native of sentence vector.
Moreover, an importan-t advantage of the proposed conceptual sentenceembeddings is that they could be learned from un-labeled data.
Another advantage is that we takethe word order into account, in the same way of n-gram model, while bag-of-n-grams model wouldcreate a very high-dimensional representation thattends to generalize poorly.To summarize, this work contributes on thefollowing aspects: We integrate concepts andattention-based strategy into basic sentence em-bedding representation, and allow the resultingconceptual sentence embedding to model differ-ent meanings of a word under different concep-t.
The experimental results on text classificationtask and information retrieval task demonstratethat this concept-level sentence representation isrobust.
The outline of the paper is as follows.
Sec-tion 2 surveys related researches.
Section 3 for-mally de-scribes the proposed model of concep-tual sentence embedding.
Corresponding experi-mental results are shown in Section 4.
Finally, weconclude the paper.2 Related WorksConventionally, one-hot sentence representationhas been widely used as the basis of bag-of-words(BOW) text model.
However, it can-not take thesemantic information into consideration.
Recent-ly, in sentence representation and classification,deep neural network approaches have achievedstate-of-the-art results (Le and Mikolov, 2014; Li-u et al, 2015; Ma et al, 2015; Palangi et al, 2015;Wieting et al, 2015), most of which are inspiredby word embedding (Mikolov et al, 2013a).
(Leand Mikolov, 2014) proposed the paragraph vector(PV) that learns fixed-length representations fromvariable-length pieces of texts.
Their model rep-resents each document by a dense vector which istrained to predict words in the document.
Howev-er, their model depends only on word surface, ig-noring semantic information such as topics or con-cepts.
In this paper, we extent PV by introducingconcept information.Aiming at enhancing discriminativeness for u-biquitous polysemy, (Liu et al, 2015) employedlatent topic models to assign topics for each wordin the text corpus, and learn topical word em-beddings (TWE) and sentence embeddings basedon both words and their topics.
Besides, tocombine deep learning with linguistic structures,many syntax-based embedding algorithms havebeen proposed (Severyn et al, 2014; Wang etal., 2015b) to utilize long-distance dependencies.However, short-texts usually do not observe thesyntax of a written language, nor do they con-tain enough signals for statistical inference (e.g.,topic model).
Therefore, neither parsing nor top-ic modeling works well because there are simplynot enough signals in the input, and we must de-rive more semantic signals from the input, e.g.,concepts, which have been demonstrated effectivein knowledge representation (Wang et al, 2015c;Song et al, 2015).
Shot-text conceptualization, isan interesting task to infer the most likely conceptsfor terms in the short-text, which could help bet-ter make sense of text data, and extend the textswith categorical or topical information (Song etal., 2011).
Therefore, our models utilize short-text conceptualization algorithm to discriminateconcept-level sentence senses and provide a goodperformance on short-texts.Recently, attention model has been used to im-prove many neural natural language pro-cessingresearches by selectively focusing on parts of thesource data (Bahdanau et al, 2014; Bansal et al,2014; Wang et al, 2015a).
To the best of ourknowledge, there has not been any other work ex-ploring the use of attentional mechanism for sen-tence embeddings.3 Conceptual Sentence EmbeddingThis paper proposes four conceptual sentence em-bedding models.
The first one is based on continu-506ous bag-of-word model (denoted as CSE-1) whichhave not taken word order into consideration.
Toovercome this drawback, its extension model (de-noted as CSE-2), which is based on Skip-Grammodel, is proposed.
Based on the basic concep-tual sentence embedding models above, we obtaintheir variants (aCSE-1 and aCSE-2) by introduc-ing attention model.3.1 CBOW Model & Skip-Gram ModelAs inspiration of the proposed conceptual sen-tence embedding models, we start by dis-cussing previous models for learning word vec-tors (Mikolov et al, 2013a; Mikolov et al, 2013b)firstly.Let us overview the framework of ContinuousBag-of-Words (CBOW) firstly, which is shown inFigure 1(a).
Each word is typically mapped toan unique vector, represented by a column in aword matrix W ?
<d?|V |.
Wherein, V denotesthe word vocabulary and d is embedding dimen-sion of word.
The column is indexed by posi-tion of the word in V .
The concatenation or av-erage of the vectors, the context vector wt, is thenused as features for predicting the target word inthe current context.
Formally, Given a sentenceS = {w1, w2, .
.
.
, wl}, the objective of CBOW isto maximize the average log probability:L(S)=1(l?2k?2)?l?kt=k+1logPr(wt|wt?k,??
?,wt+k) (1)Wherein, k is the context windows size of targetword wt.
The prediction task is typically done viaa softmax function, as follows:Pr(wt|wt?k, ?
?
?
, wt+k) =eywt?wi?Veywi(2)Each of y(wt) is an un-normalized log-probability for each target word wt, as follows:ywt= Uh(wt?k, .
.
.
, wt+k); W) + b (3)Wherein, U and b are softmax parameters.
Andh(?)
is constructed by a concatenation or averageof word vectors {wt?k, .
.
.
,wt+k} extracted fromword matrix W according to {wt?k, .
.
.
, wt+k}.For illustration purposes, we utilize average here.On the condition of average, the context vector ctis obtained by averaging the embeddings of eachword, as follows:ct=12k?
?k?c?k,c6=0wt+c(4)The framework of Skip-Gram (Figure 1(b))aims to predict context words given a target wordwtin a sliding window, instead of predicting thecurrent word based on its context.
Formally, givena sentence S = {w1, w2, .
.
.
, wl}, the objective ofSkip-Gram is to maximize the following averagelog probability:L(S)=1(l?2k)?l?kt=k+1?
?k?c?k,c6=0logPr(wt+c|wt)(5)Wherein, wtand wcare respectively the vectorrepresentations of the target word wtand the con-text word wc.
Usually, during the training stage ofCBOW and Skip-Gram: (i) in order to make themodels efficient for learning, the techniques of hi-erarchical softmax and negative sampling are usedto ensure the models efficient for learning (Morinand Bengio, 2005; Mikolov et al, 2013a); (ii) theword vectors are trained by using stochastic gra-dient descent where the gradient is obtained viabackpropagation (Rumelhart et al, 1986).
Afterthe training stage converges, words with similarmeaning are mapped to a similar position in the se-mantic vector space.
e.g., ?powerful?
and ?strong?are close to each other.Wwt-k wt-k+1 wt+k-1 wt+k?W WWwtwtWwt-k wt-k+1 wt+k-1 wt+k?
(a) (b)Figure 1: (a) CBOW model and (b) Skip-Grammodel.3.2 CSE based on CBOW ModelIntuitively, the proposed (attention-based) concep-tual sentence embedding model for learning sen-tence vectors, is inspired by the methods for learn-ing the word vectors.
The inspiration is that, inresearches of word embeddings: (i) The word vec-tors are asked to contribute to a prediction taskabout the target word or the surrounding wordsin the context; (ii) The word representation vec-tors are initialized randomly, however they could507finally capture precise semantics as an indirect re-sult.
Therefore, we will utilize this idea in our sen-tence vectors in a similar manner: The concept-associated sentence vectors are also asked to con-tribute to the prediction task of the target word orsurrounding words in given contextual text win-dows.
Furthermore, attention model will attributedifferent influence value to different contextualwords.We describe the first conceptual sentence em-bedding model, denoted as CSE-1, which is basedon CBOW.
In the framework of CSE-1 (Figure2(a)), each sentence, denoted by sentence ID, ismapped to a unique vector s, represented by a col-umn in matrix S. And its concept distribution ?Care generated from a knowledge-based text con-ceptualization algorithm (Wang et al, 2015c).Moreover, similar to word embedding methods,each word wiis also mapped to a unique vec-tor wi, represented by a column in matrix W.The surrounding words in contextual text window{wt?k, .
.
.
, wt+k}, sentence ID and concept dis-tribution ?Ccorresponding to this sentence are theinputs.
Besides, C is a fixed linear operator similarto the one used in (Huang et al, 2013) that con-verts the concept distribution ?Cto a concept vec-tor, denoted as c. Note that, this makes our modelvery different from (Le and Mikolov, 2014) whereno concept information is used, and experimentalresults demonstrate the efficiency of introducingconcept information.
It is clear that CSE-1 alsodoes not take word order into consideration justlike CBOW.Afterward, the sentence vector s, surroundingword vectors {wt?k, .
.
.
,wt+k} and the conceptvector c are concatenated or averaged to predic-t the target word wtin current context.
In reali-ty, the only change in this model compared to theword embedding method is in Eq.
3, where h(?
)is constructed from not only W but also C and S.Note that, the sentence vector is shared across allcontexts generated from the same sentence but notacross sentences.
Wherein, the contexts are fixed-length (length is 2k) and sampled from a slidingwindow over the current sentence.
However, theword matrix W is shared across sentences.In summary, the procedure of CSE-1 itself isdescribed as follows.
A probabilistic conceptu-alization algorithm (Wang et al, 2015c) is em-ployed here to obtain the corresponding conceptsabout given sentence: Firstly, we preprosess andSentence IDConceptualizationCSw t-kw t-k+1w t+k-1w t+k?W w t-kw t-k+1w t+k-1w t+k?WWWw tSentence IDConceptualizationCS?
C?
Ccscs(a) (b)Figure 2: CSE-1 model (a) and CSE-2 model (b).Green circles indicate word embeddings, blue cir-cles indicate concept embeddings, and purple cir-cles indicate sentence embeddings.
Besides, or-ange circles indicate concept distribution ?Cgen-erated by knowledge-based text conceptualizationalgorithm.segment the given sentence into a set of words;Then, based on a probabilistic lexical knowledge-base Probase (Wu et al, 2012), the heteroge-neous semantic graph for these words and theircorresponding concepts are constructed (Figure 3shows an example); Finally, we utilize a simpleiterative process to identify the most likely map-ping from words to concepts.
After efforts above,we could conceptualize words in given sentence,and access the concepts and corresponding proba-bilities, which is the concept distribution ?Cmen-tioned before.
Note that, the concept distributionyields an important influence on the entire frame-work of conceptual sentence embedding, by con-tributing greatly to the semantic representation.During the training stage, we aim at obtainingword matrix W, sentence matrix S, and softmaxweights {U, b} on already observed sentences.The techniques of hierarchical softmax and nega-tive sampling are used to make the model efficientfor learning.
W and S are trained using stochas-tic gradient descent: At each step of stochasticgradient descent, we sample a fixed-length con-text from the given sentence, compute the errorgradient which is obtained via backpropagation,and then use the gradient to update the parame-ters.
During the inferring stage, we get sentencevectors for new sentences (unobserved before) byadding more columns in S and gradient descend-ing on S while holding W, U and b fixed.
Finally,we use S to make a prediction about multi-labelsby using a standard classifier in output layer.508FRUITmicrosoft officeapple ipadCOMPANYBRAND PRODUCTLOCATIONBUILDING0.800.410.160.860.810.910.31ACCESSORY0.530.820.760.37Figure 3: Semantic graph of example sentence mi-crosoft unveils office for apples ipad.
Rectanglesindicate terms occurred in given sentence, and el-lipses indicate concept defined in knowledge-base(e.g., Probase).
Bule solid links indicate isA re-lationship between terms and concepts, and reddashed lines indicate correlation relationship be-tween two concepts.
Numerical values on the lineis corresponding probabilities.3.3 CSE based on Skip-Gram ModelThe above method considers the combination ofthe sentence vector with the surrounding wordvectors and concept vector to predict the targetword in given text window.
However, it loss in-formation about word order somehow, just likeCBOW.
In fact, there exists another for modelingthe prediction procedure: we could ignore the con-text words in the input, but force the model to pre-dict words randomly sampled from the fix-lengthcontexts in the output.
As is shown in Figure 2(b), only sentence vector s and concept vector care used to predict the next word in a text window.That means, contextual words are no longer usedas inputs, whereas they become what the outputlayer predict.
Hence, this model is similar to theSkip-Gram model in word embedding (Mikolovet al, 2013b).
In reality, what this means is thatat each iteration of stochastic gradient descent,we sample a text window {wt?k, .
.
.
, wt+k}, thensample a random word from this text window andform a classification task given the sentence vectors and corresponding concept vector c.We denote this sort of conceptual sentence em-bedding model as CSE-2.
The scheme of CSE-2is similar to that of CSE-1 as described above.
Inaddition to being conceptually simple, CSE-2 re-quires to store less data.
We only need to store{U,b,S} as opposed to {U,b,S,W} in CSE-1.3.4 CSE based on Attention ModelAs mentioned above, setting a good value for con-textual window size k is difficult.
Because a largervalue of k may introduce a degenerative behav-ior in the model, and more effort is spent predict-ing words that are conditioned on unrelated words,while a smaller value of k may lead to cases wherethe window size is not large enough include wordsthat are semantically related (Bansal et al, 2014;Wang et al, 2015a).
To solve these problems , weextend the proposed models by introducing atten-tion model (Bahdanau et al, 2014; Rush et al,2015), by allowing it to consider contextual word-s within the window in a non-uniform way.
Forillustration purposes, we extend CSE-1 here withattention model.
Following (Wang et al, 2015a),we rewrite Eq.
(4) as follows:ct=12k?
?k?c?k,c6=0at+c(wt+c)wt+c(6)Wherein we replace the average of the sur-rounding word vectors in Eq.
(4) with a weightedsum of the these vectors.
That means, each con-textual wordwt+cis attributed a different attentionlevel, representing how much the attention modelbelieves whether it is important to look at in orderto predict the target word wt.
The attention factorai(wi) for word wiin position i is formulated asa softmax function over contextual words (Bah-danau et al, 2014), as follows:ai(w) =edw,i+ ri?
?k?c?k,c6=0edw,c+ rc(7)Wherein, dw,iis an element of matrix D ?<|V |?2k, which is a set of parameters determiningthe importance of each word type in each relativeposition i (distance to the left/right of target wordwt).
Moreover, ri, an element of R ?
<2k, isa bias, which is conditioned only on the relativeposition i.
Note that, attention models have beenreported expensive for large tables in terms of s-torage and performance (Bahdanau et al, 2014;Wang et al, 2015a).
Nevertheless the computa-tion consumption here is simple, and compute theattention of all words in the input requires 2k op-erations, as it simply requires retrieving on valuefrom the lookup-matrix D for each word and onevalue from the bias vector R for each word in thecontext.
Although this strategy may not be thebest approach and there exist more elaborate at-tention models (Bahdanau et al, 2014; Luong etal., 2015), the proposed attention model is a properbalance of computational efficiency and complex-ity.Thus, besides {W,C,S} in CSE models, D andR are added into parameter set which relates to509gradients of the loss function Eq.(1).
All parame-ters are computed with backpropagation and up-dated after each training instance using a fixedlearning rate.
We denote the attention-based CSE-1 model above as aCSE-1.
With limitation ofspace, attention variant of CSE-2, denoted asaCSE-2, is not described here, however the prin-ciple is similar to aCSE-1.Ww 1microsoftw 2unveilw 4forw 5ipadW WWwappleSentence IDConceptualizationC Sw 3officeWa1 a2 a3 a4 a5...?cc sFigure 4: aCSE-1 model.
The illustration of ex-ample sentence ?mcrosoft unveils office for apple?sipad?
for predicting word ?apple?.Taking example ?microsoft unveils office for ap-ple?s ipad?
into consideration.
The prediction ofthe polysemy word ?apple?
by CSE-1 is shown inFigure 4, and darker cycle cell indicate higher at-tention value.
We could observe that prepositionword ?for?
tend to be attributed very low atten-tion, while context words, especially noun-wordswhich contribute much to conceptualization (suchas ?ipad?, ?office?, and ?microsoft?)
are attributedhigher weights as these word own more predictivepower.
Wherein, ?ipad?
is assigned the highest at-tention value as it close to the predicted word andco-occurs with it more frequently.As described before, concept distribution ?Cyields a considerable influence on conceptual sen-tence embedding.
This is because, each dimen-sionality of this distribution denotes the probabili-ty of the concept (topic or category) this sentenceis respect to.
In other words, the concept distribu-tion is a solid semantic representation of the sen-tence.
Nevertheless, the information in each di-mensionality of sentence (or word) vector makesno sense.
Hence, there exist a linear operatorin CSE-1, CSE-2, aCSE-1, and aCSE-2, whichtransmit the concept distribution into word vectorand sentence vector, as shown in Figure 2 and Fig-ure 3.4 Experiments and ResultsIn this section, we show experiments on two tex-t understanding problems, text classification andinformation retrieval, to evaluate related modelsin several aspects.
These tasks are always usedto evaluate the performance of sentence embed-ding methods (Liu et al, 2015; Le and Mikolov,2014).
The source codes and datasets of this paperare publicly available1.4.1 DatasetsWe utilize four datasets for training and evalu-ating.
For text classification task, we use threedatasets: NewsTile, TREC and Twitter.
DatasetTweet11 is used for evaluation in information re-trieval task.
Moreover, we construct dataset Wikito fully train topic model-based models.NewsTitle: The news articles are extractedfrom a large news corpus, which contains aboutone million articles searched from Web pages.
Weorganize volunteers to classify these news articlesmanually into topics according its article content(Song et al, 2015), and we select six topics: com-pany, health, entertainment, food, politician, andsports.
We randomly select 3,000 news articles ineach topic, and only keep its title and its first oneline of article.
The average word count of titles is9.41.TREC: It is the corpus for question classifica-tion on TREC (Li and Roth, 2002), which is wide-ly used as benchmark in text classification task.There are 5,952 sentences in the entire dataset,classified into the 6 categories as follows: person,abbreviation, entity, description, location and nu-meric.Tweet11: This is the official tweet collection-s used in TREC Microblog Task 2011 and 2012(Ounis et al, 2011; Soboroff et al, 2012).
Usingthe official API, we crawled a set of local copiesof the corpus.
Our local Tweets11 collection hasa sample of about 16 million tweets, and a set of49 (TMB2011) and 60 (TMB2012) timestampedtopics.Twitter: This dataset is constructed by manu-ally labeling the previous dataset Tweet11.
Simi-lar to dataset NewsTitle, we ask our volunteers tolabel these tweets.
After manually labeling, thedataset contains 12,456 tweets which are in four1http://hlipca.org/index.php/2014-12-09-02-55-58/2014-12-09-02-56-24/58-acse510categories: company, country, entertainment, anddevice.
The average length of the tweets is 13.16words.
Because of its noise and sparsity, this so-cial media dataset is very challenging for the com-parative models.Moreover, we also construct a Wikipediadataset (denoted as Wiki) for training.
We pre-process the Wikipedia articles2with the followingrules.
First, we remove the articles less than 100words, as well as the articles less than 10 links.Then we remove all the category pages and disam-biguation pages.
Moreover, we move the contentto the right redirection pages.
Finally we obtainabout 3.74 million Wikipedia articles for indexingand training.4.2 Alternative AlgorithmsWe compare the proposed models with the follow-ing comparative algorithms.BOW: It is a simple baseline which representseach sentence as bag-of-words, and uses TF-IDFscores (Salton and Mcgill, 1986) as features togenerate sentence vector.LDA: It represents each sentence as its topicdistribution inferred by latent dirichlet alocation(Blei et al, 2003).
We train this model in twoways: (i) on both Wikipedia articles and the eval-uation datasets above, and (ii) only on the evalua-tion datasets.
We report the better of the two.PV: Paragraph Vector models are variable-length text embedding models, including the dis-tributed memory model (PV-DM) and the dis-tributed bag-of-words model (PV-DBOW).
It hasbeen reported to achieve the state-of-the-art per-formance on task of sentiment classification (Leand Mikolov, 2014), however it only utilizes wordsurface.TWE: By taking advantage of topic model, itovercome ambiguity to some extent (Liu et al,2015).
Typically, TWE learn topic models ontraining set.
It further learn topical word embed-dings using the training set, then generate sentenceembeddings for both training set and testing set.
(Liu et al, 2015) proposed three models for topicalword embedding, and we present the best result-s here.
Besides, We also train TWE in two wayslike LDA.2http://en.wikipedia.org/wiki/Wikipedia:Databasedown-load4.3 Experiment SetupThe details about parameter settings of the com-parative algorithms are described in this section,respectively.
For TWE, CSE-1, CSE-2 and theirattention variants aCSE-1, and aCSE-2, the struc-ture of the hierarchical softmax is a binary Huff-man tree (Mikolov et al, 2013a; Mikolov et al,2013b), where short codes are assigned to frequentwords.
This is a good speedup trick because com-mon words are accessed quickly (Le and Mikolov,2014).We set the dimensions of sentence, word,topic and concept embeddings as 5,000, whichis like the number of concept clusters in Probase(Wu et al, 2012; Wang et al, 2015c).
Meanwhile,we have done many experiments on choosing thecontext window size (k).
We perform experimentson increasing windows size from 3 to 11, and d-ifferent size works differently on different datasetwith different average length of short-texts.
Andwe choose the result of windows size of 5 presenthere, because it performs best in almost datasets.Usually, in project layer, the sentence vector, thecontext vector and the concept vectors could beaveraged or concatenated for combination to pre-dict the next word in a context.
We perform exper-iments following these two strategies respectively,and report the better of the two.
In fact, the con-catenation performs better since averaging differ-ent types of vectors may cause loss of informationsomehow.For BOW and LDA, we remove stop words byusing InQuery stop-word list.
For BOW, we se-lect top 50,000 words according to TF-IDF scoresas features.
For both LDA and TWE, in the textclassification task, we set the topic number to bethe cluster number or twice, and report the betterof the two; while in the information retrieval task,we experimented with a varying number of topicsfrom 100 to 500, which gives similar performance,and we report the final results of using 500 topics.In summary, we use the sentence vectors gener-ated by each algorithm as features and run a linearclassifier using Liblinear (Fan et al, 2010) for e-valuation.4.4 Text ClassificationIn this section, we run the multi-class text clas-sification experiments on the dataset NewsTitle,Twitter, and TREC.
We report precision, recalland F-measure for comparison (as shown in Ta-ble 1).
Statistical t-test are employed here.
To de-511NewsTitle Twitter TRECModel P R F P R F P R FBOW 0.782 0.791 0.786 0.437 0.429 0.433 0.892 0.891 0.891LDA 0.717 0.705 0.711 0.342 0.308 0.324 0.813 0.809 0.811PV-DBOW 0.725 0.719 0.722 0.413 0.408 0.410 0.824 0.819 0.821PV-DM 0.748 0.740 0.744 0.426 0.424 0.425 0.836 0.825 0.830TWE 0.811?0.803?0.807?0.459?0.438 0.448?0.898?0.886?0.892?CSE-1 0.815 0.809 0.812 0.461 0.449 0.454 0.896 0.890 0.893CSE-2 0.827 0.817 0.822 0.475 0.447 0.462 0.901 0.895 0.898aCSE-1 0.824 0.818 0.821 0.471 0.454 0.462 0.901 0.897 0.899aCSE-2 0.831??0.820??0.825??0.477??0.450??0.463??0.909??0.904??0.906?
?Table 1: Evaluation results of multi-class text classification task.cide whether the improvement by method A overmethod B is significant, the t-test calculates a val-ue p based on the performance of A and B. Thesmaller p is, the more significant is the improve-ment.
If the p is small enough (p < 0.05), weconclude that the improvement is statistically sig-nificant.
In Table 1, the superscript ?
and ?
re-spectively denote statistically significant improve-ments over TWE and PV-DM.Without regard to attention-based model firstly,we could conclude that CSE-2 outperforms all thebaselines significantly (expect for recall in Twit-ter).
This fully indicates that the proposed mod-el could capture more precise semantic informa-tion of sentence as compared to topic model-basedmodels and other embedding models.
Becausethe concepts we obtained contribute significantlyto the semantic representation of sentence, mean-while suffer slightly from texts noisy and sparsi-ty.
Moreover, as compared to BOW, CSE-1 andCSE-2 manage to reduce the feature space by 90percent, while among them, CSE-2 needs to storeless data comparing with CSE-1.
By introducingattention model, performances of CSE models areentirely promoted, as compared aCSE-2 with o-riginal CSE-2, which demonstrates the advantageof attention model.PV-DM and PV-DBOW are reported as thestate-of-the-art model for sentence embedding.From the results we can also see that, the proposedmodel CSE-2 and aCSE-2 significantly outper-forms PV-DBOW.
As expected, LDA performsworst, even worse than BOW, because it is trainedon very sparse short-texts (i.e., question and so-cial media text), where there is no enough sta-tistical information to infer word co-occurrenceand word topics, and latent topic model suffer ex-tremely from the sparsity of the short-text.
Be-sides, the number of topics slightly impacts theperformance of LDA.
In future, we may conductmore experiments to explore genuine reasons.
Asdescribed in section 3, aCSE-2 (CSE-2) performsbetter than aCSE-1 (CSE-1), because the formerone take word order into consideration.
Based onSkip-Gram similarly, CSE-2 outperforms TWE.Although TWE aims at enhancing sentence repre-sentation by using topic model, neither parsing nortopic modeling would work well because short-texts lack enough signals for inference.
Whatsmore, sentence embeedings are generated by sim-ple aggregating over all topical word embeddingsof each word in this sentence in TWE, which lim-its its capability of semantic representation.Overall, nearly all the alternative algorithmsperform worse on Twitter, especially LDA andTWE.
This is mainly because that data in Twitterare more challenging for topic model as short-textsare noisy, sparse, and ambiguous.
Although thetraining on larger corpus, i.e., way (i), contributesgreatly to improving the performance of thesetopic-model based algorithms, they only have sim-ilar performance to CSE-1 and could not tran-scend the attention-based variants.
Certainly, wecould also train TWE (even LDA) on a very larg-er corpus, and could expect a letter better result-s.
However, training latent topic model on verylarge dataset is very slow, although many fast al-gorithms of topic models are available (Smolaand Narayanamurthy, 2010; Ahmed et al, 2012).Whats more, from the complexity analysis, wecould conclude that, compared with PV, CSE onlyneed a little more space to store look-ups matrix D512and R; while compared with CSE and PV, TWErequire more parameters to store more discrimina-tive information for word embedding.4.5 Information RetrievalThe information retrieval task is also utilized toevaluate the proposed models, and we want to ex-amine whether a sentence should be retrieved giv-en a query.
Specially, we mainly focus on short-text retrieval by utilizing official tweet collectionTweet11, which is the benchmark dataset for mi-croblog retrieval.
We index all tweets in this col-lection by using Indri toolkit, and then perform ageneral relevance-pseudo feedback procedure, asfollows: (i) Given a query, we firstly obtain asso-ciated tweets, which are before query issue time,via preliminary retrieval as feedback tweets.
(ii)We generate the sentence representation vector ofboth original query and these feedback tweets bythe alternative algorithms above.
(iii) With effortsabove, we compute cosine scores between queryvector and each tweet vector to measure the se-mantic similarity between the query and candidatetweets, and then re-rank the feedback tweets withdescending cosine scores.We utilize the official metric for the TREC Mi-croblog track, i.e., Precision at 30 (P@30), andMean Average Precision (MAP), for evaluatingthe ranking performance of different algorithms.Experimental results for this task are shown in Ta-ble 2.
Besides, we also operate a query-by-queryanalysis and conduct t-test to demonstrate the im-provements on both metrics are statistically sig-nificant.
In Table 2, the superscript ?
and ?
re-spectively denote statistically significant improve-ments over TWE and PV-DM (p < 0.05).As shown in Table 2, the CSE-2 significant-ly outperforms all these models, and exceeds thebest baseline model (TWE) by 11.9% in MAP and4.5% in P@30, which is a statistically significan-t improvement.
Without regard to attention-basedmodel firstly, such an improvement comes fromthe CSE-2?s ability to embed the contextual andsemantic information of the sentences into a finitedimension vector.
Topic model based algorithm-s (e.g., LDA and TWE) suffer extremely from thesparsity and noise of tweet collection.
For the twit-ter data, since we are not able to find appropriatelong texts, latent topic models are not performed.We could observe that attention-based CSEmodel (aCSE-1 and aCSE-2) improves over o-TMB2011 TMB2012Model MAP P@30 MAP P@30BOW 0.304 0.412 0.321 0.494LDA 0.281 0.409 0.311 0.486PV-DBOW 0.285 0.412 0.324 0.491PV-DM 0.327 0.431 0.340 0.524TWE 0.331 0.446?0.347?0.511CSE-1 0.337 0.451 0.344 0.512CSE-2 0.367 0.461 0.360 0.517aCSE-1 0.342 0.459 0.351 0.516aCSE-2 0.370??0.464??0.364??0.522?
?Table 2: Results of information retrieval.riginal CSE model (CSE-1 and CSE-2).
Howev-er, attention model promotes CSE-1 significant-ly, while aCSE-2 obtain similar results comparedto CSE-2, indicating that attention model leadsto small improvement for Skip-Gram based CSEmodel.
We argue that it is because Skip-Gram it-self gives less weight to the distant words by sam-pling less from those words, which is essentiallysimilar to attention model somehow.5 ConclusionBy inducing concept information, the proposedconceptual sentence embedding maintains and en-hances the semantic information of sentence em-bedding.
Furthermore, we extend the proposedmodels by introducing attention model, which al-lows it to consider contextual words within thewindow in a non-uniform way while maintainingthe efficiency.
We compare them with differen-t algorithms, including bag-of-word models, topicmodel-based model and other state-of-the-art sen-tence embedding models.
The experimental re-sults demonstrate that the proposed method per-forms the best and shows improvement over thecompared methods, especially for short-texts.AcknowledgmentsThe work was supported by National Natural Sci-ence Foundation of China (Grant No.
61132009),National Basic Research Program of China (973Program, Grant No.
2013CB329303), and Na-tional Hi-Tech Research & Development Program(863 Pro-gram, Grant No.
2015AA015404).513ReferencesAmr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-van Narayanamurthy, and Alexander J. Smola.2012.
Scalable inference in latent variable model-s.
In International Conference on Web Search andWeb Data Mining, WSDM 2012, Seattle, Wa, Usa,February, pages 123?132.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
Eprint Arxiv.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Meeting of the Associationfor Computational Linguistics, pages 809?815.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Rongen Fan, Kaiwei Chang, Cho Jui Hsieh, XiangruiWang, and Chih Jen Lin.
2010.
Liblinear: A libraryfor large linear classification.
Journal of MachineLearning Research, 9(12):1871?1874.Zellig S. Harris.
1970.
Distributional Structure.Springer Netherlands.Posen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search usingclickthrough data.
In ACM International Confer-ence on Conference on Information and KnowledgeManagement, pages 2333?2338.Quoc V. Le and Tomas.
Mikolov.
2014.
Distributedrepresentations of sentences and documents.
EprintArxiv, 4:1188?1196.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In COLING.Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and MaosongSun.
2015.
Topical word embeddings.
In Twenty-Ninth AAAI Conference on Artificial Intelligence.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Effective approaches to attention-basedneural machine translation.
In EMNLP.Mingbo Ma, Liang Huang, Bing Xiang, and BowenZhou.
2015.
Dependency-based convolutional neu-ral networks for sentence embedding.
In Meeting ofthe Association for Computational Linguistics andthe International Joint Conference on Natural Lan-guage Processing.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
Computer Science.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-do, and Jeffrey Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
Advances in Neural Information ProcessingSystems, 26:3111?3119.Frederic Morin and Yoshua Bengio.
2005.
Hierar-chical probabilistic neural network language model.Aistats.Iadh Ounis, Craig MacDonald, Jimmy Lin, and IanSoboroff.
2011.
Overview of the trec-2011 mi-croblog track.H Palangi, L Deng, Y Shen, J Gao, X He, J Chen,X Song, and R Ward.
2015.
Deep sentence em-bedding using the long short term memory network:Analysis and application to information retrieval.Arxiv, 24(4):694?707.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning representations by back-propagating errors.
Nature, 323(6088):533?536.Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing.Gerard Salton and Michael J. Mcgill.
1986.
Introduc-tion to modern information retrieval.
McGraw-Hill,.Aliaksei Severyn, Alessandro Moschitti, Manos T-sagkias, Richard Berendsen, and Maarten De Rijke.2014.
A syntax-aware re-ranker for microblog re-trieval.
In SIGIR, pages 1067?1070.Alexander Smola and Shravan Narayanamurthy.
2010.An architecture for parallel topic models.
Proceed-ings of the Vldb Endowment, 3(1):703?710.Ian Soboroff, Iadh Ounis, Craig MacDonald, and Jim-my Lin.
2012.
Overview of the trec-2012 microblogtrack.
In TREC.Yangqiu Song, Haixun Wang, Zhongyuan Wang,Hongsong Li, and Weizhu Chen.
2011.
Short textconceptualization using a probabilistic knowledge-base.
In Proceedings of the Twenty-Second inter-national joint conference on Artificial Intelligence -Volume Volume Three, pages 2330?2336.Yangqiu Song, Shusen Wang, and Haixun Wang.
2015.Open domain short text conceptualization: a gener-ative + descriptive modeling approach.
In Proceed-ings of the 24th International Conference on Artifi-cial Intelligence.Ling Wang, Tsvetkov Yulia, Amir Silvio, FermandezRamon, Dyer Chris, Black Alan W, Trancoso Isabel,and Lin Chu-Cheng.
2015a.
Not all contexts arecreated equal: Better word representations with vari-able attention.
In Conference on Empirical Methodsin Natural Language Processing, pages 1367?1372.Mingxuan Wang, Zhengdong Lu, Hang Li, and QunLiu.
2015b.
Syntax-based deep matching of shorttexts.
Computer Science.514Zhongyuan Wang, Kejun Zhao, Haixun Wang, Xi-aofeng Meng, and Ji-Rong Wen.
2015c.
Queryunderstanding through knowledge-based conceptu-alization.
In Proceedings of the 24th InternationalConference on Artificial Intelligence.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
Towards universal paraphrastic sen-tence embeddings.
Computer Science.Wentao Wu, Hongsong Li, Haixun Wang, and Ken-ny Q. Zhu.
2012.
Probase: a probabilistic taxonomyfor text understanding.
In Proceedings of the 2012ACM SIGMOD International Conference on Man-agement of Data, pages 481?492.515
