Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 233?240, Vancouver, October 2005. c?2005 Association for Computational LinguisticsEffective Use of Prosody in Parsing Conversational SpeechJeremy G. Kahn?
Matthew Lease?Eugene Charniak?
Mark Johnson?
Mari Ostendorf?University of Washington, SSLI?
Brown University?
{jgk,mo}@ssli.ee.washington.edu {mlease,ec,mj}@cs.brown.eduAbstractWe identify a set of prosodic cues for parsing con-versational speech and show how such features canbe effectively incorporated into a statistical parsingmodel.
On the Switchboard corpus of conversa-tional speech, the system achieves improved parseaccuracy over a state-of-the-art system which usesonly lexical and syntactic features.
Since removalof edit regions is known to improve downstreamparse accuracy, we explore alternatives for edit de-tection and show that PCFGs are not competitivewith more specialized techniques.1 IntroductionFor more than a decade, the Penn Treebank?s WallStreet Journal corpus has served as a benchmark fordeveloping and evaluating statistical parsing tech-niques (Collins, 2000; Charniak and Johnson, 2005).While this common benchmark has served as a valu-able shared task for focusing community effort, ithas unfortunately led to the relative neglect of othergenres, particularly speech.
Parsed speech stands tobenefit from practically every application envisionedfor parsed text, including machine translation, infor-mation extraction, and language modeling.
In con-trast to text, however, speech (in particular, conver-sational speech) presents a distinct set of opportu-nities and challenges.
While new obstacles arisefrom the presence of speech repairs, the possibilityof word errors, and the absence of punctuation andsentence boundaries, speech also presents a tremen-dous opportunity to leverage multi-modal input, inthe form of acoustic or even visual cues.As a step in this direction, this paper identifies aset of useful prosodic features and describes howthey can be effectively incorporated into a statisti-cal parsing model, ignoring for now the problemof word errors.
Evaluated on the Switchboard cor-pus of conversational telephone speech (Graff andBird, 2000), our prosody-aware parser out-performsa state-of-the-art system that uses lexical and syntac-tic features only.
While we are not the first to employprosodic cues in a statistical parsing model, previousefforts (Gregory et al, 2004; Kahn et al, 2004) in-corporated these features as word tokens and therebysuffered from the side-effect of displacing words inthe n-gram models by the parser.
To avoid this prob-lem, we generate a set of candidate parses using anoff-the-shelf, k-best parser, and use prosodic (andother) features to rescore the candidate parses.Our system architecture combines earlier modelsproposed for parse reranking (Collins, 2000) andfiltering out edit regions (Charniak and Johnson,2001).
Detecting and removing edits prior to parsingis motivated by the claim that probabilistic context-free grammars (PCFGs) perform poorly at detect-ing edit regions.
We validate this claim empirically:two state-of-the-art PCFGs (Bikel, 2004; Charniakand Johnson, 2005) are both shown to perform sig-nificantly below a state-of-the-art edit detection sys-tem (Johnson et al, 2004).2 Previous WorkAs mentioned earlier, conversational speechpresents a different set of challenges and opportu-nities than encountered in parsing text.
This paperfocuses on the challenges associated with disfluen-cies (Sec.
2.1) and the opportunity of leveragingacoustic-prosodic cues at the sub-sentence level(Sec.
2.2).
Here, sentence segmentation is assumedto be known (though punctuation is not available);233. .
.
while I think,?
??
?Reparandum+ uh, I mean,?
??
?Editing phraseI know?
??
?Repairthat.
.
.Figure 1: The structure of a typical repair, with ?+?
indicating the interruption point.the impact of automatic segmentation is addressedin other work (Kahn et al, 2004).2.1 Speech Repairs and ParsingSpontaneous speech abounds with disfluencies suchas partial words, filled pauses (e.g., ?uh?, ?um?
),conversational fillers (e.g., ?you know?
), and par-enthetical asides.
One type of disfluency that hasproven particularly problematic for parsing is speechrepairs: when a speaker amends what he is sayingmid-sentence (see Figure 1).
Following the analy-sis of (Shriberg, 1994), a speech repair can be un-derstood as consisting of three parts: the reparan-dum (the material repaired), the editing phrase (thatis typically either empty or consists of a filler), andthe repair.
The point between the reparandum andthe editing phrase is referred to as the interruptionpoint (IP), and it is the point that may be acousti-cally marked.
We refer to the reparandum and edit-ing phrase together as an edit or edit region.
Speechrepairs are difficult to model with HMM or PCFGmodels, because these models can induce only linearor tree-structured dependencies between words.
Therelationship between reparandum and repair is quitedifferent: the repair is often a ?rough copy?
of thereparandum, using the same or very similar wordsin roughly the same order.
A language model char-acterizing this dependency with hidden stack opera-tions is proposed in (Heeman and Allen, 1999).Several parsing models have been proposed whichaccord special treatment to speech repairs.
Mostprior work has focused on handling disfluenciesand continued to rely on hand-annotated transcriptsthat include punctuation, case, and known sentenceboundaries (Hindle, 1983; Core and Schubert, 1999;Charniak and Johnson, 2001; Engel et al, 2002).Of particular mention is the analysis of the rela-tionship between speech repairs and parsing accu-racy presented by Charniak and Johnson (2001), asthis directly influenced our work.
They presentedevidence that improved edit detection (i.e.
detect-ing the reparandum and edit phrase) leads to betterparsing accuracy, showing a relative reduction in F -score error of 14% (2% absolute) between oracle andautomatic edit removal.
Thus, this work adopts theiredit detection preprocessing approach.
They havesubsequently presented an improved model for de-tecting edits (Johnson et al, 2004), and our resultshere complement their analysis of the edit detectionand parsing relationship, particularly with respect tothe limitations of PCFGs in edit detection.2.2 Prosody and parsingWhile spontaneous speech poses problems for pars-ing due to the presence of disfluencies and lack ofpunctuation, there is information in speech associ-ated with prosodic cues that can be taken advantageof in parsing.
Certainly, prosodic cues are usefulfor sentence segmentation (Liu et al, 2004), andthe quality of automatic segmentation can have asignificant impact on parser performance (Kahn etal., 2004).
There is also perceptual evidence thatprosody provides cues to human listeners that aidin syntactic disambiguation (Price et al, 1991), andthe most important of these cues seems to be theprosodic phrases (perceived groupings of words) orthe boundary events marking them.
However, theutility of sentence-internal prosody in parsing con-versational speech is not well established.Most early work on integrating prosody in parsingwas in the context of human-computer dialog sys-tems, where parsers typically operated on isolatedutterances.
The primary use of prosody was to ruleout candidate parses (Bear and Price, 1990; Batlineret al, 1996).
Since then, parsing has advanced con-siderably, and the use of statistical parsers makes thecandidate pruning benefits of prosody less impor-tant.
This raises the question of whether prosodyis useful for improving parsing accuracy for con-versational speech, apart from its use in sentence234Figure 2: System architectureboundary detection.
Extensions of Charniak andJohnson (2001) look at using quantized combina-tions of prosodic features as additional ?words?,similar to the use of punctuation in parsing writtentext (Gregory et al, 2004), but do not find that theprosodic features are useful.
It may be that with theshort ?sentences?
in spontaneous speech, sentence-internal prosody is rarely of use in parsing.
How-ever, in edit detection using a parsing model (John-son et al, 2004), posterior probabilities of automati-cally detected IPs based on prosodic cues (Liu et al,2004) are found to be useful.
The seeming discrep-ancy between results could be explained if prosodiccues to IPs are useful but not other sub-sentenceprosodic constituents.
Alternatively, it could be thatincluding a representation of prosodic features asterminals in (Gregory et al, 2004) displaces wordsin the parser n-gram model history.
Here, prosodicevent posteriors are used, with the goal of providinga more effective way of incorporating prosody thana word-like representation.3 Approach3.1 Overall architectureOur architecture, shown in Figure 2, combines theparse reranking framework of (Collins, 2000) withthe edit detection and parsing approach of (Charniakand Johnson, 2001).
The system operates as follows:1.
Edit words are identified and removed.2.
Each resulting string is parsed to produce a setof k candidate parses.3.
Edit words reinserted into the candidates witha new part-of-speech tag EW.
Consecutive se-quences of edit words are inserted as single, flatEDITED constituents.4.
Features (syntactic and/or prosodic) are ex-tracted for each candidate, i.e.
candidates areconverted to feature vector representation.5.
The candidates are rescored by the reranker toidentify the best parse.Use of Collins?
parse reranking model has severaladvantages for our work.
In addition to allowing usto incorporate prosody without blocking lexical de-pendencies, the discriminative model makes it rela-tively easy to experiment with a variety of prosodicfeatures, something which is considerably more dif-ficult to do directly with a typical PCFG parser.Our use of the Charniak-Johnson approach of sep-arately detecting disfluencies is motivated by theirresult that edit detection error degrades parser accu-racy, but we also include experiments that omit thisstep (forcing the PCFG to model the edits) and con-firm the practical benefit of separating responsibili-ties between the edit detection and parsing tasks.3.2 Baseline systemWe adopt an existing parser-reranker as our base-line (Charniak and Johnson, 2005).
The parsercomponent supports k-best parse generation, andthe reranker component is used to rescore candi-date parses proposed by the parser.
In detail, thereranker selects from the set of k candidates T ={t1, .
.
.
tk} the parse t?
?
T with the highest bracketF -score (in comparison with a hand-annotated ref-erence).
To accomplish this, a feature-extractor con-verts each candidate parse t ?
T into a vector ofreal-valued features f(t) = (f1(t), .
.
.
, fm(t)) (e.g.,the value fj(t) of the feature fj might be the num-ber of times a certain syntactic structure appears int).
The reranker training procedure associates eachfeature fj with a real-valued weight ?j , and ?
?f(t)(the dot product of the feature vector and the weightvector ?)
is a single scalar weight for each parse can-didate.
The reranker employs a maximum-entropyestimator that selects the ?
that minimizes the logloss of the highest bracket F -score parse t?
condi-tioned on T (together with a Gaussian regularizerto prevent overtraining).
Informally, ?
is chosen to235make high F -score parses as likely as possible un-der the (conditional) distribution defined by f and ?.As in (Collins, 2000), we generate training data forthe reranker by reparsing the training corpus, usingn ?
1 folds as training data to parse the n-th fold.The existing system also includes a feature extrac-tor that identifies interesting syntactic relationshipsnot included in the PCFG parsing model (but usedin the reranker).
These features are primarily relatedto non-local dependencies, including parallelism ofconjunctions, the number of terminals dominated bycoordinated structures, right-branching root-to-leaflength, lexical/functional head pairs, n-gram stylesibling relationships, etc.3.3 Prosodic FeaturesMost theories of prosody have a symbolic represen-tation for prosodic phrasing, where different combi-nations of acoustic cues (fundamental frequency, en-ergy, timing) combine to give categorical perceptualdifferences.
Our approach to integrating prosody inparsing is to use such symbolic boundary events, in-cluding prosodic break labels that build on linguisticnotions of intonational phrases and hesitation phe-nomena.
These events are predicted from a com-bination of continuous acoustic correlates, ratherthan using the acoustic features directly, becausethe intermediate representation simplifies trainingwith high-level (sparse) structures.
Just as phone-based acoustic models are useful in speech recogni-tion systems as an intermediate level between wordsand acoustic features (especially for characterizingunseen words), the small set of prosodic boundaryevents are used here to simplify modeling the inter-dependent set of continuous-valued acoustic cues re-lated to prosody.
However, also as in speech recog-nition, we use posterior probabilities of these eventsas features rather than making hard decisions aboutpresence vs. absence of a constituent boundary.In the past, the idea of using perceptual categorieshas been dismissed as impractical due to the highcost of hand annotation.
However, with advancesin weakly supervised learning, it is possible to trainprosodic event classifiers with only a small amountof hand-labeled data by leveraging information insyntactic parses of unlabeled data.
Our strategy issimilar to that proposed in (No?th et al, 2000), whichuses categorical labels defined in terms of syntacticstructure and pause duration.
However, their sys-tem?s category definitions are without reference tohuman perception, while we leverage learned re-lations between perceptual events and syntax withother acoustic cues, without predetermining the re-lation or requiring a direct coupling to syntax.More specifically, we represent three classes ofprosodic boundaries (or, breaks): major intonationalphrase, hesitation, and all other word boundaries.1A small set of hand-labeled data from the treebankedportion of the Switchboard corpus (Ostendorf et al,2001) was used to train initial break prediction mod-els based on both parse and acoustic cues.
Next, thefull set of treebanked Switchboard data is used withan EM algorithm that iterates between: i) findingprobabilities of prosodic breaks in unlabeled databased on the current model, again using parse andacoustic features, and ii) re-estimating the model us-ing the probabilities as weighted counts.
Finally, anew acoustic-only break prediction model was de-signed from this larger data set for use in the parsingexperiments.In each stage, we use decision trees for models, inpart because of an interest in analyzing the prosody-syntax relationships learned.
The baseline systemtrained on hand-labeled data has error rates of 9.6%when all available cues are used (both syntax andprosody) and 16.7% when just acoustic and part-of-speech cues are provided (our target environment).Using weakly supervised (EM) training to incorpo-rate unannotated data led to a 15% reduction in errorrate to 14.2% for the target trees.
The final decisiontree was used to generate posteriors for each of thethree classes, one for each word in a sentence.
?From perceptual studies and decision tree analy-ses, we know that major prosodic breaks tend to co-occur with major clauses, and that hesitations oftenoccur in edit regions or at high perplexity points inthe word sequence.
To represent the co-occurrenceas a feature for use in parse reranking, we treatthe prosodic break posteriors as weighted counts inaccumulating the number of constituents in parset of type i with break type j at their right edge,which (with some normalization and binning) be-comes feature fij .
Note that the unweighted count1The intonational phrase corresponds to a break of ?4?
in theToBI labeling system (Pitrelli et al, 1994), and a hesitation ismarked with the ?p?
diacritic.236for constituent i corresponds directly to a featurein the baseline set, but the baseline set of featuresalso includes semantic information via associationwith specific words.
Here, we simply use syntacticconstituents.
It is also known that major prosodicbreaks tend to be associated with longer syntacticconstituents, so we used the weighted count strategywith length-related features as well.
In all, the vari-ous attributes associated with prosodic break countswere the constituent label of the subtree, its length(in words), its height (maximal distance from theconstituent root to any leaf), and the depth of therightmost word (distance from the right word to thesubtree root).
For each type in each of these cate-gories, there are three prosodic features, correspond-ing to the three break types.3.4 Edit detectionTo provide a competitive baseline for our parsingexperiments, we used an off-the-shelf, state-of-the-art TAG-based model as our primary edit detec-tor (Johnson et al, 2004).2 This also provided us acompetitive benchmark for contrasting the accuracyof PCFGs on the edit detection task (Section 4.2).Whereas the crossing-dependencies inherent inspeech repairs makes them difficult to model us-ing HMM or PCFG approaches (Section 2.1), TreeAdjoining Grammars (TAGs) are capable of cap-turing these dependencies.
To model both thecrossed-dependencies of speech repairs and the lin-ear or tree-structured dependencies of non-repairedspeech, Johnson et al?s system applies the noisychannel paradigm: a PCFG language model definesa probability distribution over non-repaired speech,and a TAG is used to model the optional insertion ofedits.
The output of this noisy channel model is aset of candidate edits which are then reranked usinga max-ent model (similar to what is done here forparse reranking).
This reranking step enables incor-poration of features based on an earlier word-basedclassifier (Charniak and Johnson, 2001) in additionto output features of the TAG model.
Acoustic fea-tures are not yet incorporated.2We also evaluated another state-of-the-art edit detectionsystem (Liu et al, 2004) but found that it suffered from a mis-match between the current LDC specification of edits (LDC,2004) and that used in the treebank.4 Experimental design4.1 CorpusExperiments were carried out on conversationalspeech using the hand-annotated transcripts associ-ated with the Switchboard treebank (Graff and Bird,2000).
As was done in (Kahn et al, 2004), weresegmented the treebank?s sentences into V5-stylesentence-like units (SUs) (LDC, 2004), since our ul-timate goal was to be able to parse speech given au-tomatically detected boundaries.
Unfortunately, theoriginal transcription effort did not provide punctu-ation guidelines, and the Switchboard treebankingwas performed on the transcript unchanged, with-out reference to the audio.
As a result, the sentenceboundaries sometimes do not match human listenerdecisions using SU annotation guidelines, with dif-ferences mainly corresponding to treatment of dis-course markers and backchannels.
In the years sincethe original Switchboard annotation was performed,LDC has iteratively refined guidelines for annotatingSUs, and significant progress has been made in au-tomatically recovering SU boundaries annotated ac-cording to this standard (Liu et al, 2004).
To even-tually leverage this work, we have taken the Meteer-annotated SUs (Meteer et al, 1995), for which thereexists treebanked training data, and automaticallyadjusted them to be more like the V5 LDC stan-dard, and resegmented the Switchboard treebank ac-cordingly.
In cases where the original syntactic con-stituents span multiple SUs, we discard any con-stituents violating the SU boundary, and in the eventthat an SU spans a treebank sentence boundary, anew top-level constituent SUGROUP is inserted toproduce a proper tree (and evaluated like any otherconstituent in the gold tree).3 While this SU reseg-mentation makes it difficult to compare our experi-mental results to past work, we believe this is a nec-essary step towards developing a more realistic base-line for fully automated parsing of speech.In addition to resegmention, we removed all punc-tuation and case from the corpus to more closelyreflect the form of output available from a speechrecognizer.
We retained partial words for consis-3SU and treebank segments disagree at about 5% in each di-rection, due mostly to the analysis of discourse markers as con-junctions (sentences of >1 SU) and the separation of backchan-nels into separate treebank sentences (SUs of >1 sentence).237Table 1: Statistics on the Switchboard division used.Section Sides SUs WordsTrain 1,031 87,599 659,437Tune 126 13,147 103,500Test 128 8,726 61,313Total 1,285 109,472 824,250tency with other work (Liu et al, 2004; Johnson etal., 2004), although word fragments would not typ-ically be available from ASR.
Finally, of the 1300total conversation sides, we discarded 15 for whichwe did not have prosodic data.
Our corpus divisionstatistics are given in Table 1.
During development,experiments were carried out on the tune section; thetest section was reserved for a final test run.4.2 Experimental VariablesOur primary goal is to evaluate the extent to whichprosodic cues could augment and/or stand-in for lex-ical and syntactic features.
Correspondingly, wereport on using three flavors of feature extraction:syntactic and lexical features (Section 3.2), prosodicfeatures (Section 3.3), and both sets of features com-bined.
For all three conditions, the first-stage scorefor each parse (generated by the off-the-shelf k-bestparser) was always included as a feature.A second parameter varied in the experiments wasthe method of upstream edit detection employedprior to parsing: PCFG, TAG-based, and oracleknowledge of treebank edit annotations.
While ithad been claimed that PCFGs perform poorly as editdetectors (Charniak and Johnson, 2001), we couldnot find empirical evidence in the literature quan-tifying the severity of the problem.
Therefore, weevaluated two PCFGs (Bikel, 2004; Charniak andJohnson, 2005) on edit detection and compared theirperformance to a state-of-the-art TAG-based edit de-tection system (Johnson et al, 2004).
For this ex-periment, we evaluated edit detection accuracy on aper-word basis, where any tree terminal is consid-ered an edit word if and only if it is dominated byan EDITED constituent in the gold tree.
The PCFGswere trained on the train section of the treebank datawith the flattened edit regions included4 and then4Training on flattened EDITED nodes improved detection ac-curacy for both PCFGs: as much as 15% for Bikel-Collins.Table 2: Edit word detection performance for twoword-based PCFGs and the TAG-based edit detec-tor.
F -score and error are word-based measures.Edit Detector Edit F -score Edit ErrorBikel-Collins PCFG 65.3 62.1Charniak PCFG 65.8 59.9TAG-based 78.2 42.2Table 3: Parsing F -score of various feature and edit-detector combinations.PCFG TAG OracleEdit F (Table 2) 65.8 78.2 100.0Parser 1-best 84.4 85.0 86.9Prosodic feats 85.0 85.6 87.6Syntactic feats 85.9 86.4 88.4Combined feats 86.0 86.6 88.6Oracle-rate 92.6 93.2 95.2used to parse the test data.5 The TAG-based de-tector was trained on the same conversation sides,with its channel model trained on the Penn Treebankdisfluency-annotated files and its language modeltrained on trees with the EDITED nodes excised.
Asshown in Table 2, we did find that both PCFGs per-formed significantly below the TAG-based detector.5 ResultsIn evaluating parse accuracy, we adopt the relaxededited revision (Charniak and Johnson, 2001) to thestandard PARSEVAL metric, which penalizes sys-tems that get EDITED spans wrong, but does not pe-nalize disagreements in the attachment or internalstructure of edit regions.
This metric is based on theassumption that there is little reason to recover syn-tactic structure in regions of speech that have beenrepaired or restarted by the speaker.Table 3 shows the F -scores for the top-rankedparses after reranking, where the first-stage PCFGparser was run with a beam-size of 50.
The firstand last rows show lower and upper bounds, respec-tively, for reranked parsing accuracy on each editcondition.
As the oracle rate6 shows, there is con-5For the Charniak parser, edits were detected using only itsPCFG component in 1-best mode, not its 2nd stage reranker.6Oracle F uses the best parse in the 50-best list.238siderable room for improvement.
Statistical signif-icance was computed using a non-parametric shuf-fle test similar to that in (Bikel, 2004).
For TAGand oracle edit detection conditions, the improve-ment from using the combined features over eitherprosodic or syntactic features in isolation was sig-nificant (p < 0.005).
(For PCFG edit detection,p < 0.04.)
Similarly, for all three feature extractionconditions, the improvement from using the TAG-based edit detector instead of the PCFG edit detectorwas also significant (p < 0.001).
Interestingly, theTAG?s 34% reduction in edit detection error relativeto the PCFG yielded only about 23% of the parseaccuracy differential between the PCFG and oracleconditions.
Nevertheless, there remains a promising2.0% difference in parse F -score between the TAGand oracle detection conditions to be realized by fur-ther improvements in edit detection.
Training forthe syntactic feature condition resulted in a learnedweight ?
with approximately 50K features, whilethe prosodic features used only about 1300 features.Despite this difference in the length of the ?
vectors,the prosodic feature condition achieved 40?50% ofthe improvement of the syntactic features.In some situations, e.g.
for language modeling,improving the ordering and weights of the entireparse set (an not just the top ranked parse) is im-portant.
To illustrate the overall improvement of thereranked order, in Table 4 we report the reranked-oracle rate over the top s parses, varying the beam s.The error for each feature condition, relative to usingthe PCFG parser in isolation, is shown in Figure 3.Both the table and figure show that the rerankedbeam achieves a consistent trend in parse accuracyimprovement relative to the PCFG beam, similar towhat is demonstrated by the 1-best scores (Table 3).Table 4: Reranked-oracle rate parse F -score for thetop s parses with reference edit detection.s 1 2 3 5 10 25PCFG 86.9 89.8 91.0 92.2 93.4 94.6Pros.
87.6 90.3 91.5 92.7 93.9 94.8Syn.
88.4 91.3 92.4 93.4 94.3 95.0Comb.
88.6 91.5 92.5 93.5 94.4 95.0Figure 3: Reduction in error (Error = 1?F ) for thes-best reranked-oracle relative to the parser-only or-acle, for different feature rerankings (reference editdetection).6 ConclusionThis study shows that incorporating prosodic infor-mation into the parse selection process, along withnon-local syntactic information, leads to improvedparsing accuracy on accurate transcripts of conver-sational speech.
Gains are shown to be robust to dif-ficulties introduced by automatic edit detection and,in addition to improving the one-best performance,the overall ordering of the parse candidates is im-proved.
While the gains from combining prosodicand syntactic features are not additive, since theprosodic features incorporates some constituent-structure information, the combined gains are sig-nificant.
These results are consistent with related ex-periments with a different type of prosodically cuedevent, which showed that automatically detected IPsbased on prosodic cues (Liu et al, 2004) are usefulin the reranking stage of a TAG-based speech repairdetection system (Johnson et al, 2004).The experiments described here used automat-ically extracted prosodic features in combinationwith human-produced transcripts.
It is an open ques-tion as to whether the conclusions will hold for er-rorful ASR transcripts and automatically detectedSU boundaries.
However, there is reason to believethat relative gains from using prosody may be largerthan those observed here for reference transcripts239(though overall performance will degrade), based onprior work combining prosody and lexical cues todetect other language structures (Shriberg and Stol-cke, 2004).
While the prosody feature extraction de-pends on timing of the hypothesized word sequence,the acoustic cues are relatively robust to word errorsand the break model can be retrained on recognizeroutput to automatically learn to discount the lexicalevidence.
Furthermore, if parse reranking operateson the top N ASR hypotheses, the reranking pro-cedure can improve recognition outputs, as demon-strated in (Kahn, 2005) for syntactic features alone.Allowing for alternative SU hypotheses in rerankingmay also lead to improved SU segmentation.In addition to assessing the impact of prosodyin a fully automatic system, other avenues for fu-ture work include improving feature extraction.
Onecould combine IP and prosodic break features (sofar explored separately), find new combinations ofprosody and syntactic structure, and/or incorporateother prosodic events.
Finally, it may also be use-ful to integrate the prosodic events directly into thePCFG, in addition to their use in reranking.This work was supported by the NSF under grants DMS-0074276, IIS-0085940, IIS-0112432, IIS-0326276, and LIS-9721276.
Conclusions are those of the authors and do not nec-essarily reflect the views of the NSF.ReferencesA.
Batliner et al 1996.
Prosody, empty categories andparsing - a success story.
Proc.
ICSLP, pp.
1169-1172.J.
Bear and P. Price.
1990.
Prosody, syntax and parsing.Proc.
ACL, pp.
17-22.D.
Bikel.
2004.
On the Parameter Space of LexicalizedStatistical Parsing Models.
Ph.D. thesis, U. Penn.E.
Charniak and M. Johnson.
2001.
Edit detection andparsing for transcribed speech.
NAACL, pp.
118-126.E.
Charniak and M. Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.Proc.
ACL.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
Proc.
ICML, pp.
175-182.M.
Core and L. Schubert.
1999.
A syntactic frameworkfor speech repairs and other disruptions.
Proc.
ACL,pp.
413-420.D.
Engel, E. Charniak, and M. Johnson.
2002.
Parsingand disfluency placement.
Proc.
EMNLP, pp.
49-54.D.
Graff and S. Bird.
2000.
Many uses, many annota-tions for large speech corpora: Switchboard and TDTas case studies.
Proc.
LREC, pp.
427-433.M.
Gregory, M. Johnson, and E. Charniak.
2004.Sentence-internal prosody does not help parsing theway punctuation does.
Proc.
NAACL, pp.
81-88.P.
A. Heeman and J. F. Allen.
1999.
Speech repairs,intonational phrases, and discourse markers: Model-ing speaker?s utterances in spoken dialogue.
Compu-tational Linguistics, 25(4):527-571.D.
Hindle.
1983.
Deterministic parsing of syntactic non-fluencies.
Proc.
ACL, pp.
123-128.M.
Johnson, E. Charniak, and M. Lease.
2004.
An im-proved model for recognizing disfluencies in conver-sational speech.
Proc.
Rich Transcription Workshop.J.
G. Kahn, M. Ostendorf, and C. Chelba.
2004.
Pars-ing conversational speech using enhanced segmenta-tion.
Proc.
HLT-NAACL 2004, pp.
125-128.J.
G. Kahn.
2005.
Moving beyond the lexical layer inparsing conversational speech.
M.A.
thesis, U. Wash.LDC.
2004.
Simple metadata annotation specification.Tech.
report, Linguistic Data Consortium.
Availableat http://www.ldc.upenn.edu/Projects/MDE.Y.
Liu et al 2004.
The ICSI-SRI-UW metadata extrac-tion system.
Proc.
ICSLP, pp.
577-580.M.
Meteer, A. Taylor, R. MacIntyre, and R. Iyer.
1995.Dysfluency annotation stylebook for the switchboardcorpus.
Tech.
report, Linguistic Data Consortium.E.
No?th et al 2000.
Verbmobil: The use of prosody inthe linguistic components of a speech understandingsystem.
IEEE Trans.
SAP, 8(5):519-532.M.
Ostendorf et al 2001.
A prosodically labeleddatabase of spontaneous speech.
ISCA Workshop onProsody in Speech Recognition and Understanding,pp.
119-121, 10.J.
Pitrelli, M. Beckman, and J. Hirschberg.
1994.
Eval-uation of prosodic transcription labeling reliability inthe ToBI framework.
Proc.
ICSLP, pp.
123-126.P.
J.
Price et al 1991.
The use of prosody in syntacticdisambiguation.
JASA, 90(6):2956-2970, 12.E.
Shriberg.
1994.
Preliminaries to a Theory of SpeechDisfluencies.
Ph.D. thesis, U.C.
Berkeley.E.
Shriberg and A. Stolcke.
2004.
Prosody modelingfor automatic speech recognition and understanding.Mathematical Foundations of Speech and LanguageProcessing.
Springer-Verlag, pp.
105-114.240
