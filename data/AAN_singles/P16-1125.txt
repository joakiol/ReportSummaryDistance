Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1319?1329,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLarger-Context Language Modelling with Recurrent Neural Network?Tian WangCenter for Data ScienceNew York Universityt.wang@nyu.eduKyunghyun ChoCourant Institute of Mathematical Sciencesand Center for Data ScienceNew York Universitykyunghyun.cho@nyu.eduAbstractIn this work, we propose a novel method toincorporate corpus-level discourse infor-mation into language modelling.
We callthis larger-context language model.
We in-troduce a late fusion approach to a recur-rent language model based on long short-term memory units (LSTM), which helpsthe LSTM unit keep intra-sentence depen-dencies and inter-sentence dependenciesseparate from each other.
Through theevaluation on four corpora (IMDB, BBC,Penn TreeBank, and Fil9), we demonstratethat the proposed model improves per-plexity significantly.
In the experiments,we evaluate the proposed approach whilevarying the number of context sentencesand observe that the proposed late fusionis superior to the usual way of incorporat-ing additional inputs to the LSTM.
By an-alyzing the trained larger-context languagemodel, we discover that content words, in-cluding nouns, adjectives and verbs, bene-fit most from an increasing number of con-text sentences.
This analysis suggests thatlarger-context language model improvesthe unconditional language model by cap-turing the theme of a document better andmore easily.1 IntroductionThe goal of language modelling is to estimate theprobability distribution of various linguistic units,e.g., words, sentences (Rosenfeld, 2000).
Amongthe earliest techniques were count-based n-gramlanguage models which intend to assign the prob-ability distribution of a given word observed af-?Recently, (Ji et al, 2015) independently proposed a sim-ilar approach.ter a fixed number of previous words.
Later Ben-gio et al (2003) proposed feed-forward neurallanguage model, which achieved substantial im-provements in perplexity over count-based lan-guage models.
Bengio et al showed that this neu-ral language model could simultaneously learn theconditional probability of the latest word in a se-quence as well as a vector representation for eachword in a predefined vocabulary.Recently recurrent neural networks have be-come one of the most widely used models in lan-guage modelling (Mikolov et al, 2010).
Longshort-term memory unit (LSTM, Hochreiter andSchmidhuber, 1997) is one of the most commonrecurrent activation function.
Architecturally, thememory state and output state are explicitly sep-arated by activation gates such that the vanish-ing gradient and exploding gradient problems de-scribed in Bengio et al (1994) is avoided.
Moti-vated by such gated model, a number of variantsof RNNs (e.g.
Cho et al (GRU, 2014b), Chunget al (GF-RNN, 2015)) have been designed to eas-ily capture long-term dependencies.When modelling a corpus, these language mod-els assume the mutual independence among sen-tences, and the task is often reduced to as-signing a probability to a single sentence.
Inthis work, we propose a method to incorporatecorpus-level discourse dependency into neural lan-guage model.
We call this larger-context lan-guage model.
It models the influence of con-text by defining a conditional probability in theform of P (wn|w1:n?1, S), where w1, ..., wnarewords from the same sentence, and S representsthe context which consists a number of previoussentences of arbitrary length.We evaluated our model on four different cor-pora (IMDB, BBC, Penn TreeBank, and Fil9).Our experiments demonstrate that the proposedlarger-context language model improve perplex-1319ity for sentences, significantly reducing per-wordperplexity compared to the language models with-out context information.
Further, through Part-Of-Speech tag analysis, we discovered that contentwords, including nouns, adjectives and verbs, ben-efit the most from increasing number of contextsentences.
Such discovery led us to the conclu-sion that larger-context language model improvesthe unconditional language model by capturing thetheme of a document.To achieve such improvement, we proposed alate fusion approach, which is a modification tothe LSTM such that it better incorporates the dis-course context from preceding sentences.
In theexperiments, we evaluated the proposed approachagainst early fusion approach with various num-bers of context sentences, and demonstrated thelate fusion is superior to the early fusion approach.Our model explores another aspect of context-dependent recurrent language model.
It is novelin that it also provides an insightful way to feedinformation into LSTM unit, which could benefitall encoder-decoder based applications.2 Statistical Language Modelling withRecurrent Neural NetworkGiven a document D = (S1, S2, .
.
.
, SL) whichconsists of L sentences, statistical language mod-elling aims at computing its probability P (D).It is often assumed that each sentence in thewhole document is mutually independent fromeach other:P (D) ?L?l=1P (Sl).
(1)We call this probability (before approximation) acorpus-level probability.
Under this assumption ofmutual independence among sentences, the task oflanguage modelling is often reduced to assigninga probability to a single sentence P (Sl).A sentence Sl= (w1, w2, .
.
.
, wTl) is avariable-length sequence of words or tokens.
Byassuming that a word at any location in a sentenceis largely predictable by preceding words, we canrewrite the sentence probability intoP (S) =Tl?t=1p(wt|w<t), (2)where w<tdenotes all the preceding words.
Wecall this a sentence-level probability.This rewritten probability expression can be ei-ther directly modelled by a recurrent neural net-work (Mikolov et al, 2010) or further approxi-mated as a product of n-gram conditional proba-bilities such thatP (S) ?Tl?t=1p(wt|wt?1t?
(n?1)), (3)where wt?1t?
(n?1)= (wt?
(n?1), .
.
.
, wt?1).
Thelatter is called n-gram language modelling.A recurrent language model is composed of twofunctions?transition and output functions.
Thetransition function reads one word wtand updatesits hidden state such thatht= ?
(wt,ht?1) , (4)where h0is an all-zero vector.
?
is a recurrentactivation function.
For more details on widely-used recurrent activation units, we refer the readerto (Jozefowicz et al, 2015; Greff et al, 2015).At each timestep, the output function computesthe probability over all possible next words in thevocabulary V .
This is done byp(wt+1= w?|wt1) ?
exp (gw?
(ht)) .
(5)g is commonly an affine transformation:g(ht) = Woht+ bo,where Wo?
R|V |?dand bo?
R|V |.The whole model is trained by maximizing thelog-likelihood of a training corpus often usingstochastic gradient descent with backpropagationthrough time (see, e.g., Rumelhart et al, 1988).This conventional approach to statistical lan-guage modelling often treats every sentence in adocument to be independent from each other Thisis often due to the fact that downstream tasks, suchas speech recognition and machine translation, aredone sentence-wise.
In this paper, we ask howstrong an assumption this is, how much impact thisassumption has on the final language model qual-ity and how much gain language modelling can getby making this assumption less strong.Long Short-Term Memory Here let us brieflydescribe a long short-term memory unit which iswidely used as a recurrent activation function ?
(see Eq.
(4)) for language modelling (see, e.g.,Graves, 2013).1320A layer of long short-term memory (LSTM)unit consists of three gates and a single memorycell.
They are computed byit=?
(Wixt+Uiht?1+ bi)ot=?
(Woxt+Uoht?1+ bo)ft=?
(Wfxt+Ufht?1+ bf) ,where ?
is a sigmoid function.
xtis the input attime t. The memory cell is computed byct= ftct?1+ ittanh (Wcx+Ucht?1+ bc) ,where  is an element-wise multiplication.
Thisadaptive leaky integration of the memory cell al-lows the LSTM to easily capture long-term depen-dencies in the input sequence.The output, or the activation of this LSTM layer,is then computed by ht= ottanh(ct).3 Larger-Context Language ModellingIn this paper, we aim not at improving thesentence-level probability estimation P (S) (seeEq.
(2)) but at improving the corpus-level prob-ability P (D) from Eq.
(1) directly.
One thing wenoticed at the beginning of this work is that it is notnecessary for us to make the assumption of mutualindependence of sentences in a corpus.
Rather,similarly to how we model a sentence probability,we can loosen this assumption byP (D) ?L?l=1P (Sl|Sl?1l?n), (6)where Sl?1l?n= (Sl?n, Sl?n+1, .
.
.
, Sl?1).
n de-cides on how many preceding sentences each con-ditional sentence probability conditions on, sim-ilarly to what happens with a usual n-gram lan-guage modelling.From the statistical modelling?s perspective, es-timating the corpus-level language probability inEq.
(6) is equivalent to build a statistical modelthat approximatesP (Sl|Sl?1l?n) =Tl?t=1p(wt|w<t, Sl?1l?n), (7)similarly to Eq.
(2).
One major difference from theexisting approaches to statistical language mod-elling is that now each conditional probability ofa next word is conditioned not only on the preced-ing words in the same sentence, but also on then?
1 preceding sentences.A conventional, count-based n-gram languagemodel is not well-suited due to the issue of datasparsity.
In other words, the number of rows in thetable storing n-gram statistics will explode as thenumber of possible sentence combinations growsexponentially with respect to both the vocabularysize, each sentence?s length and the number ofcontext sentences.Either neural or recurrent language modellinghowever does not suffer from this issue of datasparsity.
This makes these models ideal for mod-elling the larger-context sentence probability inEq.
(7).
More specifically, we are interested inadapting the recurrent language model for this.In doing so, we answer two questions in thefollowing subsections.
First, there is a questionof how we should represent the context sentencesSl?1l?n.
We consider two possibilities in this work.Second, there is a large freedom in how we build arecurrent activation function to be conditioned onthe context sentences.
We also consider two alter-natives in this case.3.1 Context RepresentationA sequence of preceding sentences can be repre-sented in many different ways.
Here, let us de-scribe two alternatives we test in the experiments.The first representation is to simply bag all thewords in the preceding sentences into a single vec-tor s ?
[0, 1]|V |.
Any element of s correspondingto the word that exists in one of the preceding sen-tences will be assigned the frequency of that word,and otherwise 0.
This vector is multiplied fromleft by a matrix P which is tuned together with allthe other parameters: p = Ps.
We call this repre-sentation p a bag-of-words (BoW) context.Second, we try to represent the preceding con-text sentences as a sequence of bag-of-words.Each bag-of-word sjis the bag-of-word represen-tation of the j-th context sentence, and they are putinto a sequence (sl?n, .
.
.
, sl?1).
Unlike the firstBoW context, this allows us to incorporate the or-der of the preceding context sentences.This sequence of BoW vectors are read bya recurrent neural network which is separatelyfrom the one used for modelling a sentence (seeEq.
(4).)
We use LSTM units as recurrent acti-vations, and for each context sentence in the se-quence, we get zt= ?
(xt, zt?1) , for t = l ?n, .
.
.
, l ?
1.
We set the last hidden state zl?1ofthis context recurrent neural network as the con-1321text vector p.Attention-based Context Representation Thesequence of BoW vectors can be used in a bit dif-ferent way from the above.
Instead of a unidi-rectional recurrent neural network, we first use abidirectional recurrent neural network to read thesequence.
The forward recurrent neural networkreads the sequence as usual in a forward direction,and the reverse recurrent neural network in the op-posite direction.
The hidden states from these twonetworks are then concatenated for each contextsentence in order to form a sequence of annotationvectors (zl?n, .
.
.
, zl?1).Unlike the other approaches, in this case, thecontext vector p differs for each word wtin thecurrent sentence, and we denote it by pt.
The con-text vector ptfor the t-th word is computed as theweighted sum of the annotation vectors:pt=l?1?l?=l?n?t,l?zl?,where the attention weight ?t,l?is computed by?t,l?=exp score (zl?,ht)?l?1k=l?nexp score (zk,ht).htis the hidden state of the recurrent languagemodel of the current sentence from Eq.
(5).
Thescoring function score(zl?,ht) returns a relevancescore of the l?-th context sentence w.r.t.
ht.3.2 Conditional LSTMEarly Fusion Once the context vector p is com-puted from the n preceding sentences, we need tofeed this into the sentence-level recurrent languagemodel.
One most straightforward way is to simplyconsider it as an input at every time step such thatx = E>wt+Wpp,where E is the word embedding matrix that trans-forms the one-hot vector of the t-th word into acontinuous word vector.
We call this approach anearly fusion of the context.Late Fusion In addition to this approach, wepropose here a modification to the LSTM suchthat it better incorporates the context from the pre-ceding sentences (summarized by pt.)
The ba-sic idea is to keep dependencies within the sen-tence being modelled (intra-sentence dependen-cies) and those between the preceding sentences(a) Early Fusion(b) Late FusionFigure 1: Proposed fusion methodsand the current sent (inter-sentence dependencies)separately from each other.We let the memory cell ctof the LSTM tomodel intra-sentence dependencies.
This simplymeans that there is no change to the existing for-mulation of the LSTM.The inter-sentence dependencies are reflectedon the interaction between the memory cell ct,which models intra-sentence dependencies, andthe context vector p, which summarizes the n pre-ceding sentences.
We model this by first comput-ing the amount of influence of the preceding con-text sentences asrt= ?
(Wr(Wpp) +Wrct+ br) .This vector rtcontrols the strength of each of theelements in the context vector p. This amountof influence from the n preceding sentences isdecided based on the currently captured intra-sentence dependency structures and the precedingsentences.This controlled context vector rt(Wpp) isused to compute the output of the LSTM layer:ht= ottanh (ct+ rt(Wpp)) .This is illustrated in Fig.
1 (b).We call this approach a late fusion, as the ef-fect of the preceding context is fused together with1322the intra-sentence dependency structure in the laterstage of the recurrent activation.Late fusion is a simple, but effective way tomitigate the issue of vanishing gradient in corpus-level language modelling.
By letting the contextrepresentation flow without having to pass throughsaturating nonlinear activation functions, it pro-vides a linear path through which the gradient forthe context flows easily.4 Related WorkContext-dependent Language Model Thispossibility of extending a neural or recurrentlanguage modeling to incorporate larger contextwas explored earlier.
Especially, (Mikolov andZweig, 2012) proposed an approach, calledcontext-dependent recurrent neural networklanguage model, very similar to the proposedapproach here.
The basic idea of their approachis to use a topic distribution, represented as avector of probabilities, of previous n words whencomputing the hidden state of the recurrent neuralnetwork each time.There are three major differences in the pro-posed approach from the work by Mikolov andZweig (2012).
First, the goal in this work isto explicitly model preceding sentences to bet-ter approximate the corpus-level probability (seeEq.
(6)) rather than to get a better context of thecurrent sentence.
Second, Mikolov and Zweig(2012) use an external method, such as latentDirichlet alocation (Blei et al, 2003) or latent se-mantics analysis (Dumais, 2004) to extract a fea-ture vector, whereas we learn the whole model, in-cluding the context vector extraction, end-to-end.Third, we propose a late fusion approach whichis well suited for the LSTM units which have re-cently been widely adopted many works involv-ing language models (see, e.g., Sundermeyer et al,2015).
This late fusion is later shown to be supe-rior to the early fusion approach.Dialogue Modelling with Recurrent NeuralNetworks A more similar model to the pro-posed larger-context recurrent language model isa hierarchical recurrent encoder decoder (HRED)proposed recently by Serban et al (2015).
TheHRED consists of three recurrent neural networksto model a dialogue between two people from theperspective of one of them, to which we refer as aspeaker.
If we consider the last utterance of thespeaker, the HRED is a larger-context recurrentlanguage model with early fusion.Aside the fact that the ultimate goals differ (intheir case, dialogue modelling and in our case,document modelling), there are two technical dif-ferences.
First, they only test with the early fusionapproach.
We show later in the experiments thatthe proposed late fusion gives a better languagemodelling quality than the early fusion.
Second,we use a sequence of bag-of-words to represent thepreceding sentences, while the HRED a sequenceof sequences of words.
This allows the HRED topotentially better model the order of the words ineach preceding sentence, but it increases computa-tional complexity (one more recurrent neural net-work) and decreases statistical efficient (more pa-rameters with the same amount of data.
)Skip-Thought Vectors Perhaps the most simi-lar work is the skip-thought vector by Kiros et al(2015).
In their work, a recurrent neural networkis trained to read a current sentence, as a sequenceof words, and extract a so-called skip-thought vec-tor of the sentence.
There are two other recurrentneural networks which respectively model preced-ing and following sentences.
If we only con-sider the prediction of the following sentence, thenthis model becomes a larger-context recurrent lan-guage model which considers a single precedingsentence as a context.As with the other previous works we have dis-cussed so far, the major difference is in the ulti-mate goal of the model.
Kiros et al (2015) fullyfocused on using their model to extract a good,generic sentence vector, while in this paper weare focused on obtaining a good language model.There are less major technical differences.
First,the skip-thought vector model conditions only onthe immediate preceding sentence, while we ex-tend this to multiple preceding sentences.
Second,similarly to the previous works by Mikolov andZweig (2012), the skip-thought vector model onlyimplements early fusion.Neural Machine Translation Neural machinetranslation is another related approach (Forcadaand?Neco, 1997; Kalchbrenner and Blunsom,2013; Cho et al, 2014b; Sutskever et al, 2014;Bahdanau et al, 2014).
In neural machine transla-tion, often two recurrent neural networks are used.The first recurrent neural network, called an en-coder, reads a source sentence, represented as asequence of words in a source language, to forma context vector, or a set of context vectors.
The1323other recurrent neural network, called a decoder,then, models the target translation conditioned onthis source context.This is similar to the proposed larger-context re-current language model, if we consider the sourcesentence as a preceding sentence in a corpus.
Themajor difference is in the ultimate application, ma-chine translation vs. language modelling, andtechnically, the differences between neural ma-chine translation and the proposed larger-contextlanguage model are similar to those between theHRED and the larger-context language model.Context-Dependent Question-Answering Mod-els Context-dependent question-answering is atask in which a model is asked to answer a ques-tion based on the facts from a natural languageparagraph.
The question and answer are often for-mulated as filling in a missing word in a querysentence (Hermann et al, 2015; Hill et al, 2015).This task is closely related to the larger-contextlanguage model we proposed in this paper in thesense that its goal is to build a model to learnp(qk|q<k, q>k, D), (8)where qkis the missing k-th word in a query Q,and q<kand q>kare the context words from thequery.
D is the paragraph containing facts aboutthis query.
It is explicitly constructed so that thequery q does not appear in the paragraph D.It is easy to see the similarity between Eq.
(8)and one of the conditional probabilities in ther.h.s.
of Eq.
(7).
By replacing the context sen-tences Sl?1l?nin Eq.
(7) with D in Eq.
(8) and con-ditioning wton both the preceding and follow-ing words, we get a context-dependent question-answering model.
In other words, the pro-posed larger-context language model can be usedfor context-dependent question-answering, how-ever, with computational overhead.
The overheadcomes from the fact that for every possible answerthe conditional probability completed query sen-tence must be evaluated.5 Experimental Settings5.1 ModelsThere are six possible combinations of the pro-posed methods.
First, there are two ways of rep-resenting the context sentences; (1) bag-of-words(BoW) and (2) a sequence of bag-of-words (Se-qBoW), from Sec.
3.1.
There are two separateways to incorporate the SeqBoW; (1) with atten-tion mechanism (ATT) and (2) without it.
Then,there are two ways of feeding the context vectorinto the main recurrent language model (RLM);(1) early fusion (EF) and (2) late fusion (LF), fromSec.
3.2.
We will denote them by1.
RLM-BoW-EF-n2.
RLM-SeqBoW-EF-n3.
RLM-SeqBoW-ATT-EF-n4.
RLM-BoW-LF-n5.
RLM-SeqBoW-LF-n6.
RLM-SeqBoW-ATT-LF-nn denotes the number of preceding sentences tohave as a set of context sentences.
We test fourdifferent values of n; 1, 2, 4 and 8.As a baseline, we also train a recurrent languagemodel without any context information.
We referto this model by RLM.
Furthermore, we also re-port the result with the conventional, count-basedn-gram language model with the modified Kneser-Ney smoothing with KenLM (Heafield et al,2013).Each recurrent language model uses 1000LSTM units and is trained with Adadelta (Zeiler,2012) to maximize the log-likelihood; L(?)
=1K?Kk=1log p(Sk|Sk?1k?n).
We early-stop trainingbased on the validation log-likelihood and reportthe perplexity on the test set using the best modelaccording to the validation log-likelihood.We use only those sentences of length up to 50words when training a recurrent language modelfor the computational reason.
For KenLM, weused all available sentences in a training corpus.5.2 DatasetsWe evaluate the proposed larger-context languagemodel on three different corpora.
For detailedstatistics, see Table 1.IMDB Movie Reviews A set of movie reviewsis an ideal dataset to evaluate many differentsettings of the proposed larger-context languagemodels, because each review is highly likely of asingle theme (the movie under review.)
A set ofwords or the style of writing will be well deter-mined based on the preceding sentences.We use the IMDB Movie Review Corpus(IMDB) prepared by Maas et al (2011).1This cor-pus has 75k training reviews and 25k test reviews.1http://ai.stanford.edu/?amaas/data/sentiment/1324(a) IMDB (b) Penn Treebank(c) BBC (d) Fil9Figure 2: Corpus-level perplexity on (a) IMDB, (b) Penn Treebank, (c) BBC and (d) Fil9.
The count-based 5-gram language models with Kneser-Ney smoothing respectively resulted in the perplexities of110.20, 148, 127.32 and 65.21, and are not shown here.We use the 30k most frequent words in the trainingcorpus for recurrent language models.BBC Similarly to movie reviews, each new ar-ticle tends to convey a single theme.
We use theBBC corpus prepared by Greene and Cunningham(2006).2Unlike the IMDB corpus, this corpuscontains news articles which are almost alwayswritten in a formal style.
By evaluating the pro-posed approaches on both the IMDB and BBCcorpora, we can tell whether the benefits fromlarger context exist in both informal and formallanguages.
We use the 10k most frequent words inthe training corpus for recurrent language models.Both with the IMDB and BBC corpora, we didnot do any preprocessing other than tokenization.3Penn Treebank We evaluate a normal recurrentlanguage model, count-based n-gram languagemodel as well as the proposed RLM-BoW-EF-nand RLM-BoW-LF-n with varying n = 1, 2, 4, 8on the Penn Treebank Corpus.
We preprocess the2http://mlg.ucd.ie/datasets/bbc.html3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perlcorpus according to (Mikolov et al, 2011) and usea vocabulary of 10k words from the training cor-pus.Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney?s website.4We tok-enized the corpus and used the 44k most frequentwords in the training corpus for recurrent languagemodels.6 Results and AnalysisCorpus-level Perplexity We evaluated the mod-els, including all the proposed approaches (RLM-{BoW,SeqBoW}-{ATT,?
}-{EF,LF}-n), on theIMDB corpus.
In Fig.
2 (a), we see three ma-jor trends.
First, RLM-BoW, either with theearly fusion or late fusion, outperforms both thecount-based n-gram and recurrent language model(LSTM) regardless of the number of context sen-tences.
Second, the improvement grows as thenumber n of context sentences increases, and thisis most visible with the novel late fusion.
Lastly,4http://mattmahoney.net/dc/textdata1325(i)RLM-BoW-LF(ii)RLM-SeqBoW-ATT-LF(a) IMDB (b) BBC (b) Penn TreebankFigure 3: Perplexity per POS tag on the (a) IMDB, (b) BBC and (c) Penn Treebank corpora.we see that the RLM-SeqBoW does not workwell regardless of the fusion type (RLM-SeqBow-EF not shown), while the attention-based model(RLM-SeqBow-ATT) outperforms all the others.After observing that the late fusion clearlyoutperforms the early fusion, we evaluatedonly RLM-{BoW,SeqBoW}-{ATT}-LF-n?s onthe other two corpora.On the other two corpora, PTB and BBC,we observed a similar trend of RLM-SeqBoW-ATT-LF-n and RLM-BoW-LF-n outperformingthe two conventional language models, and thatthis trend strengthened as the number n of the con-text sentences grew.
We also observed again thatthe RLM-SeqBoW-ATT-LF outperforms RLM-SeqBoW-LF and RLM-BoW in almost all thecases.Observing the benefit of RLM-SeqBoW-ATT-LF, we evaluated only such model on Fil9 to val-idate its performance on large corpus.
Similar tothe results on all three previous corpora, we con-tinue to observe the advantage of RLM-SeqBoW-ATT-LF-n on Fil9 corpus.From these experiments, the benefit of allow-ing larger context to a recurrent language model isclear, however, with the right choice of the contextrepresentation (see Sec.
3.1) and the right mech-anism for feeding the context information to therecurrent language model (see Sec.
3.2.)
In theseexperiments, the sequence of bag-of-words repre-sentation with attention mechanism, together withthe late fusion was found to be the best choice inall four corpora.One possible explanation on the failure of theSeqBoW representation with a context recurrentneural network is that it is simply difficult for thecontext recurrent neural network to compress mul-tiple sentences into a single vector.
This difficultyin training a recurrent neural network to com-press a long sequence into a single vector has beenobserved earlier, for instance, in neural machinetranslation (Cho et al, 2014a).
Attention mech-anism, which was found to avoid this problemin machine translation (Bahdanau et al, 2014), isfound to solve this problem in our task as well.Perplexity per Part-of-Speech Tag Next, weattempted at discovering why the larger-contextrecurrent language model outperforms the uncon-ditional one.
In order to do so, we computed theperplexity per part-of-speech (POS) tag.We used the Stanford log-linear part-of-speechtagger (Stanford POS Tagger, Toutanova et al,2003) to tag each word of each sentence in the cor-pora.5We then computed the perplexity of eachword and averaged them for each tag type sepa-rately.
Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities ofthe ten most frequent tags (NN, IN, DT, JJ, RB,NNS, VBZ, VB, PRP, CC), of which we combinedNN and NNS into a new tag Noun and VB andVBZ into a new tag Verb.We show the results using the RLM-BoW-LF and RLM-SeqBoW-ATT-LF on three corpora?IMDB, BBC and Penn Treebank?
in Fig.
3.
Weobserve that the predictability, measured by theperplexity (negatively correlated), grows most fornouns (Noun) and adjectives (JJ) as the numberof context sentences increases.
They are followedby verbs (Verb).
In other words, nouns, adjec-5http://nlp.stanford.edu/software/tagger.shtml1326IMDB BBC Penn TreeBank Fil9# Sentences # Words # Sentences # Words # Sentences # Words # Sentences # WordsTraining 930,139 21M 37,207 890K 42,068 888K 6,619,098 115MValidation 152,987 3M 1,998 49K 3,370 70K 825,919 14MTest 151,987 3M 2,199 53K 3,761 79K 827,416 14MTable 1: Statistics of IMDB, BBC, Penn TreeBank and Fil9.tives and verbs are the ones which become morepredictable by a language model given more con-text.
We however noticed the relative degradationof quality in coordinating conjunctions (CC), de-terminers (DT) and personal pronouns (PRP).It is worthwhile to note that nouns, adjectivesand verbs are open-class, content, words, and con-junctions, determiners and pronouns are closed-class, function, words (see, e.g., Miller, 1999).The functions words often play grammatical roles,while the content words convey the content of asentence or discourse, as the name indicates.
Fromthis, we may carefully conclude that the larger-context language model improves upon the con-ventional, unconditional language model by cap-turing the theme of a document, which is reflectedby the improved perplexity on ?content-heavy?open-class words (Chung and Pennebaker, 2007).In our experiments, this came however at the ex-pense of slight degradation in the perplexity offunction words, as the model?s capacity stayedsame (though, it is not necessary.
)This observation is in line with a recent find-ing by Hill et al (2015).
They also observed sig-nificant gain in predicting open-class, or content,words when a question-answering model, includ-ing humans, was allowed larger context.7 ConclusionIn this paper, we proposed a method to improvelanguage model on corpus-level by incorporatinglarger context.
Using this model results in the im-provement in perplexity on the IMDB, BBC, PennTreebank and Fil9 corpora, validating the advan-tage of providing larger context to a recurrent lan-guage model.From our experiments, we found that the se-quence of bag-of-words with attention is betterthan bag-of-words for representing the contextsentences (see Sec.
3.1), and the late fusion isbetter than the early fusion for feeding the con-text vector into the main recurrent language model(see Sec.
3.2).
Our part-of-speech analysis re-vealed that content words, including nouns, adjec-tives and verbs, benefit most from an increasingnumber of context sentences.
This analysis sug-gests that larger-context language model improvesperplexity because it captures the theme of a doc-ument better and more easily.To explore the potential of such a model, thereare several aspects in which more research needsto be done.
First, the four datasets we used in thispaper are relatively small in the context of lan-guage modelling, therefore the proposed larger-context language model should be evaluated onlarger corpora.
Second, more analysis, beyond theone based on part-of-speech tags, should be con-ducted in order to better understand the advantageof such larger-context models.
Lastly, it is impor-tant to evaluate the impact of the proposed larger-context models in downstream tasks such as ma-chine translation and speech recognition.AcknowledgmentsThis work is done as a part of the course DS-GA1010-001 Independent Study in Data Science atthe Center for Data Science, New York Univer-sity.
KC thanks Facebook, Google (Google Fac-ulty Award 2016) and NVidia (GPU Center of Ex-cellence 2015?2016).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and YoshuaBengio.
2014.
Neural machine translation byjointly learning to align and translate.
arXivpreprint arXiv:1409.0473 .Yoshua Bengio, R?ejean Ducharme, Pascal Vin-cent, and Christian Janvin.
2003.
A neural prob-abilistic language model.
The Journal of Ma-chine Learning Research 3:1137?1155.Yoshua Bengio, Patrice Simard, and Paolo Fras-coni.
1994.
Learning long-term dependencieswith gradient descent is difficult.
Neural Net-works, IEEE Transactions on 5(2):157?166.David M Blei, Andrew Y Ng, and Michael I Jor-dan.
2003.
Latent dirichlet alocation.
the Jour-nal of machine Learning research 3:993?1022.1327Kyunghyun Cho, Bart van Merrienboer, DzmitryBahdanau, and Yoshua Bengio.
2014a.
Onthe properties of neural machine translation:Encoder-decoder approaches.
In Eighth Work-shop on Syntax, Semantics and Structure in Sta-tistical Translation (SSST-8).Kyunghyun Cho, Bart Van Merri?enboer, CaglarGulcehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2014b.Learning phrase representations using rnnencoder-decoder for statistical machine transla-tion.
arXiv preprint arXiv:1406.1078 .Cindy Chung and James W Pennebaker.
2007.
Thepsychological functions of function words.
So-cial communication pages 343?359.Junyoung Chung, Caglar Gulcehre, KyunghyunCho, and Yoshua Bengio.
2015.
Gated feedbackrecurrent neural networks.
In Proceedings ofthe 32nd International Conference on MachineLearning (ICML-15).
pages 2067?2075.Susan T Dumais.
2004.
Latent semantic analysis.Annual review of information science and tech-nology 38(1):188?230.Mikel L Forcada and Ram?on P?Neco.
1997.
Re-cursive hetero-associative memories for transla-tion.
In Biological and Artificial Computation:From Neuroscience to Technology, Springer,pages 453?462.Alex Graves.
2013.
Generating sequences withrecurrent neural networks.
arXiv preprintarXiv:1308.0850 .Derek Greene and P?adraig Cunningham.
2006.Practical solutions to the problem of diagonaldominance in kernel document clustering.
InProc.
23rd International Conference on Ma-chine learning (ICML?06).
ACM Press, pages377?384.Klaus Greff, Rupesh Kumar Srivastava, JanKoutn?
?k, Bas R Steunebrink, and J?urgenSchmidhuber.
2015.
Lstm: A search spaceodyssey.
arXiv preprint arXiv:1503.04069 .Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable mod-ified Kneser-Ney language model estimation.In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics.Sofia, Bulgaria, pages 690?696.Karl Moritz Hermann, Tom?a?s Ko?cisk`y, EdwardGrefenstette, Lasse Espeholt, Will Kay, MustafaSuleyman, and Phil Blunsom.
2015.
Teach-ing machines to read and comprehend.
arXivpreprint arXiv:1506.03340 .Felix Hill, Antoine Bordes, Sumit Chopra, andJason Weston.
2015.
The goldilocks prin-ciple: Reading children?s books with ex-plicit memory representations.
arXiv preprintarXiv:1511.02301 .Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation9(8):1735?1780.Yangfeng Ji, Trevor Cohn, Lingpeng Kong, ChrisDyer, and Jacob Eisenstein.
2015.
Docu-ment context language models.
arXiv preprintarXiv:1511.03962 .Rafal Jozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An empirical exploration ofrecurrent network architectures.
In Proceedingsof the 32nd International Conference on Ma-chine Learning (ICML-15).
pages 2342?2350.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In EMNLP.pages 1700?1709.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S Zemel, Antonio Torralba, Raquel Ur-tasun, and Sanja Fidler.
2015.
Skip-thought vec-tors.
arXiv preprint arXiv:1506.06726 .Andrew L Maas, Raymond E Daly, Peter TPham, Dan Huang, Andrew Y Ng, and Christo-pher Potts.
2011.
Learning word vectors forsentiment analysis.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies-Volume 1.
Association for Com-putational Linguistics, pages 142?150.Tomas Mikolov, Martin Karafi?at, Lukas Bur-get, Jan Cernock`y, and Sanjeev Khudanpur.2010.
Recurrent neural network based lan-guage model.
In INTERSPEECH 2010, 11thAnnual Conference of the International SpeechCommunication Association, Makuhari, Chiba,Japan, September 26-30, 2010. pages 1045?1048.Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget,Jan Honza?Cernock`y, and Sanjeev Khudanpur.2011.
Extensions of recurrent neural networklanguage model.
In Acoustics, Speech and Sig-nal Processing (ICASSP), 2011 IEEE Interna-tional Conference on.
IEEE, pages 5528?5531.1328Tomas Mikolov and Geoffrey Zweig.
2012.
Con-text dependent recurrent neural network lan-guage model.
In SLT .
pages 234?239.George A Miller.
1999.
On knowing a word.
An-nual Review of psychology 50(1):1?19.Ronald Rosenfeld.
2000.
Two decades of statis-tical language modeling: where do we go fromhere.
In Proceedings of the IEEE.David E Rumelhart, Geoffrey E Hinton, andRonald J Williams.
1988.
Learning represen-tations by back-propagating errors.
Cognitivemodeling 5:3.Iulian V Serban, Alessandro Sordoni, YoshuaBengio, Aaron Courville, and Joelle Pineau.2015.
Hierarchical neural network generativemodels for movie dialogues.
arXiv preprintarXiv:1507.04808 .Martin Sundermeyer, Hermann Ney, and RalfSchluter.
2015.
From feedforward to recur-rent lstm neural networks for language model-ing.
Audio, Speech, and Language Processing,IEEE/ACM Transactions on 23(3):517?529.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.2014.
Sequence to sequence learning with neu-ral networks.
In Advances in neural informationprocessing systems.
pages 3104?3112.Kristina Toutanova, Dan Klein, Christopher DManning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In Proceedings of the 2003Conference of the North American Chapter ofthe Association for Computational Linguisticson Human Language Technology-Volume 1.
As-sociation for Computational Linguistics, pages173?180.Matthew D Zeiler.
2012.
Adadelta: An adap-tive learning rate method.
arXiv preprintarXiv:1212.5701 .1329
