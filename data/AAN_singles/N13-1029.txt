Proceedings of NAACL-HLT 2013, pages 288?297,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsImproving Syntax-Augmented Machine Translation byCoarsening the Label SetGreg Hanneman and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ghannema,alavie}@cs.cmu.eduAbstractWe present a new variant of the Syntax-Augmented Machine Translation (SAMT) for-malism with a category-coarsening algorithmoriginally developed for tree-to-tree gram-mars.
We induce bilingual labels into theSAMT grammar, use them for category coars-ening, then project back to monolingual la-beling as in standard SAMT.
The result is a?collapsed?
grammar with the same expres-sive power and format as the original, butmany fewer nonterminal labels.
We show thatthe smaller label set provides improved trans-lation scores by 1.14 BLEU on two Chinese?English test sets while reducing the occur-rence of sparsity and ambiguity problemscommon to large label sets.1 IntroductionThe formulation of statistical machine translation interms of synchronous parsing has become both the-oretically and practically successful.
In a parsing-based MT formalism, synchronous context-freegrammar rules that match a source-language inputcan be hierarchically composed to produce a corre-sponding target-language output.
SCFG translationgrammars can be extracted automatically from data.While formally syntactic approaches with a singlegrammar nonterminal have often worked well (Chi-ang, 2007), the desire to exploit linguistic knowl-edge has motivated the use of translation grammarswith richer, linguistically syntactic nonterminal in-ventories (Galley et al 2004; Liu et al 2006; Lavieet al 2008; Liu et al 2009).Linguistically syntactic MT systems can derivetheir label sets, either monolingually or bilingually,from parallel corpora that have been annotated withsource- and/or target-side parse trees provided bya statistical parser.
The MT system may exactlyadopt the parser?s label set or modify it in some way.Larger label sets are able to represent more precise,fine-grained categories.
On the other hand, they alsoexacerbate a number of computational and modelingproblems by increasing grammar size, derivationalambiguity, and data sparsity.In this paper, we focus on the Syntax-AugmentedMT formalism (Zollmann and Venugopal, 2006), amonolingually labeled version of Hiero that can cre-ate up to 4000 ?extended?
category labels based onpairs of parse nodes.
We take a standard SAMTgrammar with target-side labels and extend its label-ing to a bilingual format (Zollmann, 2011).
We thencoarsen the bilingual labels following the ?label col-lapsing?
algorithm of Hanneman and Lavie (2011).This represents a novel extension of the tree-to-treecollapsing algorithm to the SAMT formalism.
Af-ter removing the source-side labels, we obtain a newSAMT grammar with coarser target-side labels thanthe original.Coarsened grammars provide improvement of upto 1.14 BLEU points over the baseline SAMT resultson two Chinese?English test sets; they also outper-form a Hiero baseline by up to 0.60 BLEU on oneof the sets.
Aside from improved translation quality,in analysis we find significant reductions in deriva-tional ambiguity and rule sparsity, two problems thatmake large nonterminal sets difficult to work with.Section 2 provides a survey of large syntax-based288MT label sets, their associated problems of deriva-tional ambiguity and rule sparsity, and previous at-tempts at addressing those problems.
The sectionalso summarizes the tree-to-tree label collapsing al-gorithm and the process of SAMT rule extraction.We then describe our method of label collapsing inSAMT grammars in Section 3.
Experimental resultsare presented in Section 4 and analyzed in Section5.
Finally, Section 6 offers some conclusions andavenues for future work.2 Background2.1 Working with Large Label SetsAside from the SAMT method of grammar extrac-tion, which we treat more fully in Section 2.3, sev-eral other lines of work have explored increasingthe nonterminal set for syntax-based MT.
Huang andKnight (2006), for example, augmented the standardPenn Treebank labels for English by adding lexi-calization to certain types of nodes.
Chiang (2010)and Zollmann (2011) worked with a bilingual exten-sion of SAMT that used its notion of ?extended cat-egories?
on both the source and target sides.
Takingstandard monolingual SAMT as a baseline, Baker etal.
(2012) developed a tagger to augment syntacticlabels with some semantically derived information.Ambati et al(2009) extracted tree-to-tree rules withsimilar extensions for sibling nodes, resulting againin a large number of labels.Extended categories allow for the extraction ofa larger number of rules, increasing coverage andtranslation performance over systems that are lim-ited to exact constituent matches only.
However,the gains in coverage come with a correspondingincrease in computational and modeling complexitydue to the larger label set involved.Derivational ambiguity ?
the condition of hav-ing multiple derivations for the same output string?
is a particular problem for parsing-based MT sys-tems.
The same phrase pair may be represented witha large number of different syntactic labels.
Fur-ther, new hierarchical rules are created by abstract-ing smaller phrase pairs out of larger ones; each ofthese substitutions must also be marked by a labelof some kind.
Keeping variantly labeled copies ofthe same rules fragments probabilities during gram-mar scoring and creates redundant hypotheses in thedecoder at run time.A complementary problem ?
when a desired ruleapplication is impossible because its labels do notmatch ?
has been variously identified as ?data spar-sity,?
the ?matching constraint,?
and ?rule sparsity?in the grammar.
It arises from the definition ofSCFG rule application: in order to compose tworules, the left-hand-side label of the smaller rulemust match a right-hand-side label in the larger ruleit is being plugged in to.
With large label sets, itbecomes less likely that two arbitrarily chosen rulescan compose, making the grammar less flexible forrepresenting new sentences.Previous research has attempted to address bothof these problems in different ways.
Preferencegrammars (Venugopal et al 2009) are a techniquefor reducing derivational ambiguity by summingscores over labeled variants of the same deriva-tion during decoding.
Chiang (2010) addressed rulesparsity by introducing a soft matching constraint:the decoder may pay a learned label-pair-specificpenalty for substituting a rule headed by one labelinto a substitution slot marked for another.
Combin-ing properties of both of the above methods, Huanget al(2010) modeled monolingual labels as distribu-tions over latent syntactic categories and calculatedsimilarity scores between them for rule composition.2.2 Label Collapsing in Tree-to-Tree RulesAiming to reduce both derivational ambiguity andrule sparsity, we previously presented a ?label col-lapsing?
algorithm for systems in which bilinguallabels are used (Hanneman and Lavie, 2011).
Itcoarsens the overall label set by clustering monolin-gual labels based on which labels they appear joinedwith in the other language.The label collapsing algorithm takes as its inputa set of SCFG rule instances extracted from a par-allel corpus.
Each time a tree-to-tree rule is ex-tracted, its left-hand side is a label of the form s::t,where s is a label from the source-language cate-gory set S and t is a label from the target-languagecategory set T .
Operationally, the joint label meansthat a source-side subtree rooted at s was the trans-lational equivalent of a target-side subtree rooted att in a parallel sentence.
Figure 1 shows several suchsubtrees, highlighted in grey and numbered.
Jointleft-hand-side labels for the collapsing algorithm,289Figure 1: Sample extraction of bilingual nonterminals forlabel collapsing.
Labels extracted from this tree pair in-clude VBD::VV and NP::AD.such as VBD::VV and NP::AD, can be assembledby matching co-numbered nodes.From the counts of the extracted rules, it is thusstraightforward to compute for all values of s andt the observed P (s | t) and P (t | s), the probabilityof one half of a joint nonterminal label appearingin the grammar given the other half.
In the figure,for example, P (JJ |NN) = 0.5.
The conditionalprobabilities accumulated over the whole grammargive rise to a simple L1 distance metric over any pairof monolingual labels:d(s1, s2) =?t?T|P (t | s1)?
P (t | s2)| (1)d(t1, t2) =?s?S|P (s | t1)?
P (s | t2)| (2)An agglomerative clustering algorithm then com-bines labels in a series of greedy iterations.
At eachstep, the algorithm finds the pair of labels that is cur-rently the closest together according to the distancemetrics of Equations (1) and (2), combines those twolabels into a new one, and updates the set of P (s | t)and P (t | s) values appropriately.
The choice of la-bel pair to collapse in each iteration can be expressedformally asargmin(si,sj)?S2,(tk,t`)?T 2{d(si, sj), d(tk, t`)} (3)That is, either a source label pair or a target label pairmay be chosen by the algorithm in each iteration.2.3 SAMT Rule ExtractionSAMT grammars pose a challenge to the label col-lapsing algorithm described above because their la-bel sets are usually monolingual.
The classic SAMTformulation (Zollmann and Venugopal, 2006) pro-duces a grammar labeled on the target side only.Nonterminal instances that exactly match a target-language syntactic constituent in a parallel sentenceare given labels of the form t. Labels of the formt1+t2 are assigned to nonterminals that span exactlytwo contiguous parse nodes.
Categorial grammar la-bels such as t1/t2 and t1\t2 are given to nontermi-nals that span an incomplete t1 constituent missinga t2 node to its right or left, respectively.
Any non-terminal that cannot be labeled by one of the abovethree schemes is assigned the default label X.Figure 2(a) shows the extraction of a VP-levelSAMT grammar rule from part of a parallel sen-tence.
At the word level, the smaller English phrasesupported each other (and its Chinese equivalent) isbeing abstracted as a nonterminal within the largerphrase supported each other in international affairs.The larger phrase corresponds to a parsed VP nodeon the target side; this will become the label ofthe extracted rule?s left-hand side.
Since the ab-stracted sub-phrase does not correspond to a singleconstituent, the SAMT labeling conventions assignit the label VBD+NP.
We can thus write the ex-tracted rule as:(4)While the SAMT label formats can be triviallyconverted into joint labels X::t, X::t1+t2, X::t1/t2,X::t1\t2, and X::X, they cannot be usefully fed intothe label collapsing algorithm because the necessaryconditional label probabilities are meaningless.
Toacquire meaningful source-side labels, we turn to a290(a) (b)Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax.bilingual SAMT extension used by Chiang (2010)and Zollmann (2011).
Both a source- and a target-side parse tree are used to extract rules from a par-allel sentence; two SAMT-style labels are workedout independently on each side for each nonterminalinstance, then packed into a joint label.
It is there-fore possible for a nonterminal instance to be labeleds::t, s1\s2::t, s1+s2::t1/t2, or various other combi-nations depending on what parse nodes the nonter-minal spans in each tree.Such a bilingually labeled rule is extracted in Fig-ure 2(b).
The target-side labels from Figure 2(a) arenow paired with source-side labels extracted from anadded Chinese parse tree.
In this case, the abstractedsub-phrase supported each other is given the jointlabel VP::VBD+NP, while the rule?s left-hand sidebecomes LCP+VP::VP.We implement bilingual SAMT grammar extrac-tion by modifying Thrax (Weese et al 2011), anopen-source, Hadoop-based framework for extract-ing standard SAMT grammars.
By default, Thraxcan produce grammars labeled either on the sourceor target side, but not both.
It also outputs rulesthat are already scored according to a user-specifiedset of translation model features, meaning that theraw rule counts needed to compute the label condi-tional probabilities P (s | t) and P (t | s) are not di-rectly available.
We implement a new subclass ofgrammar extractor with logic for independently la-beling both sides of an SAMT rule in order to get thenecessary bilingual labels; an adaptation to the exist-ing Thrax ?rarity?
feature provides the rule counts.3 Label Collapsing in SAMT RulesOur method of producing label-collapsed SAMTgrammars is shown graphically in Figure 3.We first obtain an SAMT grammar with bilinguallabels, together with the frequency count for eachrule, using the modified version of Thrax describedin Section 2.3.
The rules can be grouped accordingto the target-side label of their left-hand sides (Fig-ure 3(a)).The rule counts are then used to compute label-ing probabilities P (s | t) and P (t | s) over left-hand-side usages of each source label s and each targetlabel t. These are simple maximum-likelihood es-timates: if #(si, tj) represents the combined fre-quency counts of all rules with si::tj on the left-hand291(a) (b) (c) (d)Figure 3: Stages of preparing label-collapsed rules for SAMT grammars.
(a) SAMT rules with bilingual nonterminalsare extracted and collected based on their target left-hand sides.
(b) Probabiliites P (t | s) and P (s | s) are computed.
(c)Nonterminals are clustered according to the label collapsing algorithm.
(d) Source sides of nonterminals are removedto create a standard SAMT grammar.side, the source-given-target labeling probability is:P (si | tj) =#(si::tj)?t?T #(si::t)(5)The computation for target given source is analo-gous.
Each monolingual label can thus be repre-sented as a distribution over the labels it is alignedto in the opposite language (Figure 3(b)).Such distributions over labels are the input to thelabel-collapsing algorithm, as described in Section2.2.
As shown in Figure 3(c), the algorithm resultsin the original target-side labels being combined intodifferent groups, denoted in this case as new labelsCA and CB.
We run label collapsing for varyingnumbers of iterations to produce varying degrees ofcoarsened label sets.Given a mapping from original target-side labelsto collapsed groups, all nonterminals in the originalSAMT grammar are overwritten accordingly.
Thesource-side labels are dropped at this point: we usethem only for the purpose of label collapsing, but notin assembling or scoring the final grammar.
The re-sulting monolingual SAMT-style grammar with col-lapsed labels (Figure 3(d)) can now be scored andused for decoding in the usual way.For constructing a baseline SAMT grammar with-out label collapsing, we merely extract a bilingualgrammar as in the first step of Figure 3, immediatelyremove the source-side labels from it, and proceedto grammar scoring.All grammars are scored according to a set ofeight features.
For an SCFG rule with left-hand-sidelabel t, source right-hand side f , and target right-hand side e, they are:?
Standard maximum-likelihood phrasal transla-tion probabilities P (f | e) and P (e | f)?
Maximum-likelihood labeling probabilityP (t | f, e)?
Lexical translation probabilities Plex(f | e) andPlex(e | f), as calculated by Thrax?
Rarity scoreexp( 1c )?1exp(1)?1 for a rule with extractedcount c?
Binary indicator features that mark phrase pair(as opposed to hierarchical) rules and glue rulesScored grammars are filtered down to the sen-tence level, retaining only those rules whose source-side terminals match an individual tuning or testingsentence.
In addition to losslessly filtering gram-mars in this way, we also carry out two types oflossy pruning in order to reduce overall grammar292System Labels Rules Per Sent.SAMT 4181 69,401,006 48,444Collapse 1 913 64,596,618 35,004Collapse 2 131 60,526,479 24,510Collapse 3 72 58,483,310 20,445Hiero 1 36,538,657 7,738Table 1: Grammar statistics for different degrees of labelcollapsing: number of target-side labels, unique rules inthe whole grammar, and average number of pruned rulesafter filtering to individual sentences.size.
One pruning pass keeps only the 80 most fre-quently observed target right-hand sides for eachsource right-hand side.
A second pass globally re-moves hierarchical rules that were extracted fewerthan six times in the training data.4 ExperimentsWe conduct experiments on Chinese-to-English MT,using systems trained from the FBIS corpus of ap-proximately 302,000 parallel sentence pairs.
Weparse both sides of the training data with the Berke-ley parsers (Petrov and Klein, 2007) for Chineseand English.
The English side is lowercased afterparsing; the Chinese side is segmented beforehand.Unidirectional word alignments are obtained withGIZA++ (Och and Ney, 2003) and symmetrized, re-sulting in a parallel parsed corpus with Viterbi wordalignments for each sentence pair.
Our modified ver-sion of Thrax takes the parsed and aligned corpus asinput and returns a list of rules, which can then belabel-collapsed and scored as previously described.In Thrax, we retain most of the default settings forHiero- and SAMT-style grammars as specified in theextractor?s configuration file.
Inheriting from Hiero,we require the right-hand side of all rules to con-tain at least one pair of aligned terminals, no morethan two nonterminals, and no more than five termi-nals and nonterminal elements combined.
Nonter-minals are not allowed to be adjacent on the sourceside, and they may not contain unaligned boundarywords.
Rules themselves are not extracted from anyspan in the training data longer than 10 tokens.Our initial bilingual SAMT grammar uses 2699unique source-side labels and 4181 unique target-side labels, leading to the appearance of 29,088 jointbilingual labels in the rule set.
We provide the jointlabels (along with their counts) to the label collaps-ing algorithm, while we strip out the source-sidelabels to create the baseline SAMT grammar with4181 unique target-side labels.
Table 1 summarizeshow the number of target labels, unique extractedrules, and the average number of pruned rules avail-able per sentence change as the initial grammar islabel-collapsed to three progressively coarser de-grees.
Once the collapsing process has occurred ex-haustively, the original SAMT grammar becomes aHiero-format grammar with a single nonterminal.Each of the five grammars in Table 1 is used tobuild an MT system.
All systems are tuned and de-coded with cdec (Dyer et al 2010), an open-sourcedecoder for SCFG-based MT with arbitrary rule for-mats and nonterminal labels.
We tune the systemson the 1664-sentence NIST Open MT 2006 data set,optimizing towards the BLEU metric.
Our test setsare the NIST 2003 data set of 919 sentences and theNIST 2008 data set of 1357 sentences.
The tun-ing set and both test sets all have four English ref-erences.We evaluate systems on BLEU (Papineni et al2002), METEOR (Denkowski and Lavie, 2011), andTER (Snover et al 2006), as calculated in all threecases by MultEval version 0.5.0.1 These scores forthe MT ?03 test set are shown in Table 2, and thosefor the MT ?08 test set in Table 3, combined by Mult-Eval over three optimization runs on the tuning set.MultEval also implements statistical significancetesting between systems based on multiple optimizerruns and approximate randomization.
This process(Clark et al 2011) randomly swaps outputs betweensystems and estimates the probability that the ob-served score difference arose by chance.
We reportthese results in the tables as well for three MERTruns and a p-value of 0.05.
Systems that were judgedstatistically different from the SAMT baseline havetriangles in the appropriate ?Sig.
SAMT??
columns;systems judged different from the Hiero baselinehave triangles under the ?Sig.
Hiero??
columns.
Anup-triangle (N) indicates that the system was better,while a down-triangle (O) means that the baselinewas better.1https://github.com/jhclark/multeval293Metric Scores Sig.
SAMT?
Sig.
Hiero?System BLEU MET TER B M T B M TSAMT 31.18 30.64 61.02 O O OCollapse 1 31.42 31.31 60.95 N O OCollapse 2 31.90 31.73 60.98 N N O N OCollapse 3 32.32 31.75 60.54 N N N N OHiero 32.30 31.42 60.10 N N NTable 2: MT ?03 test set results.
The first section gives automatic metric scores; the remaining sections indicatewhether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.Metric Scores Sig.
SAMT?
Sig.
Hiero?System BLEU MET TER B M T B M TSAMT 22.10 24.94 63.78 O O OCollapse 1 23.01 26.03 63.35 N N N NCollapse 2 23.53 26.50 63.29 N N N N NCollapse 3 23.61 26.37 63.07 N N N N N NHiero 23.01 25.72 63.53 N N NTable 3: MT ?08 test set results.
The first section gives automatic metric scores; the remaining sections indicatewhether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.Figure 4: Extracted frequency of each target-side label, with labels arranged in order of decreasing frequency count.Note the log?log scale of the plot.2945 AnalysisTables 2 and 3 show that the coarsened grammarssignificantly improve translation performance overthe SAMT baseline.
This is especially true for the?Collapse 3?
setting of 72 labels, which scores 1.14BLEU higher on MT ?03 and 1.51 BLEU higher onMT ?08 than the uncollapsed system.On the easier MT ?03 set, label-collapsed systemsdo not generally outperform Hiero, although Col-lapse 3 achieves a statistical tie according to BLEU(+0.02) and a statistical improvement over Hiero ac-cording to METEOR (+0.33).
MT ?08 appears asa significantly harder test set: metric scores for allsystems are drastically lower, and we find approxi-mately 7% to 8% fewer phrase pair matches per sen-tence.
In this case the label-collapsed systems per-form better, with all three of them achieving statisti-cal significance over Hiero in at least one metric andstatistical ties in the other.
The coarsened systems?comparatively better performance on the harder testset suggests that the linguistic information encodedin multiple-nonterminal grammars helps the systemsmore accurately parse new types of input.Table 1 already showed at a global scale the strongeffect of label collapsing on reducing derivationalambiguity, as labeled variants of the same basicstructural rule were progressively combined.
Sincecategory coarsening is purely a relabeling operation,any reordering pattern implemented in the originalSAMT grammar still exists in the collapsed ver-sions; therefore, any reduction in the size of thegrammar is a reduction in variant labelings.
Figure4 shows this process in more detail for the baselineSAMT grammar and the three collapsed grammars.For each grammar, labels are arranged in decreas-ing order of extracted frequency, and the frequencycount of each label is plotted.
The long tail of rarecategories in the SAMT grammar (1950 labels seenfewer than 100 times each) is combined into a pro-gressively sharper distribution at each step.
Not onlyare there fewer rare labels, but these hard-to-modelcategories consume a proportionally smaller fractionof the total label set: from 47% in the baseline gram-mar down to 26% in Collapse 3.We find that label collapsing disproportionatelyaffects frequently extracted and hierarchical rulesover rarer rules and phrase pairs.
The 15.7% re-duction in total grammar size between the SAMTbaseline and the Collapse 3 system affects 18.0% ofthe hierarchical rules, but only 1.6% of the phrasepairs.
If rules are counted separately each time theymatch another source sentence, the average reduc-tion in size of a sentence-filtered grammar is 57.8%.Intuitively, hierarchical rules are more affected bylabel collapsing because phrase pairs do not havemany variant left-hand-side labels to begin with,while the same hierarchical rule pattern may be in-stantiated in the grammar by a large number of vari-ant labelings.
We can see this situation in more de-tail by counting variants of a particular set of rules.Labeled forms of the Hiero-style ruleX ?
[X1 X2] :: [the X2 of X1] (6)are among the most frequently used rules in all fiveof our systems.
The way they are treated by labelcollapsing thus has a strong impact on the results ofruntime decoding.In the SAMT baseline, Rule (6) appears in thegrammar with 221 different labels in the X1 nonter-minal slot, 53 labels for the X2 slot, and 90 choicesof left-hand side ?
a total of 1330 different label-ings all together.
More than three-fourths of thesevariants were extracted three times or fewer from thetraining data; even if they can be used in a test sen-tence, statistical features for such low-count rulesare poorly estimated.
During label collapsing, thenumber of labeled variations of Rule (6) drops from1330 to 325, to 96, and finally to 63 in the Collapse3 grammar.
There, the pattern is instantiated with 14possible X1 labels, five X2 labels, and three differentleft-hand sides.It is difficult to measure rule sparsity directly (i.e.to count the number of rules that are missing duringdecoding), but a reduction in rule sparsity betweensystems should be manifested as an increased num-ber of hierarchical rule applications.
Figure 5 showsthe average number of hierarchical rules applied persentence, distinguishing syntactic rules from gluerules, on both test sets.
The collapsed grammars al-low for approximately one additional syntactic ruleapplication per sentence compared to the SAMTbaseline, or three additional applications comparedto Hiero.
This shows an implicit reduction in miss-ing syntactic rules in the collapsed grammars.
In the295MT 2003 MT 2008Figure 5: Average number of hierarchical rules (both syntactic and glue rules) applied per sentence on each test set.glue rule columns, we note that label collapsing alsopromotes a shift away from generic glue rules, pos-sibly via the creation of more permissive ?
but stillmeaningfully labeled ?
syntactic rules.6 ConclusionWe demonstrated a viable technique for reducing thelabel set size in SAMT grammars by temporarily in-ducing bilingual syntax and using it in an existingtree-to-tree category coarsening algorithm.
In col-lapsing SAMT category labels, we were able to sig-nificantly improve translation quality while using agrammar less than half the size of the original.
Webelieve it is also more robust to test-set or domainvariation than a single-nonterminal Hiero grammar.Collapsed grammars confer practical benefits duringboth model estimation and runtime decoding.
Weshowed that, in particular, they suffer less from rulesparsity and derivational ambiguity problems thatare common to larger label sets.We can highlight two areas for potential improve-ments in future work.
In our current implementationof label collapsing, we indiscriminately allow eithersource labels or target labels to be collapsed at eachiteration of the algorithm (see Equation 3).
This isan intuitively sensible setting when collapsing bilin-gual labels, but it is perhaps less obviously so for amonolingually labeled system such as SAMT.
An al-ternative would be to collapse target-side labels only,leaving the source-side labels alone since they do notappear in the final grammar anyway.
In this case, thetarget labels would be represented and clustered asdistributions over a static set of latent categories.A larger area of future concern is the stoppingpoint of the collapsing algorithm.
In our previ-ous work (Hanneman and Lavie, 2011), we manu-ally identified iterations in our run of the algorithmwhere the L1 distance between the most recentlycollapsed label pair was markedly lower than theL1 difference of the pair in the previous iteration.Such an approach is more feasible in our previousruns of 120 iterations than in ours here of nearly2100, where it is not likely that three manually cho-sen stopping points represent the optimal collapsingresults.
In future work, we plan to work towards thedevelopment of an automatic stopping criterion, amore principled test for whether each successive it-eration of label collapsing provides some useful ben-efit to the underlying grammar.AcknowledgmentsThis research work was supported in part by com-puting resources provided by the NSF-sponsoredXSEDE program under grant TG-CCR110017.Thanks to Chris Dyer for providing the word-aligned and preprocessed corpus we used in our ex-periments.
We also thank the anonymous reviewersfor helpful comments and suggestions for analysis.ReferencesVamshi Ambati, Alon Lavie, and Jaime Carbonell.
2009.Extraction of syntactic translation models from paral-lel data using syntax from source and target languages.296In Proceedings of the 12th Machine Translation Sum-mit, pages 190?197, Ottawa, Canada, August.Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,Chris Callison-Burch, Nathaniel W. Filardo, ChristinePiatko, Lori Levin, and Scott Miller.
2012.
Use ofmodality and negation in semantically-informed syn-tactic MT.
Computational Linguistics, 38(2):411?438.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Crontrolling for optimizer insta-bility.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: ShortPapers, pages 176?181, Portland, OR, June.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic metric for reliable optimization and evalu-ation of machine translation systems.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 85?91, Edinburgh, United Kingdom, July.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the ACL 2010 System Demonstrations, pages7?12, Uppsala, Sweden, July.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InHLT-NAACL 2004: Main Proceedings, pages 273?280, Boston, MA, May.Greg Hanneman and Alon Lavie.
2011.
Automatic cate-gory label coarsening for syntax-based machine trans-lation.
In Proceedings of SSST-5: Fifth Workshop onSyntax, Semantics, and Structure in Statistical Trans-lation, pages 98?106, Portland, OR, June.Bryant Huang and Kevin Knight.
2006.
Relabeling syn-tax trees to improve syntax-based machine translationquality.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe ACL, pages 240?247, New York, NY, June.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 138?147, Cambridge, MA, October.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proceedings of the Second ACL Work-shop on Syntax and Structure in Statistical Transla-tion, pages 87?95, Columbus, OH, June.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the ACL, pages 609?616, Sydney, Aus-tralia, July.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of the 47th Annual Meeting of the ACL andthe Fourth IJCNLP of the AFNLP, pages 558?566,Suntec, Singapore, August.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eva-lution of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,July.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, NY, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of the Seventh Conference of the Associ-ation for Machine Translation in the Americas, pages223?231, Cambridge, MA, August.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: Soft-ening syntactic constraints to improve statistical ma-chine translation.
In Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the ACL, pages 236?244, Boulder, CO,June.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with the Thraxgrammar extractor.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 478?484, Edinburgh, United Kingdom, July.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, pages 138?141, New York, NY, June.Andreas Zollmann.
2011.
Learning Multiple-Nonterminal Synchronous Grammars for MachineTranslation.
Ph.D. thesis, Carnegie Mellon University.297
