Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsTowards a Better Understanding of Discourse:Integrating Multiple Discourse Annotation Perspectives Using UIMAClaudiu Miha?ila?
?, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,Paul Thompson?, Ioannis Korkontzelos and Sophia AnaniadouThe National Centre for Text Mining,School of Computer Science, The University of Manchester{mihailac,kontonag,batistar,thompsop,korkonti,ananiads}@cs.man.ac.ukAbstractThere exist various different discourse an-notation schemes that vary both in theperspectives of discourse structure consid-ered and the granularity of textual unitsthat are annotated.
Comparison and inte-gration of multiple schemes have the po-tential to provide enhanced information.However, the differing formats of cor-pora and tools that contain or producesuch schemes can be a barrier to theirintegration.
U-Compare is a graphical,UIMA-based workflow construction plat-form for combining interoperable natu-ral language processing (NLP) resources,without the need for programming skills.In this paper, we present an extensionof U-Compare that allows the easy com-parison, integration and visualisation ofresources that contain or output annota-tions based on multiple discourse anno-tation schemes.
The extension works byallowing the construction of parallel sub-workflows for each scheme within a singleU-Compare workflow.
The different typesof discourse annotations produced by eachsub-workflow can be either merged or vi-sualised side-by-side for comparison.
Wedemonstrate this new functionality by us-ing it to compare annotations belongingto two different approaches to discourseanalysis, namely discourse relations andfunctional discourse annotations.
Integrat-ing these different annotation types withinan interoperable environment allows us tostudy the correlations between differenttypes of discourse and report on the newinsights that this allows us to discover.
?The authors have contributed equally to the developmentof this work and production of the manuscript.1 IntroductionOver the past few years, there has been an increas-ing sophistication in the types of available naturallanguage processing (NLP) tools, with named en-tity recognisers being complemented by relationand event extraction systems.
Such relations andevents are not intended to be understood in isola-tion, but rather they are arranged to form a coher-ent discourse.
In order to carry out complex taskssuch as automatic summarisation to a high degreeof accuracy, it is important for systems to be ableto analyse the discourse structure of texts automat-ically.
To facilitate the development of such sys-tems, various textual corpora containing discourseannotations have been made available to the NLPcommunity.
However, there is a large amount ofvariability in the types of annotations containedwithin these corpora, since different perspectiveson discourse have led to the development of anumber of different annotation schemes.Corpora containing discourse-level annotationsusually treat the text as a sequence of coherent tex-tual zones (e.g., clauses and sentences).
One lineof research has been to identify which zones arelogically connected to each other, and to charac-terise these links through the assignment of dis-course relations.
There are variations in the com-plexity of the schemes used to annotate these dis-course relations.
For example, Rhetorical Struc-ture Theory (RST) (Mann and Thompson, 1988)defines 23 types of discourse relations that areused to structure the text into complex discoursetrees.
Whilst this scheme was used to enrich thePenn TreeBank (Carlson et al 2001), the PennDiscourse TreeBank (PDTB) (Prasad et al 2008)used another scheme to identify discourse rela-tions that hold between pairs of text spans.
It cate-gorises the relations into types such as ?causal?,?temporal?
and ?conditional?, which can be ei-ther explicit or implicit, depending on whether or79not they are represented in text using overt dis-course connectives.
In the biomedical domain, theBiomedical Discourse Relation Bank (BioDRB)(Prasad et al 2011) annotates a similar set of re-lation types, whilst BioCause focusses exclusivelyon causality (Miha?ila?
et al 2013).A second line of research does not aim to linktextual zones, but rather to classify them accord-ing to their specific function in the discourse.
Ex-amples of functional discourse annotations includewhether a particular zone asserts new informationinto the discourse or represents a speculation orhypothesis.
In scientific texts, knowing the typeof information that a zone represents (e.g., back-ground knowledge, hypothesis, experimental ob-servation, conclusion, etc.)
allows for automaticisolation of new knowledge claims (Sa?ndor and deWaard, 2012).
Several annotation schemes havebeen developed to classify textual zones accord-ing to their rhetorical status or general informa-tion content (Teufel et al 1999; Mizuta et al2006; Wilbur et al 2006; de Waard and Pan-der Maat, 2009; Liakata et al 2012a).
Relatedto these studies are efforts to capture informationrelating to discourse function at the level of events,i.e., structured representations of pieces of knowl-edge which, when identified, facilitate sophisti-cated semantic searching (Ananiadou et al 2010).Since there can be multiple events in a sentenceor clause, the identification of discourse informa-tion at the event level can allow for a more de-tailed analysis of discourse elements than is possi-ble when considering larger units of text.
Certainevent corpora such as ACE 2005 (Walker, 2006)and GENIA-MK (Thompson et al 2011) havebeen annotated with various types of functionaldiscourse information.It has previously been shown that consideringseveral functional discourse annotation schemes inparallel can be beneficial (Liakata et al 2012b),since each scheme offers a different perspective.For a common set of documents, the cited studyanalysed and compared functional discourse an-notations at different levels of textual granular-ity (i.e., sentences, clauses and events), showinghow the different schemes could complement eachother in order to lay the foundations for a possiblefuture harmonisation of the schemes.
The resultsof this analysis provide evidence that it would beuseful to carry out further such analyses involv-ing other such schemes, including an investiga-tion of how discourse relations and functional dis-course annotations could complement each other,e.g., which types of functional annotations occurwithin the arguments of discourse relations.
Thereare, however, certain barriers to carrying out suchan analysis.
For example, a comparison of an-notation schemes would ideally allow the differ-ent types of annotations to be visualised simul-taneously or seamlessly merged together.
How-ever, the fact that annotations in different corporaare encoded using different formats (e.g., stand-offor in-line) and different encoding schemes meansthat this can be problematic.A solution to the challenges introduced above isoffered by the Unstructured Information Manage-ment Architecture (UIMA) (Ferrucci and Lally,2004), which defines a common workflow meta-data format facilitating the straightforward combi-nation of NLP resources into a workflow.
Basedon the interoperability of the UIMA framework,numerous researchers distribute their own tools asUIMA-compliant components (Kano et al 2011;Baumgartner et al 2008; Hahn et al 2008;Savova et al 2010; Gurevych et al 2007; Raket al 2012b).
However, UIMA is only intendedto provide an abstract framework for the interop-erability of language resources, leaving the actualimplementation to third-party developers.
Hence,UIMA does not explicitly address interoperabilityissues of tools and corpora.U-Compare (Kano et al 2011) is a UIMA-based workflow construction platform that pro-vides a graphical user interface (GUI) via whichusers can rapidly create NLP pipelines using adrag-and-drop mechanism.
Conforming to UIMAstandards, U-Compare components and pipelinesare compatible with any UIMA application via acommon and sharable type system (i.e., a hier-archy of annotation types).
In defining this typesystem, U-Compare promotes interoperability oftools and corpora, by exhaustively modelling awide range of NLP data types (e.g., sentences, to-kens, part-of-speech tags, named entities).
Thistype system was recently extended to include dis-course annotations to model three discourse phe-nomena, namely causality, coreference and meta-knowledge (Batista-Navarro et al 2013).In this paper, we describe our extensions to U-Compare, supporting the integration and visuali-sation of resources annotated according to mul-tiple discourse annotation schemes.
Our method80decomposes pipelines into parallel sub-workflows,each linked to a different annotation scheme.The resulting annotations produced by each sub-workflow can be either merged within a singledocument or visualised in parallel views.2 Related workPrevious studies have shown the advantages ofcomparing and integrating different annotationschemes on a corpus of documents (Guo et al2010; Liakata et al 2010; Liakata et al 2012b).Guo et al(2010) compared three different dis-course annotation schemes applied to a corpusof biomedical abstracts on cancer risk assess-ment and concluded that two of the schemes pro-vide more fine-grained information than the otherscheme.
They also revealed a subsumption rela-tion between two schemes.
Such outcomes fromcomparing schemes are meaningful for users whowish to select the most appropriate scheme for an-notating their data.
Liakata et al(2012) under-line that different discourse annotation schemescapture different dimensions of discourse.
Hence,there might be complementary information acrossdifferent schemes.
Based on this hypothesis, theyprovide a comparison of three annotation schemes,namely CoreSC (Liakata et al 2012a), GENIA-MK (Thompson et al 2011) and DiscSeg (deWaard, 2007), on a corpus of three full-text pa-pers.
Their results showed that the categories inthe three schemes can complement each other.
Forexample, the values of the Certainty Level dimen-sion of the GENIA-MK scheme can be used to as-sign confidence values to the Conclusion, Result,Implication and Hypothesis categories of CoreSCand DiscSeg.
In contrast to previous studies, ourproposed approach automatically integrates mul-tiple annotation schemes.
The proposed mecha-nism allows users to easily compare, integrate andvisualise multiple discourse annotation schemesin an interoperable NLP infrastructure, i.e., U-Compare.There are currently a number of freely-availableNLP workflow infrastructures (Ferrucci and Lally,2004; Cunningham et al 2002; Scha?fer, 2006;Kano et al 2011; Grishman, 1996; Baumgartneret al 2008; Hahn et al 2008; Savova et al 2010;Gurevych et al 2007; Rak et al 2012b).
Mostof the available infrastructures support the devel-opment of standard NLP applications, e.g., part-of-speech tagging, deep parsing, chunking, namedentity recognition and several of them allow therepresentation and analysis of discourse phenom-ena (Kano et al 2011; Cunningham et al 2002;Savova et al 2010; Gurevych et al 2007).
How-ever, none of them has demonstrated the integra-tion of resources annotated according to multipleannotation schemes within a single NLP pipeline.GATE (Cunningham et al 2002) is an opensource NLP infrastructure that has been used forthe development of various language processingtasks.
It is packaged with an exhaustive numberof NLP components, including discourse analy-sis modules, e.g., coreference resolution.
Further-more, GATE offers a GUI environment and wrap-pers for UIMA-compliant components.
However,GATE implements a limited workflow manage-ment mechanism that does not support the execu-tion of parallel or nested workflows.
In addition tothis, GATE does not promote interoperability oflanguage resources since it does not define any hi-erarchy of NLP data types and components do notformally declare their input/output capabilities.In contrast to GATE, UIMA implements a moresophisticated workflow management mechanismthat supports the construction of both paralleland nested pipelines.
In this paper, we exploitthis mechanism to integrate multiple annotationschemes in NLP workflows.
cTAKES (Savovaet al 2010) and DKPro (Gurevych et al 2007)are two repositories containing UIMA-compliantcomponents that are tuned for the medical andgeneral domain, respectively.
However, both ofthese repositories support the representation ofonly one discourse phenomenon, i.e., coreference.Argo (Rak et al 2012a; Rak et al 2012b) is aweb-based platform that allows multiple branch-ing and merging of UIMA pipelines.
It incorpo-rates several U-Compare components and conse-quently, supports the U-Compare type system.3 A UIMA architecture for processingmultiple annotation schemesIn UIMA, a document, together with its associatedannotations, is represented as a standardised datastructure, namely the Common Analysis Struc-ture (CAS).
Each CAS can contain any numberof nested sub-CASes, i.e., Subjects of Analysis(Sofas), each of which can associate a differenttype of annotation with the input document.
Inthis paper, we employ this UIMA mechanism toallow the integration and comparison of multiple81Collection of DocumentsMulti-SofaReaderParallel Annotation Viewer Annotation MergerComparingSchemes Integrating SchemesComponentC_1Sofa S_1ComponentC_2Sofa S_2ComponentC_N-1Sofa S_N-1ComponentC_NSofa S_Nsub-workflowsFigure 1: Integrating annotations from multipleannotation schemes in UIMA workflowsannotation schemes in a single U-Compare work-flow.
Assume that we have a corpus of documentswhich has been annotated according to n differentschemes, S1, S2, ..., Sn?1, Sn.
Also, assume thatwe will use a library of m text analysis compo-nents, C1, C2, ..., Cm?1, Cm, to enrich the corpuswith further annotations.Our implemented architecture is illustrated inFigure 1.
Using multiple Sofas, we are able to splita UIMA workflow into parallel sub-workflows.Starting from a Multi-Sofa reader, we create nsub-workflows, i.e., Sofas, each of which is linkedto a particular scheme for a different annotationtype.
Each sub-workflow can then apply the anal-ysis components that are most suitable for pro-cessing the annotations from the correspondingscheme.U-Compare offers two different modes for visu-alising corpora that have been annotated accord-ing to multiple schemes.
In the comparison mode,the default annotation viewer is automatically splitto allow annotations from different schemes to bedisplayed side-by-side.
The second type of visu-alisation merges the annotations produced by theparallel sub-workflows into a single view.
Themost appropriate view may depend on the prefer-ences of the user and the task at hand, e.g., iden-tifying similarities, differences or complementaryinformation between different schemes.4 Application WorkflowsIn this section, we demonstrate two workflow ap-plications that integrate multiple discourse anno-tation schemes.
The first workflow exploits U-Compare?s comparison mode to visualise in par-allel functional discourse annotations from twoschemes, namely, CoreSC (Liakata et al 2012a)and GENIA-MK (Thompson et al 2011).
Thesecond application integrates functional discourseannotations in the ACE 2005 corpus with dis-course relations obtained by an automated tool.4.1 Visualising functional discourseannotations from different schemesThe purpose of this workflow application is to re-veal the different interpretations given by two dis-course annotation schemes applied to a biomed-ical corpus of three full-text papers (Liakata etal., 2012b).
The pipeline contains two read-ers that take as input the annotations (in theBioNLP Shared Task stand-off format) from thetwo schemes and map them to U-Compare?stype system.
In this way, the annotations be-come interoperable with existing components inU-Compare?s library.
U-Compare detects that theworkflow contains two annotation schemes andautomatically creates two parallel sub-workflowsas explained earlier.
Furthermore, we configurethe workflow to use the comparison mode.
There-fore, the annotation viewer will display the twodifferent types of annotations based on the inputschemes side-by-side.
Figure 2 illustrates the par-allel viewing of a document annotated accordingto both the CoreSC (left-hand side) and GENIA-MK (right-hand side) annotation schemes.
TheCoreSC scheme assigns a single category per sen-tence.
The main clause in the highlighted sen-tence on the left-hand side constitutes the hypoth-esis that transcription factors bind to exon-1.
Ac-cordingly, as can be confirmed from the annota-tion table on the far right-hand side of the figure,the (Hyp)othesis category has been assigned to thesentence.In the GENIA-MK corpus, the different piecesof information contained within the sentence havebeen separately annotated as structured events.One of these events corresponds to the hypothe-sis, but this is not the only information expressed:information about a previous experimental out-come from the authors, i.e., that exon1 is impli-cated in CCR3 transcription, is annotated as a sep-82Figure 2: Comparing discourse annotations schemes in U-Compare.
The pipeline uses two Sofas corre-sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.arate event.
Since functional discourse informa-tion is annotated directly at the event level in theGENIA-MK corpus, the bind event is consideredindependently from the other event as represent-ing an Analysis.
Furthermore, the word hypoth-esized is annotated as a cue for this categorisa-tion.
There are several ways in which the an-notations of the two schemes can be seen to becomplementary to each other.
For example, thefiner-grained categorisation of analytical informa-tion in the CoreSC scheme could help to determinethat the analytical bind event in the GENIA-MKcorpus specifically represents a hypothesis, ratherthan, e.g., a conclusion.
Conversely, the event-based annotation in the GENIA-MK corpus canhelp to determine exactly which part of the sen-tence represents the hypothesis.
Furthermore, thecue phrases annotated in the GENIA-MK corpuscould be used as additional features in a systemtrained to assign CoreSC categories.
Although inthis paper we illustrate only the visualisation ofdifferent types of functional discourse annotations,it is worth noting that U-Compare provides sup-port for further processing.
Firstly, unlike annota-tion platforms such as brat (Stenetorp et al 2012),U-Compare allows for analysis components to beintegrated into workflows in a straightforward anduser-interactive manner.
If, for example, it is of in-terest to determine the tokens (and the correspond-ing parts-of-speech) which frequently act as cuesin Analysis events, syntactic analysis components(e.g., tokenisers and POS taggers) can be incorpo-rated via a drag-and-drop mechanism.
Also, U-Compare allows the annotations to be saved in acomputable format using the provided Xmi WriterCAS Consumer component.
This facilitates fur-ther automatic comparison of annotations.4.2 Integrating discourse relations withfunctional discourse annotationsTo demonstrate the integration of annotations orig-inating from two completely different perspectiveson discourse, we have created a workflow thatmerges traditional discourse relations with func-tional discourse annotations in a general domaincorpus.
For this application, we used the ACE2005 corpus, which consists of 599 documentscoming from broadcast conversation, broadcastnews, conversational telephone speech, newswire,weblog and usenet newsgroups.
This corpuscontains event annotations which have been en-riched by attributes such as polarity (positive ornegative), modality (asserted or other), generic-ity (generic or specific) and tense (past, present,future or unspecified).
We treat the values ofthese attributes as functional discourse annota-tions, since they provide further insight into theinterpretation of the events.
We created a compo-nent that reads the event annotations in the corpusand maps them to U-Compare?s type system.To obtain discourse relation annotations (whichare not available in the ACE corpus) we em-ployed an end-to-end discourse parser trainedon the Penn Discourse TreeBank (Lin et al2012).
It outputs three general types of anno-tations, namely, explicit relations, non-explicitrelations and attribution spans.
Explicit rela-tions (i.e., those having overt discourse connec-tives) are further categorised into the following 16PDTB level-2 types: Asynchronous, Synchrony,Cause, Pragmatic cause, Contrast, Concession,Conjunction, Instantiation, Restatement, Alterna-tive, List, Condition, Pragmatic condition, Prag-matic contrast, Pragmatic concession and Excep-83Figure 3: Integrating different discourse annotation schemes in U-Compare.tion.
Non-explicit relations, on the other hand,consist of EntRel and NoRel types, in addition tothe same first 11 explicit types mentioned above.We created a workflow consisting of the ACEcorpus reader and the discourse parser (availablein U-Compare as a UIMA web service).
This al-lowed us to merge traditional discourse relationswith event-based functional discourse annotations,and to visualise them in the same document (Fig-ure 3).
Furthermore, with the addition of theXmi Writer CAS Consumer in the workflow, themerged annotations can be saved in a computableformat for further processing, allowing users toperform deeper analyses on the discourse annota-tions.
This workflow has enabled us to gain someinsights into the correlations between functionaldiscourse annotations and discourse relations.5 Correlations between discourserelations and functional discourseannotationsBased on the merged annotation format describedin the previous section, we computed cases inwhich at least one of the arguments of a discourserelation also contains an event.
Figure 4 is aheatmap depicting the correlations between differ-ent types of discourse relations and the attributevalues of ACE events that co-occur with these re-lations.
The darker the colour, the smaller the ratioof the given discourse relation co-occurring withthe specified ACE event attribute value.
For in-stance, the Cause relation co-occurs mostly withpositive events (over 95%) and the correspond-ing cell is a very light shade of green.
These arediscussed and exemplified below.
In the exam-ples, the following marking convention is used:discourse connectives are capitalised, whilst argu-ments are underlined.
Event triggers are shown inbold, and cues relating to functional discourse cat-egories are italicised.For all discourse relation types, at least 50% ofco-occurring events are assigned the specific valueof the Genericity attribute.
Specific events arethose that describe a specific occurrence or situ-ation, rather than a more generic situation.
In gen-eral, this high proportion of specific events is to beexpected.
The types of text contained within thecorpus, consisting largely of news and transcrip-tions of conversions, would be expected to intro-duce a large amount of information about specificevents.For two types of discourse relations, i.e.
Condi-tion and Concession, there are more or less equalnumbers of specific and generic events.
The na-ture of these relation types helps to explain theseproportions.
Conditional relations often describehow a particular, i.e., specific, situation will holdif some hypothetical situation is true.
Since hypo-thetical situations do not denote specific instances,they will usually be labelled as generic.
Con-cessions, meanwhile, usually describe how a spe-cific situation holds, even though another (moregeneric) situation would normally hold, that wouldbe inconsistent with this.
For the Instantiation re-lation category, it may once again be expected thatsimilar proportions of generic and specific eventswould co-occur within their arguments, since aninstantiation describes a specific instance of amore generic situation.
However, contrary to these84AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronousGenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecified010.50.250.75Genericity Modality Polarity Tense010.50.250.75010.50.250.75010.50.250.75Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-basedfunctional discourse categories.
A darker shade indicates a smaller percentage of instances of a discourserelation co-occurring with an event attribute.expectations, the ratio of specific to generic eventsis 3:1.
A reason for this is that discourse argu-ments corresponding to the description of a spe-cific instance may contain several different events,as illustrated in Example (1).
(1) Toefting has been convicted before.
In1999 he was given a 20-day suspended sentencefor assaulting a fan who berated him forplaying with German club Duisburg.In terms of the Modality attribute, most dis-course relations correlate with definite, assertedevents.
Simillarly to the Genericity attribute, thiscan be largely explained by the nature of the texts.However, there are two relation types, i.e., Condi-tion and Consession, which have particularly highproportions of co-occurring events whose modal-ity is other.
Events that are assigned this attributevalue correspond to those that are not described asthough they are real occurrences.
This includes,e.g., speculated or hypothetical events.
The factthat Condition relations are usually hypotheticalin nature explain why 76% of events that co-occurwith such relations are assigned the other valuefor the Modality attribute.
Example (2) illustratesa sentence containing this relation type.
(2) And I?ve said many times, IF we allagreed on everything, everybody would want tomarry Betty and we would really be in a mess,wouldn?t we, Bob.An even higher proportion of Concession re-lations co-occurs with events whose modality isother.
Example (3) helps to explain this.
In thefirst clause (the generic situation), the mention ofminimising civilian casualties is only described asan effort, rather than a definite situation.
The hedg-ing of this generic situation is necessary in order toconcede that the more specific situation describedin the second clause could actually be true, i.e.,that a large number of civilians have already beenkilled.
Due to the nature of news reporting, whichmay come from potentially unreliable sources, thekilled event in this second clause is also hedged,through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders haverepeatedly assured that every effort would bemade to minimize civilian casualties in thecurrent Iraq war, at least 130 Iraqi civilians havebeen reportedly killed since the war started fivedays ago.Almost 96% of events that co-occur with argu-ments of discourse relations have positive polarity.Indeed, for eight relation types, 100% of the cor-responding events are positive.
This can partly beexplained by the fact that, in texts reporting news,85there is an emphasis on reporting events that havehappened, rather than events that did not happen.It can, however, be noted that events that co-occurwith certain discourse relation types have a greaterlikelihood of having negative polarity.
These rela-tions include Contrast (9% of events having neg-ative polarity) and Cause (5% negative events).Contrasts can include comparisons of positive andnegative situations, as in Example (4), whilst forCauses, it can sometimes be relevant to state thata particular situation caused a specific event not totake place, as shown in Example (5).
(4) The message from the Israeli governmentis that its soldiers are not targeting journalists,BUT that journalists who travel to places wherethere could be live fire exchange betweenIsraeli forces and Palestinian gunmen have aresponsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-CAUSE of all these problems they?re havingnow.For most relation types, around 60% of their co-occurring events are annotated as describing pasttense situations.
This nature of newswire and con-versations mean that this is largely to be expected,since they normally report mainly on events thathave already happened.
The proportion of eventsassigned the future tense value is highest whenthey co-occur with discourse relations of type Al-ternative.
In this relation type, it is often the casethat one of the arguments describes a possible fu-ture alternative to a current situation, as the case inExample (6).
This possible information pattern forAlternative relations, where one of the argumentsrepresents a currently occurring situation, wouldalso help to explain why, even though very fewevents in general are annotated as present tense,almost 10% of events that co-occur with Alter-native relations describe events that are currentlyongoing.
As for events whose Tense value is un-specified, two of the most common discourse re-lation types with which they occur are Conditionand Concession.
As exemplified above, Conditionrelations are often hypothetical in nature, meaningthat no specific tense can be assigned.
The genericargument of a Concession relation can also remainunmarked for tense.
As in Example (3), it is notclear whether the effort to minimise civilian casu-alties has already been initiated, or will be initiatedin the future.
(6) Saddam wouldn?t be destroying missilesUNLESS he thought he was going to bedestroyed if he didn?t.6 ConclusionsGiven the level of variability in existing discourse-annotated corpora, it is meaningful for users toidentify the relative merits of different schemes.In this paper, we have presented an extension ofthe U-Compare infrastructure that facilitates thecomparison, integration and visualisation of doc-uments annotated according to different annota-tion schemes.
U-Compare constructs multiple andparallel annotation sub-workflows nested within asingle workflow, with each sub-workflow corre-sponding to a distinct scheme.
We have appliedthe implemented method to visualise the similar-ities and differences of two functional discourseannotation schemes, namely CoreSC and GENIA-MK.
To demonstrate the integration of multipleschemes in U-Compare, we developed a work-flow that merged event annotations from the ACE2005 corpus (which include certain types of func-tional discourse information) with discourse rela-tions obtained by an end-to-end parser.
Moreover,we have analysed the merged annotations obtainedby this workflow and this has allowed us to iden-tify various correlations between the two differenttypes of discourse annotations.Based on the intuition that there is comple-mentary information across different types of dis-course annotations, we intend to examine how theintegration of multiple discourse schemes, e.g.,features obtained by merging annotations, affectsthe performance of machine learners for discourseanalysis.7 AcknowledgementsWe are grateful to Dr. Ziheng Lin (Na-tional University of Singapore) for providing uswith the discourse parser used for this work.This work was partially funded by the Euro-pean Community?s Seventh Framework Program(FP7/2007-2013) [grant number 318736 (OSS-METER)]; Engineering and Physical Sciences Re-search Council [grant numbers EP/P505631/1,EP/J50032X/1]; and MRC Text Mining andScreening (MR/J005037/1).86ReferencesSophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, andDouglas B. Kell.
2010.
Event extraction for sys-tems biology by text mining the literature.
Trends inBiotechnology, 28(7):381 ?
390.Riza Theresa B. Batista-Navarro, Georgios Kontonat-sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,Raheel Nawaz, Ioannis Korkontzelos, and SophiaAnaniadou.
2013.
Facilitating the analysis of dis-course phenomena in an interoperable NLP plat-form.
In Computational Linguistics and IntelligentText Processing, volume 7816 of Lecture Notes inComputer Science, pages 559?571.
Springer BerlinHeidelberg, March.William A. Baumgartner, Kevin Bretonnel Cohen, andLawrence Hunter.
2008.
An open-source frame-work for large-scale, flexible evaluation of biomedi-cal text mining systems.
Journal of biomedical dis-covery and collaboration, 3:1+, January.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2001.
Building a discourse-tagged cor-pus in the framework of Rhetorical Structure Theory.In Proceedings of the Second SIGdial Workshop onDiscourse and Dialogue - Volume 16, SIGDIAL ?01,pages 1?10, Stroudsburg, PA, USA.
Association forComputational Linguistics.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:an architecture for development of robust HLTapplications.
In In Recent Advanced in LanguageProcessing, pages 168?175.Anita de Waard and Henk Pander Maat.
2009.
Epis-temic segment types in biology research articles.
InProceedings of the Workshop on Linguistic and Psy-cholinguistic Approaches to Text Structuring (LPTS2009).Anita de Waard.
2007.
A pragmatic structure for re-search articles.
In Proceedings of the 2nd inter-national conference on Pragmatic web, ICPW ?07,pages 83?89, New York, NY, USA.
ACM.David Ferrucci and Adam Lally.
2004.
Building anexample application with the unstructured informa-tion management architecture.
IBM Systems Jour-nal, 43(3):455?475.Ralph Grishman.
1996.
TIPSTER Text Phase II archi-tecture design version 2.1p 19 june 1996.
In Pro-ceedings of the TIPSTER Text Program: Phase II,pages 249?305, Vienna, Virginia, USA, May.
Asso-ciation for Computational Linguistics.Yufan Guo, Anna Korhonen, Maria Liakata,Ilona Silins Karolinska, Lin Sun, and Ulla Ste-nius.
2010.
Identifying the information structure ofscientific abstracts: An investigation of three differ-ent schemes.
In Proceedings of the 2010 Workshopon Biomedical Natural Language Processing, pages99?107.
Association for Computational Linguistics.Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.2007.
Darmstadt knowledge processing repositorybased on UIMA.
In Proceedings of the First Work-shop on Unstructured Information Management Ar-chitecture at Biannual Conference of the GSCL.Udo Hahn, Ekaterina Buyko, Rico Landefeld, MatthiasMu?hlhausen, Michael Poprat, Katrin Tomanek, andJoachim Wermter.
2008.
An overview of JCoRe,the JULIE lab UIMA component repository.
InLREC?08 Workshop ?Towards Enhanced Interoper-ability for Large HLT Systems: UIMA for NLP?,pages 1?7, Marrakech, Morocco, May.Yoshinobu Kano, Makoto Miwa, Kevin Cohen,Lawrence Hunter, Sophia Ananiadou, and Jun?ichiTsujii.
2011.
U-Compare: A modular NLP work-flow construction and evaluation system.
IBM Jour-nal of Research and Development, 55(3):11.Maria Liakata, Simone Teufel, Advaith Siddharthan,and Colin Batchelor.
2010.
Corpora for the concep-tualisation and zoning of scientific papers.
In Pro-ceedings of LREC, volume 10.Maria Liakata, Shyamasree Saha, Simon Dobnik,Colin Batchelor, and Dietrich Rebholz-Schuhmann.2012a.
Automatic recognition of conceptualizationzones in scientific articles and two life science appli-cations.
Bioinformatics, 28(7):991?1000.Maria Liakata, Paul Thompson, Anita de Waard, Ra-heel Nawaz, Henk Pander Maat, and Sophia Ana-niadou.
2012b.
A three-way perspective on scien-tic discourse annotation for knowledge extraction.In Proceedings of the ACL Workshop on DetectingStructure in Scholarly Discourse (DSSD), pages 37?46, July.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.
APDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, FirstView:1?34, 10.William C. Mann and Sandra A. Thompson.
1988.Rhetorical Structure Theory: Toward a functionaltheory of text organization.
Text, 8(3):243?281.Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, andSophia Ananiadou.
2013.
BioCause: Annotatingand analysing causality in the biomedical domain.BMC Bioinformatics, 14(1):2, January.Yoko Mizuta, Anna Korhonen, Tony Mullen, and NigelCollier.
2006.
Zone analysis in biology articles as abasis for information extraction.
International Jour-nal of Medical Informatics, 75(6):468 ?
487.
Re-cent Advances in Natural Language Processing forBiomedical Applications Special Issue.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and Bon-nie Webber.
2008.
The Penn Discourse Tree-Bank 2.0.
In Nicoletta Calzolari, Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios87Piperidis, and Daniel Tapias, editors, In Proceedingsof the 6th International Conference on language Re-sources and Evaluation (LREC), pages 2961?2968.Rashmi Prasad, Susan McRoy, Nadya Frid, AravindJoshi, and Hong Yu.
2011.
The biomedicaldiscourse relation bank.
BMC Bioinformatics,12(1):188.Rafal Rak, Andrew Rowley, and Sophia Ananiadou.2012a.
Collaborative development and evaluationof text-processing workflows in a UIMA-supportedweb-based workbench.Rafal Rak, Andrew Rowley, William Black, and SophiaAnaniadou.
2012b.
Argo: an integrative, in-teractive, text mining-based workbench supportingcuration.
Database: The Journal of BiologicalDatabases and Curation, 2012.A?gnes Sa?ndor and Anita de Waard.
2012.
Identifyingclaimed knowledge updates in biomedical researcharticles.
In Proceedings of the Workshop on De-tecting Structure in Scholarly Discourse, ACL ?12,pages 10?17, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Guergana Savova, James Masanz, Philip Ogren, Jiap-ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,and Christopher Chute.
2010.
Mayo clini-cal text analysis and knowledge extraction system(cTAKES): architecture, component evaluation andapplications.
Journal of the American Medical In-formatics Association, 17(5):507?513.Ulrich Scha?fer.
2006.
Middleware for creating andcombining multi-dimensional nlp markup.
In Pro-ceedings of the 5th Workshop on NLP and XML:Multi-Dimensional Markup in Natural LanguageProcessing, pages 81?84.
ACL.Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a web-based tool for NLP-assistedtext annotation.
In Proceedings of the Demonstra-tions Session at EACL 2012, Avignon, France, April.Association for Computational Linguistics.Simone Teufel, Jean Carletta, and Marc Moens.
1999.An annotation scheme for discourse-level argumen-tation in research articles.
In Proceedings of theninth conference on European chapter of the Asso-ciation for Computational Linguistics, EACL ?99,pages 110?117, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Paul Thompson, Raheel Nawaz, John McNaught, andSophia Ananiadou.
2011.
Enriching a biomedi-cal event corpus with meta-knowledge annotation.BMC Bioinformatics, 12(1):393.Christopher Walker.
2006.
ACE 2005 MultilingualTraining Corpus.W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.2006.
New directions in biomedical text annota-tion: definitions, guidelines and corpus construction.BMC Bioinformatics, 7(1):356.88
