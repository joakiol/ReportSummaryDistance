Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98?107,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBilingual Lexicon Generation Using Non-Aligned SignaturesDaphna ShezafInstitute of Computer ScienceHebrew University of Jerusalemdaphna.shezaf@mail.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractBilingual lexicons are fundamental re-sources.
Modern automated lexicon gen-eration methods usually require parallelcorpora, which are not available for mostlanguage pairs.
Lexicons can be gener-ated using non-parallel corpora or a pivotlanguage, but such lexicons are noisy.We present an algorithm for generatinga high quality lexicon from a noisy one,which only requires an independent cor-pus for each language.
Our algorithm in-troduces non-aligned signatures (NAS), across-lingual word context similarity scorethat avoids the over-constrained and inef-ficient nature of alignment-based methods.We use NAS to eliminate incorrect transla-tions from the generated lexicon.
We eval-uate our method by improving the qualityof noisy Spanish-Hebrew lexicons gener-ated from two pivot English lexicons.
Ouralgorithm substantially outperforms otherlexicon generation methods.1 IntroductionBilingual lexicons are useful for both end usersand computerized language processing tasks.They provide, for each source language word orphrase, a set of translations in the target language,and thus they are a basic component of dictio-naries, which also include syntactic information,sense division, usage examples, semantic fields,usage guidelines, etc.Traditionally, when bilingual lexicons are notcompiled manually, they are extracted from par-allel corpora.
However, for most language pairsparallel bilingual corpora either do not exist or areat best small and unrepresentative of the generallanguage.Bilingual lexicons can be generated using non-parallel corpora or pivot language lexicons (seeSection 2).
However, such lexicons are noisy.
Inthis paper we present a method for generating ahigh quality lexicon given such a noisy one.
Ourevaluation focuses on the pivot language case.Pivot language approaches deal with thescarcity of bilingual data for most language pairsby relying on the availability of bilingual data foreach of the languages in question with a third,pivot, language.
In practice, this third languageis often English.A naive method for pivot-based lexicon genera-tion goes as follows.
For each source headword1,take its translations to the pivot language using thesource-to-pivot lexicon, then for each such transla-tion take its translations to the target language us-ing the pivot-to-target lexicon.
This method yieldshighly noisy (?divergent?)
lexicons, because lexi-cons are generally intransitive.
This intransitivitystems from polysemy in the pivot language thatdoes not exist in the source language.
For ex-ample, take French-English-Spanish.
The Englishword spring is the translation of the French wordprintemps, but only in the season of year sense.Further translating spring into Spanish yields boththe correct translation primavera and an incorrectone, resorte (the elastic object).To cope with the issue of divergence due to lex-ical intransitivity, we present an algorithm for as-sessing the correctness of candidate translations.The algorithm is quite simple to understand andto implement and is computationally efficient.
Inspite of its simplicity, we are not aware of previouswork applying it to our problem.The algorithm utilizes two monolingual cor-pora, comparable in their domain but otherwiseunrelated, in the source and target languages.
Itdoes not need a pivot language corpus.
The al-gorithm comprises two stages: signature genera-1In this paper we focus on single word head entries.Multi-word expressions form a major topic in NLP and theirhandling is deferred to future work.98tion and signature ranking.
The signature of wordw is the set of words that co-occur with w moststrongly.
While co-occurrence scores are usedto compute signatures, signatures, unlike contextvectors, do not contain the score values.
Foreach given source headword we compute its sig-nature and the signatures of all of its candidatetranslations.
We present the non-aligned signa-tures (NAS) similarity score for signature and useit to rank these translations.
NAS is based on thenumber of headword signature words that may betranslated using the input noisy lexicon into wordsin the signature of a candidate translation.We evaluate our algorithm by generating abilingual lexicon for Hebrew and Spanish usingpivot Hebrew-English and English-Spanish lexi-cons compiled by a professional publishing house.We show that the algorithm outperforms exist-ing algorithms for handling divergence induced bylexical intransitivity.2 Previous Work2.1 Parallel CorporaParallel corpora are often used to infer word-oriented machine-readable bilingual lexicons.
Thetexts are aligned to each other, at chunk- and/orword-level.
Alignment is generally evaluated byconsistency (source words should be translated toa small number of target words over the entire cor-pus) and minimal shifting (in each occurrence, thesource should be aligned to a translation nearby).For a review of such methods see (Lopez, 2008).The limited availability of parallel corpora of suffi-cient size for most language pairs restricts the use-fulness of these methods.2.2 Pivot Language Without Corpora2.2.1 Inverse ConsultationTanaka and Umemura (1994) generated a bilin-gual lexicon using a pivot language.
They ap-proached lexical intransitivity divergence usingInverse Consultation (IC).
IC examines the inter-section of two pivot language sets: the set of pivottranslations of a source-language word w, and theset of pivot translations of each target-languageword that is a candidate for being a translationto w. IC generally requires that the intersectionset contains at least two words, which are syn-onyms.
For example, the intersection of the En-glish translations of French printemps and Spanishresorte contains only a single word, spring.
Theintersection for a correct translation pair printempsand primavera may include two synonym words,spring and springtime.
Variations of this methodwere proposed by (Kaji and Aizono, 1996; Bondet al, 2001; Paik et al, 2004; Ahn and Frampton,2006).One weakness of IC is that it relies on pivot lan-guage synonyms to identify correct translations.In the above example, if the relatively rare spring-time had not existed or was missing from the inputlexicons, IC would not have been able to discernthat primavera is a correct translation.
This mayresult in low recall.2.2.2 Multiple Pivot LanguagesMausam et al (2009) used many input bilinguallexicons to create bilingual lexicons for new lan-guage pairs.
They represent the multiple inputlexicons in a single undirected graph, with wordsfrom all the lexicons as nodes.
The input lexi-cons translation pairs define the edges in the graph.New translation pairs are inferred based on cyclesin the graph, that is, the existence of multiple pathsbetween two words in different languages.In a sense, this is a generalization of the pivotlanguage idea, where multiple pivots are used.
Inthe example above, if both English and Germanare used as pivots, printemps and primavera wouldbe accepted as correct because they are linked byboth English spring and German Fruehling, whileprintemps and resorte are not linked by any Ger-man pivot.
This multiple-pivot idea is similar toInverse Consultation in that multiple pivots are re-quired, but using multiple pivot languages frees itfrom the dependency on rich input lexicons thatcontain a variety of synonyms.
This is replaced,however, with the problem of coming up with mul-tiple suitable input lexicons.2.2.3 Micro-Structure of Dictionary EntriesDictionaries published by a single publishinghouse tend to partition the semantic fields of head-words in the same way.
Thus the first translationof some English headword in the English-Spanishand in the English-Hebrew dictionaries would cor-respond to the same sense of the headword, andwould therefore constitute translations of eachother.
The applicability of this method is lim-ited by the availability of machine-readable dic-tionaries produced by the same publishing house.Not surprisingly, this method has been proposedby lexicographers working in such companies (Sk-99oumalova, 2001).2.3 Cross-lingual Co-occurrences in LexiconConstructionRapp (1999) and Fung (1998) discussed seman-tic similarity estimation using cross-lingual con-text vector alignment.
Both works rely on apre-existing large (16-20K entries), correct, one-to-one lexicon between the source and targetlanguages, which is used to align context vec-tors between languages.
The context vectordata was extracted from comparable (monolingualbut domain-related) corpora.
Koehn and Knight(2002) were able to do without the initial large lex-icon by limiting themselves to related languagesthat share a writing system, and using identically-spelled words as context words.
Garera et al(2009) and Pekar et al (2006) suggested differentmethods for improving the context vectors data ineach language before aligning them.
Garera et al(2009) replaced the traditional window-based co-occurrence counting with dependency-tree basedcounting, while Pekar et al (2006) predicted miss-ing co-occurrence values based on similar wordsin the same language.
In the latter work, the one-to-one lexicon assumption was not made: whena context word had multiple equivalents, it wasmapped into all of them, with the original prob-ability equally distributed between them.Pivot Language.
Using cross-lingual co-occurrences to improve a lexicon generated usinga pivot language was suggested by Tanaka andIwasaki (1996).
Schafer and Yarowsky (2002)created lexicons between English and a targetlocal language (e.g.
Gujarati) using a relatedlanguage (e.g.
Hindi) as pivot.
An English pivotlexicon was used in conjunction with pivot-targetcognates.
Cross-lingual co-occurrences were usedto remove errors, together with other cues such asedit distance and Inverse Document Frequencies(IDF) scores.
It appears that this work assumed asingle alignment was possible from English to thetarget language.Kaji et al (2008) used a pivot English lexiconto generate initial Japanese-Chinese and Chinese-Japanese lexicons, then used co-occurrences in-formation, aligned using the initial lexicon, toidentify correct translations.
Unlike other works,which require alignments of pairs (i.e., two co-occurring words in one language translatable intotwo co-occurring words in the other), this methodrelies on alignments of 3-word cliques in eachlanguage, every pair of which frequently co-occurring.
This is a relatively rare occurrence,which may explain the low recall rates of their re-sults.3 AlgorithmOur algorithm transforms a noisy lexicon into ahigh quality one.
As explained above, in this paperwe focus on noisy lexicons generated using pivotlanguage lexicons.
Other methods for obtainingan initial noisy lexicon could be used as well; theirevaluation is deferred to future work.In the setting evaluated in this paper, we firstgenerate an initial noisy lexicon iLex possiblycontaining many translation candidates for eachsource headword.
iLex is computed from twopivot-language lexicons, and is the only place inwhich the algorithm utilizes the pivot language.Afterwards, for each source headword, we com-pute its signature and the signatures of each of itstranslation candidates.
Signature computation uti-lizes a monolingual corpus to discover the wordsthat are most strongly related to the word.
We nowrank the candidates according to the non-alignedsignatures (NAS) similarity score, which assessesthe similarity between each candidate?s signatureand that of the headword.
For each headword,we select the t translations with the highest NASscores as correct translations.3.1 Input ResourcesThe resources required by our algorithm as evalu-ated in this paper are: (a) two bilingual lexicons,one from the source to the pivot language and theother from the pivot to the target language.
Inprinciple, these two pivot lexicons can be noisy,although in our evaluation we use manually com-piled lexicons; (b) two monolingual corpora, onefor each of the source and target languages.
Wehave tested the method with corpora of compa-rable domains, but not covering the same well-defined subjects (the corpora contain news fromdifferent countries and over non-identical time pe-riods).3.2 Initial Lexicon ConstructionWe create an initial lexicon from the source to thetarget language using the pivot language: we lookup each source language word s in the source-pivot lexicon, and obtain the set Ps of its pivot100translations.
We then look up each of the mem-bers of Ps in the pivot-target lexicon, and obtaina set Ts of candidate target translations.
iLex istherefore a mapping from the set of source head-words to the set of candidate target translations.Note that it is possible that not all target lexiconwords appear as translation candidates.
To createa target to source lexicon, we repeat the processwith the directions reversed.3.3 SignaturesThe signature of a word w in a language is theset of N words most strongly related to w. Thereare various possible ways to formalize this notion.We use a common and simple one, the words hav-ing the highest tendency to co-occur with w in acorpus.
We count co-occurrences using a slidingfixed-length window of size k. We compute, foreach pair of words, their Pointwise Mutual Infor-mation (PMI), that is:PMI(w1, w2) = logPr(w1, w2)Pr(w1)Pr(w2)where Pr(w1, w2) is the co-occurrence count, andPr(wi) is the total number of appearance of wiin the corpus (Church and Hanks, 1990).
We de-fine the signature G(w)N,k of w to be the set of Nwords with the highest PMI with w.Note that a word?s signature includes words inthe same language.
Therefore, two signatures ofwords in different languages cannot be directlycompared; we compare them using a lexicon L asexplained below.Signature is a function of w parameterized byN and k. We discuss the selection of these param-eters in section 4.1.5.3.4 Non-aligned Signatures (NAS) SimilarityScoringThe core strength of our method lies in the wayin which we evaluate similarity between words inthe source and target languages.
For a lexicon L,a source word s and a target word t, NASL(s, t)is defined as the number of words in the signatureG(s)N,k of s that may be translated, using L, towords in the signature G(t)N,k of t, normalized bydividing it by N. Formally,NASL(s, t) =|{w?G(s)|L(w)?G(t)6=?
}|NWhere L(x) is the set of candidate translationsof x under the lexicon L. Since we use a singleLanguage Sites TokensHebrew haartz.co.il, ynet.co.il,nrg.co.il510MSpanish elpais.com,elmundo.com, abc.es560MTable 1: Hebrew corpus data.lexicon, iLex, throughout this work, we usuallyomit the L subscript when referring to NAS.4 Lexicon Generation ExperimentsWe tested our algorithm by generating bilinguallexicons for Hebrew and Spanish, using Englishas a pivot language.
We chose a language pair forwhich basically no parallel corpora exist2, and thatdo not share ancestry or writing system in a waythat can provide cues for alignment.We conducted the test twice: once creatinga Hebrew-Spanish lexicon, and once creating aSpanish-Hebrew one.4.1 Experimental Setup4.1.1 CorporaThe Hebrew and Spanish corpora were extractedfrom Israeli and Spanish newspaper websites re-spectively (see table 1 for details).
Crawling asmall number of sites allowed us to use special-tailored software to extract the textual data fromthe web pages, thus improving the quality of theextracted texts.
Our two corpora are comparablein their domains, news and news commentary.No kind of preprocessing was used for the Span-ish corpus.
For Hebrew, closed-class words thatare attached to the succeeding word (e.g., ?the?,?and?, ?in?)
were segmented using a simple un-supervised method (Dinur et al, 2009).
Thismethod compares the corpus frequencies of thenon-prefixed form x and the prefixed form wx.
If xis frequent enough, it is assumed to be the correctform, and all the occurrences of wx are segmentedinto two tokens, w x.
This method was chosen forbeing simple and effective.
However, the segmen-tation it produces is not perfect.
It is context insen-sitive, segmenting all appearances of a token in thesame way, while many wx forms are actually am-biguous.
Even unambiguous token segmentationsmay fail when the non-segmented form is very fre-quent in the domain.2Old testament corpora are for biblical Hebrew, which isvery different from modern Hebrew.101Lexicon # headwords BFEng-Spa 55057 2.4Spa-Eng 44349 2.9Eng-Heb 48857 2.5Heb-Eng 33439 3.7Spa-Heb 34077 12.6Heb-Spa 27591 14.8Table 2: Number of words in lexicons, and branch-ing factors (BF).Hebrew orthography presents additional diffi-culties: there are relatively many homographs, andspelling is not quite standardized.
These consid-erations lead us to believe that our choice of lan-guage pair is more challenging than, for example,a pair of European languages.4.1.2 LexiconsThe source of the Hebrew-English lexicon was theBabylon on-line dictionary3.
For Spanish-English,we used the union of Babylon with the OxfordEnglish-Spanish lexicon.
Since the corpus wassegmented to words using spaces, lexicon entriescontaining spaces were discarded.Lexicon directionality was ignored.
All trans-lation pairs extracted for Hebrew-Spanish via En-glish, were also reversed and added to the Spanish-Hebrew lexicon, and vice-versa.
Therefore, everyL1-L2 lexicon we mention is identical to the cor-responding L2-L1 lexicon in the set of translationpairs it contains.
Our lexicon is thus the ?noisi-est?
that can be generated using a pivot languageand two source-pivot-target lexicons, but it alsoprovides the most complete candidate set possible.Ignoring directionality is also in accordance withthe reversibility principle of the lexicographic lit-erature (Tomaszczyk, 1998).Table 2 details the sizes and branching factors(BF) (the average number of translations for head-word) of the input lexicons, as well as those of thegenerated initial noisy lexicon.4.1.3 BaselineThe performance of our method was compared tothree baselines: Inverse Consultation (IC), averagecosine distance, and average city block distance.The first is a completely different algorithm, andthe last two are a version of our algorithm in which3www.babylon.com.the NAS score is replaced by other scores.IC (see section 2.2.1) is a corpus-less method.It ranks t1, t2, ..., the candidate translations of asource word s, by the size of the intersections ofthe sets of pivot translations of ti and s. Note thatIC ranking is a partial order, as the intersectionsize may be the same for many candidate transla-tions.
IC is a baseline for our algorithm as a whole.Cosine and city block distances are widelyused methods for calculating distances of vectorswithin the same vector space.
They are definedhere as4Cosine(v, u) = 1??viui?
?vi?uiCityBlock(v, u) = ?
?i|vi ?
ui|In the case of context vectors, the vector in-dices, or keys, are words, and their values are co-occurrence based scores.
We used the words inour signatures as context vector keys, and PMIscores as values.
In this way, the two scores are?plugged?
into our method and serve as baselinesfor our NAS similarity score.Since the context vectors are in different lan-guages, we had to translate, or align, the baselinecontext vectors for the source and target words.Our initial lexicon is a many-to-many relation, somultiple alignments were possible; in fact, thenumber of possible alignments tends to be verylarge5.
We therefore generated M random possiblealignments, and used the average distance metricacross these alignments.4.1.4 Test Sets and Gold StandardFollowing other works (e.g.
(Rapp, 1999)), and tosimplify the experimental setup, we focused in ourexperiments on nouns.A p-q frequency range in a corpus is the set oftokens in the places between p and q in the list ofcorpus tokens, sorted by frequency from high tolow.
Two types of test sets were used.
The first(R1) includes all the singular, correctly segmented(in Hebrew) nouns among the 500 words in the1001-1500 frequency range.
The 1000 highest-frequency tokens were discarded, as a large num-ber of these are utilized as auxiliary syntactic4We modified the standard cosine and city block metricsso that for all measures higher values would be better.5This is another advantage of our NAS score.102R1 R2Precision Recall Precision RecallNAS 82.1% 100% 56% 100%Cosine 60.7% 100% 28% 100%City block 56.3% 100% 32% 100%IC 55.2% 85.7% 52% 88%Table 3: Hebrew-Spanish lexicon generation:highest-ranking translation.words.
This yielded a test set of 112 Hebrewnouns and 169 Spanish nouns.
The second (R2),contains 25 words for each of the two languages,obtained by randomly selecting 5 singular cor-rectly segmented nouns from each of the 5 fre-quency ranges 1-1000 to 4001-5000.For each of the test words, the correct transla-tions were extracted from a modern professionalconcise printed Hebrew-Spanish-Hebrew dictio-nary (Prolog, 2003).
This dictionary almost al-ways provides a single Spanish translation for He-brew headwords.
Spanish headwords had 1.98 He-brew translations on the average.
In both casesthis is a small number of correct translation com-paring to what we might expect with other evalu-ation methods; therefore this evaluation amountsto a relatively high standard of correctness.
Ourscore comparison experiments (section 5) extendthe evaluation beyond this gold standard.4.1.5 ParametersThe following parameter values were used.
Thewindow size for co-occurrence counting, k, was 4.This value was chosen in a small pre-test.
Signa-ture size N was 200 (see Section 6.1).
The numberof alignments M for the baseline scores was 100.The number of translations selected for each head-word, t, was set to 1 for ease of testing, but seefurther notes under results.4.2 ResultsTables 3 and 4 summarize the results of theHebrew-Spanish and Spanish-Hebrew lexicongeneration respectively, for both the R1 and R2test sets.In the three co-occurrence based methods, NASsimilarity, cosine distance and and city block dis-tance, the highest ranking translation was selected.Recall is always 100% as a translation from thecandidate set is always selected, and all of this setis valid.
Precision is computed as the number ofR1 R2Precision Recall Precision RecallNAS 87.6% 100% 80% 100%Cosine 68% 100% 44% 100%City block 69.8% 100% 36% 100%IC 76.4% 100% 48% 92%Table 4: Spanish-Hebrew Lexicon Generation:highest-ranking translation.test words whose selected translation was one ofthe translations in the gold standard.IC translations ranking is a partial order, as usu-ally many translations are scored equally.
Whenall translations have the same score, IC is effec-tively undecided.
We calculate recall as the per-centage of cases in which there was more than onescore rank.
A result was counted as precise if anyof the highest-ranking translations was in the gold-standard, even if other translations were equallyranked, creating a bias in favor of IC.In both of the Hebrew-Spanish and the Spanish-Hebrew cases, our method significantly outper-formed all baselines in generating a precise lexi-con on the highest-ranking translations.All methods performed better in R1 than inR2, which included also lower-frequency words,and this was more noticeable with the corpus-based methods (Hebrew-Spanish) than with IC.This suggests, not surprisingly, that the perfor-mance of corpus-based methods is related to theamount of information in the corpus.That the results for the Spanish-Hebrew lexi-con are higher may arise from the difference in thegold standard.
As mentioned, Hebrew words onlyhad one ?correct?
Spanish translation, while Span-ish had 1.98 correct translations on the average.If we had used a more comprehensive resource totest against, the precision of the method would behigher than shown here.In translation pairs generation, the results be-yond the top-ranking pair are also of importance.Tables 5 and 6 present the accuracy of the firstthree translation suggestions, for the three co-occurrence based scores, calculated for the R1 testset.
IC results are not included, as they are incom-parable to those of the other methods: IC tends toscore many candidate translations identically, andin practice, the three highest-scoring sets of trans-lation candidates contained on average 77% of all1031st 2nd 3rd totalNAS 82.1% 6.3% 1.8% 90.2%Cosine 60.7% 9.8% 2.7% 73.2%City block 56.3% 4.5% 10.7% 71.4%Table 5: Hebrew-Spanish lexicon generation: ac-curacy of 3 best translations for the R1 condition.The table shows how many of the 2nd and 3rdtranslations are correct.
Note that NAS is alwaysa better solution, even though its numbers for 2ndand 3rd are smaller, because its accumulative per-centage, shown in the last column, is higher.1st 2nd 3rd totalNAS 87.6% 77.5% 16% 163.9%Cosine 68% 66.3% 10.1% 144.4%City block 69.8% 64.5% 7.7% 142%Table 6: Spanish-Hebrew lexicon generation: ac-curacy of 3 best translations for the R1 condition.The total exceeds 100% because Spanish wordshad more than one correct translation.
See alsothe caption of Table 5.the candidates, thus necessarily yielding mostlyincorrect translations.
Recall was omitted from thetables as it is always 100%.For all methods, many of the correct translationsthat do not rank first, rank as second or third.
Forboth languages, NAS ranks highest for total ac-curacy of the three translations, with considerableadvantage.5 Score Comparison ExperimentsLexicon generation, as defined in our experiment,is a relatively high standard for cross-linguistic se-mantic distance evaluation.
This is especially cor-Heb-Spa Spa-HebSCE1 SCE2 SCE1 SCE2NAS 93.8% 76.2% 94.1% 83.7%Cosine 74.1% 57.1% 70.7% 63.2%City block 74.1% 68.3% 78,1% 75.2%Table 7: Precision of score comparison experi-ments.
The percentage of cases in which eachof the scoring methods was able to successfullydistinguish the correct (SCE1) or possible correct(SCE2) translation from the random translation.rect since our gold standard gives only a small setof translations.
The set of possible translations iniLex tends to include, besides the ?correct?
transla-tion of the gold standard, other translations that aresuitable in certain contexts or are semantically re-lated.
For example, for one Hebrew word, kvuza,the gold standard translation was grupo (group),while our method chose equipo (team), which wasat least as plausible given the amount of sportsnews in the corpus.Thus to better compare the capability of NAS todistinguish correct and incorrect translations withthat of other scores, we performed two more ex-periments.
In the first score comparison experi-ment (SCE1), we used the two R1 test sets, He-brew and Spanish, from the lexicon generation test(section 4.1.4).
For each word in the test set, weused our method to select between one of twotranslations: a correct translation, from the goldstandard, and a random translation, chosen ran-domly among all the nouns similar in frequencyto the correct translation.The second score comparison experiment(SCE2) was designed to test the score with a moreextensive test set.
For each of the two languages,we randomly selected 1000 nouns, and used ourmethod to select between a possibly correct trans-lation, chosen randomly among the translationssuggested in iLex, and a random translation, cho-sen randomly among nouns similar in frequencyto the possibly correct translation.
This test, whileusing a more extensive test set, is less accuratebecause it is not guaranteed that any of the inputtranslations is correct.In both SCE1 and SCE2, cosine and city blockdistance were used as baselines.
Inverse Consul-tation is irrelevant here because it can only scoretranslation pairs that appear in iLex.Table 7 presents the results of the two scorecomparison experiments, each of them for each ofthe translation directions.
Recall is by definition100% and is omitted.Again, NAS performs better than the baselinesin all cases.
With all scores, precision values inSCE1 are higher than in the lexicon generationexperiment.
This is consistent with the expecta-tion that selection between a correct and a ran-dom, probably incorrect, translation is easier thanselecting among the translations in iLex.
The pre-cision in SCE2 is lower than that in SCE1.
Thismay be a result of both translations in SCE2 being104Figure 1: NAS values (not algorithm precision) forvarious N sizes.
NAS is not sensitive to the valueof N (see text).in some cases incorrect.
Yet this may also reflect aweakness of all three scores with lower-frequencywords, which are represented in the 1000-wordsamples but not in the ones used in SCE1.6 NAS Score Properties6.1 Signature SizeNAS values are in the range [0, 1].
The values de-pend on N, the size of the signature used.
With anextremely small N, NAS values would usually be0, and would tend to be noisy, due to accidentalinclusion of high-frequency or highly ambiguouswords in the signature.
As N approaches the sizeof the lexicon used for alignment, NAS values ap-proach 1 for all word pairs.This suggests that choosing a suitable value ofN is critical for effectively using NAS.
Yet an em-pirical test has shown that NAS may be useful fora wide range of N values: we computed NAS val-ues for the correct and random translations usedin the Hebrew-Spanish SCE1 experiment (section5), using N values between 50 and 2000.Figure 1 shows the average score values (notethat these are not precision values) for the correctand random translations across that N range.
Thescores for the correct translations are consistentlyhigher than those of the random translations, evenwhile there is a discernible decline in the differ-ence between them.
In fact, the precision of the se-lection between the correct and random translationis persistent throughout the range.
This suggeststhat while extreme N values should be avoided, theselection of N is not a major issue.6.2 Dependency on Alignment LexiconNASL values depend on L, the lexicon in use.Clearly again, in the extremes, an almost emptylexicon or a lexicon containing every possible pairof words (a Cartesian product), this score wouldnot be useful.
In the first case, it would yield 0for every pair, and in the second, 1.
However asour experiments show, it performed well with real-world examples of a noisy lexicon, with branchingfactors of 12.6 and 14.8 (see table 2).6.3 LemmatizationLemmatization is the process of extracting thelemmas of words in the corpus.
Our experimentsshow that good results can be achieved withoutlemmatization, at least for nouns in the pair of lan-guages tested (aside from the simple prefix seg-mentation we used for Hebrew, see section 4.1.1).For other language pairs lemmatization may beneeded.
In general, correct lemmatization shouldimprove results, since the signatures would con-sist of more meaningful information.
If automaticlemmatization introduces noise, it may reduce theresults?
quality.6.4 Alternative Models for RelatednessCosine and city block, as well as other related dis-tance metrics, rely on context vectors.
The contextvector of a word w collects words and maps themto some score of their ?relatedness?
to w; in thiscase, we used PMI.
NAS, in contrast, relies on thesignature, the set of N words most related to w.That is, it requires a Boolean relatedness indica-tion, rather than a numeric relatedness score.
Weused PMI to generate this Boolean indication, andnaturally, other similar measures could be used aswell.
More significantly, it may be possible to useit with corpus-less sources of ?relatedness?, suchas WordNet or search result snippets.7 ConclusionWe presented a method to create a high qualitybilingual lexicon given a noisy one.
We focusedon the case in which the noisy lexicon is createdusing two pivot language lexicons.
Our algorithmuses two unrelated monolingual corpora.
At theheart of our method is the non-aligned signatures(NAS) context similarity score, used for remov-ing incorrect translations using cross-lingual co-occurrences.105Words in one language tend to have multipletranslations in another.
The common method forcontext similarity scoring utilizes some algebraicdistance between context vectors, and requires asingle alignment of context vectors in one lan-guage into the other.
Finding a single correctalignment is unrealistic even when a perfectly cor-rect lexicon is available.
For example, alignmentforces us to choose one correct translation for eachcontext word, while in practice a few possibleterms may be used interchangeably in the otherlanguage.
In our task, moreover, the lexicon usedfor alignment was automatically generated frompivot language lexicons and was expected to con-tain errors.NAS does not depend on finding a single correctalignment.
While it measures how well the sets ofwords that tend to co-occur with these two wordsalign to each other, its strength may lie in bypass-ing the question of which word in one languageshould be aligned to a certain context word in theother language.
Therefore, unlike other scoringmethods, it is not effected by incorrect alignments.We have shown that NAS outperforms the moretraditional distance metrics, which we adapted tothe many-to-many scenario by amortizing acrossmultiple alignments.
Our results confirm thatalignment is problematic in using co-occurrencemethods across languages, at least in our settings.NAS constitutes a way to avoid this problem.While the purpose of this work was to discerncorrect translations from incorrect one, it is worthnoting that our method actually ranks translationcorrectness.
This is a stronger property, whichmay render it useful in a wider range of scenarios.In fact, NAS can be viewed as a general mea-sure for word similarity between languages.
Itwould be interesting to further investigate this ob-servation with other sources of lexicons (e.g., ob-tained from parallel or comparable corpora) andfor other tasks, such as cross-lingual word sensedisambiguation and information retrieval.ReferencesKisuh Ahn and Matthew Frampton.
2006.
Automaticgeneration of translation dictionaries using interme-diary languages.
In EACL 2006 Workshop on Cross-Language Knowledge Induction.Francis Bond, Ruhaida Binti Sulong, Takefumi Ya-mazaki, and Kentaro Ogura.
2001.
Design and con-struction of a machine-tractable japanese-malay dic-tionary.
In MT Summit VIII: Machine Translation inthe Information Age, Proceedings, pages 53?58.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16:22?29.Elad Dinur, Dmitry Davidov, and Ari Rappoport.
2009.Unsupervised concept discovery in hebrew usingsimple unsupervised word prefix segmentation forhebrew and arabic.
In EACL 2009 Workshop onComputational Approaches to Semitic Languages.Pascale Fung.
1998.
A statistical view on bilin-gual lexicon extraction:from parallel corpora to non-parallel corpora.
In The Third Conference of the As-sociation for Machine Translation in the Americas.Nikesh Garera, Chris Callison-Burch, and DavidYarowsky.
2009.
Improving translation lexi-con induction from monolingual corpora via depen-dency contexts and part-of-speech equivalences.
InCoNLL.Hiroyuki Kaji and Toshiko Aizono.
1996.
Extractingword correspondences from bilingual corpora basedon word co-occurrence information.
In COLING.Hiroyuki Kaji, Shin?ichi Tamamura, and DashtserenErdenebat.
2008.
Automatic construction of ajapanese-chinese dictionary via english.
In LREC.Philipp Koehn and Kevin Knight.
2002.
Learn-ing a translation lexicon from monolingual corpora.In Proceedings of ACL Workshop on UnsupervisedLexical Acquisition.Adam Lopez.
2008.
Statistical machine translation.ACM Computing Surveys, 40(3):1?49.Mausam, Stephen Soderland, Oren Etzioni, Daniel S.Weld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In Proceedings of the 47thAnnual Meeting of the Association for Computa-tional Linguistics and 4th International Joint Con-ference on Natural Language Processing.Kyonghee Paik, Satoshi Shirai, and Hiromi Nakaiwa.2004.
Automatic construction of a transfer dictio-nary considering directionality.
In COLING, Multi-lingual Linguistic Resources Workshop.Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-drea Mulloni.
2006.
Finding translations for low-frequency words in comparable corpora.
MachineTranslation, 20:247 ?
266.Prolog.
2003.
Practical Bilingual Dictionary:Spanish-Hebew/Hebrew-Spanish.
Israel.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In ACL.106Charles Schafer and David Yarowsky.
2002.
Inducingtranslation lexicons via diverse similarity measuresand bridge languages.
In CoNLL.Hana Skoumalova.
2001.
Bridge dictionaries asbridges between languages.
International Journalof Corpus Linguistics, 6:95?105.Kumiko Tanaka and Hideya Iwasaki.
1996.
Extractionof lexical translations from non-aligned corpora.
InConference on Computational linguistics.Kumiko Tanaka and Kyoji Umemura.
1994.
Construc-tion of a bilingual dictionary intermediated by a thirdlanguage.
In Conference on Computational Linguis-tics.Jerzy Tomaszczyk.
1998.
The bilingual dictionary un-der review.
In ZuriLEX?86.107
