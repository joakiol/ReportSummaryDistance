Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46?54,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsA Usage-Based Model of Early Grammatical DevelopmentBarend BeekhuizenLUCLLeiden Universityb.f.beekhuizen@hum.leidenuniv.nlRens BodILLCUniversity of Amsterdaml.w.m.bod@uva.nlAfsaneh Fazly and Suzanne StevensonDepartment of Computer ScienceUniversity of Torontoafsaneh,suzanne@cs.toronto.eduArie VerhagenLUCLLeiden Universitya.verhagen@hum.leidenuniv.nlAbstractThe representations and processes yield-ing the limited length and telegraphic styleof language production early on in acqui-sition have received little attention in ac-quisitional modeling.
In this paper, wepresent a model, starting with minimal lin-guistic representations, that incrementallybuilds up an inventory of increasingly longand abstract grammatical representations(form+meaning pairings), in line with theusage-based conception of language ac-quisition.
We explore its performance ona comprehension and a generation task,showing that, over time, the model bet-ter understands the processed utterances,generates longer utterances, and better ex-presses the situation these utterances in-tend to refer to.1 IntroductionA striking aspect of language acquisition is the dif-ference between children?s and adult?s utterances.Simulating early grammatical production requiresa specification of the nature of the linguistic repre-sentations underlying the short, telegraphic utter-ances of children.
In the usage-based view, youngchildren?s grammatical representions are thoughtto be less abstract than adults?, e.g.
by havingstricter constraints on what can be combined withthem (cf.
Akhtar and Tomasello 1997; Bannardet al.
2009; Ambridge et al.
2012).
The represen-tations and processes yielding the restricted lengthof these early utterances, however, have receivedlittle attention.
Following Braine (1976), we adoptthe working hypothesis that the early learner?sgrammatical representations are more limited inlength (or: arity) than those of adults.Similarly, in computational modeling of gram-mar acquisition, comprehension has received moreattention than language generation.
In this pa-per we attempt to make the mechanisms underly-ing early production explicit within a model thatcan parse and generate utterances, and that in-crementally learns constructions (Goldberg, 1995)on the basis of its previous parses.
The model?ssearch through the hypothesis space of possiblegrammatical patterns is highly restricted.
Start-ing from initially small and concrete representa-tions, it learns incrementally long representations(syntagmatic growth) as well as more abstractones (paradigmatic growth).
Several models ad-dress either paradigmatic (Alishahi and Stevenson,2008; Chang, 2008; Bannard et al., 2009) or syn-tagmatic (Freudenthal et al., 2010) growth.
Thismodel aims to explain both, thereby contribut-ing to the understanding of how different learningmechanisms interact.
As opposed to other modelsinvolving grammars with semantic representations(Alishahi and Stevenson, 2008; Chang, 2008), butsimilar to Kwiatkowski et al.
(2012), the modelstarts without an inventory of mappings of singlewords to meanings.Based on motivation from usage-based and con-struction grammar approaches, we define severallearning principles that allow the model to buildup an inventory of linguistic representations.
Themodel incrementally processes pairs of an utter-ance U , consisting of a string of words w1.
.
.
wn,and a set of situations S, one of which is the situa-tion the speaker intends to refer to.
The other situ-ations contribute to propositional uncertainty (theuncertainty over which proposition the speaker istrying to express; Siskind 1996).
The model triesto identify the intended situation and to understandhow parts of the utterance refer to certain parts ofthat situation.
To do so, the model uses its growinginventory of linguistic representations (Section 2)to analyze U , producing a set of structured seman-tic analyses or parses (Fig.
1, arrow 1; Section 3).46The resulting best parse, U and the selected situa-tion are then stored in a memory buffer (arrow 2),which is used to learn new constructions (arrow3) using several learning mechanisms (Section 4).The learned constructions can then be used to gen-erate utterances as well.
We describe two experi-ments: in the comprehension experiment (Section5), we evaluate the model?s ability to parse thestream of input items.
In the generation experi-ment (Section 6), the model generates utteranceson the basis of a given situation and its linguisticknowledge.
We evaluate the generated utterancesgiven different amounts of training items to con-sider the development of the model over time.2 RepresentationsWe represent linguistic knowledge as construc-tions: pairings of a signifying form and a signi-fied (possibly incomplete) semantic representation(Goldberg, 1995).
The meaning is represented asa graph with the nodes denoting entities, events,and their relations, connected by directed unla-beled edges.
The conceptual content of each nodeis given by a set of semantic features.
We assumethat meaning representations are rooted trees.
Thesignifying form consists of a positive number ofconstituents.
Every constituent has two elements:a phonological form, and a pointer to a node in thesignified meaning (in line with Verhagen 2009).Both can be specified, or one can be left empty.Constituents with unspecified phonological formsare called open, denoted with  in the figures.
Thehead constituent of a construction is defined asthe constituent that has a pointer to the root nodeof the signified meaning.
We furthermore requirethat no two constituents point to the same node ofthe signified meaning.This definition generalizes over lexical ele-ments (one phonologically specified constituent)as well as larger linguistic patterns.
Fig.
2, for in-stance, shows two larger constructions being com-bined with each other.
We call the set of construc-tions the learner has at some moment in time theconstructicon C (cf.
Goldberg 2003).3 Parsing3.1 Parsing operationsWe first define a derivation d as an assemblyof constructions in C, using four parsing opera-tions defined below.
In parsing, derivations areconstrained by the utterance U and the situationsutterancesituation 1situation nsituation 2...situationsinput itemconstruction 1construction 2construction 3construction nconstructiconanalysis(utterance, intended situation, analysis)......memory buffer1 123(utterance, intended situation, analysis)(utterance, intended situation, analysis)Figure 1: The global flow of the modelS, whereas in production, only a situation s con-strains the derivation.
The leaf nodes of a deriva-tion must consist of phonological constraints ofconstructions that (in parsing) are satisfied by U .All constructions used in a derivation must map tothe same situation s ?
S. A construction cmaps tos iff the meaning of c constitutes a subgraph of s,with the features on each of the nodes in the mean-ing of c being a subset of the features on the corre-sponding node of s. Moreover, each constructionmust map to a different part of s. This constitutesa mutual exclusivity effect in analyzing U : everypart of the analysis must contribute to the compos-ite meaning.
A derivation d thus gives us a map-ping between the composed meaning of all con-structions used in d and one situation s ?
S. Theaggregate mapping specifies a subgraph of s thatconstitutes the interpretation of that derivation.The central parsing operation is the COMBINA-TION operator ?.
In ci?
cj, the leftmost open con-stituent of ciis combined with cj.
Fig.
2 illus-trates COMBINATION.
COMBINATION succeeds ifboth the semantic pointer of the leftmost open con-stituent of ciand the semantic pointer of the headconstituent of cjmap to the same semantic nodeof a situation sInitially, the model has few constructions to an-alyze the utterance with.
Therefore, we definethree other operations that allow the model to cre-ate a derivation over the full utterance withoutcombining constructions.
First, a known or un-known word that cannot be fit into a derivation,can be IGNOREd.
Second, an unknown word canbe used to fill an open constituent slot of a con-struction with the BOOTSTRAP operator.
Boot-strapping entails that the unknown word will beassociated with the semantics of the node.
Finally,the learner can CONCATENATE multiple deriva-tions, by linearly sequencing them, thus creating amore complex derivation without combining con-47uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn uly1(c.an,d)bf n??????
?f ,d)bf?
???
?futeranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn??
?
??
?)
?,d)bf n?????
?f ,d)bf?
???
?futeranu?copaio tera3n u2e.copei sec2nue?
?.o aiopo?
uly1(c.anu?copaio tera3nue?
?.o aiopo?
,d)bf n??????
?f ,d)bf?
???
?futeranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn,d)bf n?????
?f ,d)bf?
???
?futeranu?copaio tera3n u2e.copei sec2nue?
?.o aiopo?
uly1(c.an??????n????n????????????n???????
??
?n??????n?n??????????n???????
?Figure 2: Combining constructions.
The dashed lines represent semantic pointers, either from con-stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).uttereancn sio ta121n i o. p .
m21n i o. pn p n m n 3. p21n i o.t .ne1 ne1Figure 3: The CONCATENATE, IGNORE andBOOTSTRAP operators (internal details of the con-structions left out).structions.
This allows the learner to interpret alarger part of the situation than with COMBINA-TION only.
The resulting sequences may be ana-lyzed in the learning process as constituting onelarger construction, consisting of the parts of theconcatenated derivations.
Fig.
3 illustrates thesethree operations.3.2 Selecting the best analysisMultiple derivations can be highly similar in theway they map parts of U to parts of an s ?
S. Wedefine a parse to be a set of derivations that havethe same internal structure and the same mappingsto a situation, but that use different constructionsin doing so (cf.
multiple licensing; Kay 2002).
Wetake the most probable parse of U to be the bestanalysis of U .
The most probable parse points to asituation, which the model then assumes to be theidentified situation or sidentified.
If no parse can bemade, sidentifiedis selected at random from S.The probability of a parse p is given by the sumof the probabilities of the derivations d subsumedunder that parse, which in turn are defined as theproduct of the probabilities of the constructions cused in d.P (p) =?d?pP (d) (1)P (d) =?c?dP (c) (2)The probability of a construction P (c) is givenby its relative frequency (count) in the construc-ticon C, smoothed with Laplace smoothing.
Weassume that the simple parsing operations of IG-NORE, BOOTSTRAP, and CONCATENATION reflectusages of an unseen construction with a count of0.P (c) =c.count+ 1?c?
?Cc?.count+ |C|+ 1(3)The most probable parse, U and sidentifiedareadded to the memory buffer.
The memory bufferhas a pre-set maximal length, discarding the oldestexemplars upon reaching this length.
In the future,we plan to consider more realistic mechanisms forthe memory buffer, such as graceful degradation,and attention effects.484 Learning mechanismsThe model uses the best parse of the utterance todevelop its knowledge of the constructions in theconstructicon C. Two simple operations, UPDATEand ASSOCIATION, are used to create initial con-structions and reinforce existing ones respectively.Two additional operations, PARADIGMATIZATIONand SYNTAGMATIZATION, are key to the model?sability to extend these initial representations byinducing novel constructions that are richer andmore abstract than existing ones.4.1 Direct learning from the best parseThe best parse is used to UPDATE C. For thismechanism, the model uses the concrete mean-ing of sidentifiedrather than the (potentially moreabstract) meaning of the constructions in the bestparse.1Every construction in the parse is assignedthe subgraph of sidentifiedit maps to as its newmeaning, and the count of the adjusted construc-tion is incremented with 1, or added to C with acount of 1, if it does not yet exist.
This includesapplications of the BOOTSTRAP operation, creat-ing a mapping of the previously unknown word toa situational meaning.ASSOCIATE constitutes a form of simple cross-situational learning over the memory buffer.
Theintuition is that co-occurring word sequencesand meaning components that remain unanalyzedacross multiple parses might themselves comprisethe form-meaning pairing of a construction.
If theunanalyzed parts of two situations contain an over-lapping subgraph, and the unanalyzed parts of twoutterances an overlapping subsequence of words,the two are mapped to each other and added to Cwith a count of 0.4.2 Qualitative extension of the best parseSyntagmatization Some of the processes de-scribed thus far yield analyses of the input inwhich constructions are linearly associated butlack appropriate relational structure among them.The model requires a process, which we call SYN-TAGMATIZATION, that enables it to induce furtherhierarchical structure.In order for the learner to acquire constructionsin which the different constituents point to differ-ent parts of the construction?s meaning, the ASSO-1This follows Langacker?s (2009) claim that the processedconcrete usage events should leave traces in the learner?smind.CIATE operation does not suffice.
We assume thatthe learner is able to learn such constructions byusing concatenated derivations.
The process wepropose is SYNTAGMATIZATION.
In this process,the various concatenated derivations are taken asconstituents of a novel construction.
This instanti-ates the idea that joint processing of two (or more)events gradually leads to a joint representation ofthese, previously independent, events.More precisely, the process starts by taking thetop nodes T of the derivations in the best parse,where T consists of the single top node if no CON-CATENATION has been applied, or the set of con-catenated nodes of the parse tree if CONCATENA-TION has been applied (e.g.
for the derivation inFig.
3, |T | = 2).
For each top node t ?
T , we takethe root node of the construction?s meaning, anddefine its semantic frame to consist of all children(roles) and grandchildren (role-fillers) of the nodein the situation it maps to.
The model then forms anovel construction csynby taking all the construc-tions in the parse whose semantic root nodes pointto a node in this semantic frame, referring to thoseas the set R of semantically related constructions.As the novel meaning of csyn, the model takes thesubgraph of the situation mapped to by the jointmapping of all constructional meanings of con-structions in R.R, as well as all phonologically specified con-stituents of t itself, are then linearized as the con-stituents of csyn.
The novel construction thus con-stitutes a construction with a higher arity, ?joining?several previously independent constructions.
Fig.4 illustrates the syntagmatization mechanism.Paradigmatization Due to our usage-driven ap-proach, all learning mechanisms so far give usmaximally concrete constructions.
In order for themodel to generalize beyond the observed input,some degree of abstraction is needed.
The modeldoes so with the PARADIGMATIZATION mecha-nism.
This mechanism recursively looks for min-imal abstractions (cf.
Tomasello 2003, 123) overthe constructions in C and adds those to C, thuscreating a full-inheritance network (cf.
Langacker1989, 63-76).An abstraction over a set of constructions ismade if there is an overlapping subgraph betweenthe meanings of the constructions, where everynode of the subgraph is the non-empty featureset intersection between two mapped nodes of theconstructional meanings.
Furthermore, the con-49uterauncsiricots 111autoi2tr.
p.tm.mauio3.l.o3.ory.
(i,rad)b d)bf???
n?
f???
?uter 2cn.auncsiricots 111a uio3.l.o3.ory.(i,rad)bf???
?
?ecoetr.otr.uc?.er .orir?
?ssad)bf???
?
?uterauncsiricots 111autoi2tr.
p.tm.mauio3.l.o3.ory.
(i,rad)b d)bf???
n?
f???
?
?uc?.er .orir?
?ssad)bf???
???????????????????
??rt?
?r ?
?????????????????
?Figure 4: The SYNTAGMATIZATION mechanism.
The mechanism takes a derivation as its input andreinterprets it as a novel construction of higher arity).stituents must be mappable: both constructionshave the same number of constituents and thepaired constituents point to a mapped node of themeaning.
The meaning of the abstracted construc-tion is then set to this overlapping subgraph, whichis the lowest possible semantic abstraction overthe constructions.
The constituents of this new ab-straction have a specified phonological form if themore concrete constructions share the same word,and an unspecified one otherwise.
The count of anabstracted construction is given by the cardinalityof the set of its direct descendants in the network.This generalizes Bybee?s (1995) idea about typefrequency as a proxy for productivity to a networkstructure.
Fig.
5 illustrates the paradigmatizationmechanism.5 Experimental set-upThe model is incrementally presented with U, Spairings based on Alishahi & Stevenson?s (2010)generation procedure.
In this procedure, an utter-ance and a semantic frame expressing its meaning(a situation) are generated.
The generation pro-cedure follows distributions occurring in a corpusof child-directed speech.
As we are interested inthe performance of the model under propositionaluncertainty, we add a parametrized number of ran-domly sampled situations, so that S consists of thesituation the speaker intends to refer to (scorrect)and a number of situations the speaker does notintend to refer to.2Here, we set the number of ad-2We are currently researching the effects of sampling non-correct situations that have a greater likelihood of overlapditional situations to be 1 or 5; the other parameterof the model, the size of the memory buffer, is setto 5 exemplars.For the comprehension experiment, we eval-uate the model?s performance parsing the inputitems, averaging over every 50 U, S pairs.
Wetrack the ability to identify the intended situationfrom S. Identification succeeds if the best parsemaps to scorrect, i.e.
if sidentified= scorrect.
Next,situation coverage expresses what proportion ofsidentifiedhas been interpreted and thus how rich themeanings of the used constructions are.
It is de-fined as the number of nodes of the interpretationof the best parse, divided by the number of nodesof sidentified.
Finally, utterance coverage tells uswhat proportion of U has been parsed with con-structions (excluding IGNORED; including BOOT-STRAPPED words).
The measure expresses theproportion of the signal that the learner (correctlyor incorrectly) is able to interpret.For exploring language production, the modelreceives a situation, and (given the constructicon)finds the most probable, maximally expressive,fully lexicalized derivation expressing it.
That is:among all derivations terminating in phonologi-cally specified constituents, it selects the deriva-tions that cover the most semantic nodes of thegiven situation.
In the case of multiple suchderivations, it selects the most probable one, fol-lowing the probability model in Section 3.
Weonly allow for the COMBINATION operator in thederivations, as BOOTSTRAPPING and IGNORE re-with the intended situation, to reflect more realistic input (cf.Siskind 1996).50uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.
(ct3e2l,d) ,d) ,d)bf??
n?
bf??
??
bf??
??
?uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.
(c.e?nuterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.
(,d) ,d) ,d)bf??
n?
bf??
??
bf??
?,d) ,d) ,d)bf??
n?
bf??
??
bf??
?????
????
??
???n???n?????
????
?
???
??
???
???
t3e2l???
.e?n ??
??????
?????
????u1ite.2ipcnp.2.
(ct3e2l ???
?u1ite.2ipcnp.2.
(c?.e?nFigure 5: The PARADIGMATIZATION mechanism.
The construction on top is an abstraction obtainedover the two constructions at the bottom.fer to words in a given U , and CONCATENATE is aback-off method for analyzing more of U than theconstructicon allows for.
The situations used in thegeneration experiment do not occur in the trainingitems, so that we truly measure the model?s abilityto generate utterances for novel situations.The phonologically specified leaf nodes of thebest derivation constitute the generated utteranceUgen.
Ugenis evaluated on the basis of its meanlength, in number of words, its situation cover-age, as defined in the comprehension experiment,and its utterance precision and utterance recall.To calculate these, we take the maximally overlap-ping subsequenceUoverlapbetween the actual utter-ance Uactassociated with the situation and Ugen.Utterance precision (how many words are gener-ated correctly) and utterance recall (how many ofthe correct words are generated) are defined as:Utterance precision =|Uoverlap||Ugen|(4)Utterance recall =|Uoverlap||Uact|(5)Because the U, S-pairs on which the model wastrained, are generated randomly, we show resultsfor comprehension and production averaged over5 simulations.6 ExperimentsA central motivation for the development of thismodel is to account for early grammatical produc-tion: can we simulate the developmental patternof the growth of utterance length and a growingpotential for generalization?
The same construc-tions underlying these productions should, at thesame time, also account for the learner?s increas-ing grasp of the meaning of U .
To explore themodel?s performance in both domains, we presenta comprehension and a generation experiment.6.1 Comprehension resultsFig.
6a gives us the results over time of the com-prehension measures given a propositional un-certainty of 1, i.e.
one situation besides scorrectin S. Overall, the model understands the utter-ances increasingly well.
After 2000 input items,the model identifies scorrectin 95% of the cases.With higher levels of propositional uncertainty(not shown here), performance is still relativelyrobust: given 5 incorrect situations in S, scorrectis identified in 62% of all cases (random guess-ing gives a score of 17%, or16).
Similarly, theproportion of the situation interpreted and the pro-portion of the utterance analyzed go up over time.This means that the model builds up an increasingrepertoire of constructions that allow it to analyzelarger parts of the utterance and the situations itidentifies.
It is important to realize that these mea-510 500 1000 1500 20000.00.20.40.60.81.0timeproportionmeasuressituation coverageutterance coverageidentification(a) Comprehension results over time0 500 1000 1500 20000.00.51.01.52.02.53.0timeutterance length inwords(b) Length of Ugenover time0 500 1000 1500 20000.00.20.40.60.81.0timeproportionmeasuressituation coverageutterance precisionutterance recall(c) Generation results over timeFigure 6: Quantitative results for the comprehension and generation experimentssures do not display what proportion of the utter-ance or situation is analyzed correctly.6.2 Generation resultsQuantitative results Fig.
6b shows that the av-erage utterance length increases over time.
Thisindicates that the number of constituents of theused constructions grows.
Next, Fig.
6c shows theperformance of the model on the generation task.After 2000 input items, the model generates pro-ductions expressing 93% of the situation, with anutterance precision of 0.91, and an utterance recallof 0.81.
Given a propositional uncertainty of 5,these go down to 79%, 0.76 and 0.59 respectively.Comparing the utterance precision and recallover time, we can see that the utterance preci-sion is high from the start, whereas the recallgradually increases.
This is in line with the ob-servation that children predominantly produce er-rors of omission (leaving linguistic material out anadult speaker would produce), and few errors ofcomission (producing linguistic material an adultspeaker would not produce).Qualitative results Tracking individual produc-tions given specific situations over time allows usto study in detail what the model is doing.
Here,we look at one case qualitatively.
Given the sit-uation for which the Uactis she put them away,the model generates, over time, the utterances inTable 1.
The brackets show the internal hierarchi-cal structure of the derivation.
This developmentillustrates several interesting aspects of the model.First, as discussed earlier, the model mostly makeserrors of omission: earlier productions leave outmore words found in the adult utterances.
Only att = 550, the model makes an error of commission,using the word in erroneously.
[[she]put][she[put]][[she][put][in]][[she]putthem[away]][[she]put[them]][[she]putthem[away]][[she]put[them]away][[she]putthemaway]t 50 500 550 600 950 1000 1050 1400Table 1: Generations over time t for one situation.Starting from t = 600 (except at t = 950),the model generates the correct utterance, but thederivations leading to this production differ.
Att = 550, for instance, the learner combines acompletely non-phonologically specific construc-tion for which the constituents refer to the agent,action and goal location, with three ?lexical?
con-structions that fill in the words for those items..The constructions used after t = 550 are all morespecific, combining 3, or even only 2 constructions(t ?
1400) where the entire sequence of words?put them away?
arises from a single construction.Using less abstract constructions over timeseems contrary to the usage-based idea that con-structions become more abstract over the course ofacquisition.
However, this result follows from theway the probability model is defined.
More spe-cific constructions that are able to account for theinput will entail fewer combinations, and a deriva-tion with fewer combination operations will oftenbe more likely than one with more such opera-tions.
Given equal expressivity of the situation,the former derivation will be selected over the lat-ter in generation.The effect is indeed in line with another concepthypothesized to play a role in language acquisitionon a usage-based account, viz.
pre-emption (Gold-52uterancsion uoi12.2ipe1cmmmccteran31ite.2ip u1ite.2ipclna.2pe.2ip uep2se.ncmm ueyynt.nlcmmmca.e.2ipe(, uid)nt.ce(.nyet.
uid)nt.cbe(.nyet.
f??
f??
f??
f??
f????
?b n????
?b ?
??
?b ?
??
?b ?
??
?b ?u(na.cet.
uoi12.2ipe1cbmmm uep2se.ncmmm ulna.2pe.2ipc1ite.2ip u1ite.2ipcnp.2., f??
f??
f?????
?
???
?
???
?uet.
uoi12.2ipe1cmmm uep2se.nc?ne(n( u2pln?np3lnp.3n?a.
uid)nt.ce(.nyet.
f??
f??
f????
?b ?
?
??
?b ????
(b) (c)(a)Figure 7: Some representations at t = 2000berg, 2006, 94-95).
Pre-emption is the effect thata language user will select a more concrete rep-resentation over the combination of more abstractones.
The effect can be reconceptualized in thismodel as an epiphenomenon of the way the prob-ability model works: simply because combiningfewer constructions in a derivation is often moreprobable than combining more constructions, theformer derivation will be selected over the lat-ter.
Pre-emption is typically invoked to explain theblocking of overgeneralization patterns, and an in-teresting future step will be to see if the model cansimulate developmental patterns for well-knowncases of overgeneralization errors.The potential for abstraction The paradigma-tization operation allows the model to go beyondobserved concrete instances of form-meaningpairings: without it, unseen situations could neverbe fully expressed.
Despite this potential, we haveseen that the model relies on highly concrete con-structions.
The concreteness of the used patterns,however, does not imply the absence of more ab-stract representations.
Fig.
7 gives three exam-ples of constructions in C in one simulation.
Con-struction (a) could be seen as a verb-island con-struction (Tomasello, 1992, 23-24).
The secondconstituent is phonologically specified with put,and the other arguments are open, but mapped tospecific semantic functions.
This pattern allowsfor the expression of many caused-motion events.Construction (b) is the inverse of (a): the argu-ments are phonologically specified, but the verb-slot is open.
This would be a case of a pronominalargument frame [you V it], which have been foundto be helpful in the bootstrapping of verbal mean-ings (Tomasello, 2001).
Finally, (c) presents a caseof full abstraction.
This construction licenses ut-terances such as I sit here, you stay there and er-roneous ones like he sits on (which, again, will bepre-empted in the generation of utterances if moreconcrete constructions licence he sits on it).Summarizing, abstract constructions are ac-quired, but only used for those cases in which noconcrete construction is available.
This is in linewith the usage-based hypotheses that abstract con-structions do emerge, but that for much of lan-guage production, a language user can rely onhighly concrete patterns.
A next step will beto measure the development of abstractness andlength over the constructions themselves, ratherthan the parses and generations they allow.7 ConclusionThis, admittedly complex, model forms an attemptto model different learning mechanisms in interac-tion from a usage-based constructionist perspec-tive.
Starting with an empty set of linguistic rep-resentations, the model acquires words and gram-matical constructions simultaneously.
The learn-ing mechanisms allow the model to build up in-creasingly abstract, as well as increasingly longconstructions.
With these developing representa-tions, we showed how the model gets better overtime at understanding the input item, performingrelatively robustly under propositional uncertainty.Moreover, in the generation experiment, themodel shows patterns of production (increasinglylong utterances) similar to those of children.
Animportant future step will be to look at these pro-ductions more closely and investigate if they alsoconverge on more detailed patterns of develop-ment in the production of children (e.g.
item-specificity, as hypothesized on the usage-basedview).
Despite highly concrete constructions suf-ficing for most of production, inspection of the ac-quired representations tells us that more abstractconstructions are learned as well.
Here, an inter-esting next step would be to simulate patterns ofovergeneralization in children?s production.AcknowledgementsWe would like to thank three anonymous review-ers for their valuable and thoughtful comments.We gratefully acknowledge the funding of BBthrough NWO of the Netherlands (322.70.001)and AF and SS through NSERC of Canada.53ReferencesNameera Akhtar and Michael Tomasello.
1997.Young Children?s Productivity With Word Or-der and Verb Morphology.
Developmental Psy-chology, 33(6):952?965.Afra Alishahi and Suzanne Stevenson.
2008 AComputational Model of Early Argument Struc-ture Acquisition.
Cognitive Science, 32(5):789?834.Afra Alishahi and Suzanne Stevenson.
2010.
Acomputational model of learning semantic rolesfrom child-directed language.
Language andCognitive Processes, 25(1):50?93.Ben Ambridge, Julian M Pine, and Caroline FRowland.
2012.
Semantics versus statistics inthe retreat from locative overgeneralization er-rors.
Cognition, 123(2):260?79.Colin Bannard, Elena Lieven, and MichaelTomasello.
2009.
Modeling children?s earlygrammatical knowledge.
Proceedings of theNational Academy of Sciences of the UnitedStates of America, 106(41):17284?9.Martin D.S.
Braine.
1976.
Children?s first wordcombinations.
University of Chicago Press,Chicago, IL.Joan Bybee.
1995.
Regular morphology and thelexicon.
Language and Cognitive Processes, 10(5):425?455.Nancy C.-L. Chang.
2008.
Constructing Gram-mar: A computational model of the emergenceof early constructions.
Dissertation, Universityof California, Berkeley.Daniel Freudenthal, Julian Pine, and Fernand Go-bet.
2010.
Explaining quantitative variation inthe rate of Optional Infinitive errors across lan-guages: a comparison of MOSAIC and the Vari-ational Learning Model.
Journal of Child Lan-guage, 37(3):643?69.Adele E. Goldberg.
1995.
Constructions.
AConstruction Grammar Approach to ArgumentStructure.
Chicago University Press, Chicago,IL.Adele E Goldberg.
2003.
Constructions: a newtheoretical approach to language.
Trends inCognitive Sciences, 7(5):219?224.Adele E. Goldberg.
2006.
Constructions at Work.The Nature of Generalization in Language.
Ox-ford University Press, Oxford.Paul Kay.
2002.
An Informal Sketch of a FormalArchitecture for Construction Grammar.
Gram-mars, 5:1?19.Tom Kwiatkowski, Sharon Goldwater, LukeZettlemoyer, and Mark Steedman.
2012.
AProbabilistic Model of Syntactic and Seman-tic Acquisition from Child-Directed Utterancesand their Meanings.
In Proceedings EACL.Ronald W. Langacker.
1989.
Foundations of Cog-nitive Grammar, Volume I. Stanford UniversityPress.Ronald W. Langacker.
2009.
A dynamic view ofusage and language acquisition.
Cognitive Lin-guistics, 20(3):627?640.Jeffrey M Siskind.
1996.
A computational study ofcross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):39?91.Michael Tomasello.
1992.
First Verbs: A studyof early grammatical development.
CambridgeUniversity Press, Cambridge, UK.Michael Tomasello.
2001 Perceiving intentionsand learning words in the second year of life.In Melissa Bowerman and Stephen C. Levinson,editors, Language Acquisition and ConceptualDevelopment, chapter 5, pages 132?158.
Cam-bridge University Press, Cambridge, UK.Michael Tomasello.
2003.
Constructing a lan-guage: A Usage-Based Theory of LanguageAcquisition.
Harvard University Press, Cam-bridge, MA.Arie Verhagen.
2009 The conception of construc-tions as complex signs.
Emergence of struc-ture and reduction to usage.
Constructions andFrames, 1:119?152.54
