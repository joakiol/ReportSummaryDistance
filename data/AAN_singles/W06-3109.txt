Proceedings of the Workshop on Statistical Machine Translation, pages 64?71,New York City, June 2006. c?2006 Association for Computational LinguisticsGeneralized Stack Decoding Algorithms for Statistical Machine Translation?Daniel Ortiz Mart??nezInst.
Tecnolo?gico de Informa?ticaUniv.
Polite?cnica de Valencia46071 Valencia, Spaindortiz@iti.upv.esIsmael Garc?
?a VareaDpto.
de InformaticaUniv.
de Castilla-La Mancha02071 Albacete, Spainivarea@info-ab.uclm.esFrancisco Casacuberta NollaDpto.
de Sist Inf.
y Comp.Univ.
Polite?c.
de Valencia46071 Valencia, Spainfcn@dsic.upv.esAbstractIn this paper we propose a generalizationof the Stack-based decoding paradigm forStatistical Machine Translation.
The wellknown single and multi-stack decodingalgorithms defined in the literature havebeen integrated within a new formalismwhich also defines a new family of stack-based decoders.
These decoders allowsa tradeoff to be made between the ad-vantages of using only one or multiplestacks.
The key point of the new formal-ism consists in parameterizeing the num-ber of stacks to be used during the de-coding process, and providing an efficientmethod to decide in which stack each par-tial hypothesis generated is to be inserted-during the search process.
Experimentalresults are also reported for a search algo-rithm for phrase-based statistical transla-tion models.1 IntroductionThe translation process can be formulated from astatistical point of view as follows: A source lan-guage string fJ1 = f1 .
.
.
fJ is to be translated intoa target language string eI1 = e1 .
.
.
eI .
Every tar-get string is regarded as a possible translation for thesource language string with maximum a-posterioriprobability Pr(eI1|fJ1 ).
According to Bayes?
theo-rem, the target string e?I1 that maximizes1 the product?This work has been partially supported by the Spanishproject TIC2003-08681-C02-02, the Agencia Valenciana deCiencia y Tecnolog?
?a under contract GRUPOS03/031, the Gen-eralitat Valenciana, and the project HERMES (Vicerrectoradode Investigacio?n - UCLM-05/06)1Note that the expression should also be maximized by I ;however, for the sake of simplicity we suppose that it is known.of both the target language model Pr(eI1) and thestring translation model Pr(fJ1 |eI1) must be chosen.The equation that models this process is:e?I1 = arg maxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (1)The search/decoding problem in SMT consists insolving the maximization problem stated in Eq.
(1).In the literature, we can find different techniques todeal with this problem, ranging from heuristic andfast (as greedy decoders) to optimal and very slowdecoding algorithms (Germann et al, 2001).
Also,under certain circumstances, stack-based decoderscan obtain optimal solutions.Many works (Berger et al, 1996; Wang andWaibel, 1998; Germann et al, 2001; Och et al,2001; Ort?
?z et al, 2003) have adopted different typesof stack-based algorithms to solve the global searchoptimization problem for statistical machine trans-lation.
All these works follow two main differentapproaches according to the number of stacks usedin the design and implementation of the search algo-rithm (the stacks are used to store partial hypotheses,sorted according to their partial score/probability,during the search process) :?
On the one hand, in (Wang and Waibel, 1998;Och et al, 2001) a single stack is used.
Inthat case, in order to make the search feasible,the pruning of the number of partial hypothe-ses stored in the stack is needed.
This causesmany search errors due to the fact that hy-potheses covering a different number of source(translated) words compete in the same condi-tions.
Therefore, the greater number of coveredwords the higher possibility to be pruned.?
On the other hand (Berger et al, 1996; Ger-mann et al, 2001) make use of multiple stacks64(one for each set of source covered/translatedwords in the partial hypothesis) in order tosolve the disadvantages of the single-stack ap-proach.
By contrast, the problem of findingthe best hypothesis to be expanded introducesan exponential term in the computational com-plexity of the algorithm.In (Ort?
?z et al, 2003) the authors present an em-pirical comparison (about efficiency and translationquality) of the two approaches paying special atten-tion to the advantages and disadvantages of the twoapproaches.In this paper we present a new formalism consist-ing of a generalization of the classical stack-baseddecoding paradigm for SMT.
This new formalismdefines a new family of stack-based decoders, whichalso integrates the well known stack-based decodingalgorithms proposed so far within the framework ofSMT, that is single and multi-stack decoders.The rest of the paper is organized as follows: insection 2 the phrase-based approach to SMT is de-picted; in section 3 the main features of classicalstack-based decoders are presented; in section 4 thenew formalism is presented and in section 5 exper-imental results are shown; finally some conclusionsare drawn in section 6.2 Phrase Based Statistical MachineTranslationDifferent translation models (TMs) have been pro-posed depending on how the relation between thesource and the target languages is structured; that is,the way a target sentence is generated from a sourcesentence.
This relation is summarized using the con-cept of alignment; that is, how the constituents (typ-ically words or group-of-words) of a pair of sen-tences are aligned to each other.
The most widelyused single-word-based statistical alignment mod-els (SAMs) have been proposed in (Brown et al,1993; Ney et al, 2000).
On the other hand, modelsthat deal with structures or phrases instead of singlewords have also been proposed: the syntax trans-lation models are described in (Yamada and Knight,2001) , alignment templates are used in (Och, 2002),and the alignment template approach is re-framedinto the so-called phrase based translation (PBT)in (Marcu and Wong, 2002; Zens et al, 2002; Koehnet al, 2003; Toma?s and Casacuberta, 2003).For the translation model (Pr(fJ1 |eI1)) in Eq.
(1),PBT can be explained from a generative point ofview as follows (Zens et al, 2002):1.
The target sentence eI1 is segmented into Kphrases (e?K1 ).2.
Each target phrase e?k is translated into a sourcephrase f?
.3.
Finally, the source phrases are reordered in or-der to compose the source sentence f?K1 = fJ1 .In PBT, it is assumed that the relations betweenthe words of the source and target sentences canbe explained by means of the hidden variable a?K1 ,which contains all the decisions made during thegenerative story.Pr(fJ1 |eI1) =?K,a?K1Pr(, f?K1 , a?K1 |e?K1 )=?K,a?K1Pr(a?K1 |e?K1 )Pr(f?K1 |a?K1 , e?K1 )(2)Different assumptions can be made from the pre-vious equation.
For example, in (Zens et al, 2002)the following model is proposed:p?
(fJ1 |eI1) = ?
(eI1)?K,a?K1K?k=1p(f?k|e?a?k ) (3)where a?k notes the index of the source phrase e?which is aligned with the k-th target phrase f?k andthat all possible segmentations have the same proba-bility.
In (Toma?s and Casacuberta, 2001; Zens et al,2002), it also is assumed that the alignments must bemonotonic.
This led us to the following equation:p?
(fJ1 |eI1) = ?
(eI1)?K,a?K1K?k=1p(f?k|e?k) (4)In both cases the model parameters that have to beestimated are the translation probabilities betweenphrase pairs (?
= {p(f?
|e?
)}), which typically are es-timated as follows:p(f?
|e?)
= N(f?
, e?)N(e?)
(5)65where N(f?
|e?)
is the number of times that f?
havebeen seen as a translation of e?
within the trainingcorpus.3 Stack-Decoding AlgorithmsThe stack decoding algorithm, also called A?
algo-rithm, was first introduced by F. Jelinek in (Jelinek,1969).
The stack decoding algorithm attempts togenerate partial solutions, called hypotheses, until acomplete translation is found2; these hypotheses arestored in a stack and ordered by their score.
Typi-cally, this measure or score is the probability of theproduct of the translation and the language modelsintroduced above.
The A?
decoder follows a se-quence of steps for achieving a complete (and possi-bly optimal) hypothesis:1.
Initialize the stack with an empty hypothesis.2.
Iterate(a) Pop h (the best hypothesis) off the stack.
(b) If h is a complete sentence, output h andterminate.
(c) Expand h.(d) Go to step 2a.The search is started from a null string and obtainsnew hypotheses after an expansion process (step 2c)which is executed at each iteration.
The expansionprocess consists of the application of a set of op-erators over the best hypothesis in the stack, as itis depicted in Figure 1.
Thus, the design of stackdecoding algorithms involves defining a set of oper-ators to be applied over every hypothesis as well asthe way in which they are combined in the expansionprocess.
Both the operators and the expansion algo-rithm depend on the translation model that we use.For the case of the phrase-based translation modelsdescribed in the previous section, the operator add isdefined, which adds a sequence of words to the tar-get sentence, and aligns it with a sequence of wordsof the source sentence.The number of hypotheses to be stored during thesearch can be huge.
In order then to avoid mem-2Each hypothesis has associated a coverage vector of lengthJ , which indicates the set of source words already cov-ered/translated so far.
In the following we will refer to thissimply as coverage.Figure 1: Flow chart associated to the expansion ofa hypothesis when using an A?
algorithm.ory overflow problems, the maximum number of hy-potheses that a stack may store has to be limited.
Itis important to note that for a hypothesis, the higherthe aligned source words, the worse the score.
Thesehypotheses will be discarded sooner when an A?search algorithm is used due to the stack length lim-itation.
Because of this, the multi-stack algorithmswere introduced.Multi-stack algorithms store those hypotheseswith different subsets of source aligned words in dif-ferent stacks.
That is to say, given an input sentencefJ1 composed of J words, multi-stack algorithmsemployes 2J stacks to translate it.
Such an organi-zation improves the pruning of the hypotheses whenthe stack length limitation is exceeded, since onlyhypotheses with the same number of covered posi-tions can compete with each other.All the search steps given for A?
algorithm canalso be applied here, except step 2a.
This is dueto the fact that multiple stacks are used instead ofonly one.
Figure 2 depicts the expansion processthat the multi-stack algorithms execute, which isslightly different than the one presented in Figure 1.Multi-stack algorithms have the negative property ofspending significant amounts of time in selecting thehypotheses to be expanded, since at each iteration,the best hypothesis in a set of 2J stacks must besearched for (Ort?
?z et al, 2003).
By contrast, for theA?
algorithm, it is not possible to reduce the lengthof the stack in the same way as in the multi-stackcase without loss of translation quality.Additionally, certain translation systems, e.g.
thePharaoh decoder (Koehn, 2003) use an alternative66Figure 2: Flow chart associated to the expansion ofa hypothesis when using a multi-stack algorithm.approach which consists in assigning to the samestack, those hypotheses with the same number ofsource words covered.4 Generalized Stack-Decoding AlgorithmsAs was mentioned in the previous section, given asentence fJ1 to be translated, a single stack decod-ing algorithm employs only one stack to perform thetranslation process, while a multi-stack algorithmemploys 2J stacks.
We propose a possible way tomake a tradeoff between the advantages of both al-gorithms that introduces a new parameter which willbe referred to as the granularity of the algorithm.The granularity parameter determines the number ofstacks used during the decoding process.4.1 Selecting the granularity of the algorithmThe granularity (G) of a generalized stack algorithmis an integer which takes values between 1 and J ,where J is the number of words which compose thesentence to translate.Given a sentence fJ1 to be translated, a general-ized stack algorithm with a granularity parameterequal to g, will have the following features:?
The algorithm will use at most 2g stacks to per-form the translation?
Each stack will contain hypotheses which have2J?g different coverages of fJ1?
If the algorithm can store at most S = s hy-potheses, then, the maximum size of each stackwill be equal to s2g4.2 Mapping hypotheses to stacksGeneralized stack-decoding algorithms require amechanism to decide in which stack each hypothesisis to be inserted.
As stated in section 4.1, given aninput sentence fJ1 and a generalized stack-decodingalgorithm with G = g, the decoder will work with2g stacks, and each one will contain 2J?g differentcoverages.
Therefore, the above mentioned mecha-nism can be expressed as a function which will bereferred to as the ?
function.
Given a hypothesiscoverage composed of J bits, the ?
function returna stack identifier composed of only g bits:?
: ({0, 1})J ??
({0, 1})g (6)Generalized stack algorithms are strongly in-spired by multi-stack algorithms; however, bothtypes of algorithms differ in the way the hypothesisexpansion is performed.
Figure 3 shows the expan-sion algorithm of a generalized stack decoder witha granularity parameter equal to g and a function ?which maps hypotheses coverages to stacks.Figure 3: Flow chart associated to the expansion ofa hypothesis when using a generalized-stack algo-rithm.The function ?
can be defined in many ways,but there are two essential principles which must betaken into account:?
The ?
function must be efficiently calculated?
Hypotheses whose coverage have a similarnumber of bits set to one must be assigned tothe same stack.
This requirement allows thepruning of the stacks to be improved, since the67hypotheses with a similar number of coveredwords can compete fairlyA possible way to implement the ?
function,namely ?1, consists in simply shifting the coveragevector J ?
g positions to the right, and then keepingonly the first g bits.
Such a proposal is very easyto calculate, however, it has a poor performance ac-cording to the second principle explained above.A better alternative to implement the ?
function,namely ?2, can be formulated as a composition oftwo functions.
A constructive definition of such aimplementation is detailed next:1.
Let us suppose that the source sentence is com-posed by J words, we order the set of J bitnumbers as follows: first the numbers which donot have any bit equal to one, next, the numberswhich have only one bit equal to one, and so on2.
Given the list of numbers described above, wedefine a function which associates to each num-ber of the list, the order of the number withinthis list3.
Given the coverage of a partial hypothesis, x,the stack on which this partial hypothesis is tobe inserted is obtained by a two step process:First, we obtain the image of x returned by thefunction described above.
Next, the result isshifted J ?
g positions to the right, keeping thefirst g bitsLet ?
be the function that shifts a bit vector J ?
gpositions to the right, keeping the first g bits; and let?
be the function that for each coverage returns itsorder:?
: ({0, 1})J ??
({0, 1})J (7)Then, ?2 is expressed as follows:?2(x) = ?
?
?
(x) (8)Table 1 shows an example of the values which re-turns the ?1 and the ?2 functions when the input sen-tence has 4 words and the granularity of the decoderis equal to 2.
As it can be observed, ?2 functionperforms better than ?1 function according to thesecond principle described at the beginning of thissection.x ?1(x) ?
(x) ?2(x)0000 00 0000 000001 00 0001 000010 00 0010 000100 01 0011 001000 10 0100 010011 00 0101 010101 01 0110 010110 01 0111 011001 10 1000 101010 10 1001 101100 11 1010 100111 01 1011 101011 10 1100 111101 11 1101 111110 11 1110 111111 11 1111 11Table 1: Values returned by the ?1 and ?2 functiondefined as a composition of the ?
and ?
functions4.3 Single and Multi Stack AlgorithmsThe classical single and multi-stack decoding al-gorithms can be expressed/instantiated as particularcases of the general formalism that have been pro-posed.Given the input sentence fJ1 , a generalized stackdecoding algorithm with G = 0 will have the fol-lowing features:?
The algorithm works with 20 = 1 stacks.?
Such a stack may store hypotheses with 2J dif-ferent coverages.
That is to say, all possiblecoverages.?
The mapping function returns the same stackidentifier for each coverageThe previously defined algorithm has the samefeatures as a single stack algorithm.Let us now consider the features of a generalizedstack algorithm with a granularity value of J :?
The algorithm works with 2J stacks?
Each stack may store hypotheses with only20 = 1 coverage.?
The mapping function returns a different stackidentifier for each coverageThe above mentioned features characterizes themulti-stack algorithms described in the literature.68EUTRANS-I XEROXSpanish English Spanish EnglishTrainingSentences 10,000 55,761Words 97,131 99,292 753,607 665,400Vocabulary size 686 513 11,051 7,957Average sentence leng.
9.7 9.9 13.5 11.9TestSentence 2,996 1,125Words 35,023 35,590 10,106 8,370Perplexity (Trigrams) ?
3.62 ?
48.3Table 2: EUTRANS-I and XEROX corpus statistics5 Experiments and ResultsIn this section, experimental results are presented fortwo well-known tasks: the EUTRANS-I (Amengualet al, 1996), a small size and easy translation task,and the XEROX (Cubel et al, 2004), a medium sizeand difficult translation task.
The main statistics ofthese corpora are shown in Table 2.
The translationresults were obtained using a non-monotone gener-alized stack algorithm.
For both tasks, the trainingof the different phrase models was carried out us-ing the publicly available Thot toolkit (Ortiz et al,2005).Different translation experiments have been car-ried out, varying the value of G (ranging from 0 to8) and the maximum number of hypothesis that thealgorithm is allow to store for all used stacks (S)(ranging from 28 to 212).
In these experiments thefollowing statistics are computed: the average score(or logProb) that the phrase-based translation modelassigns to each hypothesis, the translation quality(by means of WER and Bleu measures), and the av-erage time (in secs.)
per sentence3.In Figures 4 and 5 two plots are shown: the av-erage time per sentence (left) and the average score(right), for EUTRANS and XEROX corpora respec-tively.
As can be seen in both figures, the bigger thevalue of G the lower the average time per sentence.This is true up to the value of G = 6.
For highervalues of G (keeping fixed the value of S) the aver-age time per sentence increase slightly.
This is dueto the fact that at this point the algorithm start tospend more time to decide which hypothesis is to beexpanded.
With respect to the average score similarvalues are obtained up to the value of G = 4.
Higher3All the experiments have been executed on a PC with a2.60 Ghz Intel Pentium 4 processor with 2GB of memory.
Allthe times are given in seconds.values of G slightly decreases the average score.
Inthis case, as G increases, the number of hypothe-ses per stack decreases, taking into account that thevalue of S is fixed, then the ?optimal?
hypothesiscan easily be pruned.In tables 3 and 4 detailed experiments are shownfor a value of S = 212 and different values of G, forEUTRANS and XEROX corpora respectively.G WER Bleu secsXsent logprob0 6.6 0.898 2.4 -18.881 6.6 0.898 1.9 -18.802 6.6 0.897 1.7 -18.814 6.6 0.898 1.3 -18.776 6.7 0.896 1.1 -18.838 6.7 0.896 1.5 -18.87Table 3: Translation experiments for EUTRANS cor-pus using a generalized stack algorithm with differ-ent values of G and a fixed value of S = 212G WER Bleu secsXsent logProb0 32.6 0.658 35.1 -33.921 32.8 0.657 20.4 -33.862 33.1 0.656 12.8 -33.794 32.9 0.657 7.0 -33.706 33.7 0.652 6.3 -33.698 36.3 0.634 13.7 -34.10Table 4: Translation experiments for XEROX cor-pus using a generalized stack algorithm with differ-ent values of G and a fixed value of S = 212According to the experiments presented here wecan conclude that:?
The results correlates for the two consideredtasks: one small and easy, and other larger anddifficult.?
The proposed generalized stack decodingparadigm can be used to make a tradeoff be-6900.511.522.50  1  2  3  4  5  6  7  8timeGS=512S=1024S=2048S=4096-20-19.5-19-18.5-180  1  2  3  4  5  6  7  8Avg.
ScoreGS=512S=1024S=2048S=4096Figure 4: Average time per sentence (in secs.)
and average score per sentence.
The results are shown fordifferent values of G and S for the EUTRANS corpus.05101520253035400  1  2  3  4  5  6  7  8timeGS=512S=1024S=2048S=4096-37-36-35-34-33-32-310  1  2  3  4  5  6  7  8Avg.
ScoreGS=512S=1024S=2048S=4096Figure 5: Average time per sentence (in secs.)
and average score per sentence.
The results are shown fordifferent values of G and S for the XEROX corpus.tween the advantages of classical single andmulti-stack decoding algorithms.?
As we expected, better results (regarding effi-ciency and accuracy) are obtained when usinga value of G between 0 and J .6 Concluding RemarksIn this paper, a generalization of the stack-decodingparadigm has been proposed.
This new formalismincludes the well known single and multi-stack de-coding algorithms and a new family of stack-basedalgorithms which have not been described yet in theliterature.Essentially, generalized stack algorithms use a pa-rameterized number of stacks during the decodingprocess, and try to assign hypotheses to stacks suchthat there is ?fair competition?
within each stack,i.e., brother hypotheses should cover roughly thesame number of input words (and the same words)if possible.The new family of stack-based algorithms allowsa tradeoff to be made between the classical singleand multi-stack decoding algorithms.
For this pur-pose, they employ a certain number of stacks be-tween 1 (the number of stacks used by a single stackalgorithm) and 2J (the number of stacks used by amultiple stack algorithm to translate a sentence withJ words.
)According to the experimental results, it has beenproved that an appropriate value of G yields in astack decoding algorithm that outperforms (in effi-70ciency and acuraccy) the single and multi-stack al-gorithms proposed so far.As future work, we plan to extend the experimen-tation framework presented here to larger and morecomplex tasks as HANSARDS and EUROPARL cor-pora.ReferencesJ.C.
Amengual, J.M.
Bened?
?, M.A.
Castao, A. Marzal,F.
Prat, E. Vidal, J.M.
Vilar, C. Delogu, A. di Carlo,H.
Ney, and S. Vogel.
1996.
Definition of a ma-chine translation task and generation of corpora.
Tech-nical report d4, Instituto Tecnolo?gico de Informa?tica,September.
ESPRIT, EuTrans IT-LTR-OS-20268.Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,Vincent J. Della Pietra, John R. Gillett, A. S. Kehler,and R. L. Mercer.
1996.
Language translation ap-paratus and method of using context-based translationmodels.
United States Patent, No.
5510981, April.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.E.
Cubel, J. Civera, J. M. Vilar, A. L. Lagarda,E.
Vidal, F. Casacuberta, D.
Pico?, J. Gonza?lez, andL.
Rodr??guez.
2004.
Finite-state models for computerassisted translation.
In Proceedings of the 16th Euro-pean Conference on Artificial Intelligence (ECAI04),pages 586?590, Valencia, Spain, August.
IOS Press.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proc.of the 39th Annual Meeting of ACL, pages 228?235,Toulouse, France, July.F.
Jelinek.
1969.
A fast sequential decoding algorithmusing a stack.
IBM Journal of Research and Develop-ment, 13:675?685.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of theHLT/NAACL, Edmonton, Canada, May.Phillip Koehn.
2003.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
User manual and description.
Technical report,USC Information Science Institute, December.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of the EMNLP Conference, pages1408?1414, Philadelphia, USA, July.Hermann Ney, Sonja Nie?en, Franz J. Och, HassanSawaf, Christoph Tillmann, and Stephan Vogel.
2000.Algorithms for statistical translation of spoken lan-guage.
IEEE Trans.
on Speech and Audio Processing,8(1):24?36, January.Franz J. Och, Nicola Ueffing, and Hermann Ney.
2001.An efficient A* search algorithm for statistical ma-chine translation.
In Data-Driven Machine Transla-tion Workshop, pages 55?62, Toulouse, France, July.Franz Joseph Och.
2002.
Statistical Machine Trans-lation: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, Computer Science Department,RWTH Aachen, Germany, October.D.
Ort?
?z, Ismael Garc?
?a-Varea, and Francisco Casacu-berta.
2003.
An empirical comparison of stack-baseddecoding algorithms for statistical machine transla-tion.
In New Advance in Computer Vision, Lec-ture Notes in Computer Science.
Springer-Verlag.
1stIberian Conference on Pattern Recongnition and Im-age Analysis -IbPRIA2003- Mallorca.
Spain.
June.D.
Ortiz, I. Garca-Varea, and F. Casacuberta.
2005.
Thot:a toolkit to train phrase-based statistical translationmodels.
In Tenth Machine Translation Summit, pages141?148, Phuket, Thailand, September.J.
Toma?s and F. Casacuberta.
2001.
Monotone statisticaltranslation using word groups.
In Procs.
of the Ma-chine Translation Summit VIII, pages 357?361, Santi-ago de Compostela, Spain.J.
Toma?s and F. Casacuberta.
2003.
Combining phrase-based and template-based models in statistical ma-chine translation.
In Pattern Recognition and ImageAnalisys, volume 2652 of LNCS, pages 1021?1031.Springer-Verlag.
1st bPRIA.Ye-Yi Wang and Alex Waibel.
1998.
Fast decodingfor statistical machine translation.
In Proc.
of theInt.
Conf.
on Speech and Language Processing, pages1357?1363, Sydney, Australia, November.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
of the 39thAnnual Meeting of ACL, pages 523?530, Toulouse,France, July.R.
Zens, F.J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Advances in artificialintelligence.
25.
Annual German Conference on AI,volume 2479 of Lecture Notes in Computer Science,pages 18?32.
Springer Verlag, September.71
