Automatic Retrieval and Clustering of Similar WordsDekang LinDepartment of Computer ScienceUniversity of ManitobaWinnipeg, Manitoba, Canada R3T 2N2lindek@ cs.umanitoba.caAbstractBootstrapping semantics from text is one of thegreatest challenges in natural language learning.We first define a word similarity measure based onthe distributional pattern of words.
The similaritymeasure allows us to construct a thesaurus using aparsed corpus.
We then present a new evaluationmethodology for the automatically constructed the-saurus.
The evaluation results show that the the-saurns is significantly closer to WordNet han RogetThesaurus is.1 IntroductionThe meaning of an unknown word can often beinferred from its context.
Consider the following(slightly modified) example in (Nida, 1975, p.167):(1) A bottle of tezgiiino is on the table.Everyone likes tezgiiino.Tezgiiino makes you drunk.We make tezgiiino out of corn.The contexts in which the word tezgiiino is usedsuggest hat tezgiiino may be a kind of alcoholicbeverage made from corn mash.Bootstrapping semantics from text is one of thegreatest challenges in natural anguage learning.
Ithas been argued that similarity plays an importantrole in word acquisition (Gentner, 1982).
Identify-ing similar words is an initial step in learning thedefinition of a word.
This paper presents a methodfor making this first step.
For example, given a cor-pus that includes the sentences in (1), our goal is tobe able to infer that tezgiiino is similar to "beer","wine", "vodka", etc.In addition to the long-term goal of bootstrap-ping semantics from text, automatic identificationof similar words has many immediate applications.The most obvious one is thesaurus construction.
Anautomatically created thesaurus offers many advan-tages over manually constructed thesauri.
Firstly,the terms can be corpus- or genre-specific.
Man-ually constructed general-purpose dictionaries andthesauri nclude many usages that are very infre-quent in a particular corpus or genre of documents.For example, one of the 8 senses of "company" inWordNet 1.5 is a "visitor/visitant", which is a hy-ponym of "person".
This usage of the word is prac-tically never used in newspaper articles.
However,its existance may prevent a co-reference r cognizerto rule out the possiblity for personal pronouns torefer to "company".
Secondly, certain word us-ages may be particular to a period of time, whichare unlikely to be captured by manually compiledlexicons.
For example, among 274 occurrences ofthe word "westerner" in a 45 million word San JoseMercury corpus, 55% of them refer to hostages.
Ifone needs to search hostage-related articles, "west-emer" may well be a good search term.Another application of automatically extractedsimilar words is to help solve the problem of datasparseness in statistical natural language process-ing (Dagan et al, 1994; Essen and Steinbiss, 1992).When the frequency of a word does not warrant reli-able maximum likelihood estimation, its probabilitycan be computed as a weighted sum of the probabil-ities of words that are similar to it.
It was shown in(Dagan et al, 1997) that a similarity-based smooth-ing method achieved much better esults than back-off smoothing methods in word sense disambigua-tion.The remainder of the paper is organized as fol-lows.
The next section is concerned with similari-ties between words based on their distributional pat-terns.
The similarity measure can then be used tocreate a thesaurus.
In Section 3, we evaluate theconstructed thesauri by computing the similarity be-tween their entries and entries in manually createdthesauri.
Section 4 briefly discuss future work inclustering similar words.
Finally, Section 5 reviewsrelated work and summarize our contributions.7682 Word  S imi lar i tyOur similarity measure is based on a proposal in(Lin, 1997), where the similarity between two ob-jects is defined to be the amount of information con-tained in the commonality between the objects di-vided by the amount of information in the descrip-tions of the objects.We use a broad-coverage parser (Lin, 1993; Lin,1994) to extract dependency triples from the textcorpus.
A dependency triple consists of two wordsand the grammatical relationship between them inthe input sentence.
For example, the triples ex-tracted from the sentence "I have a brown dog" are:(2) (have subj I), (I subj-of have), (dog obj-ofhave), (dog adj-mod brown), (brownadj-mod-of dog), (dog det a), (a det-of dog)We use the notation IIw, r, w'll to denote the fre-quency count of the dependency triple (w, r, w ~) inthe parsed corpus.
When w, r, or w ~ is the wildcard (*), the frequency counts of all the depen-dency triples that matches the rest of the pattern aresummed up.
For example, Ilcook, obj, *11 is the to-tal occurrences of cook-object relationships in theparsed corpus, and I1., *, *11 is the total number ofdependency triples extracted from the parsed cor-pus.The description of a word w consists of the fre-quency counts of all the dependency triples thatmatches the pattern (w, .
,  .).
The commonality be-tween two words consists of the dependency triplesthat appear in the descriptions of both words.
Forexample, (3) is the the description of the word"cell".
(3) Ilcell, subj-of, absorbll=lIlcell, subj-of, adapt\[l=lIlcell, subj-of, behavell=l\[Icell, pobj-of, in11=159\[\[cell, pobj-of, insidell=16Ilcell, pobj-of, intoll=30Ilcell, nmod-of, abnormalityll=3Ilcell, nmod-of, anemiall=8Ilcell, nmod-of, architecturell=l\[\[cell, obj-of, attackl\[=6\[\[cell, obj-of, bludgeon\[\[=l\[Icell, obj-of, callll=l 1Hcell, obj-of, come froml\[=3Ilcell, obj-of, containll--4Ilcell, obj-of, decoratell=2.
.
.
* * *I\[cell, nmod, bacteriall=3Ilcell, nmod, blood vesselH=lIIcell, nmod, bodYll=2Ilcell, nmod, bone marrowll=2Ilcell, nmod, burialH=lIlcell, nmod, chameleonll=lAssuming that the frequency counts of the depen-dency triples are independent of each other, the in-formation contained in the description of a word isthe sum of the information contained in each indi-vidual frequency count.To measure the information contained in thestatement IIw, r, w' H=c, we first measure the amountof information in the statement that a randomly se-lected dependency triple is (w, r, w') when we donot know the value of IIw, r,w'll.
We then mea-sure the amount of information in the same state-ment when we do know the value of II w, r, w' II.
Thedifference between these two amounts is taken to bethe information contained in Hw, r, w' \[l=c.An occurrence of a dependency triple (w, r, w')can be regarded as the co-occurrence of threeevents:A: a randomly selected word is w;B: a randomly selected ependency t pe is r;C: a randomly selected word is w ~.When the value of Ilw, r,w'll is unknown, weassume that A and C are conditionally indepen-dent given B.
The probability of A, B and C co-occurring is estimated byPMLE( B ) PMLE( A\[B )PMLE( C\[B ),where PMLE is the maximum likelihood estimationof a probability distribution andP.LE(B)= I I * , * , * l l 'P.,~E(AIB )= I I* ,~,* l l  'P, LE(CIB) =When the value of Hw, r, w~H is known, we canobtain PMLE(A, B, C) directly:PMLE(A, B, C) = \[\[w, r, wll/\[\[*, , *HLet I (w,r ,w ~) denote the amount informationcontained in Hw, r,w~\]\]=c.
Its value can be corn-769simgindZe(Wl, W2) = ~'~(r,w)eTCwl)NTCw2)Are{subj.of.obj-of} min(I(Wl, r, w), I(w2, r, w) )simHindte, (Wl, W2) = ~,(r,w)eT(w,)nT(w2) min(I(wl,  r, w), I(w2, r, w))\]T(Wl)NT(w2)I simcosine(Wl,W2) = x/IZ(w~)l?lZ(w2)l2x IT(wl)nZ(w2)l simDice(Wl, W2) = iT(wl)l+lT(w2) IsimJacard (Wl,  W2) = T(wl )OT(w2)lT(wl) + T(w2)l-IT(Wl)rlT(w2)lFigure 1: Other Similarity Measuresputed as follows:I(w,r,w')= _ Iog(PMLE(B)PMLE(A\]B)PMLE(CIB))--(-- log PMLE(A, B, C))- log IIw,r,wfl?ll*,r,*ll- -  IIw,r,*ll xll*,r,w'llIt is worth noting that I(w,r,w') is equal tothe mutual information between w and w' (Hindle,1990).Let T(w) be the set of pairs (r, w') such thatlog Iw'r'w'lr?ll*'r'*ll is positive.
We define the sim-wlr~* X *~r~w !ilarity sim(wl, w2) between two words wl and w2as follows:)"~(r,w)eT(w, )NT(w~)(I(Wl, r w) + I(w2, r, w) )~-,(r,w)eT(wl) I(Wl, r, w) q- ~(r,w)eT(w2) I(w2, r, w)We parsed a 64-million-word corpus consistingof the Wall Street Journal (24 million words), SanJose Mercury (21 million words) and AP Newswire(19 million words).
From the parsed corpus, weextracted 56.5 million dependency triples (8.7 mil-lion unique).
In the parsed corpus, there are 5469nouns, 2173 verbs, and 2632 adjectives/adverbs thatoccurred at least 100 times.
We computed the pair-wise similarity between all the nouns, all the verbsand all the adjectives/adverbs, u ing the above sim-ilarity measure.
For each word, we created a the-saurus entry which contains the top-N !
words thatare most similar to it.
2 The thesaurus entry for wordw has the following format:w (pos) : Wl,  81, W2, 82, .
?
?
, WN, 8Nwhere pos is a part of speech, wi is a word,si=sim(w, wi) and si's are ordered in descending'We used N=200 in our experiments2The resulting thesaurus i available at:http://www.cs.umanitoba.caflindek/sims.htm.order.
For example, the top-10 words in the noun,verb, and adjective ntries for the word "brief" areshown below:brief (noun): affidavit 0.13, petition 0.05, memo-randum 0.05, motion 0.05, lawsuit 0.05, depo-sition 0.05, slight 0.05, prospectus 0.04, docu-ment 0.04 paper 0.04 ....brief(verb): tell 0.09, urge 0.07, ask 0.07, meet0.06, appoint 0.06, elect 0.05, name 0.05, em-power 0.05, summon 0.05, overrule 0.04 ....brief (adjective): lengthy 0.13, short 0.12, recent0.09, prolonged 0.09, long 0.09, extended 0.09,daylong 0.08, scheduled 0.08, stormy 0.07,planned 0.06 ....Two words are a pair of respective nearest neigh-bors (RNNs) if each is the other's most similarword.
Our program found 543 pairs of RNN nouns,212 pairs of RNN verbs and 382 pairs of RNNadjectives/adverbs in the automatically created the-saurus.
Appendix A lists every 10th of the RNNs.The result looks very strong.
Few pairs of RNNs inAppendix A have clearly better alternatives.We also constructed several other thesauri us-ing the same corpus, but with the similarity mea-sures in Figure 1.
The measure simHinate is thesame as the similarity measure proposed in (Hin-dle, 1990), except hat it does not use dependencytriples with negative mutual information.
The mea-sure simHindle,, is the same as simHindle xcept hatall types of dependency relationships are used, in-stead of just subject and object relationships.
Themeasures imcosine, simdice and simdacard are ver-sions of similarity measures commonly used in in-formation retrieval (Frakes and Baeza-Yates, 1992).Unlike sim, simninale and simHinater, they only770210g P(c) ,~simwN(wl, w2) = maxc~ eS(w~)Ac2eS(w2) maxcesuper(c~)nsuper(c2) log P(cl+log P(c2) !21R(~l)nR(w2)l simRoget(Wl, W2) = IR(wx)l+lR(w2)lwhere S(w) is the set of senses of w in the WordNet, super(c) is the set of (possibly indirect)superclasses of concept c in the WordNet, R(w) is the set of words that belong to a same Rogetcategory as w.Figure 2: Word similarity measures based on WordNet and Rogetmake use of the unique dependency triples and ig-nore their frequency counts.3 Eva luat ionIn this section, we present an evaluation of automat-ically constructed thesauri with two manually com-piled thesauri, namely, WordNetl.5 (Miller et al,1990) and Roget Thesaurus.
We first define twoword similarity measures that are based on the struc-tures of WordNet and Roget (Figure 2).
The simi-larity measure simwN is based on the proposal in(Lin, 1997).
The similarity measure simRoget treatsall the words in Roget as features.
A word w pos-sesses the feature f if f and w belong to a sameRoget category.
The similarity between two wordsis then defined as the cosine coefficient of the twofeature vectors.With simwN and simRoget, we transform Word-Net and Roget into the same format as the automat-ically constructed thesauri in the previous ection.We now discuss how to measure the similarity be-tween two thesaurus entries.
Suppose two thesaurusentries for the same word are as follows:'tO : '//31~ 81~'//12~ 82~.
.
.
~I)N~S NTheir similarity is defined as:(4)sisFor example, (5) is the entry for "brief (noun)" inour automatically generated thesaurus and (6) and(7) are corresponding entries in WordNet hesaurusand Roget thesaurus.
(5) brief (noun): affidavit 0.13, petition 0.05,memorandum 0.05, motion 0.05, lawsuit 0.05,deposition 0.05, slight 0.05, prospectus 0.04,document 0.04 paper 0.04.
(6) brief (noun): outline 0.96, instrument 0.84,summary 0.84, affidavit 0.80, deposition0.80, law 0.77, survey 0.74, sketch 0.74,resume 0.74, argument 0.74.
(7) brief (noun): recital 0.77, saga 0.77,autobiography 0.77, anecdote 0.77, novel0.77, novelist 0.77, tradition 0.70, historian0.70, tale 0.64.According to (4), the similarity between (5) and(6) is 0.297, whereas the similarities between (5)and (7) and between (6) and (7) are 0.Our evaluation was conducted with 4294 nounsthat occurred at least 100 times in the parsed cor-pus and are found in both WordNetl.5 and the Ro-get Thesaurus.
Table 1 shows the average similaritybetween corresponding entries in different hesauriand the standard deviation of the average, whichis the standard eviation of the data items dividedby the square root of the number of data items.Since the differences among simcosine, simdice andsimJacard are very small, we only included the re-sults for simcosine in Table 1 for the sake of brevity.It can be seen that sire, Hindler and cosine aresignificantly more similar to WordNet than Rogetis, but are significantly less similar to Roget thanWordNet is.
The differences between Hindle andHindler clearly demonstrate that the use of othertypes of dependencies in addition to subject and ob-ject relationships i very beneficial.The performance of sim, Hindler and cosine arequite close.
To determine whether or not the dif-ferences are statistically significant, we computedtheir differences in similarities to WordNet and Ro-get thesaurus for each individual entry.
Table 2shows the average and standard eviation of the av-erage difference.
Since the 95% confidence inter-771Table I: Evaluation with WordNet and RogetWordNetRogetsimHindle~cosineHindleaverage0.1783970.2121990.2041790.1994020.164716~av~0.0016360.0014840.0014240.0013520.001200RogetaverageWordNet 0.178397sim 0.149045Hindler 0.14663cosine 0.135697Hindle 0.115489aav 80.0016360.0014290.0013830.0012750.001140vals of all the differences in Table 2 are on the posi-tive side, one can draw the statistical conclusion thatsimis better than simnindle ~, which is better thansimcosine.Table 2: Distribution of Differencessim-Hindle~sim-cosineHindler-cosinesim-Hindle~sim-cosineHindle~-cosineWordNetaverage ffavg0.008021 0.0004280.012798 0.0003860.004777 0.000561Rogetaverage trav80.002415 0.0004010.013349 0.0003750.010933 0.0005094 Future  WorkReliable extraction of similar words from text cor-pus opens up many possibilities for future work.
Forexample, one can go a step further by constructing atree structure among the most similar words so thatdifferent senses of a given word can be identifiedwith different subtrees.
Let w l , .
.
.
,  Wn be a list ofwords in descending order of their similarity to agiven word w. The similarity tree for w is createdas follows:?
Initialize the similarity tree to consist of a sin-gle node w.?
For i=l, 2 .
.
.
.
.
n, insert wi as a child of wjsuch that wj is the most similar one to wiamong {w, Wl .
.
.
.
.
wi-1}.For example, Figure 3 shows the similarity tree forthe top-40 most similar words to duty.
The firstnumber behind a word is the similarity of the wordto its parent.
The second number is the similarity ofthe word to the root node of the tree.dutyresponsib i l i ty  0.21role 0.12 0.iiI act ion 0.ii0.210.i0change 0.24 0.08l__.rule 0.16 0.08l__.restr ict ion 0.27 0.08I I ban 0.30 0.08I l__.sanction 0.19 0.08I schedule 0.Ii 0.07I regulat ion 0.37 0.07chal lenge 0.13 0.07l__.issue 0.13 0.07I reason 0.14 0.07I matter  0.28 0.07measure 0.22 0.07 'obl igat ion 0.12 0.10power 0.17 0.08I jur isdict ion 0.13 0.08I r ight 0.12 0.07I control  0.20 0.07I ground 0.08 0.07accountabi l i ty  0.14 0.08exper ience 0.12 0.07post 0.14 0.14job 0.17 0.I0l _ _work  0.17 0.i0I tra ining 0.Ii 0.07pos i t ion 0.25 0.10task 0.10 0.10I chore 0.ii 0.07operat ion 0.10 0.10I function 0.i0 0.08I miss ion 0.12 0.07I I patrol  0.07 0.07I staff 0.i0 0.07penalty  0.09 0.09I fee 0.17 0.08I tariff  0.13 0.08I tax 0.19 0.07reserv i s t  0.07 0.07Figure 3: Similarity tree for "duty"Inspection of sample outputs hows that this al-gorithm works well.
However, formal evaluation ofits accuracy remains to be future work.5 Related Work  and  Conclus ionThere have been many approaches to automatic de-tection of similar words from text corpora.
Ours is772similar to (Grefenstette, 1994; Hindle, 1990; Ruge,1992) in the use of dependency relationship as theword features, based on which word similarities arecomputed.Evaluation of automatically generated lexical re-sources is a difficult problem.
In (Hindle, 1990),a small set of sample results are presented.
In(Smadja, 1993), automatically extracted colloca-tions are judged by a lexicographer.
In (Dagan etal., 1993) and (Pereira et al, !
993), clusters of sim-ilar words are evaluated by how well they are ableto recover data items that are removed from the in-put corpus one at a time.
In (Alshawi and Carter,1994), the collocations and their associated scoreswere evaluated indirectly by their use in parse treeselection.
The merits of different measures for as-sociation strength are judged by the differences theymake in the precision and the recall of the parseroutputs.The main contribution of this paper is a new eval-uation methodology for automatically constructedthesaurus.
While previous methods rely on indirecttasks or subjective judgments, our method allowsdirect and objective comparison between automati-cally and manually constructed thesauri.
The resultsshow that our automatically created thesaurus i sig-nificantly closer to WordNet than Roget Thesaurusis.
Our experiments also surpasses previous experi-ments on automatic thesaurus construction i  scaleand (possibly) accuracy.AcknowledgementThis research as also been partially supported byNSERC Research Grant OGP121338 and by the In-stitute for Robotics and Intelligent Systems.ReferencesHiyan Alshawi and David Carter.
1994.
Trainingand scaling preference functions for disambiguation.Computational Linguistics, 20(4):635-648, Decem-ber.Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1993.Contextual word similarity and estimation from sparsedata.
In Proceedings of ACL-93, pages 164-171,Columbus, Ohio, June.Ido Dagan, Fernando Pereira, and Lillian Lee.
1994.Similarity-based estimation of word cooccurrenceprobabilities.
In Proceedings of the 32nd AnnualMeeting of the ACL, pages 272-278, Las Cruces, NM.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based method for word sense disambigua-tion.
In Proceedings of the 35th Annual Meeting ofthe ACL, pages 56-63, Madrid, Spain.Ute Essen and Volker Steinbiss.
1992.
Cooccurrencesmoothing for stochastic language modeling.
In Pro-ceedings oflCASSP, volume 1, pages 161-164.W.
B. Frakes and R. Baeza-Yates, editors.
1992.
In.formation Retrieval, Data Structure and Algorithms.Prentice Hall.D.
Gentner.
1982.
Why nouns are learned before verbs:Linguistic relativity versus natural partitioning.
InS.
A. Kuczaj, editor, Language development: Vol.
2.Language, thought, and culture, pages 301-334.
Erl-baum, Hillsdale, NJ.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Press,Boston, MA.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In Proceedings ofACL-90, pages 268-275, Pittsburg, Pennsylvania,June.Dekang Lin.
1993.
Principle-based parsing withoutovergeneration.
In Proceedings of ACL-93, pages112-120, Columbus, Ohio.Dekang Lin.
1994.
Principarman efficient, broad-coverage, principle-based parser.
In Proceedings ofCOLING-94, pages 482-488.
Kyoto, Japan.Dekang Lin.
1997.
Using syntactic dependency aslocalcontext to resolve word sense ambiguity.
In Proceed-ings of ACL/EACL-97, pages 64-71, Madrid, Spain,July.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: An on-line lexical database.International Journal of Lexicography, 3(4):235-244.George A. Miller.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicography,3(4):235-312.Eugene A. Nida.
1975.
ComponentialAnalysis of Mean-ing.
The Hague, Mouton.F.
Pereira, N. Tishby, and L. Lee.
1993.
DistributionalClustering of English Words.
In Proceedings ofACL-93, pages 183-190, Ohio State University, Columbus,Ohio.Gerda Ruge.
1992.
Experiments on linguistically basedterm associations.
Information Processing & Man-agement, 28(3):317-332.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1): 143-178.773Appendix A: Respective Nearest NeighborsNounsRank Respective Nearest Neighbors Similarity1 earnings profit 0.57252511 plan proposal 0.4747521 employee worker 0.41393631 battle fight 0.38977641 airline carrier 0.37058951 share stock 0.35129461 rumor speculation 0.32726671 outlay spending 0.32053581 accident incident 0.31012191 facility plant 0.284845101 charge count 0.278339111 babyinfant 0.268093121 actor actress 0.255098131 chance likelihood 0.248942141 catastrophe disaster 0.241986151 fine penalty 0.237606161 legislature parliament 0.231528171 oil petroleum 0.227277181 strength weakness 0.218027191 radio television 0.215043201 coupe sedan 0.209631211 turmoil upheaval 0.205841221 music song 0.202102231 bomb grenade 0.198707241 gallery museum 0.194591251 leaf leave 0.192483261 fuel gasoline 0.186045271 door window 0.181301281 emigration immigration 0.176331291 espionage treason 0.17262301 peril pitfall 0.169587311 surcharge surtax 0.166831321 ability credibility 0.163301331 pub tavern .
0.158815341 lmense permit 0.156963351 excerpt transcript 0.150941361 dictatorshipreglme 0.148837371 lake river 0.145586381 disc disk 0.142733391 interpreter translator 0.138778401 bacteria organism 0.135539411 ballet symphony 0.131688421 silk wool 0.128999431 intent intention 0.12523644 1 waiter waitress 0.122373451 blood urine 0.118063461 mosquito tick 0.115499471 fervor zeal 0.112087481 equal equivalent 0.107159491 freezer efrigerator 0.103777501 humor wit 0.0991108511 cushion pillow 0.0944567521 purse wallet 0.0914273531 learning listening 0.0859118541 clown cowboy 0.0714762VerbsRank Respective Nearest Neighbors Similarity1 fall rise 0.67411311 injure kill 0.37825421 concern worry 0.34012231 convict sentence 0.28967841 limit restrict 0.27158851 narrow widen 0.25838561 attract draw 0.24233171 discourage encourage 0.23442581 hit strike 0.2217191 disregard ignore 0.21027101 overstate understate 0.199197111 affirm reaffirm 0.182765121 inform notify 0.170477131 differ vary 0.161821141 scream yell 0.150168151 laugh smile 0.142951161 compete cope 0.135869171 add whisk 0.129205181 blossom mature 0.123351191 smell taste 0.112418201 bark howl 0.101566211 black white 0.0694954Adjective/AdverbsRank Respective Nearest Neighbors Similarity1 high low 0.58040811 bad good 0.37674421 extremely very 0.35760631 deteriorating improving 0.33266441 alleged suspected 0.31716351 clerical salaried 0.30544861 often sometimes 0.28144471 bleak gloomy 0.27555781 adequate inadequate 0.26313691 affiliated merged 0.257666101 stormy turbulent 0.252846111 paramilitary uniformed 0.246638121 sharp steep 0.240788131 communist leftist 0.232518141 indoor outdoor 0.224183151 changed changing 0.219697161 defensive offensive 0.211062171 sad tragic 0.206688181 enormously tremendously 0.199936191 defective faulty 0.193863201 concerned worried 0.186899211 dropped fell 0.184768221 bloody violent 0.183058231 favorite popular 0.179234241 permanently temporarily 0.174361251 confidential secret 0.17022261 privately publicly 0.165313271 operating sales 0.162894281 annually apiece 0.159883291 ~gentle kind 0.154554301 losing winning 0.149447311 experimental test 0.146435321 designer dress 0.142552331 dormant inactive 0.137002341 commercially domestically 0.13291835l complimentary free 0.128117361 constantly continually 0.122342371 hardy resistant 0.112133381 anymore anyway 0.103241774
