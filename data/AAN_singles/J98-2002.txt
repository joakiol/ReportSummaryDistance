Generalizing Case Frames Using aThesaurus and the MDL PrincipleHang Li*NEC CorporationNaok i  Abe*NEC CorporationA new method for automatically acquiring case frame patterns from large corpora is proposed.In particular, the problem of generalizing values of a case frame slot for a verb is viewed asthat of estimating a conditional probability distribution over a partition of words, and a newgeneralization method based on the Minimum Description Length (MDL) principle is proposed.In order to assist with efficiency, the proposed method makes use of an existing thesaurus andrestricts its attention to those partitions that are present as "cuts" in the thesaurus tree, thusreducing the generalization problem to that of estimating a "tree cut model" of the thesaurus tree.An efficient algorithm is given, which provably obtains the optimal tree cut model for the givenfrequency data of a case slot, in the sense of MDL.
Case frame patterns obtained by the methodwere used to resolve PP-attachment ambiguity.
Experimental results indicate that the proposedmethod improves upon or is at least comparable with existing methods.1.
IntroductionWe address the problem of automatically acquiring case frame patterns (selectionalpatterns, subcategorization patterns) from large corpora.
A satisfactory solution to thisproblem would have a great impact on various tasks in natural anguage processing,including the structural disambiguation problem in parsing.
The acquired knowledgewould also be helpful for building a lexicon, as it would provide lexicographers withword usage descriptions.In our view, the problem of acquiring case frame patterns involves the followingtwo issues: (a) acquiring patterns of individual case frame slots; and (b) learningdependencies that may exist between different slots.
In this paper, we confine ourselvesto the former issue, and refer the interested reader to Li and Abe (1996), which dealswith the latter issue.The case frame (case slot) pattern acquisition process consists of two phases: extrac-tion of case frame instances from corpus data, and generalization of those instances tocase frame patterns.
The generalization step is needed in order to represent the inputcase frame instances more compactly as well as to judge the (degree of) acceptabilityof unseen case frame instances.
For the extraction problem, there have been variousmethods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grish-man and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997).
The generalizationproblem, in contrast, is a more challenging one and has not been solved completely.
Anumber of methods for generalizing values of a case frame slot for a verb have been* C&C Media Res.
Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan.E-mail: { |ihang,abe }@ccm.cl.nec.co.jp@ 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 2proposed.
Some of these methods make use of prior knowledge in the form of anexisting thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al 1994; Tanaka1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowl-edge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994).
In thispaper, we propose a new generalization method, belonging to the first of these twocategories, which is both theoretically well-motivated and computationally efficient.Specifically, we formalize the problem of generalizing values of a case frame slotfor a given verb as that of estimating a conditional probability distribution over apartition of words, and propose a new generalization method based on the MinimumDescription Length principle (MDL): a principle of data compression and statisticalestimation from information theory.
1 In order to assist with efficiency, our methodmakes use of an existing thesaurus and restricts its attention on those partitions thatare present as "cuts" in the thesaurus tree, thus reducing the generalization problemto that of estimating a "tree cut model" of the thesaurus tree.
We then give an efficientalgorithm that provably obtains the optimal tree cut model for the given frequency dataof a case slot, in the sense of MDL.
In order to test the effectiveness of our method, weconducted PP-attachment disambiguation experiments using the case frame patternsobtained by our method.
Our experimental results indicate that the proposed methodimproves upon or is at least comparable to existing methods.The remainder of this paper is organized as follows: In Section 2, we formalize theproblem of generalizing values of a case frame slot as that of estimating a conditionaldistribution.
In Section 3, we describe our MDL-based generalization method.
In Sec-tion 4, we present our experimental results.
We then give some concluding remarksin Section 5.2.
The Problem2.1 The Data Sparseness ProblemSuppose that the data available to us are of the type shown in Table 1, which are slotvalues for a given verb (verb,slot_name,slot_value triples) automatically extracted froma corpus using existing techniques.
By counting the frequency of occurrence of eachnoun at a given slot of a verb, the frequency data shown in Figure 1 can be obtained.We will refer to this type of data as co-occurrence data.
The problem of generalizingvalues of a case frame slot for a verb (or, in general, a head) can be viewed as theproblem of learning the underlying conditional probability distribution that givesrise to such co-occurrence data.
Such a conditional distribution can be represented bya probability model that specifies the conditional probability P(n I v, r) for each n inthe set of nouns .M = {nl, n2 .
.
.
.
.
nN}, V in the set of verbs V = {vl, v2 .
.
.
.
.
Vv}, and rin the set of slot names T~ = {rl, r2 .
.
.
.
.
rR}, satisfying:P(n Iv, r) = 1.
(1)nGMThis type of probability model is often referred to as a word-based model.
Since thenumber of probability parameters in word-based models is large (O(N. V. R)), accurate1 Recently, MDL and related techniques have become popular in corpus-based natural anguageprocessing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke andOmohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright1996; Grunwald 1996).
In this paper, we introduce MDL into the context of case frame patternacquisition.218Li and Abe Generalizing Case FramesTable 1Example (verb, slot_name,slot_value) triple data.verb slot_name slot_valuefly argl beefly argl birdfly argl birdfly argl crowfly argl birdfly argl eaglefly argl beefly argl eaglefly argl birdfly argl crow"Freq."
- -swallow crow eagle bird bug bee insectFigure 1Frequency data for the subject slot of verb fly.estimation of a word-based model is difficult with the data size that is available inpractice--a problem usually referred to as the data sparseness problem.
For example,suppose that we employ the maximum-likelihood estimation (or MLE for short) toestimate the probability parameters of a conditional probability distribution, as de-scribed above, given the co-occurrence data in Figure 1.
In this case, MLE amountsto estimating the parameters by simply normalizing the frequencies so that they sumto one, giving, for example, the estimated probabilities of 0, 0.2, and 0.4 for swallow,eagle, and bird, respectively (see Figure 2).
Since in general the number of parametersexceeds the size of data that is typically available, MLE will result in estimating mostof the probability parameters to be zero.To address this problem, Grishman and Sterling (1994) proposed a method ofsmoothing conditional probabilities using the probability values of similar words,where the similarity between words is judged based on co-occurrence data (see alsoDagan, Marcus, and Makovitch \[1992\] and Dagan, Pereira, and Lee \[1994\]).
Morespecifically, conditional probabilities of words are smoothed by taking the weightedaverage of those of similar words using the similarity measure as the weights.
Theadvantage of this approach is that it does not rely on any prior knowledge, but itappears difficult to find a smoothing method that is both efficient and theoreticallysound.
As an alternative, a number of authors have proposed the use of class-based219Computational Linguistics Volume 24, Number 2"Prob."
- -0.450.40.350.30.250.20.150.10.050swallow crow eagle bird bug bee insectFigure 2Word-based istribution estimated using MLE.models, which assign (conditional) probability values to (existing) classes of words,rather than individual words.2.2 Class-based ModelsAn example of the class-based approach is Resnik's method of generalizing values ofa case frame slot using a thesaurus and the so-called selectional association measure(Resnik 1993a, 1993b).
The selectional association, denoted A(C I v, r), is defined asfollows:P(CIv,  r) (2) A(C I v, F) = P(C I v, F) x log P(C)where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name,as described earlier.
In generalizing a given noun n to a noun class, this method selectsthe noun class C having the maximum A(C I v, r), among all super classes of n in agiven thesaurus.
This method is based on an interesting intuition, but its interpretationas a method of estimation is not clear.
We propose a class-based generalization methodwhose performance as a method of estimation is guaranteed to be near optimal.We define the class-based model as a model that consists of a partition of the set.N" of nouns, and a parameter associated with each member of the partition.
Here, apartition F of .M is any collection of mutually disjoint subsets of iV" that exhaustivelycover N.  The parameters specify the conditional probability P(C I v, r) for each class(subset) C in that partition, such thatP(CIv,  r) = 1.
(3)CEFWithin a given class C, it is assumed that each noun is generated with equal probability,namely1 Vn E C: P(n l v, r) = ~ x P(C I v, F).
(4)Here, we assume that a word belongs to a single class.
In practice, however,many words have sense ambiguity and a word can belong to several different classes,e.g., bird is a member of both BIRD and MEAT.
Thorough treatment of this problemis beyond the scope of the present paper; we simply note that one can employ anexisting word-sense disambiguation technique (e.g.,Yarowsky 1992, 1994) in prepro-cessing, and use the disambiguated word senses as virtual words in the following220Li and Abe Generalizing Case FramesANIMALBIRD INSECTswallow crow eagle bird bug bee insectFigure 3An example thesaurus.case-pattern acquisition process.
It is also possible to extend our model so that eachword probabilistically belongs to several different classes, which would allow us toresolve both structural and word-sense ambiguities at the time of disambiguation.
2Employing probabilistic membership, however, would make the estimation processsignificantly more computationally demanding.
We therefore leave this issue as a fu-ture topic, and employ a simple heuristic of equally distributing each word occurrencein the data to all of its potential word senses in our experiments.
Since our learningmethod based on MDL is robust against noise, this should not significantly degradeperformance.2.3 The Tree Cut ModelSince the number of partitions for a given set of nouns is extremely large, the problemof selecting the best model from among all possible class-based models is most likelyintractable.
In this paper, we reduce the number of possible partitions to consider byusing a thesaurus as prior knowledge, following a basic idea of Resnik's (1992).In particular, we restrict our attention to those partitions that exist within thethesaurus in the form of a cut.
By thesaurus, we mean a tree in which each leaf nodestands for a noun, while each internal node represents a noun class, and dominationstands for set inclusion (see Figure 3).
A cut in a tree is any set of nodes in the treethat defines a partition of the leaf nodes, viewing each node as representing the setof all leaf nodes it dominates.
For example, in the thesaurus of Figure 3, there arefive cuts: \[ANIMAL\], \[BIRD, INSECT\], \[BIRD, bug, bee, insect\], \[swallow, crow, eagle,bird, INSECT\], and \[swallow, crow, eagle, bird, bug, bee, insect\].
The class of tree cutmodels of a fixed thesaurus tree is then obtained by restricting the partition P in thedefinition of a class-based model to be those partitions that are present as a cut in thatthesaurus tree.Formally, a tree cut model M can be represented by a pair consisting of a tree cutlP and a probability parameter vector 0 of the same length, that is:V = (r, e) (5)where lP and 0 are:r = \[C1, C2 .
.
.
.
.
Ck+l\], e = \[P(C1), P(C2) .
.
.
.
.
P(Ck+l)\] (6)k+l where C1, C2 .
.
.
.
.
Ck+l is a cut in the thesaurus tree and ~i=1 P(Ci) = 1 is satisfied.For simplicity we sometimes write P(Ci), i = 1 .
.
.
.
.
(k + 1) for P(Ci \[ v, r).If we use MLE for the parameter estimation, we can obtain five tree cut modelsfrom the co-occurrence data in Figure 1; Figures 4-6 show three of these.
For example,2 The model used by Pereira, Tishby, and Lee (1993) is indeed along this direction.221Computational Linguistics Volume 24, Number 20.450.40.350.30.250.20.150.10.050- r~- ' - - - - - - -~- -~ ,Prob r-'~--swallow crow eagle bird bug bee insectFigure 4A tree cut model with \[swallow, crow, eagle, bird, bug, bee, insect\].0.450.40.350.30.250.20.150.10.050"Prob.
"swa'll .
.
.
.
ow eagle bi'rd bug bee ins'ectFigure 5A tree cut model with \[BIRD, bug, bee, insect\].~- (\[BIRD, bug, bee, insect\], \[0.8,0,0.2,0\]) shown in Figure 5 is one such tree cutmodel.
Recall that M defines a conditional probability distribution PM(n I v,r) asfollows: For any noun that is in the tree cut, such as bee, the probability is given asexplicitly specified by the model, i.e., PM(bee I flY, argl) = 0.2.
For any class in the treecut, the probability is distributed uniformly to all nouns dominated by it.
For example,since there are four nouns that fall under the class BIRD, and swallow is one of them,the probability of swallow is thus given by Pt~(swallow I flY, argl) = 0.8/4 = 0.2.
Notethat the probabilities assigned to the nouns under BIRD are smoothed, even if thenouns have different observed frequencies.We have thus formalized the problem of generalizing values of a case frame slot asthat of estimating a model from the class of tree cut models for some fixed thesaurustree; namely, selecting a model that best explains the data from among the class oftree cut models.3.
Generalization Method Based On MDLThe question now becomes what strategy (criterion) we should employ to select the besttree-cut model.
We adopt the Minimum Description Length principle (Rissanen 1978,222Li and Abe Generalizing Case Frames0.450.40,350,30.250.2O,lS0.10,050 swallow crow eagle bi'rdFigure 6A tree cut model with \[BIRD, INSECT\]."Prob."
- -Table 2Number of parameters and KL distance from the empirical distribution for the fivetree cut models.P Number of Parameters KL Distance\[ANIMAL\]\[BIRD, INSECT\]\[BIRD, bug, bee, insect\]\[swallow, crow, eagle, bird, INSECT\]\[swallow, crow, eagle, bird, bug, bee, insect\]0 0.891 0.723 0.44 0.326 01983, 1984, 1986, 1989), which has various desirable properties, as will be describedlater.
3MDL is a principle of data compression and statistical estimation from informa-tion theory, which states that the best probability model for given data is that whichrequires the least code length in bits for the encoding of the model itself and the givendata observed through it.
4 The former is the model description length and the latterthe data description length.In our current problem, it tends to be the case, in general, that a model nearer theroot of the thesaurus tree, such as that in Figure 6, is simpler (in terms of the numberof parameters), but tends to have a poorer fit to the data.
In contrast, a model nearerthe leaves of the thesaurus tree, such as that in Figure 4, is more complex, but tendsto have a better fit to the data.
Table 2 shows the number of free parameters andthe KL distance from the empirical distribution of the data (namely, the word-baseddistribution estimated by MLE) shown in Figure 2 for each of the five tree cut models.
5In the table, one can see that there is a trade-off between the simplicity of a modeland the goodness of fit to the data.In the MDL framework, the model description length is an indicator of model3 Estimation strategies related to MDL have been independently proposed and studied by variousauthors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992).4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction tothe MDL principle.5 The KL distance (alsO known as KL-divergence orrelative ntropy), which is widely used ininformation theory and statistics, is a measure of distance between two distributions (e.g., Cover andThomas 1991).
It is always normegative and is zero if and only if the two distributions are identical,but is asymmetric and hence not a metric (the usual notion of distance).223Computational Linguistics Volume 24, Number 2complexity, while the data description length indicates goodness of fit to the data.
TheMDL principle stipulates that the model that minimizes the sum total of the descriptionlengths hould be the best model (both for data compression and statistical estimation).In the remainder of this section, we will describe how we apply MDL to ourcurrent problem.
We will then discuss the rationale behind using MDL in our presentcontext.3.1 Calculat ing Descr ipt ion LengthWe first show how the description length for a model is calculated.
We use S todenote a sample (or set of data), which is a multiset of examples, each of which is anoccurrence of a noun at a given slot r of a given verb v (i.e., duplication is allowed).We let ISI denote the size of S as a multiset, and n E S indicate the inclusion of nin S as a multiset.
For example, the column labeled slot_value in Table 1 represents asample S for the subject slot offly, and in this case ISI = 10.Given a sample S and a tree cut F, we employ MLE to estimate the parame-ters of the corresponding tree cut model ~,I = (F, 0), where 6 denotes the estimatedparameters.The total description length L(/~,I, S) of the tree cut model/vl and the sample Sobserved through M is computed as the sum of the model description length L(P),parameter description length L(0 I P), and data description length L(S I F, 6):L(M,S) = L((F,6),S) = L(r) + L(6 I r)  +L(Str,6).
(7)Note that we sometimes refer to L(F) + L(0 I F) as the model description length.The model description length L(F) is a subjective quantity, which depends on thecoding scheme mployed.
Here, we choose to assign the same code length to each cutand let:L(F) = log IG\[ (8)where ~ denotes the set of all cuts in the thesaurus tree T. 6 This corresponds to assum-ing that each tree cut model is equally likely a priori, in the Bayesian interpretation ofMDL.
(See Section 3.4.
)The parameter description length L(O I F) is calculated by:k L(0 I r) = ~ x log IsI (9)where ISI denotes the sample size and k denotes the number of free parameters in thetree cut model, i.e., k equals the number of nodes in P minus one.
It is known to bebest to use this number of bits to describe probability parameters in order to minimizethe expected total description length (Rissanen 1984, 1986).
An intuitive explanationof this is that the standard deviation of the maximum-likelihood estimator of eachparameter is of the order ~,  and hence describing each parameter using more than1 1 log ISI bits would be wasteful for the estimation accuracy possible with - log x /~ - 2the given sample size.Finally, the data description length L(S I F, 0) is calculated by:L(S I r, 0) = - ~ log P(n) (10)nES6 Here and throughout, log denotes the logarithm to the base 2.
For reasons why Equation 8 holds, see,for example, Quinlan and Rivest (1989).224Li and Abe Generalizing Case FramesTable 3Calculating the description length for themodel of Figure 5.C B IRD bug bee insectf(C) 8 0 2 0ICI 4 1 1 1P(C) 0.8 0.0 0.2 0.0P(n) 0.2 0.0 0.2 0.0P \[BIRD, bug, bee, insect\]L(0 1 r) (47l) x log 10 = 4.98L(S I P,~) - (2+4+2+2)  x log0.2 = 23.22where for simplicity we write P(n) for PM(n \[ v, r).
Recall that P(n) is obtained byMLE, namely, by normalizing the frequencies:1 P(n) = ~ x P(C) (11)for each C c P and each n E C, where for each C c P:= d(C)  (12)ISIwheref(C) denotes the total frequency of nouns in class C in the sample S, and F is atree cut.
We note that, in fact, the maximum-likelihood estimate is one that minimizesthe data description length L(S I F, 0).With description length defined in the above manner, we wish to select a modelwith the minimum description length and output it as the result of generalization.Since we assume here that every tree cut has an equal L(P), technically we need onlycalculate and compare L'(/\[d, S) = L(~ I F) + L(S t F, ~) as the description length.
Forsimplicity, we will sometimes write just L'(F) for L'(7\[/I, S), where I ~ is the tree cut ofM, when ~,I and S are clear from context.The description lengths for the data in Figure 1 using various tree cut modelsof the thesaurus tree in Figure 3 are shown in Table 4.
(Table 3 shows how the de-scription length is calculated for the model of tree cut \]BIRD, bug, bee, insect\].)
Thesefigures indicate that the model in Figure 6 is the best model, according to MDL.
Thus,given the data in Table 1 as input, the generalization result shown in Table 5 is ob-tained.3.2 An Efficient AlgorithmIn generalizing values of a case flame slot using MDL, we could, in principle, calculatethe description length of every possible tree cut model and output a model with theminimum description length as the generalization result, if computation time were ofno concern.
But since the number of cuts in a thesaurus tree is exponential in the sizeof the tree (for example, it is easy to verify that for a complete b-ary tree of depth d it isof the order o(2ba-1)), it is impractical to do so.
Nonetheless, we were able to devise a225Computational Linguistics Volume 24, Number 2Table 4Description length of the five tree cut models.r L(~ I r) L(S \] r,~) L'(P)\[ANIMAL\] 0 28.07 28.07\[BIRD, INSECT\] 1.66 26.39 28.05\[BIRD, bug, bee, insect\] 4.98 23.22 28.20\[swallow, crow, eagle, bird, INSECT\] 6.64 22.39 29.03\[swallow, crow, eagle, bird, bug, bee, insect\] 9.97 19.22 29.19Table 5Generalization result.verb slot~name slot_value probabilityfly argl BIRD 0.8fly argl INSECT 0.2Here we let t denote a thesaurus (sub)tree, root(t) the root of the tree t.Initially t is set to the entire tree.Also input to the algorithm is a co-occurrence data.algorithm Find-MDL(t) := cut1.
if2.
t is a leaf node3.
then4.
retum(\[t\])5. else6.
For each child tree ti of t ci :=Find-MDL(ti)7. c:= append(ci)8. if9.
L'(\[root(t)\]) < L'(c)10. then11.
return(\[root(t)\])12. else13.
return(c)Figure 7The algorithm: Find-MDL.simple and efficient algorithm based on dynamic programming, which is guaranteedto find a model with the minimum description length.Our algorithm, which we call Find-MDL, recursively finds the optimal MDL modelfor each child subtree of a given tree and appends all the optimal models of these sub-trees and returns the appended models, unless collapsing all the lowerqevel optimalmodels into a model consisting of a single node (the root node of the given tree) re-duces the total description length, in which case it does so.
The details of the algorithmare given in Figure 7.
Note that for simplicity we describe Find-MDL as outputting atree cut, rather than a complete tree cut model.Note in the above algorithm that the parameter description length is calculated as226Li and Abe Generalizing Case FramesL'(\[ARTIFACT\])=41.09L'(\[VEHICLE,AIRPLANE\])=40.97ENTITY L'(\[AIR PLAN E\])=32.27r,airplane\])=32.72RO~iNS A a ~ 5A'C"I .
.
.
.
.
.
.
.
.
.. 7o.2-.. ?
0.23 BI CT l VEHICLE AIRPLANEswallow crow eagle bird bug ~lS~"~insect car bike jet helicopter airplanef(swallow)=4,f(crow)=4,f(eagle)=4,f(bird)=6,f(bee)=8,f(car)=l ,f(jet)=4,f(airplane)=4Figure 8An example application of Find-MDL.log ISI, where k + 1 is the number of nodes in the current cut, both when t is the 2entire tree and when it is a proper subtree.
This contrasts with the fact that the numberof free parameters i k for the former, while it is k + 1 for the latter.
For the purposeof finding a tree cut with the minimum description length, however, this distinctioncan be ignored (see Appendix A).Figure 8 illustrates how the algorithm works (on the co-occurrence data shownat the bottom): In the recursive application of Find-MDL on the subtree rooted atAIRPLANE, the if-clause on line 9 evaluates to true since L'(\[AIRPLANE\]) = 32.27,L'(~et, helicopter, airplane\]) = 32.72, and hence \[AIRPLANE\] is returned.
Then in thecall to Find-MDL on the subtree rooted at ARTIFACT, the same if-clause evaluatesto false since L'(\[VEHICLE, AIRPLANE\]) = 40.97, L'(\[ARTIFACT\]) = 41.09, and hence\[VEHICLE, AIRPLANE\] is returned.Concerning the above algorithm, we show that the following proposition holds:Proposition 1The algorithm Find-MDL terminates in time O(N x ISI), where N denotes the numberof leaf nodes in the input thesaurus tree T and ISI denotes the input sample size, andoutputs a tree cut model of T with the minimum description length (with respect othe encoding scheme described in Section 3.1).Here we will give an intuitive explanation of why the proposition holds, and givethe formal proof in Appendix A.
The MLE of each node (class) is obtained simply bydividing the frequency of nouns within that class by the total sample size.
Thus, theparameter estimation for each subtree can be done independently from the estimationof the parameters outside the subtree.
The data description length for a subtree thusdepends olely on the tree cut within that subtree, and its calculation can be performedindependently for each subtree.
As for the parameter description length for a subtree,it depends only on the number of classes in the tree cut within that subtree, and hencecan be computed independently as well.
The formal proof proceeds by mathematicalinduction, which verifies that the optimal model in any (sub)tree is either the model227Computational Linguistics Volume 24, Number 2consisting of the root of the tree or the model obtained by appending the optimalsubmodels for its child subtrees.
73.3 Estimation, Generalization, and MDLWhen a discrete model (a partition F of the set of nouns W" in our present context) isfixed, and the estimation problem involves only the estimation of probability parame-ters, the classic maximum-likelihood estimation (MLE) is known to be satisfactory.
Inparticular, the estimation of a word-based model is one such problem, since the parti-tion is fixed and the size of the partition equals \[.M\[.
Furthermore, for a fixed discretemodel, it is known that MLE coincides with MDL: Given data S = {xi : i = 1 .
.
.
.
.
m},MLE estimates parameter P, which maximizes the likelihood with respect to the data;that is:m= arg mpax H P(xi).
(13)i=1It is easy to see that P also satisfies:m= arg nun ~ - log P(xi).
(14)i=1This is nothing but the MDL estimate in this case, since ~i~1 -log P(xi) is the datadescription length.When the estimation problem involves model selection, i.e., the choice of a tree cutin the present context, MDUs behavior significantly deviates from that of MLE.
Thisis because MDL insists on minimizing the sum total of the data description lengthand the model description length, while MLE is still equivalent to minimizing the datadescription length only.
So, for our problem of estimating a tree cut model, MDL tendsto select a model that is reasonably simple yet fits the data quite well, whereas themodel selected by MLE will be a word-based model (or a tree cut model equivalentto the word-based modelS), as it will always manage to fit the data.In statistical terms, the superiority of MDL as an estimation method is related tothe fact we noted earlier that even though MLE can provide the best fit to the givendata, the estimation accuracy of the parameters is poor, when applied on a sample ofmodest size, as there are too many parameters to estimate.
MLE is likely to estimatemost parameters tobe zero, and thus suffers from the data sparseness problem.
Notein Table 4, that MDL avoids this problem by taking into account the model complexityas well as the fit to the data.MDL stipulates that the model with the minimum description length should beselected both for data compression and estimation.
This intimate connection betweenestimation and data compression can also be thought of as that between estimation andgeneralization, since in order to compress information, generalization is necessary.
Inour current problem, this corresponds tothe generalization f individual nouns presentin case frame instances in the data as classes of nouns present in a given thesaurus.
Forexample, given the thesaurus in Figure 3 and frequency data in Figure 1, we would7 The process of finding the MDL model tends to be computationally demanding and is oftenintractable.
When the model class under consideration is restricted to tree structures, however, dynamicprogramming is often applicable and the MDL model can be efficiently found.
For example, Rissanen(1995) has devised an algorithm for learning decision trees.8 Consider, for example, the case when the co-occurrence data is given asf(swal low) = 2,f(crow) = 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2.228Li and Abe Generalizing Case Frameslike our system to judge that the class BIRD and the noun bee can be the subject slot ofthe verb fly.
The problem of deciding whether to stop generalizing at BIRD and bee, orgeneralizing further to ANIMAL has been addressed by a number of authors (Websterand Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992).
Minimizationof the total description length provides a disciplined criterion to do this.A remarkable fact about MDL is that theoretical findings have indeed verified thatMDL, as an estimation strategy, is near optimal in terms of the rate of convergence ofits estimated models to the true model as data size increases.
When the true modelis included in the class of models considered, the models selected by MDL convergeto the true model at the rate of O/~C:~9~_i~!~ where k* is the number of parameters in2.1Sl J'the true model, and \[S\] the data size, which is near optimal (Barron and Cover 1991;Yamanishi 1992).Thus, in the current problem, MDL provides (a) a way of smoothing probabilityparameters to solve the data sparseness problem, and at the same time, (b) a wayof generalizing nouns in the data to noun classes of an appropriate level, both as acorollary to the near optimal estimation of the distribution of the given data.3.4 The Bayesian Interpretation of MDL and the Choice of Encoding SchemeThere is a Bayesian interpretation of MDL: MDL is essentially equivalent to the "pos-terior mode" in the Bayesian terminology (Rissanen 1989).
Given data S and a numberof models, the Bayesian estimator (posterior mode) selects a model M that maximizesthe posterior probability:= argn~x(P(M).
P(S I M)) (15)where P(M) denotes the prior probability of the model M and P(S \[ M) the probabilityof observing the data S given M. Equivalently, M satisfies~'I = argn~m(- logP(M) - logP(S I M)).
(16)This is equivalent to the MDL estimate, if we take - log P(M) to be the model descrip-tion length.
Interpreting - log P(M) as the model description length translates, in theBayesian estimation, to assigning larger prior probabilities on simpler models, since itis equivalent to assuming that P(M) = (?
)t(a), where I(M) is the description length ofM.
(Note that if we assign uniform prior probability P(M) to all models M, then (15)becomes equivalent to (13), giving the maximum-likelihood estimate.
)Recall, that in our definition of parameter description length, we assign a shorterparameter description length to a model with a smaller number of parameters k,which admits the above interpretation.
As for the model description length (for treecuts) we assigned an equal code length to each tree cut, which translates to placingno bias on any cut.
We could have employed a different coding scheme assigningshorter code lengths to cuts nearer the root.
We chose not to do so partly because, forsufficiently large sample sizes, the parameter description length starts dominating themodel description length anyway.Another important property of the definition of description length is that it af-fects not only the effective prior probabilities on the models, but also the procedurefor computing the model minimizing the measure.
Indeed, our definition of modeldescription length was chosen to be compatible with the dynamic programming tech-nique, namely, its calculation is performable locally for each subtree.
For a differentchoice of coding scheme, it is possible that a simple and efficient MDL algorithm like229Computational Linguistics Volume 24, Number 2Find-MDL may not exist.
We believe that our choice of model description length isderived from a natural encoding scheme with reasonable interpretation as Bayesianprior, and at the same time allows an efficient algorithm for finding a model with theminimum description length.3.5 The Uniform Distribution Assumption and the Level of GeneralizationThe uniform distribution assumption made in (4), namely that all nouns belongingto a class contained in the tree cut model are assigned the same probability, seemsto be rather stringent.
If one were to insist that the model be exactly accurate, thenit would seem that the true model would be the word-based model resulting fromno generalization at all.
If we allow approximations, however, it is likely that somereasonable tree cut model with the uniform probability assumption will be a goodapproximation of the true distribution; in fact, a best model for a given data size.
Aswe remarked earlier, as MDL balances between the fit to the data and the simplicityof the model, one can expect that the model selected by MDL will be a reasonablecompromise.Nonetheless, it is still a shortcoming of our model that it contains an oversimplifiedassumption, and the problem is especially pressing when rare words are involved.
Rarewords may not be observed at a slot of interest in the data simply because they arerare, and not because they are unfit for that particular slot.
9 To see how rare is toorare for our method, consider the following example.Suppose that the class BIRD contains 10 words, bird, swallow, crow, eagle, parrot,waxwing, etc.
Consider co-occurrence data having 8 occurrences of bird, 2 occurrencesof swallow, 1 occurrence of crow, 1 occurrence of eagle, and 0 occurrence of all otherwords, as part of, say, 100 data obtained for the subject slot of verb fly.
For this dataset, our method would select the model that generalizes bird, swallow, etc.
to the classBIRD, since the sum of the data and parameter description lengths for the BIRD subtreeis 76.57 + 3.32 = 79.89 if generalized, and 53.73 + 33.22 = 86.95 if not generalized.
Forcomparison, consider the data with 10 occurrences of bird, 3 occurrences of swallowand 1 occurrence of crow, and 0 occurrence of all other words, also as part of 100data for the subject slot of fly.
In this case, our method would select the model thatstops generalizing at bird, swallow, eagle, etc., because the description length for thesame subtree now is 86.22 + 3.32 = 89.54 if generalized, and 55.04 + 33.22 = 88.26 ifnot generalized.
These examples eem to indicate that our MDL-based method wouldchoose to generalize, even when there are relatively large differences in frequencies ofwords within a class, but knows enough to stop generalizing when the discrepancyin frequencies i especially noticeable (relative to the given sample size).4.
Experimental Results4.1 Experiment 1: A Qualitative EvaluationWe applied our generalization method to large corpora and inspected the obtainedtree cut models to see if they agreed with human intuition.
In our experiments, weextracted verbs and their case frame slots (verb, slot_name, slot_value triples) from thetagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of126,084 sentences, using existing techniques (specifically, those in Smadja \[1993\]), then9 There are several possible measures that one could take to address this issue, including theincorporation fabsolute frequencies of the words (inside and outside the particular slot in question).This is outside the scope of the present paper, and we simply refer the interested reader to one possibleapproach (Abe and Li 1996).230Li and Abe Generalizing Case FramesTable 6Example input data (for the direct object slot of eat).eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1eat arg2 pizza 1 eat arg2 oyster 1applied our method to generalize the slot_values.
Table 6 shows some example tripledata for the direct object slot of the verb eat.There were some extraction errors present in the data, but we chose not to removethem, because in general there will always be extraction errors and realistic evaluationshould leave them in.When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller1995) as our thesaurus.
The noun taxonomy of WordNet has a structure of directedacyclic graph (DAG), and its nodes stand for a word sense (a concept) and oftencontain several words having the same word sense.
WordNet thus deviates from ournotion of thesaurus--a tree in which each leaf node stands for a noun, each internalnode stands for the class of nouns below it, and a noun is uniquely represented by aleaf node- -so we took a few measures to deal with this.First, we modified our algorithm FInd-MDL so that it can be applied to a DAG;now, Find-MDL effectively copies each subgraph having multiple parents (and itsassociated ata) so that the DAG is transformed to a tree structure.
Note that withthis modification it is no longer guaranteed that the output model is optimal.
Next,we dealt heuristically with the issue of word-sense ambiguity by equally dividing theobserved frequency of a noun between all the nodes containing that noun.
Finally,when an internal node contained nouns actually occurring in the data, we assignedthe .frequencies of all the nodes below it to that internal node, and excised the wholesubtree (subgraph) below it.
The last of these measures, in effect, defines the "startingcut" of the thesaurus from which to begin generalizing.
Since (word senses of) nounsthat occur in natural language tend to concentrate in the middle of a taxonomy, thestarting cut given by this method usually falls around the middle of the thesaurus.
1?Figure 9 shows the starting cut and the resulting cut in WordNet for the directobject slot of eat with respect o the data in Table 6, where / .
.
.
/  denotes a node inWordNet.
The starting cut consists of nodes/plant .
.
.
/ , / food/ ,etc ,  which are the high-est nodes containing values of the direct object slot of eat.
Since/ food/has significantlyhigher frequencies than its ne ighbors /so l id /and/ f lu id / ,  thegeneralization stops thereaccording to MDL.
In contrast, the nodes under / l i fe_ form.
.
.
/have relatively small dif-ferences in their frequencies, and thus they are generalized to the node/ l i fe_form.. .
/ .The same is true of the nodes under /artifact/.
Since / .
.
-amount .
.
.
/  has a much10 Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be moreimportant with respect to learning, recognition, and memory, and their linguistic expressions occurmore frequently innatural language--a phenomenon k own as basic level primacy.
See Lakoff (1987).231Computational Linguistics Volume 24, Number 2TOPt~ <abstraction>o.i0 ~ - -  / .
.
.
.
.
.
~-  -~--..., -~<life_form...> O.
11 <object> ( <measure,~:ljjafl~it~,amount...> <space> <time>/ \  / / ",9-089 '<plant...> % <animal...>| <substance> {<artifact...~: ; ;, k \ ' ,  ',I I I ~ ~ /  \ ~ 0 .39  ~1 ~ tJ I I ~ _ ~ .
.
.
.
, .
~  ~ t. resulting cuta a , ,,tsohd> <fluid> <foo~>~.
~ - - - ~t~rtinn c.==tI I I I ~ %% / / \ I % II I \ I I:mushroom> <lobster> <horse> <lobster> <pizza> <rope>Figure 9An example generalization, result (for the direct object slot of eat).higher frequency than its neighbors /time/ and {space), the generalization does notgo up higher.
All of these results eem to agree with human intuition, indicating thatour method results in an appropriate l vel of generalization.Table 7 shows generalization results for the direct object slot of eat and someother arbitrarily selected verbs, where classes are sorted in descending order of theirprobability values.
(Classes with probabilities less than 0.05 are discarded ue to spacelimitations.
)Table 8 shows the computation time required (on a SPARC "Ultra 1" work station)to obtain the results hown in Table 7.
(The computation time for loading the WordNetwas excluded since it need be done only once.)
Even though the noun taxonomy ofWordNet is a large thesaurus containing approximately 50,000 nodes, our methodstill manages to efficiently generalize case slots using it.
The table also shows theaverage number of levels generalized for each slot, namely, the average number oflinks between a node in the starting cut and its ancestor node in the resulting cut.
(For example, the number of levels generalized for/plant.
.
- /  is one in Figure 9.)
Onecan see that a significant amount of generalization is performed by our method--theresulting tree cut is about 5 levels higher than the starting cut, on the average.4.2 Experiment 2: PP-Attachment DisambiguationCase frame patterns obtained by our method can be used in various tasks in natu-ral language processing.
In this paper, we test its effectiveness in a structural (PP-attachment) disambiguation experiment.Disambiguation Methods.
It has been empirically verified that the use of lexical semanticknowledge is effective in structural disambiguation, such as the PP-attachment prob-lem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990).
There have been232Li and Abe Generalizing Case FramesTable 7Examples of generalization results.Class Probability Example WordsDirect Object of eat(food,nutrient) 0.39 pizza, egg(life_form,organism,being,living_thing) 0.11 lobster, horse/measure,quantity, amount,quantum) 0.10 amount of(artifact,article,artefact) 0.08 as if eat ropeDirect Object of buy(object, inanimate-object,physical-object / 0.30 computer, painting(asset) 0.10 stock, share(group,grouping) 0.07 company, bank(legal_document,legal_instrument,official_document .... ) 0.05 security, ticketDirect Object of .fly(entity) 0.35 airplane, flag, executive(linear_measure,long_measure) 0.28 mile/group,grouping) 0 .08  delegationDirect Object of operate/group,grouping/ 0 .13  company, fleet(act,human_action,human_activity) 0.13 flight, operation(structure,construction/ 0.12 center(abstraction) 0.11 service, unit(possession/ 0.06 profit, earningsTable 8Required computation time and number of generalized levels.Verb CPU Time (second) Average Number of Generalized Levelseat 1.00 5.2buy 0.66 4.6fly 1.11 6.0operate 0.90 5.0Average 0.92 5.2many probabilistic methods proposed in the literature to address the PP-attachmentproblem using lexical semantic knowledge which, in our view, can be classified intothree types.The first approach (Hindle and Rooth 1991, 1993) takes doubles of the form(verb, prep) and (nounl, prep), like those in Table 9, as training data to acquire semanticknowledge and judges the attachment sites of the prepositional phrases in quadru-ples of the form (verb, nounl, prep, noun2) e.g., (see, girl, with, telescope)--based onthe acquired knowledge.
Hindle and Rooth (1991) proposed the use of the lexicalassociation measure calculated based on such doubles.
More specifically, they esti-mate P(prep I verb) and P(prep \[ noun1), and calculate the so-called t-score, which isa measure of the statistical significance of the difference between P(prep I verb) andP(prep \[ nounl).
If the t-score indicates that the former probability is significantly larger,233Computational Linguistics Volume 24, Number 2Table 9Example input data as doubles.see insee withgirl withman withTable 10Example input data as triples.see in parksee with telescopegirl with scarfsee with friendman with hatTable 11Example input data as quadruples andlabels.see girl in park ADVsee man with telescope ADVsee girl with scarf ADNthen the prepositional phrase is attached to verb, if the latter probability is significantlylarger, it is attached to nounl, and otherwise no decision is made.The second approach (Sekine et al 1992; Chang, Luo, and Su 1992; Resnik 1993a;Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2)and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semanticknowledge and performs PP-attachment disambiguation on quadruples.
For example,Resnik (1993a) proposes the use of the selectional ssociation measure calculated basedon such triples, as described in Section 2.
More specifically, his method comparesmaxclassi~noun2 A(Classi \[ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to makedisambiguation decisions.The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994;Collins and Brooks 1995) receives quadruples (verb, noun1, prep, noun2) and labels indi-cating which way the PP-attachment goes, like those in Table 11, and learns a disam-biguation rule for resolving PP-attachment ambiguities.
For example, Brill and Resnik,(1994) propose a method they call transformation-based error-driven learning (see alsoBrill \[1995\]).
Their method first learns IF-THEN type rules, where the IF parts repre-sent conditions like (prep is with) and (verb is see), and the THEN parts representtransformations from (attach to verb) to (attach to nounl), or vice versa.
The first ruleis always a default decision, and all the other rules indicate transformations (changesof attachment sites) subject o various IF conditions.We note that, for the disambiguation problem, the first two approaches are basi-cally unsupervised learning methods, in the sense that the training data are merelypositive examples for both types of attachments, which could in principle be extractedfrom pure corpus data with no human intervention.
(For example, one could justuse unambiguous sentences.)
The third approach, on the other hand, is a supervisedlearning method, which requires labeled data prepared by a human being.234Li and Abe Generalizing Case FramesTable 12Number of different types of data.Training DataAverage number of doubles per data set 91218.1Average number of triples per data set 91218.1Average number of quadruples per data set 21656.6Test DataAverage number of quadruples per data set 820.4The generalization method we propose falls into the second category, although itcan also be used as a component in a combined scheme with many of the above meth-ods (see Brill and Resnik \[1994\], Alshawi and Carter \[1994\]).
We estimate P(noun2 Iverb, prep) and P(noun2 I nount, prep) from training data consisting of triples, and com-pare them: If the former exceeds the latter (by a certain margin) we attach it to verb,else if the latter exceeds the former (by the same margin) we attach it to noun1.In our experiments, described below, we compare the performance of our proposedmethod, which we refer to as MDL, against the methods proposed by Hindle andRooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively asLA, SA, and TEL.Data Set.
We used the bracketed corpus of the Penn Treebank (Wall Street Journal cor-pus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data.
First we randomlyselected one of the 26 directories of the WSJ files as the test data and what remains asthe training data.
We repeated this process 10 times and obtained 10 sets of data con-sisting of different raining data and test data.
We used these 10 data sets to conductcross-validation as described below.From the test data in each data set, we extracted (verb, noun1, prep, noun2) quadru-ples using the extraction tool provided by the Penn Treebank called "tgrep."
At thesame time, we obtained the answer for the PP-attachment site for each quadruple.We did not double-check if the answers provided in the Penn Treebank were actuallycorrect or not.
Then from the training data of each data set, we extracted (verb, prep)and (noun, prep) doubles, and (verb, prep, noun2) and (nounl,prep, noun2) triples usingtools we developed ourselves.
We also extracted quadruples from the training data asbefore.
We then applied 12 heuristic rules to further preprocess the data, which include(1) changing the inflected form of a word to its stem form, (2) replacing numerals withthe word number, (3) replacing integers between 1,900 and 2,999 with the word year, (4)replacing co., ltd., etc.
with the words company, limited, etc.
11 After preprocessing therestill remained some minor errors, which we did not remove further, due to the lackof a good method for doing so automatically.
Table 12 shows the number of differenttypes of data obtained by the above process.Experimental Procedure.
We first compared the accuracy and coverage for each of thethree disambiguation methods based on unsupervised learning: MDL, SA, and LA.11 The experimental results obtained here are better than those obtained in our preliminary experiment(Li and Abe 1995), in part because we only adopted rule (1) in the past.235Computational Linguistics Volume 24, Number 20.980.960.940.920.90.880.660.840.820.80"'-.
.
," ' " - , .
, .
," " , .
,"E3,"'"', ..,"D,.x* ' ,f I I {0.2 0.4 0.6 0.8coverageFigure 10Accuracy-coverage curves for MDL, SA, and LA.i"MDL""SA" -4--"LA" -~--"LA.t" x ?'D,.
"~t"ElFor MDL, we generalized noun2 given (verb, prep, noun2) and (nounl,prep, noun2)triples as training data for each data set, using WordNet as the thesaurus in the samemanner as in experiment 1.
When disambiguating, we actually compared P(Classl \[verb, prep) and P(Class2 I noun1, prep), where Class1 and Class2 are classes in the out-put tree cut models dominating noun2 in place of P(noun2 \] verb, prep) and P(noun2 \]nounl,prep).
12 We found that doing so gives a slightly better esult.
For SA, we em-ployed a somewhat simplified version in which noun2 is generalized given (verb, prep,noun2) and (nounl,prep, noun2) triples using WordNet, and maxcl~ss,~,o,,2 A(Classi Iverb, prep) and maxctass,~no,n2 A(Classi l nounl, prep) are compared for disambiguation:If the former exceeds the latter then the prepositional phrase is attached to verb, andotherwise to noun1.
For LA, we estimated P(prep \] verb) and P(prep \] noun1) from thetraining data of each data set and compared them for disambiguation.
We then eval-uated the results achieved by the three methods in terms of accuracy and coverage.Here, coverage refers to the proportion as a percentage, of the test quadruples onwhich the disambiguation method could make a decision, and accuracy refers to theproportion of correct decisions among them.In Figure 10, we plot the accuracy-coverage curves for the three methods.
In plot-ting these curves, the attachment site is determined by simply seeing if the differencebetween the appropriate measures for the two alternatives, be it probabilities or selec-tional association values, exceeds a threshold.
For each method, the threshold was setsuccessively to 0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, and 0.75.
When the difference betweenthe two measures i less than a threshold, we rule that no decision can be made.
Thesecurves were obtained by averaging over the 10 data sets.12 Recall that a node in WordNet represents a word sense and not a word; noun2 can belong to severalclasses in the thesaurus.
We thus use maxciassignou,2 (P(Classi \[ verb, prep)) andmaxclassi gno,m2 ( P( Classi \[ nounl, prep) in place of P( Classl \] verb, prep) and P( Class2 \[ nounl, prep).236Li and Abe Generalizing Case FramesTable 13Results of PP-attachment disambiguation.Coverage(%) Accuracy(%)Default 100 56.2MDL + Default 100 82.2SA + Default 100 76.7LA + Default 100 80.7LA.t + Default 100 78.1TEL 100 82.4We also implemented the exact method proposed by Hindle and Rooth (1991),which makes disambiguation judgement using the t-score.
Figure 10 shows the re-sult as LA.t, where the threshold for t-score is set to 1.28 (significance l vel of 90percent.)
From Figure 10 we see that with respect o accuracy-coverage curves, MDLoutperforms both SA and LA throughout, while SA is better than LA.Next, we tested the method of applying a default rule after applying each method.That is, attaching (prep, noun2) to verb for the part of the test data for which no deci-sion was made by the method in question.
13We refer to these combined methods asMDL+Default, SA+Default, LA+Default, and LA.t+Default.
Table 13 shows the results,again averaged over the 10 data sets.Finally, we used the transformation-based error-driven learning (TEL) to acquiretransformation rules for each data set and applied the obtained rules to disambiguatethe test data.
The average number of obtained rules for a data set was 2,752.3.
Table 13shows the disambiguation result averaged over the 10 data sets.
From Table 13, wesee that TEL performs the best, edging over the second place MDL+Default by a smallmargin, and then followed by LA+Default, and SA+Default.
Below we discuss furtherobservations concerning these results.MDL and SA.
According to our experimental results, the accuracy and coverage ofMDL appear to be somewhat better than those of SA.
As Resnik (1993b) pointed~ P(qv,r) out, the use of selectional ssociation Iu~ ~ seems to be appropriate for cognitivemodeling.
Our experiments show, however, that the generalization method currentlyemployed by Resnik has a tendency to overfit he data.
Table 14 shows example gener-alization results for MDL (with classes with probability less than 0.05 discarded) andSA.
Note that MDL tends to select a tree cut closer to the root of the thesaurus tree.This is probably the key reason why MDL has a wider coverage than SA for the samedegree of accuracy.
One may be concerned that MDL is "overgeneralizing" here, 14 butas shown in Figure 10, its disambiguation accuracy does not seem to be degraded.Another problem that must be dealt with concerning SA is how to remove noise(resulting, for example, from erroneous extraction) from the generalization results.P(Clv,r) Since SA estimates the ratio between two probability values, namely -~y- ,  the gen-eralization result may be lead astray if one of the estimates of P(C I v, r) and P(C) isunreliable.
For instance, a high estimated value for/drop, bead, pearl / at protect against13 Interestingly, for the entire data set it is more favorable to attach (prep, noun2) to noun1, but for whatremains after applying LA and MDL, it turns out to be more favorable to attach (prep, noun2) to verb.14 Note that in Experiment 1, there were more data available, and thus the data were more appropriatelygeneralized.237Computational Linguistics Volume 24, Number 2Table 14Example generalization results for SA and MDL.InputVerb Preposition Noun Frequencyprotect against accusation 1protect against damage 1protect against decline 1protect against drop 1protect against loss 1protect against resistance 1protect against squall 1protect against vagary 1Generalization Result of MDLVerb Preposition Noun Class Probabilityprotect against (act,human_action,human_activity) 0.212protect against (phenomenon) 0.170protect against (psychological_feature) 0.099protect against (event) 0.097protect against (abstraction) 0.093Generalization Result of SAVerb Preposition Noun Class SAprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect againstprotect against(caprice,impulse,vagary, whim) 1.528(phenomenon) 0.899(happening,occurrence,natural_event) 0.339(deterioration,worsening,decline,declination) 0.285(act,human_action,human_activity) 0.260(drop,bead,pearl) 0.202(drop) 0.202(descent,declivity, fall,decline,downslope) 0.188(resistor, resistance) 0.130(underground,resistance) 0.130{immunity, resistance) O.
124(resistance, opposition) 0.111(loss,deprivation) 0.105(loss) 0.096(cost,price,terms,damage / 0.052shown in Table 14 is rather odd, and is because the estimate of P(C) is unreliable (toosmall).
This problem apparently costs SA a nonnegligible drop in disambiguation ac-curacy.
In contrast, MDL does not suffer from this problem since a high estimatedprobability value is only possible with high frequency, which cannot result just fromextraction errors.
Consider, for example, the occurrence of car in the data shown inFigure 8, which has supposedly resulted from an erroneous extraction.
The effect ofthis datum gets washed away, as the estimated probability for VEHICLE, to which carhas been generalized, is negligible.On the other hand, SA has a merit not shared by MDL, namely its use of theassociation ratio factors out the effect of absolute frequencies of words, and focuses238Li and Abe Generalizing Case FramesTable 15Some hard examples for LA.Attached to verb Attached to noun1acquire interest in yearbuy stock in tradeease restriction on exportforecast sale for yearmake payment on millionmeet standard for resistancereach agreement in augustshow interest in sessionwin verdict in winteracquire interest in firmbuy stock in indexease restriction on typeforecast sale for venturemake payment on debtmeet standard for carreach agreement in principleshow interest in stockwin verdict in caseon their co-occurrence r lation.
Since both MDL and SA have pros and cons, it wouldbe desirable to develop a methodology that combines the merits of the two methods(cf.
Abe and Li \[1996\]).MDL and LA.
LA makes its disambiguation decision completely ignoring noun2.
AsResnik (1993b) pointed out, if we hope to improve disambiguation performance byincreasing training data, we need a richer model such as those used in MDL and SA.We found that 8.8% of the quadruples in our entire test data were such that they sharedthe same verb, prep, noun1 but had different noun2, and their PP-attachment si es go bothways in the same data, i.e., both to verb and to noun1.
Clearly, for these examples, thePP-attachment site cannot be reliably determined without knowing noun2.
Table 15shows some of these examples.
(We adopted the attachment sites given in the PennTree Bank, without correcting apparently wrong judgements.
)MDL and TEL.
We chose TEL as an example of the quadruple approach.
This methodwas designed specifically for the purpose of resolving PP-attachment ambiguities, andseems to perform slightly better than ours.As we remarked earlier, however, the input data required by our method (triples)could be generated automatically from unparsed corpora making use of existingheuristic rules (Brent 1993; Smadja 1993), although for the experiments we reporthere we used a parsed corpus.
Thus it would seem to be easier to obtain more datain the future for MDL and other methods based on unsupervised learning.
Also notethat our method of generalizing values of a case slot can be used for purposes otherthan disambiguation.5.
Conc lus ionsWe proposed a new method of generalizing case frames.
Our approach of applyingMDL to estimate a tree cut model in an existing thesaurus i not limited to just theproblem of generalizing values of a case frame slot.
It is potentially useful in othernatural language processing tasks, such as the problem of estimating n-gram models(Brown et al 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997).We believe that our method has the following merits: (1) it is theoretically sound; (2) itis computationally efficient; (3) it is robust against noise.
Our experimental resultsindicate that the performance of our method is better than, or at least comparableto, existing methods.
One of the disadvantages of our method is that its performance239Computational Linguistics Volume 24, Number 2depends on the structure of the particular thesaurus used.
This, however, is a prob-lem commonly shared by any generalization method that uses a thesaurus as priorknowledge.Appendix A: Proof of Proposition 1ProofFor an arbitrary subtree T' of a thesaurus tree T and an arbitrary tree cut modelM = (F,0) of T, let MT, = (FT , ,0T , )  denote the submodel of M that is contained inT'.
Also for any sample S and any subtree T' of T, let ST, denote the subsample of Scontained in T'.
(Note that MT = M, ST = S.) Then define, in general for any submodelMT, and subsample ST,, L(ST, \[ FT,, ~T') to be the data description length of subsampleST, using submodel MT,, L(~T, \[ FT,) to be the parameter description length for thesubmodel MT,, and L'(MT,,ST,) to  be L(ST, I FT ' ,~T ' )  q- L(~T, \[ FT,).
(Note that, whencalculating the parameter description length for a submodel, the sample size of theentire sample \]S\] is used.
)First note that for any (sub)tree T, (sub)model MT = (FT, ~T) contained in T, and(sub)sample ST contained in T, and T's child subtrees Ti : i = 1,. .
.
,  k, we have:kL(ST I PT, g )  =  L(ST, I PT,,g,) (17)i=1provided that Fz is not a single node (root node of T).
This follows from the mutualdisjointness of the Ti, and the independence of the parameters in the Ti.We also have, when T is a proper subtree of the thesaurus tree:kL(OT I FT) = ~ L(OT, I FT,).
(18)i=1Since the number of free parameters of a model in the entire thesaurus tree equalsthe number of nodes in the model minus one due to the stochastic ondition (that theprobability parameters must sum to one), when T equals the entire thesaurus tree,theoretically the parameter description length for a tree cut model of T should be:L(g I rr) = L r)k=  L(0r, I rr,)i=1log Isl (19)where ISI is the size of the entire sample.
Since the second term - ~  in (19) isconstant once the input sample S is fixed, for the purpose of finding a model with theminimum description length, it is irrelevant.
We will thus use the identity (18) bothwhen T is the entire tree and when it is a proper subtree.
(This allows us to use thesame recursive algorithm, Find-MDL, in all cases.
)It follows from (17) and (18) that the minimization of description length can bedone essentially independently for each subtree.
Namely, if we let Clmin (MT, ST) denotethe minimum description length (as defined by \[17\] and \[18\]) achievable for (sub)modelMr on (sub)sample ST contained in (sub)tree T, \[)s(~) the MLE estimate for node ~\]240Li and Abe Generalizing Case Framesusing the entire sample S, and root(T) the root node of tree T, then we have:L~nin(MT, ST) min L~nin (MTi, ST i),k i=1L'( (\[root(T)\], \[Ps(root(T) )\]), ST) } (20)The rest of the proof proceeds by induction.
First, when T is of a single leafnode, the submodel consisting solely of the node and the MLE of the generationprobability for the class represented by T is returned, which is clearly a submodelwith minimum description length in the subtree T. Next, inductively assume thatFind-MDL(T ~) correctly outputs a (sub)model with the minimum description lengthfor any tree T' of size less than n. Then, given a tree T of size n whose root node has atleast two children, say Ti : i = 1 .
.
.
.
.
k, for each Ti, Find-MDL(Ti) returns a (sub)modelwith the minimum description length by the inductive hypothesis.
Then, since (20)holds, whichever way the if-clause on lines 8, 9 of Find-MDL evaluates to, what isreturned on line 11 or line 13 will still be a (sub)model with the minimum descriptionlength, completing the inductive step.It is easy to see that the running time of the algorithm is linear in both the numberof leaf nodes of the input thesaurus tree and the input sample size.
?AcknowledgmentsWe are grateful to K. Nakamura ndT.
Fujita of NEC C&C Res.
Labs.
for theirconstant encouragement.
WethankK.
Yaminishi and J. Takeuchi of C&C Res.Labs.
for their suggestions and comments.We thank T. Futagami of NIS for hisprogramming efforts.
We also express ourspecial appreciation to the two anonymousreviewers who have provided manyvaluable comments.
We acknowledge theACL for providing the ACL/DCI CD-ROM,LDC of the University of Pennsylvania forproviding the Penn Treebank corpus data,and Princeton University for providingWordNet, and E. Brill and P. Resnik forproviding their PP-attachmentdisambiguation program.ReferencesAbe, Naoki and Hang Li.
1996.
Learningword association orms using tree cutpair models.
Proceedings ofthe ThirteenthInternational Conference on MachineLearning, pages 3-11.Almuallim, Hussein, Yasuhiro Akiba,Takefumi Yamazaki, Akio Yokoo, andShigeo Kaneda.
1994.
Two methods forALT-J/E translation rules from examplesand a semantic hierarchy.
Proceedings oftheFifteenth International Conference onComputational Linguistics, pages 57-63.Alshawi, Hiyan and David Carter.
1994.Training and scaling preference functionsfor disambiguation.
ComputationalLinguistics, 20(4):635-648.Barron, Andrew R. and Thomas M. Cover.1991.
Minimum complexity densityestimation.
IEEE Transaction on InformationTheory, 37(4):1034--1054.Brent, Michael R. 1993.
From grammar tolexicon: Unsupervised learning of lexicalsyntax.
Computational Linguistics,19(2):243-262.Brent, Michael R. and Timothy A.Cartwright.
1996.
Distributional regularityand phonotactic constraints are useful forsegmentation.
Cognition, 61:93-125.Brent, Michael R., Sreerama K. Murthy, andAndrew Lundberg.
1995.
Discoveringmorphemic suffixes: A case study inminimum description length induction.Proceedings ofthe Fifth InternationalWorkshop on Artificial Intelligence andStatistics.Brill, Eric.
1995.
Transformation-basederror-driven learning and naturallanguage processing: A case study inpart-of-speech tagging.
ComputationalLinguistics, 21(4):543-565.Brill, Eric and Philip Resnik.
1994.
Arule-based approach to prepositionalphrase attachment disambiguation.241Computational Linguistics Volume 24, Number 2Proceedings ofthe Fifteenth InternationalConference on Computational Linguistics,pages 1198-1204.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
Proceedings ofthe FifthConference on Applied Natural LanguageProcessing.Brown, Peter E, Vincent J. Della Pietra,Peter V. deSouza, Jenifer C. Lai, andRobert L. Mercer.
1992.
Class-basedn-gram models of natural anguage.Computational Linguistics, 18(4):283-298.Cartwright, Timothy A. and Michael R.Brent.
1994.
Segmenting speech without alexicon: The roles of phonotactics andspeech source.
Proceedings ofthe FirstMeeting of the ACL Special Interest Group inComputational Phonology, pages 83-90.Chang, Jing-Shin, Yih-Fen Luo, and Keh-YihSu.
1992.
GPSM: A generalizedprobabilistic semantic model forambiguity resolution.
Proceedings ofthe30th Annual Meeting, pages 177-184.Association for ComputationalLinguistics.Collins, Michael and James Brooks.
1995.Prepositional phrase attachment througha backed-off model.
Proceedings ofthe ThirdWorkshop on Very Large Corpora.Cover, Thomas M. and Joy A. Thomas.1991.
Elements of Information Theory.
JohnWiley & Sons Inc., New York.CucchiareUi, Alessandro and Paola Velardi.1997.
Automatic selection of class labelsfrom a thesaurus for an effective semantictagging of corpora.
Proceedings ofthe FifthConference on Applied Natural LanguageProcessing, pages 380-387.Dagan, Ido, Shaul Marcus, and ShaulMakovitch.
1992.
Contextual wordsimilarity and estimation from sparsedata.
Proceedings ofthe 30th AnnualMeeting, pages 164-171.
Association forComputational Linguistics.Dagan, Ido, Fernando Pereira, and LillianLee.
1994.
Similarity-based stimation ofword cooccurrence probabilities.Proceedings ofthe 32nd Annual Meeting,pages 272-278.
Association forComputational Linguistics.Ellison, T. Mark.
1991.
Discovering planarsegregations.
Proceedings ofAAAI SpringSymposium on Machine Learning of NaturalLanguage and Ontology, pages 42-47.Ellison, T. Mark.
1992.
Discovering vowelharmony.
In Walter Daelmans and DavidPowers, editors, Background andExperiments inMachine Learning of NaturalLanguage, pages 205-207.Framis, Francesc Ribas.
1994.
Anexperiment on learning appropriateselectional restrictions from a parsedcorpus.
Proceedings ofthe FifteenthInternational Conference on ComputationalLinguistics, pages 769-774.Grefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Publishers, Boston.Grishman, Ralph and John Sterling.
1992.Acquisition of selectional patterns.Proceedings ofthe Fourteenth InternationalConference on Computational Linguistics,pages 658-664.Grishman, Ralph and John Sterling.
1994.Generalizing automatically generatedselectional patterns.
Proceedings ofthe?
Fifteenth International Conference onComputational Linguistics, pages 742-747.Grunwald, Peter.
1996.
A minimumdescription length approach to grammarinference.
In S. Wemter, E. Riloff, andG.
Scheler, editors, Symbolic, Connectionistand Statistical Approaches toLearning forNatural Language Processing, Lecture Note inAI.
Springer Verlag, pages 203-216.Hindle, Donald and Mats Rooth.
1991.Structural ambiguity and lexical relations.Proceedings ofthe 29th Annual Meeting,pages 229-236.
Association forComputational Linguistics.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103-120.Hobbs, Jerry R. and John Bear.
1990.
Twoprinciples of parse preference.
Proceedingsof the Thirteenth International Conference onComputational Linguistics, pages 162-167.Lakoff, George.
1987.
Women, Fire, andDangerous Things: What Categories Revealabout he Mind.
The University of ChicagoPress.Li, Hang and Naoki Abe.
1995.
Generalizingcase frames using a thesaurus and theMDL principle.
Proceedings ofRecentAdvances in Natural Language Processing,pages 239-248.Li, Hang and Naoki Abe.
1996.
Learningdependencies between case frame slots.Proceedings ofthe Sixteenth InternationalConference on Computational Linguistics,pages 10-15.Manning, Christopher D. 1992.
Automaticacquisition of a large subcategorizationdictionary from corpora.
Proceedings ofthe30th Annual Meeting, pages 235-242.Association for ComputationalLinguistics.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: The242Li and Abe Generalizing Case FramesPenn Treebank.
Computational Linguistics,19(1):313-330.Miller, George A.
1995.
WordNet: A lexicaldatabase for English.
Communications ofthe ACM, pages 39-41.Nomiyama, Hiroshi.
1992.
Machinetranslation by case generalization.Proceedings ofthe Fourteenth InternationalConference on Computational Linguistics,pages 714-720.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional c usteringof English words.
Proceedings ofthe 31stAnnual Meeting, pages 183-190.Association for ComputationalLinguistics.Quinlan, J. Ross and Ronald L. Rivest.
1989.Inferring decision trees using theminimum description length principle.Information and Computation, 80:227-248.Ratnaparkhi, Adwait, Jeff Reynar, and SalimRoukos.
1994.
A maximum entropy modelfor prepositional phrase attachment.Proceedings ofARPA Workshop on HumanLanguage Technology, pages 250-255.Resnik, Philip.
1992.
WordNet anddistributional nalysis: A class-basedapproach to lexical discovery.
Proceedingsof AAAI Workshop on Statistically-based NLPTechniques.Resnik, Philip.
1993a.
Selection andInformation: A Class-based Approach toLexical Relationships.
Ph.D. Thesis, Univ.
ofPennsylvania.Resnik, Philip.
1993b.
Semantic lasses andsyntactic ambiguity.
Proceedings ofARPAWorkshop on Human Language Technology.Rissanen, Jorma.
1978.
Modeling by shortestdata description.
Automatic, 14:37-38.Rissanen, Jorma.
1983.
A universal prior forintegers and estimation by minimumdescription length.
The Annals of Statistics,11(2):416--431.Rissanen, Jorma.
1984.
Universal coding,information, predication and estimation.IEEE Transaction on Information Theory,30(4):629-636.Rissanen, Jorma.
1986.
Stochastic omplexityand modeling.
The Annals of Statistics,14(3):1080-1100.Rissanen, Jorma.
1989.
Stochastic Complexityin Statistical Inquiry.
World ScientificPublishing Co., Singapore.Rissanen, Jorma.
1995.
Stochastic omplexityin learning.
Proceedings ofthe SecondEuropean Conference on ComputationalLearning Theory (Euro Colt'95), pages196-210.Ristad, Eric Sven and Robert G. Thomas.1995.
New techniques for contextmodeling.
Proceedings ofthe 33rd AnnualMeeting.
Association for ComputationalLinguistics.Schwarz, G. 1978.
Estimation of thedimension of a model.
Annals of Statistics,6:416-446.Sekine, Satoshi, Jeremy J. Carroll, SofiaAnaniadou, and Jun'ichi Tsujii.
1992.Automatic learning for semanticcollocation.
Proceedings ofthe ThirdConference on Applied Natural LanguageProcessing, pages 104-110.Smadja, Frank.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguistics,19(1):143-177.Solomonoff, R.J. 1964.
A formal theory ofinductive inference 1 and 2.
Informationand Control, 7:1-22;224-254.Stolcke, Andreas and Stephen Omohundro.1994.
Inducing probabilistic grammars bybayesian model merging.
In Rafael C.Carrasco and Jose Oncina, editors,Grammatical Inference and Applications.Springer Verlag, pages 106--118.Tanaka, Hideki.
1994.
Verbal case frameacquisition from a bilingual corpus:Gradual knowledge acquisition.Proceedings ofthe Fifteenth InternationalConference on Computational Linguistics,pages 727-731.Tanaka, Hideki.
1996.
Decision tree learningalgorithm with structured attributes:Application to verbal case frameacquisition.
Proceedings ofthe SixteenthInternational Conference on ComputationalLinguistics, pages 943-948.Utsuro, Takehito and Yuji Matsumoto.
1997.Learning probabilistic subcategorizationpreference by identifying casedependencies and optimal noun classgeneralization level.
Proceedings ofthe FifthConference on Applied Natural LanguageProcessing, pages 364-371.Utsuro, Takehito, Yuji Matsumoto, andMakoto Nagao.
1992.
Lexical knowledgeacquisition from bilingual corpora.Proceedings ofthe Fourteenth InternationalConference on Computational Linguistics,pages 581-587.Velardi, Paola, Maria Teresa Pazienza, andMichela Fasolo.
1991.
How to encodesemantic knowledge: A method formeaning representation a dcomputer-aided acquisition.
ComputationalLinguistics, 17(2):153-170.Wallace, C. and D. M. Boulton.
1968.
Aninformation measure for classification.Computer Journal, 11:185-195.Wallace, C. and P. Freeman.
1992.Single-factor analysis by minimummessage length estimation.
Journal of RoyalStatistical Society, B, 54:195-209.243Computational Linguistics Volume 24, Number 2Webster, Mort and Mitch Marcus.
1989.Automatic acquisition of the lexicalsemantics of verbs from sentence frames.Proceedings ofthe 27th Annual Meeting,pages 177-184.
Association forComputational Linguistics.Whittemore, Greg, Kathleen Ferrara, andHans Brunner.
1990.
Empirical study ofpredictive powers of simple attachmentschemes for post-modifier prepositionalphrases.
Proceedings ofthe 28th AnnualMeeting, pages 23-30.
Association forComputational Linguistics.Yamanishi, Kenji.
1992.
A learning criterionfor stochastic rules.
Machine Learning,9:165-203.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoger's categories trained on largecorpora.
Proceedings ofthe fourteenthInternational Conference on ComputationalLinguistics, pages 454-460.Yarowsky, David.
1994.
Decision lists forlexical ambiguity resolution: Applicationto accent restoration i Spanish andFrench.
Proceedings ofthe 32nd AnnualMeeting, pages 88-95.
Association forComputational Linguistics.244
