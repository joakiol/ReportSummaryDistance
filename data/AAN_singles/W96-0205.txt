Automat ic  Extract ion of New Words from Japanese Texts usingGeneral ized Forward-Backward SearchMasaak i  NAGATANTT  Information and Communicat ion Systems Laboratories1-2356 Take, Yokosuka-Shi, Kanagawa,  238-03 Japannagat a?nttnly.
?
sl.
ntt.
j pAbstractWe present a novel new word extraction methodfrom Japanese texts based on expected wordfrequencies.
First, we compute expected wordfrequencies from Japanese texts using a robuststochastic N-best word segmenter.
We then ex-tract new words by filtering out erroneous wordhypotheses whose expected word frequencies arelower than the predefined threshold.
The methodis derived from an approximation of the general-ized version of the Forward-Backward algorithm.When the Japanese word segmenter is trained ona 4.7 million word segmented corpus and testedon 1000 sentences whose out-of-vocabulary ateis 2.1%, the accuracy of the new word extractionmethod is 43.7% recall and 52.3% precision.In t roduct ionSegmentation f sentences into words is trivial inEnglish because words are delimited by spaces.It is a simple task to count word frequencies ina given text.
It is also a simple task to list allnew words (unknown words), namely, the wordsin a given text that are not found in the systemdictionary.
However, several anguages uch asJapanese, Chinese and Thai do not put spacesbetween words and so in these languages wordsegmentation, word frequency counting, and newword extraction remain unsolved problems in com-putational linguistics.Most Japanese NLP applications require wordsegmentation as a first stage because there arephonological units and semantic units whose pro-nunciation and/or meaning is not trivially deriv-able from the pronunciation a d/or meaning of theindividual characters.
It is well known that theaccuracy of word segmentation greatly dependson the coverage of the dictionary, in other words,the Out-Of-Vocabulary (00V) rate of the targettexts.Our goal is to provide a method to automati-cally extract new words from Japanese texts.
Thisnmthod should adapt the dictionary of the wordsegmenter to new domains and applications.
Itshould also maintain the dictionary by collectingnew words in the target domain.
The applica-tion of the word segmenter is described elsewhere(Nagata, 1996).The approach we take is as follows: First, wedesign a statistical language model that can as-sign a reasonable word probability to an arbitrarysubstring in the input sentence, whether or notit is truly a word.
Second, we devised a methodto obtain the expected word N-gram count in thetarget texts, using an N-best word segmentationalgorithm (Nagata, 1994).
Finally, we extract newwords by filtering out spurious word hypotheseswhose expected word frequencies are lower thanthe threshold.Japanese  Morpho log ica l  Ana lys i sBefore we start, we briefly explain the difficul-ties of Japanese morphological nalysis, especiallywhen the input sentence includes unknown words.Suppose the input sentence is " -~4)~p/~7~}~ ENIAC 69 50 ~ 3o ", which means"University of Pennsylvania celebrates the 50thanniversary of ENIAC", where the words ~Y5~JP /~7 (transliteration of 'Pennsylvania') andENIAC (the name of the world's first computer)are not registered in the system dictionary.
Fig-ure 1 shows three possible analyses of the inputsentence, where each box represents a word hy-pothesis whose meaning and part of speech areshown above and under the box.
The tag <UNK>represents an unknown word.One of the hardest problems in handling unre-stricted Japanese text is the identification of un-known words.
In Figure 1, the string ENIAC issuccessfully tokenized as an unknown word.
How-ever, there is ambiguity in the segmentation f thestring ~ 5/zL~J<~-7~.In the first analysis, the system considers -~'-.~5//1~/~_~7 ('Pennsylvania') s an unknown word,48Logprob "'4 ~ ~ \]1t \]< ~ 7" 5k: ~ ~ E N l A C cO(rel prob)~_~y ~j  ENIAC of Pennsylvania "\[ ENIAC -108.95 \[ -"~5.
'J1.-'-'<-~7" \] \] \ [ \ ](0.790) <UNK> noun part.
<UNK> part.pcnqil Vania university ~ j .
ENIAC of-no.49 i~ ,v  I 1"<=7~'~ I I ENIA?
I \ [ \ ](0.169) noun <UNK> part.
<UNK> part.-ll 1.90 oencil Vania ~ i~y ~j .
ENIAC ~\](0.041) I -'<~5"7~ I \[ )<=7 \] I ENIAC \]noun <UNK> noun part.
<UNK> part.numeral suffix part.
verb intl.
sym.numeral suffix part.
verb intl.
sym.numeral suffix part.
verb infl.sym.Figure 1: Japanese Morphological Analysis Examplebecause ~:  ('university') is registered in the dic-tionary.
This is correct.
In the second analysis,the system guesses .
'<~-7"~: ('Vania university')as an unknown word, because -'<~/5.
'A~ (transliter-ation of 'pencil') is registered in the dictionary andsome university names are registered in the dictio-nary, such as Y,~ ~/7~- - -b '~ ('Stanford Uni-versity') and ~r~'~ ~3~-~ (.
'Cambridge Uni-versity').
In the third analysis, the system consid-ers ,'<-~7" ('Vania') as an unknown word, becauseboth ~:/5,',,1~ and ~ are registered in the dic-tionary.It is often the case that we have overlappingword hypotheses if the input sentence contains un-known words, such as -'<~'~\]P.'<~7, :7~,and ,,<:'T in Figure 1.
We need a criteria to se-lect the most likely word hypothesis from amongthe overlapping candidates.
In fact, it is fairly dif-ficult to get plausible analyses like the ones shownin Figure 1, because failure to identify an unknownword affects the segmentation f the neighboringwords.
Obviously, a robust word segmenter is theessential first step.In the following sections, we first describe astatistical language model to cope with unknownwords.
We then describe the word segmentationalgorithm and the new word extraction method,with their derivation as an approximation of ageneralization f the Forward-Backward algorithm(Baum, 1972).
Finally, we show experiment re-sults and prove its effectiveness.Statistical Language ModelSegmentat ion  Mode l  (Tagging Mode l )Let the input Japanese character sequence beC = ca c2... cm, and segment i into word sequenceW = wl w2 ... wn whose part of speech sequence is7' = t lt2.
.
.
tn.
The word segmentation task canbe defined as finding the set of word segmentationand parts of speech assignment (~V, T) that max-imize the joint probability of word sequence andtag sequence given character sequence P(W, TIC ).Since the maximization is carried out with fixedcharacter sequence C, the word segmenter onlyhas to maximize the joint probability of word se-quence and tag sequence P(W, T).
(w, '/1) = arg ,nax P(W, TIC)W,T= arg alas P(W, 7') (i)We call P(W,T) the segmentation model, al-though it is usually called tagging model in En-glish tagger esearch.
In this paper, we comparethree segmentation models: part of speech tri-gram, word unigram, and word bigram.In the part-of-speech trigram model (POS tri-gram model), the joint probability P(W, T) is ap-proximated by the product of parts of speech tri-gram probabilities P(tilti_2,ti_l) and word out-put probabilities for given part of speech P(wiItl)nP(W,T) = ~I  P(tilt,_2,q_OP(wilt,) (2)i=1In the word unigram and word bigram mod-els, the joint probability P(W,T) is approxi-mated by the product of word unigram proba-bilities P(wi,ti) and word bigram probabilitiesP( wl, ti lwi- a, ti- a), respectively.P(W,T)  = H p(w,,t,) (3)i=aP(W,T)  = l'I P(wi,tilwi_a,ti_a) (4)i=aBasically, parameters of these segmentationmodels are estimated by computing the relativefrequencies ofthe corresponding events in the seg-mented training corpus.
However, in order to hamdle unknown words, we have introduced a slightmodification i computing the relative frequencies,as is described in the next section.49Word ModelWe think of an unknown word as a word having aspecial part of speech <U~IK>.
We define a statis-tical word model to assign a word probability toeach word hypothesis.
It is formally defined as thejoint probability of the character sequence cl ... ckif wi is the unknown word.
We decompose it intothe product of word length probability and wordspelling probability,P(wi\[<U~K>) = P(e , .
.
.
ck I<UNK>)= P(k )P (C l .
.
.
ck IZ~) (5)where k is the length of the character se-quence.
We call P(k) the word length model, andP(cl ... ck I k) the word spelling model.We assume that word length probability P(k)obeys a Poisson distribution whose paraineter isthe average word length ,~ in the training corpus,(~ - 1)~-~ P(k ) -  "~':~)T. e-()~-l) (6)This means that we regard word length as theinterval between hidden word boundary markers,which are randomly placed with an average inter-val equal to the average word length.
Althoughthis word length model is very simple, it plays akey role in making the word segmentation algo-rithm robust.We approximate he spelling probability givenword length P(O ... ck \[k) by the word-based char-acter bigram model, regardless of word length.Since there are more than 3,000 characters inJapanese, the amount of training data would betoo small if we divided them by word length.kP(cl.
.
.
ca) = P(c, I#) 1Y\[ P(c, I~,-,)P(#1~)(7)Here, special symbol "#" indicates the wordboundary marker.Note that the word-based character bigrammodel is different from the sentence-based charac-ter bigram model.
The former is estimated fromthe corpus egmented into words.
It assigns alargeprobability to a character sequence that appearsin the beginning (prefixes), the middle, and theend (suffixes) of a word.
It also assigns a smallprobability to a character sequence that appearsacross a word boundary.By using the word model, we can createmodified segmentation models that take unknownwords into consideration.
The parameters of themodified POS trigram, word unigram, and wordbigram are estimated by Equations (8), (9), (10),and (11), in Figure 2.hi Figure 2, C(.)
denotes the count of the spec-ified event in the training corpus.
In the part ofspeech trigram model, P(wi\[ti) for an unknownword wi is obtained, by definition, from the wordmodel P(wi\]<UNK>).
In the word unigram model,the unigram count C(wi) for unknown word wi isgiven as the product of the total unigram countof unknown words C(<UNK>) and the word modelprobability P(wil<UNK>).
The higher order N-gram counts involving unknown words are also ob-tained in the same manner.In order to compute the parameters in Fig-ure 2, we need the counts involving unknownwords, such as C(ti-2, ti-1, <UNK>), C(<UNK>), andC((wi-~,tl-a),<UNK>).
These counts are impor-tant because they represent the contexts in whichunknown words likely to appear.
To estimatethese counts, we replace all words appearing onlyonce in the training corpus with unknown wordtags <UNK>, before computing relative frequen-cies.
The underlying idea of the replacement isthe same as Turing's estimates in back-off smooth-ing (Katz, 1987).
We redistribute the probabil-ity mass of low count sequences to "unseen" se-quences.Genera l i zed  Forward  BackwardReest imat ionGeneral izat ion of the Forward andViterbi A lgor i thmIn English part of speech taggers, the maximiza-tion of Equation (1) to get the most likely tag se-quence, is accomplished by the Viterbi algorithm(Church, 1988), and the maximum likelihood es-timates of the parameters of Equation (2) areobtained from untagged corpus by the Forward-Backward algorithm (Cutting et al, 1992).
How-ever, it is impossible to apply the Viterbi algo-rithm and the Forward-Backward algorithm forword segmentation f those languages that haveno delimiter between words, such as Japanese andChinese, because word segmentation hypothesesoverlap one another.Figure 3 shows an example of overlappingword hypotheses and possible word segmentationsfor the string ~N~t~ig-f~ ('all prefectures in thenation').
We assume ~\ [ \ ]  ('all nation'), ~ ('all'),\ [~ l  ('national capital'), ~ii~;g~t,~ ('prefectures'),~i.~ ('metropolitan road'), ~li ('metropolis'), ~Kfft.~ ('prefectures'), ~ ('road'), ~ ('prefectures'),~.f ('prefecture'), and ~ ('prefecture') are regis-tered in the dictionary.
There are 15 possible wordsegmentations in this example.
In Japanese, alot of words consist of one character.
Moreover,sequence of characters may constitute a differentword.50C(t,_2,t,_ x,<UNK>)P(t i l t i -2 , t i -~)  = c(t,_2,t,_t) ifti ---- <lINK>c(~,_~,t,_~) otherwiset:'(wiI<UNK>) if tl = <UNK>P(wi Its) = _ ~  otherwisec(<U~K>)P(wi , t i )  ~_ .c(,o,.t,) ?
P(w~I<U\]K>) if/~ = <lINK>: ~(w,,t,) otherwise ~,  c(w,*,)c((w,_~,~,_O,<U~iK>)P(wl , t i lwi_~,t i -~)  : c(,o,_~,t,_~) x P(wiI<UNK>) i f t i  = <UNK>c((w,_~,t,_x),(w,,,,)) otherwise c(w,_x,t,-x)Figure 2: Modified Segmentation Models with Consideration to Unknown Words.
(8)(9)(10)(11)IlFigure 3: Overlapping Word Hypotheses and Pos-sible Word SegmentationsFor Japanese word segmentation, we definea generalized Forward algorithm and a general-ized Viterbi algorithm as follows.
Let the inputJapanese character sequence of length n be C =cl c2 .
.
.
c,, and cg denote the substring cp+ l .
.
.
%.We define a flmction D that maps a charactersequence c_q to a list of word hypotheses {wi}.Function/~ is the generalization f the dictionary.Here, wi denotes a combination of orthography(formally denoted by wi) and part of speech ti,for simplicity.
We use word bigram as the seg-mentation model in the following example.
Othersegmentation models, such as part of speech tri-gram and word unigrarn, can be used in the samemanner.In the generalized forward algorithm, the for-ward probability o~(wi) is the joint probability ofthe character sequence c~ and the event that thefinal word in the segmentation f cq0 is wi thatspans the substring d .
Forward probabilities canbe recurslvely computed as follows.O<p<q wiED(c~)e o < q <., q < <.
02)The generalized forward algorithm starts fromthe beginning of the input sentence, and proceedscharacter by character.
At each point q in thesentence, it sums over the product of the forwardprobability of the word segmentation hypothesesending at the point ~pq(wl) and the transitionprobability to the word hypotheses starting at thatpoint P(wi+l \[wi).o~ i ~ 2~ 3~ 4~ s~ 6,Figure 4: One Step in the Generalized ForwardAlgorithrn.Figure 4 shows a snapshot of the generalizedforward algorithm.
Tile input is ~\ [ \ ]~ i~,  andthe current point q is 2.
The word hypothesesending at point 2 (wi 6 n(c~))  are ~I~ (Co 2) and\[\] (c~).
Those starting at point 2 (wi+x 6 D(c~))are ~ J .~  (c~), ~_  (c~), and ~li (c~).
The string~$~ (c25) is not registered in the dictionary.
Allcombinations of these words are examined.The generalized Viterbi algorithm can be ob-51tained by replacing summation with maximizationin Equation (12).
Here, Cpq(wi) is the probabil-ity of the most likely word segmentation sequencefor the character sequence cq0 whose final word wispans the substring c~.6;(wi+l) = max max ?q~(w,)P(w,+~lw,) o_<p<q ~,ev(?~)w,+l e D(c;),O _< q < u,q < r < n (13)Note that tile original Forward algorithm andtile Viterbi algorithin is the special case in Equa-tion (12) and (13) where p and q are fixed asp=q-1  andr=q+i .In order to handle unknown words, the dictio-nary function D returns a word hypothesis taggedas unknown word if the substring cpq is not regis-tered in the dictionary, such as ~i.~gf (%5) in Fig-ure 4.
The word model assigns a reasonable prob-ability to the unknown word.
Therefore, in thegeneralized forward algorithm and the generalizedViterbi algorithm, we hypothesize all substringsin the input sentence as words, and examine allpossible combinations of these word hypotheses.Since we can define the generalized Back-ward algorithm in the same manner, we can de-fine the generalized Forward-Backward algorithmto estimate the word N-gram counts in Japanesetexts, and to reestimate the word N-gram prob-abilities in the segmentation model.
However,we give a more intuitive account of the methodto introduce an approximation of the generalizedForward-Backward algorithm.Expected  Word  N-gram CountBy using the above mentioned word segmentationalgorithm, we can get al word segmentation hy-potheses of the input sentence.
Once we get them,we can estimate word N-gram count in an unseg-mented Japanese corpus.Let Oj be the j th  word segmentation hypoth-esis for the ith sentence in the corpus.
P(O~) can?
dbe cornputed by using the segmentahon modelThe Bayes a posleriori estimate of the word un-igram count Ci(wi) and the word bigram countCi(wi_l, wi) ill the ith sentence can be computedas ,C'(wo) = ~"~" P(Oj) x n~(w~)) (14) z..,t P(oD3i r - , ,  P(O}) xn~(w~, c (wo,w ) = P(O;) --3Here,.
n}(w~) and.
ni'(w~'w3 Z) denote the numberof tunes the umgram w~ and the bigram w~, w~appeared in tile j th  candidate of tile ith sentence1The estimate of the total unigram countC(w~) and the total bigram count C(w~, wE) canbe obtained by summing the counts over all sen-tences in the corpus.c( , ,o)  = (16)ic(wo, = (17)iThe estimate of the unigram probability andthe bigram probability can be obtained as the rel-ative frequency of the associated events.c(wo) (is) f(w~) -- 'wC(wo, (19) f(wfllwc')-- C(w~)If necessary, we can reestimate the word N-gramprobabilities by replacing P(w~) and P(w~lw,~ )with f(w~) and f(wolw~).Ext ract ion  o f  New Words  in TextsExpected word unigram counts (expected wordfrequencies) in the corpus (Equation (16)) can beused as a measure of likelihood that a particularsubstring in the input texts is actually a word.
Let0 denote the minimum expected word frequencythat we use to classify a given word hypothesis w~as a word.C(w.) > o (20)Those words that are not found in the dictionaryand whose expected frequencies in the corpus arelarger than the threshold O are extracted as thenew words in the input texts.In theory, expected word N-gram counts canbe obtained by the generalized Forward-Backwardalgorithm.
In order to save computation time,however, we approximated the weighted sum ofthe word N-gram counts over all the word seg-mentation hypotheses in a sentence (Equation(14)), by that of the N-best word segmentationhypotheses 2.1Note that the (Generalized) Forward-Backwardalgorithm is devised to compute these expected wordN-gram count without listing all word segmentationhypotheses.2If we only use the best word segmentation, it iscalled the Vitcrbi reestimation.
Our method mightbe called N-best reestimation.
It is designed to bemore accurate than the Viterbi rcestimation and moreefficient han the generafized Forward-Backward algo-rithm.520.2 \ [~ i  -~0.1 ~ ~Figure 5: An example of computing the expectedword frequenciesN-best word segmentation hypotheses can beobtained by using the Forward-DP Backward-A*algorithm (Nagata, 1994).
It consists of a for-ward dynamic programming search to record tlleprobabilities of all partial word segmentation hy-potheses, and a backward A* algorithm to extractthe N-best hypotheses.
It is a generalization ofthe tree-trellis earch (Soong and Huang, 1991),in the sense that its forward Viterbi search isreplaced with the generalized Viterbi search de-scribed in this paper.In reestimating the word N-gram probabili-ties, we introduce two modifications to the normalreestimation procedure.
The first modification isthat, instead of using the relative frequency in anunsegmented corpus (Equation (18) and (19)), wecombine the N-gram count in the segmented cor-pus with the estimated N-gram count in the un-segmented corpus to increase stimate reliability.This is because a fairly large amount of segmentedJapanese corpus were available in our experiments.c,~(w~) +c..,o~(w~) (2~)f(w?,) = ~?~ Cseo(wc~ ) + ~?, C .... o(wc,)f(~,lw~) = c~?~(w~,w,) + c .
.
.
.
A w. ,w, ) -  c~?~(w~) + ~2- )  (22)where C,~a(. )
denotes the count in the segmentedcorpus, and Cuns,a(') denotes the estimated countin tile unsegmented corpus.The second modification is that we prune theexpected N-gram counts in the unsegmented cor-pus if they are lower than a predefined threshold,before computing Equation (21) and (22).
Thisis because Cunse#(') is unreliable, especially whenC%,,,a(. )
is low.Examples  o f  Es t imat ing  ExpectedWord F requenc iesFinally, we show a simple example of estimat-ing the word N-gram counts in an unsegmentedsentence.
Assume that the ith input sentence isthe character sequence ~-~-~-)kPq, which means"introduction to linguistics", and its best threeword segmentation hypotheses are as shown inFigure 5.
The leftmost nmnbers in Figure 5 arethe relative probabilities of the word segmentationP(O)) hypotheses, corresponding to ~ p(oD ill Equa-tion (14).
The expected word unigram count ofeach word hypothesis in the sentence is,C~(.z.Pq) = 0.7 + 0.2 + 0.1 = 1.0o,n-~-) ---- 0.7c ' (~-~)  = c~(~:) = 0.2c~(~ -) = c~(~)  = 0.1The expected total number of tile words in tile sen-tence ~ Ci(w~) is 2.3.
If all word hypotheses arenot registered in tile dictionary and the threshold0 is 0.15, we regard )kPq ('introduction'), ,~-~liq:('linguistics'), ~ ('language'), and q: ('study')as tile new words.
~" ('say') and ~/iq: ('study oflanguages') are discarded.Let us give another example that shows theeffect of summing tlle expected word unigramcounts over all the sentences in the corpus.
Sup-pose tile sentence "-"-~ 5 / J~ ,~7~q:~:  ENIAC?
50 J~ l~ 5o ", which means "University ofPennsylvania celebrates the 50th anniversary ofENIAC.
", is in the corpus, and the first threeword segmentation hypotheses are as shown inFigure 1.
The expected word unigram counts for~/ "~A- ,~= 7" ('Pennsylvania'), ,<2 7~ ('Va-nia University'), and \]<~7" ('Vania') are 0.790,0.169, and 0.041, respectively.
Suppose also thesentence "zh~4" b\]~gc~2:~'.-~/5/A~'<=7~ 9 ~5~~b 7~o ", which means "White House lies at Penn-sylvania Avenue.
", is in the corpus, and the ex-pected word unigram counts for -~-:/~/z~,<:7" ('Pennsylvania'), .
'<-:7"~ V ('Vania Avenue'),and J<~7 ('Vania') are 0.825, 0.127, and 0.048,respectively.
The expected word unigram countsin the corpus are,C( -" -~/~/~,<~7) = 0.790 + 0.825 = 1.615C( ,<=7~)  = 0.169C( ,<~-7~9)  = 0.127C( ,<~7)  = 0.041+0.048 = 0.089Therefore,-'<>'5/z11~,<=7 is definitely more likelyto be a new word.
Tile more often the unknownword appears in the corpus, the more it is likelyto be extracted, even if there is word segmentationambiguity in each sentence.Exper imentsLanguage DataWe used the EDR .Japanese Corpus Version 1.0(EDR, 1995) to train and test the word segmen-53tation program.
It is a corpus of approximately5 million words (200,000 sentences).
It was col-lected to build a Japanese Electronic Dictionary,and contains a variety of Japanese sentences takenfrom newspapers, magazines, dictionaries, ency-clopedias, textbooks, etc.
It has a variety of an-notations on morphology, syntax, and semantics.We used word segmentation, pronunciation, andpart of speech in the morphology information fieldof the annotation.In this experiment, we randomly selected 90%of the sentences in the EDR Corpus for trainingthe word segmentation program.
We made twotest sets from the rest of the corpus, one for a smallsize experiment (100 sentences) and the other fora medium size experiment (1000 sentences).
Ta-ble 1 shows the number of sentences, words, andcharacters for training and test sets.
Note that thetest sets were not part of the training set.
Thatis, open data were tested in the experiment.Table 1: The amount of training and test datatraining test-1 test-2Sentences 192802 100 1000Words 4746461 2463 25177Characters 7521293 3912 39875The training texts contained 133281 wordtypes.
We discarded word types that appearedonly once in the training texts.
This resulted in65152 word types being registered in the dictio-nary of the word segmenter.
We trained threesegmentation models, namely, part of speech tri-gram, word unigram, and word trigram, after wereplaced those words appeared only once in thetraining texts with the unknown word tag <UNK>,as described in the section of word model.
Af-ter this replacement, here were 758172 distinctword bigrams.
Again, we discarded word bigramsthat appeared only once in the training textsfor saving main memory, and used the remaining294668 word bigrams.
The word bigram proba-bilities were smoothed using deleted interpolation(Jelinek, 1985).The training texts contained 3534 charactertypes.
We discarded characters that appearedonly once in the training texts; 3167 charactertypes remained.
We then replaced the discardedcharacters with the unknown character tag totrain the word spelling model.
There were 91198distinct character bigrams in the words in thetraining texts 3aThere are more than 3000 (some say nlore than10000) charters in Japanese, and their frequency dis-tribution is skewed.
In order to save memory, we useda type of character bigram model that considers un-We made two spelling models.
The first wastrained using all words in the training texts, whilethe second was trained using those words whosefrequency is less than or equal to 2.
In princi-ple, the spelling model of unknown words must betrained using the low frequency words.
However, itnlight suffer from the sparse data problem becausethe total number of word tokens for training is de-creased from 4746461 to 103919.
We also madetwo length models.
The average word lengths ofall words and that of low frequency words were1.58 and 4.49, respectively.
Note that the aver-age word length is the only parameter of the wordlength model.Evaluation MeasuresWord Segmentation accuracy is expressed in ternrsof recall and precision.
First, we count the numberof words in corpus segmentation (Std), the num-ber of words in system segmentation (Sys), andtile number of matching word segmentations (M).Recail is defined as M/Std, and precision is definedas M/Sys.Figure 6 shows an example of computing pre-cision and recall for the sentence "ta ~ ~ 7 ~ 2 - -~c~J~-~..fi~"~"~'% ", which means "RockefellerLaboratory is an academic laboratory founded byan American millionaire, Rockefeller".
Because ofthe difference in the segmentation of ~ ~ ~ 7 z:~- - iT~p~,  the number of words in corpus seg-mentation (Std=15) differs from that of systemsegmentation (Sys=14).
Note that the system cor-rectly tokenized -~fbJ~E~, although it is not reg-istered in the dictionary.New word extraction accuracy is described interms of recall, precision, and F-measure.
First,we count the number of unknown words in the cor-pus segmentation (Std), the number of unknownwords in the system segmentation (Sys), and thenumber of matching words (M).
Here, unknownwords are those that are not registered in the sys-tem dictionary.
Recall is defined as M/Std, andprecision is defined as M/Sys.
Since recall andprecision greatly depend on the frequency thresh-old, we used the F-measure to indicate the overallperformance.
F-measure is used in InformationRetrieval, and is calculated byF= (/32+l.O) xPxR/32 x P+R(23)where P is precision, R is recall, and/3 is the rel-ative importance given to recall over precision.known characters, like the word bigram model used inthe segmentation model.54IJC00092627corpus segmentat ion~,~ I / x  i ~j~~ ~ ~ ~ / ~.~ ~J 2 / ~ ~@/ / / ~j~i~l'~ '~/7~' ,2  / ~,~i~\]7Y: I ~" I ~j~Jb / ~ /~)$vg- I  .~.x I IJJ~J~lsystem segmentationI ~ 7 ~ 9 - - / ~ 7 ~> ~ / ~ = ~ - ~  / -~11:: / /~  / ~ l? )
/ )  / ~Jj~a\]~/7~'~ / ~p~?1-~'Y~J '~ I i~ ,~L.
I +.." I ~f)=$re_ I J' I ~JJ1D~.~3I --~--t$f@?~/~IIL/<UNK>o /o /~Rockefellerlaboratoryparticle (topic)Americaofbigrich manRockefellerpar t i c le  ( sub jec t )foundinflectional suffixauxiliary verb (past)academic laboratorybe(period)sys=lS, std=14, matched=13prec is ion=87.7  (13/18) ,  reca l l=92.9  (13/14)Figure 6: Comparison between the corpus segmentation (left) and the system segmentation (right).words are listed in UNIX sd i f f  style.AllWord  Segmentat ion  AccuracyIn order to decide the best configuration of the un-derlying Japanese word segmenter, we comparedthree segmentatio n models: part of speech tri-gram, word unigram, and word bigram.
We alsocompared three word models: all words, low fre-quency words, and the combination of the two.The third word model consisted of the spellingmodel trained using all words and the lengthmodel trained using low frequency words.Table 2 shows, for the small test set (100 sen-tences), the segmentation accuracy of the variouscombinations of the segmentation models and theword models.It is obvious that word bigram outperformedthe part of speech trigram as well as word unigram.As for the word model, it seems the combinationof the spelling model for all words and the lengthmodel for low frequency words is the best, but thedifference is small.
In the following experiment, wedecided to use word bigram as the segmentationmodel, and the combination of the spelling modelof all words and the length model of low frequencywords as the word model.New Word  Ext ract ion  AccuracyWe tested the new word extraction method us-ing the medium size test set (1000 sentences).
Itcontains 538 unknown word types.
8 word typesappeared twice in the test set.
The other 530 wordtypes appeared only once.
The out-of-vocabularyrate of the test set is 2.2%.
To count the expectedword frequencies, we used the top-10 word seg-mentation hypotheses.
We limited tile maximumcharacter length of the a unknown word to 8 inorder to save computation time.We tested three variations of the new wordextraction method.
The first one was "No Reesti-marion"; it uses the word segmenter's outputs asthey are when extracting new words.
The secondand the third ones carry out reestimation beforeextraction, where the pruning thresholds of the ex-pected N-gram counts in the reestimation are 0.95and 0.50, respectively.
Reestimations were carriedout three times.Table 3 shows the new word extraction ac-curacies for a variety of expected word frequencythresholds 0, with and without reestimation.
InTable 3, we set fl = 1.0 to compute F-measure.As Table 3 shows, the higher the threshold is,the higher the precision and the lower the recallbecome.
When we put equal importance on recalland precision, the best value for the expected wordfrequency threshold is around 0.10 where the recallis 43.7% and the precision is 52.3%.Figure 7 shows excerpts of correctly extractednew words (matched), incorrectly extracted wordhypotheses (sys-matched), and new words thatwere not extracted (std-matched), when the fre-quency threshold was 0.5 and reestimation was notcarried out.
We find that the overall quality ofthe extracted word hypotheses i satisfactory, al-55Table 2: Language Models and Segmentation Accuracies (100 test sentences)POS trigram word unigram word bigramword model recall prec.
recall prec.
recall prec.all words 91.6 88.8 88.7 87.3 94.6 : 89.4low frequency words 91.5 89.5 88.4 88.0 94.3 90.1all words + l.f.w, length 91.5 89.3 88.8 87.6 94.7 89.9Table 3: New Word Extraction Accuracy (1000 test sentences)freq.>0.00>0.10>0.50>0.90>0.95>0.99No Reestimation freq>0.95, 3 iter.
freq>0.50, 3 iter.recall prec.
F recall prec.
F recall prec.
F56.1 34.2 42.5 50.6 37.9 43.4 39.6 56.7 46.643.7 52.3 47.6 43.1 52.1 47.2 37.9 63.6 47.536.4 65.6 46.8 36.1 65.8 46.6 36.6 65.2 46.925.3 76.8 35.8 25.3 77.3 38.1 36.6 65.2 46.923.2 78.1 35.8 23.4 78.3 36.1 36.6 65.2 46.917.3 81.6 28.5 23.4 78.3 36.1 36.6 65.2 46.9though the values of recall and precision are notso high.
We discuss the reason for this in the nextsection.DiscussionThe problem of Japanese word segmentation isthat people often can not agree on a single wordsegmentation.
Therefore, the reported perfor-mance could be greatly underestimated.
Most ofthe new words extracted by the system are ac-ceptable as a word (at least for us), and nmy notnecessarily be a wrong word entry.
On the otherhand, most of the new words not extracted by thesystem can be divided into shorter words that areregistered in the dictionary.For example, in the first sentence of Fig-ure 8, W'~/~' ?
~ :2 ~-~- -5 /~ .-/( 'data coinmu-nication') is regarded as one word in corpus seg-mentation and counted as an unknown word inthe test sentence.
However, the system seg-mented it into -U--~ ('data') and = :2 :~0- - -5/~ Z/('communication'), both of which are foundin the dictionary.
In the second sentence of Fig-ure 8, the system extracted ."
,3- -  ~'7/c~ ('Dukeof Hanover') as a new word, while this word is di-vided into ~ ' , / - -~ '~ ('Hanover') and ~ ('Duke')in corpus segmentation.
Most of extraction errorsare of this category.There are three types of obvious extractionerrors.
The first type is the truncation of longwords.
Some transliterated Western-origin wordsexceed the predefined maximum length for un-known word.
The third sentence of Figure 8 is anexample of this type.
In Japanese, 'illustration' istransliterated into 9 characters ~ ~ 7, b 1/--5/:/, which exceeds tile maximum unknown wordlength of 8 characters in our system.
Since 4" ~1- (the transliteration of 'illust', which also meansillustration in Japanese) is registered in the dictio-nary, t / - -5 /~ .
/ ( the  transliteration of 'ration') isincorrectly extracted as a new word.The second type is the fragmentation of nu-merals.
Since we did not use any tokenizers,numerals tend to be divided arbitrarily.
In thesecond sentence in Figure 8, the system divided"1676" into "16" and "76".
In fact, it may output"1" and "676", "16 .... 7" and "6", or whatever.The third type is the concatenation f noun(s)and particle.
In other words, the system some-times erroneously recognizes a noun phrase as aword.
For example, the Japanese counterparts of"A of B", "A and B", and "A, B" are recognizedas a word.
This may be because the probabilityof one long unknown word can be higher than theproduct of the probabilities of two short unknown(or infrequent) words and one known word.
Thefourth sentence of Figure 8 is an example of thistype of error.
The system considered ~li~l\]li~lh~-'v~l~tlJ ('controllable and observable') as a word,while it is divided into ~-I ('able'), fi~Jt~ ('control'),~-o  ('and'), ~f ('able'), and ~t J  ('observe') in thecorpus.As for reestimation, Table 3 shows no signif-icant improvements in the new word extractionaccuracy.
The only effect of reestimation, in ourexperiment, is to increase the expected word fre-quencies of the unknown word hypotheses whoseexpected word frequencies are greater than thepruning threshold of reestimation.This result does not necessarily mean thatreestimation is useless.
This is because most tin-56Imat ched=1963~1487 b~t ,~2000 ~-P~ ~A'~,~ F ,A .~- -~ b~l ,y~ 7~) - -b '~p,~ I /b~J~,~ays-ma~ched=10390Zf7  000R STK~ m~Y~$J= mix b~lyS;  -~- - \ ]1 .
,~  77"F-T~7"{~ 7t~- -~4~/?"
~?
:~std-mat ched=342404 BBNTF.,<>'~ b" =:/~=~--~--~f X~2~ ~bo~SP~ ~-~,9y7~ t~-P~?--~-~-threshold=0.5std=538, sys=299, matched=196recal l=36.4 (196/538), precis ion=65.6 (196/299)Figure 7: Excerpts of correctly extracted new words (matched), incorrectly extracted word hypotheses(sys-raatched), and not extracted new words (std-matched).known words appeared only once in the test sen-tences.
An ideal example to confirm that reesti-marion works well would have an unknown wordappearing more than twice in the test sentences,and it is trivial to extract he word in one appear-ance, while it is difficult in the others, becauseof, for example, successive unknown words.
If thetest set were larger, or the out-of-vocabulary ratewere higher, we believe that the effectiveness ofreestimation would be more clearly shown.Re la ted  WorkRecent years have seen several works on corpus-based word segmentation a d dictionary construc-tion for both Japanese and Chinese.
For Chi-nese, (Sproat et al, 1994) used the word unigrammodel in their word segmenter based on weightedfinite-state transducer.
Word frequencies were es-timated by the Viterbi reestimation (a reesthna-tion procedure using the best analysis) from anunsegmented corpus of 20 million words.
Initial es-timates of the word frequencies were derived fromthe frequencies in the corpus of the strings of hanzimaking up each word in the lexicon whether or noteach string is actually an instance of the word inquestion.
(Chang et al, 1995) proposed an automaticdictionary construction method for Chinese from alarge unsegmented corpus (311591 sentences) withthe help of a small segmented seed corpus (1000sentences).
They combined Viterbi reestimationusing the word unigram model with a post filtercalled the "Two-Class Classifier", which is a lin-ear discrimination function to decide whether thestring is actually a word or not based on featuresderived from the character N-gram in a large un-segmented corpus.
The system's performance iscompared with a word list derived from two on-line Chinese dictionaries (21141 words).
Tile re-ported recall and precision values were 56.88% and77.37% for two character words, and 6.12% and85.97% for three character words, respectively.For Japanese, (Nagao and Mori, 1994) pro-posed a method of computing an arbitrary lengthcharacter N-gram, and showed that the charac-ter N-gram statistics obtained from a large cor-pus includes information useful for word extrac-tion.
However, they did not report any evaluationof their word extraction method.
(Teller and Batchelder, 1994) proposed a verynaive probabilistic word segmentation method forJapanese, based on character type informationand hiragana bigram frequencies.
They claimed98% word segmentation accuracy, while we clMrn94.7%.
However, their evaluation method is veryoptimistic, and completely different from ours.They count an error only when the system segmen-tation violates morpheme boundaries.
In otherwords, they count an error only when the systemsegmentation is not acceptable to human judge-men% while we count an error whenever tim sys-tem segmentation does not exactly match the cor-pus segmentation, even if it is inconsistent.We used the word bigram model for wordsegmentation, and expected word frequency forunknown word extraction.
We compared theresults with a segmented Japanese corpus, andreported 43.7% recall and 52.3% precision for1000 sentences whose out-of-vocabulary ate is2.1%.
It is impossible to compare our results with(Chang et al, 1995), because the experiment con-ditions are completely different in terms of lan-guage (Chinese vs. Japanese), the size of seedsegmented corpus, the size of target unsegmentedcorpus and its out-of-vocabulary ate, the size ofinitial word list, and the type of reference data57(on-line dictionary vs. segmented corpus).Our idea of filtering erroneous word hypoth-esis by expected word frequency is simple andstraightforward.
The major contribution of thispaper is that we present a more accurate methodfor estimating word frequencies in an unsegmentedcorpus, even if it includes unknown words.
Thisis achieved by introducing an explicit statisticalmodel of unknown words, and by using an N-best word segmentation algorithm (Nagata, 1994)as an approximation of the generalized Forward-Backward algorithm.In English taggers, (Weischedel t al., 1993)proposed a statistical model to estimate word out-put probability p(wi\]tl) for an unknown word fromspelling information such as inflectional endings,derivational endings, hyphenation, and capitaliza-tion.
Our word model can be thought of a gener-alization of their statistical model.
One potentialbenefit of our statistical model and segmentationalgorithm is that they are completely independentof the target language and its writing system.
Weintend to test our word segmentation method onother languages, uch as Chinese and Thai.ConclusionWe present a new word extraction method forJapanese based on expected word frequency, whichis computed by using a statistical language modeland an N-best word segmentation algorithm.
Al-though we have encouraging initial results, thereare a number of questions to be answered, for ex-ample, the minimmn seed segmented corpus sizerequired, the minimum initial word list required,the effect of reestimation for a large unsegmentedcorpus with various out-of-vocabulary ates.
Be-sides these questions, we are also thinking of as-signing the part of speech to the extracted newwords in order to construct a Japanese dictionaryautomatically.References\[Baum, 1972\] Leonard E. Baum.
1972.
An In-equality and Associated Maximization Tech-nique in Statistical Estimation for ProbabilisticFunctions of Markov Processes.
Inequalilies, 3,pages 1-8.\[Chang et al, 1995\] Jing-Shin Chang, Yi-ChungLin, and Keh-Yih Su.
1995.
Automatic Con-struction of a Chinese Electronic Dictionary, InProceedings of VLC-95, pages 107-120.\[Church, 1988\] Kenneth W. Church.
1988.
AStochastic Parts Program and Noun PhraseParser for Unrestricted Text, \[n Proceedings offANLP-88, pages 136-143.\[Cutting et al, 1992\] Doug Cutting, Julian Ku-piec, Jan Pedersen, and Penelope Sibun.
1992.A Practical Part-of-Speech Tagger, In Proceed-ings of ANLP-92, pages 133-140.\[EDR, 1995\] JapanElectronic Dictionary Research Institute.
1995.EDR Electronic Diclionary Version 1 Techni-cal Guide, EDR TR2-003.
Also available as TheSlructure of the EDR Eleclronic Dictionary,http:///www, iijnet, or.
jp/edr/.\[Jelinek, 1985\] FrederickJelinek.
1985.
Self-organized Language Model-ing for Speech Recognition.
IBM Report.\[Katz, 1987\] Slava M. Katz.
1987.
Estimation ofProbabilities from Sparse Data for the Lan-guage Model Component of a Speech Recog-nizer, IEEE Trans.
ASSP-35, No.3, pp.400-401.\[Nagao and Mori, 1994\] Makoto Nagao and Shin-suke Mori.
1994.
A New Method of N-gramStatistics for Large Number of n and Auto-matic Extraction of Words and Phrases fromLarge Text Data of Japanese, in Proceedings ofCOLING-94, pages 611-615.\[Nagata, 1994\] Masaaki Nagata.
1994.
A Stochas-tic Japanese Morphological Analyzer Using aForward-DP Backward-A* N-Best Search Al-gorithm.
In Proceedings off COLING-94, pages201-207.\[Nagata, 1996\] Masaaki Nagata.
1996.
Context-Based Spelling Correction for Japanese OCR.To appear in Proceedings off COLING-96.\[Soong and Huang, 1991\] Frank K. Soong andEng-Fong Huang.
1991.
A Tree-Trellis BasedFast Search for Finding the N Best SentenceHypotheses in Continuous Speech Recognition.In Proceedings off ICASSP-91, pages705-708.\[Sproat et al, 1994\] Richard Sproat, ChinlinShih, William Gale, and Nancy Chang.
1994.
AStochastic Finite-State Word-Segmentation Al-gorithm for Chinese, In Proceedings ofA CL-94,pages 66-73.\[Teller and Batchelder, 1994\] Virginia Teller andEleanor Olds Batchelder.
1994.
A ProbabilisticAlgorithm for Segmenting Non-Kanji JapaneseStrings, In Proceedings off AAAI-94, pages 742-747.\[Weischedel et al, 1993\] Ralph Weischedel, MarieMeteer, Richard Schwartz, Lance Ramshaw,and Jeff Palmucci.
1993.
Coping with Ambigu-ity and Unknown Words through ProbabilisticModels, in Cornpulalional Linguistics, Vol.19,No.2, pages 359-382.58JC00076244Computers are increasingly getting connected through data communication such as satellites andoptical fibers.11c11,13~--~ ?
=~.=~-~- - '2~>' /  I ~ - 9 / ~ - 9 / ~>.
/ .
/~-dat a- (hyphen)c ommunic at ionJC00001185In  1676, he became the  consu l tant  of Duke of Hanover and the  head of the  l ib rary ,  and he workedhard  to  found Ber l in  sc ience  academy, then ,  in  1700, he became the  pres ident .lc l ,21676/  1 6 7 6 /~-~:  116/16/~>76/76/~3,4c4~,}- -~7- - I  ~ / - -~- - /  I~1 =~I~4~ <8,9c8\[~\]-~-~( / t--~ ~ .~ ~ / ~ I~I  ~ ~ / f~ l i  <14c13,14~ /  ~-~ ~:...-?
~ ~/Duke of  Hanoverhead of l i b raryI "<~ ~ 2 / -"<\]~, ') >" / ~ Ber l in> ~W~-- Ig~gY#Tg~ science academyJC00071929He held public exhibition of illustration every year, and found muny new talents, such as Mr.gatsuhiko Hibino.ic i ,2i0c l l14c15I I~t~ / ~ w}2~ = /> ~-- -~ ~/INILI<UNK>I ft ~t~/~IL /<~.mK>ilustrat ionevery yearKatsuhiko HibinoJC00165663If linear system is controllable and observable, Karman filter is asymptotic stable.3,7c3/ ~: / ~ l}  I ~7~JW~, -o~7~ l~ZLl4mg>,~'JW / ~4 -~ ~/~6~ <~,o  / .~: 7 / ~ 1  <,~f I "~ I ~f i~  <~, l  / ~: >" Y ~, / ~ ~,~1 <10c6~/  p/ l~ .
l  I ~/  p / ~15c11,12> ~17>'~41&~controllable and observableinflectional suffixasympt ot i?stabilityFigure 8: Comparison between the corpus segmentation (left) and the system segmentation (right).
Onlydifferences are listed in UNIX sdif:f -s  style.59
