Keeping the initiative: an empirically-motivated approach to predictinguser-initiated dialogue contributions in HCIKerstin Fischer and John A. BatemanFaculty of Linguistics and Literary Sciences and SFB/TR8University of BremenBremen, Germany{kerstinf,bateman}@uni-bremen.deAbstractIn this paper, we address the problemof reducing the unpredictability of user-initiated dialogue contributions in human-computer interaction without explicitly re-stricting the user?s interactive possibili-ties.
We demonstrate that it is possible toidentify conditions under which particularclasses of user-initiated contributions willoccur and discuss consequences for dia-logue system design.1 IntroductionIt is increasingly recognised that human-computerdialogue situations can benefit considerably frommixed-initiative interaction (Allen, 1999).
Interac-tion where there is, or appears to be, little restric-tion on just when and how the user may make a di-alogue contribution increases the perceived natu-ralness of an interaction, itself a valuable goal, andalso opens up the application of human-computerinteraction (HCI) to tasks where both system anduser are contributing more equally to the task be-ing addressed.Problematic with the acceptance of mixed-initiative dialogue, however, is the radically in-creased interpretation load placed on the dialoguesystem.
This flexibility impacts negatively onperformance at all levels of system design, fromspeech recognition to intention interpretation.
Inparticular, clarification questions initiated by theuser are difficult to process because they may ap-pear off-topic and can occur at any point.
But pre-venting users from posing such questions leads tostilted interaction and a reduced sense of controlover how things are proceeding.In this paper we pursue a partial solution to theproblem of user-initiated contributions that takesits lead from detailed empirical studies of howsuch situations are handled in human-human inter-action.
Most proposed computational treatmentsof this situation up until now rely on formalisednotions of relevance: a system attempts to inter-pret a user contribution by relating it to sharedgoals of the system and user.
When a connectioncan be found, then even an apparently off-topicclarification can be accomodated.
In our approach,we show how the search space for relevant connec-tions can be constrained considerably by incorpo-rating the generic conversation analytic principleof recipient design (Sacks et al, 1974, p727).
Thistreats user utterances as explicit instructions forhow they are to be incorporated into the unfold-ing discourse?an approach that can itself be ac-comodated within much current discourse seman-tic work whereby potential discourse interpreta-tion is facilitated by drawing tighter structural andsemantic constraints from each discourse contri-bution (Webber et al, 1999; Asher and Lascarides,2003).
We extend this here to include constraintsand conditions for the use of clarification subdia-logues.Our approach is empirically driven through-out.
In Section 2, we establish to what extentthe principles of recipient design uncovered fornatural human interaction can be adopted for thestill artificial situation of human-computer inter-action.
Although it is commonly assumed that re-sults concerning human-human interaction can beapplied to human-computer interaction (Horvitz,1999), there are also revealing differences (Amal-berti et al, 1993).
We report on a targetted com-parison of adopted dialogic strategies in naturalhuman interaction (termed below HHC: human-human communication) and human-computer in-teraction (HCI).
The study shows significant andreliable differences in how dialogue is being man-aged.
In Section 3, we interpret these results withrespect to their implications for recipient design.The results demonstrate not only that recipient de-sign is relevant for HCI, but also that it leads tospecific and predictable kinds of clarification dia-logues being taken up by users confronted with anartificial dialogue system.
Finally, in Section 4, wediscuss the implications of the results for dialogic185system design in general and briefly indicate howthe required mechanisms are being incorporated inour own dialogue system.2 A targetted comparison of HHC andHCI dialoguesIn order to ascertain the extent to which tech-niques of recipient design established on the ba-sis of human-human natural interaction can betransferred to HCI, we investigated comparabletask-oriented dialogues that varied according towhether the users believed that that they were in-teracting with another human or with an artificialagent.
The data for our investigation were takenfrom three German corpora collected in the mid-1990s within a toy plane building scenario usedfor a range of experiments in the German Collab-orative Research Centre Situated Artificial Com-municators (SFB 360) at the University of Biele-feld (Sagerer et al, 1994).
In these experiments,one participant is the ?constructor?
who actuallybuilds the model plane, the other participant is the?instructor?, who provides instructions for the con-structor.The corpora differ in that the constructor in theHHC setting was another human interlocutor; inthe other scenario, the participants were seated infront of a computer but were informed that theywere actually talking to an automatic speech pro-cessing system (HCI).1 In all cases, there was novisual contact between constructor and instructor.Previous work on human-human task-oriented dialogues going back to, for example,Grosz (1982), has shown that dialogue structurecommonly follows task structure.
Moreover,it is well known that human-human interactionemploys a variety of dialogue structuring mech-anisms, ranging from meta-talk to discoursemarkers, and that some of these can usefully beemployed for automatic analysis (Marcu, 2000).If dialogue with artificial agents were then to bestructured as it is with human interlocutors, therewould be many useful linguistic surface cuesavailable for guiding interpretation.
And, indeed,a common way of designing dialogue structure inHCI is to have it follow the structure of the task,since this defines the types of actions necessaryand their sequencing.1In fact, the interlocutors were always humans, as the ar-tificial agent in the HCI conditions was simulated employingstandard Wizard-of-Oz methods allowing tighter control ofthe linguistic responses received by the user.Figure 1: Contrasting dialogue structures for HHCand HCI conditionsPrevious studies have not, however, addressedthe issue of dialogue structure in HCI system-atically, although a decrease in framing signalshas been noted by Hitzenberger and Womser-Hacker (1995)?indicating either that the dis-course structure is marked less often or that thereis less structure to be marked.
A more precisecharacterisation of how task-structure is used orexpressed in HCI situations is then critical for fur-ther design.
For our analysis here, we focusedon properties of the overall dialogue structure andhow this is signalled via linguistic cues.
Our re-sults show that there are in fact significant differ-ences in HCI and HHC and that it is not possi-ble simply to take the human-human interactionresults and transpose results for one situation tothe other.The structuring devices of the human-to-humanconstruction dialogues can be described as fol-lows.
The instructors first inform their communi-cation partners about the general goal of the con-struction.
Subsequently, and as would be expectedfor a task-oriented dialogue from previous stud-ies, the discourse structure is hierarchical.
At thetop level, there is discussion of the assembly ofthe whole toy airplane, which is divided into in-dividual functional parts, such as the wings orthe wheels.
The individual constructional stepsthen usually comprise a request to identify one ormore parts and a request to combine them.
Eachstep is generally acknowledged by the communi-cation partner, and the successful combination ofthe parts as a larger structure is signalled as well.All the human-to-human dialogues were similar inthese respects.
This discourse structure is showngraphically in the outer box of Figure 1.Instructors mark changes between phases withsignals of attention, often the constructor?s firstname, and discourse particles or speech routinesthat mark the beginning of a new phase such as186goal discourse marker explicit markingusage HHC HCI HHC HCI HHC HCInone 27.3 100 0 52.5 13.6 52.5single 40.9 0 9.1 25.0 54.5 27.5frequent 31.8 0 90.9 22.5 31.8 20.0Percentage of speakers making no,single or frequent use of a particularstructuring strategy.HCI: N=40; HHC: N=22.
All differ-ences are highly significant (ANOVAp<0.005).Table 1: Distribution of dialogue structuring devices across experimental conditionsalso [so] or jetzt geht?s los [now].
This structur-ing function of discourse markers has been shownin several studies and so can be assumed to bequite usual for human-human interaction (Swerts,1998).
Furthermore, individual constructionalsteps are explicitly marked by means of als er-stes, dann [first of all, then] or der erste Schritt[the first step].
In addition to the marking of theconstruction phases, we also find marking of thedifferent activities, such as description of the maingoal versus description of the main architecture,or different phases that arise through the address-ing of different addressees, such as asides to theexperimenters.Speakers in dialogues directed at human inter-locutors are therefore attending to the followingthree aspects of discourse structure:?
marking the beginning of the task-orientedphase of the dialogue;?
marking the individual constructional steps;?
providing orientations for the hearer as to thegoals and subgoals of the communication.When we turn to the HCI condition, however,we find a very different picture?indicating that astraightforward tuning of dialogue structure for anartificial agent on the basis of the HHC conditionwill not produce an effective system.These dialogues generally start as the HHC di-alogues do, i.e., with a signal for getting the com-munication partner?s attention, but then diverge bygiving very low-level instructions, such as to finda particular kind of component, often even beforethe system has itself given any feedback.
Sincethis behaviour is divorced from any possible feed-back or input produced by the artificial system, itcan only be adopted because of the speaker?s ini-tial assumptions about the computer.
When thisstrategy is successful, the speaker continues to useit in following turns.
Instructors in the HCI condi-tion do not then attempt to give a general orienta-tion to their hearer.
This is true of all the human-computer dialogues in the corpus.
Moreover, thedialogue phases of the HCI dialogues do not cor-respond to the assembly of an identifiable part ofthe airplane, such as a wing, the wheels, or thepropeller, but to much smaller units that consistof successfully identifying and combining someparts.
The divergent dialogue structure of the HCIcondition is shown graphically in the inner dashedbox of Figure 1.These differences between the experimentalconditions are quantified in Table 1, which showsfor each condition the frequencies of occurrencefor the use of general orienting goal instructions,describing what task the constructor/instructor isabout to address, the use of discourse markers,and the use of explicit signals of changes in taskphase.
These differences prove (a) that users areengaging in recipient design with respect to theirpartner in these comparable situations and (b) thatthe linguistic cues available for structuring an in-terpretation of the dialogue in the HCI case areconsiderably impoverished.
This can itself obvi-ously lead to problems given the difficulty of theinterpretation task.3 Interpretation of the observeddifferences in terms of recipient designExamining the results of the previous section moreclosely, we find signs that the concept of the com-munication partner to which participants were ori-enting was not the same for all participants.
Somespeakers believed structural marking also to beuseful in the HCI situation, for example.
In thissection, we turn to a more exact consideration ofthe reasons for these differences and show that di-rectly employing the mechanisms of recipient de-sign developed by Schegloff (1972) is a beneficialstrategy.
The full range of variation observed, in-cluding intra-corpus variation that space precludedus describing in detail above, is seen to arise froma single common mechanism.
Furthermore, weshow that precisely the same mechanism leads toa predictive account of user-initiated clarificatorydialogues.187The starting point for the discussion is theconversation analytic notion of the insertion se-quence.
An insertion sequence is a subdialogueinserted between the first and second parts of anadjacency pair.
They are problematic for artificialagents precisely because they are places where theuser takes the initiative and demands informationfrom the system.
Clarificatory subdialogues areregularly of this kind.
Schegloff (1972) analysesthe kinds of discourse contents that may constituteinsertion sequences in human-to-human conversa-tions involving spatial reference.
His results im-ply a strong connection between recipient designand discourse structure.
This means that we candescribe the kind of local sequential organisationproblematic for mixed-initiative dialogue interpre-tation on the basis of more general principles.Insertion sequences have been found to addressthe following kinds of dialogue work:Location Analysis: Speakers check upon spa-tial information regarding the communica-tion partners, such as where they are when ona mobile phone, which may lead to an inser-tion sequence and is also responsible for oneof the most common types of utterances whenbeginning a conversation by mobile phone:i.e., ?I?m just on the bus/train/tram?.Membership Analysis: Speakers check uponinformation about the recipient because thecommunication partner?s knowledge mayrender some formulations more relevant thanothers.
As a ?member?
of a particular class ofpeople, such as the class of locals, or of theclass of those who have visited the place be-fore, the addressee may be expected to knowsome landmarks that the speaker may use forspatial description.
Membership groups mayalso include differentiation according to ca-pabilities (e.g., perceptual) of the interlocu-tors.Topic or Activity Analysis: Speakers attend towhich aspects of the location addressed arerelevant for the given topic and activity.
Theyhave a number of choices at their disposalamong which they can select: geographicaldescriptions, e.g.
2903 Main Street, descrip-tions with relation to members, e.g.
John?splace, descriptions by means of landmarks,or place names.These three kinds of interactional activity eachgive rise to potential insertion sequences; that is,they serve as the functional motivation for par-ticular clarificatory subdialogues being exploredrather than others.
In the HCI situation, however,one of them stands out.
The task of membershipanalysis is extremely challenging for a user facedwith an unknown artificial agent.
There is little ba-sis for assigning group membership; indeed, thereare not even grounds for knowing which kind ofgroups would be applicable, due to lack of experi-ence with artificial communication partners.Since membership analysis constitutes a pre-requisite for the formulation of instructions, recip-ient design can be expected to be an essential forceboth for the discourse structure and for the motiva-tion of particular types of clarification questions inHCI.
We tested this prediction by means of a fur-ther empirical study involving a scenario in whichthe users?
task was to instruct a robot to measurethe distance between two objects out of a set ofseven.
These objects differed only in their spatialposition.
The users had an overview of the robotand the objects to be referred to and typed their in-structions into a notebook.
The relevant objectswere pointed at by the instructor of the experi-ments.
The users were not given any informationabout the system and so were explicitly faced witha considerable problem of membership analysis,making the need for clarification dialogues partic-ularly obvious.
The results of the study confirmedthe predicted effect and, moreover, provide a clas-sification of clarification question types.
Thus, theparticular kinds of analysis found to initiate inser-tion sequences in HHC situations are clearly activein HCI clarification questions as well.21 subjects from varied professions and withdifferent experience with artificial systems partic-ipated in the study.
The robot?s output was gener-ated by a simple script that displayed answers ina fixed order after a particular ?processing?
time.The dialogues were all, therefore, absolutely com-parable regarding the robot?s linguistic material;moreover, the users?
instructions had no impact onthe robot?s linguistic behaviour.
The robot, a Pio-neer 2, did not move, but the participants were toldthat it could measure distances and that they wereconnected to the robot?s dialogue processing sys-tem by means of a wireless LAN connection.
Therobot?s output was either ?error?
(or later in thedialogues a natural language variant) or a distance188usr11-1 hallo# [hello#]sys ERRORusr11-2 siehst du was [do you see anything?
]sys ERRORusr11-3 was siehst du [what do you see?
]sys ERROR 652-a: input is invalid.usr11-4 mi?
den abstand zwischen der vordersten tasse undder linken tasse [measure the distance betweenthe frontmost cup and the left cup]Figure 2: Example dialogue extract showingmembership analysis clarification questionsin centimeters.
This forced users to reformulatetheir dialogue contributions?an effective method-ology for obtaining users?
hypotheses about thefunctioning and capabilities of a system (Fischer,2003).
In our terms, this leads directly to an ex-plicit exploration of a user?s membership analysis.As expected in a joint attention scenario, verylimited location analysis occurred.
Topic analysisis also restricted; spatial formulations were chosenon the basis of what users believed to be ?most un-derstandable?
for the robot, which also leads backto the task of membership analysis.In contrast, there were many cases of member-ship analysis.
There was clearly great uncertaintyabout the robot?s prerequisites for carrying out thespatial task and this was explicitly specified in theusers?
varied formulations.
A simple example isgiven in Figure 2.The complete list of types of questions relatedto membership analysis and which digress fromthe task instructions in our corpus is given in Ta-ble 2.
Each of these instances of membership anal-ysis constitutes a clarification question that wouldinitiate an off-topic subdialogue if the robot hadreacted to it.4 Consequences for system designSo far our empirical studies have shown that thereare particular kinds of interactional problems thatwill regularly trigger user-initiated clarificationsubdialogues.
These might appear off-topic orout of place but when understood in terms ofthe membership and topic/activity analysis, it be-comes clear that all such contributions are, in avery strong sense, ?predictable?.
These results can,and arguably should,2 be exploited in the follow-ing ways.
One is to extend dialogue system de-sign to be able to meet these contingently rele-2Doran et al (2001) demonstrate a negative relationshipbetween number of initiative attempts and their success rate.vant contributions whenever they occur.
That is,we adapt dialogue manager, lexical database etc.so that precisely these apparently out-of-domaintopics are covered.
A second strategy is to de-termine discourse conditions that can be used toalert the dialogue system to the likely occurrenceor absence of these kinds of clarificatory subdia-logues (see below).
Third, we can design explicitstrategies for interaction that will reduce the like-lihood that a user will employ them: for example,by providing information about the agent?s capa-bilities, etc.
as listed in Table 2 in advance bymeans of system-initiated assertions.
That is, wecan guide, or shape, to use the terminology intro-duced by Zoltan-Ford (1991), the users?
linguisticbehaviour.
A combination of these three capabil-ities promises to improve the overall quality of adialogue system and forms the basis for a signifi-cant part of our current research.We have already ascertained empirically dis-course conditions that support the second strat-egy above, and these follow again directly fromthe basic notions of recipient design and mem-bership analysis.
If a user already has a strongmembership analysis in place?for example, dueto preconceptions concerning the abilities (or,more commonly, lack of abilities) of the artifi-cial agent?then this influences the design of thatuser?s utterances throughout the dialogue.
As aconsequence, we have been able to define distinc-tive linguistic profiles that lead to the identifica-tion of distinct user groups that differ reliably intheir dialogue strategies, particularly in their ini-tiation of subdialogues.
In the human-robot dia-logues just considered, for example, we found thateight out of 21 users did not employ any clarifica-tion questions at all and an additional four usersasked only a single clarification question.
Provid-ing these users with additional information aboutthe robot?s capabilities is of limited utility becausethese users found ways to deal with the situationwithout asking clarification questions.
The sec-ond group of participants consisted of nine users;this group used many questions that would haveled into potentially problematic clarification dia-logues if the system had been real.
For these users,the presentation of additional information on therobot?s capabilities would be very useful.It proved possible to distinguish the membersof these two groups reliably simply by attend-ing to their initial dialogue contributions.
This is189domain example (translation)perception VP7-3 [do you see the cups?
]readiness VP4-25 [Are you ready for another task?
]functional capabilities VP19-11 [what can you do?
]linguistic capabilities VP18-7 [Or do you only know mugs?
]cognitive capabilities VP20-15 [do you know where is left and right of you?
]Table 2: Membership analysis related clarification questionsuse of task-oriented greetingsclarification beginningsnone 58.3 11.1single 25.0 11.1frequent 16.7 77.8N = 21; average number of clarification questionsfor task-oriented group: 1.17 clarification ques-tions per dialogue; average number for ?greeting?-group 3.2; significance by t-test p<0.01Table 3: Percentage of speakers using no, a sin-gle, or frequent clarification questions dependingon first utterancewhere their pre-interaction membership analysiswas most clearly expressed.
In the human-robotdialogues investigated, there is no initial utterancefrom the robot, the user has to initiate the inter-action.
Two principally different types of first ut-terance were apparent: whereas one group of usersbegins the interaction with task-instructions, a sec-ond group begins the dialogue by means of a greet-ing, an appeal for help, or a question with regardto the capabilities of the system.
These two dif-ferent ways of approaching the system had sys-tematic consequences for the dialogue structure.The dependent variable investigated is the num-ber of utterances that initiate clarification subdia-logues.
The results of the analysis show that thosewho greet the robot or interact with it other thanby issuing commands initiate clarificatory subdi-alogues significantly more often than those whostart with an instruction (cf.
Table 3).
Thus,user modelling on the basis of the first utterancein these dialogues can be used to predict muchof users?
linguistic behaviour with respect to theinitiation of clarification dialogues.
Note that forthis type of user modelling no previous informa-tion about the user is necessary and group assign-ment can be carried out unobtrusively by means ofsimple key word spotting on the first utterance.Whereas the avoidance of clarificatory user-initiated subdialogues is clearly a benefit, we canalso use the results of our empirical investigationsto motivate improvements in the other areas of in-teractive work undertaken by speakers.
In particu-lar topic and activity analysis can become prob-lematic when the decompositions adopted by auser are either insufficient to structure dialogue ap-propriately for interpretation or, worse, are incom-patible with the domain models maintained by theartificial agent.
In the latter case, communicationwill either fail or invoke rechecking of member-ship categories to find a basis for understanding(e.g., ?do you know what cups are??).
Thus, whatcan be seen on the part of a user as reducing thecomplexity of a task can in fact be removing in-formation vital for the artificial agent to effect suc-cessful interpretation.The results of a user?s topic and activity analy-sis make themselves felt in the divergent dialoguestructures observed.
As shown above in Figure 1,the structure of the dialogues is thus much flatterthan the one found in the corresponding HHC dia-logues, such that goal description and marking ofsubtasks is missing, and the only structure resultsfrom the division into selection and combinationof parts.
In our second study, precisely the sameeffects are observed.
The task of measuring dis-tances between objects is often decomposed into?simpler?
subtasks; for example, the complexity ofthe task is reduced by achieving reference to eachof the objects first before the robot is requested tomeasure the distance between them.This potential mismatch between user and sys-tem can also be identified on the basis of the inter-action.
Proceeding directly to issuing low-level in-structions rather than providing background gen-eral goal information is a clear linguisticallyrecognisable cue that a nonaligned topic/activityanalysis has been adopted.
A successful dialoguesystem can therefore rely on this dialogue tran-sition as providing an indication of problems tocome, which can again be avoided in advance byexplicit system-initiated assertions of information.190Our main focus in this paper has been on settingout and motivating some generic principles for di-alogue system design.
These principles could finddiverse computational instantiations and it has notbeen our aim to argue for any one instantationrather than another.
However, to conclude, wesummarise briefly the approach that we are adopt-ing to incorporating these mechanisms within ourown dialogue system (Ross et al, 2005).Our system augments an information-statebased approach with a distinguished vocabularyof discourse transitions between states.
We attach?conceptualisation-conditions?
to these transitionswhich serve to post discourse goals whose partic-ular function is to head off user-initiated clarifi-cation.
The presence of a greeting is one suchcondition; the immediate transition to basic-levelinstructions is another.
Recognition and produc-tion of instructions is aided by treating the seman-tic types that occur (?cups?, ?measure?, ?move?,etc.)
as elements of a domain ontology.
The di-verse topic/activity analyses then correspond tothe specification of the granularity and decom-position of activated domain ontologies.
Sim-ilarly, location analyses correspond to commonsense geographies, which we model in terms simi-lar to those of ontologies now being developed forGeographic Information Systems (Fonseca et al,2002).The specification of conceptualisation-conditions triggered by discourse transitionsand classifications of the topic/activity analysisgiven by the semantic types provided in user ut-terances represents a direct transfer of the implicitstrategies found in conversation analyses to thedesign of our dialogue system.
For example, inour case many simple clarifications like ?do yousee the cups?,?
?how many cups do you see??
aswell as ?what can you do??
are prevented by pro-viding information in advance on what the robotcan perceive to those users that use greetings.Similarly, during a scene description where thesystem has the initiative, the opportunity is takento introduce terms for the objects it perceives aswell as appropriate ways of describing the scene,e.g., by means of ?There are two groups of cups.What do you want me to do??
a range of otherwisenecessary clarificatory questions is avoided.
Evenin the case of failure, users will not doubt thosecapabilities of the system that it has displayed it-self, due to alignment processes also observable inhuman-to-human dialogical interaction (Pickeringand Garrod, 2004).
After a successful interaction,users expect the system to be able to processparallel instructions because they reliably expectthe system to behave consistently (Fischer andBatliner, 2000).5 ConclusionsIn this paper, the discourse structure initiated byusers in HCI situations has been investigated andthe results have been three-fold.
The structuresinitiated in HCI are much flatter than in HHC; nogeneral orientation with respect to the aims of asub-task are presented to the artificial communica-tion partner, and marking is usually reduced.
Thisneeds to be accounted for in the mapping of thetask-structure onto the discourse model, irrespec-tive of the kind of representation chosen.
Sec-ondly, the contents of clarification subdialogueshave also been identified as particularly depen-dent on recipient design.
That is, they concernthe preconditions for formulating utterances par-ticularly for the respective hearer.
Here, the lessthat is known about the communication partner,the more needs to be elicited in clarification dia-logues: however, crucially, we can now state pre-cisely which kinds of elicitations will be found(cf.
Table 2).
Thirdly, users have been shown todiffer in the strategies that they take to solve theuncertainty about the speech situation and we canpredict which strategies they in fact will follow intheir employment of clarification dialogues on thebasis of their initial interaction with the system (cf.Table 3).Since the likelihood for users to initiate suchclarificatory subdialogues has been found to bepredictable, we have a basis for a range of implicitstrategies for addressing the users?
subsequent lin-guistic behaviour.
Recipient design has thereforebeen shown to be a powerful mechanism that, withthe appropriate methods, can be incorporated inuser-adapted dialogue management design.Information of the kind that we have uncoveredempirically in the work reported in this paper canbe used to react appropriately to the different typesof users in two ways: either one can adapt thesystem or one can try to adapt the user (Ogdenand Bernick, 1996).
Although techniques for bothstrategies are supported by our results, in generalwe favour attempting to influence the user?s be-haviour without restricting it a priori by means191of computer-initiated dialogue structure.
Since thereasons for the users?
behaviour have been shownto be located on the level of their conceptualisationof the communication partner, explicit instructionmay in any case not be useful?explicit guidanceof users is not only often impractical but also isnot received well by users.
The preferred choice isthen to influence the users?
concepts of their com-munication partner and thus their linguistic be-haviour by shaping (Zoltan-Ford, 1991).
In par-ticular, Schegloff?s analysis shows in detail thehuman interlocutors?
preference for those locationterms that express group membership.
Therefore,in natural dialogues the speakers constantly signalto each other who they are, what the other per-son can expect them to know.
Effective systemdesign should therefore provide users with pre-cisely those kinds of information that constitutetheir most frequent clarification questions initiallyand in the manner that we have discussed.AcknowledgementThe authors gratefully acknowledge the support ofthe Deutsche Forschungsgemeinschaft (DFG) forthe work reported in this paper.ReferencesChristine Doran, John Aberdeen, Laurie Damianos andLynette Hirschman.
2001.
Comparing Several As-pects of Human-Computer and Human-Huamn Di-alogues.
Proceedings of the 2nd SIGdial Workshopon Discourse and Dialogue, Aalborg, Denmark.James Allen.
1999.
Mixed-initiative interaction.
IEEEIntelligent Systems, Sept./Oct.:14?16.R.
Amalberti, N. Carbonell, and P. Falzon.
1993.User representations of computer systems in human-computer speech interaction.
International Journalof Man-Machine Studies, 38:547?566.Nicholas Asher and Alex Lascarides.
2003.
Logicsof conversation.
Cambridge University Press, Cam-bridge.Kerstin Fischer and Anton Batliner.
2000.
Whatmakes speakers angry in human-computer conver-sation.
In Proceedings of the Third Workshop onHuman-Computer Conversation, Bellagio, Italy.Kerstin Fischer.
2003.
Linguistic methods for in-vestigating concepts in use.
In Thomas Stolz andKatja Kolbe, editors, Methodologie in der Linguis-tik.
Frankfurt a.M.: Peter Lang.Frederico T. Fonseca, Max J. Egenhofer, PeggyAgouris, and Gilberto Ca?mara.
2002.
Using ontolo-gies for integrated geographic information systems.Transactions in GIS, 6(3).Barbara J. Grosz.
1982.
Discourse analysis.
InRichard Kittredge and John Lehrberger, editors,Sublanguage.
Studies of Language in Restricted Se-mantic Domains, pages 138?174.
Berlin, New York:De Gruyter.L.
Hitzenberger and C. Womser-Hacker.
1995.Experimentelle Untersuchungen zu multimodalennatu?rlichsprachigen Dialogen in der Mensch-Computer-Interaktion.
Sprache und Datenverar-beitung, 19(1):51?61.Eric Horvitz.
1999.
Uncertainty, action, and interac-tion: In pursuit of mixed-initiative computing.
IEEEIntelligent Systems, Sept./Oct.
:17?20.Daniel Marcu.
2000.
The rhetorical parsing of unre-stricted texts: a surface-based approach.
Computa-tional Linguistics, 26(3):395?448, Sep.W.C.
Ogden and P. Bernick.
1996.
Using natural lan-guage interfaces.
In M. Helander, editor, Handbookof Human-Computer Interaction.
Elsevier SciencePublishers, North Holland.Martin J. Pickering and Simon Garrod.
2004.
Towardsa mechanistic psychology of dialogue.
Behaviouraland Brain Sciences, 27(2):169?190.R.J.
Ross, J. Bateman, and H. Shi.
2005.
ApplyingGeneric Dialogue Models to the Information StateApproach.
In Proceedings of Symposium on Dia-logue Modelling and Generation.
Amsterdam.H.
Sacks, E. Schegloff, and G. Jefferson.
1974.
A sim-plest systematics for the organisation of turn-takingfor conversation.
Language, 50:696?735.Gerhard Sagerer, Hans-Ju?rgen Eikmeyer, and GertRickheit.
1994.
?Wir bauen jetzt ein Flugzeug?
:Konstruieren im Dialog.
Arbeitsmaterialien.
ReportSFB 360, University of Bielefeld.E.
A. Schegloff.
1972.
Notes on a conversational prac-tice: formulating place.
In D. Sudnow, editor, Stud-ies in Social Interaction, pages 75?119.
The FreePress, New York.Marc Swerts.
1998.
Filled pauses as markers of dis-course structure.
Journal of Pragmatics, 30:485?496.Bonnie Webber, Alistair Knott, Matthew Stone, andAravind Joshi.
1999.
Discourse relations: a struc-tural and presuppositional account using lexicalizedTAG.
In Proceedings of the 37th.
Annual Meetingof the American Association for Computational Lin-guistics (ACL?99), pages 41?48, University of Mary-land.
American Association for Computational Lin-guistics.Elizabeth Zoltan-Ford.
1991.
How to get people tosay and type what computers can understand.
Inter-national journal of Man-Machine Studies, 34:527?647.192
