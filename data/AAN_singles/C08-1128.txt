Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017?1024Manchester, August 2008Bayesian Semi-Supervised Chinese Word Segmentationfor Statistical Machine TranslationJia Xu?, Jianfeng Gao?, Kristina Toutanova?, Hermann Ney?Computer Science 6?Microsoft Corporation?RWTH Aachen University One Microsoft WayD-52056 Aachen, Germany Redmond, WA 98052, USA{xujia,ney}@cs.rwth-aachen.de {jfgao,kristout}@microsoft.comAbstractWords in Chinese text are not naturallyseparated by delimiters, which poses achallenge to standard machine translation(MT) systems.
In MT, the widely usedapproach is to apply a Chinese word seg-menter trained from manually annotateddata, using a fixed lexicon.
Such wordsegmentation is not necessarily optimalfor translation.
We propose a Bayesiansemi-supervised Chinese word segmenta-tion model which uses both monolingualand bilingual information to derive a seg-mentation suitable for MT.
Experimentsshow that our method improves a state-of-the-art MT system in a small and a largedata environment.1 IntroductionChinese sentences are written in the form of a se-quence of Chinese characters, and words are notseparated by white spaces.
This is different frommost European languages and poses difficulty inmany natural language processing tasks, such asmachine translation.It is difficult to define ?correct?
Chinese wordsegmentation (CWS) and various definitions havebeen proposed.
In this work, we explore the ideathat the best segmentation depends on the task, andconcentrate on developing a CWS method for MT,which leads to better translation performance.The common solution in Chinese-to-Englishtranslation has been to segment the Chinese textusing an off-the-shelf CWS method, and to applya standard translation model given the fixed seg-mentation.
The most widely applied method forMT is unigram segmentation, such as segmenta-tion using the LDC (LDC, 2003) tool, which re-quires a manual lexicon containing a list of Chi-nese words and their frequencies.
The lexicon andc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.frequencies are obtained using manually annotateddata.
This method is sub-optimal for MT.
For ex-ample, ?
(paper) and ](card) can be two wordsor composed into one word ?](cards).
Since ?
]does not exist in the manual lexicon, it cannotbe generated by this method.In addition to unigram segmentation, othermethods have been proposed.
For example, (Gaoet al, 2005) described an adaptive CWS system,and (Andrew, 2006) employed a conditional ran-dom field model for sequence segmentation.
How-ever, these methods are not specifically devel-oped for the MT application, and significant im-provements in translation performance need to beshown.In (Xu et al, 2004) and (Xu et al, 2005),word segmentations are integrated into MT sys-tems during model training and translation.
We re-fine the method in training using a Bayesian semi-supervised CWS approach motivated by (Goldwa-ter et al, 2006).
We describe a generative modelwhich consists of a word model and two alignmentmodels, representing the monolingual and bilin-gual information, respectively.
In our methods, wefirst segment Chinese text using a unigram seg-menter, and then learn new word types and worddistributions, which are suitable for MT.Our experiments on both large (NIST) and small(IWSLT) data tracks of Chinese-to-English trans-lation show that our method improves the per-formance of a state-of-the-art machine translationsystem.2 Review of the Baseline System2.1 Word segmentationIn statistical machine translation, we are given aChinese sentence in characters cK1= c1.
.
.
cKwhich is to be translated into an English sentenceeI1= e1.
.
.
eI.
In order to obtain a more adequatemapping between Chinese and English words, cK1is usually segmented into words fJ1= f1.
.
.
fJinpreprocessing.In our baseline system, we apply the commonly1017used unigram model to generate the segmenta-tion.
Given a manually compiled lexicon contain-ing words and their relative frequencies Ps(f?j),the best segmentation fJ1is the one that maximizesthe joint probability of all words in the sentence,with the assumption that words are independent ofeach other1:fJ1= argmaxf?J?1Pr(f?J?1|cK1)?
argmaxf?J?1J?
?j=1Ps(f?j),where the maximization is taken over Chineseword sequences whose character sequence is cK1.2.2 Translation systemOnce we have segmented the Chinese sentencesinto words, we train standard alignment modelsin both directions with GIZA++ (Och and Ney,2002) using models of IBM-1 (Brown et al, 1993),HMM (Vogel et al, 1996) and IBM-4 (Brown etal., 1993).Our MT system uses a phrase-based decoderand the log-linear model described in (Zens andNey, 2004).
Features in the log-linear model in-clude translation models in two directions, a lan-guage model, a distortion model and a sentencelength penalty.
The feature weights are tuned onthe development set using a downhill simplex al-gorithm (Press et al, 2002).
The language modelis a statistical ngram model estimated using modi-fied Kneser-Ney smoothing.3 Unigram Dirichlet Process Model forCWSThe simplest version of our model is based on aunigram Dirichlet Process (DP) model, using onlymonolingual information.
Different from a stan-dard unigram model for CWS, our model can in-troduce new Chinese word types and learn worddistributions automatically from unlabeled data.According to this model, a corpus of Chinesewords f1, .
.
.
fm, .
.
.
, fMis generated via:G|?, P0?
DP (?, P0)fm|G ?
Gwhere G is a distribution over words drawn from aDirichlet Process prior with base measure P0andconcentration parameter ?.We never explicitly estimate G but insteadintegrate over its possible values and performBayesian inference.
It is easy to compute the1The notational convention will be as follows: we use thesymbol Pr(?)
to denote general probability distributions with(nearly) no specific assumptions.
In contrast, for model-basedprobability distributions, we use the generic symbol P (?
).probability of a Chinese word given a set of al-ready generated words, while integrating over G.This is done by casting Chinese word generationas a Chinese restaurant process (CRP) (Aldous,1985), i.e.
a restaurant with an infinite num-ber of tables (approximately corresponding to Chi-nese word types), each table with infinite numberof seats (approximately corresponding to Chineseword frequencies).The Dirichlet Process model can be viewed in-tuitively as a cache model (Goldwater et al, 2006).Each word fjin the corpus is either retrieved froma cache or generated anew given the previously ob-served words f?j:P (fj|f?j) =N(fj)+?P0(fj)N + ?, (1)whereN(fj) is the number of Chinese words fjinthe previous context.
N is the total number of Chi-nese words, P0is the base probability over words,and ?
influences the probability of introducing anew word at each step and controls the size of thelexicon.
The probability of generating a word fromthe cache increases as more instances of that wordare seen.For the base distribution P0, which governs thegeneration of new words, we use the following dis-tribution (called the spelling model):P0(f) = P (L)P0(f |L)=?LL!e?
?uL(2)where1uis the number of characters in the docu-ment, i.e.
character vocabulary size, and L is thenumber of Chinese characters of word f .
We notethat this is a Poisson distribution on word lengthand a unigram distribution on characters given thelength.
We used ?
= 2 and ?
= 0.3 in our experi-ments.4 CWS Model for MTAs a solution to the problems with the conventionalapproach to CWS mentioned in Section 1, we pro-pose a generative model for CWS in Section 4.1,and then extend the model to a more general butdeficient model, similar to a maximum entropymodel in which most features are derived from thesubmodels of the generative model.4.1 Generative ModelThe generative model assume that a corpus of par-allel sentences (c1K,e1I) is generated along with ahidden sequence of Chinese words f1Jand a hid-den word alignment b1Ifor every sentence.
Thealignment indicates the aligned Chinese word fbifor each English word ei, where f0indicates a spe-cial null word as in the IBM models.1018Without assuming any special form for the prob-ability of a sentence pair along with hidden vari-ables, we can factor it into a monolingual Chi-nese sentence probability and a bilingual transla-tion probability as follows:Pr(c1K, e1I, f1J, b1I)=Pr(cK1, fJ1)Pr(eI1, bI1|fJ1)=Pr(fJ1)?
(f1J, c1K)Pr(eI1, bI1|fJ1),where ?
(fJ1, cK1) is 1 if the characters of the se-quence of words f1Jare c1K, and to 0 other-wise.
We can drop the conditioning on c1KinPr(eI1, bI1|fJ1), because the characters are deter-ministic given the words.The joint probability of the observations(c1K, eI1) can be obtained by summing over allpossible values of the hidden variables fJ1and bI1.In Sections 4.1.1 and 4.1.2, we will describethe modeling assumptions behind the monolingualChinese sentence model and the translation model,respectively.4.1.1 Monolingual Chinese sentence modelWe use the Dirichlet Process unigram wordmodel introduced in section 3.
In this model, theparameters of a distribution over words G are firstdrawn from the Dirichlet prior DP (?, P0).
Wordsare then independently generated according to G.The probability of a sequence of Chinese words ina sentence is thus:Pr(fJ1) ?J?j=1P (fj|G) (3)4.1.2 Translation modelWe employ the Dirichlet Process inverse IBMmodel 1 to generate English words and alignmentgiven the Chinese words.
In this model, for everyChinese word f (including the null word), a distri-bution over English words Gfis first drawn froma Dirichlet Process prior DP (?, P0(e)), whereP0(e) we used the empirical distribution over En-glish words in the parallel data.
Then, given theseparameters, the probability of an English sentenceand alignment given a Chinese sentence (sequenceof words) is given by:P (eI1, bI1|fJ1, Gf) =I?i=11J + 1P (ei|Gfbi)This is the same model form as inverse IBMmodel 1, except we have placed Dirichlet Processpriors on the Chinese-word specific distributionsover English words.22fbiis the Chinese word aligned to eiand Gfbiis thedistribution over English words conditioned on the word fbi.Similarly, eajis the English word aligned to fjin the other di-rection and Geajis the distribution over Chinese words con-ditioned on eaj.In practice, we observed that using a word-alignment model in one direction is not sufficient.We then added a factor to our model which in-cludes word alignment in the other direction , i.e.
aDirichlet Process IBM model 1.
We ignore the de-tailed description here, because the calculation isthe same as that of the inverse IBM model 1.
Ac-cording to this model, for every English word e (in-cluding the null word), a distribution over Chinesewords Geis first drawn from a Dirichlet Processprior DP (?, P0(f)).
Here, for the base distribu-tion P0(f) we used the same spelling model as forthe monolingual unigram Dirichlet Process prior.The probability of a sequence of Chinese wordsfJ1and a word alignment aJ1given a sequence ofEnglish words eI1is then:P (fJ1, aJ1|eI1, Ge) =J?j=11I + 1P (fj|Geaj)4.2 Final ModelWe put the monolingual model and the transla-tion models in both directions together into a sin-gle model, where each of the component modelsis weighted by a scaling factor.
This is similar toa maximum entropy model.
We fit the weights ofthe sub-models on a development set by maximiz-ing the BLEU score of the final translation.P (cK1, eI1, fJ1, aJ1, bI1) (4)?1ZP (fJ1)?1?
P (eI1, bI1|fJ1)?2?P (fJ1, aJ1|eI1)?3,where Z is the normalization factor.In practice we do not re-normalize the proba-bilities and our model is thus deficient because itdoes not sum to 1 over valid observations.
How-ever, we found the model work very good in ourexperiments.
Similar deficient models have beenused very successfully before, for example, in theIBM models 3?6 and in the unsupervised grammarinduction model of (Klein and Manning, 2002).5 Gibbs Sampling TrainingIt is generally impossible to find the most likelysegmentation according to our Bayesian model us-ing exact inference, because the hidden variablesdo not allow exact computation of the integrals.Nonetheless, it is possible to define algorithms us-ing Markov chain Monte Carlo (MCMC) that pro-duce a stream of samples from the posterior dis-tribution of the hidden variables given the obser-vations.
We applied the Gibbs sampler (Gemanand Geman, 1984) ?
one of the simplest MCMCmethods, in which transitions between states of the1019Figure 1: Case I, transition from a no-boundary toa boundary state, f to f?f?
?.Figure 2: Case II, transition from a boundary to ano-boundary state, f?f?
?to f .Markov chain result from sampling each compo-nent of the state conditioned on the current valueof all other variables.In our problem, the observations are D =(d1, ..dn, .., dN), where dn=(cK1, eI1) indicates abilingual sentence pair, the hidden variables are theword segmentations fJ1and the alignments in twodirections aJ1and bI1.To perform Gibbs sampling, we start withan initial word segmentation and initial wordalignments, and iteratively re-sample the word-segmentation and alignments according to ourmodel of Equation 4.Note that for efficiency, we only allow limitedmodifications to the initial word alignments.
Thuswe only use models derived from IBM-1 (insteadof IBM-4) for comparing different word segmenta-tions.
On the other hand, re-sampling the segmen-tation causes re-linking alignment points to partsor groups of the original words.Hence, we organize our sampling processaround possible word boundaries.
For each char-acter ckin each sentence, we consider two alterna-tive segmentations: ck+indicates the segmentationwhere there is a boundary after ckand ck?indi-cates the segmentation where there is no boundaryafter ck, keeping all other boundaries fixed.
Let fdenote the single word spanning character ckwhenthere is no boundary after it, and f?,f?
?denote thetwo adjacent words resulting if there is a bound-ary: f?includes ckand f?
?starts just to the right,with character ck+1.
The introduction of f?andf?
?leads to M new possible alignments in the E-to-C direction b+k1, .
.
.
, b+kM, such as in Figure 1.Together with the boundary vs no-boundary stateat each character position, we re-sample a set ofalignment links between English words and any ofthe Chinese words f ,f?, and f?
?, keeping all otherword alignments in the sentence pair fixed.
(SeeFigures 1 and 2.
)Table 1: General Algorithm of GS for CWS.Input: D with an initial segmentation and alignmentsOutput: D with sampled segmentation and alignmentsfor n = 1 to?Nfor k = 1 to K that ck?
dnCreate M+1 candidates, cba+k,mand cba?k, wherecba+k,m: there is a word boundary after ckcba?k: there is no word boundary after ckCompute probabilitiesP (cba+k,m|dhnk?
)P (cba?k|dhnk?
)Sample boundary and relevant alignmentsUpdate countsThus at each step in the Gibbs sampler, we con-sider a set of alternatives for the boundary afterckand relevant alignment links, keeping all otherhidden variables fixed.
At each step, we need tocompute the probability of each of the alternatives,given the fixed values of the other hidden variables.We introduce some notation to make the presen-tation easier.
For every position k in sentence pairn, we denote by dhnk?the observations and hid-den variables for all sentences other than sentencen, and the observations and hidden variables in-side sentence n, not involving character positionck.
The fixed variables inside the sentence are thewords not neighboring position k, and the align-ments in both directions to these words.In the process of sampling, we consider a setof alternatives: segmentation ck+along with theproduct space of relevant alignments in both direc-tions b+k1, .
.
.
, b+kM, and a+k, and segmentation c?kalong with relevant alignments bk?and a?k.
Forbrevity, we denote these alternatives by cbak,m+and cbak?.We describe how we derive the set of alterna-tives in section 5.2 and how we compute theirprobabilities in section 5.1.Table 1 shows schematically one iteration ofGibbs sampling through the whole training corpusof parallel sentences, where?N is the number ofparallel sentences.5.1 Computing probabilities of alternativesFor the Gibbs sampling algorithm in Table 1, weneed to compute the probability of each alternativesegmentation/alignments, given the fixed values ofthe rest of the data dhnk?.
The probability of thehidden variables in the alternatives is proportionalto the joint probability of the hidden variables andobservations, and thus it is sufficient to computethe probability of the latter.
We compute theseprobabilities using the Chinese restaurant processsampling scheme for the Dirichlet Process, thus in-1020tegrating over all of the possible values of the dis-tributions G, Gfand Ge.Let cbakdenote an alternative hypothesis in-cluding boundary or no boundary at position k,and relevant alignments to English words in bothdirections of the one or two Chinese words result-ing from the segmentation at k. The probability ofthis configuration given by our model is:P (cbak|dhnk?)
?
Pm(cbak|dhnk?)?1(5)?Pef(cbak|dhnk?)?2?
Pfe(cbak|dhnk?
)?3,where Pm(cbak|dhnk?)
is the monolingualword probability, and Pfe(cbak|dhnk?)
andPef(cbak|dhnk?)
are the translation probabilitiesin the two directions.We now describe the computations of each ofthe component probabilities.5.1.1 Word model probabilityThe word model probability Pm(cabk|dhnk?
)in Equation 5 is derived from Equations 3 and 1:There are two cases, depending on whether thehypothesis specifies that there is a boundary aftercharacter ck, in which case we need the probabili-ties of the two resulting words f?, and f?
?, or thereis no boundary, in which case we need the proba-bility of the single word f .
(See the initial states inFigures 1 and 2, respectively.
)Let N denote the total number of word tokensin the rest of the corpus dhnk?, and N(f) denotethe number of instances of word f in dhnk?.
Theprobabilities in the two cases arePm(c+k|dhnk?)
?N(f?)
+ ?P0(f?
)N + ??N(f??)
+ ?P0(f??
)N + ?Pm(c?k|dhnk?)
?N(f) + ?P0(f)N + ?Here P0(f) is computed using Equation 2.5.1.2 Translation model probabilityThe translation model probabilities depend onwhether or not there is a segmentation boundaryat ckand which English words are aligned to therelevant Chinese words.In the first case, assume that there is a wordboundary in cabk, and that English words {e?}
arealigned to f?and words {e??}
are aligned to f?
?inthe E-to-C direction according to the alignment bk,and that f?is aligned to e?
?and f?
?is aligned to e??
?in the C-to-E direction according to the alignmentak(see the initial state in Figure 1).
Here we over-loaded notations and use bkand akto indicate thealignments of the relevant Chinese words at posi-tion k to any English words.
Let I denote the totalnumber of English words in the sentence, and J+1denote the number of Chinese words according tothis segmentation.
We also denote the total num-ber of English words aligned to either f?or f?
?inthe E-to-C direction by P .The translation model probability in the E-to-Cdirection is thus:Pef(c+k, bk, ak|dhnk?)
?1(J + 2)P?e?P (e?|f?, dhnk?)?e?
?P (e??|f?
?, dhnk?
)Here we compute P (e|f, dhnk?)
as:P (e|f, dhnk?)
=N(e, f) + ?P0(e)N(f) + ?,where the counts are computed over the fixed as-signments dhnk?.The translation probability in the other directionis similarly computed as:Pfe(c+k, bk, ak|dhnk?)
?
(1I + 1)2P (f?|e?, dhnk?
)P (f?
?|e?, dhnk?
)And P (f |e, dhnk?)
is computed as:P (f |e, dhnk?)
=N(f, e) + ?P0(f)N(e) + ?,where the counts are computed over the fixed as-signments dhnk?.In the second case, if the hypothesis in evalua-tion does not have a word boundary at position k,the total number of Chinese words would be oneless, i.e.
J instead of J +1 in the equations above,and there would be a single set of English wordsaligned to the word f in the E-to-C direction, and asingle word e?aligned to f in the C-to-E direction(see the initial state in Figure 2.
The probability ofthis hypothesis is computed analogously.5.2 Determining the set of alternativehypothesesAs mentioned earlier, we consider alternativealignments which deviate minimally from the cur-rent alignments, and which satisfy the constraintsof the IBM model 1 in both directions.
In orderto describe the set of alternatives, we consider twocases, depending on whether there is a boundary atthe current character before sampling at position k.Case 1.
There was no boundary at ckin the previ-ous state (see Figure 1).1021If there is no boundary at ck, there is a sin-gle word f spanning that position.
We denote by{e} the set of English words aligned to f at thatstate in the E-to-C direction and by e?the En-glish word aligned to f in the C-to-E direction.Since every state we consider satisfies the IBMone-to-many constraints, there is exactly one En-glish word aligned to f in the C-to-E direction andthe words {e} have no other words aligned to themin the E-to-C direction.In this case, we consider as hypothesis cbak?the same segmentation and alignment as in the pre-vious state.
(see Table 1 for an overview of thealternative hypotheses.
)We consider M different hypotheses which in-clude a boundary at k in this case, where M de-pends on the number of words {e} aligned to fin the previous state.
Because we are breakingthe word f into two words f?and f?
?by placinga boundary at ck, we need to re-align the words{e} to either f?or f??.
Additionally we need toalign f?and f?
?to English words in the C-to-Edirection.
The number of different hypotheses isequal to 2Pwhere P = |{e}|.
These alternativesarise by considering that each of the words in {e}needs to align to either f?or f?
?, and there are 2Pcombinations of these alignments.
For example, if{e} = {e1, e2}, after splitting the word f there arefour possible alignments, illustrated in Figure 1:I.
(f?, e1) and (f?
?, e2), II.
(f?, e2) and (f?
?, e1),III.
(f?, e1) and (f?, e2), IV.
(f?
?, e1) and (f?
?, e2).For the alignment akin the C-to-E direction, weconsider only one option, in which both resultingwords f?and f?
?align to e?.
These alternativesform cbak,m+in Table 1.Case 2.
There was a boundary at ckin the previousstate (see Figure 2).In this case, for the hypotheses c+kwe consideronly one alternative, which is exactly the same asthe assignment of segmentation and alignments inthe previous state.
Thus we have M = 1 in Table1.Let f?and f?
?denote the two words at positionk in the previous state, {e?}
and {e??}
denote thesets of English words aligned to them in the E-to-Cdirection, respectively, and e?
?and e??
?denote theEnglish words aligned to f?and f?
?in the C-to-Edirection.We consider only one hypothesis cbak?wherethere is no boundary at ck.
In this hypothesis, thereis a single word f = f?f?
?spanning position k,and all words {e?}
?
{e??}
align to f in the E-to-C direction.
For the C-to-E direction we considerthe ?better?
of the alignments (f, e??)
and (f, e???
)where the better alignment is defined as the onehaving higher probability according to the C-to-Eword translation probabilities.Table 2: Complete Algorithm of Gibbs Samplerfor CWS including Alignment Models.Input: D, F0Output: AT, FTfor t = 1 to TRun GIZA++ on (D,Ft?1) to obtain AtRun GS on (D,Ft?1, At) to obtain Ft5.3 Complete segmentation algorithmSo far, we have described how we re-sample wordsegmentation and alignments according to ourmodel, starting from an initial segmentation andalignments from GIZA++.
Putting these pieces to-gether, the algorithm is summarized in Table 1.We found that we can further improve perfor-mance by repeatedly aligning the corpus usingGIZA++, after deriving a new segmentation us-ing our model.
The complete algorithm which in-cludes this step is shown in Table 2, where Ftin-dicates the word segmentation at iteration t and Atdenotes the GIZA++ corpus alignment in both di-rections.
The GS re-segmentation step is done ac-cording to the algorithm in Table 1.Using this algorithm, we obtain a new segmen-tation of the Chinese data and train the translationmodels using this segmentation as in the baselineMT system.
To segment the test data for transla-tion, we use a unigram model, trained with maxi-mum likelihood estimation off of the final segmen-tation of the training corpus FT.6 Translation ExperimentsWe performed experiments using our models forCWS on a large and a small data track.
We evalu-ated performance by measuring WER (word errorrate), PER (position-independent word error rate),BLEU (Papineni et al, 2002) and TER (translationerror rate) (Snover et al, 2006) using multiple ref-erences.6.1 Translation Task: Large Track NISTWe first report the experiments using our mono-lingual unigram Dirichlet Process model for wordsegmentation on the NIST machine translation task(NIST, 2005).
Because of the computational re-quirements, we only employed the monolingualword model for this large data track, i.e.
the fea-ture weights were ?1= 1, ?2= 0, ?3= 0.
There-fore, no alignment information needs to be main-tained in this case.The bilingual training corpus is a superset ofcorpora in the news domain collected from differ-ent sources.We took LDC (LDC, 2003) as a baseline CWSmethod (Base).
As shown in Table 3, the trainingcorpus in each language contains more than twomillion sentences.
There are 56 million Chinese1022Table 3: Statistics of corpora in task NIST.Data Sents.
Words[K] Voc.[K]Cn.
En.
Cn.
En.Chars 2M 56M 49.5M 65.4 211Base 39.2M 95.7GS 40.5M 95.402 878 23.1 28.0 2.04 4.3403 919 24.6 29.2 2.21 4.9104 1788 49.8 60.7 2.61 6.7105 1082 30.8 34.2 2.30 5.39Table 4: Translation performance [% BLEU] withthe baseline(LDC) and GS method on NIST.MT-eval LDC(Base) GS2005 32.85 33.262002 34.32 34.362003 33.41 33.752004 33.74 34.06characters.
The LDC and GS word segmentationmethods generated 39.2 and 40.5 million runningwords, respectively.The scaling factors of the translation models de-scribed in Section 2.2 were optimized on the devel-opment corpus, MT-eval 05 with 1082 sentences.The resulting systems were evaluated on the testcorpora MT-eval 02-04.
For convenience, we onlylist the statistics of the first English reference.Starting from the baseline LDC output as ini-tial word segmentation, we performed Gibbs sam-pling (GS) of word segmentations using 30 itera-tions over the Chinese training corpus.Since BLEU is the official NIST measure oftranslation performance, we show the translationresults measured in BLEU score only.
As shownin Table 4, on the development data MT-eval 05,the BLEU score was improved by 0.4% absolute ormore than 1% relative using GS.
Similarly, the ab-solute BLEU scores are also improved on all othertest sets, in the range of 0.04% to 0.4%.We can see that even a monolingual semi-supervised word segmentation method can outper-form a supervised one in MT, probably because thetraining/test corpora contain many unknown wordsand words have different frequencies in our MTdata from they do in the manually labeled CWSdata.6.2 Translation Task: Small Track IWSLTWe evaluate our full model, using both monolin-gual and bilingual information, on the IWSLT data.As shown in Table 5, the Chinese trainingcorpus was segmented using the unigram seg-menter (Base) described in Section 2.1 and our GSmethod.
Since the unigram segmenter performsbetter in our experiments, we took it as the base-line and the method for initialization in later ex-periments.
We see that the vocabulary size of theChinese training corpus was reduced more signif-icantly by GS than by the baseline method, evenTable 5: Statistics of corpora in task IWSLT.Test Sents.
Words[K] Voc.Cn.
En.
Cn.
En.Chars 42.9K 520 420 2780 9930Base 394 8800GS 398 6230Dev2 500 3.74 3.82 1004 821Dev3 506 4.01 3.90 980 820Eval 489 3.39 3.72 904 810Table 6: Translation performance with differentCWS methods on IWSLT[%].Test Method WER PER BLEU TERDev2 Unigram (Base) 38.2 31.2 55.4 37.0GS 36.8 30.0 56.6 35.5Dev3 Unigram (Base) 33.5 27.5 60.4 32.1GS 32.3 26.6 61.0 31.4Eval Characters 49.3 41.8 35.4 47.5LDC 46.2 40.0 39.2 45.0ICT 45.9 40.4 40.1 44.9Unigram (Base) 46.8 40.2 41.6 45.69-gram 46.9 40.4 40.1 45.4GS 45.9 40.0 41.6 44.8though they resulted in a similar number of run-ning words.
This shows that the distribution ofChinese words is more concentrated when usingGS.The parameter optimizations were performed onthe Dev2 data with 500 sentences, and evaluationswere done both on Dev3 and on Eval data, i.e.
theevaluation corpus of (IWSLT, 2007).The model weights ?
of GS from Section 5.1.2were optimized using the Powell (Press et al,2002) algorithm with respect to the BLEU score.We obtained ?1= 1.4, ?2= 1 and ?3= 0.8 asoptimal values and T = 4 as the optimal numberof iterations of re-alignment with GIZA++.For a fair comparison, we evaluated on variousCWS methods including translation on characters, LDC (LDC, 2003), ICT (Zhang et al, 2003), uni-gram, 9-gram and GS.
Improvements using GS canbe seen in Table 6.
Under all test sets and evalua-tion criteria, GS outperforms the baseline method.The absolute WER decreases with 1.2% on Dev3and with 1.1% on Eval data over baseline.We compared the translation outputs using GSwith the baseline method.
On the Eval data, 196sentences are different out of 489 lines, where 64sentences from GS are better, 33 sentences areworse, and the rests have similar translation qual-ities.
Table 7 shows two examples from the Evalcorpus.
We list segmentations produced by thebaseline and GS methods, as well as the transla-tions corresponding to these segmentations.
TheGS method generates better translation results thanthe baseline method in these cases.1023Table 7: Segmentation and translation outputs withbaseline and GS methods.a) Baseline ?
?4 m?do you have a ?GS ?
?4m?do you have a shorter way ?REF is there a shorter route ?b) Baseline >??
??please show me the in .GS >??
??please show me the total price .REF can you tell me the total amount ?7 Conclusion and future workWe showed that it is possible to learn Chinese wordboundaries such that the translation performanceof Chinese-to-English MT systems is improved.We presented a Bayesian generative model forparallel Chinese-English sentences which usesword segmentation and alignment as hidden vari-ables, and incorporates both monolingual andbilingual information to derive a segmentationsuitable for MT.Starting with an initial word segmentation, ourmethod learns both new Chinese words and dis-tributions for these words.
In a large and a smalldata environment, our method outperformed thestandard Chinese word segmentation approach interms of the Chinese to English translation quality.In future work, we plan to enrich our monolingualand bilingual models to better represent the truedistribution of the data.8 AcknowledgmentsJia Xu conducted this research during her intern-ship at Microsoft Research.
This material is alsopartly based upon work supported by the DefenseAdvanced Research Projects Agency (DARPA)under Contract No.
HR0011-06-C-0023.ReferencesAldous, D. 1985.
Exchangeability and related topics.In?Ecole d?
?et?e de probabilit?es de Saint-Flour, XIII-1983, pages 1?198, Springer, Berlin.Andrew, G. 2006.
A hybrid markov/semi-markov con-ditional random field for sequence segmentation.
InProceedings of EMNLP, Sydney, July.Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.Gao, J., M. Li, A. Wu, and C. Huang.
2005.
Chi-nese word segmentation and named entity recogni-tion: A pragmatic approach.
Computational Lin-guistics, 31(4).Goldwater, S., T. L. Griffiths, and M. Johnson.
2006.Contextual dependencies in unsupervised word seg-mentation.
In Proceedings of Coling/ACL, Sydney,July.IWSLT.
2007. International workshop onspoken language translation home page.http://www.slt.atr.jp/IWSLT2007.Klein, D. and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proceedings of ACL, pages 128?135.LDC.
2003.
Linguistic data consor-tium Chinese resource home page.http://www.ldc.upenn.edu/Projects/Chinese.NIST.
2005.
Machine translation home page.http://www.nist.gov/speech/tests/mt/index.htm.Och, F. J. and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proceedings of ACL, pages 295?302,Philadelphia, PA, July.Papineni, K. A., S. Roukos, T. W., and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL, pages 311?318,Philadelphia, July.Press, W. H., S. A. Teukolsky, W. T. Vetterling, andB.
P. Flannery.
2002.
Numerical Recipes in C++.Cambridge University Press, Cambridge, UK.Snover, M., B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAMTA, pages 223?231, Cambridge, MA, August.Vogel, S., H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proceed-ings of COLING.Xu, J., R. Zens, and H. Ney.
2004.
Do we needChinese word segmentation for statistical machinetranslation?
In Proceedings of the SIGHAN Work-shop on Chinese Language Learning, pages 122?128, Barcelona, Spain, July.Xu, J., E. Matusov, R. Zens, and H. Ney.
2005.
Inte-grated Chinese word segmentation in statistical ma-chine translation.
In Proceedings of IWSLT, pages141?147, Pittsburgh, PA, October.Zens, R. and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proceedingsof HLT/NAACL, Boston, MA, May.Zhang, H., H. Yu, D. Xiong, and Q. Liu.
2003.HHMM-based Chinese lexical analyzer ICTCLAS.In Proceedings of the Second SIGHAN Workshop onChinese Language Learning, pages 184?187, July.1024
