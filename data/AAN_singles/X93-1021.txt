CRL/BRANDEIS :THE DIDEROT SYSTEMJim Cowie, Louise Guthrie, Wang Jin, William Ogden, James Pustejovsky t,Rong Wang, Takahiro Wakao, Scott Waterman t, Yorick WilksComput ing Research Laboratory, New Mexico State UniversityEmail :  \]cowie@nmsu.edutComputer  Science Department ,  Brandeis UniversityEmail:  j amesp@cs.brandeis.edu1.
Description of Final SystemDiderot is an information extraction system built at CRLand Brandeis University over the past two years.
It wasproduced as part of our efforts in the Tipster project.The same overall system architecture has been used forEnglish and Japanese and for the micro-electronics andjoint venture domains.The past history of the system is discussed and the op-eration of its major components described.
A summaryof scores at the 24 month workshop is given.Because of the emphasis on different languages and dif-ferent subject areas the research as focused on the de-velopment of general purpose, re-usable techniques.
TheCRL/Brandeis group have implemented statistical meth-ods for focusing on the relevant parts of texts, programswhich recognize and mark names of people, places andorganizations and also dates.
The actual analysis of thecritical parts of the texts is carried out by a parser con-trolled by lexical structures for the 'key' words in thetext.
To extend the system's coverage of English andJapanese some of the content of these lexical structureswas derived from machine readable dictionaries.
Thesewere then enhanced with information extracted from cor-pora.The system has already been evaluated in the 4th Mes-sage Understanding Conference (MUC-4) where it wasrequired to extract information from 200 texts on SouthAmerican terrorism.
Considering the very short develop-ment ime allowed for this additional domain the systemperformed adequately.
The system was then adapted tohandle the business domain and also to process Japanesetexts.
Further extensions to the system allowed it toprocess texts on micro-electronics development.
Perfor-mance at the 12 and 18 month evaluations was good forJapanese, but less good for English where we have beenattempting to automate much of the development pro-cess.
A more pragmatic approach was adopted for thefinal 24 month evaluation, using the same hand-craftedtechniques for English as had been used for Japanese.We estimate the amount of effort used directly to buildthe systems described here is around sixty man months.1.1.
Techn ica l  ApproachOur objectives in this research ave been as follows:?
to develop and implement a language-independentframework for lexical semantic representation, anddevelop and implement a robust integration of thatframework into a language-independent theory ofsemantic processing;?
to investigate and implement language independenttechniques for automating the building of lexicalknowledge bases from machine readable resources;?
to implement statistical tools for the tuning of lex-ical structures to specific domains;?
to implement the use of language independent s a-tistical techniques for identifying relevant passagesof documents for more detailed analysis;?
to develop and implement a set of robust multi-passfinite-state f ature taggers;?
to develop and implement the equivalent methodsfor Japanese.1.2.
P rocess  F lowAn outline of the functions of the main system modulesare given here.
This is intended to provide a contextfor the more detailed escription of each module whichfollows.
The structures of the Japanese and English sys-tems are very similar.
In the examples of intermediateoutput either Japanese or English may be shown.
Thesystem architecture is shown in figure 1.The input text to the system is processed by three inde-pendent pre-processing modules:223RelevanceStatisticsI ~ \[ Panof \[Semantic i 1 SpeechTagger \[ Tagger .
~_~.~.~/ ,~,  \[Y TY @Noun PhraseRecognizerParserTransformerReferenceResolver- -l.nferencer~ TemplateFormattertFigure I: System Overview?
A chain of finite-state feature taggers - these mark:names, organization ames, place names, date ex-pressions and other proper names (depending on thedomain),?
A part of speech tagger,?
A statistically based determiner of text relevance(micro only).If the statistical determination rejects the text process-ing proceeds to the final output stage and an empty tem-plate is produced.
Otherwise the results of the other twostages are converted to Prolog facts and these then passinto the head of a chain of processes each of which givesrise to further refinements of the text:?
Merge - Here semantic tags, which may markphrasal units, are merged with POS tags, whichmark individual words.?
Compound noun recognizer - this groups words andphrases into compound nouns using POS and se-mantic information.?
Parser - the relevant paragraph information is usedto select which sentences to process further.
Thesentences containing the marked up noun-phrasegroups are then parsed to produce a partially com-pleted representation of the relevant semantic on-tent of the sentence (frames).?
Reference resolver - the frames are then mergedbased on name matching and noun compounds be-ginning with definite articles.?
Template formatter - this transforms the resolvedframes into the final output form.1 .3 .
Descr ip t ion  o f  key  modu les  ands tagesStat i s t i ca l  F i l te r ing  Techn iques  Statistical infor-mation is used to predict whether a text holds importantinformation that is relevant to completing a template.This allows the parser to skip non-relevant texts.
This isbased on word lists which are derived from training onrelevant and irrelevant exts.
The theoretical results onwhich the method \[?\] is based assure us that documentscan be classified correctly if appropriate sets of wordscan be chosen for each document ype.
The method wasonly applied to the micro domain for MUC-5 as almostall texts in the joint venture domain are relevant andthe use of this statistical method is essentially a way ofimproving precision in text filtering.The results for the micro electronics domain for text fil-tering are 84% recall and 90% precision (73 and 83 at 18month) for Japanese, and 78% recall and 83% precision(7T and 76 at 18 month) for English.1 .4 .
Semant ic  Tagg ingThis component is based on a pipeline of programs.These are all written in 'C'  or flex.
It marks organi-zation names, human names, place names, date expres-sions, equipment names, process types and a variety of224measurements (including money).
Many of these haveconverted forms and additional values attached by thetagger.The tagging programs use three separate methods - -?
Direct recognition of already known unambiguousnames, using a longest string match.?
Recognition using textual patterns only.?
Two pass method marking ambiguous, but potentialnames, and subsequently verifying they fit a pat-tern.?
final pass recognizing short forms and isolated oc-currences of names not in a strong contextThe system uses the case of letters used when available.The final text is tagged using SGML-like markers.BRIDGESTONESPORTS CO. SAID FRIDAYIT HAS SET UP A JOINT VENTURE INTAIWAN WITH A LOCAL CONCERNAND AJAPANESE TRADING HOUSE TOPRODUCE GOLF CLUBS TO BE SHIPPEDTO JAPAN.<organ> BRIDGESTONE SPORTS CO.{type(\[\[entity_type,'COMPANY'\]\])} <\endorgan>said<date> FRIDAY{type(\[\[date,'241189'\]\])}<\enddate> it has set up a jointventure in <country> TAIWAN{type(\[\[nationality,'TAIWAN'\]\])}<\endcountry> with a local concern and a<country>japanese {type(\[\[nationality,'JAPAN'\]\])}<\endcountry>trading house to produce golf clubs to be shippedto <country> JAPAN {type(\[\[nationality,'JAPAN'\]\])}<\endcountry>.At this point the tags are converted into Prolog facts:organ('BRIDGESTONE SPORTS CO.',type(\[\[entity_type, 'COMPANY'\]\])),res('said',type(\[\[undefined,'said'\]\] )),time('FRIDAY',type(\[\[date_adverb,'UNSPEC'\],\[date,'241189'\]\])),cs('it',type(\[\[it,\[pron\]\]\])),cs('has',type(\[\[has,\[pastv,presv\]\]\])),gls('set up',type(\[\['set up',v\]\])),cs('a',type(\[\[a,\[determiner\]\]\])),gls('joint venture',type(\[\['joint venture',comp\]\])),date_adverb('in',type(\[\[date_adverb,during\]\])),country('TAIWAN',type(\[\[nationality,'TAIWAN'\]\])),cs('with',type(\[\[with,\[prep\]\]\])),The Japanese system preprocesses the article to changethe original encoding (Shift JIS) to EUC for a given ar-ticle.
The original and unsegmented text goes througha series of taggers for known names, i.e.
organizations,places, GLS verbs.
This process is exactly the same asin the English system.
The next step is to tag organi-zation, personal and place names which are not knownto the system.
These are detected by using local con-text, using Japanese-specific patterns, which use parti-cles, specific words and the text tags to recognize the un-known names.
In addition, date expressions are taggedand changed into the normalized form.
Date expressionsin the Japanese articles eem straightforward, for exam-ple, '20 nichi' (20 day) is used even if the document dateis 21st and 20th can he expressed as 'yesterday', and thisconvention 'XX day' (where XX is a number) to express adate is consistently used in the articles.
Era names uchas '~ '  (Showa) or '~Ji~' (Heisei) are Japanese specificand the year in the era, e.g. "
(Showa 60th year), iscorrectly recognized and normalized.
Here is the firstsentence of a typical article after the tagging process.<\organ> ~,~J~~{type ( \[ \[entity_type, ' COMPANY' \] \] )}<\endorgan> ~<\time> ~ ~ ~ {type ( \[ \[date_adverb, alter\],\[dat e, ' 850 I' \] \] ) } <\endtime><\organ> P k ~{type ( \[ lent ity_type, ' COMPANY' \] \] ) }<\endorgan><\gZs> ~b '~{type( \ [ '~T~ ' ,v\] )} <\endgls>Just as for the English system this is then convertedinto the form of Prolog facts ready to be read into themerging phase.Par t -Of -Speech Tagging English text is also fedthrough the POST part of speech tagger.
This attachesthe Penn Treebank parts of speech to the text.
The out-put is converted to Prolog facts.
The Japanese text issegmented with part-of-speech information by the JU-MAN program, which was developed by Kyoto Univer-sity.
The following is the result for exactly the samesentence.
The segmented units are converted to Prologfacts ready to input to the next stage.j uman (' ~ ,  ' , ' proper_noun' ).juman( ' ~}_L' , 'proper_noun' ).j uman (' ~' , '  normal_noun ' ).juman( ' ~ ' ,  'normal_noun' ).juman( ' ~ ', ' topic_particle' ).juman(' ~)I ' ,  'normal_noun' ).225juman( ' ~ 6 ' ,  ' case_par t i c le '  ).juman('~', 'normal_noun' ).jtman( '~ ' ,  'normal_noun' ) .juman( ' ~ ' ,  ' case_particle' ).juman(' ~ ' , 'noun_verb '  ).juman( ' UT ' ,  ' verb '  ).Merg ing  The semantic and syntactic information aremerged to give lexical items in the form of triples.
Themerging is done in such a way that if it is not possibleto match up words (eg due to different reatments of hy-phens) a syntactic tag of 'UNK' is allocated and mergingcontinues with the next word.Noun Phrase  Group ing  Noun phrases are identifiedby scanning back through a sentence to identify headnouns.
Both semantically and syntactically marked unitsqualify as nouns.
The grouping stops when closed classwords are encountered.
A second forward pass gathersany trailing adjectives.
The main use of the noun phrasein the present system is to attach related strings to com-pany names to help with the reference resolution.
Theyare also used by a retrieval process which uses the stringto determine the SIC code industry type.A similar grouping is carried out for Japanese.noun_phrase ( \[\[under ined, house\] \ ,\[unit (cs, a, type ( \[ \[a, \[determiner\] \] \] ), \[' DT' \] ),unit (country, japanese, type ( \[ \[nat ional i ty,  ' JAPAN ' \ ] ,\[word_type, sp_noun\] \] ), \[' J J '  \] ),unit (res, trading, type ( \[ \[under ined, trading\] ] ),\['NN'\]),unit (res,house, type ( \[ \[under ined,house\] \] ),\['NN'\])\])noun_phrase (money,\[unit (num, '20', type ( \[ \[hum_value, 20\] \] ), \[ ' CD' \] ),unit (num,million, type ( \[\[num_value, 1000000\] \] ),\[ 'CD'\]),unit (money, 'NEW TAIWAN dollars ',type(\[\[denom, 'TWD'\]\] ), \['NP', 'NP', 'NNS'\])\])Pars ing  The parser has GLS cospecification patternsbuilt into it.
It uses these and ancillary rules for therecognition of semantic objects to fill a frame formatwhich was given as an application specific field in theGLS entry.
The frame formats provide a bridge betweenthe sentence level parse and the final template output.Semantic objects are named in the cospecification andspecial rules which handle type checking, conjunctionand co-ordination are used to return a structure for theobject.
The following shows an example of a tie-up be-tween two companies.
The child company is unmatched,shown by an underscore.
The parser has grouped a datewith one of the companies.
The tie-up status is providedby the GLS template semantics.prim_tie_up(1, I, \[\[ \[f (name, _9947, \ [uni t  (o rgan , '  ~A~?.~' ,type ( \[ \[entity_type, ' COMPANY' \] \] ),\[proper_noun\] )\] ) ,f (ent ity_type, _9953, \[unit (organ,type ( \[ lent  i ty_ type ,  'COMPANY' \] \] ) ,\[proper_noun\] )\] ) \ ] \ ] ,\[ \[f (name, _10102, \ [un i t  (o rgan , '  Jq~l \ ]~ ' ,type ( \[ \[entity_type, ' COMPANY' \] \] ),\[proper_noun\] )\] ) ,f (ent i ty_ type ,  _10108, \ [uni t  (o rgan , '  : : k ;~\ ]~ ' ,type ( \[ \ [ent i ty_ type ,  'COMPANY' \] \] ) ,\[proper_noun\] )\] ) ,f ( t ime,_ lOl l4 ,  \[unit(time, '~  ~ ' ,type ( \[ \[date_adverb, a f ter \ ] ,  \[date, ' 8501 '3 \] ) ,\[proper_noun\] ) \] ) \] \] \ ] ,  _,\[f ( t ie_up_s ta tus ,  ex is t ing ,  \[\] )\] ) .T rans forming  The transformer module takes inputfrom the parser and does the following things-?
format changes?
generation of values for all the factoids?
frame restructuring (e.g.
form a simple set for allmanufacturers found in a capability frame producedby the parser).Re ference  Reso lu t ion  The task of this componentis to gather all the relevant information scattered in atext together.
The major task is to resolve referenceor anaphora.
For the current application only referencesbetween tie-up events, between entities, and between en-tity relations are considered.Since entities are expressed in noun phrases, the refer-ences for entities are resolved by resolving the referencebetween noun phrases.
Since the entity can either bereferred to by definite or indefinite noun phrase or byname, it is necessary to detect he reference between twodefinite or indefinite noun phrases, between two names,as well as between one name and one definite or indefi-nite noun phrase.
All entities are represented as framesof the form:entity(Sen#, Para#, Noun-phrase, Name,Location, Nationality,Ent-type, alias-list, rip-list).226The reference between two entities is resolved by lookingat the similarity between their names and/or their nounphrases.
Since companies are often referred by their na-tionality or location, the Location and Nationality slotfillers in the entity frame also contribute to the referenceresolution.
Some special noun phrases which refer tosome particular ole of a tie-up (the newly formed ven-turein particular) are also recognized and resolved.
Forexample, a phrase which refers to the child entity, suchas 'the new company' or 'the venture', will be recognizedann merged with the child of the tie-up event in focus.A stack of entities found in the text is maintained.Definite noun phrases can only be used for local refer-ence.
So they can only be used to refer to entities in-volved in the tie-up event which is in focus.
On thecontrary, names can be used for both local and globalreference, so they can refer to any entity referred to be-fore in the text.When a reference relation between two entities is re-solved they are merged to create one single entity whichcontains all the information about that particular entity.Since a tie-up is generally referenced by an entire sen-tence rather than a single noun phrase, the reference oftie-up events is handled by resolving the reference be-tween its participants and some other information men-tioned about the event.
Other heuristics are also applied.These mostly block the overapplication of merging.
Forexample, two tie-ups cannot be merged if their dates aredifferent; similarly, entities with different locations willnot be merged.
There are currently two types of textstructures which are considered.
In the first type, onetie-up-event is in focus until the next one is mentionedand after the new one is mentioned the old one will notbe mentioned again.
In the second type, a list of tie-up-events are mentioned shortly in one paragraph, and moredetails of each event are given sequentially ater.
Finally,when the reference between two tie-ups is resolved theywill also be merged to form a single tie-up event.
Thefinal result is a set of new frames which are linked in sucha way as to reduce the requirement on the final stage ofmaintaining pointers to the various objects.With the exception of the use of definite articles - -anobvious cross-linguistic difference between the languagesstudied-- the reference resolution process for Japaneseis identical to English.
The resolved entities, entity-relation, and tie-up for a typical text are shown below.final_entity(2,If(name, \['~', '~', '~', '~'\], 'UNSPEC'),f(entity_type,'COMPANY',~UNSPEC'),f(industry_product,'(63 "~") ' ,w j ) ,f(time,\[after,'B50i'\],wj),f(entity_relationship,l,inf),f(entity_relationship,3,inf)\]).final_entity(9,'UNSPEC'),f(entiCy_type,~COMPANY','UNSPEC'),f (name, \ [~ ' , '~ , ' , '~ ' ,~ Jz ' \ ] , 'UNSPEC' ) ,f (ent i ty_ re la t ionsh ip , l , in f ) ,f (ent i ty_ re la t ionsh ip ,3 , in f ) \ ] ) .f ina l _ re l  ( 1, \[9,2\], ' UNSPEC ', ' PARTNER ', ' UBSPEC ' ).final_tie_up(1,\[9,2\],'UNSPEC','UNSPEC','UNSPEC~,exis?ing,'UNSPEC~,I,'UNSPEC').The Japanese system uses character-based rules for iden-tifying aliases.
The followings are examples of rules usedin the system.?
First two characters used for an alias.
'E I~'  (Hi-tachi) for ' H~P/ i '  (Hitachi Manufacturing).?
First and third characters used.
'ELM' (Nikkou)for'H*~'  (Nihonnkoukuu or Japan Airlines).?
First and last characters for an alias of a foreigncompany name.
'7~'  (A sha or A Co) for '77?~4 F ?
"~ I) 7)1/~'  (Applied Material Co).?
The system has a knowledge base for difficultaliases. '
J A L '  for ' \ [ I~ '  (Japan Airlines) and'GE '  for '~x  ~,~l l / ?
3511P b i) ,~, p ,  (GeneralElectric).Template  Format t ing  The final stage generates e-quence numbers and incorporates document numbersinto the labels.
It also eliminates objects which are com-pletely empty.
The final output from the English systemexample text, #0592, is shown below.<TEMPLATE-0592-1> :=DOC NR: 0592DOC DATE: 241189DOCUMENT SOURCE: "Jiji Press Ltd."CONTENT: <TIE_UP_RELATIONSHIP-0592-1><TIE_UP_RELATIONSHIP-O592-1> :=TIE-UP STATUS: existingENTITY: <ENTITY-O592-3>JOINT VENTURE CO: <ENTITY-0592-1>OWNERSHIP: <OWNERSHIP-0592-1><ENTITY-O592-1> :=NAME: BRIDGESTONE SPORTS TAIWAN CO227ALIASES: "BRIDGESTONE SPORTS"TYPE: COMPANYENTITY RELATIONSHIP:<ENTITY_RELATIONSHIP-0592-1><ENTITY-0592-3> :=NAME: BRIDGESTONE SPORTS COALIASES: "BRIDGESTONE SPORTS"TYPE: COMPANYENTITY RELATIONSHIP:<ENTITY_RELATIONSHIP-O592-1><ENTITY_RELATIONSHIP-0592-1> :=ENTITY1: <ENTITY-OS92-3>ENTITY2: <ENTITY-0592-1>REL OF ENTITY2 TO ENTITY1: CHILDSTATUS: CURRENT<OWNERSHIP-O592-1> :=OWNED: <ENTITY-0592-1>TOTAL-CAPITALIZATION: 20000000 TWDOWNERSHIP-E: (<ENTITY-0592-3> 75 )1.5.
Hardware  and  So f twareRequ i rementsHardware The system runs on Sun 4 Workstations.It should run on any Unix machine with the appropri-ate compilers and has in fact been ported onto an IBMRS6000 system.Software1.
Operating SystemThe system runs under UNIX.
Currently we are us-ing SunOS Release 4.1..
Segmentation programsPOST (BBN) : 24 MegabytesJUMAN (KYOTO/MCC version) : 8 Megabytes3.
Programming languagesQuintus Prolog : Release 3.1.1, requires 64Megabytes of disk space.CCMU Common Lisp : 16 Megabytes of memory and25 Megabytes of disk space are recommended.4.
Unix toolsflex/lex5.
Size of the data and programsEnglish Total 103 MegabytesData 16 Megabytes, Code 87 MegabytesJapanese 49 MegabytesData 0.7 Megabytes, Code 48 Megabytes1.6 .
Speed/Throughput  Stat i s t i csOn average, the time for the English systems to processone article is 3 minutes.
The Japanese systems are muchfaster, taking about 40 seconds per article.1.7.
Key  Innovat ions  o f  F ina l  Sys temThe methods used in the Diderot system have notchanged significantly since the original system was as-sembled for the MUC-4 terrorist message valuation.Our conviction has always been that simple, easily con-figurable, modular methods were the only approachwhich would work in the short term on general text.
Fouraspects of the system have proven to be key to its opera-tion.
These are - finite state tagging methods, semanticpartial parsing, domain and language specific referenceresolution and statistical judgement of relevance.F in i te  State  Tagging Methods  These are an essen-tim component of our extraction system.
They allow atext to be marked up with semantic lasses of all theobjects mentioned in it by the use of patterns and database files.This component is language specific and to some extentdomain specific.
It would seem likely that as more ex-traction systems are built a growing number of recog-nizers will become available.
For micro electronics wedeveloped specific recognizers for equipment and devicenames.We also tested the performance ofour organization andhuman name recognizers by scoring them automaticallyagainst human tagged text.
This allowed us to enhancethe performance ofthe taggers independent of the rest ofthe system.
Development ofspecific evaluation methodsfor components is time consuming and expensive, but ithas enormous paybacks in terms of measuring the per-formance of specific components.
(The scoring softwareand data is available to members of the Consortium forLexical Research, as it much other data and software de-veloped by Tipster contractors.
Mail lexical@nmsu.edufor further information.
)Semantic Part ia l  Parsing The parser has two levelsof operation.
The first is a set of rules for identifyingappropriate semantic objects in a text.
The second isa lexical pattern driven parse which identifies the rolesof the objects in a specific sentence.
These two operatetogether to produce frames closely related to the finalsemantics of a template.228The approach bypasses the normal two stage approachof parsing to a tree structure and then applying infer-ence mechanisms to derive the final logical form for thesentence.The recognition of objects uses two lists of allowableand required semantic types for each object.
Thus alocation is allowable as part of an organization semanticobject, but either an organization ame or an organiza-tion noun phrase must be found to satisfy the semanticconstraints for an organization.
These constraints arespecified in a declarative form.
It is this level of theparser which recognizes conjunctions and lists of objects.These are nested according to a set of precedence rulesand the resulting tree is unwound to produce lists foreach object identified for the parse.
Thus the pattern<entity> manufacture <product> will recognize a listof organizations in the subject position and one or moreproducts in the object.The hand development of patterns for the parser is rel-atively simple as there is a clear mapping to the finaltemplate.
A very small number of frames were used torepresent these template semantic structures.
The def-inition of these frames was the same for Japanese andEnglish.Reference Resolut ion and Domain  IndependenceThe task of reference resolution module in Diderot is tosort the partially filled frames produced by the parserfrom single sentences in the text and to search for coref-erential frames and merge them.
Frames are used torepresent entities (e.g.
companies and persons) as wellas events (e.g.
tie-ups and relations).
Frames are de-fined recursively such that some frames might have otherframes to fill their slots.
Frames contain not only the in-formation that needs to be finMly extracted from thetext but also other information (includes syntactic in-formation) that will help to resolve the reference (e.g.noun phrases).
The resolution program consists of thefollowing parts:1. a set of conditions uch that if two frames meet acondition then they are considered to be coreferen-tial.2.
a bottom-up syntax driven algorithm to find all thecoreferential frames and merge them into a singleframe.3.
methods on how to merge two coreferential frames.The coreferential conditions can be categorized into syn-tactic constraints and semantic constraints.
The syn-tactic constraints are harder to specify as declarativeconditions and they are coded as procedures that guidethe search for coreferential frames.
On the other hand,these constraints are domain independent.
Semanticconstraints are mostly domain dependent and they arespecified for each type of frame.
Since different syntac-tic constraints suggest different search patterns and putdifferent requirements on the semantic onstraints, thesemantic onstraints associated with different syntacticconstraints may also be different.The recursively defined frames uggest a frame hierarchy.Our resolution algorithm works from the lowest levelframes upwards.
At each level, all the search schemessuggested by different applicable syntactic constraintsare tried for each frame.
If the associated semantic on-straints are also satisfied, a corefer.ential pair is found.Finally, the coreferential frames get merged into one sin-gle frame.
Since the merge of higher level frames maycause lower level frames to be merged, the merge processis recursive.
Here a set of contradiction conditions thatresist two frames being merged are used.The domain independent parts of our reference resolu-tion module are the resolution algorithm and syntacticconstraints.
The domain dependent parts are seman-tic constraints, merge methods and contradiction condi-tions.
Trying to make semantic onstraints domain in-dependent, we believe, is very difficult if not impossible.For instance the set of conditions that indicate two com-pany frames (such as the ones for name or aliases) arecoreferential re very different from that for equipmentframes.
Besides, unless we have a semantic interpreta-tion module that is intelligent and rich enough, it is im-possible to have a domain independent mechanism thatcan correctly interpret, say, definite descriptions (con-sider possessive modifiers for company and equipment).To make things even worse, it is also very difficult tospecify some of these conditions declaratively.
A goodexample is the company names and device names wheredifferent naming conventions force us to write differentprocedures to manipulate name strings in order to findout alias relations.So, we believe the best solution to make adapting toa new domain easier is a yacc/lex type of precompiler.Here, to port the system to a new domain, we onlyneed to provide domain dependent conditions and mergemethods for each frame type and/or each syntactic on-straint.
We can write our own predicates/procedures oruse ones provided in a system library to specify the con-ditions and the methods.
The precompiler will combinethem together with the resolution algorithm and syntac-tic constraints to produce a reference resolution programfor that domain.229Statistical Relevance Judgement  We have contin-ued to work on a procedure for detecting document typesin any language.
The system requires training texts forthe types of documents o be classified.
The method isdeveloped on a sound statistical basis using probabilisticmodels of word occurrence \[?\].
This may operate on let-ter grams of appropriate size or on actual words of thelanguage being targeted and develops optimal detectionalgorithms from automatically generated "word" lists.For the Japanese micro-electronics system, texts were fil-tered to decide whether or not they were relevant to thedomain.
The decision was based on whether an incomingdocument "resembled" a set of documents judged "rele-vant" by human analysts (i.e.
human analysts produceda corresponding on-empty template for the document).We varied the meaning of "resemble" in a series of sta-tistical experiments using the frequencies of words, bi-grams, trigrams and four grams found in the documentto be classified, and found to be good "distinguishing"words/grams in the texts which were judged relevant byhumans.
All experiments used a multinomial model forthe problem and maximum likelihood ratio test for thedecision.
Similar experiments were performed on theEnglish micro-electronics texts.
The entire set of docu-ments judged relevant by humans was used for trainingsince it was felt that the number of texts of this typewhich were available was relatively small, and for thissame reason, the decisions in both systems are based onwords rather than grams at this time.2.
Original Project GoalsWe list our original project goals and comment brieflyon how far our present effort has gone in achieving thesegoals and how they have been modified based on therealities of the Tipster information extraction task.1.
language modularity: allowing the addition of newlanguages with a minimum of effort through use ofa limited interlingual representation for lexical anddomain knowledge;Since the English and Japanese systems use thesame system architecture in both domains and thesame internal representation is used in English andJapanese system, the conversion from English sys-tem to corresponding Japanese system was rela-tively easy.
The English Joint Venture system wasconverted to give the Japanese JV system and theEnglish Micro-Electronic system was converted togive the Japanese ME system by one native speakerof Japanese.
The differences between English andJapanese systems are as follows:..?
Data for tagging.
Company, human, title, andplace names and time expressions are languagespecific.?
Patterns for GLS cospecification.
There is a setof Japanese verbs for indicating various kindsof tie-ups such as import tie-up, sales tie-up,and business tie-up.
Besides, the majority ofthe tie-ups in Japanese Joint-Venture articlesinvolve only two parent companies and thereis no mention of the JV company.
Thus thisfact is relfected on the cospec patterns of theseverbs.?
Patterns for recognizing company namealiases.
As explained above, the Japanese sys-tem uses character-based and language-specificrules for recognizing aliases.acquisition of benefits of scale through the additionof lexical information automatically from existingmachine-readable dictionaries;We used the Longman Dictionary of ContemporaryEnglish to generate the initial verb patterns usingverb subcategorization information, which is sup-plied in the dictionary, supplemented by exampledefinitions which sometimes preferred subject andobject information i  the form of bracketed exam-ple subject and object types.
This was then ex-tended by finding additional pattern information ithe Wall Street Journal Corpus.
The dictionary,however, did not prove rich enough to provide allthe possible ways of expressing information foundin newspaper text, For example team up with, joinforces, and so on.
These have been added using pat-terns for equivalent senses found in the dictionary.Additionally the dictionary was used to generate se-mantic classes of nouns, for example all the wordslike factory which represent an industrial site.
Thiswas done for several classes of noun.
The othersource of this type of information was the keys pro-vided for training data.the use of well-motivated Lexical Structures (LS's)to capture the presuppositional nd anaphoric as-pects of texts structures, essential for successful ex-traction;The lexicM structures used in Diderot specify pos-sible patterns occurring in the text and the typesof appropriate objects found at specific locations inthe patterns.
By allowing noun phrases with ap-propriate heads to satisfy these constraints he lexi-cat structures allow the generation ofpartially com-pleted frames which can then be processed by thereference resolution module.2304.
the initial seeding of structures automatically bythe techniques of (2) above, and the tuning of theLS's against corpora for particular languages (e.g.Japanese);Tuning of lexieal structures against he corpus hasbeen a major effort in our project.
This has notproduced the results we had hoped for.
This maybe partially due to the lack of specificity of the cor-pus we were using.
In addition some of the methodsdeveloped epended on having corpora tagged withreasonably accurate semantic information.
Our se-mantic tagging module has increased in accuracyduring the course of the project.
During the initialdevelopment phase it was probably not of sufficientquality to support he corpus development effort.5.
the use of strong semantic resolution techniques(based on Wilks' Preference Semantics \[?\]) for theresolution of lexical ambiguity, and the impositionof appropriate structure on real (i.e.
potentially ill-formed, multi-sentence) input text;Semantic onstraints are applied to the structureswhich occupy the various fields in the cospecificationpattern.
These impose necessary conditions on theinformation gathered for each field.
This proved suf-ficient to disambiguate he uses of the forms foundin both domains.6.
given that full parsing of very large-scale text sam-ples is out of the question in the current state of theart, in the sense of parsing every sentence of a largetext into a formal structure of any depth and con-tent, we propose a set of alternative partial parsersand segmenters, all parsing to a canonical interlin-gum representation for selected sentences;This statement is almost a thumbnail sketch of ourcurrent system.
Our system essentially operateswith patterns at a variety of levels.
These produce avery specific domain dependent canonical represen-tation containing the essential information requiredfor the construction of a set of templates.7.
we shall define a set of "minimalist AI  techniques"to connect inferentially the information carried bythe slot-names of the TIPSTER templates: amongthese will be Finite State Acceptor demons that knowabout, e.g., the structures of dates, places, personnames in English and Japanese and have access tolarge publicly-available word lists;Our system is dependent on a multiplicity of finitestate machines which recognize the basic buildingblocks of a template.
These processes often rely onlarge lists of terms for the specific class of item be-ing recognized.
In other case they rely on patternsderived by using corpus analysis tools such as Key-word in Context (KWIC) indexes (for example forequipment names).8. although statistical techniques used alone and un-aided for traditionally AI  tasks give poor results andseem to offer no clear path to optimization, the useof some such techniques i now firmly established inconjunction with symbolic techniques and we shallpropose statistical techniques for gathering what weshall refer to as the "true lexicon" of the texts, andusing these to locate relevant "text points" for de-tailed analysis;Our statistical techniques have been used in a va-riety of ways during the development of Diderot.In the original MUC-4 system they were usedto identify specific paragraphs, for Tipster micro-electronics they marked relevant texts.
These meth-ods have already been discussed.
In addition themethods allow us to identify important vocabularyfor a domain.
This has been less important for thewell defined omains we have worked on, but wouldprove useful to an analyst moving into a new do-main who already had a collection of relevant andirrelevant texts.9.
closely connected to (7) will be Metallel proce-dures that determine standard metonymic and hi-erarchical relations between text items and otheritems available to the domain knowledge base (e.g.Moscow often should be replaced by Soviet Govern-men O.
Like the procedures of (7) it has access toan automatically-generated tangled genus hierarchyfrom the methodology of (2).A study of the metaphor and metonymy occurringin the joint venture domain was made at an earlystage in the project.
Various classes of metaphorswere identified.
However, the large majority of theseproved to occur in standard ways and could be clas-sifted as dead metaphors.
The most appropriate ap-proach seemed to be to code these explicitly into thelexicons used by the system.2.1.
Mach ine  Ass i s ted  Human In forma-t ion  Ext ract ionIn addition to work on the automatic extraction of in-formation from documents, CRL was also involved inthe human side of the Tipster project.
To prepare theTipster data, human analysts performed the informationextraction task on over five thousand ocuments.
CRLcreated and maintained software tools to aid in this taskfor each of the domains and languages.
These window-based tools allow human analysts to build the key tern-231plates by selecting pieces of the original text, or pickingstandardized field information from menus.
These toolswere used by all of the analysts and all of the sites per-forming this task.Based on this experience with the human extractiontask, and our own automatic extraction system, our vi-sion for the future is one of integrated extraction com-ponents which aid human in the loop analysis.
For manyapplications the current information extraction systemsare insufficiently accurate and have too long a develop-ment time.
Even in cases where the technology is ade-quate there is still a need for some completed keys bothto 'prime the pump' and to allow objective testing ofsystem performance.
In both cases this means a humananalyst carrying out the template filling task.We have developed an initial version of a system whichsupports integrated machine assisted human informationextraction, with fills for fields being both suggested andconverted to standard forms by automatic extractionmodules.
This system, Tabula Rasa, is an interactivedesign tool and interface code generator which allowsan analyst to define a new domain and to produce amatching machine assisted information extraction toolin minutes.
This is intended to allow a more rapid de-velopment of the definition of the extraction task and anintegration of automatic extraction techniques in a toolused by human analysts.With Tabula Rasa an analyst can define windows foreach data object which is to be extracted from the text.The fields in these objects are created and labeled by theanalyst and a definition of the type of information theycan hold is specified.
Other attributes can also be set,for example if it is a required or optional fill.
Some fieldscan be set-up with automatic extraction capabilities.
Forexample, a field can be specified as a 'name' field andif the texts are preprocessed by the Didero~ system, alist of automatically extracted names are presented ascandidate fill values.
The structured ata specificationis controlled with an interactive graphical user interfaceand is used to produce a tool which can be used imme-diately to test if the output specified is appropriate.
Adefinition of the data structure developed (in standardBNF form), and a set of texts describing specific fieldsand objects in the template are automatically produced.These can be used as the basis of both on-line and paperdocumentation a d we intend to build a simple genera-tor which will create the first draft of this documentationautomatically.Tabula Rasa is an attempt o reduce two of the ma-jor bottlenecks of information extraction; the definitionsof the text extraction task and the production of toolsintergrating automatic extraction to aid the human an-alyst in the production of structured ata.
We intend toinvestigate how successful Tabula Rasa is by researchingits actual use by analysts.
This investigation will focuson the usefulness of automatically extracted ata for hu-man in ~he loop analysis systems.
Future versions willembody ways of integrating well tested improvements inautomatic techniques that will aid the analyst as sug-gested by the actual use of the tool.3.
Evo lu t ion  o f  sys tem over  two  yearsThe Diderot system was developed from scratch for theTipster information extraction project.
A diagram show-ing the chronology of the system can be found at the endof this paper.The first version of the system was developed in fivemonths and was evMuated in the 4th Message Under-standing Conference (MUC-4) where it extracted infor-mation from 200 texts on South American terrorism.
Atthis point the system depended very heavily on statis-tical recognition of relevant sections of text and on theability to recognize semantically significant phrases (e.g.a car bomb) and proper names.
Much of this informationwas derived from the keys.The next version of the system used a semantically basedparser to structure the information found in relevant sen-tences in the text.
The parsing program was derived au-tomatically from semantic patterns.
For English thesewere derived from the Longman Dictionary of Contem-porary English, augmented by corpus information andthese were then hand translated to equivalent Japanesepatterns.
The Japanese patterns were confirmed using aphrasal concordance tool.
A simple reference resolvingmodule was also written.
The system contained largelists of company names and human names derived froma variety of online sources.
This system handled a subsetof the joint venture template definition and was evalu-ated at twelve months into the project.Attention was then focused on the micro-electronics do-main.
Much of the semantic information here was de-rived from the extraction rules for the domain.
A singlephrase in micro-electronics can contribute to several dif-ferent parts of the template, to allow for this a new se-mantic unit the factoid was produced by the parser.
Thisproduced multiple copies of a piece of text, each markedwith a key showing how the copy should be routed andprocessed in subsequent stages of processing.
This rout-ing was performed by a hew processing module, whichtransformed the output from the parser.
The statisticalbased recognition of text relevance was used for micro-electronics only~ as a much higher percentage of articles232in the corpus are irrelevant.
This system was evaluatedat 18 months.Finally the improvements from micro-electronics werefed back to the joint venture system.
An improved se-mantic unit recognizer was added to the parser.
Thishandles conjunctions of names, possessives and bracket-ing.
An information retrieval style interface to the Stan-dard Industrial Classification Manual was linked into theEnglish system.
The reference resolving mechanism wasextended to handle a richer set of phenomenon (e.g.
plu-ral references).
This, current, version was evaluated at24 months.4.
Accompl i shments :  What  worked  andwhat  fa i led ,  and  whyThe Tipster task is an extremely complex one in termsof the number of components involved and the volume ofdata needed to support he task.
It is extremely difficultto point at individual components of the system and saythis works, and this does not.
Throughout the process-ing each component is dependent on the performance ofprevious tages.Our main accomplishment was in the construction of fiveworking extraction systems over the two years of theproject.
We are particularly pleased with the perfor-mance of our two Japanese systems.For the English systems we adhered to our plan of at-tempting to automate as much as possible the develop-ment of the system, in particular the lexicon and as-sociated semantic patterns.
This work is going to con-tinue, but at the moment he performance of a systemdeveloped in this manner is unlikely to match one whichdepends on careful hand tuning.Our name and object recognizing software is a standalone component and has now reached levels of precisionand recall of 75% for both languages.The automatic generation of our parser from the GLSlexical entries is also a useful method developed in thesystem.
However, we need more sophisticated debuggingtechniques to enable us to track parse failures and errors.We feel that we have explored the problems involvedin implementing a linguistic theory (Pustejovsky's Gen-erative Lexical Semantics) in an operational system.This has lead to additions to the theory to support thespecifics of extraction and also to ignoring interestingaspects which did not support the task.
In particularwe have failed to achieve the generative aspect of thetheory which allows the lexical attributes of nouns tobe incorporated in the more general sense of a verb.
Wehave relied on a much simpler semantic typing for propernouns and noun phrases.Our other main research theme was to develop lexicalentries from corpora.
This proved to be a very time con-suming process and based as it is in a kind of averagingmay not produce data specific enough for the task.
Ananalyst with some knowledge of how the system oper-ates could write patterns for actual sentences that filltemplates more specifically than those we generated forour English systems.
The contrast here is clear betweenour English and Japanese systems.We have advocated partial parsing and regular expres-sion based pattern matching methods ince the projectbegan.
This approach certainly appears to be the mostappropriate for the information extraction task.5.
Eva luat ion  Summary5 .1 .
Of f i c ia l  T ips ter /MUC ScoresThe summary scores for each system are given in theappendix to this paper.
Graphs are also given show-ing the improvement of the final systems compared tothose at the eighteen month evaluation.
The systemswere all designed to attempt o fill all the possible slotsin the template.
For the joint venture domain, in par-ticular, where many slots occurred only a few times inthe training keys this made developing accurate systemsvery much harder.It is also clear from our experience of system develop-ment that the interaction between the parts of a systemis complex and that modifications at one level can of-ten, due to bugs or changes in the representation, leadto a significant drop in performance.
The ideal approachwould seem to be to iteratively test small changes on arelatively stable system, by scoring performance againsta series of test sets.
This is the approach adopted forboth our Japanese systems.
The English systems re-ceived no detailed hand tuning at this level, althoughthe micro electronics was improved by producing appro-priate lexical entries for all short texts in the test collec-tion, which originally had no template output producedby the system.Engl ish Jo in t  Venture  This system was the most re-liant on automatic development and least on human tun-ing.
The recall in particular was very low 24%, with aprecision of 51% for the all objects measure.
In particu-lar some of the simpler slots entity location and national-ity, should have been subjected to much stronger inspec-tion.A large number of fills were generated for these, butwith very low precision.
Other slots such as the prod-uct service code, which produced 818 entries, were much233harder to fill correctly depending as they did on a correctanalysis of the relevant sentences, a correct coreferencematch to the appropriate ntities and finally the correctidentification of the product string and SIC code.Our performance lies somewhere in the middle of theMUC-5 systems and is the lowest of the Tipster systems.Engl ish Micro-e lect ron ics  This system had a similarprecision to our English joint venture system, but hadhigher recall.
This was largely due to a last minute at-tempt to produce a greater coverage by hand coding lex-ical entries.
There is a great deal of variation in the ac-curacy of the recognizers for the variety of fields found inEME.
Further tuning would focus first on this aspect ofthe system.
That is until etchants, materials, equipmentnames can be identified accurately there is no possibilityof extracting this information i  the present system.
Theother significant problem we faced was the roles of theorganizations mentioned in the text.
Our precision forthese was far lower (19% - 34%) than the precision weobtained for the process object(58%).
The actual iden-tification of appropriate ntities was much higher (60%)and for entity name recognition (54%).Japanese Joint Venture Ourperformance in Japanese is significantly better than En-glish, with the CRL system lying in second place behindthe extremely high performing GE system.
The differ-ences between the two systems are that the GE systemhas better recall with high precision.
The CRL systemhas lower recall and slightly higher precision.
In fact,in terms of the precision, the CRL system has the bestscore.
The error rate and under generation for the GEsystem is lower than that of CRL system.
Thus the GEsystem has shown good recall with good precision, whichmeans lower scores in error rate and under generation.Japanese Micro-electronics Again, the GE systemis the top performer with the CRL system coming sec-ond.
In JME, GE's system has lower precision than itsJV system.
It seems that recall was emphasized in GE'sME system.
On the other hand, CRL's ME system fo-cused on precision.
The CRL system has the highestprecision.
The GE system has lower scores in error ratesand under generation, and the CRL system has lowerscores in over generation and substitution.5.2.
Explanation and Interpretation ofResultsThe scores for Japanese, using an identical architecture,but with much more intensive human tuning, are muchhigher.
We feel the huge difference between performancein Japanese and English is principally due to one personbeing dedicated for Japanese to running and tuning thesystem.
All other personnel were working on particularcomponents to be used first in the English and then inthe Japanese system and no one person was repeatedlytesting the operation of the English System.
Another dif-ference might be due to the focus of effort on automaticand semi-automatic pattern generation for the Englishsystems, aprocess which was not attempted for Japanesedevelopment.6.
ConclusionsWe have learned a great deal over the past two years,partly through the many mistakes we have made.
Theproject has depended a great deal on the skill and careof the people working on it to ensure consistency in ourdata and code.
Given the large number of knowledgebases in our system this is an onerous task and one taskneeded for the future is a system which allows this knowl-edge to be integrated and held in one central data-base,where consistency can be maintained.
The second is todevelop an easily configurable and portable reference res-olution engine.There are no major differences in the structure of theEnglish and Japanese systems.
It would seem that acritical part of achieving high precision and recall is tohave at least one person with a reasonable knowledgeof the whole system to carry out repeated test/improvecycles.The current system is robust and provides a good start-ing point for the application of more sophisticated tech-niques, some of them simply refined versions of the cur-rent architecture.
Given appropriate data it should bepossible to produce a similar system for a different do-main in a matter of months.
Many parts of the systemare portable in particular the semantic tagging mecha-nisms, the statistical filtering component.
Dates, com-panies and people - all of which occur in many kinds oftext - we now handle with good levels of accuracy.7.
AcknowledgementsThe system described here has been funded by DARPAunder contract number MDA904-91-C-9328.We would like to express our thanks to our colleaguesat BBN who have shared their part of speech tagger(POST) with us.
Thanks also to Kyoto University forallowing us to use the JUMAN segementor and part ofspeech tagger.Diderot is a team effort and is a result of the work ofmany people.
The following colleagues at CRL andBrandeis have contributed time, ideas, programming234ability and enthusiasm to the development ofthe Diderotsystem; Paul Buitellar, Federica Busa, Peter Dilworth,Steve Helmreich and Fang Lin.References1.
DARPA.
Proceedings o\] the Third Message Understand-ing Con\]erence (MUC-3), San Mateo, CA, 1991.
MorganKaufmann.2.
DARPA.
Proceedings o\] the Fourth Message Under-standing Con\]erence (MUC-4), San Mateo, CA, 1992.Morgan Kanfmann.3.
Cowie, J., Guthrie, L., Wakao, T., Jin, W., Pustejovsky,J.
and Waterman, S., The Diderot Information Ex-traction System.
In Proceedings o\] the First Con\]erenceo\] the Pacific Association \]or Computational Linguistics(PACLING93}, Vancouver, Canada, 1993.4.
Grishman, R., and Sterling, J., Acquisition of selec-tional patterns.
In Proceedings o\] the l~th InternationalCon\]erence onComputational Linguistics (COLING92),Nantes, France, 1992.5.
Guthrie, L., Bruce, R., Stein, G.C., and Weng, F.,Development of an application independent lexicon:Lexbase.
Technical Report MCCS-92-247, Comput-ing Research Laboratory, New Mexico State University,1992.6.
Gnthrie, L., and Walker, E., Some comments on classi-fication by machine.
Technical Report MCCS-92-935,Computing Research Laboratory, New Mexico StateUniversity, 1992.7.
Lehnert, W., and Sundheim, B., Art evaluation of textanalysis technologies.
AI Magazine, 12(3):81-94, 1991.8.
Proctor, P., editor.
Longman Dictionary o/Contempo-rary English.
Longman, Harlow, 1978.9.
Pustejovsky, J., The generative l xicon.
ComputationalLinguistics, 17(4), 1991.10.
Pustejovsky, J., The acquisition of lexical semanticknowledge from large corpora.
In Proceedings of theDARPA Spoken and Written Language Workshop.
Mor-gan Kaufmann, 1992.11.
Pustejovsky, J., Bergler, S., and Anick, P., Lexical se-mantic techniques for corpus analysis.
ComputationalLinguistics, 1993.12.
Pustejovsky, J. and Boguraev, B., Lexical knowledgerepresentation a d natural anguage processing.
Artifi-cial Intelligence, 1993.13.
Pustejovsky, J., The Generative Lexicon: A Computa-tional Theory of Lexical Semantics MIT Press, Cam-bridge, MA, 1994.14.
Schabes, Y. and Shieber, S., An alternative concep-tion of tree-adjoining derivation.
In Proceedings o\] 30thAnnual Meeting of the Association for ComputationalLinguistics, 1992.15.
Wilks, Y., A Preferential Pattern-Seeking Semanticsfor Natural Language Inference.
Artificial Intelligence,1975.235System Development HistoryCreationMajorAdditionsMUC4Coarse PatternsContext Free ParsingBetter PatternsTipster \[ EJV !
More Semantic features12th month I Original nversi~o n JapaneseSignificant | o PatternsChanges T JJVTipster EME Data resources18th month Modified onversion F"Feedback ofChanges JMETipster24 monthNew EJVConversion New JJVq TuningTuning236Progress since 18 month workshop7060_50_302010705020ENGLlSH IV iBM'I'H AND 24MTH COMPARBON10I I -18 momhs 24 mon'r~;JAPANESE JV 18MTH AND 24M'rH COMPAR.BONI FiB months 24 months70605040302.0107060PaR_ 40302010ENGI..~H MF., iSMTH AND 24.M'11.1 OOMPA.R~O:N/' P&RI I L24 months 1B monu~sJAPAI~ESE ME 18MTH AND 24MTH COMPARISON/II18 months 24 monks237Summary of Error-based ScoresJAPANESE MICROMin ERR UND OVG SUB18-Month 72 60 28 18 .74 .8024-Month 65 54 24 12 .69 .73ERRJAPANESE JVUND OVG SUB MinMaxMax18-Month 79 71 22 22 .86 .8624-Month 63 51 23 12 .70 .72ENGL ISH MICROMin ERR UND OVG SUB18-Month 86 76 33 37 .87 .9324-Month 74 60 33 24 .80 .84MaxENGLISH JV18-Month24-MonthERR9179UND7667OVG4028SUB5628Min1.060.89Max1.080.91238Summary of Recall/Precision-based ScoresJAPANESE MICRO18 - Month24 - Month'IF(R/P)73/8384/90REC3240PRE?5966P&R41.9950.37JAPANESE JVTF(R/P) REC18 - Month 82/99 2624 - Month 88/98 42PRE P&R61 32.867 52.1ENGL ISH MICROTF(R/P) REC18 - Month 77/76 1524 - Month 78/83 31PRE4251P&R22.2838.49ENGLISH JVTF(R/P) REC18 - Month 67/86 1024 - Month 76/92 24PRE P & R26 15.1051 32.64239
