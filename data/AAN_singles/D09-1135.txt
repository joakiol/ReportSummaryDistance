Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1298?1307,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPRefining Grammars for Parsing with Hierarchical Semantic KnowledgeXiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu?, Huisheng ChiSpeech and Hearing Research CenterKey Laboratory of Machine Perception (Ministry of Education)School of Electronics Engineering and Computer SciencePeking University, Beijing, 100871, China{linxj, fanyang, zhangm, wxh}@cis.pku.edu.cn, chi@pku.edu.cnAbstractThis paper proposes a novel method torefine the grammars in parsing by utiliz-ing semantic knowledge from HowNet.Based on the hierarchical state-split ap-proach, which can refine grammars au-tomatically in a data-driven manner, thisstudy introduces semantic knowledge intothe splitting process at two steps.
Firstly,each part-of-speech node will be anno-tated with a semantic tag of its termi-nal word.
These new tags generated inthis step are semantic-related, which canprovide a good start for splitting.
Sec-ondly, a knowledge-based criterion is usedto supervise the hierarchical splitting ofthese semantic-related tags, which can al-leviate overfitting.
The experiments arecarried out on both Chinese and EnglishPenn Treebank show that the refined gram-mars with semantic knowledge can im-prove parsing performance significantly.Especially with respect to Chinese, ourparser achieves an F1score of 87.5%,which is the best published result we areaware of.1 IntroductionAt present, most high-performance parsers arebased on probabilistic context-free grammars(PCFGs) in one way or another (Collins, 1999;Charniak and Johnson, 2005; Petrov and Klein,2007).
However, restricted by the strong context-free assumptions, the original PCFG model whichsimply takes the grammars and probabilities off atreebank, does not perform well.
Therefore, a va-riety of techniques have been developed to enrichand generalize the original grammar, ranging fromlexicalization to symbol annotation.
?Corresponding author: Xihong Wu.Lexicalized PCFGs use the structural featureson the lexical head of phrasal node in a tree, andget significant improvements for parsing (Collins,1997; Charniak, 1997; Collins, 1999; Charniak,2000).
However, they suffer from the problem offundamental sparseness of the lexical dependencyinformation.
(Klein and Manning, 2003).In order to deal with this limitation, a varietyof unlexicalized parsing techniques have been pro-posed.
Johnson (1998) annotates each node byits parent category in a tree, and gets significantimprovements compared with the original PCFGson the Penn Treebank.
Then, some manual andautomatic symbol splitting methods are presented,which get comparable performance with lexical-ized parsers (Klein and Manning, 2003; Matsuzakiet al, 2005).
Recently, Petrov et al (2006) in-troduces an automatic hierarchical state-split ap-proach to refine the grammars, which can alter-nately split and merge the basic nonterminals bythe Expectation-Maximization (EM) algorithm.
Inthis method, the nonterminals are split to differ-ent degrees, as appropriate to the actual complex-ity in the data.
The grammars refined in this wayare proved to be much more accurate and compactthan previous work on automatic annotation.
Thisdata-driven method still suffers from the overfit-ting problem, which may be improved by integrat-ing other external information.In this paper, we propose a novel method thatcombines the strengths of both data-driven andknowledge-driven strategies to refine grammars.Based on the work proposed by Petrov et al(2006), we use the semantic knowledge fromHowNet (Dong and Dong, 2000) to supervisethe hierarchical state-split process at the part-of-speech(POS) level.
At first, we define the mostgeneral hypernym in HowNet as the semantic classof a word, and then use this semantic class to ini-tialize the tag of each POS node.
In this way, anew set of semantic-related tags is generated, and1298a good starting annotation is provided to reducethe search space for the EM algorithm in the split-ting process.
Then, in order to mitigate the overfit-ting risk, the hierarchical hypernym-hyponym re-lation between hypernyms in HowNet is utilizedto supervise the splitting of these new semantic-related tags.
By introducing a knowledge-basedcriterion, these new tags are decided whether ornot to split into subcategories from a semantic per-spective.
To investigate the effectiveness of thepresented approach, several experiments are con-duced on both Chinese and English.
They revealthat the semantic knowledge is potentially usefulto parsing.The remainder of this paper is organized asfollows.
Section 2 reviews some closely relatedworks, including the lexical semantic related pars-ing and the hierarchical state-split unlexicalizedparsing.
In section 3, the presented method forgrammar refining is described in detail, and sev-eral experiments are carried out for evaluation inSection 4.
Conclusions are drawn in Section 5.2 BackgroundThis paper tries to refine the grammars throughan improved hierarchical state-split process in-tegrated with semantic knowledge.
The relatedworks are reviewed as follows.2.1 Lexical Semantic Related ParsingSemantic knowledge is useful to resolving syntac-tic ambiguities, and a variety of researches focuson how to utilize it.
Especially in recent years,a conviction arose that semantic knowledge couldbe incorporated into the lexicalized parsing.Based on the lexicalized grammars, Bikel(2000) attempts at combining parsing and wordsense disambiguation in a unified model, using asubset of SemCor (Miller et al, 1994).
Bikel(2000) evaluates this model in a parsing contextwith sense information from WordNet, but doesnot get improvements on parsing performance.Xiong et al (2005) combines word sense fromCiLin and HowNet (two Chinese semantic re-sources) in a generative parsing model, which gen-eralizes standard bilexical dependencies to word-class dependencies, and indeed help to tackle thesparseness problem in lexicalized parsing.
Theexperiments show that the parse model combinedwith word sense and the most special hypernymsachieves a significant improvement on Penn Chi-nese Treebank.
This work only considers the mostspecial hypernym of a word, rather than otherhypernyms at different levels of the hypernym-hyponym hierarchy.Then, Fujita et al (2007) uses the Hinoki tree-bank as training data to train a discriminative parseselection model combining syntactic features andword sense information.
Instead of utilizing themost special hypernym, the word sense informa-tion in this model is embodied with more generalconcepts.
Based on the hand-craft sense informa-tion, this model is proved to be effective for parseselection.Recently, Agirre et al (2008) train two lexical-ized models (Charniak, 2000; Bikel, 2004) on pre-processed inputs, where content words are substi-tuted with semantic classes from WordNet.
By in-tegrating the word semantic classes into the pro-cess of parser training directly, these two modelsobtain significant improvements in both parsingand prepositional phrase attachment tasks.
Zhang(2008) does preliminary work on integrating POSwith semantic class of words directly, which cannot only alleviate the confusion in parsing, but alsoinfer syntax and semantic information at the sametime.2.2 The Hierarchical State-split ParsingIn order to alleviate the context-free assumptions,Petrov et al (2006) proposes a hierarchical state-split approach to refine and generalize the orig-inal grammars, and achieves state-of-the-art per-formance.
Starting with the basic nonterminals,this method repeats the split-merge (SM) cycle toincrease the complexity of grammars.
That is, itsplits every symbol into two, and then re-mergessome new subcategories based on the likelihoodcomputation.SplittingIn each splitting stage, the previous syntactic sym-bol is split into two subcategories, and the EM al-gorithm is adopted to learn probability of the rulesfor these latent annotations to maximize the like-lihood of trees in the training data.
Finally, eachsymbol generates a series of new subcategories ina hierarchical fashion.
With this method, the split-ting strategy introduces more context information,and the refined grammars cover more linguistic in-formation which helps resolve the syntactic ambi-guities.1299However, it is worth noting that the EM algo-rithm does not guarantee a global optimal solution,and often gets stuck in a suboptimal configuration.Therefore, a good starting annotation is expectedto help alleviate this problem, as well as reduce thesearch space for EM.MergingIt is obvious that using more derived subcategoriescan increase accuracy, but the refined grammars fittighter to the training data, and may lead to over-fitting to some extent.
In addition, different sym-bols should have their specific numbers of subcat-egories.
For example, the comma POS tag shouldhave only one subcategory, as it always producesthe terminal comma.
On the contrary, the nounPOS tag and the verb POS tag are expected to havemuch more subcategories to express their contextdependencies.
Therefore, it is not reasonable tosplit them in the same way.The symbol merging stage is introduced to al-leviate this defect.
This approach splits symbolsonly where needed, and it is implemented by split-ting each symbol first and then measure the loss inlikelihood incurred when removing this subcate-gory.
If the loss is small, it means that this subcate-gory does not take enough information and shouldbe removed.
In general it is hard to decide thethreshold of the likelihood loss, and this mergingstage is often executed by removing a certain pro-portion of subcategories, as well as giving priorityto the most informative subcategories.By splitting and merging alternately, thismethod can refine the grammars step by step tomitigate the overfitting risk to some extent.
How-ever, this data-driven method can not solve thisproblem completely, and we need to find other ex-ternal information to improve it.AnalysisThe hierarchical state-split approach is used tosplit all the symbols in the same way.
Table 1 citesthe subcategories for several POS tags, along withtheir two most frequent words.
Results show thatthe words in the same subcategory of POS tags aresemantic consistent in some cases.
Therefore, itis expected to optimize the splitting and mergingprocess at the POS level with semantic knowledge.NRNR-0 ???
(Daja river) ???
(Nepal)NR-1 ??
(Sony) ???
(Bole Co.)NR-2 ??(C.
Hua) ???(T.
Wen)NR-3 ???(S.
Yue) ?
(Shang)LCLC-0 ??
(middle) ??
(right)LC-1 ??
(before) ??
(since)LC-2 ??
(start) ?
(end)LC-3 ??
(till) ?
(end)PP-0 ??
(whenever) ??
(as for)P-1 ??
(like) ??
(as)P-2 ??
(look to) ??
(according to)P-3 ?
(be close to) ??
(contrast)Table 1: The two most frequent words in the sub-categories of several POS tag.3 Integration with Semantic KnowledgeIn this paper, the semantic knowledge is used to re-fine grammars by improving the automatic hierar-chical state-split approach.
At first, in order to pro-vide good starting annotations to reduce the searchspace for the EM algorithm, we try to annotate thetag of each POS node with the most general hyper-nym of its terminal word.
In this way, we generatea new set of semantic-related tags.
And then, in-stead of splitting and merging all symbols togetherautomatically, we propose a knowledge-based cri-terion with hierarchical semantic knowledge to su-pervise the splitting of these new semantic-relatedtags.3.1 HowNetThe semantic knowledge resource we use isHowNet, which is a common sense knowledgebase unveiling concepts and inter-conceptual re-lations in Chinese and English.As a knowledge base of graph structure,HowNet is devoted to demonstrating the proper-ties of concepts through sememes and relationsbetween sememes.
Broadly speaking, a sememerefers to the smallest basic semantic unit that can-not be reduced further, which can be representedin English and their Chinese equivalents, such asthe sememe institution|??.
The relations expli-cated in HowNet include hypernym-hyponym re-lations, location-event relations, time-event rela-tions and so on.
In this work, we mainly focus on1300.vitalityis full ofThe goveronmentThe goveronment is full of.IPNP VP PUNN VV NP ???
??
NNvitalitya.?
?IPNP VP PUNN-Entity VV-Event NP ???
??
NN-Attributeb.?
?Figure 1: The two syntax trees of the sentence "The government is full of vitality".
a. is the originalsyntax tree, b. is the syntax tree in which each tag of the POS node is annotated with the most generalhypernym of its terminal word.the hypernym-hyponym relations.
Take the word??
(government) as an example, its hypernyms withthe hierarchical hypernym-hyponym relations arelisted below from speciality to generality, whichwe call hierarchical semantic information in thispaper.institution|???group|???thing|???entity|?
?It is clear that this word ??
(government) has hy-pernyms from the most special hypernym institu-tion|??
to the most general hypernym entity|?
?in a hierarchical way.In HowNet(Update 2008), there are 173535concepts, with 2085 sememes.
The sememes arecategorized into entity, event, attribute, attributevalue, etc., each corresponding to a sememe hi-erarchy tree.3.2 Annotating the Training DataOne of the original motivations for the grammarrefinement is that the original symbols, especiallythe POS tags, are usually too general to distin-guish the context dependencies.
Take the sentencein Figure 1 for example, the word ??
(government)should have different context dependencies com-pared with the word ??
(vitality), although both ofthem have the same POS tag "NN".
In fact, thetwo words are defined in HowNet with differenthypernyms.
The word ??
(government) is definedas a kind of objective things, while the word ??
(vitality) is defined as a property that is often usedto describe things.
It is obvious that the differentsenses can represent their different syntax struc-tures, and we expect to refine the POS tags withsemantic knowledge.In the automatic hierarchical state-split ap-proach introduced above, the EM algorithm isused to search for the maximum of the likelihoodduring the splitting process, which can generatesubcategories for POS tags to express the contextdependencies.
However, this method often getsstuck in a suboptimal configuration, which variesdepending on the start point.
Therefore, a goodstart of the annotations is very important.
As it isdisplayed in Figure 1, we annotate the tag of eachPOS node with the hypernym of its terminal wordas the starting annotation.
There are two problemsthat we have to consider in this process: a) how tochoose the appropriate semantic granularity, andb) how to deal with the polysemous words.As mentioned above, the semantic informationof each word can be represented as hierarchi-cal hypernym-hyponym relations among its hyper-nyms.
In general, it is hard to decide the appro-priate level of granularity to represent the word.The semantic class is only used as the starting an-notations of POS tags to reduce the search spacefor EM in our method.
It is followed by the hi-erarchical state-split process to further refine thestarting annotations based on the structural infor-mation.
If more special kinds of semantic classesare chosen, it will make the structural informationweaker.
As annotations with the special hyper-nym always defeat some of the advantage of au-tomatically latent annotations learning, we anno-tate the training data with the most general hyper-nym.
For example, as shown in Figure 1, the POStag "NN" of ??
(government) is annotated as "NN-Entity", and "NN" of ??
(energy) is annotated as"NN-Attribute".Another problem is how to deal with the polyse-mous words in HowNet.
In fact, when we choosethe most general hypernym as the word?s semantic1301??(beast)??(insect)??(banana)??(orange)??(noon)??(forenoon)??(north)??
(south)noon forenoonnorth southnoon forenoon north southbeast insect banana orangeentity| ?
?thing| ??
time| ??
direction| ?
?animal| ?
fruit| ?
?Continue Splitting...Having hyponyms...NN-Entity HowNetbeast insectbanana orangeFigure 2: A schematic figure for the hierarchical state-split process of the semantic-related tag "NN-Entity".
Each subcategory of this tag has its own word set, and corresponds to one hypernym at theappropriate level in HowNet.representation, this problem has been alleviated toa large extent.
In this paper we adopt the first senseoption as our word sense disambiguation (WSD)strategy to determine the sense of each token in-stance of a target word.
That is to say, all token in-stances of a given word are tagged with the sensethat occurs most frequently in HowNet.
In addi-tion, we keep the tag of the POS node whose ter-minal word is not defined in HowNet unchanged.3.3 Supervising the Hierarchical State-splitProcessWith the method proposed above, we can producea good starting annotation with semantic knowl-edge, which is of great use to constraining the au-tomatic splitting process.
Our parser is trained onthe good starting annotations with the automatichierarchical state-split process, and gets improve-ments compared with the original training data.However, during this process, only the most gen-eral hypernyms are used as the semantic repre-sentation of words, and the hierarchical semanticknowledge is not explored.
In addition, the auto-matic process tries to refine all symbols togetherthrough a data-driven manner, which suffers theoverfitting risk.After annotating the training data with hyper-nyms, a new set of semantic-related tags such as"NN-Entity" is produced.
We treat the refiningprocess of these semantic-related tags as the spe-cializing process of hypernym with hierarchicalsemantic knowledge.
Each subcategory of thesetags corresponds to a appropriate special level ofhypernym in the HowNet.
For example, every sub-category of "NN-Entity" could corresponds to aappropriate hyponym of entity|?
?.We integrate the hierarchical semantic knowl-edge into the original hierarchical state-split pro-cess to refine these semantic-related tags.
Firstof all, it is necessary to establish the mappingfrom each subcategory of these semantic-relatedtags to the hypernym at the appropriate level inHowNet.
Then, instead of likelihood judgment, aknowledge-based criterion is proposed, to decidewhether or not to remove the new subcategoriesof these tags.
That is to say, once the parent tagof this new subcategory is mapped onto the mostspecial hypernym without any hyponym, it shouldbe removed immediately.The schematic Figure 2 demonstrates this se-mantically supervised splitting process.
The leftpart of this figure is the subcategories of thesemantic-related tag "NN-Entity", which is splithierarchically.
As expressed by the dashed line,each subcategory corresponds to one hypernym inthe right part of this figure.
If the hypernym nodehas no hyponym, the corresponding subcategorywill stop splitting.The mapping from each subcategory of thesesemantic-related tags to the hypernym at the ap-propriate level is implemented with the word setrelated to this subcategory.
As it is shown in Fig-1302DataSetChinese EnglishXue et al (2002) Marcus et al (1993)TrainSet Art.
1-270,400-1151 Sections 2-21DevSet Articles 301-325 Section 22TestSet Articles 271-300 Section 23Table 2: Experimental setup.ure 2, the original tag "NN-Entity" treats all thewords it products as its word set.
Once the orig-inal category is split into two subcategories, itsword set is also split, through forcedly dividingeach word in the word set into one subcategorywhich is most frequent with this word.
And then,each subcategory is mapped onto the most specifichypernym that contains its related word set en-tirely in HowNet.
On this basis, a new knowledge-based criterion is introduced to enrich and gener-alize these semantic-related tags, with purpose offitting to the hierarchical semantic structure ratherthan the training data.4 ExperimentsIn this section, we designed several experiments toinvestigate the validity of refining grammars withsemantic knowledge.4.1 Experimental SetupWe did experiments on Chinese and English.
Inorder to make a fair comparison with previousworks, we split the standard corpora as shownin Table 2.
Our parsers were evaluated by theEVALB parseval reference implementation1.
TheBerkeley parser2was used to train the models withthe original automatic hierarchical state-split pro-cess.
The semantic resource we used to improveparsing was HowNet, which has been introducedin Subsection 3.1.
Statistical significance waschecked using Dan Bikel?s randomized parsingevaluation comparator with the default setting of10,000 iterations3.4.2 Semantic Representation ExperimentsFirst of all, we ran experiments with different se-mantic representation methods on Chinese.
Thepolysemous words in the training set were anno-tated with the WSD strategy of first sense option,1http://nlp.cs.nyu.edu/evalb/.2http://code.google.com/p/berkeleyparser/.3http://www.cis.upenn.edu/ dbikel/software.html.which was proved to be useful in Agirre et al(2008).As mentioned in Subsection 3.1, the semanticinformation of each word can be represented asa hierarchical relation among its hypernyms fromspecialty to generalization in HowNet.
In order tochoose the appropriate level of granularity to rep-resent words, we annotated the training set withdifferent levels of granularity as semantic repre-sentation.
In our experiments, the automatic hier-archical state-split process is used to train modelson these training sets with different level of seman-tic representation.We tried two kinds of semantic representations,one is using the most general hypernym, and theother is using the most special hypernym.
Resultsin Figure 3 proved the effectiveness of our methodin Subsection 3.2.
When we annotated the tag ofeach POS node with the most general hypernym ofits terminal word, the parser performs much bet-ter than both the baseline and the one annotatedwith the most special hypernym.
Moreover, the F1score starts dropping after 3 training iterations onthe training set annotated with the most special hy-pernyms, while it is still improving with the mostgeneral one, indicating overfitting.1 2 3 46870727476788082848688Parsingaccuracy(F1)Times of split-merge iterationBaselineMost Special HypernymMost General HypernymFigure 3: Performances on Chinese with differentsemantic representations: the training set withoutsemantic representation, the training set annotatedwith the most special hypernyms, and the trainingset annotated with the most general hypernyms.When the training set was annotated with themost general hypernyms, there were only 57 newsemantic-related tags such as "NN-Entity", "NN-Attribute" and so on.
However, when the train-ing set was annotated with the most special hyper-nyms, 4313 new tags would be introduced.
Ob-1303viously, it introduces too many tags at once and isdifficult to refine appropriate grammars in the sub-sequent step starting with this over-splitting train-ing set.4.3 Grammar Refinement ExperimentsSeveral experiments were carried out on Chineseand English to verify the effectiveness of refininggrammars with semantic knowledge.
We took themost general hypernym as the semantic represen-tation, and the polysemous words in the trainingset were annotated with the WSD strategy of firstsense option.In our experiments, three kinds of method werecompared.
The baseline was trained on the rawtraining set with the automatic hierarchical state-split approach.
Then, we improved it with the se-mantic annotation, which annotated the raw train-ing set with the most general hypernyms as se-mantic representations, while keeping the train-ing approach used in the baseline unchanged.Further, our knowledge-based criterion was in-troduced to supervise the automatic hierarchicalstate-split process with semantic knowledge.In this section, since most of the parsers (includ-ing the baseline parser and our advanced parsers)had the same behavior on development set that theaccuracy continued increasing in the five begin-ning iterations and then dropped at the sixth iter-ation, we chose the results at the fifth iteration asour final test set parsing performance.Performances on ChineseFigure 4 shows that refining grammars with se-mantic knowledge can help improve parsing per-formance significantly on Chinese (sentences oflength 40 or less).
Benefitting from the good start-ing annotations, our parser achieved significantimprovements compared with the baseline (86.8%vs.
86.1%, p<.08).
It proved that the good start-ing annotations with semantic knowledge were ef-fective in the splitting process.
Further, we su-pervised the splitting of the new semantic-relatedtags from the semantic annotations, and achievedthe best results at the fifth iteration.
The best F1score reached 87.5%, with an error rate reductionof 10.1%, relative to the baseline (p<.004).Table 3 compared our methods with the bestprevious works on Chinese.
The result showedthat refining grammars integrated with semanticknowledge could resolve syntactic ambiguities re-LP LR F1828384858687888990919287.586.886.186.085.784.988.988.0Evaluation CriterionBaselineSemantic AnnotationSemantic Annotation & Knowledge-based Criterion87.3Figure 4: Performances at the fifth iteration onChinese (sentences of length 40 or less) with threemethods: the baseline, the parser trained on thesemantic annotations with automatic method, andthe parser trained on the semantic annotations withknowledge-based criterion.Parser?
40 words allLP LR F1LP LR F1Chiang and81.1 78.8 79.9 78.0 75.2 76.6Bikel (2002)Petrov and86.9 85.7 86.3 84.8 81.9 83.3Klein (2007)This Paper 88.9 86.0 87.5 86.0 83.1 84.5Table 3: Our final parsing performance comparedwith the best previous works on Chinese.markably and achieved the state-of-the-art perfor-mance on Chinese.Performances on EnglishIn order to verify the effectiveness of our methodon other languages, we carried out some experi-ments on English.
HowNet is a common senseknowledge base in Chinese and English, there-fore, it was still utilized as the knowledge sourcein these experiments.The same three methods were compared on En-glish (sentences of length 40 or less), and the re-sults were showed in Table 4.
Compared with thebaseline (90.1%), the parsers trained with the se-mantic annotation, while using different splittingmethods introduced in Section 3, achieved an F1score of 90.2% and 90.3% respectively.
The re-sults showed that our methods could get a smallbut stable improvements on English (p<.08).1304Subcategory Refined from the Original Training SetPN-0??
(aid foreign),??(aunt),??(self),?(you),?(donate),??(those),??(appearence),???(self),??(we),?(that),??
(the above),??(there),??(other),??
(below)Subcategories Fefined from the Good Starting AnnotationsPN-0 ??(aunt),??(self),???(self),??(we),?
(you)PN-Event-0 ??
(aid foreign),?
(donate)PN-AttributeValue-2 ??
(the above),??(those),?(that),??(other),??
(below)Table 5: Several subcategories that generated from the original training set and the good starting annota-tions respectively.Method F1Baseline 90.1Semantic Annotations 90.2Semantic Annotations &90.3Knowledge-based CriterionTable 4: Performances at the fifth iteration on En-glish (sentences of length 40 or less) with threemethods: the baseline, the parser trained on thesemantic annotations with automatic method, andthe parser trained on the semantic annotations withknowledge-based criterion.These results on English were preliminary, andwe did not introduce any language dependent op-eration such as morphological processing.
Sinceonly the lemma of English words can be foundin HowNet, we just annotated two kinds of POStags "VB"(Verb, base form) and "NN"(Noun, sin-gular or mass) with semantic knowledge, on thecontrary, we annotated almost all POS tags whosecorresponding words could be found in HowNeton Chinese.
This might be the reason that theimprovement on the English Treebank was muchsmaller than that of Chinese.
It is expected toachieve more improvements through some mor-phological analysis in the future.4.4 Results and AnalysisSo far, a new strategy has been introduced to re-fine the grammars in two steps, and achieved sig-nificant improvements on parsing performance.
Inthis section, we analyze the grammars learned atdifferent steps, attempting to explain how the se-mantic knowledge works.It is hard to inspect all the grammars by hand.Since the semantic knowledge is mainly used forgenerating and splitting new semantic-related tagsin our method, we focus on the refined subcate-gories of these tags.First, we examine the refined subcategories ofPOS tags, which are generated from the originaltraining set and the good starting annotations re-spectively.
Several subcategories are listed andcompared in Table 5, along with their frequentwords.
It can be seen that the subcategories refinedwith semantic knowledge are more consistent thanthe previous one.
For example, the subcategory"PN-0", which is refined from the original trainingset, produces a lot of words without semantic con-sistence.
In contrast, we refine the subcategories"PN-0", "PN-Event-0" and "PN-AttributeValue-2"from the good starting annotations.
Each of themproduces a small but semantic consistent word set.In order to inspect the difference between theautomatic splitting process and the semantic basedone, we compare the numbers of subcategories re-fined in these two processes.
Since it is hard to listall the semantic-related tags here, three parts ofthe semantic-related tags were selected and listedin Table 6, along with the number of their subcat-egories.
The first part is the noun and verb relatedtags, which are most heavily split in both two pro-cesses.
It is clear that the semantic based split-ting process can generate more subcategories thanthe automatic one, because the semantic structuresof noun and verb are sophisticated.
The secondpart lists the tags that have much more subcate-gories (?
4) from the automatic splitting processthan the semantic based one, and the third partvice verse.
It can be seen that most of the sub-categories in the second part are functional cate-gories, while most of the subcategories in the thirdpart are content categories.
It means that the se-mantic based splitting process is prone to generat-ing less subcategory for the functional categories,but more subcategories for the content categories.This tendency is in accordance with the linguis-tic intuition.
We believe that it is the main effect1305Semantic-relatedAutomatic split Semantic basedtag number split numebrNN-Attribute 30 30NN-AttributeValue 25 27NN-Entity 32 32NN-Event 31 30VV-Attribute 2 2VV-AttributeValue 27 27VV-Entity 22 26VV-Event 29 32BA-event 13 5CS-AttributeValue 29 16CS-entity 22 15OD-Attribute 13 7PN-Attribute 26 22AS-AttributeValue 2 7JJ-event 4 8NR-AttributeValue 9 13NT-event 12 18VA-AttributeValue 22 27VA-event 7 11Table 6: The number of subcategories learnedfrom two approaches: the automatic hierarchicalstate-splitting, and the semantic based splitting.of our knowledge-based criterion, because it ad-justs the splitting results dynamically with seman-tic knowledge, which can alleviate the overfittingrisk.5 ConclusionsIn this paper, we present a novel approach to in-tegrate semantic knowledge into the hierarchicalstate-split process for grammar refinement, whichyields better accuracies on Chinese than previ-ous methods.
The improvements are mainly ow-ing to two aspects.
Firstly, the original treebankis initialized by annotating the tag of each POSnode with the most general hypernym of its ter-minal word, which reduces the search space forthe EM algorithm and brings an initial restrict tothe following splitting step.
Secondly, the splittingprocess is supervised by a knowledge-based crite-rion with the new semantic-related tags.
Benefit-ting from the hierarchical semantic knowledge, theproposed approach alleviates the overfitting risk ina knowledge-driven manner.
Experimental resultsreveal that the semantic knowledge is of great useto syntactic disambiguation.
The further analysison the refined grammars shows that, our methodtends to split the content categories more oftenthan the baseline method and the function classesless often.AcknowledgmentsWe thank Yaozhong Zhang for the enlighten-ing discussions.
We also thank the anony-mous reviewers who gave very helpful com-ments.
The work was supported in part by theNational Natural Science Foundation of China(60535030; 60605016), the National High Tech-nology Research and Development Program ofChina (2006AA010103), the National Key Ba-sic Research Program of China (2004CB318005,2004CB318105).ReferencesE.
Agirre, T. Baldwin and D. Martinez.
2008.
Improv-ing parsing and PP attachment performance withsense information.
In Proc.
of ACL?08, pages 317-325.D.
Bikel.
2000.
A statistical model for pars-ing and word-sense disambiguation.
In Proc.
ofEMNLP/VLC?2000, pages 155-163.D.
Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479-511.?E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
ofAAAI?97, pages 598-603.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
of NAACL?00, pages 132-139.E Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxEnt discriminative reranking.In Proc.
of ACL?05, pages 173-180.D.
Chiang and D. Bikel.
2002.
Recovering latent infor-mation in treebanks.
In Proc.
of COLING?02, pages183-189.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proc.
of ACL?97, pages 16-23.M.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
Ph.D. thesis, U. of Penn-sylvania.Z.
Dong and Q. Dong.
2000.
HowNet Chinese-English conceptual database.
Technical Re-port Online Software Database, Released at ACL.http://www.keenage.com.1306S.
Fujita, F. Bond, S. Oepen and T. Tanaka 2007.
Ex-ploiting semantic information for HPSG parse se-lection.
In ACL 2007 Workshop on Deep LinguisticProcessing, pages 25-32.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24(4):613-631.D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Proc.
of ACL?03, pages 423-430.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313-330.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Prob-abilistic CFG with latent annotations.
In Proc.
ofACL?05, pages 75-82.George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G. Thomas.
1994.
Us-ing a semantic concordance for sense identification.In Proc.
of ARPA-HLT Workshop., pages 240-243.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proc.
of COLING-ACL?06, pages443?440.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of HLT-NAACL?07,pages 404-411.D.
Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian.
2005.Parsing the Penn Chinese treebank with semanticknowledge.
In Proc.
of IJCNLP?05, pages 70-81.N.
Xue, F.-D. Chiou, and M. Palmer.
2002.
Buildinga large scale annotated Chinese corpus.
In Proc.
ofCOLING?02, pages 1-8.Y.
Zhang.
2008.
The Study and Realization of ChineseParsing with Semantic and Sentence Type Informa-tion.
Master thesis, Peking University.1307
