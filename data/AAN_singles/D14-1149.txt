Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1426?1434,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsExtracting Clusters of Specialist Terms from Unstructured TextAaron GerowComputation InstituteUniversity of ChicagoChicago, IL, USAgerow@uchicago.eduAbstractAutomatically identifying related special-ist terms is a difficult and important taskrequired to understand the lexical struc-ture of language.
This paper developsa corpus-based method of extracting co-herent clusters of satellite terminology ?terms on the edge of the lexicon ?
us-ing co-occurrence networks of unstruc-tured text.
Term clusters are identi-fied by extracting communities in the co-occurrence graph, after which the largestis discarded and the remaining words areranked by centrality within a community.The method is tractable on large corpora,requires no document structure and min-imal normalization.
The results suggestthat the model is able to extract coher-ent groups of satellite terms in corporawith varying size, content and structure.The findings also confirm that languageconsists of a densely connected core (ob-served in dictionaries) and systematic, se-mantically coherent groups of terms at theedges of the lexicon.1 IntroductionNatural language consists of a number of rela-tional structures, many of which can be obscuredby lexical idiosyncrasies, regional variation anddomain-specific conventions.
Despite this, pat-terns of word use exhibit loose semantic struc-ture, namely that proximate words tend to be re-lated.
This distributional hypothesis has been op-erationalized in a variety of ways, providing in-sights and solutions into practical and theoreticalquestions about meaning, intention and the use oflanguage.
Distributional analyses rely primarilyon observing natural language to build statisticalrepresentations of words, phrases and documents(Turney and Pantel, 2010).
By studying dictio-naries and thesauri, lexicographic and terminolog-ical research has proposed that a core lexicon isused to define the remaining portions of vocabu-lary (It?o and Mester, 1995; Mass?e et al., 2008).Though many words that comprise general lan-guage use reside in this core lexicon, even themost general language contains specialist or so-called ?satellite?
words.
This paper introduces amethod of extracting this peripheral structure, withco-occurrence networks of unstructured text.The core-periphery structure has been observedin dictionaries where definitions tend to use a re-stricted vocabulary, repetitively employing a coreset of words to define others (Sinclair, 1996; Pi-card et al., 2013).
In the farther regions of the lex-icon, it is more difficult to find systematic seman-tic definition with corpus-based techniques due tothe overwhelming number of infrequent words.Unfortunately, the fringe of the lexicon can bemore important than the core because this is wheredomain-specific terminology resides ?
featuresthat may be more important than frequent.Examining dictionaries, (Picard et al., 2013)propose that the lexicon consists of four mainparts: a core set of ubiquitous words used to de-fine other words, a kernel that makes up most ofthe lexicon, a minimal grounding set that includesmost of the core and some of the kernel, leav-ing a set of satellites in the periphery.
This to-pography, reproduced in Figure 1, has been foundin the way dictionary entries use words to defineone another.
In networks of dictionary defini-tions, the core component tends to form a stronglyconnected component (SCC) leaving satellites insmaller SCCs with relatively weak links to thecore.
This paper explores whether these thesesatellites form systematic, cohesive groups andwhether they are observable in natural language.1426Figure 1: Dictionary studies have proposed thatthe lexicon consists of a strongly connected core,around which there is a kernel, an asymmetricgrounding set and satellites.
Adapted from (Picardet al., 2013).Words with relatively specific definitions withinsubjects, referred to as terms in lexicographic re-search, are apparent in nearly all domains of dis-course.
Here, the goal is to explore structureamong these peripheral terms without a dictio-nary.
To do this, a method based on commu-nity detection in textual co-occurrence networksis developed.
Such graph-based methods have be-come increasingly popular in a range of language-related tasks such as word clustering, documentclustering, semantic memory, anaphora resolutionand dependency parsing (see Mihalcea and Radev,2011 for a review).This paper seeks to address two important ques-tions about the observed landscape of the lexiconin natural language: to investigate whether satel-lite clusters found in dictionaries can be observedin text, and more importantly, to explore whetherstatistical information in co-occurrence networkscan elucidate this peripheral structure in the lexi-con.
We frame these questions as the task of ex-tracting clusters of related terms.
If satellites aresystematically organized, then we can expect tofind cohesive clusters in this region.
Moreover, ifthe networked structure of dictionary entries sup-ports the landscape in Figure 1, a similar structuremay be present in co-occurrence patterns in natu-ral text.2 MethodWord clustering, as a means to explore underly-ing lexical structure, should accommodate fuzzyand potentially contradictory notions of similarity.For example, red and green are at once similar,being colors, but as colors, they are very differ-ent.
Alternatively, the words car, fast, wheel, ex-port and motorists share a thematic similarity intheir relation to automobiles.
One conception ofword clustering is to construct a thesaurus of syn-onyms (Calvo et al., 2005), but clustering couldallow other lexical semantic relationships.
Onesuch database, WordNet, defines specific seman-tic relationships and has been used to group wordsaccording to explicit measures of relatedness andsimilarity (Miller, 1995; Pedersen et al., 2004).Distributional, corpus-based techniques that de-fine words as feature vectors (eg.
word-documentco-occurrences), can address many limitations ofmanually created lexicons (see Turney et al., 2010for a review).
Clustering nouns by argument struc-ture can uncover naturally related objects (Hin-dle, 1990) and spectral methods can relate dis-tinct classes of nouns with certain kinds of verbsto induce selectional preferences (Resnik, 1997;Sun and Korhonen, 2009; Wilks, 1975) and assistmetaphor processing (Shutova et al., 2013).A pervasive weakness of many existing ap-proaches to word-clustering, is an underlying pri-oritization of frequent words.
To help addressthis sparsity, many models collapse words intostems, preclude uncommon words, or underesti-mate the relevance of infrequent words (Daganet al., 1999).
Probabilistic topic models haveemerged as a uniquely flexible kind of word-clustering used in content analysis (Steyvers andGriffiths, 2007), text classification (Wei and Croft,2006) and provide an extensible framework to ad-dress other tasks (Blei, 2012).
Because the struc-ture of satellite terms is not likely to rely on spe-cific (much less consistent) lexical semantic re-lationships, we adopt a measure of semantic co-herence, commonly used to qualify the results oftopic models, as an indirect measure of what peo-ple tend to view as a cohesive set of words.
Thismeasure, which is defined in the next section, isparticularly attractive because it is corpus-based,does not assume any specific semantic relationshipand correlates with expert evaluations (Mimno etal., 2011; Newman et al., 2010).
Using semanticcoherence provides a way of measuring the qual-1427ity of word-associations without appeal to a dic-tionary or assuming rigid relationships among theclustered words.The first step is to construct a co-occurrencegraph from which communities are extracted.Then the centrality of each word is computedwithin a community to generate cluster-specificrankings.
The goal is not to categorize wordsinto classes, nor to provide partitions that sepa-rate associated words across a corpus.
Instead, themethod is designed to extract qualifiable sets ofspecialist terms found in arbitrary text.
Crucially,the method is designed to require no documentstructure and minimal pre-processing: stop-wordsand non-words are not removed and no phrasal,sentence or document structure is required.
Al-though stemming or lemmatization could pro-vide more stream-lined interpretations, the mini-mal pre-processing allows the method to operateefficiently on large amounts of unstructured textof any language.Co-occurrence networks have been used in avariety of NLP applications, the basic idea be-ing to construct a graph where proximate wordsare connected.
Typically, words are connectedif they are observed in an n-word window.
Weset this window to a symmetric 7 words on ei-ther side of the target and did not use any weight-ing1.
In the resulting network, edge frequen-cies are set to the number of times the given co-occurrence is observed.
The resulting networksare typically quite dense and exhibit small-worldstructure where most word-pairs are only a fewedges apart (Baronchelli et al., 2013; Ferror i Can-cho and Sol?e, 2001).
To explore the effect ofthis density, different minimum node- and edge-frequencies were tested (analogous to the word-and co-occurrence frequencies in text).
It wasfound that not setting any thresholds provided thebest results (see Figure 2), supporting our minimalpre-processing approach.To extract clusters from the co-occurrence ma-trix, the Infomap community detection algorithmwas used.
Infomap is an information-theoreticmethod that optimizes a compression dictionaryusing it to describe flow through connected nodes(Rosvall and Bergstrom, 2008).
By minimizing adescription of this flow, the algorithm can also ex-tract nested communities (Rosvall and Bergstrom,17 was found to be the optimal window-size in terms ofcoherence.
These preliminary results are available at knowl-edgelab.org/docs/coherent clusters-data.xls.Corpus Docs Tokens Nodes EdgesTASA 38,972 10.7M 58,357 1,319,534NIPS 3,742 5.2M 28,936 1,612,659enTenTen 92,327 72.2M 69,745 7,721,413Table 1: Co-occurrence networks of each corpus.2011).
In our experiments, we used the co-occurrence frequencies as edge-weights and ran50 trials for each run of the algorithm.
Co-occurrence networks tended to form one mono-lithic community, corresponding to the lexicon?score SCC, surrounded by a number of smallercommunities.
The monolithic community is dis-carded out-right, as it represents the core of thelexicon where few specialist terms reside.
As wewill see, the community detection algorithm nat-urally identifies this SCC, distinguishing satelliteclusters of terminology.
Though we do not exploreits effect, the sensitivity of Infomap can be tunedto vary the relative size of the core SCC comparedto the satellites, effectively allowing less modularcommunities to be considered satellites.To compare and interpret the resulting clus-ters, various measures of centrality were tested forranking words within their communities.
The goalof this ranking is to find words that typify or de-fine their community without assuming its under-lying semantics.
The results in the next sectionshow that a number of common centrality mea-sures work comparably well for this task.
The fi-nal output of the system is a set of communities,in which words are ranked by their centrality.3 Results & AnalysisThree corpora were used for evaluation: theTASA, NIPS and enTenTen collections.
TASAconsists of paragraph-length excerpts from high-school level, American English texts (Landauerand Dumais, 1997).
The NIPS collection contains17 volumes of annual proceedings from the con-ference of the same name.
The enTenTen corpusis a web-based collection of text-heavy, Englishweb-sites.
Table 1 summarizes the collections andtheir co-occurrence networks.The extracted communities, which consist ofword-centrality pairs, are similarly structured tothe output of topic models.
Because appeals tohuman judgement are expensive and can introduceissues of consistency (Chang et al., 2009; Hu et al.,2011), a corpus-based measure of semantic coher-ence has been proposed (Mimno et al., 2011).
Co-1428herence is used as a proxy for human judgments.A general form of semantic coherence can be de-fined as the mean pair-wise similarity over the topn words in a topic or cluster tC(t) =1nn?
(wi,wj)?ti<jS(wi, wj)where S is a symmetric measure of similarity.Newman, et al.
(2010) surveyed a number of simi-larity metrics and found that mean point-wise mu-tual information (PMI) correlated best to humanjudgements.
PMI is a commonly used measure ofhow much more information co-occurring wordsconvey together compared to their independentcontributions (Church and Hanks, 1990; Bouma,2009).
Using PMI as S, we can define a version ofcoherence, known as UCI Coherence:CUCI(t) =1nn?
(wi,wj)?ti<jlogp(wi, wj)p(wi)p(wj)where p(w) is estimated as relative frequency ina corpus:f(w)?if(wi).
Using coherence to optimizetopic models, Mimno et al.
(2011) found that asimplified measure, termed UMass Coherence, ismore strongly correlated to human judgments thanCUCI.
For topic t, CUMassis defined as follows:CUMass(t) =1nn?
(wi,wj)?ti<jlogD(wi, wj) + 1D(wj)where D(w) is the number of documents con-taining w, and D(w,w?)
is the number of doc-uments containing both w and w?.
Note that Drelies crucially on document segmentation in thereference corpus, which is not encoded in the co-occurrence networks derived by the method de-scribed above.
Thus, though the networks be-ing analyzed and the coherence scores are bothbased on co-occurrence information, they are dis-tinct from one another.
Following convention,we compute coherence for the top 10 words in agiven community.
CUMasswas used as the mea-sure of semantic coherence.
and D was computedover the TASA corpus, which means the resultingscores are not directly comparable to (Mimno etal., 2011), though comparisons to other publishedresults are provided below.3.1 Ranking Functions & FrequencyThresholdsAfter communities are extracted from the co-occurrence graph, words are ranked by their cen-trality in a community.
Six centrality measureswere tested as ranking functions: degree centrality,closeness centrality, eigenvector centrality, Page-rank, hub-score and authority-score (Friedl et al.,2010).
Degree centrality uses a node?s degreeas its centrality under the assumption that highlyconnected nodes are central.
Closeness centralitymeasures the average distance between a node andall other nodes, promoting nodes that are ?close?to the rest of the network.
Eigenvector centralityfavors well-connected nodes that are themselvesconnected to well-connected nodes.
Pagerank issimilar to eigenvector centrality, but also promotesnodes that mediate connections between stronglyconnected nodes.
Hub and authority scores mea-sure interconnectedness (hubs) and connectednessto interconnected nodes (authorities).
Figure 2shows the average coherence, across all commu-nities extracted from the TASA corpus, for eachcentrality measure.
The average coherence scoresare highest using hub-score, though not signifi-cantly better than auth-score, eigenvector central-ity or closeness centrality.
In the results that fol-low, hub-scores were used to rank nodes withincommunities.Figure 2: Mean coherence for six centrality mea-sures.
Error-bars are ?2 SE of the mean.1429Imposing minimum node and edge frequenciesin the co-occurrence graph was also tested.
How-ever, applying no thresholds provided the high-est average coherence.
Figure 3 shows the aver-age coherence for eight threshold configurations.Though we used the TASA corpus for these tests,we have no reason to believe the results would dif-fer significantly for the other corpora.Figure 3: Mean coherence for different mini-mum node and edge frequencies, correspondingto thresholds for word and co-occurrence counts.Error-bars are ?2 SE of the mean.3.2 Community CoherenceTable 2 shows three communities of specialistterms extracted from each text collection, withtheir normalized hub-scores.
Normalizing thescores preserves their rank-ordering and providesan indication of relative centrality within the com-munity itself.
For example, compare the firstand last words from the top TASA and NIPSclusters: the difference between thou and craven(TASA) is considerably more than model and net-work (NIPS).
In general, higher ranked words ap-pear to typify their communities, with words likemodel, university and nuclear in the NIPS ex-amples.
These clusters are typical of those pro-duced by the method, though in some cases, thecommunities contain less than 10 terms and werenot included in the coherence analysis.
Note thatthese clusters are not systematic in any lexical se-mantic sense, though in almost every case thereare discernible thematic relations (middle-Englishwords, Latin America and seafood in TASA).TASA NIPS enTenTenthou 1.00 model 1.00 cortex 1.00shalt 0.72 learning 0.99 prefrontal 0.88hast 0.49 data 0.96 anterior 0.41thyself 0.26 neural 0.94 cingulate 0.33dost 0.24 using 0.85 medulla 0.28wilt 0.24 network 0.85 parietal 0.13canst 0.12 training 0.73 insula 0.13knowest 0.10 algorithm 0.66 cruciate 0.11mayest 0.10 function 0.63 striatum 0.11craven 0.01 networks 0.62 ventral 0.10peru 1.00 university 1.00 pradesh 1.00ecuador 0.84 science 0.85 andhra 0.67bolivia 0.80 computer 0.83 madhya 0.56argentina 0.67 department 0.74 uttar 0.50paraguay 0.54 engineering 0.30 bihar 0.21chile 0.52 report 0.30 rajasthan 0.19venezuela 0.48 technical 0.29 maharashtra 0.16uruguay 0.28 institute 0.26 haryana 0.12lima 0.17 abstract 0.25 himachal 0.10parana 0.11 california 0.23 arunachal 0.04clams 1.00 nuclear 1.00 cilia 1.00crabs 0.87 weapons 0.66 peristomal 0.73oysters 0.87 race 0.57 stalk 0.62crab 0.67 countries 0.40 trochal 0.51lobsters 0.66 rights 0.37 vorticella 0.35shrimp 0.62 india 0.27 campanella 0.32hermit 0.50 russia 0.26 hairlike 0.17mussels 0.27 philippines 0.26 swimmers 0.15lice 0.23 brazil 0.25 epistylis 0.12scallops 0.20 waste 0.22 telotroch 0.11Table 2: Sample clusters from the TASA, NIPSand enTenTen collections.
Shown are the clusters?top ten words, ranked by their normalized hub-score within the community.
Note the differencesin hub-score distributions between clusters.Figure 4 shows the average coherence for ourmethod, compared to that of a 20-topic latentDirichlet allocation (LDA) model fit to the samecorpora.
Results from an LDA model fit to ourcorpora, as well as from a sample of publishedtopics, are provided as a baseline to calibrate read-ers?
intuitions about coherence2.
Although topicsfrom LDA do not necessarily consist of special-ist terms those in the current model, the expec-tation of coherence remains: probable or centralwords should comprise a cohesive group.
In everycase, coherence is calculated over the top 10 wordsranked using within-community hub-scores, forevery community of 10 or more words.
The resultsshow that LDA provides relatively consistent co-herence across collections, though with generallymore variance than the communities of specialistterms.
The term clusters are more coherent for theenTenTen collection than the others, which may2Coherence was computed for the published results withCUMassusing TASA as the reference corpus.1430be due to its larger size.
This up-tick on the largestcorpus may have to do with the proportional sizeof the monolithic community for the less struc-tured documents in enTenTen.
Figure 5 depictshow the proportional size of the core would effectthe number and size of satellite clusters.
It wasfound that the largest community (the core SCC)comprised 95% of TASA, 90% of NIPS and 97%of enTenTen.
It may be that specialized languagewill have a proportionally smaller core and moresatellite communities, whereas more general lan-guage will have a larger core and fewer satellites.A critical question remains as to whether themethod is actually observing the core-peripherystructure of the lexicon or if it is an artifact.
Totest this, the frequencies of words in satellite com-munities were compared to those in the monolithiccases.
If the monolithic community does indeedcorrespond to the core proposed in Figure 1, wordsin the satellites should have significantly lower fre-quencies.
Indeed, the monolithic community inevery corpus contained words that were signifi-cantly more frequent than those in the communi-ties (Wilcoxon rank-sum test; Table 3).
Taken withFigure 4: Mean coherence (CUMass) for satelliteclusters and topics from LDA on the TASA, NIPSand enTenTen collections (top).
Also shown arethe mean coherence of topics found in publishedmodels (LDA, a dynamic topic model, DTM and acorrelated topic model, CTM; bottom).
Error-barsare ?2 SE of the mean.the coherence scores, these results show that thereis coherent structure in the periphery of the lexi-con, that can be extracted from unstructured text.Figure 5: A proportionally larger core SCC (right)would force satellite communities to be smaller,less numerous and more isolated.
Alternatively,with a small core (left), satellite communitieswould be more numerous and prominent.Corpus mean fcmean fsW dfTASA 112.3 7.3 39985454 40895NIPS 211.5 10.7 28342663 25077enTenTen 365.1 15.9 246095083 72695Table 3: Comparison of frequency for core words,fc, found in the monolithic community and spe-cialist terms, fs, found in the satellite communities(Wilcoxon rank-sum test).
All differences weresignificant at p < 0.001.4 DiscussionThe results of our method show that outlyingstructure in the lexicon can be extracted directlyfrom large collections of unstructured text.
Thelexicon?s topography, previously explored in dic-tionary studies, contains modular groups of satel-lite terms that are observable without appeal to ex-ternal resources or document structure and withminimal normalization.
The contribution of thismethod is two-fold: it confirms the structure of theobserved lexicon is similar to that apparent in theorganization of dictionaries (Picard et al., 2013).Second, it offers a tractable, reliable means of ex-tracting and summarizing structure in the fringesof the lexicon.The output of the model developed here is sim-ilar to topic models, but with some importantdifferences.
Topic models produce a probabilitydistribution over words to define a topic, whichcan be summarized by the top 10 to 20 mostlikely words.
Instead of probabilities, the within-1431community hub-scores were used to rank words ineach cluster.
This means that the actual structureof the community (to which topics have no ana-logue) is responsible for producing the scores thatrate words?
internal relevance.
Another crucialdifference is that topic size from a single samplingiteration tends to correlate with coherence (Mimnoet al., 2011), but in the current method, there isno correlation between cluster size and coherence(p = 0.98).
The other important difference is thatwhereas topic models produce a topic-documentmixture that can be used for posterior inference, toperform such inference with our method, the out-put would have to be used indirectly.One understated strength of the communitydetection method is the minimal required pre-processing.
Whereas many solutions in NLP(including topic models) require document seg-mentation, lexical normalization and statisticalnormalizations on the co-occurrence matrix it-self, the only variable in our method is the co-occurrence window size.
However, lemmatiza-tion (or stemming) could help collapse morpho-syntactic variation among terms in the results,but stop-word removal, sentence segmentation andTF-IDF weighting appear unnecessary.
Whatmight be most surprising given the examples in Ta-ble 2 is that word-document occurrence informa-tion is not used at all.
This makes the the methodparticularly useful for large collections with littleto no structure.One question overlooked in our analysis con-cerns the effect the core has on the satellites.
Itcould be that the proportional size of a collection?score is indicative of the degree of specialist ter-minology contained in the collection.
Also, theraw number of satellite communities might indi-cate the level of diversity in a corpus.
Addressingthese questions could yield measures of previouslyvague and latent variables like specialty or topi-cal diversity, without employing a direct semanticanalysis.
By measuring a collection?s core size,relative to its satellites, one could also use mea-sure changes in specialization.
The Infomap algo-rithm could accommodate such an experiment: byvarying the threshold of density that constitutes acommunity, the core could be made smaller, yield-ing more satellites, the coherence of which couldbe compared to those reported here.
One could ex-amine the position of individual words in the satel-lite(s) to explore what features signal important,emerging and dying terms or to track diachronicmovement of terms like computer or gene from thespecialized periphery to core of the lexicon.At the level of inter-related term clusters, thereare likely important or central groups that influ-ence other satellites.
There is no agreed upon mea-sure of ?community centrality?
in a network sense(Eaton and Mansbach, 2012).
One way to measurethe importance of a community would be to usesignificance testing on the internal link mass com-pared to the external (Csardi and Nepusz, 2006).However, this approach discards some factors forwhich one might want to account, such as cen-trality in the network of communities and theircomposition.
Future work could seek to com-bine graph-theoretic notions of centrality and in-tuitions about the defining features of term clus-ters.
Another avenue for future research wouldbe to use mixed membership community detection(Gopalan and Blei, 2013).
Allowing terms to berepresented in more than one community wouldaccommodate words like nuclear, that might befound relating to weaponry, energy production andphysics research at the same time.
Using co-occurrence networks to extract clusters of special-ist terms, though an important task, is perhaps onlya starting point for exploring the observed lexicon.Network-based analysis of language offers a gen-eral and powerful potential to address a range ofquestions about the lexicon, other NLP tasks andlanguage more generally.AcknowledgmentsThanks to E. Duede and three reviewers for com-ments on earlier versions of this manuscript.
Thiswork was supported by a grant from the TempletonFoundation to the Metaknowledge Research Net-work and grant #1158803 from the National Sci-ence Foundation.ReferencesAndrea Baronchelli, Ramon Ferrer i Cancho, Ro-mualdo Pastor-Satorras, Nick Chater, and Morten H.Christiansen.
2013.
Networks in cognitive science.Trends in cognitive sciences, 17(7):348?360.David M. Blei.
2012.
Probabilistic topic models.Communications of the ACM, 55(4):77?84.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
Proceedingsof the German Society for Computational Linguis-tics & Language Technology, pages 31?40.1432Hiram Calvo, Alexander Gelbukh, and Adam Kilgar-riff.
2005.
Distributional thesaurus versus word-net: A comparison of backoff techniques for unsu-pervised pp attachment.
In Computational Linguis-tics and Intelligent Text Processing, pages 177?188.Springer.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNeural Information Processing Systems, volume 22,pages 288?296.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Gabor Csardi and Tamas Nepusz.
2006.
The igraphsoftware package for complex network research.
In-terJournal, Complex Systems:1695.Ido Dagan, Lillian Lee, and Fernando C.N.
Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
Machine Learning, 34(1-3):43?69.Eric Eaton and Rachael Mansbach.
2012.
A spin-glassmodel for semi-supervised community detection.
InAssociation for the Advancement of Artificial Intel-ligence.Ramon Ferror i Cancho and Richard V. Sol?e.
2001.The small world of human language.
Proceedingsof the Royal Society of London.
Series B: BiologicalSciences, 268(1482):2261?2265.Dipl-Math Bettina Friedl, Julia Heidemann, et al.2010.
A critical review of centrality measures insocial networks.
Business & Information SystemsEngineering, 2(6):371?385.Prem K. Gopalan and David M. Blei.
2013.
Efficientdiscovery of overlapping communities in massivenetworks.
Proceedings of the National Academy ofSciences, 110(36):14534?14539.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In Proceedings of the28th annual meeting on Association for Computa-tional Linguistics, pages 268?275.Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-noff.
2011.
Interactive topic modeling.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 248?257.Junko It?o and Armin Mester.
1995.
The core-peripherystructure of the lexicon and constraints on rerank-ing.
University of Massachusetts occasional papersin linguistics, 18:181?209.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,104(2):211.A.
Blondin Mass?e, Guillaume Chicoisne, Yassine Gar-gouri, Stevan Harnad, Olivier Picard, and OdileMarcotte.
2008.
How is meaning grounded indictionary definitions?
In Proceedings of the 3rdTextgraphs Workshop on Graph-Based Algorithmsfor Natural Language Processing, pages 17?24.Rada Mihalcea and Dragomir Radev.
2011.
Graph-based natural language processing and informationretrieval.
Cambridge University Press.George A. Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 262?272.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation oftopic coherence.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 100?108.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring the re-latedness of concepts.
In Demonstration Papers atHLT-NAACL 2004, pages 38?41.Olivier Picard, M?elanie Lord, Alexandre Blondin-Mass?e, Odile Marcotte, Marcos Lopes, and StevanHarnad.
2013.
Hidden structure and function in thelexicon.
NLPCS 2013: 10th International Workshopon Natural Language Processing and Cognitive Sci-ence, Marseille, France.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How, pages 52?57.Martin Rosvall and Carl T. Bergstrom.
2008.
Maps ofrandom walks on complex networks reveal commu-nity structure.
Proceedings of the National Academyof Sciences, 105(4):1118?1123.Martin Rosvall and Carl T. Bergstrom.
2011.
Mul-tilevel compression of random walks on networksreveals hierarchical organization in large integratedsystems.
PloS one, 6(4):e18209.Ekaterina Shutova, Simone Teufel, and Anna Korho-nen.
2013.
Statistical metaphor processing.
Com-putational Linguistics, 39(2):301?353.John Sinclair.
1996.
The empty lexicon.
InternationalJournal of Corpus Linguistics, 1(1):99?119.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
Handbook of latent semantic analysis,427(7):424?440.1433Lin Sun and Anna Korhonen.
2009.
Improving verbclustering with automatically acquired selectionalpreferences.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing: Volume 2-Volume 2, pages 638?647.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Xing Wei and Bruce Croft.
2006.
Lda-based docu-ment models for ad-hoc retrieval.
In Proceedingsof the 29th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 178?185.Yorick Wilks.
1975.
A preferential, pattern-seeking,semantics for natural language inference.
ArtificialIntelligence, 6(1):53?74.1434
