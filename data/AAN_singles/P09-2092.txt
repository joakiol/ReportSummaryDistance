Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 365?368,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPComposite Kernels For Relation ExtractionFrank ReichartzFraunhofer IAISSt.
Augustin, GermanyHannes KorteFraunhofer IAISSt.
Augustin, Germany{frank.reichartz,hannes.korte,gerhard.paass}@iais.fraunhofer.deGerhard PaassFraunhofer IAISSt.
Augustin, GermanyAbstractThe automatic extraction of relations be-tween entities expressed in natural lan-guage text is an important problem for IRand text understanding.
In this paper weshow how different kernels for parse treescan be combined to improve the relationextraction quality.
On a public benchmarkdataset the combination of a kernel forphrase grammar parse trees and for depen-dency parse trees outperforms all knowntree kernel approaches alone suggestingthat both types of trees contain comple-mentary information for relation extrac-tion.1 IntroductionThe same semantic relation between entities innatural text can be expressed in many ways, e.g.
?Obama was educated at Harvard?, ?Obama is agraduate of Harvard Law School?, or, ?Obamawent to Harvard College?.
Relation extractionaims at identifying such semantic relations in anautomatic fashion.As a preprocessing step named entity taggersdetect persons, locations, schools, etc.
men-tioned in the text.
These techniques have reacheda sufficient performance level on many datasets(Tjong et al, 2003).
In the next step relations be-tween recognized entities, e.g.
person-educated-in-school(Obama,Harvard) are identified.Parse trees provide extensive information onsyntactic structure.
While feature-based meth-ods may compare only a limited number of struc-tural details, kernel-based methods may explorean often exponential number of characteristicsof trees without explicitly representing the fea-tures.
Zelenko et al (2003) and Culotta andSorensen (2004) proposed kernels for dependencytrees (DTs) inspired by string kernels.
Zhang etal.
(2006) suggested a kernel for phrase grammarparse trees.
Bunescu and Mooney (2005) investi-gated a kernel that computes similarities betweennodes on the shortest path of a DT connecting theentities.
Reichartz et al (2009) presented DT ker-nels comparing substructures in a more sophisti-cated way.Up to now no studies exist on how kernels fordifferent types of parse trees may support eachother.
To tackle this we present a study on howthose kernels for relation extractions can be com-bined.
We implement four state-of-the-art ker-nels.
Subsequently we combine pairs of kernelslinearly or by polynomial expansion.
On a pub-lic benchmark dataset we show that the combinedphrase grammar parse tree kernel and dependencyparse tree kernel outperforms all others by 5.7%F-Measure reaching an F-Measure of 71.2%.
Thisresult shows that both types of parse trees containrelevant information for relation extraction.The remainder of the paper is organized as fol-lows.
In the next section we describe the inves-tigated tree kernels.
Subsequently we present themethod to combine two kernels.
The fourth sec-tion details the experiments on a public benchmarkdataset.
We close with a summary and conclu-sions.2 Kernels for Relation ExtractionRelation extraction aims at learning a relationfrom a number of positive and negative instancesin natural language sentences.
As a classifier weuse Support Vector Machines (SVMs) (Joachims,1999) which can compare complex structures, e.g.trees, by kernels.
Given the kernel function, theSVM tries to find a hyperplane that separates pos-itive from negative examples of the relation.
Thistype of max-margin separator has been shown bothempirically and theoretically to provide good gen-eralization performance on new examples.3652.1 Parse TreesA sentence can be processed by a parser to gener-ate a parse tree, which can be further categorizedin phrase grammar parse trees (PTs) and depen-dency parse trees (DTs).
For DTs there is a bijec-tive mapping between the words in a sentence andthe nodes in the tree.
DTs have a natural orderingof the children of the nodes induced by the posi-tion of the corresponding words in the sentence.
Incontrast PTs introduce new intermediate nodes tobetter express the syntactical structures of a sen-tence in terms of phrases.2.2 Path-enclosed PT KernelThe Path-enclosed PT Tree Kernel (Zhang et al,2008) operates on PTs.
It is based on the Convolu-tion Tree Kernel of Collins and Duffy (2001).
ThePath-enclosed Tree is the parse tree pruned to thenodes that are connected to leaves (words) that be-long to the path connecting both relation entities.The leaves (and connected inner nodes) in front ofthe first relation entity node and behind the sec-ond one are simply removed.
In addition, for theentities there are new artificial nodes labeled withthe relation argument index, and the entity type.Let KCD(T1, T2) be the Convolution Tree Kernel(Collins and Duffy, 2001) of two trees T1, T2, thenthe Path-enclosed PT Kernel (ZhangPT) is de-fined asKZhangPT(X,Y ) = KCD(X?, Y?
)where X?and Y?are the subtrees of the origi-nal tree pruned to the nodes enclosed by the pathconnecting the two entities in the phrase grammerparse trees as described by Zhang et al (2008).2.3 Dependency Tree KernelThe Dependency Tree Kernel (DTK) of Culottaand Sorensen(2004) is based on the work of Ze-lenko et al (2003).
It employs a node kernel?
(u, v) measuring the similarity of two tree nodesu, v and its substructures.
Nodes may be describedby different features like POS-tags, chunk tags,etc..
If the corresponding word describes an en-tity, the entity type and the mention is provided.
Tocompare relations in two instance sentences X,YCulotta and Sorensen (2004) proposes to comparethe subtrees induced by the relation argumentsx1, x2and y1, y2, i.e.
computing the node kernelbetween the two lowest common ancestors (lca) inthe dependecy tree of the relation argument nodesKDTK(X,Y ) = ?
(lca(x1, x2), lca(y1, y2))The node kernel ?
(u, v) is definend over twonodes u and v as the sum of the node similarityand their children similarity.
The children simi-larity function C(s, t) uses a modified version ofthe String Subsequence Kernel of Shawe-Taylorand Christianini (2004) to compute recursively thesum of node kernel values of subsequences ofnode sequences s and t. The function C(s, t) sumsup the similarities of all subsequences in which ev-ery node matches its corresponding node.2.4 All-Pairs Dependency Tree KernelThe All-Pairs Dependency Tree Kernel (All-Pairs-DTK) (Reichartz et al, 2009) sums up the nodekernels of all possible combinations of nodes con-tained in the two subtrees implied by the relationargument nodes asKAll-Pairs(X,Y ) =?u?Vx?v?Vy?
(u, v)where Vxand Vyare sets containing the nodes ofthe complete subtrees rooted at the respective low-est common ancestors.
The consideration of allpossible pairs of nodes and their similarity ensurethat relevant information in the subtrees is utilized.2.5 Dependency Path Tree KernelThe Dependency Path Tree Kernel (Path-DTK)(Reichartz et al, 2009) not only measures thesimilarity of the root nodes and its descendents(Culotta and Sorensen, 2004) or the similari-ties of nodes on the path (Bunescu and Mooney,2005).
It considers the similarities of all nodes(and substructures) using the node kernel ?
onthe path connecting the two relation argument en-tity nodes.
To this end the pairwise comparisonis performed using the ideas of the subsequencekernel of Shawe-Taylor and Cristianini (2004),therefore relaxing the ?same length?
restriction of(Bunescu and Mooney, 2005).
The Path-DTK ef-fectively compares the nodes from paths with dif-ferent lengths while maintaining the ordering in-formation and considering the similarities of sub-structures.The parameter q is the upper bound on the nodedistance whereas the parameter ?, 0 < ?
?
1,is a factor that penalizes gaps.
The Path-DTK is3665-times 5-fold Cross-Validation on Training Set Test SetKernel At Part Role Prec Rec F At Part Role Prec Rec FDTK 54.9 52.8 72.3 71.7 53.7 61.4 (0.32) 50.3 43.4 68.5 79.5 44.0 56.7All-Pairs-DTK 59.1 53.6 73.0 73.1 57.8 64.5 (0.26) 54.3 53.9 71.8 80.2 49.6 61.3Path-DTK 64.8 62.9 77.2 80.2 61.2 69.4 (0.09) 54.9 55.6 73.5 76.7 52.8 62.5ZhangPT 66.8 69.1 77.7 80.6 65.0 71.9 (0.21) 62.9 64.2 72.2 82.0 54.5 65.5ZhangPT + Path-DTK 70.1 76.6 80.8 84.6 68.2 75.5 (0.20) 66.3 71.3 77.7 85.7 60.9 71.2Table 1: F-values for 3 selected relations and micro-averaged precision, recall and F-score (with standarderror) for all 5 relations on the training (CV) and test set in percent.defined asKPath-DTK(X,Y ) =?i?I|x|, j?I|y|,|i|=|j|, d(i),d(j)?q?d(i)+d(j)??
(x(i), y(j))where x and y are the paths in the dependencytree between the relation arguments and x(i) isthe subsequence of the nodes indexed by i, anal-ogously for j. Ikis the set of all possible in-dex sequences with highest index k and d(i) =max(i)?min(i)+1 is the covered distance.
Thefunction ?
?is the sum of the pairwise applicationsof the node kernel ?.3 Kernel compositionIn this paper we use the following two ap-proaches to combine two normalized1kernelsK1,K2(Schoelkopf and Smola, 2001).
For aweighting factor ?
we have the composite kernel:Kc(X,Y ) = ?K1(X,Y ) + (1?
?
)K2(X,Y )Furthermore it is possible to use polynomial ex-pansion on the single kernels, i.e.
Kp(X,Y ) =(K(X,Y ) + 1)p. Our experiments are performedwith ?
= 0.5 and the sum of linear kernels (L) orpoly kernels (P) with p = 2.4 ExperimentsIn this section we present the results of the ex-periments with kernel-based methods for relationextraction.
Throughout this section we will com-pare the approaches considering their classifica-tion quality on the publicly available benchmarkdataset ACE-2003 (Mitchell et al, 2003).
It con-sists of news documents containing 176825 wordssplitted in a test and training set.
Entities and therelations between them were manually annotated.1Kernel normalization: Kn(X,Y ) =K(X,Y )?K(X,X)?K(Y,Y )The entities are marked by the types named (e.g.
?Albert Einstein?)
, nominal (e.g.
?University?
)and pronominal (e.g.
?he?).
There are 5 top levelrelation types role, part, near, social and at, whichare further differentiated into 24 subtypes.4.1 Experimental SetupWe implemented the tree-kernels for relationextraction in Java and used Joachim?s (1999)SVMlightwith the JNI Kernel Extension usingthe implementation details from the original pa-pers.
For the generation of the parse trees we usedthe Stanford Parser (Klein and Manning, 2003).We restricted our experiments to relations betweennamed entities, where NER approaches may beused to extract the arguments.
Without any modi-fication the kernels could also be applied to the alltypes setting as well.
We conducted classificationtests on the five top level relations of the dataset.For each relation we trained a separate SVM fol-lowing the one vs. all scheme for multi-classclassification.
We also employed a standard grid-search on the training set with a 5-times repeated5-fold cross validation to optimize the parametersof all kernels as well as the SVM-parameter C forthe classification runs on the separate test set.
Weuse the standard evaluation measures for classifi-cation accuracy: precision, recall and F-measure.4.2 ResultsTable 1 shows F-values for three selected rela-tions and micro-averaged results for all 5 relationson the training and test set.
In addition the F-scores for the three relations containing the mostinstances are provided.
Kernel and SVM parame-ters are optimized solely on the training set.
Notethat the training set results were obtained on theleft-out folds of cross-validation.
The compositekernel ZhangPT + Path-DTK performs the best onthe cross validations run as well as on the test-set.It outperforms all previously suggested solutions367DTK All-Pairs-DTK Path-DTK ZhangPTZhangPT 63.5 (70.2) PP 67.9 (72.8) PP 71.2 (75.5) LP 65.5 (71.9)Path-DTK 62.7 (67.7) PP 62.9 (69.5) PL 62.5 (69.4)All-Pairs-DTK 60.0 (64.7) PP 61.3 (64.5)DTK 56.7 (61.4)Table 2: Micro-averaged F-values for the Single and Combined Kernels on the Test Set (outside paren-thesis) and with 5-times repeated 5-fold CV on the Training Set (inside parenthesis).
LP denotes thecombination type linear and polynomial, analogously PP and PL.by at least 5.7% F-Measure on the prespecifiedtest-set and by 3.6% F-Measure on the cross val-idation.
Table 2 shows the F-values of the differ-ent combinational kernels on the test set as wellas on the cross validation on the training set.
TheZhangPT + Path-DTK performs the best out of allpossible combinations.
The difference in F-valuesbetween ZhangPT + Path-DTK and ZhangPT isaccording to corrected resampled t-test (Bouckaertand Frank, 2004) significant at a level of 99.9%.These results show that the simultanous consider-ation of phrase grammar parse trees and depen-dency parse trees by the combination of the twokernels is meaningful for relation extraction.5 Conclusion and Future WorkIn this paper we presented a study on the combi-nation of state of the art kernels to improve re-lation extraction quality.
We were able to showthat a combination of a kernel for phrase gram-mar parse trees and one for dependency parse treesoutperforms all other published parse tree ker-nel approaches indicating that both kernels cap-tures complementary information for relation ex-traction.
A promising direction for future work isthe usage of more sophisticated features aiming atcapturing the semantics of words e.g.
word sensedisambiguation (Paa?
and Reichartz, 2009).
Otherpromising directions are the study on the applica-bility of the kernel to other languages and explor-ing combinations of more than two kernels.6 AcknowledgementThe work presented here was funded by the Ger-man Federal Ministry of Economy and Technol-ogy (BMWi) under the THESEUS project.ReferencesRemco R. Bouckaert and Eibe Frank.
2004.
Evaluat-ing the replicability of significance tests for compar-ing learning algorithms.
In PAKDD ?04.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proc.
HLT/EMNLP, pages 724 ?
731.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proc.
NIPS ?01.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In ACL ?04.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods -Support Vector Learning.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
ACL ?03.Alexis Mitchell et al 2003.
ACE-2 Version 1.0; Cor-pus LDC2003T11.
Linguistic Data Consortium.Gerhard Paa?
and Frank Reichartz.
2009.
Exploitingsemantic constraints for estimating supersenses withCRFs.
In Proc.
SDM 2009.Frank Reichartz, Hannes Korte, and Gerhard Paass.2009.
Dependency tree kernels for relation extrac-tion from natural language text.
In ECML ?09.Bernhard Schoelkopf and Alexander J. Smola.
2001.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Erik F. Tjong, Kim Sang, and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InCoRR cs.CL/0306050:.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relation ex-traction.
J. Mach.
Learn.
Res., 3:1083?1106.Min Zhang, Jie Zhang, and Jian Su.
2006.
Explor-ing syntactic features for relation extraction using aconvolution tree kernel.
In Proc.
HLT/NAACL?06.Min Zhang, GuoDong Zhou, and Aiti Aw.
2008.
Ex-ploring syntactic structured features over parse treesfor relation extraction using kernel methods.
Inf.Process.
Manage., 44(2):687?701.368
