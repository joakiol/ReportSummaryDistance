Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 873?882,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsImproving Word Representations via Global Contextand Multiple Word PrototypesEric H. Huang, Richard Socher?, Christopher D. Manning, Andrew Y. NgComputer Science Department, Stanford University, Stanford, CA 94305, USA{ehhuang,manning,ang}@stanford.edu, ?richard@socher.orgAbstractUnsupervised word representations are veryuseful in NLP tasks both as inputs to learningalgorithms and as extra word features in NLPsystems.
However, most of these models arebuilt with only local context and one represen-tation per word.
This is problematic becausewords are often polysemous and global con-text can also provide useful information forlearning word meanings.
We present a newneural network architecture which 1) learnsword embeddings that better capture the se-mantics of words by incorporating both localand global document context, and 2) accountsfor homonymy and polysemy by learning mul-tiple embeddings per word.
We introduce anew dataset with human judgments on pairs ofwords in sentential context, and evaluate ourmodel on it, showing that our model outper-forms competitive baselines and other neurallanguage models.
11 IntroductionVector-space models (VSM) represent word mean-ings with vectors that capture semantic and syntac-tic information of words.
These representations canbe used to induce similarity measures by computingdistances between the vectors, leading to many use-ful applications, such as information retrieval (Man-ning et al, 2008), document classification (Sebas-tiani, 2002) and question answering (Tellex et al,2003).1The dataset and word vectors can be downloaded athttp://ai.stanford.edu/?ehhuang/.Despite their usefulness, most VSMs share acommon problem that each word is only repre-sented with one vector, which clearly fails to capturehomonymy and polysemy.
Reisinger and Mooney(2010b) introduced a multi-prototype VSM whereword sense discrimination is first applied by clus-tering contexts, and then prototypes are built usingthe contexts of the sense-labeled words.
However, inorder to cluster accurately, it is important to captureboth the syntax and semantics of words.
While manyapproaches use local contexts to disambiguate wordmeaning, global contexts can also provide usefultopical information (Ng and Zelle, 1997).
Severalstudies in psychology have also shown that globalcontext can help language comprehension (Hess etal., 1995) and acquisition (Li et al, 2000).We introduce a new neural-network-based lan-guage model that distinguishes and uses both localand global context via a joint training objective.
Themodel learns word representations that better cap-ture the semantics of words, while still keeping syn-tactic information.
These improved representationscan be used to represent contexts for clustering wordinstances, which is used in the multi-prototype ver-sion of our model that accounts for words with mul-tiple senses.We evaluate our new model on the standardWordSim-353 (Finkelstein et al, 2001) dataset thatincludes human similarity judgments on pairs ofwords, showing that combining both local andglobal context outperforms using only local orglobal context alone, and is competitive with state-of-the-art methods.
However, one limitation of thisevaluation is that the human judgments are on pairs873Global ContextLocal Contextscorel scoregDocumenthe walks to the bank... ...sumscoreriverwatershoreglobal semantic vector?playweighted averageFigure 1: An overview of our neural language model.
The model makes use of both local and global context to computea score that should be large for the actual next word (bank in the example), compared to the score for other words.When word meaning is still ambiguous given local context, information in global context can help disambiguation.of words presented in isolation, ignoring meaningvariations in context.
Since word interpretation incontext is important especially for homonymous andpolysemous words, we introduce a new dataset withhuman judgments on similarity between pairs ofwords in sentential context.
To capture interestingword pairs, we sample different senses of words us-ing WordNet (Miller, 1995).
The dataset includesverbs and adjectives, in addition to nouns.
We showthat our multi-prototype model improves upon thesingle-prototype version and outperforms other neu-ral language models and baselines on this dataset.2 Global Context-Aware Neural LanguageModelIn this section, we describe the training objective ofour model, followed by a description of the neuralnetwork architecture, ending with a brief descriptionof our model?s training method.2.1 Training ObjectiveOur model jointly learns word representations whilelearning to discriminate the next word given a shortword sequence (local context) and the document(global context) in which the word sequence occurs.Because our goal is to learn useful word representa-tions and not the probability of the next word givenprevious words (which prohibits looking ahead), ourmodel can utilize the entire document to provideglobal context.Given a word sequence s and document d inwhich the sequence occurs, our goal is to discrim-inate the correct last word in s from other randomwords.
We compute scores g(s, d) and g(sw, d)where sw is swith the last word replaced by wordw,and g(?, ?)
is the scoring function that represents theneural networks used.
We want g(s, d) to be largerthan g(sw, d) by a margin of 1, for any other wordw in the vocabulary, which corresponds to the train-ing objective of minimizing the ranking loss for each(s, d) found in the corpus:Cs,d =?w?Vmax(0, 1?
g(s, d) + g(sw, d)) (1)Collobert and Weston (2008) showed that this rank-ing approach can produce good word embeddingsthat are useful in several NLP tasks, and allowsmuch faster training of the model compared to op-timizing log-likelihood of the next word.2.2 Neural Network ArchitectureWe define two scoring components that contributeto the final score of a (word sequence, document)pair.
The scoring components are computed by twoneural networks, one capturing local context and theother global context, as shown in Figure 1.
We nowdescribe how each scoring component is computed.The score of local context uses the local word se-quence s. We first represent the word sequence s as874an ordered list of vectors x = (x1, x2, ..., xm) wherexi is the embedding of word i in the sequence, whichis a column in the embedding matrix L ?
Rn?|V |where |V | denotes the size of the vocabulary.
Thecolumns of this embedding matrix L are the wordvectors and will be learned and updated during train-ing.
To compute the score of local context, scorel,we use a neural network with one hidden layer:a1 = f(W1[x1;x2; ...;xm] + b1) (2)scorel = W2a1 + b2 (3)where [x1;x2; ...;xm] is the concatenation of them word embeddings representing sequence s, f isan element-wise activation function such as tanh,a1 ?
Rh?1 is the activation of the hidden layer withh hidden nodes, W1 ?
Rh?
(mn) and W2 ?
R1?hare respectively the first and second layer weights ofthe neural network, and b1, b2 are the biases of eachlayer.For the score of the global context, we representthe document also as an ordered list of word em-beddings, d = (d1, d2, ..., dk).
We first compute theweighted average of all word vectors in the docu-ment:c =?ki=1w(ti)di?ki=1w(ti)(4)where w(?)
can be any weighting function that cap-tures the importance of word ti in the document.
Weuse idf-weighting as the weighting function.We use a two-layer neural network to compute theglobal context score, scoreg, similar to the above:a1(g) = f(W (g)1 [c;xm] + b(g)1 ) (5)scoreg = W(g)2 a(g)1 + b(g)2 (6)where [c;xm] is the concatenation of the weightedaverage document vector and the vector of the lastword in s, a1(g) ?
Rh(g)?1 is the activation ofthe hidden layer with h(g) hidden nodes, W (g)1 ?Rh(g)?
(2n) and W (g)2 ?
R1?h(g) are respectively thefirst and second layer weights of the neural network,and b(g)1 , b(g)2 are the biases of each layer.
Note thatinstead of using the document where the sequenceoccurs, we can also specify a fixed k > m that cap-tures larger context.The final score is the sum of the two scores:score = scorel + scoreg (7)The local score preserves word order and syntacticinformation, while the global score uses a weightedaverage which is similar to bag-of-words features,capturing more of the semantics and topics of thedocument.
Note that Collobert and Weston (2008)?slanguage model corresponds to the network usingonly local context.2.3 LearningFollowing Collobert and Weston (2008), we samplethe gradient of the objective by randomly choosinga word from the dictionary as a corrupt example foreach sequence-document pair, (s, d), and take thederivative of the ranking loss with respect to the pa-rameters: weights of the neural network and the em-bedding matrix L. These weights are updated viabackpropagation.
The embedding matrix L is theword representations.
We found that word embed-dings move to good positions in the vector spacefaster when using mini-batch L-BFGS (Liu and No-cedal, 1989) with 1000 pairs of good and corrupt ex-amples per batch for training, compared to stochas-tic gradient descent.3 Multi-Prototype Neural LanguageModelDespite distributional similarity models?
successfulapplications in various NLP tasks, one major limi-tation common to most of these models is that theyassume only one representation for each word.
Thissingle-prototype representation is problematic be-cause many words have multiple meanings, whichcan be wildly different.
Using one representa-tion simply cannot capture the different meanings.Moreover, using all contexts of a homonymous orpolysemous word to build a single prototype couldhurt the representation, which cannot represent anyone of the meanings well as it is influenced by allmeanings of the word.Instead of using only one representation per word,Reisinger and Mooney (2010b) proposed the multi-prototype approach for vector-space models, whichuses multiple representations to capture differentsenses and usages of a word.
We show how our875model can readily adopt the multi-prototype ap-proach.
We present a way to use our learnedsingle-prototype embeddings to represent each con-text window, which can then be used by clustering toperform word sense discrimination (Schu?tze, 1998).In order to learn multiple prototypes, we firstgather the fixed-sized context windows of all occur-rences of a word (we use 5 words before and afterthe word occurrence).
Each context is representedby a weighted average of the context words?
vectors,where again, we use idf-weighting as the weightingfunction, similar to the document context represen-tation described in Section 2.2.
We then use spheri-cal k-means to cluster these context representations,which has been shown to model semantic relationswell (Dhillon and Modha, 2001).
Finally, each wordoccurrence in the corpus is re-labeled to its associ-ated cluster and is used to train the word representa-tion for that cluster.Similarity between a pair of words (w,w?)
us-ing the multi-prototype approach can be computedwith or without context, as defined by Reisinger andMooney (2010b):AvgSimC(w,w?)
=1K2k?i=1k?j=1p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w?
))(8)where p(c, w, i) is the likelihood that word w is inits cluster i given context c, ?i(w) is the vector rep-resenting the i-th cluster centroid of w, and d(v, v?
)is a function computing similarity between two vec-tors, which can be any of the distance functions pre-sented by Curran (2004).
The similarity measure canbe computed in absence of context by assuming uni-form p(c, w, i) over i.4 ExperimentsIn this section, we first present a qualitative analysiscomparing the nearest neighbors of our model?s em-beddings with those of others, showing our embed-dings better capture the semantics of words, with theuse of global context.
Our model also improves thecorrelation with human judgments on a word simi-larity task.
Because word interpretation in context isimportant, we introduce a new dataset with humanjudgments on similarity of pairs of words in senten-tial context.
Finally, we show that our model outper-forms other methods on this dataset and also that themulti-prototype approach improves over the single-prototype approach.We chose Wikipedia as the corpus to train allmodels because of its wide range of topics andword usages, and its clean organization of docu-ment by topic.
We used the April 2010 snapshot ofthe Wikipedia corpus (Shaoul and Westbury, 2010),with a total of about 2 million articles and 990 mil-lion tokens.
We use a dictionary of the 30,000 mostfrequent words in Wikipedia, converted to lowercase.
In preprocessing, we keep the frequent num-bers intact and replace each digit of the uncommonnumbers to ?DG?
so as to preserve information suchas it being a year (e.g.
?DGDGDGDG?).
The con-verted numbers that are rare are mapped to a NUM-BER token.
Other rare words not in the dictionaryare mapped to an UNKNOWN token.For all experiments, our models use 50-dimensional embeddings.
We use 10-word windowsof text as the local context, 100 hidden units, and noweight regularization for both neural networks.
Formulti-prototype variants, we fix the number of pro-totypes to be 10.4.1 Qualitative EvaluationsIn order to show that our model learns more seman-tic word representations with global context, we givethe nearest neighbors of our single-prototype modelversus C&W?s, which only uses local context.
Thenearest neighbors of a word are computed by com-paring the cosine similarity between the center wordand all other words in the dictionary.
Table 1 showsthe nearest neighbors of some words.
The nearestneighbors of ?market?
that C&W?s embeddings giveare more constrained by the syntactic constraint thatwords in plural form are only close to other wordsin plural form, whereas our model captures that thesingular and plural forms of a word are similar inmeaning.
Other examples show that our model in-duces nearest neighbors that better capture seman-tics.Table 2 shows the nearest neighbors of our modelusing the multi-prototype approach.
We see thatthe clustering is able to group contexts of different876CenterWordC&W Our Modelmarkets firms, industries,storesmarket, firms,businessesAmerican Australian,Indian, ItalianU.S., Canadian,Africanillegal alleged, overseas,bannedharmful, prohib-ited, convictedTable 1: Nearest neighbors of words based on cosine sim-ilarity.
Our model is less constrained by syntax and ismore semantic.Center Word Nearest Neighborsbank 1 corporation, insurance, companybank 2 shore, coast, directionstar 1 movie, film, radiostar 2 galaxy, planet, mooncell 1 telephone, smart, phonecell 2 pathology, molecular, physiologyleft 1 close, leave, liveleft 2 top, round, rightTable 2: Nearest neighbors of word embeddings learnedby our model using the multi-prototype approach basedon cosine similarity.
The clustering is able to find the dif-ferent meanings, usages, and parts of speech of the words.meanings of a word into separate groups, allowingour model to learn multiple meaningful representa-tions of a word.4.2 WordSim-353A standard dataset for evaluating vector-space mod-els is the WordSim-353 dataset (Finkelstein et al,2001), which consists of 353 pairs of nouns.
Eachpair is presented without context and associated with13 to 16 human judgments on similarity and re-latedness on a scale from 0 to 10.
For example,(cup,drink) received an average score of 7.25, while(cup,substance) received an average score of 1.92.Table 3 shows our results compared to previousmethods, including C&W?s language model and thehierarchical log-bilinear (HLBL) model (Mnih andHinton, 2008), which is a probabilistic, linear neu-ral model.
We downloaded these embeddings fromTurian et al (2010).
These embeddings were trainedon the smaller corpus RCV1 that contains one yearof Reuters English newswire, and show similar cor-relations on the dataset.
We report the result ofModel Corpus ??
100Our Model-g Wiki.
22.8C&W RCV1 29.5HLBL RCV1 33.2C&W* Wiki.
49.8C&W Wiki.
55.3Our Model Wiki.
64.2Our Model* Wiki.
71.3Pruned tf-idf Wiki.
73.4ESA Wiki.
75Tiered Pruned tf-idf Wiki.
76.9Table 3: Spearman?s ?
correlation on WordSim-353,showing our model?s improvement over previous neuralmodels for learning word embeddings.
C&W* is theword embeddings trained and provided by C&W.
OurModel* is trained without stop words, while Our Model-g uses only global context.
Pruned tf-idf (Reisinger andMooney, 2010b) and ESA (Gabrilovich and Markovitch,2007) are also included.our re-implementation of C&W?s model trained onWikipedia, showing the large effect of using a dif-ferent corpus.Our model is able to learn more semantic wordembeddings and noticeably improves upon C&W?smodel.
Note that our model achieves higher corre-lation (64.2) than either using local context alone(C&W: 55.3) or using global context alone (OurModel-g: 22.8).
We also found that correlation canbe further improved by removing stop words (71.3).Thus, each window of text (training example) con-tains more information but still preserves some syn-tactic information as the words are still ordered inthe local context.4.3 New Dataset: Word Similarity in ContextThe many previous datasets that associate humanjudgments on similarity between pairs of words,such as WordSim-353, MC (Miller and Charles,1991) and RG (Rubenstein and Goodenough, 1965),have helped to advance the development of vector-space models.
However, common to all datasets isthat similarity scores are given to pairs of words inisolation.
This is problematic because the mean-ings of homonymous and polysemous words dependhighly on the words?
contexts.
For example, in thetwo phrases, ?he swings the baseball bat?
and ?the877Word 1 Word 2Located downtown along the east bank of the DesMoines River ...This is the basis of all money laundering , a track recordof depositing clean money before slipping through dirtymoney ...Inside the ruins , there are bats and a bowl with Pokeysthat fills with sand over the course of the race , and themusic changes somewhat while inside ...An aggressive lower order batsman who usually bats atNo.
11 , Muralitharan is known for his tendency to backaway to leg and slog ...An example of legacy left in the Mideast from thesenobles is the Krak des Chevaliers ?
enlargement by theCounts of Tripoli and Toulouse ...... one should not adhere to a particular explanation ,only in such measure as to be ready to abandon it if itbe proved with certainty to be false ...... and Andy ?s getting ready to pack his bags and headup to Los Angeles tomorrow to get ready to fly backhome on Thursday... she encounters Ben ( Duane Jones ) , who arrivesin a pickup truck and defends the house against anotherpack of zombies ...In practice , there is an unknown phase delay betweenthe transmitter and receiver that must be compensatedby ?
synchronization ?
of the receivers local oscillator... but Gilbert did not believe that she was dedicatedenough , and when she missed a rehearsal , she wasdismissed ...Table 4: Example pairs from our new dataset.
Note that words in a pair can be the same word and have different partsof speech.bat flies?, bat has completely different meanings.
Itis unclear how this variation in meaning is accountedfor in human judgments of words presented withoutcontext.One of the main contributions of this paper is thecreation of a new dataset that addresses this issue.The dataset has three interesting characteristics: 1)human judgments are on pairs of words presented insentential context, 2) word pairs and their contextsare chosen to reflect interesting variations in mean-ings of homonymous and polysemous words, and 3)verbs and adjectives are present in addition to nouns.We now describe our methodology in constructingthe dataset.4.3.1 Dataset ConstructionOur procedure of constructing the dataset consistsof three steps: 1) select a list a words, 2) for eachword, select another word to form a pair, 3) for eachword in a pair, find a sentential context.
We nowdescribe each step in detail.In step 1, in order to make sure we select a diverselist of words, we consider three attributes of a word:frequency in a corpus, number of parts of speech,and number of synsets according to WordNet.
Forfrequency, we divide words into three groups, top2,000 most frequent, between 2,000 and 5,000, andbetween 5,000 to 10,000 based on occurrences inWikipedia.
For number of parts of speech, we groupwords based on their number of possible parts ofspeech (noun, verb or adjective), from 1 to 3.
Wealso group words by their number of synsets: [0,5],[6,10], [11, 20], and [20, max].
Finally, we sam-ple at most 15 words from each combination in theCartesian product of the above groupings.In step 2, for each of the words selected in step1, we want to choose the other word so that the paircaptures an interesting relationship.
Similar to Man-andhar et al (2010), we use WordNet to first ran-domly select one synset of the first word, we thenconstruct a set of words in various relations to thefirst word?s chosen synset, including hypernyms, hy-ponyms, holonyms, meronyms and attributes.
Werandomly select a word from this set of words as thesecond word in the pair.
We try to repeat the abovetwice to generate two pairs for each word.
In addi-tion, for words with more than five synsets, we allowthe second word to be the same as the first, but withdifferent synsets.
We end up with pairs of words aswell as the one chosen synset for each word in thepairs.In step 3, we aim to extract a sentence fromWikipedia for each word, which contains the wordand corresponds to a usage of the chosen synset.We first find all sentences in which the word oc-curs.
We then POS tag2 these sentences and filter outthose that do not match the chosen POS.
To find the2We used the MaxEnt Treebank POS tagger in the pythonnltk library.878Model ??
100C&W-S 57.0Our Model-S 58.6Our Model-M AvgSim 62.8Our Model-M AvgSimC 65.7tf-idf-S 26.3Pruned tf-idf-S 62.5Pruned tf-idf-M AvgSim 60.4Pruned tf-idf-M AvgSimC 60.5Table 5: Spearman?s ?
correlation on our newdataset.
Our Model-S uses the single-prototype approach,while Our Model-M uses the multi-prototype approach.AvgSim calculates similarity with each prototype con-tributing equally, while AvgSimC weighs the prototypesaccording to probability of the word belonging to thatprototype?s cluster.word usages that correspond to the chosen synset,we first construct a set of related words of the chosensynset, including hypernyms, hyponyms, holonyms,meronyms and attributes.
Using this set of relatedwords, we filter out a sentence if the document inwhich the sentence appears does not include one ofthe related words.
Finally, we randomly select onesentence from those that are left.Table 4 shows some examples from the dataset.Note that the dataset alo includes pairs of the sameword.
Single-prototype models would give the maxsimilarity score for those pairs, which can be prob-lematic depending on the words?
contexts.
Thisdataset requires models to examine context when de-termining word meaning.Using Amazon Mechanical Turk, we collected 10human similarity ratings for each pair, as Snow etal.
(2008) found that 10 non-expert annotators canachieve very close inter-annotator agreement withexpert raters.
To ensure worker quality, we onlyallowed workers with over 95% approval rate towork on our task.
Furthermore, we discarded allratings by a worker if he/she entered scores out ofthe accepted range or missed a rating, signaling low-quality work.We obtained a total of 2,003 word pairs and theirsentential contexts.
The word pairs consist of 1,712unique words.
Of the 2,003 word pairs, 1328 arenoun-noun pairs, 399 verb-verb, 140 verb-noun, 97adjective-adjective, 30 noun-adjective, and 9 verb-adjective.
241 pairs are same-word pairs.4.3.2 Evaluations on Word Similarity inContextFor evaluation, we also compute Spearman corre-lation between a model?s computed similarity scoresand human judgments.
Table 5 compares differentmodels?
results on this dataset.
We compare againstthe following baselines: tf-idf represents words ina word-word matrix capturing co-occurrence countsin all 10-word context windows.
Reisinger andMooney (2010b) found pruning the low-value tf-idffeatures helps performance.
We report the resultof this pruning technique after tuning the thresh-old value on this dataset, removing all but the top200 features in each word vector.
We tried thesame multi-prototype approach and used sphericalk-means3 to cluster the contexts using tf-idf repre-sentations, but obtained lower numbers than single-prototype (55.4 with AvgSimC).
We then tried usingpruned tf-idf representations on contexts with ourclustering assignments (included in Table 5), but stillgot results worse than the single-prototype versionof the pruned tf-idf model (60.5 with AvgSimC).This suggests that the pruned tf-idf representationsmight be more susceptible to noise or mistakes incontext clustering.By utilizing global context, our model outper-forms C&W?s vectors and the above baselines onthis dataset.
With multiple representations perword, we show that the multi-prototype approachcan improve over the single-prototype version with-out using context (62.8 vs. 58.6).
Moreover, usingAvgSimC4 which takes contexts into account, themulti-prototype model obtains the best performance(65.7).5 Related WorkNeural language models (Bengio et al, 2003; Mnihand Hinton, 2007; Collobert and Weston, 2008;Schwenk and Gauvain, 2002; Emami et al, 2003)have been shown to be very powerful at languagemodeling, a task where models are asked to ac-curately predict the next word given previouslyseen words.
By using distributed representations of3We first tried movMF as in Reisinger and Mooney (2010b),but were unable to get decent results (only 31.5).4probability of being in a cluster is calculated as the inverseof the distance to the cluster centroid.879words which model words?
similarity, this type ofmodels addresses the data sparseness problem thatn-gram models encounter when large contexts areused.
Most of these models used relative local con-texts of between 2 to 10 words.
Schwenk and Gau-vain (2002) tried to incorporate larger context bycombining partial parses of past word sequences anda neural language model.
They used up to 3 previ-ous head words and showed increased performanceon language modeling.
Our model uses a similarneural network architecture as these models and usesthe ranking-loss training objective proposed by Col-lobert and Weston (2008), but introduces a new wayto combine local and global context to train wordembeddings.Besides language modeling, word embeddings in-duced by neural language models have been use-ful in chunking, NER (Turian et al, 2010), parsing(Socher et al, 2011b), sentiment analysis (Socher etal., 2011c) and paraphrase detection (Socher et al,2011a).
However, they have not been directly eval-uated on word similarity tasks, which are importantfor tasks such as information retrieval and summa-rization.
Our experiments show that our word em-beddings are competitive in word similarity tasks.Most of the previous vector-space models use asingle vector to represent a word even though manywords have multiple meanings.
The multi-prototypeapproach has been widely studied in models of cat-egorization in psychology (Rosseel, 2002; Griffithset al, 2009), while Schu?tze (1998) used clusteringof contexts to perform word sense discrimination.Reisinger and Mooney (2010b) combined the twoapproaches and applied them to vector-space mod-els, which was further improved in Reisinger andMooney (2010a).
Two other recent papers (Dhillonet al, 2011; Reddy et al, 2011) present modelsfor constructing word representations that deal withcontext.
It would be interesting to evaluate thosemodels on our new dataset.Many datasets with human similarity ratings onpairs of words, such as WordSim-353 (Finkelsteinet al, 2001), MC (Miller and Charles, 1991) andRG (Rubenstein and Goodenough, 1965), have beenwidely used to evaluate vector-space models.
Moti-vated to evaluate composition models, Mitchell andLapata (2008) introduced a dataset where an intran-sitive verb, presented with a subject noun, is com-pared to another verb chosen to be either similar ordissimilar to the intransitive verb in context.
Thecontext is short, with only one word, and only verbsare compared.
Erk and Pado?
(2008), Thater et al(2011) and Dinu and Lapata (2010) evaluated wordsimilarity in context with a modified task where sys-tems are to rerank gold-standard paraphrase candi-dates given the SemEval 2007 Lexical SubstitutionTask dataset.
This task only indirectly evaluates sim-ilarity as only reranking of already similar words areevaluated.6 ConclusionWe presented a new neural network architecture thatlearns more semantic word representations by us-ing both local and global context in learning.
Theselearned word embeddings can be used to representword contexts as low-dimensional weighted averagevectors, which are then clustered to form differentmeaning groups and used to learn multi-prototypevectors.
We introduced a new dataset with humanjudgments on similarity between pairs of words incontext, so as to evaluate model?s abilities to capturehomonymy and polysemy of words in context.
Ournew multi-prototype neural language model outper-forms previous neural models and competitive base-lines on this new dataset.AcknowledgmentsThe authors gratefully acknowledges the support ofthe Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181, and the DARPA DeepLearning program under contract number FA8650-10-C-7020.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe view of DARPA, AFRL, or the US government.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent,Christian Jauvin, Jaz K, Thomas Hofmann, TomasoPoggio, and John Shawe-taylor.
2003.
A neural prob-abilistic language model.
Journal of Machine Learn-ing Research, 3:1137?1155.880Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, ICML ?08, pages 160?167, New York, NY, USA.ACM.James Richard Curran.
2004.
From distributional to se-mantic similarity.
Technical report.Inderjit S. Dhillon and Dharmendra S. Modha.
2001.Concept decompositions for large sparse text data us-ing clustering.
Mach.
Learn., 42:143?175, January.Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.2011.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information ProcessingSystems (NIPS), volume 24.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 1162?1172,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ahmad Emami, Peng Xu, and Frederick Jelinek.
2003.Using a connectionist model in a syntactical based lan-guage model.
In Acoustics, Speech, and Signal Pro-cessing, pages 372?375.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 897?906, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the con-cept revisited.
In Proceedings of the 10th internationalconference on World Wide Web, WWW ?01, pages406?414, New York, NY, USA.
ACM.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of the20th international joint conference on Artifical intel-ligence, IJCAI?07, pages 1606?1611, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Thomas L Griffiths, Kevin R Canini, Adam N Sanborn,and Daniel J Navarro.
2009.
Unifying rational modelsof categorization via the hierarchical dirichlet process.Brain, page 323328.David J Hess, Donald J Foss, and Patrick Carroll.
1995.Effects of global and local context on lexical process-ing during language comprehension.
Journal of Ex-perimental Psychology: General, 124(1):62?82.Ping Li, Curt Burgess, and Kevin Lund.
2000.
The ac-quisition of word meaning through global lexical co-occurrences.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528, December.Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach,and Sameer S Pradhan.
2010.
Semeval-2010 task14: Word sense induction & disambiguation.
WordJournal Of The International Linguistic Association,(July):63?68.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.George A Miller and Walter G Charles.
1991.
Contextualcorrelates of semantic similarity.
Language & Cogni-tive Processes, 6(1):1?28.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In In Proceedings ofACL-08: HLT, pages 236?244.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InProceedings of the 24th international conference onMachine learning, ICML ?07, pages 641?648, NewYork, NY, USA.
ACM.Andriy Mnih and Geoffrey Hinton.
2008.
A scalablehierarchical distributed language model.
In In NIPS.Ht Ng and J Zelle.
1997.
Corpus-based approaches tosemantic interpretation in natural language processing.AI Magazine, 18(4):45?64.Siva Reddy, Ioannis Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011.
Dynamic and static proto-type vectors for semantic composition.
In Proceedingsof 5th International Joint Conference on Natural Lan-guage Processing, pages 705?713, Chiang Mai, Thai-land, November.
Asian Federation of Natural Lan-guage Processing.Joseph Reisinger and Raymond Mooney.
2010a.
A mix-ture model with sharing for lexical semantics.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 1173?1182, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Joseph Reisinger and Raymond J. Mooney.
2010b.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 109?117, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yves Rosseel.
2002.
Mixture models of categorization.Journal of Mathematical Psychology, 46:178?210.881Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8:627?633, October.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Journal of Computational Linguistics, 24:97?123.Holger Schwenk and Jean-luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary con-tinuous speech recognition.
In In International Con-ference on Acoustics, Speech and Signal Processing,pages 765?768.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Comput.
Surv., 34:1?47, March.Cyrus Shaoul and Chris Westbury.
2010.
The westburylab wikipedia corpus.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 254?263, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011a.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems 24.Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-pher D. Manning.
2011b.
Parsing natural scenes andnatural language with recursive neural networks.
InProceedings of the 26th International Conference onMachine Learning (ICML).Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011c.Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-des, and Gregory Marton.
2003.
Quantitative evalu-ation of passage retrieval algorithms for question an-swering.
In Proceedings of the 26th Annual Interna-tional ACM SIGIR Conference on Search and Devel-opment in Information Retrieval, pages 41?47.
ACMPress.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: a simple and effec-tive vector model.
In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing, IJCNLP ?11.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 384?394, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.882
