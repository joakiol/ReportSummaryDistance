A STOCHASTIC PROCESS FOR WORD FREQUENCYDISTRIBUTIONSHarald Baayen*Maz-Planck-Institut fiir PsycholinguistikWundtlaan 1, NL-6525 XD NijmegenInternet: baayen@mpi.nlABSTRACTA stochastic model based on insights of Man-delbrot (1953) and Simon (1955) is discussedagainst the background of new criteria of ade-quacy that have become available recently as aresult of studies of the similarity relations be-tween words as found in large computerized textcorpora.FREQUENCY D ISTRIBUT IONSVarious models for word frequency distributionshave been developed since Zipf (1935) appliedthe zeta distribution to describe a wide range oflexical data.
Mandelbrot (1953, 1962)extendedZipf's distribution 'law'Kf, = ?x, (i)where fi is the sample frequency of the i th typein a ranking according to decreasing frequency,with the parameter B,Kf~ = B + i~ ' (2)by means of which fits are obtained that are moreaccurate with respect to the higher frequencywords.
Simon (1955, 1960) developed a stochas-tic process which has the Yule distributionf, = AB(i ,  p + 1), (3)with the parameter A and B(i, p + i) the Betafunction in (i, p + I), as its stationary solutions.For i --~ oo, (3) can be written asf~ ~ r(p + 1)i -(.+I) ,in other words, (3) approximates Zipf's law withrespect to the lower frequency words, the tail of*I am indebted to Kl~as van Ham,  Richard Gill, BertHoeks and Erlk Schils for stimulating discussions on thestatistical analysis of lexical similarity relations.the distribution.
Other models, such as Good(1953), Waring-Herdan (Herdan 1960, Muller1979) and Sichel (1975), have been put forward,all of which have Zipf's law as some special orlimiting form.
Unrelated to Zipf's law is thelognormal hypothesis, advanced for word fre-quency distributions by Carroll (1967, 1969),which gives rise to reasonable fits and is widelyused in psycholinguistic research on word fre-quency effects in mental processing.A problem that immediately arises in the con-text of the study of word frequency distribu-tions concerns the fact that these distributionshave two important characteristics which theyshare with other so-called large number of rareevents (LNRE) distributions (Orlov and Chi-tashvili 1983, Chltashvili and Khmaladze 1989),namely that on the one hand a huge number ofdifferent word types appears, and that on theother hand it is observed that while some eventshave reasonably stable frequencies, others occuronly once, twice, etc.
Crucially, these rare eventsoccupy a significant portion of the list of alltypes observed.
The presence of such large num-bers of very low frequency types effects a signifi-cant bias between the rank-probability distribu-tion and the rank-frequency distributions lead-ing to the contradiction of the common meanof the law of large numbers, so that expressionsconcerning frequencies cannot be taken to ap-proximate expressions concerning probabilities.The fact that for LNRE distributions the rank-probability distributions cannot be reliably esti-mated on the basis of rank-frequency distribu-tions is one source of the lack of goodness-of-fitoften observed when various distribution 'laws'are applied to empirical data.
Better results areobtained with Zipfian models when Orlov andChitashvili's (1983) extended generalized Zipf'slaw is used.A second problem which arises when the ap-propriateness of the various lexical models is271considered, the central issue of the present dis-cussion, concerns the similarity relations amongwords in lexical distributions.
These empiricalsimilarity relations, as observed for large corporaof words, impose additional criteria on the ad-equacy of models for word frequency distribu-tions.S IM ILARITY  RELAT IONSThere is a growing consensus in psycholinguis-tic research that word recognition depends notonly on properties of the target word (e.g.
itslength and frequency), but also upon the numberand nature of its lexical competitors or neigh-bors.
The first to study similarity relationsamong lexical competitors in the lexicon in re-lation to lexical frequency were Landauer andStreeter (1973).
Let a seighbor be a word thatdiffers in exactly one phoneme (or letter) froma given target string, and let the neighborhoodbe the set of all neighbors, i.e.
the set of allwords at Hamming distance 1 from the target.Landauer and Streeter observed that (1) high-frequency words have more neighbors than low-frequency words (the neighborhood density ef-fect), and that (2) high-frequency words havehigher-frequency neighbors than low-frequencywords (the neighborhood frequency effect).
Inorder to facilitate statistical analysis, it is con-venient to restate the neighborhood frequencyeffect as a correlation between the target's num-ber of neighbors and the frequencies of theseneighbors, rather than as a relation betweenthe target's frequency and the frequencies of itsneighbors - -  targets with many neighbors havinghigher frequency neighbors, and hence a highermean neighborhood frequency .f,~ than targetswith few neighbors.
In fact, both the neighbor-hood density and the neighborhood frequencyeffect are descriptions of a single property oflexical space, namely that its dense similarityregions are populated by the higher frequencytypes.
A crucial property of word frequency dis-tributions is that the lexical similarity effects oc-cur not only across but also within word lengths.Figure 1A displays the rank-frequency distri-bution of Dutch monomorphemic phonologicallyrepresented stems, function words excluded, andcharts the lexical similarity effects of the subsetof words with length 4 by means of boxplots.These show the mean (dotted line), the median,the upper and lower quartiles, the most extremedata points within 1.5 times the interquartilerange, and remaining outliers for the number ofneighbors (#n) against arget frequency (neigh-borhood density), and for the mean frequency ofthe neighbors of a target (f,~) against he hum-Table i: Spearman rank correlation analysis ofthe neighborhood density and frequency effectsfor empirical and theoretical words of length 4.Dutch Mand.
Mand.-Simondens.freq.r, 0.24 0.65 0.310.06 0.42 O. I0 ~et 9.16 68.58 11.97df 1340 6423 1348rs 0.51 0.62 0.612 0.26 0.38 0.37 7" it 21.65 63.02 28.22df 1340 6423 1348ber of neighbors of the target (neighborhood fre-quency), for targets grouped into frequency anddensity classes respectively.
Observe that therank-frequency distribution of monomorphemicDutch words does not show up as a straightline in a double logarithmic plot, that there isa small neighborhood density effect and a some-what more pronounced neighborhood frequencyeffect.
A Spearman rank correlation analysisreveals that the lexlcal similarity effects of fig-ure 1A are statistically highly significant trends(p <~ 0.001), even though the correlations them-selves are quite weak (see table 1, column 1): inthe case of lexical density only 6% of the varianceis explained.
1STOCHASTIC  MODELL INGBy themselves, models of the kind proposedby Zipf, Herdan and Muller or Sichel, eventhough they may yield reasonable fits to partic-ular word frequency distributions, have no bear-ing on the similarity relations in the lexicon.The only model that is promising in this respectis that of Mandelbrot (1953, 1962).
Mandel-brot derived his modification of Zipf's law (2)on the basis of a Markovlan model for generat-ing words as strings of letters, in combinationwith some assumptions concerning the cost oftransmitting the words generated in some op-timal code, giving a precise interpretation toZipf's 'law of abbreviation'.
Miller (1057), wish-ing to avoid a teleological explanation, showedthat the Zipf-Mandelbrot law can also be de-rived under slightly different assumptions.
Inter-estingiy, Nusbaum (1985), on the basis of sim-ulation results with a slightly different neighbordefinition, reports that the neighborhood densityand neighborhood frequency effects occur withinXNote that the larger value of r~ for the neighborhoodfrequency eiTect is a direct consequence of the fact thatthe frequencies of the neighbors of each target are a~-eraged before they enter into the calculations, maskingmuch of the variance.272lO t10 a10 ~10 xI0 ?~Tt201612840I2000I00050010050 \]l \]l \]I I l\[ 0FC 1 !
!12 456~,e ~e3 ~vs 3~s xe, oe ea # itemso" i DC I I I | I I l I I 1 2 3 4 5 610?
lOt 102 lO!
lOt ~li J.o ~ox I,.
,o x.
# itenmA: Dutch monomorphemic  stems in the CELEX database, standardized at 1,00O,0OO.
For the totaldistr ibut ion,  N = 224567, V = 4455.
For str ings of length 4 , /V  = 64854, V = 1342.l0 t 5510 a 110 ~ "?
.
-- 443322 1110?
~ , i 0I0 ?
I0 x 102 I0 ~ lO t I0 ~, , , , , FC  11 3 4 5 6 7:sso xs,41oo:svv :8v x~J s7 # itenmI000500I0050I0 iilIlii, , DC1 2 3 4 5 6 73s4 ..'r .es  e~x so.
e .uoo~ # i temsB: Simulated Dutch monomorphemic  stems, as generated by a Markov process.
For the total distribu-tion, N = 224567, V = 58300.
For strings of length 4, N = 74618, V -- 6425./, #n / .104 35 I000 ~ ?
I"" .
!
\[II\]ll103 28I0021 50I0 s1410101 710?
, i 0 FC  1 DC345I0?
10z 10s 103 104 3w 2s~ 20, ~ss z~o ,.
xs~ # items xg~ ~o 23~ ~ ~ov ~v a~ # itemsC: S imulated Dutch monomorphemic  stems,  as generated by the Mande lbrot -S imon model  (a = 0.01,Vc = 2000).
For the total  d istr ibut ion,  N = 291944, V = 4848.
For str ings of length 4, N = 123317,V = 1350.F igure 1: Rank- f requency and lexical s imilar ity character ist ics  of the empir ical  and two s imulateddistributions of Dutch phonological stems.
F rom left to right: double logarithmic plot of rank i versusfrequency fi, boxplot of frequency class FC  (1:1;2:2-4;3:5-12;4:13-33;5:34-90;6:91-244;7:245+) versusnumber of neighbors #n (length 4), and boxplot of density class DC ( 1:1-3;2:4-6;3:7-9;4:10-12;5:13-15;6:16-19;7:20+) versus mean frequency of neighbors fn (length 4).
(Note that not all axes are scaledequally across the three distributions).
N: number of tokens, V: number of types.273a given word length when the transition proba-bilities are not uniformly distributed.
Unfortu-nately, he leaves unexplained why these effectsoccur, and to what extent his simulation is arealistic model of lexical items as used in realspeech.In order to come to a more precise understand-ing of the source and nature of the lexical simi-larity effects in natural anguage we studied twostochastic models by means of computer simu-lations.
We first discuss the Markovian modelfiguring in Mandelbrot's derivation of (2).Consider a first-order Markov process.
LetA = {0,1 , .
.
.
,k}  be the set of phonemes ofthe language, with 0 representing the terminat-ing character space, and let T ~ : (P~j)i,jeA withP00 = 0.
If X,~ is the letter in the r, th position ofa string, we define P(Xo = i) = po~, i E A. Lety be a finite string ( /o , /1 , .
.
.
, /m-z)  for m E Nand define X (m) := (Xo, X I , .
.
.
,Xm-1),  thenPv := p(X(" )  = l~) = Po~01~0~l...l~.._0~,_,.
(4)The string types of varying length m, terminat-ing with the space and without any interveningspace characters, constitute the words of the the-oretical vocabularys,,, := {(io, i~, .
.
.
,~, ,_=,o) :ij E A \ O,j =O, I , .
.
.
,m-  2, mE N}.With N~ the token frequency of type y andV the number of different types, the vec-tor (N~,N~= , .
.
.
.
N~v) is multinomially dis-tributed.
Focussing on the neighborhood en-sity effect, and defining the neighborhood of atarget string yt for fixed length rn asCt := ~y Esuchwe have that theof Yt equalsS,,, : 3!i e {0, 1 , .
.
.
,  m - 2}that yl ?
yt} ,expected number of neighborsE\[V(Ct)\] = ~ {1 - (1 - p~)N}, (5)IIEC,with N denoting the number of trials (i.e.
thenumber of tokens sampled).
Note that when thetransition matrix 7 ) defines a uniform distribu-tion (all pi# equal), we immediately have thatthe expected neighborhood density for length rnlis identical for all targets Yt, while for lengthm~ > rnl the expected density will be less thanthat at length ml, since p(,n=) < p(,m) given(4).
With E\[Ny\] = Np~, we find that the neigh-borhood density effect does occur across wordlengths, even though the transition probabilitiesare uniformly distributed.In order to obtain a realistic, non-trivial the-oretical word distribution comparable with theempirical data of figure 1A, the transition matrix7 ~ was constructed such that it generated a sub-set of phonotactically egal (possible) monomor-phematic strings of Dutch by conditioning con-sonant CA in the string X~XjC~ on Xj and thesegmental nature (C or V) of Xi, while vowelswere conditioned on the preceding segment only.This procedure allowed us to differentiate be-tween e.g.
phonotactically egal word initial knand illegal word final k sequences, at the sametime avoiding full conditioning on two preced-ing segments, which, for four-letter words, wouldcome uncomfortably close to building the prob-abilities of the individual words in the databaseinto the model.The rank-frequency distribution of 58300types and 224567 tokens (disregarding strings oflength 1) obtained by means of this (second or-der) Markov process shows up in a double Iog-arithrnic plot as roughly linear (figure IB).
Al-though the curve has the general Zipfian shape,the deviations at head and tail are present by ne-cessity in the light of Rouault (1978).
A compar-ison with figure 1A reveals that the large surplusof very low frequency types is highly unsatisfac-tory.
The model (given the present transitionmatrix) fails to replicate the high rate of use ofthe relatively limited set of words of natural lan-guage.The lexlcal similarity effects as they emergefor the simulated strings of length 4 are displayedin the boxplots of figure lB.
A very pronouncedneighborhood density effect is found, in combi-nation with a subdued neighborhood frequencyeffect (see table 1, column 2).The appearance of the neighborhood densityeffect within a fixed string length in the Marko-vian scheme with non-uniformly distributed p~jcan be readily understood in the simple caseof the first order Markov model outlined above.Since neighbors are obtained by substitution ofa single element of the phoneme inventory A,two consecutive transitional probabilities of (4)have to be replaced.
For increasing target prob-ability p~,, the constituting transition probabil-ities Pij must increase, so that, especially fornon-trivial m, the neighbors y E Ct will gen-erally be protected against low probabilities py.Consequently, by (5), for fixed length m, higherfrequency words will have more neighbors thanlower frequency words for non-uniformly dis-tributed transition probabilities.The fact that the lexical similarity effectsemerge for target strings of the same length isa strong point in favour of a Markovian source274for word frequency distributions.
Unfortunately,comparing the results of figure 1B with thoseof figure 1A, it appears that the effects are ofthe wrong order of magnitude: the neighborhooddensity effect is far too strong, the neighborhoodfrequency effect somewhat too weak.
The sourceof this distortion can be traced to the extremelylarge number of types generated (6425) for anumber of tokens (74618) for which the empiricaldata (64854 tokens) allow only 1342 types.
Thislarge surplus of types gives rise to an inflatedneighborhood ensity effect, with the concomi-tant effect hat neighborhood frequency is scaleddown.
Rather than attempting to address thisissue by changing the transition matrix by usinga more constrained but less realistic data set,another option is explored here, namely the ideato supplement the Markovian stochastic processwith a second stochastic process developed bySimon (1955), by means of which the intensiveuse can be modelled to which the word types ofnatural language are put.Consider the frequency distribution of e.g.
acorpus that is being compiled, and assume thatat some stage of compilation N word tokens havebeen observed.
Let n (Jr) be the number of wordtypes that have occurred exactly r times in thesefirst N words.
If we allow for the possibilitiesthat both new types can be sampled, and oldtypes can be re-used, Simon's model in its sim-plest form is obtained under the three assump-tions that (1) the probability that the (N + 1)-stword is a type that has appeared exactly r timesis proportional to r~ Iv), the summed token fre-quencies of all types with token frequency r atstage N, that (2) there is a constant probabilityc~ that the (N-f 1)-st word represents a new type,and that (3) all frequencies grow proportionalywith N, so thatn~ (Iv+l) N + 1g~'-----V = "-W-- for all r, lv.Simon (1955) shows that the Yule-distribution(3) follows from these assumptions.
When thethird assumption is replaced by the assumptionsthat word types are dropped with a probabil-ity proportional to their token frequency, andthat old words are dropped at the same rate atwhich new word types are introduced so thatthe total number of tokens in the distribution isa constant, the Yule-distribution is again foundto follow (Simon 1960).By itself, this stochastic process has no ex-planatory value with respect to the similarityrelations between words.
It specifies use and re-use of word types, without any reference to seg-mental constituency or length.
However, when aMarkovian process is fitted as a front end to Si-mon's stochastic process, a hybrid model resultsthat has the desired properties, since the latterprocess can be used to force the required highintensity of use on the types of its input distri-bution.
The Markovian front end of the modelcan be thought of as defining a probability dis-tribution that reflects the ease with which wordscan be pronounced by the human vocal tract,an implementation of phonotaxis.
The secondcomponent of the model can be viewed as simu-lating interfering factors pertaining to languageuse.
Extralinguistic factors codetermine the ex-tent to which words are put to use, indepen-dently of the slot occupied by these words in thenetwork of similarity relations, ~ and may effecta substantial reduction of the lexlcal similarityeffects.Qualitatively satisfying results were obtainedwith this 'Mandelbrot-Simon' stochastic model,using the transition matrix of figure IB for theMarkovlan front end and fixing Simon's birthrate a at 0.01. s An  additional parameter, Vc,the critical number of types for which the switchfrom the front end to what we will refer to asthe component of use is made, was fixed at 2000.Figure 1C shows that both the general shape ofthe rank-frequency curve in a double logarith-mic grid, as well as the lexical similarity effects(table 1, column 3) are highly similar to the em-pirical observations (figure 1A).
Moreover, theoverall number of types (4848) and the numberof types of length 4 (1350) closely approximatethe empirical numbers of types (4455 and 1342respectively), and the same holds for the overallnumbers of tokens (291944 and 224567) respec-tively.
Only the number of tokens of length 4is overestimated by a factor 2.
Nevertheless, thetype-token ratio is far more balanced than in theoriginal Markovian scheme.
Given that the tran-sition matrix models only part of the phonotaxisof Dutch, a perfect match between the theoret-ical and empirical distributions is not to be ex-pected.The present results were obtained by imple-menting Simon's stochastic model in a slightlymodified form, however.
Simon's derivation ofthe Yule-distribution builds on the assumptionthat each r grows proportionaly with N, an as-2For instance, the Dutch word kuip, 'barrel', is a low-frequency type in the present-day language, due to thefact that its denotatum has almost completely droppedout of use.
Nevertheless, it was a high-frequency wordin earlier centuries, to which the high frequency of thesurname ku~per bears witness.~The new types entering the distribution at ratewere generated by means of the tr~nsitlon matrix of figure113.275sumption that does not lend itself to implemen-tation in a stochastic process.
Without this as-sumption, rank-frequency distributions are gen-erated that depart significantly from the empir-ical rank-frequency curve, the highest frequencywords attracting a very large proportion of alltokens.
By replacing Simon's assumptions 1 and3 by the 'rule of usage' thatthe probability that the (N+ 1)-st wordis a type that has appeared exactly rtimes is proportional toH,.
:= \ ]~,  ~'~ log , (6)theoretical rank-frequency distributions of thedesired form can be obtained.
Writingrn~ v( , ' )  "=for the probability of re-using any type that hasbeen used r times before, H, can be interpretedas the contribution of all types with frequencyr to the total entropy H of the distribution ofranks r, i.e.
to the average amount of informa-tionl z  =PSelection of ranks according to (6) rather thanproportional to rnT (Simon's assumption I) en-sures that the highest ranks r have lowered prob-abilities of being sampled, at the same timeslightly raising the probabilities of the inter-mediate ranks r. For instance, the 58 highestranks of the distribution of figure 1C have some-what raised, the complementary 212 ranks some-what lowered probability of being sampled.
Theadvantage of using (6) is that unnatural rank-frequency distributions in which a small numberof types assume exceedingly high token frequen-cies are avoided.The proposed rule of usage can be viewed as ameans to obtain a better trade-off in the distri-bution between maximalization of informationtransmission and optimalization of the cost ofcoding the information.
To see this, consideran individual word type Z/.
In order to mini-malize the cost of coding C(y) = -log(Pr(y)),high-frequency words should be re-used.
Unfor-tunately, these high-frequency words have thelowest information content.
However, it can beshown that maximalization of information trans-mission requires the re-use of the lowest fre-quency types (H, is maximal for uniformly dis-tributed p(r)).
Thus we have two opposing re-quirements, which balance out in favor of a moreintensive use of the lower and intermediate fre-quency ranges when selection of ranks is propor-tional to (6).The 'rule of usage' (6) implies that higherfrequency words contribute less to the averageamount of information than might be expectedon the basis of their relative sample frequen-cies.
Interestingly, there is independent evidencefor this prediction.
It is well known that thehigher-frequency t pes have more (shades of)meaning(s) than lower-frequency words (see e.g.Reder, Anderson and Bjork 1974, Paivio, Yuilleand Madigan 1968).
A larger number of mean-ings is correlated with increased contextual de-pendency for interpretation.
Hence the amountof information contributed by such types out ofcontext (under conditions of statistical indepen-dence) is less than what their relative samplefrequencies suggest, exactly as modelled by ourrule of usage.Note that this semantic motivation for se-lection proportional to H, makes it possibleto avoid invoking external principles such as'least effort' or 'optimal coding' in the mathe-matical definition of the model, principles thathave been criticized as straining one's credulity(Miller 1957).
4FUNCTION WORDSUp till now, we have focused on the modellingof monomorphemic Dutch words, to the exclu-sion of function words and morphologically com-plex words.
One of the reasons for this ap-proach concerns the way in which the shape ofthe rank-frequency curves differs substantiallydepending on which kinds of words are includedin the distribution.
As shown in figure 2, thecurve of monomorphemic words without func-tion words is highly convex.
When functionwords are added, the head of the tail is straight-ened out, while the addition of complex wordsbrings the tail of the distribution (more or less)in line with Zipf's law.
Depending on what kindof distribution is being modelled, different crite-ria of adequacy have to be met.Interestingly, function words, - -  articles, pro-nouns, conjunctions and prepositions, the so-called closed classes, among which we have alsoreckoned the auxiliary verbs - -  typically show upas the shortest and most frequent (Zipf) words infrequency distributions.
In fact, they are foundwith raised frequencies in the the empirical rank-frequency distribution when compared with thecurve of content words only, as shown in the first4In this respect, Miller's (1957) alternative derivationof (2) in terms of random spacing isunconvincing in thelight of the phonotactlc constraints on word structure.276105104l0 sI02101I00I0 5 lO soe ee104 104?
ooIO s lO sI02 102101 I01z .~" i I0 ?
, i , , , , , , , i 10 0I0 ?
101 I0 = l0 s 104 l0 s I0 ?
I01 I0= l0 s 104 l0 s I0 ?
I01 I0= I0 ~ 104 l0 sFigure 2: Rank-frequency plots for Dutch phonological sterns.
From left to right: monomorphemicwords without function words, monomorphemic words and function words, complete distribution.two graphs of figure 2.
Miller, Newman & Fried-man (1958), discussing the finding that the fre-quential characteristics of function words differmarkedly from those of content words, arguedthat (1958:385)Inasmuch as the division into twoclasses of words was independent of thefrequencies of the words, we might haveexpected it to simply divide the sam-ple in half, each half retaining the sta-tistical properties of the whole.
Sincethis is clearly not the case, it is ob-vious that Mandelbrot's approach isincomplete.
The general trends forall words combined seem to follow astochastic pattern, but when we lookat syntactic patterns, differences beginto appear which will require linguistic,rather than mere statistical, explana-tions.In the Mandelbrot-Simon model developed here,neither the Markovian front end nor the pro-posed rule of usage are able to model the ex-tremely high intensity of use of these functionwords correctly without unwished-for side effectson the distribution of content words.
However,given that the semantics of function words arenot subject to the loss of specificity that char-acterizes high-frequency content words, functionwords are not subject to selection proportionalto H~.
Instead, some form of selection propor-tional to rn~ probably is more appropriate here.MORPHOLOGYThe Mandelbrot-Simon model has a single pa-rameter ~ that allows new words to enter the dis-tribution.
Since the present theory is of a phono-logical rather than a morphological nature, thisparameter models the (occasional) appearanceof new simplex words in the language only, andcannot be used to model the influx of morpho-logically complex words.First, morphological word formation processesmay give rise to consonant clusters that are per-mitted when they span morpheme boundaries,but that are inadmissible within single mor-phemes.
This difference in phonotactic pattern-ing within and across morphemes already re-reales that morphologically complex words havea dLf\[erent source than monomorpherpJc words.Second, each word formation process, whethercompounding or affixation of sufr-txes like -messand -ity, is characterized by its own degree ofproductivity.
Quantitatively, differences in thedegree of productivity amount o differences inthe birth rates at which complex words appearin the vocabulary.
Typically, such birth rates,which can be expressed as E\[n~\] where n~ andNl  ,A r' denote the number of types occurring onceonly and the number of tokens of the frequencydistributions of the corresponding morphologi-cal categories (Basyen 1989), assume values thatare significantly higher that the birth rate c~ ofmonomorphemic words.
Hence it is impossibleto model the complete lexical distribution with-out a worked-out morphological component thatspecifies the word formation processes of the lan-guage and their degrees of productivity.While actual modelling of the complete distri-bution is beyond the scope of the present paper,we may note that the addition of birth rates forword formation processes to the model, neces-sitated by the additional large numbers of rare277words that appear in the complete distribution,ties in nicely with the fact that the frequencydistributions of productive morphological cate-gories are prototypical LNRE distributions, forwhich the large values for the numbers of typesoccurring once or twice only are characteristic.With respect to the effect of morphologicalstructure on the lexical similarity effects, we fi-nally note that in the empirical data the longerword lengths show up with sharply diminishedneighborhood density.
However, it appears thatthose longer words which do have neighbors aremorphologically complex.
Morphological struc-ture raises lexical density where the phonotaxisfails to do so: for long monomorphemic wordsthe huge space of possible word types is sampledtoo sparcely for the lexical similarity effects toemerge.REFERENCESBaayen, R.H. 1989.
A Corpus-Based Approachto Morphological Productivity.
Statistical Anal-ysis and Psycholinguistic Interpretation.
Diss.Vrije Universiteit, Amsterdam.Carroll, J.B. 1967.
On Sampling from a Log-normal Model of Word Frequency Distribution.In: H.Ku~era 0 W.N.Francis 1967, 406-424.Carroll, 3.B.
1969.
A Rationale for an Asymp-totic Lognormal Form of Word Frequency Distri-butions.
Research Bulletin -- Educational Test.ing Service, Princeton, November 1969.Chitaivili, P~J.
& Khmaladse, E.V.
1989.
Sta-tistical Analysis of Large Number of Rare Eventsand Related Problems.
~Vansactions of the Tbil-isi Mathematical Instflute.Good, I.J.
1953.
The population frequencies ofspecies and the estimation of population param-eters, Biometrika 43, 45-63.Herdan, G. 1960.
Type-toke~ Mathematics,The Hague, Mouton.Ku~era~ H. & Francis, W.N.
1967.
Compa-Lational Analysis of Prese~t-Day American En-glish.
Providence: Brown University Press.Landauer, T.K.
& Streeter, L.A. 1973.
Struc-tural differences between common and rarewords: failure of equivalence assumptions fortheories of word recognition, Journal of VerbalLearning and Verbal Behavior 12, 119-131.Mandelbrot, B.
1953.
An informational the-ory of the statistical structure of language, in:W.Jackson (ed.
), Communication Theory, But-terworths.Mandelbrot, B.
1962.
On the theory of wordfrequencies and on related Markovian modelsof discourse, in: R.Jakobson, Structure of Lan-guage and its Mathematical Aspects.
Proceedingsof Symposia in Applied Mathematics Vol XII,Providence, Rhode Island, Americal Mathemat-ical Society, 190-219.Miller, G.A.
1954.
Communication, AnnualReview of Psychology 5, 401-420.Miller, G.A.
1957.
Some effects of intermittentsilence, The American Jo~trnal of Psychology 52,311-314.Miller, G.A., Newman, E.B.
& Friedman, E.A.1958.
Length-Frequency Statistics for WrittenEnglish, Information and control 1, 370-389.Muller, Ch.
1979.
Du nouveau sur les distri-butions lexicales: la formule de Waring-Herdan.In: Ch.
Muller, Langue Frangaise et Linguis-tique Quantitative.
Gen~ve: Slatkine, 177-195.Nusbaum, H.C. 1985.
A stochastic accountof the relationship between lexical density andword frequency, Research on Speech PerceptionReport # 1I, Indiana University.Orlov, J.K. & Chitashvili, R.Y.
1983.
Gener-alized Z-distribution generating the well-known'rank-distributions', Bulletin of the Academy ofSciences, Georgia 110.2, 269-272.Paivio, A., Yuille, J.C. & Madigan, S. 1968.Concreteness, Imagery and Meaningfulness Val-ues for 925 Nouns.
Journal of Ezperimental Psy-chology Monograph 76, I, Pt.
2.Reder, L.M., Anderson, J.R. & Bjork, R.A.1974.
A Semantic Interpretation of EncodingSpecificity.
Journal of Ezperimental Psychology102: 648-656.Rouault, A.
1978.
Lot de Zipf et sourcesmarkoviennes, Ann.
Inst.
H.Poincare 14, 169-188.Sichel, H.S.
1975.
On a Distribution Law forWord Frequencies.
Journal of Lhe American Sta-tistical Association 70, 542-547.Simon, H.A.
1955.
On a class of skew distri-bution functions, Biometrika 42, 435-440.Simon, H.A.
1960.
Some further notes on aclass of skew distribution functions, Informationand Control 3, 80-88.Zipf, G.K. 1935.
The Psycho.Biology of Lan-guage, Boston, Houghton Mifflin.278
