Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 905?912Manchester, August 2008A Uniform Approach to Analogies, Synonyms, Antonyms,and AssociationsPeter D. TurneyNational Research Council of CanadaInstitute for Information TechnologyM50 Montreal RoadOttawa, Ontario, CanadaK1A 0R6peter.turney@nrc-cnrc.gc.caAbstractRecognizing analogies, synonyms, anto-nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al-gorithms.
In the past, the four tasks havebeen treated independently, using a widevariety of algorithms.
These four seman-tic classes, however, are a tiny sample ofthe full range of semantic phenomena, andwe cannot afford to create ad hoc algo-rithms for each semantic phenomenon; weneed to seek a unified approach.
We pro-pose to subsume a broad range of phenom-ena under analogies.
To limit the scope ofthis paper, we restrict our attention to thesubsumption of synonyms, antonyms, andassociations.
We introduce a supervisedcorpus-based machine learning algorithmfor classifying analogous word pairs, andwe show that it can solve multiple-choiceSAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym ques-tions, and similar-associated-both ques-tions from cognitive psychology.1 IntroductionA pair of words (petrify:stone) is analogous to an-other pair (vaporize:gas) when the semantic re-lations between the words in the first pair arehighly similar to the relations in the second pair.Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be in-terchanged (imposed a tax), they are are antony-mous when they have opposite meanings (blackc?
2008, National Research Council of Canada (NRC).Licensed to the Coling 2008 Organizing Committee for pub-lication in Coling 2008 and for re-publishing in any form ormedium.and white), and they are associated when they tendto co-occur (doctor and hospital).On the surface, it appears that these are four dis-tinct semantic classes, requiring distinct NLP al-gorithms, but we propose a uniform approach toall four.
We subsume synonyms, antonyms, andassociations under analogies.
In essence, we saythat X and Y are antonyms when the pair X:Yis analogous to the pair black:white, X and Yare synonyms when they are analogous to the pairlevied:imposed, and X and Y are associated whenthey are analogous to the pair doctor:hospital.There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Du-mais, 1997), antonyms (Lin et al, 2003), and asso-ciations (Lesk, 1969), but each of these four taskshas been examined separately, in isolation from theothers.
As far as we know, the algorithm proposedhere is the first attempt to deal with all four tasksusing a uniform approach.
We believe that it isimportant to seek NLP algorithms that can han-dle a broad range of semantic phenomena, becausedeveloping a specialized algorithm for each phe-nomenon is a very inefficient research strategy.It might seem that a lexicon, such as Word-Net (Fellbaum, 1998), contains all the informationwe need to handle these four tasks.
However, weprefer to take a corpus-based approach to seman-tics.
Veale (2004) used WordNet to answer 374multiple-choice SAT analogy questions, achievingan accuracy of 43%, but the best corpus-based ap-proach attains an accuracy of 56% (Turney, 2006).Another reason to prefer a corpus-based approachto a lexicon-based approach is that the former re-quires less human labour, and thus it is easier toextend to other languages.In Section 2, we describe our algorithm for rec-ognizing analogies.
We use a standard supervised905machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor-pus.
We use a support vector machine (SVM)to learn how to classify the feature vectors (Platt,1998; Witten and Frank, 1999).Section 3 presents four sets of experiments.
Weapply our algorithm for recognizing analogies tomultiple-choice analogy questions from the SATcollege entrance test, multiple-choice synonymquestions from the TOEFL (test of English as aforeign language), ESL (English as a second lan-guage) practice questions for distinguishing syn-onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel-oped for experiments in cognitive psychology.We discuss the results of the experiments in Sec-tion 4.
The accuracy of the algorithm is competi-tive with other systems, but the strength of the al-gorithm is that it is able to handle all four tasks,with no tuning of the learning parameters to theparticular task.
It performs well, although it iscompeting against specialized algorithms, devel-oped for single tasks.Related work is examined in Section 5 and lim-itations and future work are considered in Sec-tion 6.
We conclude in Section 7.2 Classifying Analogous Word PairsAn analogy, A:B::C:D, asserts that A is to B as Cis to D; for example, traffic:street::water:riverbedasserts that traffic is to street as water is to riverbed;that is, the semantic relations between traffic andstreet are highly similar to the semantic relationsbetween water and riverbed.
We may view thetask of recognizing word analogies as a problemof classifying word pairs (see Table 1).Word pair Class labelcarpenter:wood artisan:materialmason:stone artisan:materialpotter:clay artisan:materialglassblower:glass artisan:materialtraffic:street entity:carrierwater:riverbed entity:carrierpackets:network entity:carriergossip:grapevine entity:carrierTable 1: Examples of how the task of recogniz-ing word analogies may be viewed as a problem ofclassifying word pairs.We approach this as a standard classificationproblem for supervised machine learning.
The al-gorithm takes as input a training set of word pairswith class labels and a testing set of word pairswithout labels.
Each word pair is represented as avector in a feature space and a supervised learningalgorithm is used to classify the feature vectors.The elements in the feature vectors are based onthe frequencies of automatically defined patternsin a large corpus.
The output of the algorithm is anassignment of labels to the word pairs in the test-ing set.
For some of the experiments, we selecta unique label for each word pair; for other ex-periments, we assign probabilities to each possiblelabel for each word pair.For a given word pair, such as mason:stone, thefirst step is to generate morphological variations,such as masons:stones.
In the following experi-ments, we use morpha (morphological analyzer)and morphg (morphological generator) for mor-phological processing (Minnen et al, 2001).1The second step is to search in a large corpus forall phrases of the following form:?
[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]?In this template, X:Y consists of morphologicalvariations of the given word pair, in either or-der; for example, mason:stone, stone:mason, ma-sons:stones, and so on.
A typical phrase for ma-son:stone would be ?the mason cut the stone with?.We then normalize all of the phrases that are found,by using morpha to remove suffixes.The template we use here is similar to Turney(2006), but we have added extra context wordsbefore the X and after the Y .
Our morpholog-ical processing also differs from Turney (2006).In the following experiments, we search in a cor-pus of 5 ?
1010 words (about 280 GB of plaintext), consisting of web pages gathered by a webcrawler.2 To retrieve phrases from the corpus, weuse Wumpus (Bu?ttcher and Clarke, 2005), an effi-cient search engine for passage retrieval from largecorpora.3The next step is to generate patterns from allof the phrases that were found for all of the in-put word pairs (from both the training and testingsets).
To generate patterns from a phrase, we re-place the given word pairs with variables, X andY , and we replace the remaining words with a wildcard symbol (an asterisk) or leave them as they are.1http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html.2The corpus was collected by Charles Clarke, Universityof Waterloo.
We can provide copies on request.3http://www.wumpus-search.org/.906For example, the phrase ?the mason cut the stonewith?
yields the patterns ?the X cut * Y with?, ?
*X * the Y *?, and so on.
If a phrase contains nwords, then it yields 2(n?2) patterns.Each pattern corresponds to a feature in the fea-ture vectors that we will generate.
Since a typi-cal input set of word pairs yields millions of pat-terns, we need to use feature selection, to reducethe number of patterns to a manageable quantity.For each pattern, we count the number of inputword pairs that generated the pattern.
For example,?
* X cut * Y *?
is generated by both mason:stoneand carpenter:wood.
We then sort the patterns indescending order of the number of word pairs thatgenerated them.
If there are N input word pairs(and thus N feature vectors, including both thetraining and testing sets), then we select the topkN patterns and drop the remainder.
In the fol-lowing experiments, k is set to 20.
The algorithmis not sensitive to the precise value of k.The reasoning behind the feature selection al-gorithm is that shared patterns make more usefulfeatures than rare patterns.
The number of features(kN ) depends on the number of word pairs (N ),because, if we have more feature vectors, then weneed more features to distinguish them.
Turney(2006) also selects patterns based on the numberof pairs that generate them, but the number of se-lected patterns is a constant (8000), independent ofthe number of input word pairs.The next step is to generate feature vectors, onevector for each input word pair.
Each of the Nfeature vectors has kN elements, one element foreach selected pattern.
The value of an element ina vector is given by the logarithm of the frequencyin the corpus of the corresponding pattern for thegiven word pair.
For example, suppose the givenpair is mason:stone and the pattern is ?
* X cut* Y *?.
We look at the normalized phrases thatwe collected for mason:stone and we count howmany match this pattern.
If f phrases match thepattern, then the value of this element in the fea-ture vector is log(f +1) (we add 1 because log(0)is undefined).
Each feature vector is then normal-ized to unit length.
The normalization ensures thatfeatures in vectors for high-frequency word pairs(traffic:street) are comparable to features in vectorsfor low-frequency word pairs (water:riverbed).Now that we have a feature vector for each in-put word pair, we can apply a standard supervisedlearning algorithm.
In the following experiments,we use a sequential minimal optimization (SMO)support vector machine (SVM) with a radial ba-sis function (RBF) kernel (Platt, 1998), as imple-mented in Weka (Waikato Environment for Knowl-edge Analysis) (Witten and Frank, 1999).4 Thealgorithm generates probability estimates for eachclass by fitting logistic regression models to theoutputs of the SVM.
We disable the normalizationoption in Weka, since the vectors are already nor-malized to unit length.
We chose the SMO RBFalgorithm because it is fast, robust, and it easilyhandles large numbers of features.For convenience, we will refer to the above algo-rithm as PairClass.
In the following experiments,PairClass is applied to each of the four problemswith no adjustments or tuning to the specific prob-lems.
Some work is required to fit each probleminto the general framework of PairClass (super-vised classification of word pairs) but the core al-gorithm is the same in each case.3 ExperimentsThis section presents four sets of experiments, withanalogies, synonyms, antonyms, and associations.We explain how each task is treated as a problemof classifying analogous word pairs, we give theexperimental results, and we discuss past work oneach of the four tasks.3.1 SAT AnalogiesIn this section, we apply PairClass to the taskof recognizing analogies.
To evaluate the perfor-mance, we use a set of 374 multiple-choice ques-tions from the SAT college entrance exam.
Table 2shows a typical question.
The target pair is calledthe stem.
The task is to select the choice pair thatis most analogous to the stem pair.Stem: mason:stoneChoices: (a) teacher:chalk(b) carpenter:wood(c) soldier:gun(d) photograph:camera(e) book:wordSolution: (b) carpenter:woodTable 2: An example of a question from the 374SAT analogy questions.The problem of recognizing word analogies wasfirst attempted with a system called Argus (Reit-4http://www.cs.waikato.ac.nz/ml/weka/.907man, 1965), using a small hand-built semantic net-work with a spreading activation algorithm.
Tur-ney et al (2003) used a combination of 13 in-dependent modules.
Veale (2004) used a spread-ing activation algorithm with WordNet (in effect,treating WordNet as a semantic network).
Turney(2006) used a corpus-based algorithm.We may view Table 2 as a binary classifica-tion problem, in which mason:stone and carpen-ter:wood are positive examples and the remainingword pairs are negative examples.
The difficulty isthat the labels of the choice pairs must be hiddenfrom the learning algorithm.
That is, the trainingset consists of one positive example (the stem pair)and the testing set consists of five unlabeled exam-ples (the five choice pairs).
To make this task moretractable, we randomly choose a stem pair fromone of the 373 other SAT analogy questions, andwe assume that this new stem pair is a negative ex-ample, as shown in Table 3.Word pair Train or test Class labelmason:stone train positivetutor:pupil train negativeteacher:chalk test hiddencarpenter:wood test hiddensoldier:gun test hiddenphotograph:camera test hiddenbook:word test hiddenTable 3: How to fit a SAT analogy question intothe framework of supervised pair classification.To answer the SAT question, we use PairClass toestimate the probability that each testing exampleis positive, and we guess the testing example withthe highest probability.
Learning from a trainingset with only one positive example and one nega-tive example is difficult, since the learned modelcan be highly unstable.
To increase the stability,we repeat the learning process 10 times, using adifferent randomly chosen negative training exam-ple each time.
For each testing word pair, the 10probability estimates are averaged together.
Thisis a form of bagging (Breiman, 1996).PairClass attains an accuracy of 52.1%.
Forcomparison, the ACL Wiki lists 12 previously pub-lished results with the 374 SAT analogy ques-tions.5 Only 2 of the 12 algorithms have higheraccuracy.
The best previous result is an accuracyof 56.1% (Turney, 2006).
Random guessing would5For more information, see SAT Analogy Questions (Stateof the art) at http://aclweb.org/aclwiki/.yield an accuracy of 20%.
The average seniorhigh school student achieves 57% correct (Turney,2006).3.2 TOEFL SynonymsNow we apply PairClass to the task of recogniz-ing synonyms, using a set of 80 multiple-choicesynonym questions from the TOEFL (test of En-glish as a foreign language).
A sample question isshown in Table 4.
The task is to select the choiceword that is most similar in meaning to the stemword.Stem: leviedChoices: (a) imposed(b) believed(c) requested(d) correlatedSolution: (a) imposedTable 4: An example of a question from the 80TOEFL questions.Synonymy can be viewed as a high degree ofsemantic similarity.
The most common way tomeasure semantic similarity is to measure the dis-tance between words in WordNet (Resnik, 1995;Jiang and Conrath, 1997; Hirst and St-Onge,1998).
Corpus-based measures of word similarityare also common (Lesk, 1969; Landauer and Du-mais, 1997; Turney, 2001).We may view Table 4 as a binary classifica-tion problem, in which the pair levied:imposed is apositive example of the class synonymous and theother possible pairings are negative examples, asshown in Table 5.Word pair Class labellevied:imposed positivelevied:believed negativelevied:requested negativelevied:correlated negativeTable 5: How to fit a TOEFL question into theframework of supervised pair classification.The 80 TOEFL questions yield 320 (80 ?
4)word pairs, 80 labeled positive and 240 labelednegative.
We apply PairClass to the word pairs us-ing ten-fold cross-validation.
In each random fold,90% of the pairs are used for training and 10%are used for testing.
For each fold, the model thatis learned from the training set is used to assignprobabilities to the pairs in the testing set.
With908ten separate folds, the ten non-overlapping test-ing sets cover the whole dataset.
Our guess foreach TOEFL question is the choice with the high-est probability of being positive, when paired withthe corresponding stem.PairClass attains an accuracy of 76.2%.
Forcomparison, the ACL Wiki lists 15 previously pub-lished results with the 80 TOEFL synonym ques-tions.6 Of the 15 algorithms, 8 have higher accu-racy and 7 have lower.
The best previous resultis an accuracy of 97.5% (Turney et al, 2003), ob-tained using a hybrid of four different algorithms.Random guessing would yield an accuracy of 25%.The average foreign applicant to a US universityachieves 64.5% correct (Landauer and Dumais,1997).3.3 Synonyms and AntonymsThe task of classifying word pairs as either syn-onyms or antonyms readily fits into the frameworkof supervised classification of word pairs.
Table 6shows some examples from a set of 136 ESL (En-glish as a second language) practice questions thatwe collected from various ESL websites.Word pair Class labelgalling:irksome synonymsyield:bend synonymsnaive:callow synonymsadvise:suggest synonymsdissimilarity:resemblance antonymscommend:denounce antonymsexpose:camouflage antonymsunveil:veil antonymsTable 6: Examples of synonyms and antonymsfrom 136 ESL practice questions.Lin et al (2003) distinguish synonyms fromantonyms using two patterns, ?from X to Y ?
and?either X or Y ?.
When X and Y are antonyms,they occasionally appear in a large corpus in oneof these two patterns, but it is very rare for syn-onyms to appear in these patterns.
Our approachis similar to Lin et al (2003), but we do not relyon hand-coded patterns; instead, PairClass patternsare generated automatically.Using ten-fold cross-validation, PairClass at-tains an accuracy of 75.0%.
Always guessingthe majority class would result in an accuracy of65.4%.
The average human score is unknown and6For more information, see TOEFL Synonym Questions(State of the art) at http://aclweb.org/aclwiki/.there are no previous results for comparison.3.4 Similar, Associated, and BothA common criticism of corpus-based measures ofword similarity (as opposed to lexicon-based mea-sures) is that they are merely detecting associations(co-occurrences), rather than actual semantic sim-ilarity (Lund et al, 1995).
To address this criti-cism, Lund et al (1995) evaluated their algorithmfor measuring word similarity with word pairs thatwere labeled similar, associated, or both.
Theselabeled pairs were originally created for cogni-tive psychology experiments with human subjects(Chiarello et al, 1990).
Table 7 shows some ex-amples from this collection of 144 word pairs (48pairs in each of the three classes).Word pair Class labeltable:bed similarmusic:art similarhair:fur similarhouse:cabin similarcradle:baby associatedmug:beer associatedcamel:hump associatedcheese:mouse associatedale:beer bothuncle:aunt bothpepper:salt bothfrown:smile bothTable 7: Examples of word pairs labeled similar,associated, or both.Lund et al (1995) did not measure the accuracyof their algorithm on this three-class classificationproblem.
Instead, following standard practice incognitive psychology, they showed that their al-gorithm?s similarity scores for the 144 word pairswere correlated with the response times of humansubjects in priming tests.
In a typical priming test,a human subject reads a priming word (cradle) andis then asked to complete a partial word (completebab as baby).
The time required to perform thetask is taken to indicate the strength of the cogni-tive link between the two words (cradle and baby).Using ten-fold cross-validation, PairClass at-tains an accuracy of 77.1% on the 144 word pairs.Since the three classes are of equal size, guessingthe majority class and random guessing both yieldan accuracy of 33.3%.
The average human scoreis unknown and there are no previous results forcomparison.9094 DiscussionThe four experiments are summarized in Tables 8and 9.
For the first two experiments, where thereare previous results, PairClass is not the best, butit performs competitively.
For the second two ex-periments, PairClass performs significantly abovethe baselines.
However, the strength of this ap-proach is not its performance on any one task, butthe range of tasks it can handle.As far as we know, this is the first time a stan-dard supervised learning algorithm has been ap-plied to any of these four problems.
The advantageof being able to cast these problems in the frame-work of standard supervised learning problems isthat we can now exploit the huge literature on su-pervised learning.
Past work on these problemshas required implicitly coding our knowledge ofthe nature of the task into the structure of the algo-rithm.
For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con-tains a theory of synonymy (Landauer and Dumais,1997).
The problem with this approach is that itcan be very difficult to work out how to modify thealgorithm if it does not behave the way we want.On the other hand, with a supervised learning algo-rithm, we can put our knowledge into the labelingof the feature vectors, instead of putting it directlyinto the algorithm.
This makes it easier to guidethe system to the desired behaviour.With our approach to the SAT analogy ques-tions, we are blurring the line between supervisedand unsupervised learning, since the training setfor a given SAT question consists of a single realpositive example (and a single ?virtual?
or ?simu-lated?
negative example).
In effect, a single exam-ple (mason:stone) becomes a sui generis; it con-stitutes a class of its own.
It may be possibleto apply the machinery of supervised learning toother problems that apparently call for unsuper-vised learning (for example, clustering or measur-ing similarity), by using this sui generis device.5 Related WorkOne of the first papers using supervised ma-chine learning to classify word pairs was Rosarioand Hearst?s (2001) paper on classifying noun-modifier pairs in the medical domain.
For ex-ample, the noun-modifier expression brain biopsywas classified as Procedure.
Rosario and Hearst(2001) constructed feature vectors for each noun-modifier pair using MeSH (Medical Subject Head-ings) and UMLS (Unified Medical Language Sys-tem) as lexical resources.
They then trained a neu-ral network to distinguish 13 classes of semanticrelations, such as Cause, Location, Measure, andInstrument.
Nastase and Szpakowicz (2003) ex-plored a similar approach to classifying general-domain noun-modifier pairs, using WordNet andRoget?s Thesaurus as lexical resources.Turney and Littman (2005) used corpus-basedfeatures for classifying noun-modifier pairs.
Theirfeatures were based on 128 hand-coded patterns.They used a nearest-neighbour learning algorithmto classify general-domain noun-modifier pairsinto 30 different classes of semantic relations.
Tur-ney (2006) later addressed the same problem using8000 automatically generated patterns.One of the tasks in SemEval 2007 was the clas-sification of semantic relations between nominals(Girju et al, 2007).
The problem is to classifysemantic relations between nouns and noun com-pounds in the context of a sentence.
The taskattracted 14 teams who created 15 systems, allof which used supervised machine learning withfeatures that were lexicon-based, corpus-based, orboth.PairClass is most similar to the algorithm of Tur-ney (2006), but it differs in the following ways:?
PairClass does not use a lexicon to find syn-onyms for the input word pairs.
One of ourgoals in this paper is to show that a purecorpus-based algorithm can handle synonymswithout a lexicon.
This considerably simpli-fies the algorithm.?
PairClass uses a support vector machine(SVM) instead of a nearest neighbour (NN)learning algorithm.?
PairClass does not use the singular valuedecomposition (SVD) to smooth the featurevectors.
It has been our experience that SVDis not necessary with SVMs.?
PairClass generates probability estimates,whereas Turney (2006) uses a cosine mea-sure of similarity.
Probability estimates canbe readily used in further downstream pro-cessing, but cosines are less useful.?
The automatically generated patterns in Pair-Class are slightly more general than the pat-terns of Turney (2006).?
The morphological processing in PairClass(Minnen et al, 2001) is more sophisticatedthan in Turney (2006).910Experiment Number of vectors Number of features Number of classesSAT Analogies 2,244 (374 ?
6) 44,880 (2, 244 ?
20) 374TOEFL Synonyms 320 (80 ?
4) 6,400 (320 ?
20) 2Synonyms and Antonyms 136 2,720 (136 ?
20) 2Similar, Associated, and Both 144 2,880 (144 ?
20) 3Table 8: Summary of the four tasks.
See Section 3 for explanations.Experiment Accuracy Best previous Human Baseline RankSAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15Synonyms and Antonyms 75.0% none unknown 65.4% noneSimilar, Associated, and Both 77.1% none unknown 33.3% noneTable 9: Summary of experimental results.
See Section 3 for explanations.However, we believe that the main contribution ofthis paper is not PairClass itself, but the extensionof supervised word pair classification beyond theclassification of noun-modifier pairs and seman-tic relations between nominals, to analogies, syn-onyms, antonyms, and associations.
As far as weknow, this has not been done before.6 Limitations and Future WorkThe main limitation of PairClass is the need for alarge corpus.
Phrases that contain a pair of wordstend to be more rare than phrases that contain ei-ther of the members of the pair, thus a large cor-pus is needed to ensure that sufficient numbers ofphrases are found for each input word pair.
Thesize of the corpus has a cost in terms of disk spaceand processing time.
In the future, as hardware im-proves, this will become less of an issue, but theremay be ways to improve the algorithm, so that asmaller corpus is sufficient.Another area for future work is to apply Pair-Class to more tasks.
WordNet includes more thana dozen semantic relations (e.g., synonyms, hy-ponyms, hypernyms, meronyms, holonyms, andantonyms).
PairClass should be applicable to allof these relations.
Other potential applications in-clude any task that involves semantic relations,such as word sense disambiguation, informationretrieval, information extraction, and metaphor in-terpretation.7 ConclusionIn this paper, we have described a uniform ap-proach to analogies, synonyms, antonyms, and as-sociations, in which all of these phenomena aresubsumed by analogies.
We view the problem ofrecognizing analogies as the classification of se-mantic relations between words.We believe that most of our lexical knowledgeis relational, not attributional.
That is, meaning islargely about relations among words, rather thanproperties of individual words, considered in iso-lation.
For example, consider the knowledge en-coded in WordNet: much of the knowledge inWordNet is embedded in the graph structure thatconnects words.Analogies of the form A:B::C:D are calledproportional analogies.
These types of lower-level analogies may be contrasted with higher-level analogies, such as the analogy between thesolar system and Rutherford?s model of the atom(Falkenhainer et al, 1989), which are sometimescalled conceptual analogies.
We believe that thedifference between these two types is largely amatter of complexity.
A higher-level analogy iscomposed of many lower-level analogies.
Progresswith algorithms for processing lower-level analo-gies will eventually contribute to algorithms forhigher-level analogies.The idea of subsuming a broad range of se-mantic phenomena under analogies has been sug-gested by many researchers.
Minsky (1986) wrote,?How do we ever understand anything?
Almostalways, I think, by using one or another kind ofanalogy.?
Hofstadter (2007) claimed, ?all meaningcomes from analogies.?
In NLP, analogical algo-rithms have been applied to machine translation(Lepage and Denoual, 2005), morphology (Lep-age, 1998), and semantic relations (Turney andLittman, 2005).
Analogy provides a frameworkthat has the potential to unify the field of seman-tics.
This paper is a small step towards that goal.AcknowledgementsThanks to Joel Martin and the anonymous review-ers of Coling 2008 for their helpful comments.911ReferencesBreiman, Leo.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Bu?ttcher, Stefan and Charles Clarke.
2005.
Efficiencyvs.
effectiveness in terabyte-scale information re-trieval.
In Proceedings of the 14th Text REtrievalConference (TREC 2005), Gaithersburg, MD.Chiarello, Christine, Curt Burgess, Lorie Richards, andAlma Pollock.
1990.
Semantic and associativepriming in the cerebral hemispheres: Some wordsdo, some words don?t ... sometimes, some places.Brain and Language, 38:75?104.Falkenhainer, Brian, Kenneth D. Forbus, and DedreGentner.
1989.
The structure-mapping engine:Algorithm and examples.
Artificial Intelligence,41(1):1?63.Fellbaum, Christiane, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic re-lations between nominals.
In SemEval 2007, pages13?18, Prague, Czech Republic.Hirst, Graeme and David St-Onge.
1998.
Lexicalchains as representations of context for the detec-tion and correction of malapropisms.
In Fellbaum,Christiane, editor, WordNet: An Electronic LexicalDatabase, pages 305?332.
MIT Press.Hofstadter, Douglas.
2007.
I Am a Srange Loop.
BasicBooks.Jiang, Jay J. and David W. Conrath.
1997.
Seman-tic similarity based on corpus statistics and lexicaltaxonomy.
In ROCLING X, pages 19?33, Tapei, Tai-wan.Landauer, Thomas K. and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent seman-tic analysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104(2):211?240.Lepage, Yves and Etienne Denoual.
2005.
Purestever example-based machine translation: Detailedpresentation and assessment.
Machine Translation,19(3):251?282.Lepage, Yves.
1998.
Solving analogies on words: Analgorithm.
In Proceedings of the 36th Annual Con-ference of the Association for Computational Lin-guistics, pages 728?735.Lesk, Michael E. 1969.
Word-word associations indocument retrieval systems.
American Documenta-tion, 20(1):27?38.Lin, Dekang, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distri-butionally similar words.
In IJCAI-03, pages 1492?1493.Lund, Kevin, Curt Burgess, and Ruth Ann Atchley.1995.
Semantic and associative priming in high-dimensional semantic space.
In Proceedings of the17th Annual Conference of the Cognitive Science So-ciety, pages 660?665.Minnen, Guido, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Minsky, Marvin.
1986.
The Society of Mind.
Simon &Schuster, New York, NY.Nastase, Vivi and Stan Szpakowicz.
2003.
Explor-ing noun-modifier semantic relations.
In Fifth In-ternational Workshop on Computational Semantics(IWCS-5), pages 285?301, Tilburg, The Netherlands.Platt, John C. 1998.
Fast training of support vectormachines using sequential minimal optimization.
InAdvances in Kernel Methods: Support Vector Learn-ing, pages 185?208.
MIT Press Cambridge, MA,USA.Reitman, Walter R. 1965.
Cognition and Thought: AnInformation Processing Approach.
John Wiley andSons, New York, NY.Resnik, Philip.
1995.
Using information contentto evaluate semantic similarity in a taxonomy.
InIJCAI-95, pages 448?453, San Mateo, CA.
MorganKaufmann.Rosario, Barbara and Marti Hearst.
2001.
Classify-ing the semantic relations in noun-compounds viaa domain-specific lexical hierarchy.
In EMNLP-01,pages 82?90.Turney, Peter D. and Michael L. Littman.
2005.Corpus-based learning of analogies and semantic re-lations.
Machine Learning, 60(1?3):251?278.Turney, Peter D., Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining indepen-dent modules to solve multiple-choice synonym andanalogy problems.
In RANLP-03, pages 482?489,Borovets, Bulgaria.Turney, Peter D. 2001.
Mining the Web for syn-onyms: PMI-IR versus LSA on TOEFL.
In Proceed-ings of the Twelfth European Conference on MachineLearning, pages 491?502, Berlin.
Springer.Turney, Peter D. 2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Veale, Tony.
2004.
WordNet sits the SAT: Aknowledge-based approach to lexical analogy.
InProceedings of the 16th European Conference onArtificial Intelligence (ECAI 2004), pages 606?612,Valencia, Spain.Witten, Ian H. and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann, SanFrancisco.912
