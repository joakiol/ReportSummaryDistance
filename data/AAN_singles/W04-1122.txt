An Integrated Method for Chinese Unknown Word Extraction1LUO ZhiyongCollege of Computer ScienceBeijing University ofTechnologyBeijing, PRC 100022Center for LanguageInformation ProcessingBeijing Language and CultureUniversityBeijing, PRC 100083luo_zy@blcu.edu.cnSONG RouCenter for LanguageInformation ProcessingBeijing Language and CultureUniversityBeijing, PRC 100083songrou@blcu.edu.cn1 This paper is supported by NSFC (60272055) and 863 Project (2001AA114111)AbstractUnknown word recognition is an importantproblem in Chinese word segmentation systems.In this paper, we propose an integrated methodfor Chinese unknown word extraction for off-line corpus processing, in which both context-entropy (on each side) and frequency ratioagainst background corpus are introduced toevaluate the candidate words.
Both of the meas-ures are computed efficiently on Suffix arraywith much less space overhead.
Our method canalso be reinforced when combined with a basicSegmentor by boundary-verification and arbi-trary n-gram words can be extracted by ourmethod.
We test our method on Chinese novelXiao Ao Jiang Hu, and obtain satisfactoryachievements compared to traditional criteriasuch as Likelihood Ratio.1 IntroductionThe unique feature of Chinese writing system isthat it is character-based, not word-based.
The factthat there are no delimiters between words poses thewell-known problem of word segmentation.
AnyChinese Information Processing (CIP) systems be-yond character level, such as information retrieval,automatic proofreading, text classification, text-to-speech conversion, syntactic parser, information ex-traction and machine translation, etc.
should have abuilt-in word segmentation block.
Currently, dic-tionary-based method is the basic and efficient onefor word segmentation.
A fixed Chinese electronicdictionary is required for most CIP systems.
Yetthere are many unknown words (out of the fixed dic-tionary) coming into being all the time.
The un-known words are diverse, including proper nouns(person names, place names, organization names,etc.
), domain-specific terminological nouns and ab-breviations, even author-coined terms, etc.
and theyappear frequently in real text.
This may cause ambi-guity in Chinese word segmentation and lead to er-rors in the applications.
Presently, many systems(Tan et al 1999), (Liu, 2000), (Song, 1993), (Luo etal, 2001) focus on online recognition of propernouns, and have achieved inspiring results in news-corpus but will be deteriorated in special text, suchas spoken corpus, novels.
As to the rests of unknownwords types, it is still the obstacle of application sys-tems, although they are really important for specificcollections of texts.For instance, according to our count on Chinesenovel Xiao Ao Jiang Hu (??????)
(JIN Yong(??
), 1967), there are almost 515 unknown wordtypes (out of our 243,539-item general dictionary) oftotal 39,404 occurrences and total 112,654 charac-ters, and there are 983,134 characters overall in thisnovel (that is, about 11.46% characters of the wholenovel are occupied by unknown words.).
And mostof them, such as ??????
(person name), ?????
?
(normal noun), ?????
?
(organizationname), etc.
can?t be recognized by most current CIPsystems.
It is important to note that without efficientunknown word extraction method, most CIP systemscan?t obtain satisfactory results.2 Relative research worksOffline unknown word extraction can be treated asa special kind of Automatic Term Extraction (ATE).There are many research works on ATE.
And mostsuccessful systems are based on statistics.
Many sta-tistical metrics have been proposed, including point-wise mutual information (MI) (Church et al 1990),mean and variance, hypothesis testing (t-test, chi-square test, etc.
), log-likelihood ratio (LR) (Dunning,1993), statistic language model (Tomokiyo, et al2003), and so on.
Point-wise MI is often used to findinteresting bigrams (collocations).
However, MI isactually better to think of it as a measure of inde-pendence than of dependence (Manning et al 1999).LR is one of the most stable methods for ATE so far,and more appropriate for sparse data than other met-rics.
However, LR is still biased to two frequentwords that are rarely adjacent, such as the pair (the,the) (Pantel et al 2001).
On the other aspect, MI andLR metrics are difficult to extend to extract multi-word terms.Relative frequency ratio (RFR) of terms betweentwo different corpora can also be used to discoverdomain-oriented multi-word terms that are charac-teristic of a corpus when compared with another(Damerau, 1993).
In this paper, RFR values betweensource corpus and background one will be used torank the final candidate-list.There are also many hybrid methods combinedstatistical metrics with linguistic knowledge, such asPart-of-Speech filters (Smadja, 1994).
But POS fil-ters are not appropriate for Chinese term extraction.Since all the terms extraction approaches need toaccess all the possible patterns and find their fre-quency of occurrence, a highly efficient data struc-ture based on PAT-tree (Chien, 1997), (Chien, 1998)and (Thian et al 1999) has been used popularly forthis purpose.
However, PAT-tree still has muchspace overhead, and is very expensive for construc-tion.
Now, we introduce an alternative data structureas Suffix array, with much less space overhead, tocommit this task.In this paper, we propose a four-phase offline un-known word extraction method: (a) Construct theSuffix arrays of source text and background corpus.In this phase, Suffix arrays, sorted on both left andright sides context for each occurrence of Chinesecharacter, are constructed.
We call them Left-indexand Right-index respectively; (b) Extract frequent n-gram candidate terms.
In this phase, firstly we ex-tract n-grams, appearing more than one time in dif-ferent contexts according to Left-index and Right-index of source text, into Left-list and Right-list re-spectively.
Then, we combine Left-list with Right-list, and extract n-grams which appear in both ofthem as candidates (C-list, for short).
We also com-pute frequency, context-entropy and relative fre-quency ratio against background corpus for eachcandidate in this phase; (c) Filter candidates in C-listwith context-entropy and boundary-verification cou-pled with General Purpose Word Segmentation Sys-tem (GPWS) (Lou et al 2001).
In this phase, wesegment each sentence, where each candidate ap-pears, in the source text with GPWS and eliminatethe candidates cross word boundary; (d) Output thefinal terms on relative frequency ratios.The remainder of our paper is organized as fol-lows: Section 2 describes the candidate terms extrac-tion approach on Suffix array.
Section 3 describesthe candidates?
filter approach on context-entropyand boundary-verification coupled with GPWS.
Sec-tion 4 describes the relative frequency ratios andoutput of the final list.
Section 5 gives our experi-mental result and Section 6 gives conclusion andfuture work.3 Candidates extraction on Suffix arraySuffix array (also known as String PAT-array)(Manber et al 1993) is a compact data struc-ture to handle arbitrary-length strings and performsmuch powerful on-line string search operations suchas the ones supported by PAT-tree, but has lessspace overhead.Definition 1.
Let X = x0x1x2..xn-1xn as a string oflength n. For the sake of left and right context sort-ing, we have extended X by inserting two uniqueterminators ($, less than all of the characters) as sen-tinel symbols at both ends of it, i.e.
x0 = xn = $ in X.Let LSi = xixi-1..x0 (RSi = xixi+1..xn) as the left (right)suffix of X that starts at position i.The Suffix array Left-index[0..n] (Right-index[0..n]) is an array of indexes of LSi (RSi),where LSLeft-index[i] < LSLeft-index[j] (RSRight-index[i] <RSRight-index[j]), i<j, in lexicological order.Let LLCP[i] (RLCP[i]), i=0..n-1, as the length ofLongest Common Prefix (LCP) between two adja-cent suffix strings, LSLeft-index[i] and           LSLeft-index[i+1] (RSRight-index[i] and RSRight-index[i+1]).
These ar-rays on both sides are assistant data structures forspeeding string search.Figure 1 shows a simple Suffix array sorted on leftand right context, coupled with the LCP arrays re-spectively.We apply the sort-algorithm proposed by (Manberet al 1993), which takes O(nlogn) in worst casesperformance, to construct the Suffix arrays, and sortall the suffix strings in UNICODE order.Figure 2 shows fragments of Suffix arrays of testcorpus Xiao Ao Jiang Hu in readable style.Sorted suffix arrays have clustered all similar n-grams (of arbitrary length) into continuous blocksand the frequent string patterns, as the longest com-mon prefix (LCP) of adjacent strings, can be ex-tracted by scanning through the suffix arrays sortedon left context and right respectively.String ?tobeornottobe?# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14String $ t o b e o r n o t t o b e $Suffix array# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14Left-index 0 14 3 12 4 13 7 5 8 2 11 6 1 9 10Right-index 0 14 12 3 13 4 7 11 2 5 8 6 10 1 9LCP arrays on both sides# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14LLCP 0 0 3 0 4 0 0 1 1 2 0 0 1 1 /RLCP 0 0 2 0 1 0 0 3 1 1 0 0 4 1 /Figure 1: Suffix array example*Left part is fragment of Left Suffix array, starts at position of Chinese character ???
*Right part is fragment of Right Suffix array, starts at position of Chinese character ??
?Figure 2: Fragments of Suffix array of Xiao Ao Jiang Hu????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?As show in Figure 2, on right sorted part whichstarts at the position of Chinese Character ??
?, wecan extract the repeated n-grams, such as ??????
?, ??????
?, ??????
?, ??????
?, ???????
?, ?????
?, etc., in turn andskip many substrings, such as ???
?, ????
?,etc., because they are not the LCP of adjacent suffixstrings and only appear in the upper string ??????
for their all occurrences.
We can apply the sameskill on left sorted part which start at the position ofChinese character ??
?, and extract ???????,??????
?, ??????
?, ?????????
?, ???????
?, ?????
?, etc., as re-peated n-grams and skip many substrings, such as???
?, ????
?, etc., for the same reasons.To extract candidate terms, we can scan throughboth left and right Suffix arrays and select all re-peated n-grams into Left-list and Right-list respec-tively.
The terms, which appear in both lists, can betreated as candidates (denoted by C-list).
Extractionprocedure can be done efficiently by coupled withthe arrays of length of LCP on both sides via stackoperations.
The length and frequency of candidatescan also be computed in this procedure.For example in Figure 2, term ??????
shouldappear in both Left-list and Right-list, and it is agood candidate.
Yet n-grams ???????
is not acandidate because even though ???????
doesappear in Right-list, it does not exist in our finalLeft-list (It always appears as a substring of directupper string ????????
according to rightpart of Figure 2).Term TCLeftCon-text-entropyRightCon-text-entropyRFR???
5922 6.6804 4.9900 22743.7??
1267 4.7974 3.8534 0.9???
1184 5.9656 4.8688 10104.9??
1123 4.8512 4.1473 1.0??
1053 5.5446 4.7758 89.8???
929 5.7310 4.7623 7928.6???
919 5.5887 4.5220 7843.2??
532 0.0930 4.4570 170.2??
528 5.5960 0.0412 1013.9???
525 5.5891 4.4294 4480.6????
320 4.6805 4.8253 2731.0???
284 4.0897 0.0585 2423.8????
281 4.0624 3.7344 2398.2???
176 4.3386 4.0105 1502.0????
156 1.7374 2.0613 1331.3????
153 4.6941 4.4650 1305.7???
103 4.3266 3.4258 879.0???
97 4.2815 3.1410 827.8???
80 3.0207 2.7821 682.7????
73 3.6620 3.9186 623.0Table 1: Examples of candidates order by TCTable 1 lists many examples of candidates ex-tracted from Xiao Ao Jiang Hu, order by term count(TC).4 Filter candidate termsAs what show in Table 1, not all the terms in C-list extracted in Section 3 can be treated as signifi-cant terms because of their incomplete lexicalboundaries.
There two kinds of incomplete-boundary terms: (1) terms as substring of significantterms; (2) terms overlapping the boundaries of adja-cent significant terms.
In this section, we will takemeasures, including Context-entropy test andboundary-verification with common Segmentor(GPWS) with general lexicon, to eliminate theseinvalid candidates respectively.4.1 Measure on Context-entropyAccording to our investigation, significant termsin specific collection of texts can be used frequentlyand in different contexts.
On the other hand sub-string of significant term almost locates in its corre-sponding upper string (that is, in fixed context) eventhrough it occur frequently.
In this part, we proposea metric Context-entropy as a measure of this fea-ture to filter out substrings of significant terms.Definition 2.
Assume ?
as a candidate termwhich appears n times in corpus X, ?
={a1,a2,?,as}(?= {b1,b2,?,bt}) as a set of left (right)side contexts of ?
in X.Left and right Context-entropy of ?
in X can bedefine as:??=???
?iaii naCaCnLCE ),(log),(1)(,??=???
?ibii nbCbCnRCE ),(log),(1)(where ????==???
?ii biai bCaCn ),(),( ,C(ai,?)
(C(?,bi)) is count of concurrence of ai and?
(?
and bi) in X.Significant terms, which can be used in differentcontext, will get high values of Context-entropy onboth sides.
And the substrings, which almostemerge because of their upper strings, will get com-parative low values.
The 3rd and 4th columns ofTable 1 show the values of Context-entropy on bothsides of a list of candidate terms.
Many candidates,which almost emerge because of their direct upperstrings, such as ????
(in ?????
(person name)),????
(in ?????
(person name)) , ?????(in??????
(organization name)), appear in rela-tively fixed contexts and should get much lowervalue(s) of one or both sides of Context-entropy.4.2 Boundary-verification with GPWSThe candidate list of terms includes all of the n-grams, which appear in different context on bothsides more than ones.
The unique feature of Chinesewriting system is that there are no delimiters be-tween words poses a big problem: Many of candi-date terms are invalid because of the overlappedfactual words?
boundary, i.e.
these candidates in-clude several fragments of adjacent words, such as????
(overlapping the boundary of common word????
(Hua Mountain)), ?????
(overlapping theboundary of common word ????
(Sir)), etc.
listedin Table 2.
We eliminate these candidates by verify-ing boundaries of them with a common Segmentor(GPWS (Lou et al 2001)) and a general lexicon(with 243,539 words).GPWS was built as shared framework undertak-ing different CIP applications.
It has achieved verygood performance and great adaptability across dif-ferent application domains in disambiguation,identification of proper nouns (including Chinesenames, Chinese place names, translated names offoreigners, organization and company names, etc.
),identification of high-frequency suffix phrases andnumbers.
In this part, we ONLY use the utilities ofGPWS to perform the Maximum Match (MM) tofind the boundaries of words in lexicon, and all ofthe unknown words (out of our lexicon) will be seg-mented into pieces.
Coupled with GPWS, wepropose a voting mechanism for boundary-verification as follows:For each candidate term in C-list as termBeginDeclare falseNum as integer for the number of invalidboundary-check of term;Declare trueNum as integer for the number of validboundary-check of term;falseNum = 0;trueNum = 0;For each sentence, in which term appears, in fore-ground corpus, as sentBeginSegment sent with GPWS;Compare the term?s position in sent with thesegment result of GPWS;If term crosses the adjacent words boundarySet falseNum = falseNum+1;ElseSet trueNum = trueNum+1;EndIf  falseNum > trueNumSet boundary-verification flag of term to FALSE;ElseSet boundary-verification flag of term to TRUE;EndAssistant with the segmentor, we eliminate38,697 items of total 117,807 in C-list in 96.85% ofprecision.
Table 2 shows many examples of candi-dates eliminated by sides-verification with GPWS.CandidatetermSegment result of GPWS forone sentence, in which termappears??
??/??/?/?/??/????/????
??/??/??/?/?/??/??/?/????
??/??/?/?/??/?/??/?/?/???/?/?/??/???
?/??/??/?
?/?/?/?Table 2: Examples of candidates eliminated byGPWS5 Relative frequency ratio against backgroundcorpusRelative frequency ratio (RFR) is a useful methodto be used to discover characteristic linguistic phe-nomena of a corpus when compared with another(Damerau, 1993).
RFR of term ?
in corpus X com-pared with another corpus Y, RFR(?
;X,Y), simplycompares the frequency of ?
in X (denoted asf(?,X)) to ?
in Y (denoted as f(?,Y)):RFR(?
;X,Y) = f(?,X)/f(?,Y)RFR of term is based upon the fact that the sig-nificant terms will appear frequently in specificcollection of text (treated as foreground corpus) butrarely or even not in other quite different corpus(treated as background corpus).
The higher of RFRvalues of the terms, the more informative of theterms will be in foreground corpus than in back-ground one.However, selection of background corpus is animportant problem.
Degree of difference betweenforeground and background corpus is rather difficultto measure and it will affect the values of RFR ofterms.
Commonly, large and general corpora will betreated as background corpus for comparison.
Inthis paper, for our foreground corpus (Xiao Ao Ji-ang Hu), we experientially select a group of novelsof the same author excluding Xiao Ao Jiang Hu ascompared background corpus for some reasons asfollows:(a) Same author wrote all of the novels, includingforeground and background.
The unique n-grams in writing style of the author will notemerge on RFR values.
(b) All of the novels are in the same category.
Thespecific n-grams for this category will notemerge on RFR values.So, most of the candidate terms with higher RFRvalues will be more informative and be more sig-nificant for the source novel.On the final phase, we will sort all of the filteredcandidate terms on RFR values in desc-order so thatthe forepart of the final list will get high precisionfor extraction.The last column of Table 1 shows the RFR valuesof many candidates compared with our backgroundcorpus.
Many candidates, such as ???
?, ???
?,which are frequent in both foreground and back-ground corpus, will get much lower RFR values andwill be eliminated from our final top list.6 Experimental resultWe use novel Xiao Ao Jiang Hu as foregroundcorpus compared with the rest of novels of Mr. JINYong as background corpus.
The total characters offoreground and background corpus are 983,134 and7,551,555 respectively.
We read through the novelXiao Ao Jiang Hu and 5 graduates manually se-lected 515 new terms (out of our lexicon) with exactmeaning in the novel as follows for the final test:(a) Proper nouns, such as person names: ?????,?????
?, ?????
?, place names: ????
?, ????
?, ?????
?, organizationnames: ?????
?, ??????
etc.
(b) Normal nouns, such as ?????
?, ?????
?, etc.
(c) Others, such as ???
?, ???
?, etc.By our method, we extract 117,807 candidates inthis novel.
Table 3 shows the result after filteringwith Context-entropy on both sides and boundary-verification on different total extracted numbers;We also compared our integrated method to tradi-tional measure LR.
On lower total number levels,LR will overrun our method in unknown-word re-call, and in turn overrun by us on higher levels.
Asto precision, our method always keeps ahead.We also notice that both of the methods havemuch low precision in extraction.
To retrieve termswith much certain, we rank the entire final list onRFR values in final phase.
Most significant termswill comes in the front of ranked list.Table 3 shows that our method Table 4 shows thetop 12 of final list, and Figure 3 shows the perform-ance of our method on different top levels whenranks the final list on RFR values.7 ConclusionUnknown word recognition is an important prob-lem in CIP systems.
Suffix array based method is anefficient method for exact arbitrary-length frequentterms.
And most of substring of significant terms,which almost appear in fixed contexts, can beeliminated by Context-entropy values.
Large lexi-con can help to verify the unknown word doundarisand filter incomplete-boundary n-grams.
Most sig-nificant informative candidates list on the top offinal list according to RFR values for subsequentmanual confirmation, and on the other aspect, RFRalso reflects the internal character of the extractedterms.Total NumberExtractedWordinDictUnknownWords PrecisionUnknown-wordsRecallOurmethod 306 57 0.68 0.11 534LR 222 103 0.61 0.20Ourmethod 668 126 0.60 0.24 1325LR 421 171 0.49 0.33Ourmethod 1411 225 0.55 0.44 2996LR 888 287 0.39 0.56Ourmethod 2877 346 0.50 0.67 6498LR 1608 366 0.30 0.71Ourmethod 4,643 512 0.44 0.99 11684LR 2,428 427 0.24 0.83Table 3: Result of our method compared to LRTerm TF RFRLeft Con-text-entropyRightContext-entropy???
5922 22743.7 6.6804 4.9900???
1184 10104.9 5.9656 4.8688???
929 7928.6 5.7310 4.7623???
919 7843.2 5.5887 4.5220????
915 7809.1 5.5789 4.2271??
729 6221.6 5.5360 4.4128???
722 6161.9 5.5751 4.7080???
553 4719.6 4.7371 3.8601???
525 4480.6 5.5891 4.4294???
516 4403.8 5.4427 4.1689???
482 4113.6 5.3223 4.7837??
414 3533.3 5.2607 2.6043Table 4: Top 12 terms of final list order by RFRReferencesChien, L-F. 1997, PAT-Tree-Based Keyword Extractionfor Chinese Information Retrieval.
Proceedings of the1997 ACM SIGIR, Philadelphia, PA, USA, pp.
50-58.Chien, L-F. 1998, PAT-Tree-Based Adaptive Key phraseExtraction for Intelligent Chinese Information Retrieval.In special issue on Information Retrieval with AsianLanguages, Information Processing and Management,Elsevier Press.Christopher D. Manning, Hinrich Schutze.
1999.
Founda-tions of Statistical Natural Language Processing, MITPress.Dekai Wu and Pascale Fung.
1994.
Improving Chinesetokenization with linguistic filters on statistical lexicalacquisition.
In Proceedings of the Fourth ACL Confer-ence on Applied Natural Language Processing(ANLP94), Stuttgart, Germany.Frank Z. Smadja.
1994.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1): 143-177.Fred J. Damerau.
1993.
Generating and evaluating do-main-oriented multi-word terms from texts.
InformationProcessing and Management, 29(4): 433?447.H.Y.
Tan, J.H.
Zheng, K.Y.
Liu.
1999.
A Study on theAutomatic Recognition of Chinese Place Names,Proceedings of the 5th Joint Conference onComputational Linguistics 99, Tsinghua UniversityPress.
Kenneth W. Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information, and lexicography.Computational Linguistics, volume 16.Kunihiko Sadakane.
1998.
A fast algorithm for makingsuffix arrays and for Burrows-Wheeler transformation,Proceedings of the ieee Data Compression Conference,pp.
129?138.K.Y.
Liu.
2000.
Automatic Segmentation and Tagging forChinese Text, Commercial Press.Manber, U. and Myers, G. 1993.
Suffix Arrays: A NewMethod for On-Line String Searches.
SIAM Journal onComputing 22, 935-948.R.
Song.
1993.
Recognition of Personal Names Based onCorpus and Rules, Journal of Computational Linguis-tics: Research and Applications, Beijing Language In-stitute Press.R.
Song.
1998.
The Geometric Structures of ChineseWords and Phrases, International Conference on Chi-nese Grammers 98, Beijing.Patrick Pantel and Dekang Lin.
2001.
A statistical corpus-based term extractor.
In E. Stroulia and S. Matwin, edi-tors, Lecture Notes in Artificial Intelligence, pages 36?46.
Springer-Verlag.Ted E. Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguistics,19 (1): 61?74.Thian-Huat Ong, Hsinchun Chen.
1999.
Updateable PAT-Tree Approach to Chinese Key Phrase Extraction usingMutual Information: A Linguistic Foundation forKnowledge Management, Proceedings of the SecondAsian Digital Library Conference, November 8-9, pp.63-84.T.
Lou, R. Song, W.L.
Li, and Z.Y.
Luo.
2001.
The de-sign and Implementation of a Modern General PurposeSegmentation System, Journal of Chinese InformationProcessing, Issue No.
5.T.
Tomokiyo and M. Hurst.
2003.
A Language ModelApproach to Keyphrase Extraction.
ACL-2003 Work-shop on Multiword Expressions: Analysis, Acquisitionand Treatment.Z.Y.
Luo, R. Song.
2001.
Integrated and Fast Recogni-tion of Proper Noun in Modern Chinese Word Segmen-tation, ICCC, Singapore.Figure 3: Test result on different top levels01020304050607080901005 20 80 120 160 200 240 280 320 360 400 440 480 520 560 600 640 680Top levelsPercent(%)PrecisionRecall
