Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297?304Manchester, August 2008Dependency-Based N-Gram Models forGeneral Purpose Sentence RealisationYuqing GuoNCLT, School of ComputingDublin City UniversityDublin 9, Irelandyguo@computing.dcu.ieJosef van GenabithNCLT, School of ComputingDublin City UniversityIBM CAS, Dublin, Irelandjosef@computing.dcu.ieHaifeng WangToshiba (China)Research & Development CenterBeijing, 100738, Chinawanghaifeng@rdc.toshiba.com.cnAbstractWe present dependency-based n-grammodels for general-purpose, wide-coverage, probabilistic sentence realisa-tion.
Our method linearises unordereddependencies in input representationsdirectly rather than via the applicationof grammar rules, as in traditional chart-based generators.
The method is simple,efficient, and achieves competitive accu-racy and complete coverage on standardEnglish (Penn-II, 0.7440 BLEU, 0.05sec/sent) and Chinese (CTB6, 0.7123BLEU, 0.14 sec/sent) test data.1 IntroductionSentence generation,1 or surface realisation can bedescribed as the problem of producing syntacti-cally, morphologically, and orthographically cor-rect sentences from a given semantic or syntacticrepresentation.Most general-purpose realisation systems de-veloped to date transform the input into sur-face form via the application of a set of gram-mar rules based on particular linguistic theories,e.g.
Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Com-binatory Categorial Grammar (CCG), Tree Ad-joining Grammar (TAG) etc.
These grammar rulesare either carefully handcrafted, as those used inFUF/SURGE (Elhadad, 1991), LKB (Carroll et al,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1In this paper, the term ?generation?
is used generally forwhat is more strictly referred to by the term ?tactical genera-tion?
or ?surface realisation?.1999), OpenCCG (White, 2004) and XLE (Crouchet al, 2007), or created semi-automatically (Belz,2007), or fully automatically extracted from an-notated corpora, like the HPSG (Nakanishi etal., 2005), LFG (Cahill and van Genabith, 2006;Hogan et al, 2007) and CCG (White et al,2007) resources derived from the Penn-II Treebank(PTB) (Marcus et al, 1993).Over the last decade, probabilistic models havebecome widely used in the field of natural lan-guage generation (NLG), often in the form of a re-alisation ranker in a two-stage generation architec-ture.
The two-stage methodology is characterisedby a separation between generation and selection,in which rule-based methods are used to generate aspace of possible paraphrases, and statistical meth-ods are used to select the most likely realisationfrom the space.
By and large, two statistical mod-els are used in the rankers to choose output strings:?
N-gram language models over different units,such as word-level bigram/trigram mod-els (Bangalore and Rambow, 2000; Langk-ilde, 2000), or factored language models inte-grated with syntactic tags (White et al, 2007).?
Log-linear models with different syntacticand semantic features (Velldal and Oepen,2005; Nakanishi et al, 2005; Cahill et al,2007).To date, however, probabilistic models learn-ing direct mappings from generation input to sur-face strings, without the effort to construct a gram-mar, have rarely been explored.
An exception isRatnaparkhi (2000), who presents maximum en-tropy models to learn attribute ordering and lexi-cal choice for sentence generation from a semanticrepresentation of attribute-value pairs, restricted toan air travel domain.297SNPPRPWeVPVBPbelievePPINinNPNPDTtheNNlawPPINofNPNNSaveragesf1?????????????????????????????????
?PRED ?believe?TENSE presSUBJ f2??
?PRED ?pro?PERS 1NUM pl??
?OBL f3????????????????????
?PFORM ?in?OBJ f4?????????????????
?PRED ?law?PERS 3NUM sgSPEC f5[DET f6[PRED ?the?]]ADJ????????f7?????
?PFORM ?of?OBJ f8??
?PRED ?average?PERS 3NUM pl??????????????????????????????????????????????????????????????????????????????????????????(a.)
c-structure (b.)
f-structurestring We believe in the law of averagesposition 1 2 3 4 5 6 7f1SUBJ PRED OBLf3PFORM OBJf4SPEC PRED ADJf7PFORM OBJ(c.) linearised grammatical functions / bilexical dependenciesFigure 1: C- and f-structures for the sentence We believe in the law of averages.In this paper, we develop an efficient, wide-coverage generator based on simple n-gram mod-els to directly linearise dependency relations fromthe input representations.
Our work is aimed atgeneral-purpose sentence generation but couchedin the framework of Lexical Functional Grammar.We give an overview of LFG and the dependencyrepresentations we use in Section 2.
We describethe general idea of our dependency-based gener-ation in Section 3 and give details of the n-gramgeneration models in Section 4.
Section 5 explainsthe experiments and provides results for both En-glish and Chinese data.
Section 6 compares the re-sults with previous work and between languages.Finally we conclude with a summary and outlinefuture work.2 LFG-Based Generation2.1 Lexical Functional GrammarLexical Functional Grammar (Kaplan and Bres-nan, 1982) is a constraint-based grammar for-malism which postulates (minimally) two lev-els of representation: c(onstituent)-structure andf(unctional)-structure.
As illustrated in Figure 1,a c-structure is a conventional phrase structuretree and captures surface grammatical configu-rations.
The f-structure encodes more abstractfunctional relations like SUBJ(ect), OBJ(ect) andADJ(unct).
F-structures are hierarchical attribute-value matrix representations of bilexical labelleddependencies, approximating to basic predicate-argument/adjunct structures.2 Attributes in f-structure come in two different types:?
Grammatical Functions (GFs) indicate the re-lationship between the predicate and depen-dents.
GFs can be divided into:?
arguments are subcategorised for by thepredicate, such as SUBJ(ect), OBJ(ect),and thus can only occur once in each lo-cal f-structure.?
modifiers like ADJ(unct), COORD(inate)are not subcategorised for by the predi-cate, and can occur any number of timesin a local f-structure.?
Atomic-valued features describe linguisticproperties of the predicate, such as TENSE,ASPECT, MOOD, PERS, NUM, CASE etc.2.2 Generation from F-StructuresWork on generation in LFG generally assumes thatthe generation task is to determine the set of stringsof the language that corresponds to a specified f-structure, given a particular grammar (Kaplan andWedekind, 2000).
Previous work on generation2F-structures can be also interpreted as quasi-logicalforms (van Genabith and Crouch, 1996), which more closelyresemble inputs used by some other generators.298within LFG includes the XLE,3 Cahill and vanGenabith (2006), Hogan et al (2007) and Cahill etal.
(2007).
The XLE generates sentences from f-structures according to parallel handcrafted gram-mars for English, French, German, Norwegian,Japanese, and Urdu.
Based on the German XLEresources, Cahill et al (2007) describe a two-stage,log-linear generation model.
Cahill and van Gen-abith (2006) and Hogan et al (2007) present achart generator using wide-coverage PCFG-basedLFG approximations automatically acquired fromtreebanks (Cahill et al, 2004).3 Dependency-Based Generation: theBasic IdeaTraditional LFG generation models can be re-garded as the reverse process of parsing, anduse bi-directional f-structure-annotated CFG rules.In a sense, the generation process is driven byan input dependency (or f-structure) representa-tion, but proceeds through the ?detour?
of us-ing dependency-annotated CFG (or PCFG) gram-mars and chart-based generators.
In this paper,we develop a simple n-gram and dependency-based, wide-coverage, robust, probabilistic gener-ation model, which cuts out the middle-man fromprevious approaches: the CFG-component.Our approach is data-driven: following themethodology in (Cahill et al, 2004; Guo et al,2007), we automatically convert the English Penn-II treebank and the Chinese Penn Treebank (Xueet al, 2005) into f-structure banks.
F-structuressuch as Figure 1(b.)
are unordered, i.e.
they donot carry information on to the relative surface or-der of local GFs.
In order to generate a stringfrom an f-structure, we need to linearise the GFs(at each level of embedding) in the f-structure (andmap lemmas and features to surface forms).
Wedo this in terms of n-gram models over GFs.
In or-der to build the n-gram models, we linearise the f-structures automatically produced from treebanksby associating the numerical string position (wordoffset from start of the sentence) with the predicatein each local f-structure, producing GF sequencesas in Figure 1(c.).Even though the n-gram models are exemplifiedusing LFG f-structures, they are general-purposemodels and thus suitable for any bilexical labelleddependency (Nivre, 2006) or predicate-argumenttype representations, such as the labelled feature-3http://www2.parc.com/isl/groups/nltt/xle/value structures used in HALogen and the func-tional descriptions in the FUF/SURGE system.4 N-Gram Models for Dependency-BasedGeneration4.1 Basic N-Gram ModelThe primary task of a sentence generator is to de-termine the linear order of constituents and words,represented as lemmas in predicates in f-structures.At a particular local f-structure, the task of gen-erating a string covered by the local f-structureis equivalent to linearising all the GFs present atthat local f-structure.
E.g.
in f4in Figure 1, theunordered set of local GFs {SPEC, PRED, ADJ}generates the surface sequence ?the law of aver-ages?.
We linearise the GFs in the set by com-puting n-gram models, similar to traditional word-based language models, except using the names ofGFs (including PRED) instead of words.
Givena (sub-) f-structure F containing m GFs, the n-gram model searches for the best surface sequenceSm1=s1...smgenerated by the GF linearisationGFm1= GF1...GFm, which maximises the prob-ability P (GFm1).
Using n-gram models, P (GFm1)is calculated according to Eq.
(1).P (GFm1) = P (GF1...GFm)=m?k=1P (GFk|GFk?1k?n+1) (1)4.2 Factored N-Gram ModelsIn addition to the basic n-gram model over bareGFs, we integrate contextual and fine-grainedlexical information into several factored models.Eq.
(2) additionally conditions the probability ofthe n-gram on the parent GF label of the cur-rent local f-structure fi, Eq.
(3) on the instantiatedPRED of the local f-structure fi, and Eq.
(4) lexi-calises the model, where each GF is augmentedwith its own predicate lemma.Pg(GFm1) =m?k=1P (GFk|GFk?1k?n+1, GFi) (2)Pp(GFm1) =m?k=1P (GFk|GFk?1k?n+1, P redi) (3)Pl(GFm1) =m?k=1P (Lexk|Lexk?1k?n+1) (4)299To avoid data sparseness, the factored n-grammodels P f are smoothed by linearly interpolatingthe basic n-gram model P , as in Eq.(5).
?Pf(GFm1) = ?Pf(GFm1) + (1?
?
)P (GFm1) (5)Additionally, the lexicalised n-gram models P lare combined with the other two models con-ditioned on the additional parent GF P g andPRED P p, as shown in Eqs.
(6) & (7), respectively.
?Plg(GFm1) = ?1Pl(GFm1) + ?2Pg(GFm1)+?3P (GFm1) (6)?Plp(GFm1) = ?1Pl(GFm1) + ?2Pp(GFm1)+?3P (GFm1) (7)where?
?i= 1Table 1 exemplifies the different n-gram modelsfor the local f-structure f4in Figure 1.Model N-grams Cond.basic (P ) SPEC PRED ADJgf (P g ) SPEC PRED ADJ OBLpred (Pp) SPEC PRED ADJ ?law?lex (P l) SPEC PRED[?law?]
ADJ[?of?
]Table 1: Examples of n-grams for f4in Figure 1Besides grammatical functions, we also makeuse of atomic-valued features like TENSE, PERS,NUM (etc.)
to aid linearisation.
The attributes andvalues of these features are integrated into the GFn-grams for disambiguation (see Section 5.2).4.3 Generation AlgorithmOur basic n-gram based generation model im-plements the simplifying assumption that lineari-sation at one sub-f-structure is independent oflinearisation at any other sub-f-structures.
Thisassumption is feasible for projective dependen-cies.
In most cases (at least in English andChinese), non-projective dependencies are onlyused to account for Long-Distance Dependen-cies (LDDs).
Consider sentence (1) discussedin Carroll et al (1999) and its corresponding f-structure in Figure 2.
In LFG f-structures, LDDsare represented via reentrancies between ?dislo-cated?
TOPIC, TOPIC REL, FOCUS (etc.)
GFs and?source?
GFs subcategorised for by local predi-cates, but only the dislocated GFs are instantiatedin generation.
Therefore traces of the source GFsin input f-structures are removed before genera-tion, and non-projective dependencies are trans-formed into simple projective dependencies.
(1) How quickly did the newspapers say the ath-lete ran???????????????????????????FOCUS??
?PRED ?quickly?ADJ{[PRED ?how?]}??
?1PRED ?say?SUBJ?
?PRED ?newspaper?SPEC[PRED ?the?]??COMP???????
?PRED ?run?SUBJ?
?PRED ?athlete?SPEC[PRED ?the?]?
?ADJ 1?????????????????????????????????
?Figure 2: schematic f-structure for How quicklydid the newspapers say the athlete ran?In summary, given an input f-structure f , thecore algorithm of the generator recursively tra-verses f and at each sub-f-structure fi:1. instantiates the local predicate at fiand per-forms inflections/declensions if necessary2.
calculates the GF linearisations present at fiby n-gram models3.
finds the most probable GF sequence amongall possibilities by Viterbi search4.
generates the string covered by fiaccordingto the linearised GFs5 Experiments and EvaluationTo test the performance and coverage of our n-gram-based generation models, experiments arecarried out for both English and Chinese, two lan-guages with distinct properties.5.1 Experiment DesignExperiments on English data are carried out onthe WSJ portion of the PTB, using standard train-ing/test/development splits, viz 39,832 sentencesfrom sections 02-21 are used for training, 2,416sentences from section 23 for testing, while 1,700sentences from section 22 are held out for develop-ment.
The latest version of the Penn Chinese Tree-bank 6.0 (CTB6), excluding the portion of ACEbroadcast news, is used for experiments on Chi-nese data.4 We follow the recommended splits (inthe list-of-file of CTB6) to divide the data into testset, development set and training set.
The trainingset includes 756 files with a total of 15,663 sen-tences.
The test set includes 84 files with 1,7084Sentences labelled as fragment are not included in ourdevelopment and test set.300sentences.
The development set includes 50 fileswith 1,116 sentences.
Table 2 shows some of thecharacteristics of the English and Chinese data ob-tained from the development sets.Development Set English Chinesenum of sent 1,700 1,116max length of sent (#words) 110 145ave length of sent (#words) 23 31num of local fstr 23,289 15,847num of local fstr per sent 13.70 14.20max length of local fstr (#gfs) 12 16ave length of local fstr (#gfs) 2.56 2.90Table 2: Comparison English and Chinese dataThe n-gram models are created using theSRILM toolkit (Stolcke, 2002) with Good-Turningsmoothing for both the Chinese and English data.For morphological realisation of English, a set oflexical macros is automatically extracted from thetraining data.
This is not required for Chinese sur-face realisation as Chinese has very little morphol-ogy.
Lexical macro examples are listed in Table 3.lexical macro surface wordpred=law, num=sg, pers=3 lawpred=average, num=pl, pers=3 averagespred=believe, num=pl, tense=pres believeTable 3: Examples of lexical macrosThe input to our generator are unordered f-structures automatically derived from the develop-ment and test set trees of our treebanks, which donot contain any string position information.
But,due to the particulars of the automatic f-structureannotation algorithm, the order of sub-f-structuresin set-valued GFs, such as ADJ, COORD, happensto correspond to their surface order.
To avoid un-fairly inflating evaluation results, we lexically re-order the GFs in each sub-f-structure of the devel-opment and test input before the generation pro-cess.
This resembles the ?permute, no dir?
typeexperiment in (Langkilde, 2002).5.2 Experimental ResultsFollowing (Langkilde, 2002) and other workon general-purpose generators, BLEU score (Pa-pineni et al, 2002), average NIST simplestring accuracy (SSA) and percentage of exactlymatched sentences are adopted as evaluation met-rics.
As our system guarantees that all input f-structures can generate a complete sentence, spe-cial coverage-dependent evaluation (as has beenadopted in most grammar-based generation sys-tems) is not necessary in our experiments.Experiments are carried out on an Intel Pentium4 server, with a 3.80GHz CPU and 3GB mem-ory.
It takes less than 2 minutes to generate all2,416 sentences (with average sentence length of21 words) of WSJ section 23 (average 0.05 sec persentence), and approximately 4 minutes to gener-ate 1,708 sentences (with average sentence lengthof 30 words) of CTB test data (average 0.14 secper sentence), using 4-gram models in all experi-ments.
Our evaluation results for English and Chi-nese data are shown in Tables 4 and 5, respectively.Different n-gram models perform nearly consis-tently in all the experiments on both English andChinese data.
The results show that factored n-gram models outperform the basic n-gram models,and in turn the combined n-gram models outper-form single n-gram models.
The combined modelinterpolating n-grams over lexicalised GFs with n-grams conditioned on PRED achieves the best re-sults in both experiments on English (with featurenames) and Chinese (with feature names & val-ues), with BLEU scores of 0.7440 and 0.7123 re-spectively, and full coverage.Lexicalisation plays an important role in bothEnglish and Chinese, boosting the BLEU scorewithout features from 0.5074 to 0.6741 for En-glish, and from 0.5752 to 0.6639 for Chinese.Atomic-valued features play an important rolein English, and boost the BLEU score from 0.5074in the baseline model to 0.6842 when featurenames are integrated into the n-gram models.However, feature names in Chinese only increasethe BLEU score from 0.5752 to 0.6160.
Thisis likely to be the case as English has a richermorphology than Chinese, and important func-tion words such as ?if?, ?to?, ?that?
are encodedin atomic-valued features in English f-structures,which helps to determine string order.
However,combined feature names and values work better onChinese data, but turn out to hurt the n-gram modelperformance for English data.
This may suggestthat the feature names in English already includeenough information, while the value of morpho-logical features, such as TENSE, NUM does not pro-vide any new information to help determine wordorder, but aggravate data sparseness instead.301WSJ Sec23 Without Features Feature Names Feature Names & ValuesModel ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSAbaseline 5.30% 0.5074 57.29% 15.27% 0.6842 69.48% 15.15% 0.6829 69.15%gf 6.62% 0.5318 60.06% 16.76% 0.6969 71.51% 16.68% 0.6977 71.55%pred 8.03% 0.5697 60.73% 16.72% 0.7035 70.12% 16.76% 0.7042 71.08%lex 12.87% 0.6741 69.43% 19.41% 0.7384 74.76% 18.96% 0.7375 74.12%lex+gf 12.62% 0.6611 69.41% 19.70% 0.7388 74.98% 19.74% 0.7405 75.08%lex+pred 12.25% 0.6569 68.04% 19.83% 0.7440 75.34% 19.58% 0.7422 75.04%Table 4: Results for English Penn-II WSJ section 23Test Without Features Feature Names Feature Names & ValuesModel ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSAbaseline 8.96% 0.5752 51.92% 11.77% 0.6160 54.64% 12.30% 0.6239 55.20%gf 9.54% 0.6009 53.02% 12.53% 0.6391 55.78% 13.47% 0.6486 56.60%pred 10.07% 0.6180 53.80% 13.35% 0.6608 56.72% 14.46% 0.6720 57.67%lex 13.93% 0.6639 59.61% 15.16% 0.6770 60.44% 15.98% 0.6804 60.20%lex+gf 14.81% 0.6773 59.92% 15.52% 0.6911 60.97% 16.80% 0.6957 61.07%lex+pred 16.04% 0.6952 60.82% 16.22% 0.7060 61.45% 17.51% 0.7123 61.54%Table 5: Results for Chinese CTB6 test dataWSJ Sec23 Sentence length ?
20 words All sentencesCoverage ExMatch BLEU SSA Coverage ExMatch BLEU SSALangkilde(2002) 82.7% 28.2% 0.757 69.6%Callaway(2003) 98.7% 49.0% 88.84%Nakanishi(2005) 90.75% 0.7733 83.6% 0.705Cahill(2006) 98.65% 0.7077 73.73% 98.05% 0.6651 68.08%Hogan(2007) 100% 0.7139 99.96% 0.6882 70.92%White(2007) 94.3% 6.9% 0.5768this paper 100% 35.40% 0.7625 81.09% 100% 19.83% 0.7440 75.34%Table 6: Cross system comparison of results for English WSJ section 236 Discussion6.1 Comparison to Previous WorkIt is very difficult to compare sentence generatorssince the information contained in the input rep-resentation varies greatly between systems.
Themost direct comparison is between our system andthose presented in Cahill and van Genabith (2006)and Hogan et al (2007), as they also use treebank-based automatically generated f-structures as thegenerator inputs.
The labelled feature-value struc-tures used in HALogen (Langkilde, 2002) andfunctional descriptions in FUF/SURGE (Callaway,2003) also bear some broad similarities to our f-structures.
A number of systems using differentinput but adopting the same evaluation metrics andtesting on the same data are listed in Table 6.Surprisingly (or not), the best results areachieved by a purely symbolic generationsystem?FUF/SURGE (Callaway, 2003).
How-ever the approach uses handcrafted grammarswhich are very time-consuming to produce andadapt to different languages and domains.
Langk-ilde (2002) reports results for experiments withvarying levels of linguistic detail in the inputgiven to the generator.
The type ?permute, no dir?is most comparable to the level of informationcontained in our f-structure in that the modifiers(adjuncts, coordinates etc.)
in the input are notordered.
However her labelled feature-valuestructure is more specific than our f-structureas it also includes syntactic properties such aspart-of-speech, which might contribute to thehigher BLEU score of HALogen.
And moreover,in HALogen nearly 20% of the sentences are onlypartially generated (or not at all).
Nakanishi etal.
(2005) carry out experiments on sentences upto 20 words, with BLEU scores slightly higherthan ours.
However their results without sentencelength limitation (listed in the right column), for500 sentences randomly selected from WSJ Sec22are lower than ours, even at a lower coverage.Overall our system is competitive, with best resultsfor coverage (100%), second best for BLEU andSSA scores, and third best overall on exact match.However, we admit that automatic metrics such asBLEU are not fully reliable to compare differentsystems, and results vary widely depending on thecoverage of the systems and the specificity of thegeneration input.3026.2 Error Analysis and Differences Betweenthe LanguagesThough our dependency-based n-gram models per-form well in both the English and Chinese exper-iments, we are surprised that experiments on En-glish data produce better results than those for Chi-nese.
It is widely accepted that English generationis more difficult than Chinese, due to morpholog-ical inflections and the somewhat less predictableword order of English compared to Chinese.
Thisis reflected by the results of the baseline models.Chinese has a BLEU score of 0.5752 and 8.96%exact match, both are higher than those of English.However with feature augmentation and lexicali-sation, the results for English data exceed Chinese.This is probably because of the following reasons:Data size of the English training set is more thantwice that of Chinese.Grammatical functions are more fine-grainedin English f-structures than those in Chinese.There are 32 GFs defined for English compared to20 for Chinese in our input f-structures.Properties of the languages and data sets aredifferent.
For example, due to lack of inflectionand case markers, many sequences of VPs in Chi-nese have to be treated as coordinates, whereastheir counterparts in English act as different gram-matical functions, e.g.
(2).
(2) ??
z ,????
?invest million build this construction?invest million yuan to build the construction?This results in a total of 7,377 coordinates (4.32per sentence) in the Chinese development data,compared to 2,699 (1.12 per sentence) in the En-glish data.
The most extreme case in the Chinesedata features 14 coordinates of country names ina local f-structure.
This may account for the lowSSA score for the Chinese experiments, as manycoordinates are tied in the n-gram scoring methodand can not be ordered correctly.
Examining thedevelopment data shows different types of coordi-nation errors:?
syntactic coordinates, but not semantic coor-dinates, as in sentence (2).?
syntactic and semantic coordinates, but usu-ally expressed in a fixed order, e.g.
(3).
(3) U?
m?reform opening-up?reform and opening up??
syntactic and semantic coordinates, whichcan freely swap positions, e.g.
(4).
(4) ?
 ??
?
?$g?plentiful energy and quick thinking?energetic and agile?At the current stage, our n-gram generationmodel only keeps the most likely realisation foreach local f-structure.
We believe that packing allequivalent elements, like coordinates in a local f-structure into equivalent classes, and outputing n-best candidate realisations will greatly increase theSSA score and may also further benefit the effi-ciency of the algorithm.7 Conclusions and Further WorkWe have described a number of increasingly so-phisticated n-gram models for sentence genera-tion from labelled bilexical dependencies, in theform of LFG f-structures.
The models includeadditional conditioning on parent GFs and differ-ent degrees of lexicalisation.
Our method is sim-ple, highly efficient, broad coverage and accuratein practice.
We present experiments on Englishand Chinese, showing that the method generaliseswell to different languages and data sets.
We arecurrently exploring further combinations of con-ditioning context and lexicalisation, application todifferent languages and to dependency represen-tations used to train state-of-the-art dependencyparsers (Nivre, 2006).AcknowledgmentsThis research is funded by Science Foundation Ire-land grant 04/IN/I527.
We thank Aoife Cahill forproviding the treebank-based LFG resources forthe English data.
We gratefully acknowledge thefeedback provided by our anonymous reviewers.ReferencesBangalore, Srinivas and Rambow, Owen.
2000.
Ex-ploiting a Probabilistic Hierarchical Model for Gen-eration.
Proceedings of the 18th InternationalConference on Computational Linguistics, 42?48.Saarbru?cken, Germany.Belz, Anja.
2007.
Probabilistic Generation of WeatherForecast Texts.
Proceedings of the Conference of theNorth American Chapter of the Association for Com-putational Linguistics, 164?171.
New York.Cahill, Aoife, Burke, Michael, O?Donovan, Ruth, vanGenabith, Josef and Way, Andy.
2004.
Long-Distance Dependency Resolution in Automatically303Acquired Wide-Coverage PCFG-Based LFG Ap-proximations.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics, 320-327.
Barcelona, Spain.Cahill, Aoife and van Genabith, Josef.
2006.
Ro-bust PCFG-Based Generation Using AutomaticallyAcquired LFG Approximations.
Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, 1033?1040.Sydney, Australia.Cahill, Aoife, Forst, Martin and Rohrer, Christian.2007.
Stochastic Realisation Ranking for a FreeWord Order Language.
Proceedings of the 11th Eu-ropean Workshop on Natural Language Generation,17?24.
Schloss Dagstuhl, Germany.Callaway, Charles B.. 2003.
Evaluating Coverage forLarge Symbolic NLG Grammars.
Proceedings of theEighteenth International Joint Conference on Artifi-cial Intelligence, 811?817.
Acapulco, Mexico.Carroll, John, Copestake, Ann, Flickinger, Dan andPoznanski, Victor.
1999.
An efficient chart gen-erator for (semi-)lexicalist grammars.
Proceedingsof the 7th European Workshop on Natural LanguageGeneration, 86?95.
Toulouse, France.Crouch, Dick, Dalrymple, Mary, Kaplan, Ron, King,Tracy, Maxwell, John and Newman, Paula.
2007.XLE Documentation.
Palo Alto Research Center,CA.Elhadad, Michael.
1991.
FUF: The universal unifieruser manual version 5.0.
Technical Report CUCS-038-91.
Dept.
of Computer Science, Columbia Uni-versity.Guo, Yuqing and van Genabith, Josef and Wang,Haifeng.
2007.
Treebank-based Acquisition of LFGResources for Chinese.
Proceedings of LFG07 Con-ference, 214?232.
Stanford, CA, USA.Hogan, Deirdre Cafferkey, Conor Cahill, Aoife and vanGenabith, Josef.
2007.
Exploiting Multi-Word Unitsin History-Based Probabilistic Generation.
Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andCoNLL, 267?276.
Prague, Czech Republic.Kaplan, Ronald and Bresnan, Joan.
1982.
LexicalFunctional Grammar: a Formal System for Gram-matical Representation.
The Mental Representationof Grammatical Relations, 173?282.
MIT Press,Cambridge.Kaplan, Ronald and Wedekind, Jurgen.
2000.
LFGGeneration Produces Context-free Languages.
Pro-ceedings of the 18th International Conference onComputational Linguistics, 425?431.
Saarbru?cken,Germany.Langkilde, Irene.
2000.
Forest-Based Statistical Sen-tence Generation.
Proceedings of 1st Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, 170?177.
Seattle, WA.Langkilde, Irene.
2002.
An Empirical Verificationof Coverage and Correctness for a General-PurposeSentence Generator.
Proceedings of the Second In-ternational Conference on Natural Language Gener-ation, 17?24.
New York, USA.Marcus, Mitchell P., Santorini, Beatrice andMarcinkiewicz, Mary Ann.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2).Nakanishi, Hiroko and Nakanishi, Yusuke and Tsu-jii, Jun?ichi.
2005.
Probabilistic Models for Dis-ambiguation of an HPSG-Based Chart Generator.Proceedings of the 9th International Workshop onParsing Technology, 93?102.
Vancouver, BritishColumbia.Nivre, Joakim.
2006.
Inductive Dependency Parsing.Springer.Papineni, Kishore, Roukos, Salim, Ward, Todd andZhu, Wei-Jing.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, 311-318.
Philadelphia, USA.Ratnaparkhi, Adwait.
2000.
Trainable methods for nat-ural language generation.
Proceedings of NAACL2000, 194?201.
Seattle, WA.Stolcke, Andreas.
2002.
SRILM-An Extensible Lan-guage Modeling Toolkit.
Proceedings of Interna-tional Conference of Spoken Language Processing.Denver, Colorado.van Genabith, Josef and Crouch, Dick.
1996.
Di-rect and underspecified interpretations of LFG f-structures.
Proceedings of the 16th conference onComputational linguistics, 262?267.
Copenhagen,DenmarkVelldal, Erik and Oepen, Stephan.
2005.
Maximumentropy models for realization ranking.
Proceedingsof the MTSummit ?05.White, Michael.
2004.
Reining in CCG Chart Realiza-tion.
Proceedings of the third International NaturalLanguage Generation Conference.
Hampshire, UK.White, Michael, Rajkumar, Rajakrishnan and Martin,Scott.
2007.
Towards Broad Coverage Surface Re-alization with CCG.
Proceedings of the MT SummitXI Workshop, 22?30.
Copenhagen, Danmark.Xue, Nianwen, Xia, Fei, Chiou, Fu dong and Palmer,Martha.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
NaturalLanguage Engineering, 11(2): 207?238.304
