Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 306?313,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsHand Gestures in Disambiguating Types of You Expressions in MultipartyMeetingsTyler BaldwinDepartment of ComputerScience and EngineeringMichigan State UniversityEast Lansing, MI 48824baldwin96@cse.msu.eduJoyce Y. ChaiDepartment of ComputerScience and EngineeringMichigan State UniversityEast Lansing, MI 48824jchai@cse.msu.eduKatrin KirchhoffDepartment of ElectricalEngineeringUniversity of WashingtonSeattle, WA, USAkatrin@ee.washington.eduAbstractThe second person pronoun you serves dif-ferent functions in English.
Each of thesedifferent types often corresponds to a dif-ferent term when translated into anotherlanguage.
Correctly identifying differenttypes of you can be beneficial to machinetranslation systems.
To address this is-sue, we investigate disambiguation of dif-ferent types of you occurrences in multi-party meetings with a new focus on therole of hand gesture.
Our empirical re-sults have shown that incorporation of ges-ture improves performance on differentiat-ing between the generic use of you (e.g.,refer to people in general) and the referen-tial use of you (e.g., refer to a specific per-son or a group of people).
Incorporationof gesture can also compensate for limi-tations in automated language processing(e.g., reliable recognition of dialogue acts)and achieve comparable results.1 IntroductionThe second person pronoun you is one of the mostprevalent words in conversation and it serves sev-eral different functions (Meyers, 1990).
For ex-ample, it can be used to refer to a single addressee(i.e., the singular case) or multiple addressees (i.e.,the plural case).
It can also be used to representpeople in general (i.e., the generic case) or be usedidiomatically in the phrase ?you know?.For machine translation systems, these differ-ent types of you often correspond to differenttranslations in another language.
For example,in German, there are different second-person pro-nouns for singular vs. plural you (viz.
du vs. ihr);in addition there are different forms for formalvs.
informal forms of address (du vs. Sie) and forthe generic use (man).
The following examplesdemonstrate different translations of you from En-glish (EN) into German (DE):?
Generic youEN: Sometimes you have meetings where thedecision is already taken.DE: Manchmal hat man Meetings wo dieEntscheidung schon gefallen ist.?
Singular you:EN: Do you want an extra piece of paper?DE: Mo?chtest du noch ein Blatt Papier??
Plural you:EN: Hope you are all happy!DE: Ich hoffe, ihr seid alle zufrieden!These examples show that correctly identifyingdifferent types of You plays an important role inthe correct translation of you in different context.To address this issue, this paper investigates therole of hand gestures in disambiguating differentusages of you in multiparty meetings.
Althoughidentification of you type has been investigatedbefore in the context of addressee identification(Gupta et al, 2007b; Gupta et al, 2007a; Framp-ton et al, 2009; Purver et al, 2009), our workhere focuses on two new angles.
First, because ofour different application on machine translation,rather than processing you at an utterance level toidentify addressee, our work here concerns eachoccurrence of you within each utterance.
Secondand more importantly, our work investigates therole of corresponding hand gestures in the disam-biguation process.
This aspect has not been exam-ined in previous work.When several speakers are conversing in a sit-uated environment, they often overtly gesture atone another to help manage turn order or explic-itly direct a statement toward a particular partici-pant (McNeill, 1992).
For example, consider thefollowing snippet from a multiparty meeting:A: ?Why is that?
?B: ?Because, um, based on what ev-306erybody?s saying, right, [gestures atSpeaker D] you want something sim-ple.
You [gestures at Speaker C]wantbasic stuff and [gestures at Speaker A]you want something that is easy to use.Speech recognition might not be thesimplest thing.
?The use of gesture in this example indicates thateach instance of the pronoun you is intended tobe referential, and gives some indication of the in-dented addressee.
Without the aid of gesture, itwould be difficult even for a human listener to beable to interpret each instance correctly.Therefore, we conducted an empirical study onseveral meeting segments from the AMI meetingcorpus.
We formulated our problem as a classifica-tion problem for each occurrence of you, whetherit is a generic, singular, or plural type.
We com-bined gesture features with several linguistic anddiscourse features identified by previous work andevaluated the role of gesture in two different set-tings: (1) a two stage classification that first dif-ferentiates the generic type from the referentialtype and then within the referential type distin-guishes singular and plural usages; (2) a three wayclassification between generic, singular, or pluraltypes.
Our empirical results have shown that in-corporation of gesture improves performance ondifferentiating between the generic and the refer-ential type.
Incorporation of gesture can also com-pensate for limitations in automated language pro-cessing (e.g., reliable recognition of dialogue acts)and achieve comparable results.
These findingshave important implications for machine transla-tion of you expressions from multiparty meetings.2 Related WorkPsychological research on gesture usage inhuman-human dialogues has shown that speakersgesture for a variety of reasons.
While speakers of-ten gesture to highlight objects related to the coreconversation topic (Kendon, 1980), they also ges-ture for dialogue management purposes (Bavelaset al, 1995).
While not all of the gestures pro-duced relate directly to the resolution of the wordyou, many of them give insight into which partici-pant is being addressed, which has a close correla-tion with you resolution.
Our investigation here isclosely related to two areas of previous work: ad-dressee identification based on you and the use ofgestures in coreference resolution.Addressee Identification.
Disambiguation ofyou type in the context of addressee identifica-tion has been examined in several papers in re-cent years.
Gupta et.
al.
(2007b) examinedtwo-party dialogues from the Switchboard corpus.They modeled the problem as a binary classifi-cation problem of differentiating between genericand referential usages (referential usages includethe singular and plural types).
This work has iden-tified several important linguistic and discoursefeatures for this task (which was used and ex-tended in later work and our work here).
Laterwork by the same group (Gupta et al, 2007a) ex-amined the same problem on multiparty dialoguedata.
They made adjustments to their previousmethods by removing some oracle features fromannotation and applying simpler and more realis-tic features.
A recent work (Frampton et al, 2009)has examined both the generic vs. referential andsingular vs. plural classification tasks.
A maindifference is that this work incorporated gaze fea-ture information in both classification tasks (gazefeatures are commonly used in addressee identi-fication).
More recent work (Purver et al, 2009)discovered that large gains in performance canbe achieved by including n-gram based features.However, they found that many of the most im-portant n-gram features were topic specific, andthus required training data consisting of meetingsabout the same topic.Gestures in Coreference Resolution.
Eisen-stein and Davis (2006; 2007) examined corefer-ence resolution on a corpus of speaker-listenerpairs in which the speaker had to describe theworkings of a mechanical device to the listener,with the help of visual aids.
In this gesture heavydataset, they found gesture data to be helpful in re-solving references.
In our previous work (2009),we examined gestures for the identification ofcoreference on multparty meeting data.
We foundthat gestures only provided limited help in thecoreference identification task.
Given the natureof the meetings under investigation, although ges-tures have not been shown to be effective in gen-eral, they are potentially helpful in recognizingwhether two linguistic expressions refer to a sameparticipant.Compared to these two areas of earlier work,our investigation here has two unique aspects.First, as mentioned earlier, previous work on ad-dressee identification focused the problem at the307utterance level.
Because the goal was to find theaddressee of an utterance, the assumption was thatall instances of you in an utterance were of thesame type.
However, since several instances ofyou in the same utterance may translate differently,we instead examine the classification task at theinstance level.
Second, our work here specificallyinvestigates the role of gestures in disambiguationof different types of you.
This aspect has not beenexamined in previous work.3 DataThe dataset used in our investigation was theAMI meeting corpus (Popescu-Belis and Estrella,2007), the same corpus used in previous work(Gupta et al, 2007a; Frampton et al, 2009; Purveret al, 2009; Baldwin et al, 2009).
The AMI meet-ing corpus is a large publicly available corpus ofmultiparty design meetings.
AMI meeting anno-tations contain manual speech transcriptions, aswell as annotations of several additional modali-ties, such as focus of attention and head and handgesture.For this work, six AMI meeting segments(IS1008a, IS1008b, IS1008c, IS1008d, ES2008a,TS3005a) were used.
These instances were cho-sen because they contained manual annotations ofhand gesture data, which was not available for allAMI meeting segments.
These six meeting seg-ments were from AMI ?scenario?
meetings, inwhich meeting participants had a specific task ofdesigning a hypothetical remote control.All instances of the word you and its variantswere manually annotated as either generic, singu-lar, or plural.
This produced a small dataset of 533instances.
Agreement between two human anno-tators was high (?
= 0.9).
The distribution of youtypes is shown in Figure 1.
The most prevalenttype in our data set was the generic type, whichaccounted for 47% of all instances of you present.Of the two referential types, the singular type ac-counted for about 60% of the referential instances.A total of 508 gestures are present in our dataset.
Table 1 shows the distribution of gestures.As shown, ?non-communicative gestures?, makeup nearly half (46%) of the gestures produced.These are gestures that are produced without anovert communicative intent, such as idly tappingon the table.
The other main categorization ofgestures is ?communicative gestures?, which ac-counts for 45% of all gestures produced and ismade up of the ?pointing at participants?, ?point-ing at objects?, ?interact with object?, and ?othercommunicative?
gesture types from Table 1.
A to-tal of 17% of the gestures produced were pointinggestures that pointed to people, a type of gesturethat would likely be helpful for you type identifica-tion.
A small percentage of the gestures producedwere not recorded by the meeting recording cam-eras (i.e., off camera), and thus are of unknowntype.4 MethodologyOur general methodology followed previous workand formulated this problem as a classificationproblem.
We evaluated how gesture data mayhelp you type identification using two different ap-proaches: (1) two stage binary classification, and(2) a single three class classification problem.
Intwo stage binary classification, we first attemptto distinguish between instances of you that aregeneric and those that are referential.
We then takethose cases that are referential and attempt to sub-divide them into instances that are intended to re-fer to a single person and those that refer to severalpeople.Our feature set includes features used by Guptaet.
al.
(2007a) (Hereafter referred to as Gupta) andFrampton et.
al.
(2009) (Hereafter Frampton), aswell as new features incorporating gestures.
Wesummarize these features as follows.Sentential Features.
We used several senten-tial features to capture important phrase patterns.Most of our sentential features were drawn fromGupta (2007a).
These features captured the pat-terns ?you guys?, ?you know?, ?do you?
(and sim-ilar variants), ?which you?
(and variants), ?if you?,and ?you hear?
(and variants).
Another sententialfeature captured the number of times the word youappeared in the sentence.
Additionally, other fea-tures captured sentence patterns not related to you,such as the presence of the words ?I?
and ?we?.A few other sentential features were drawn fromFrampton et.
al.
(2009).
These include the pattern?<auxiliary> you?
(a more general version of the?do you?
feature) and a count of the number oftotal words in the utterances.Part-of-Speech Features.
Several featuresbased on automatic part-of-speech tagging of thesentence containing you were used.
Quality of au-tomatic tagging was not assessed.
From the taggedresults, we extracted 5 features based on sentence308Distributionof YouTypes050100150200250300GenericSingularPluralTypeCount(a) Distribution of You typesGesture Distribution050100150200250 Non- Communicative Pointing at Participants Pointing at Objects Other Communicative Interact with ObjectOff_cameraTypeCount(b) Distribution of gesture typesFigure 1: Data distributionsand tag patterns: whether or not the sentence thatcontained you also contained I, or we followed bya verb tag (3 separate features), and whether ornot the sentence contains a comparative JJR (ad-jective) tag.
All of these features were adaptedfrom Gupta (2007a).Dialog Act Features.
We used the manually an-notated dialogue act tags provided by the AMI cor-pus to produce our dialogue act features.
Three di-alogue act features were used: the dialogue act tagof the current sentence, the previous sentence, andthe sentence prior to that.
Dialog act tags were in-corporated into the feature set in one of two differ-ent ways: 1) using the full tag set provided by theAMI corpus, and 2) using a binary feature record-ing if the dialogue act tag was of the elicit type.The latter way of dialogue act incorporation rep-resents a simpler and more realistic treatment ofdialogue acts.Question Mark Feature.
The question markfeature captures whether or not the current sen-tence ends in a question mark.
This feature cap-tures similar information to the elicit dialogue acttag and was used in Gupta as an automatically ex-tractable replacement to the manually extracted di-alogue act tags (2007a).Backward Looking/Forward Looking Fea-tures.
Several features adapted from Frampton et.al.
(2009) used information about previous andnext sentences and speakers.
These features con-nected the current utterance with previous utter-ances by the other participants in the room.
Foreach listener, a feature was recorded that indicatedhow many sentences elapsed between the currentsentence and the last/next time the person spoke.Additionally, two features captured the number ofspeakers in the previous and next five sentences.Gesture Features.
Several different featureswere used to capture gesture information.
Threetypes of gesture data were considered: all pro-duced gestures, only those gestures that weremanually annotated as being communicative, andonly those gestures that were manually annotatedas pointing towards another meeting participant.For each of these types, one gesture feature cap-tures the total number of gestures that co-occurwith the current sentence, while another featurerecords only whether or not a gesture co-occurswith the utterance of you.
Since previous work(Kendon, 1980) has indicated that gesture produc-tion tends to precede the onset of the expression,gestures were considered to have co-occurred withinstances if they directly overlapped with them orpreceded them by a short window of 2.5 seconds.Note that in this investigation, we used anno-tated gestures provided by the AMI corpus.
Al-though automated extraction of reliable gesturefeatures can be challenging and should be pursuedin the future, the use of manual annotation allowsus to focus on our current goal, which is to under-stand whether and to what degree hand gesturesmay help disambiguation of you Type.It is also important to note that although previ-ous work (Purver et al, 2009) showed that n-gramfeatures produced large performance gains, thesefeatures were heavily topic dependent.
The AMImeeting corpus provides several meetings on ex-actly the same topic, which allowed the n-gramfeatures to learn topic-specific words such as but-ton, channel, and volume.
However, as real world309AccuracyMajority Class Baseline 53.3%Gupta automatic 70.7%Gupta manual 74.7%Gupta + Frampton automatic 73.2%Gupta + Frampton manual 74.3%All (+ gesture) 79.0%Table 1: Accuracy values for Generic vs. Referen-tial Classificationmeetings occur with a wider range of goals andtopics, we would like to build a topic and domainindependent model that does not require a corpusof topic specific training data.
As such, we haveexcluded n-gram features from our study.Additionally, we have not implemented gazefeatures.
Although previous work (Frampton etal., 2009) showed that these features were able toimprove performance, we decided to focus solelyon gesture to the exclusion of other non-speechmodalities.
However, we are currently in the pro-cess of evaluating the overlap between gesture andgaze feature coverage.5 ResultsDue to the small number of meeting segments inour data, leave-one-out cross validation was pre-formed for evaluation.
Since a primary focus ofthis paper is to understand whether and to whatdegree gesture is able to aid in the you type iden-tification task, experiments were run using a deci-sion tree classifier due to its simplicity and trans-parency 1.5.1 Two Stage ClassificationWe first evaluated the role of gesture via two stagebinary classification.
That is, we performed twobinary classification tasks, first differentiating be-tween generic and referential instances, and thenfurther dividing the referential instances into thesingular and plural types.
This provides a moredetailed analysis of where gesture may be helpful.Results for the generic vs. referential and singu-lar vs. plural binary classification tasks are shownin Table 1 and Table 2, respectively.
Tables 1and 2 present several different configurations.
The1In order to get a more direct comparison to previous work(Gupta et al, 2007a; Frampton et al, 2009), we also experi-mented with classification via a bayesian network.
We foundthat the overall results were comparable to those obtainedwith the decision tree.AccuracyMajority Class Baseline 59.5%Gupta automatic 72.2%Gupta manual 73.6%Gupta + Frampton automatic 73.2%Gupta + Frampton manual 72.5%All (+ gesture) 74.6%Table 2: Accuracy values for Singular vs. PluralClassification?Gupta?
feature configurations consist of all fea-tures used by Gupta et.
al.
(2007a).
These in-clude all part-of-speech features, all dialogue actfeatures, the question mark feature, and all sen-tential features except the ?<auxiliary> you?
fea-ture and the word count feature.
Results from twotypes of processing are presented: automatic andmanual.?
Automatic feature extraction (automatic) -The automatic configurations consist of onlyfeatures that were automatically extractedfrom the text.
This includes all of the featureswe examined except for the dialogue act andgesture features.
These features are extractedfrom meeting transcriptions.?
Manual feature extraction (manual) - Manualconfigurations apply manual annotations ofdialogue acts and gestures together with theautomatically extracted features.The Frampton configurations add the addi-tional sentential features as well as the backward-looking and forward-looking features.
As before,results are presented for a manual and an auto-matic run.
The final configuration (?All?)
includesthe entire feature set with the addition of gesturefeatures.
The All configuration is the only config-uration that includes gesture features.Although they are not directly comparable, theresults for generic vs. referential classificationshown in Table 1 appear consistent with those re-ported by Gupta (2007a).
Adding additional fea-tures from Frampton et.
al.
did not produce anoverall increase in performance when dialogue actfeatures were present.
Including gesture featuresleads to a significant increase in performance (Mc-Nemar Test, p < 0.01), an absolute increase of4.3% over the best performing feature set that doesnot include gesture.
This result seems to confirmour hypothesis that, because gestures are likely310AccuracyMajority Class Baseline 46.7%Gupta automatic 61.5%Gupta manual 66.2%Gupta + Frampton automatic 63.6%Gupta + Frampton manual 70.2%All (+ gesture) 70.4%Table 3: Accuracy values for several different fea-ture configurations on the three class classificationproblem.to accompany referential instances of you but notgeneric instances, gesture information is able tohelp differentiate between the two.
Manual in-spection of the decision tree produced indicatesthat gesture features were among the most dis-criminative features.The results on the singular vs. plural task shownin Table 2 are less clear.
Although (Gupta et al,2007a) did not report results on singular vs. pluralclassification, their feature set produced reason-able classification accuracy of 73.6%.
Includinggesture and other features did not produce a statis-tically significant improvement in the overall ac-curacy.
This suggests that while gesture is helpfulfor predicting referentiality, it does not appear tobe a reliable predictor of whether an instance ofyou is singular or plural.
Inspection on the deci-sion tree confirms that gesture features were notseen to be highly discriminative.5.2 Three Class ClassificationThe results presented for singular vs. plural classi-fication are based on performance on the subset ofyou instances that are referential, which assumesthat we are able to filter out generic referenceswith 100% accuracy.
While this gives us an eval-uation of how well the singular vs. plural task canbe performed without the generic references pre-senting a confounding factor, it presents unrealis-tic performance for a real system.
To account forthis, we present results on a three class problem ofdetermining whether an instance of you is generic,singular, or plural.
The results are shown in Table3.
A simple majority class classifier yields accu-racy of 46.7% (In our data, the generic class wasthe majority class).As we can see from Table 3, adding addi-tional features gives improved performance overthe original implementation by Gupta et.
al., re-sulting in an overall accuracy of about 70%.
Wealso observed that the dialogue act features wereimportant; manual configurations produced abso-lute gains of about 7% accuracy over fully auto-matic configurations.
The gesture feature, how-ever, did not provide a significant increase in per-formance over the same feature set without gestureinformation.Table 4 shows the precision, recall, and F-measure values for each you type for several dif-ferent configurations.
As shown, the generic classproved to be the easiest for the classifiers to iden-tify.
This is not suprising, as not only are genericinstance our majority class, but many of the fea-tures used were originally tailored towards the twoclass problem of differentiating generic instancesfrom the other classes.
The performance on theplural and singular classes is comparable to oneanother when the basic feature set is used.
How-ever, as more features are added, the performanceon the singular class increases while the perfor-mance on the plural class does not.
This seemsto suggest that future work should attempt to in-clude more features that are indicative of pluralinstances.When manual dialogue acts are applied, it ap-pears incorporation of gestures does not lead toany overall performance improvement (as shownin Table 3).
One possible explanation is that ges-ture features as they are incorporated here do pro-vide some disambiguating information (as shownin the two stage classification), but this informa-tion is subsumed by other features, such as dia-logue acts.
To test this hypothesis, we ran an ex-periment with a feature set that contained all fea-tures except dialogue act features.
That is, a fea-ture set that contains all of the automatic features,as well as gesture features.
Results are shown inTable 5.Our ?automatic + gesture?
feature configurationproduced accuracy of 66.2%.
When compared tothe same feature set without gesture features (the?Gupta + Frampton automatic?
row in Table 3) wesee a statistically significant (p < 0.01) absoluteaccuracy improvement of about 2.6%.
This seemsto suggest that gesture features are providing somesmall amount of relevant information that is notcaptured by our automatically extractable features.Up until this point we have incorporated dia-logue acts using the full set of dialogue act tagsprovided by the AMI corpus.
As we have men-311Precision Recall F-MeasureGupta automatic Plural 0.553 0.548 0.550Singular 0.657 0.408 0.504Generic 0.624 0.787 0.696Gupta manual Plural 0.536 0.513 0.524Singular 0.675 0.503 0.576Generic 0.704 0.839 0.766All (+ gesture) Plural 0.542 0.565 0.553Singular 0.745 0.604 0.667Generic 0.754 0.835 0.792Table 4: Precision, recall, and F-measure results for each you type based on three class classification.AccuracyGupta + Frampton automatic 63.6%Gupta + Frampton automatic + gesture 66.2%Gupta + Frampton automatic + simple dialogue act 66.6%Gupta + Frampton automatic + simple dialogue act + gesture 69.0%Table 5: Accuracy for 3-way classification by combining gesture information with automatically ex-tracted features based on the Decision Tree modeltioned, this level of granularity may not be prac-tically extractable for use in a current state-of-the-art system.
As a result, we implemented thesimpler dialogue act incorporation method pro-posed by (Gupta et al, 2007a), in which onlythe presence or absence of the elicit dialogue acttype is considered.
Using this feature with theautomatically extracted features yielded accuracyof 66.6%, a statistically significant improvement(p < 0.01) of an absolute 3% over a fully auto-matic run.
Furthermore, if we incorporate gesturefeatures with this configuration, the performanceincreases to 69.0% (statistically significantly, p <0.01).
This suggests that while gesture featuresmay be redundant with information provided bythe full set of dialogue act tags, it is largely com-plementary with the simpler dialogue act incorpo-ration.
The incorporation of gesture along withsimpler and more reliable dialogue acts can po-tentially approach the performance gained by in-corporation of more complex dialogue acts, whichare often difficult to obtain.
Of course, gesture fea-tures themselves are often difficult to obtain.
How-ever, redundancy in two potentially error-pronefeature sources can be an asset, as data from onesource may help to compensate for errors in theother.
Although addressing a different problem ofmultimodal integration, previous work (Oviatt etal., 1997) appears to indicate that this is the case.6 ConclusionIn this paper, we investigate the role of hand ges-tures in disambiguating types of You expressionsin multiparty meetings for the purpose of machinetranslation.Our results have shown that on the binarygeneric vs. referential classification problem, theinclusion of gesture data provides a statisticallysignificant increase in performance over the samefeature set without gesture.
This result is consis-tent with our hypothesis that gesture data would behelpful because speakers are more likely to gesturewhen producing referential instances of you.To produce results more akin to those thatwould be expected during incorporation in a realmachine translation system, we experimented withthe type identification problem as a three classclassification problem.
It was discovered thatwhen a full set of dialogue act tags were used asfeatures, the incorporation of gesture features doesnot provide an increase in performance.
However,when simpler dialogue act tags are used, the in-corporation of gestures helps to make up for lostperformance.
Since it remains a difficult prob-lem to automatically predict complex dialog actswith high accuracy, the incorporation of gesturefeatures may prove beneficial to current systems.3127 AcknowledgementThis work was supported by IIS-0855131 (to thefirst two authors) and IIS-0840461 (to the third au-thor) from the National Science Foundation.
Theauthors would like to thank anonymous reviewersfor valuable comments and suggestions.ReferencesTyler Baldwin, Joyce Y. Chai, and Katrin Kirchhoff.2009.
Communicative gestures in coreference iden-tification in multiparty meetings.
In ICMI-MLMI?09: Proceedings of the 2009 international con-ference on Multimodal interfaces, pages 211?218.ACM.J.
B. Bavelas, N. Chovil, L. Coates, and L. Roe.
1995.Gestures specialized for dialogue.
Personality andSocial Psychology Bulletin, 21:394?405.Jacob Eisenstein and Randall Davis.
2006.
Gestureimproves coreference resolution.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers, pages37?40, New York City, USA, June.
Association forComputational Linguistics.Jacob Eisenstein and Randall Davis.
2007.
Condi-tional modality fusion for coreference resolution.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 352?359, Prague, Czech Republic, June.
Association forComputational Linguistics.Matthew Frampton, Raquel Ferna?ndez, Patrick Ehlen,Mario Christoudias, Trevor Darrell, and Stanley Pe-ters.
2009. Who is ?you??
: combining linguis-tic and gaze features to resolve second-person refer-ences in dialogue.
In EACL ?09: Proceedings of the12th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 273?281, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Surabhi Gupta, John Niekrasz, Matthew Purver, andDan Jurafsky.
2007a.
Resolving you in multi-partydialog.
In Proceedings of the 8th SIGdial Workshopon Discourse and Dialogue.Surabhi Gupta, Matthew Purver, and Dan Jurafsky.2007b.
Disambiguating between generic and refer-ential you in dialog.
In Proceedings of the 42th An-nual Meeting of the Association for ComputationalLinguistics (ACL).Adam Kendon.
1980.
Gesticulation and speech: Twoaspects of the process of utterance.
In Mary RichieKey, editor, The Relationship of Verbal and Nonver-bal Communication, pages 207?227.D.
McNeill.
1992.
Hand and Mind: What GesturesReveal about Thought.
University of Chicago Press.W.
M. Meyers.
1990.
Current generic pronoun usage.American Speech, 65(3):228?237.Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.1997.
Integration and synchronization of inputmodes during multimodal human-computer interac-tion.
In CHI ?97: Proceedings of the SIGCHI con-ference on Human factors in computing systems,pages 415?422, New York, NY, USA.
ACM.Andrei Popescu-Belis and Paula Estrella.
2007.
Gen-erating usable formats for metadata and annotationsin a large meeting corpus.
In Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 93?96,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Matthew Purver, Raquel Ferna?ndez, Matthew Framp-ton, and Stanley Peters.
2009.
Cascaded lexicalisedclassifiers for second-person reference resolution.In SIGDIAL ?09: Proceedings of the SIGDIAL 2009Conference, pages 306?309, Morristown, NJ, USA.Association for Computational Linguistics.313
