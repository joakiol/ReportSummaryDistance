Reordering Constraints for Phrase-BasedStatistical Machine TranslationRichard Zens1, Hermann Ney1, Taro Watanabe2 and Eiichiro Sumita21Lehrstuhl fu?r Informatik VI 2 Spoken Language Translation Research LaboratoriesComputer Science Department ATRRWTH Aachen University, Germany Kyoto, Japan{zens,ney}@cs.rwth-aachen.de {watanabe,sumita}@slt.atr.co.jpAbstractIn statistical machine translation, the gen-eration of a translation hypothesis is com-putationally expensive.
If arbitrary re-orderings are permitted, the search prob-lem is NP-hard.
On the other hand,if we restrict the possible reorderingsin an appropriate way, we obtain apolynomial-time search algorithm.
We in-vestigate different reordering constraintsfor phrase-based statistical machine trans-lation, namely the IBM constraints andthe ITG constraints.
We present effi-cient dynamic programming algorithmsfor both constraints.
We evaluate the con-straints with respect to translation qualityon two Japanese?English tasks.
We showthat the reordering constraints improvetranslation quality compared to an un-constrained search that permits arbitraryphrase reorderings.
The ITG constraintspreform best on both tasks and yield sta-tistically significant improvements com-pared to the unconstrained search.1 IntroductionIn statistical machine translation, we are givena source language (?French?)
sentence fJ1 =f1 .
.
.
fj .
.
.
fJ , which is to be translated intoa target language (?English?)
sentence eI1 =e1 .
.
.
ei .
.
.
eI .
Among all possible target lan-guage sentences, we will choose the sentencewith the highest probability:e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}This decomposition into two knowledgesources is known as the source-channel ap-proach to statistical machine translation(Brown et al, 1990).
It allows an independentmodeling of target language model Pr(eI1) andtranslation model Pr(fJ1 |eI1).
The target lan-guage model describes the well-formedness ofthe target language sentence.
The translationmodel links the source language sentence tothe target language sentence.
It can be fur-ther decomposed into alignment and lexiconmodel.
The argmax operation denotes thesearch problem, i.e.
the generation of the out-put sentence in the target language.
We haveto maximize over all possible target languagesentences.An alternative to the classical source-channel approach is the direct modeling of theposterior probability Pr(eI1|fJ1 ).
Using a log-linear model (Och and Ney, 2002), we obtain:Pr(eI1|fJ1 ) = exp( M?m=1?mhm(eI1, fJ1 ))?
Z(fJ1 )Here, Z(fJ1 ) denotes the appropriate normal-ization constant.
As a decision rule, we obtain:e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}This approach is a generalization of thesource-channel approach.
It has the advan-tage that additional models or feature func-tions can be easily integrated into the over-all system.
The model scaling factors ?M1 aretrained according to the maximum entropyprinciple, e.g.
using the GIS algorithm.
Al-ternatively, one can train them with respectto the final translation quality measured bysome error criterion (Och, 2003).In this paper, we will investigate the re-ordering problem for phrase-based translationapproaches.
As the word order in source andtarget language may differ, the search algo-rithm has to allow certain reorderings.
If arbi-trary reorderings are allowed, the search prob-lem is NP-hard (Knight, 1999).
To obtain anefficient search algorithm, we can either re-strict the possible reorderings or we have touse an approximation algorithm.
Note that inthe latter case we cannot guarantee to find anoptimal solution.The remaining part of this work is struc-tured as follows: in the next section, wewill review the baseline translation system,namely the alignment template approach.
Af-terward, we will describe different reorderingconstraints.
We will begin with the IBM con-straints for phrase-based translation.
Then,we will describe constraints based on inver-sion transduction grammars (ITG).
In the fol-lowing, we will call these the ITG constraints.In Section 4, we will present results for twoJapanese?English translation tasks.2 Alignment Template ApproachIn this section, we give a brief description ofthe translation system, namely the alignmenttemplate approach.
The key elements of thistranslation approach (Och et al, 1999) are thealignment templates.
These are pairs of sourceand target language phrases with an alignmentwithin the phrases.
The alignment templatesare build at the level of word classes.
Thisimproves the generalization capability of thealignment templates.We use maximum entropy to train themodel scaling factors (Och and Ney, 2002).As feature functions we use a phrase transla-tion model as well as a word translation model.Additionally, we use two language model fea-ture functions: a word-based trigram modeland a class-based five-gram model.
Further-more, we use two heuristics, namely the wordpenalty and the alignment template penalty.To model the alignment template reorderings,we use a feature function that penalizes re-orderings linear in the jump width.A dynamic programming beam search al-gorithm is used to generate the translationhypothesis with maximum probability.
Thissearch algorithm allows for arbitrary reorder-ings at the level of alignment templates.Within the alignment templates, the reorder-ing is learned in training and kept fix duringthe search process.
There are no constraintson the reorderings within the alignment tem-plates.This is only a brief description of the align-ment template approach.
For further details,see (Och et al, 1999; Och and Ney, 2002).3 Reordering ConstraintsAlthough unconstrained reordering looks per-fect from a theoretical point of view, we findthat in practice constrained reordering showsJuncovered positioncovered positionuncovered position for extension1 jFigure 1: Illustration of the IBM constraintswith k = 3, i.e.
up to three positions may beskipped.better performance.
The possible advantagesof reordering constraints are:1.
The search problem is simplified.
As aresult there are fewer search errors.2.
Unconstrained reordering is only helpfulif we are able to estimate the reorder-ing probabilities reliably, which is unfor-tunately not the case.In this section, we will describe two variantsof reordering constraints.
The first constraintsare based on the IBM constraints for single-word based translation models.
The secondconstraints are based on ITGs.
In the follow-ing, we will use the term ?phrase?
to mean ei-ther a sequence of words or a sequence of wordclasses as used in the alignment templates.3.1 IBM ConstraintsIn this section, we describe restrictions on thephrase reordering in spirit of the IBM con-straints (Berger et al, 1996).First, we briefly review the IBM constraintsat the word level.
The target sentence is pro-duced word by word.
We keep a coverage vec-tor to mark the already translated (covered)source positions.
The next target word has tobe the translation of one of the first k uncov-ered, i.e.
not translated, source positions.
TheIBM constraints are illustrated in Figure 1.For further details see e.g.
(Tillmann and Ney,2003).For the phrase-based translation approach,we use the same idea.
The target sentence isproduced phrase by phrase.
Now, we allowskipping of up to k phrases.
If we set k = 0,we obtain a search that is monotone at thephrase level as a special case.The search problem can be solved using dy-namic programming.
We define a auxiliaryfunction Q(j, S, e).
Here, the source positionj is the first unprocessed source position; withunprocessed, we mean this source position isneither translated nor skipped.
We use theset S = {(jn, ln)|n = 1, ..., N} to keep trackof the skipped source phrases with lengths lnand starting positions jn.
We show the formu-lae for a bigram language model and use thetarget language word e to keep track of thelanguage model history.
The symbol $ is usedto mark the sentence start and the sentenceend.
The extension to higher-order n-gramlanguage models is straightforward.
We useM to denote the maximum phrase length inthe source language.
We obtain the followingdynamic programming equations:Q(1, ?, $) = 1Q(j, S, e) = max{maxe?,e?
{maxj?M?j?<jQ(j?, S, e?)
?
p(f j?1j?
|e?)
?
p(e?|e?
),max(j?,l)?S?S=S?\{(j?,l)}Q(j, S?, e?)
?
p(f j?+l?1j?
|e?)
?
p(e?|e?)},maxj?M?j?<jS?:S=S??{(j?,j?j?
)}?|S?|<kQ(j?, S?, e)}Q(J + 2, ?, $) = maxe Q(J + 1, ?, e) ?
p($|e)In the recursion step, we have distinguishedthree cases: in the first case, we translate thenext source phrase.
This is the same expan-sion that is done in monotone search.
In thesecond case, we translate a previously skippedphrase and in the third case we skip a sourcephrase.
For notational convenience, we haveomitted one constraint in the preceding equa-tions: the final word of the target phrase e?
isthe new language model state e (using a bi-gram language model).Now, we analyze the complexity of this al-gorithm.
Let E denote the vocabulary size ofthe target language and let E?
denote the max-imum number of phrase translation candidatesfor a given source phrase.
Then, J ?
(J ?M)k ?Eis an upper bound for the size of the Q-table.Once we have fixed a specific element of thistable, the maximization steps can be done inO(E ?
E?
?
(M + k ?
1) + (k ?
1)).
There-fore, the complexity of this algorithm is inO(J ?
(J ?M)k ?E ?
(E ?E?
?
(M+k?1)+(k?1))).Assuming k < M , this can be simplified toO((J ?M)k+1 ?E2 ?
E?).
As already mentioned,source positionstargetpositionswithout inversion with inversionsource positionstargetpositionsFigure 2: Illustration of monotone andinverted concatenation of two consecutiveblocks.setting k = 0 results in a search algorithm thatis monotone at the phrase level.3.2 ITG ConstraintsIn this section, we describe the ITG con-straints (Wu, 1995; Wu, 1997).
Here, we inter-pret the input sentence as a sequence of blocks.In the beginning, each alignment template is ablock of its own.
Then, the reordering processcan be interpreted as follows: we select twoconsecutive blocks and merge them to a singleblock by choosing between two options: eitherkeep the target phrases in monotone order orinvert the order.
This idea is illustrated in Fig-ure 2.
The dark boxes represent the two blocksto be merged.
Once two blocks are merged,they are treated as a single block and they canbe only merged further as a whole.
It is notallowed to merge one of the subblocks again.3.2.1 Dynamic Programming AlgorithmThe ITG constraints allow for a polynomial-time search algorithm.
It is based on the fol-lowing dynamic programming recursion equa-tions.
During the search a table Qjl,jr,eb,etis constructed.
Here, Qjl,jr,eb,et denotes theprobability of the best hypothesis translatingthe source words from position jl (left) to po-sition jr (right) which begins with the targetlanguage word eb (bottom) and ends with theword et (top).
This is illustrated in Figure 3.The initialization is done with the phrase-based model described in Section 2.
We in-troduce a new parameter pm (m=?
monotone),which denotes the probability of a monotonecombination of two partial hypotheses.
Here,we formulate the recursion equation for a bi-gram language model, but of course, the samemethod can also be applied for a trigram lan-jl jre betFigure 3: Illustration of the Q-table.guage model.Qjl,jr,eb,et =maxjl?k<jr,e?,e??
{Q0jl,jr,eb,et ,Qjl,k,eb,e?
?Qk+1,jr,e?
?,et ?
p(e??|e?)
?
pm,Qk+1,jr,eb,e?
?Qjl,k,e?
?,et ?
p(e??|e?)
?
(1?
pm)}The resulting algorithm is similar to the CYK-parsing algorithm.
It has a worst-case com-plexity of O(J3 ?E4).
Here, J is the length ofthe source sentence and E is the vocabularysize of the target language.3.2.2 Beam Search AlgorithmFor the ITG constraints a dynamic program-ming search algorithm exists as described inthe previous section.
It would be more prac-tical with respect to language model recom-bination to have an algorithm that generatesthe target sentence word by word or phraseby phrase.
The idea is to start with the beamsearch decoder for unconstrained search andmodify it in such a way that it will produceonly reorderings that do not violate the ITGconstraints.
Now, we describe one way to ob-tain such a decoder.
It has been pointed outin (Zens and Ney, 2003) that the ITG con-straints can be characterized as follows: a re-ordering violates the ITG constraints if andonly if it contains (3, 1, 4, 2) or (2, 4, 1, 3) asa subsequence.
This means, if we select fourcolumns and the corresponding rows from thealignment matrix and we obtain one of the twopatterns illustrated in Figure 4, this reorderingcannot be generated with the ITG constraints.Now, we have to modify the beam searchdecoder such that it cannot produce these twopatterns.
We implement this in the follow-ing way.
During the search, we have a cover-age vector cov of the source sentence availablefor each partial hypothesis.
A coverage vec-1234a b c d1234a b c dFigure 4: Illustration of the two reorderingpatterns that violate the ITG constraints.tor is a binary vector marking the source sen-tence words that have already been translated(covered).
Additionally, we know the currentsource sentence position jc and a candidatesource sentence position jn to be translatednext.To avoid the patterns in Figure 4, we haveto constrain the placement of the third phrase,because once we have placed the first threephrases we also have determined the positionof the fourth phrase as the remaining uncov-ered position.
Thus, we check the followingconstraints:case a) jn < jc (1)?jn < j < jc : cov[j] ?
cov[j + 1]case b) jc < jn (2)?jc < j < jn : cov[j] ?
cov[j ?
1]The constraints in Equations 1 and 2 enforcethe following: imagine, we traverse the cover-age vector cov from the current position jc tothe position to be translated next jn.
Then,it is not allowed to move from an uncoveredposition to a covered one.Now, we sketch the proof that these con-straints are equivalent to the ITG constraints.It is easy to see that the constraint in Equa-tion 1 avoids the pattern on the left-hand sidein Figure 4.
To be precise: after placing thefirst two phrases at (b,1) and (d,2), it avoidsthe placement of the third phrase at (a,3).Similarly, the constraint in Equation 2 avoidthe pattern on the right-hand side in Fig-ure 4.
Therefore, if we enforce the constraintsin Equation 1 and Equation 2, we cannot vio-late the ITG constraints.We still have to show that we can gener-ate all the reorderings that do not violate theITG constraints.
Equivalently, we show thatany reordering that violates the constraints inEquation 1 or Equation 2 will also violate theITG constraints.
It is rather easy to see thatany reordering that violates the constraint inTable 1: Statistics of the BTEC corpus.Japanese Englishtrain Sentences 152 KWords 1 044 K 893 KVocabulary 17 047 12 020dev sentences 500words 3 361 2 858test sentences 510words 3 498 ?Table 2: Statistics of the SLDB corpus.Japanese Englishtrain Sentences 15 KWords 201 K 190 KVocabulary 4 757 3 663test sentences 330words 3 940 ?Equation 1 will generate the pattern on theleft-hand side in Figure 4.
The conditions toviolate Equation 1 are the following: the newcandidate position jn is to the left of the cur-rent position jc, e.g.
positions (a) and (d).Somewhere in between there has to be an cov-ered position j whose successor position j + 1is uncovered, e.g.
(b) and (c).
Therefore, anyreordering that violates Equation 1 generatesthe pattern on the left-hand side in Figure 4,thus it violates the ITG constraints.4 Results4.1 Corpus StatisticsTo investigate the effect of reordering con-straints, we have chosen two Japanese?Englishtasks, because the word order in Japanese andEnglish is rather different.
The first task is theBasic Travel Expression Corpus (BTEC) task(Takezawa et al, 2002).
The corpus statisticsare shown in Table 1.
This corpus consists ofphrasebook entries.The second task is the Spoken LanguageDataBase (SLDB) task (Morimoto et al,1994).
This task consists of transcription ofspoken dialogs in the domain of hotel reser-vation.
Here, we use domain-specific trainingdata in addition to the BTEC corpus.
Thecorpus statistics of this additional corpus areshown in Table 2.
The development corpus isthe same for both tasks.4.2 Evaluation CriteriaWER (word error rate).
The WER is com-puted as the minimum number of substitution,insertion and deletion operations that have tobe performed to convert the generated sen-tence into the reference sentence.PER (position-independent word er-ror rate).
A shortcoming of the WER is thatit requires a perfect word order.
The word or-der of an acceptable sentence can be differentfrom that of the target sentence, so that theWER measure alone could be misleading.
ThePER compares the words in the two sentencesignoring the word order.BLEU.
This score measures the precisionof unigrams, bigrams, trigrams and fourgramswith respect to a reference translation with apenalty for too short sentences (Papineni etal., 2002).
The BLEU score measures accu-racy, i.e.
large BLEU scores are better.NIST.
This score is similar to BLEU.
It isa weighted n-gram precision in combinationwith a penalty for too short sentences (Dod-dington, 2002).
The NIST score measures ac-curacy, i.e.
large NIST scores are better.Note that for each source sentence, we haveas many as 16 references available.
We com-pute all the preceding criteria with respect tomultiple references.4.3 System ComparisonIn Table 3 and Table 4, we show the trans-lation results for the BTEC task.
First, weobserve that the overall quality is rather highon this task.
The average length of the usedalignment templates is about five source wordsin all systems.
The monotone search (mon)shows already good performance on short sen-tences with less than 10 words.
We concludethat for short sentences the reordering is cap-tured within the alignment templates.
On theother hand, the monotone search degrades forlong sentences with at least 10 words resultingin a WER of 16.6% for these sentences.We present the results for various nonmono-tone search variants: the first one is with theIBM constraints (skip) as described in Sec-tion 3.1.
We allow for skipping one or twophrases.
Our experiments showed that if weset the maximum number of phrases to beskipped to three or more the translation re-sults are equivalent to the search without anyreordering constraints (free).
The results forthe ITG constraints as described in Section 3.2are also presented.The unconstrained reorderings improve thetotal translation quality down to a WER of11.5%.
We see that especially the long sen-tences benefit from the reorderings resulting inan improvement from 16.6% to 13.8%.
Com-paring the results for the free reorderings andTable 3: Translation performance WER[%]for the BTEC task (510 sentences).
Sentencelengths: short: < 10 words, long: ?
10 words;times in milliseconds per sentence.WER[%]sentence lengthreorder short long all time[ms]mon 11.4 16.6 12.7 73skip 1 10.8 13.5 11.4 1342 10.8 13.4 11.4 169free 10.8 13.8 11.5 194ITG 10.6 12.2 11.0 164Table 4: Translation performance for theBTEC task (510 sentences).error rates[%] accuracy measuresreorder WER PER BLEU[%] NISTmon 12.7 10.6 86.8 14.14skip 1 11.4 10.1 88.0 14.192 11.4 10.1 88.1 14.20free 11.5 10.0 88.0 14.19ITG 11.0 9.9 88.2 14.25the ITG reorderings, we see that the ITGsystem always outperforms the unconstrainedsystem.
The improvement on the whole testset is statistically significant at the 95% level.1In Table 5 and Table 6, we show the re-sults for the SLDB task.
First, we observethat the overall quality is lower than for theBTEC task.
The SLDB task is a spoken lan-guage translation task and the training cor-pus for spoken language is rather small.
Thisis also reflected in the average length of theused alignment templates that is about threesource words compared to about five words forthe BTEC task.The results on this task are similar to theresults on the BTEC task.
Again, the ITGconstraints perform best.
Here, the improve-ment compared to the unconstrained search isstatistically significant at the 99% level.
Com-pared to the monotone search, the BLEU scorefor the ITG constraints improves from 54.4%to 57.1%.5 Related WorkRecently, phrase-based translation approachesbecame more and more popular.
Marcu andWong (2002) present a joint probability modelfor phrase-based translation.
In (Koehn et1The statistical significance test were done for theWER using boostrap resampling.Table 5: Translation performance WER[%]for the SLDB task (330 sentences).
Sentencelengths: short: < 10 words, long: ?
10 words;times in milliseconds per sentence.WER[%]sentence lengthreorder short long all time[ms]mon 32.0 52.6 48.1 911skip 1 31.9 51.1 46.9 3 1752 32.0 51.4 47.2 4 549free 32.0 51.4 47.2 4 993ITG 31.8 50.9 46.7 4 472Table 6: Translation performance for theSLDB task (330 sentences).error rates[%] accuracy measuresreorder WER PER BLEU[%] NISTmon 48.1 35.5 54.4 9.45skip 1 46.9 35.0 56.8 9.712 47.2 35.1 57.1 9.74free 47.2 34.9 57.1 9.75ITG 46.7 34.6 57.1 9.76al., 2003), various aspects of phrase-basedsystems are compared, e.g.
the phrase ex-traction method, the underlying word align-ment model, or the maximum phrase length.In (Vogel, 2003), a phrase-based system isused that allows reordering within a windowof up to three words.
Improvements for aChinese?English task are reported comparedto a monotone search.The ITG constraints were introduced in(Wu, 1995).
The applications were, for in-stance, the segmentation of Chinese charactersequences into Chinese words and the bracket-ing of the source sentence into sub-sententialchunks.
Investigations on the IBM constraints(Berger et al, 1996) for single-word based sta-tistical machine translation can be found e.g.in (Tillmann and Ney, 2003).
A comparison ofthe ITG constraints and the IBM constraintsfor single-word based models can be found in(Zens and Ney, 2003).
In this work, we investi-gated these reordering constraints for phrase-based statistical machine translation.6 ConclusionsWe have presented different reordering con-straints for phrase-based statistical machinetranslation, namely the IBM constraints andthe ITG constraints, as well as efficient dy-namic programming algorithms.
Transla-tion results were reported for two Japanese?English translation tasks.
Both type of re-ordering constraints resulted in improvementscompared to a monotone search.
Restrict-ing the reorderings according to the IBM con-straints resulted already in a translation qual-ity similar to an unconstrained search.
Thetranslation results with the ITG constraintseven outperformed the unconstrained searchconsistently on all error criteria.
The improve-ments have been found statistically significant.The ITG constraints showed the best per-formance on both tasks.
Therefore we plan tofurther improve this method.
Currently, theprobability model for the ITG constraints isvery simple.
More sophisticated models, suchas phrase dependent inversion probabilities,might be promising.AcknowledgmentsThis work was partially done at the SpokenLanguage Translation Research Laboratories(SLT) at the Advanced TelecommunicationResearch Institute International (ATR), Ky-oto, Japan.
This research was supported inpart by the Telecommunications AdvancementOrganization of Japan.
This work has beenpartially funded by the EU project PF-Star,IST-2001-37599.ReferencesA.
L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D.Pietra, J. R. Gillett, A. S. Kehler, and R. L.Mercer.
1996.
Language translation apparatusand method of using context-based translationmodels, United States patent, patent number5510981, April.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J.Della Pietra, F. Jelinek, J. D. Lafferty, R. L.Mercer, and P. S. Roossin.
1990.
A statisti-cal approach to machine translation.
Compu-tational Linguistics, 16(2):79?85, June.G.
Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proc.
ARPA Workshopon Human Language Technology.K.
Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25(4):607?615, December.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Sta-tistical phrase-based translation.
In Proc.
ofthe Human Language Technology Conf.
(HLT-NAACL), pages 127?133, Edmonton, Canada,May/June.D.
Marcu and W. Wong.
2002.
A phrase-based,joint probability model for statistical machinetranslation.
In Proc.
Conf.
on Empirical Meth-ods for Natural Language Processing, pages 133?139, Philadelphia, PA, July.T.
Morimoto, N. Uratani, T. Takezawa, O. Furuse,Y.
Sobashima, H. Iida, A. Nakamura, Y. Sag-isaka, N. Higuchi, and Y. Yamazaki.
1994.
Aspeech and language database for speech trans-lation research.
In Proc.
of the 3rd Int.
Conf.
onSpoken Language Processing (ICSLP?94), pages1791?1794, Yokohama, Japan, September.F.
J. Och and H. Ney.
2002.
Discriminative train-ing and maximum entropy models for statisti-cal machine translation.
In Proc.
of the 40thAnnual Meeting of the Association for Com-putational Linguistics (ACL), pages 295?302,Philadelphia, PA, July.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Im-proved alignment models for statistical machinetranslation.
In Proc.
of the Joint SIGDAT Conf.on Empirical Methods in Natural Language Pro-cessing and Very Large Corpora, pages 20?28,University of Maryland, College Park, MD,June.F.
J. Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
ofthe 41th Annual Meeting of the Association forComputational Linguistics (ACL), pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
of the 40thAnnual Meeting of the Association for Com-putational Linguistics (ACL), pages 311?318,Philadelphia, PA, July.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto,and S. Yamamoto.
2002.
Toward a broad-coverage bilingual corpus for speech translationof travel conversations in the real world.
InProc.
of the Third Int.
Conf.
on Language Re-sources and Evaluation (LREC), pages 147?152,Las Palmas, Spain, May.C.
Tillmann and H. Ney.
2003.
Word reorderingand a dynamic programming beam search algo-rithm for statistical machine translation.
Com-putational Linguistics, 29(1):97?133, March.S.
Vogel.
2003.
SMT decoder dissected: Word re-ordering.
In Proc.
of the Int.
Conf.
on NaturalLanguage Processing and Knowledge Engineer-ing (NLP-KE), pages 561?566, Beijing, China,October.D.
Wu.
1995.
Stochastic inversion transductiongrammars, with application to segmentation,bracketing, and alignment of parallel corpora.In Proc.
of the 14th International Joint Conf.on Artificial Intelligence (IJCAI), pages 1328?1334, Montreal, August.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, September.R.
Zens and H. Ney.
2003.
A comparative studyon reordering constraints in statistical machinetranslation.
In Proc.
of the 41th Annual Meet-ing of the Association for Computational Lin-guistics (ACL), pages 144?151, Sapporo, Japan,July.
