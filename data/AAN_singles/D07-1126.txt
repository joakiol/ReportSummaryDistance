Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1149?1155,Prague, June 2007. c?2007 Association for Computational LinguisticsA Multilingual Dependency Analysis System using OnlinePassive-Aggressive LearningLe-Minh Nguyen, Akira Shimazu, and Phuong-Thai NguyenJapan Advanced Institute of Science and Technology (JAIST)Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{nguyenml,shimazu,thai}@jaist.ac.jpXuan-Hieu PhanTohoku UniversityAobayama 6-3-09, Sendai, 980-8579, Japanhieuxuan@ecei.tohoku.ac.jpAbstractThis paper presents an online algorithm fordependency parsing problems.
We proposean adaptation of the passive and aggressiveonline learning algorithm to the dependencyparsing domain.
We evaluate the proposedalgorithms on the 2007 CONLL SharedTask, and report errors analysis.
Experimen-tal results show that the system score is bet-ter than the average score among the partici-pating systems.1 IntroductionResearch on dependency parsing is mainly basedon machine learning methods, which can be calledhistory-based (Yamada and Matsumoto, 2003; Nivreet al, 2006) and discriminative learning methods(McDonald et al, 2005a; Corston-Oliver et al,2006).
The learning methods using in discrimina-tive parsing are Perceptron (Collins, 2002) and on-line large-margin learning (MIRA) (Crammer andSinger, 2003).The difference of MIRA-based parsing in com-parison with history-based methods is that theMIRA-based parser were trained to maximize theaccuracy of the overall tree.
The MIRA basedparsing is close to maximum-margin parsing as inTaskar et al (2004) and Tsochantaridis et al (2005)for parsing.
However, unlike maximum-marginparsing, it is not limited to parsing sentences of 15words or less due to computation time.
The perfor-mance of MIRA based parsing achieves the state-of-the-art performance in English data (McDonald etal., 2005a; McDonald et al, 2006).In this paper, we propose a new adaptation of on-line larger-margin learning to the problem of depen-dency parsing.
Unlike the MIRA parser, our methoddoes not need an optimization procedure in eachlearning update, but users only an update equation.This might lead to faster training time and easier im-plementation.The contributions of this paper are two-fold: First,we present a training algorithm called PA learningfor dependency parsing, which is as easy to im-plement as Perceptron, yet competitive with largemargin methods.
This algorithm has implicationsfor anyone interested in implementing discrimina-tive training methods for any application.
Second,we evaluate the proposed algorithm on the multilin-gual data task as well as the domain adaptation task(Nivre et al, 2007).The remaining parts of the paper are organized asfollows: Section 2 proposes our dependency pars-ing with Passive-Aggressive learning.
Section 3discusses some experimental results and Section 4gives conclusions and plans for future work.2 Dependency Parsing withPassive-Aggressive LearningThis section presents the modification of Passive-Aggressive Learning (PA) (Crammer et al, 2006)for dependency parsing.
We modify the PA algo-rithm to deal with structured prediction, in whichour problem is to learn a discriminant function thatmaps an input sentence x to a dependency tree y.Figure 1 shows an example of dependency parsingwhich depicts the relation of each word to anotherword within a sentence.
There are some algorithms1149Figure 1: This is an example of dependency treeto determine these relations of each word to anotherwords, for instance, the modified CKY algorithm(Eisner, 1996) is used to define these relations fora given sentence.2.1 Parsing AlgorithmDependency-tree parsing as the search for the maxi-mum spanning tree (MST) in a graph was proposedby McDonald et al (2005b).
In this subsection,we briefly describe the parsing algorithms based onthe first-order MST parsing.
Due to the limitationof participation time, we only applied the first-orderdecoding parsing algorithm in CONLL-2007.
How-ever, our algorithm can be used for the second orderparsing.Let the generic sentence be denoted by x ; theith word of x is denoted by wi.
The generic de-pendency tree is denoted by y.
If y is a dependencytree for sentence x, we write (i, j) ?
y to indicatethat there is a directed edge from word xwi to wordxwj in the tree, that is, xwi is the parent of xwj .T = {(xt, yt)}nt=1 denotes the training data.
We fol-low the edge based factorization method of Eisner(Eisner, 1996) and define the score of a dependencytree as the sum of the score of all edges in the tree,s(x, y) =?
(i,j)?ys(i, j) =?
(i,j)?yw ?
?
(i, j) (1)where ?
(i, j) is a high-dimensional binary fea-ture representation of the edge from xwi to xwj .For example in Figure 1, we can present an example?
(i, j) as follows;?
(i, j) ={1 if xwi =?
hit?
andxwj =?
ball?0 otherwiseThe basic question must be answered for modelsof this form: how to find the dependency tree y withthe highest score for sentence x?
The two algorithmswe employed in our dependency parsing model arethe Eisner parsing (Eisner, 1996) and Chu-Liu?s al-gorithm (Chu and Liu, 1965).
The algorithms arecommonly used in other online-learning dependencyparsing, such as in (McDonald et al, 2005a).In the next subsection we will address the problemof how to estimate the weight wi associated with afeature ?i in the training data using an online PAlearning algorithm.2.2 Online PA LearningThis section presents a modification of PA algo-rithm for structured prediction, and its use in de-pendency parsing.
The Perceptron style for naturallanguage processing problems as initially proposedby (Collins, 2002) can provide state of the art re-sults on various domains including text chunking,syntactic parsing, etc.
The main drawback of thePerceptron style algorithm is that it does not have amechanism for attaining the maximize margin of thetraining data.
It may be difficult to obtain high accu-racy in dealing with hard learning data.
The struc-tured support vector machine (Tsochantaridis et al,2005) and the maximize margin model (Taskar et al,2004) can gain a maximize margin value for giventraining data by solving an optimization problem (i.equadratic programming).
It is obvious that usingsuch an optimization algorithm requires much com-putational time.
For dependency parsing domain,McDonald et al(2005a) modified the MIRA learn-ing algorithm (McDonald et al, 2005a) for struc-tured domains in which the optimization problemcan be solved by using Hidreth?s algorithm (Censorand Zenios, 1997), which is faster than the quadraticprogramming technique.
In contrast to the previousmethod, this paper presents an online algorithm fordependency parsing in which we can attain the max-imize margin of the training data without using opti-mization techniques.
It is thus much faster and eas-ier to implement.
The details of PA algorithm fordependency parsing are presented below.Assume that we are given a set of sentences xiand their dependency trees yi where i = 1, ..., n. Letthe feature mapping between a sentence x and a treey be: ?
(x, y) = ?1(x, y),?2(x, y), ...,?d(x, y)where each feature mapping ?j maps (x, y) to a realvalue.
We assume that each feature ?
(x, y) is asso-1150ciated with a weight value.
The goal of PA learningfor dependency parsing is to obtain a parameter wthat minimizes the hinge-loss function and the mar-gin of learning data.Input:S = {(xi; yi), i = 1, 2, ..., n} in which1xi is the sentence and yi is a dependency treeAggressive parameter C2Output: the PA learning model3Initialize: w1 = (0, 0, ..., 0)4for t=1, 2... do5Receive an sentence xt6Predict y?t = argmaxy?Y (wt ?
?
(xt, yt))7Suffer loss: lt =8wt ??
(xt, y?t )?wt ??
(xt, yt) +??
(yt, y?t )Set:9PA: ?t = lt||?
(xt,y?t )??
(xt,yt)||2PA-I: ?t = min{C, lt||?
(xt,y?t )??
(xt,yt)||2 }PA-II: ?t = lt||?
(xt,y?t )??
(xt,yt)||2+ 12CUpdate:wt+1 = wt + ?t(?
(xt, yt)?
?
(xt, y?t ))end10Algorithm 1: The Passive-Aggressive algo-rithm for dependency parsing.Algorithm 1 shows the PA learning algorithm fordependency parsing in which its three variants aredifferent only in the update formulas.
In Algorithm1, we employ two kinds of argmax algorithms: Thefirst is the decoding algorithm for projective lan-guage data and the second one is for non-projectivelanguage data.
Algorithm 1 shows (line 8) p(y, yt)is a real-valued loss for the tree yt relative to thecorrect tree y.
We define the loss of a dependencytree as the number of words which have an incorrectparent.
Thus, the largest loss a dependency tree canhave is the length of the sentence.
The similar lossfunction is designed for the dependency tree with la-beled.
Algorithm 1 returns an averaged weight vec-tor: an auxiliary weight vector v is maintained thataccumulates the values of w after each iteration, andthe returned weight vector is the average of all theweight vectors throughout training.
Averaging hasbeen shown to help reduce overfitting (McDonald etal., 2005a; Collins, 2002).
It is easy to see that themain difference between the PA algorithms and thePerceptron algorithm (PC) (Collins, 2002) as well asthe MIRA algorithm (McDonald et al, 2005a) is inline 9.
As we can see in the PC algorithm, we donot need the value ?t and in the MIRA algorithm weneed an optimization algorithm to compute ?t.
Wealso have three updated formulations for obtaining?t in Line 9.
In the scope of this paper, we onlyfocus on using the second update formulation (PA-Imethod) for training dependency parsing data.2.3 Feature SetWe denote p-word: word of parent node in depen-dency tree.
c-word: word of child node.
p-pos: POSof parent node.
c-pos: POS of child node.
p-pos+1:POS to the right of parent in sentence.
p-pos-1: POSto the left of parent.
c-pos+1: POS to the right ofchild.
c-pos-1: POS to the left of child.
b-pos: POSof a word in between parent and child nodes.
Thep-word,p-posp-wordp-posc-word, c-posc-wordc-posTable 1: Feature Set 1: Basic Unit-gram featuresp-word, p-pos, c-word, c-posp-pos, c-word, c-posp-word, c-word, c-posp-word, p-pos, c-posp-word, p-pos, c-wordp-word, c-wordp-pos, c-posTable 2: Feature Set 2: Basic bi-gram featuresp-pos, b-pos, c-posp-pos, p-pos+1, c-pos-1, c-posp-pos-1, p-pos, c-pos-1, c-posp-pos, p-pos+1, c-pos, c-pos+1p-pos-1, p-pos, c-pos, c-pos+1Table 3: Feature Set 3: In Between POS Featuresand Surrounding Word POS Featuresfeatures used in our system are described below.?
Tables 1 and 2 show our basic features.
These1151features are added for entire words as well asfor the 5-gram prefix if the word is longer than5 characters.?
In addition to these features shown in Table 1,the morphological information for each pair ofwords p-word and c-word are represented.
Inaddition, we also add the conjunction morpho-logical information of p-word and c-word.
Wedo not use the LEMMA and CPOSTAG infor-mation in our set features.
The morphologicalinformation are obtained from FEAT informa-tion.?
Table 3 shows our Feature set 3 which take theform of a POS trigram: the POS of the par-ent, of the child, and of a word in between,for all words linearly between the parent andthe child.
This feature was particularly helpfulfor nouns identifying their parent (McDonaldet al, 2005a).?
Table 3 also depicts these features taken theform of a POS 4-gram: The POS of the par-ent, child, word before/after parent and wordbefore/after child.
The system also used back-off features with various trigrams where one ofthe local context POS tags was removed.?
All features are also conjoined with the direc-tion of attachment, as well as the distance be-tween the two words being attached.3 Experimental Results and DiscussionWe test our parsing models on the CONLL-2007(Hajic?
et al, 2004; Aduriz et al, 2003; Mart??
etal., 2007; Chen et al, 2003; Bo?hmova?
et al, 2003;Marcus et al, 1993; Johansson and Nugues, 2007;Prokopidis et al, 2005; Csendes et al, 2005; Mon-temagni et al, 2003; Oflazer et al, 2003) data set onvarious languages including Arabic, Basque, Cata-lan, Chinese, English, Italian, Hungarian, and Turk-ish.
Each word is attached by POS tags for each sen-tence in both the training and the testing data.
Table4 shows the number of training and testing sentencesfor these languages.
The table shows that the sen-tence length in Arabic data is largest and its size oftraining data is smallest.
These factors might be af-fected to the accuracy of our proposed algorithm aswe will discuss later.The training and testing were conducted on a pen-tium IV at 4.3 GHz.
The detailed information aboutthe data are shown in the CONLL-2007 shared task.We applied non-projective and projective parsingalong with PA learning for the data in CONLL-2007.Table 5 reports experimental results by using thefirst order decoding method in which an MST pars-ing algorithm (McDonald et al, 2005b) is appliedfor non-projective parsing and the Eisner?s methodis used for projective language data.
In fact, in ourmethod we applied non-projective parsing for theItalian data, the Turkish data, and the Greek data.This was because we did not have enough time totrain all training data using both projective and non-projective parsing.
This is the problem of discrimi-native learning methods when performing on a largeset of training data.
In addition, to save time in train-ing we set the number of best trees k to 1 and theparameter C is set to 0.05.Table 5 shows the comparison of the proposedmethod with the average, and three top systems onthe CONLL-2007.
As a result, our method yieldsresults above the average score on the CONLL-2007shared task (Nivre et al, 2007).Table 5 also indicates that the Basque results ob-tained a lower score than other data.
We obtained69.11 UA score and 58.16 LA score, respectively.These are far from the results of the Top3 scores(81.13 and 75.49).
We checked the outputs of theBasque data to understand the main reason for theerrors.
We see that the errors in our methods areusually mismatched with the gold data at the labels?ncmod?
and ?ncsubj?.
The main reason might bethat the application of projective parsing for this datain both training and testing is not suitable.
This wasbecause the number of sentences with at least 1 nonprojective relation in the data is large (26.1).The Arabic score is lower than the scores of otherdata because of some difficulties in our method asfollows.
Morphological and sentence length prob-lems are the main factors which affect the accuracyof parsing Arabic data.
In addition, the training sizein the Arabic is also a problem for obtaining a goodresult.
Furthermore, since our tasks was focused onimproving the accuracy of English data, it might beunsuitable for other languages.
This is an imbalance1152Languages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPRArabic 2,900 112,000 38.3 0.4 10.1Basque 3,200 51,000 15.8 2.9 26.2Catalan 15,000 431,000 28.8 0.1 2.9Chinese 57,000 337,000 5.9 0.0 0.0Czech 25,400 432,000 17.0 1.9 23.2English 18,600 447,000 24.0 0.3 6.7Greek 2,700 65,000 24.2 1.1 20.3Hungarian 6,000 132,000 21.8 2.9 26.4Italian 3,100 71,000 22.9 0.5 7.4Turkish 5,600 65,000 11.6 0.5 33.3Table 4: The data used in the multilingual track (Nivre et al, 2007).
NPR means non-projective-relations.AL-1-NPR means at-least-least 1 non-projective relation.problem in our method.
Table 5 also shows the com-parison of our system to the average score and theTop3 scores.
It depicts that our system is accuratein English data, while it has low accuracy in Basqueand Arabic data.We also evaluate our models in the domain adap-tation tasks.
This task is to adapt our model trainedon PennBank data to the test data in the Biomedicaldomain.
The pchemtb-closed shared task (Marcuset al, 1993; Johansson and Nugues, 2007; Kulicket al, 2004) is used to illustrate our models.
We donot use any additional unlabeled data in the Biomed-ical domain.
Only the training data in the PennBankis used to train our model.
Afterward, we selectedcarefully a suitable parameter using the developmenttest set.
We set the parameter C to 0.01 and se-lect the non projective parsing for testing to obtainthe highest result in the development data after per-forming several experiments.
After that, the trainedmodel was used to test the data in Biomedical do-main.
The score (UA=82.04; LA=79.50) shows thatour method yields results above the average score(UA=76.42; LA=73.03).
In addition, it is officiallycoming in 4th place out of 12 teams and within 1.5%of the top systems.The good result of performing our model in an-other domain suggested that the PA learning seemssensitive to noise.
We hope that this problem issolved in future work.4 ConclusionsThis paper presents an online algorithm for depen-dency parsing problem which have tested on variouslanguage data in CONLL-2007 shared task.
The per-formance in English data is close to the Top3 score.We also perform our algorithm on the domain adap-tation task, in which we only focus on the training ofthe source data and select a suitable parameter usingthe development set.
The result is very good as itis close to the Top3 score of participating systems.Future work will also be focused on extending ourmethod to a version of using semi-supervised learn-ing that can efficiently be learnt by using labeled andunlabeled data.
We hope that the application of thePA algorithm to other NLP problems such as seman-tic parsing will be explored in future work.AcknowledgmentsWe would like to thank D. Yuret for his helps inchecking errors of my parser?s outputs.
We wouldlike to thank Vinh-Van Nguyen his helps during therevision process and Mary Ann Mooradian for cor-recting the paper.We would like to thank to anonymous review-ers for helpful discussions and comments on themanuscript.
Thank also to Sebastian Riedel forchecking the issues raised in the reviews.The work on this paper was supported by a Mon-bukagakusho 21st COE Program.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.1153Languages Unlabled Accuracy Labeled Accuracy NTeamsPA-I Average Top3 Top2 Top1 PA-I Average Top3 Top2 Top1Arabic 73.46 78.84 84.21 85.81 86.09 68.34 74.75 83.0 75.08 76.52 20Basque 69.11 75.15 81.13 81.93 81.13 58.16 68.06 75.49 75.73 76.92 20Catalan 88.12 87.98 93.12 93.34 93.40 83.23 79.85 87.90 88.16 88.70 20Chinese 84.05 81.98 87.91 88.88 88.94 79.77 76.59 83.51 83.84 84.69 21Czech 80.91 77.56 84.19 85.16 86.28 72.54 70.12 77.98 78.60 80.19 20English 88.01 82.67 89.87 90.13 90.63 86.73 80.95 88.41 89.01 89.61 23Greek 77.56 77.78 81.37 82.04 84.08 70.42 70.22 74.42 74.65 76.31 20Hungarian 78.13 76.34 82.49 83.51 83.55 68.12 71.49 78.09 79.53 80.27 21Italian 80.40 82.45 87.68 87.77 87.91 75.06 78.06 78.09 79.53 80.27 20Turkish 80.19 73.19 85.77 85.77 86.22 67.63 73.19 79.24 79.79 79.81 20Multilingual-average 79.99 71.13 85.62 85.71 86.55 72.52 65.77 79.90 80.28 80.32 23pchemtb-closed 82.04 76.42 83.08 83.38 83.42 79.50 73.03 80.22 80.40 81.06 8Table 5: Dependency accuracy in the CONLL-2007 shared task.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.Y.
Censor and S.A. Zenios.
1997.
Parallel optimization:theory, algorithms, and applications.
In Oxford Uni-versity Press.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
In Science Sinica.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proceedings of EMNLP.S.
Corston-Oliver, A. Aue, K. Duh, , and E. Ringger.2006.
Multilingual dependency parsing using bayespoint machines.
In Proceedings of HLT/NAACL.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
Journal ofMachine Learning Research, 3:951?991.K.
Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:581?585.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings ofCOLING 1996, pages 340?345.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human Language Technol-ogy Conference and the Annual Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT/NAACL).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.R.
McDonald, K. Cramer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
of the Human LanguageTechnology Conf.
and the Conf.
on Empirical Meth-ods in Natural Language Processing (HLT/EMNLP),pages 523?530.R.
McDonald, K. Crammer, and F. Pereira.
2006.
Multi-lingual dependency parsing with a two-stage discrim-inative parser.
In Conference on Natural LanguageLearning.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and1154R.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.
In Proc.
of the TenthConf.
on Computational Natural Language Learning(CoNLL), pages 221?225.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.B.
Taskar, D. Klein, M. Collins, D. Koller, and C.D.
Man-ning.
2004.
Max-margin parsing.
In proceedings ofEMNLP.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2005.
Support vector machine learning for interde-pendent and structured output spaces.
In proceedingsICML 2004.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.8th International Workshop on Parsing Technologies(IWPT), pages 195?206.1155
