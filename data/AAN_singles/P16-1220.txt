Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2326?2336,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsQuestion Answering on Freebase via Relation Extraction andTextual EvidenceKun Xu1, Siva Reddy2, Yansong Feng1,?, Songfang Huang3and Dongyan Zhao11Institute of Computer Science & Technology, Peking University, Beijing, China2School of Informatics, University of Edinburgh, UK3IBM China Research Lab, Beijing, China{xukun, fengyansong, zhaody}@pku.edu.cnsiva.reddy@ed.ac.ukhuangsf@cn.ibm.comAbstractExisting knowledge-based question an-swering systems often rely on small an-notated training data.
While shallow meth-ods like relation extraction are robust todata scarcity, they are less expressive thanthe deep meaning representation methodslike semantic parsing, thereby failing at an-swering questions involving multiple con-straints.
Here we alleviate this problem byempowering a relation extraction methodwith additional evidence from Wikipedia.We first present a neural network based re-lation extractor to retrieve the candidateanswers from Freebase, and then infer overWikipedia to validate these answers.
Ex-periments on the WebQuestions questionanswering dataset show that our methodachieves an F1of 53.3%, a substantial im-provement over the state-of-the-art.1 IntroductionSince the advent of large structured knowledgebases (KBs) like Freebase (Bollacker et al, 2008),YAGO (Suchanek et al, 2007) and DBpedia (Aueret al, 2007), answering natural language questionsusing those structured KBs, also known as KB-based question answering (or KB-QA), is attract-ing increasing research efforts from both naturallanguage processing and information retrieval com-munities.The state-of-the-art methods for this task canbe roughly categorized into two streams.
The firstis based on semantic parsing (Berant et al, 2013;Kwiatkowski et al, 2013), which typically learnsa grammar that can parse natural language to a so-phisticated meaning representation language.
Butsuch sophistication requires a lot of annotated train-ing examples that contains compositional struc-tures, a practically impossible solution for largeKBs such as Freebase.
Furthermore, mismatchesbetween grammar predicted structures and KBstructure is also a common problem (Kwiatkowskiet al, 2013; Berant and Liang, 2014; Reddy et al,2014).On the other hand, instead of building a for-mal meaning representation, information extractionmethods retrieve a set of candidate answers fromKB using relation extraction (Yao and Van Durme,2014; Yih et al, 2014; Yao, 2015; Bast and Hauss-mann, 2015) or distributed representations (Bordeset al, 2014; Dong et al, 2015).
Designing largetraining datasets for these methods is relatively easy(Yao and Van Durme, 2014; Bordes et al, 2015;Serban et al, 2016).
These methods are often goodat producing an answer irrespective of their correct-ness.
However, handling compositional questionsthat involve multiple entities and relations, still re-mains a challenge.
Consider the question whatmountain is the highest in north america.
Relationextraction methods typically answer with all themountains in North America because of the lack ofsophisticated representation for the mathematicalfunction highest.
To select the correct answer, onehas to retrieve all the heights of the mountains, andsort them in descending order, and then pick thefirst entry.
We propose a method based on textualevidence which can answer such questions withoutsolving the mathematic functions implicitly.Knowledge bases like Freebase capture realworld facts, and Web resources like Wikipedia pro-vide a large repository of sentences that validateor support these facts.
For example, a sentencein Wikipedia says, Denali (also known as MountMcKinley, its former official name) is the highestmountain peak in North America, with a summitelevation of 20,310 feet (6,190 m) above sea level.To answer our example question against a KB us-ing a relation extractor, we can use this sentence2326as external evidence, filter out wrong answers andpick the correct one.Using textual evidence not only mitigates rep-resentational issues in relation extraction, but alsoalleviates the data scarcity problem to some extent.Consider the question, who was queen isabella?smother.
Answering this question involves predict-ing two constraints hidden in the word mother.
Oneconstraint is that the answer should be the parentof Isabella, and the other is that the answer?s gen-der is female.
Such words with multiple latentconstraints have been a pain-in-the-neck for bothsemantic parsing and relation extraction, and re-quires larger training data (this phenomenon iscoined as sub-lexical compositionality by Wanget al (2015)).
Most systems are good at trigger-ing the parent constraint, but fail on the other, i.e.,the answer entity should be female.
Whereas thetextual evidence from Wikipedia, .
.
.
her motherwas Isabella of Barcelos .
.
.
, can act as a furtherconstraint to answer the question correctly.We present a novel method for question answer-ing which infers on both structured and unstruc-tured resources.
Our method consists of two mainsteps as outlined in ?2.
In the first step we extractanswers for a given question using a structured KB(here Freebase) by jointly performing entity link-ing and relation extraction (?3).
In the next stepwe validate these answers using an unstructuredresource (here Wikipedia) to prune out the wronganswers and select the correct ones (?4).
Our evalu-ation results on a benchmark dataset WebQuestionsshow that our method outperforms existing state-of-the-art models.
Details of our experimental setupand results are presented in ?5.
Our code, data andresults can be downloaded from https://github.com/syxu828/QuestionAnsweringOverFB.2 Our MethodFigure 1 gives an overview of our method for thequestion ?who did shaq first play for?.
We havetwo main steps: (1) inference on Freebase (KB-QAbox); and (2) further inference on Wikipedia (An-swer Refinement box).
Let us take a close look intostep 1.
Here we perform entity linking to identifya topic entity in the question and its possible Free-base entities.
We employ a relation extractor topredict the potential Freebase relations that couldexist between the entities in the question and theanswer entities.
Later we perform a joint inferencestep over the entity linking and relation extractionwho did shaq first play forKB-QAEntity Linking Relation ExtractionJoint Inferenceshaq: m.012xdfshaq: m.05n7bpshaq: m.06_ttvhsports.pro_athlete.teams..sports.sports_team_roster.teambasketball.player.statistics..basketball.player_stats.team?
?Answer Refinementm.012xdf  sports.pro_athlete.teams..sports.sports_team_roster.teamLos Angeles Lakers,Boston Celtics,Orlando Magic,Miami HeatFreebaseShaquille O'NealO'Neal signedas a free agent with the Los Angeles LakersShaquille O'NealO'Neal played forthe Boston Celtics in the 2010-11 season beforeretiringShaquille O'NealO'Neal was draftedin the 1992 NBA draftby the Orlando Magic with the first overall pickLos Angeles LakersBoston Celtics Orlando MagicO?Neal was drafted by the OrlandoMagic with the first overall pick inthe 1992 NBA draftO?Neal played for the Boston Celticsin the 2010-11 season before retiringO?Neal signed as a free agentwith the Los Angeles LakersRefinement Model+--Orlando MagicWikipedia Dump(with CoreNLP annotations)Figure 1: An illustration of our method to findanswers for the given question who did shaq firstplay for.results to find the best entity-relation configura-tion which will produce a list of candidate answerentities.
In the step 2, we refine these candidateanswers by applying an answer refinement modelwhich takes the Wikipedia page of the topic entityinto consideration to filter out the wrong answersand pick the correct ones.While the overview in Figure 1 works for ques-tions containing single Freebase relation, it alsoworks for questions involving multiple Freebaserelations.
Consider the question who plays anakinskywalker in star wars 1.
The actors who are the an-swers to this question should satisfy the followingconstraints: (1) the actor played anakin skywalker;and (2) the actor played in star wars 1.
Inspiredby Bao et al (2014), we design a dependency tree-based method to handle such multi-relational ques-tions.
We first decompose the original questioninto a set of sub-questions using syntactic patternswhich are listed in Appendix.
The final answer setof the original question is obtained by intersectingthe answer sets of all its sub-questions.
For the2327example question, the sub-questions are who playsanakin skywalker and who plays in star wars 1.These sub-questions are answered separately overFreebase and Wikipedia, and the intersection oftheir answers to these sub-questions is treated asthe final answer.3 Inference on FreebaseGiven a sub-question, we assume the questionword1that represents the answer has a distinct KBrelation r with an entity e found in the question,and predict a single KB triple (e, r, ?)
for each sub-question (here ?
stands for the answer entities).
TheQA problem is thus formulated as an informationextraction problem that involves two sub-tasks, i.e.,entity linking and relation extraction.
We first in-troduce these two components, and then present ajoint inference procedure which further boosts theoverall performance.3.1 Entity LinkingFor each question, we use hand-built sequencesof part-of-speech categories to identify all possi-ble named entity mention spans, e.g., the sequenceNN (shaq) may indicate an entity.
For each men-tion span, we use the entity linking tool S-MART2(Yang and Chang, 2015) to retrieve the top 5 en-tities from Freebase.
These entities are treated ascandidate entities that will eventually be disam-biguated in the joint inference step.
For a givenmention span, S-MART first retrieves all possi-ble entities of Freebase by surface matching, andthen ranks them using a statistical model, whichis trained on the frequency counts with which thesurface form occurs with the entity.3.2 Relation ExtractionWe now proceed to identify the relation betweenthe answer and the entity in the question.
Inspiredby the recent success of neural network models inKB question-answering (Yih et al, 2015; Dong etal., 2015), and the success of syntactic dependen-cies for relation extraction (Liu et al, 2015; Xuet al, 2015), we propose a Multi-Channel Convo-lutional Neural Network (MCCNN) which couldexploit both syntactic and sentential informationfor relation extraction.1who, when, what, where, how, which, why, whom, whose.2S-MART demo can be accessed athttp://msre2edemo.azurewebsites.net/[Who]did[shaq] firstplay forplaydid first play forWordRepresentationFeature Extractionmax(  ).ConvolutionFeature VectorOutputSoftmaxdobjnsubjdobjauxnsubj???
?KB relationsWeW1W2W3Figure 2: Overview of the multi-channel convolu-tional neural network for relation extraction.
Weisthe word embedding matrix, W1is the convolutionmatrix, W2is the activation matrix and W3is theclassification matrix.3.2.1 MCCNNs for Relation ClassificationIn MCCNN, we use two channels, one for syn-tactic information and the other for sentential in-formation.
The network structure is illustrated inFigure 2.
Convolution layer tackles an input ofvarying length returning a fixed length vector (weuse max pooling) for each channel.
These fixedlength vectors are concatenated and then fed into asoftmax classifier, the output dimension of whichis equal to the number of predefined relation types.The value of each dimension indicates the confi-dence score of the corresponding relation.Syntactic Features We use the shortest path be-tween an entity mention and the question word inthe dependency tree3as input to the first channel.Similar to Xu et al (2015), we treat the path asa concatenation of vectors of words, dependencyedge directions and dependency labels, and feedit to the convolution layer.
Note that, the entitymention and the question word are excluded fromthe dependency path so as to learn a more generalrelation representation in syntactic level.
As shownin Figure 2, the dependency path between who andshaq is?
dobj ?
play ?
nsubj ?.3We use Stanford CoreNLP dependency parser (Manninget al, 2014).2328Sentential Features This channel takes thewords in the sentence as input excluding the ques-tion word and the entity mention.
As illustrated inFigure 2, the vectors for did, first, play and for arefed into this channel.3.2.2 Objective Function and LearningThe model is learned using pairs of question andits corresponding gold relation from the trainingdata.
Given an input question x with an annotatedentity mention, the network outputs a vector o(x),where the entry ok(x) is the probability that thereexists the k-th relation between the entity and theexpected answer.
We denote t(x) ?
RK?1as thetarget distribution vector, in which the value forthe gold relation is set to 1, and others to 0.
Wecompute the cross entropy error between t(x) ando(x), and further define the objective function overthe training data as:J(?)
= ?
?xK?k=1tk(x) log ok(x) + ?||?||22where ?
represents the weights, and ?
the L2 reg-ularization parameters.
The weights ?
can be ef-ficiently computed via back-propagation throughnetwork structures.
To minimize J(?
), we applystochastic gradient descent (SGD) with AdaGrad(Duchi et al, 2011).3.3 Joint Entity Linking & Relation Extrac-tionA pipeline of entity linking and relation extractionmay suffer from error propagations.
As we know,entities and relations have strong selectional prefer-ences that certain entities do not appear with certainrelations and vice versa.
Locally optimized modelscould not exploit these implicit bi-directional pref-erences.
Therefore, we use a joint model to find aglobally optimal entity-relation assignment fromlocal predictions.
The key idea behind is to lever-age various clues from the two local models andthe KB to rank a correct entity-relation assignmenthigher than other combinations.
We describe thelearning procedure and the features below.3.3.1 LearningSuppose the pair (egold, rgold) represents thegold entity/relation pair for a question q. Wetake all our entity and relation predictions forq, create a list of entity and relation pairs{(e0, r0), (e1, r1), ..., (en, rn)} from q and rankthem using an SVM rank classifier (Joachims, 2006)which is trained to predict a rank for each pair.
Ide-ally higher rank indicates the prediction is closerto the gold prediction.
For training, SVM rankclassifier requires a ranked or scored list of entity-relation pairs as input.
We create the training datacontaining ranked input pairs as follows: if bothepred= egoldand rpred= rgold, we assign it witha score of 3.
If only the entity or relation equalsto the gold one (i.e., epred= egold, rpred6= rgoldor epred6= egold, rpred= rgold), we assign a scoreof 2 (encouraging partial overlap).
When both en-tity and relation assignments are wrong, we assigna score of 1.3.3.2 FeaturesFor a given entity-relation pair, we extract the fol-lowing features which are passed as an input vectorto the SVM ranker above:Entity Clues.
We use the score of the predictedentity returned by the entity linking system as afeature.
The number of word overlaps between theentity mention and entity?s Freebase name is alsoincluded as a feature.
In Freebase, most entitieshave a relation fb:description which describes theentity.
For instance, in the running example, shaqis linked to three potential entities m.06 ttvh (ShaqVs.
Television Show), m.05n7bp (Shaq Fu VideoGame) and m.012xdf (Shaquille O?Neal).
Interest-ingly, the word play only appears in the descriptionof Shaquille O?Neal and it occurs three times.
Wecount the content word overlap between the givenquestion and the entity?s description, and include itas a feature.Relation Clues.
The score of relation returned bythe MCCNNs is used as a feature.
Furthermore, weview each relation as a document which consists ofthe training questions that this relation is expressedin.
For a given question, we use the sum of the tf-idfscores of its words with respect to the relation as afeature.
A Freebase relation r is a concatenation ofa series of fragments r = r1.r2.r3.
For instance,the three fragments of people.person.parents arepeople, person and parents.
The first two fragmentsindicate the Freebase type of the subject of this re-lation, and the third fragment indicates the objecttype, in our case the answer type.
We use an indica-tor feature to denote if the surface form of the thirdfragment (here parents) appears in the question.Answer Clues.
The above two feature classes in-dicate local features.
From the entity-relation (e, r)2329pair, we create the query triple (e, r, ?)
to retrievethe answers, and further extract features from theanswers.
These features are non-local since we re-quire both e and r to retrieve the answer.
One suchfeature is using the co-occurrence of the answertype and the question word based on the intuitionthat question words often indicate the answer type,e.g., the question word when usually indicates theanswer type type.datetime.
Another feature is thenumber of answer entities retrieved.4 Inference on WikipediaWe use the best ranked entity-relation pair fromthe above step to retrieve candidate answers fromFreebase.
In this step, we validate these answersusing Wikipedia as our unstructured knowledgeresource where most statements in it are verifiedfor factuality by multiple people.Our refinement model is inspired by the intuitionof how people refine their answers.
If you asksomeone: who did shaq first play for, and givethem four candidate answers (Los Angeles Lakers,Boston Celtics, Orlando Magic and Miami Heat),as well as access to Wikipedia, that person mightfirst determine that the question is about ShaquilleO?Neal, then go to O?Neal ?s Wikipedia page, andsearch for the sentences that contain the candidateanswers as evidence.
By analyzing these sentences,one can figure out whether a candidate answer iscorrect or not.4.1 Finding Evidence from WikipediaAs mentioned above, we should first find theWikipedia page corresponding to the topic entity inthe given question.
We use Freebase API to con-vert Freebase entity to Wikipedia page.
We extractthe content from the Wikipedia page and processit with Wikifier (Cheng and Roth, 2013) which rec-ognizes Wikipedia entities, which can further belinked to Freebase entities using Freebase API.
Ad-ditionally we use Stanford CoreNLP (Manning etal., 2014) for tokenization and entity co-referenceresolution.
We search for the sentences containingthe candidate answer entities retrieved from Free-base.
For example, the Wikipedia page of O?Nealcontains a sentence ?O?Neal was drafted by the Or-lando Magic with the first overall pick in the 1992NBA draft?, which is taken into account by the re-finement model (our inference model on Wikipedia)to discriminate whether Orlando Magic is the an-swer for the given question.4.2 Refinement ModelWe treat the refinement process as a binary classi-fication task over the candidate answers, i.e., cor-rect (positive) and incorrect (negative) answer.
Weprepare the training data for the refinement modelas follows.
On the training dataset, we first in-fer on Freebase to retrieve the candidate answers.Then we use the annotated gold answers of thesequestions and Wikipedia to create the training data.Specifically, we treat the sentences that containcorrect/incorrect answers as positive/negative ex-amples for the refinement model.
We use LIBSVM(Chang and Lin, 2011) to learn the weights forclassification.Note that, in the Wikipedia page of the topic en-tity, we may collect more than one sentence thatcontain a candidate answer.
However, not all sen-tences are relevant, therefore we consider the can-didate answer as correct if at least there is onepositive evidence.
On the other hand, sometimes,we may not find any evidence for the candidateanswer.
In these cases, we fall back to the resultsof the KB-based approach.4.3 Lexical FeaturesRegarding the features used in LIBSVM, we use thefollowing lexical features extracted from the ques-tion and a Wikipedia sentence.
Formally, given aquestion q = <q1, ... qn> and an evidence sentences = <s1, ... sm>, we denote the tokens of q and sby qiand sj, respectively.
For each pair (q, s), weidentify a set of all possible token pairs (qi, sj),the occurrences of which are used as features.
Aslearning proceeds, we hope to learn a higher weightfor a feature like (first, drafted ) and a lower weightfor (first, played ).5 ExperimentsIn this section we introduce the experimental setup,the main results and detailed analysis of our system.5.1 Training and Evaluation DataWe use the WebQuestions (Berant et al, 2013)dataset, which contains 5,810 questions crawledvia Google Suggest service, with answers anno-tated on Amazon Mechanical Turk.
The questionsare split into training and test sets, which contain3,778 questions (65%) and 2,032 questions (35%),respectively.
We further split the training questionsinto 80%/20% for development.2330To train the MCCNNs and the joint inferencemodel, we need the gold standard relations of thequestions.
Since this dataset contains only question-answer pairs and annotated topic entities, insteadof relying on gold relations we rely on surrogategold relations which produce answers that have thehighest overlap with gold answers.
Specifically, fora given question, we first locate the topic entity ein the Freebase graph, then select 1-hop and 2-hoprelations connected to the topic entity as relationcandidates.
The 2-hop relations refer to the n-aryrelations of Freebase, i.e., first hop from the sub-ject to a mediator node, and the second from themediator to the object node.
For each relation can-didate r, we issue the query (e, r, ?)
to the KB,and label the relation that produces the answer withminimal F1-loss against the gold answer, as thesurrogate gold relation.
From the training set, wecollect 461 relations to train the MCCNN, and thetarget prediction during testing time is over theserelations.5.2 Experimental SettingsWe have 6 dependency tree patterns based on Baoet al (2014) to decompose the question into sub-questions (See Appendix).
We initialize the wordembeddings with Turian et al (2010)?s word rep-resentations with dimensions set to 50.
The hyperparameters in our model are tuned using the devel-opment set.
The window size of MCCNN is setto 3.
The sizes of the hidden layer 1 and the hiddenlayer 2 of the two MCCNN channels are set to 200and 100, respectively.
We use the Freebase versionof Berant et al (2013), containing 4M entities and5,323 relations.5.3 Results and DiscussionWe use the average question-wise F1as our eval-uation metric.4To give an idea of the impact ofdifferent configurations of our method, we comparethe following with existing methods.Structured.
This method involves inference onFreebase only.
First the entity linking (EL) systemis run to predict the topic entity.
Then we runthe relation extraction (RE) system and select thebest relation that can occur with the topic entity.We choose this entity-relation pair to predict theanswer.4We use the evaluation script available at http://www-nlp.stanford.edu/software/sempre.Method average F1Berant et al (2013) 35.7Yao and Van Durme (2014) 33.0Xu et al (2014) 39.1Berant and Liang (2014) 39.9Bao et al (2014) 37.5Bordes et al (2014) 39.2Dong et al (2015) 40.8Yao (2015) 44.3Bast and Haussmann (2015) 49.4Berant and Liang (2015) 49.7Reddy et al (2016) 50.3Yih et al (2015) 52.5This workStructured 44.1Structured + Joint 47.1Structured + Unstructured 47.0Structured + Joint + Unstructured 53.3Table 1: Results on the test set.Structured + Joint.
In this method instead ofthe above pipeline, we perform joint EL and RE asdescribed in ?3.3.Structured+Unstructured.
We use thepipelined EL and RE along with inferenceon Wikipedia as described in ?4.Structured + Joint + Unstructured.
This is ourmain model.
We perform inference on Freebaseusing joint EL and RE, and then inference onWikipedia to validate the results.
Specifically, wetreat the top two predictions of the joint inferencemodel as the candidate subject and relation pairs,and extract the corresponding answers from eachpair, take the union, and filter the answer set usingWikipedia.Table 1 summarizes the results on the test dataalong with the results from the literature.5We cansee that joint EL and RE performs better than thedefault pipelined approach, and outperforms mostsemantic parsing based models, except (Berant andLiang, 2015) which searches partial logical formsin strategic order by combining imitation learn-ing and agenda-based parsing.
In addition, infer-ence on unstructured data helps the default model.The joint EL and RE combined with inferenceon unstructured data further improves the defaultpipelined model by 9.2% (from 44.1% to 53.3%),and achieves a new state-of-the-art result beatingthe previous reported best result of Yih et al (2015)(with one-tailed t-test significance of p < 0.05).5We use development data for all our ablation experiments.Similar trends are observed on both development and testresults.2331Entity Linking Relation ExtractionAccuracy AccuracyIsolated Model 79.8 45.9Joint Inference 83.2 55.3Table 2: Impact of the joint inference on the devel-opment setMethod average F1Structured (syntactic) 38.1Structured (sentential) 38.7Structured (syntactic + sentential) 40.1Structured + Joint (syntactic) 43.6Structured + Joint (sentential) 44.1Structured + Joint (syntactic + sentential) 45.8Table 3: Impact of different MCCNN channels onthe development set.5.3.1 Impact of Joint EL & REFrom Table 1, we can see that the joint EL & REgives a performance boost of 3% (from 44.1 to47.1).
We also analyze the impact of joint inferenceon the individual components of EL & RE.We first evaluate the EL component using thegold entity annotations on the development set.
Asshown in Table 2, for 79.8% questions, our entitylinker can correctly find the gold standard topicentities.
The joint inference improves this result to83.2%, a 3.4% improvement.
Next we use the sur-rogate gold relations to evaluate the performanceof the RE component on the development set.
Asshown in Table 2, the relation prediction accuracyincreases by 9.4% (from 45.9% to 55.3%) whenusing the joint inference.5.3.2 Impact of the Syntactic and theSentential ChannelsTable 3 presents the results on the impact of individ-ual and joint channels on the end QA performance.When using a single-channel network, we tune theparameters of only one channel while switching offthe other channel.
As seen, the sentential featuresare found to be more important than syntactic fea-tures.
We attribute this to the short and noisy natureof WebQuestions questions due to which syntacticparser wrongly parses or the shortest dependencypath does not contain sufficient information to pre-dict a relation.
By using both the channels, we seefurther improvements than using any one of thechannels.Question & Answers1.
what is the largest nation in europeBefore: Kazakhstan, Turkey, Russia, ...After: Russia2.
which country in europe has the largest land areaBefore: Georgia, France, Russia, ...After: Russian Empire, Russia3.
what year did ray allen join the nbaBefore: 2007, 2003, 1996, 1993, 2012After: 19964. who is emma stone fatherBefore: Jeff Stone, Krista StoneAfter: Jeff Stone5.
where did john steinbeck go to collegeBefore: Salinas High School, Stanford UniversityAfter: Stanford UniversityTable 4: Example questions and corresponding pre-dicted answers before and after using unstructuredinference.
Before uses (Structured + Joint) model,and After uses Structured + Joint + Unstructuredmodel for prediction.
The colors blue and red indi-cate correct and wrong answers respectively.5.3.3 Impact of the Inference onUnstructured DataAs shown in Table 1, when structured inference isaugmented with the unstructured inference, we seean improvement of 2.9% (from 44.1% to 47.0%).And when Structured + Joint uses unstructuredinference, the performance boosts by 6.2% (from47.1% to 53.3%) achieving a new state-of-the-artresult.
For the latter, we manually analyzed thecases in which unstructured inference helps.
Ta-ble 4 lists some of these questions and the corre-sponding answers before and after the unstructuredinference.
We observed the unstructured infer-ence mainly helps for two classes of questions: (1)questions involving aggregation operations (Ques-tions 1-3); (2) questions involving sub-lexical com-positionally (Questions 4-5).
Questions 1 and 2contain the predicate largest an aggregation oper-ator.
A semantic parsing method should explicitlyhandle this predicate to trigger max(.)
operator.For Question 3, structured inference predicts theFreebase relation fb:teams..from retrieving all theyears in which Ray Allen has played basketball.Note that Ray Allen has joined Connecticut Uni-versity?s team in 1993 and NBA from 1996.
To an-swer this question a semantic parsing system wouldrequire a min(?)
operator along with an additionalconstraint that the year corresponds to the NBA ?sterm.
Interestingly, without having to explicitlymodel these complex predicates, the unstructuredinference helps in answering these questions moreaccurately.
Questions 4-5 involve sub-lexical com-2332positionally (Wang et al, 2015) predicates fatherand college.
For example in Question 5, the userqueries for the colleges that John Steinbeck at-tended.
However, Freebase defines the relationfb:education..institution to describe a person?s ed-ucational information without discriminating thespecific periods such as high school or college.
In-ference using unstructured data helps in alleviatingthese representational issues.5.3.4 Error analysisWe analyze the errors of Structured + Joint + Un-structured model.
Around 15% of the errors arecaused by incorrect entity linking, and around 50%of the errors are due to incorrect relation predic-tions.
The errors in relation extraction are dueto (i) insufficient context, e.g., in what is duncanbannatyne, neither the dependency path nor sen-tential context provides enough evidence for theMCCNN model; (ii) unbalanced distribution of re-lations (3022 training examples for 461 relations)heavily influences the performance of MCCNNmodel towards frequently seen relations.
The re-maining errors are the failure of unstructured infer-ence due to insufficient evidence in Wikipedia ormisclassification.Entity Linking.
In the entity linking component,we had handcrafted POS tag patterns to identifyentity mentions, e.g., DT-JJ-NN (noun phrase), NN-IN-NN (prepositional phrase).
These patterns aredesigned to have high recall.
Around 80% of entitylinking errors are due to incorrect entity predictioneven when the correct mention span was found.Question Decomposition.
Around 136 ques-tions (15%) of dev data contains compositionalquestions, leading to 292 sub-questions (around2.1 subquestions for a compositional question).Since our question decomposition component isbased on manual rules, one question of interestis how these rules perform on other datasets.
Byhuman evaluation, we found these rules achieves95% on a more general but complex QA datasetQALD-56.5.3.5 LimitationsWhile our unstructured inference alleviates repre-sentational issues to some extent, we still fail atmodeling compositional questions such as who isthe mother of the father of prince william involving6http://qald.sebastianwalter.org/index.php?q=5multi-hop relations and the inter alia.
Our currentassumption that unstructured data could provide ev-idence for questions may work only for frequentlytyped queries or for popular domains like movies,politics and geography.
We note these limitationsand hope our result will foster further research inthis area.6 Related WorkOver time, the QA task has evolved into two mainstreams ?
QA on unstructured data, and QA onstructured data.
TREC QA evaluations (Voorheesand Tice, 1999) were a major boost to unstruc-tured QA leading to richer datasets and sophisti-cated methods (Wang et al, 2007; Heilman andSmith, 2010; Yao et al, 2013; Yih et al, 2013;Yu et al, 2014; Yang et al, 2015; Hermann etal., 2015).
While initial progress on structuredQA started with small toy domains like GeoQuery(Zelle and Mooney, 1996), recent focus has shiftedto large scale structured KBs like Freebase, DB-Pedia (Unger et al, 2012; Cai and Yates, 2013;Berant et al, 2013; Kwiatkowski et al, 2013; Xuet al, 2014), and on noisy KBs (Banko et al, 2007;Carlson et al, 2010; Krishnamurthy and Mitchell,2012; Fader et al, 2013; Parikh et al, 2015).
Anexciting development in structured QA is to exploitmultiple KBs (with different schemas) at the sametime to answer questions jointly (Yahya et al, 2012;Fader et al, 2014; Zhang et al, 2016).
QALD tasksand linked data initiatives are contributing to thistrend.Our model combines the best of both worldsby inferring over structured and unstructured data.Though earlier methods exploited unstructured datafor KB-QA (Krishnamurthy and Mitchell, 2012;Berant et al, 2013; Yao and Van Durme, 2014;Reddy et al, 2014; Yih et al, 2015), these methodsdo not rely on unstructured data at test time.
Ourwork is closely related to Joshi et al (2014) whoaim to answer noisy telegraphic queries using bothstructured and unstructured data.
Their work is lim-ited in answering single relation queries.
Our workalso has similarities to Sun et al (2015) who doesquestion answering on unstructured data but enrichit with Freebase, a reversal of our pipeline.
Otherline of very recent related work include Yahya etal.
(2016) and Savenkov and Agichtein (2016).Our work also intersects with relation extrac-tion methods.
While these methods aim to predicta relation between two entities in order to pop-2333ulate KBs (Mintz et al, 2009; Hoffmann et al,2011; Riedel et al, 2013), we work with sentencelevel relation extraction for question answering.
Kr-ishnamurthy and Mitchell (2012) and Fader et al(2014) adopt open relation extraction methods forQA but they require hand-coded grammar for pars-ing queries.
Closest to our extraction method isYao and Van Durme (2014) and Yao (2015) whoalso uses sentence level relation extraction for QA.Unlike them, we can predict multiple relations perquestion, and our MCCNN architecture is more ro-bust to unseen contexts compared to their logisticregression models.Dong et al (2015) were the first to use MCCNNfor question answering.
Yet our approach is verydifferent in spirit to theirs.
Dong et al aim tomaximize the similarity between the distributedrepresentation of a question and its answer entities,whereas our network aims to predict Freebase re-lations.
Our search space is several times smallerthan theirs since we do not require potential an-swer entities beforehand (the number of relationsis much smaller than the number of entities in Free-base).
In addition, our method can explicitly handlecompositional questions involving multiple rela-tions, whereas Dong et al learn latent representa-tion of relation joins which is difficult to compre-hend.
Moreover, we outperform their method by7 points even without unstructured inference.7 Conclusion and Future WorkWe have presented a method that could infer bothon structured and unstructured data to answer natu-ral language questions.
Our experiments reveal thatunstructured inference helps in mitigating represen-tational issues in structured inference.
We havealso introduced a relation extraction method usingMCCNN which is capable of exploiting syntax inaddition to sentential features.
Our main modelwhich uses joint entity linking and relation extrac-tion along with unstructured inference achieves thestate-of-the-art results on WebQuestions dataset.
Apotential application of our method is to improveKB-question answering using the documents re-trieved by a search engine.Since we pipeline structured inference first andthen unstructured inference, our method is limitedby the coverage of Freebase.
Our future work in-volves exploring other alternatives such as treatingstructured and unstructured data as two indepen-dent resources in order to overcome the knowledgegaps in either of the two resources.AcknowledgmentsWe would like to thank Weiwei Sun, Liwei Chen,and the anonymous reviewers for their helpful feed-back.
This work is supported by National HighTechnology R&D Program of China (Grant No.2015AA015403, 2014AA015102), Natural Sci-ence Foundation of China (Grant No.
61202233,61272344, 61370055) and the joint project withIBM Research.
For any correspondence, pleasecontact Yansong Feng.AppendixThe syntax-based patterns for question decomposi-tion are shown in Figure 3.
The first four patternsare designed to extract sub-questions from simplequestions, while the latter two are designed forcomplex questions involving clauses.verbsubj obj1 prepobj2verbsubjobj1prep*obj2(a)(b)andverbsubjprep*objk(c)prep*obj1?
?verbsubjprep*obj2(d)prep*obj1and verbWDTsubj(e)verbobj1WDTsubj(f)verbprep*obj1Figure 3: Syntax-based patterns for question de-composition.ReferencesSren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary G. Ives.2007.
Dbpedia: A nucleus for a web of open data.In ISWC/ASWC.Michele Banko, Michael J Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction for the web.
In IJCAI.Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In ACL.2334Hannah Bast and Elmar Haussmann.
2015.
More ac-curate question answering on freebase.
In CIKM.Jonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In ACL.Jonathan Berant and Percy Liang.
2015.
Imitationlearning of agenda-based semantic parsers.
Transac-tions of the Association for Computational Linguis-tics, 3:545?558.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In EMNLP.Kurt D. Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In SIGMOD.Antoine Bordes, Sumit Chopra, and Jason Weston.2014.
Question answering with subgraph embed-dings.
In EMNLP.Antoine Bordes, Nicolas Usunier, Sumit Chopra, andJason Weston.
2015.
Large-scale simple ques-tion answering with memory networks.
CoRR,abs/1506.02075.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In ACL.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In AAAI.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTIST, 2(3):27.Xiao Cheng and Dan Roth.
2013.
Relational inferencefor wikification.
In ACL.Li Dong, Furu Wei, Ming Zhou, and Ke Xu.2015.
Question answering over freebase with multi-column convolutional neural networks.
In ACL-IJCNLP.John C. Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159.Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In ACL.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In SIGKDD.Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In NAACL.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural Informa-tion Processing Systems.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In ACL.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In SIGKDD.Mandar Joshi, Uma Sawant, and Soumen Chakrabarti.2014.
Knowledge graph and corpus driven segmen-tation and answer inference for telegraphic entity-seeking queries.
In EMNLP.Jayant Krishnamurthy and Tom M Mitchell.
2012.Weakly supervised training of semantic parsers.
InEMNLP-CoNLL.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, andLuke S. Zettlemoyer.
2013.
Scaling semanticparsers with on-the-fly ontology matching.
InEMNLP.Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,and Houfeng WANG.
2015.
A dependency-basedneural network for relation classification.
In ACL.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In ACL System Demon-strations.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In ACL.Ankur P. Parikh, Hoifung Poon, and KristinaToutanova.
2015.
Grounded semantic parsing forcomplex knowledge extraction.
In NAACL.Siva Reddy, Mirella Lapata, and Mark Steedman.
2014.Large-scale semantic parsing without question-answer pairs.
Transactions of the Association ofComputational Linguistics, pages 377?392.Siva Reddy, Oscar T?ackstr?om, Michael Collins, TomKwiatkowski, Dipanjan Das, Mark Steedman, andMirella Lapata.
2016.
Transforming DependencyStructures to Logical Forms for Semantic Parsing.Transactions of the Association for ComputationalLinguistics, 4.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InNAACL.Denis Savenkov and Eugene Agichtein.
2016.
Whena knowledge base is not enough: Question answer-ing over knowledge bases with external text data.
InSIGIR.2335Iulian Vlad Serban, Alberto Garc?
?a-Dur?an, C?aglarG?ulc?ehre, Sungjin Ahn, Sarath Chandar, Aaron C.Courville, and Yoshua Bengio.
2016.
Generat-ing factoid questions with recurrent neural networks:The 30m factoid question-answer corpus.
In ACL.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In WWW.Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,Jingjing Liu, and Ming-Wei Chang.
2015.
Open do-main question answering via semantic enrichment.In WWW.Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-gio.
2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
In ACL2010, Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pages 384?394.Christina Unger, Lorenz B?uhmann, Jens Lehmann,Axel-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-based questionanswering over rdf data.
In WWW.Ellen M Voorhees and Dawn M. Tice.
1999.
The trec-8question answering track report.
In TREC.Mengqiu Wang, Noah A Smith, and Teruko Mita-mura.
2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
In EMNLP-CoNLL.Yushi Wang, Jonathan Berant, and Percy Liang.
2015.Building a semantic parser overnight.
In ACL.Kun Xu, Sheng Zhang, Yansong Feng, and DongyanZhao.
2014.
Answering natural language ques-tions via phrasal semantic parsing.
In Natural Lan-guage Processing and Chinese Computing - ThirdCCF Conference, NLPCC 2014, Shenzhen, China,December 5-9, 2014.
Proceedings, pages 333?344.Kun Xu, Yansong Feng, Songfang Huang, andDongyan Zhao.
2015.
Semantic relation classifica-tion via convolutional neural networks with simplenegative sampling.
In EMNLP.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural language questions for theweb of data.
In EMNLP.Mohamed Yahya, Denilson Barbosa, Klaus Berberich,Qiuyue Wang, and Gerhard Weikum.
2016.
Rela-tionship queries on extended knowledge graphs.
InWSDM.Yi Yang and Ming-Wei Chang.
2015.
S-mart: Noveltree-based structured learning algorithms applied totweet entity linking.
In ACL-IJNLP.Yi Yang, Wen-tau Yih, and Christopher Meek.
2015.Wikiqa: A challenge dataset for open-domain ques-tion answering.
In EMNLP.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In ACL.Xuchen Yao, Benjamin Van Durme, and Peter Clark.2013.
Answer extraction as sequence tagging withtree edit distance.
In NAACL.Xuchen Yao.
2015.
Lean question answering over free-base from scratch.
In NAACL.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
In ACL.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
In ACL.Wen-tau Yih, Ming-Wei Chang, Xiaodong He, andJianfeng Gao.
2015.
Semantic parsing via stagedquery graph generation: Question answering withknowledge base.
In ACL-IJCNLP.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep learning for answersentence selection.
arXiv preprint arXiv:1412.1632.John M Zelle and Raymond J Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In AAAI.Yuanzhe Zhang, Shizhu He, Kang Liu, and Jun Zhao.2016.
A joint model for question answering overmultiple knowledge bases.
In AAAI.2336
