Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 725?734,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsIncorporating Word Correlation Knowledge into Topic ModelingPengtao XieSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USApengtaox@cs.cmu.eduDiyi YangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdiyiy@cs.cmu.eduEric P. XingSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAepxing@cs.cmu.eduAbstractThis paper studies how to incorporate the ex-ternal word correlation knowledge to improvethe coherence of topic modeling.
Existingtopic models assume words are generated in-dependently and lack the mechanism to utilizethe rich similarity relationships among wordsto learn coherent topics.
To solve this prob-lem, we build a Markov Random Field (MRF)regularized Latent Dirichlet Allocation (LDA)model, which defines a MRF on the latenttopic layer of LDA to encourage words la-beled as similar to share the same topic label.Under our model, the topic assignment of eachword is not independent, but rather affected bythe topic labels of its correlated words.
Simi-lar words have better chance to be put into thesame topic due to the regularization of MRF,hence the coherence of topics can be boosted.In addition, our model can accommodate thesubtlety that whether two words are similardepends on which topic they appear in, whichallows word with multiple senses to be put intodifferent topics properly.
We derive a vari-ational inference method to infer the poste-rior probabilities and learn model parametersand present techniques to deal with the hard-to-compute partition function in MRF.
Exper-iments on two datasets demonstrate the effec-tiveness of our model.1 IntroductionProbabilistic topic models (PTM), such as proba-bilistic latent semantic indexing(PLSI) (Hofmann,1999) and latent Dirichlet alocation(LDA) (Blei etal., 2003) have shown great success in documentsmodeling and analysis.
Topic models posit doc-ument collection exhibits multiple latent semantictopics where each topic is represented as a multino-mial distribution over a given vocabulary and eachdocument is a mixture of hidden topics.
To generatea document d, PTM first samples a topic proportionvector, then for each wordw in d, samples a topic in-dicator z and generatesw from the topic-word multi-nomial corresponding to topic z.A key limitation of the existing PTMs is thatwords are assumed to be uncorrelated and generatedindependently.
The topic assignment for each wordis irrelevant to all other words.
While this assump-tion facilitates computational efficiency, it loses therich correlations between words.
In many applica-tions, users have external knowledge regarding wordcorrelation, which can be taken into account to im-prove the semantic coherence of topic modeling.
Forexample, WordNet (Miller, 1995a) presents a largeamount of synonym relationships between words,Wikipedia1provides a knowledge graph by linkingcorrelated concepts together and named entity rec-ognizer identifies the categories of entity mentions.All of these external knowledge can be leveraged tolearn more coherent topics if we can design a mech-anism to encourage similar words, correlated con-cepts, entities of the same category to be assigned tothe same topic.Many approaches (Andrzejewski et al, 2009; Pet-terson et al, 2010; Newman et al, 2011) have at-tempted to solve this problem by enforcing hard andtopic-independent rules that similar words shouldhave similar probabilities in all topics, which is1https://www.wikipedia.org/725questionable in that two words with similar rep-resentativeness of one topic are not necessarily ofequal importance for another topic.
For example,in the fruit topic, the words apple and orange havesimilar representativeness, while in an IT companytopic, apple has much higher importance than or-ange.
As another example, church and bible aresimilarly relevant to a religion topic, whereas theirrelevance to an architecture topic are vastly differ-ent.
Exiting approaches are unable to differentiatethe subtleties of word sense across topics and wouldfalsely put irrelevant words into the same topic.
Forinstance, since orange and microsoft are both la-beled as similar to apple and are required to havesimilar probabilities in all topics as apple has, in theend, they will be unreasonably allocated to the sametopic.The existing approaches fail to properly use theword correlation knowledge, which is usually a listof word pairs labeled as similar.
The similarity iscomputed based on statistics such as co-occurrencewhich are unable to accommodate the subtlety thatwhether two words labeled as similar are truly sim-ilar depends on which topic they appear in, as ex-plained by the aforementioned examples.
Ideally,the knowledge would be word A and B are similarunder topic C. However, in reality, we only knowtwo words are similar, but not under which topic.
Inthis paper, we aim to abridge this gap.
Gaining in-sights from (Verbeek and Triggs, 2007; Zhao et al,2010; Zhu and Xing, 2010), we design a MarkovRandom Field regularized LDA model (MRF-LDA)which utilizes the external knowledge in a soft andtopic-dependent manner to improve the coherence oftopic modeling.
We define a MRF on the latent topiclayer of LDA to encode word correlations.
Within adocument, if two words are labeled as similar ac-cording to the external knowledge, their latent topicnodes will be connected by an undirected edge anda binary potential function is defined to encouragethem to share the same topic label.
This mecha-nism gives correlated words a better chance to beput into the same topic, thereby, improves the co-herence of the learned topics.
Our model providesa mechanism to automatically decide under whichtopic, two words labeled as similar are truly simi-lar.
We encourage words labeled as similar to sharethe same topic label, but do not specify which topiclabel they should share, and leave this to be de-cided by data.
In the above mentioned apple, or-ange, microsoft example, we encourage apple andorange to share the same topic label A and try topush apple and microsoft to the same topic B. ButA and B are not necessarily the same and they willbe inferred according to the fitness of data.
Dif-ferent from the existing approaches which directlyuse the word similarities to control the topic-worddistributions in a hard and topic-independent way,our method imposes constraints on the latent topiclayer by which the topic-word multinomials are in-fluenced indirectly and softly and are topic-aware.The rest of the paper is organized as follows.
InSection 2, we introduce related work.
In Section 3,we propose the MRF-LDA model and present thevariational inference method.
Section 4 gives exper-imental results.
Section 5 concludes the paper.2 Related WorkDifferent from purely unsupervised topics modelsthat often result in incoherent topics, knowledgebased topic models enable us to take prior knowl-edge into account to produce more meaningful top-ics.
Various approaches have been proposed to ex-ploit the correlations and similarities among wordsto improve topic modeling instead of purely rely-ing on how often words co-occur in different con-texts (Heinrich, 2009).
For instance, Andrzejewskiet al (2009) imposes Dirichlet Forest prior over thetopic-word multinomials to encode the Must-Linksand Cannot-Links between words.
Words withMust-Links are encouraged to have similar proba-bilities within all topics while those with Cannot-Links are disallowed to simultaneously have largeprobabilities within any topic.
Similarly, Pettersonet al (2010) adopted word information as featuresrather than as explicit constraints and defined a priorover the topic-word multinomials such that similarwords share similar topic distributions.
Newmanet al (2011) proposed a quadratic regularizer anda convolved Dirichlet regularizer over topic-wordmultinomials to incorporate the correlation betweenwords.
All of these methods directly incorporatethe word correlation knowledge into the topic-worddistributions in a hard and topic-independent way,which ignore the fact that whether two words are726correlated depends on which topic they appear in.There are several works utilizing knowledge withmore complex structure to improve topic modeling.Boyd-Graber et al (2007) incorporate the synsetstructure in WordNet (Miller, 1995b) into LDA forword sense disambiguation, where each topic is arandom process defined over the synsets.
Hu et al(2011) proposed interactive topic modeling, whichallows users to iteratively refine the discovered top-ics by adding constraints such as certain set of wordsmust appear together in the same topic.
Andrze-jewski et al (2011) proposed a general frameworkwhich uses first order logic to encode various do-main knowledge regarding documents, topics andside information into LDA.
The vast generality andexpressivity of this model makes its inference to bevery hard.
Chen et al (2013) proposed a topic modelto model multi-domain knowledge, where each doc-ument is an admixture of latent topics and each topicis a probability distribution over domain knowledge.Jagarlamudi et al (2012) proposed to guide topicmodeling by setting a set of seed words in the begin-ning that user believes could represent certain topics.While these knowledge are rich in structure, they arehard to acquire in the real world applications.
In thispaper, we focus on pairwise word correlation knowl-edge which are widely attainable in many scenarios.In the domain of computer vision, the idea ofusing MRF to enforce topical coherence betweenneighboring patches or superpixels has been ex-ploited by several works.
Verbeek and Triggs (2007)proposed Markov field aspect model where each im-age patch is modeled using PLSA (Hofmann, 1999)and a Potts model is imposed on the hidden topiclayer to enforce spatial coherence.
Zhao et al (2010)proposed topic random field model where each su-perpixel is modeled using a combination of LDAand mixture of Gaussian model and a Potts model isdefined on the topic layer to encourage neighboringsuperpixels to share the same topic.
Similarly, Zhuand Xing (2010) proposed a conditional topic ran-dom field to incorporate features about words anddocuments into topic modeling.
In their model, theMRF is restricted to be a linear chain, which canonly capture the dependencies between neighboringwords and is unable to incorporate long range wordcorrelations.
Different from these works, the MRFin our model is not restricted to Potts or chain struc-ture.
Instead, its structure is decided by the wordcorrelation knowledge and can be arbitrary.3 Markov Random Field RegularizedLatent Dirichlet AllocationIn this section, we present the MRF-LDA model andthe variational inference technique.3.1 MRF-LDAWe propose the MRF-LDA model to incorporateword similarities into topic modeling.
As shownin Figure 1, MRF-LDA extends the standard LDAmodel by imposing a Markov Random Field on thelatent topic layer.
Similar to LDA, we assume a doc-ument possesses a topic proportion vector ?
sampledfrom a Dirichlet distribution.
Each topic ?kis amultinomial distribution over words.
Each word whas a topic label z indicating which topic w belongsto.In many scenarios, we have access to exter-nal knowledge regarding the correlations betweenwords, such as apple and orange are similar, churchand bible are semantically related.
These similarityrelationships among words can be leveraged to im-prove the coherence of learned topics.
To do this,we define a Markov Random Field over the latenttopic layer.
Given a document d containingN words{wi}Ni=1, we examine each word pair (wi, wj).
Ifthey are correlated according to the external knowl-edge, we create an undirected edge between theirtopic labels (zi, zj).
In the end, we obtain an undi-rected graph G where the nodes are latent topic la-bels {zi}Ni=1and edges connect topic labels of cor-related words.
In the example shown in Figure 1, Gcontains five nodes z1, z2, z3, z4, z5and four edgesconnecting (z1, z3), (z2, z5), (z3, z4), (z3, z5).Given the undirected graph G, we can turn it intoa Markov Random Field by defining unary poten-tials over nodes and binary potentials over edges.We define the unary potential for zias p(zi|?
),which is a multinomial distribution parameterizedby ?.
In standard LDA, this is how a topic is sampledfrom the topic proportion vector.
For binary poten-tial, with the goal to encourage similar words to havesimilar topic assignments, we define the edge po-tential between (zi, zj) as exp{I(zi= zj)}, whereI(?)
is the indicator function.
This potential func-727tion yields a larger value if the two topic labels arethe same and a smaller value if the two topic labelsare different.
Hence, it encourages similar wordsto be assigned to the same topic.
Under the MRFmodel, the joint probability of all topic assignmentsz = {zi}Ni=1can be written asp(z|?, ?)
=1A(?,?)N?i=1p(zi|?)exp{??
(m,n)?PI(zm= zn)}(1)where P denotes the edges in G and A(?, ?)
is thepartition functionA(?)
=?zN?i=1p(zi|?)
exp{??
(m,n)?PI(zm= zn)}(2)We introduce ?
?
0 as a trade-off parameter be-tween unary potential and binary potential.
In stan-dard LDA, topic label zionly depends on topic pro-portion vector ?.
In MRF-LDA, zinot only dependson ?, but also depends on the topic labels of similarwords.
If ?
is set to zero, the correlation betweenwords is ignored and MRF-LDA is reduced to LDA.Given the topic labels, the generation of words is thesame as LDA.
wiis generated from the topic-wordsmultinomial distribution ?zicorresponding to zi.In MRF-LDA, the generative process of a docu-ment is summarized as follows:?
Draw a topic proportion vector ?
?
Dir(?)?
Draw topic labels z for all words from the jointdistribution defined in Eq.(1)?
For each word wi, drawn wi?
multi(?zi)Accordingly, the joint distribution of ?, z and wcan be written asp(?, z,w|?,?, ?)
= p(?|?
)p(z|?, ?)?Ni=1p(wi|zi,?
)(3)3.2 Variational Inference and ParameterLearningThe key inference problem we need to solve inMRF-LDA is to compute the posterior p(?, z|w) oflatent variables ?, z given observed data w. As inLDA (Blei et al, 2003), exact computation is in-tractable.
What makes things even challenging in?
?3z 3wK?
4z 4w2z 2w 5z 5w1z1wFigure 1: Markov Random Field Regularized LatentDirichlet Allocation ModelMRF-LDA is that, an undirected MRF is coupledwith a directed LDA and the hard-to-compute parti-tion function of MRF makes the posterior inferenceand parameter learning very difficult.
To solve thisproblem, we resort to variational inference (Wain-wright and Jordan, 2008), which uses a easy-to-handle variational distribution to approximate thetrue posterior of latent variables.
To deal with thepartition function in MRF, we seek lower bound ofthe variational lower bound to achieve tractability.We introduce a variational distributionq(?, z) = q(?|?
)N?i=1q(zi|?i) (4)where Dirichlet parameter ?
and multinomial pa-rameters {?i}Ni=1are free variational parameters.Using Jensen?s inequality (Wainwright and Jordan,2008), we can obtain a variational lower boundL = Eq[log p(?|?)]
+ Eq[log p(z|?, ?)]+Eq[logN?i=1p(wi|zi,?)]?
Eq[log q(?|?
)]?Eq[logN?i=1q(zi|?i)](5)728in which Eq[log p(z|?, ?)]
can be expanded asEq[log p(z|?, ?
)]= ?Eq[logA(?, ?)]
+ ??(m,n)?PK?k=1?mk?nk+N?i=1K?k=1?ik(?(?k)??
(K?j=1?j))(6)The item Eq[logA(?, ?)]
involves the hard-to-compute partition function, which has no analyticalexpressions.
We discuss how to deal with it in thesequel.
With Taylor expansion, we can obtain anupper bound of Eq[logA(?, ?
)]Eq[logA(?, ?)]
?
c?1Eq[A(?, ?)]?
1 + log c (7)where c ?
0 is a new variational parameter.Eq[A(?, ?)]
can be further upper bounded asEq[logA(?, ?)]
?
exp{?(m,n)?P?}?n1,n2,???
,nKEq[K?k=1?nk](8)where nkdenotes the number of words assignedwith topic label k andK?k=1nk= N .
We furtherbound?n1,n2,???
,nKEq[K?k=1?nk] as follows?n1,n2,???
,nKEq[K?k=1?nk]=?n1,n2,???
,nK?(K?k=1?k)K?k=1?(?k)?K?k=1?nk+?k?1d?=?n1,n2,???
,nK?(K?k=1?k)K?k=1?(?k)K?k=1?(nk+?k)?(K?k=1nk+?k)=?n1,n2,???
,nK?Kk=1(?k)nk(K?k=1?k)N??n1,n2,???
,nKK?k=1(nk)!(N)!
(9)where (a)ndenotes the Pochhammer symbol, whichis defined as (a)n= a(a + 1) .
.
.
(a + n ?
1) and?n1,n2,???
,nK?Kk=1(nk)!
(N)!is a constant.
Setting c =c/?n1,n2,???
,nK?Kk=1(nk)!
(N)!, we getEq[logA(?, ?)]
?
c?1exp{?(i,j)?P?}
?
1 + log c(10)Given this upper bound, we can obtain a lowerbound of the variational lower bound defined inEq.(5).
Variational parameters and model parame-ters can be learned by maximizing the lower boundusing iterative EM algorithm.
In E-step, we fixthe model parameters and compute the variationalparameters by setting the derivatives of the lowerbound w.r.t the variational parameters to zero?k= ?k+N?i=1?ik, c = exp{?(m,n)?P?}
(11)?ik?
exp{?(?k)??
(K?j=1?j) + ?
?j?N (i)?jk+V?v=1wivlog ?kv}(12)In Eq.
(12), N (i) denotes the words that are labeledto be similar to i.
As can be seen from this equa-tion, the probability ?ikthat word i is assigned totopic k depends on the probability ?jkof i?s cor-related words j.
This explains how our model canincorporate word correlations in topic assignments.In M-step, we fix the variational parameters and up-date the model parameters by maximizing the lowerbound defined on the set of documents {wd}Dd=1?kv?D?d=1Nd?i=1?d,i,kwd,i,v(13)?
=1|P |logD?d=1?
(m,n)?PdK?k=1?d,m,k?d,n,k|P |D?d=11cd(14)4 ExperimentIn this section, we corroborate the effectiveness ofour model by comparing it with three baseline meth-ods on two datasets.729dataset 20-Newsgroups NIPS# documents 18846 1500# words 40343 12419Table 1: Dataset Statistics4.1 Experiment Setup?
Dataset: We use two datasets in the exper-iments: 20-Newsgroups2and NIPS3.
Theirstatistics are summarized in Table 1.?
External Knowledge: We extract word cor-relation knowledge from Web Eigenwords4,where each word has a real-valued vector cap-turing the semantic meaning of this word basedon distributional similarity.
Two words are re-garded as correlated if their representation vec-tors are similar enough.
It is worth mentioningthat, other sources of external word correlationknowledge, such as Word2Vec (Mikolov et al,2013) and Glove (Pennington et al, 2014), canbe readily incorporated into MRF-LDA.?
Baselines: We compare our model with threebaseline methods: LDA (Blei et al, 2003), DF-LDA (Andrzejewski et al, 2009) and Quad-LDA (Newman et al, 2011).
LDA is the mostwidely used topic model, but it is unable to in-corporate external knowledge.
DF-LDA andQuad-LDA are two models designed to incor-porate word correlation to improve topic mod-eling.
DF-LDA puts a Dirichlet Forest priorover the topic-word multinomials to encode theMust-Links and Cannot-Links between words.Quad-LDA regularizes the topic-word distri-butions with a structured prior to incorporateword relation.?
Parameter Settings: For all methods, we learn100 topics.
LDA parameters are set to theirdefault settings in (Andrzejewski et al, 2009).For DF-LDA, we set its parameters as ?
= 1,?
= 0.01 and ?
= 100.
The Must/Cannot linksbetween words are generated based on the co-sine similarity of words?
vector representations2http://qwone.com/ jason/20Newsgroups/3http://archive.ics.uci.edu/ml/datasets/Bag+of+Words4http://www.cis.upenn.edu/ ungar/eigenwords/in Web Eigenwords.
Word pairs with similar-ity higher than 0.99 are set as Must-Links, andpairs with similarity lower than 0.1 are put intoCannot-Link set.
For Quad-LDA, ?
is set as0.01; ?
is defined as0.05?ND?T, where N is the to-tal occurrences of all words in all documents, Dis the number of documents and T is topic num-ber.
For MRF-LDA, word pairs with similarityhigher than 0.99 are labeled as correlated.4.2 ResultsWe compare our model with the baseline methodsboth qualitatively and quantitatively.4.2.1 Qualitative EvaluationTable 2 shows some exemplar topics learned bythe four methods on the 20-Newsgroups dataset.Each topic is visualized by the top ten words.
Wordsthat are noisy and lack representativeness are high-lighted with bold font.
Topic 1 is about crime andguns.
Topic 2 is about sex.
Topic 3 is about sportsand topic 4 is about health insurance.
As can be seenfrom the table, our method MRF-LDA can learnmore coherent topics with fewer noisy and meaning-less words than the baseline methods.
LDA lacks themechanism to incorporate word correlation knowl-edge and generates the words independently.
Thesimilarity relationships among words cannot be uti-lized to imporve the coherence of topic modeling.Consequently, noise words such as will, year, usedwhich cannot effectively represent a topic, show updue to their high frequency.
DF-LDA and Quad-LDA proposed to use word correlations to enhancethe coherence of learned topics.
However, they im-properly enforce words labeled as similar to havesimilar probabilities in all topics, which violates thefact that whether two words are similar depend onwhich topic they appear in.
As a consequence, thetopics extracted by these two methods are unsatis-factory.
For example, topic 2 learned by DF-LDAmixed up a sex topic and a reading topic.
Less rele-vant words such as columbia, year, write show up inthe health insurance topic (topic 4) learned by Quad-LDA.
Our method MRF-LDA incorporates the wordcorrelation knowledge by imposing a MRF over thelatent topic layer to encourage correlated words toshare the same topic label, hence similar words havebetter chance to be put into the same topic.
Conse-730Table 2: Topics Learned from 20-Newsgroups DatasetLDA DF-LDATopic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4(Crime) (Sex) (Sports) (Health) (Crime) (Sex) (Sports) (Health)gun sex team government gun book game moneyguns men game money police men games payweapons homosexuality hockey private carry books players insurancecontrol homosexual season people kill homosexual hockey policyfirearms gay will will killed homosexuality baseball taxcrime sexual year health weapon reference fan companiespolice com play tax cops gay league todaycom homosexuals nhl care warrant read played planweapon people games insurance deaths male season healthused cramer teams program control homosexuals ball jobsQuad-LDA MRF-LDATopic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4(Crime) (Sex) (Sports) (Health) (Crime) (Sex) (Sports) (Health)gun homosexuality game money gun men game careguns sex team insurance guns sex team insurancecrime homosexual play columbia weapons women hockey privatepolice sin games pay child homosexual players costweapons marriage hockey health police homosexuality play healthfirearms context season tax control child player costscriminal people rom year kill ass fans companycriminals sexual period private deaths sexual teams companiespeople gay goal care death gay fan taxlaw homosexuals player write people homosexuals best publicquently, the learned topics are of high coherence.
Asshown in Table 2, the topics learned by our methodare largely better than those learned by the baselinemethods.
The topics are of high coherence and con-tain fewer noise and irrelevant words.Our method provides a mechanism to automati-cally decide under which topic, two words labeled assimilar are truly similar.
The decision is made flex-ibly by data according to their fitness to the model,rather than by a hard rule adopted by DF-LDA andQuad-LDA.
For instance, according to the externalknowledge, the word child is correlated with gunand with men simultaneously.
Under a crime topic,child and gun are truly correlated because they co-occur a lot in youth crime news, whereas, child andmen are less correlated in this topic.
Under a sextopic, child and men are truly correlated whereaschild and gun are not.
Our method can differentiatethis subtlety and successfully put child and gun intothe crime topic and put child and men into the sextopic.
This is because our method encourages childand gun to be put into the same topic A and encour-ages child and men to be put into the same topic B,but does not require A and B to be the same.
A andB are freely decided by data.Table 3 shows some topics learned on NIPSdataset.
The four topics correspond to vision, neuralnetwork, speech recognition and electronic circuitsrespectively.
From this table, we observe that thetopics learned by our method are better in coherencethan those learned from the baseline methods, whichagain demonstrates the effectiveness of our model.4.2.2 Quantitative EvaluationWe also evaluate our method in a quantitativemanner.
Similar to (Xie and Xing, 2013), we usethe coherence measure (CM) to assess how coherentthe learned topics are.
For each topic, we pick up thetop 10 candidate words and ask human annotators tojudge whether they are relevant to the topic.
First,annotators needs to judge whether a topic is inter-pretable or not.
If not, the ten candidate words in this731Table 3: Topics Learned from NIPS DatasetLDA DF-LDATopic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4(Vision) (Neural Net) (Speech) (Circuits) (Vision) (Neural Net) (Speech) (Circuits)image network hmm chip images network speech analogimages neural mlp analog pixel system context chippixel feedforward hidden weight view connection speaker vlsivision architecture context digital recognition application frame implementsegment research model neural face artificial continuous digitalvisual general recognition hardware ica input processing hardwarescene applied probabilities bit vision obtained number voltagetexture vol training neuron system department dependent bitcontour paper markov implement natural fixed frames transistoredge introduction system vlsi faces techniques spectral designQuad-LDA MRF-LDATopic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4(Vision) (Neural Net) (Speech) (Circuits) (Vision) (Neural Net) (Speech) (Circuits)image training speech circuit image network hmm chipimages set hmm analog images model speech synapsepixel network speaker chip pixel learning acoustic digitalregion learning acoustic voltage disparity function context analogvision net phonetic current color input word boardscene number vocabulary vlsi intensity neural phonetic chargesurface algorithm phone neuron stereo set frames synaptictexture class utterance gate scene algorithm speaker hardwarelocal input utterances input camera system phone vlsicontour examples frames transistor detector data vocabulary programmabletopic are automatically labeled as irrelevant.
Other-wise, annotators are asked to identify words that arerelevant to this topic.
Coherence measure (CM) isdefined as the ratio between the number of relevantwords and total number of candidate words.
In ourexperiments, four graduate students participated thelabeling.
For each dataset and each method, 10% oftopics were randomly chosen for labeling.Table 4 and 5 summarize the coherence mea-sure of topics learned on 20-Newsgroups dataset andNIPS dataset respectively.
As shown in the table, ourmethod significantly outperforms the baseline meth-ods with a large margin.
On the 20-Newsgroupsdataset, our method achieves an average coherencemeasure of 60.8%, which is two times better thanLDA.
On the NIPS dataset, our method is also muchbetter than the baselines.
In summary, we concludethat MRF-LDA produces much better results onboth datasets compared to baselines, which demon-strates the effectiveness of our model in exploitingword correlation knowledge to improve the qual-ity of topic modeling.
To assess the consistency ofthe labelings made by different annotators, we com-puted the intraclass correlation coefficient (ICC).The ICCs on 20-Newsgroups and NIPS dataset are0.925 and 0.725 respectively, which indicate goodagreement between different annotators.5 ConclusionIn this paper, we propose a MRF-LDA model, aim-ing to incorporate word correlation knowledge toimprove topic modeling.
Our model defines a MRFover the latent topic layer of LDA, to encourage cor-related words to be put into the same topic.
Ourmodel provides the flexibility to enable a word tobe similar to different words under different top-ics, which is more plausible and allows a word toshow up in multiple topics properly.
We evaluateour model on two datasets and corroborate its effec-tiveness both qualitatively and quantitatively.732Method Annotator1 Annotator2 Annotator3 Annotator4 Mean Standard DeviationLDA 30 33 22 29 28.5 4.7DF-LDA 35 41 35 27 36.8 2.9Quad-LDA 32 36 33 26 31.8 4.2MRF-LDA 60 60 63 60 60.8 1.5Table 4: CM (%) on 20-Newsgroups DatasetMethod Annotator1 Annotator2 Annotator3 Annotator4 Mean Standard DeviationLDA 75 74 74 69 73 2.7DF-LDA 65 74 72 47 66 9.5Quad-LDA 40 40 38 25 35.8 7.2MRF-LDA 86 85 87 84 85.8 1.0Table 5: CM (%) on NIPS DatasetReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via dirichlet forest priors.
In Proceedings ofthe 26th Annual International Conference on MachineLearning, pages 25?32.
ACM.David Andrzejewski, Xiaojin Zhu, Mark Craven, andBenjamin Recht.
2011.
A framework for incorporat-ing general domain knowledge into latent dirichlet allocation using first-order logic.
In Proceedings of theTwenty-Second international joint conference on Ar-tificial Intelligence-Volume Volume Two, pages 1171?1177.
AAAI Press.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In EMNLP-CoNLL, pages 1024?1033.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Malu Castellanos, and Riddhiman Ghosh.
2013.Leveraging multi-domain prior knowledge in topicmodels.
In Proceedings of the Twenty-Third Interna-tional Joint Conference on Artificial Intelligence, IJ-CAI?13, pages 2071?2077.Gregor Heinrich.
2009.
A generic approach to topicmodels.
In Machine Learning and Knowledge Discov-ery in Databases, pages 517?532.
Springer.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 50?57.
ACM.Yuening Hu, Jordan L Boyd-Graber, and Brianna Sati-noff.
2011.
Interactive topic modeling.
In ACL, pages248?257.Jagadeesh Jagarlamudi, Hal Daum?e, III, and Raghaven-dra Udupa.
2012.
Incorporating lexical priors intotopic models.
In Proceedings of the 13th Conferenceof the European Chapter of the Association for Com-putational Linguistics, EACL ?12, pages 204?213.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.George A Miller.
1995a.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.George A. Miller.
1995b.
Wordnet: A lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.David Newman, Edwin V Bonilla, and Wray Buntine.2011.
Improving topic coherence with regularizedtopic models.
In Advances in Neural Information Pro-cessing Systems, pages 496?504.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
Proceedings of the Empiricial Methods inNatural Language Processing (EMNLP 2014), 12.James Petterson, Wray Buntine, Shravan M Narayana-murthy, Tib?erio S Caetano, and Alex J Smola.
2010.Word features for latent dirichlet alocation.
In Ad-vances in Neural Information Processing Systems,pages 1921?1929.Jakob Verbeek and Bill Triggs.
2007.
Region classifi-cation with markov field aspect models.
In ComputerVision and Pattern Recognition, 2007.
CVPR?07.
IEEEConference on, pages 1?8.
IEEE.Martin J Wainwright and Michael I Jordan.
2008.
Graph-ical models, exponential families, and variational in-ference.
Foundations and TrendsR?
in Machine Learn-ing, 1(1-2):1?305.733Pengtao Xie and Eric P Xing.
2013.
Integrating docu-ment clustering and topic modeling.
Proceedings ofthe 30th Conference on Uncertainty in Artificial Intel-ligence.Bin Zhao, Li Fei-Fei, and Eric Xing.
2010.
Image seg-mentation with topic random field.
Computer Vision?ECCV 2010, pages 785?798.Jun Zhu and Eric P Xing.
2010.
Conditional topic ran-dom fields.
In Proceedings of the 27th InternationalConference on Machine Learning (ICML-10), pages1239?1246.734
