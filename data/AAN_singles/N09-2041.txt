Proceedings of NAACL HLT 2009: Short Papers, pages 161?164,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExploiting Named Entity Classes in CCG Surface RealizationRajakrishnan Rajkumar Michael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{raja,mwhite,espinosa}@ling.osu.eduDominic EspinosaAbstractThis paper describes how named entity (NE)classes can be used to improve broad cover-age surface realization with the OpenCCG re-alizer.
Our experiments indicate that collaps-ing certain multi-word NEs and interpolatinga language model where NEs are replaced bytheir class labels yields the largest quality in-crease, with 4-grams adding a small additionalboost.
Substantial further benefit is obtainedby including class information in the hyper-tagging (supertagging for realization) compo-nent of the system, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 ofthe CCGbank.
A targeted manual evaluationconfirms that the BLEU score increase corre-sponds to a significant rise in fluency.1 IntroductionHogan et al (2007) have recently shown that betterhandling of named entities (NEs) in broad coveragesurface realization with LFG can lead to substan-tial improvements in BLEU scores.
In this paper,we confirm that better NE handling can likewise im-prove broad coverage surface realization with CCG,even when employing a more restrictive notion ofnamed entities that better matches traditional real-ization practice.
Going beyond Hogan et al (2007),we additionally show that NE classes can be usedto improve realization quality through better lan-guage models and better hypertagging (supertaggingfor realization) models, yielding a state-of-the-artBLEU score of 0.8173 on Section 23 of the CCG-bank.A question addressed neither by Hogan et alnor anyone else working on broad coverage surfacerealization recently is whether reported increasesin BLEU scores actually correspond to observableimprovements in quality.
We view this situationas problematic, not only because Callison-Burchet al (2006) have shown that BLEU does not al-ways rank competing systems in accord with hu-man judgments, but also because surface realiza-tion scores are typically much higher than those inMT?where BLEU?s performance has been repeat-edly assessed?even when using just one reference.Thus, in this paper, we present a targeted manualevaluation confirming that our BLEU score increasecorresponds to a significant rise in fluency, a practicewe encourage others to adopt.2 CCG Surface RealizationCCG (Steedman, 2000) is a unification-based cat-egorial grammar formalism defined almost en-tirely in terms of lexical entries that encode sub-categorization as well as syntactic features (e.g.number and agreement).
OpenCCG is a pars-ing/generation library which includes a hybridsymbolic-statistical chart realizer (White, 2006).
Avital component of the realizer is the hypertagger(Espinosa et al, 2008), which predicts lexical cat-egory assignments using a maxent model trained oncontexts within a directed graph structure represent-ing the logical form (LF) input; features and rela-tions in the graph as well as parent child relation-ships are the main features used to train the model.The realizer takes as input an LF description (seeFigure 1 of Espinosa et al, 2008), but here we also161use LFs with class information on some elementarypredications (e.g.
@x:MONEY($ 10,000)).
Chart re-alization proceeds in iterative beta-best fashion, witha progressively wider hypertagger beam width.
If nocomplete realization is found within the time limit,fragments are greedily assembled.
Alternative real-izations are ranked using integrated n-gram scoring;n-gram models help in choosing word order and, toa lesser extent, making lexical choices.3 Collapsing Named EntitiesAn error analysis of the OpenCCG baseline outputreveals that out of 2331 NEs annotated by the BBNcorpus, 238 are not realized correctly.
For exam-ple, multi-word NPs like Texas Instruments JapanLtd.
are realized as Japan Texas Instruments Ltd..Inspired by Hogan et al?s (2007)?s Experiment 1,we decided to use the BBN corpus NE annotation(Weischedel and Brunstein, 2005) to collapse cer-tain classes of NEs.
But unlike their experimentwhere all the NEs annotated by the BBN corpus arecollapsed, we chose to collapse into single tokensonly NEs whose exact form can be reasonably ex-pected to be specified in the input to the realizer.For example, while some quantificational or com-paratives phrases like more than $ 10,000 are anno-tated as MONEY in the BBN corpus, in our viewonly $ 10,000 should be collapsed into an atomicunit, with more than handled compositionally ac-cording to the semantics assigned to it by the gram-mar.
Thus, after transferring the BBN annotations tothe CCGbank corpus, we (partially) collapsed NEswhich are CCGbank constituents according to thefollowing rules: (1) completely collapse the PER-SON, ORGANIZATION, GPE, WORK OF ARTmajor class type entitites; (2) ignore phrases likethree decades later, which are annotated as DATEentities; and (3) collapse all phrases with POS tagsCD or NNP(S) or lexical items % or $, ensuring thatall prototypical named entities are collapsed.4 Exploiting NE ClassesGoing beyond Hogan et al (2007) and collaps-ing experiments, we also experiment with NEclasses in language models and hypertagging mod-els.
BBN annotates both major types and subtypes(DATE:AGE, DATE:DATE etc).
For all our experi-ments, we use both of these.4.1 Class replaced n-gram modelsFor both the original CCGbank as well as the col-lapsed corpus, we created language model trainingdata with semantic classes replacing actual words,in order to address data sparsity issues caused byrare words in the same semantic class.
For exam-ple, in the collapsed corpus, the Section 00 sen-tence Pierre Vinken , 61 years old , will join theboard as a nonexecutive director Nov. 29 .
be-comes PERSON , DATE:AGE DATE:AGE old ,will join the ORG DESC:OTHER as a nonexecutivePER DESC DATE:DATE DATE:DATE .
During re-alization, word forms are generated, but are then re-placed by their semantic classes and scored usingthe semantic class replaced n-gram model, similarto (Oh and Rudnicky, 2002).
As the specific wordsmay still matter, the class replaced model is interpo-lated at the word level with an ordinary, word-basedlanguage model, as well as with a factored languagemodel over POS tags and supertags.4.2 Class features in hypertaggingWe also experimented with a hypertagging modeltrained over the collapsed corpus, where the seman-tic classes of the elementary lexical predications,along with the class features of their adjacent nodes,are added as features.5 Evaluation5.1 Hypertagger evaluationAs Table 2 indicates, the hypertagging model doesworse in terms of per-logical predication accuracy& per-whole-graph accuracy on the collapsed cor-pus.
To some extent this is not surprising, as collaps-ing eliminates many easy tagging cases; however, afull explanation is still under investigation.
Note thatclass information does improve performance some-what on the collapsed corpus.5.2 Realizer evaluationFor a both the original CCGbank and the col-lapsed corpus, we extracted a section 02?21 lexico-grammars and used it to derive LFs for the devel-opment and test sections.
We used the languagemodels in Table 1 to score realizations and for the162Condition ExpansionLM baseline-LM: word 3g+ pos 3g*stag 3gHT baseline HypertaggerLM4 LM with 4g wordLMC LM with class-rep model interpolatedLM4C LM with bothHTC HT with classes on nodes as extra featsTable 1: Legend for Experimental ConditionsCorpus Condition Tags/pred Pred GraphUncollapsed HT 1.0 93.56% 39.14%HT 1.5 98.28% 78.06%Partly HT 1.0 92.22% 35.04%Collapsed HTC 1.0 92.89% 38.31%HT 1.5 97.87% 73.14%HTC 1.5 98.02% 75.30%Table 2: Hypertagger testing on Section 00 of the uncol-lapsed corpus (1896 LFs & 38104 predicates) & partiallycollapsed corpus (1895 LFs & 35370 predicates)collapsed corpus, we also tried a class-based hyper-tagging model.
Hypertagger ?-values were set foreach corpus and for each hypertagging model suchthat the predicted tags per pred was the same at eachlevel.
BLEU scores were calculated after removingthe underscores between collapsed NEs.5.3 ResultsOur baseline results are much better than those pre-viously reported with OpenCCG in large part due toimproved grammar engineering efforts and bug fix-ing.
Table 3 shows development set results whichindicate that collapsing appears to improve realiza-tion on the whole, as evidenced by the small increasein BLEU scores.
The class-replaced word modelprovides a big boost on the collapsed corpus, from0.7917 to 0.7993, much more than 4-grams.
Addingsemantic classes to the hypertagger improves its ac-curacy and gives us another half BLEU point in-crease.
Standard test set results, reported in Table 4,confirm the overall increase, from 0.7940 to 0.8173.In analyzing the Section 00 results, we found thatwith the collapsed corpus, NE errors were reducedfrom 238 to 99, which explains why the BLEUscore increases despite the drop in exact matches andgrammatically complete realizations from the base-line.
A semi-automatic analysis reveals that mostof the corrections involve proper names that are nolonger mangled.
Correct adjective ordering is alsoachieved in some cases; for example, Dutch publish-Corpus Condition %Exact %Complete BLEUUncollapsed LM+HT 29.27 84.02 0.7900(98.6% LM4+HT 29.14 83.61 0.7899coverage) LMC+HT 30.64 83.70 0.7937LM4C+HT 30.85 83.65 0.7946Partly collapsed LM+HT 28.28 82.48 0.7917(98.6% LM4+HT 28.68 82.54 0.7929coverage) LMC+HT 30.74 82.33 0.7993LM4C+HT 31.06 82.33 0.7995LM4C+HTC 32.01 83.17 0.8042Table 3: Section 00 blind testing resultsCondition %Exact %Complete BLEULM+HT 29.38 82.53 0.7940LM4C+HTC 33.74 85.04 0.8173Table 4: Section 23 results: LM+HT baseline on origi-nal corpus (97.8% coverage), LM4C+HTC best case oncollapsed corpus (94.8% coverage)ing group is enforced by the class-replaced models,while all the other models realize this as publishingDutch group.
Additionally, the class-replaced modelsometimes helps with animacy marking on relativepronouns, as in Mr. Otero , who .
.
.
instead of Mr.Otero , which .
.
.
.
(Note that our input LFs do notdirectly specify the choice of function words suchas case-marking prepositions, relative pronouns andcomplementizers, and thus class-based scoring canhelp to select the correct surface word form.
)5.4 Targeted manual evaluationWhile the language models employing NE classescertainly improve some examples, others are madeworse, and some are just changed to different, butequally acceptable paraphrases.
For this reason, wecarried out a targeted manual evaluation to confirmthe BLEU results.5.4.1 ProcedureAlong the lines of (Callison-Burch et al, 2006),two native speakers (two of the authors) providedratings for a random sample of 49 realizations thatdiffered between the baseline and best conditions onthe collapsed corpus.
Note that the selection pro-cedure excludes exact matches and thus focuses onsentences whose realization quality may be loweron average than in an arbitrary sample.
Sentenceswere rated in the context of the preceding sentence(if any) for both fluency and adequacy in compari-son to the original sentence.
The judges were not163LEU scoreB22.533.544.550.66  0.68  0.7  0.72  0.74  0.76  0.78  0.8AdequacyFluencyBaselineBestHumanScoreFigure 1: BLEU scores plotted against human judge-ments of fluency and adequacyaware of the condition (best/baseline) while doingthe rating.
Ratings of the two judges were averagedfor each item.5.4.2 ResultsIn the human evaluation, the best system?s meanscores were 4.4 for adequacy and 3.61 for fluency,compared with the baseline?s scores of 4.35 and 3.36respectively.
Figure 1 shows these results includingthe standard error for each measurement, with theBLEU scores for this specific test set.
The samplesize was sufficient to show that the increase in flu-ency from 3.36 to 3.61 represented a significant dif-ference (paired t-test, 1-tailed, p = 0.015), while theadequacy scores did not differ significantly.5.4.3 Brief comparison to related systemsWhile direct comparisons cannot really be madewhen inputs vary in their semantic depth and speci-ficity, we observe that our all-sentences BLEU scoreof 0.8173 exceeds that of Hogan et al (2007), whoreport a top score of 0.6882 (though with coveragenear 100%).
Nakanishi et al (2005) and Langkilde-Geary (2002) report scores of 0.7733 and 0.7570, re-spectively, though the former is limited to sentencesof length 20 or less, and the latter?s coverage is muchlower.6 Conclusion and Future WorkIn this paper, we have shown how named entityclasses can be used to improve the OpenCCG re-alizer?s language models and hypertagging models,helping to achieve a state-of-the-art BLEU score of0.8173 on CCGbank Section 23.
We have also con-firmed the increase in quality through a targetedmanual evaluation, a practice we encourage othersworking on surface realization to adopt.
In futurework, we plan to investigate the unexpected drop inhypertagger performance on our NE-collapsed cor-pus, which we conjecture may be resolved by takingadvantage of Vadas and Curran?s (2008) correctionsto the CCGbank?s NP structures.7 AcknowledgementsThis work was supported in part by NSF IIS-0812297 and by an allocation of computing timefrom the Ohio Supercomputer Center.
Our thanksalso to Josef Van Genabith, the OSU Clippers groupand the anonymous reviewers for helpful commentsand discussion.ReferencesChris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU in ma-chine translation research.
In Proc.
EACL.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proc.
ACL-08:HLT.Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josefvan Genabith.
2007.
Exploiting multi-word unitsin history-based probabilistic generation.
In Proc.EMNLP-CoNLL.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proc.
INLG-02.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation of anHPSG-based chart generator.
In Proc.
IWPT-05.Alice H. Oh and Alexander I. Rudnicky.
2002.
Stochas-tic natural language generation for spoken dialog sys-tems.
Computer, Speech & Language, 16(3/4):387?407.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.David Vadas and James R. Curran.
2008.
Parsing nounphrase structure with CCG.
In Proc.
ACL-08:HLT.Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus.
Technicalreport, BBN.Michael White.
2006.
Efficient Realization of Coordi-nate Structures in Combinatory Categorial Grammar.Research on Language and Computation, 4(1):39?75.164
