Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 49?54,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing emotion to gain rapport in a spoken dialog systemJaime C. AcostaDepartment of Computer ScienceUniversity of Texas at El PasoEl Paso, TX 79968, USAjcacosta@miners.utep.eduAbstractThis paper describes research on automati-cally building rapport.
This is done by adapt-ing responses in a spoken dialog system tousers?
emotions as inferred from nonverbalvoice properties.
Emotions and their acous-tic correlates will be extracted from a persua-sive dialog corpus and will be used to imple-ment an emotionally intelligent dialog system;one that can recognize emotion, choose an op-timal strategy for gaining rapport, and rendera response that contains appropriate emotion,both lexically and auditory.
In order to deter-mine the value of emotion modeling for gain-ing rapport in a spoken dialog system, the finalimplementation will be evaluated using differ-ent configurations through a user study.1 IntroductionAs information sources become richer and technol-ogy advances, the use of computers to deliver in-formation is increasing.
In particular, interactivevoice technology for information delivery is becom-ing more common due to improvements in tech-nologies such as automatic speech recognition, andspeech synthesis.Several problems exist in these voice technologiesincluding speech recognition accuracy and lack ofcommon sense and basic knowledge.
Among theseproblems is the inability to achieve rapport.Gratch et al (2007) defines rapport as a feel-ing of connectedness that seems to arise from rapidand contingent positive feedback between partnersand is often associated with socio-emotional pro-cesses.
In the field of neuro-linguistics, O?Conneland Seymour (1990) stated that matching or com-plimenting voice features such as volume, speed,and intonation, is important to gain rapport.
Shep-ard et al?s Communication Accommodation The-ory (2001) states that humans use prosody andbackchannels in order to adjust social distance withan interlocutor.
These features of voice can also beassociated with emotions.Previous work has shown that automated systemscan gain rapport by reacting to user gestural nonver-bal behavior (Chartrand and Bargh, 1999; Gratch etal., 2007; Cassell and Bickmore, 2003).
In contrast,this research looks at how rapport can be gainedthrough voice-only interaction.Preliminary analysis of human-human dialog pro-vides evidence that shifts in pitch, associated withemotion by two judges, are used by an interlocu-tor for persuasion.
Figure 1 shows the pitch of asound snippet from the corpus and how it differsfrom neutral, computer synthesized voice (producedusing MaryTTS).
This illustrates the more generalfact that when humans speak to each other, we dis-play a variety of nonverbal behaviors in voice, es-pecially when trying to build rapport.
The main hy-pothesis of this research is that a spoken dialog sys-tem with emotional intelligence will be effective forgaining rapport with human users.The rest of this paper is structured as follows:first, related work is reviewed and current limitationsfor building automated rapport are described.
After-wards, the hypotheses and expected contributions ofthis work are described along with the research ap-proach.
Lastly, broader significance of this work isdiscussed.49Figure 1: Pitch levels of a conversation taken from the persuasive dialog corpus includes a student (Std) and a graduatecoordinator (Grad.Coord).
Pitch was analyzed using the Praat software.
It can be seen that the student displays richprosody in voice (tree parents) and that the human response (left branch) contains more varied prosody than thecomputer synthesized voice (right branch).2 Related WorkCommunication Accommodation Theory states thatpeople use nonverbal feedback to establish socialdistance during conversation.
In order to gain rap-port, people would most likely want to decreasesocial distance in order to achieve the connected-ness and smoothness in conversation that is seenin human social interaction.
Research in human-computer interaction has pursued these nonverbalbehaviors through appropriate backchanneling, headnods, and gaze techniques, but still missing is atten-tion to user emotional state, which can be detectedthrough some of these nonverbal behaviors in voice.Two methods for describing emotions are discreteand dimensional.
Discrete emotions include anger,disgust, fear, joy, sadness, and surprise.
Dimen-sional emotions use two or more components to de-scribe affective state.
More commonly used dimen-sions are Osgood et al?s (1957) evaluation (a.k.a.valence), activity, and potency (a.k.a.
power).
Emo-tion research has had limited success at detectingdiscrete emotions, e.g.
(D?Mello et al, 2008).
Inthe tutoring domain, some have looked at appropri-ately responding to students based on their prosodyin voice (Hollingsed and Ward, 2007).
The dif-ficulty of recognizing discrete emotions exists be-cause humans typically show more subtle emotionsin most real human-human interactions (Batliner etal., 2000).
Forbes et al (2004) had promising resultsby looking at a three-class set of emotions (positive,negative, neutral).The intent of this research is to develop a methodfor detecting three dimensions of emotion fromvoice in order to build rapport.
There is a possibilitythat using a dimensional approach will enable moreaccurate modeling of subtle emotions that exist inspontaneous human-human dialogs.3 Hypotheses and Expected ContributionsThe main hypothesis of this work is that a spokendialog system with emotional intelligence will bemore effective for gaining rapport than a spoken di-alog system without emotional intelligence.
In or-der to test this hypothesis, I will implement andevaluate a spoken dialog system.
This system will50choose topics and content depending on user emo-tional state.
The resulting system will advance thestate of the art in three technologies: recognizingappropriate emotion, planning accordingly, and syn-thesizing appropriate emotion.
The system will alsodemonstrate how to integrate these components.In addition to choosing the correct content basedon user emotional state, this research will investi-gate the effect of adding emotion to voice for rap-port.
The second hypothesis of the research is thatexpressing emotion in voice and choosing words,compared to expressing emotion only by choosingwords, will be more effective for building rapportwith users.4 ApproachThis section outlines the steps that have been com-pleted and those that are still pending to accomplishthe goals of the research.4.1 Corpus Analysis and Baseline SystemThis work is based on a persuasive dialog corpusconsisting of audio recordings of 10 interactions av-eraging 16 minutes in length.
The corpus consistsof rougly 1000 turns between a graduate coordina-tor and individual students.
The graduate coordina-tor was a personable female staff member who washired by the University to raise the graduate studentcount.
The students were enrolled in an introduc-tory Computer Science course and participated inthe study as part of a research credit required forcourse completion.
The students had little knowl-edge of the nature or value of graduate school and ofthe application process.
Preliminary analysis of thecorpus showed evidence of a graduate coordinatorbuilding rapport with students by using emotion.A baseline system built using commercial state-of-the-art software was implemented based on thecorpus (mainly the topics covered).
Informal usercomments about the baseline system helped deter-mine missing features for automated rapport build-ing technology.
One salient feature that is missingis attention to emotion in voice.
This confirmed thedirection of this research.This corpus was transcribed and annotated withdimensional emotions (activation, valence, andpower) by two judges.
Activation is defined assounding ready to take action, valence is the amountof positive or negative sound in voice, and poweris measured by the amount of dominance in voice.The dimensions are annotated numerically on scalesfrom -100 to +100.The following are examples taken from the corpuswith annotated acoustic features.?
Example 1Grad.Coord(GC1): So you?re in the 1401class?
[rising pitch]Subject(S1): Yeah.
[higher pitch]GC2: Yeah?
How are you liking it sofar?
[falling pitch]S2: Um, it?s alright, it?s just the labs arekind of difficult sometimes, they can, they givelike long stuff.
[slower speed]GC3: Mm.
Are the TAs helping you?
[lower pitch and slower speed]S3: Yeah.
[rising pitch]GC4: Yeah.
[rising pitch]S4: They?re doing a good job.
[normal pitch and normal speed]GC5: Good, that?s good, that?s good.
[normal pitch and normal speed]?
Example 2GC6: You?re taking your first CS class huh.
[slightly faster voice]S5: Yeah, I barely started.
[faster voice]GC7: How are you liking it?
[faster voice, higher pitch]S6: Uh, I like it a lot, actually, it?s prob-ably my favorite class.
[faster, louder]GC8: Oh good.
[slower, softer]51S7: That I?m taking right now yeah.
[slightly faster, softer]GC9: Oh that?s good.
That?s exciting.
[slow and soft then fast and loud]GC10: Then you picked the right majoryou?re not gonna change it three times like Idid.
[faster, louder]In the first example, the coordinator noticablyraises her pitch at the end of her utterance.
Thisis probably so that she can sound polite or inter-ested.
On line S2, the subject displays a fallingpitch (which sounds negative) and the coordinatorresponds with a lower fundamental frequency and aslower speed.
The subject sounds unsure by display-ing a rising pitch in his answer (S3).
The coordinatormirrors his response (GC4) and finally both inter-locutors end with normal pitch and normal speed.In the second example, the subject speaks fasterthan usual (S5).
The coordinator compensates byadjusting her speed as well.
From S6 through GC8,when the subject?s voice gets louder, the coordina-tor?s voice gets softer, almost as though she is back-ing off and letting the subject have some space.
InGC9 the coordinator responds to the student?s posi-tive response (liking the class) and becomes imme-diately faster and louder.A next step for the analysis is to determine themost expressive acoustic correlates for emotions.
In-formal auditory comparisons show some possiblecorrelations (see Table 1).
These correlations seempromising because many correspond with previouswork (Schroder, 2004).The emotion annotations of the two judges showthat strategies for adaptive emotion responses canbe extracted from the corpus.
Communication Ac-comodation Theory states that interlocutors mir-ror nonverbal behaviors during interaction when at-tempting to decrease social distance.
The coordina-tor?s emotional responses were correlated with thestudent?s emotional utterances to determine if emo-tional mirroring (matching student emotion and co-ordinator response) was present in the persuasive di-alog corpus.
This was the case in the valence dimen-sion, which showed a correlation coefficient of 0.34.Table 1: Informal analysis reveals acoustic correlatespossibly associated with the dimensions of emotionDimension High LowActiveness Faster, morevaried pitch,louderSlower, lessvaried pitch,softerValence Higher pitchthroughout,laughter, speedupFalling endingpitch, articula-tion of words,increasingloudnessPower Faster, louder,falling endingpitch, articu-lation of wordbeginnings,longer vowelsSofter, higherpitch through-out, quickrise in pitch,smoother wordconnectionHowever, regarding power, there was an inverse re-lationship; if the student showed more power, thecoordinator showed less (?0.30 correlation coeffi-cient).
Activation showed a small correlation coeffi-cient (?0.14).To realize a spoken dialog system that couldmodel this responsive behavior, machine learningwas used.
The students?
three emotion dimensionswere taken as attributes and were used to predictthe coordinators emotional responses using Baggingwith REPTrees.
Measuring the correlations betweenthe predictions of the model and the actual values inthe corpus revealed correlation coefficients of 0.347,0.344, and 0.187 when predicting the coordinator?svalence, power, and activation levels, respectively.4.2 Full SystemThe full system will provide a means to evaluatewhether emotion contributes to automated rapportbuilding.
This system will be based on several avail-able technologies and previous research in spokendialog systems.Figure 2 shows the different components antici-pated for the full system.
The components that willbe implemented for this research include emotionrecognition, user modeling components, and textand emotion strategy databases.
The other compo-nents will be based on available open source soft-ware packages.
The implementation effort also in-52Figure 2: Full System Dataflow Diagramcludes the integration of all components.The following is a scenario that depicts how thefull system will operate.1.
The system begins by saying ?How are you do-ing today??2.
The user says ?I?m doing good?
with a negativesounding voice.3.
The voice signal is then processed through thespeech recognizer and emotion recognizer inparallel.
The speech recognizer extracts wordsfrom the voice signal while the emotion recog-nizer extracts emotion.4.
This data is sent to the user modeling com-ponent which determines the immediate userstate based only on the current emotion and thewords spoken.
In this scenario, the user?s statewill be negative even though the user statedotherwise.5.
This user state update information is thenpassed to the user model which updates thecurrent user state.
This component containsknowledge, beliefs and feelings of the user.Since there was no previous user state, the cur-rent emotion is set to negative.
Stored in userknowledge will be the fact that the user wasasked ?How are you doing today??.
Some in-formation about the user?s contradictory state isstored as user beliefs: stated good, but soundsnegative.6.
Next, this information is used to select somepredefined text from the lexical generationalong with an associated emotion from theemotion strategy database (these two are donein parallel).
Since the user?s state is negative,the system may choose to ask another questionsuch as ?ok, do you have any concerns??
with anegative sounding voice (to mirror the valencedimension).
In contrast, if the user was pos-itive, the system may have chosen somethingsimilar to ?great, let?s get going then?
with ahighly positive voice.7.
Lastly, the text with corresponding emotioncoloring is rendered to speech and played to theuser by the speech synthesis component.4.3 EvaluationTo achieve the final goal of determining whetheremotion helps gain rapport, the final system de-scribed herein will be evaluated.The final system will be configurable; it will allowfor enabling emotion in voice (voiced) or disablingthe emotions in voice (not voiced).
In addition, there53will be a control configuration, perhaps one that willdisplay a random emotion (random).
A user study(hopefully within subjects) will be conducted thatwill ask users to interact with four versions of thesystem (baseline, voiced, not voiced, and random).A post-test questionnaire consisting of Likert scaleswill ask users how much rapport they felt with eachversion of the system.
In addition, some objectivemetrics such as disfluency count and interaction timewill be collected.
This will help test the two hy-potheses of this research.
First, it is expected thatsubjects will have more rapport with the not voicedconfiguration than with the baseline system.
Thesecond hypothesis will be verified by determiningif subjects have more rapport with the voiced thanwith the not voiced system.
The random configura-tion will be used to determine whether the system?sadaptive responses are better than random responses.5 Broader SignificanceThis research addresses methods for gaining rap-port as an important dimension of successful human-computer interaction, and one likely to be usefuleven for business-like dialogs.
For example, build-ing rapport with customers can decrease the numberof disfluencies, which are currently a problem forspeech recognizers.
In addition, customer supportsystems will have the ability to tailor responses todecrease negative emotion.Similarly, the learned rules for detecting emotionand responding appropriately could be used to trainpeople how to more effectively gain rapport.
Lastly,this work can supplement other rapport research thatuses other forms of nonverbal behavior such as gazeand gestures seen especially in embodied conversa-tional agents.6 AcknowledgementsI would like to thank my advisor, Nigel Ward for hishelp.
Also, I would like to thank Anais Rivera andSue Walker for the collection of the persuasive dia-log corpus and Jun Zheng for his help in fine tuningthe baseline system.
This work is supported in partby NSF grant IIS-0415150.ReferencesA.
Batliner, K. Fischer, R. Huber, J. Spilker, and E. No?th.2000.
Desperately Seeking Emotions or: Actors,Wizards, and Human Beings.
In ISCA Tutorial andResearch Workshop (ITRW) on Speech and Emotion.ISCA.J.
Cassell and T. Bickmore.
2003.
Negotiated Collusion:Modeling Social Language and its Relationship Ef-fects in Intelligent Agents.
User Modeling and User-Adapted Interaction, 13(1):89?132.T.L.
Chartrand and J.A.
Bargh.
1999.
The chameleoneffect: The perception-behavior link and social inter-action.
Journal of Personality and Social Psychology,76(6):893?910.S.K.
D?Mello, S.D.
Craig, A. Witherspoon, B. McDaniel,and A. Graesser.
2008.
Automatic detection of learn-ers affect from conversational cues.
User Modelingand User-Adapted Interaction, 18(1):45?80.K.
Forbes-Riley and D. Litman.
2004.
Predictingemotion in spoken dialogue from multiple knowledgesources.
Proc.
Human Language Technology Conf.
ofthe North American Chap.
of the Assoc.
for Computa-tional Linguistics (HLT/NAACL).J.
Gratch, N. Wang, A. Okhmatovskaia, F. Lamothe,M.
Morales, R.J. van der Werf, and L. Morency.
2007.Can Virtual Humans Be More Engaging Than RealOnes?
12th International Conference on Human-Computer Interaction.Tasha K. Hollingsed and Nigel G. Ward.
2007.
A com-bined method for discovering short-term affect-basedresponse rules for spoken tutorial dialog.
Workshopon Speech and Language Technology in Education(SLaTE).J.
O?Connor and J. Seymour.
1990.
Introducing neuro-linguistic programming.
Mandala.C.E.
Osgood.
1957.
The Measurement of Meaning.
Uni-versity of Illinois Press.M.
Schroder.
2004.
Dimensional Emotion Representa-tion as a Basis for Speech Synthesis with Non-extremeEmotions.
In Proceedings Workshop Affective Dia-logue Systems, 3068:209?220.C.A.
Shepard, H. Giles, and B.A.
Le Poire.
2001.
Com-munication accommodation theory.
The new hand-book of language and social psychology, pages 33?56.54
