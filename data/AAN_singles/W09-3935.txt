Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 244?252,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsLearning to Predict Engagement with aSpoken Dialog System in Open-World SettingsDan BohusMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052dbohus@microsoft.comEric HorvitzMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052horvitz@microsoft.comAbstractWe describe a machine learning approach thatallows an open-world spoken dialog system tolearn to predict engagement intentions in situ,from interaction.
The proposed approach doesnot require any developer supervision, and le-verages spatiotemporal and attentional featuresautomatically extracted from a visual analysisof people coming into the proximity of the sys-tem to produce models that are attuned to thecharacteristics of the environment the system isplaced in.
Experimental results indicate that asystem using the proposed approach can learnto recognize engagement intentions at low falsepositive rates (e.g.
2-4%) up to 3-4 secondsprior to the actual moment of engagement.1 IntroductionWe address the challenge of predicting the forthcomingengagement of people with open-world conversationalsystems (Bohus and Horvitz, 2009a), i.e.
systems thatoperate in relatively unconstrained environments, wheremultiple participants might come and go, establish,maintain and break the communication frame, and si-multaneously interact with a system and with others.Examples of such systems include interactive billboardsin a mall, robots in a home environment, intelligenthome control systems, interactive systems that provideassistance and support during procedural tasks, etc.In traditional closed-world dialog systems the en-gagement problem is generally resolved via simple, un-ambiguous signals.
For example, engagement is gener-ally assumed once a phone call is answered by a tele-phony dialog system.
Similarly, a push-to-talk buttoncan provide a clear engagement signal for a speechenabled mobile application.
These solutions are howev-er inappropriate for systems that must operate conti-nuously in open, dynamic environments, and engagewith multiple people and groups over time.
Such sys-tems should ideally be ready to initiate dialog in a fluid,natural manner.
They should manage engagement withparticipants who are close by, and with those who are ata distance, with participants who have a standing plan tointeract with a system, and with those whom opportu-nistically decide to engage, in-stream with their otherongoing activities.
In recognizing engagement inten-tions, such systems need to minimize false positives,while also minimizing the unnatural delays and discon-tinuities that come with false negatives about engage-ment intentions.The work described in this paper is set in the largercontext of a computational model for supporting fluidengagement in open-world dialog systems that we havepreviously described in (Bohus and Horvitz, 2009b).The above mentioned model harnesses components forsensing the engagement state, actions, and intentions ofmultiple participants in the scene, for making engage-ment control decisions, and for rendering these deci-sions into coordinated low-level behaviors, such as thechanging pose and expressions of the face of an embo-died agent.
In this paper, we focus on the sensing sub-component of this larger model and describe an ap-proach for automatically learning to detect engagementintentions from interaction.2 Related WorkThe challenges of engagement between people, and be-tween people and computational systems, have alreadyreceived some attention in the conversational analysis,sociolinguistics, and human-computer interaction com-munities.
For instance, in an early treatise Goffman(1963) discusses how people use cues to detect engage-ment in an effort to avoid the social costs of engaging ininteraction with an unwilling participant.
In later work,Kendon (1990a) presents a detailed investigation ofvideo sequences of greetings in human-human interac-tion, and identifies several stages of complex coordi-nated action (pre-sighting, sighting, distance salutation,244approach, close salutation), together with the head andbody gestures that they typically involve.
In (1990b),Kendon also introduces the notion of an F-formation, apattern said to arise when ?two or more people sustain aspatial and orientational relationship in which they haveequal, direct, and exclusive access,?
and discusses therole of F-formations in establishing and maintainingsocial interactions.
Argyle and Cook (1976) as well asothers (e.g., Duncan, 1972; Vertegaal et al, 2001) haveidentified and discussed the various functions of eyegaze in maintaining social and communicative engage-ment.
Overall, this body of work suggests that engage-ment is a rich, mixed-initiative, and well-coordinatedprocess that involves non-verbal cues and signals, suchas spatial trajectory and proximity, gaze and mutualattention, head and hand gestures, and verbal greetings.More recently, several researchers have investigatedissues of engagement in human-computer and human-robot interaction contexts.
Sidner et al (2004; 2005)define engagement as ?the process by which two (ormore) participants establish, maintain and end their per-ceived connection during interactions they jointly un-dertake,?
and conduct a user study that explores theprocess of maintaining engagement.
They show thatpeople direct their attention to a robot more often whenthe robot makes engagement gestures throughout aninteraction, i.e.
tracks the user?s face, and points to rele-vant objects at appropriate times in the conversation.Peters et al(2005a; 2005b) use an alternative defini-tion of engagement as ?the value that a participant in aninteraction attributes to the goal of being together withthe other participant(s) and of continuing the interac-tion,?
and present the high-level schematics for an algo-rithm for establishing and maintaining engagement.
Theproposed algorithm highlights the importance of eyegaze and mutual attention in this process and relies on aheuristically computed interest level to decide when tobegin a conversation.Michalowski et al(2006) propose and conduct expe-riments with a spatial model of engagement, groundedin proxemics (Hall, 1966).
Their model classifies rele-vant agents in the scene in four different categoriesbased on their distance to the robot: present (standingfar), attending (standing closer), engaged (next to therobot), and interacting (standing right in front of therobot).
The robot?s behaviors are in turn conditioned onthese categories: the robot turns towards attendingpeople, greets engaged people and verbally promptsinteracting people for input.
The authors discuss severallessons learned from an observational study conductedwith this robot in a building lobby.
They find that thefast-paced movements of people in the environmentpose a number of challenges: often the robot greetedpeople too late (earlier anticipation was needed), orgreeted people that did not intend to engage (more accu-rate anticipation was needed).
The authors recognizethat these limitations stem partly from their reliance onstatic models, and hypothesize that temporal informa-tion such as speed and trajectory may provide additionalcues regarding a person?s engagement with the robot.In this paper, we expand on our previous work on asituated multiparty engagement model (Bohus and Hor-vitz, 2009b).
Specifically, we focus on a key subcom-ponent in this model: detecting whether or not a userintends to engage in an interaction with a system.
Weintroduce an approach that improves upon the existingwork (Peters 2005a, 2005b; Michalowski et.
al, 2006) inseveral significant ways.
First, the approach is data-driven: the use of machine learning techniques allowsthe system to adapt to the specific characteristics of itsphysical location and to the behaviors of the surround-ing population of potential participants.
Second, weleverage a wide array of observations, including tem-poral features.
Finally, no developer supervision is re-quired for training the model: the supervision signal isextracted automatically, in-stream with the interactions,allowing for online learning and adaptation.3 Situated Multiparty Engagement ModelTo set the broader context for the work described in thispaper, we now briefly review the overall model formanaging engagement in an open-world setting intro-duced in (Bohus and Horvitz, 2009b).
The model is cen-tered on a reified notion of interaction, defined as a ba-sic unit of sustained, interactive problem-solving.
Eachinteraction can involve two or more participants, andthis number may vary in time; new participants mayjoin an existing interaction and current participants mayleave an interaction at any point in time.
The system isactively engaged in at most one interaction at a time(with one or multiple participants), but it can simulta-neously keep track of additional, suspended interactions.In this context, engagement is viewed as the processsubsuming the joint, coordinated activities by whichparticipants initiate, maintain, join, abandon, suspend,resume, or terminate an interaction.Successfully managing this process requires that thesystem (1) senses and reasons about the engagementstate, actions and intentions of multiple agents in thescene, (2) makes high-level engagement control deci-sions (i.e.
about whom to engage or disengage with, andwhen) and (3) executes and signals these decisions tothe other participants in an appropriate manner (e.g.
viaa set of coordinated behaviors such as gestures, greet-ings, etc.)
The proposed model, illustrated in Figure 1,subsumes these three components.The sensing subcomponent in the model tracks theengagement state, engagement actions, and engagementintention for each agent in the visual scene.
The en-gagement state, ????
(?
), denotes whether an agent ?
is245Figure 1.
Graphical model showing key variables anddependencies in managing engagement.ESEAEIt   t+1SEAES?EIA?additionalcontextengagementsensing?GAGEASEB?engaged in interaction ?
and is modeled as a determinis-tic variable with two possible values: engaged and not-engaged.
The state is updated based on the joint actionsof the system and the agent.A second engagement variable, ????
(?
), models theactions that an agent takes to initiate, maintain or termi-nate engagement.
There are four possible engagementactions: engage, no-action, maintain, disengage.
Theseactions are tracked by means of a conditional probabilis-tic model that takes into account the engagement state????
(?
), the previous agent and system actions, as wellas additional sensory evidence ?
capturing committedengagement actions, such as: salutations (e.g.
?Hi!?
);calling behaviors (e.g.
?Laura!?
); the establishment orthe breaking of an F-formation (Kendon, 1990b); ex-pected opening dialog moves (e.g.
?Come here!?)
etc.A third variable in the proposed model, ????
(?)
,tracks whether or not each agent intends to be engagedin a conversation with the system.
Like the engagementstate, the intention can either be engaged or not-engaged.
Intentions are tracked separately from actionssince an agent might intend to engage or disengage thesystem, but not yet take an explicit engagement action.For instance, let us consider the case in which the sys-tem is already engaged in an interaction and another useis waiting in line to interact with the system: althoughthe waiting user does not take an explicit, committedengagement action, she might signal (e.g.
via a glancethat makes brief but clear eye contact with the interac-tive system) that her intention is to engage in a newconversation once the opportunity arises.
More general-ly, the engagement intention captures whether or not anagent would respond positively should the system in-itiate engagement.
In that sense, it roughly correspondsto Peters?
(2005; 2005b) ?interest level?, i.e.
to the valuethe agent attaches to being engaged in a conversationwith the system.
Like engagement actions, engagementintentions are inferred based on probabilistic modelsthat take into account the current engagement state, theprevious agent and system actions, the previous en-gagement intention, as well as additional evidence thatcaptures implicit engagement cues, e.g.
the spatiotem-poral trajectory of the participant, the level of sustainedmutual attention, etc.Based on the inferred engagement state, actions, andintentions of the agents in the scene, as well as otheradditional high-level evidence such as the agents?
in-ferred goals (?
), activities (?)
and relationships (?
), theproposed model outputs engagement actions ?
denotedby the ???
decision node in Figure 1.
The action-spaceconsists of the same four actions previously discussed:engage, disengage, maintain and no-action.
At the low-er level, the engagement decisions taken by the systemare translated into a set of coordinated lower-level be-haviors (???)
such as head gestures, making eye con-tact, facial expressions, salutations, interjections, etc.In related work (Bohus and Horvitz, 2009a; 2009b),we have demonstrated how this model can be used toeffectively create and support multiparty interactions inan open-world context.
Here, we focus on one specificsubcomponent of this framework: the model for detect-ing engagement intentions.4 ApproachTo illustrate the problem of detecting engagement inten-tions, consider for instance a situated conversationalsystem that examines through its sensors the scenesfrom Figure 3.
How can such a system detect whetherthe person in the image intends to engage in a conversa-tion or is just passing-by?
Studies of human-humanconversational engagement (Goffman, 1963; Argyle andCook, 1976; Duncan, 1972; Kendon, 1990, 1990b) indi-cate that people signal and detect engagement intentionsby producing and monitoring for a variety of cues, in-cluding gaze and sustained attention, trajectory andproximity, head and hand gestures, body pose, etc.In the proposed approach, we use machine learningtechniques, and leverage a wide array of observationsfrom the sensors to create a model that allows an open-world interactive system to detect the specific patternscharacterizing an engagement intention.
Existing workon detecting engagement intentions has focused on stat-ic heuristic models that leverage proximity and attentionfeatures (Peters, 2005, 2005b; Michalowski, 2006).
Aspreviously discussed, psychologists have shown theimportant role played by geometric relationships, trajec-tories, and sustained attention in signaling and detectingengagement.
The use of machine learning allows us toconsider a wide array of such features, including trajec-tory, speed, and the attention of agents over time.246Figure 3.
Placement and visual fields of view forside (right) and front (left) orientations.pillarKitchenetteCorridorpillarKitchenetteCorridorIn general, as discussed in the previous section, theengagement intentions of an agent may evolve tempo-rally under the proposed model, as a function of thevarious system actions and behaviors (e.g.
an embodiedsystem that makes eye contact, or smiles, or moves to-ward a participant might alter the engagement intentionof that participant).
In this work we concentrate on asimplified problem, in which the system?s behavior isfixed (e.g.
system always tracks people that pass by),and the engagement intention can be assumed constantwithin a limited time window.The central idea of the proposed approach is to startby using a very conservative (i.e., low false-positives)detector for engagement intentions, such as a push-to-engage button, and automatically gather sensor datasurrounding the moments of engagement, together withlabels that indicate whether someone actually engagedor not.
Note that the system eventually finds out if aperson becomes engaged with it.
If we assume that anintention to engage existed for a limited window of timeprior to the moment of engagement, the collected datacan be used to learn a model for predicting this intentionahead of the actual moment of engagement.
The pro-posed approach therefore enables a system to learn in-situ models for predicting forthcoming engagement, andthe models are attuned to the specifics of the environ-ment the system is in.
No explicit developer supervisionis required, as the training labels are extracted automati-cally from interaction.5 Experimental SetupTo provide an ecologically valid basis for data collec-tion and for evaluating the proposed approach, we de-veloped a situated conversational agent and deployed itin the real-world.
The system, illustrated in Figure 2, isan interactive multimodal kiosk that displays a realisti-cally rendered avatar head.
The avatar can engage andinteract via natural language with one or more partici-pants, and plays a simple game in which the users haveto respond to multiple-choice trivia questions.
The sys-tem, and sample interactions are described in more de-tail in (Bohus and Horvitz, 2009.
)The hardware and software architecture is also illu-strated in Figure 2.
Data gathered from a wide-anglecamera, a 4-element linear microphone array, and a 19?touch-screen is forwarded to a scene analysis modulethat fuses the incoming streams and constructs in real-time a coherent picture of the dynamics in the surround-ing environment.
The system detects and tracks the lo-cation of multiple agents in the scene, tracks the headpose for engaged agents, and infers the focus of atten-tion, activities, goals and (group) relationships amongdifferent agents in the scene.
An in-depth description ofthese scene analysis components falls beyond the scopeof this paper, but more details are available in (Bohusand Horvitz, 2009).
The scene analysis results are for-warded to the control level, which is structured in a two-layer reactive-deliberative architecture.
The reactivelayer implements and coordinates low-level behaviors,including engagement, conversational floor manage-ment and turn-taking, and coordinating spoken and ges-tural outputs.
The deliberative layer plans the system?sdialog moves and high-level engagement actions.We deployed the system described above in an open-space near the kitchenette area in our building.
As wewere interested in exploring the influence of the spatialsetup on the engagement models, we deployed the sys-tem in two different spatial orientations, illustrated to-gether with the resulting visual fields of view in Figure3.
Even though the location is similar, the two orienta-tions create considerable differences in the relative tra-jectories of people that go by (dashed lines) and peoplethat engage with the system (continuous lines).
In theside orientation, people typically enter the system?s fieldFigure 2.
System prototype and architectural overview.Dialog ManagementBehavioral ControlScene Analysis Output PlanningVision Speech Synthesis Avatarwide-angle camera4-element linear microphone arraytouch screenspeakers247of view and approach it from the sides.
In the frontorientation, people enter the field of view and approacheither frontally, or from the immediate right side.6 Data and ModelingThe system was deployed during regular business hoursfor 10 days in each of the two orientations describedabove, for a total of 158 hours and 32 minutes.
No in-structions were provided and most people that interactedwith the system did so for the first time.6.1 Corpus and Implicit LabelsThroughout the data collection, the system used a con-servative heuristic to detect engagement intentions: itconsidered that a user wanted to engage when they ap-proached the system and entered in an F-formation(Kendon, 1990b) with it.
Specifically, if a sufficientlylarge (close by) frontal face was detected in front of it,the system triggered an engaging action and started theinteraction.
We found this F-formation heuristic to befairly robust, having a false-positive rate of 0.18% (6false engagements out of 3274 total faces tracked).
In 2of these cases the face tracker committed an error andfalsely identified a large nearby face, and in 4 cases aperson passed by very close to the system but withoutany visible intention to engage.Although details on false-negative statistics have notyet been calculated (this would require a careful exami-nation of all 158 hours of data), our experience with theface detector suggests this number is near 0.
In monthsof usage, we never observed a case where the systemfailed to detect a close by, frontal face.
At the same time,we note that there is an important distinction betweenpeople who actually engage with the system, and peoplewho intend to engage, but perhaps not come in close-enough proximity for the system to detect this intention(according to the heuristic described above).
In thissense, while our heuristic can detect people who engageat a 0 false-negative rate, the false-negative rate withrespect to engagement intentions is non-zero.
Despitethese false-negatives, we found that the proposed heu-ristic still represents a good starting point for learning todetect engagement intentions.
As we shall see later, em-pirical results indicate that, by learning to detect whoactually engages, the system can learn to also detectpeople who might intend to engage, but who ultimatelydo not engage with the dialog system.In the experiments described here, we focus on de-tecting engagement intentions for people that ap-proached while the system was idle.
We therefore au-tomatically eliminated all faces that were temporallyoverlapping with the periods when the system was al-ready engaged in an interaction.
For the remaining facetraces, we automatically generate labels as follows:?
if a person entered in an F-formation and becameengaged in interaction with the system at time ??
,the corresponding face trace was labeled with apositive engagement intention label from ?
?-20sec;until ??
; the initial portion of the trace, from themoment it was detected until ?
?-20sec was markedwith a negative engagement intention label.
Final-ly, the remainder of the trace (from ??
until theface disappeared) was discarded, as the user wasactively engaged with the system during this time.?
if the face was never engaged in interaction (i.e.
aperson was just passing by), the entire trace waslabeled with a negative engagement intention.Note that in training the models described below weused these automatic labels, which are not entirely accu-rate: they include a small number of false-positives, asdiscussed above.
However, for evaluation purposes, weused the corrected labels (no false-positives).6.2 ModelsTo review, the task at hand is to learn a model for pre-dicting engagement intentions, based on informationthat can be extracted at runtime from face traces, includ-ing spatiotemporal trajectory and cues about attention.We cast this problem as a frame-by-frame binary classi-fication task: at each frame, the model must classifyeach visible face as either intending to engage or not.We used a maximum entropy model to make this pre-diction:?
??
?
=1?(?)???
??
?
??(?
)?The key role in the proposed maximum entropymodel is played by the set of features ??(?
), which mustcapture cues that are relevant for detecting an engage-ment intention.
We designed several subsets of features,summarized in Table 2.
The location subset, loc, in-cludes the x and y location of the detected face in thevisual scene, and the width and height of the face region,which indirectly reflect the proximity of the agent.
Thesecond feature subset, loc+ff, also includes a probabilityscore (and a binarized version of it) produced by theface detector which reflects the confidence that the faceis frontal and thus provides an automatic measure of thefocus-of-attention of the agent.
Apart from these auto-Table 1.
Corpus statistics.Side Front TotalSize (hours:minutes) 83:16 75:15 158:32# face traces 2025 1249 3274# engaged% engaged723.55%745.92%1464.46%# false-positive engaged% false-positive engaged10.04%50.40%60.18%# not-engaged% not-engaged195396.45%117594.08%312895.54%248matically generated attention features, we also experi-mented with a manually annotated binary attentionscore, attn.
The attention of each detected face was ma-nually tagged throughout the entire dataset.
This infor-mation is not available to the system at runtime; we useit only to identify an upper performance baseline.The maximum entropy model is not temporally struc-tured.
The temporal structure of the spatial and atten-tional trajectory is captured via a set of additional fea-tures, derived as follows.
Given an existing feature f, wecompute a set of trajectory features traj.w(f) by accumu-lating aggregate statistics for the feature f over a pastwindow of size w frames.
We explored windows of size5, 10, 20, 30.
For continuous features, the trajectorystatistics include the min, max, mean, and variance ofthe features in the specified window.
In addition, weperformed a linear and a quadratic fit of f in this window,and used the resulting coefficients (2 for the linear fitand 3 for the quadratic fit) as features (see the examplein Figure 4).
For the binary features, the trajectory sta-tistics include the number and proportion of times thefeature had a value of 1 in the given window, and thenumber of frames since the feature last had a value of 1.7 Experimental ResultsWe trained and evaluated (using a 10-fold cross-validation process) a set of models for each of the twosystem orientations shown in Figure 3 and for each ofthe 5 feature subsets shown in Table 2.
The results onthe per-frame classification task, including the ROCcurves for the different models are presented and dis-cussed in more detail in Appendix A.At runtime, the system uses these frame-based mod-els to predict across time the likelihood that a givenagent intends to engage (see Figure 5).
In this context,an evaluation that counts the errors per person (i.e., pertrace), rather than errors per frame is more informative.Furthermore, since early detection is important for sup-porting a natural engagement process, an informativeevaluation should also capture how soon a model candetect a positive engagement intention (see Figure 5).Making decisions about an agent?s engagement in-tentions typically involves comparing the probability ofengagement against a preset threshold.
Given a thre-shold, we can compute for each model the number offalse-positives at the trace level: if the prediction ex-ceeds the threshold at any point in the trace, we considerthat a positive detection.
We note that, if we aim todetect people who will actually engage, there are nofalse negatives at the trace level.
The system can use themachine learned models in conjunction with the pre-vious heuristic (a user is detected standing in front ofthe system), to eventually detect when people engage.Also, given a threshold, we can identify how early amodel can correctly detect the intention to engage(compared to the existing F-formation heuristic thatdefined the moment of engagement in the training data).These durations are illustrated for a threshold of 0.5 inFigure 5, and are referred to in the sequel as early detec-tion time.
By varying the threshold between 0 and 1, wecan obtain a profile that links the false-positive rate atthe trace level to how early the system can detect en-gagement, i.e.
to the mean early detection time.Figure 6 shows the false-positive rate as a function ofthe mean early detection time for models trained usingeach of the five feature subsets shown in Table 2, in theside orientation.
The model that uses only location in-formation (including the size of the face and proximity)performs worst.
Adding automatically extracted infor-mation about attention leads only to a marginal im-provement.
However, adding information about the tra-Feature sets Description [total # of features in set]Loc location features: x, y, width and height [4]loc+fflocation features plus a confidence score indicat-ing whether the face is frontal (ff), as well as abinary version of this score (ff=1) [6]traj(loc)location features plus trajectory of location fea-tures over windows of 5, 10, 20, 30 frames [118]traj(loc+ff)location and face frontal features, as well astrajectory of location and of face-frontal featuresover windows of 5, 10, 20, 30 frames [172]traj(loc+attn)location and manually labeled attention features,as well as trajectory of location and of attentionover windows of 5, 10, 20, 30 frames [133]Table 2.
Feature sets for detecting engagement intention.0 10 20 30 40 5010020030040050060030 frame windowcurrent framexFigure 4.
Trajectory features extracted by fitting linear andquadratic functions.Figure 5.
Example predictions for three different models.0 5 10 1500.51050100064000.51xwidthfrontaltraj(loc+ff)traj(loc)loc early detection time = 10.4 sec5.4 sec4.0 sec249jectory of location and of attention, leads to larger cu-mulative gains.
Adding the more accurate (manuallytagged) information about attention yields the best mod-el.
The relative performance of these models (which canbe observed at the frame-level in Appendix A) confirmsour expectations and the importance of trajectory fea-tures (both spatial and attentional) in detecting engage-ment intentions.
The results also indicate that the differ-ences, and hence the importance of these features, arelarger when trying to detect engagement early on, i.e.
atlarger early detection times.
Tables 3 and 4 further high-light these differences.
For instance, when detectingengagement intentions at a mean early detection above 3seconds, the model that uses trajectory information,traj(loc+ff), decreases the false positive rate by a factor of3 compared to the location-only model.Figure 7 and Tables 5 and 6 show the results for thefront orientation.
The relative trends are similar to thoseobserved in the side orientation, highlighting again theimportance of trajectory features.
At the same time, themodels are performing slightly worse in absolute terms,which is consistent with the increased difficulty of thetask.
Several contributing factors can be identified inFigure 3: people may simply pass by in closer proximityto the system; people who come from the corridor aregenerally frontally oriented towards the system, makingfrontal face cues less informative; and finally, peoplewho will engage need to deviate less from the regulartrajectory of people who are just passing by.Next, we review how well the models trained gene-ralize across the two different setups, by evaluating thetrajectory models traj(loc+ff) across the two datasets.
Theresults indicate that the models are attuned to the datasetthey are trained on (see Figure 7).
As we discussed ear-lier, we expect this result given the different geometryof the relative trajectories of engagement in the twoorientations.
These results highlight the importance oflearning in situ, and show that the proposed approachcan be used to learn the specific patterns of engagementin a given environment automatically, without explicitdeveloper supervision.Finally, we performed an error analysis.
We focusedon the side orientation and visually inspected the 79(4%) false-positive errors committed by the traj(loc+ff)ModelEarly detection timeFP=2.5% FP=5% FP=10% FP=20%loc 1.14 1.97 2.29 2.92loc+ff 1.70 2.25 2.74 3.18traj(loc) 1.93 2.57 3.13 3.66traj(loc+ff) 1.99 2.64 3.44 4.02traj(loc+attn) 1.97 2.47 3.52 4.15ModelEarly detection timeFP=2.5% FP=5% FP=10% FP=20%loc 2.18 2.72 3.09 3.59loc+ff 2.25 2.74 3.08 3.63traj(loc) 2.51 3.03 3.53 4.07traj(loc+ff) 2.68 3.20 3.68 4.22traj(loc+attn) 3.08 3.52 4.13 4.49Figure 6.
False-positives vs. early detection time (side).FalsepositivesMean early detection time (seconds)FalsepositivesMean early detection time (seconds)0 1 2 3 4 50%10%20%30%locloc+fftraj(loc)traj(loc+ff)traj(loc+attn)0 1 2 3 4 50%10%20%30%locloc+fftraj(loc)traj(loc+ff)raj(loc+attn)Table 3.
*False-positive rate at different EDT (side) Table 5.
*False-positive rate at different EDT (front)Table 4.
*Early detection times at different FP rates (side).
Table 6 * Early detection times at different FP rates (front).Figure 7.
False-positives vs. early detection time (front).ModelFalse positive rateEDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4loc 0.31% 1.6% 4.3% 9.4% 18.4% 32.6%loc+ff 0.31% 1.5% 4.1% 8.7% 18.3% 28.6%traj(loc) 0.31% 1.1% 2.6% 4.8% 9.3% 18.6%traj(loc+ff) 0.15% 0.9% 2.0% 4.0% 7.1% 14.3%traj(loc+attn) 0.26% 0.6% 1.1% 2.2% 5.1% 8.9%ModelFalse positive rateEDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4loc 2.3% 5.8% 11.3% 23.0% 35.2% 44.5%loc+ff 1.6% 3.7% 7.3% 15.8% 28.5% 41.7%traj(loc) 1.1% 3.1% 4.7% 8.2% 15.6% 36.8%traj(loc+ff) 1.2% 2.7% 4.7% 7.2% 10.9% 19.8%traj(loc+attn) 0.8% 2.9% 5.4% 5.4% 10.3% 16.1%*shaded cells in Tables 3-6 show statistically significant improvements in performance (p<0.05) over the corresponding model that uses the immediately previousfeature set (e.g.
the cell right above).
The traj(loc), traj(loc+ff), traj(loc+attn) always statistically significantly (p<0.05) improve upon the loc models250model when using a threshold corresponding to a meanearly detection time of 3 seconds.
This analysis indi-cates that in 22 out of these 79 errors (28%) the persondid actually exhibit behaviors consistent with an inten-tion to engage the system, such as stopping by or turn-ing around after passing the system, and approachingand maintaining sustained attention for a significantamount of time.
These cases represent false-negativescommitted by our conservative F-formation heuristicwith respect to engagement intention; the user did notapproach close enough for the system to trigger en-gagement.
The actual false-positive rate of the trainedmodel is therefore 2.9% rather than 4%.
The system wasable to correctly identify these cases because the beha-vioral patterns are similar to the ones exhibited bypeople who did approach close enough for the heuristicdetector to fire.
We plan to assess the false-negative rateof the current heuristic more closely and explore howmany false negatives are actually recovered by thetrained model.
This analysis will require that multiplejudges assess engagement intentions on all 3274 traces.8 Summary and Future WorkWe described an approach to learning engagement in-tentions in a situated conversational system.
The pro-posed models fit into a larger framework for supportingmultiparty, situated engagement and open-world dialog(Bohus and Horvitz, 2009a; 2009b).
Experimental re-sults indicate that a system using the proposed approachcan learn to detect engagement intentions at low falsepositive rates up to 3-4 seconds prior to the actual mo-ment of engagement.
The models leverage features thatcapture spatiotemporal and attentional cues that aretuned to the specifics of the physical environment inwhich the system operates.
Furthermore, the models canbe trained in previously unseen environments, withoutany explicit developer supervision.We believe the methods and results describedrepresent a first step towards supporting fluid, naturalengagement in open-world interaction.
Numerous chal-lenges remain.
While we confirmed the importance ofspatiotemporal and attentional features in detecting en-gagement intentions, we believe that leveraging addi-tional and more accurate sensory information (e.g.
bodypose, eye gaze, more accurate depth information, agentidentity coupled with longer term memory features)may improve performance.
Secondly, while the currentmodels where trained in a batch fashion, the proposedmethod naturally lends itself to an online approach,where the system starts with a prior model for detectingengagement intentions, and refines this model online.More importantly, rather than just learning to detectengagement intentions, we plan to focus on the moregeneral problem of controlling the engagement process:how should the system time its actions (i.e.
gaze andsustained attention, smiles, greeting, etc.)
to create natu-ral, fluid engagements in the open world.
Introducingmobility to dialog systems brings yet another interestingdimension to this problem: how can a mobile system,such as a robot, detect engagement intentions and re-spond to support a natural engagement process?
Webelieve that there is great opportunity to address thesechallenges by learning predictive models from data.ReferencesM.
Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-bridge University Press, New YorkD.
Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-lenges, Directions and Prototype, to appear in KRPD?09,Pasadena, CAD.
Bohus and E. Horvitz, 2009b, Computational Models forMultiparty Engagement in Open-World Dialog, submittedto SIGdial?09, London, UK.E.
Goffman, 1963, Behaviour in public places: notes on thesocial order of gatherings, The Free Press, New YorkE.T.
Hall, 1966, The Hidden Dimension: man?s use of space inpublic and private, New York: Doubleday.A.
Kendon, 1990a, A description of some human greetings,Conducting Interaction: Patterns of behavior in focused en-counters, Studies in International Sociolinguistics, Cam-bridge University PressA.
Kendon, 1990b, Spatial organization in social encounters:the F-formation system, Conducting Interaction: Patterns ofbehavior in focused encounters, Studies in InternationalSociolinguistics, Cambridge University PressM.P.
Michalowski, S. Sabanovic, and R. Simmons, A spatialmodel of engagement for a social robot, in 9th IEEE Work-shop on Advanced Motion Control, pp.
762-767C.
Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005a,A model of attention and interest using gaze behavior, Lec-ture Notes in Computer Science, pp.
229-240.C.
Peters, 2005b, Direction of Attention Perception for Con-versation Initiation in Virtual Environments, in IntelligentVirtual Agents, 2005, pp.
215-228.C.L.
Sidner, C.D.
Kidd, C. Lee, and N. Lesh, 2004, Where toLook: A Study of Human-Robot Engagement, IUI?2004, pp.78-84, Madeira, PortugalC.L.
Sidner, C. Lee, C.D.
Kidd, N. Lesh, and C. Rich, 2005,Explorations in engagement for humans and robots, Artifi-cial Intelligence, 166 (1-2), pp.
140-164R.
Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001,Eye gaze patterns in conversations: there is more to con-versational agents than meets the eyes, CHI?01Figure 7.
Model evaluation across orientations.0 1 2 3 4 50%10%20%30%traj(loc+ff) trained on side datatraj(loc+ff) trained on front dataFalsepositivesFalsepositivesMean early detection time0 1 2 3 4 50%10%20%30%traj(loc+ff) trained on front datatraj(loc+ff) traine  on side dataMean early detection timeEvaluation on side data Evaluation on front data2510 0.2 0.4 0.6 0.8 100.20.40.60.81locloc+fftraj(loc)traj(loc+ff)traj(loc+attn)0 0.2 0.4 0.6 0.8 100.20.40.60.81locloc+fftraj(loc)traj(loc+ff)traj(loc+a tn)Appendix A. Per-frame evaluation of maximum entropy models for detecting engagement intentionsModel Avg.
log-likelihood Hard errorBase Train CV Base Train CVloc -0.1651 -0.1222 -0.1259 3.91% 3.22% 3.25%loc+ff -0.1651 -0.0962 -0.0984 3.91% 3.01% 3.07%traj(loc) -0.1651 -0.0947 -0.1073 3.91% 2.88% 3.06%traj(loc+ff) -0.1651 -0.0836 -0.0904 3.91% 2.69% 2.85%traj(loc+attn) -0.1651 -0.0765 -0.0810 3.91% 2.47% 2.56%Figure 1.
Per-frame ROC for side orientation modelsTruepositives(sensitivity)False positives (1-specificity)Figure 2.
Per-frame ROC for front orientation modelsFalse positives (1-specificity)Truepositives(sensitivity)Model Avg.
log-likelihood Hard errorBase Train CV Base Train CVloc -0.1875 -0.1451 -0.1498 4.63% 4.58% 4.72%loc+ff -0.1875 -0.1326 -0.1392 4.63% 4.22% 4.39%traj(loc) -0.1875 -0.1262 -0.1338 4.63% 3.99% 4.24%traj(loc+ff) -0.1875 -0.1159 -0.1298 4.63% 3.91% 4.38%traj(loc+attn) -0.1875 -0.1150 -0.1267 4.63% 4.04% 4.47%Table 1.
Baseline, training-set and cross-validationperformance (data average log-likelihood and classifi-cation error) for side orientation modelsTable 2.
Baseline, training-set and cross-validationperformance (data average log-likelihood and classifi-cation error) for front orientation models252
