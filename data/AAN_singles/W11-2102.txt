Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12?21,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsA Lightweight Evaluation Framework for Machine Translation ReorderingDavid Talbot1 and Hideto Kazawa2 and Hiroshi Ichikawa2Jason Katz-Brown2 and Masakazu Seno2 and Franz J. Och11 Google Inc. 2 Google Japan1600 Amphitheatre Parkway Roppongi Hills Mori TowerMountain View, CA 94043 6-10-1 Roppongi, Tokyo 106-6126{talbot, och}@google.com {kazawa, ichikawa}@google.com{jasonkb, seno}@google.comAbstractReordering is a major challenge for machinetranslation between distant languages.
Recentwork has shown that evaluation metrics thatexplicitly account for target language word or-der correlate better with human judgments oftranslation quality.
Here we present a simpleframework for evaluating word order indepen-dently of lexical choice by comparing the sys-tem?s reordering of a source sentence to ref-erence reordering data generated from manu-ally word-aligned translations.
When used toevaluate a system that performs reordering asa preprocessing step our framework allows theparser and reordering rules to be evaluated ex-tremely quickly without time-consuming end-to-end machine translation experiments.
Anovelty of our approach is that the translationsused to generate the reordering reference dataare generated in an alignment-oriented fash-ion.
We show that how the alignments aregenerated can significantly effect the robust-ness of the evaluation.
We also outline someways in which this framework has allowed ourgroup to analyze reordering errors for Englishto Japanese machine translation.1 IntroductionStatistical machine translation systems can performpoorly on distant language pairs such as Englishand Japanese.
Reordering errors are a major sourceof poor or misleading translations in such systems(Isozaki et al, 2010).
Unfortunately the stan-dard evaluation metrics used by the statistical ma-chine translation community are relatively insensi-tive to the long-distance reordering phenomena en-countered when translating between such languages(Birch et al, 2010).The ability to rapidly evaluate the impact ofchanges on a system can significantly accelerate theexperimental cycle.
In a large statistical machinetranslation system, we should ideally be able to ex-periment with separate components without retrain-ing the complete system.
Measures such as per-plexity have been successfully used to evaluate lan-guage models independently in speech recognitioneliminating some of the need for end-to-end speechrecognition experiments.
In machine translation,alignment error rate has been used with some mixedsuccess to evaluate word-alignment algorithms butno standard evaluation frameworks exist for othercomponents of a machine translation system (Fraserand Marcu, 2007).Unfortunately, BLEU (Papineni et al, 2001) andother metrics that work with the final output of a ma-chine translation system are both insensitive to re-ordering phenomena and relatively time-consumingto compute: changes to the system may require therealignment of the parallel training data, extractionof phrasal statistics and translation of a test set.
Astraining sets grow in size, the cost of end-to-end ex-perimentation can become significant.
However, it isnot clear that measurements made on any single partof the system will correlate well with human judg-ments of the translation quality of the whole system.Following Collins et al (2005a) and Wang (2007),Xu et al (2009) showed that when translating fromEnglish to Japanese (and to other SOV languagessuch as Korean and Turkish) applying reordering as12a preprocessing step that manipulates a source sen-tence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machinetranslation systems.
This result is corroborated byBirch et al (2009) whose results suggest that bothphrase-based and hierarchical translation systemsfail to capture long-distance reordering phenomena.In this paper we describe a lightweight frameworkfor measuring the quality of the reordering compo-nents in a machine translation system.
While ourframework can be applied to any translation sys-tem in which it is possible to derive a token-levelalignment from the input source tokens to the out-put target tokens, it is of particular practical interestwhen applied to a system that performs reorderingas a preprocessing step (Xia and McCord, 2004).
Inthis case, as we show, it allows for extremely rapidand sensitive analysis of changes to parser, reorder-ing rules and other reordering components.In our framework we evaluate the reordering pro-posed by a system separately from its choice of tar-get words by comparing it to a reference reorderingof the sentence generated from a manually word-aligned translation.
Unlike previous work (Isozakiet al, 2010), our approach does not rely on the sys-tem?s output matching the reference translation lexi-cally.
This makes the evaluation more robust as theremay be many ways to render a source phrase in thetarget language and we would not wish to penalizeone that simply happens not to match the reference.In the next section we review related work onreordering for translation between distant languagepairs and automatic approaches to evaluating re-ordering in machine translation.
We then describeour evaluation framework including certain impor-tant details of how our reference reorderings werecreated.
We evaluate the framework by analyz-ing how robustly it is able to predict improvementsin subjective translation quality for an English toJapanese machine translation system.
Finally, wedescribe ways in which the framework has facili-tated development of the reordering components inour system.2 Related Work2.1 Evaluating ReorderingThe ability to automatically evaluate machine trans-lation output has driven progress in statistical ma-chine translation; however, shortcomings of thedominant metric, BLEU (Papineni et al, 2001) , par-ticularly with respect to reordering, have long beenrecognized (Callison-burch and Osborne, 2006).Reordering has also been identified as a major fac-tor in determining the difficulty of statistical ma-chine translation between two languages (Birch etal., 2008) hence BLEU scores may be most unreli-able precisely for those language pairs for which sta-tistical machine translation is most difficult (Isozakiet al, 2010).There have been many results showing that met-rics that account for reordering are better correlatedwith human judgements of translation quality (Lavieand Denkowski, 2009; Birch and Osborne, 2010;Isozaki et al, 2010).
Examples given in Isozaki etal.
(2010) where object and subject arguments arereversed in a Japanese to English statistical machinetranslation system demonstrate how damaging re-ordering errors can be and it should therefore notcome as a surprise that word order is a strong pre-dictor of translation quality; however, there are otheradvantages to be gained by focusing on this specificaspect of the translation process in isolation.One problem for all automatic evaluation metricsis that multiple equally good translations can be con-structed for most input sentences and typically ourreference data will contain only a small fraction ofthese.
Equally good translations for a sentence maydiffer both in terms of lexical choice and word or-der.
One of the potential advantages of designing ametric that looks only at word order, is that it may,to some extent, factor out variability along the di-mension of the lexical choice.
Previous work on au-tomatic evaluation metrics that focus on reordering,however, has not fully exploited this.The evaluation metrics proposed in Isozaki et al(2010) compute a reordering score by comparingthe ordering of unigrams and bigrams that appearin both the system?s translation and the reference.These scores are therefore liable to overestimatethe reordering quality of sentences that were poorlytranslated.
While Isozaki et al (2010) does propose13a work-around to this problem which combines thereordering score with a lexical precision term, thisclearly introduces a bias in the metric whereby poortranslations are evaluated primarily on their lexicalchoice and good translations are evaluated more onthe basis of their word order.
In our experienceword order is particularly poor in those sentencesthat have the lowest lexical overlap with referencetranslations; hence we would like to be able to com-pute the quality of reordering in all sentences inde-pendently of the quality of their lexical choice.Birch and Osborne (2010) are closer to our ap-proach in that they use word alignments to induce apermutation over the source sentence.
They com-pare a source-side permutation generated from aword alignment of the reference translation with onegenerated from the system?s using various permuta-tion distances.
However, Birch and Osborne (2010)only demonstrate that these metrics are correlatedwith human judgements of translation quality whencombined with BLEU score and hence take lexicalchoice into account.Birch et al (2010) present the only results weare aware of that compute the correlation be-tween human judgments of translation quality anda reordering-only metric independently of lexicalchoice.
Unfortunately, the experimental set-up thereis somewhat flawed.
The authors ?undo?
reorderingsin their reference translations by permuting the ref-erence tokens and presenting the permuted transla-tions to human raters.
While many machine trans-lation systems (including our own) assume that re-ordering and translation can be factored into sepa-rate models, e.g.
(Xia and McCord, 2004), and per-form these two operations in separate steps, the lat-ter conditioned on the former, Birch et al (2010) aremaking a much stronger assumption when they per-form these simulations: they are assuming that lexi-cal choice and word order are entirely independent.It is easy to find cases where this assumption doesnot hold and we would in general be very surprisedif a similar change in the reordering component inour system did not also result in a change in the lex-ical choice of the system; an effect which their ex-periments are unable to model.Another minor difference between our evaluationframework and (Birch et al, 2010) is that we usea reordering score that is based on the minimumnumber of chunks into which the candidate and ref-erence permutations can be concatenated similar tothe reordering component of METEOR (Lavie andDenkowski, 2009).
As we show, this is better cor-related with human judgments of translation qualitythan Kendall?s ?
.
This may be due to the fact thatit counts the number of ?jumps?
a human reader hasto make in order to parse the system?s order if theywish to read the tokens in the reference word order.Kendall?s ?
on the other hand penalizes every pairof words that are in the wrong order and hence hasa quadratic (all-pairs) flavor which in turn might ex-plain why Birch et al (2010) found that the square-root of this quantity was a better predictor of trans-lation quality.2.2 Evaluation Reference DataTo create the word-aligned translations from whichwe generate our reference reordering data, we useda novel alignment-oriented translation method.
Themethod (described in more detail below) seeksto generate reference reorderings that a machinetranslation system might reasonably be expected toachieve.
Fox (2002) has analyzed the extent towhich translations seen in a parallel corpus can bebroken down into clean phrasal units: they foundthat most sentence pairs contain examples of re-ordering that violate phrasal cohesion, i.e.
the cor-responding words in the target language are notcompletely contiguous or solely aligned to the cor-responding source phrase.
These reordering phe-nomena are difficult for current statistical transla-tion models to learn directly.
We therefore deliber-ately chose to create reference data that avoids thesephenomena as much as possible by having a singleannotator generate both the translation and its wordalignment.
Our word-aligned translations are cre-ated with a bias towards simple phrasal reordering.Our analysis of the correlation between reorder-ing scores computed on reference data created fromsuch alignment-oriented translations with scorescomputed on references generated from standardprofessional translations of the same sentences sug-gests that the alignment-oriented translations aremore useful for evaluating a current state-of-the-artsystem.
We note also that while prior work has con-jectured that automatically generated alignments area suitable replacement for manual alignments in the14context of reordering evaluation (Birch et al, 2008),our results suggest that this is not the case at least forthe language pair we consider, English-Japanese.3 A Lightweight Reordering EvaluationWe now present our lightweight reordering evalu-ation framework; this consists of (1) a method forgenerating reference reordering data from manualword-alignments; and (2) a reordering metric forscoring a sytem?s proposed reordering against thisreference data; and (3) a stand-alone evaluation tool.3.1 Generating Reference Reordering DataWe follow Birch and Osborne (2010) in using ref-erence reordering data that consists of permuationsof source sentences in a test set.
We generate thesefrom word alignments of the source sentences toreference translations.
Unlike previous work, how-ever, we have the same annotator generate both thereference translation and the word alignment.
Wealso explicitly encourage the translators to generatetranslations that are easy to align even if this doesresult in occasionally unnatural translations.
For in-stance in English to Japanese translation we requirethat all personal pronouns are translated; these areoften omitted in natural Japanese.
We insist thatall but an extremely small set of words (articles andpunctuation for English to Japanese) be aligned.
Wealso disprefer non-contiguous alignments of a sin-gle source word and require that all target words bealigned to at least one source token.
In Japanesethis requires deciding how to align particles thatmark syntactic roles; we choose to align these to-gether with the content word (jiritsu-go) of the cor-responding constituent (bunsetsu).
Asking annota-tors to translate and perform word alignment on thesame sentence in a single session does not necessar-ily increase the annotation burden over stand-aloneword alignment since it encourages the creation ofalignment-friendly translations which can be alignedmore rapidly.
Annotators need little special back-ground or training for this task, as long as they canspeak both the source and target languages.To generate a permutation from word alignmentswe rank the source tokens by the position of the firsttarget token to which they are aligned.
If multiplesource tokens are aligned to a single target wordor span we ignore the ordering within these sourcespans; this is indicated by braces in Table 2.
Weplace unaligned source words immediately beforethe next aligned source word or at the end of thesentence if there is none.
Table 2 shows the ref-erence reordering derived from various translationsand word alignments.3.2 Fuzzy Reordering ScoreTo evaluate the quality of a system?s reorderingagainst this reference data we use a simple reorder-ing metric related to METEOR?s reordering compo-nent (Lavie and Denkowski, 2009) .
Given the refer-ence permutation of the source sentence ?ref and thesystem?s reordering of the source sentence ?sys ei-ther generated directly by a reordering component orinferred from the alignment between source and tar-get phrases used in the decoder, we align each wordin ?sys to an instance of itself in ?ref taking the firstunmatched instance of the word if there is more thanone.
We then define C to be the number chunks ofcontiguously aligned words.
If M is the number ofwords in the source sentence then the fuzzy reorder-ing score is computed as,FRS(?sys, ?ref) = 1?C ?
1M ?
1.
(1)This metric assigns a score between 0 and 1 where1 indicates that the system?s reordering is identicalto the reference.
C has an intuitive interpretation asthe number of times a reader would need to jump inorder to read the system?s reordering of the sentencein the order proposed by the reference.3.3 Evaluation ToolWhile the framework we propose can be applied toany machine translation system in which a reorder-ing of the source sentence can be inferred from thetranslation process, it has proven particularly use-ful applied to a system that performs reordering asa separate preprocessing step.
Such pre-orderingapproaches (Xia and McCord, 2004; Collins et al,2005b) can be criticized for greedily committing toa single reordering early in the pipeline but in prac-tice they have been shown to perform extremely wellon language pairs that require long distance reorder-ing and have been successfully combined with othermore integrated reordering models (Xu et al, 2009).15The performance of a parser-based pre-orderingcomponent is a function of the reordering rules andparser; it is therefore desirable that these can be eval-uated efficiently.
Both parser and reordering rulesmay be evaluated using end-to-end automatic met-rics such as BLEU score or in human evaluations.Parsers may also be evaluated using intrinsic tree-bank metrics such as labeled accuracy.
Unfortu-nately these metrics are either expensive to computeor, as we show, unpredictive of improvements in hu-man perceptions of translation quality.Having found that the fuzzy reordering score pro-posed here is well-correlated with changes in humanjudgements of translation quality, we established astand-alone evaluation tool that takes a set of re-ordering rules and a parser and computes the re-ordering scores on a set of reference reorderings.This has become the most frequently used methodfor evaluating changes to the reordering componentin our system and has allowed teams working onparsing, for instance, to contribute significant im-provements quite independently.4 Experimental Set-upWe wish to determine whether our evaluation frame-work can predict which changes to reordering com-ponents will result in statistically significant im-provements in subjective translation quality of theend-to-end system.
To that end we created a num-ber of systems that differ only in terms of reorder-ing components (parser and/or reordering rules).
Wethen analyzed the corpus- and sentence-level corre-lation of our evaluation metric with judgements ofhuman translation quality.Previous work has compared either quite separatesystems, e.g.
(Isozaki et al, 2010), or systems thatare artificially different from each other (Birch et al,2010).
There has also been a tendency to measurecorpus-level correlation.
We are more interested incomparing systems that differ in a realistic mannerfrom one another as would typically be required indevelopment.
We also believe sentence-level cor-relation is more important than corpus-level corre-lation since good sentence-level correlation impliesthat a metric can be used for detailed analysis of asystem and potentially to optimize it.4.1 SystemsWe carried out all our experiments using a state-of-the-art phrase-based statistical English-to-Japanesemachine translation system (Och, 2003).
Dur-ing both training and testing, the system reorderssource-language sentences in a preprocessing stepusing a set of rules written in the framework pro-posed by (Xu et al, 2009) that reorder an Englishdependency tree into target word order.
During de-coding, we set the reordering window to 4 words.In addition to the regular distance distortion model,we incorporate a maximum entropy based lexical-ized phrase reordering model (Zens and Ney, 2006).For parallel training data, we use an in-house collec-tion of parallel documents.
These come from vari-ous sources with a substantial portion coming fromthe web after using simple heuristics to identify po-tential document pairs.
We trained our system onabout 300 million source words.The reordering rules applied to the English de-pendency tree define a precedence order for the chil-dren of each head category (a coarse-grained part ofspeech).
For example, a simplified version of theprecedence order for child labels of a verbal headHEADVERB is: advcl, nsubj, prep, [other children],dobj, prt, aux, neg, HEADVERB, mark, ref, compl.The dependency parser we use is an implementa-tion of a transition-based dependency parser (Nivre,2008).
The parser is trained using the averaged per-ceptron algorithm with an early update strategy asdescribed in Zhang and Clark (2008).We created five systems using different parsers;here targeted self-training refers to a training pro-cedure proposed by Katz-Brown et al (2011) thatuses our reordering metric and separate reference re-ordering data to pick parses for self-training: an n-best list of parses is generated for each English sen-tence for which we have reference reordering dataand the parse tree that results in the highest fuzzyreordering score is added to our parser?s training set.Parsers P3, P4 and P5 differ in how that frameworkis applied and how much data is used.?
P1 Penn Treebank, perceptron, greedy search?
P2 Penn Treebank, perceptron, beam search?
P3 Penn Treebank, perceptron, beam search,targeted self-training on web data16?
P4 Penn Treebank, perceptron, beam search,targeted self-training on web data?
P5 Penn Treebank, perceptron, beam search,targeted self-training on web data, case insen-sitiveWe also created five systems using the fifth parser(P5) but with different sets of reordering rules:?
R1 No reordering?
R2 Reverse reordering?
R3 Head final reordering with reverse reorder-ing for words before the head?
R4 Head final reordering with reverse reorder-ing for words after the head?
R5 Superset of rules from (Xu et al, 2009)Reverse reordering places words in the reverse of theEnglish order.
Head final reordering moves the headof each dependency after all its children.
Rules in R3and R4 overlap significantly with the rules for nounand verb subtrees respectively in R5.
Otherwise allsystems were identical.
The rules in R5 have beenextensively hand-tuned while R1 and R2 are rathernaive.
System P5R5 was our best performing systemat the time these experiments were conducted.We refer to systems by a combination of parserand reordering rules set identifiers, for instance, sys-tem P2R5, uses parser P2 with reordering rules R5.We conducted two subjective evaluations in whichbilingual human raters were asked to judge trans-lations on a scale from 0 to 6 where 0 indicatesnonsense and 6 is perfect.
The first experiment(Parsers) contrasted systems with different parsersand the second (Rules) varied the reordering rules.In each case three bilingual evaluators were shownthe source sentence and the translations produced byall five systems.4.2 Meta-analysisWe perform a meta-analysis of the following metricsand the framework by computing correlations withthe results of these subjective evaluations of transla-tion quality:1.
Evaluation metrics: BLEU score on final trans-lations, Kendall?s ?
and fuzzy reordering scoreon reference reordering data2.
Evaluation data: both manually-generated andautomatically-generated word alignments onboth standard professional and alignment-oriented translations of the test sentencesThe automatic word alignments were generated us-ing IBM Model 1 in order to avoid directional biasesthat higher-order models such as HMMs have.Results presented in square parentheses are 95percent confidence intervals estimated by bootstrapresampling on the test corpus (Koehn, 2004).Our test set contains 500 sentences randomlysampled from the web.
We have both professionaland alignment-friendly translations for these sen-tences.
We created reference reorderings for thisdata using the method described in Section 3.1.The lack of a broad domain and publically avail-able Japanese test corpus makes the use of this non-standard test set unfortunately unavoidable.The human raters were presented with the sourcesentence, the human reference translation and thetranslations of the various systems simultaneously,blind and in a random order.
Each rater was allowedto rate no more than 3 percent of the sentences andthree ratings were elicited for each sentence.
Rat-ings were a single number between 0 and 6 where 0indicates nonsense and 6 indicates a perfectly gram-matical translation of the source sentence.5 ResultsTable 2 shows four reference reorderings generatedfrom various translations and word alignments.
Theautomatic alignments are significantly sparser thanthe manual ones but in these examples the refer-ence reorderings still seem reasonable.
Note how thealignment-oriented translation includes a pronoun(translation for ?I?)
that is dropped in the slightlymore natural standard translation to Japanese.Table 1 shows the human judgements of transla-tion quality for the 10 systems (note that P5R5 ap-pears in both experiments but was scored differentlyas human judgments are affected by which othertranslations are present in an experiment).
There is aclear ordering of the systems in each experiment and171.
Parsers Subjective Score (0-6) 2.
Rules Subjective Score (0-6)P1R5 2.173 [2.086, 2.260] P5R1 1.258 [1.191, 1.325]P2R5 2.320 [2.233, 2.407] P5R2 1.825 [1.746, 1.905]P3R5 2.410 [2.321, 2.499] P5R3 1.849 [1.767, 1.931]P4R5 2.453 [2.366, 2.541] P5R4 2.205 [2.118, 2.293]P5R5 2.501 [2.413, 2.587] P5R5 2.529 [2.441, 2.619]Table 1: Human judgements of translation quality for 1.
Parsers and 2.
Rules.Metric Sentence-level correlationr ?Fuzzy reordering 0.435 0.448Kendall?s ?
0.371 0.450BLEU 0.279 0.302Table 6: Pearson?s correlation (r) and Spearman?s rankcorrelation (?)
with subjective translation quality atsentence-level.we see that both the choice of parser and reorderingrules clearly effects subjective translation quality.We performed pairwise significance tests usingbootstrap resampling for each pair of ?improved?systems in each experiment.
Tables 3, 4 and 5shows which pairs were judged to be statisticallysignificant improvements at either 95 or 90 percentlevel under the different metrics.
These tests werecomputed on the same 500 sentences.
All pairsbut one are judged to be statistically significant im-provements in subjective translation quality.
Sig-nificance tests performed using the fuzzy reorder-ing metric are identical to the subjective scores forthe Parsers experiment but differ on one pairwisecomparison for the Rules experiment.
According toBLEU score, however, none of the parser changesare significant at the 95 percent level and only onepairwise comparison (between the two most differ-ent systems) was significant at the 90 percent level.BLEU score appears more sensitive to the largerchanges in the Rules experiment but is still in dis-agreement with the results of the human evaluationon four pairwise comparisons.Table 6 shows the sentence-level correlation ofdifferent metrics with human judgments of transla-tion quality.
Here both the fuzzy reordering scoreand Kendall?s ?
are computed on the referencereordering data generated as described in Section3.1.
Both metrics are computed by running ourTranslation Alignment Sentence-levelr ?Alignment-oriented Manual 0.435 0.448Alignment-oriented Automatic 0.234 0.252Standard Manual 0.271 0.257Standard Automatic 0.177 0.159Table 7: Pearson?s correlation (r) and Spearman?s rankcorrelation (?)
with subjective translation quality at thesentence-level for different types of reordering referencedata: (i) alignment-oriented translation vs. standard, (ii)manual vs. automatic alignment.lightweight evaluation tool and involve no transla-tion whatsoever.
These lightweight metrics are alsomore correlated with subjective quality than BLEUscore at the sentence level.Table 7 shows how the correlation between fuzzyreordering score and subjective translation qualitydegrades as we move from manual to automaticalignments and from alignment-oriented translationsto standard ones.
The automatically aligned refer-ences, in particular, are less correlated with subjec-tive translation scores then BLEU; we believe thismay be due to the poor quality of word alignmentsfor languages such as English and Japanese due tothe long-distance reordering between them.Finally we present some intrinsic evaluation met-rics for the parsers used in the first of our experi-ments.
Table 8 demonstrates that certain changesmay not be best captured by standard parser bench-marks.
While the first four parser models improveon the WSJ benchmarks as they improve subjectivetranslation quality the best parser according to sub-jective translation qualtiy (P5) is actually the worstunder both metrics on the treebank data.
We con-jecture that this is due to the fact that P5 (unlike theother parsers) is case insensitive.
While this helps ussignificantly on our test set drawn from the web, it18Standard / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        A Mortgage {{ Tax Deduction }} For I Qualify How Can ?Translation                        ??
???
??
?
??
?
??
?
??
?
?
??
??
?
??
??
?
?Alignment          6,6,7_8,4,3,3,3,3,3,0,0,0,0,0,1,1,9,9Alignment-oriented / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        I How A Mortgage {{ Tax Deduction }} For Qualify Can ?Translation                         ?
?
??
?
??
??
???
?
??
?
??
?
???
??
?
??
??
?
?Alignment         2,2,0,0,0,6,6,6,7_8,4,3,3,3,1,1,1,1,1,9Standard / AutomaticSource              We do not claim to cure , prevent or treat any disease .Reordering        any disease cure , prevent or treat claim to We do not .Translation           ????
??
?
??
,  ??
,            ???
??
?
??
??
??
?
?
??
??
?
.Alignment         10,11,,5,6,7,,8,9,,,4,,,,2,2,2,12Alignment-oriented / AutomaticSource            We do not claim to cure , prevent or treat any disease .Reordering        We any disease cure , prevent or treat claim to do not .Translation              ?
?
?
????
??
?
??
,           ??
????
??
?
??
?
??
?
??
?
.Alignment          0,0,,10,11,,5,6,7,8,9,,,,3,4,2,2,12Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)manual vs. automatic word alignmentExp.
1 Parsers Exp.
2 Reordering RulesP2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5P1R5 +** +** +** +** P5R1 +** +** +** +**P2R5 +** +** +** P5R2 0 +** +**P3R5 +** P5R3 +** +**P4R5 0 P5R4 +**Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).Exp.
1 Parsers Exp.
2 Reordering RulesP2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5P1R5 +** +** +** +** P5R1 0 +** +** +**P2R5 +** +** +** P5R2 +** +** +**P3R5 +** +** P5R3 +** +**P4R5 0 P5R4 +**Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).Exp.
1 Parsers Exp.
2 Reordering RulesP2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5P1R5 0 0 +* +* P5R1 +** +** +** +**P2R5 0 0 0 P5R2 0 +** +**P3R5 0 0 P5R3 0 +*P4R5 0 P5R4 0Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).19Parser Labeled attachment POS accuracyP1 0.807 0.954P2 0.822 0.954P3 0.827 0.955P4 0.830 0.955P5 0.822 0.944Table 8: Intrinsic parser metrics on WSJ dev set.Figure 1: P1 and P5?s parse trees and automatic reorder-ing (using R5 ruleset) and fuzzy score.hurts parsing performance on cleaner newswire.6 DiscussionWe have found that in practice this evaluation frame-work is sufficiently correlated with human judg-ments of translation quality to be rather useful forperforming detailed error analysis of our English-to-Japanese system.
We have used it in the followingways in simple error analysis sessions:?
To identify which words are most frequently re-ordered incorrectly?
To identify systematic parser and/or POS errors?
To identify the worst reordered sentences?
To evaluate individual reordering rulesFigures 1 and 2 show pairs of parse trees togetherwith their resulting reorderings and scores againstFigure 2: P1 and P5?s parse trees and automatic reorder-ing (using R5 ruleset) and fuzzy score.the reference.
These are typical of the parser er-rors that impact reordering and which are correctlyidentified by our framework.
In related joint work(Katz-Brown et al, 2011) and (Hall et al, 2011), itis shown that the framework can be used to optimizereordering components automatically.7 ConclusionsWe have presented a lightweight framework for eval-uating reordering in machine translation and demon-strated that this is able to accurately distinguish sig-nificant changes in translation quality due to changesin preprocessing components such as the parser orreordering rules used by the system.
The sentence-level correlation of our metric with judgements ofhuman translation quality was shown to be higherthan other standard evaluation metrics while ourevaluation has the significant practical advantage ofnot requiring an end-to-end machine translation ex-periment when used to evaluate a separate reorder-ing component.
Our analysis has also highlightedthe benefits of creating focused evaluation data thatattempts to factor out some of the phenomena foundin real human translation.
While previous work hasprovided meta-analysis of reordering metrics acrossquite independent systems, ours is we believe thefirst to provide a detailed comparison of systems20that differ only in small but realistic aspects such asparser quality.
In future work we plan to use theframework to provide a more comprehensive analy-sis of the reordering capabilities of a broad range ofmachine translation systems.ReferencesAlexandra Birch and Miles Osborne.
2010.
Lrscore forevaluating lexical and reordering quality in mt.
In Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, pages 327?332,Uppsala, Sweden, July.Alexandra Birch, Miles Osborne, and Philipp Koehn.2008.
Predicting success in machine translation.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 745?754, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Alexandra Birch, Phil Blunsom, and Miles Osborne.2009.
A quantitative analysis of reordering phenom-ena.
In Proceedings of the Fourth Workshop on Sta-tistical Machine Translation, pages 197?205, Athens,Greece, March.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for mt evaluation: evaluating reorder-ing.
Machine Translation, 24:15?26, March.Chris Callison-burch and Miles Osborne.
2006.
Re-evaluating the role of bleu in machine translation re-search.
In In EACL, pages 249?256.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005a.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL?05), pages 531?540, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005b.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 531?540, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 304?3111, July.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
Comput.
Linguist., 33:293?303, September.Keith Hall, Ryan McDonald, and Jason Katz-Brown.2011.
Training dependency parsers by jointly optimiz-ing multiple objective functions.
In Proc.
of EMNLP2011.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automatic evalu-ation of translation quality for distant language pairs.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 944?952, Cambridge, MA, October.
Association for Com-putational Linguistics.Jason Katz-Brown, Slav Petrov, Ryan McDonald, FranzOch, David Talbot, Hiroshi Ichikawa, Masakazu Seno,and Hideto Kazawa.
2011.
Training a Parser for Ma-chine Translation Reordering.
In Proc.
of EMNLP2011.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.Alon Lavie and Michael J. Denkowski.
2009.
The me-teor metric for automatic evaluation of machine trans-lation.
Machine Translation, 23(2-3):105?115.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4):513?553.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL ?03.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Chao Wang.
2007.
Chinese syntactic reordering forstatistical machine translation.
In In Proceedings ofEMNLP, pages 737?745.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of the 20th international con-ference on Computational Linguistics, COLING ?04,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages245?253, Boulder, Colorado, June.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings of the Workshop on Statistical MachineTranslation, pages 55?63.Y.
Zhang and S. Clark.
2008.
A Tale of TwoParsers: Investigating and Combining Graph-basedand Transition-based Dependency Parsing.
In Proc.of EMNLP.21
