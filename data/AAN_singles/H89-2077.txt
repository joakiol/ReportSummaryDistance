WHITE PAPERONSPOKEN LANGUAGE SYSTEMSDr.
John Makhoul, ChairmanBBN Systems and Technologies CorporationDr.
Fred JelinekIBM TJ Watson Research CenterDr.
Larry RabinerAT&T Bell LaboratoriesDr.
Clifford WeinsteinMIT Lincoln LaboratoryDr.
Victor ZueM1TI.
SCOPEULTIMATE GOALSpoken language is the most natural and common form of human-humancommunication, whether face to face, over the telephone, or through variouscommunication media such as radio and television.
In contrast, human-machine interactionis currently achieved largely through keyboard strokes, pointing, or other mechanicalmeans, using highly stylized languages.
Communication, whether human-human orhuman-machine, suffers greatly when the two communicating a ents do not "speak" thesame language.
The ultimate goal of work on spoken language systems i  to overcome thislanguage barrier by building systems that provide the necessary interpretive functionbetween various languages, thus establishing spoken language as a versatile and naturalcommunication medium between humans and machines and among humans peakingdifferent languages.GRAND CHALLENGESSpoken language systems differ widely in their capabilities and requirements.Three grand challenges for spoken language systems include:?
INTERACTIVE PROBLEM SOLVING -- interactive command, control, andinformation retrieval using voice input/output -- The system would require full integrationof speech recognition and natural language understanding for input, and may requirenatural language generation and speech synthesis for output.
Example applications includedatabase query (e.g., airline reservations, library search, yellow pages with voice input),command and control, resource management (such as battle management workstation orlogistics upport), computer-assisted instruction, and aids for the handicapped.463?
AUTOMATIC DICTATION (transcription) -- The challenge lies in the system'sability to transcribe arbitrary spoken input with virtually unlimited vocabulary and types ofsentence construction.?
AUTOMATIC TRANSLATION -- multi-language voice input/output withautomatic translation -- Example applications include automatic nterpreter for multi-language speeches and meetings, translating telephone, and NATO field communications.While these challenges constitute long-term goals for spoken language systems,there are challenging but achievable shorter-term goals that would have significanteconomic impact.
One near-term challenge would be to develop robust, operational voice-operated ata entry and query systems with limited language understanding capabilities inactual applications.The grand challenges li ted above require advances in speech processing(recognition and synthesis), natural language processing, and automatic translation.
Thisreport on spoken language systems has been written largely from a speech recognitionpoint of view; the modeling of natural language, however, has such a great impact on theperformance ofa speech recognition system that language modeling becomes an importantarea for speech recognition research as well.
The reader is referred to another documentwhich covers the natural language processing areas in more detail.
Also, natural speechsynthesis an important area that requires treatment beyond that allotted to it in this report.MISSING SCIENCEThe areas of missing science fall in three general categories:?
Complete modeling of the speech signal and its variabilities tofacilitate fficientinformation extraction for recognition and synthesis.
These variabilities include phoneticand other linguistic effects, inter- and intra-speaker variabilities (including health conditionand emotional state), and environmental acoustic variabilities.?
Automatic acquisition and modeling of linguistic phenomena, including domain-dependent and domain-independent knowledge (lexicon, syntax, semantics, discourse,pragmatics, task structure), especially the modeling of actual spoken language.?
Developing human factors methods for the design of user-friendly spokenlanguage systems, including the use of clarification dialogues and the efficient training ofusers.Statistical methods capable of modeling signal variability in parameter space as wellas time, such as hidden Markov models, have put the speech recognition problem on asolid theoretical basis and have resulted in significant advances in continuous speechrecognition i  the last decade.
The performance of such systems, however, is still far fromadequate for the ultimate goals stated above and far inferior to human performance.
Onecan improve the performance ofcurrent systems somewhat through better signal processingand feature xtraction, and through extensions of the existing theoretical paradigms.However, significant improvement i  performance will require a more comprehensivemodeling of the speech signal and its variabilities, including possibly the development ofnew theoretical paradigms.
A prerequisite odeveloping improved speech models isacquiring the knowledge of how to extract the needed information from the speech signaland how to build appropriate r cognition structures that can take advantage of thisinformation.
To be useful, this knowledge must be developed in the context of buildingadvanced speech recognition systems.
An important aspect to operational speech464recognition systems will be their robustness tospeaker and environmental variabilities.
Ofspecial interest to certain military applications, for example, is robustness to high levels ofnoise and stress.
Methods that would adapt automatically and quickly to changes inspeaker or in environment characteristics will need to be developed.More comprehensive models of the speech signal will also benefit he automaticsynthesis of speech from text.
Current commercial synthesis devices may be adequate forsome applications, but their synthetic speech quality limits their wide use.
The ability of amachine to produce natural speech quality will be an important output modality for anadvanced interactive human-machine interface.
Speech output with natural quality willrequire significant research into improved speech signal models, including proper modelingof prosody, many aspects of which depend on the linguistic onstructs of the text to besynthesized.For humans, the speech understanding decisions depend on the acoustic ontent ofthe speech signal and the listener's expectation of what might be said.
It is the purpose ofthe language model in the human to sharpen that expectation and to effect he understandingof the message.
Similarly, in automatic speech understanding, it is the purpose of thelanguage model to constrain the possible sequences ofwords, leading to improvedrecognition performance, and to interpret what was said if understanding of the message isdesired.
Even though much work remains to be done to solve the speech recognitionproblem as such, a major barrier to full realization of an advanced spoken language systemarguably rests with the development ofa mature natural language understandingtechnology.
The speech recognition problem has benefited from the fact that the problemis well defined, where the input is the speech signal and the output is a set of words.Given the input and desired output, automatic methods have been developed for modelingvarious peech phenomena.
These automatic methods have been crucial in advancing thestate of the art in speech recognition.
Furthermore, the performance of a speechrecognition system can be evaluated by simply measuring the word error rate, for example,thus allowing for systematic and objective valuations that can be used to improve systemdesign.
In contrast, he natural language understanding problem has not been as welldefined.
While the input here is taken to be a set of words, the output (i.e., the meaning ofthe utterance) isnot well defined for all cases, nor is it well modeled computationally.
Oneresult has been the lack of rigorous evaluation of the performance ofnatural languagesystems.
Partial theories for modeling meaning exist, such that limited languageunderstanding systems have been built which may be useful in certain applications.
Thebuilding of such systems has required the enormously abor-intensive process ofdeveloping rammars and semantic rules that map an input sentence into its meaningrepresentation.
The extent o which existing theories for modeling language are complete,however, has not been rigorously tested.
While new linguistic theories may be needed tomodel a larger ange of linguistic phenomena, there is a dire need to develop automatic orsemiautomatic methods for the modeling of linguistic phenomena.It is important to note that the language modeling problem is significant for speechrecognition whether complete understanding of the input speech is required, as in theproblem solving application, or merely transcription of what has been said, as in thedictation application.
Statistical language models, for example, have been quite successfulfor the dictation application, without requiring understanding onthe part of the machine.The translation problem, however, is directly affected by progress in modeling of syntax,semantics and discourse, especially in interactive applications over the telephone.One of the major obstacles to the fielding of spoken language systems i  often thelack of an ergonomically sound design.
It is therefore important to develop a humanfactors technology that is appropriate for the design of user-friendly spoken language465systems.
In support of possibly deficient language models, or genuine ambiguity on thepart of the user, it would be important to develop graceful and effective methods formachine generation of cooperative clarification dialogues with the user to resolve possibleerrors or ambiguities.
It would also be useful to develop methods for training users ofspoken language systems to learn the limitations of such systems in a relatively short periodof time.
The learning by users of the capabilities and limitations of complex systems is, ofcourse, a genetic problem in other fields as well.
For some applications, poken languageinput and output will need to be integrated with other input/output modalities uch astyping, graphics, and pointing (by mouse or touch screen).BARRIERS TO PROGRESSThe most important barriers to progress are the areas of missing science mentionedabove.
Our fundamental lack of understanding of spoken language must be overcomethrough concentrated and substantial research efforts.
In addition, there are barriers toprogress in the form of computing, data, human resources, and support:?
Lack of very fast computing and large on-line storage for research.?
Unavailability of adequate and relevant speech and language databases andcorpora.?
No comprehensive educational nd training programs for scientists and engineersin speech and natural language.?
Lack of long-term research programs of sufficient size to build and test completeexperimental systems.Many of the advances in speech recognition in the last decade have benefiteddirectly from the availability of faster computing.
Significant additional advances can stillbe achieved simply by increasing computational power which would allow experimentationwith more compute-intensive id as.
It is estimated that future spoken language systemsmay require computing speeds of 100 gigaflops or more.
However, the ready availabilityof machines for research which compute at rates of 100-1000 megaflops would advance thestate of the art considerably.
These should be general-purpose machines that are easilyprogrammable in a higher-level language for research purposes.
In addition, on-linestorage capabilities of at least 100 gigabytes would be needed to store data for regularlyperformed experiments.
(See the Appendix for a detailed analysis of computing andstorage needs.
)It is rather difficult to model phenomena that one is not able to observe adequately.Another deficiency in resources, in fact, has been the dearth of speech and natural languagecorpora that manifest the various speech and linguistic variabilities.
Efforts have begun todefine some of the needed corpora.
It is estimated that hundreds of hours of speech andbillions of words may be required to represent all the natural language phenomena ofinterest for the different applications.
Additionally, labor-intensive special inguisticlabeling of large portions of the collected ata will be needed for training and test purposes,especially if semi-automatic modeling of spoken linguistic phenomena is to beaccomplished.
(See the Appendix for a detailed analysis of the need for large coropora ofspeech and text.
)The third barrier to progress is another resource problem: the lack of properlytrained scientists and engineers in spoken language research.
First and foremost, a solidbackground is needed in a number of abstract and applied mathematical disciplines,466including probability, statistics, linear systems, theory of computation and algorithms(including formal grammars, parsing, and search algorithms), logic, pattern recognition,and information theory.
This background should be acquired by both speech scientists andcomputational linguists.
In addition, speech scientists need to be trained in signalprocessing and speech communication, including phonetics, speech production andperception, speech analysis/synthesis, and speech recognition.
Computational linguistsneed to be trained in phonology, morphology, syntax, semantics, and discourse, withspecial emphasis on data-driven, empirically-based linguistics.
Of  benefit o all also wouldbe courses in psycholinguistics, cognitive science, and human factors (ergonomics).Currently, programs do not exist which offer the areas mentioned above in a coherentfashion.
It is not typical for training in computational linguistics, for example, to includecourses in probability and statistics.
Nor are speech scientists usually properly trained incomputer science.
Many prominent schools do not even offer certain basic courses, suchas pattern recognition.Finally, it is obvious that insufficient funding would be a serious barrier toprogress.
The funding would have to be sufficient o support several research groups withcritical mass on a long-term basis, i.e., groups with sufficient size to be able to build andtest complete systems.
Support for smaller groups that concentrate on research issues inspecific areas is also necessary; however, such groups will need to integrate their work intothe larger systems.
Support will also be required for the purchase of adequate computingfacilities and for the specification and collection of massive corpora for the purpose ofsystem training and testing.POTENTIAL BREAKTHROUGHSWithin the next decade, amajor potential breakthrough is that large-vocabularycontinuous peech recognition systems will have improved sufficiently to allow them to beintegrated in some everyday applications.
To make this happen, the following componenttechnical breakthroughs are needed and are likely:?
A reduction in the word error rate for such systems by a factor of about five fromthe present, brought about possibly by improved signal representations, better modeling ofvarious linguistic units, and enhanced methods for estimation of model parameters.?
The ability to handle out-of-vocabulary words in a graceful manner.?
Fast system training to new speakers and new environments.?
The integration of speech with limited natural language understanding capabilityfor interactive applications.In speech synthesis, a potential breakthrough is synthesis of speech from text withmore natural quality than currently possible, including more natural-sounding intonationand prosody.
However, work in this area would have to be increased significantly foradvances to take place.For breakthroughs in the natural language and machine translation areas, the readeris referred to another document on the topic.467II.
BACKGROUNDASSESSMENT OF THE FIELDWe will limit our discussion here to the assessment of speech recognition systems.A separate document will assess more thoroughly the natural language and translationareas.
Speech synthesis ystems from arbitrary text currently have a rather stylizedsynthetic quality, with rather unnatural intonation patterns.The performance ofcontinuous peech recognition systems i  typically measured interms of total word error ate, including insertions and deletions.
For small vocabularies ofless than 20 words, usually the digits plus some control words, speaker-independentperformance (i.e., no special training needed for each speaker) has been measured at lessthan 1% word error rate.
Systems with medium-size vocabularies of 100-200, aconstrained grammar with perplexity (average branching factor) of on the order of 10, haveachieved aword error rate of less than 1% in speaker-dependent mode.
Both types ofsystems are available commercially.Large-vocabulary s stems of 1000 words, with grammars of perplexity 60, showcontinuous recognition performance of 5-10% word error rate.
Systems with largervocabularies typically are not operated in continuous mode, but rather the words are spokenin isolation, which tends to decrease the error ate.
Very large-vocabulary s stems of20,000 words, spoken in isolation in speaker-dependent mode, perform at a 5% word errorrate with a perplexity of 200.
Large vocabulary systems are, for the most part, laboratorysystems (some commercial isolated-word systems exist).The results mentioned above have been obtained largely in relatively controlledenvironments.
Performance typically degrades under hostile acoustic onditions, especiallyin noisy military platforms; however, good performance has been obtained for restrictedtasks over dialed-up telephone lines and moderate amounts of background noise.Most of the grammars employed with speech recognition systems are quiteconstrained in their ability to model natural language.
For interactive applications, whereunderstanding of the input is necessary, the grammars utilized are typically small and tree-like, with well-defined semantics.
The integration of speech recognition with existingnatural language understanding has started only recently.RELATIONSHIP TO OTHER FIELDSThe design of spoken language systems depends on the integration of several ofthe following technologies, depending on the application: speech recognition and synthesis,natural language understanding and generation, automatic translation, and human factorsengineering.
It also depends on advances in computer architecture and accompanyingsoftware.
Progress in all these areas is necessary for the reliable fielding of advancedsystems.An area that is intimately related to speech recognition is that of machine learningand self-organizing systems.
In fact, the recent advances in speech recognition rest almostexclusively on the development ofcomputational models (e.g., hidden Markov models andothers) for which automatic training (i.e., learning) methods have existed for some timeand are often taken for granted.
These learning algorithms estimate the values of the modelparameters directly from data, typically in a few iterations, and are able to generalize themodels to unseen data.
Because the performance of speech recognition systems can be468measured rigorously in terms of word error rate, the speech recognition problem, therefore,could serve as a convenient testbed for the comparative t sting of other learning algorithms,such as those associated with artificial neural networks.Another area that is closely related to speech recognition is that of  speakerrecognition, with its two branches: speaker verification (of a claimed identity) and speakeridentification (of an unknown speaker) from a given speech utterance.
The state of the artin these two areas already exceeds human performance doing the same task.
It appears thathumans are far better at recognizing what is being said, irrespective of who is talking, thanat recognizing who is talking.
As in speech recognition, the most successful approaches tospeaker recognition have been those that characterize statistically the short-term spectralcharacteristics of a speaker.
We expect hat by working on recognizing speakers from theirvoices, we should be able to learn more about how to adapt a speech recognition system tothe voice of a particular speaker.Speaker verification is typically used for secure access to information media (suchas the telephone) or to physical locations.
The performance of a speaker verificationsystem is often measured by the average of the rate of false rejection of correct alkers(customers) and the rate of false acceptance of incorrect talkers (imposters).
The state ofthe art is less than 1% average rror ate; this number is relatively independent of thenumber of talkers that the system can handle.
Relative to speech recognition, speakerverification technology is considered quite mature and commercial products already exist.Partly because of human factors issues, these products do not appear to be in widespreaduse as yet.In contradistncfion with speaker verification where the system can prompt he userto say a particular utterance, in speaker identification the identity of the speaker must bedetermined independent of what the speaker utters, which is inherently a more difficulttask.
Two possible applications of this technology are the automatic dentification ofspeakers when transcribing the proceedings ofa meeting or conference (human transcribersare good at transcription but not at identifying the speakers), or for identifying speakers forintelligence purposes.
The performance of speaker identification systems depends on alarge number of factors which include the number of speakers in the set to be identified, theclass of communication channels being used, the total amount of speech and the number ofconversations for each speaker used in training the speaker models, and the amount of dataused in the identification process.
If, for example, we wish to identify among 20 speakerswith speaker models developed from a total of 60 s of speech from different conversationsand 20 s are used for identification, we expect o achieve at least 90% correct identification.With as little as 10 s for training and 2 s for identification and with communication takingplace over highly variable radio channels, nearly 70% correct identification has beenachieved.CENTERS OF EXCELLENCEThe following US research organizations have strong capabilities in two or more ofthe needed technologies: AT&T Bell Laboratories, Bolt Beranek & Newman Inc.,Carnegie-Mellon University, IBM T.J. Watson Research Center, Institute for DefenseAnalyses, I'IT, Lincoln Laboratory, Massachusetts Institute of Technology, SRIIntelrnational, nd Texas Instruments.469III.
RESEARCH OPPORTUNITIESSCIENTIFIC OBJECTIVESThe scientific objectives flow from the areas ofmissing science above:?
Develop comprehensive models of the speech signal that take into accountlinguistic, as well as speaker, environmental, nd channel variabilities.
The ultimate goalhere would be a robust continuous peech recognition system that can achieve on naturalspeech, using vocabularies of 50,000 words or more, word error rates of 1% or less.?
Perform rigorous testing of the quality of existing language modeling methods tonatural language data, and develop more comprehensive models of natural language whichinclude the phenomena not accounted for.
As an aid in this process, develop methods forthe automatic acquisition of linguistic knowledge.
Incorporate the results in completespoken language systems.
The ultimate goal would be a system that is capable ofunderstanding at least 95% of the sentences given to it by a user, and graceful handling ofutterances not fully understood.?
Study the interaction of users with spoken language systems and develop criteriafor the design of user-friendly systems (e.g., how to deal with out-of-vocabulary wordsand with new semantic oncepts).
Design procedures for training users on the capabilitiesof the system quickly, and for providing helpful clarification dialogues.To develop the needed comprehensive models of the speech signal mentionedabove, a significant program of research into various aspects of speech modeling will needto be undertaken.
Areas of research include auditory modeling, acoustic phonetics,interaction of linguistic structure and speech at the phonetic and prosodic levels, adaptationto a speaker and to an environment, and new mathematical modeling techniques (e.g.,maximum mutual information, stochastic segment models, and artificial neural networks).MEASURES OF PROGRESSIt is important to monitor the progress of spoken language technology throughperiodic performance and evaluation tests.
Speech recognition performance, for example,can be measured in terms of word error rate or sentence rror rate under various testconditions.
Performance of language understanding systems i  more difficult to measure; itcould be measured in terms of syntactic and semantic error ate, i.e., the number ofsentences not understood correctly by the machine, provided that criteria for measuringcorrectness are well defined.
However, what is really important in many cases is theaccomplishment of certain tasks efficiently; therefore, number of tasks accomplishedcorrectly within a certain time period would be another measure of performance.
Many ofthese measures, especially those relating to natural language modeling, will have to bedefined more rigorously.
The performance measures, once defined, should be monitoredon a regular basis as technology progresses.Proper evaluation of progress will require the specification, collection, labelling,and distribution of sizeable databases and corpora on which system training and testing willbe performed.
Different ypes of coropora will need to be collected for the differentapplications.
These corpora should be acquired and made available as a national resource.The National Bureau of Standards has been very active in the evaluation of speechrecognition technology; they have helped in specifying testing standards (in collaborationwith academia and industry) and in acting as a repository and distributor of national470databases in speech.
It would be appropriate for NBS to play a similar ole for work inspoken language systems.Monitonng technology progress i  important, but ultimately acceptance by the userwill be the real measure of progress in spoken language technology.IV.
IMPACTPOTENTIAL IMPACTSuccessful spoken language systems will redefine and revolutionize the wayhumans interact with machines.
Such systems could be used anywhere humans come incontact with machines: at home or at school, in the car, over the telephone, in the factory,in the private and the public sectors, etc.
The potential benefits are a simpler interface withcertain machines and higher productivity.
(Spoken language input, for example, couldeliminate the need for delayed and typically tedious data entry.)
The economic impact willbe such that spoken language systems could result in a dominant share of the informationprocessing industry into the next century.Of special importance is the potential impact of spoken language understanding onmilitary platforms which incorporate complex operational systems that interface withhuman operators.
Such systems currently abound in DOD in the form of logistical supportsystems, command and control systems, tactical and battle mangagement systems, andsystems where the eyes and hands are otherwise busy (such as in avionics).
Spokenlanguage technology will also facilitate the automatic analysis of massive amounts of voicecommunications for intelligence and security purposes.
The latter applications wouldbenefit signifcantly even with modest improvements in speech recognition accuracy.TRANSITION TO THE REAL WORLDWe are rather fortunate that the area of spoken language systems has always hadgeneral appeal, not only for the general public, but also in the commercial nd financialcommunities.
This is attested to by the proliferation of companies in the last two decades inthe areas of speech recognition and natural language access to database managementsystems.
Therefore, the main thing that is needed in transferring spoken languagetechnology to the real world would be simply to show feasibility by demonstrating thetechnology in real-world applications.
Successful demonstrations would then sparkcommercial interest.
The process of building such systems would also help identify wheresome of the crucial problems are, and hence impact he research towards more advancedsystems.An important aspect to such demonstrations is that they occur in real time, i.e., theuser should not have to wait a long time for the machine to understand what is being said.This implies that the hardware, perhaps including special purpose accelerators, needs to befast enough to accomplish the understanding in real time.
Cost of the total system is, ofcourse, another important aspect to transferring the technology to the real world.However, hardware costs would be expected to fall fast enough so that it cannot be viewedas a major limitation at this time.A key ingredient to transitioning speech recognition technology to the real world isrobust system operation under a variety of conditions, including the use of differentmicrophones, background noise, reverberation, speaking rate, and various aspects of471spontaneous speech.
Even more important is language-related robustness, e.g., handlingout-of-vocabulary words and various linguistic constructs: a robust spoken languagesystem would be able to respond properly to many different: phrasings of requests andcommands.
There are also important robustness i sues related to the user-machineinterface, for which human factors technology can be employed profitably.
All issues ofrobustness need to be dealt with in the context of systems working under operationalconditions.The US is generally ahead in the basic requisite technologies for spoken languagesystems, but other countries (e.g., Japan) are generally ahead in the commercial use ofthese technologies (especially in speech recognition).
One reason for this imbalance in thedevelopment and the use of the technology in the US may be that the US consumer is verydemanding in terms of requiring a higher degree of convenience and ease of use of theproduct.
If the human factors issues can be dealt with satisfactorily, there is in the nearterm a potentially substantial market for speech recognition systems with small or medium-sized vocabularies.
The development of more advanced systems with large vocabulariesand natural language understanding capabilities will increase the economic impactsignificantly in the future.TRANSITION TO DODBecause the commercial and government sectors are often different in their needsand requirements, special issues typically need to be addressed in transferring thetechnology to the government sector, especially the military part of it.
Here, it would beimportant to coordinate research efforts with the potential user organizations, uch as theServices, and to couple demonstrations to specific application areas, so that the userorganizations would adopt the technology effectively as it is being developed.
Below, wepresent three specific areas in which spoken language technology could be used profitably.1.
AvionicsLimited-vocabulary (few hundred words) isolated or continuous peech recognitionsystems could be useful for a pilot to manipulate various functions in his fighter aircraft orhelicopter.
This application is characterized by a hands-busy, eyes-busy environment.Speech recognition would provide an additional crucial input modality for the pilot tocommunicate with his cockpit's computers without moving his hands from the controls orhis eyes from scanning the space outside his aircraft.
The payoff or allowing the pilot tokeep his eyes on his target, rather than on his instruments, could be substantial.
In thisapplication, the speech recognition system must have very high recognition accuracy(>98% word accuracy) and be robust o varying noise and stress conditions.2.
Command and ControlHere, the need is for large-vocabulary (few thousand words) continuous peechrecognition integrated with natural language technology for understanding purposes.However, the environment is typically not as severe as in the avionics application.
Thesystem must allow the speaker to speak in a natural, goal-directed manner, with gracefulhandling of out-of-vocabulary words and linguistic constructs.
Typical applicationsinclude resource management and battle management, which are characterized by multi-medal interaction, including text, tables, menus, pointing, and graphics.
The use of voicecould obviate the need for complicated menu-based requests.
The use of natural spokenlanguage could also ameliorate the need for extensive user training of such complexsystems.4723.
IntelligenceAutomatic spotting of key words and phrases in continuous peech can be used insurveillance and message classfication applications; it addresses the problem of reducingthe workload and enhancing the efficiency of intelligence analysts.
These applications arecharacterized bydifferent types of noise and channel distortions and by the need to performspeaker-independent r cognition of the key words.
Because of the vast amounts of speechto be processed, however, even modest recognition accuracy could reap enormousbenefits.
There is a potential to integrate the technologies for speech recognition, speakerrecognition, and language modeling, and apply them to message classification -- of which asimple but useful form is classification i to messages of interest and messages of nointerest.
Of substantial utility also is unconstrained recognition of speech in a specificlimited domain, such as monitoring of speech used in air traffic communication.V.
CONCLUSIONS AND RECOMMENDATIONSCONCLUSIONSAdvanced spoken language systems will require the integration of the followingtechnologies: peech recognition and natural language understanding for spoken input, andspeech synthesis and natural language generation for spoken output.
If the input andoutput languages are different, hen automatic translation is also needed.
In addition,speaker recognition could be an important capability for certain applications.
Each of thesecomponent technologies has separate and relatively independent applications of its own,and work on each of them can be justified and should be continued independently.However, the synergism created by the integration of these technologies into spokenlanguage systems promises to revolutionize the way humans interact with machines and toenhance communication among humans peaking different languages.Speech recognition technology has matured sufficiently to allow for limitedapplications with vocabularies ofhundreds of words, where the allowable grammaticalconstructs are limited and their semantics are well defined.
Large-vocabulary applicationswill require reducing the word error ate further by a factor of about five.
This reduction isachievable by developing more complete models of signal variabilities, more realisticlanguage models, and utilizing larger speech and natural language databases.
To increasethe chances of practical utility, speech recognition technology needs to be integrated intoreal-world applications to guide the research.
Special robustness i sues need to beaddressed by devoting specific effort to demonstrating speech interfaces to military andintelligence applications.The state of the art in natural language understanding and automatic translationappears now to be sufficient for certain applications; uch as limited database querysystems or automatic translation of limited domains with the aid of a post-editor.However, advanced systems that allow a larger ange of natural linguistic onstructs willrequire the development ofmore comprehensive linguistic models, the development ofmethods for the automatic acquisition of linguistic and domain knowledge, and theevaluation of progress through rigorous evaluation procedures.The integration of speech and natural language technologies, which has just barelybegun, should be an ongoing endeavor that not only will incorporate the latest from the twotechnologies, but will also solve many problems unique to it, such as the proper utilizationof the different sources of knowledge to limit the search space and result in the highest473understanding rate.
The research for these integrated spoken language systems will requirethe availability of multi-disciplinary teams of highly trained individuals in the relevantdisciplines, substantial computing power and storage, and adequate spoken languagedatabases.Research in speech synthesis will have to be actively supported at a significantlyhigher level ff natural speech synthesis from text is to become areality.RECOMMENDATIONSIt may be the case that whoever controls the development ofhuman-machineinterfaces may own the key to controlling the information technology going into the nextcentury.
The importance of spoken language systems i  in providing a simple, cost-effective interface to machines.
Therefore, the US should make an effort at maintainingand enhancing its leadership osition in the design of advanced spoken language systems.First and foremost, he US must support and maintain a strong R&D program inspoken language systems in several academic and industrial centers of excellence.
Theprogram must foster basic research in the various disciplines but focus on the design,implementation, and testing of complete working systems.
For the latter to take place,several research groups of sufficient size and longevity will need to be maintained.
Sinceadvanced spoken language systems do not exist as yet, many of the design issues cannot beforeseen in advance.
Therefore, itwill be important to promote abootstrapping operationin which we would be able to learn from the fielding of complete working systems o thatbetter ones can be designed, implemented, and fielded, and so on.Second, the barriers to progress mentioned above must be overcome.
Specifically,we must: make available fast computing and large on-line storage facilities for research;collect and distribute adequate speech and natural language corpora; and encouragemultidisciplinary academic programs, including cooperative efforts with industry.
Thelatter aspect may be especially important for universities that are not likely to build completeworking systems.
One avenue for cooperation would be for students to perform theirthesis work in collaboration with industry, where the student incorporates the thesis workin an existing working spoken language system.
Another avenue would be to sponsorefforts for developing certain standard software packages and complete modular systems,which implement many of the more mature aspects of the technology (e.g., standardrecognition algorithms, parsers, etc.).
Research at universities and other small researchgroups could then be performed within the context of complete working systems.Third, it would be beneficial to nurture certain cooperative efforts with othercountries, especially in the areas of automatic translation and development ofcommondatabases and corpora for the purposes of system development and evaluation.Finally, in the application of spoken language technology to DOD, three areasshould be targeted for the integration of this technology: avionics, command and control,and intelligence.
The pilot's associate program should integrate the use of voice in thedesign of an intelligent pilot-machine interface in the cockpit.
Resource or battlemanagement offers a particularly rich command and control environment for integratingspeech recognition and natural language understanding technologies with other modalitiesto aid the user in performing complex tasks.
To assess their utility in these applications,spoken language systems must be built and tested, preferably under ealistic fieldconditions.
Lastly, research in the automatic spotting of key words and phrases and itsintegration with language modeling techniques for the purpose of message classificationwill need to be supported at a higher level if significant progress i  to be made in this area.474APPENDIX.
COMPUTATIONAL,  DATA, AND STORAGE REQUIREMENTSIt is important to point out at the outset hat he fast computing that is needed forresearch in spoken language systems does not defive so much from the need to achievereal-time with compute-intensive algofithms, for, given a particular algorithm, it has alwaysbeen possible to build special-purpose hardware to perform the computations in real-time.Rather, it is the process of discovery of the algofithrns and variations thereupon, and thetesting of the various combinations of algorithms on real data, that require a fast, flexible,easily prograrnrnable computing environment.
Below, we first give an example of real-time computing needs based on one of the most successful speech recognition algorithms todate.
Then, we estimate future needs in terms of speech corpora that will be required todevelop high-performance speech recognition systems.
Based on these two data points,we estimate the computing needs to perform research in this area.
We then estimate theamount of text corpora that will be needed for research in language modeling, and finally,we estimate the amount of on-line storage that will be needed for spoken language systemresearch.REAL-TIME COMPUTINGAssume that we are performing phonetic recognition of continuous speech usinghidden Markov models for the different phonetic ontexts.
Because the acoustic realizationof each phoneme depends not only on the identity of that phoneme but also on the identitiesof the left and fight phonemes at least, for best recognition performance one needs aseparate model for each of the possible triphone contexts, which we shall call "phones".
Ifwe assume that here are 50 phonemes in English, then the total number of triphones orphone models needed theoretically is 50"'3=125,000.
Not all of these will occur in actualspeech because of the constraints on the allowable phoneme sequences in a language.However, because many of these constraints do not apply across word boundaries, asubstantial fraction of the possible triphones can occur in continuous speech.Assume that each phone is modeled by a finite-state hidden Markov model, andassociated with each state is a probability density defined over the space of input vectors(typically representing the input spectrum every 10 ms).
Further, assume that themultidimensional probability density is modeled as a mixture of Gaussian densities.
Foreach frame of speech, i.e., every 10 ms, the recognition process consists in computing thelikelihood of a sequence of phones using the given models and finding that sequence thatmaximizes the likelihood given the sequence of input spectra.
Now, let:P = Number of different phonesS = Number of states in each phone modelG = Number of Gaussians per stateD = Number of dimensions (components) in spectral vectorL = Number of multiply-adds per likelihood computationF = Number of speech frames per secondThe total number of computations per second of speech is then the product of all termsdefmed above:C = P*S*G*D*L*FSubstituting the following realistic values for the different parameters: P=4000, S=3, G=3,D=25, L=10, and F=100, we obtain C=9"10"'8 multiply-adds per second, i.e.,475approximately 10"'9 or 1 gigaflops to perform the recognition i real time.
(Here we haveassumed that the covariance matrix of the Gaussian mixtures is diagonal.
)New phonetic models that are being considered currently will require an order ofmagnitude increase in computation.
Further, if natural language understanding isincorporated in the whole process, one could expect another order of magnitude increase incomputation.
Therefore, future spoken language systems might require computing speedsof about 100 gigaflops for real-time operation.SPEECH CORPORAThe estimation of the parameter values of phone models, a process known astraining, requires the availability of sufficient speech data for training purposes.
Typically,at least 50 samples of each phone are needed to estimate a robust model of that phone, i.e.,one that does almost as well on test data as on training data.
(Robustness here is withregard to the amount of training; there are other forms of robustness that are not intended inthis discussion.)
To get an idea of how much speech is needed to collect sufficient trainingdata, we give below an accounting of the number of triphones found in the speaker-dependent training data of the standard DARPA 1000-word resource management task.The training data for each speaker totals 600 sentences, orabout 30 minutes of speech.The total number of distinct riphones in the training set is 2524, out of a total ofapproximately 18,000 triphones in the 30 minutes.
The table below shows a table wherethe first column gives ranges for the number of occurrences of triphones and the secondcolumn gives the number of triphones whose occurrences in the training set are in thatrange.No.
Occurrences No.
Triphones1 5182-5 11106-10 37811-20 24321-50 200>50 752524For example, 518 distinct riphones occur only once in the whole training set, and 1110triphones occur two to five times.
Only 75 triphones have more than 50 instances; thesetriphones will be the ones that will have robust models.
Therefore, most of the triphoneswill not be well modeled when only 30 minutes are used for training.
The 518 triphonesthat occurred only once constitute l ss than 3% of the total number of triphones in the 30minutes of data.
This means that 3% is approximately the probability that we will see anew triphone by adding a single phoneme to the training data.
In every additional secondof speech (which contains approximately ten phonemes), the probability of seeing a newtriphone is about 25% only.
We conclude then that, to observe amuch larger set oftriphones and sufficiently to estimate robust models, will require the collection of manyhours of speech.To give an example of the importance of the amount of training on speechrecognition performance, we give an example using hidden Markov models with theDARPA 1000-word atabase.
Using the standard word-pair grammar supplied with this476database (this grammar has a perplexity of 60), the word error rate for a typical speakerunder several conditions are summarized in the table below.WORD ERROR RATETraining Set Test Data Training Data(minutes)15 8.9% 1.1%30 5.5% 1.6%Note that with 15 minutes of training, the word error rate on an independent data set is8.9%, which drops to 5.5% when 30 minutes of training speech is used (this is considereda significant reduction in word error rate).
To show that we would still expect a significantreduction in word error rate by increasing the training set size further, we look at the lastcolumn in the table above.
This column gives the word error rate when the recognition isperformed on the training data itself rather than on an independent test set.
In each of thetwo rows in the table, the difference between the two word error rates is a measure of therobustness of the recognition system: the less the difference the more the robustness.
Witha smaller training set, one would expect a lower error rate on the training data but a largererror rate on test data, as is indeed the case in the table.
By increasing the training to morethan 30 minutes, the two columns in the table will trace two converging curves, with thedifference between them decreasing with increasing training set size.
Because of the largedifference between the 5.5% and the 1.6% for 30-minute training, we will need a muchlarger amount of training data for the two word error rates to become similar.
Increasingthe training set size to several hours would be expected to reap benefits in reduced errorrates and a more robust system.The arguments made above, both in terms of the number of triphones that are likelyto be observed with new data and in terms of improvement of performance with increasedtraining, point to the need for substantial mounts of speech data for training purposes.
Tobe able to observe a larger number of triphones and sufficiently to estimate robust modelswill likely require the collection of hundreds of hours of speech.
While it may not bepractical to collect his amount of speech from each speaker, it would not be unreasonable,for example, to collect a few hours from each of twenty speakers for speaker-dependentresearch, and one hour from each of 1000 speakers for speaker-independent r search.COMPUTING FOR RESEARCHMuch of the time in speech recognition research is spent in trying out differentalgorithms and variations thereof.
For each experiment, one must estimate the modelsusing the available training data and then test the models using independent test data.
If ittakes 1 gigaflops to do real-time recognition, then given a 1-gigaflops machine, one coulddo recognition on ten hours of speech data in ten hours of actual time, which means thatone could do one experiment overnight.
However, a 100-megaflops machine wouldrequire 100 hours for the same experiment, which means that one would not run many ofthose experiments.Clearly, there may be ways to cut down the computation significantly.
In fact, formany research groups today, a large fraction of the researcher's time is spent speeding uptheir algorithms; otherwise they would not be able to do any interesting experiments.However, as we have seen in the examples above, the computing needs could be justifiablyincreased substantially, for example to include a larger number of phonetic ontexts or touse larger amounts of training data.477TEXT CORPORAText corpora or transcriptions of spoken corpora re needed in the development oflanguage models for spoken language systems.
The language model is that part of therecognizer that uses a priori knowledge of sentence generation tofacilitate the decisionabout what was said.
It must be capable of determining the likelihood that any possiblesequence of words was (will be) uttered.
The resulting knowledge is referred to as agrammar.
We use the term "grammar" here whether it involves familiar concepts uch as"noun", "verb", "subject", "noun phrase", "predicate", etc., or is defined in terms ofrelative frequency of occurrence of word sequences.For applications where understanding of the speech is not necessary, such asautomatic dictation, the currently most successful language model is based simply on therelative frequency of occurrence of word trigrams taken from several corpora of texttotaling 250 million words with a vocabulary of 20,000 words.
This trigram languagemodel is unexpectedly powerful, but it has a number of limitations: (1) It can base itspredictions only on a very short past (the last two words) of the utterance.
(2) The amountof text on which it is based is miniscule (2.5 x 10"'8) when compared to the number ofpossible trigrams from a 20,000 word vocabulary (8 x 10"'12).
(3) The language modelreflects anarrow field of discourse: office correspondence in the data processing industry.
(4) This method of model construction is inflexible as it requires huge data bases for eachdifferent recognizer application.
It seems clear that future language models hould in partbe based on more conventional grammatical pproaches.
Unfortunately, the utility ofcurrently existing rammars i quite limited for a variety of reasons, some of which are: (1)Limited coverage: Many naturally occurring sentences receive ither a great many analysesor none at all.
(2) Lack of statistical characterization: A sentence isdeemed either legal ornot; estimation of its likelihood is not attempted.
(3) Linguistic phenomena often receiveattention i accordance with their intrinsic intellectual interest, rather than with thefrequency of their use in natural discourse.
(4) Performance has been typically judged morevia counterexample or by system demonstration than by rigorous testing on a large corpusof data.For progress in realistic language modeling, it is clear that large data bases will haveto be acquired, independent of the language modeling approach taken.
To observe all ormost of the naturally occurring linguistic phenomena in a 20,000-word vocabulary, forexample, would require in principle billions of words of text.
A practical long-term goal toshoot for, however, would be to collect 100 million words of text to be used as a nationalresource.
The corpora must consist of a Variety of sources o as to contain statisticallysignificant samples of grammatical phenomena ofEnglish occurring in many applications,e.g., during text creation, dialog, interactive problem solving, etc.
To enable automatic"rule" discovery and evaluation of grammar quality and development progress, the textshould be linguistically annotated as to its linguistic structure (syntactic structure at aminimum, and semantic structure if possible).
An initial effort to collect 10 million wordsover a period of two years would be a reasonable initial goal.In support of the need for large corpora, we give the following concrete researchfact.
IBM has started annotating a corpus of debates from the Canadian parliament.
Afterprocessing a text of 500,000 words, it has been observed that the annotation utilizes a (sofar) unused context-free production rule at the rate of approximately one per additionalsentence!
Thus at least an order of magnitude larger sample will be required to exhibit asatisfactory fraction of linguistic phenomena occurring in even such a relativelyhomogeneous discourse domain.478STORAGE REQUIREMENTSComputer on-line storage will need to support the ready availability of largeamounts of speech and text data for training and test purposes.
This is data that will beused on an on-going basis for the development and testing of algorithms and, therefore,should reside for the most part on-line.
Currently, speech researchers utilize severalgigabytes of on-line storage for their work.
Advanced work in spoken language systemswill deal with at least an order of magnitude increase in the amount of data to be analyzed.Therefore, 100 gigabytes of on-line storage is a reasonable estimate of what would beneeded for future work.479
