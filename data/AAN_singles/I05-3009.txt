Domain Specific Word Extraction from Hierarchical Web Documents:A First Step Toward Building Lexicon Trees from Web CorporaJing-Shin ChangDepartment of Computer Science & Information EngineeringNational Chi-Nan University1, University Road, Puli, Nantou, Taiwan 545, ROC.jshin@csie.ncnu.edu.twAbstractDomain specific words and ontologicalinformation among words are importantresources for general natural languageapplications.
This paper proposes astatistical model for finding domainspecific words (DSW?s) in particulardomains, and thus building theassociation among them.
When applyingthis model to the hierarchical structureof the web directories node-by-node, thedocument tree can potentially beconverted into a large semanticallyannotated lexicon tree.
Somepreliminary results show that the currentapproach is better than a conventionalTF-IDF approach for measuring domainspecificity.
An average precision of65.4% and an average recall of 36.3%are observed if the top-10% candidatesare extracted as domain-specific words.1 Domain Specific Words and LexiconTrees as Important NLP ResourcesDomain specific words (DSW?s) are important?anchoring words?
for natural languageprocessing applications that involve word sensedisambiguation (WSD).
It is appreciated thatmulti-sense words appearing in the samedocument tend to be tagged with the same wordsense if they belong to the same common domainin the semantic hierarchy (Yarowsky, 1995).
Theexistence of some DSW?s in a document willtherefore be a strong evidence of a specific sensefor words within the document.
For instance, theexistence of ?basketball?
in a document wouldstrongly suggest the ?sport?
sense of the word????
(?Pistons?
), rather than its ?mechanics?sense.
It is also a personal belief that DSW-basedsense disambiguation, document classificationand many similar applications would be easierthan sense-based models since sense-taggeddocuments are rare while domain-aware trainingdocuments are abundant on the Web.
DSWidentification is therefore an important issue.On the other hand, the semantics hierarchyamong words (especially among sets of domainspecific words) as well as the membership ofdomain specific words are also importantresources for general natural language processingapplications, since the hierarchy will providesemantic links and ontological information (suchas ?is-A?
and ?part-of?
relationships) for words,and, domain specific words belonging to thesame domain may have the ?synonym?
or?antonym?
relationships.
A hierarchical lexicontree (or a network, in general) (Fellbaum, 1998;Jurafsky and Martin, 2000), indicative of sets ofhighly associated domain specific words andtheir hierarchy, is therefore invaluable for NLPapplications.Manually constructing such a lexiconhierarchy and acquiring the associated words foreach node in the hierarchy, however, is mostlikely unaffordable both in terms of time and cost.In addition, new words (or new usages of words)are dynamically produced day by day.
Forinstance, the Chinese word ????
(pistons) ismore frequently used as the ?sport?
or?basketball?
sense (referring to the ?Detroit64Pistons?)
in Chinese web pages rather than the?mechanics?
or ?automobile?
sense.
It istherefore desirable to find an automatic andinexpensive way to construct the wholehierarchy.Since the hierarchical web pages providesemantic tag information (explicitly from theHTML/XML tags or implicitly from thedirectory names) and useful semantic links, it isdesirable that the lexicon construction processcould be conducted using the web corpora.Actually, the directory hierarchy of the Web canbe regarded as a kind of classification tree forweb documents, which assigns an implicit hiddentag (represented by the directory name) to eachdocument and hence the embedded domainspecific words.
Converting such a hierarchy intoa lexicon tree is therefore feasible, provided thatwe can remove non-specific terms from theassociated document sets.For instance, the domain-specific words fordocuments under the ?sport?
hierarchy are likelyto be tagged with a ?sport?
tag.
These tags, inturn, can be used in various word sensedisambiguation (WSD) tasks and other hotapplications like anti-spamming mail filters.Such rich annotation provides a usefulknowledge source for mining various semanticlinks among words.We therefore will explore a non-conventionalview for constructing a lexicon tree from the webhierarchy, where domain-specific wordidentification turns out to be a key issue and thefirst step toward such a construction process.
Aninter-domain entropy (IDE) measure will beproposed for this purpose.2 Conventional Clustering View forConstructing Lexicon TreesOne conventional way to construct the lexiconhierarchy from web corpora is to collect theterms in all web documents and measure thedegree of word association between word pairsusing some well-known association metrics(Church and Hanks, 1989; Smadja et al, 1996)as the distance measure.
Terms of highassociation are then clustered bottom-up usingsome clustering techniques to build the hierarchy.The clustered hierarchy is then submitted tolexicographers to assign a semantic label to eachsub-cluster.
The cost will be reduced in this way,but could still be unaffordable.
Besides, it stilldepends on the lexicographers to assignappropriate semantic tags to the list of highlyassociated words.There are several disadvantages with thisapproach.
Firstly, the hierarchical relationshipamong the web documents, and hence theembedded DSW?s, is lost during the documentcollection process, since the words are collectedwithout considering where they come from in thedocument hierarchy.
The loss of suchhierarchical information implies that theclustered one will not match human perceptionquite well.
Secondly, the word association metricand the clustering criteria used by the clusteringalgorithm are not directly related to humanperception.
Therefore, the lexicographers maynot be able to adjust the clustered hierarchycomfortably.
Thirdly, most clustering algorithmsmerge terms in a binary way; this may not matchhuman perception as well.
As far as thecomputation cost is concerned, computation ofword association based on pairwise wordassociation metrics will be time consuming.Actually, such an approach may not be theonly option today, thanks to the large number ofweb documents, which are natively arranged in ahierarchical manner.3 Lexicon Tree Construction asDomain Specific Word Detectionfrom Web HierarchySince the web documents virtually form anextremely huge document classification tree, wepropose here a simple approach to convert it intoa lexicon tree, and assign implicit semantic tagsto the domain specific words in the webdocuments automatically.This simple approach is inspired by the factthat most text materials (webpages) in websitesare already classified in a hierarchical manner;the hierarchical directory structures implicitlysuggest that the domain specific terms in the text65materials of a particular subdirectory are closelyrelated to a common subject, which is identifiedby the name of the subdirectory.If we can detect domain specific words withineach document, and remove words that arenon-specific, and tag the DSW?s thus acquiredwith the directory name (or any appropriate tag),then we virtually get a hierarchical lexicon tree.In such a tree, each node is semantically linkedby the original web document hierarchy, andeach node has a set of domain specific wordsassociated with it.For instance, a subdirectory entitled?HQWHUWDLQPHQW? LV OLNHO\ WRKDYH D ODUJH QXPEHUof web pages containing domain specific termsOLNH ?VLQJHU? ?SRS VRQJV? ?URFN 	 UROO??Ah-Mei?
(nickname of a pop song singer),?DOEXP?DQGVRRQ6LQFHWKHVHZRUGVDUHKLJKO\associated with the ?HQWHUWDLQPHQW?
domain, wewill be able to collect the domain specific wordsof the ?entertainment?
domain from such adirectory.In the extraction process, the directory namescan be regarded as implicit sense labels orimplicit semantic tags (which may be differentfrom linguistically motivated semantic tags), andthe action to put the web pages into properlynamed directories can be regarded as an implicittagging process by the webmasters.
And, thehierarchical directory itself provides informationon the hierarchy of the semantic tags.From a well-organized web site, we will thenbe able to acquire an implicitly tagged corpusfrom that site.
Thanks to the webmasters, whosedaily work include the implicit tagging of thecorpora in their websites, there is almost no costto extract DSW?s from such web corpora.
Thisidea actually extends equally well for otherInternet resources, such as news groups and BBSarticles, that are associated with hierarchicalgroup names.
Extending the idea to wellorganized book chapters, encyclopedia andthings like that would not be surprised too.The advantages of such a constructionprocess, by removing non-specific terms, aremany folds.
First, the original hierarchicalstructure reflects human perception on document(and term) classification.
Therefore, the need foradjustment may be rare, and the lexicographersmay be more comfortable to adjust the hierarchyeven if necessary.
Second, the directory namesmay have higher correlation with linguisticallymotivated sense tags than those assigned by aclustering algorithm, since the web hierarchy wascreated by a human tagger (i.e., the webmaster).As far as the computation cost is concerned,pairwise word association computation is nowreplaced by the computation of ?domainspecificity?
of words against domains.
Thereduction is significant, from O(|W|x|W|) toO(|W|x|D|), where |W| and |D| represent thevocabulary size and number of domains,respectively.4 Domain Specific Word Extraction asthe Key Technology: AnInter-Domain Entropy ApproachSince the terms (words or compound words)in the documents include general terms as well asdomain-specific terms, the only problem then isan effective model to exclude thosedomain-independent terms from the implicittagging process.
The degree of domainindependency can be measured with theinter-domain entropy (IDE) as will be defined inthe following DSW (Domain-Specific Word)Extraction Algorithm.
Intuitively, a term thatdistributes evenly in all domains is likely to beindependent of any domain.
We therefore weightsuch terms less probable as DSW?s.
The methodcan be summarized in the following algorithm:Domain-Specific Word Extraction &Lexicon Tree Construction Algorithm:Step1 (Data Collection): Acquire a largecollection of web documents using a webspider while preserving the directoryhierarchy of the documents.
Strip unusedmarkup tags from the web pages.Step2 (Word Segmentation or Chunking):Identify word (or compound word)boundaries in the documents by applying aword segmentation process, such as66(Chiang et al, 1992; Lin et al, 1993), toChinese-like documents (where wordboundaries are not explicit) or applying acompound word chunking algorithms toEnglish-like documents (where wordboundaries are clear) in order to identifyinterested word entities.Step3 (Acquiring Normalized Term Frequenciesfor all Words in Various Domains): Foreach subdirectory d j , find the number ofoccurrences nij of each term wi in allthe documents, and derive the normalizedterm frequency f n Nij ij j / bynormalizing nij with the total documentsize, N nj iji{?
, in that directory.
Thedirectory is then associated with a set of!w d fi j ij, , tuples, where wi is thei-th words of the complete word list for alldocuments, d j is the j-th directory name(refer to as the domain hereafter), andf n Nij ij j / is the normalized relativefrequency of occurrence of wi in domaind j .Step4 (Identifying Domain-Independent Terms):Domain-independent terms are identified asthose terms which distributed evenly in alldomains.
That is, terms with largeInter-Domain Entropy (IDE) defined asfollows:logi i ij ijjijijijjH H w P PfPf{ { {?
?Terms whose IDE?s are above a thresholdare likely to be removed from the lexicontree since such terms are unlikely to beassociated with any particular domain.Terms with a low IDE, on the other hand,may be retained in a few domains with highnormalized frequencies.To appreciate the fact that a high frequencyterm may be more important in a domain, theIDE is further weighted by the termfrequency in the particular domain whendeciding whether a term should be removed.Currently, the weighting method is the sameas the conventional TF-IDF method(Baeza-Yates and Ribeiro-Neto, 1998;Jurafsky and Martin, 2000) for informationretrieval.
In brief, a word with entropy Hican be think of as a term that spreads in2**Hi domains on average.
The equivalentnumber of domains a term could be foundthen can be equated to 2 iHiNd  .
The termweight for wi in the j-th domain can thenbe estimated as:2logij ijiNW nNd?
?
u ?
??
?where N is the total number of domains.Unlike the conventional TF-IDF method,however, the expected number of domainsthat a term could be found is estimated byconsidering its probabilistic distribution,instead of simple counting.Step5 (Output): Sort the words in each domainby decreasing weights, Wij, and output thetop-k% candidates as the domain specificwords of the domain.
The percentage (k)can be determined empirically, or based onother criteria, such as their classificationperformance in a DSW-based text classifier(Chang, 2005).
The directory tree nowrepresents a hierarchical classification of thedomain specific terms for different domains.Since the document tree may not be reallyperfect, we have the option to adjust thehierarchy or the sets of words associated witheach node, after eliminating domain-independentterms from the directory tree.
The terms can befurther clustered into highly associated word lists,with other association metrics.
On the other hand,we can further move terms that are less specificto the current domain upward toward the root.This action will associate such terms with aslightly more general domain.
All these issueswill be left as our future works.67However, the current method is independentof the source web hierarchy.
Given a weborganized as an encyclopedium of biology, thecurrent method is likely to find out the livingspecies associated with each node of theunderlying taxonomy automatically.
With moreand more well organized web sites of variouskinds of knowledge online, the problems withimperfect web hierarchy will hopefully become aless important issue.5 EvaluationTo see how the above algorithm could be usefulas a basis for building a large lexicon tree fromweb pages, some preliminary results will beexamined in this section.A large collection of Chinese web pages wascollected from a local news site.
The size of theHTML web pages amounts to about 200M bytesin 138 subdomains (including the most specificdomains at the leaf nodes and their ancestordomains).
About 16,000 unique words areidentified after word segmentation is applied tothe text parts.It was observed, from some small sampledomains, that only around 10% of the words ineach subdomain are deemed domain specific.
(The percentages, however, may vary fromdomain to domain.)
The large vocabulary sizeand the small percentage of DSW?s suggest thatthe domain specific word identification task isnot an easy one.Table 1 shows a list of highly associateddomain-specific words of low inter-domainentropies and their domain names.
(LiteralEnglish translation for each term is enclosed inthe parenthesis.)
They are sampled from 4 out of138 subdomains.
The domain names virtually actas the semantic tags for such word lists.
The tags,being extracted from manually created directory,well reflect the senses of the words in eachsubdomains.Table 1 shows that many domain-specificwords can really be extracted with the proposedapproach in their respect domains.
For instance,the word ?pitcher?
(????)
is specifically usedin the ?baseball?
domain.
The domain specificwords and their domain tags are well associated.As a result of such association, lowinter-domain entropy words in the same domainare also highly correlated.
For instance, the term????
for calling a Japanese baseball team?manager?
is specifically used with?????
?
(Japanese professional baseball team),instead of a Chinese team, where ?manager?
iscalled differently.In addition, new usages of words, such as???
(Pistons)?
with the ?basketball?
sense,could also be identified by the current approach.Furthermore, it was also observed that manyirrelevant words (such as those words in thewebmasters?
advertisement) are rejected as theDSW candidates automatically since they havevery high inter-domain entropies.One can also find interesting lexicalrelations (Fellbaum, 1998) among the domaintags and domain specific words, form Table 1,such as:Hypernym/Hyponym: athlete ( ?
? )
vs.baseball game (???
); car (??)
vs.small car (???
).Has-Member/Member-Of: baseball team (??)vs.
manager (??
), pitcher (??
).Has-Part/Part-Of: car (??)
vs. engine cover(???
), tank (??
), safety system (????
), trunk (???
).Antonym: shot (??)
vs. defense (??
).Such lexical relations are, in general,interesting to lexical database builders.Furthermore, for data driven applications, suchfine details are unlikely to be listed in a generalpurpose lexical database.
([WUDFWLQJ'6:?s withthe inter-domain entropy (IDE) metric istherefore well founded.68In order to have a quantitative evaluation, wehave inspected a few domains of small sizes(each containing about 300 unique words or less)for a preliminary estimation.
The top-10%candidates with lowest inter-domain entropy,weighted by their term frequencies in theirrespect domains, are evaluated.
(The 10%threshold is selected arbitrarily.)
Table 2 showsthe results in terms of precision (P), recall (R)and F-measure (F).
The column with the?#Words?
label shows the numbers of uniquewords used in the 5 domains.Since it is difficult sometimes to have aconsistent judgment on ?domain specificity?, theestimation could vary drastically on otherdomains by other persons.
For this reason, thedegree of domain specificity is ranked from 0(irrelevant) to 5 (absolutely specific to thedomain) points.
Therefore, when computing theprecision and recall measures, a completely?correct?
answer should have a grading point ?5?.Fortunately, most terms are assigned the gradingpoint 5, with a few less certain cases assigned ?3?or ?4?.Baseball Broadcast-TV Basketball Car????
(Japanese professionalbaseball)????
(cable TV)??
(one minute)???(Kilo-c.c.)???
(baseball games)??
(the Dong Fong TVStation)??
(three seconds)???
(small car)??
(warm up)??
(start to work)???
(girl?s teams)??
(used car)??
(athlete) ???
(on air) ??
(fold; clip) ???
(engine cover)??
(time table)???
(radio-tvoffice)??
??
(tank)??
(cost) ??
??
(foul) ????
(baseball team) ??
(Ho-Hsin TVStation)??
(shot) ????
(marketatmosphere)??
(manager) ???
(governmentinformation office)???
(male team) ???
(destination)??
(practicing) ??
??
(defense) ??
(car delivery)??
(Hsin-Lung team) ??
(channel) ???
(championship) ??
(of the same grade)??
(course; diamond) ??(TV)??
(fullback) ????(co-development)??
(pitcher) ??
(movie) ??
(Pistons team)????
(safety system)??
(season) ??(hot)??
(national maleteam)??
(luggage)??
(schedule) ??
(video) ??
(Wallace) ???
(trunk)??
(the Sun team) ??
(entertainment) ??
(Philadelphia) ??
(c.c.
)Table 1.
Sampled domain specific words with low entropies.69Domain #Words R P FBaseball 149 29.7 68.0 41.3Basketball 277 26.2 60.7 36.6Broadcast-TV161 47.6 50.0 48.8Education 255 40.0 81.5 53.7Health-care263 38.2 66.9 48.6(Average) 36.3 65.4 45.8Table 2.
Performance of the Top-10% DSWcandidate lists in 5 sample domains.Table 2 shows that, by only gathering the first10% of the word lists, we can identify about 36%of the embedded domain specific words, and theprecision is as high as 65%.
Therefore, we canidentify significant amount of DSW?s aboutevery 1.5 entries from the top-10% list of lowentropy words.Since the TF-IDF (term frequency-inversedocument frequency) approach (Baeza-Yates andRibeiro-Neto, 1998; Jurafsky and Martin 2000) iswidely used in information retrieval applicationsfor indexing important terms within documents,it can also be applied to identify domain specificwords in various domains.
To make acomparison, the TF-IDF term weighting methodis also applied to the same corpus.
The?baseball?
domain is then inspected for theirdifferences.
It turns out that the top-10%candidate lists of both methods show the sameperformance.
However, the IDE measure appearsto reach the highest precision faster than theTF-IDF approach.
Furthermore, the IDE measurehas a better top-20% performance than that of theTF-IDF approach as listed in Table 3.Model R P FIDE 48.3 55.3 51.6TF-IDF 44.8 51.3 47.8Table 3.
Comparison of the top-20% candidatelist performance between IDE and TFIDF-based approaches.Although it is not sure whether the superiorityof the IDE approach will retain when examininglarger corpora, it does have its advantages inindicating the ?degree of specificity?.
Inparticular, the degree of domain specificity of aterm is estimated by considering thecross-domain probability distribution of the termin the current IDE-based approach.
Instead, theTF-IDF approaches only count the number ofdomains a term was found as a measure ofrandomness.
The IDE approach is thereforegaining a little bit performance than a TF-IDFmodel.The results partially confirm our expectationto build a large semantically annotated lexicontree from the web pages using the implicit tagsassociated with the web directory hierarchy andremoving non-specific words from webdocuments.6 Error Sources and Future WorksIn spite of some encouraging results, we alsoobserved some adverse effects in using the singleinter-domain entropy metric to identify domainspecific words.
For instance, some non-specificwords may also have low entropy simply becausethey appear in only one domain (IDE=0).
Sincesuch words cannot be distinguished from real?GRPDLQ-VSHFLILF? ZRUGV there should be otherknowledge sources to reduce false alarms of thiskind (known as the Type II errors.
)On the other hand, some multiple sense wordsmay have too many senses such that they areconsidered non-specific in each domain(although the sense is unique in each respectdomain).
This is a typical Type I error we haveobserved.As a result, further refinement of the purelystatistical model is required to improve theprecision of the current approach.
Currently, weprefer a co-training approach inspired by theworks in (Chang and Su, 1997; Chang, 1997),which is capable of augmenting a single IDE-likemetric with other information sources.We have also assumed that the directories ofall web sites are well organized in the sense thatthe domain labels (directory names) areappropriate representatives of the documentsunder the directories.
This assumption is notalways satisfied since it depends on the siteRZQHUV?YLHZVRQWKHGRFXPHQWV7KHUHDUHJRRG70chances that the hierarchies differ from site tosite.
Therefore, we may need some measures ofsite similarity, and approaches to unify thedifferent hierarchies and naming policies as well.The answers to such problems are not yet clear.However, we believe that the hierarchy of thedirectories (even though not well named) hadsubstantially reduces the cost for lexicogropherswho want to build a large semantically annotatedlexicon tree.
And the whole process will becomemore and more automatic as we refine the abovemodel against more and more data.7 Concluding RemarksThe major contribution of the proposed model isto extract highly associated sets ofdomain-specific words, and keeping theirhierarchical links with other sets of domainspecific words at low cost.
These sets of highlyassociated domain specific words can thus beused directly for sense disambiguation andsimilar applications.
The proposed model takesadvantages of the rich web text resources and theimplicit semantic tags implied in the directoryhierarchy for web documents.
Therefore, therequirement for manual tagging is negligible.The extracted lists of DSW?s are not only usefulfor word sense disambiguation but also useful asa basis for constructing lexicon databases withrich semantic links.
So far, an average precisionof 65.4% and an average recall of 36.3% areobserved if the top-10% candidates are extractedas domain-specific words.
And it outperforms theTF-IDF method for term weighting in the currenttask.AcknowledgementsPart of the work was supported by the NationalScience Council (NSC), ROC, under the contractNSC 90-2213-E-260-015-.ReferencesChristiane Fellbaum (Ed.).
1998.
WordNet: AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction toNatural Language Processing, ComputationalLinguistics, and Speech Recognition, Prentice-Hall,NJ, USA.David Yarowsky.
1995.
"Unsupervised Word SenseDisambiguation Rivaling Supervised Methods," inProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pp.189-196, MIT, MA, USA.Frank Smadja, Kathleen R. McKeown, and VasileiosHatzivassiloglou.
1996.
"Translating Collocationsfor Bilingual Lexicons: A Statistical Approach,"Computational Linguistics, 22(1):1-38.Jing-Shin Chang and Keh-Yih Su.
1997.
"AnUnsupervised Iterative Method for Chinese NewLexicon Extraction", International Journal ofComputational Linguistics and Chinese LanguageProcessing (CLCLP), 2(2):97-148.Jing-Shin Chang.
1997.
Automatic LexiconAcquisition and Precision-Recall Maximization forUntagged Text Corpora, Ph.D. dissertation,Department of Electrical Engineering, NationalTsing-Hua University, Hsinchu, Taiwan, R.O.C.Jing-Shin Chang.
2005.
"Web DocumentClassification into a Hierarchical Document TreeBased on Domain Specific Words", submitted.Ken Church and Patrick Hanks.
1989.
"WordAssociation Norms, Mutual Information, andLexicography," Proc.
27th Annual Meeting of theACL, pp.
76-83, University of British Columbia,Vancouver, British Columbia, Canada.Ming-Yu Lin, Tung-Hui Chiang and Keh-Yih Su.1993.
"A Preliminary Study on Unknown WordProblem in Chinese Word Segmentation,"Proceedings of ROCLING VI, pp.
119-142.Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval, Addison Wesley,New York.Tung-Hui Chiang, Jing-Shin Chang, Ming-Yu Lin andKeh-Yih Su.
1992.
"Statistical Models for WordSegmentation and Unknown Word Resolution,"Proceedings of ROCLING-V, pp.
123-146, Taipei,Taiwan, R.O.C.71
