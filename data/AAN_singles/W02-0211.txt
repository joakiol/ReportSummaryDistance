Discourse Processing for Explanatory Essays in Tutorial ApplicationsPamela W. Jordan and Kurt VanLehnLearning Research and Development CenterUniversity of PittsburghPittsburgh PA 15260[pjordan,vanlehn]@pitt.eduAbstractThe Why-Atlas tutoring system presentsstudents with qualitative physics questionsand encourages them to explain their an-swers via natural language.
Althoughthere are inexpensive techniques for ana-lyzing explanations, we claim that betterunderstanding is necessary for use withintutoring systems.
In this paper we de-scribe how Why-Atlas creates and utilizesa proof-based representation of student es-says.
We describe how it creates the proofgiven the output of sentence-level under-standing, how it uses the proofs to givestudents feedback, some preliminary run-time measures, and the work we are cur-rently doing to derive additional benefitsfrom a proof-based approach for tutoringapplications.1 IntroductionWhereas most explanations are produced andadapted to benefit or inform a hearer, a self-explanation is produced for the benefit of thespeaker.
If there is a hearer he often already knowsall about the topic as in a tutoring context.
Self-explanation is a cognitively valuable pedagogical ac-tivity because it leads students to construct knowl-edge (Chi et al, 1994), and it can expose deep mis-conceptions (Slotta et al, 1995).
But it is diffi-cult to encourage self-explanation without giving thestudent substantive feedback on what they generate(Aleven and Koedinger, 2000; Chi et al, 2001).
Togive substantive feedback the system has to be ableto understand student explanations to some degree.The Why-Atlas system presents students withqualitative physics problems and encourage them towrite their answers along with detailed explanationsfor their answers.
While physics misconceptionshave proven to be particularly resistant to repair,practice with qualitative physics questions helps inovercoming some of these misconceptions (Hake,1998).The student explanation shown in (1), whichis from our corpus of human-human computer-mediated tutoring sessions, illustrates how challeng-ing these explanations are for a system to under-stand.
The problems we have examined require ashort essay with an average of 6.9 sentences to fullyexplain to the satisfaction of experienced physics in-structors.
(1) Question: Suppose you are running in a straight lineat constant speed.
You throw a pumpkin straight up.Where will it land?
Explain.Explanation: Once the pumpkin leaves my hand,the horizontal force that I am exerting on it no longerexists, only a vertical force (caused by my throwingit).
As it reaches it?s maximum height, gravity (exertedvertically downward) will cause the pumpkin to fall.Since no horizontal force acted on the pumpkin fromthe time it left my hand, it will fall at the same placewhere it left my hands.Statistical text classification approaches, such aslatent semantic analysis (Landauer et al, 1998),have shown promise for classifying a student expla-nation into medium-grained good and bad categories(Graesser et al, 2000).
For instance, a medium-Philadelphia, July 2002, pp.
74-83.
Association for Computational Linguistics.Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,grained category that should match (1) is the often-observed impetus misconception:If there is no force on a moving object, itslows down.Such medium-grained categories typically havemultiple propositions and contain multiple contentwords.
While successful with medium-grainedclasses, statistical approaches are not yet able to dis-tinguish subtle but important differences betweengood and bad explanations.
Statistical classificationis insensitive to negations1 , anaphoric references2 ,and argument ordering variations3 and its inferenc-ing is weak4.
To capture these subtle differencesand to allow us to respond more directly to what thestudent actually said5, we need the precision possi-ble so far only with symbolic approaches.
So Why-Atlas parses each sentence into a propositional rep-resentation.The PACT Geometry Tutor is an operational pro-totype that does a finer-grained symbolic classifi-cation (Aleven et al, 2001).
PACT also parses astudent explanation into a propositional representa-tion but then uses LOOM to classify these into fine-grained categories that typically express one propo-sition.
This approach looks promising (Aleven etal., 2001), but the system?s goal is to elicit a justifi-cation for a step in a geometry proof and generallythese can be expressed with a single sentence thatsuccinctly translates into a small number of proposi-tions.
It isn?t clear that this approach will work wellfor the longer, more complex explanations that theWhy-Atlas system elicits.Instead of classifying propositions, the Why-Atlas system constructs abductive proofs of them.1A good explanation followed by ?But I don?t think that willhappen.?
would be classified as good.2In (1) above, it would tend to misclassify the last clause asthe correct answer ?the pumpkin will land in my hands?
becauseit does not understand the temporal anaphora.3The difference between x accelerates faster than y and yaccelerates faster than x would not be detected.4In (1), the student has the extreme belief that the pumpkinhas no horizontal velocity.
This would probably not be recog-nized as a case of ?slowing down?
by statistical classification.5When a true statement lacks precision, the tutor should ac-knowledge the correct statement and elicit more precision ratherthan continuing as if it were wrong.
For example, if a studentmakes a correct statement about the velocity of an object but didnot report it in terms of the horizontal and vertical componentsof the velocity, the tutor should ask which was intended.A proof-based approach gives more insight intothe line of reasoning the student may be follow-ing across multiple sentences because proofs of thepropositions share subproofs.
Indeed, one propo-sition?s entire proof may be a subproof of the nextproposition.
Moreover, subtle misconceptions suchas impetus are revealed when they must be used toprove a proposition.Abductive inference has a long history in planrecognition, text understanding and discourse pro-cessing (Appelt and Pollack, 1992; Charniak, 1986;Hobbs et al, 1993; McRoy and Hirst, 1995; Las-carides and Asher, 1991; Rayner and Alshawi,1992).
We are using an extended version of SRI?sTacitus-lite weighted abductive inference engine(Hobbs et al, 1993) as our main tool for buildingabductive proofs.
We had to extend it in order to useit for domain as well as language reasoning.
As ad-vised in (Appelt and Pollack, 1992), abductive infer-ence requires some application specific engineeringto become a practical technique.In this paper we describe how the system createsand utilizes a proof-based representation of studentessays.
We describe how it creates the proof giventhe output of sentence-level understanding, how ituses the proofs to give students feedback, some pre-liminary run-time measures, and the work we arecurrently doing to derive additional benefits from aproof-based approach for tutoring applications.First we give an overview of the Why-Atlas tutor-ing system architecture.
Next we give some back-ground on weighted abduction and Tacitus-lite+ anddescribe how it builds an abductive proof.
Next wedescribe how the system uses the proofs to give stu-dents feedback on their essays.
Finally, we discussefficiency issues and our future evaluation plans.2 Overview of the Why-Atlas TutoringSystemThe architecture for the Why-Atlas qualitativephysics tutoring system is shown in Figure 1.
Theuser interface for the system is a screen area in whichthe physics question is displayed along with an essayentry window and a dialogue window.
As the stu-dent enters an answer and explanation for a qualita-tive physics question the sentence-level understand-ing module builds sets of propositions and passes(APE)DialogueEngineInterfaceTutorialStrategistpropositionsDiscourse?levelUnderstandingInference Engine(Tacitus?lite+)proofsproofsorderedSentence?levelRealization(RealPro)Discourse Managerordered searchqueuestudentstringsSentence?levelUnderstanding(Carmel/Rainbow)HistoryLanguage andgoalsgoal orclassDomain axiomsandaxiomsqueuesearchresponseKCDpropositions orresponse classes directive goaland propositionstutorstringstutor stringtutor stringFigure 1: Why-Atlas Tutoring System Architecturethem, via the discourse manager, to the discourse-level understanding module.
Each set of proposi-tions represents one interpretation of a sentence.
Theuser interface and the sentence-level understandingcomponents are described in detail in (Rose?, 2000;Freedman et al, 2000).The discourse-level understanding module useslanguage and domain reasoning axioms and theTacitus-lite+ abductive inference engine to create aset of proofs that offer an explanation for the stu-dent?s essay and give some insight into what thestudent may believe about physics and how to ap-ply that knowledge.
The discourse-level under-standing module updates the propositions and thesearch queue for proofs in the history with the resultsfrom Tacitus-lite+.
This part of the history supportsanaphora resolution and processing of revisions astudent may make to his essay.
The discourse man-ager module selects and sends the best proofs to thetutorial strategist.The tutorial strategist identifies relevant commu-nicative goals.
Currently there are four categories ofcommunicative goals.
Two of these, disambiguatingterminology and clarifying the essay, are addressedvia directives to modify the essay.
The other two,remediating misconceptions and eliciting more com-plete explanations, are addressed via dialogue.
Mis-conceptions are detected when the proof includesan axiom that is incorrect or inapplicable.
Incom-pleteness is detected under two conditions.
First,there may be multiple proofs that are equally good.This condition indicates that the student did not sayenough in his explanation for the system to decidewhich proof best represents what the student?s rea-soning may be.
Each possible line of reasoningcould point to different underlying problems withthe student?s physics knowledge.
The second con-dition occurs when the student fails to explicitlystate a mandatory point, which is a proposition thatdomain instructors require of any acceptably com-plete essay.
Once the tutorial strategist has identi-fied communicative goals it prioritizes them accord-ing to curriculum constraints and sends them to thediscourse manager, which selects the highest prior-ity goal after taking dialogue coherency into accountand sends the goal to either the dialogue engine orthe sentence-level realization module.The dialogue engine initiates and carries out a dia-logue plan that will either help the student recognizeand repair a misconception or elicit a more com-plete explanation from the student.
The main mech-anism for addressing these goals are what we call aknowledge construction dialogue (KCD) specifica-tion.
A KCD specification is a hand-authored push-down network.
Nodes in the network are either thesystem?s questions to students or pushes and popsto other networks.
The links exiting a node corre-spond to anticipated responses to the question.
Eachquestion is a canned string, ready for presentationto a student.
The last state of the network is savedin the history and the sentence-level understandingmodule accesses this in order to get information foranalysis of student responses.
The sentence-levelunderstanding module uses a classification approachfor dialogue responses from the student since cur-rently the dialogue plans are limited to ones that ex-pect short, direct responses.
During a dialogue, re-sponse class information is delivered directly to thedialogue engine via the discourse manager.
The di-alogue engine is described further in (Rose?
et al,2001).The other communicative goals, disambiguatingterminology and clarifying the essay, are addressedby the discourse manager as directives for the stu-dent to modify the essay.
It passes propositions and agoal to the sentence-level realization module whichuses templates to build the deep syntactic structuresrequired by the RealPro realizer (Lavoie and Ram-bow, 1997) for generating a string that communi-cates the goal.When the discourse manager is ready to end itsturn in the dialogue, it passes the accumulated natu-ral language strings to the user interface.
This out-put may also include transitions between the goalsselected for the turn.While a dialogue is in progress, the discourse-level understanding and tutorial strategist modulesare bypassed until the essay is revised.
Once the stu-dent revises his essay, it is reanalyzed and the cy-cle repeats until no additional communicative goalsarise from the system?s analysis of the essay.Although the overall architecture of the system isa pipeline, there is feedback to earlier modules viathe history.
Only the discourse-level understand-ing and discourse manager modules are internallypipelines, the rest are rule-based.3 Background on Weighted Abduction andTacitus-lite+Abduction is a process of reasoning from an obser-vation to possible explanations for that observation.In the case of the Why-Atlas system the observationsare what the student said and the possible explana-tions for why the student said this are the physicsqualitative axioms (both good and bad) and order-ings of those axioms that support what the studentsaid.
To arrive at the explanation, some assump-tions have to be made along the way since all theinferences that underly an explanation will not beexpressed.Weighted abduction is one of several possible for-malisms for realizing abductive reasoning.
Withweighted abduction there is a cost associated withmaking an assumption during the inference process.Following the weighted abductive inference algo-rithm described in (Stickel, 1988), Tacitus-lite is acollection of axioms where each axiom is expressedas a Horn clause.
Further, each conjunct pi has aweight wi associated with it, as in (2).
The weight isused to calculate the cost of assuming pi instead ofproving it where cost(pi) = cost(r) ?
wi.
(2) p1w1 ?
?
?
?
?
pnwn ?
rGiven a goal or observation to be proven, Tacitus-lite takes one of four actions; 1) assumes the obser-vation at the cost associated with it 2) unifies with afact for zero cost 3) unifies with a literal that has al-ready been assumed or proven at no additional cost4) attempts to prove it with an axiom.All possible proofs could be generated.
How-ever, Tacitus-lite allows the applications builder toset depth bounds on the number of axioms appliedin proving an observation and on the global num-ber of proofs generated during search.
Tacitus-litemaintains a queue of proofs where the initial proofreflects assuming all the observations and each ofthe four above actions adds a new proof to the queue.The proof generation can be stopped at any point andthe proofs with the lowest cost can be selected as themost plausible proofs for the observations.Tacitus-lite uses a best-first search guided byheuristics that select which proof to expand, whichobservation or goal in that proof to act upon, whichaction to apply and which axiom to use when that isthe selected action.
Most of the heuristics in Why-Atlas are specific to the domain and application.SRI?s release of Tacitus-lite was subsequently ex-tended by the first author of this paper for the re-search project described in (Thomason et al, 1996).It was named Tacitus-lite+ at that time.
Two mainextensions from that work that we are making useof are: 1) proofs falling below a user defined costthreshold halt the search 2) a simple variable typingsystem reduces the number of axioms written andthe size of the search space (Hobbs et al, 1988, pg102).Unlike the earlier applications of Tacitus-lite+,Why-Atlas uses it for both shallow qualitativephysics reasoning and discourse-level language rea-soning.
To support qualitative physics reasoningwe?ve made a number of general inference engineextensions, such as improved consistency checking,detecting and avoiding reasoning loops and allowingthe axiom author to express both good and bad ax-ioms in the same axiom set.
These recent extensionsare described further in (Jordan et al, 2002).4 Building an Abductive ProofThe discourse-level understanding module uses lan-guage axioms and the Tacitus-lite+ abductive in-ference engine to resolve pronominal and temporalanaphora and make other discourse-level languagerelated inferences.
It transforms the sentence-levelpropositions into more complete propositions giventhe context of the problem the student is solving(represented as facts) and the context of the preced-ing sentences of the essay.From these discourse-level propositions, proofsare built and analyzed to determine appropriate com-municative actions.
To build these proofs, thediscourse-level understanding module uses domainaxioms, the above resulting propositions and againthe Tacitus-lite+ abductive inference engine.We?ve separated the discourse-level language ax-ioms from the domain axioms both for efficiencyand modularity because there is generally only asmall amount of interaction between the languageand domain axioms.
Separating them reduces thesearch space.
In cases where interaction within asingle axiom is necessary, we?ve place these axiomsin the set of language axioms.
The system currentlyhas 90 language axioms and 95 domain axioms.
Thedomain axioms fully cover 5 problems as well asparts of many other problems.We will describe in more detail each of thesestages of building the proof in the sections that fol-low.4.1 Applying Discourse-level Language Axiomsto Sentence-level PropositionsThe discourse-level language axioms are currentlyaddressing the local resolution of pronominal andtemporal anaphora, flattening out embedded rela-tionships and canonicalizing some lexical choicesthat can only be resolved given the context of theproblem.
We are still developing and testing ax-ioms that will better address pronominal and tempo-ral anaphora inter-sententially and axioms that willgenerate additional propositions for quantifiers andplurals.Pronominal Anaphora.
It is generally easy to re-solve pronominal anaphora in the context of a qual-itative physics problem because there are a smallnumber of candidates to consider.
For example, inthe case of the pumpkin problem in (1), there areonly four physics bodies that are likely to be dis-cussed in a student essay; the pumpkin, the runner,the earth and air.The system is able to resolve simple intra-sentential pronominal references using language ax-ioms.
The objects described in a single sentenceare the candidate set and argument restrictions ruleout many of these candidates.
But to resolve inter-sentential anaphora, as in (3), the system currentlyrelies on the domain axioms.
The domain axiomswill bind the body variables to their most likelyreferents during unification with facts, and previ-ously assumed and proven propositions similarly to(Hobbs et al, 1988).
(3) The man is exerting a force on it.But in the case of anaphoric references to physicalquantities such as velocity, acceleration and force,as in (4), we need to extend the language axioms tohandle these cases because it involves too much un-constrained search for the domain axioms to resolvethese.
This is because the physical quantities are thepredicates that most strongly influence the domainreasoning.
(4) The velocity is constant before the pumpkin isthrown.
But after the release, it will decreasebecause there is no force.To extend the language axioms to address inter-sentential anaphora we need to implement and testa recency ordering of the physics bodies and quan-tities that have already been discussed in the essay.But we expect this to be simple to do since the essaysgenerally only involve one discourse segment.Temporal Anaphora.
As with pronominalanaphora, temporal anaphora is usually clear be-cause the student often explicitly indicates whenan event or state occurs relative to another event orstate as with the first sentence of the explanationpresented in (1).
In these cases, the domain-levelreasoning will be able to unify the anchor event orstate with an already known event or state in theproof it is constructing.When there is no temporal anchor the domain-level search is too under-constrained so the languageaxioms resolve the temporal orderings.
In somecases world knowledge is used to infer the temporalrelationships as in (5).
Here we know that to catchan object it must have been thrown or dropped be-forehand and so the event in (5a) must occur afterthe event in (5b).
(5) a.
The man catches the pumpkin.b.
This is because they had the same velocitywhen he threw it.Otherwise, the language axioms use informationabout tense and aspect and default orderings rela-tive to these to guide inferences about temporal rela-tionships ((Kamp, 1993; Dowty, 1986; Partee, 1984;Webber, 1988) inter alia).Embedded Relationships.
In the physics essayswe are addressing, there is a tendency to expressmultiple relations within a single sentence as in (6).Here the ?equal?
and ?opposite?
relations are em-bedded in a temporal ?when?
relation.
In this casethe sentence-level understanding module is not inthe best position to indicate the specific constraintsthat each of these relations imposes so this is han-dled by discourse-level understanding.
It wouldalso impose a greater burden on the domain-levelproof building if these relationships were not re-solved beforehand.
For example, in the case of thelast clause in (6) there is an elliptical reference thatcould cause the domain-level a great deal of uncon-strained search.
(6) When the magnitude of the pumpkin?s veloc-ity equals the man?s, the pumpkin?s velocityis in the opposite direction.Canonicalizing Lexical Usage.
One simple casein which the language axioms canonicalize lexicalitems has to do with direction.
For example, saying?move up the inclined plane?
should be interpretedas a positive direction for the horizontal componenteven though the phrase contains ?up?.
The axiomsare able to canonicalize references such as up, down,left, right, north, south into a positive or negative di-rection relative to an axis in a coordinate system thatmay be tilted slightly to align with planes.
This is anexample of the kinds of axioms in which languageand domain knowledge are interacting within a sin-gle axiom.Quantifiers and Plurals In our target essays,there is frequent usage of quantifiers and pluralswith respect to physics bodies and frequent use ofquantifiers with respect to parameters of physicalquantities (e.g.
?at all times?
?all the magnitudesof the velocities?
).We have recently completed our specification fora sentence-level representation of quantifiers andplurals.
From this representation the language ax-ioms will generate an appropriate number of newpropositions to use in the proof building stage, giventhe context of the problem and the expression recog-nized from sentence-level processing.Although we have not yet implemented and testedthis set of language axioms, we have successfullyhand-encoded sentences such as (7) into both theirsentence-level and discourse-level representationsand have used the latter successfully in the finalproof building process.
For example, for (7), thesystem creates two equivalent propositions about ac-celeration, each referring to different balls.
In ad-dition, both of these propositions are related to twohorizontal component of velocity of pumpkin is decreasinghorizontal component of force of air on pumpkin is 0Student said: velocity of the pumpkin is decreasinghorizontal component of the total force on pumpkin is 0(assume)(assume)(given)horizontal component of force of man on pumpkin is 0man applies a force of 0 to the pumpkinhave impetus bugFigure 2: Example of Simplified Abductive Proof for ?The pumpkin moves slower because the man is notexerting a force on it.
?additional propositions about the force of gravity ap-plying to the same ball as in its related accelerationproposition.
(7) The acceleration of both balls is increasingdue to the force of earth?s gravity.4.2 Applying Domain-level Axioms to Build anExplanatory ProofThe propositions produced by applying the languageaxioms are the goals that are to be proven usingdomain-level axioms.
Figure 2 is an example of asimplified abductive proof for sentence (8).
(8) The pumpkin moves slower because the manis not exerting a force on it.Each level of downward arrows from the gloss ofa proposition in Figure 2 represents a domain ax-iom that can be used to prove that proposition.
Oneway to prove that the velocity of the pumpkin is de-creasing is to prove that just the horizontal compo-nent of the velocity vector is the one that is decreas-ing since the context of the question (see (1)) makesthis a likely interpretation.
Alternatively, the sys-tem could request that the student be more preciseby asking which components of the velocity vectorare decreasing.In the case of trying to prove that the horizon-tal component is decreasing, Tacitus-lite+ is apply-ing a bad physics axiom that is one manifestation ofthe impetus misconception; the student thinks that aforce is necessary to maintain a constant velocity.
Inthis case it assumes the student has this misconcep-tion but alternatively the system could try to gathermore evidence that this is true by asking the studentdiagnostic questions.Next Tacitus-lite+ proves that the total force onthe pumpkin is zero by proving that the possible ad-dend forces are zero.
In the context of this problem,it is a given that air resistance is negligible and so itunifies with a fact for zero cost.
Next it assumes thatthe student believes the man is applying a horizontalforce of 0 to the pumpkin.Finally, it still needs to prove another propositionthat was explicitly asserted by the student; that theforce of the man on the pumpkin is 0.
As with thevelocity, it will try to prove this by proving that thehorizontal component of that force is zero.
Since ithas already assumed that this is true, the abductiveproof is finished and ready to be further analyzedby the tutorial strategist module to give additionalfeedback to the student.4.3 Incrementally Processing an EssayWe have also extended Tacitus-lite+ to run incre-mentally so that it can start processing before thestudent completes his essay.
In this way it can takeadvantage of the processing lull as the student com-poses his essay.
In simulations of various typingspeeds, (Rose?
et al, 2002) estimated that there is a60 second processing lull during the completion of asentence after subtracting out a 5 second average in-cremental parsing cost.
During this lull it can buildproofs using the previous sentences in the essay.To run Tacitus-lite+ incrementally, we added afunction that takes as input a proof queue and thenew goals that are to be proven and returns a newproof queue.
The discourse-level understandingmodule builds the input proof queue by finding theproofs in the most recent queue with which the newgoals are consistent and adding the new goals to acopy of each of those proofs.
We then modifiedTacitus-lite+ to take an arbitrary proof queue as in-put.The discourse-level understanding module storesand selects proof queues, which are returned byTacitus-lite+ after it attempts to prove a sentence.Suppose for example that each sentential input istreated as a separate input to Tacitus-lite+ and thatsentence Sk has already been processed and yieldedproof queue Qk.
As the next sentence Sk+1 arrives,a copy of Qk is updated with proofs that includeSk+1 as new information to be proven.
But if Sk+1conflicts with every proof in the copy of Qk, then anearlier proof queue is tried.
Similarly, if a studentmodifies a previously processed sentence, the origi-nal sentence is regarded as having been deleted.
Theinference process backs up to the point just beforethe deleted sentence was processed and reprocessesthe substituted sentence and all that follows it.
Thismechanism for backing-up allows the inference pro-cess to be incremental.At the end of composing an essay, the student willin the best case have to wait the length of time thatit takes to finish parsing the last sentence of the es-say plus the length of time that it takes to extend theproof by one sentence.
In the worst case, which iswhen he modifies the first sentence or inserts a newfirst sentence, he will have to wait the same amountof time as he would for non-incremental discourse-level understanding.5 Deriving Feedback for Students FromPlausible ProofsTo identify communicative goals the tutorial strate-gist next analyzes the best proofs.
Currently it exam-ines just one of the best proofs by applying a set oftest patterns to parts of the proof.
It can test for com-binations of patterns for givens (mainly to get bind-ings for variables in a pattern), for assumed proposi-tions, for propositions asserted in the student?s essay,and for inferred propositions.
In addition it can alsotest for missing patterns in the proof and for particu-lar domain axioms to have been used.
Each goal thatthe system is capable of addressing is linked to setsof patterns that are expected to be indicative of it.In the case of the proof for (8), the tutorial strategistidentifies a dialogue goal that addresses the impe-tus misconception as being relevant since an impetusaxiom is part of the proof.In addition to engaging students in a dialogue, thesystem can also give direct, constructive feedback onthe essays they are composing.
When there are mul-tiple interpretations, it is better to ask the student tomake certain things in the essay clearer.
The tutorialstrategist includes test patterns that target importantdetails that students often leave out.
For example,suppose the student says that the velocity is increas-ing but this is only true for the vertical componentof the velocity vector.
It may then be important toclarify which component of the velocity the studenthas in mind since thinking that the horizontal com-ponent is increasing indicates a misconception.It is also possible that two propositions in an essaywill be contradictory.
In this case the system pointsout that there is a conflict, describes the conflict anddirects the student to repair it.We expect to extend the tutorial strategist moduleso that if there are multiple best proofs, it will askthe student questions that will help it disambiguatewhich proof is most representative of the student?sintended meaning for the essay.6 Preliminary Results and Future PlansAlthough we?ve found that incremental understand-ing is successful at taking advantage of the pro-cessing lull during which the student composes hisessay, we still need to fine-tune it so as to mini-mize both the need to back-up and how much under-constrained searching it does (i.e.
the more Tacitus-lite+ has of the student?s explanation the more con-strained the search is).
Currently, Tacitus-lite+ runsafter every new sentence that is recognized by thesentence-level understanding module.
During eachof these runs Tacitus-lite+ continues until one of itsrun-time thresholds is exceeded.We plan to also experiment with other ways ofbounding the run-time for Tacitus-lite+ during incre-mental processing.
For example, we might impose aspecific time-limit that is based on the expected 60second processing lull while the student composeshis next sentence.In initial timing tests, using a set of 5 correct es-says that involved no backing up, the average incre-mental processing time per sentence when we set thesearch bound to 50 proofs and the assumption costthreshold to .056, is 21.22 seconds.
The worst casetime for extending a proof by one sentence was 98seconds and the best was 1 second.
So in the bestcase, which is when no previous sentences have beenmodified, the student will wait on average 21.22 sec-onds after he completes the last sentence in his essayfor a response from Why-Atlas.In human-human computer-mediated tutoring, wefound that in the worst case the student waits 2 min-utes for a reply from the tutor after completing theessay.
The wait time in the case of the human tu-tor is a combination of the time it takes to read andanalyze the student?s response and then compose areply.7 Although the timings are inconclusive andnot directly comparable, it gives us an order of mag-nitude for tolerable wait times.We will complete a 5 week formative evaluationof the Why-Atlas system in which we will comparethe learning gains of 24 students to other sets ofstudents in three other conditions; 1) a text control2) human tutoring 3) another tutoring system thatuses statistical classification only.
During these tri-als, we will log decisions and processing times foreach module of the system.
From these detailed logswe will be able to better evaluate the speed and cor-rectness of each system module.AcknowledgmentsThis research was supported by MURI grantN00014-00-1-0600 from ONR Cognitive Scienceand by NSF grant 9720359.
We thank the entireNLT team for their many contributions in creatingand building the Why-Atlas system.
In particularwe thank Michael Ringenberg, Maxim Makatchev,Uma Pappswamy and Michael Bo?ttner for theirwork with Tacitus-lite+ and the domain axioms andRoy Wilson for his work with the sentence-level re-alization module.6An assumption cost of 1 means everything is assumed anda cost of 0 means that nothing was assumed.7In these timing studies, we also did not allow the tutor tosee the student input until the student had finished composingit.
This was because our previous experiences with computer-mediated human tutoring have shown that some human tutorshave a propensity for referring to something the student hadstarted to write and then deleted.
Our goal was to try to collectinteractions that would be closer to those we expected with anintelligent tutoring system and was not primarily for comparingefficiency of a computer tutor to a human one.ReferencesVincent Aleven and Kenneth R. Koedinger.
2000.
Theneed for tutorial dialog to support self-explanation.
InBuilding Dialogue System for Tutorial Applications,Papers of the 2000 AAAI Fall Symposium.Vincent Aleven, Octav Popescu, and Kenneth R.Koedinger.
2001.
A tutorial dialogue system withknowledge-based understanding and classification ofstudent explanations.
In Working Notes of 2nd IJCAIWorkshop on Knowledge and Reasoning in PracticalDialogue Systems.Douglas Appelt and Martha Pollack.
1992.
Weighted ab-duction for plan ascription.
User Modeling and User-Adapted Interaction, 2(1 ?
2):1 ?
25.Eugene Charniak.
1986.
A neat theory of marker pass-ing.
In Proceedings of the 5th National Conference onArtificial Intelligence (AAAI?86), pages 584 ?
588.Michelene T. H. Chi, Nicholas de Leeuw, Mei-HungChiu, and Christian LaVancher.
1994.
Eliciting self-explanations improves understanding.
Cognitive Sci-ence, 18:439?477.Michelene T. H. Chi, Stephanie A. Siler, Heisawn Jeong,Takashi Yamauchi, and Robert G. Hausmann.
2001.Learning from human tutoring.
Cognitive Science,25(4):471?533.David Dowty.
1986.
The effects of aspectual class onthe temporal structure of discourse: Semantics or prag-matics?
Linguistics and Philosophy, 9(1).Reva Freedman, Carolyn Rose?, Michael Ringenberg, andKurt VanLehn.
2000.
ITS tools for natural languagedialogue: A domain-independent parser and planner.In Proceedings of the Intelligent Tutoring SystemsConference.Arthur C. Graesser, Peter Wiemer-Hastings, KatjaWiemer-Hastings, Derek Harter, Natalie Person, andthe TRG.
2000.
Using latent semantic analysis toevaluate the contributions of students in autotutor.
In-teractive Learning Environments, 8:129?148.Richard R. Hake.
1998.
Interactive-engagement versustraditional methods: A six-thousand student survey ofmechanics test data for introductory physics students.American Journal of Physics, 66(4):64?74.Jerry Hobbs, Mark Stickel, Paul Martin, and Douglas Ed-wards.
1988.
Interpretation as abduction.
In Proc.26th Annual Meeting of the ACL, Association of Com-putational Linguistics, pages 95?103.Jerry Hobbs, Mark Stickel, Douglas Appelt, and PaulMartin.
1993.
Interpretation as abduction.
ArtificialIntelligence, 63(1?2):69?142.Pamela W. Jordan, Maxim Makatchev, Michael Ringen-berg, and Kurt VanLehn.
2002.
Engineering theTacitus-lite weighted abductive inference engine foruse in the Why-Atlas qualitative physics tutoring sys-tem.
Manuscript, University of Pittsburgh.Hans Kamp.
1993.
From Discourse to Logic; Intro-duction to Modeltheoretic Semantics of Natural Lan-guage, Formal Logic and Discourse RepresentationTheory.
Kluwer Academic Publishers, Dordrecht Hol-land.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25:259?284.Alex Lascarides and Nicholas Asher.
1991.
Discourserelations and defeasible knowledge.
In 29th AnnualMeeting of the Association for Computational Linguis-tics, pages 55 ?
62.Benoit Lavoie and Owen Rambow.
1997.
A fast andportable realizer for text generation systems.
In Pro-ceedings of the Fifth Conference on Applied NaturalLanguage Processing Chapter of the Association forComputational Linguistics, pages 265?268, Washing-ton, D.C.Susan McRoy and Graeme Hirst.
1995.
The repairof speech act misunderstandings by abductive infer-ence.
Computational Linguistics, 21(4):435?478, De-cember.Barbara Partee.
1984.
Nominal and temporal anaphora.Linguistics and Philosophy, 7:243 ?
286.Manny Rayner and Hiyan Alshawi.
1992.
Derivingdatabase queries from logical forms by abductive defi-nition expansion.
In Proceedings of the Third Confer-ence of Applied Natural Language Processing, pages1 ?
8, Trento, Italy.Carolyn Rose?, Pamela Jordan, Michael Ringenberg,Stephanie Siler, Kurt VanLehn, and Anders Weinstein.2001.
Interactive conceptual tutoring in atlas-andes.In Proceedings of AI in Education 2001 Conference.Carolyn P.
Rose?, Antonio Roque, Dumisizwe Bhembe,and Kurt VanLehn.
2002.
An efficient incremental ar-chitecture for robust interpretation.
In Proceedings ofHuman Language Technology Conference, San Diego,CA.Carolyn P. Rose?.
2000.
A framework for robust seman-tic interpretation.
In Proceedings of the First Meetingof the North American Chapter of the Association forComputational Linguistics.James D. Slotta, Michelene T.H.
Chi, and Elana Jo-ram.
1995.
Assessing students?
misclassifications ofphysics concepts: An ontological basis for conceptualchange.
Cognition and Instruction, 13(3):373?400.Mark Stickel.
1988.
A prolog-like inference systemfor computing minimum-cost abductive explanationsin natural-language interpretation.
Technical Report451, SRI International, 333 Ravenswood Ave., MenloPark, California.Richmond H. Thomason, Jerry Hobbs, and Johanna D.Moore.
1996.
Communicative goals.
In K. Jokinen,M.
Maybury, M. Zock, and I. Zukerman, editors, Pro-ceedings of the ECAI 96 Workshop Gaps and Bridges:New Directions in Planning and Natural LanguageGeneration.Bonnie Webber.
1988.
Tense as discourse anaphor.Computational Linguistics, 14(2):61 ?
71.
