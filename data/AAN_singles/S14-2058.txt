Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 346?350,Dublin, Ireland, August 23-24, 2014.INSIGHT Galway: Syntactic and Lexical Features for Aspect BasedSentiment AnalysisSapna NegiInsight Centre for Data AnalyticsNational University of IrelandGalway{sapna.negi, paul.buitelaar}@insight-centre.orgPaul BuitelaarInsight Centre for Data AnalyticsNational University of IrelandGalwayAbstractThis work analyses various syntactic andlexical features for sentence level aspectbased sentiment analysis.
The task fo-cuses on detection of a writer?s sentimenttowards an aspect which is explicitly men-tioned in a sentence.
The target sentimentpolarities are positive, negative, conflictand neutral.
We use a supervised learningapproach, evaluate various features andreport accuracies which are much higherthan the provided baselines.
Best featuresinclude unigrams, clauses, dependency re-lations and SentiWordNet polarity scores.1 IntroductionThe term aspect refers to the features or aspectsof a product, service or topic being discussed in atext.
The task of detection of sentiment towardsthese aspects involves two major processing steps,identifying the aspects in the text and identifyingthe sentiments towards these aspects.
Our workdescribes a submitted system in the Aspect BasedSentiment Analysis task of SemEval 2014 (Pontikiet al., 2014).
The task was further divided into 4subtasks; our work corresponds to the subtask 2,called Aspect Term Polarity Detection.
We pre-dict the polarity of sentiments expressed towardsthe aspect terms which are already annotated in asentence.
The target polarity types are positive,negative, neutral and conflict.We employ a statistical classifier and experimentwith various syntactic and lexical features.
Se-lected features for the submitted system includewords which hold certain dependency relationswith the aspect terms, clause in which the aspectThis work is licensed under a Creative Commons Attribu-tion 4.0 International Licence.
Page numbers and proceed-ings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/term appears, unigrams, and sum of lexicon basedsentiment polarities of the words in the clause.2 Related WorkPang et al.
(2002) proved that unigrams andbigrams, adjectives and part of speech tags areimportant features for a machine learning basedsentiment classifier.
Later, verbs and adjectiveswere also identified as important features (Ches-ley, 2006).
Meena and Prabhakar (2007) per-formed sentence level sentiment analysis usingrules based on clauses of a sentence.
However,in our case we cannot simply consider the adjec-tives and verbs as features, since they might re-late to different aspects.
For example, in the sen-tence ?The pizza is the best if you like thin crustedpizza.
?, sentiment towards ?pizza?
is positive be-cause of the adjective ?best?
; however for the term?thin crusted pizza?, ?like?
would be the sentimentverb.
Therefore, only those adjectives and verbswhich relate to the target aspect, can be consid-ered as the indicator of their polarity.
Wilson etal.
(2009) showed that the words which share cer-tain dependency relations with aspect terms, tendto indicate the sentiments expressed towards thoseterms.Saif et al.
(2012) showed the co-relation betweentopic and sentiment polarity in tweets, and as-serted that majority of people tend to express sim-ilar sentiments towards same topics, especially inthe case of positive sentiments.
The baseline ap-proach for this task (Pontiki et al., 2014) also as-sociates polarity with aspect terms.
Therefore, wealso consider aspect term as a potential feature.Our approach for this task is based on our obser-vation of the data, with a provenance of the abovementioned findings.3 ApproachWe employ a statistical classifier which trains onthe provided training datasets.346Datasets: Training datasets comprise of 3000 sen-tences from laptop and restaurant reviews.
Train-ing sentences were tagged with the target aspectterm and the corresponding polarity, where morethan one aspect term can be tagged in a sentence.3.1 Feature SetsWe divide the candidate features into four featuresets.1.
Non-contextual: These features comprise oftraining vocabulary.
They do not target as-pect based sentiments, but the overall senti-ment of the sentence.
There might be caseswhere the aspect based sentiment is same asthe overall sentiment of the sentence.
Thefeature set comprises of three feature types,unigrams, bigrams, adjectives and verbs ofthe sentence.2.
Lexicon Non-Contextual: These featuresare the Sentiwordnet v3.0 polarity scores(Andrea Baccianella and Sebastiani, 2010)of the words obtained from the best non-contextual feature type.
This feature setwould include two numerical features, posi-tive polarity score and negative polarity scoreof the best non-contextual feature types.
Bestnon-contextual feature type is decided bycomparing the classification accuracies of in-dividual feature types, with cross validationon the training data (Table 1).
We evaluatedtwo algorithms to obtain the positive and neg-ative polarities of words using SentiWordNet.Later, we would provide details of these algo-rithms.3.
Contextual: These features target aspectbased sentiments.
Feature types comprise ofthe clause in which an aspect term appears,the adjective and verbs of this clause, aspectterm itself, and the words which hold certaindependency relations with aspect term.
Weonly considered the Stanford parser depen-dencies ?nn?, ?amod?, and ?nsubj?.
The de-pendency relations were chosen on the basisof best classification accuracy in a cross vali-dation trial, where the only features were thewords holding different dependency relationswith the aspect term.
However, we only listthe accuracy from the best performing depen-dency relations in the Tables 1, 3.
By the fea-ture type clause, we mean the unigrams con-tained in a clause.4.
Lexicon Contextual: Similar to LexiconNon-Contextual features, these are the nu-meric values obtained from SentiWordNetpolarity scores of the best performing contex-tual feature type.Polarity Calculation using SentiWordNet:WordNet (Fellbaum, 1998) is a lexical databasefor the English language.
It assigns each listedword the senses to which it may belong, whereeach unique sense is represented by a synset id.SentiWordNet is built on the top of WordNet,where a pair of positive and negative polarityscore is assigned to each sense of a word.
Senti-Wordnet entry for each word comprises of all thepossible parts of speech in which the word couldappear, all the senses corresponding to each partof speech, and a pair of polarity scores associatedwith each sense1.
The magnitude of positive andnegative polarity scores for each sense rangesfrom 0 to 1.In order to automatically obtain the polarityscores corresponding to the desired sense of aword, word sense disambiguation is requiredto be performed.
We did not perform sensedisambiguation, and picked the polarity scoressimply on the basis of word and part of speechmatching.
This gives more than one candidatesenses, and thus more than one pair of polarityscores for each word.
We evaluated the following2 methods to assign single values of sentimentpolarity scores to each word.1.
Default: The SentiWordnet website2pro-vides a basic algorithm to assign sentiword-net based polarities to a word.
SentiWordnetassigns a rank to each sense of a word, wheremost commonly appearing sense is rankedas 1.
The default algorithm first calculatesan overall polarity (Positive score - Negativescore), for each sense of a word.
It then cal-culates a weighted sum of the overall polarityscores of all the senses of a word, where theweights are the ranks of senses.
This sum isconsidered as a single value polarity score ofa word, which can be a positive or negativenumber.1http://sentiwordnet.isti.cnr.it/search.php?q=good2http://sentiwordnet.isti.cnr.it3472.
Our algorithm: We do not obtain an overallpolarity score for each word, but we obtain apair of aggregated negative and positive scorefor each word.
Aggregate positive score isobtained by taking the average of the positivescores of each sense of the word, and samegoes for the aggregate negative score.One reason for keeping the positive and negativescores separate in our algorithm is that the taskalso involves sentiment classes conflict and neu-tral.
Using only the overall polarity score resultsin a loss of information in the case of very low neg-ativity and positivity (neutral sentiments), or highbut comparable negativity and positivity (conflict-ing sentiments).
Also, our algorithm producedbetter results when used with an SVM classifier,with features as unigrams and their polarity scores.3.2 Classifier ModelOur system is built on the state of the art LibSVMclassifier (EL-Manzalawy and Honavar, 2005).We used Weka 3.7.10 toolkit (Hall et al., 2009) forour experiments.
The parameters3of the SVMclassifier are tuned to the values which give bestresults with unigrams.
Table 2 provides the tunedparameters, rest of the parameters are set at defaultvalues.Pre-processing: We perform stemming usingWeka?s implementation of Snowball stemmer,convert strings to lower case and filter out stop-words.
We use a customised list of stopwords,based on our observations of the data.
The cus-tomised list is created using the stopword list ofWeka, with certain words removed.
For example,negators like ?not?, ?didn?t?
etc.
are important fornegative sentiments, for example ?I can barely useany usb devices because they will not stay prop-erly connected?.
Words like ?but?, ?however?
areprominent in conflicting sentiments, for example?No backlit keyboard, but not an issue for me?.
Ta-bles 1, 3 show the difference in results on using fil-tered stopword list, compared against no stopwordremoval, and original stopword list.G R C E Z0.10 1.0 2 1.0 normaliseTable 2: Parameter Settings for SVM Classifier.3http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LibSVM.html3.3 Feature EvaluationWe evaluated our features using 8-fold cross val-idation on the training data.
We evaluated eachfeature by using it as the only feature for the clas-sifier (Tables 1, 3).
We performed experimentson different combinations of features, but we onlypresent the best performing combination of fea-tures in the last row of the tables.
The baselineapproach (Pontiki et al., 2014) provided by the or-ganisers, produced an accuracy of 47% for laptopand 57% for restaurant, by splitting the trainingdata.Metrics include, F score for each class, and overallclassification accuracy.
F score ranges from 0-1,and overall accuracy range from 0-100.4 Submission and ResultsSubmission involved the prediction of sentimentpolarity towards the already tagged aspect termsin two test datasets.
There were 800 sentencesin each test dataset.
The laptop test dataset wasobtained by dividing the original laptop data intotraining and test.
However, restaurant test datasetand training dataset come from different sources.We trained our classifier using the provided train-ing dataset and the highlighted features (last row)in the Tables 1, 3.
In order to evaluate the submis-sion, gold standard datasets corresponding to eachtest dataset were later released, and submission?saccuracy was compared against it.Results: The system performance was evaluatedand ranked on the basis of overall accuracy of sen-timent prediction.
We were ranked as 20/32 forthe laptop domain, and 16/34 for the restaurantdomain.
The task organisers reported that 8 polar-ity predictions for laptop data, and 34 for restau-rant data were missing from our submission.
Welater debugged our system, and obtained the actualaccuracy which our system is capable of produc-ing with the given test data.
The results are sum-marised in Table 4.5 Observations and AnalysisWe hypothesize that aspect terms should serve asfeatures when training data and test data comefrom same source, which means that they relateto the same brand, product, service etc.
This isbecause aspect terms change with data, for exam-ple names of dishes would change with differentrestaurants even if the domain is same.
In ourcase, the laptop test data was obtained from the348Feature Set Features Positive Negative Conflict Neutral Accuracynon-contextualunigrams,bigrams 0.827 0.590 0.210 0.422 70.699unigrams 0.830 0.584 0.154 0.413 70.962adjectives,verbs 0.704 0.412 0.000 0.257 63.465adjectives 0.623 0.430 0.000 0.000 56.410non-contextual + lexicon unigrams, unigram polarity scores 0.833 0.596 0.154 0.414 71.300contextualclause 0.823 0.571 0.117 0.0.456 71.170adjective, verbs within clause 0.784 0.472 0.000 0.257 66.
465aspects 0.734 0.154 0.000 0.264 59.442dependencies 0.751 0.235 0.000 0.061 61.257contextual + lexicon clause, clause polarity score 0.735 0.000 0.000 0.000 58.101combinedunigrams, clause, dependencies,clause polarity score, filteredstopword list0.837 0.610 0.162 0.418 71.960used original stopword 0.825 0.587 0.078 0.371 70.830no stopwords used 0.830 0.610 0.151 0.435 72.000Table 1: Feature Analysis for Restaurant Reviews.Feature Set Features Positive Negative Conflict Neutral AccuracyNon-Contextualunigrams,bigrams 0.827 0.590 0.210 0.422 70.699unigrams 0.781 0.747 0.110 0.484 71.202adjectives,verbs 0.569 0.620 0.000 0.164 54.516adjectives 0.521 0.613 0.000 0.090 51.230non-contextual + lexicon unigrams, unigram polarity scores 0.783 0.754 0.179 0.529 71.850ContextualClause 0.823 0.571 0.117 0.0.456 71.170adjective, verbs within clause 0.569 0.620 0.000 0.164 54.510aspects 0.602 0.259 0.000 0.050 45.240dependencies 0.590 0.078 0.000 0.000 42.480contextual + lexicon clause, clause polarity score 0.750 0.705 0.000 0.407 67.230combinedunigrams, clause, dependencies,clause polarity score, filteredstopword list0.786 0.752 0.100 0.498 71.600weka stopword list 0.780 0.744 0.113 0.442 70.590no stopwords 0.782 0.758 0.154 0.530 72.170Table 3: Feature Analysis for Laptop Reviews.same dataset which was used to prepare trainingdata, while restaurant was from a different source.We observed that, although aspect terms producedbetter results with cross validation, it did not hap-pen in the case of test data.
The restaurant test dataproduced better accuracy without aspect term fea-tures, while laptop test data produced better accu-racy with aspect term features.
We submitted oursystems without using aspect terms as features.
Ifaspect terms were used as features, the laptop testdata would have been classified with an accuracyof 60.8 %.
Another interesting observation is, uni-grams produce better results on their own, as com-pared to adjectives and verbs.
Dependency andclauses also seem to be very important features,since they produce an accuracy of above 60% ontheir own.
We also observed that some stopwordsare important features for this task, and completeremoval of stopwords lowers the classification ac-curacy.Domain Baseline BestSystemSubmittedSystemDebuggedSystemlaptop 51.07 70.48 57.03 59.15restaurant 64.28 80.95 70.70 71.44Table 4: Results on Gold Standard Data.6 ConclusionWe presented an analysis and evaluation of syn-tactic and lexical features for performing sentencelevel aspect based sentiment analysis.
Our fea-tures depend on part of speech tagging and depen-dency parsing, and therefore the accuracy mightvary with different parsers.
Although our systemdid not produce the highest accuracy for the task, itis capable of achieving accuracies much above thebaselines.
Therefore, the proposed features can beworth testing on different datasets and can be usedin combination with other features.AcknowledgementThis work has been funded by the Euro-pean project EUROSENTIMENT under grant no.296277, and the Science Foundation Ireland underGrant Number SFI/12/RC/2289 (Insight Center).ReferencesStefano Esuli Andrea Baccianella and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation349(LREC?10).
European Language Resources Associ-ation (ELRA).Paula Chesley.
2006.
Using verbs and adjectives toautomatically classify blog sentiment.
In In Pro-ceedings of AAAI-CAAW-06, the Spring Symposiaon Computational Approaches, pages 27?29.Yasser EL-Manzalawy and Vasant Honavar, 2005.WLSVM: Integrating LibSVM into Weka Environ-ment.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software, an update.SIGKDD Explorations, 11:10?18.Arun Meena and Prabhakar T.V.
2007.
Sentence levelsentiment analysis in the presence of conjuncts us-ing linguistic analysis.
In ECIR, volume 4425 ofLecture Notes in Computer Science.
Springer.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation,SemEval 2014, Dublin, Ireland.Hassan Saif, Yulan He, and Harith Alani.
2012.
Se-mantic sentiment analysis of twitter.
In Interna-tional Semantic Web Conference (1), volume 7649of Lecture Notes in Computer Science, pages 508?524.
Springer.Wilson Theresa, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics, pages 399?433.350
