Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 221?226,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTwo Improvements to Left-to-Right Decoding for HierarchicalPhrase-based Machine TranslationMaryam Siahbani and Anoop SarkarSchool of Computing ScienceSimon Fraser UniversityBurnaby BC.
Canadamsiahban,anoop@cs.sfu.caAbstractLeft-to-right (LR) decoding (Watanabe etal., 2006) is promising decoding algorithmfor hierarchical phrase-based translation(Hiero) that visits input spans in arbitraryorder producing the output translation inleft to right order.
This leads to far fewerlanguage model calls, but while LR decod-ing is more efficient than CKY decoding,it is unable to capture some hierarchicalphrase alignments reachable using CKYdecoding and suffers from lower transla-tion quality as a result.
This paper in-troduces two improvements to LR decod-ing that make it comparable in translationquality to CKY-based Hiero.1 IntroductionHierarchical phrase-based translation (Hi-ero) (Chiang, 2007) uses a lexicalized syn-chronous context-free grammar (SCFG) extractedfrom word and phrase alignments of a bitext.
De-coding for Hiero is typically done with CKY-styledecoding with time complexity O(n3) for sourceinput with n words.
Computing the languagemodel score for each hypothesis within CKY de-coding requires two histories, the left and the rightedge of each span, due to the fact that the targetside is built inside-out from sub-spans (Heafieldet al., 2011; Heafield et al., 2013).LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010)and syntax-based (Huang and Mi, 2010; Feng etal., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani etal., 2013), which is our focus in this paper.Watanabe et al.
(2006) first proposed left-to-right (LR) decoding for Hiero (LR-Hiero hence-forth) which uses beam search and runs in O(n2b)in practice where n is the length of source sentenceand b is the size of beam (Huang and Mi, 2010).To simplify target generation, SCFG rules are con-strained to be prefix-lexicalized on target side akaGriebach Normal Form (GNF).
Throughout thispaper we abuse the notation for simplicity and usethe term GNF grammars for such SCFGs.
Thisconstraint drastically reduces the size of gram-mar for LR-Hiero in comparison to Hiero gram-mar (Siahbani et al., 2013).
However, the orig-inal LR-Hiero decoding algorithm does not per-form well in comparison to current state-of-the-artHiero and phrase-based translation systems.
Siah-bani et al.
(2013) propose an augmented versionof LR decoding to address some limitations in theoriginal LR-Hiero algorithm in terms of transla-tion quality and time efficiency.Although, LR-Hiero performs much faster thanHiero in decoding and obtains BLEU scores com-parable to phrase-based translation system onsome language pairs, there is still a notable gap be-tween CKY-Hiero and LR-Hiero (Siahbani et al.,2013).
We show in this paper using instructive ex-amples that CKY-Hiero can capture some complexphrasal re-orderings that are observed in languagepairs such as Chinese-English that LR-Hiero can-not (c.f.
Sec.3).We introduce two improvements to LR decod-ing of GNF grammars: (1) We add queue diversityto the cube pruning algorithm for LR-Hiero, and(2) We extend the LR-Hiero decoder to capture allthe hierarchical phrasal alignments that are reach-able in CKY-Hiero (restricted to using GNF gram-mars).
We evaluate our modifications on threelanguage pairs and show that LR-Hiero can reachthe translation scores comparable to CKY-Hiero intwo language pairs, and reduce the gap betweenHiero and LR-Hiero on the third one.2 LR Decoding with Queue DiversityLR-Hiero uses a constrained lexicalized SCFGwhich we call a GNF grammar: X ?
?
?,?b ?
?where ?
is a string of non-terminal and terminalsymbols,?b is a string of terminal symbols and ?
isa possibly empty sequence of non-terminals.
Thisensures that as each rule is used in a derivation,221Algorithm 1: LR-Hiero Decoding1: Input sentence: f = f0f1.
.
.
fn2: F = FutureCost(f) (Precompute future cost1for spans)3: S0= {} (Create empty initial stack)4: h0= (?s?, [[0, n]], ?,F[0,n]) (Initial hypothesis 4-tuple)5: Add h0to S0(Push initial hyp into first Stack)6: for i = 1, .
.
.
, n do7: cubeList = {} (MRL is max rule length)8: for p = max(i?
MRL, 0), .
.
.
, i?
1 do9: {G} = Grouped(Sp) (based on the first uncoveredspan)10: for g ?
{G} do11: [u, v] = gspan12: R = GetSpanRules([u, v])13: for Rs?
R do14: cube = [ghyps, Rs]15: Add cube to cubeList16: Si= Merge(cubeList,F) (Create stack Siand addnew hypotheses to it, see Figure 1)17: return argmax(Sn)18: Merge(CubeList,F)19: heapQ = {}20: for each (H,R) in cubeList do21: hypList = getBestHypotheses((H,R),F , d) (dbest hypotheses of each cube)22: for each h?in hypList do23: push(heapQ, (h?c, h?, [H,R]) (Push new hypin queue)24: hypList = {}25: while |heapQ| > 0 and |hypList| < K do26: (h?c, h?, [H,R]) = pop(heapQ) (pop the besthypothesis)27: push(heapQ,GetNeighbours([H,R]) (Pushneighbours to queue)28: Add h?to hypList29: return hypListthe target string is generated from left to right.The rules are obtained from a word and phrasealigned bitext using the rule extraction algorithmin (Watanabe et al., 2006).LR-Hiero decoding uses a top-down depth-firstsearch, which strictly grows the hypotheses in tar-get surface ordering.
Search on the source sidefollows an Earley-style search (Earley, 1970), thedot jumps around on the source side of the rulesbased on the order of nonterminals on the targetside.
This search is integrated with beam searchor cube pruning to find the k-best translations.Algorithm 1 shows the pseudocode for LR-Hiero decoding with cube pruning (Chiang, 2007)(CP).
LR-Hiero with CP was introduced in (Siah-bani et al., 2013).
In this pseudocode, we have in-troduced the notion of queue diversity (explainedbelow).
However to understand our change weneed to understand the algorithm in more detail.1The future cost is precomputed in a way similar to thephrase-based models (Koehn et al., 2007) using only the ter-minal rules of the grammar.9.18.28.3 8.58.058.1 8.48.68.88.93.21.30.9 6.66.76.98.97.1 8.58.79.39.08.17.21.51.31.26.76.86.9...S iFigure 1: Cubes (grids) are fed to a priority queue (trian-gle) and generated hypotheses are iteratively popped from thequeue and added to stack Si.
Lower scores are better.
Scoresof rules and hypotheses appear on the top and left side of thegrids respectively.
Shaded entries are hypotheses in the queueand black ones are popped from the queue and added to Si.Each source side non-terminal is instantiated withthe legal spans given the input source string, e.g.if there is a Hiero rule ?aX1, a?X1?
and if a onlyoccurs at position 3 in the input then this rule canbe applied to span [3, i] for all i, 4 < i ?
n for in-put of length n and source side X1is instantiatedto span [4, i].
A worked out example of how thedecoder works is shown in Figure 2.
Each partialhypothesis h is a 4-tuple (ht, hs, hcov, hc): con-sisting of a translation prefix ht, a (LIFO-ordered)list hsof uncovered spans, source words coverageset hcovand the hypothesis cost hc.
The initial hy-pothesis is a null string with just a sentence-initialmarker ?s?
and the list hscontaining a span of thewhole sentence, [0, n].
The hypotheses are storedin stacks S0, .
.
.
, Sn, where Spcontains hypothe-ses covering p source words just like in stack de-coding for phrase-based SMT (Koehn et al., 2003).To fill stack Siwe consider hypotheses in eachstack Sp2, which are first partitioned into a set ofgroups {G}, based on their first uncovered span(line 9).
Each group g is a 2-tuple (gspan, ghyps),where ghypsis a list of hypotheses which share thesame first uncovered span gspan.
Rules matchingthe span gspanare obtained from routine GetSpan-Rules.
Each ghypsand possible Rscreate a cubewhich is added to cubeList.The Merge routine gets the best hypothesesfrom all cubes (see Fig.1).
Hypotheses (rows) andcolumns (rules) are sorted based on their scores.GetBestHypotheses((H,R),F , d) uses currenthypothesis H and rule R to produce new hypothe-ses.
The first best hypothesis, h?along with itsscore h?cand corresponding cube (H,R) is placedin a priority queue heapQ (triangle in Figure 1and line 23 in Algorithm 1).
Iteratively the K best2As the length of rules are limited (at most MRL), we canignore stacks with index less than i?
MRL222ruleshypotheses?s?
[0, 15]G 1)?Taiguo shi X1/Thailand X1?
?s?
Thailand [2,15]G 2)?yao X1/wants X1?G 3)?liyong X1/to utilize X1?4)?zhe bi qian X1/this money X1?5)?X1zhuru geng duo X2/to inject more X2X1?6)?liudong X1/circulating X1?G 7)?zijin X1/capital X1?8)?./.
?9)?xiang jingji/to the economy?
?s?Thailand wants [3,15]?s?Thailand wants to utilize [4,15]?s?Thailand wants to utilize this money [7,15]?s?Thailand wants to utilize this money to inject more [12,15][7,9]?s?Thailand wants to utilize this money to inject more circulating [13,15][7,9]?s?Thailand wants to utilize this money to inject more circulating capital [14,15][7,9]?s?Thailand wants to utilize this money to inject more circulating capital .
[7,9]?s?Thailand wants to utilize this money to inject more circulating capital .
to the economy?/s?Figure 2: The process of translating the Chinese sentence in Figure 3(b) in LR-Hiero.
Left side shows the rules used in thederivation (G indicates glue rules as defined in (Watanabe et al., 2006)).
The hypotheses column shows the translation prefixand the ordered list of yet-to-be-covered spans.T?
b ch ng shu  ,?
?
?
li?nh?
zh?ngf?
, b?ngqi?
y u n?ngl??
gu?nch?
.m?qi?nHe added that the coalition government carrying out the economic reform plancapable ofandj?ngj?
g ig?
j?hu?
?is now in stable .X1conditionzhu?ngku?ng w?nd?ng 0      1                   2            3   4               5                    6                      7                                  8                         9   10                11        12              13                     14             15             16            17      18(a)T?igu?
sh?
y?o zh?
b?
qi?n xi?ng j?ngj?
zh?r?
g?ng du?
.l?y?ngThailand wants to circulating capital to the economyinject morethis money toli?d?ng z?j?nutilize .S 1 S 20               1          2              3                  4            5     6            7                 8             9               10           11         12                    13           14      15(b)Figure 3: Two Chinese-English sentence pairs from devset data in experiments.
(a) Correct rule cannot be matched to [6,18],our modifications match the rule to the first subspan [6,9] (b) LR-Hiero detects a wrong span for X2[12,15], we modify therule matching match X2to all subspans [12,13], [12,14] and [12,15], corresponding to 3 hypotheses.hypotheses in the queue are popped (line 26) andfor each hypothesis its neighbours in the cube areadded to the priority queue (line 27).
Decodingfinishes when stack Snhas been filled.The language model (LM) score violates thehypotheses generation assumption of CP and cancause search errors.
In Figure 1, the topmostand leftmost entry of the right cube has a scoreworse than many hypotheses in the left cube dueto the LM score.
This means the right cubehas hypotheses that are ignored.
This type ofsearch error hurts LR-Hiero more than CKY-Hiero, due to the fact that hypotheses scores inLR-Hiero rely on a future cost, while CKY-Hierouses the inside score for each hypothesis.
Tosolve this issue for LR-Hiero we introduce the no-tion of queue diversity which is the parameter din GetBestHypotheses((H,R),F , d).
This pa-rameter guarantees that each cube will produce atleast d candidate hypotheses for the priority queue.d=1 in standard cube pruning for LR-Hiero (Siah-bani et al., 2013).
We apply the idea of diver-sity at queue level, before generating K best hy-pothesis, such that the GetBestHypotheses rou-tine generates d best hypotheses from each cubeand all these hypotheses are pushed to the prior-ity queue (line 22-23).
We fill each stack differ-ently from CKY-Hiero and so queue diversity isdifferent from lazy cube pruning (Pust and Knight,2009) or cube growing (Huang and Chiang, 2007;Vilar and Ney, 2009; Xu and Koehn, 2012).3 Capturing Missing AlignmentsFigure 3(a) and Figure 3(b) show two examples ofa common problem in LR-Hiero decoding.
Thedecoder steps for Figure 3(b) are shown in Fig-ure 2.
The problem occurs in Step 5 of Figure 2where rule #5 is matched to span [7, 15].
Dur-ing decoding LR-Hiero maintains a stack (last-in-first-out) of yet-to-be-covered spans and triesto translate the first uncovered span (span [7, 15]in Step 5).
LR-Hiero should match rule #5 tospan [7, 15], therefore X2is forced to match span[12, 15] which leads to the translation of span [7, 9](corresponding to X1) being reordered around it223Corpus Train/Dev/TestCs-En Europarl(v7) + CzEng(v0.9); Newscommentary(nc) 2008&2009; nc 20117.95M/3000/3003De-En Europarl(v7); WMT2006; WMT2006 1.5M/2000/2000Zh-En HK + GALE phase-1; MTC part 1&3;MTC part 42.3M/1928/919Table 1: Corpus statistics in number of sentences.
Tuning and test sets for Chinese-English has 4 references.Model Cs-En De-En Zh-EnHiero 20.77 25.72 27.65LR-Hiero (Watanabe et al., 2006) 20.72 25.10 25.99LR-Hiero+CP (Siahbani et al., 2013) 20.15 24.83 -LR-Hiero+CP (QD=1) 20.68 25.14 24.44LR-Hiero+CP (QD=15) - - 26.10LR-Hiero+CP+(ab) 20.88 25.22 26.55LR-Hiero+CP+(abc) 20.89 25.22 26.52(a) BLEU scores for different baselines and modifications of this paper.QD=15 for Zh-En in last three rows.
(b) Average number of language model queries.Table 2: (a) BLEU (b) LM callscausing the incorrect translation in Step 9.
If weuse the same set of rules for translation in Hi-ero (CKY-based decoder), the decoder is able togenerate the correct translation for span [7, 14] (itworks bottom-up and generate best translation foreach source span).
Then it combines translation of[7, 14] with translation of spans [0, 7] and [14, 15]using glue rules (monotonic combination).In Figure 3(a) monotonic translations after span[6, 9] are out of reach of the LR-Hiero decoderwhich has to use the non-terminals to supportthe reordering within span [6, 9].
In this exam-ple the first few phrases are translated monoton-ically, then for span [6, 18] we have to apply rule?muqian X1wending, is now in stable X1?
to ob-tain the correct translation.
But this rule cannotbe matched to span [6, 18] and the decoder failsto generate the correct translation.
While CKY-Hiero can apply this rule to span [6, 9], generatecorrect translation for this span and monotonicallycombine it with translation of other spans ([0, 6],[9, 18]).In both these cases, CKY-Hiero has no diffi-culty in reaching the target sentence with the sameGNF rules.
The fact that we have to process spansas they appear in the stack in LR-Hiero meansthat we cannot combine arbitrary adjacent spansto deal with such cases.
So purely bottom-up de-coders such as CKY-Hiero can capture the align-ments in Figure 3 but LR-Hiero cannot.We extend the LR-Hiero decoder to handle suchcases by making the GNF grammar more expres-sive.
Rules are partitioned to three types based onthe right boundary in the source and target side.The rhs after the?
shows the new rules we createwithin the decoder using a new non-terminal Xrto match the right boundary.
(a) ??a?,?b??
?
??a?Xr,?b?Xr?
(b) ??Xn,?b?Xn?
?
??XnXr,?b?XnXr?
(c) ??Xn,?b?Xm?
?
??XnXr,?b?XmXr?
(1)where ?
is a string of terminals and non-terminals,a?
and?b are terminal sequences of source and tar-get respectively, ?
is a possibly empty sequenceof non-terminals and Xnand Xmare differentnon-terminals distinct from Xr3.
The extra non-terminal Xrlets us add a new yet-to-be-coveredspan to the bottom of the stack at each rule appli-cation which lets us match any two adjacent spansjust as in CKY-Hiero.
This captures the missingalignments that could not be previously capturedin the LR-Hiero decoder4.In Table 4 we translated devset sentences usingforced decoding to show that our modifications toLR-Hiero in this section improves the alignmentcoverage when compared to CKY-Hiero.4 ExperimentsWe evaluate our modifications to LR-Hiero de-coder on three language pairs (Table 1): German-English (De-En), Czech-English (Cs-En) andChinese-English (Zh-En).3In rule type (c) Xnwill be in ?
and Xmwill be in ?.4For the sake of simplicity, in rule type (b) we can mergeXnand Xras they are in the same order on both source andtarget side.224We use a 5-gram LM trained on the Gigawordcorpus and use KenLM (Heafield, 2011).
Wetune weights by minimizing BLEU loss on the devset through MERT (Och, 2003) and report BLEUscores on the test set.
Pop limit for Hiero and LR-Hiero+CP is 500 and beam size LR-Hiero is 500.Other extraction and decoder settings such as max-imum phrase length, etc.
were identical across set-tings.
To make the results comparable we use thesame feature set for all baselines, Hiero as well(including new features proposed by (Siahbani etal., 2013)).We use 3 baselines: (i) our implementation of(Watanabe et al., 2006): LR-Hiero with beamsearch (LR-Hiero) and (ii) LR-Hiero with cubepruning (Siahbani et al., 2013): (LR-Hiero+CP);and (iii) Kriya, an open-source implementation ofHiero in Python, which performs comparably toother open-source Hiero systems (Sankaran et al.,2012).Table 3 shows model sizes for LR-Hiero (GNF)and Hiero (SCFG).
Typical Hiero rule extractionexcludes phrase-pairs with unaligned words onboundaries (loose phrases).
We use similar ruleextraction as Hiero, except that exclude non-GNFrules and include loose phrase-pairs as terminalrules.Table 2a shows the translation quality of dif-ferent systems in terms of BLEU score.
Row3 is from (Siahbani et al., 2013)5.
As we dis-cussed in Section 2, LR-Hiero+CP suffers fromsevere search errors on Zh-En (1.5 BLEU) but us-ing queue diversity (QD=15) we fill this gap.
Weuse the same QD(=15) in next rows for Zh-en.For Cs-En and De-En we use regular cube prun-ing (QD=1), as it works as well as beam search(compare rows 4 and 2).We measure the benefit of the new modifiedrules from Section 3: (ab): adding modificationsfor rules type (a) and (b); (abc): modificationof all rules.
We can see that for all languagepairs (ab) constantly improves performance of LR-Hiero, significantly better than LR-Hiero+CP andLR-Hiero (p-value<0.05) on Cs-En and Zh-En,evaluated by MultEval (Clark et al., 2011).
Butmodifying rule type (c) does not show any im-provement due to spurious ambiguity created by5We report results on Cs-En and De-En in (Siahbani etal., 2013).
Row 4 is the same translation system as row 3(LR-Hiero+CP).
We achieve better results than our previouswork (Siahbani et al., 2013) (row 4 vs. row 3) due to bugcorrections and adding loose phrases as terminal rules.Model Cs-En De-En Zh-EnHiero 1,961.6 858.5 471.9LR-Hiero 266.5 116.0 100.9Table 3: Model sizes (millions of rules).Model Cs-En De-En Zh-EnHiero 318 351 187LR-Hiero 278 300 132LR-Hiero+(abc) 338 361 174Table 4: No.
of sentence covered in forced decoding of a sam-ple of sentences from the devset.
We improve the coverageby 31% for Chinese-English and more than 20% for the othertwo language pairs.type (c) rules.Figure 2b shows the results in terms of averagenumber of language model queries on a sample setof 50 sentences from test sets.
All of the base-lines use the same wrapper to KenLM (Heafield,2011) to query the language model, and we haveinstrumented the wrapper to count the statistics.In (Siahbani et al., 2013) we discuss that LR-Hierowith beam search (Watanabe et al., 2006) does notperform at the same level of state-of-the-art Hi-ero (more LM calls and less translation quality).As we can see in this figure, adding new mod-ified rules slightly increases the number of lan-guage model queries on Cs-En and De-En so thatLR-Hiero+CP still works 2 to 3 times faster thanHiero.
On Zh-En, LR-Hiero+CP applies queuediversity (QD=15) which reduces search errorsand improves translation quality but increases thenumber of hypothesis generation as well.
LR-Hiero+CP with our modifications works substan-tially faster than LR-Hiero while obtain signifi-cantly better translation quality on Zh-En.Comparing Table 2a with Figure 2b we can seethat overall our modifications to LR-Hiero decodersignificantly improves the BLEU scores comparedto previous LR decoders for Hiero.
We obtaincomparable results to CKY-Hiero for Cs-En andDe-En and remarkably improve results on Zh-En,while at the same time making 2 to 3 times lessLM calls on Cs-En and De-En compared to CKY-Hiero.AcknowledgmentsThis research was partially supported by NSERC,Canada RGPIN: 262313 and RGPAS: 446348grants to the second author.
The authors wish tothank Baskaran Sankaran for his valuable discus-sions and the anonymous reviewers for their help-ful comments.225ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
Commun.
ACM, 13(2):94?102, February.Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.2012.
Left-to-right tree-to-string decoding with pre-diction.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, EMNLP-CoNLL ?12, pages 1191?1200,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Michel Galley and Christopher D. Manning.
2010.Accurate non-hierarchical phrase-based translation.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 966?974, Los Angeles, California, June.
As-sociation for Computational Linguistics.Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tet-suo Kiso, and Marcello Federico.
2011.
Left lan-guage model state for syntactic machine translation.In Proceedings of the International Workshop onSpoken Language Translation, pages 183?190, SanFrancisco, California, USA, 12.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013.
Grouping language model boundary wordsto speed K-Best extraction from hypergraphs.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Atlanta, Georgia, USA, 6.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In In Proc.
of the SixthWorkshop on Statistical Machine Translation.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In In ACL 07.Liang Huang and Haitao Mi.
2010.
Efficient incre-mental decoding for tree-to-string translation.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages273?283, Cambridge, MA, October.
Association forComputational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2004.
Pharaoh: A beam search de-coder for phrase-based statistical machine transla-tion models.
In AMTA, pages 115?124.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Michael Pust and Kevin Knight.
2009.
Faster mtdecoding through pervasive laziness.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 141?144,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Baskaran Sankaran, Majid Razmara, and AnoopSarkar.
2012.
Kriya - an end-to-end hierarchi-cal phrase-based mt system.
The Prague Bulletinof Mathematical Linguistics (PBML), 97(97):83?98,apr.Maryam Siahbani, Baskaran Sankaran, and AnoopSarkar.
2013.
Efficient left-to-right hierarchicalphrase-based translation with improved reordering.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, Seattle,USA, October.
Association for Computational Lin-guistics.David Vilar and Hermann Ney.
2009.
On lm heuris-tics for the cube growing algorithm.
In Annual Con-ference of the European Association for MachineTranslation, pages 242?249, Barcelona, Spain, may.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of ACL.Wenduan Xu and Philipp Koehn.
2012.
Extending hi-ero decoding in moses with cube growing.
PragueBull.
Math.
Linguistics, 98:133?.226
