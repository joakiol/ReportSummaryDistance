Extending the Lexicon by Exploiting Subregularitles*Robert WilenskyComputer Science DivisionDepartment of EECSUniversity of California, BerkeleyBerkeley, CA 94720wflensky@teak.berkeley.edu1.
InlroducfionThis paper is concerned with the acquisition of the lexi-con.
In particular, we propose a method that uses ana-logical reasoning to hypothesize new polysemous wordsenses.
This method is one of a number of knowledgeacquisition devices to be included in DIRC (DomainIndependent Retargetable Consultan0.
DIRC is a kindof intelligent, natural language-capable consultant kitthat can be retargeted at different domains.
DIRC isessentially "empty-UC" (UNIX Consultant, Wilenskyet al, 1988).
DIRC is to include the language and rea-soning mechanisms of UC, plus a large grammar and ageneral lexicon.
The user must then add domainknowledge, user knowledge and lexical knowledge forthe area of interest.2.
Previous Work in Acquisition of the Lexicon.There have been numerous attempts to build systemsthat automatically acquire word meanings.
Mostly,these have been either dictionary readers or attempts tohypothesize meanings of completely unfamiliar wordsfrom context (e.g., Selfridge (1982), Granger (1977)).In contrast, we have focussed on the problem of acquir-ing word senses that are related to ones already known.gle meaning may be involved in any number of senses,each of which has grammatical or other differences.Typically, a word has at least one core meaning fromwhich the meanings involved in other senses are in somesense synchronically based.For example, the word "open" has adjectival and verbalsenses; the verbal senses include some whose meaningis, roughly, making physical access available to anenclosed region by moving some object (e.g., "open ajar", "open a draw", "open the door").
This is prob-ably a core meaning of the word.
There are severalsenses involving this meaning, just among the verbalsenses.
These senses are differentiated from one anotherby how the components of the meaning relate to theverb's valence.
For example, one sense has the objectmoved as the patient, and hence as the direct object ofthe lransitive verb (as in "open the door"); another usesthe container itself as the direct object (e.g., "open thejar"); perhaps another involves some son of apenarethat widens (e.g., "open your throat" or "open the pupilof your eye").
Additionally, each of these componentsof the meaning can be realized as patients by appearingas the subject of the inwansitive version of the verb.
Weconsider each differentiable valence structure for boththe Iransitive and intransitive verb forms as constitutingdifferent senses, although we presume that the sameconceptual su'ucture is in all of these xamples.2.1.
A Note on Word SensesFor the purposes at hand, we are only concerned withword senses that are synchronically related.
These maybe polysemous senses of individuals words, as well asrelated senses of different words.
In addition, we distin-guish meanings or conception structures of a word fromsenses.
(We will use the term "meaning" and "con-cepmat structure" interehangely in this contexL) A sin-*The resea~ reg~ed here is the product of the BerkeleyA~ifichd Intelligmce and Namnd l.au~uage Processing seminar;contributers include Michael Braverman, Narcisco JaramiUo,Dan Jurafsky, Eric Kadson, Marc Luria, Peter Nocvig, MichaelSchfff, Nigel Ward, and Dekal Wu.
This research was sponsoredEy the Defense Advanced Research Projects Agency (DoD),monitored by Space and Naval War f~ Systems Commandunder Contract N00039-88-C-0292 and by the Office of NavalResearch, under contract N00014-89-J-3205.Yet other senses of "open" have the meaning of caus-ing an information-containing item to come intoexistence (e.g., "open a bank account" or "open a fileon someone").
This second meaning is probably basedon the first one.
ALso, the various adjective uses (e.g.,"the open door") are separate senses in this view hav-ing some as yet unspecified relation to the sensesdescribed above.
Finally, other words, e.g., "close",have senses that we presume to be related to the varioussenses of "open" just dL~cussed.2.2.
MIDASPreviously, we have succeeded in doing some automaticlexical acquisition by exploiting conventio,al metaphorsas motivations for linguistic forms.
In particular, Martin(1988) implemented the MIDAS system which both usesmetaphoric word senses to help with language under-365standing, and to extend the lexicon when a new meta-phoric use of a word is encountered.
For example, thesentence "John have Mary a cold."
is presumed tomake recourse to a "a cold is a possession" metaphor.We call such a conventionalized metaphor a core meta-phor, since it seems to serve as the basis for relatedmetaphoric uses.
Thus, the sentence "John gave Mary acold" is presumed to involve the "infecting with a coldis giving the cold" metaphor, which entails the previous"cold is possession" metaphor.Suppose the system encounters an utterance like "Johngot the flu from Mary", but is not familiar with this useof the verb "get", nor with the notion of a flu beingtreated as a possession.
Then both the available non-metaphoric sense of "'get", along with the metaphorsinvolving diseases and possession, arc brought o bear tohypothesize the word sense that might be in play.Hypotheses are generated by two kinds of lexical exten-sion processes: core extension and similarity extension.Understanding "get a cold" given an understanding of"give a cold" involves core extension, as the core meta-phor "cold is possession" is extended to the "getting"concept; understanding "get the flu" given an under-standing of "get a cold" involves imilarity extension,as the generalization about a role in the metaphoricsu'ucture must be extended from colds to diseases ingeneral.
Understanding "get the flu" given an under-standing of "give a cold" involves both kinds of exten-sion.The MIDAS system has been used in conjunction withUC to extend metaphoric word senses in the computerdomain.
The following is an example of MIDAS learn-ing a new sense of the word "kill", given that it knowssome metaphoric extensions of this sense outside thecomputer domain.Abstracting Terminate-Conversation toancestor conceptCreating new metaphor:Mapping main source concept Killingto main target conceptTerminate-Computer-ProcessMapping source role killer to targetrole c-proc-termer.Mapping source role kill-victim totarget role c-proc-termed.Calling UC:You can kill a computer process bytyping "c to the shell.Here MIDAS first retrieves a number of metaphorsrelated to the input; of these, "Kill.Conversation" ischosen as most applicable.
A simple similarity exten-sion is attempted, resulting in a proposed "Terminate-Computer-Process" metaphor for interlxetation of theinput.
The interpretation thus provided is passed alongto UC, which can answer this question.
Meanwhile, themetaphor is incorporated into UC's knowledge base,which allows UC's language generator to use the sameterminology inencoding the answer.MIDAS is discussed in detail in Martin (1988).3.
Why MIDAS Works# How can I kill a process?No valid interpretations.
Attemptingto extend existing metaphor.Searching for related known metaphors.Metaphors found: Kill-ConversationKill-Delete-Line Kill-Sports-DefeatSelecting metaphor Kill-Conversationto extend from.Attempting a similarity extensioninference.Extending similar metaphorConversation with targetTerminate-Conversation.Kill-conceptWe believe that MIDAS works because it is exploitingmetaphoric subregularity by a form of analogical rea-soning.
That is, it finds a metaphorical usage that isclosest o the given case according to some conceptualmetric; it then exploits the structure of the prior meta-phor usage to construct an analogous one for the case athand, and proposes this new sl~'ucture as a hypotheticalword sense.
Note that according to this explanation,metaphor does not play a crucial role in the extensionprocess.
Rather, it is the fact that the metaphor is asubregularity rather than the fact that it is a metaphorthat makes it amenable to analogical exploitation.Analogy, of course, has played a prominent role in tradi-tional linguistics.
Indeed, rather influential inguists (forexample, Paul (1891) and Bloomfield (1933) seemed toattribute all novel language use to analogy.
However,today, analogy seems almost entirely relegated todiachronic Ixocessses.
A notable xception to this trend366is the work of Skousen (in press), who appears to advo-cate a view quite similar to our own, although the pri-mary focus of his work is morphological.Analogy has also been widely studied in artificial intelli-gence and cognitive psychology.
The work of Carbonell(1982) and Burstein (1983) is most relevant to our enter-prise, as it explores the role of analogy in knowledgeacquisition.
Similarly, Alterman's (1985, 1988)approach to planning shares some of the same concerns.However, many of the details of Carbonell's andAlterman's proposals are specific to problem solving,and Burstein's work is focused on formnla:ing con-straints on the relations to be considered for analogicalmapping.
Thus, their work does not appear to have anobvious application to our problem.
Many of the differ-ences between analogical reasoning for problem solvingand language knowledge acquisition are discussed atlength in Martin (1988).Another line of related work is the connectionistapproach initiated by Rumelhart and McClelland (1987),and explicitly considered as an aiterative to acquisitionby analogy by MacWhinney et al (1989).
However,there are numerous reasons we believe an explicitly ana-logical framework to be superior.
The Rumelhart-McClelland model maintains no memory of specificcases, but only a statistical summary of them.
Also, theanalogy-based model can use its knowledge more flexi-bly, for example, to infer that a word encountered is thepast tense of a known word, a task that an associationistnetworks could not easily be made to perform.
In addi-tion, we interpret as evidence supportive of a positionlike ours psycholinguistic results uch as those of Cutler(1983) and Buuerworth (1983), which suggest thatwords are represented in their full "undecomposed"form, along with some sorts of relations between relatedwords.3.1.
Other Kinds of Lexical SubregularitiesIf MIDAS works by applying analogical reasoning toexploit metaphoric subregularities, then the questionarises as what other kinds of lexical subregularities theremight be.
One set of candidates i  approached in thework of Brugman (1981, 1984) and Norvig and l~koff(1987).
In particular, Norvig and Lakoff (1987) offersix types of links between word senses in what they calllexical network theory.
However, their theory is con-cerned only with senses of one word.
Also, there appearto be many more links than these.
Indeed, we have noreason to believe that the number of such subregularitiesis bounded in principle.We present a partial list of some of the subregularitieswe have encountered.
The list below uses a rather infor-mal rule format, and gives a couple of examples ofwords to which the rule is applicable.
It is hoped thatexplicating a few examples below will let the readerinfer the meanings of some of the others:(1) function-object-noun -> primary-activity-"determinerless"-noun("the bed" -> "in bed, go to bed"; "a school -> atschool"; "my lunch -> at lunch'~ "the conference ->in conference")(2) noun -> resembling-in-appearance-noun( " tee"  -> "(rose) tree"; "tree" -> "(shoe) tree");"tiger" -> "(stuffed) tiger", "penci l " -> "pencil (oflight)")(3) noun -> having-the-same-function-noun("bed" -> "bed (of leaves)")(4) noun -> "involve-concretion"-verb("a tree" -> "to tree (a ca0"; "a knife" -> "to knife(someone)")(5) verb -> verb-w-role-splitting("take a book" -> "take a book to Mary", "Johnshaved" -> "John shaved Bill")(6) verb -> profiled-component-verb("take a book" -> "take a book to the Cape")(7) verb-> frame-imposition-verb("take a book" -> "take someone to dinner", "go" ->"go dancing")(8) activity-verb-t -> concretion-activity.verb-i("eat an apple" -> "eat \[a meal\]", "'drink a coke" ->"drink \[alcohol\]", "feed the dog" -> "the dog feeds")(9) activity-verb-t -> dobj-subj-middle-voice-verb-i("drive a car" -> "the car drives well' ')(10) activity-verb.i -> activity-verb+primary-category("John dreamed" -> "John dreamed a dream"; "Johnslept" -> "John slept he sleep of the innocent")(II) activity-verb-i > do-cause-activity-verb-t(patientas subjecO("John slept" -> "The bed sleeps five")(12) activity-verb -> activity-of-noun("m cry" -> "a cry (in the wilderness)"; "w  punch"-> "a punch (in the mouth)")(13) activity-verb <-> product-of-activity-noun("copy the paper" <-> "a copy of the paper"; xerox,telegram, telegraph)367(14) functional-noun -> use-function-verb("the telephone" -> "telephone John"; machine,motorcycle, telegraph)(15) objecbnoun -> central-component-of-object("a bed" -> "bought a bed \[=frame with not mattress\];"an apple" -> "eat an apple \[=without the core\]"))Consider the first rule.
This rule states that, for somenoun whose core meaning is a functional object, there isanother sense, also a noun, that occurs without determi-nation, and means the tximary activity associated withthe first sense.
For example, the word "bed" has as acore meaning a functional object used for sleeping.However, the word can also be used in uueraw.es like"go to bed" and "before bed" (but not, say, "*duringbed").
In these cases, the noun is detenninedess, andmeans something akin to sleeping.
Other examplesinclude "jail", "conference", school" and virtuallyall the meal terms, e.g., "lunch", "tea", "dinner".British English allows "in hospital", while AmericanEnglish does not.The dialect difference underscores the point that this istruly a subregularity: concepts that might be expressedthis way are not necessarily expressed this way.
Also,we chose this example not because it in itself is a partic-ularly important generalization about English, but pre-cisely because it is not.
That is, there appear to be manysuch facts of limited scope, and each of them may beuseful for learning analogous cases.Consider also rule 4, which relates function nouns toverbs.
Examples of this are "tree" as in "The dogtreed the cat" and "knife" as in "The murderer knifedhis victim".
The applicable rule states that the verbmeans ome specific activity involving the core meaningof the noun.
I.e., the verbs are ueated as a sort of con-ventionalized denominalization.
Note that the activity ispresumed to be specific, and that the way in which itmust be "concreted" is assumed to be pragmaticallydetermined.
Thus, the rule can only tell us that "tree-ing" involves a Ire.e, but only our world knowledgemight suggest to us that it involves cornering; similarly,the rule can tell us that "knifing" involves the use of aknife, but cannot ell us that it means tabbing a person,and not say, just cutting.As a final illuswation, consider ule 5, so-called "rolesplitting" (this is the same as Norvig and Lakoffssemantic role differentiation rink).
This rule suggeststhat, given a verb in which two thematic roles are real-ized by a single complement may have another sense inwhich these two complements are realized separately.For example, in "John took a book from Mary", John isboth the recipient and the agent.
However, in "Johntook a book to Mary", John is only the agent, and Maryis the recipient.
Thus, the sense of "'take" involved inthe first sentence, which we suggest corresponds to acore meaning, is the basis for the sense used in thesecond, in which the roles coinciding in the first are real-ized separately.
A similar prediction might be madefrom an imransitive verb like "shave", in which agentand patient coincide, to the existence of a Iransitive verb"shave" in which the patient is rfsdiTe~d separately asthe direct object.
(Of course, the tendency of patients toget realized as direct objects in English should also helpmotivate this fact, and can presumably also be exploitedanalogically.)4.
An Analogy.based Model of Lexical AcquisitionWe have been attempting to extend MIDAS-style wordhypothesizing tobe able to propose new word senses byusing analogy to exploit these other kinds of lexicalsubregularifies.
At this point, our work has been ratherpreliminary, but we can at least sketch out the basicarchitecture of our proposal and comment on the prob-lems we have yet to resolve.
(A) Detect unknown word sense.
For example, supposethe system encountered the following phrase:"at breakfast"Suppose further that the function noun "breakfa.~t"were known to the system, but the determinerless u agewere not.
In this case, the system would hypothesizethat it is lacking a word sense because of a failure toparse the sentence.
(B) Find relevant cases/subregularities.
Cues from theinput would be used to suggest prior relevant lexicalknowledge.
In our example, the retrieved cases mightinclude the following:bed-I/bed-3, class- 1/class-4Here we have numbered word senses o that the firstelement of each pair designates a sense involving a coremeaning, and the latter a determineless-activity type ofsense.
We may have also already computed and storedrelevant subregularities.
If so, then these would beretrieved as well.Relevant issues here are the indexing and retrieval ofcases and subregularities.
Our assumption is that we canretrieve relevant cases by a conjunction of simple cues,like "noun", "functional meaning", "extended eter-minerless noun sense", etc., and then rely on the nextphase to discriminate further among these.
(C) Chose the most pertinent case or subregularity.Again, by analogy to MIDAS, some distance metric isused to pick the best datum to analogize from.
In this368case, perhaps the correct choice would be the following:class- l/class-4One motivation for this selection is that "class" is com-patible with "at", as is the case in point.Finding the right metric is the primary issue here.
TheMIDAS meuic is a simple sum of two factors: (i) thelength of the core-relationship f 'om the input source tothe source of the candidate metaphor, and (ii) hierarchi-cal distance between the two concepts.
Both factors aremeasured by the number of finks in the representationthat must be traversed to get from one concept o theother.
The hierarchical distance factor of the MIDASmetric seems directly relevant to other cases.
However,there is no obvious counterpart to the core-relationshipcomponent.
One possible reason for this is that meta-phoric extensions are more complex than most otherkinds; if so, then the MIDAS metric may still be applica-ble to the other subregularities, which are just simplerspecial cases.
(D) Analogize to a new meaning.
Given the best case orsubregularity, the system will attempt to hypothesize anew word sense.
For example, in the case at hand, wewould like a representation forthe meaning in quotes tobe produced.class- l/class..d ::breakfast-If'period of eating breakfast"In the case of MIDAS, the metaphoric structure of pre-vious examples was assumed to be available.
Then,once a best match was established, it is relativelystraightforward to generalize or extend this structure toapply to the new input.
The same would be true in thegeneral case, provided that the relation between storedpolysemous word senses is readily available.
(E) Determine the extent of generaliTation.
Supposingthat a single new word sense can be successfully pro-posed, the question arises as to whether just this particu-lar word sense is all the system can hypothesize, orwhether some "local productivity" is possible.
Forexample, if this is the first meal term the system has seenas having a determinerless activity sense, we suspectthat only the single sense should be generated.
How-ever, if it is the second such meal term, then the first onewould have been the likely basis for the analogy, and ageneraliTmion to meal terms in general may beattempted.
(F) Record a new entry.
The new sense needs to bestored in the lexicon, and indexed for further eference.This task may interact closely with (E), although gen-eralizing to unattested cases and computing expficitsubregularities are logically independent.There are many additional problems to be addressedbeyond the ones alluded to above.
In particular', there isthe issue of the role of world knowledge in the proposedprocess.
In the example above, the system must knowthat the activity of eating is the primary one associatedwith breakfast.
A more dramatic example is the role ofworld knowledge in hypothesizing the meaning of"treed" in expressions like "the..dog treed the cat",assuming that the system is acquainted with the noun"tree".
All an analogical reasoning mechanism can dois suggest hat some specific activity associated withtrees is involved; the application of world knowledgewould have to do the rest.$.
Other Directions of InvestigationWe have also been investigating exploiting subregulari-ties in "intelligent dictionary reading".
This projectinvolves an additional idea, namely, that one could bestuse a dictionary to gain lexical knowledge by bringing tobear on it a full natural language processing capability.One problem we have encountered is that dictionariesare full of inaccuracies about he meaning of words.
Forexample, even relatively good dictionaries have poorenuies for the likes of determinerless nouns like "bed".E.g., Webster's New World (Second Edition) simplylists "bedtime" as a sense of "bed"; Longman's Dic-tionary of Contemporary English (New Edition) uses"in bed" as an example of the ordinary noun "bed",then explicitly lists the phrase "time for bed" as mean-ing "time to go to sleep", and gives a few other deter-minerless usages, leaving it to the reader to infer a gen-eralization.
* However, a dictionary reader withknowledge of the subregularity mentioned above mightbe able to correct such deficiencies, and come up with abetter meaning that the one the dictionary supplies.Thus, we plan to explore augmenting our intelligent dic-tionary reader with the abifity to use subregularities tocompensate for inadequate dictionary entries.We are also auempting to apply the same approach toacquiring the semantics of constructions.
In particular,we are investigating verb.particular combinations andconventionalized noun phrases (e.g., nominal com-pounds).
We are also looking at constructions like theditransitive (i.e., dative alternation), which seem also todisplay a kind of polysemy.
Specifically, Goldberg(1989, 1990) has argued that much of the data on thisconstruction can be accounted for in terms of subclassesthat are conventionally associated with the constructionitself, rather than with lexical rules and transformationsas proposed, for example, by Gropen et al (1989).
Ifso, then the techniques for the acquisition of polysemous*Longman's also defines "make the bed" u "make it ready fordeepin s in".
We have no idea bow to cope with such ~rrurz, butthey do undenoore the pmble~n.369lexical items should prove equally applicable to theacquisition of knowledge about such constructions.
Weare attempting todetermine whether this is the case.6.
ReferencesAlterman, Richard.
Adaptive Planning: Refitting OldPlans To New Situations.
In the Proceedings of TheSeventh Annual Conference of the Cognitive ScienceSociety, 1985.Alterman, Richard.
Adaptive Planning.
In CognitiveScience, vol.
12, pp.
393-421, 1988.Bloomfield, L. Language.
New York: Holt, Rinehart &Winston, 1933.Brugman, Claudia.
The Story of Over.
University ofCalifornia, Berkeley M.A.
thesis, unpublished.
Distri-butted by the Indiana University Linguistics Club.
1981.Brugman, Claudia.
The Very Idea: A Case-Study inPolysemy and Cross-Lexical GeneraliTation.
In Papersfrom the Twentieth Regional Meeting of the ChicagoLinguistics Society.
pp.
21-38.
1984.Burstein, Mark H. Concept Formation by IncrementalAnalogical Reasoning and Debugging.
In R. S. Michal-ski, J. G. Carbonell, & T. M. Mitchell (eds.
), MachineLearning: An Artificial Intelligence Approach, vol.
II.Tioga Press, Palo Alto, California, 1982.Butlerworth, B. Lexical representation.
In B. Butter-worth (ed.
), Language Production , vol.
2.
AcademicPress, New York, 1983.Carbonell, Jaime.
Learning by analogy: Formulatingand Generalizing Plans from Past Experience.
In R. S.Michalski, J. G. Carbonell, & T. M. Mitchell (eds.
)Machine Learning: An Artificial Intelligence Approach.Tioga Press, Palo Alto, California, 1982.Cutler, A. Lexical complexity and sentence processing.In G. B. Flores d'Arcais & and R. J. Jarvella (eds.
), TheProcess of Language Understanding, pp.
43:79.
Wiley,New York, 1983.Goldberg, Adele.
A Unified Account of the Semanticsof the Ditransifive Construction.
BLS 15, 1989.Goldberg, Adele.
The Inherent Semantics of ArgumentStructure: The Case of the English Ditransitive Con-struction.
Unpublished manuscript, 1990.Granger, R. H. FOUL-UP: A Program that figures outthe meanings of words from context In the Proceedingsof the Fifth International Joint Conference on Aru~cialIntelligence.
Cambridge, MA.
1977.MacWhinney, B.
Competition and Lexical Categoriza-tion.
In R. Corrigan, F. Eckman and M. Noonan,Linguistic Categorization.
John Benjamins PublishingCompany, Amsterdam/Philadelphia, 1989.Martin, James.
Knowledge Acqui~ fion through NaturalLanguage Dialogue.
In the Proceedings of the 2ridConference on Artificial Intelligence Applications.Miami, Florida.
December, 1985.Martin, James.
A Computational Theory of Metaphor.Berkeley Computer Science Technical Report no.UCB/CSD 88/465.
November 1988.Norvig, Peter.
Building a large lexicon with lexical net-work theory.
In the Proceedings ofthe IJCAI Workshopon Lexical Acquisition.
August 1989.Norvig, Peter and Lakoff, George.
Taking: A Study inLexical Network Theory.
In the Proceedings of theThirteenth Annual Meeting of the Berkeley LinguisticsSociety.
Berkeley, CA.
February 1987.Paul, H. Principles of the History of Language.
Long-marts, Green, London, 1891.Rumelhatt, D., & McClelland, L Learning the pasttenses of English verbs: Implicit rules of parallel distri-buted processes?
In B. MacWhinney (ed.
), Mechan-isms of Language Acquisition.
Lawrence ErlbaumAssociates, Hillsdale, New Jersey, 1987.Selfridge, M. Computer Modeling of ComprehensionDevelopment.
In W. G. Lehnert & M. H. Ringle, Stra-tegies for Natural Language Processing.
Lawrence Eft-baum Associates, Hillsdale, New Jersey, 1982.Skousen, R. Analogical Models of Language.
Kluwer,Dordrecht, (in press).Wilensky, R., Mayfield, L, Chin, D., Lm'ia, M., Martin,L and Wu, D. The Berkeley UNIX Consultant Project.Computational Linguistics 14-4, December 1988.370
