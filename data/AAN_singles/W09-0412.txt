Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 75?79,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsNUS at WMT09: Domain Adaptation Experiments for English-SpanishMachine Translation of News Commentary TextPreslav NakovDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nakov@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractWe describe the system developed by theteam of the National University of Singa-pore for English to Spanish machine trans-lation of News Commentary text for theWMT09 Shared Translation Task.
Ourapproach is based on domain adaptation,combining a small in-domain News Com-mentary bi-text and a large out-of-domainone from the Europarl corpus, from whichwe built and combined two separate phrasetables.
We further combined two languagemodels (in-domain and out-of-domain),and we experimented with cognates, im-proved tokenization and recasing, achiev-ing the highest lowercased NIST score of6.963 and the second best lowercased Bleuscore of 24.91% for training without us-ing additional external data for English-to-Spanish translation at the shared task.1 IntroductionModern Statistical Machine Translation (SMT)systems are typically trained on sentence-alignedparallel texts (bi-texts) from a particular domain.When tested on text from that domain, theydemonstrate state-of-the art performance, but onout-of-domain test data the results can deterioratesignificantly.
For example, on the WMT06 SharedTranslation Task, the scores for French-to-Englishtranslation dropped from about 30 to about 20Bleu points for nearly all systems when tested onNews Commentary instead of the Europarl1 text,which was used for training (Koehn and Monz,2006).1See (Koehn, 2005) for details about the Europarl corpus.Subsequently, in 2007 and 2008, the WMTShared Translation Task organizers provided alimited amount of bilingual News Commentarytraining data (1-1.3M words) in addition to thelarge amount of Europarl data (30-32M words),and set up separate evaluations on News Commen-tary and on Europarl data, thus inviting interest indomain adaptation experiments for the News do-main (Callison-Burch et al, 2007; Callison-Burchet al, 2008).
This year, the evaluation is on NewsCommentary only, which makes domain adapta-tion the central focus of the Shared TranslationTask.The team of the National University of Singa-pore (NUS) participated in the WMT09 SharedTranslation Task with an English-to-Spanish sys-tem.2 Our approach is based on domain adapta-tion, combining the small in-domain News Com-mentary bi-text (1.8M words) and the large out-of-domain one from the Europarl corpus (40Mwords), from which we built and combined twoseparate phrase tables.
We further used twolanguage models (in-domain and out-of-domain),cognates, improved tokenization, and additionalsmart recasing as a post-processing step.2 The NUS SystemBelow we describe separately the standard and thenonstandard settings of our system.2.1 Standard SettingsIn our baseline experiments, we used the follow-ing general setup: First, we tokenized the par-2The task organizers invited submissions translating for-ward and/or backward between English and five other Euro-pean languages (French, Spanish, German, Czech and Hun-garian), but we only participated in English?Spanish, due totime limitations.75allel bi-text, converted it to lowercase, and fil-tered out the overly-long training sentences, whichcomplicate word alignments (we tried maximumlength limits of 40 and 100).
We then built sep-arate English-to-Spanish and Spanish-to-Englishdirected word alignments using IBM model 4(Brown et al, 1993), combined them using the in-tersect+grow heuristic (Och and Ney, 2003), andextracted phrase-level translation pairs of maxi-mum length 7 using the alignment template ap-proach (Och and Ney, 2004).
We thus obtaineda phrase table where each phrase translation pairis associated with the following five standard pa-rameters: forward and reverse phrase translationprobabilities, forward and reverse lexical transla-tion probabilities, and phrase penalty.We then trained a log-linear model using thestandard feature functions: language model proba-bility, word penalty, distortion costs (we tried dis-tance based and lexicalized reordering models),and the parameters from the phrase table.
Weset al feature weights by optimizing Bleu (Pap-ineni et al, 2002) directly using minimum errorrate training (MERT) (Och, 2003) on the tuningpart of the development set (dev-test2009a).We used these weights in a beam search decoder(Koehn et al, 2007) to translate the test sentences(the English part of dev-test2009b, tokenizedand lowercased).
We then recased the output us-ing a monotone model that translates from low-ercase to uppercase Spanish, we post-cased it us-ing a simple heuristic, de-tokenized the result, andcompared it to the gold standard (the Spanish partof dev-test2009b) using Bleu and NIST.2.2 Nonstandard SettingsThe nonstandard features of our system can besummarized as follows:Two Language Models.
Following Nakovand Hearst (2007), we used two language mod-els (LM) ?
an in-domain one (trained on a con-catenation of the provided monolingual SpanishNews Commentary data and the Spanish side of thetraining News Commentary bi-text) and an out-of-domain one (trained on the provided monolingualSpanish Europarl data).
For both LMs, we used5-gram models with Kneser-Ney smoothing.Merging Two Phrase Tables.
FollowingNakov (2008), we trained and merged two phrase-based SMT systems: a small in-domain one usingthe News Commentary bi-text, and a large out-of-domain one using the Europarl bi-text.
As a result,we obtained two phrase tables, Tnews and Teuro,and two lexicalized reordering models, Rnews andReuro.
We merged the phrase table as follows.First, we kept all phrase pairs from Tnews.
Thenwe added those phrase pairs from Teuro whichwere not present in Tnews.
For each phrase pairadded, we retained its associated features: forwardand reverse phrase translation probabilities, for-ward and reverse lexical translation probabilities,and phrase penalty.
We further added two new fea-tures, Fnews and Feuro, which show the source ofeach phrase.
Their values are 1 and 0.5 when thephrase was extracted from the News Commentarybi-text, 0.5 and 1 when it was extracted from theEuroparl bi-text, and 1 and 1 when it was extractedfrom both.
As a result, we ended up with seven pa-rameters for each entry in the merged phrase table.Merging Two Lexicalized Reordering Tables.When building the two phrase tables, we alsobuilt two lexicalized reordering tables (Koehn etal., 2005) for them, Rnews and Reuro, which wemerged as follows: We first kept all phrases fromRnews, then we added those from Reuro whichwere not present in Rnews.
This resulting lexical-ized reordering table was used together with theabove-described merged phrase table.Cognates.
Previous research has shown that us-ing cognates can yield better word alignments (Al-Onaizan et al, 1999; Kondrak et al, 2003), whichin turn often means higher-quality phrase pairs andbetter SMT systems.
Linguists define cognatesas words derived from a common root (Bickfordand Tuggy, 2002).
Following previous researchersin computational linguistics (Bergsma and Kon-drak, 2007; Mann and Yarowsky, 2001; Melamed,1999), however, we adopted a simplified definitionwhich ignores origin, defining cognates as wordsin different languages that are mutual translationsand have a similar orthography.
We extracted andused such potential cognates in order to bias thetraining of the IBM word alignment models.
Fol-lowing Melamed (1995), we measured the ortho-graphic similarity using longest common subse-quence ratio (LCSR), which is defined as follows:LCSR(s1, s2) = |LCS(s1,s2)|max(|s1|,|s2|)where LCS(s1, s2) is the longest common subse-quence of s1 and s2, and |s| is the length of s.Following Nakov et al (2007), we combined theLCSR similarity measure with competitive linking(Melamed, 2000) in order to extract potential cog-76nates from the training bi-text.
Competitive link-ing assumes that, given a source English sentenceand its Spanish translation, a source word is ei-ther translated with a single target word or is nottranslated at all.
Given an English-Spanish sen-tence pair, we calculated LCSR for all cross-lingualword pairs (excluding stopwords and words oflength 3 or less), which induced a fully-connectedweighted bipartite graph.
Then, we performed agreedy approximation to the maximum weightedbipartite matching in that graph (competitive link-ing) as follows: First, we aligned the most sim-ilar pair of unaligned words and we discardedthese words from further consideration.
Then, wealigned the next most similar pair of unalignedwords, and so forth.
The process was repeated un-til there were no words left or the maximal wordpair similarity fell below a pre-specified threshold?
(0 ?
?
?
1), which typically left some wordsunaligned.3 As a result we ended up with a list Cof potential cognate pairs.
Following (Al-Onaizanet al, 1999; Kondrak et al, 2003; Nakov et al,2007) we filtered out the duplicates in C , and weadded the remaining cognate pairs as additional?sentence?
pairs to the bi-text in order to bias thesubsequent training of the IBM word alignmentmodels.Improved (De-)tokenization.
The default to-kenizer does not split on hyphenated compoundwords like nation-building, well-rehearsed, self-assured, Arab-Israeli, domestically-oriented, etc.While linguistically correct, this can be problem-atic for machine translation since it can cause datasparsity issues.
For example, the system mightknow how to translate into Spanish both well andrehearsed, but not well-rehearsed, and thus attranslation time it would be forced to handle it asan unknown word, i.e., copy it to the output un-translated.
A similar problem is related to doubledashes, as illustrated by the following training sen-tence: ?So the question now is what can China doto freeze--and, if possible, to reverse--North Ko-rea?s nuclear program.?
We changed the tokenizerso that it splits on ?-?
and ?--?
; we altered the de-tokenizer accordingly.Improved Recaser.
The default recaser sug-gested by the WMT09 organizers was based on amonotone translation model.
We trained such arecaser on the Spanish side of the News Commen-3For News Commentary, we used ?
= 0.4, which wasfound by optimizing on the development set; for Europarl,we set ?
= 0.58 as suggested by Kondrak et al (2003).tary bi-text that translates from lowercase to up-percase Spanish.
While being good overall, it hada problem with unknown words, leaving them inlowercase.
In a News Commentary text, however,most unknown words are named entities ?
persons,organization, locations ?
which are spelled with acapitalized initial in Spanish.
Therefore, we usedan additional recasing script, which runs over theoutput of the default recaser and sets the casing ofthe unknown words to the original casing they hadin the English input.
It also makes sure all sen-tences start with a capitalized initial.Rule-based Post-editing.
We did a quick studyof the system errors on the development set, andwe designed some heuristic post-editing rules,e.g.,?
?
or !
without ?
or ?
to the left: if so, weinsert ?/?
at the sentence beginning;?
numbers: we change English numbers like1,185.32 to Spanish-style 1.185,32;?
duplicate punctuation: we remove dupli-cate sentence end markers, quotes, commas,parentheses, etc.3 Experiments and EvaluationTable 1 shows the performance of a simplebaseline system and the impact of differentcumulative modifications to that system whentuning on dev-test2009a and testing ondev-test2009b.
The table report the Bleu andNIST scores measured on the detokenized out-put under three conditions: (1) without recasing(?Lowercased?
), 2) using the default recaser (?Re-cased (default)?
), and (3) using an improved re-caser and post-editing rules Post-cased & Post-edited?).
In the following discussion, we will dis-cuss the Bleu results under condition (3).System 1 uses sentences of length up to 40tokens from the News Commentary bi-text, thedefault (de-)tokenizer, distance reordering, and a3-gram language model trained on the Spanishside of the bi-text.
Its performance is quite mod-est: 15.32% of Bleu with the default recaser, and16.92% when the improved recaser and the post-editing rules are used.System 2 increases to 100 the maximum lengthof the sentences in the bi-text, which yields 0.55%absolute improvement in Bleu.System 3 uses the new (de-)tokenizer, but thisturns out to make almost no difference.77Recased Post-cased &Lowercased (default) Post-edited# Bitext System Bleu NIST Bleu NIST Bleu NIST1 news News Commentary baseline 18.38 5.7837 15.32 5.2266 16.92 5.50912 news + max sentence length 100 18.91 5.8540 15.93 5.3119 17.47 5.58743 news + improved (de-)tokenizer 18.96 5.8706 15.97 5.3254 17.48 5.60204 news + lexicalized reordering 19.81 5.9422 16.64 5.3793 18.28 5.66965 news + LM: old+monol.
News, 5-gram 22.29 6.2791 18.91 5.6901 20.55 5.99246 news + LM2: Europarl, 5-gram 22.46 6.2438 19.10 5.6606 20.75 5.95707 news + cognates 23.14 6.3504 19.64 5.7478 21.32 6.04788 euro Europarl (?
system 6) 23.73 6.4673 20.23 5.8707 21.89 6.15779 euro + cognates (?
system 7) 23.95 6.4709 20.44 5.8742 22.10 6.160710 both Combining 7 & 9 24.40 6.5723 20.74 5.9575 22.37 6.2506Table 1: Impact of the combined modifications for English-to-Spanish machine translation ondev-test2009b.
We report the Bleu and NIST scores measured on the detokenized output underthree conditions: (1) without recasing (?Lowercased?
), (2) using the default recaser (?Recased (default)?
),and (3) using an improved recaser and post-editing rules (?Post-cased & Post-edited?).
The News Com-mentary baseline system uses sentences of length up to 40 tokens from the News Commentary bi-text,the default tokenizer and de-tokenizer, a distance-based reordering model, and a trigram language modeltrained on the Spanish side of the bi-text.
The Europarl system is the same as system 6, except that ituses the Europarl bi-text instead of the News Commentary bi-text.System 4 adds a lexicalized re-ordering model,which yields 0.8% absolute improvement.System 5 improves the language model.
It addsthe additional monolingual Spanish News Com-mentary data provided by the organizers to theSpanish side of the bi-text, and uses a 5-gram lan-guage model instead of the 3-gram LM used bySystems 1-4.
This yields a sizable absolute gain inBleu: 2.27%.System 6 adds a second 5-gram LM trained onthe monolingual Europarl data, gaining 0.2%.System 7 augments the training bi-text withcognate pairs, gaining another 0.57%.System 8 is the same as System 6, except thatit is trained on the out-of-domain Europarl bi-text instead of the in-domain News Commentarybi-text.
Surprisingly, this turns out to work bet-ter than the in-domain System 6 by 1.14% ofBleu.
This is a quite surprising result since inboth WMT07 and WMT08, for which compara-ble kinds and size of training data was provided,training on the out-of-domain Europarl was al-ways worse than training on the in-domain NewsCommentary.
We are not sure why it is differentthis year, but it could be due to the way the dev-train and dev-test was created for the 2009 data ?by extracting alternating sentences from the origi-nal development set.System 9 augments the Europarl bi-text withcognate pairs, gaining another 0.21%.System 10 merges the phrase tables of systems7 and 9, and is otherwise the same as them.
Thisadds another 0.27%.Our official submission to WMT09 is the post-edited System 10, re-tuned on the full developmentset: dev-test2009a + dev-test2009b (inorder to produce more stable results with MERT).4 Conclusion and Future WorkAs we can see in Table 1, we have achieved notonly a huge ?vertical?
absolute improvement of5.5-6% in Bleu from System 1 to System 10, butalso a significant ?horizontal?
one: our recased andpost-edited result for System 10 is better than thatof the default recaser by 1.63% in Bleu (22.37%vs.
20.74%).
Still, the lowercased Bleu of 24.40%suggests that there may be a lot of room for fur-ther improvement in recasing ?
we are still about2% below it.
While this is probably due primarilyto the system choosing a different sentence-initialword, it certainly deserves further investigation infuture work.AcknowledgmentsThis research was supported by research grantPOD0713875.78ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz JosephOch, David Purdy, Noah Smith, and DavidYarowsky.
1999.
Statistical machine translation.Technical report, CLSP, Johns Hopkins University,Baltimore, MD.Shane Bergsma and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL?07), pages656?663, Prague, Czech Republic.Albert Bickford and David Tuggy.
2002.Electronic glossary of linguistic terms.http://www.sil.org/mexico/ling/glosario/E005ai-Glossary.htm.Peter Brown, Vincent Della Pietra, Stephen DellaPietra, and Robert Mercer.
1993.
The mathematicsof statistical machine translation: parameter estima-tion.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 136?158, Prague, CzechRepublic.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on Statisti-cal Machine Translation, pages 70?106, Columbus,OH, USA.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweenEuropean languages.
In Proceedings of the FirstWorkshop on Statistical Machine Translation, pages102?121, New York, NY, USA.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation (IWSLT?05), Pitts-burgh, PA, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL?07).
Demonstration session, pages 177?180,Prague, Czech Republic.P.
Koehn.
2005.
Europarl: A parallel corpus for eval-uation of machine translation.
In Proceedings of theX MT Summit, pages 79?86, Phuket, Thailand.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical translationmodels.
In Proceedings of the Annual Meeting of theNorth American Association for Computational Lin-guistics (NAACL?03), pages 46?48, Sapporo, Japan.Gideon Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.In Proceedings of the Annual Meeting of the NorthAmerican Association for Computational Linguis-tics (NAACL?01), pages 1?8, Pittsburgh, PA, USA.Dan Melamed.
1995.
Automatic evaluation and uni-form filter cascades for inducing N-best translationlexicons.
In Proceedings of the Third Workshop onVery Large Corpora, pages 184?198, Cambridge,MA, USA.Dan Melamed.
1999.
Bitext maps and alignmentvia pattern recognition.
Computational Linguistics,25(1):107?130.Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.Preslav Nakov and Marti Hearst.
2007.
UCB systemdescription for the WMT 2007 shared task.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 212?215, Prague, CzechRepublic.Preslav Nakov, Svetlin Nakov, and Elena Paskaleva.2007.
Improved word alignments using the Web asa corpus.
In Proceedigs of Recent Advances in Nat-ural Language Processing (RANLP?07), pages 400?405, Borovets, Bulgaria.Preslav Nakov.
2008.
Improving English-Spanish sta-tistical machine translation: Experiments in domainadaptation, sentence paraphrasing, tokenization, andrecasing.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 147?150,Columbus, OH, USA.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics (ACL?03), pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics (ACL?02), pages 311?318,Philadelphia, PA, USA.79
