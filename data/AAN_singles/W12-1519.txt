INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 115?119,Utica, May 2012. c?2012 Association for Computational LinguisticsInteractive Natural Language Query Construction for Report Generation?Fred PopowichSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, CANADApopowich@sfu.caMilan MosnyResponse42 IncNorth Vancouver, BC, CanadaMilan.Mosny@response42.comDavid LindbergSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, CANADAdll4@sfu.caAbstractQuestion answering is an age old AI chal-lenge.
How we approach this challenge is de-termined by decisions regarding the linguis-tic and domain knowledge our system willneed, the technical and business acumen ofour users, the interface used to input ques-tions, and the form in which we should presentanswers to a user?s questions.
Our approachto question answering involves the interactiveconstruction of natural language queries.
Wedescribe and evaluate a question answeringsystem that provides a point-and-click, web-based interface in conjunction with a seman-tic grammar to support user-controlled naturallanguage question generation.
A preliminaryevaluation is performed using a selection of 12questions based on the Adventure Works sam-ple database.1 IntroductionThere is a long history of systems that allow usersto pose questions in natural language to obtain ap-propriate responses from information systems (Katz,1988; El-Mouadib et al, 2009).
Information sys-tems safeguard a wealth of information, but tradi-tional interfaces to these systems require relativelysophisticated technical know-how and do not alwayspresent results in the most useful or intuitive way fornon-technical users.
Simply put, people and com-puters do not speak the same language.
The ques-tion answering challenge is thus the matter of devel-oping a method that allows users with varying levels?This research was supported in part by a discovery grantfrom the Natural Sciences and Engineering Research Councilof Canada.
The authors would also like to thank the referees fortheir insights and suggestions.of technical proficiency to ask questions using natu-ral language and receive answers in an appropriate,intuitive format.
Using natural language to ask thesequestions may be easy for users, but is challengingdue to the ambiguity inherent in natural languageanaylsis.
Proposals involving controlled natural lan-guage, such as (Nelken and Francez, 2000), can dealwith some of the challenges, but the task becomesmore difficult when we seek to answer natural lan-guage questions in a way that is domain portable.Before we can attempt to design and implement aquestion answering system, we need to address sev-eral key issues.
First, we need to decide what knowl-edge our system needs.
Specifically, we must decidewhat linguistic knowledge is needed to properly in-terpret users?
questions.
Then we need to considerwhat kind of domain-specific knowledge the systemmust have and how that knowledge will be storedand accessed.
We must address the challenges posedby users with varying levels of technical sophistica-tion and domain knowledge.
The sophistication ofthe user and the environment in which the systemis used will also affect how users will give input tothe system.
Will we need to process text, speech,or will a simpler point-and-click interface be suf-ficient?
Finally, we must decide how to best an-swer the user?s questions, whether it be by fetch-ing pre-existing documents, dynamically generat-ing structured database reports, or producing nat-ural language sentences.
These five issues do notpresent us with a series of independent choices thatare merely stylistic or cosmetic.
The stance we takeregarding each of these issues strongly influencesdesign decisions, ease of installation/configuration,and the end-user experience.Here we solve this problem in the context of ac-115cessing information from a structured database ?
anatural language interface to a database (NLIDB)(Kapetanios et al, 2010).
However, instead of treat-ing it as a natural language analysis problem, wewill consider it as a task involving natural languagegeneration (NLG) where users build natural lan-guage questions by making choices that add wordsand phrases.
Using our method, users constructqueries in a menu driven manner (Tennant et al,1983; Evans and Power, 2003) to ask questions thatare always unambiguous and easy for anyone to un-derstand, getting answers in the form of interactivedatabase reports (not textual reports) that are bothimmediate and consistent.This approach retains the main advantage of tra-ditional NLIDBs that allow input of a question in afree form text ?
the ability for the user to communi-cate with the information system in English.
Thereis no need for the user to master a computer querylangauge such as SQL or MDX.
Many disadvant-ges of traditional free input NLIDBs are removed(Tennant et al, 1983).
Traditional NLIDBs fail toanalyze some questions and indicate so to the user,greatly decreasing the user?s confidence in the sys-tem.
The problem is even worse when the NLIDBanalyzes the question incorrectly and produces awrong or unpexpected result.
In contrast, our systemis able to answer every question correctly.
In tradi-tional free input NLIDBs, the user can make gram-matical or spelling mistakes that may lead to othererrors.
Using a menu-based technique, the user isforced to input only valid and wellformed queries.The complexity of the system is greatly reduced asthe language that the system has to process is sim-ple and unambiguous.
Portability to other domainsis improved because there is no need for vocabularythat fully covers the domain.2 Our approachWe begin with an overview of our approach to thisquestion answering problem involving NLG.
We de-scribe how we address each of the afore-mentionedissues and give our rationale for each of thosechoices.
Following a brief discussion of our useof online analytical processing (OLAP) (Janus andFouche, 2009) in section 2.2, we then decribe howwe use the OLAP model as the basis for interactivenatural query generation, and describe the databaseused in our evaluation, along with the grammar usedfor NLG.2.1 OverviewOur approach to the question answering problem isbased on the following decisions and assumptions:Linguistic knowledge We use a semantic grammarto support user-controlled NLG rather than languageanalysis.
By guiding the construction process, weavoid difficult analysis tasks, such as resolving am-biguities and clarifying vague language.
We alsoeliminate the possibility of out-of-domain queries.Domain-specific knowledge We model domainknowledge using an OLAP cube, a widely-usedapproach to model domain-specific data.
OLAPcubes provide a standard semantic representationthat is well-suited to historical business data andallows us to automatically generate both the lexiconand the semantic grammar for our system.Users The prototypical user of our system is famil-iar with business issues but does not have a high-degree of technical expertise.
We provide a simpleand intuitive interface suitable for such users but stillpowerful enough for users of any level of technicalproficiency.Input A web-based, point-and-click interface willguide users in the creation of a natural languagequery string.
Users click on words and phrases toconstruct a question in plain English.Answers We will answer questions with an interac-tive database report.
Users can click on parts of thereport to get detailed information, making it more ofan interactive dashboard rather than a report.An approach governed by these principles offersmany benefits.
It simplifies database report creationand lowers the associated costs, allows businesses toleverage existing investments in data warehouse andreporting technology, offers a familiar and comfort-able interface, does not require installation on clientmachines, and is simple to install and configure.2.2 Role of OLAPAn OLAP cube is produced as a result of process-ing a datawarehouse into datastructures optimized116for query processing.
The OLAP query languagemakes reference to measure groups (that roughlycorrespond to fact tables), measures (that come fromthe numerical values in the fact tables) and dimen-sions (that come from dimension tables).
For ex-ample, the order fact table might include total or-der price, order quantity, freight cost, and discountamount.
These are the essential figures that describeorders, but to know more we need to examine thesefacts along one or more dimensions.
Accordingly,the dimension tables associated with this fact tableinclude time (order date, year, quarter, and month),customer (name, address, city, and zip code), andproduct (name, category, and price).2.3 Interactive Natural Language GenerationAt the heart of the system is a semantic grammar.Our goal was to create a grammar that is suitable todatabase querying application, but is simple enoughso that it can be automatically adapted to differentdomains.
The semantic model makes use of bothentities (unary predicates) and relationships (binarypredicates) that are automatically derived from theOLAP model.
These entities and relationships canbe directly and automatically mapped to the lexicalitems and phrases that the user sees on the screenduring query construction.
Once a user has com-pleted the construction of a natural language query,a corresponding first order logic formula is createdwhich can then be translated into a database queryin SQL or MDX.Our assumption was that many database queriescan be expressed within the following templateShow <Show> and ... and <Show> foreach <GroupBy> and ... and foreach <GroupBy> limit to <LimitTo>and ... and to <LimitTo>where <Show>, <GroupBy> and <LimitTo> aredifferent classes of nominals.
<Show> may refer toa measure or to a level in a dimension which maytake an additional constraint in a form of a preposi-tional clause.
<GroupBy> may refer to a level ina dimension which may take a constraint in a formof a prepositional phrase or to a set of members of adimension.
<LimitTo> may refer to a set of mem-bers of a dimension.
A prepositional phrase express-ing a constraint has a formwith <NounPhrase>QuestionElementTerminalEntityTerminalGroupByEntityTerminalNonterminalTopLevelGroupByLimitToShowPrepositionalClauseDeterminerNounPhraseListFigure 1: Semantic Grammar Element Classeswhere the noun phrase consists of a determiner suchas ?some?, ?no?, ?at least N?, ?exactly N?
and anoun referring to a measure.The semantic grammar makes use of classes in aninheritance hierarchy as shown in Figure 1.
Eachquestion element corresponds to a parametrized ter-minal or nonterminal.
That is, it can play a role ofone of multiple terminals or nonterminals depend-ing on its initialization parameters.
There are alto-gether 13 classes that comprise the elements of thegrammar.
The implementations of the different classelements make use of semantic constraints as appro-priate.
Only minimal human intervention is requiredwhen adapting the system to a new OLAP cube.
Theintervention consists of ?cleaning up?
the automat-ically generated terminal symbols of the semanticgrammar so that the plural and singular forms thatwere present in the cube metadata are used consis-tently and so that the mass vs. countable attribute ofeach measure is set appropriately.3 EvaluationAn evaluation of this kind of system requires anexamination of three performance metrics: domaincoverage, ease of use, and query efficiency.
Howwell the system covers the target domain is cruciallyimportant.
In order to measure domain coverage, weneed to determine how many answerable questionscan actually be answered using the system.
We cananswer this question in part by examining the userinterface.
Does the interface restrict users?
accessto domain elements and relationships?
A more thor-ough assessment of domain coverage requires exten-117sive user studies.Ease of use is often thought of as a qualitativemeasure of performance, but a systematic, objectiveevaluation requires us to define a quantitative mea-sure.
The primary action used to generate queriesin our system is the ?click.?
Users click on items torefine their queries, so the number of clicks requiredto generate queries seems like a reasonable startingpoint for evaluating ease of use.
The time it takesusers to make those clicks is important.
A four-clickquery sounds efficient, but if it takes the user twominutes to figure out which four clicks need to bemade, not much is gained.
It would be ideal if thenumber of clicks and the time needed to make thoseclicks grow proportionally.
That is, we do not wantto penalize users who need to build longer queries.Query efficiency is measured by the time betweenthe user submitting a query and the system present-ing the answer.
How long must a user wait whiledata is being fetched and the report generated?
Un-like ease of use, this is objectively measurable andeasy to benchmark.In our initial evaluation, we applied these metricsto a selection of 12 natural language questions aboutthe data in the Adventure Works (Codeplex OpenSource Community, 2008) database that could beanswered by our natural language query construc-tion system.
These questions were generated by auser with prior exposure to the Adventure Worksdatabase but no prior exposure to the query construc-tion software system or its design or algorithms, sothe questions are not purposely fine-tuned to yieldartificially optimal results.
Eight of these questionswere directly answerable, while four were indirectlyanswerable.
For each of these questions, we mea-sured the number of clicks required to generate thequery string, the time it took to make the requiredclicks, and the time required to retrieve the neededrecords and generate a report.
The distinction be-tween directly answerable and indirectly answerablequestions deserves a short explanation.
A questionis deemed directly answerable if the answer is thesole result returned in the report or if the answer isincluded in a group of results returned.
A question isdeemed indirectly answerable if the report generatedbased on a related query can be used to calculate theanswer or if the information relevant to the answeris a subset of the information returned.
So, the ques-tion What are the top 20 products based on inter-net sales was directly answerable through the con-structed query Show products with one of 20 highestinternet sales amount, while the question What is theaveragefreight cost for internet orders over $1000could only be answered Show internet freight costfor customers with more than 1000 dollars of inter-net sales amount and for each date.We found that a user was able to construct nat-ural language queries using between 2 and 6 clickswhich required 10 and 57 seconds of elaspsed timefor the construction process.
On average 3.3 clickswere required to create a query with an average timeof 33 seconds, where the time grew in a linear man-ner based on the number of clicks.
Once a query wasconstructed, the average time to generate a reportwas 6.7 seconds with the vast majority of queriesproducing a report from the database system in 4seconds or less.
The median values for query con-struction was 2.5 clicks, query construction was 31.5seconds, and report generation was 4 seconds..4 Analysis and ConclusionsOur evaluation suggests that the menu driven NLGapproach results in the rapid creation of unambigu-ous queries that can retrieve the relevant databaseinformation corresponding to the query.
It has beenembedded in a system that uses OLAP cubes toproduce database reports (and dashboards) that al-low user interaction with the retrieved information.The system was automatically adapded to a givenOLAP cube (only minimal human intervention wasrequired) and can be equally easily adapted to otherOLAP cubes serving other domains.Our results build on semantic web related work(Paiva et al, 2010) that shows that use of NLG forguided queries construction can be an effective al-ternative to a natural language interface to an in-formation retrieval system.
We deal with a highlyconstrained natural language (cf.
the analysis gram-mars used by (Nelken and Francez, 2000; Thorneand Calvanese, 2012)) that is effective in generationof database queries and the generation (not analysis)of natural language.
Like (Paiva et al, 2010), werely on a semantic grammar, but instead build on theinformation that can be automatically extracted fromthe database model, rather than leveraging knowl-118edge from semantic web resources.
Furthermore, weprovide a more detailed evaluation as to the effec-tiveness of the guided query construction technique.Use of OLAP in NLG has also been explored inthe context of content planning (Favero and Robin,2000), and can play an important role in dealing withdomain portability issues not only in the context ofNLG but also in other natural language database ap-plications.
Our technique for leveraging the datamodel and OLAP cube avoids human customizationtechniques like those reported by (Minock, 2010)where an explicit mapping between phrases anddatabase relations and entities needs to be provided,and (Evans and Power, 2003) where explicit domaininformation needs to be entered.The NLG query construction approach does havelimitations, since users will likely have questionsthat either cannot be constructed by the seman-tic grammar, or that cannot be answered from theunderlying database.
However, issues related tochoice or ambiguity that are frequently encounteredby NLG systems in particular, and natural languageprocessing systems in general, can be avoided byhaving a human ?in the loop.
?Efficiency and effectiveness is derived from howwe leverage human knowledge, both in query com-position and result interpretation.
In traditional,non-intelligent query scenarios, users know whatthey want to ask but not necessarily how to ask it.By guiding the user through the NLG process, theuser can focus on the what not the how.
Databasereports are generated quickly, providing unambigu-ous answers in a clear, flexible format.
and in a fa-miliar, comfortable, un-intimidating web-based en-vironment.
Aside from usability benefits, this web-based approach has the added benefit of minimizingconfiguration and maintenance.Our results are only suggestive, since they involveonly 12 questions.
They suggest it would be worth-while to expend the resources for a full study thatincludes multiple users with different levels of ex-perience, multiple domains and larger sets of ques-tions.
A more fine-grained analysis of the differ-ence between the results sets of constructed Englishqueries and the expected answers to original ques-tions should also be performed along with an evalu-ation of how easy it is for the user to find the answerto the question within the database report.ReferencesCodeplex Open Source Community.
2008.
Adventure-works SQL Database Product Samples.
CODEPLEX.http://msftdbprodsamples.codeplex.com.Faraj A. El-Mouadib, Zakaria S. Zubi, Ahmed A. Alma-grous, and Irdess S. El-Feghi.
2009.
Generic inter-active natural language interface to databases (GIN-LIDB).
Int Journal of Computers, 3:301?310.Roger Evans and Richard Power.
2003.
WYSIWYM- building user interfaces with natural language feed-back.
In Proc.
of EACL 2003, 10th Conf.
of the Euro-pean Chapter of the ACL, pages 203?206, Budapest,Hungary.Eloi Favero and Jacques Robin.
2000.
Using OLAPand data mining for content planning in natural lan-guage generation.
In NLDB ?00 Proc.
5th Interna-tional Conference on Applications of Natural Lan-guage to Information Systems-Revised Papers, pages164?175.
Springer-Verlag, London.Phil Janus and Guy Fouche.
2009.
Introduction to olap.In Pro SQL Server 2008 Analysis Services, pages 1?14.
Springer-Verlag.Epaminondas Kapetanios, Vijayan Sugumaran, and MyraSpiliopoulou.
2010.
Special issue: 13th internationalconference on natural language and information sys-tems (NLDB 2008) five selected and extended papers.Data and Knowledge Engineering, 69.Boris Katz.
1988.
Using english for indexing and re-trieving.
In Proceedings of the First RIAO Conferenceon User-Oriented Content-Based Text and Image Han-dling (RIAO ?88).
CID.Michael Minock.
2010.
C-PHRASE: a system for build-ing robust natural language interfaces to databases.Data and Knowledge Engineering, 69:290?302.Rani Nelken and Nissim Francez.
2000.
Queryingtemporal databases using controlled natural language.In Proc 18th International Conference on Computa-tional Linguistics (COLING 2000), pages 1076?1080,Saarbru?cken, Germany, August.Sara Paiva, Manuel Ramos-Cabrer, and Alberto Gil-Solla.
2010.
Automatic query generation in guidedsystems: natural language generation from graphicallybuilt query.
In Proc 11th ACIS Intl Conf on SoftwareEngineering, Artificial Intelligence, Networking andParallel/Distributed Computing (SNPD 2010), pages165?170.
IEEE Conf Publishing Services.Harry Tennant, Kenneth Ross, Richard Saenz, CraigThompson, and James Miller.
1983.
Menu-basednatural language understanding.
In Proc 21st annualmeeting of the Association of Computational Linguis-tics, pages 151?158.
ACL.Camilo Thorne and Diego Calvanese.
2012.
Tractabil-ity and intractability of controlled languages for dataaccess.
Studia Logica, to appear.119
