NOKIA Research Center Beijing Chinese Word Segmentation Systemfor the SIGHAN Bakeoff 2007Jiang LI1, 2, Rile HU1, Guohua ZHANG1, Yuezhong TANG1,Zhanjiang SONG1 and Xia WANG1NOKIA Research Center, Beijing1Beijing University of Posts and Telecommunications2{ext-jiang.1.li, ext-rile.hu,ext-guohua.zhang,yuezhong.tang,zhanjiang.song,xia.s.wang}@nokia.comAbstractThis paper presents the Chinese wordsegmentation system developed by NOKIAResearch Center (NRC), which wasevaluated in the Fourth InternationalChinese Language Processing Bakeoff andthe First CIPS Chinese LanguageProcessing Evaluation organized bySIGHAN.
In our system, a preprocessingmodule was used to discover the out-of-vocabulary words which occur repeatedlyin the text, then an improved n-gram modelwas used for segmentation and some postprocessing strategies are adopted in systemto recognize the organization names andnew words.
We took part in three tracks,which are called the open and closed trackon corpora State Language Commission ofP.R.C.
(NCC), and closed track on corporaShanxi University (SXU).
Our systemachieved good performance, especially inthe open track on NCC, our system ranks1st among 11 systems.1 IntroductionChinese word segmentation is an essential and coretechnology in Chinese language processing, andgenerally it is the first stage for later processing,such as machine translation, text summarization,information retrieval and etc.
The topic of Chineseword segmentation has been researched for manyyears.
Many approaches have been developed tosolve the problems under this topic.
Among theseapproaches, statistical approaches are most widelyused.Our system based on a pragmatic approach,integrating a lot of features and information, theframework is similar to (Jianfeng Gao, 2005).
Inour system, the model is simplified to n-gramarchitecture.
First, all the possible paths ofsegmentation will be considered and each of thecandidate word will be categorized into a certaintype.
Second, each word will be given a value;each type has different computational strategy andis processed in different ways.
At last, all thepossible paths of segmentation are calculated andthe best path is selected as the final result.N-gram language model is a generative model,and it could express the correlation of the contextword very well.
But it is powerless to detect theout-of-vocabulary word (OOV).
In the post-processing module, we detect the OOV throughsome Chinese character information instead of theword information.
In addition, to deal with the longorganization names in NCC corpus, a module forcombining organization name is adopted.The remainder of this paper is organized asfollow: section 2 describes our system in detail;section 3 presents the experiment results andanalysis; in last section we give our conclusionsand future research directions.2 System DescriptionThe basic architecture of our system is shown infigure 1, and the detailed description of eachmodule is provided in the following subsections.86Sixth SIGHAN Workshop on Chinese Language Processing2.1 FrameworkThe input of the system is text to be segmented.First, the system scans the text and finds out thecharacter strings appear many times but notlexicon words.
These strings are called recurring-OOV.
Second, all the candidate words arecategorized into different types and the optimalpath is calculated by Viterbi algorithm.
Finally,some post-processing strategies are used to modifythe results: NW detection is used to merge twosingle characters as a new word, and organizationcombination is provided to combine some words asan organization name.Figure 1: Framework of system2.2 Recurring-OOV DetectionWe found that there are some words which appearmany times in different position of the context.
Forexample, a verb ???
?
appears 2 times in asentence, ????
???????????????????????
?, and a person name?????
appears in different sentences, ?????????????????
????????????????.
These words are defined asRecurring-OOV.Therefore, without any prior knowledge, theChinese text is scanned; the sentence of the text iscompared with itself and compared with otherswhich were close to it for finding out the repeatedstrings.
All these repeated character strings weresaved in list.
Not all of them are considered as thecandidates of OOV word.
Only 2-character or 3-character repeated strings are considered as thecandidates of OOV words.
And some simple rulesare used to avoid some wrong classification.
Forexample, if there is a repeated string containscharacter ??
?, which is a high frequent one-character word, this repeated string is notconsidered as a recurring OOV.A value (probability) will be given to eachRecurring-OOV.
Two factors will be considered inthe value evaluation?1?
The repeating times of the Recurring-OOV in the testing corpus.
The more itrepeats, the bigger the value will be.2?
Character-based statistical informationand some other information are alsoconsidered to calculate the probability ofthis string to be a word.
The computingmethod is described in Section 2.5, NWDetection.2.3 Word CategorizationIn our system, Chinese words are categorized intoa set of types as follows:1.
Lexicon Words (LW).
The words in this typecan be found in the library we get from the trainingcorpus.2.
Factoids (FT).
This type includes the Englishletter, Arabic numerals and etc.3.
Named Entity (NE).
This type includes personname and location name.
Being different from theGao?s system, the organization detection is a postprocessing in our system.4.
Recurring-OOV.
This type is described in thesection 2.2.5.
Others.2.4 N-gram ModelEach candidate word Wi is relegated to a type Tjand assigned a value P(Wi |Tj) .
The transferprobability between word-types is also assigned avalue P(Tj-1|Tj).
We assume that W = W1W2?Wnis a word sequence, the segmentation module?stask is to find out the most likely word sequenceamong all possible paths:W* = arg max ?
P(Wi|Tj)P(Tj-1|Tj)        (1)87Sixth SIGHAN Workshop on Chinese Language ProcessingViterbi algorithm is used to search the optimizedpath described in Equation (1).2.5 NW DetectionThis module is used to detect the New Words,which ?refer to OOV words that are neitherrecognized as named entities or factoids?
(JianfengGao, 2005).
And in particular, we only considerthe 2-character words for the reason that 2-character words are the most common in Chineselanguage.We identify the new words through somefeatures of Chinese character information:1.
The probability of a character occurring in theleft/right position.In fact, most Chinese characters have theirfavorite position.
For example, ???
almost occursin the left, ???
almost occurs in the right and ??
?always compose a single word by itself.
So thestring ????
is much more possible to be a newword than ????
?and string ????
is not likely tobe a word.2.
The similarity of different characters.If two characters often occur in the sameposition with the same character to form a word, itis considered that the two characters are similar, orthere is a short distance between them.
Forexample, the character ???
is very similar to ??
?in respect that they are almost in the left positionwith some same characters, such as ??
?, ??
?, toconstruct the word ????,????,????,???
?.So if we know the ????
is a word, we canspeculate the string ????
is also a word.The strict mathematical formula which used todescribe the similarity of characters is reported in(Rile Hu, 2006).2.6 Organization combinationThe organization name is recognized as a longword in the NCC corpus, but during the n-gramprocessing, these long words will be segmentedinto several shorter words.
In our system, theorganization names are combined in this module.First, a list of suffix-words of organization name,such as ???
?
???
?, is selected from thetraining set.
Second, the string that has beensegmented in previous module is searched to findout the suffix-word, which is considered as acandidate of organization name.
At last, weestimate the possibility of the candidate string andjudge it is an organization name or not.3 Evaluation Results3.1 ResultsWe took part in three segmentation tasks inBakeoff-2007, which are named as the open andclosed track on corpora State LanguageCommission of P.R.C.
(NCC), and closed track oncorpora Shanxi University (SXU).Precision (P), Recall (R) and F-measure (F) areadopted to measure the performance of wordsegmentation system.
In addition, OOV Recall(ROOV), OOV Precision (POOV) and OOV F-measure (FOOV) are very important indicatorswhich reflect the system?s ability to deal with theOOV words.The results of our system in three tasks areshown in Table 1.Table 1: Test set results on NCC, SXUCorpus NCC-O NCC-C SXU-CR 0.9735 0.9417 0.9558P 0.9779 0.9272 0.9442F 0.9757 0.9344 0.95ROOV 0.8893 0.4001 0.5176POOV 0.8867 0.6454 0.6966FOOV 0.888 0.494 0.5939RIV 0.9777 0.9687 0.9794PIV 0.9824 0.9356 0.9539FIV 0.98 0.9518 0.96653.2 NCC Open TrackFor the open track of NCC, an external corpus isused for training and the size of training set isabout 54M.
In addition, there are some specialdictionary were added to identify some specialwords.
For example, an idiom dictionary is used tofind the idioms and a personal-name dictionary isused to identify the common Chinese names.3.3 Error AnalysisApart from ranking 1st in NCC open test, oursystem got not so good results in NCC close testand SXU close test.The comparison between our system results andbest results in bakeoff-2007 are shown in table 2.88Sixth SIGHAN Workshop on Chinese Language ProcessingTable 2: The comparison between our systemresults and best resultsF-Measures TypeBakeoff-2007 Our systemNCC-O 0.9757 0.9757NCC-C 0.9405 0.9344SXU-C 0.9623 0.95Table 3: The comparison between our systemresults and best Top 3 results in OOVidentificationROOV POOV FOOVOur  0.4001 0.6454 0.4941st  0.6179 0.5984 0.6082nd  0.4502 0.6196 0.5215NCC-C3rd  0.6158 0.5542 0.5834Our  0.5176 0.6966 0.59391st  0.7429 0.7159 0.72922nd  0.6454 0.7022 0.6726SXU-C3rd  0.6626 0.6639 0.6632In table 3, 1st, 2nd and 3rd are the best Top 3systems in the test.
It shows that in the close trackin NCC, the OOV Precision of our system is thebest, but the OOV Recall is the worst in all the foursystem.
Similarly, in the close track in SXU, theOOV Precision is very close to the best one, andthe OOV Recall is the worst.
It means that oursystem is too cautious in identifying the OOVwords.Our system was carefully tuned on NCCtraining set.
The NCC training set contains articlesfrom many domains; the OOV words can not beeasily detected.
Therefore, in parameter tuning, weraise the threshold of OOV.
This strategy increasesthe precision of the OOV detection, but decreasesthe recall of this.
And we also use some simplerules to filter the OOV candidates.
These rules caneasily pick out the wrongly detected OOVs, but atthe same time, they remove some correctcandidates by mistake.The performance of our system is good in NCCclose test but not so good in SXU close test.
Thismeans that our strategies for OOV detection is toocautious for SXU close test.4 Conclusion and Future WorkIn this paper, a detailed description on a Chineseword segmentation system is presented.
N-grammodel is adopted as the language model, and somepreprocessing and post processing methods areintegrated as a unified framework.
The evaluationresults show the efficiency of our approaches.In future research, we will continue to enhanceour system with other new techniques, especiallywe will focus on improving the recall of OOVwords.ReferencesJianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang.2005.
Chinese Word Segmentation and Named EntityRecognition: A Pragmatic Approach.
ComputationalLinguistics, Vol.31(4): 531-574.Hai Zhao, Chang-Ning Huang and Mu Li.
2006.
AnImproved Chinese Word Segmentation System withConditional Random Field.
Proceedings of the fifthSIGHAN Workshop on Chinese LanguageProcessing, 162-165.
Sydney, Australia.Adwait Ratnaparkni.
1996.
A Maximum Entropy Part-of-speech Tagger.
In Proceedings of the EmpiricalMethod in Natural Language Processing Conference,133-142.
University of Pennsylvania.JK Low, HT Ng, W Guo.
2005.
A Maximum EntropyApproach to Chinese Word Segmentation.Proceedings of the fourth SIGHAN Workshop onChinese Language Processing.
Jeju Island, Korea.Manning, Christopher D. and Hinrich Schutze.
1999.Foundations of Statistical Natural LanguageProcessing.
The MIT Press, Cambridge,Massachusetts, London, England.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
International Journal ofComputational Linguistics and Chinese LanguageProcessing, 8(1).Rile Hu, Chengqing Zong, and Bo Xu.
An Approach toAutomatic Acquisition of Translation TemplatesBased on Phrase Structure Extraction and Alignment.IEEE Transaction on Speech and Audio Processing.Vol.
14, No.5, September 2006.89Sixth SIGHAN Workshop on Chinese Language Processing
