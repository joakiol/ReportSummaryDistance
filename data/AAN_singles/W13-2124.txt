Proceedings of the 14th European Workshop on Natural Language Generation, pages 178?182,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGenNext: A Consolidated Domain Adaptable NLG SystemFrank Schilder, Blake Howald and Ravi Kondadadi?Thomson Reuters, Research & Development610 Opperman Drive, Eagan, MN 55123firstname.lastname@thomsonreuters.comAbstractWe introduce GenNext, an NLG systemdesigned specifically to adapt quickly andeasily to different domains.
Given a do-main corpus of historical texts, GenNextallows the user to generate a template bankorganized by semantic concept via deriveddiscourse representation structures in con-junction with general and domain-specificentity tags.
Based on various featurescollected from the training corpus, thesystem statistically learns template rep-resentations and document structure andproduces well?formed texts (as evaluatedby crowdsourced and expert evaluations).In addition to domain adaptation, Gen-Next?s hybrid approach significantly re-duces complexity as compared to tradi-tional NLG systems by relying on tem-plates (consolidating micro-planning andsurface realization) and minimizing theneed for domain experts.
In this descrip-tion, we provide details of GenNext?s the-oretical perspective, architecture and eval-uations of output.1 IntroductionNLG systems are typically tailored to very spe-cific domains and tasks such as text summariesfrom neonatal intensive care units (SUMTIME-NEONATE (Portet et al 2007)) or offshore oilrig weather reports (SUMTIME-METEO (Reiter etal., 2005)) and require significant investments indevelopment resources (e.g.
people, time, etc.
).For example, for SUMTIME-METEO, 12 personmonths were required for two of the system com-ponents alone (Belz, 2007).
Given the subjectmatter of such systems, the investment is perfectly?Ravi Kondadadi is now affiliated with Nuance Commu-nications, Inc.reasonable.
However, if the domains to be gener-ated are comparatively more general, such as fi-nancial reports or biographies, then the scaling ofdevelopment costs becomes a concern in NLG.NLG in the editorial process for companies andinstitutions where content can vary must be do-main adaptable.
Spending a year or more of devel-opment time to produce high quality market sum-maries, for example, is not a viable solution if it isnecessary to start from scratch to produce other re-ports.
GenNext, a hybrid system that statisticallylearns document and sentence template represen-tations from existing historical data, is developedto be consolidated and domain adaptable.
In par-ticular, GenNext reduces complexity by avoidingthe necessity of having a separate document plan-ner, surface realizer, etc., and extensive expert in-volvement at the outset of system development.Section 2 describes the theoretical background,architecture and implementation of GenNext.
Sec-tion 3 discusses the results of a non?expert and ex-pert crowdsourced sentence preference evaluationtask.
Section 4 concludes with several future ex-periments for system improvement.2 Architecture of GenNextIn general, NLG systems follow a prototypical ar-chitecture where some input data from a given do-main is sent to a ?document planner?
which de-cides content and structuring to create a documentplan.
That document plan serves as an input toa ?micro planner?
where the content is convertedinto a syntactic expression (with associated con-siderations of aggregation and referring expres-sion generation) and a text specification is created.The text specification then goes through the finalstage of ?surface realization?
where everything isput together into an output text (McKeown, 1985;Reiter and Dale, 2000; Bateman and Zock, 2003).In contrast, the architecture of GenNext (sum-marized in Figure 1) is driven by a domain-specific178Figure 1: GenNext System Architecture.corpus text.
There is often a structured databaseunderlying the domains of corpus text, the fieldsof which are used for domain specific entity tag-ging (in addition to domain general entity tagging[e.g.
DATE, LOCATION, etc.]).
An overview ofthe different stages, which are a combination ofstatistical (e.g., Langkilde and Knight (1998)) andtemplate?based (e.g., van Deemter, et al(2005))approaches, follows in (A-E).1A: Semantic Representation - We take a do-main specific training corpus and reduce eachsentence to a Discourse Representation Structure(DRS) - formal semantic representations of sen-tences (and texts) from Discourse RepresentationTheory (Kamp and Reyle, 1993; Basile and Bos,2011).
Each DRS is a combination of domain gen-eral named entities, predicates (content words) andrelational elements (function words).
In parallel,domain specific named entity tags are identifiedand are used to create templates that syntacticallyrepresent some conceptual meaning; for example,the short biography in (1):(1) Sentencea.
Mr. Mitsutaka Kambe has been serving as ManagingDirector of the 77 Bank, Ltd. since June 27, 2008.b.
He holds a Bachelor?s in finance from USC and a MBAfrom UCLA.Conceptual Meaningc.
SERVING | MANAGING | DIRECTOR | PERSON | ...d. HOLDS | BACHELOR | FINANCE | MBA | HOLD | ...Once the semantic representations are created,they are organized and identified by semantic con-cept (?CuId?)
(described in (B)).
Our assumptionis that each cluster equates with a CuId repre-sented by each individual sentence in the clusterand is contrastive with other CuIds (for similar ap-1For more detail see Howald, et al(2013) - semanticclustering and micro-planning and Kondadadi, et al(2013) -document planning.proaches, see Barzilay and Lapata (2005), Angeli,et al(2010) and Lu and Ng (2011)).B: Creating Conceptual Units - To create theCuIds (a semi-automatic process), we cluster thesentences using k-means clustering with k set ar-bitrarily high to over-generate (Witten and Frank,2005).
This facilitates manual verification of thegenerated clusters to merge (rather than split) themif necessary.
We assign a unique CuId to eachcluster and associate each template in the corpus toa corresponding CuId.
For example, in (2), usingthe sentences in (1a-b), the identified named en-tities are assigned to a clustered CuId (2a-b) andthen each sentence in the training corpus is re-duced to a template (2c-d).
(2) Content Mappinga.
{CuId : 000} ?
Information: person: Mr. MitsutakaKambe; title: Managing Director; company: 77 Bank,Ltd.
; date: June 27, 2008b.
{CuId : 001} ?
Information: person: he; degree:Bachelor?s, MBA; subject: finance; institution: USC;UCLATemplatesc.
{CuId : 000}: [person] has been serving as [title] of the[company] since [date].d.
{CuId : 001}: [person] holds a [degree] in [subject]from [institution] and a [degree] from [institution].At this stage, we will have a set of CuIds with cor-responding template collections which representthe entire ?micro-planning?
aspect of our system.C: Collecting Statistics - For the ?document plan-ning?
stage, we collect a number of statistics foreach domain, for example:?
Frequency distribution of CuIds by position?
Frequency distribution of templates by position?
Frequency distribution of entity sequence?
Average number of entities by CuId and positionThese statistics, in addition to entity tags and tem-plates, are used in building different features usedby the ranking model (D).D: Building a Ranking Model - The core compo-nent of our system is a statistical model that ranksa set of templates for a given position (e.g.
sen-tence 1, sentence 2, ..., sentence n) based on theinput data (see also Konstas and Lapata (2012).The learning task is to find the rank for all the tem-plates from all CuIds at each position.
To gener-ate the training data, we first exclude the templatesthat have named entities not specified in the inputdata (ensuring completeness).
We then rank tem-plates according to the edit distance (Levenshtein,1791966) from the template corresponding to the cur-rent sentence in the training document.
For eachtemplate, we build a ranking model with features,for example:?
Prior template and CuId?
Difference in number of words given position?
Most likely CuId given position and previous CuId?
Template 1-3grams given position and CuIdWe use a linear kernel for a ranking SVM(Joachims, 2002) to learn the weights associatedwith each feature.
Each domain has its own modelthat is used when generating texts (E).E: Generation: At generation time, our systemhas a set of input data, a semantically organizedtemplate bank and a model from training on agiven domain of texts.
For each sentence, we firstexclude those templates that contain a named en-tity not present in the input data.
Then we cal-culate the feature values times the model weightfor each of the remaining templates.
The tem-plate with the highest score is selected, filledwith matching entities from the input data and ap-pended to the generated text.
Example generationsfor each domain are included in (3).
(3) Financiala.
First quarter profit per share for Brown-FormanCorporation expected to be $0.91 per share by analysts.b.
Brown-Forman Corporation July first quarter profits willbe below that previously estimated by Wall Street witha range between $0.89 and $0.93 per share and a projectedmean per share of $0.91 per share.c.
The consensus recommendation is Hold.Biographyd.
Mr. Satomi Mitsuzaki has been serving as ManagingDirector of Mizuho Bank since June 27, 2008.e.
He was previously Director of Regional Compliance ofKyoto Branch.f.
He is a former Managing Executive Officer and ChiefExecutive Officer of new Industrial Finance BusinessGroup in Mitsubishi Corporation.Weatherg.
Complex low from southern Norway will drift slowly NNEto the Lofoten Islands by early tomorrow.h.
A ridge will persist to the west of British Isles for Saturdaywith a series of weak fronts moving east acrossthe North Sea.i.
A front will move ENE across the northern North SeaSaturday.3 Evaluation and DiscussionWe have tested GenNext on three domains: Corpo-rate Officer and Director Biographies (1150 textsranging from 3-10 period ended sentences), Fi-nancial Texts (Mutual Fund Performances [162texts, 2-4 sentences] and Broker Recommenda-tions [905 texts, 8-20 sentences]), and OffshoreOil Rig Weather Reports (1054 texts, 2-6 sen-tences) from SUMTIME-METEO (Reiter et al2005).
The total number of templates for the finan-cial domain is 1379 distributed across 38 differentsemantic concepts; 2836 templates across 19 con-cepts for biography; and 2749 templates across 9concepts for weather texts.We have conducted several evaluation experi-ments comparing two versions of GenNext, oneapplying the ranking model (rank) and one withrandom selection of templates (non-rank) (bothsystems use the same template bank, CuId as-signment and filtering) and the original texts fromwhich the data was extracted (original).We used a combination of automatic (e.g.BLEU?4 (Papineni et al 2002), METEOR(Denkowski and Lavie, 2011)) and human metrics(using crowdsourcing) to evaluate the output (seegenerally, Belz and Reiter (2006).
However, in theinterest of space, we will restrict the discussion toa human judgment task on output preferences.
Wefound this evaluation task to be most informativefor system improvement.
The task asks an evalu-ator to provide a binary preference determination(100 sentence pairs/domain): ?Do you prefer Sen-tence A (from original) or the corresponding Sen-tence B (from rank or non-rank)?.
This task wasperformed for each domain.2 We also engaged 3experts from the financial and 4 from the biogra-phy domains to perform the same preference task(average agreement was 76.22) as well as providetargeted feedback.For the preference results, summarized in Fig-ure 2, we would like to see no statistically signifi-cant difference between GenNext-rank and orig-inal, but statistically significant differences be-tween GenNext-rank and GenNext-non-rank, andoriginal and GenNext-non-rank.
If this is the case,then GenNext-rank is producing texts similar tothe original texts, and is providing an observ-able improvement over not including the model atall (GenNext-non-rank).
This is exactly what wesee for all domains.3 However, in general, there2Over 100 native English speakers contributed, each onerestricted to providing no more than 50 responses and onlyafter they successfully answered 4 initial gold data questionscorrectly and continued to answer periodic gold data ques-tions.
The pair orderings were randomized to prevent clickbias.
8 judgments per sentence pair was collected (2400 judg-ments) and average agreement was 75.87.3Original vs. GenNext-rank : financial - ?2=.29, p?.59;biography - ?2=3.01, p?.047; weather - ?2=.95, p?.32.Original vs. GenNext-non-rank : financial - ?2=16.71,p?.0001; biography - ?2=45.43, p?.0001; weather -180Figure 2: Cross-Domain Non-Expert Preference Evaluations.is a greater difference between the original andGenNext-rank biographies compared to the finan-cial and weather texts.
We take it as a goal to ap-proach, as close as possible, the preferences forthe original texts.The original financial documents were machinegenerated from a different existing system.
Assuch, it is not surprising to see similarity in perfor-mance compared to GenNext-rank and potentiallyexplains why preferences for the originals is some-what low (assuming a higher preference rating forwell-formed human texts).
Further, the originalweather documents are highly technical and noteasily understood by the lay person, so, again, it isnot surprising to see similar performance.
Biogra-phies were human generated and easy to under-stand for the average reader.
Here, both GenNext-rank and GenNext-non-rank have some ground tomake up.
Insights from domain experts are poten-tially helpful in this regard.Expert evaluations provided similar results andagreements compared to the non?expert crowd.Most beneficial about the expert evaluations wasthe discussion of integrating certain editorial stan-dards into the system.
For example, shorter textswere preferred to longer texts in the financial do-main, but not the biographies.
Consequently, wecould adjust weights to favor shorter templates.Also, in biographies, sentences with subordinatedelaborations were not preferred because these con-tained subjective comments (e.g.
a leader in in-dustry, a well respected individual, etc.).
Here,?2=24.27, p?.0001.
GenNext-rank vs. GenNext-non-rank: financial - ?2=12.81, p?.0003; biography - ?2=25.19,p?.0001; weather - ?2=16.19, p?.0001.we could manually curate or could automaticallydetect templates with subordinated clauses and re-move them.
These types of comments are usefulto adjust the system accordingly to end user ex-pectations.4 Conclusion and Future WorkWe have presented our system GenNext which isdomain adaptable, given adequate historical data,and has a significantly reduced complexity com-pared to other NLG systems (see generally, Robinand McKeown (1996)).
To the latter point, devel-opment time for semantically processing the cor-pus, applying domain general and specific tags,and building a model is accomplished in days andweeks as opposed to months and years.Future experimentation will focus on being ableto automatically extract templates for different do-mains to create preset banks of templates in theabsence of adequate historical data.
We are alsolooking into different ways to increase the vari-ability of output texts from selecting templateswithin a range of top scores (rather than just thehighest score) to providing additional generatedinformation from input data analytics.AcknowledgmentsThis research is made possible by ThomsonReuters Global Resources (TRGR) with particu-lar thanks to Peter Pircher, Jaclyn Sprtel and BenHachey for significant support.
Thank you alsoto Khalid Al-Kofahi for encouragement, LeszekMichalak and Andrew Lipstein for expert evalua-tions and three anonymous reviewers for construc-tive feedback.181ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2012.
Asimple domain-independent probabilistic approachto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods for Natural LanguageProcessing (EMNLP 2010), pages 502?512.Regina Barzilay and Mirella Lapata.
2005.
Collectivecontent selection for concept-to-text generation.
InProceedings of the 2005 Conference on EmpiricalMethods for Natural Language Processing (EMNLP2005), pages 331?338.Valerio Basile and Johan Bos.
2011.
Towards generat-ing text from discourse representation structures.
InProceedings of the 13th European Workshop on Nat-ural Language Generation (ENLG), pages 145?150.John Bateman and Michael Zock.
2003.
Naturallanguage generation.
In R. Mitkov, editor, OxfordHandbook of Computational Linguistics, Researchin Computational Semantics, pages 284?304.
Ox-ford University Press, Oxford.Anja Belz and Ehud Reiter.
2006.
Comparing au-tomatic and human evaluation of NLG systems.
InProceedings of the European Association for Com-putational Linguistics (EACL?06), pages 313?320.Anja Belz.
2007.
Probabilistic generation of weatherforecast texts.
In Proceedings of Human LanguageTechnologies 2007: The Annual Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL-HLT?07), pages164?171.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the EMNLP 2011 Workshop on Statisti-cal Machine Translation, pages 85?91.Blake Howald, Ravi Kondadadi, and Frank Schilder.2013.
Domain adaptable semantic clustering in sta-tistical NLG.
In Proceedings of the 10th Inter-national Conference on Computational Semantics(IWCS 2013), pages 143?154.
Association for Com-putational Linguistics, March.Thorsten Joachims.
2002.
Learning to Classify TextUsing Support Vector Machines.
Kluwer.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic; An Introduction to Modeltheoretic Seman-tics of Natural Language, Formal Logic and DRT.Kluwer, Dordrecht.Ravi Kondadadi, Blake Howald, and Frank Schilder.2013.
A statistical NLG framework for aggregatedplanning and realization.
In Proceedings of the An-nual Conference for the Association of Computa-tional Linguistics (ACL 2013).
Association for Com-putational Linguistics.Ioannis Konstas and Mirella Lapata.
2012.
Concept-to-text generation via discriminative reranking.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 369?378.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics (ACL?98),pages 704?710.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
So-viet Physics Doklady, 10:707?710.Wei Lu and Hwee Tou Ng.
2011.
A probabilisticforest-to-string model for language generation fromtyped lambda calculus expressions.
In Proceed-ings of the 2011 Conference on Empirical Methodsfor Natural Language Processing (EMNLP 2011),pages 1611?1622.Kathleen R. McKeown.
1985.
Text Generation: UsingDiscourse Strategies and Focus Constraints to Gen-erate Natural Language Text.
Cambridge UniversityPress.Kishore Papineni, Slim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL?02), pages 311?318.Franois Portet, Ehud Reiter, Jim Hunter, and Somaya-julu Sripada.
2007.
Automatic generation of tex-tual summaries from neonatal intensive care data.
InIn Proccedings of the 11th Conference on ArtificialIntelligence in Medicine (AIME 07).
LNCS, pages227?236.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.Ehud Reiter, Somayajulu Sripada, Jim Hunter, and JinYu.
2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.Jacques Robin and Kathy McKeown.
1996.
Empiri-cally designing and evaluating a new revision-basedmodel for summary generation.
Artificial Intelli-gence, 85(1-2).Kees van Deemter, Marie?t Theune, and Emiel Krahmer.2005.
Real vs. template-based natural language gen-eration: a false opposition?
Computational Linguis-tics, 31(1):15?24.Ian Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Techniques with Java Imple-mentation (2nd Ed.).
Morgan Kaufmann, San Fran-cisco, CA.182
