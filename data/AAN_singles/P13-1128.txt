Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304?1311,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEntity Linking for TweetsXiaohua Liu?, Yitong Li?, Haocheng Wu?, Ming Zhou?, Furu Wei?, Yi Lu?
?Microsoft Research Asia, Beijing, 100190, China?School of Electronic and Information EngineeringBeihang University, Beijing, 100191, China?University of Science and Technology of ChinaNo.
96, Jinzhai Road, Hefei, Anhui, China?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, 150001, China?
{xiaoliu, mingzhou, fuwei}@microsoft.com?tong91222@126.com ?v-haowu@microsoft.com ?v-y@microsoft.comAbstractWe study the task of entity linking fortweets, which tries to associate eachmention in a tweet with a knowledge baseentry.
Two main challenges of this task arethe dearth of information in a single tweetand the rich entity mention variations.To address these challenges, we proposea collective inference method thatsimultaneously resolves a set of mentions.Particularly, our model integrates threekinds of similarities, i.e., mention-entrysimilarity, entry-entry similarity, andmention-mention similarity, to enrichthe context for entity linking, and toaddress irregular mentions that are notcovered by the entity-variation dictionary.We evaluate our method on a publiclyavailable data set and demonstrate theeffectiveness of our method.1 IntroductionTwitter is a widely used social networking service.With millions of active users and hundreds ofmillions of new published tweets every day1,it has become a popular platform to captureand transmit the human experiences of themoment.
Many tweet related researches areinspired, from named entity recognition (Liu et al,2012), topic detection (Mathioudakis and Koudas,2010), clustering (Rosa et al, 2010), to eventextraction (Grinev et al, 2009).In this work, we study the entity linking taskfor tweets, which maps each entity mention ina tweet to a unique entity, i.e., an entry IDof a knowledge base like Wikipedia.
Entity1http://siteanalytics.compete.com/twitter.com/linking task is generally considered as a bridgebetween unstructured text and structured machine-readable knowledge base, and represents a criticalrole in machine reading program (Singh et al,2011).
Entity linking for tweets is particularlymeaningful, considering that tweets are often hardto read owing to its informal written style andlength limitation of 140 characters.Current entity linking methods are built on topof a large scale knowledge base such asWikipedia.A knowledge base consists of a set of entities,and each entity can have a variation list2.
Todecide which entity should be mapped, they maycompute: 1) the similarity between the context ofa mention, e.g., a text window around the mention,and the content of an entity, e.g., the entity page ofWikipedia (Mihalcea and Csomai, 2007; Han andZhao, 2009); 2) the coherence among the mappedentities for a set of related mentions, e.g, multiplementions in a document (Milne and Witten, 2008;Kulkarni et al, 2009; Han and Zhao, 2010; Han etal., 2011).Tweets pose special challenges to entity linking.First, a tweet is often too concise and toonoisy to provide enough information for similaritycomputing, owing to its short and grass rootnature.
Second, tweets have rich variations ofnamed entities3, and many of them fall out ofthe scope of the existing dictionaries mined fromWikipedia (called OOV mentions hereafter).
On2Entity variation lists can be extracted from theentity resolution pages of Wikipedia.
For example, thelink ?http://en.wikipedia.org/wiki/Svm?
will lead us to aresolution page, where ?Svm?
are linked to entities like?Space vector modulation?
and ?Support vector machine?.As a result, ?Svm?
will be added into the variation lists of?Space vector modulation?
and ?Support vector machine?
,respectively.3According to Liu et al (2012), on average a named entityhas 3.3 different surface forms in tweets.1304the other hand, the huge redundancy in tweetsoffers opportunities.
That means, an entitymention often occurs in many tweets, whichallows us to aggregate all related tweets tocompute mention-mention similarity and mention-entity similarity.We propose a collective inference methodthat leverages tweet redundancy to address thosetwo challenges.
Given a set of mentions, ourmodel tries to ensure that similar mentions arelinked to similar entities while pursuing thehigh total similarity between matched mention-entity pairs.
More specifically, we definelocal features, including context similarity andedit distance, to model the similarity betweena mention and an entity.
We adopt in-linkbased similarity (Milne and Witten, 2008), tomeasure the similarity between entities.
Finally,we introduce a set of features to computethe similarity between mentions, including howsimilar the tweets containing the mentions are,whether they come from the tweets of the sameaccount, and their edit distance.
Notably, ourmodel can resolve OOV mentions with the helpof their similar mentions.
For example, for theOOVmention ?LukeBryanOnline?, our model canfind similar mentions like ?TheLukeBryan?
and?LukeBryan?.
Considering that most of its similarmentions are mapped to the American countrysinger ?Luke Bryan?, our model tends to link?LukeBryanOnline?
to the same entity.We evaluate our method on the public availabledata set shared by Meij et al (2012)4.Experimental results show that our methodoutperforms two baselines, i.e., Wikify!
(Mihalceaand Csomai, 2007) and system proposed by Meijet al (2012).
We also study the effectivenessof features related to each kind of similarity, anddemonstrate the advantage of our method for OOVmention linkage.We summarize our contributions as follows.1.
We introduce a novel collective inferencemethod that integrates three kinds ofsimilarities, i.e., mention-entity similarity,entity-entity similarity, and mention-mentionsimilarity, to simultaneously map a set oftweet mentions to their proper entities.2.
We propose modeling the mention-mentionsimilarity and demonstrate its effectiveness4http://ilps.science.uva.nl/resources/wsdm2012-adding-semantics-to-microblog-posts/in entity linking for tweets, particularly forOOV mentions.3.
We evaluate our method on a public dataset, and show our method compares favorablywith the baselines.Our paper is organized as follows.
In the nextsection, we introduce related work.
In Section3, we give the formal definition of the task.
InSection 4, we present our solution, includingthe framework, features related to different kindsof similarities, and the training and decodingprocedures.
We evaluate our method in Section 5.Finally in Section 6, we conclude with suggestionsof future work.2 Related WorkExisting entity linking work can roughly bedivided into two categories.
Methods of thefirst category resolve one mention at each time,and mainly consider the similarity betweena mention-entity pair.
In contrast, methodsof the second category take a set of relatedmentions (e.g., mentions in the same document)as input, and figure out their corresponding entitiessimultaneously.Examples of the first category include the firstWeb-scale entity linking system SemTag (Dillet al, 2003), Wikify!
(Mihalcea and Csomai,2007), and the recent work of Milne and Witten(2008).
SemTag uses the TAP knowledgebase5, and employs the cosine similarity withTF-IDF weighting scheme to compute thematch degree between a mention and an entity,achieving an accuracy of around 82%.
Wikify!identifies the important concepts in the textand automatically links these concepts to thecorresponding Wikipedia pages.
It introduces twoapproaches to define mention-entity similarity,i.e., the contextual overlap between the paragraphwhere the mention occurs and the correspondingWikipedia pages, and a Naive Bayes classifierthat predicts whether a mention should be linkedto an entity.
It achieves 80.69% F1 when twoapproaches are combined.
Milne and Wittenwork on the same task of Wikify!, and alsotrain a classifier.
However, they cleverly use the5TAB (http://www.w3.org/2002/05/tap/) is a shallowknowledge base that contains a broad range of lexical andtaxonomic information about popular objects like music,movies, authors, sports, autos, health, etc.1305links found within Wikipedia articles for training,exploiting the fact that for every link, aWikipedianhas manually selected the correct destination torepresent the intended sense of the anchor.
Theirmethod achieves an F1 score of 75.0%.Representative studies of the second categoryinclude the work of Kulkarni et al (2009),Han et al (2011), and Shen et al (2012).One common feature of these studies is thatthey leverage the global coherence betweenentities.
Kulkarni et al (2009) proposea graphical model that explicitly models thecombination of evidence from local mention-entity compatibility and global document-leveltopical coherence of the entities, and show thatconsidering global coherence between entitiessignificantly improves the performance.
Han etal.
(2011) introduce a graph-based representation,called Referent Graph, to model the globalinterdependence between different entity linkingdecisions, and jointly infer the referent entities ofall name mentions in a document by exploitingthe interdependence captured in Referent Graph.Shen et al (2012) propose LIEGE, a frameworkto link the entities in web lists with the knowledgebase, with the assumption that entities mentionedin a Web list tend to be a collection of entities ofthe same conceptual type.Most work of entity linking focuses on webpages.
Recently, Meij et al (2012) studythis task for tweets.
They propose a machinelearning based approach using n-gram features,concept features, and tweet features, to identifyconcepts semantically related to a tweet, andfor every entity mention to generate links to itscorresponding Wikipedia article.
Their methodbelongs to the first category, in the sense thatthey only consider the similarity between mention(tweet) and entity (Wikipedia article).Our method belongs to the second category.However, in contrast with existing collectiveapproaches, our method works on tweets whichare short and often noisy.
Furthermore, ourmethod is based on the ?similar mention withsimilar entity?
assumption, and explicitly modelsand integrates the mention similarity into theoptimization framework.
Compared with Meij etal.
(2012), our method is collective, and integratesmore features.3 Task DefinitionGiven a sequence of mentions, denoted byM?
= (m1,m2, ?
?
?
,mn), our task is tooutput a sequence of entities, denoted byE?
= (e1, e2, ?
?
?
, en), where ei is the entitycorresponding to mi.
Here, an entity refersto an item of a knowledge base.
Followingmost existing work, we use Wikipedia as theknowledge base, and an entity is a definition pagein Wikipedia; a mention denotes a sequence oftokens in a tweet that can be potentially linked toan entity.Several notes should be made.
First, weassume that mentions are given, e.g., identified bysome named entity recognition system.
Second,mentions may come from multiple tweets.
Third,mentions with the same token sequence mayrefer to different entities, depending on mentioncontext.
Finally, we assume each entity e hasa variation list6, and a unique ID through whichall related information about that entity can beaccessed.Here is an example to illustrate the task.
Givenmentions ?nbcbightlynews?, ?Santiago?, ?WH?and ?Libya?
from the following tweet ?ChuckTodd: Prepping for @nbcnightlynews here inSantiago, reporting on WH handling of Libyasituation.
?, the expected output is ?NBC NightlyNews(194735)?, ?Santiago Chile(51572)?,?White House(33057)?
and ?Libya(17633)?,where the numbers in the parentheses are the IDsof the corresponding entities.4 Our MethodIn this section, we first present the framework ofour entity linking method.
Then we introducefeatures related to different kinds of similarities,followed by a detailed discussion of the trainingand decoding procedures.4.1 FrameworkGiven the input mention sequence M?
=(m1,m2, ?
?
?
,mn), our method outputs the entitysequence E??
= (e?1, e?2, ?
?
?
, e?n) according toFormula 1:6For example, the variation list of the entity ?Obama?
maycontain ?Barack Obama?, ?Barack Hussein Obama II?, etc.1306E??
= argmax?E??C(M?)?n?i=1w?
?
f?
(ei,mi)+(1 ?
?
)?i ?=jr(ei, ej)s(mi,mj)(1)Where:?
C(M?)
is the set of all possible entitysequences for the mention sequence M?
;?
E?
denotes an entity sequence instance,consisting of e1, e2, ?
?
?
, en;?
f?
(ei,mi) is the feature vector that models thesimilarity between mention mi and its linkedentity ei;?
w?
is the feature weight vector related to f?
,which is trained on the training data set; w?
?f?
(ei,mi) is the similarity between mentionmi and entity ei;?
r(ei, ej) is the function that returns thesimilarity between two entities ei and ej ;?
s(mi,mj) is the function that returns thesimilarity between two mentions mi and mj ;?
?
?
(0, 1) is a systematic parameter, whichis determined on the development data set; itis used to adjust the tradeoff between localcompatibility and global consistence.
It isexperimentally set to 0.8 in our work.From Formula 1, we can see that: 1) ourmethod considers the mention-entity similarly,entity-entity similarity and mention-mentionsimilarity.
Mention-entity similarly is used tomodel local compatibility, while entity-entitysimilarity and mention-mention similaritycombined are to model global consistence; and 2)our method prefers configurations where similarmentions have similar entities and with high localcompatibility.C(M?)
is worth of more discussion here.It represents the search space, which can begenerated using the entity variation list.
Toachieve this, we first build an inverted indexof all entity variation lists, with each uniquevariation as an entry pointing to a list of entities.Then for any mention m, we look up the index,and get al possible entities, denoted by C(m).In this way, given a mention sequence M?
=(m1,m2, ?
?
?
,mn), we can enumerate all possibleentity sequence E?
= (e1, e2, ?
?
?
, en), where ei ?C(m).
This means |C(M?
)| = ?m?M |C(m)| ,which is often large.
There is one special case:if m is an OOV mention, i.e., |C(m)| = 0, then|C(M?
)| = 0, and we get no solution.
To addressthis problem, we can generate a list of candidatesfor an OOV mention using its similar mentions.Let S(m) denote OOV mention m?s similarmentions, we define C(m) = ?m?
?S(m) C(m?
).If still C(m) = 0, we remove m from M?
, andreport we cannot map it to any entity.Here is an example to illustrate our framework.Suppose we have the following tweets:?
UserA: Yeaaahhgg #habemusfut..I love monday night futbol =)#EnglishPremierLeague ManU vsLiverpool1?
UserA: Manchester United 3 - Liverpool22 #EnglishPremierLeague GLORY, GLORY,MAN.UNITED!?
?
?
?Figure 1: An illustrative example to show ourframework.
Ovals in orange and in blue representmentions and entities, respectively.
Each mentionpair, entity pair, and mention entity pair havea similarity score represented by s, r and f ,respectively.We need find out the best entity sequenceE??
for mentions M?
= { ?Liverpool1?,?Manchester United?, ?ManU?, ?Liverpool2?
},from the entity sequences C(M?)
= { (Liverpool(film), Manchester United F.C., ManchesterUnited F.C., Liverpool (film)), ?
?
?
, (Liverpool,F.C.,Manchester United, F.C., Manchester UnitedF.C., Liverpool (film) }.
Figure 1 illustrateour solution, where ?Liverpool1?
(on the left)and ?Liverpool2?
(on the right) are linked1307to ?Liverpool F.C.?
(the football club), and?Manchester United?
and ?ManU?
are linked to?Manchester United F.C.?.
Notably, ?ManU?is an OOV mention, but has a similar mention?Manchester United?, with which ?ManU?
issuccessfully mapped.4.2 FeaturesWe group features into three categories: localfeatures related to mention-entity similarity(f?
(e,m)), features related to entity-entitysimilarity (r(ei, ej)) , and features related tomention-mention similarity (s(mi,mj)).4.2.1 Local Features?
Prior Probability:f1(mi, ei) =count(ei)?
?ek?C(mi) count(ek)(2)where count(e) denotes the frequency ofentity e in Wikipedia?s anchor texts.?
Context Similarity:f2(mi, ei) =coocurence numbertweet length (3)where: coccurence number is the thenumber of the words that occur in both thetweet containing mi and the Wikipedia pageof ei; tweet length denotes the number oftokens of the tweet containing mention mi.?
Edit Distance Similarity:IfLength(mi)+ED(mi, ei) = Length(ei),f3(mi, ei) = 1, otherwise 0.
ED(?, ?
)computes the character level edit distance.This feature helps to detect whethera mention is an abbreviation of itscorresponding entity7.?
Mention Contains Title: If the mentioncontains the entity title, namely the title ofthe Wikipedia page introducing the entity ei,f4(mi, ei) = 1, else 0.?
Title Contains Mention: If the entry titlecontains the mention, f5(mi, ei) = 1,otherwise 0.7Take ?ms?
and ?Microsoft?
for example.
The length of?ms?
is 2, and the edit distance between them is 7.
2 plus 7equals to 9, which is the length of ?Microsoft?.4.2.2 Features Related to Entity SimilarityThere are two representative definitions of entitysimilarity: in-link based similarity (Milne andWitten, 2008) and category based similarity (Shenet al, 2012).
Considering that the Wikipediacategories are often noisy (Milne and Witten,2008), we adopt in-link based similarity, asdefined in Formula 4:r(ei, ej) =log|g(ei) ?
g(ej)| ?
log max(|g(ei)|, |g(ej)|)log(Total)?
log min(|g(ei)|, |g(ej)|)(4)Where:?
Total is the total number of knowledge baseentities;?
g(e) is the number of Wikipedia definitionpages that have a link to entity e.4.2.3 Features Related to Mention SimilarityWe define 5 features to model the similaritybetween two mentions mi and mj , as listedbelow, where t(m) denotes the tweet that containsmention m:?
s1(mi,mj): The cosine similarity of t(mi)and t(mj); and tweets are represented as TF-IDF vectors;?
s2(mi,mj): The cosine similarity of t(mi)and t(mj); and tweets are represented astopic distribution vectors;?
s3(mi,mj): Whether t(mi) and t(mj) arepublished by the same account;?
s4(mi,mj): Whether t(mi) and t(mj)contain any common hash tag;?
s5(mi,mj): Edit distance related similaritybetween mi and mj , as defined in Formula 5.s5(mi,mj) = 1, if min{Length(mi), Length(mj)}+ED(mi,mj) = max{Length(mi), Length(mj)},else s5(mi,mj) = 1 ?
ED(mi,mj)max{Length(mi), Length(mj)}(5)Note that: 1) before computing TF-IDF vectors,stop words are removed; 2) we use the StanfordTopic Modeling Toolbox8 to compute the topicmodel, and experimentally set the number oftopics to 50.8http://nlp.stanford.edu/software/tmt/tmt-0.4/1308Finally, Formula 6 is used to integrate all thefeatures.
a?
= (a1, a2, a3, a4, a5) is the featureweight vector for mention similarity, where ak ?
(0, 1), k = 1, 2, 3, 4, 5, and?5k=1 ak = 1.s(mi,mj) =5?k=1aksk(mi,mj) (6)4.3 Training and DecodingGiven n mentions m1,m2, ?
?
?
,mn and theircorresponding entities e1, e2, ?
?
?
, en, the goal oftraining is to determine: w?
?, the weights of localfeatures, and a?
?, the weights of the features relatedto mention similarity, according to Formula 7 9.(w?
?, a??)
= arg minw?,?a{1nn?i=1L1(ei,mi)+?1||w?||2 +?22n?i,j=1s(mi,mj)L2(?a, ei, ej)}(7)Where:?
L1 is the loss function related to localcompatibility, which is defined as1w??f?(ei,mi)+1;?
L2(?a, ei, ej) is the loss function relatedto global coherence, which is defined as1r(ei,ej)?5k=1 aksk(mi,mj)+1;?
?1 is the weight of regularization, which isexperimentally set to 1.0;?
?2 is the weight of L2 loss, which isexperimentally set to 0.2.Since the decoding problem defined byFormula 1 is NP hard (Kulkarni et al, 2009), wedevelop a greedy hill-climbing approach to tacklethis challenge, as demonstrated in Algorithm 1.In Algorithm 1, it is the number of iterations;Score(E?, M?)
= ?
?ni=1 w?
?
f?
(ei,mi) + (1 ??
)?i?=j r(ei, ej)s(mi,mj); E?ij is the vector afterreplacing ei with ej ?
C(mi) for current E?
;scij is the score of E?ij , i.e., Score(E?ij , M?).
Ineach iteration, this rounding solution iterativelysubstitute entry ei in E?
to increase the total scorecur.
If the score cannot be further improved, itstops and returns current E?.9This optimization problem is non-convex.
We usecoordinate descent to get a local optimal solution.Algorithm 1 Decoding Algorithm.Input: Mention Set M?
= (m1,m2, ?
?
?
,mn)Output: Entity Set E?
= (e1, e2, ?
?
?
, en)1: for i = 1 to n do2: Initialize e(0)i as the entity with the largest priorprobability given mention mi.3: end for4: cur = Score(E?
(0), M?
)5: it = 16: while true do7: for i = 1 to n do8: for ej ?
C(mi) do9: if ej ?= e(it?1)i then10: E?
(it)ij = E?
(it?1) ?
{e(it?1)i } + {ej}.11: end if12: scij = Score(E?
(it)ij , M?
).13: end for14: end for15: (l,m) = argmax(i,j)scij .16: sc?
= sclm17: if sc?
> cur then18: cur = sc?.19: E?
(it) = E?
(it?1) ?
{e(it?1)l } + {em}.20: it = it + 1.21: else22: break23: end if24: end while25: return E?
(it).5 ExperimentsIn this section, we introduce the data set andexperimental settings, and present results.5.1 Data PreparationFollowing most existing studies, we chooseWikipedia as our knowledge base10.
We indexthe Wikipedia definition pages, and prepare allrequired prior knowledge, such as count(e), g(e),and entity variation lists.
We also build an invertedindex with about 60 million entries for the entityvariation lists.For tweets, we use the data set shared by Meij etal.
(2012)11.
This data set is annotated manuallyby two volunteers.
We get 502 annotated tweetsfrom this data set.
We keep 55 of them for10We download the December 2012 version of Wikipedia,which contains about four million articles.11http://ilps.science.uva.nl/resources/wsdm2012-adding-semantics-to-microblog-posts/.1309development, and the remaining for 5 fold cross-validation.5.2 SettingsWe consider following settings to evaluate ourmethod.?
Comparing our method with two baselines,i.e., Wikify!
(Mihalcea and Csomai, 2007)and the system proposed byMeij et al (2012)12;?
Using only local features;?
Using various mention similarity features;?
Experiments on OOV mentions.5.3 ResultsTable 1 reports the comparison results.
Ourmethod outperforms both systems in terms ofall metrics.
Since the main difference betweenour method and the baselines is that our methodconsiders not only local features, but also globalfeatures related to entity similarity and mentionsimilarity, these results indicate the effectivenessof collective inference and global features.
Forexample, we find two baselines incorrectly link?Nickelodeon?
in the tweet ?BOH will make aspecial appearance on Nickelodeon?s ?Yo GabbaGabba?
tomorrow?
to the theater instead of a TVchannel.
In contrast, our method notices that ?YoGabba Gabba?
in the same tweet can be linkedto ?Yo Gabba Gabba (TV show)?, and thus itcorrectly maps ?Nickelodeon?
to ?Nickelodeon(TV channel)?.System Pre.
Rec.
F1Wikify!
0.375 0.421 0.396Meij?s Method 0.734 0.632 0.679Our Method 0.752 0.675 0.711Table 1: Comparison with Baselines.Table 2 shows the results when local featuresare incrementally added.
It can be seen that:1) using only Prior Probability feature alreadyyields a reasonable F1; and 2) Context Similarityand Edit Distance Similarity feature have littlecontribution to the F1, while Mention and EntityTitle Similarity feature greatly boosts the F1.12We re-implement Wikify!
since we use a new evaluationdata set.Local Feature Pre.
Rec.
F1P.P.
0.700 0.599 0.646+C.S.
0.694 0.597 0.642+E.D.S.
0.696 0.598 0.643+M.E.T.S.
0.735 0.632 0.680Table 2: Local Feature Analysis.
P.P.,C.S., E.D.S.,and M.E.T.S.
denote Prior Probability, ContextSimilarity, Edit Distance Similarity, and Mentionand Entity Title Similarity, respectively.The performance of our method with variousmention similarity features is reported in Table 3.First, we can see that with this kind of features,the F1 can be significantly improved from 0.680to 0.704.
Second, we notice that TF-IDF (s1) andTopic Model (s2) features perform equally well,and combining all mention similarity featuresyields the best performance.Global Feature Pre.
Rec.
F1s3+s4+s5 0.744 0.653 0.700s3+s4+s5 +s1 0.759 0.652 0.702s3+s4+s5+s2 0.760 0.653 0.703s3+s4+s5+s1+s2 0.764 0.653 0.704Table 3: Mention Similarity Feature Analysis.For any OOV mention, we use the strategyof guessing its possible entity candidates usingsimilar mentions, as discussed in Section 4.1.Table 4 shows the performance of our system forOOV mentions.
It can be seen that with ourOOV strategy, the recall is improved from 0.653to 0.675 (with p < 0.05) while the Precision isslightly dropped and the overall F1 still gets better.A further study reveals that among all the 125OOV mentions, there are 48 for which our methodcannot find any entity; and nearly half of these48 OOV mentions do have corresponding entities13.
This suggests that we may need enlarge thesize of variation lists or develop some mentionnormalization techniques.OOV Method Precision Recall F1Ignore OOV Mention 0.764 0.653 0.704+ OOV Method 0.752 0.675 0.711Table 4: Performance for OOV Mentions.13?NATO-ukraine cooperations?
is such an example.
Itis mapped to NULL but actually has a corresponding entity?Ukraine-NATO relations?13106 Conclusions and Future workWe have presented a collective inference methodthat jointly links a set of tweet mentions totheir corresponding entities.
One distinguishedcharacteristic of our method is that it integratesmention-entity similarity, entity-entity similarity,and mention-mention similarity, to address theinformation lack in a tweet and rich OOVmentions.
We evaluate our method on apublic data set.
Experimental results show ourmethod outperforms two baselines, and suggeststhe effectiveness of modeling mention-mentionsimilarity, particularly for OOV mention linking.In the future, we plan to explore two directions.First, we are going to enlarge the size of entityvariation lists.
Second, we want to integratethe entity mention normalization techniques asintroduced by Liu et al (2012).AcknowledgmentsWe thank the anonymous reviewers for theirvaluable comments.
We also thank all theQuickView team members for the helpfuldiscussions.ReferencesS.
Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha.2003.
Semtag and seeker: bootstrapping thesemantic web via automated semantic annotation.
InProceedings of the 12th international conference onWorld Wide Web, WWW ?03, pages 178?186, NewYork, NY, USA.
ACM.Maxim Grinev, Maria Grineva, Alexander Boldakov,Leonid Novak, Andrey Syssoev, and DmitryLizorkin.
2009.
Sifting micro-blogging stream forevents of user interest.
In Proceedings of the 32ndinternational ACM SIGIR conference on Researchand development in information retrieval, SIGIR?09, pages 837?837, New York, NY, USA.
ACM.Xianpei Han and Jun Zhao.
2009.
Nlpr-kbp in tac 2009kbp track: A two-stage method to entity linking.
InProceedings of Test Analysis Conference.Xianpei Han and Jun Zhao.
2010.
Structuralsemantic relatedness: a knowledge-based methodto named entity disambiguation.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collectiveentity linking in web text: A graph-based method.In SIGIR?11.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collectiveannotation of wikipedia entities in web text.In Proceedings of the 15th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 457?465.Xiaohua Liu, Ming Zhou, Xiangyang Zhou,Zhongyang Fu, and Furu Wei.
2012.
Joint inferenceof named entity recognition and normalization fortweets.
In ACL (1), pages 526?535.Michael Mathioudakis and Nick Koudas.
2010.Twittermonitor: trend detection over the twitterstream.
In Proceedings of the 2010 ACM SIGMODInternational Conference on Management of data,SIGMOD ?10, pages 1155?1158, New York, NY,USA.
ACM.Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.2012.
Adding semantics to microblog posts.In Proceedings of the fifth ACM internationalconference on Web search and data mining.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:linking documents to encyclopedic knowledge.In Proceedings of the sixteenth ACM conferenceon Conference on information and knowledgemanagement, CIKM ?07, pages 233?242, NewYork,NY, USA.
ACM.David Milne and Ian H. Witten.
2008.
Learningto link with wikipedia.
In Proceeding of the 17thACM conference on Information and knowledgemanagement.Kevin Dela Rosa, Rushin Shah, Bo Lin, AnatoleGershman, and Robert Frederking.
2010.
Topicalclustering of tweets.
In SWSM?10.Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.2012.
Liege: Link entities in web lists withknowledge base.
In KDD?12.Sameer Singh, Amarnag Subramanya, FernandoPereira, and Andrew McCallum.
2011.
Large-scale cross-document coreference using distributedinference and hierarchical models.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 793?803, Stroudsburg, PA, USA.
Association forComputational Linguistics.1311
