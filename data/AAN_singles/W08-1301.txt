Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1?8Manchester, August 2008The Stanford typed dependencies representationMarie-Catherine de MarneffeLinguistics DepartmentStanford UniversityStanford, CA 94305mcdm@stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305manning@stanford.eduAbstractThis paper examines the Stanford typeddependencies representation, which wasdesigned to provide a straightforward de-scription of grammatical relations for anyuser who could benefit from automatic textunderstanding.
For such purposes, we ar-gue that dependency schemes must followa simple design and provide semanticallycontentful information, as well as offer anautomatic procedure to extract the rela-tions.
We consider the underlying designprinciples of the Stanford scheme from thisperspective, and compare it to the GR andPARC representations.
Finally, we addressthe question of the suitability of the Stan-ford scheme for parser evaluation.1 IntroductionThe Stanford typed dependencies representationwas designed to provide a simple description ofthe grammatical relationships in a sentence thatcould easily be understood and effectively used bypeople without linguistic expertise who wanted toextract textual relations.
The representation wasnot designed for the purpose of parser evaluation.Nevertheless, we agree with the widespread senti-ment that dependency-based evaluation of parsersavoids many of the problems of the traditional Par-seval measures (Black et al, 1991), and to the ex-tent that the Stanford dependency representationis an effective representation for the tasks envi-sioned, it is perhaps closer to an appropriate task-based evaluation than some of the alternative de-pendency representations available.
In this paperc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.we examine the representation and its underlyingdesign principles, look at how this representationcompares with other dependency representationsin ways that reflect the design principles, and con-sider its suitability for parser evaluation.A major problem for the natural language pro-cessing (NLP) community is how to make thevery impressive and practical technology whichhas been developed over the last two decades ap-proachable to and usable by everyone who has textunderstanding needs.
That is, usable not only bycomputational linguists, but also by the computerscience community more generally and by all sortsof information professionals including biologists,medical researchers, political scientists, law firms,business and market analysts, etc.
Thinking aboutthis issue, we were struck by two facts.
First, wenoted how frequently WordNet (Fellbaum, 1998)gets used compared to other resources, such asFrameNet (Fillmore et al, 2003) or the Penn Tree-bank (Marcus et al, 1993).
We believe that muchof the explanation for this fact lies in the differ-ence of complexity of the representation used bythe resources.
It is easy for users not necessarilyversed in linguistics to see how to use and to getvalue from the straightforward structure of Word-Net.
Second, we noted the widespread use of Mini-Par (Lin, 1998) and the Link Parser (Sleator andTemperley, 1993).
This clearly shows that (i) it isvery easy for a non-linguist thinking in relation ex-traction terms to see how to make use of a depen-dency representation (whereas a phrase structurerepresentation seems much more foreign and for-bidding), and (ii) the availability of high quality,easy-to-use (and preferably free) tools is essentialfor driving broader use of NLP tools.11On the other hand, evaluation seems less important; to thebest of our knowledge there has never been a convincing andthorough evaluation of either MiniPar or the Link Grammar1This paper advocates for the Stanford typed de-pendencies representation (henceforth SD) being apromising vehicle for bringing the breakthroughsof the last 15 years of parsing research to this broadpotential user community.
The representation aimsto provide a simple, habitable design.
All infor-mation is represented as binary relations.
Thismaps straightforwardly on to common representa-tions of potential users, including the logic formsof Moldovan and Rus (Moldovan and Rus, 2001),2semantic web Resource Description Framework(RDF) triples (http://www.w3.org/RDF/), and graphrepresentations (with labeled edges and nodes).Unlike many linguistic formalisms, excessive de-tail is viewed as a defect: information that users donot understand or wish to process detracts from up-take and usability.
The user-centered design pro-cess saw the key goal as representing semanticallycontentful relations suitable for relation extractionand more general information extraction uses.
Thedesign supports this use by favoring relations be-tween content words, by maintaining semanticallyuseful closed class word information while ignor-ing linguistic decisions less relevant to users, andby not representing less used material about lin-guistic features such as tense and agreement.
TheSD scheme thus provides a semantic representa-tion simple and natural enough for people who arenot (computational) linguists but can benefit fromNLP tools.2 Design choices and their implications2.1 Design principlesThe style of the SD representation bears a strongintellectual debt to the framework of Lexical-Functional Grammar (Bresnan, 2001), and, moredirectly, it owes a debt to both the sets of gram-matical relations and the naming defined in tworepresentations that follow an LFG style: the GR(Carroll et al, 1999) and PARC (King et al, 2003)schemes.
These were used as a starting point fordeveloping the Stanford dependencies (de Marn-effe et al, 2006).
But where the SD scheme devi-ates from GR, PARC, and its LFG roots is that ithas been designed to be a practical model of sen-tence representation, particularly in the context ofrelation extraction tasks.parser.2The logic forms of Moldovan and Rus are in the formof a predicate calculus representation, although not one thatrepresents such things as operator scope in a way that mostwould expect of a predicate calculus representation.SD makes available two options, suited to dif-ferent use cases: in one, every word of the origi-nal sentence is present as a node with relations be-tween it and other nodes, whereas in the latter, cer-tain words are ?collapsed?
out of the representa-tion, making such changes as turning prepositionsinto relations.
The former is useful when a closeparallelism to the source text words must be main-tained, whereas the latter is intended to be moreuseful for relation extraction and shallow languageunderstanding tasks.
Here, we discuss only the lat-ter representation; see (de Marneffe et al, 2006)for a discussion of both options and the precise re-lationship between them.The intended use cases of usability by peoplewho are not (computational) linguists and suitabil-ity for relation extraction applications led SD to tryto adhere to the following design principles (DPs):1.
Everything is represented uniformly as somebinary relation between two sentence words.2.
Relations should be semantically contentfuland useful to applications.3.
Where possible, relations should use notionsof traditional grammar for easier comprehen-sion by users.4.
Underspecified relations should be availableto deal with the complexities of real text.5.
Where possible, relations should be betweencontent words, not indirectly mediated viafunction words.6.
The representation should be spartan ratherthan overwhelming with linguistic details.We illustrate many of them in the rest of this sec-tion, using example sentences which were madeavailable for the Parser Evaluation Shared Task.The grammatical relations of SD are arranged ina hierarchy, rooted with the most generic relation,dependent.
The hierarchy contains 56 grammaticalrelations.
When the relation between a head andits dependent can be identified more precisely, re-lations further down in the hierarchy are used, butwhen it is unclear, more generic dependencies arepossible (DP1, DP4).
For example, the dependentrelation can be specialized to aux (auxiliary), arg(argument), or mod (modifier).
The arg relation isfurther divided into the subj (subject) relation andthe comp (complement) relation, and so on.
Thebackbone of this hierarchy is quite similar to thatin GR, but there are some crucial differences.22.2 Comparison with GR and PARCThe SD scheme is not concerned with the argu-ment/adjunct distinction which is largely useless inpractice.
In contrast, NP-internal relations are aninherent part of corpus texts and are critical in real-world applications.
The SD scheme therefore in-cludes many relations of this kind: appos (apposi-tive modifier), nn (noun compound), num (numericmodifier), number (element of compound num-ber) and abbrev (abbreviation), etc.
(DP2).
Forinstance, in the sentence ?I feel like a little kid,?says a gleeful Alex de Castro, a car salesman, whohas stopped by a workout of the Suns to slip sixCampaneris cards to the Great Man Himself to beautographed (WSJ-R), we obtain the following re-lations under the SD representation:SD appos(Castro, salesman)num(cards, six)nn(cards, Campaneris)The numeric modifier relation between cards andsix is also standard in the PARC and GR schemes.PARC provides an apposition relation betweensalesman and Alex de Castro, whereas GR onlyidentifies salesman as a text adjunct of Castro.But on the whole, SD makes more fine-graineddistinctions in the relations, which are needed inpractice.
The adjunct dependency of the PARCscheme lumps together different relations.
For ex-ample, the adjectival modifier gleeful in the sen-tence above will not be marked distinctively fromthe preposition modifying workout, nor from therelation between the verbs stop and slip:PARC adjunct(Alex de Castro, gleeful)adjunct(kid, little)adjunct(stop, slip)adjunct(workout, of)The SD output for the relations between thesewords looks as follows:SD amod(Castro, gleeful)amod(kid, little)xcomp(stop, slip)prep of(workout, Suns)The comparison between the two outputs showsthat SD proposes a larger set of dependencies, cap-turing relation differences which can play a rolein applications (DP2), while sticking to notions oftraditional grammar (DP3).The SD scheme also chooses content words asheads of the dependencies (DP5).
Auxiliaries,complementizers, and so on, are dependents ofthem.
This choice in design is driven by the kind ofinformation that is useful for applications.
For in-stance, in the sentence Considered as a whole, Mr.Lane said, the filings required under the proposedrules ?will be at least as effective, if not more so,for investors following transactions?
(WSJ-R), ef-fective is chosen as the head of the quoted phrase.This enables the representation to have a direct de-pendency (nsubj for nominal subject) between thekey content words effective and filings.
Such alink is more difficult to infer from the GR scheme,where be is chosen as the head.
However the re-lation between effective and filings is key to ex-tracting the gist of the sentence semantics, and itis therefore important for applications to be ableto retrieve it easily.
Also, in the case of struc-tures involving copular verbs, a direct link betweenthe subject and the complement enables equiva-lent representations across languages (in Chinese,for example, copulas are not explicitly expressed).Such parallel representations should presumablyhelp machine translation, and this was a furthermotivation for choosing content words as heads.Another instance where direct links betweencontent words is useful is the case of prepositionalcomplements.
The SD scheme offers the optionof ?collapsing?
dependencies involving a preposi-tion (DP5).
In the example above, instead of hav-ing two relations adjunct(workout, of) and obj(of,Suns) as in PARC or ncmod(workout, of) anddobj(of, Suns) as in GR, SD provides a direct rela-tion between the content words: prep of (workout,Suns).
Prepositions often work as role markers,and this type of link facilitates the extraction ofhow the two content words are related; and thusthese links are often used by downstream applica-tions (Lin and Pantel, 2001; Snow et al, 2005).The usefulness of the representation is exemplifiedin the sentence A similar technique is almost im-possible to apply to other crops, such as cotton,soybeans and rice (WSJ-R) for which SD gives di-rect links between the entities joined through thepreposition such as:SD prep such as(crops, cotton)prep such as(crops, soybeans)prep such as(crops, rice)A similar collapsing treatment takes place forconjuncts (DP5).
Consider the following sentence:Bell, based in Los Angeles, makes and distributes3SD nsubj(makes-8, Bell-1)nsubj(distributes-10, Bell-1)partmod(Bell-1, based-3)nn(Angeles-6, Los-5)prep in(based-3, Angeles-6)conj and(makes-8, distributes-10)amod(products-16, electronic-11)conj and(electronic-11, computer-13)amod(products-16, computer-13)conj and(electronic-11, building-15)amod(products-16, building-15)dobj(makes-8, products-16)Figure 1: SD representation for Bell, based in LosAngeles, makes and distributes electronic, com-puter and building products.electronic, computer and building products (WSJ-R).
Figures 1 and 2 give the full dependency out-put from SD and GR, respectively.
The numbersafter the words in the SD representation indicatethe word position in the sentence.3From the SDrepresentation, one can easily see that the sentencetalks about electronic products and computer prod-ucts as well as building products.
By collapsing thedependencies involving conjuncts, the output pro-duced is closer to the semantics of the sentence,and this facilitates information extraction (DP2).This information is not straightforwardly apparentin the GR scheme (see figure 2), nor in the PARCscheme which follows a similar treatment of con-juncts.Another choice in the design has been to con-sistently have binary relations (DP1).
All the de-pendencies form a triple: a grammatical relationholding between two words (head and dependent).This gives uniformity to the representation andrenders it very readable, critical features for a user-centered design.
Furthermore, all the informationcan be represented by a directed graph, enablingthe creation of both a limpid visual representationfor humans and a canonical data structure for soft-ware.
Moreover, it maps straightforwardly on tosemantic web representations such as OWL andRDF triples, as exploited in (Zouaq et al, 2006;Zouaq et al, 2007).This design choice limits the kind of informa-tion offered by the SD scheme.
For instance, thePARC scheme contains much more information3Without word position, the representation is deficient ifthe same word occurs more than once in a sentence.GR (passive based)(ncsubj based Bell obj)(ta bal Bell based)(iobj based in)(dobj in Angeles)(ncmod Angeles Los)(conj and makes)(conj and distributes)(conj and electronic)(conj and computer)(conj and building)(ncsubj and Bell )(dobj and products)(ncmod products and)Figure 2: GR representation for Bell, based in LosAngeles, makes and distributes electronic, com-puter and building products.about individual words, such as verb tense andaspect, noun number and person, type of NE forproper nouns, pronoun form, adjective degree, etc.For the sentence in figures 1 and 2, the followinginformation is available for the word Los Angelesin the PARC scheme:PARC num(Los Angeles?5, sg)pers(Los Angeles?5, 3)proper(Los Angeles?5, location)This kind of information is indubitably valuable,but is often less used in practice, and does not perse pertain to dependency data.
Adding it lengthensan output already complex enough, and impedesreadability and convenience.
Thus, SD does notprovide such overwhelming detail (DP6).2.3 Trading off linguistic fidelity and usabilityWe feel that turning prepositions into relations isuseful for 98% of users 98% of the time.
Neverthe-less opting for usability in this way causes the SDscheme to sacrifice some linguistic fidelity.
Oneinstance is that modifiers of prepositions are de-pendent on the verb (or more precisely, on the headof the clause in which they appear) and not on thepreposition itself.
In Bill went over the river andright through the woods, right will be an adverbialmodifier of went.
In He had laughed, simultane-ously mocking the stupidity of government by cos-metics and confessing that he was also a part of it,just as he was part of government by voice coachand acting coach (BNC), just which modifies aswill be a dependent of the head of the adverbial4clause, i.e., part.
This induces some distortion inthe exact semantics of the sentence.The interaction between preposition collapsingand PP conjunction is another instance in whichthe SD treatment slightly alters the semantics ofthe sentence.
Consider again the sentence Billwent over the river and right through the woods.Both prepositions, over and through, are governedby the verb went.
To avoid disjoint subgraphswhen collapsing the relations, examples like thisare transformed into VP coordination, which re-quires making a copy of the word went.
This givesthe following representation, which corresponds toa sentence like Bill went over the river and wentright through the woods:SD prep over(went-2, river-5)prep through(went-2?, woods-10)conj and(went-2, went-2?
)Not collapsing the relations in such a case wouldprevent the alteration of the semantics, but wouldlead to a non-uniform treatment of prepositions.Uniformity is key for readability and user con-venience.
It seems therefore reasonable to use arepresentation which sacrifices the exact semanticsof the original sentence by producing a sentenceroughly equivalent, but which ensures uniformityacross relations.3 The formalism and the toolTwo vital conditions for the success of a depen-dency scheme are to provide a suitable represen-tation for users as well as a tool that is easy touse.
Sagae et al (2008) note that the availability ofan automatic procedure to convert phrase structureparses to SD is the reason for its use in evaluationsof parsers in the biomedical domain.
The primaryfocus of the SD scheme, however, has been to offergrammatical relations appropriate for end-users.The Stanford parser4comes with a tool, de-scribed in (de Marneffe et al, 2006), which pro-vides for the rapid extraction of the grammati-cal relations from phrase structure parses.
Struc-tural configurations are used to define grammaticalroles: the semantic head of each constituent of theparse is identified, using rules akin to the Collinshead rules, but modified to retrieve the semantichead of the constituent rather than the syntactichead.
As mentioned, content words are chosen asheads, and all the other words in the constituent4http://nlp.stanford.edu/software/lex-parser.shtmldepend on this head.
To retrieve adequate headsfrom a semantic point of view, heuristics are usedto inject more structure when the Penn Treebankgives only flat constituents, as is often the case forconjuncts, e.g., (NP the new phone book and tourguide), and QP constituents, e.g., (QP more than300).
Then for each grammatical relation, patternsare defined over the phrase structure parse tree us-ing the tree-expression syntax defined by tregex(Levy and Andrew, 2006).
Conceptually, each pat-tern is matched against every tree node, and thematching pattern with the most specific grammati-cal relation is taken as the type of the dependency.The automatic extraction of the relations is notinfallible.
For instance, in the sentence Behindtheir perimeter walls lie freshly laundered flowers,verdant grass still sparkling from the last shower,yew hedges in an ecstasy of precision clipping(BNC), the system will erroneously retrieve ap-position relations between flowers and grass, aswell as between flowers and hedges whereas theseshould be conj and relations.
The system is clue-less when there is no overt maker of conjunction.Another limitation of the tool is the treat-ment of long-distance dependencies, such as wh-movement and control/raising: the system can-not handle long-distance dependencies that crossclauses.
In a sentence like What does he think?,the system will correctly find that what is a directobject of think:SD dobj(think-4, What-1)aux(think-4, does-2)nsubj(think-4, he-3)However in a sentence such as Who the hell doeshe think he?s kidding?
(BNC), the automatic ex-traction will fail to find that who is the direct ob-ject of kidding.
Here, it is vital to distinguish be-tween SD as a representation versus the extant con-version tool.
Long-distance dependencies are notabsent from the formalism, but the tool does notaccurately deal with them.54 Stanford dependencies in practiceSD has been successfully used by researchers indifferent domains.
In the PASCAL Recognizing5As possible future work, we have thought of using a toolsuch as the one of Levy and Manning (2004) to correctly de-termine long distance dependencies, as input to the currentdependency conversion system.
This would presumably beeffective, but would make the conversion process much heav-ier weight.5Textual Entailment (RTE) challenges (Dagan et al,2006; Giampiccolo et al, 2007), the increase inthe use of SD is clearly apparent.
The goal inthese challenges consists of identifying whetherone sentence follows from a piece of text and gen-eral background knowledge, according to the intu-itions of an intelligent human reader.
In 2007, outof the 21 systems which participated in the chal-lenge, 5 used the SD representation, whereas theyear before only the Stanford entry was using it.SD is also widely present in the bioinformaticworld where it is used with success (Erkan et al,2007; Greenwood and Stevenson, 2007; Urbain etal., 2007; Clegg, 2008).
Fundel et al (2007) foundthat, in extraction of relations between genes andproteins, a system based on the SD scheme greatlyoutperformed the previous best system on the LLLchallenge dataset (by an 18% absolute improve-ment in F-measure).
Airola et al (2008) providemore systematic results on a number of protein-protein interaction datasets.
Their graph kernel ap-proach uses an all-dependency-paths kernel whichallows their system to consider full dependencygraphs.
Their system is based on the SD scheme,and they demonstrate state-of-the-art performancefor this approach.In the biomedical domain, SD has recently beenused in evaluations of parsers (Clegg and Shep-herd, 2007; Pyysalo et al, 2007a).
Pyysalo et al(2007a) assessed the suitability of the SD schemeover the Link Grammar dependency scheme in anapplication-oriented evaluation.
The Link Parserindeed uses a very fine-grained set of relations,which often makes distinctions of a structuralrather than a semantic nature.
One example is theMX relation which ?connects modifying phraseswith commas to preceding nouns (?The DOG, aPOODLE, was black?
; ?JOHN, IN a black suit,looked great?).?
The Link Parser uses a differentset of dependency types for dependencies appear-ing in questions and relative clauses.
Another ex-ample is the prepositional phrase where alterna-tive attachment structures are indicated by differ-ent relations.
Many of these distinctions are toofine and non-semantic to be of practical value.
TheSD scheme, by aiming for an intermediate level ofgranularity, and targeting semantic dependencies,provides a more adequate representation for appli-cations.
Therefore, to increase the usability of theBioInfer corpus (Pyysalo et al, 2007b), which pro-vides manually annotated data for information ex-traction in the biomedical domain and originallyfollowed the Link Grammar scheme, Pyysalo etal.
(2007a) developed a version of the corpus an-notated with the SD scheme.
They also madeavailable a program and conversion rules that theyused to transform Link Grammar relations into SDgraphs, which were then hand-corrected (Pyysaloet al, 2007b).
While a limited amount of gold stan-dard annotated data was prepared for the ParserEvaluation Shared Task, this is the main source ofgold-standard SD data which is currently available.In other domains, Zhuang et al (2006) uses therepresentation to extract opinions about features inreviews and Meena and Prabhakar (2007) uses itto improve the quality of sentence-level sentimentanalysis.
The open information extraction systemTEXTRUNNER (Banko et al, 2007) also makes useof the SD graph representation: its first moduleuses the Stanford parser and the dependency toolto automatically identify and label trustworthy anduntrustworthy extractions.
Even in theoretical lin-guistic work, SD has proven very useful: it hashugely facilitated data extraction from corpora, inthe context of the NSF-funded project ?Dynamicsof probabilistic grammar?
carried out at the Stan-ford Linguistics department.5 Suitability for parser evaluationWhen seeking a gold-standard dependency schemefor parser evaluation, the ultimate goal of such anevaluation is an important question.
It is necessaryto contrast the two different forms that evaluationcan take: extrinsic task-based evaluation and in-trinsic evaluation.
We tend to agree with Moll?aand Hutchinson (2003) that intrinsic evaluationshave limited value and that task-based evaluationis the correct approach.
Some of the results of theprevious section at least broadly support the util-ity of the SD scheme for practical use in higher-level tasks.
Nevertheless, given the current trendin the NLP community as well as in other fieldssuch as bioinformatics, where the advantage of de-pendency representations for shallow text under-standing tasks has become salient, we would ar-gue, following Clegg and Shepherd (2007), thatdependency-based evaluation is close to typicaluser tasks.
Moreover, it avoids some of the knowndeficiencies of other parser evaluation measuressuch as Parseval (Carroll et al, 1999).Recent work on parser evaluation using depen-dency graphs in the biomedical domain confirms6that researchers regard dependency-based evalu-ation as a more useful surrogate for extrinsictask-based evaluation (Clegg and Shepherd, 2007;Pyysalo et al, 2007a).
In their evaluation, Cleggand Shepherd (2007) aimed at analyzing the ca-pabilities of syntactic parsers with respect to se-mantically important tasks crucial to biologicalinformation extraction systems.
To do so, theyused the SD scheme, which provides ?a de factostandard for comparing a variety of constituentparsers and treebanks at the dependency level,?
andthey assessed its suitability for evaluation.
Theyfound that the SD scheme better illuminates theperformance differences between higher rankedparsers (e.g., Charniak-Lease parser (Lease andCharniak, 2005)), and lower ranked parsers (e.g.,the Stanford parser (Klein and Manning, 2003)).Their parser evaluation accommodates user needs:they used the collapsed version of the dependencygraphs offered by the SD scheme, arguing that thisis the kind of graph one would find most useful inan information extraction project.
Although Cleggand Shepherd (2007) also favor dependency graphrepresentations for parser evaluation, they advo-cate retention of parse trees so information lost inthe dependency structures can be accessed.In essence, any existing dependency schemecould be adopted as the gold-standard for evalu-ation.
However if one believes in ultimately valu-ing extrinsic task-based evaluation, a dependencyrepresentation which proposes a suitable design forusers and user tasks is probably the best surrogatefor intrinsic evaluation.
Moreover, the existenceof tools for automatically generating and convert-ing dependency representations has aided greatlyin making parser comparison possible across dif-ferent formalisms.
We believe that the SD schemeapproaches these goals.
If one accepts the goalsset here, in order to enforce uniformity betweenapplication and evaluation, it seems sensible tohave a unique scheme for both purposes.
Someof the positive results from use of the SD represen-tation, as well as the evaluations carried out in thebiomedical field, point to the usability of the SDscheme for both purposes.AcknowledgmentsWe wish to thank Andrew Brian Clegg and SampoPyysalo for their useful feedback on the depen-dency extraction tool.
Their comments enabled usto improve the tool.
We also thank the workshopreviewers for their helpful comments.ReferencesAirola, Antti, Sampo Pyysalo, Jari Bj?orne, TapioPahikkala, Filip Ginter, and Tapio Salakoski.
2008.A graph kernel for protein-protein interaction ex-traction.
In Proceedings of BioNLP 2008: CurrentTrends in Biomedical Natural Language Processing(ACL08).Banko, Michele, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Pro-ceedings of the 20th International Joint Conferenceon Artificial Intelligence (IJCAI 2007).Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cov-erage of English grammars.
In Proceedings, Speechand Natural Language Workshop, pages 306?311,Pacific Grove, CA.
DARPA.Bresnan, Joan.
2001.
Lexical-Functional Syntax.Blackwell, Oxford.Carroll, John, Guido Minnen, and Ted Briscoe.
1999.Corpus annotation for parser evaluation.
In Proceed-ings of the EACL workshop on Linguistically Inter-preted Corpora (LINC).Clegg, Andrew B. and Adrian J. Shepherd.
2007.Benchmarking natural-language parsers for biolog-ical applications using dependency graphs.
BMCBioinformatics, 8:24.Clegg, Andrew B.
2008.
Computational-LinguisticApproaches to Biological Text Mining.
Ph.D. the-sis, School of Crystallography, Birkbeck, Universityof London.Dagan, Ido, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In et al, Quinonero-Candela, editor,MLCW 2005, LNAI Volume 3944, pages 177?190.Springer-Verlag.de Marneffe, Marie-Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC-06.Erkan, Gunes, Arzucan Ozgur, and Dragomir R. Radev.2007.
Semi-supervised classification for extractingprotein interaction sentences using dependency pars-ing.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL).Fellbaum, Christiane.
1998.
WordNet: an electroniclexical database.
MIT Press.7Fillmore, Charles J., Christopher R. Johnson, and Mir-iam R.L.
Petruck.
2003.
Background to FrameNet.International Journal of Lexicography, 16:235?250.Fundel, Katrin, Robert K?uffner, and Ralf Zimmer.2007.
RelEx relation extraction using dependencyparse trees.
Bioinformatics, 23.Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third PASCAL recogniz-ing textual entailment challenge.
In Proceedings ofthe ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 1?9.Greenwood, Mark A. and Mark Stevenson.
2007.A semi-supervised approach to learning relevantprotein-protein interaction articles.
In Proceedingsof the Second BioCreAtIvE Challenge Workshop,Madrid, Spain.King, Tracy H., Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald Kaplan.
2003.
The PARC700 dependency bank.
In 4th International Work-shop on Linguistically Interpreted Corpora (LINC-03).Klein, Dan and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics.Lease, Matthew and Eugene Charniak.
2005.
Parsingbiomedical literature.
In Proceedings of the SecondInternational Joint Conference on Natural LanguageProcessing (IJCNLP?05).Levy, Roger and Galen Andrew.
2006.
Tregexand Tsurgeon: tools for querying and manipulatingtree data structures.
In LREC 2006. http://www-nlp.stanford.edu/software/tregex.shtml.Levy, Roger and Christopher D. Manning.
2004.
Deepdependencies from context-free statistical parsers:correcting the surface dependency approximation.In ACL 42, pages 328?335.Lin, Dekang and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Lin, Dekang.
1998.
Dependency-based evaluation ofMINIPAR.
In Workshop on the Evaluation of Pars-ing Systems, Granada, Spain.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The Penn Treebank.
Computa-tional Linguistics, 19 (2).Meena, Arun and T. V. Prabhakar.
2007.
Sentence levelsentiment analysis in the presence of conjuncts usinglinguistic analysis.
In Advances in Information Re-trieval, volume 4425 of Lecture Notes in ComputerScience.
Springer.Moldovan, Dan I. and Vasile Rus.
2001.
Logic formtransformation of wordnet and its applicability toquestion answering.
In Meeting of the Associationfor Computational Linguistics, pages 394?401.Moll?a, Diego and Ben Hutchinson.
2003.
Intrinsic ver-sus extrinsic evaluations of parsing systems.
In Pro-ceedings of the Workshop on Evaluation Initiativesin Natural Language Processing, pages 43?50.
Eu-ropean Association for Computational Linguistics.Pyysalo, Sampo, Filip Ginter, Katri Haverinen, JuhoHeimonen, Tapio Salakoski, and Veronika Laippala.2007a.
On the unification of syntactic annotationsunder the Stanford dependency scheme: A casestudy on BioInfer and GENIA.
In Proceedings ofBioNLP 2007: Biological, translational, and clini-cal language processing (ACL07).Pyysalo, Sampo, Filip Ginter, Juho Heimonen, JariBj?orne, Jorma Boberg, Jouni J?arvinen, and TapioSalakoski.
2007b.
BioInfer: A corpus for infor-mation extraction in the biomedical domain.
BMCBioinformatics, 8:50.Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii.
2008.Challenges in mapping of syntactic representationsfor framework-independent parser evaluation.
InProceedings of the Workshop on Automated SyntaticAnnotations for Interoperable Language Resourcesat the First International Conference on Global In-teroperability for Language Resources (ICGL?08).Sleator, Daniel D. and Davy Temperley.
1993.
ParsingEnglish with a link grammar.
In Third InternationalWorkshop on Parsing Technologies.Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Proceedings of NIPS 2004.Urbain, Jay, Nazli Goharian, and Ophir Frieder.
2007.IIT TREC 2007 genomics track: Using concept-based semantics in context for genomics literaturepassage retrieval.
In The Sixteenth Text REtrievalConference (TREC 2007) Proceedings.Zhuang, Li, Feng Jing, Xiao yan Zhu, and Lei Zhang.2006.
Movie review mining and summarization.
InProc.
ACM Conference on Information and Knowl-edge Management (CIKM).Zouaq, Amal, Roger Nkambou, and Claude Frasson.2006.
The knowledge puzzle: An integrated ap-proach of intelligent tutoring systems and knowledgemanagement.
In Proceedings of the 18th IEEE Inter-national Conference on Tools with Artificial Intelli-gence (ICTAI 2006), pages 575?582.Zouaq, Amal, Roger Nkambou, and Claude Frasson.2007.
Building domain ontologies from text for edu-cational purposes.
In Proceedings of the Second Eu-ropean Conference on Technology Enhanced Learn-ing: Creating new learning experiences on a globalscale.8
