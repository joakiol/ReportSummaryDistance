Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 45?53,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 6:Supervised Semantic Parsing of Robotic Spatial CommandsKais DukesSchool of Computing, University of LeedsLeeds LS2 9JT, United Kingdomsckd@leeds.ac.ukAbstractSemEval-2014 Task 6 aims to advancesemantic parsing research by providing ahigh-quality annotated dataset to com-pare and evaluate approaches.
The taskfocuses on contextual parsing of roboticcommands, in which the additional con-text of spatial scenes can be used to guidea parser to control a robot arm.
Six teamssubmitted systems using both rule-basedand statistical methods.
The best per-forming (hybrid) system scored 92.5%and 90.5% for parsing with and withoutspatial context.
However, the best per-forming statistical system scored 87.35%and 60.84% respectively, indicating thatgeneralized understanding of commandsgiven to a robot remains challenging, de-spite the fixed domain used for the task.1 IntroductionSemantic parsers analyze sentences to produceformal meaning representations that are used forthe computational understanding of natural lan-guage.
Recently, state-of-the-art semantic pars-ing methods have used for a variety of applica-tions, including question answering (Kwiat-kowski et al., 2013; Krishnamurthy and Mitchell,2012), dialog systems (Artzi and Zettlemoyer,2011), entity relation extraction (Kate andMooney, 2010) and robotic control (Tellex,2011; Kim and Mooney, 2012).Different parsers can be distinguished by thelevel of supervision they require during training.Fully supervised training typically requires anannotated dataset that maps natural language(NL) to a formal meaning representation such aslogical form.
However, because annotated data isoften not available, a recent trend in semanticparsing research has been to eschew supervisedtraining in favour of either unsupervised orweakly-supervised methods that utilize addi-tional information.
For example, Berant and Li-ang (2014) use a dataset of 5,810 question-answer pairs without annotated logical forms toinduce a parser for a question-answering system.In comparison, Poon (2013) converts NL ques-tions into formal queries via indirect supervisionthrough database interaction.In contrast to previous work, the shared taskdescribed in this paper uses the Robot Com-mands Treebank (Dukes, 2013a), a new datasetmade available for supervised semantic parsing.The chosen domain is robotic control, in whichNL commands are given to a robot arm used tomanipulate shapes on an 8 x 8 game board.
De-spite the fixed domain, the task is challenging ascorrectly parsing commands requires understand-ing spatial context.
For example, the command inFigure 1 may have several plausible interpreta-tions, given different board configurations.
?Move the pyramid on the blue cube on the gray one.
?Figure 1: Example scene with a contextual spatialcommand from the Robot Commands Treebank.This work is licensed under a Creative Commons Attribution4.0 International License.
License details:http://creativecommons.org/licenses/by/4.045The task is inspired by the classic AI systemSHRLDU, which responded to NL commands tocontrol a robot for a similar game board (Wino-grad, 1972), although that system is reported tonot have generalized well (Dreyfus, 2009; Mit-kov, 1999).
More recent research in commandunderstanding has focused on parsing jointlywith grounding, the process of mapping NL de-scriptions of entities within an environment to asemantic representation.
Previous work includesTellex et al.
(2011), who develop a small corpusof commands for a simulated fork lift robot, withgrounding performed using a factor graph.
Simi-larly, Kim and Mooney (2012) perform jointparsing and grounding using a corpus of naviga-tion commands.
In contrast, this paper focuses onparsing using additional situational context fordisambiguation and by using a larger NL dataset,in comparison to previous robotics research.In the remainder of this paper, we describe thetask, the dataset and the metrics used for evalua-tion.
We then compare the approaches used byparticipant systems and conclude with suggestedimprovements for future work.2 Task DescriptionThe long term research goal encouraged by thetask is to develop a system that will robustlyexecute NL robotic commands.
In general, this isa highly complex problem involving computa-tional processing of language, spatial reasoning,contextual awareness and knowledge representa-tion.
To simplify the problem, participants wereprovided with additional tools and resources,allowing them to focus on developing a semanticparser for a fixed domain that would fit into anexisting component architecture.
Figure 2 showshow these components interact.Semantic parser: Systems submitted by partici-pants are semantic parsers that accept an NLcommand as input, mapping this to a formal Ro-bot Control Language (RCL), described furtherin section 3.3.
The Robot Commands Treebankused for the both training and evaluation is anannotated corpus that pairs NL commands withcontextual RCL statements.Spatial planner: A spatial planner is providedas an open Java API1.
Commands in the treebankare specified in the context of spatial scenes.
Byinterfacing with the planner,  participant systems1 https://github.com/kaisdukes/train-robotsFigure 2: Integrated command understanding system.have access to this additional information.
Forexample, given an RCL fragment for the expres-sion ?the red cube on the blue block?, the plannerwill ground the entity, returning a list of zero ormore board coordinates corresponding to possi-ble matches.
The planner also validates com-mands to determine if they are compatible withspatial context.
It can therefore be used to con-strain the search space of possible parses, as wellas enabling early resolution of attachment ambi-guity during parsing.Robotic simulator: The simulated environmentconsists of an 8 x 8 board that can hold prismsand cubes which occur in eight different colors.The robot?s gripper can move to any discrete po-sition within an 8 x 8 x 8 space above the board.The planner uses the simulator to enforce physi-cal laws within the game.
For example, a blockcannot remain unsupported in empty space dueto gravity.
Similarly, prisms cannot lie belowother block types.
In the integrated system, theparser uses the planner for context, then providesthe final RCL statement to the simulator whichexecutes the command by moving the robot armto update the board.3 Data3.1 Data CollectionFor the shared task, 3,409 sentences were se-lected from the treebank.
This data size compareswith related corpora used for semantic parsingsuch as the ATIS (Zettlemoyer and Collins,2007), GeoQuery (Kate et al., 2005), Jobs (Tangand Mooney, 2001) and RoboCup (Kuhlmann etal., 2004) datasets, consisting of 4,978; 880; 640and 300 sentences respectively.The treebank was developed via a game with apurpose (www.TrainRobots.com), in which play-ers were shown  before  and after  configurationsSemantic parserSpatial plannerRobotic simulatorNL commandparsingRCLspatial context46Figure 3: Semantic tree from the treebank with an elliptical anaphoric node and its annotated antecedent.and asked to give a corresponding command to ahypothetical robot arm.
To make the game morecompetitive and to promote data quality, playersrated each other?s sentences and were rewardedwith points for accurate entries (Dukes, 2013b).3.2 AnnotationIn total, over 10,000 commands were collectedthrough the game.
During an offline annotationphase, sentences were manually mapped to RCL.However, due to the nature of the game, playerswere free to enter arbitrarily complex sentencesto describe moves, not all of which could be rep-resented by RCL.
In addition, some commandswere syntactically well-formed, but not compati-ble with the corresponding scenes.
The 3,409commands selected for the task had RCL state-ments that  were both understood by the  planner(sequence:(event:(action: take)(entity:(id: 1)(color: cyan)(type: prism)(spatial-relation:(relation: above)(entity:(color: white)(type: cube)))))(event:(action: drop)(entity:(type: reference)(reference-id: 1))(destination:(spatial-relation:(relation: above)(entity:(color: blue)(color: green)(type: stack))))))Figure 4: RCL representation with co-referencing.and when given to the robotic simulator resultedin the expected move being made between beforeand after board configurations.
Due to this extravalidation step, all RCL statements provided forthe task were contextually well-formed.3.3 Robot Control LanguageRCL is a novel linguistically-oriented semanticrepresentation.
An RCL statement is a semantictree (Figure 3) where leaf nodes generally alignto words in the corresponding sentence, and non-leaves are tagged using a pre-defined set of cate-gories.
RCL is designed to annotate rich linguis-tic structure, including ellipsis (such as ?place [it]on?
), anaphoric references (?it?
and ?one?
), multi-word spatial expressions (?on top of?)
and lexicaldisambiguation (?one?
and ?place?).
Due to ellip-sis, unaligned words and multi-word expressions,a leaf node may align to zero, one or more wordsin a sentence.
Figure 4 shows the RCL syntax forthe tree in Figure 3, as accepted by the spatialplanner and the simulator.
As these componentsdo not require NL word alignment data, this ad-ditional information was made available to taskparticipants for training via a separate Java API.The tagset used to annotate RCL nodes can bedivided into general tags (that are arguably ap-plicable to other domains) and specific tags thatwere customized for the domain in the task (Ta-bles 1 and 2 overleaf, respectively).
The generalelements are typed entities (labelled with seman-tic features) that are connected using relationsand events.
This universal formalism is not do-main-specific, and is inspired by semantic frames(Fillmore and Baker, 2001), a practical represen-tation used for NL understanding systems (Dzik-ovska, 2004; UzZaman and Allen, 2010; Coyneet al., 2010; Dukes, 2009).In the remainder of this section we summarizeaspects of RCL that are relevant to the task; a47more detailed description is provided by Dukes(2013a; 2014).
In an RCL statement such as Fig-ure 4, a preterminal node together with its childleaf node correspond to a feature-value pair(such as the feature color and the constant blue).Two special features which are distinguished bythe planner are id and reference-id, which areused for co-referencing such as for annotatinganaphora and their antecedents.
The remainingfeatures model the simulated robotic domain.
ForRCL Element DescriptionactionAligned to a verbal group in NL,e.g.
?drop?
or ?pick up?.cardinal Number (e.g.
2 or ?three?
).color Colored attribute of an entity.destination A spatial destination.entity Entity within the domain.event Specification of a command.id Id for anaphoric references.indicator Spatial attribute of an entity.measure Used for distance metrics.reference-id A resolved reference.relation Relation type (e.g.
?above?
).sequenceUsed to specify a sequence ofevents or statements.spatial-relationUsed to specify a spatial relationbetween two entities or to de-scribe a location.type Used to specify an entity type.Table 1: Universal semantic elements in RCL.Category ValuesActions move, take, dropRelationsleft, right, above, below,forward, backward, adjacent,within, between, nearest, near,furthest, far, partIndicatorsleft, leftmost, right, rightmost,top, highest, bottom, lowest,front, back, individual, furthest,nearest, centerentity typescube, prism, corner, board stack,row, column, edge, tile, robot,region, reference, type-referenceColorsblue, cyan, red, yellow,green, magenta, gray, whiteTable 2: Semantic categories customized for the task.example, the values of the action feature are themoves used to control the robotic arm, whilevalues of the type and relation features are theentity and relation types understood by the spa-tial planner (Table 2).
As well as qualitative rela-tions (such as ?below?
or ?above?
), the planneralso accepts spatial relations that include quanti-tative measurements, such as in ?two squares leftof the red prism?
(Figure 5).Fig.ure 5: A quantitative relation with a landmark.RCL distinguishes between relations whichrelate entities and indicators, which are attributesof entities (such as ?left?
in ?the left cube?).
Forthe task, participants are asked to map NL sen-tences to well-formed RCL by identifying spatialrelations and indicators, then parsing higher-levelentities and events.
Finally, a well-formed RCLtree with an event (or sequence of events) at top-level is given the simulator for execution.4 Evaluation MetricsOut of the 3,400 sentences annotated for the task,2,500 sentences were provided to participants forsystem training.
During evaluation, trained sys-tems were presented with 909 previously unseensentences and asked to generate correspondingRCL statements, with access to the spatial plan-ner for additional context.
To keep the evaluationprocess as simple as possible, each parser?s out-put for a sentence was scored as correct if it ex-actly matched the expected RCL statement in thetreebank.
Participants were asked to calculatetwo metrics, P and NP, which are the proportionof exact matches with and without using the spa-tial planner respectively:48System Authors Statistical?
Strategy P NP NP - PUW-MRS Packard Hybrid Rule-based ERG + Berkeley parser 92.50 90.50 -2.00AT&T Labs Stoyanchev et al.
Statistical Statistical maximum entropy parser 87.35 60.84 -26.51RoBox Evang and Bos Statistical CCG parser + structured perceptron 86.80 79.21 -7.59Shrdlite Ljungl?f Rule-based Hand crafted domain-specific grammar 86.10 51.50 -34.60KUL-Eval Mattelaer et al.
Statistical CCG parser 71.29 57.76 -13.53UWM Kate Statistical KRISP parser N/A 45.98 N/ATable 3: System results for supervised semantic parsing of the Robot Commands Treebank(P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning,NP - P = drop in performance without integrated spatial planning, N/A = performance not available).These metrics contrast with measures for par-tially correct parsed structures, such as Parseval(Black et al., 1991) or the leaf-ancestor metric(Sampson and Babarczy, 2003).
The rationale forusing a strict match is that in the integrated sys-tem, a command will only be executed if it iscompletely understood, as both the spatial plan-ner and the simulator require well-formed RCL.5 Systems and ResultsSix teams participated in the shared task using avariety of strategies (Table 3).
The last measurein the table gives the performance drop withoutspatial context.
The value NP - P = -2 for thebest performing system suggests this as an upperbound for the task.
The different values of thismeasure indicate the sensitivity to (or possiblyreliance on) context to guide the parsing process.In the remainder of this section we compare theapproaches and results of the six systems.UW-MRS: Packard (2014) achieved the bestscore for parsing both with and without spatialcontext, at 92.5% and 90.5%, respectively, usinga hybrid system that combines a rule-basedgrammar with the Berkeley parser (Petrov et al.,2006).
The rule-based component uses the Eng-lish Resource Grammar, a broad coverage hand-written HPSG grammar for English.
The ERGproduces a ranked list of Minimal RecursionSemantics (MRS) structures that encode predi-cate argument relations (Copestake et al., 2005).Approximately 80 rules were then used to con-vert MRS to RCL.
The highest ranked result thatis validated by the spatial planner was selected asthe output of the rule-based system.
Using thisapproach, Packard reports scores of P = 82.4%and NP = 80.3% for parsing the evaluation data.To further boost performance, the Berkeleyparser was used for back-off.
To train the parser,the RCL treebank was converted to phrase struc-ture by removing non-aligned nodes and insert-ing additional nodes to ensure one-to-one align-ment with words in NL sentences.
Performanceof the Berkeley parser alone was NP = 81.5% (noP-measure was available as spatial planning wasnot integrated).To combine components, the ERG was usedinitially, with fall back to the Berkeley parserwhen no contextually compatible RCL statementwas produced.
The hybrid approach improvedaccuracy considerably, with P = 92.5% and NP =90.5%.
Interestingly, Packard also performs pre-cision and recall analysis, and reports that therule-based component had higher precision,while the statistical component had higher recall,with the combined system outperforming eachseparate component in both precision and recall.AT&T Labs Research: The system by Stoy-anchev et al.
(2014) scored second best for con-textual parsing and third best for parsing withoutusing the spatial planner (P = 87.35% and NP =60.84%).
In contrast to Packard?s UW-MRSsubmission, the AT&T system is a combinationof three statistical models for tagging, parsingand reference resolution.
During the taggingphase, a two-stage sequence tagger first assigns apart-of-speech tag to each word in a sentence,followed by an RCL feature-value pair such as(type: cube) or (color: blue), with unalignedwords tagged as ?O?.
For parsing, a constituencyparser was trained using non-lexical RCL trees.Finally, anaphoric references were resolved us-ing a maximum entropy feature model.
Whencombined, the three components generate a listof weighted RCL trees, which are filtered by thespatial planner.
Without integrated planning, themost-probable parse tree is selected.In their evaluation, Stoyanchev et al.
reportaccuracy scores for the separate phases as well asfor the combined system.
For the tagger, theyreport an accuracy score of 95.2%, using the49standard split of 2,500 sentences for training and909 for evaluation.
To separately measure thejoint accuracy of the parser together with refer-ence resolution, gold-standard tags were usedresulting in a performance of P = 94.83% and NP= 67.55%.
However, using predicted tags, thesystem?s final performance dropped to P =87.35% and NP = 60.84%.
To measure the effectof less supervision, the models were additionallytrained on only 500 sentences.
In this scenario,the tagging model degraded significantly, whilethe parsing and reference resolution models per-formed nearly as well.RoBox: Using Combinatory Categorial Grammar(CCG) as a semantic parsing framework hasbeen previously shown to be suitable for translat-ing NL into logical form.
Inspired by previouswork using a CCG parser in combination with astructured perceptron (Zettlemoyer and Collins,2007), RoBox (Evang and Bos, 2014) was thebest performing CCG system in the shared taskscoring P = 86.8% and NP = 79.21%.Using a similar approach to UW-MRS for itsstatistical component, RCL trees were interpretedas phrase-structure and converted to CCG deriva-tions for training.
During decoding, RCL state-ments were generated directly by the CCGparser.
However, in contrast to the approach usedby the AT&T system, RoBox interfaces with theplanner during parsing instead of performingspatial validation a post-processing step.
Thisenables early resolution of attachment ambiguityand helps constrain the search space.
However,the planner is only used to validate entity ele-ments, so that event and sequence elements werenot validated.
As a further difference to theAT&T system, anaphora resolution was not per-formed using a statistical model.
Instead, multi-ple RCL trees were generated with different can-didate anaphoric references, which were filteredout contextually using the spatial planner.RoBox suffered only a 7.59% absolute drop inperformance without using spatial planning, sec-ond only to UW-MRS at 2%.
Evang and Bosperform error analysis on RoBox and report thatmost errors relate to ellipsis, the ambiguous wordone, anaphora or attachment ambiguity.
Theysuggest that the system could be improved withbetter feature selection or by integrating the CCGparser more closely with the spatial planner.Shrdlite: The Shrdlite system by Ljungl?f(2014), inspired by the Classic SHRDLU systemby Winograd (1972), is a purely rule-based sys-tem that was shown to be effective for the task.Scoring P = 86.1% and NP = 51.5%, Shrdliteranked fourth for parsing with integrated plan-ning, and fifth without using spatial context.However, it suffered the largest absolute drop inperformance without planning (34.6 points), in-dicating that integration with the planner is es-sential for the system?s reported accuracy.Shrdlite uses a hand-written compact unifica-tion grammar for the fragment of English appear-ing in the training data.
The grammar is small,consisting of only 25 grammatical rules and 60lexical rules implemented as a recursive-descentparser in Prolog.
The lexicon consists of 150words (and multi-word expressions) divided into23 lexical categories, based on the RCL pre-terminal nodes found in the treebank.
In a post-processing phase, the resulting parse trees arenormalized to ensure that they are well-formedby using a small set of supplementary rules.However, the grammar is highly ambiguousresulting in multiple parses for a given input sen-tence.
These are filtered by the spatial planner.
Ifmultiple parse trees were found to be compatiblewith spatial context (or when not using the plan-ner), the tree with the smallest number of nodeswas selected as the parser?s final output.
Addi-tionally, because both the training and evaluationdata were collected via crowdsourcing, sentencesoccasionally contain spelling errors, which wereintentionally included in the task.
To handle mis-spelt words, Shrdlite uses Levenshtein edit dis-tance with a penalty to reparse sentences whenthe parser initially fails to produce any analysis.KUL-Eval: The CCG system by Mattelaer et al.
(2014) uses a different approach to the RoBoxsystem described previously.
KUL-Eval scored P= 71.29% and NP = 57.76% in comparison to theRoBox scores of P = 86.8% and NP = 79.21%.During training, the RCL treebank was con-verted to ?-expressions.
This process is fully re-versible, so that no information in an RCL tree islost during conversion.
In contrast to RoBox, butin common with the AT&T parser, KUL-Evalperforms spatial validation as a post-processingstep and does not integrate the planner directlyinto the parsing process.
A probabilistic CCG isused for parsing, so that multiple ?-expressionsare returned (each with an associated confidencemeasure) that are translated into RCL.
Finally, inthe validation step, the spatial planner is used todiscard RCL statements that are incompatiblewith spatial context and the remaining most-probable parse is returned as the system?s output.50Mattelaer et al.
note that in several cases theparser produced partially correct statements butthat these outputs did not contribute to the finalscore, given the strictly matching measures usedfor the P and NP metrics.
However, well-formedRCL statements are required by the spatial plan-ner and robotic simulator for the integrated sys-tem to robustly execute the specified NL com-mand.
Partially correct structures included state-ments which almost matched the expected RCLtree with the exception of incorrect feature-values, or the addition or deletion of nodes.
Themost common errors were feature-values withincorrect entity types (such as ?edge?
and ?re-gion?)
and mismatched spatial relations (such asconfusing ?above?
and ?within?
and confusing?right?, ?left?
and ?front?
).UWM: The UWM system submitted by Kate(2014) uses an existing semantic parser, KRISP,for the shared task.
KRISP (Kernel-based RobustInterpretation for Semantic Parsing) is a trainablesemantic parser (Kate and Mooney, 2006) thatuses Support Vector Machines (SVMs) as themachine learning method with a string subse-quence kernel.
As well as training data consistingof RCL paired with NL commands, KRISP re-quired a context-free grammar for RCL, whichwas hand-written for UWM.
During training, idnodes were removed from the RCL trees.
Thesewere recovered after parsing in a post-processingphase to resolve anaphora by matching to thenearest preceding antecedent.In contrast to other systems submitted for thetask, UWM does not interface with the spatialplanner and parses purely non-contextually.
Be-cause the planner was not used, the system?s ac-curacy was negatively impacted by simple issuesthat may have been easily resolved using spatialcontext.
For example, in RCL, the verb ?place?can map to either drop or move actions, depend-ing on whether or not a block is held in the grip-per in the corresponding spatial scene.
Withoutusing spatial context, it is hard to distinguish be-tween these cases during parsing.The system scored a non-contextual measureof NP = 45.98%, with Kate reporting a 51.18%best F-measure (at 72.67% precision and 39.49%recall).
No P-measure was reported as the spatialplanner was not used.
Due to memory constraintswhen training the SVM classifiers, only 1,500out of 2,500 possible sentences were used fromthe treebank to build the parsing model.
How-ever, it may be possible to increasing the size oftraining data in future work through sampling.6 DiscussionThe six systems evaluated for the task employeda variety of semantic parsing strategies.
With theexception of one submission, all systems inter-faced with the spatial planner, either in a post-processing phase, or directly during parsing toenable early disambiguation and to help con-strain the search space.
An open question thatremains following the task is how applicablethese methods would be to other domains.
Sys-tems that relied heavily on the planner to guidethe parsing process could only be adapted to do-mains for a which a planner could conceivablyexist.
For example, nearly all robotic tasks suchas such as navigation, object manipulation andtask execution involve aspects of planning.
NLquestion-answering interfaces to databases orknowledge stores are also good candidates forthis approach, since parsing NL questions into asemantic representation within the context of adatabase schema or an ontology could be guidedby a query planner.However, approaches with a more attractiveNP - P measure (such as UW-MRS and RoBox)are arguably more easily generalized to otherdomains, as they are less reliant on a planner.Additionally, the usual arguments for rule-basedsystems verses supervised statistical systems ap-ply to any discussion on domain adaptation: rule-based systems require human manual effort,while supervised statistical systems required an-notated data for the new domain.In comparing the best two statistical systems(AT&T and RoBox) it is interesting to note thatthese performed similarly with integrated plan-ning (P = 87.35% and 86.80%, respectively), butdiffered considerably without planning (NP =60.84% and 79.21%).
As these two systems em-ployed different parsers (a constituency parserand a CCG parser), it is difficult to perform adirect comparison to understand why the AT&Tsystem is more reliant on spatial context.
Itwould also be interesting to understand, in fur-ther work, why the two CCG-based systems dif-fered considerably in their P and NP scores.It is also surprising that the best performingsystem, UW-MRS, suffered only a 2% drop inperformance without using the planner, demon-strating clearly that in the majority of sentencesin the evaluation data, spatial context is not actu-ally required to perform semantic parsing.
Al-though as shown by the NP - P scores, spatialcontext can dramatically boost performance ofcertain approaches for the task when used.517 Conclusion and Future WorkThis paper described a new task for SemEval:Supervised Semantic Parsing of Robotic SpatialCommands.
Despite its novel nature, the taskattracted high-quality submissions from sixteams, using a variety of semantic parsing strate-gies.It is hoped that this task will reappear at Se-mEval.
Several lessons were learnt from this firstversion of the shared task which can be used toimprove the task in future.
One issue which sev-eral participants noted was the way in which thetreebank was split into training and evaluationdatasets.
Out of the 3,409 sentences in the tree-bank, the first 2,500 sequential sentences werechosen for training.
Because this data was notrandomized, certain syntactic structures wereonly found during evaluation and were not pre-sent in the training data.
Although this may haveaffected results, all participants evaluated theirsystems against the same datasets.
Based on par-ticipant feedback, in addition to reporting P andNP-measures, it would also be illuminating toinclude a metric such as Parseval F1-scores tomeasure partial accuracy.
An improved versionof the task could also feature a better dataset byexpanding the treebank, not only in terms of sizebut also in terms of linguistic structure.
Manycommands captured in the annotation game arenot yet represented in RCL due to linguistic phe-nomena such as negation and conditional state-ments.Looking forward, a more promising approachto improving the spatial planner could be prob-abilistic planning, so that semantic parsers couldinterface with probabilistic facts with confidencemeasures.
This approach is particularly suitablefor robotics, where sensors often supply noisysignals about the robot?s environment.AcknowledgementsThe author would like to thank the numerousvolunteer annotators who helped develop thedataset used for the task using crowdsourcing, byparticipating in the online game-with-a-purpose.ReferencesYoav Artzi and Luke Zettlemoyer.
2011.
Bootstrap-ping Semantic Parsers from Conversations.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP (pp.421?432).Jonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In Proceedings of theConference of the Association for ComputationalLinguistics, ACL (pp.
1415?1425).Ezra Black, Steven Abney, Dan Flickinger, ClaudiaGdaniec, et al.
1991.
A Procedure for Quantita-tively Comparing the Syntactic Coverage of Eng-lish Grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop (pp.
306-311).
San Mateo, California.Ann Copestake, et al.
2005.
Minimal Recursion Se-mantics: An Introduction.
Research on Languageand Computation, 3(2) (pp.
281-332).Bob Coyne, Owen Rambow, et al.
2010.
Frame Se-mantics in Text-to-Scene Generation.
Knowledge-Based and Intelligent Information and EngineeringSystems (pp.
375-384).
Springer, Berlin.Hubert Dreyfus and Stuart Dreyfus.
2009.
Why Com-puters May Never Think Like People.
Readings inthe Philosophy of Technology.Kais Dukes.
2009.
LOGICON: A System for Extract-ing Semantic Structure using Partial Parsing.
In In-ternational Conference on Recent Advances inNatural Language Processing, RANLP (pp.
18-22).
Borovets, Bulgaria.Kais Dukes.
2013a.
Semantic Annotation of RoboticSpatial Commands.
In Proceedings of the Lan-guage and Technology Conference, LTC.Kais Dukes.
2013b.
Train Robots: A Dataset forNatural Language Human-Robot Spatial Interac-tion through Verbal Commands.
In InternationalConference on Social Robotics.
Embodied Com-munication of Goals and Intentions Workshop.Kais Dukes.
2014.
Contextual Semantic Parsing usingCrowdsourced Spatial Descriptions.
Computationand Language, arXiv:1405.0145 [cs.CL]Myroslava Dzikovska 2004.
A Practical SemanticRepresentation For Natural Language Parsing.
PhDThesis.
University of Rochester.Kilian Evang and Johan Bos.
2014.
RoBox: CCGwith Structured Perceptron for Supervised Seman-tic Parsing of Robotic Spatial Commands.
In Pro-ceedings of the International Workshop on Seman-tic Evaluation, SemEval.Charles Fillmore and Collin Baker.
2001.
Frame se-mantics for Text Understanding.
In Proceedings ofWordNet and Other Lexical Resources Workshop.Rohit Kate and Ray Mooney.
2006.
Using StringKernels for Learning Semantic Parsers.
In Pro-ceedings of the International Conference on Com-putational Linguistics and Annual Meeting of theAssociation for Computational Linguistics, COL-ING-ACL (pp.
913?920).52Rohit Kate and Raymond Mooney.
2010.
Joint Entityand Relation Extraction using Card-Pyramid Pars-ing.
In Proceedings of the Conference on Compu-tational Natural Language Learning, CoNLL (pp.203-212).Rohit Kate, Yuk Wah Wong and Raymond Mooney.2005.
Learning to Transform Natural to FormalLanguages.
In Proceedings of the National Confer-ence on Artificial Intelligence (pp.
1062-1068).Rohit Kate.
2014.
UWM: Applying an ExistingTrainable Semantic Parser to Parse Robotic SpatialCommands.
In Proceedings of the InternationalWorkshop on Semantic Evaluation, SemEval.Joohyun Kim and Raymond Mooney.
2012.
Unsuper-vised PCFG Induction for Grounded LanguageLearning with Highly Ambiguous Supervision.
InProceedings of the Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning,EMNLP-CoNLL (pp.
433-444).Jayant Krishnamurthy and Tom Mitchell.
2012.Weakly Supervised Training of Semantic Parsers.In Proceedings of the Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning,EMNLP-CoNLL.Gregory Kuhlmann et al.
2004.
Guiding a Reinforce-ment Learner with Natural Language Advice: Ini-tial Results in RoboCup Soccer.
In Proceedings ofthe AAAI Workshop on Supervisory Control ofLearning and Adaptive Systems.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi andLuke Zettlemoyer.
2013.
Scaling Semantic Parserswith On-the-fly Ontology Matching.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, EMNLP.Peter Ljungl?f.
2014.
Shrdlite: Semantic Parsing us-ing a Handmade Grammar.
In Proceedings of theInternational Workshop on Semantic Evaluation,SemEval.Willem Mattelaer, Mathias Verbeke and Davide Nitti.2014.
KUL-Eval: A Combinatory CategorialGrammar Approach for Improving Semantic Pars-ing of Robot Commands using Spatial Context.
InProceedings of the International Workshop on Se-mantic Evaluation, SemEval.Ruslan Mitkov.
1999.
Anaphora Resolution: TheState of the Art.
Technical Report.
University ofWolverhampton.Woodley Packard.
2014.
UW-MRS: Leveraging aDeep Grammar for Robotic Spatial Commands.
InProceedings of the International Workshop on Se-mantic Evaluation, SemEval.Slav Petrov, et al.
2006.
Learning Accurate, Compact,and Interpretable Tree Annotation.
In Proceedingsof the International Conference on ComputationalLinguistics and the Annual Meeting of the Associa-tion for Computational Linguistics, COLING-ACL(pp.
433-440).Hoifung Poon.
2013.
Grounded Unsupervised Seman-tic Parsing.
In Proceedings of the Conference of theAssociation for Computational Linguistics, ACL(pp.
466-477).Geoffrey Sampson and Anna Babarczy.
2003.
A Testof the Leaf-Ancestor Metric for Parse Accuracy.Natural Language Engineering, 9.4 (pp.
365-380).Svetlana Stoyanchev, et al.
2014.
AT&T Labs Re-search: Tag&Parse Approach to Semantic Parsingof Robot Spatial Commands.
In Proceedings of theInternational Workshop on Semantic Evaluation,SemEval.Lappoon Tang and Raymond Mooney.
2001.
UsingMultiple Clause Constructors in Inductive LogicProgramming for Semantic Parsing.
MachineLearning, ECML.Stefanie Tellax, et al.
2011.
Approaching the SymbolGrounding Problem with Probabilistic GraphicalModels.
AI Magazine, 32:4 (pp.
64-76).Naushad UzZaman and James Allen.
2010.
TRIPSand TRIOS System for TempEval-2.
In Proceed-ings of the International Workshop on SemanticEvaluation, SemEval  (pp.
276-283).Terry Winograd.
1972.
Understanding Natural Lan-guage.
Cognitive Psychology, 3:1 (pp.
1-191).Luke Zettlemoyer and Michael Collins.
2007.
OnlineLearning of Relaxed CCG Grammars for Parsing toLogical Form.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, EMNLP-CoNLL (pp.
878-887).53
