Automatic Segmentation of Multiparty DialoguePei-Yun HsuehSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBp.hsueh@ed.ac.ukJohanna D. MooreSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBJ.Moore@ed.ac.ukSteve RenalsSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBs.renals@ed.ac.ukAbstractIn this paper, we investigate the prob-lem of automatically predicting segmentboundaries in spoken multiparty dialogue.We extend prior work in two ways.
Wefirst apply approaches that have been pro-posed for predicting top-level topic shiftsto the problem of identifying subtopicboundaries.
We then explore the impacton performance of using ASR output asopposed to human transcription.
Exam-ination of the effect of features showsthat predicting top-level and predictingsubtopic boundaries are two distinct tasks:(1) for predicting subtopic boundaries,the lexical cohesion-based approach alonecan achieve competitive results, (2) forpredicting top-level boundaries, the ma-chine learning approach that combineslexical-cohesion and conversational fea-tures performs best, and (3) conversationalcues, such as cue phrases and overlappingspeech, are better indicators for the top-level prediction task.
We also find thatthe transcription errors inevitable in ASRoutput have a negative impact on modelsthat combine lexical-cohesion and conver-sational features, but do not change thegeneral preference of approach for the twotasks.1 IntroductionText segmentation, i.e., determining the points atwhich the topic changes in a stream of text, playsan important role in applications such as topicdetection and tracking, summarization, automaticgenre detection and information retrieval and ex-traction (Pevzner and Hearst, 2002).
In recentwork, researchers have applied these techniquesto corpora such as newswire feeds, transcripts ofradio broadcasts, and spoken dialogues, in orderto facilitate browsing, information retrieval, andtopic detection (Allan et al, 1998; van Mulbregtet al, 1999; Shriberg et al, 2000; Dharanipragadaet al, 2000; Blei and Moreno, 2001; Christensenet al, 2005).
In this paper, we focus on segmenta-tion of multiparty dialogues, in particular record-ings of small group meetings.
We compare mod-els based solely on lexical information, which arecommon in approaches to automatic segmentationof text, with models that combine lexical and con-versational features.
Because tasks as diverse asbrowsing, on the one hand, and summarization, onthe other, require different levels of granularity ofsegmentation, we explore the performance of ourmodels for two tasks: hypothesizing where ma-jor topic changes occur and hypothesizing wheremore subtle nested topic shifts occur.In addition, because we do not wish to make theassumption that high quality transcripts of meet-ing records, such as those produced by humantranscribers, will be commonly available, we re-quire algorithms that operate directly on automaticspeech recognition (ASR) output.2 Previous WorkPrior research on segmentation of spoken ?docu-ments?
uses approaches that were developed fortext segmentation, and that are based solely ontextual cues.
These include algorithms based onlexical cohesion (Galley et al, 2003; Stokes etal., 2004), as well as models using annotated fea-tures (e.g., cue phrases, part-of-speech tags, coref-erence relations) that have been determined to cor-relate with segment boundaries (Gavalda et al,1997; Beeferman et al, 1999).
Blei et al (2001)273and van Mulbregt et al (1999) use topic lan-guage models and variants of the hidden Markovmodel (HMM) to identify topic segments.
Recentsystems achieve good results for predicting topicboundaries when trained and tested on humantranscriptions.
For example, Stokes et al (2004)report an error rate (Pk) of 0.25 on segmentingbroadcast news stories using unsupervised lexicalcohesion-based approaches.
However, topic seg-mentation of multiparty dialogue seems to be aconsiderably harder task.
Galley et al (2003) re-port an error rate (Pk) of 0.319 for the task of pre-dicting major topic segments in meetings.1Although recordings of multiparty dialoguelack the distinct segmentation cues commonlyfound in text (e.g., headings, paragraph breaks,and other typographic cues) or news story segmen-tation (e.g., the distinction between anchor andinterview segments), they contain conversation-based features that may be of use for automaticsegmentation.
These include silence, overlap rate,speaker activity change (Galley et al, 2003), andcross-speaker linking information, such as adja-cency pairs (Zechner and Waibel, 2000).
Manyof these features can be expected to be compli-mentary.
For segmenting spontaneous multipartydialogue into major topic segments, Galley etal.
(2003) have shown that a model integrating lex-ical and conversation-based features outperformsone based on solely lexical cohesion information.However, the automatic segmentation modelsin prior work were developed for predicting top-level topic segments.
In addition, compared toread speech and two-party dialogue, multi-partydialogues typically exhibit a considerably higherword error rate (WER) (Morgan et al, 2003).We expect that incorrectly recognized words willimpair the robustness of lexical cohesion-basedapproaches and extraction of conversation-baseddiscourse cues and other features.
Past researchon broadcast news story segmentation using ASRtranscription has shown performance degradationfrom 5% to 38% using different evaluation metrics(van Mulbregt et al, 1999; Shriberg et al, 2000;Blei and Moreno, 2001).
However, no prior studyhas reported directly on the extent of this degra-dation on the performance of a more subtle topicsegmentation task and in spontaneous multipartydialogue.
In this paper, we extend prior work by1For the definition of Pk and Wd, please refer to section3.4.1investigating the effect of using ASR output on themodels that have previously been proposed.
In ad-dition, we aim to find useful features and modelsfor the subtopic prediction task.3 Method3.1 DataIn this study, we used the ICSI meeting corpus(LDC2004S02).
Seventy-five natural meetings ofICSI research groups were recorded using close-talking far field head-mounted microphones andfour desktop PZM microphones.
The corpus in-cludes human transcriptions of all meetings.
Weadded ASR transcriptions of all 75 meetings whichwere produced by Hain (2005), with an averageWER of roughly 30%.The ASR system used a vocabulary of 50,000words, together with a trigram language modeltrained on a combination of in-domain meetingdata, related texts found by web search, conver-sational telephone speech (CTS) transcripts andbroadcast news transcripts (about 109 words in to-tal), resulting in a test-set perplexity of about 80.The acoustic models comprised a set of context-dependent hidden Markov models, using gaussianmixture model output distributions.
These wereinitially trained on CTS acoustic training data, andwere adapted to the ICSI meetings domain usingmaximum a posteriori (MAP) adaptation.
Furtheradaptation to individual speakers was achieved us-ing vocal tract length normalization and maximumlikelihood linear regression.
A four-fold cross-validation technique was employed: four recog-nizers were trained, with each employing 75% ofthe ICSI meetings as acoustic and language modeltraining data, and then used to recognize the re-maining 25% of the meetings.3.2 Fine-grained and coarse-grained topicsWe characterize a dialogue as a sequence of top-ical segments that may be further divided intosubtopic segments.
For example, the 60 minutemeeting Bed003, whose theme is the planning ofa research project on automatic speech recognitioncan be described by 4 major topics, from ?open-ing?
to ?general discourse features for higher lay-ers?
to ?how to proceed?
to ?closing?.
Dependingon the complexity, each topic can be further di-vided into a number of subtopics.
For example,?how to proceed?
can be subdivided to 4 subtopicsegments, ?segmenting off regions of features?,274?ad-hoc probabilities?, ?data collection?
and ?ex-perimental setup?.Three human annotators at our site used a tai-lored tool to perform topic segmentation in whichthey could choose to decompose a topic intosubtopics, with at most three levels in the resultinghierarchy.
Topics are described to the annotatorsas what people in a meeting were talking about.Annotators were asked to provide a free text la-bel for each topic segment; they were encour-aged to use keywords drawn from the transcrip-tion in these labels, and we provided some stan-dard labels for non-content topics, such as ?open-ing?
and ?chitchat?, to impose consistency.
Forour initial experiments with automatic segmenta-tion at different levels of granularity, we flattenedthe subtopic structure and consider only two levelsof segmentation?top-level topics and all subtopics.To establish reliability of our annotation proce-dure, we calculated kappa statistics between theannotations of each pair of coders.
Our analy-sis indicates human annotators achieve ?
= 0.79agreement on top-level segment boundaries and?
= 0.73 agreement on subtopic boundaries.
Thelevel of agreement confirms good replicability ofthe annotation procedure.3.3 Probabilistic modelsOur goal is to investigate the impact of ASR er-rors on the selection of features and the choice ofmodels for segmenting topics at different levels ofgranularity.
We compare two segmentation mod-els: (1) an unsupervised lexical cohesion-basedmodel (LM) using solely lexical cohesion infor-mation, and (2) feature-based combined models(CM) that are trained on a combination of lexicalcohesion and conversational features.3.3.1 Lexical cohesion-based modelIn this study, we use Galley et al?s (2003)LCSeg algorithm, a variant of TextTiling (Hearst,1997).
LCSeg hypothesizes that a major topicshift is likely to occur where strong term repeti-tions start and end.
The algorithm works with twoadjacent analysis windows, each of a fixed sizewhich is empirically determined.
For each utter-ance boundary, LCSeg calculates a lexical cohe-sion score by computing the cosine similarity atthe transition between the two windows.
Low sim-ilarity indicates low lexical cohesion, and a sharpchange in lexical cohesion score indicates a highprobability of an actual topic boundary.
The prin-cipal difference between LCSeg and TextTiling isthat LCSeg measures similarity in terms of lexicalchains (i.e., term repetitions), whereas TextTilingcomputes similarity using word counts.3.3.2 Integrating lexical andconversation-based featuresWe also used machine learning approaches thatintegrate features into a combined model, cast-ing topic segmentation as a binary classificationtask.
Under this supervised learning scheme, atraining set in which each potential topic bound-ary2 is labelled as either positive (POS) or neg-ative (NEG) is used to train a classifier to pre-dict whether each unseen example in the test setbelongs to the class POS or NEG.
Our objectivehere is to determine whether the advantage of in-tegrating lexical and conversational features alsoimproves automatic topic segmentation at the finergranularity of subtopic levels, as well as whenASR transcriptions are used.For this study, we trained decision trees (c4.5)to learn the best indicators of topic boundaries.We first used features extracted with the optimalwindow size reported to perform best in Galley etal.
(2003) for segmenting meeting transcripts intomajor topical units.
In particular, this study usesthe following features: (1) lexical cohesion fea-tures: the raw lexical cohesion score and proba-bility of topic shift indicated by the sharpness ofchange in lexical cohesion score, and (2) conver-sational features: the number of cue phrases inan analysis window of 5 seconds preceding andfollowing the potential boundary, and other inter-actional features, including similarity of speakeractivity (measured as a change in probability dis-tribution of number of words spoken by eachspeaker) within 5 seconds preceding and follow-ing each potential boundary, the amount of over-lapping speech within 30 seconds following eachpotential boundary, and the amount of silence be-tween speaker turns within 30 seconds precedingeach potential boundary.3.4 EvaluationTo compare to prior work, we perform a 25-fold leave-one-out cross validation on the set of25 ICSI meetings that were used in Galley et2In this study, the end of each speaker turn is a potentialsegment boundary.
If there is a pause of more than 1 secondwithin a single speaker turn, the turn is divided at the begin-ning of the pause creating a potential segment boundary.275al.
(2003).
We repeated the procedure to eval-uate the accuracy using the lexical cohesion andcombined models on both human and ASR tran-scriptions.
In each evaluation, we trained the au-tomatic segmentation models for two tasks: pre-dicting subtopic boundaries (SUB) and predictingonly top-level boundaries (TOP).3.4.1 Evaluation metricsIn order to be able to compare our results di-rectly with previous work, we first report our re-sults using the standard error rate metrics of Pkand Wd.
Pk (Beeferman et al, 1999) is the prob-ability that two utterances drawn randomly from adocument (in our case, a meeting transcript) are in-correctly identified as belonging to the same topicsegment.
WindowDiff (Wd) (Pevzner and Hearst,2002) calculates the error rate by moving a slidingwindow across the meeting transcript counting thenumber of times the hypothesized and referencesegment boundaries are different.3.4.2 BaselineTo compute a baseline, we follow Kan (2003)and Hearst (1997) in using Monte Carlo simu-lated segments.
For the corpus used as trainingdata in the experiments, the probability of a poten-tial topic boundary being an actual one is approxi-mately 2.2% for all subtopic segments, and 0.69%for top-level topic segments.
Therefore, the MonteCarlo simulation algorithm predicts that a speakerturn is a segment boundary with these probabilitiesfor the two different segmentation tasks.
We exe-cuted the algorithm 10,000 times on each meetingand averaged the scores to form the baseline forour experiments.3.4.3 ToplineFor the 24 meetings that were used in training,we have top-level topic boundaries annotated bycoders at Columbia University (Col) and in our labat Edinburgh (Edi).
We take the majority opinionon each segment boundary from the Col annota-tors as reference segments.
For the Edi annota-tions of top-level topic segments, where multipleannotations exist, we choose one randomly.
Thetopline is then computed as the Pk score compar-ing the Col majority annotation to the Edi annota-tion.4 Results4.1 Experiment 1: Predicting top-level andsubtopic segment boundariesThe meetings in the ICSI corpus last approxi-mately 1 hour and have an average of 8-10 top-level topic segments.
In order to facilitate meet-ing browsing and question-answering, we believeit is useful to include subtopic boundaries in or-der to narrow in more accurately on the portionof the meeting that contains the information theuser needs.
Therefore, we performed experimentsaimed at analysing how the LM and CM seg-mentation models behave in predicting segmentboundaries at the two different levels of granular-ity.All of the results are reported on the test set.Table 1 shows the performance of the lexical co-hesion model (LM) and the combined model (CM)integrating the lexical cohesion and conversationalfeatures discussed in Section 3.3.2.3 For the taskof predicting top-level topic boundaries from hu-man transcripts, CM outperforms LM.
LM tendsto over-predict on the top-level, resulting in ahigher false alarm rate.
However, for the task ofpredicting subtopic shifts, LM alone is consider-ably better than CM.Error Rate Transcript ASRModels Pk Wd Pk WdLM SUB 32.31% 38.18% 32.91% 37.13%(LCSeg) TOP 36.50% 46.57% 38.02% 48.18%CM SUB 36.90% 38.68% 38.19% n/a(C4.5) TOP 28.35% 29.52% 28.38% n/aTable 1: Performance comparison of probabilisticsegmentation models.In order to support browsing during the meetingor shortly thereafter, automatic topic segmentationwill have to operate on the transcriptions producedby ASR.
First note from Table 1 that the prefer-ence of models for segmentation at the two differ-ent levels of granularity is the same for ASR andhuman transcriptions.
CM is better for predictingtop-level boundaries and LM is better for predict-ing subtopic boundaries.
This suggests that these3We do not report Wd scores for the combined model(CM) on ASR output because this model predicted 0 segmentboundaries when operating on ASR output.
In our experi-ence, CM routinely underpredicted the number of segmentboundaries, and due to the nature of the Wd metric, it shouldnot be used when there are 0 hypothesized topic boundaries.276are two distinct tasks, regardless of whether thesystem operates on human produced transcriptionor ASR output.
Subtopics are better characterizedby lexical cohesion, whereas top-level topic shiftsare signalled by conversational features as well aslexical-cohesion based features.4.1.1 Effect of feature combinations:predicting from human transcriptsNext, we wish to determine which features inthe combined model are most effective for predict-ing topic segments at the two levels of granularity.Table 2 gives the average Pk for all 25 meetingsin the test set, using the features described in Sec-tion 3.3.2.
We group the features into four classes:(1) lexical cohesion-based features (LF): includinglexical cohesion value (LCV) and estimated pos-terior probability (LCP); (2) interaction features(IF): the amount of overlapping speech (OVR),the amount of silence between speaker segments(GAP), similarity of speaker activity (ACT); (3)cue phrase feature (CUE); and (4) all available fea-tures (ALL).
For comparison we also report thebaseline (see Section 3.4.2) generated by MonteCarlo algorithm (MC-B).
All of the models us-ing one or more features from these classes out-perform the baseline model.
A one-way ANOVArevealed this reliable effect on the top-level seg-mentation (F (7, 192) = 17.46, p < 0.01) as wellas on the subtopic segmentation task (F (7, 192) =5.862, p < 0.01).TRANSCRIPT Error Rate(Pk)Feature set SUB TOPMC-B 46.61% 48.43%LF(LCV+LCP) 38.13% 29.92%IF(ACT+OVR+GAP) 38.87% 30.11%IF+CUE 38.87% 30.11%LF+ACT 38.70% 30.10%LF+OVR 38.56% 29.48%LF+GAP 38.50% 29.87%LF+IF 38.11% 29.61%LF+CUE 37.46% 29.18%ALL(LF+IF+CUE) 36.90% 28.35%Table 2: Effect of different feature combinationsfor predicting topic boundaries from human tran-scripts.
MC-B is the randomly generated baseline.As shown in Table 2, the best performing modelfor predicting top-level segments is the one us-ing all of the features (ALL).
This is not surpris-ing, because these were the features that Galleyet al (2003) found to be most effective for pre-dicting top-level segment boundaries in their com-bined model.
Looking at the results in more de-tail, we see that when we begin with LF featuresalone and add other features one by one, the onlymodel (other than ALL) that achieves significant4improvement (p < 0.05) over LF is LF+CUE,the model that combines lexical cohesion featureswith cue phrases.When we look at the results for predictingsubtopic boundaries, we again see that the bestperforming model is the one using all features(ALL).
Models using lexical-cohesion featuresalone (LF) and lexical cohesion features with cuephrases (LF+CUE) both yield significantly betterresults than using interactional features (IF) alone(p < 0.01), or using them with cue phrase features(IF+CUE) (p < 0.01).
Again, none of the interac-tional features used in combination with LF sig-nificantly improves performance.
Indeed, addingspeaker activity change (LF+ACT) degrades theperformance (p < 0.05).Therefore, we conclude that for predicting bothtop-level and subtopic boundaries from humantranscriptions, the most important features are thelexical cohesion based features (LF), followedby cue phrases (CUE), with interactional featurescontributing to improved performance only whenused in combination with LF and CUE.However, a closer look at the Pk scores in Ta-ble 2, adds further evidence to our hypothesis thatpredicting subtopics may be a different task frompredicting top-level topics.
Subtopic shifts oc-cur more frequently, and often without clear con-versational cues.
This is suggested by the factthat absolute performance on subtopic predictiondegrades when any of the interactional featuresare combined with the lexical cohesion features.In contrast, the interactional features slightly im-prove performance when predicting top-level seg-ments.
Moreover, the fact that the feature OVRhas a positive impact on the model for predictingtop-level topic boundaries, but does not improvethe model for predicting subtopic boundaries re-veals that having less overlapping speech is a moreprominent phenomenon in major topic shifts than4Because we do not wish to make assumptions about theunderlying distribution of error rates, and error rates are notmeasured on an interval level, we use a non-parametric signtest throughout these experiments to compute statistical sig-nificance.277in subtopic shifts.4.1.2 Effect of feature combinations:predicting from ASR outputFeatures extracted from ASR transcripts are dis-tinct from those extracted from human transcriptsin at least three ways: (1) incorrectly recognizedwords incur erroneous lexical cohesion features(LF), (2) incorrectly recognized words incur erro-neous cue phrase features (CUE), and (3) the ASRsystem recognizes less overlapping speech (OVR).In contrast to the finding that integrating conver-sational features with lexical cohesion features isuseful for prediction from human transcripts, Ta-ble 3 shows that when operating on ASR output,neither adding interactional nor cue phrase fea-tures improves the performance of the model usingonly lexical cohesion features.
In fact, the modelusing all features (ALL) is significantly worse thanthe model using only lexical cohesion based fea-tures (LF).
This suggests that we must explore newfeatures that can lessen the perplexity introducedby ASR outputs in order to train a better model.ASR Error Rate(Pk)Feature set SUB TOPMC-B 43.41% 45.22%LF(LCV+LCP) 36.83% 25.27%IF(ACT+OVR+GAP) 36.83% 25.27%IF+CUE 36.83% 25.27%LF+GAP 36.67% 24.62%LF+IF 36.83% 28.24%LF+CUE 37.42% 25.27%ALL(LF+IF+CUE) 38.19% 28.38%Table 3: Effect of different feature combinationsfor predicting topic boundaries from ASR output.4.2 Experiment 2: Statistically learned cuephrasesIn prior work, Galley et al (2003) empiricallyidentified cue phrases that are indicators of seg-ment boundaries, and then eliminated all cues thathad not previously been identified as cue phrasesin the literature.
Here, we conduct an experimentto explore how different ways of identifying cuephrases can help identify useful new features forthe two boundary prediction tasks.In each fold of the 25-fold leave-one-out crossvalidation, we use a modified5 Chi-square test to5In order to satisfy the mathematical assumptions under-calculate statistics for each word (unigram) andword pair (bi-gram) that occurred in the 24 train-ing meetings.
We then rank unigrams and bigramsaccording to their Chi-square scores, filtering outthose with values under 6.64, the threshold for theChi-square statistic at the 0.01 significance level.The unigrams and bigrams in this ranked list arethe learned cue phrases.
We then use the occur-rence counts of cue phrases in an analysis windowaround each potential topic boundary in the testmeeting as a feature.Table 4 shows the performance of models thatuse statistically learned cue phrases in their featuresets compared with models using no cue phrasefeatures and Galley?s model, which only uses cuephrases that correspond to those identified in theliterature (Col-cue).
We see that for predictingsubtopics, models using the cue word features(1gram) and the combination of cue words and bi-grams (1+2gram) yield a 15% and 8.24% improve-ment over models using no cue features (NOCUE)(p < 0.01) respectively, while models using onlycue phrases found in the literature (Col-cue) im-prove performance by just 3.18%.
In contrast, forpredicting top-level topics, the model using cuephrases from the literature (Col-cue) achieves a4.2% improvement, and this is the only model thatproduces statistically significantly better resultsthan the model using no cue phrases (NOCUE).The superior performance of models using statis-tically learned cue phrases as features for predict-ing subtopic boundaries suggests there may exist adifferent set of cue phrases that serve as segmen-tation cues for subtopic boundaries.5 DiscussionAs observed in the corpus of meetings, the lackof macro-level segment units (e.g., story breaks,paragraph breaks) makes the task of segmentingspontaneous multiparty dialogue, such as meet-ings, different from segmenting text or broadcastnews.
Compared to the task of segmenting expos-itory texts reported in Hearst (1997) with a 39.1%chance of each paragraph end being a target topicboundary, the chance of each speaker turn be-ing a top-level or sub-topic boundary in our ICSIcorpus is just 2.2% and 0.69%.
The imbalancedclass distribution has a negative effect on the per-lying the test, we removed cases with an expected value thatis under a threshold (in this study, we use 1), and we applyYate?s correction, (|ObservedV alue?ExpectedV alue| ?0.5)2/ExpectedV alue.278NOCUE Col-cue 1gram 2gram 1+2gram MC-B ToplineSUB 38.11% 36.90% 32.39% 36.86% 34.97% 46.61% n/aTOP 29.61% 28.35% 28.95% 29.20% 29.27% 48.43% 13.48%Table 4: Performance of models trained with cue phrases from the literature (Col-cue) and cue phraseslearned from statistical tests, including cue words (1gram), cue word pairs (2gram), and cue phrasescomposed of both words and word pairs (1+2gram).
NOCUE is the model using no cue phrase features.The Topline is the agreement of human annotators on top-level segments.0 10 20 30 40 50 60 70 800.260.280.30.320.340.360.380.40.420.44Training Set Size (In meetings)ErrorRate(Pk)TRAN?ALLTRAN?TOPASR?ALLASR?TOPFigure 1: Performance of the combined modelover the increase of the training set size.formance of machine learning approaches.
In apilot study, we investigated sampling techniquesthat rebalance the class distribution in the train-ing set.
We found that sampling techniques pre-viously reported in Liu et al(2004) as useful fordealing with an imbalanced class distribution inthe task of disfluency detection and sentence seg-mentation do not work for this particular data set.The implicit assumption of some classifiers (suchas pruned decision trees) that the class distributionof the test set matches that of the training set, andthat the costs of false positives and false negativesare equivalent, may account for the failure of thesesampling techniques to yield improvements in per-formance, when measured using Pk and Wd.Another approach that copes with the im-balanced class prediction problem but does notchange the natural class distribution is to increasethe size of the training set.
We conducted an ex-periment in which we incrementally increased thetraining set size by randomly choosing ten meet-ings each time until all meetings were selected.We executed the process three times and averagedthe scores to obtain the results shown in Figure 1.However, increasing training set size adds to theperplexity in the training phase.
We see that in-creasing the size of the training set only improvesthe accuracy of segment boundary prediction forpredicting top-level topics on ASR output.
Thefigure also indicates that training a model to pre-dict top-level boundaries requires no more than fif-teen meetings in the training set to reach a reason-able level of performance.6 ConclusionsDiscovering major topic shifts and finding nestedsubtopics are essential for the success of speechdocument browsing and retrieval.
Meeting recordscontain rich information, in both content and con-versation behavioral form, that enable automatictopic segmentation at different levels of granular-ity.
The current study demonstrates that the twotasks ?
predicting top-level and subtopic bound-aries ?
are distinct in many ways: (1) for pre-dicting subtopic boundaries, the lexical cohesion-based approach achieves results that are com-petitive with the machine learning approach thatcombines lexical and conversational features; (2)for predicting top-level boundaries, the machinelearning approach performs the best; and (3) manyconversational cues, such as overlapping speechand cue phrases discussed in the literature, arebetter indicators for top-level topic shifts thanfor subtopic shifts, but new features such as cuephrases can be learned statistically for the subtopicprediction task.
Even in the presence of a rela-tively higher word error rate, using ASR outputmakes no difference to the preference of model forthe two tasks.
The conversational features also didnot help improve the performance for predictingfrom ASR output.In order to further identify useful features forautomatic segmentation of meetings at differentlevels of granularity, we will explore the use of279multimodal, i.e., acoustic and visual, cues.
In ad-dition, in the current study, we only extracted fea-tures from within the analysis windows immedi-ately preceding and following each potential topicboundary; we will explore models that take intoaccount features of longer range dependencies.7 AcknowledgementsMany thanks to Jean Carletta for her invaluablehelp in managing the data, and for advice andcomments on the work reported in this paper.Thanks also to the AMI ASR group for produc-ing the ASR transcriptions, and to the anonymousreviewers for their helpful comments.
This workwas supported by the European Union 6th FWPIST Integrated Project AMI (Augmented Multi-party Interaction, FP6-506811).ReferencesJ.
Allan, J.G.
Carbonell, G. Doddington, J. Yamron,and Y. Yang.
1998.
Topic detection and tracking pi-lot study: Final report.
In Proceedings of the DARPABroadcast News Transcription and UnderstandingWorkshop.D.
Beeferman, A. Berger, and J. Lafferty.
1999.
Statis-tical models for text segmentation.
Machine Learn-ing, 34:177?210.D.
M. Blei and P. J. Moreno.
2001.
Topic segmentationwith an aspect hidden Markov model.
In Proceed-ings of the 24th Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval.
ACM Press.H.
Christensen, B. Kolluru, Y. Gotoh, and S. Renals.2005.
Maximum entropy segmentation of broad-cast news.
In Proceedings of the IEEE InternationalConference on Acoustic, Speech, and Signal Pro-cessing, Philadelphia, USA.S.
Dharanipragada, M. Franz, J.S.
McCarley, K. Pap-ineni, S. Roukos, T. Ward, and W. J. Zhu.
2000.Statistical methods for topic segmentation.
In Pro-ceedings of the International Conference on SpokenLanguage Processing, pages 516?519.M.
Galley, K. McKeown, E. Fosler-Lussier, andH.
Jing.
2003.
Discourse segmentation of multi-party conversation.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics.M.
Gavalda, K. Zechner, and G. Aist.
1997.
High per-formance segmentation of spontaneous speech usingpart of speech and trigger word information.
In Pro-ceedings of the Fifth ANLP Conference, pages 12?15.T.
Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,V.
Wan, R. Ordelman, and S. Renals.
2005.
Tran-scription of conference room meetings: an investi-gation.
In Proceedings of Interspeech.M.
Hearst.
1997.
Texttiling: Segmenting text into mul-tiparagraph subtopic passages.
Computational Lin-guistics, 25(3):527?571.M.
Kan. 2003.
Automatic text summarization asapplied to information retrieval: Using indicativeand informative summaries.
Ph.D. thesis, ColumbiaUniversity, New York USA.Y.
Liu, E. Shriberg, A. Stolcke, and M. Harper.
2004.Using machine learning to cope with imbalancedclasses in natural sppech: Evidence from sentenceboundary and disfluency detection.
In Proceedingsof the Intl.
Conf.
Spoken Language Processing.N.
Morgan, D. Baron, S. Bhagat, H. Carvey,R.
Dhillon, J. Edwards, D. Gelbart, A. Janin,A.
Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, , and C. Wooters.
2003.
Meetings about meet-ings: research at icsi on speech in multiparty conver-sations.
In Proceedings of the IEEE InternationalConference on Acoustic, Speech, and Signal Pro-cessing.L.
Pevzner and M. Hearst.
2002.
A critique and im-provement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.E.
Shriberg, A. Stolcke, D. Hakkani-tur, and G. Tur.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech Commu-nications, 31(1-2):127?254.N.
Stokes, J. Carthy, and A.F.
Smeaton.
2004.
Se-lect: a lexical cohesion based news story segmenta-tion system.
AI Communications, 17(1):3?12, Jan-uary.P.
van Mulbregt, J. Carp, L. Gillick, S. Lowe, andJ.
Yamron.
1999.
Segmentation of automaticallytranscribed broadcast news text.
In Proceedings ofthe DARPA Broadcast News Workshop, pages 77?80.
Morgan Kaufman Publishers.Klaus Zechner and Alex Waibel.
2000.
DIASUMM:Flexible summarization of spontaneous dialogues inunrestricted domains.
In Proceedings of COLING-2000, pages 968?974.280
