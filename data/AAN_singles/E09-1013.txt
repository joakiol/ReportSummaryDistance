Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103?111,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsBayesian Word Sense InductionSamuel BrodyDept.
of Biomedical InformaticsColumbia Universitysamuel.brody@dbmi.columbia.eduMirella LapataSchool of InformaticsUniversity of Edinburghmlap@inf.ed.ac.ukAbstractSense induction seeks to automaticallyidentify word senses directly from a cor-pus.
A key assumption underlying pre-vious work is that the context surround-ing an ambiguous word is indicative ofits meaning.
Sense induction is thus typ-ically viewed as an unsupervised cluster-ing problem where the aim is to partitiona word?s contexts into different classes,each representing a word sense.
Our workplaces sense induction in a Bayesian con-text by modeling the contexts of the am-biguous word as samples from a multi-nomial distribution over senses whichare in turn characterized as distributionsover words.
The Bayesian framework pro-vides a principled way to incorporate awide range of features beyond lexical co-occurrences and to systematically assesstheir utility on the sense induction task.The proposed approach yields improve-ments over state-of-the-art systems on abenchmark dataset.1 IntroductionSense induction is the task of discovering automat-ically all possible senses of an ambiguous word.
Itis related to, but distinct from, word sense disam-biguation (WSD) where the senses are assumed tobe known and the aim is to identify the intendedmeaning of the ambiguous word in context.Although the bulk of previous work has beendevoted to the disambiguation problem1, there aregood reasons to believe that sense induction maybe able to overcome some of the issues associ-ated with WSD.
Since most disambiguation meth-ods assign senses according to, and with the aid1Approaches to WSD are too numerous to list; We referthe interested reader to Agirre et al (2007) for an overviewof the state of the art.of, dictionaries or other lexical resources, it is dif-ficult to adapt them to new domains or to lan-guages where such resources are scarce.
A re-lated problem concerns the granularity of the sensedistinctions which is fixed, and may not be en-tirely suitable for different applications.
In con-trast, when sense distinctions are inferred directlyfrom the data, they are more likely to representthe task and domain at hand.
There is little riskthat an important sense will be left out, or that ir-relevant senses will influence the results.
Further-more, recent work in machine translation (Vickreyet al, 2005) and information retrieval (Ve?ronis,2004) indicates that induced senses can lead to im-proved performance in areas where methods basedon a fixed sense inventory have previously failed(Carpuat and Wu, 2005; Voorhees, 1993).Sense induction is typically treated as an un-supervised clustering problem.
The input to theclustering algorithm are instances of the ambigu-ous word with their accompanying contexts (rep-resented by co-occurrence vectors) and the outputis a grouping of these instances into classes cor-responding to the induced senses.
In other words,contexts that are grouped together in the sameclass represent a specific word sense.
In this paperwe adopt a novel Bayesian approach and formalizethe induction problem in a generative model.
Foreach ambiguous word we first draw a distributionover senses, and then generate context words ac-cording to this distribution.
It is thus assumed thatdifferent senses will correspond to distinct lexicaldistributions.
In this framework, sense distinctionsarise naturally through the generative process: ourmodel postulates that the observed data (word con-texts) are explicitly intended to communicate a la-tent structure (their meaning).Our work is related to Latent Dirichlet Allo-cation (LDA, Blei et al 2003), a probabilisticmodel of text generation.
LDA models each doc-ument using a mixture over K topics, which arein turn characterized as distributions over words.103The words in the document are generated by re-peatedly sampling a topic according to the topicdistribution, and selecting a word given the chosentopic.
Whereas LDA generates words from globaltopics corresponding to the whole document, ourmodel generates words from local topics chosenbased on a context window around the ambiguousword.
Document-level topics resemble general do-main labels (e.g., finance, education) and cannotfaithfully model more fine-grained meaning dis-tinctions.
In our work, therefore, we create an in-dividual model for every (ambiguous) word ratherthan a global model for an entire document col-lection.
We also show how multiple informationsources can be straightforwardly integrated with-out changing the underlying probabilistic model.For instance, besides lexical information we maywant to consider parts of speech or dependen-cies in our sense induction problem.
This is inmarked contrast with previous LDA-based mod-els which mostly take only word-based informa-tion into account.
We evaluate our model on arecently released benchmark dataset (Agirre andSoroa, 2007) and demonstrate improvements overthe state-of-the-art.The remainder of this paper is structured as fol-lows.
We first present an overview of related work(Section 2) and then describe our Bayesian modelin more detail (Sections 3 and 4).
Section 5 de-scribes the resources and evaluation methodologyused in our experiments.
We discuss our results inSection 6, and conclude in Section 7.2 Related WorkSense induction is typically treated as a cluster-ing problem, where instances of a target wordare partitioned into classes by considering theirco-occurring contexts.
Considerable latitude isallowed in selecting and representing the co-occurring contexts.
Previous methods have usedfirst or second order co-occurrences (Purandareand Pedersen, 2004; Schu?tze, 1998), parts ofspeech (Purandare and Pedersen, 2004), and gram-matical relations (Pantel and Lin, 2002; Dorowand Widdows, 2003).
The size of the context win-dow also varies, it can be a relatively small, such astwo words before and after the target word (Gauchand Futrelle, 1993), the sentence within which thetarget is found (Bordag, 2006), or even larger, suchas the 20 surrounding words on either side of thetarget (Purandare and Pedersen, 2004).In essence, each instance of a target wordis represented as a feature vector which subse-quently serves as input to the chosen clusteringmethod.
A variety of clustering algorithms havebeen employed ranging from k-means (Purandareand Pedersen, 2004), to agglomerative clustering(Schu?tze, 1998), and the Information Bottleneck(Niu et al, 2007).
Graph-based methods have alsobeen applied to the sense induction task.
In thisframework words are represented as nodes in thegraph and vertices are drawn between the tar-get and its co-occurrences.
Senses are induced byidentifying highly dense subgraphs (hubs) in theco-occurrence graph (Ve?ronis, 2004; Dorow andWiddows, 2003).Although LDA was originally developed as agenerative topic model, it has recently gainedpopularity in the WSD literature.
The inferreddocument-level topics can help determine coarse-grained sense distinctions.
Cai et al (2007) pro-pose to use LDA?s word-topic distributions as fea-tures for training a supervised WSD system.
In asimilar vein, Boyd-Graber and Blei (2007) inferLDA topics from a large corpus, however for un-supervised WSD.
Here, LDA topics are integratedwith McCarthy et al?s (2004) algorithm.
For eachtarget word, a topic is sampled from the docu-ment?s topic distribution, and a word is generatedfrom that topic.
Also, a distributional neighbor isselected based on the topic and distributional sim-ilarity to the generated word.
Then, the word senseis selected based on the word, neighbor, and topic.Boyd-Graber et al (2007) extend the topic mod-eling framework to include WordNet senses as alatent variable in the word generation process.
Inthis case the model discovers both the topics ofthe corpus and the senses assigned to each of itswords.Our own model is also inspired by LDA but cru-cially performs word sense induction, not disam-biguation.
Unlike the work mentioned above, wedo not rely on a pre-existing list of senses, and donot assume a correspondence between our auto-matically derived sense-clusters and those of anygiven inventory.2 A key element in these previousattempts at adapting LDA forWSD is the tendencyto remain at a high level, document-like, setting.In contrast, we make use of much smaller unitsof text (a few sentences, rather than a full doc-ument), and create an individual model for each(ambiguous) word type.
Our induced senses arefew in number (typically less than ten).
This is inmarked contrast to tens, and sometimes hundreds,2Such a mapping is only performed to enable evaluationand comparison with other approaches (see Section 5).104of topics commonly used in document-modelingtasks.Unlike many conventional clustering meth-ods (e.g., Purandare and Pedersen 2004; Schu?tze1998), our model is probabilistic; it specifiesa probability distribution over possible values,which makes it easy to integrate and combine withother systems via mixture or product models.
Fur-thermore, the Bayesian framework allows the in-corporation of several information sources in aprincipled manner.
Our model can easily handle anarbitrary number of feature classes (e.g., parts ofspeech, dependencies).
This functionality in turnenables us to evaluate which linguistic informa-tion matters for the sense induction task.
Previousattempts to handle multiple information sourcesin the LDA framework (e.g., Griffiths et al 2005;Barnard et al 2003) have been task-specific andlimited to only two layers of information.
Ourmodel provides this utility in a general framework,and could be applied to other tasks, besides senseinduction.3 The Sense Induction ModelThe core idea behind sense induction is that con-textual information provides important cues re-garding a word?s meaning.
The idea dates back to(at least) Firth (1957) (?You shall know a word bythe company it keeps?
), and underlies most WSDand lexicon acquisition work to date.
Under thispremise, we should expect different senses to besignaled by different lexical distributions.We can place sense induction in a probabilis-tic setting by modeling the context words aroundthe ambiguous target as samples from a multino-mial sense distribution.
More formally, we willwrite P(s) for the distribution over senses s ofan ambiguous target in a specific context win-dow and P(w|s) for the probability distributionover context words w given sense s. Each word wiin the context window is generated by first sam-pling a sense from the sense distribution, thenchoosing a word from the sense-context distribu-tion.
P(si = j) denotes the probability that the jthsense was sampled for the ith word token andP(wi|si = j) the probability of context word wi un-der sense j.
The model thus specifies a distributionover words within a context window:P(wi) =S?j=1P(wi|si = j)P(si = j) (1)where S is the number of senses.
We assume thateach target word hasC contexts and each context c?
?
s w NcC?(?
)Figure 1: Bayesian sense induction model; shadednodes represent observed variables, unshadednodes indicate latent variables.
Arrows indi-cate conditional dependencies between variables,whereas plates (the rectangles in the figure) referto repetitions of sampling steps.
The variables inthe lower right corner refer to the number of sam-ples.consists of Nc word tokens.
We shall write ?
( j) as ashorthand for P(wi|si = j), the multinomial distri-bution over words for sense j, and ?
(c) as a short-hand for the distribution of senses in context c.Following Blei et al (2003) we will assume thatthe mixing proportion over senses ?
is drawn froma Dirichlet prior with parameters ?.
The role ofthe hyperparameter ?
is to create a smoothed sensedistribution.
We also place a symmetric Dirichlet ?on ?
(Griffiths and Steyvers, 2002).
The hyper-parmeter ?
can be interpreted as the prior observa-tion count on the number of times context wordsare sampled from a sense before any word fromthe corpus is observed.
Our model is representedin graphical notation in Figure 1.The model sketched above only takes word in-formation into account.
Methods developed for su-pervised WSD often use a variety of informationsources based not only on words but also on lem-mas, parts of speech, collocations and syntactic re-lationships (Lee and Ng, 2002).
The first idea thatcomes to mind, is to use the same model whiletreating various features as word-like elements.
Inother words, we could simply assume that the con-texts we wish to model are the union of all ourfeatures.
Although straightforward, this solutionis undesirable.
It merges the distributions of dis-tinct feature categories into a single one, and istherefore conceptually incorrect, and can affect theperformance of the model.
For instance, parts-of-speech (which have few values, and therefore highprobability), would share a distribution with words(which are much sparser).
Layers containing moreelements (e.g.
10 word window) would overwhelm105?
?s f Nc1Cs f Nc2...s f Ncn?1(?1)?2(?2)?n(?n)Figure 2: Extended sense induction model; innerrectangles represent different sources (layers) ofinformation.
All layers share the same, instance-specific, sense distribution (?
), but each have theirown (multinomial) sense-feature distribution (?
).Shaded nodes represent observed features f ; thesecan be words, parts of speech, collocations or de-pendencies.smaller ones (e.g.
1 word window).Our solution is to treat each information source(or feature type) individually and then combineall of them together in a unified model.
Our un-derlying assumption is that the context windowaround the target word can have multiple represen-tations, all of which share the same sense distribu-tion.We illustrate this in Figure 2 where each innerrectangle (layer) corresponds to a distinct featuretype.
We will naively assume independence be-tween multiple layers, even though this is clearlynot the case in our task.
The idea here is to modeleach layer as faithfully as possible to the empiricaldata while at the same time combining informationfrom all layers in estimating the sense distributionof each target instance.4 InferenceOur inference procedure is based on Gibbs sam-pling (Geman and Geman, 1984).
The procedurebegins by randomly initializing all unobservedrandom variables.
At each iteration, each randomvariable si is sampled from the conditional distri-bution P(si|s?i) where s?i refers to all variablesother than si.
Eventually, the distribution over sam-ples drawn from this process will converge to theunconditional joint distribution P(s) of the unob-served variables (provided certain criteria are ful-filled).In our model, each element in each layer is avariable, and is assigned a sense label (see Fig-ure 2, where distinct layers correspond to differ-ent representations of the context around the tar-get word).
From these assignments, we must de-termine the sense distribution of the instance as awhole.
This is the purpose of the Gibbs samplingprocedure.
Specifically, in order to derive the up-date function used in the Gibbs sampler, we mustprovide the conditional probability of the i-th vari-able being assigned sense si in layer l, given thefeature value fi of the context variable and the cur-rent sense assignments of all the other variables inthe data (s?i):p(si|s?i, f ) ?
p( fi|s, f?i,?)
?
p(si|s?i,?)
(2)The probability of a single sense assignment, si,is proportional to the product of the likelihood (offeature fi, given the rest of the data) and the priorprobability of the assignment.
(3)p( fi|s, f?i,?)
=Zp( fi|l,s,?)
?
p(?| f?i,?l)d?=#( fi,si)+?l#(si)+Vl ?
?lFor the likelihood term p( fi|s, f?i,?
), integratingover all possible values of the multinomial feature-sense distribution ?
gives us the rightmost term inEquation 3, which has an intuitive interpretation.The term #( fi,si) indicates the number of timesthe feature-value fi was assigned sense si in therest of the data.
Similarly, #(si) indicates the num-ber of times the sense assignment si was observedin the data.
?l is the Dirichlet prior for the feature-sense distribution ?
in the current layer l, and Vlis the size of the vocabulary of that layer, i.e., thenumber of possible feature values in the layer.
In-tuitively, the probability of a feature-value givena sense is directly proportional to the number oftimes we have seen that value and that sense-assignment together in the data, taking into ac-count a pseudo-count prior, expressed through ?.This can also be viewed as a form of smoothing.A similar approach is taken with regards to theprior probability p(si|s?i,?).
In this case, how-ever, all layers must be considered:p(si|s?i,?)
=?l?l ?
p(si|l,s?i,?l) (4)106Here ?l is the weight for the contribution of layer l,and ?l is the portion of the Dirichlet prior for thesense distribution ?
in the current layer.
Treatingeach layer individually, we integrate over the pos-sible values of ?, obtaining a similar count-basedterm:(5)p(si|l,s?i,?l) =Zp(si|l,s?i,?)
?
p(?| f?i,?l)d?=#l(si)+?l#l+S ?
?lwhere #l(si) indicates the number of elements inlayer l assigned the sense si, #l indicates the num-ber of elements in layer l, i.e., the size of the layerand S the number of senses.To distribute the pseudo counts represented by?
in a reasonable fashion among the layers, wedefine ?l = #l#m ??
where #m = ?l #l, i.e., the totalsize of the instance.
This distributes ?
accordingto the relative size of each layer in the instance.p(si|l,s?i,?l)=#l(si)+ #l#m ?
?#l+S ?
#l#m ?
?=#m ?
#l(si)#l +?#m+S ??
(6)Placing these values in Equation 4 we obtain thefollowing:p(si|s?i,?)
=#m ?
?l ?l ?#l(si)#l +?#m+S ??
(7)Putting it all together, we arrive at the final updateequation for the Gibbs sampling:p(si|s?i, f )?#( fi,si)+?l#(si)+Vl ?
?l?#m ?
?l ?l ?#l(si)#l +?#m+S ??
(8)Note that when dealing with a single layer, Equa-tion 8 collapses to:p(si|s?i, f ) ?#( fi,si)+?#(si)+V ??
?#m(si)+?#m+S ??
(9)where #m(si) indicates the number of elements(e.g., words) in the context window assigned tosense si.
This is identical to the update equationin the original, word-based LDA model.The sampling algorithm gives direct estimatesof s for every context element.
However, in viewof our task, we are more interested in estimating ?,the sense-context distribution which can be ob-tained as in Equation 7, but taking into accountall sense assignments, without removing assign-ment i.
Our system labels each instance with thesingle, most probable sense.5 Evaluation SetupIn this section we discuss our experimental set-upfor assessing the performance of the model pre-sented above.
We give details on our training pro-cedure, describe our features, and explain how oursystem output was evaluated.Data In this work, we focus solely on inducingsenses for nouns, since they constitute the largestportion of content words.
For example, nouns rep-resent 45% of the content words in the British Na-tional Corpus.
Moreover, for many tasks and ap-plications (e.g., web queries, Jansen et al 2000)nouns are the most frequent and most importantpart-of-speech.For evaluation, we used the Semeval-2007benchmark dataset released as part of the senseinduction and discrimination task (Agirre andSoroa, 2007).
The dataset contains texts from thePenn Treebank II corpus, a collection of articlesfrom the first half of the 1989 Wall Street Jour-nal (WSJ).
It is hand-annotated with OntoNotessenses (Hovy et al, 2006) and has 35 nouns.
Theaverage noun ambiguity is 3.9, with a high (almost80%) skew towards the predominant sense.
This isnot entirely surprising since OntoNotes senses areless fine-grained than WordNet senses.We used two corpora for training as we wantedto evaluate our model?s performance across differ-ent domains.
The British National Corpus (BNC)is a 100 million word collection of samples ofwritten and spoken language from a wide range ofsources including newspapers, magazines, books(both academic and fiction), letters, and school es-says as well as spontaneous conversations.
Thisserved as our out-of-domain corpus, and con-tained approximately 730 thousand instances ofthe 35 target nouns in the Semeval lexical sample.The second, in-domain, corpus was built from se-lected portions of the Wall Street Journal.
We usedall articles (excluding the Penn Treebank II por-tion used in the Semeval dataset) from the years1987-89 and 1994 to create a corpus of similar sizeto the BNC, containing approximately 740 thou-sand instances of the target words.Additionally, we used the Senseval 2 and 3 lex-ical sample data (Preiss and Yarowsky, 2001; Mi-halcea and Edmonds, 2004) as development sets,for experimenting with the hyper-parameters ofour model (see Section 6).Evaluation Methodology Agirre and Soroa(2007) present two evaluation schemes for as-sessing sense induction methods.
Under the first107scheme, the system output is compared to thegold standard using standard clustering evalua-tion metrics (e.g., purity, entropy).
Here, no at-tempt is made to match the induced senses againstthe labels of the gold standard.
Under the secondscheme, the gold standard is partitioned into a testand training corpus.
The latter is used to derive amapping of the induced senses to the gold stan-dard labels.
The mapping is then used to calculatethe system?s F-Score on the test corpus.Unfortunately, the first scheme failed to dis-criminate among participating systems.
The one-cluster-per-word baseline outperformed all sys-tems, except one, which was only marginally bet-ter.
The scheme ignores the actual labeling anddue to the dominance of the first sense in the data,encourages a single-sense approach which is fur-ther amplified by the use of a coarse-grained senseinventory.
For the purposes of this work, there-fore, we focused on the second evaluation scheme.Here, most of the participating systems outper-formed the most-frequent-sense baseline, and therest obtained only slightly lower scores.Feature Space Our experiments used a featureset designed to capture both immediate local con-text, wider context and syntactic context.
Specifi-cally, we experimented with six feature categories:?10-word window (10w),?5-word window (5w),collocations (1w), word n-grams (ng), part-of-speech n-grams (pg) and dependency relations(dp).
These features have been widely adopted invarious WSD algorithms (see Lee and Ng 2002 fora detailed evaluation).
In all cases, we use the lem-matized version of the word(s).The Semeval workshop organizers provided asmall amount of context for each instance (usu-ally a sentence or two surrounding the sentencecontaining the target word).
This context, as wellas the text in the training corpora, was parsed us-ing RASP (Briscoe and Carroll, 2002), to extractpart-of-speech tags, lemmas, and dependency in-formation.
For instances containing more than oneoccurrence of the target word, we disambiguatethe first occurrence.
Instances which were not cor-rectly recognized by the parser (e.g., a target wordlabeled with the wrong lemma or part-of-speech),were automatically assigned to the largest sense-cluster.33This was the case for less than 1% of the instances.3 4 5 6 7 8 9Number of Senses838485868788F-Score(%)In-Domain (WSJ)Out-of-Domain (BNC)Figure 3: Model performance with varying num-ber of senses on the WSJ and BNC corpora.6 ExperimentsModel Selection The framework presented inSection 3 affords great flexibility in modeling theempirical data.
This however entails that severalparameters must be instantiated.
More precisely,our model is conditioned on the Dirichlet hyper-parameters ?
and ?
and the number of senses S.Additional parameters include the number of iter-ations for the Gibbs sampler and whether or notthe layers are assigned different weights.Our strategy in this paper is to fix ?
and ?and explore the consequences of varying S. Thevalue for the ?
hyperparameter was set to 0.02.This was optimized in an independent tuning ex-periment which used the Senseval 2 (Preiss andYarowsky, 2001) and Senseval 3 (Mihalcea andEdmonds, 2004) datasets.
We experimented with?
values ranging from 0.005 to 1.
The ?
parame-ter was set to 0.1 (in all layers).
This value is oftenconsidered optimal in LDA-related models (Grif-fiths and Steyvers, 2002).
For simplicity, we useduniform weights for the layers.
The Gibbs samplerwas run for 2,000 iterations.
Due to the random-ized nature of the inference procedure, all reportedresults are average scores over ten runs.Our experiments used the same number ofsenses for all the words, since tuning this numberindividually for each word would be prohibitive.We experimented with values ranging from threeto nine senses.
Figure 3 shows the results obtainedfor different numbers of senses when the model istrained on the WSJ (in-domain) and BNC (out-of-domain) corpora, respectively.
Here, we are usingthe optimal combination of layers for each system(which we discuss in the following section in de-108Senses of drug (WSJ)1.
U.S., administration, federal, against, war, dealer2.
patient, people, problem, doctor, company, abuse3.
company, million, sale, maker, stock, inc.4.
administration, food, company, approval, FDASenses of drug (BNC)1. patient, treatment, effect, anti-inflammatory2.
alcohol, treatment, patient, therapy, addiction3.
patient, new, find, effect, choice, study4.
test, alcohol, patient, abuse, people, crime5.
trafficking, trafficker, charge, use, problem6.
abuse, against, problem, treatment, alcohol7.
people, wonder, find, prescription, drink, addict8.
company, dealer, police, enforcement, patientTable 1: Senses inferred for the word drug fromthe WSJ and BNC corpora.tail).
For the model trained on WSJ, performancepeaks at four senses, which is similar to the av-erage ambiguity in the test data.
For the modeltrained on the BNC, however, the best results areobtained using twice as many senses.
Using fewersenses with the BNC-trained system can result ina drop in accuracy of almost 2%.
This is due tothe shift in domain.
As the sense-divisions of thelearning domain do not match those of the targetdomain, finer granularity is required in order to en-compass all the relevant distinctions.Table 1 illustrates the senses inferred for theword drug when using the in-domain and out-of-domain corpora, respectively.
The most probablewords for each sense are also shown.
Firstly, notethat the model infers some plausible senses fordrug on the WSJ corpus (top half of Table 1).Sense 1 corresponds to the ?enforcement?
senseof drug, Sense 2 refers to ?medication?, Sense 3to the ?drug industry?
and Sense 4 to ?drugs re-search?.
The inferred senses for drug on the BNC(bottom half of Table 1) are more fine grained.
Forexample, the model finds distinct senses for ?med-ication?
(Sense 1 and 7) and ?illegal substance?
(Senses 2, 4, 6, 7).
It also finds a separate sensefor ?drug dealing?
(Sense 5) and ?enforcement?
(Sense 8).
Because the BNC has a broader fo-cus, finer distinctions are needed to cover as manysenses as possible that are relevant to the target do-main (WSJ).Layer Analysis We next examine which indi-vidual feature categories are most informativein our sense induction task.
We also investigatewhether their combination, through our layered1-Layer10w 86.95w 86.81w 84.6ng 83.6pg 82.5dp 82.2MFS 80.95-Layers-10w 83.1-5w 83.0-1w 83.0-ng 83.0-pg 82.7-dp 84.7all 83.3Combination10w+5w 87.3%5w+pg 83.9%1w+ng 83.2%10w+pg 83.3%1w+pg 84.5%10w+pg+dep 82.2%MFS 80.9%Table 2: Model performance (F-score) on the WSJwith one layer (left), five layers (middle), and se-lected combinations of layers (right).model (see Figure 2), yields performance im-provements.
We used 4 senses for the systemtrained on WSJ and 8 for the system trained onthe BNC (?
was set to 0.02 and ?
to 0.1)Table 2 (left side) shows the performance of ourmodel when using only one layer.
The layer com-posed of words co-occurring within a ?10-wordwindow (10w), and representing wider, topical, in-formation gives the highest scores on its own.
Itis followed by the ?5 (5w) and ?1 (1w) wordwindows, which represent more immediate, localcontext.
Part-of-speech n-grams (pg) and word n-grams (ng), on their own, achieve lower scores,largely due to over-generalization and data sparse-ness, respectively.
The lowest-scoring single layeris the dependency layer (dp), with performanceonly slightly above the most-frequent-sense base-line (MFS).
Dependency information is very infor-mative when present, but extremely sparse.Table 2 (middle) also shows the results obtainedwhen running the layered model with all but oneof the layers as input.
We can use this informa-tion to determine the contribution of each layer bycomparing to the combined model with all layers(all).
Because we are dealing with multiple lay-ers, there is an element of overlap involved.
There-fore, each of the word-window layers, despite rel-atively high informativeness on its own, does notcause as much damage when it is absent, sincethe other layers compensate for the topical and lo-cal information.
The absence of the word n-gramlayer, which provides specific local information,does not make a great impact when the 1w and pglayers are present.
Finally, we can see that the ex-tremely sparse dependency layer is detrimental tothe multi-layer model as a whole, and its removalincreases performance.
The sparsity of the data inthis layer means that there is often little informa-tion on which to base a decision.
In these cases,the layer contributes a close-to-uniform estimation1091-Layer10w 84.65w 84.61w 83.6pg 83.1ng 82.8dp 81.1MFS 80.95-Layers-10w 83.3-5w 82.8-1w 83.5-pg 83.2-ng 82.9-dp 84.7all 84.1Combination10w+5w 85.5%5w+pg 83.5%1w+ng 83.5%10w+pg 83.4%1w+pg 84.1%10w+pg+dep 81.7%MFS 80.9%Table 3: Model performance (F-score) on the BNCwith one layer (left), five layers (middle), and se-lected combinations of layers (right).of the sense distribution, which confuses the com-bined model.Other layer combinations obtained similar re-sults.
Table 2 (right side) shows the most informa-tive two and three layer combinations.
Again, de-pendencies tend to decrease performance.
On theother hand, combining features that have similarperformance on their own is beneficial.
We obtainthe best performance overall with a two layeredmodel combining topical (+10w) and local (+5w)contexts.Table 3 replicates the same suite of experimentson the BNC corpus.
The general trends are similar.Some interesting differences are apparent, how-ever.
The sparser layers, notably word n-gramsand dependencies, fare comparatively worse.
Thisis expected, since the more precise, local, infor-mation is likely to vary strongly across domains.Even when both domains refer to the same senseof a word, it is likely to be used in a differentimmediate context, and local contextual informa-tion learned in one domain will be less effectivein the other.
Another observable difference is thatthe combined model without the dependency layerdoes slightly better than each of the single layers.The 1w+pg combination improves over its compo-nents, which have similar individual performance.Finally, the best performing model on the BNCalso combines two layers capturing wider (10w)and more local (5w) contextual information (seeTable 3, right side).Comparison to State-of-the-Art Table 4 com-pares our model against the two best performingsense induction systems that participated in theSemeval-2007 competition.
IR2 (Niu et al, 2007)performed sense induction using the InformationBottleneck algorithm, whereas UMND2 (Peder-sen, 2007) used k-means to cluster second orderco-occurrence vectors associated with the targetSystem F-Score10w, 5w (WSJ) 87.3I2R 86.8UMND2 84.5MFS 80.9Table 4: Comparison of the best-performingSemeval-07 systems against our model.word.
These models and our own model signif-icantly outperform the most-frequent-sense base-line (p < 0.01 using a ?2 test).
Our best sys-tem (10w+5w on WSJ) is significantly better thanUMND2 (p < 0.01) and quantitatively better thanIR2, although the difference is not statistically sig-nificant.7 DiscussionThis paper presents a novel Bayesian approach tosense induction.
We formulated sense inductionin a generative framework that describes how thecontexts surrounding an ambiguous word mightbe generated on the basis of latent variables.
Ourmodel incorporates features based on lexical in-formation, parts of speech, and dependencies in aprincipled manner, and outperforms state-of-the-art systems.
Crucially, the approach is not specificto the sense induction task and can be adapted forother applications where it is desirable to take mul-tiple levels of information into account.
For exam-ple, in document classification, one could consideran accompanying image and its caption as possi-ble additional layers to the main text.In the future, we hope to explore more rigor-ous parameter estimation techniques.
Goldwaterand Griffiths (2007) describe a method for inte-grating hyperparameter estimation into the Gibbssampling procedure using a prior over possiblevalues.
Such an approach could be adopted in ourframework, as well, and extended to include thelayer weighting parameters, which have strong po-tential for improving the model?s performance.
Inaddition, we could allow an infinite number ofsenses and use an infinite Dirichlet model (Tehet al, 2006) to automatically determine how manysenses are optimal.
This provides an elegant so-lution to the model-order problem, and eliminatesthe need for external cluster-validation methods.Acknowledgments The authors acknowledgethe support of EPSRC (grant EP/C538447/1).We are grateful to Sharon Goldwater for her feed-back on earlier versions of this work.110ReferencesAgirre, Eneko, Llu?
?s Ma`rquez, and Richard Wicentowski, ed-itors.
2007.
Proceedings of the SemEval-2007.
Prague,Czech Republic.Agirre, Eneko and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discriminationsystems.
In Proceedings of SemEval-2007.
Prague, CzechRepublic, pages 7?12.Barnard, K., P. Duygulu, D. Forsyth, N. De Freitas, D. M.Blei, andM.
I. Jordan.
2003.
Matching words and pictures.J.
of Machine Learning Research 3(6):1107?1135.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003.Latent dirichlet alocation.
Journal of Machine LearningResearch 3:993?1022.Bordag, Stefan.
2006.
Word sense induction: Triplet-basedclustering and automatic evaluation.
In Proceedings of the11th EACL.
Trento, Italy, pages 137?144.Boyd-Graber, Jordan and David Blei.
2007.
Putop: Turningpredominant senses into a topic model for word sense dis-ambiguation.
In Proceedings of SemEval-2007.
Prague,Czech Republic, pages 277?281.Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu.
2007.A topic model for word sense disambiguation.
In Pro-ceedings of the EMNLP-CoNLL.
Prague, Czech Republic,pages 1024?1033.Briscoe, Ted and John Carroll.
2002.
Robust accurate statis-tical annotation of general text.
In Proceedings of the 3rdLREC.
Las Palmas, Gran Canaria, pages 1499?1504.Cai, J. F., W. S. Lee, and Y. W. Teh.
2007.
Improving wordsense disambiguation using topic features.
In Proceedingsof the EMNLP-CoNLL.
Prague, Czech Republic, pages1015?1023.Carpuat, Marine and Dekai Wu.
2005.
Word sense disam-biguation vs. statistical machine translation.
In Proceed-ings of the 43rd ACL.
Ann Arbor, MI, pages 387?394.Dorow, Beate and Dominic Widdows.
2003.
Discoveringcorpus-specific word senses.
In Proceedings of the 10thEACL.
Budapest, Hungary, pages 79?82.Firth, J. R. 1957.
A Synopsis of Linguistic Theory 1930-1955.Oxford: Philological Society.Gauch, Susan and Robert P. Futrelle.
1993.
Experiments inautomatic word class and word sense identification for in-formation retrieval.
In Proceedings of the 3rd Annual Sym-posium on Document Analysis and Information Retrieval.Las Vegas, NV, pages 425?434.Geman, S. and D. Geman.
1984.
Stochastic relaxation, Gibbsdistribution, and Bayesian restoration of images.
IEEETransactions on Pattern Analysis and Machine Intelli-gence 6(6):721?741.Goldwater, Sharon and Tom Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
In Pro-ceedings of the 45th ACL.
Prague, Czech Republic, pages744?751.Griffiths, Thomas L., Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics and syn-tax.
In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,editors, Advances in Neural Information Processing Sys-tems 17, MIT Press, Cambridge, MA, pages 537?544.Griffiths, Tom L. and Mark Steyvers.
2002.
A probabilisticapproach to semantic representation.
In Proeedings of the24th Annual Conference of the Cognitive Science Society.Fairfax, VA, pages 381?386.Hovy, Eduard, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes: The90% solution.
In Proceedings of the HLT, Companion Vol-ume: Short Papers.
Association for Computational Lin-guistics, New York City, USA, pages 57?60.Jansen, B. J., A. Spink, and A. Pfaff.
2000.
Linguistic aspectsof web queries.Lee, Yoong Keok and Hwee Tou Ng.
2002.
An empiricalevaluation of knowledge sources and learning algorithmsfor word sense disambiguation.
In Proceedings of theEMNLP.
Morristown, NJ, USA, pages 41?48.McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-roll.
2004.
Finding predominant senses in untagged text.In Proceedings of the 42nd ACL.
Barcelona, Spain, pages280?287.Mihalcea, Rada and Phil Edmonds, editors.
2004.
Proceed-ings of the SENSEVAL-3.
Barcelona.Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan.
2007.I2r: Three systems for word sense discrimination, chineseword sense disambiguation, and english word sense dis-ambiguation.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007).
As-sociation for Computational Linguistics, Prague, CzechRepublic, pages 177?182.Pantel, Patrick and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of the 8th KDD.
NewYork, NY, pages 613?619.Pedersen, Ted.
2007.
Umnd2 : Senseclusters applied to thesense induction task of senseval-4.
In Proceedings ofSemEval-2007.
Prague, Czech Republic, pages 394?397.Preiss, Judita and David Yarowsky, editors.
2001.
Proceed-ings of the 2nd International Workshop on EvaluatingWord Sense Disambiguation Systems.
Toulouse, France.Purandare, Amruta and Ted Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and similarityspaces.
In Proceedings of the CoNLL.
Boston, MA, pages41?48.Schu?tze, Hinrich.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics 24(1):97?123.Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the AmericanStatistical Association 101(476):1566?1581.Ve?ronis, Jean.
2004.
Hyperlex: lexical cartography forinformation retrieval.
Computer Speech & Language18(3):223?252.Vickrey, David, Luke Biewald, Marc Teyssier, and DaphneKoller.
2005.
Word-sense disambiguation for machinetranslation.
In Proceedings of the HLT/EMNLP.
Vancou-ver, pages 771?778.Voorhees, Ellen M. 1993.
Using wordnet to disambiguateword senses for text retrieval.
In Proceedings of the 16thSIGIR.
New York, NY, pages 171?180.111
