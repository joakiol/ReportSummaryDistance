Combination of Symbolic and Statistical Approaches forGrammatical Knowledge AcquisitionMasak i  K IYONO* and Jun ' i ch i  TSUJ I ICentre for Computat iona l  LinguisticsUnivers i ty of Manchester  Inst i tute  of Science and TechnologyPO Box 88, Manchester ,  M60 1QD, Uni ted K ingdomkiyono?ccl .umist.
ac.uk, tsuj ii~ccl .umist.
ac .ukAbst rac tThe framework we adopted for customiz-ing linguistic knowledge to individual ap-plication domains is an integration of sym-bolic and statistical approaches.
In or-der to acquire domain specific knowledge,we have previously proposed a rule-basedmechanism to hypothesize missing knowl-edge from partial parsing results of unsuc-cessfully parsed sentences.
In this paper,we focus on the statistical process which se-lects plausible knowledge from a set of hy-potheses generated from the whole corpus.In particular, we introduce two statisticalmeasures of hypotheses, Local Plausibilityand Global Plausibility, and describe howthese measures are determined iteratively.The proposed method will be incorporatedinto the tool kit for linguistic knowledgeacquisition which we are now developing.1 I n t roduct ionCurrent technologies in natural language process-ing are not so mature as to make general purposesystems applicable to any domains; therefore rapidcustomization of linguistic knowledge to the sub-language of an application domain is vital for thedevelopment of practical systems.
In the currentlyworking systems, such customization has been car-ried out manually by linguists or lexicographers withtime-consuming effort.We have already proposed a mechanism whichacquires sublanguage-specific linguistic knowledgefrom parsing failures and which can be used as atool for linguistic knowledge customization (Kiyonoand Tsujii, 1993; Kiyono and Tsujii, 1994).
Our ap-proach is characterized bya mixture of symbolic andstatistical approaches togrammatical knowledge ac-quisition.
Unlike probabilistic parsing, proposed by(Fujisaki et al, 1989; Briscoe and Carroll, 1993),*also a staff member of Matsushita Electric IndustrialCo.,Ltd., Shinagawa, Tokyo, JAPAN.72which assumes the prior existence of comprehensivelinguistic knowledge, our system can suggest newpieces of knowledge including CFG rules, subcate-gorization frames, and other lexical features.
It alsodiffers from previous proposals on lexical acquisi-tion using statistical measures uch as (Church etal., 1991; Brent, 1991; Brown et al, 1993) which ei-ther deny the prior existence of linguistic knowledgeor use linguistic knowledge in ad hoc ways.Our system consists of two components: (1) therule-based component, which detects incompletenessof the existing knowledge and generates a set of hy-potheses of new knowledge and (2) the corpus-basedcomponent which selects plausible hypotheses on thebasis of their statistical behaviour.
As the rule-basedcomponent has been explained in our previous pa-pers, in this paper we focus on the corpus-basedcomponent.After giving a brief explanation of the framework,we describe adata structure called Hypothesis Graphwhich plays a crucial role in the corpus-based pro-cess, and then introduce two statistical measures ofhypotheses, Global Plausibility and Local Plausibil-ity, which are iteratively determined to select a setof plausible hypotheses.
An experiment which showsthe effectiveness of our method is also given.2 The  System Organ izat ion2.1 Hypothesis GenerationFigure 1 shows the framework of our system.
Whenthe parser fails to analyse a sentence, the Hypothe-sis Generator (HG) produces hypotheses of missingknowledge ach of which could rectify the defects ofthe current grammar.
As the parser is a sort of ChartParser and maintains partial parsing results in theform of inactive and active edges, a parsing failuremeans that no inactive dge of category S spanningthe whole sentence xists.The HG tries to introduce an inactive dge of S bymaking hypotheses of missing linguistic knowledge.It generates hypotheses of rewriting rules which col-lect existing sequences of inactive edges into an ex-pected category.
It also calls itself recursively to in-SentenceI Parser( Parsing Resu l t )Generator( Hypotheses- Corpus )- -   Grammar )Human t lnteractionPlausibleHypotheses )?HypothesisSelectorHypothesis DB)Rule-Based Component Corpus-Based ComponentFigure 1: Framework of Grammar Acquisitiontroduce necessary inactive edges for each rule of theexpected category whose application is preventeddue to the lack of necessary inactive edges.
Thesimplest form of the algorithm is shown below.\ [A lgor i thm\]  An inactive edge \[ie(A) : xo, xn\] canbe introduced, with label A, between word po-sitions x0 and xn by each of the hypotheses gen-erated from the following two steps.\ [Step 1\] For each sequence of inactive edges,\[~e(Bl) : x0, Xl\], .
.
.
,  \ [ ie(Bn):  Xn-l,Xn\],spanning from x0 to Xn, generates a newrule.A ==~ B1, .
- .
,B ,\ [Step 2\] For each existing rule of form A ::VA1, ?
?
-, An, finds an incomplete sequenceof inactive edges, \[ie(A1) : xo, xl\], .
.
.
,\[ie(A~_l) : x~-2, xi-1\], \[ie(Ai+l) : xi, xi+l\],.
.
.
,  \[ie(An) : xn-1,  xn\], and calls this algo-rithm for \[ie(Ai) : xi-1, xi\].This algorithm has been further augmented in or-der to treat sentences which contain more than oneconstruction ot covered by the current version ofthe grammar and to generate hypotheses concern-ing complex features like subcategorization frames.2.2 Hypothes is  F i l te r ingThe greater number of the hypotheses generated bythe algorithm are linguistically unnatural, becausethe algorithm does not embody any linguistic prin-ciple to judge the appropriateness of hypotheses, andtherefore we introduced a set of criteria to filter outunnatural hypotheses (Kiyono and Tsujii, 1993; Kiy-ono and Tsujii, 1994).
This includes, for example,?
The maximum number of daughter constituentsof a rule is set to 3.?
Supposing that the current version of the gram-mar contains all the category conversion rules,a unary rule with one daughter constituent isnot generated.?
Using generalizations embodied in the currentversion of the grammar, a rule containing a se-quence of constituents which can be collectedinto a larger constituent by the current versionof grammar is not generated.?
Distinguishing non-lexical categories from lexi-cal categories, a rule whose mother category isa lexical category is not generated.These criteria significantly reduce the number ofhypotheses to be generated.2.3 Hypothes is  GraphAs the criteria which the HG uses to filter out un-natural hypotheses are solely based on the forms ofhypotheses, they cannot identify the "correct" hy-potheses on their own.
The correct ones are ratherchosen by the Hypothesis Selector (HS), which re-sorts to examining the statistical behaviour of hy-potheses throughout a given corpus.A straightforward method is to count the fre-quency of hypotheses, but this simple method doesnot work, because hypotheses are not independent ofeach other.
A hypothesis i either competing with orcomplemenlary to other hypotheses generated fromthe same sentence.
A group of hypotheses generatedfor restoring the same inactive edge constitutes a setof competing hypotheses and only one of them con-tributes to the correct structure of the sentence.
Onthe other hand, two groups of hypotheses which aregenerated to treat two different parts of the samesentence stand in complementary elationships.A hypothesis hould be recognized as being cor-rect, only when no other competing hypothesis ismore plausible.
That is, even if a hypothesis i  gen-erated frequently, it should not be chosen as thecorrect one, if more plausible competing hypothe-ses are always generated together with it.
On theother hand, even if a hypothesis is generated onlyonce, it should be chosen as the correct one, if thereis no other competing hypothesis.In order to realize the above conception, the HSmaintains mutual relationships among hypothesesas an AND-OR graph.
In a graph, AND nodesand OR nodes express complementary relationshipsand competing relationships, respectively.
A node isshared, when different recursion steps in the HG tryto restore the same inactive dge.
Figure 2 shows theAND-OR graph for the hypotheses generated fromthe sentence '~Failing students looked embarrassed"when the current version of grammar does not con-tain rules for participles.
The top node is an ANDnode which has two groups of hypotheses that treattwo different parts of the sentence, i.e.
"failing stu-dents" and "looked embarrased".73Sentence: Failing students looked embarrassed.HPI: NP =~ VP, NP ("failing students")HP2: ADJ =?, \[failing\]HP3: VP ~ VP, VP ("looked embarrassed")HP4: ADV ~ \[embarrassed\]HP5: N ~ \[embarrassed\]HP6: ADJ ~ \[embarrassed\]Figure 2: AND-OR Graph of Hypotheses3 S ta t i s t i ca l  Ana lys i s3.1 Two Measures of  Plausibi l i tyThe HS uses two measures of plausibility of hypothe-ses.
One is computed for an instance hypothesis andthe other is for a generic hypothesis.
(See 3.3 for therelationship between the two types of hypotheses.
)(1) Local Plausibi l i ty: This value shows howplausible an instance hypothesis  as grammat-ical knowledge to contribute to the correct anal-ysis of a unsuccessfully parsed sentence.
(2) Global Plausibi l i ty: This value shows howplausible the hypothesis of the generic form isas grammatical knowledge to be acquired.As we describe in the following section, the Lo-cal Plausibility (LP) of an instance hypothesis iscomputed on the basis of the values of the GlobalPlausibility (GP) of the generic hyoptheses whichare linked to instance hypotheses in the same hy-pothesis graph.
On the other hand, the GP of ageneric hypothesis i computed from the LP valuesof its instance hypotheses across the whole corpus.Intuitively speaking, the GP of a generic hypoth-esis is high if its instances are frequently generatedand if they receive high LP values, while the LP ofa instance hypothesis i high if the GP of the cor-responding eneric hypothesis high and if the GPvalues of the generic hypotheses corresponding toitscompeting hypotheses are low.
Because of this mu-tual dependence between LP and GP, they cannotbe computed in a single step but rather computediteratively by repeating the following steps until thehalt condition is satisfied.\[Step 1\] Estimates the initial values of LP.\[Step 2\] Calculates GP values from LP values.\[Step 3\] Checks the halt condition.\[Step 4\] Calculates LP values from GP values andGOT0 \[Step 2\].3.2 Initial Est imat ion of Local P lausibi l i tyIf the current version of the grammar is reason-ably comprehensive, pieces of linguistic knowledgewhich have to be acquired are likely to be lex-ical or idiosyncratic.
That is, we assume thatsublanguage-specificity tends to be manifested byunknown words, new usages of existing words, andsyntactic onstructions idiosyncratic to the sublan-guage.
In order to quantify such plausibility, thefollowing value is given to each hypothesis.W(Hypoi) ?
H(Hypoi)iP(Hypoi)  = 1 -  W(S) x H(S)This value shows the proportion of the syntacticstructure in the whole sentence which is not coveredby the hypothesis.
It ranges from 0 to 1 and getslarger if the hypothesis rectifies a smaller part ofthe sentence.
W(Hypol), the width of the hypothe-sis, is defined as the word count of the subtree andH(Hypoi), the height, is defined as the shortest pathfrom lexical nodes to the top node of the subtree.3.3 Generic Hypothes is  and GlobalPlausibi l i tyThe GP of a hypothesis i computed based on theLP values of its instance hypotheses, but the rela-tionship between a generic hypothesis and its in-stances is not straightforward because we adopteda unification-based grammar formalism.
For exam-ple, the instance hypothesis of NP =:~ VP, NP  inFigure 2 contains not only this CFG skeleton butalso further feature descriptions of the three con-stituents which include specific surface words like"failing" and "students".
Unless we generalize them,we cannot obtain the generic form of this instancehypothesis, and therefore cannot judge whether thehypotheses generated from different sentences areidentical.Such generalization of instance hypotheses re-quires an inductive mechanism for judging whichparts of the feature specification are common to allinstance hypotheses and should be included in a hy-pothesis of the generic form.
This kind of induc-tion is beyond the scope of the current framework,because such induction may need a lot of time andspace if it is carried out from scratch.
We first gathera set of instance hypotheses which are likely to beinstances of the same generic hypothesis which, inturn, is likely to be "correct" linguistic knowledge.Our current framework uses a simple definition ofgeneric hypotheses and their instances.
That is, iftwo rule hypotheses have the same CFG skeleton,then they are judged to be instances of the samegeneric hypotheses.
As for lexical hypotheses, weuse a set of fixed templates of lexical entries in or-der to acquire detailed knowledge like subcategoriza-tion frames.
Features which are not included in thetemplates are ignored in the judgement of whethergeneric hypotheses are identical.74TOPAN(HP2 J  (HP3 J  (.HP0.05 0.1 0.9(a) Collection of Global Plausibility,)0.45TOPAN:(HP2 J  (HP3)  v.oo (HP0.06)D0.380.380.94(b) Distribution of Local PlausibilityFigure 3: Calculation of Local PlausibilityThe GP of a generic hypotheses i  defined as beingthe probability of the event hat at least one instancehypothesis recovers the true cause of a parsing fail-ure, and it is computed by the following formulawhen a set of its instance hypotheses i identified.In the formula, HP is a generic hypothesis and HPiare its instances.nGP(HP) = 1 - H(1  - LP(HPi))i=1The more instance hypotheses are generated, thecloser to 1 GP(HP) becomes.
If one of the instancesis regarded to be recovering the true cause of a pars-ing failure, the GP of the generic hypothesis is as-signed 1, because the hypothesis i indispensable tothe analysis of the corpus.3.4 Loca l  P laus ib i l i tyThe calculation of LP is carried out on each hypoth-esis graph based on the assumption that an instancehypothesis or a set of instance hypotheses which re-covers the true cause(s) of the parsing failure shouldexist in the graph.
This assumption means that thetop node of a hypothesis graph is assigned 1 as itsLP value.The LP value assigned to a node is to be dis-tributed to its daughter nodes by considering theGP values of the corresponding generic hypotheses.For example, the daughter nodes of an OR node,which constitute a set of competing hypotheses, re-ceive their LP values which are dividents of the LPvalue of the mother node proportional to their GPvalues.However, as GP is defined only for hypotheses,we first determine the GP values of all nodes in ahypothesis graph in a bottom-up manner, startingfrom the tip nodes of the graph to which instancehypotheses are attached.
Therefore, \[Step 2\] in thestatistical analysis is further divided into the follow-ing three steps.\[Step 2-1\] Bot tom-up Ca lcu la t ion  o f  GPThe GP value of an intermediate node is deter-mined as follows (See Figure 3(a)).?
The GP value of an OR node is computed by thefollowing formula based on the GP values of thedaughter nodes, which corresponds to the prob-ability that at least one of the daughter nodesrepresents "correct" grammatical knowledge.mGP(OR) = 1 - H(1  - GP(Nodei))i=1?
The GP value of an AND node is computed bythe following formula, which corresponds to theprobability that all the daughter nodes repre-sent "correct" grammatical knowledge.mGP(AND) = H GP(Nodei)i=1\[Step 2-2\] De le t ion  of  HypothesesThe nodes which have significantly smaller GPvalues than the highest one among the daughternodes of the same mother OR node (less than onetenth, in our current implementation) will be re-moved from the hypothesis graph.
For example,HP2 in Figure 3 was considered to be much less plau-sible than HP4 and removed from the graph.As a node in a hypothesis graph could have morethan one mother nodes, the hypothesis deletion isrealized by removing the link between the node rep-resenting the hypothesis and one of its mother ORnodes (not removing the node itself).
For example,in Figure 3, when HP4 is removed in comparisonwith HP2 or HP3, the link between HP4 and theOR node is removed, while the link between HP4and the AND node still remains.The deletion of less viable nodes accelerates theconvergence of the iterative process of computingGP and LP.75\ [Step 2-3\] Top-down Calculation of LPThis step distributes the LP assigned to the topnode (that is, 1) to the nodes below in a top-down way according to the following rules (See Fig-ure 3(b)).?
The LP value of an OR node is distributed to itsdaughter nodes proportional to their GP valuesso that the sum of their LP values is the sameas that of the OR node because the daughternodes of the same OR node represent mutuallyexclusive hypotheses.GP(Nodei) L " LP(gode,) = ~.~.---~l e j )  P(OR)?
The LP value of an AND node is distributed toits daughter nodes with the same values.LP(godei) = LP(AND)If a hypothesis has more than one mother nodesand its LP can be calculated through several paths,the sum of those is given to the hypothesis.
Forexample, the value for HP4 in Figure 3 is 0.56 +0.38 = 0.94.As we discussed before, these newly computed LPvalues are used to compute the GP values at \[Step2\] in the next cycle of iteration.3.5 Halt ConditionThe iterative calculation process is regarded to haveconverged if the GP values of all the generic hypothe-ses do not change in comparison with the previouscycle, but as it possibly takes a lot of time for theprocess to reach such a situation, we use an easiercondition to stop the process.
That is, we countthe number of deleted instance hypotheses at eachcycle and terminate the iteration when no instancehypothesis is deleted in a number of consecutive it-erations.
Actually, the process halts after 5 zero-deletion cycles in our current implementation.When the interative process terminates, the hy-potheses with high GP values are presented as thefinal candidates of new knowledge to be added tothe current version of grammar.4 Pre l iminary  Exper imentIn order to demonstrate how the HS works, we car-ried out a preliminary experiment with 1,000 sen-tences in the UNIX on-line manual (approximatelyone fifth of the whole manual).
As the initial knowl-edge for the experiment, we prepared a grammar setwhich contains 120 rules covering English basic ex-pressions and deliberately removed rules for partici-ples in order to check whether the HS can discoveradequate rules.
The input data to the statistical pro-cess is a set of 5,906 instance hypotheses generatedfrom 282 unsuccessfully parsed sentences.GP1.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000000.9259600.9139330.7500000.6835940.5000000.3366940.0000000.000000Generic Hypothesisvp => vp,p.np => vp,np.n => \['double-quote'\].n => \[filename\].v => \[archived\].n => \[directory\].n => \['EOF'\].adj => \['non-printing'\].n => \[pathnames\].n => \[cpp\].n => \['NEWLINE'\].n => \ [ ' .
cshrc ' \ ] .n--> \[backslash\].n--> \ [a l iases\ ] .adj--> \[nonseekable\].n => \[~ordlist\].n => \[login\].n => \['TERM'\].n => \[cmdtool\].n => \['command-line'\].n => \[filenames\].adj => \[backquoted\].np => np,np.adj => \[blocking\].#: Rank,adj => \[invisible\].N: Number  of instance hypothesesTable h List of "Correct" HypothesesThe statistical process removed 4,034 instance hy-potheses and stopped after 63 cycles of the iterativecomputation ofGP and LP.
The instance hypotheseswere grouped into 2,876 generic hypotheses and theGP values of 2,331 generic hypotheses were reducedto 0 by the hypothesis deletion.Table 1 is the list of "correct" hypotheses pickedup from the whole list of generic hypotheses sortedby GP values.
The hypothesis for participles, np =>vp,np, is one of the 128 hypotheses whose GP val-ues are 1.
This table also shows that quite a few"correct" lexical hypotheses are in higher positionsbecause lexical knowledge for unknown words is in-dispensable to the successful parsing of the corpus.The distribution of "correct" hypotheses withinthe whole list is shown in Table 2.
The fact that"correct" hypotheses exist more in higher rangessupports our mechanism.
Although some of the"correct" ones have zero GP values, they do not di-minish our framework because most of them are thehypotheses treating participles as adjectives, whichare the alternative hypotheses of np => vp,np.The parameter which we can adjust to selectmore plausible hypotheses i the threshold for thehypothesis deletion.
Generally speaking, giving ahigher threshold causes an increase of the numberof deleted hypotheses and therefore accelerates theconvergence of the iterative process.
In the experi-ment, however, the use of one fifth as the thresholdinstead of one tenth did not bring a major difference.76Range Rule Lexical CorrectHypothesis Hypothesis Hypothesis1- 100101- 200201- 300301- 400401- 500501-10001001-20002001-2876194155769547377065381594524527230223352513509129Total 2182 694 108Table 2: Distribution of "Correct" Hypotheses5 ConclusionThe statistical analysis discussed in this paper isbased on the assumption that types of linguisticknowledge to be acquired are:\[1\] Knowledge for syntactic constructions which isused frequently in the given sublanguage.\[2\] Lexical knowledge such as subcategorizationframes and number properties, which is oftenidiosyncratic to the given sublanguage.\[3\] Knowledge which belongs neither to \[1\] nor to\[2\], but is indispensable to the given corpus.\[1\] implies that knowledge for less frequent con-structions can be ignored at the initial stage of lin-guistic knowledge customization.
Such knowledgewill be discovered after major defects of the currentgrammar are rectified, because the GP of a generichypothesis is defined as being sensitive to the fre-quency of the hypothesis.\[2\] means that we assume that the set of initiallyprovided grammar ules has a comprehensive cov-erage of English basic expressions.
This assump-tion is reflected in the way of the initial estimationof LP values.
Also note that only when this as-sumption is satisfied, can the HG produce a reason-able set of hypotheses.
On the other hand, becauseof this assumption, our framework can learn struc-turally complex and linguistically meaningful lexicaldescriptions, like a subcategorization frame.\[3\] is reflected in the way of the computation ofGP values.
A generic hypothesis one of whose in-stances occurs as a single possible hypothesis thatcan recover a parsing failure will have the GP valueof 1, even though its frequency is very low.The computation mechanism of GP and LP bearsa resemblance to the EM algorithm(Dempster et al,1977; Brown et al, 1993), which iteratively com-putes maximum likelihood estimates from incom-plete data.
As the purpose of our statistical analysisis to choose "correct" hypotheses from a hypothe-sis set which contains unnatural hypotheses as well,our motivation is different from that of the EM algo-rithm.
However, if we consider that the hypothesisdeletion is maxmizing the plausibility of "correct"hypotheses, the computation procedures of both al-gorithms have a strong similarity.The grammatical knowledge acquisition methodproposed in this paper will be incorporated into thetool kit for linguistic knowledge customization whichwe are now developing.
In the practical use of ourmethod, a grammar maintainer will be shown a listof hypotheses with high GP values and renew thecurrent version of grammatical knowledge.
The re-newed knowledge will be used in the next cycle ofhypothesis generation and selection to achieve thegradual enlargement of linguistic knowledge.AcknowledgementsWe would like to thank our colleagues in UMISTwho gave us many usuful comments.
We also want tothank Mr Tsumura and Dr Kawakami of Matsushita,who allowed the first author to study at UMIST.ReferencesMichael R. Brent.
1991.
Automatic Acquisitionof Subcategorization Frames from Untagged Text.In Proc.
of the 29st ACL meeting, pages 209-214.Ted Briscoe and John Carroll.
1993.
General-ized Probabilistic LR Parsing of Natural Lan-guage (Corpora) with Unification-Based Gram-mars.
Computational Linguistics, 19(1):25-59.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguis-tics, 19(2):263-311.Kenneth Church, William Gale, Patrick Hanks, andDonald Hindle.
1991.
Using Statistics in Lexi-cal Analysis.
In Uri Zernik, editor, Lexical Acqui-sition: Exploiting On-Line Resources to Build aLexicon, chapter 6, pages 115-164.
Lawrence Erl-baum Associates.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data viathe EM Algorithm.
Journal of the Royal Statisti-cal Society, 39(B):1-38.T.
Fujisaki, F. Jelinek, J. Cocke, E. Black, and T.Nishino.
1989.
A Probabilistic Parsing Methodfor Sentence Disambiguation.
In Proc.
of the Int.Workshop on Parsing Technologies, pages 105-114.
Carnegie-Mellon University.Masaki Kiyono and Jun'ichi Tsujii.
1993.
LinguisticKnowledge Acquisition from Parsing Failures.
InProc.
of EACL-93, pages 222-231.Masaki Kiyono and Jun'ichi Tsujii.
1994.
Hypothe-sis Selection in Grammar Acquisition.
In Proc.
ofCOLING-g4.77
