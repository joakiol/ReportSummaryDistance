Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 50?58,Gothenburg, Sweden, April 27, 2014.c?2014 Association for Computational LinguisticsParsing Screenplays for Extracting Social Networks from MoviesApoorv Agarwal?, Sriramkumar Balasubramanian?, Jiehan Zheng?, Sarthak Dash??Dept.
of Computer ScienceColumbia UniversityNew York, NY, USA?Peddie SchoolHightstown, NJ, USAapoorv@cs.columbia.edu jzheng-14@peddie.orgAbstractIn this paper, we present a formalizationof the task of parsing movie screenplays.While researchers have previously moti-vated the need for parsing movie screen-plays, to the best of our knowledge, thereis no work that has presented an evalua-tion for the task.
Moreover, all the ap-proaches in the literature thus far havebeen regular expression based.
In this pa-per, we present an NLP and ML basedapproach to the task, and show that thisapproach outperforms the regular expres-sion based approach by a large and statis-tically significant margin.
One of the mainchallenges we faced early on was the ab-sence of training and test data.
We pro-pose a methodology for using well struc-tured screenplays to create training datafor anticipated anomalies in the structureof screenplays.1 IntroductionSocial network extraction from unstructured texthas recently gained much attention (Agarwal andRambow, 2010; Elson et al., 2010; Agarwal et al.,2013a; Agarwal et al., 2013b; He et al., 2013).
Us-ing Natural Language Processing (NLP) and Ma-chine Learning (ML) techniques, researchers arenow able to gain access to networks that are notassociated with any meta-data (such as email linksand self-declared friendship links).
Movies, whichcan be seen as visual approximations of unstruc-tured literary works, contain rich social networksformed by interactions between characters.
Therehas been some effort in the past to extract socialnetworks from movies (Weng et al., 2006; Wenget al., 2007; Weng et al., 2009; Gil et al., 2011).However, these approaches are primarily regularexpression based with no evaluation of how wellthey work.In this paper we introduce a formalization of thetask of parsing screenplays and present an NLPand ML based approach to the task.
By parsinga screenplay, we mean assigning each line of thescreenplay one of the following five tags: ?S?
forscene boundary, ?N?
for scene description, ?C?for character name, ?D?
for dialogue, and ?M?
formeta-data.
We expect screenplays to conform toa strict grammar but they often do not (Gil et al.,2011).
This disconnect gives rise to the need fordeveloping a methodology that is able to handleanomalies in the structure of screenplays.
Thoughthe methodology proposed in this paper is in thecontext of movie screenplays, we believe, it is gen-eral and applicable to parse other kinds of noisydocuments.One of the earliest challenges we faced was theabsence of training and test data.
Screenplays, onaverage, have 7,000 lines of text, which limits theamount of annotated data we can obtain from hu-mans.
We propose a methodology for using wellstructured screenplays to create training data foranticipated anomalies in the structure of screen-plays.
For different types of anomalies, we trainseparate classifiers, and combine them using en-semble learning.
We show that our ensemble out-performs a regular-expression baseline by a largeand statistically significant margin on an unseentest set (0.69 versus 0.96 macro-F1 measure for thefive classes).
Apart from performing an intrinsicevaluation, we also present an extrinsic evaluation.We show that the social network extracted fromthe screenplay tagged by our ensemble is closerto the network extracted from a screenplay tagged50by a human, as compared to the network extractedfrom a screenplay tagged by the baseline.The rest of the paper is structured as follows: insection 2, we present common terminology usedto describe screenplays.
We survey existing liter-ature in section 3.
Section 4 presents details ofour data collection methodology, along with thedata distribution.
Section 5 gives details of ourregular-expression based system, which we use asa baseline for evaluation purposes.
In section 6,we present our machine learning approach.
In sec-tion 7, we give details of the features we use formachine learning.
In section 8, we present our ex-periments and results.
We conclude and give fu-ture directions of research in section 9.2 TerminologyTuretsky and Dimitrova (2004) describe thestructure of a movie screenplay as follows: ascreenplay describes a story, characters, action,setting and dialogue of a film.
Additionally,they report that the structure of a screenplayfollows a (semi) regular format.
Figure 1 showsa snippet of a screenplay from the film ?
TheSilence of the Lambs.
A scene (tag ?S?)
startswith what is called the slug line (or scene bound-ary).
The slug line indicates whether the sceneis to take place inside or outside (INT, EXT),the name of the location (?FBI ACADEMYGROUNDS, QUANTICO, VIRGINIA?
), andcan potentially specify the time of day (e.g.DAY or NIGHT).
Following the scene boundaryis a scene description.
A scene descriptionis followed by a character name (tag ?C?
),which is followed by dialogues (tag ?D?
).Character names are capitalized, with an optional(V.O.)
for ?Voice Over?
or (O.S.)
for ?Off-screen.
?Dialogues, like scene descriptions, are not asso-ciated with any explicit indicators (such as INT,V.O.
), but are indented at a unique level (i.e.nothing else in the screenplay is indented at thislevel).
Screenplays may also have other elements,such as ?CUT TO:?, which are directions for thecamera, and text describing the intended moodof the speaker, which is found within parenthesesin the dialogue.
For lack of a name for theseelements, we call them ?Meta-data?
(tag ?M?
).3 Literature SurveyOne of the earliest works motivating the need forscreenplay parsing is that of Turetsky and Dim-itrova (2004).
Turetsky and Dimitrova (2004)proposed a system to automatically align writtenscreenplays with their videos.
One of the crucialsteps, they noted, is to parse a screenplay into itsdifferent elements: scene boundaries, scene de-scriptions, character names, and dialogues.
Theyproposed a grammar to parse screenplays andshow results for aligning one screenplay with itsvideo.
Weng et al.
(2009) motivated the need forscreenplay parsing from a social network analy-sis perspective.
They proposed a set of opera-tions on social networks extracted from moviesand television shows in order to find what theycalled hidden semantic information.
They pro-posed techniques for identifying lead roles in bi-lateral movies (movies with two main characters),for performing community analysis, and for au-tomating the task of story segmentation.
Gil etal.
(2011) extracted character interaction networksfrom plays and movies.
They were interested inautomatically classifying plays and movies intodifferent genres by making use of social networkanalysis metrics.
They acknowledged that thescripts found on the internet are not in consistentformats, and proposed a regular expression basedsystem to identify scene boundaries and characternames.While there is motivation in the literature toparse screenplays, none of the aforementionedwork addresses the task formally.
In this paper, weformalize the task and propose a machine learningbased approach that is significantly more effec-tive and tolerant of anomalous structure than thebaseline.
We evaluate our models on their abilityto identify scene boundaries and character names,but also on their ability to identify other importantelements of a screenplay, such as scene descrip-tions and dialogues.4 DataWe crawled the Internet Movie Script Database(IMSDB) website1to collect movie screenplays.We crawled a total of 674 movies.
Moviesthat are well structured have the property thatscene boundaries and scene descriptions, charac-ter names, and dialogues are all at different butfixed levels of indentation.2For example, in themovie in Figure 1, all scene boundaries and scene1http://www.imsdb.com2By level of indentation we mean the number of spacesfrom the start of the line to the first non-space character.51Figure 1: Example screenplay: first column shows the tags we assign to each line in the screenplay.
Mstands for ?Meta-data?, S stands for ?Scene boundary?, N stands for ?Scene description?, C stands for?Character name?, and D stands for ?Dialogue.?
We also show the lines that are at context -2 and +3 forthe line ?CRAWFORD.
?descriptions are at the same level of indentation,equal to five spaces.
All character names are ata different but fixed level of indentation, equal to20 spaces.
Dialogues are at an indentation levelof eight spaces.
These indentation levels may varyfrom one screenplay to the other, but are consis-tent within a well formatted screenplay.
Moreover,the indentation level of character names is strictlygreater than the indentation level of dialogues,which is strictly greater than the indentation levelof scene boundaries and scene descriptions.
Foreach crawled screenplay, we found the frequencyof unique indentation levels in that screenplay.
Ifthe top three unique frequencies constituted 90%of the total lines of a screenplay, we flagged thatthe movie was well-structured, and assigned tagsbased on indentation levels.
Since scene bound-aries and scene descriptions are at the same levelof indentation, we disambiguate between them byutilizing the fact that scene boundaries in well-formatted screenplays start with tags such as INT.and EXT.
We programmatically checked the sanityof these automatically tagged screenplays by usingthe following procedure: 1) check if scene descrip-tions are between scene boundaries and characternames, 2) check if dialogues are between charac-ter names, and 3) check if all character names arewithin two scene boundaries.
Using this method-ology, we were able to tag 222 movies that passthe sanity check.Data # S # N # C # D # MTRAIN 2,445 21,619 11,464 23,814 3,339DEV1 714 7,495 4,431 9,378 467DEV2 413 5,431 2,126 4,755 762TEST 164 845 1,582 3,221 308Table 1: Data distributionTable 1 gives the distribution of our training, de-velopment and test sets.
We use a random sub-set of the aforementioned set of 222 movies fortraining purposes, and another random subset fordevelopment.
We chose 14 movies for the train-ing set and 9 for the development set.
Since hu-man annotation for the task is expensive, instead ofgetting all 23 movies checked for correctness, weasked an annotator to only look at the developmentset (9 movies).
The annotator reported that oneout of 9 movies was not correctly tagged.
We re-moved this movie from the development set.
Fromthe remaining 8 movies, we chose 5 as the first de-velopment set and the remaining 3 as the seconddevelopment set.
For the test set, we asked ourannotator to annotate a randomly chosen screen-play (Silver Linings Playbook) from scratch.
Wechose this screenplay from the set of movies that52we were unable to tag automatically, i.e.
not fromthe set of 222 movies.5 Baseline SystemGil et al.
(2011) mention the use of regular expres-sions for tagging screenplays.
However, they donot specify the regular expressions or their exactmethodology.
We use common knowledge aboutthe structure of the screenplay (underlined text insection 2) to build a baseline system, that uses reg-ular expressions and takes into account the gram-mar of screenplays.Since scene descriptions, characters and dia-logues are relative to the scene boundary, we doa first pass on the screenplay to tag scene bound-aries.
We created a dictionary of words that areexpected to indicate scene boundaries.
We usethis dictionary for tagging lines in the screenplaywith the tag ?S?.
We tag all the lines that con-tain tags indicating a character (V.O., O.S.)
with?C?.
We built a dictionary of meta-data tags thatcontains patterns such as ?CUT TO:, DISSOLVETO.?
We tag all the remaining untagged lines con-taining these patterns with the tag ?M.?
This ex-hausts the list of regular expression matches thatindicate a certain tag.In the next pass, we incorporate prior knowl-edge that scene boundaries and character namesare capitalized.
For this, we tag all the untaggedlines that are capitalized, and that have more thanthree words as scene boundaries (tag ?S?).
We tagall the untagged lines that are capitalized, and thathave less than four words as character (tag ?C?
).The choice of the number four is not arbitrary;we examined the set of 222 screenplays that wastagged using indentation information and foundthat less than two percent of the character nameswere of length greater than three.Finally, we incorporate prior knowledge aboutrelative positions of dialogues and scene descrip-tions to tag the remaining untagged lines with oneof two tags: ?D?
or ?N?.
We tag all the untaggedlines between a scene boundary and the first char-acter occurrence as ?N?.
We tag all the lines be-tween consecutive character occurrences, the lastcharacter occurrence and the scene boundary as?D?.We use this baseline system, which incorporatesall of the prior knowledge about the structure ofscreenplays, to tag movies in our first developmentset DEV1 (section 8).
We report a macro-F1 mea-sure for the five tags as 0.96.
This confirms thatour baseline is well suited to parse screenplays thatare well structured.6 Machine Learning ApproachNote that our baseline system is not dependent onthe level of indentation (it achieves a high macro-F1 measure without using indentation informa-tion).
Therefore, we have already dealt with onecommon problem with screenplays found on theweb: bad indentation.
However, there are otherproblems, some of which we noticed in the lim-ited data we manually examined, and others thatwe anticipate: (1) missing scene boundary spe-cific patterns (such as INT./EXT.)
from the sceneboundary lines, (2) uncapitalized scene boundariesand (3) uncapitalized character names.
These areproblems that a regular expression based systemis not well equipped to deal with.
In this sec-tion, we discuss a strategy for dealing with screen-plays, which might have anomalies in their struc-ture, without requiring additional annotations.We synthesize training and development datato learn to handle the aforementioned three typesof anomalies.
We create eight copies of ourTRAIN set: one with no anomalies, represented asTRAIN_000,3one in which character names areuncapitalized, represented as TRAIN_001, one inwhich both scene boundaries and character namesare uncapitalized, represented as TRAIN_011,and so on.
Similarly, we create eight copiesof our DEV1 set: {DEV1_000, DEV1_001, ...,DEV1_111}.
Now we have eight training andeight development sets.
We train eight models,and choose the parameters for each model by tun-ing on the respective development set.
However,at test time, we require one model.
Moreover, ourmodel should be able to handle all types of anoma-lies (all of which could be present in a random or-der).
We experiment with three ensemble learningtechniques and choose the one that performs thebest on the second development set, DEV2.
Weadd all three types of anomalies, randomly, to ourDEV2 set.For training individual models, we use SupportVector Machines (SVMs), and represent data asfeature vectors, discussed in the next section.3Each bit refers to the one type of anomaly describedin the previous paragraph.
If the least significant bit is 1,this means, the type of anomaly is uncapitalized charactersnames.537 FeaturesWe have six sets of features: bag-of-words fea-tures (BOW), bag-of-punctuation-marks features(BOP), bag-of-terminology features (BOT), bag-of-frames features (BOF), bag-of-parts-of-speechfeatures (POS), and hand-crafted features (HAND).We convert each line of a screenplay (input ex-ample) into a feature vector of length 5,497: 3,946for BOW, 22 for BOP, 2*58 for BOT, 2*45 forPOS, 2*651 for BOF, and 21 for HAND.BOW, BOP, and BOT are binary features; werecord the presence or absence of elements of eachbag in the input example.
The number of ter-minology features is multiplied by two becausewe have one binary vector for ?contains term?,and another binary vector for ?is term.?
We havetwo sets of features for POS and BOF.
One setis binary and similar to other binary features thatrecord the presence or absence of parts-of-speechand frames in the input example.
The other setis numeric.
We record the normalized counts ofeach part-of-speech and frame respectively.
Theimpetus to design this second set of features forparts-of-speech and frames is the following: weexpect some classes to have a characteristic dis-tribution of parts-of-speech and frames.
For ex-ample, scene boundaries contain the location andtime of scene.
Therefore, we expect them to havea majority of nouns, and frames that are related tolocation and time.
For the scene boundary in Fig-ure 1 (EXT.
FBI ACADEMY ... - DAY), we findthe following distribution of parts of speech andframes: 100% nouns, 50% frame LOCALE (withframe evoking element grounds), and 50% frameCALENDRIC_UNIT (with frame evoking elementDAY).
Similarly, we expect the character names tohave 100% nouns, and no frames.We use Stanford part-of-speech tagger(Toutanova et al., 2003) for obtaining thepart-of-speech tags and Semafor (Chen et al.,2010) for obtaining the FrameNet (Baker etal., 1998) frames present in each line of thescreenplay.We devise 21 hand-crafted features.
Six-teen of these features are binary (0/1).
Welist these features here (the feature namesare self-explanatory): has-non-alphabetical-chars, has-digits-majority, has-alpha-majority,is-quoted, capitalization (has-all-caps, is-all-caps), scene boundary (has-INT, has-EXT),date (has-date, is-date), number (has-number,is-number), and parentheses (is-parenthesized,starts-with-parenthesis, ends-with-parenthesis,contains-parenthesis).
We bin the precedingnumber of blank lines into four bins: 0 for nopreceding blank lines, 1 for one preceding blankline, 2 for two preceding blank lines, and so on.We also bin the percentage of capitalized wordsinto four bins: 0 for the percentage of capitalizedwords lying between 0-25%, 1 for 25-50%, andso on.
We use three numeric features: number ofnon-space characters (normalized by the maxi-mum number of non-space characters in any linein a screenplay), number of words (normalizedby the maximum number of words in any line ina screenplay), and number of characters (normal-ized by the maximum number of characters in anyline in a screenplay).For each line, say linei, we incorporate con-text up to x lines.
Figure 1 shows the linesat context -2 and +3 for the line contain-ing the text CRAWFORD.
To do so, we ap-pend the feature vector for lineiby the fea-ture vectors of linei?1, linei?2, .
.
.
linei?xandlinei+1, linei+2, .
.
.
linei+x.
x is one of the pa-rameters we tune at the time of training.
We referto this parameter as CONTEXT.8 Experiments and ResultsIn this section, we present experiments and resultsfor the task of tagging the lines of a screenplaywith one of five tags: {S, N, C, D, M}.
Table 1shows the data distribution.
For parameter tun-ing, we use DEV1 (section 8.1).
We train sepa-rate models on different types of known and antici-pated anomalies (as discussed in section 6).
In sec-tion 8.2, we present strategies for combining thesemodels.
We select the right combination of mod-els and features by tuning on DEV2.
Finally, weshow results on the test set, TEST.
For all our ex-periments, we use the default parameters of SVMas implemented by the SMO algorithm of Weka(Hall et al., 2009).
We use a linear kernel.48.1 Tuning learning parametersWe tune two parameters: the amount of train-ing data and the amount of CONTEXT (section 7)required for learning.
We do this for each ofthe eight models (TRAIN_000/DEV1_000, ...,TRAIN_111/DEV1_111).
We merge training4We tried the polynomial kernel up to a degree of four andthe RBF kernel.
They performed worse than the linear kernel.5410 20 30 40 50 60 70 80 90 1000.70.750.80.850.90.951% TRAINING DATA USEDMACRO F?MEASURETRAIN?DEV1 (000)context?0context?1context?2context?3context?4context?5Figure 2: Learning curve for training onTRAIN_000 and testing on DEV1_000.
X-axisis the % of training data, in steps of 10%.
Y-axisis the macro-F1 measure for the five classes.data from all 14 movies into one (TRAIN).
Wethen randomize the data and split it into 10 pieces(maintaining the relative proportions of the fiveclasses).
We plot a learning curve by adding 10%of training data at each step.Figure 2 shows the learning curve for train-ing a model on TRAIN_000 and testing onDEV1_000.5The learning curve shows that theperformance of our classifier without any contextis significantly worse than the classifiers trainedon context.
Moreover, the learning saturates early,and stabilizes at about 50% of the training data.From the learning curves, we pick CONTEXTequal to 1, and the amount of training data equalto 50% of the entire training set.Table 2 shows a comparison of our rule basedbaseline with the models trained using machinelearning.
For the 000 setting, when there is noanomaly in the screenplay, our rule based base-line performs well, achieving a macro-F1 measureof 0.96.
However, our machine learning modeloutperforms the baseline by a statistically signif-icant margin, achieving a macro-F1 measure of0.99.
We calculate statistical significance usingMcNemar?s significance test, with significance de-fined as p < 0.05.6Results in Table 2 also showthat while a deterministic regular-expression basedsystem is not well equipped to handle anomalies,there is enough value in our feature set, that ourmachine learning based models learn to adapt toany combination of the three types of anomalies,achieving a high F1-measure of 0.98 on average.5Learning curves for all our other models were similar.6We use the same test for reporting other statistically sig-nificance results in the paper.8.2 Finding the right ensemble and featureselectionWe have trained eight separate models, whichneed to be combined into one model that we willmake predictions at the test time.
We explore thefollowing ways of combining these models:1.
MAJ: Given a test example, we get a votefrom each of our eight models, and take a ma-jority vote.
At times of a clash, we pick onerandomly.2.
MAX: We pick the class predicted by themodel that has the highest confidence in itsprediction.
Since the confidence values arereal numbers, we do not see any clashes.3.
MAJ-MAX: We use MAJ but at times of aclash, we pick the class predicted by the clas-sifier that has the highest confidence (amongthe classifiers that clash).Table 3 shows macro-F1 measures for the threemovies in our DEV2 set.
Note, we added the threetypes of anomalies (section 6) randomly to theDEV2 set for tuning the type of ensemble.
Wecompare the performance of the three ensembletechniques with the individual classifiers (trainedon TRAIN_000, ... TRAIN_111).The results show that all our ensembles (ex-cept MAX for the movie The Last Temptation ofChrist) perform better than the individual models.Moreover, the MAJ-MAX ensemble outperformsthe other two by a statistically significant margin.We thus choose MAJ-MAX as our final classifier.Table 4 shows results for removing one of allfeature sets, one at a time.
These results are for ourfinal model, MAJ-MAX.
The row ?All?
shows theresults when we use all our features for training.The consecutive rows show the result when we re-move the mentioned feature set.
For example, therow ?- BOW?
shows the result for our classifierthat was trained without the bag of words featureset.Table 4 shows that the performance drops themost for bag of words (BOW) and for our hand-crafted features (HAND).
The next highest dropis for the bag of frames feature set (BOF).
Er-ror analysis revealed that the major drop in per-formance because of the removal of the BOF fea-tures was not due the drop in the performanceof scene boundaries, counter to our initial intu-ition.
The drop was because the recall of dia-55000 001 010 011 100 101 110 111Rule based 0.96 0.49 0.70 0.23 0.93 0.46 0.70 0.24ML model 0.99 0.99 0.98 0.99 0.97 0.98 0.98 0.98Table 2: Comparison of performance (macro-F1 measure) of our rule based baseline with our machinelearning based models on development sets DEV1_000, DEV1_001, ..., DEV1_111.
All models aretrained on 50% of the training set, with the feature space including CONTEXT equal to 1.Movie 000 001 010 011 100 101 110 111 MAJ MAX MAJ-MAXLTC 0.87 0.83 0.79 0.94 0.91 0.86 0.79 0.96 0.97 0.95 0.98X-files 0.87 0.84 0.79 0.93 0.86 0.84 0.79 0.92 0.94 0.94 0.96Titanic 0.87 0.87 0.81 0.94 0.86 0.83 0.82 0.93 0.94 0.95 0.97Average 0.87 0.85 0.80 0.94 0.88 0.84 0.80 0.94 0.95 0.95 0.97Table 3: Macro-F1 measure for the five classes for testing on DEV2 set.
000 refers to the model trainedon data TRAIN_000, 001 refers to the model trained on data TRAIN_001, and so on.
MAJ, MAX, andMAJ-MAX are the three ensembles.
The first column is the movie name.
LTC refers to the movie ?TheLast Temptation of Christ.
?Feature set LTC X-files TitanicAll 0.98 0.96 0.97- BOW 0.94 0.92 0.94- BOP 0.98 0.97 0.97- BOT 0.97 0.95 0.96- BOF 0.96 0.93 0.96- POS 0.98 0.96 0.95- HAND 0.94 0.93 0.93Table 4: Performance of MAJ-MAX classifier withfeature removal.
Statistically significant differ-ences are in bold.logues decreases significantly.
The BOF featureswere helping in disambiguating between the meta-data, which usually have no frames associatedwith them, and dialogues.
Removing bag of punc-tuation (BOP) results in a significant increase inthe performance for the movie X-files, with a smallincrease for other two movies.
We remove thisfeature from our final classifier.
Removing partsof speech (POS) results in a significant drop in theoverall performance for the movie Titanic.
Erroranalysis revealed that the drop in performance herewas in fact due the drop in performance of sceneboundaries.
Scene boundaries almost always have100% nouns and the POS features help in cap-turing this characteristic distribution indicative ofscene boundaries.
Removing bag of terminology(BOT) results in a significant drop in the overallperformance of all movies.
Our results also showthat though the drop in performance for some fea-Baseline MAJ-MAXTag P R F1 P R F1S 0.27 1.00 0.43 0.99 1.00 0.99N 0.21 0.06 0.09 0.88 0.95 0.91C 0.89 1.00 0.94 1 0.92 0.96D 0.99 0.94 0.96 0.98 0.998 0.99M 0.68 0.94 0.79 0.94 0.997 0.97Avg 0.61 0.79 0.69 0.96 0.97 0.96Table 5: Performance comparison of our rulebased baseline with our best machine learningmodel on the five classes.NBNMAJ-MAXNG# Nodes 202 37 41# Links 1252 331 377Density 0.036 0.276 0.255Table 6: A comparison of network statistics forthe three networks extracted from the movie SilverLinings Playbook.ture sets is larger than the others, it is the conjunc-tion of all features that is responsible for a highF1-measure.8.3 Performance on the test setTable 5 shows a comparison of the performanceof our rule based baseline with our best machinelearning based model on our test set, TEST.
Theresults show that our machine learning based mod-els outperform the baseline with a large and sig-56Model Degree Weighted Degree Closeness Betweenness PageRank EigenNB0.919 0.986 0.913 0.964 0.953 0.806NMAJ-MAX0.997 0.997 0.997 0.997 0.998 0.992Table 7: A comparison of Pearson?s correlation coefficients of various centrality measures for NBandNMAJ-MAXwith NG.nificant margin on all five classes (0.96 versus0.69 macro-F1 measure respectively).
Note, as ex-pected, the recall of the baseline is generally high,while the precision is low.
Moreover, for this testset, the baseline performs relatively well on tag-ging character names and dialogues.
However, webelieve that the performance of the baseline is un-predictable.
It may get lucky on screenplays thatare well-structured (in one way or the other), butit is hard to comment on the robustness of its per-formance.
On the contrary, our ensemble is ro-bust, hedging its bets on eight models, which aretrained to handle different types and combinationsof anomalies.In tables 6 and 7, we present an extrinsic evalua-tion on the test set.
We extract a network from ourtest movie screenplay (Silver Linings Playbook)by using the tags of the screenplay as follows(Weng et al., 2009): we connect all characters hav-ing a dialogue with each other in a scene withlinks.
Nodes in this network are characters, andlinks between two characters signal their partici-pation in the same scene.
We form three such net-works: 1) based on the gold tags (NG), 2) based onthe tags predicted by MAJ-MAX (NMAJ-MAX),and 3) based on the tags predicted by our base-line (NB).
Table 6 compares the number of nodes,number of links, and graph density of the threenetworks.
It is clear from the table that the net-work extracted by using the tags predicted byMAJ-MAX is closer to the gold network.Centrality measures are one of the most funda-mental social network analysis metrics used by so-cial scientists (Wasserman and Faust, 1994).
Ta-ble 7 presents a comparison of Pearson?s correla-tion coefficient for various centrality measures for{NB, NG}, and {NMAJ-MAX, NG} for the topten characters in the movie.
The table shows thatacross all these measures, the statistics obtainedusing the network NMAJ-MAXare significantlymore correlated to the gold network (NG), as com-pared the the baseline network (NB).9 Conclusion and Future WorkIn this paper, we presented a formalization of thetask of parsing movie screenplays.
We presentedan NLP and ML based approach to the task, andshowed that this approach outperforms the regularexpression based approach by a large and signifi-cant margin.
One of the main challenges we facedearly on was the absence of training and test data.We proposed a methodology for learning to han-dle anomalies in the structure of screenplays with-out requiring additional annotations.
We believethat the machine learning approach proposed inthis paper is general, and may be used for parsingnoisy documents outside of the context of moviescreenplays.In the future, we will apply our approach toparse other semi-structured sources of social net-works such as television show series and theatricalplays.AcknowledgmentsWe would like to thank anonymous reviewers fortheir useful comments.
We would also like tothank Caronae Howell for her insightful com-ments.
Agarwal was funded by IBM Ph.D. fellow-ship 2013-2014.
This paper is based upon worksupported in part by the DARPA DEFT Program.The views expressed are those of the authors anddo not reflect the official policy or position of theDepartment of Defense or the U.S. Government.57ReferencesApoorv Agarwal and Owen Rambow.
2010.
Auto-matic detection and classification of social events.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1024?1034, Cambridge, MA, October.
Associationfor Computational Linguistics.Apoorv Agarwal, Anup Kotalwar, and Owen Ram-bow.
2013a.
Automatic extraction of social net-works from literary text: A case study on alice inwonderland.
In the Proceedings of the 6th Interna-tional Joint Conference on Natural Language Pro-cessing (IJCNLP 2013).Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, andOwen Rambow.
2013b.
Sinnet: Social interactionnetwork extractor from text.
In Sixth InternationalJoint Conference on Natural Language Processing,page 33.C.
Baker, C. Fillmore, and J. Lowe.
1998.
The berke-ley framenet project.
Proceedings of the 17th inter-national conference on Computational linguistics, 1.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
Semafor: Frame argumentresolution with log-linear models.
In Proceedings ofthe 5th International Workshop on Semantic Evalu-ation, pages 264?267, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.David K. Elson, Nicholas Dames, and Kathleen R.McKeown.
2010.
Extracting social networks fromliterary fiction.
Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 138?147.Sebastian Gil, Laney Kuenzel, and Suen Caroline.2011.
Extraction and analysis of character interac-tion networks from plays and movies.
Technical re-port, Stanford University.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11.Hua He, Denilson Barbosa, and Grzegorz Kondrak.2013.
Identification of speakers in novels.
The51st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2013).Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL.Robert Turetsky and Nevenka Dimitrova.
2004.Screenplay alignment for closed-system speakeridentification and analysis of feature films.
In Mul-timedia and Expo, 2004.
ICME?04.
2004 IEEE In-ternational Conference on, volume 3, pages 1659?1662.
IEEE.Stanley Wasserman and Katherine Faust.
1994.
SocialNetwork Analysis: Methods and Applications.
NewYork: Cambridge University Press.Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu.
2006.Movie analysis based on roles?
social network.
InProceedings of IEEE Int.
Conference Multimediaand Expo., pages 1403?1406.Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu.
2007.Rolenet: treat a movie as a small society.
In Pro-ceedings of the international workshop on Workshopon multimedia information retrieval, pages 51?60.ACM.Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu.
2009.Rolenet: Movie analysis from the perspective of so-cial networks.
Multimedia, IEEE Transactions on,11(2):256?271.58
