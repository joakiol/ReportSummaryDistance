Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31?40,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsIncorporating Source-Language Paraphrases into Phrase-Based SMT withConfusion NetworksJie Jiang,?
Jinhua Du,?
and Andy Way?
?CNGL, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland{jjiang, away}@computing.dcu.ie?School of Automation and Information Engineering,Xi?an University of Technology, Xi?an, Shaanxi, Chinajhdu@xaut.edu.cnAbstractTo increase the model coverage, source-language paraphrases have been utilized toboost SMT system performance.
Previouswork showed that word lattices constructedfrom paraphrases are able to reduce out-of-vocabulary words and to express inputs indifferent ways for better translation quality.However, such a word-lattice-based methodsuffers from two problems: 1) path dupli-cations in word lattices decrease the capac-ities for potential paraphrases; 2) lattice de-coding in SMT dramatically increases thesearch space and results in poor time effi-ciency.
Therefore, in this paper, we adoptword confusion networks as the input struc-ture to carry source-language paraphrase in-formation.
Similar to previous work, we useword lattices to build word confusion net-works for merging of duplicated paths andfaster decoding.
Experiments are carried outon small-, medium- and large-scale English?Chinese translation tasks, and we show thatcompared with the word-lattice-based method,the decoding time on three tasks is reducedsignificantly (up to 79%) while comparabletranslation quality is obtained on the large-scale task.1 IntroductionWith the rapid development of large-scale parallelcorpus, research on data-driven SMT has made goodprogress to the real world applications.
Currently,for a typical automatic translation task, the SMTsystem searches and exactly matches the input sen-tences with the phrases or rules in the models.
Obvi-ously, if the following two conditions could be sat-isfied, namely:?
the words in the parallel corpus are highlyaligned so that the phrase alignment can be per-formed well;?
the coverage of the input sentence by the paral-lel corpus is high;then the ?exact phrase match?
translation methodcould bring a good translation.However, for some language pairs, it is not easyto obtain a huge amount of parallel data, so it is notthat easy to satisfy these two conditions.
To allevi-ate this problem, paraphrase-enriched SMT systemshave been proposed to show the effectiveness of in-corporating paraphrase information.
In terms of theposition at which paraphrases are incorporated in theMT-pipeline, previous work can be organized intothree different categories:?
Translation model augmentation with para-phrases (Callison-Burch et al, 2006; Marton etal., 2009).
Here the focus is on the translationof unknown source words or phrases in the in-put sentences by enriching the translation tablewith paraphrases.?
Training corpus augmentation with para-phrases (Bond et al, 2008; Nakov, 2008a;Nakov, 2008b).
Paraphrases are incorporatedinto the MT systems by expanding the trainingdata.?
Word-lattice-based method with para-phrases (Du et al, 2010; Onishi et al,312010).
Instead of augmenting the transla-tion table, source-language paraphrases areconstructed to enrich the inputs to the SMTsystem.
Another directly related work is touse word lattices to deal with multi-sourcetranslation (Schroeder et al, 2009), in whichparaphrases are actually generated from thealignments of difference source sentences.Comparing these three methods, the word-lattice-based method has the least overheads because:?
The translation model augmentation methodhas to re-run the whole MT pipeline oncethe inputs are changed, while the word-lattice-based method only need to transform the newinput sentences into word lattices.?
The training corpus augmentation method re-quires corpus-scale expansion, which drasti-cally increases the computational complexityon large corpora, while the word-lattice-basedmethod only deals with the development setand test set.In (Du et al, 2010; Onishi et al, 2010), it is alsoobserved that the word-lattice-based method per-formed better than the translation model augmen-tation method on different scales and two differentlanguage pairs in several translation tasks.
Thusthey concluded that the word-lattice-based methodis preferable for this task.However, there are still some drawbacks for theword-lattice-based method:?
In the lattice construction processing, dupli-cated paths are created and fed into SMT de-coders.
This decreases the paraphrase capacityin the word lattices.
Note that we use the phrase?paraphrase capacity?
to represent the amountof paraphrases that are actually built into theword lattices.
As presented in (Du et al, 2010),only a limited number of paraphrases are al-lowed to be used while others are pruned duringthe construction process, so duplicate paths ac-tually decrease the number of paraphrases thatcontribute to the translation quality.?
The lattice decoding in SMT decoder havea very high computational complexity whichmakes the system less feasible in real time ap-plication.Therefore, in this paper, we use confusion net-works (CNs) instead of word lattices to carry para-phrase information in the inputs for SMT decoders.CNs are constructed from the aforementioned wordlattices, while duplicate paths are merged to increaseparaphrase capacity (e.g.
by admitting more non-duplicate paraphrases without increasing the inputsize).
Furthermore, much less computational com-plexity is required to perform CN decoding insteadof lattice decoding in the SMT decoder.
We car-ried out experiments on small-, medium- and large-scale English?Chinese translation tasks to compareagainst a baseline PBSMT system, the translationmodel augmentation of (Callison-Burch et al, 2006)method and the word-lattice-based method of (Du etal., 2010) to show the effectiveness of our novel ap-proach.The motivation of this work is to use CN asthe compromise between speed and quality, whichcomes from previous studies in speech recog-nition and speech translation: in (Hakkani-Tu?ret al, 2005), word lattices are transformed intoCNs to obtain compact representations of multiplealigned ASR hypotheses in speech understanding;in (Bertoldi et al, 2008), CNs are also adoptedinstead of word lattices as the source-side inputsfor speech translation systems.
The main contribu-tion of this paper is to show that this compromisealso works for SMT systems incorporating source-language paraphrases in the inputs.Regarding the use of paraphrases SMT system,there are still other two categories of work that arerelated to this paper:?
Using paraphrases to improve system optimiza-tion (Madnani et al, 2007).
With an English?English MT system, this work utilises para-phrases to reduce the number of manuallytranslated references that are needed in theparameter tuning process of SMT, while pre-served a similar translation quality.?
Using paraphrases to smooth translation mod-els (Kuhn et al, 2010; Max, 2010).
Eithercluster-based or example-based methods are32proposed to obtain better estimation on phrasetranslation probabilities with paraphrases.The rest of this paper is organized as follows:In section 2, we present an overview of the word-lattice-based method and its drawbacks.
Section 3proposes the CN-based method, including the build-ing process and its application on paraphrases inSMT.
Section 4 presents the experiments and resultsof the proposed method as well as discussions.
Con-clusions and future work are then given in Section5.2 Word-lattice-based methodCompared with translation model augmentationwith paraphrases (Callison-Burch et al, 2006),word-lattice-based paraphrasing for PBSMT is in-troduced in (Du et al, 2010).
A brief overview ofthis method is given in this section.2.1 Lattice construction from paraphrasesThe first step of the word-lattice-based method is togenerate paraphrases from parallel corpus.
The al-gorithm in (Bannard and Callison-Burch, 2005) isused for this purpose by pivoting through phrasesin the source- and the target- languages: for eachsource phrase, all occurrences of its target phrasesare found, and all the corresponding source phrasesof these target phrases are considered as the potentialparaphrases of the original source phrase (Callison-Burch et al, 2006).
A paraphrase probabilityp(e2|e1) is defined to reflect the similarities betweentwo phrases, as in (1):p(e2|e1) =?fp(f |e1)p(e2|f) (1)where the probability p(f |e1) is the probability thatthe original source phrase e1 translates as a partic-ular phrase f on the target side, and p(e2|f) is theprobability that the candidate paraphrase e2 trans-lates as the source phrase.
Here p(e2|f) and p(f |e1)are defined as the translation probabilities estimatedusing maximum likelihood by counting the observa-tions of alignments between phrases e and f in thewxwy...q1q2...qm...wx+1wy...wx+1wx-1wy+1q1q2?
qm......wx...wy+1wx-1Figure 1: Construct word lattices from paraphrases.parallel corpus, as in (2) and (3):p(e2|f) ?count(e2, f)?e2 count(e2, f)(2)p(f |e1) ?count(f, e1)?f count(f, e1)(3)The second step is to transform input sentencesin the development and test sets into word latticeswith paraphrases extracted in the first step.
As il-lustrated in Figure 1, given a sequence of words{w1, .
.
.
, wN} as the input, for each of the para-phrase pairs found in the source sentence (e.g.
pi ={q1, .
.
.
, qm} for {wx, .
.
.
, wy}), add in extra nodesand edges to make sure those phrases coming fromparaphrases share the same start nodes and endnodes with that of the original ones.
Subsequentlythe following empirical methods are used to assignweights on paraphrases edges:?
Edges originating from the input sentences areassigned weight 1.?
The first edges for each of the paraphrases arecalculated as in (4):w(e1pi) =1k + i(1 <= i <= k) (4)where 1 stands for the first edge of paraphrasepi, and i is the probability rank of pi amongthose paraphrases sharing with a same startnode, while k is a predefined constant as atrade-off parameter for efficiency and perfor-mance, which is related to the paraphrase ca-pacity.?
The rest of the edges corresponding to the para-phrases are assigned weight 1.33The last step is to modify the MT pipeline to tuneand evaluate the SMT system with word lattice in-puts, as is described in (Du et al, 2010; Onishi etal., 2010).For further discussion, a real example of the gen-erated word lattice is illustrated in Figure 2.
Inthe word lattice, double-line circled nodes and solidlined edges come from originated from the origi-nal sentence, while others are generated from para-phrases.
Word, weight and ranking of each edge aredisplayed in the figure.
By adopting such an inputstructure, the diversity of the input sentences is in-creased to provide more flexible translation optionsduring the decoding process, which has been shownto improve translation performance (Du et al, 2010).2.2 Path duplication and decoding efficiencyAs can be seen in Figure 2, the construction pro-cess in the previous steps tends to generate duplicatepaths in the word lattices.
For example, there are twopaths from node 6 to node 11 with the same words?secretary of state?
but different edge probabilities(the path via node 27 and 28 has the probability1/12, while the path via node 26 and 9 has the prob-ability 1/99).
This is because the aforementionedstraightforward construction process does not trackpath duplications from different spans on the sourceside.
Since the number of admitted paraphrases isrestricted by parameter k in formula (4), the pathduplication will decrease the paraphrase capacity toa certain extend.Moreover, state of the art PBSMT decoders (e.g.Moses (Koehn et al, 2007)) have a much highercomputational complexity for lattice structures thanfor sentences.
Thus even though only the test sen-tences need to be transformed into word lattices, de-coding time is still too slow for real-time applica-tions.Motivated by transforming ASR word-graphs intoCNs (Bertoldi et al, 2008), we adopt CN as thetrade-off between efficiency and quality.
We aim tomerge duplicate paths in the word lattices to increaseparaphrase capacity, and to speed up the decodingprocess via CN decoding.
Details of the proposedmethod are presented in the following section.3 Confusion-network-based methodCNs are weighted direct graphs where each pathfrom the start node to the end node goes throughall the other nodes.
Each edge is labelled with aword and a probability (or weight).
Although it iscommonly required to normalize the probability ofedges between two consecutive nodes to sum up toone, from the point of view of the decoder, this isnot a strict constraint as long as any score is pro-vided (similar to the weights on the word lattices inthe last section, and we prefer to call it ?weight?
inthis case).The benefits of using CNs are:1. the ability to represent the original word latticewith a highly compact structure;2. all hypotheses in the word lattice are totally or-dered, so that the decoding algorithm is mostlyretained except for the collection of translationoptions and the handeling of  edges (Bertoldiet al, 2008), which requires much less compu-tational resources than the lattice decoding.The rest of this section details the construction pro-cess of the CNs and the application in paraphrase-enriched SMT.3.1 Confusion Network buildingWe build our CN from the aforementioned word lat-tices.
Previous studies provide several methods todo this.
(Mangu et al, 2000) propose a method tocluster lattice words on the similarity of pronuncia-tions and frequency of occurrence, and then to createCNs using cluster orders.
Although this method hasa computational complexity of O(n3), the SRILMtoolkit (Stolcke, 2002) provides a modified algo-rithm which runs much faster than the original ver-sion.
In (Hakkani-Tu?r et al, 2005), a pivot algorithmis proposed to form CNs by normalizing the topol-ogy of the input lattices.In this paper, we use the modified methodof (Mangu et al, 2000) provided by the SRILMtoolkit to convert word lattices into CNs.
Moreover,we aim to obtain CNs with the following guidelines:?
Cluster the lattice words only by topologicalorders and edge weights without consideringword similarity.
The objective is to reduce the34Figure 2: An example of a real paraphrase lattice.
Note that it is a subsection of the whole word lattice that is too bigto fit into this page, and edge weights have been evenly distributed for CN conversion as specified by formula (5).impact of path duplications in the building pro-cess, since duplicate words will bias the impor-tance of paths.?
Assign edge weights by the ranking of para-phrase probabilities, rather than by poste-rior probabilities from the modified methodof (Mangu et al, 2000).
This is similar to thatgiven in formula (4).
The reason for this is toreduce the impact of path duplications on thecalculation of weights.Thus, we modified the construction process as fol-lows:1.
For each of the input word lattices, replaceword texts with unique identifiers (to make thelattice alignment uncorrelated to the word sim-ilarity, since in this case, all words in the latticeare different from each other).2.
Evenly distribute edge weights for each of thelattices by modifying formula (4) as in (5):w(ejpi) =1Mi?
(k + i)(1 <= i <= k) (5)where 1 <= j <= Mi, given ejpi is the jthedge of paraphrase pi, and Mi is the number ofwords in pi.
This is to avoid large weights onthe paraphrase edges for lattice alignments.3.
Transform the weighted word lattices into CNswith the SRILM toolkit, and the paraphraseranking information is carried on the edges.4.
Replace the word texts in step 1, and then foreach column of the CN, merge edges with samewords by keeping those with the highest rank-ing (a smaller number indicates a higher rank-ing, and edges from the original sentences willalways have the highest ranking).
Note that toassign ranking for each  edge which does notappear in the word lattice, we use the ranking ofnon-original edges (in the same column) whichhave the closest posterior probability to it.
(As-sign ranking 1 if failed to find a such edge).5.
Reassign the edge weights: 1) edges from orig-inal sentences are assigned with weight 1; 2)edges from paraphrases are assigned with an35empirical method as in (6):w(ecnpi ) =1k + i(1 <= i <= k) (6)where ecnpi are edges corresponding with para-phrase pi, and i is the probability rank of pi informula (4), while k is also defined in formula(4).A real example of a constructed CN is depictedin Figure 3, which is correspondent with the wordlattice in Figure 2.
Unlike the word lattices, all thenodes in the CN are generated from the original sen-tence, while solid lined edges come from the orig-inal sentence, and dotted lined edges correspond toparaphrases.As in shown in the Figures, duplicate paths in theword lattices have been merged into CN edges bystep 4.
For example, the two occurrences of ?sec-retary of state?
in the word lattices (one path fromnode 6 to 11 via 27 and 28, and one path from node6 to 11 via 26 and 9 in the word lattice) are mergedto keep the highest-ranked path in the CN (notethere is one  edge between node 9 and 10 to ac-complish the merging operation).
Furthermore, eachedge in the CN is assigned a weight by formula (6).This weight assignment procedure penalizes pathsfrom paraphrases according to the paraphrase prob-abilities, in a similar manner to the aforementionedword-lattice-based method.3.2 Modified MT pipelineBy transforming word lattices into CNs, dupli-cate paths are merged.
Furthermore the new fea-tures on the edges are introduced by formula (6),which is then tuned on the development set usingMERT (Och, 2003) in the log-linear model (Och andNey, 2002).
Since the SMT decoders are able toperform CN decoding (Bertoldi et al, 2008) in anefficient multi-stack decoding way, decoding time isdrastically reduced compared to lattice decoding.The training steps are then modified as fol-lows: 1) Extract phrase table, reordering table, andbuild target-side language models from parallel andmonolingual corpora respectively for the PBSMTmodel; 2) Transform source sentences in the devel-opment set into word lattices, and then transformthem into CNs using the method proposed in Sec-tion 3.1; 3) Tune the PBSMT model on the CNs viathe development set.
Note that the overhead of theevaluation steps are: transform each test set sentenceinto a word lattice, and also transform them into aCN, then feed them into the SMT decoder to obtaindecoding results.4 Experiments4.1 Experimental setupExperiments were carried out on three English?Chinese translation tasks.
The training corpora com-prise 20K, 200K and 2.1 million sentence pairs,where the former two corpora are derived from FBIScorpus1 which is sentence-aligned by Champollionaligner (Ma, 2006), the latter corpus comes fromHK parallel corpus,2 ISI parallel corpus,3 other newsdata and parallel dictionaries from LDC.The development set and the test set for the 20Kand 200K corpora are randomly selected from theFBIS corpus, each of which contains 1,200 sen-tences, with one reference.
For the 2.1 million cor-pus, the NIST 2005 Chinese?English current set(1,082 sentences) with one reference is used as thedevelopment set, and NIST 2003 English?Chinesecurrent set (1,859 sentences) with four references isused as the test set.Three baseline systems are built for comparison:Moses PBSMT baseline system (Koehn et al, 2007),a realization of the translation model augmentationsystem described in (Callison-Burch et al, 2006)(named ?Para-Sub?
hereafter), and the word-latticebased system proposed in (Du et al, 2010).Word alignments on the parallel corpus are per-formed using GIZA++ (Och and Ney, 2003) withthe ?grow-diag-final?
refinement.
Maximum phraselength is set to 10 words and the parameters in thelog-linear model are tuned by MERT (Och, 2003).All the language models are 5-gram built with theSRILM toolkit (Stolcke, 2002) on the monolingualpart of the parallel corpora.4.2 Paraphrase acquisitionThe paraphrases data for all paraphrase-enrichedsystem is derived from the ?Paraphrase Phrase Ta-1Paragraph-aligned corpus with LDC numberLDC2003E14.2LDC number: LDC2004T08.3LDC number: LDC2007T09.36Figure 3: An example of a real CN converted from a paraphrase lattice.
Note that it is a subsection of the whole CNthat is converted from the word lattice in Figure 2.ble?4 of TER-Plus (Snover et al, 2009).
Further-more, the following two steps are taken to filter outnoise paraphrases as described in (Du et al, 2010):1.
Filter out paraphrases with probabilities lowerthan 0.01.2.
Filter out paraphrases which are not observedin the phrase table.
This objective is to guar-antee that no extra out-of-vocabulary words areintroduced into the paraphrase systems.The filtered paraphrase table is then used to generateword lattices and CNs.4.3 Experimental resultsThe results are reported in BLEU (Papineni et al,2002) and TER (Snover et al, 2006) scores.Table 1 compares the performance of four sys-tems on three translation tasks.
As can be observedfrom the Table, for 20K and 200K corpora, theword-lattice-based system accomplished the best re-sults.
For the 20K corpus, the CN outperformedthe baseline PBSMT by 0.31 absolute (2.15% rel-ative) BLEU points and 1.5 absolute (1.99% rela-tive) TER points.
For the 200K corpus, it still out-performed the ?Para-Sub?
by 0.06 absolute (0.26%relative) BLEU points and 0.15 absolute (0.23% rel-ative) TER points.
Note that for the 2.1M corpus,although CN underperformed the best word latticeby an insignificant amount (0.06 absolute, 0.41%4http://www.umiacs.umd.edu/?snover/terp/downloads/terp-pt.v1.tgzrelative) in terms of BLEU points, it has the bestperformance in terms of TER points (0.22 abso-lute, 0.3% relative than word lattice).
Furthermore,the CN outperformed ?Para-Sub?
by 0.36 absolute(2.55% relative) BLEU points and 1.37 absolute(1.84% relative) TER points, and also beat the base-line PBSMT system by 0.45 absolute (3.21% rela-tive) BLEU points and 1.82 absolute (2.43% rela-tive) TER points.
The paired 95% confidence in-terval of significant test (Zhang and Vogel, 2004)between the ?Lattice?
and ?CN?
system is [-0.19,+0.38], which also suggests that the two system hasa comparable performance in terms of BLEU.In Table 2, decoding time on test sets is re-ported to compare the computational efficiency ofthe baseline PBSMT, word-lattice-based and CN-based methods.
Note that word lattice construc-tion time and CN building time (including word lat-tice construction and conversion from word latticesinto CNs with the SRILM toolkit (Stolcke, 2002))are counted in the decoding time and illustrated inthe table within parentheses respectively.
Althoughboth word-lattice-based and CN-based methods re-quire longer decoding times than the baseline PB-SMT system, it is observed that compared with theword lattices, CNs reduced the decoding time signif-icantly on three tasks, namely 52.06% for the 20Kmodel, 75.75% for the 200K model and 78.88% forthe 2.1M model.
It is also worth noting that the?Para-Sub?
system has a similar decoding time withbaseline PBSMT since only the translation table ismodified.3720K 200K 2.1MSystem BLEU TER BLEU TER BLEU TERBaseline PBSMT 14.42 75.30 23.60 63.65 14.04 74.88Para-Sub 14.78 73.75 23.41 63.84 14.13 74.43Word-lattice-based 15.44 73.06 25.20 62.37 14.55 73.28CN-based 14.73 73.8 23.47 63.69 14.49 73.06Table 1: Comparison on PBSMT, ?Para-Sub?, word-lattice and CN-based methods.SystemFBIS testset (1,200 inputs) NIST testset (1,859 inputs)20K model 200K model 2.1M modelBaseline 21 min 41 min 37 minLattice 102 min (+ 15 sec) 398 min (+ 20 sec) 559 min (+ 21 sec)CN 48 min (+ 61 sec) 95 min (+ 96 sec) 116 min (+ 129 sec)Table 2: Decoding time comparison of PBSMT, word-lattice (?Lattice?)
and CN-based (?CN?)
methods.4.4 DiscussionFrom the performance and decoding time reported inthe last section, it is obvious that on large scale cor-pora, the CN-based method significantly reduced thecomputational complexity while preserved the sys-tem performance of the best lattice-based method.Thus it makes the paraphrase-enriched SMT systemmore applicable to real-world applications.
On theother hand, for small- and medium-scale data, CNscan be used as a compromise between speed andquality, since decoding time is much less than wordlattices, and compared with the ?Para-Sub?
system,the only overhead is the transforming of the inputsentences.It is also interesting that the relative performanceof the CNs increases gradually with the size of thetraining corpus, which indicates that it is more suit-able for models built from large scale data.
Consid-ering the decoding time, it is preferable to use CNsinstead of word lattices for such translation tasks.However, for the small- and medium-scale data, theCN system is not competitive even compared withthe baseline.
In this case it suggests that, on thesetwo tasks, the coverage issue is not solved by in-corporating paraphrases with the CN structure.
Itmight because of the ambiguity that introduced byCNs harms the decoder to choose the appropriatesource words from paraphrases.
On the other hand,this ambiguity could be decreased with translationmodels trained on a large corpus, which providesenough observations for the decoders to favour para-phrases.5 Conclusion and future workIn this paper, CNs are used instead of word latticesto incorporate paraphrases into SMT.
Transforma-tion from word lattices into CNs is used to mergepath duplications, and decoding time is drasticallyreduced with CN decoding.
Experiments are carriedout on small-, medium- and large-scale English?Chinese translation tasks and confirm that comparedwith word lattices, it is much more computationallyefficient to use CNs, while no loss of performance isobserved on the large-scale task.In the future, we plan to apply more featuressuch as source-side language models and phraselength (Onishi et al, 2010) on the CNs to obtain bet-ter system performance.
Furthermore, we will carryout this work on other language pairs to show the ef-fectiveness of paraphrases in SMT systems.
We willalso investigate the reason for its lower performanceon the small- and medium-scale corpora, as well asthe impact of the paraphrase filtering procedure ontranslation quality.AcknowledgmentsThis research is supported by the Science Founda-tion Ireland (Grant 07/CE/I1142) as part of the Cen-tre for Next Generation Localisation (www.cngl.ie)at Dublin City University.
Thanks to the reviewersfor their invaluable comments and suggestions.38ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In 43rd An-nual meeting of the Association for ComputationalLinguistics, Ann Arbor, MI, pages 597?604.Nicola Bertoldi, Richard Zens, Marcello Federico, andWade Shen 2008.
Efficient Speech TranslationThrough Confusion Network Decoding.
In IEEETransactions on Audio, Speech, and Language Pro-cessing, 16(8), pages 1696?1705.Francis Bond, Eric Nichols, Darren Scott Appling andMichael Paul.
2008.
Improving Statistical MachineTranslation by Paraphrasing the Training Data.
InProceedings of the International Workshop on SpokenLanguage Translation (IWSLT), Hawaii, pages 150?157.Chris Callison-Burch, Philipp Koehn and Miles Osborne.2006.
Improved Statistical Machine Translation UsingParaphrases.
In Proceedings of the Human LanguageTechnology conference - North American chapter ofthe Association for Computational Linguistics (HLT-NAACL), NY, pages 17?24.Jinhua Du, Jie Jiang and Andy Way.
2010.
FacilitatingTranslation Using Source Language Paraphrase Lat-tices.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Cambridge, MA, pages 420?429.Dilek Hakkani-Tu?r, Fre?de?ric Be?chet, Giuseppe Riccardiand Gokhan Tur.
2005.
Beyond ASR 1-best: Usingword confusion networks in spoken language under-standing.
In Computer Speech and Language (2005):20(4), pages 495?514.Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, Wade Shen, C.Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin andEvan Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In ACL 2007: demoand poster sessions, Prague, Czech Republic, pages177?180.Roland Kuhn, Boxing Chen, George Foster and EvanStratford.
2010.
Phrase Clustering for Smoothing TMProbabilities - or, How to Extract Paraphrases fromPhrase Tables.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), Beijing, China, pages 608?616.Xiaoyi Ma.
2006.
Champollion: A Robust Parallel TextSentence Aligner.
LREC 2006: Fifth InternationalConference on Language Resources and Evaluation,Genova, Italy, pages 489?492.Nitin Madnani, Necip Fazil Ayan, Philip Resnik and Bon-nie J. Dorr.
2007.
Using Paraphrases for ParameterTuning in Statistical Machine Translation.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, Prague, Czech Republic, pages 120?127.Lidia Mangu, Eric Brill and Andreas Stolcke.
2000.Finding Consensus in Speech Recognition: Word Er-ror Minimization and Other Applications of ConfusionNetworks.
In Computer Speech and Language 14 (4),pages 373?400.Yuval Marton, Chris Callison-Burch and Philip Resnik.2009.
Improved Statistical Machine Translation Us-ing Monolingually-Derived Paraphrases.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), Singapore, pages381?390.Aure?lien Max.
2010.
Example-Based Paraphrasingfor Improved Phrase-Based Statistical Machine Trans-lation.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,Cambridge, MA, pages 656?666.Preslav Nakov.
2008a.
Improved Statistical MachineTranslation Using Monolingual Paraphrases In Pro-ceedings of the European Conference on Artificial In-telligence (ECAI), Patras, Greece, pages 338?342.Preslav Nakov.
2008b.
Improving English-Spanish sta-tistical machine translation: experiments in domainadaptation, sentence paraphrasing, tokenization, andrecasing.
In Proceedings of ACL-08:HLT.
Third Work-shop on Statistical Machine Translation, Columbus,Ohio, USA, pages 147?150.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 160?167.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), Philadelphia, PA, pages 295?302.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1), pages 19?51.Takashi Onishi, Masao Utiyama and Eiichiro, Sumita.2010.
Paraphrase Lattice for Statistical MachineTranslation.
In Proceedings of the ACL 2010 Confer-ence Short Papers, Uppsala, Sweden, pages 1?5.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method For AutomaticEvaluation of Machine Translation.
ACL-2002: 40thAnnual meeting of the Association for ComputationalLinguistics, pp.311-318, Philadelphia, PA.Josh Schroeder, Trevor Cohn and Philipp Koehn.
2009.Word Lattices for Multi-Source Translation.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), Athens, Greece,pages 719?727.39Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas, Cam-bridge, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie J.Dorr andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, Athens,Greece, pages 259?268.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing (ICSLP),Denver, Colorado, pages 901?904.Ying Zhang and Stephan Vogel.
2004.
Measuring confi-dence intervals for the machine translation evaluationmetrics.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI).
pages 85?94.40
