Proceedings of NAACL HLT 2007, pages 268?275,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCombining Reinforcement Learning with Information-State Update Rules?Peter A. HeemanCenter for Spoken Language UnderstandingOregon Health & Science UniversityBeaverton OR, 97006, USAheeman@cslu.ogi.eduAbstractReinforcement learning gives a way tolearn under what circumstances to per-form which actions.
However, this ap-proach lacks a formal framework for spec-ifying hand-crafted restrictions, for speci-fying the effects of the system actions, orfor specifying the user simulation.
The in-formation state approach, in contrast, al-lows system and user behavior to be spec-ified as update rules, with preconditionsand effects.
This approach can be usedto specify complex dialogue behavior ina systematic way.
We propose combiningthese two approaches, thus allowing a for-mal specification of the dialogue behavior,and allowing hand-crafted preconditions,with remaining ones determined via rein-forcement learning so as to minimize dia-logue cost.1 IntroductionTwo different approaches have become popular forbuilding spoken dialogue systems.
The first is thesymbolic reasoning approach.
Speech actions aredefined in a formal logic, in terms of the situationsin which they can be applied, and what effect theywill have on the speaker?s and the listener?s mentalstate (Cohen and Perrault, 1979; Allen and Perrault,1980).
One of these approaches is the informationstate (IS) approach (Larsson and Traum, 2000).
Theknowledge of the agent is formalized as the state.The IS state is updated by way of update rules,which have preconditions and effects.
The precondi-tions specify what must be true of the state in order?The author wishes to thank Fan Yang and Michael En-glish for helpful conversations.
Funding from the NationalScience Foundation under grant IIS-0326496 is gratefully ac-knowledged.to apply the rule.
The effects specify how the statechanges as a result of applying the rule.
At a mini-mum, two sets of update rules are used: one set, un-derstanding rules, specify the effect of an utteranceon the agent?s state and a second set, action rules,specify which speech action can be performed next.For example, a precondition for asking a question isthat the agent does not know the answer to the ques-tion.
An effect of an answer to a question is that thehearer now knows the answer.
One problem withthis approach is that although necessary precondi-tions for speech actions are easy to code, there aretypically many speech actions that can be applied atany point in a dialogue.
Determining which one isthe optimal one is a daunting task for the dialoguedesigner.The second approach for building spoken dia-logue systems is to use reinforcement learning (RL)to automatically determine what action to performin each different dialogue state so as to minimizesome cost function (e.g.
Walker, 2000; Levin et al,2000).
The problem with this approach, however, isthat it lacks the framework of IS to specify the man-ner in which the internal state is updated.
Further-more, sometimes no preconditions are even speci-fied for the actions, even though they are obviousto the dialogue designer.
Thus RL needs to searchover a much larger search space, even over dialoguestrategies that do not make any sense.
This not onlysubstantially slows down the learning procedure, butalso increases the chance of being caught in a locallyoptimal solution, rather than the global optimal.
Fur-thermore, this large search space will limit the com-plexity of the domains to which RL can be applied.In this paper, we propose combining IS and RL.IS update rules are formulated for both the systemand the simulated user, thus allowing RL to use arich formalism for specifying complex dialogue pro-cessing.
The preconditions on the action rules ofthe system, however, only need to specify the neces-268sary preconditions that are obvious to the dialoguedesigner.
Thus, preconditions on the system?s ac-tions might not uniquely identify a single action thatshould be performed in a given state.
Instead, RLis used to determine which of the applicable actionsminimizes a dialogue cost function.In the rest of the paper, we first present an exam-ple domain.
Section 3 gives an overview of apply-ing RL to dialogue strategy and Section 4 gives anoverview of IS.
Section 5 demonstrates that IS canbe used for simulating a dialogue between the sys-tem and a user.
Section 6 demonstrates how IS canbe used with RL.
Section 7 gives results on usinghand-crafted preconditions specified in the IS updaterules to simplify learning dialogue strategies withRL.
Section 8 gives concluding comments.2 Flight Information ApplicationTo illustrate our proposed approach, we use theflight information domain, similar to that of Levinet al (2000).
The goal of the system is to displaya short list of flights that meets the user?s require-ments.
The user is assumed to have a flight in mind,in terms of its destination, origin, airline, departuretime, and number of stops.
The user might be flexi-ble on some of the parameters.
It is assumed that theuser will not change his or her mind depending onwhat flights are found.In this paper, we are focusing on dialogue man-agement issues, and so we use a semantic represen-tation for both the input and output of the system.The system can ask the user the value of parame-ter p with ?askconstraint p?, and the user will an-swer with ?constrain p v?, where v is the user?s pre-system askconstraint fromuser constrain from miamisystem askconstraint touser constrain to sacramentosystem askconstraint departureuser constrain departure 6pmsystem dbquery miami sacremento - 6pmsystem askconstraint airlineuser constrain airline unitedsystem dbquery miami sacremento united ...system askrelax departureuser relax departure yessystem dbquery miami sacremento united ...system output {918 11671 13288}system finishFigure 1: Sample dialogueferred value of the parameter.1 The system can askwhether the user is flexible on the values for parame-ter p with ?askrelax p?, and the user will answer with?relax p a?, where a is either ?yes?
or ?no?.
The sys-tem can do a database query, ?dbquery?, to determinewhether any flights match the current parameters.
Ifno flights exactly match, ?dbquery?
will check if anyflights match according to the relaxed restrictions,by ignoring parameters that the system knows theuser is flexible on.
The system can display the foundflights with ?output?.
It can also quit at any time.
Asample dialogue is given in Fig.
1.3 Reinforcement Learning (RL)Given a set of system actions, a set of states, and acost function that measures the quality of a dialogue,RL searches for an optimal dialogue policy (Suttonand Barto, 1998; Levin et al, 2000).Cost Function: The cost function assesses howgood a dialogue is: the lower the cost, the better thedialogue.
RL uses the cost function to provide feed-back in its search for an optimal strategy.
The costfunction is specified by the dialogue designer, andcan take into account any number of factors, typi-cally including dialogue length and solution quality.System Actions: RL takes as input a finite numberof actions, and for each state, learns which action isbest to perform.
The dialogue designer decides whatthe actions will be, both in terms of how much tocombine into a single action, and how specific eachaction should be.State Variables: RL learns what system action toperform in each state.
The RL states are defined interms of a set of state variables: different values forthe variables define the different states that can exist.The state variables need to include all informationthat the dialogue designer thinks will be relevant indetermining what action to perform next.
Any infor-mation that is thought to be irrelevant is excluded inorder to keep the search space small.Transitions: RL treats a dialogue as a successionof states, with actions causing a transition from onestate to the next.
The transition thus encompassesthe effect of the system making the speech act,1In contrast to Levin, over-answering by the user is not al-lowed.
The system also does not have a general greeting, towhich the user can answer with any of the flight parameters.269the user?s response to the system?s speech act, andthe system?s understanding of the user?s response.Hence, the transition incorporates a user simulation.In applying RL to dialogue policies, the transitionfrom a state-action pair to the next state is usuallymodeled as a probability distribution, and is not fur-ther decomposed (e.g.
Levin et al, 2000).Policy Exploration: RL searches the space of po-lices by determining Q for each state-action pair s-a, which is the minimal cost to get to the final statefrom state s starting with action a.
From the Q val-ues, a policy can be determined: for each state s,choose the action a that has the maximum Q value.Q is determined in an iterative fashion.
The cur-rent estimates for Q for each state-action are used todetermine the current dialogue policy.
The policy,in conjunction with the transition probabilities, areused to produce a dialogue run, which is a sequenceof state-action pairs, each pair having an associatedcost to get to the next state-action pair.
Thus, for adialogue run, the cost from each state-action pair tothe final state can be determined.
These costs areused to revise the Q estimates.To produce a dialogue run, the ?-greedy methodis often used.
In this approach, with probability ?,an action other than the action specified by the cur-rent policy is chosen.
This helps ensure that newestimates are obtained for all state-action pairs, notjust ones in the current policy.
Typically, a numberof dialogue runs, an epoch, are made before the Qvalues and dialogue policy are updated.
With eachsuccessive epoch, a better dialogue policy is used,and thus the Q estimates will approach their true val-ues, which in turn, ensures that the dialogue policyis approaching the optimal one.3.1 Flight Information Task in RLTo illustrate how RL learns a dialogue policy, we usethe flight information task from Section 2.Actions: The system actions were given in Section2.
The queries for the destination, origin, airline, de-parture time, number of stops are each viewed as dif-ferent actions so that RL can reason about the indi-vidual parameters.
There are also 5 separate queriesfor checking whether each parameter can be relaxed.There is also a database query to determine whichflights match the current parameters.
This is in-cluded as an RL action, even though it is not to theuser, so that RL can decide when it should be per-formed.
There is also an output and a finish action.State Variables: We use the following variablesfor the RL state.
The variable ?fromP?
indicateswhether the origin has been given by the user andthe variable ?fromR?
indicates whether the user hasbeen asked if the origin can be relaxed, and if so,what the answer is.
Similar variables are used for theother parameters.
The variable ?dbqueried?
indicateswhether the database has been queried.
The variable?current?
indicates whether no new parameters havebeen given or relaxed since the last database query.The variable ?NData?
indicates the number of itemsthat were last returned from the database quantizedinto 5 groups: none, 1-5, 6-12, 13-30, more than 30).The variable ?outputP?
indicates whether any flightshave been given to the user.
Note that the actual val-ues of the parameters are not included in the state.This helps limit the size of the search space, but pre-cludes the values of the parameters from being usedin deciding what action to perform next.Cost Function: Our cost function is the sum offour components.
Each speech action has a cost of1.
A database query has a cost of 2 plus 0.01 for eachflight found.
Displaying flights to the user costs 0 for5 or fewer flights, 8 for 12 or fewer flights, 16 for 30or fewer flights, and 25 for 30 or more flights.
Thelast cost is the solution cost.
This cost takes into ac-count whether the user?s preferred flight is even inthe database, and if so, whether it was shown to theuser.
The solution cost is zero if appropriate infor-mation is given to the user, and 90 points otherwise.3.2 Related Work in RLIn the work of Levin, Pieraccini, and Eckert (2000),RL was used to choose between all actions.
Actionsthat resulted in infelicitous speech act sequenceswere allowed, such as asking the value of a parame-ter that is already known, asking if a parameter canbe relaxed when the value of the parameter is noteven known, or displaying values when a databasequery has not yet been performed.In other work, RL has been used to choose amonga subset of the actions in certain states (Walker,2000; Singh et al, 2002; Scheffler and Young, 2002;English and Heeman, 2005).
However, no for-mal framework is given to specify which actions tochoose from.270Furthermore, none of the approaches used a for-mal specification for updating the RL variables aftera speech action, nor for expressing the user simula-tion.
As RL is applied to more complex tasks, withmore complex speech actions, this will lead to diffi-culty in encoding the correct behavior.Georgila, Henderson, and Lemon (2005) advo-cated the use of IS to specify the dialogue contextfor learning user simulations needed in RL.
How-ever, they did not combine hand-crafted with learnedpreconditions, and it is unclear whether they used ISto update the dialogue context,4 Information State (IS)IS has been concerned with capturing how to up-date the state of a dialogue system in order to buildadvanced dialogue systems (Larsson and Traum,2000).
For example, it has been used to build sys-tems that allow for both system and user initiative,over answering, confirmations, and grounding (e.g.
(Bohlin et al, 1999; Matheson et al, 2000)).
It usesa set of state variables, whose values are manipu-lated by update rules, run by a control strategy.State Variables: The state variables specify theknowledge of the system at any point in the dia-logue.
This is similar to the RL variables, except thatthey must contain everything that is needed to com-pletely specify the action that the system should per-form, rather than just enough information to choosebetween competing actions.
A number of stan-dard variables are typically used to interface to othermodules in the system.
The variable ?lastMove?
hasthe semantic representation of what was last said, ei-ther by the user or the system and ?lastSpeaker?
in-dicates who spoke the last utterance.
Both are read-only.
The variable ?nextMove?
is set by the actionrules to the semantic representation of the next moveand ?keepTurn?
is set to indicate whether the currentspeaker will keep the turn to make another utterance.Update Rules: Update rules have preconditionsand effects.
The preconditions specify what mustbe true of the state in order to apply the rule.
The ef-fects specify how the state should be updated.
In thispaper, we will use two types of rules.
Understand-ing rules will be used to update the state to take intoaccount what was just said, by both the user and thesystem.
Action rules determine what the system willsay next and whether it will keep the turn.Control Strategy: The control strategy specifieshow the update rules should be processed.
In our ex-ample, the control strategy specifies that the under-standing rules are processed first, and then the actionrules if the system has the turn.
The control strategyalso specifies which rules should be applied: (a) justthe first applicable rule, (b) all applicable rules, or(c) randomly choose one of the applicable rules.Although there is a toolkit available for buildingIS systems (Larsson and Traum, 2000), we built asimple version in Tcl.
Update rules are written usingTcl code, which allows for simple interpretation ofthe rules.
The state is saved as Tcl variables, andthus allows strings, numbers, booleans, and lists.4.1 Flight Information Example in ISWe now express the flight information system withthe IS approach.
This allows for a precise formaliza-tion of the actions, both the conditions under whichthey should be performed and their effects.The IS state variables are similar to the RL onesgiven in Section 3.
Instead of the variable ?fromP?,it includes the variable ?from?, which has the actualvalue of the parameter if known, and ??
otherwise.The same is true for the destination, airline, depar-ture time, and number of stops.
Instead of the RLvariable ?NData?
and ?outputP, ?results?
holds theactual database and ?output?
holds the actual flightsdisplayed to the user.Figure 2 displays the system?s understandingrules, which are used to update the state variablesafter an utterance is said.
Although it is commonpractice in IS to use understanding rules even forone?s own utterances, the example application issimple enough to do without this.
Understandingrules are thus only used for understanding the user?sutterances: giving a parameter value or specifyingwhether a parameter can be relaxed.
As can be seen,any time the user specifies a new parameter or re-laxes a parameter, ?current?
is set to false.Figure 3 gives the action rules for the system.Rules for querying the destination, departure, andnumber of stops are not shown; neither are the rulesfor querying whether the destination, origin, airline,and number of stops can be relaxed.
The effects ofthe rules show how the state is updated if the ruleis applied.
For most of the rules, this is simply to271set ?nextMove?
and ?keepTurn?
appropriately.
The?dbquery?
action is more complicated: it runs thedatabase query and updates ?results?.
It then updatesthe variables ?queriedDB?, and ?current?
appropri-ately.
Note that the actions ?dbquery?
and ?output?specify that the system wants to keep the turn.The preconditions of the update rules specify theexact conditions under which the rule can be ap-plied.
The preconditions on the understanding rulesare straightforward, and simply check the user?s re-sponse.
The preconditions on the action rules aremore complex.
We divide the preconditions into the4 groups given below, both to simplify the discus-sion of the preconditions, and because we use thesegroupings in Section 7.Speech Acts: Some of the preconditions cap-ture the conditions under which the action can beperformed felicitously (Cohen and Perrault, 1979;Allen and Perrault, 1980).
Only ask the value ofa parameter if you do not know its value.
Only askif a parameter can be relaxed if you know the valueof the parameter.
Only output the data if it is stillcurrent and more than one flight was found.
Thesepreconditions are labeled as ?sa?
in Fig.
3.Application Restrictions: These preconditionsenforce the specification of the application.
Forour application, the system should only output dataonce: once data is output, the system should endthe conversation.
These preconditions are labeledas ?app?
in Fig.
3.Partial Strategy: These preconditions add addi-tion constraints that seem reasonable: ask the ?to?,?from?, and ?departure?
parameters first; never relaxthe ?to?
and ?from?
; and only ask whether ?airline?and ?stops?
can be relaxed if the database has beenUnderstand Answer to Constrain QuestionPre: [lindex $lastMove 0] == ?constrain?Eff: set [lindex LastMove 1] [lindex LastMove 2]set current 0Understand Yes Answer to RelaxPre: [lindex lastMove 0] == ?relax?
[lindex lastMove 2] == ?yes?Eff: set [lindex lastMove 1]R yesset current 0Understand No Answer to RelaxPre: [lindex lastMove 0] == ?relax?
[lindex lastMove 2] == ?no?Eff: set [lindex lastMove 1]R noFigure 2: Understanding Rules for Systemqueried.
Furthermore, the system may only outputdata if (a) the number of flights is between 1 and5, or (b) the number of flights is greater than 5 and?airline?
and ?stops?
have both been asked.
Thesepreconditions are labeled as ?ps?
in Fig.
3.Baseline: The last group of preconditions (to-gether with the previous preconditions) uniquelyidentify a single action to perform in each state, andAsk Origin of FlightPre: $from == ??
sa$output == ??
appEff: set nextMove ?askconstraint from?set keepTurn falseAsk Airline of FlightPre: $airline == ??
sa$output == ??
app$departure != ??
ps$queriedDB == true base$current == true base[llength $results] > 5 baseEff: set nextMove ?askconstraint to?set keepTurn falseAsk Whether Departure Time can be RelaxedPre: $departure != ??
sa$departureR == ??
sa$output != ??
app$queriedDB == true base$current == true base$results == {} baseEff: set nextMove ?askrelax from?set keepTurn falseQuery the DatabasePre: $current == false sa$output == ??
app$departure != ??
psEff: set results [DBQuery $from $to $airline ...]set queriedDB trueset current trueset nextMove dbqueryset keepTurn trueOutput Results to UserPre: $current == true sa$results != {} sa$output == ??
app[llength $results] < 6 || ([llength $results] > 5 ps&& $airline != ??
&& $stops != ??
)Eff: set nextMove ?output $results?set output $resultsFinishPre: $output != ??
appEff: set nextMove finishQuitPre: $output == ??
app$current == true app$results == {} app$airline != ??
|| $airlineR != ??
base$stops != ??
|| $stopsR != ??
baseEff: set nextMove finishFigure 3: Action Rules for System272thus completely specifies a strategy.
These are la-beled as ?base?
in Fig.
3.
The strategy that we giveis based on the optimal strategy found by Levin etal.
(2000).
After the system asks the values for the?from?, ?to?, and ?departure?
variables, it then per-forms a database query.
If there are between 1 and 5flights found, they are displayed to the user.
If thereare more than 5, the system asks the value of ?air-line?
if unknown, otherwise, ?number of stops?.
Ifthere are 0 items, it tries to relax one of ?departure?,?airline?, and ?stops?, in that order (but not ?from?or ?to?).
Any time new information is gained, suchas a parameter value or a parameter is relaxed, thedatabase is requeried, and the process repeats.5 Implementing the Simulated UserNormally, with IS, the system is run against an ac-tual user, and so no state variables nor update rulesare coded for the user.
To allow the combination ofIS with RL, we need to produce dialogues betweenthe system and a simulated user.
As the IS approachis very general, we will use it for implementing thesimulated user as well.
In this way, we can code theuser simulation with a well-defined formalism, thusallowing complex user behaviors.
Hence, two sepa-rate IS instantiations will be used: one for the systemand one for the user.
The system?s rules will updatethe system?s state variables, and the user?s rules willupdate the user?s state variables; but the two instan-tiations will be in lock-step with each other.We built a simulator that runs the system?s rulesagainst the user?s.
The simulator (a) runs the under-standing rules for the system and the user on the lastutterance; then (b) checks who has the turn, and runsthat agent?s action rules; and then (c) updates ?lastS-peaker?
and ?lastMove?.
It repeats these three stepsuntil the ?finish?
speech act is seen.5.1 Flight Information TaskThe user has the variables ?from?, ?to?, ?departure?,?airline?, and ?stops?, which hold the user?s idealflight, and are set before the dialogue begins.
Thevariables ?fromR?, ?toR?, ?departureR?, ?airlineR?,and ?stopsR?
are also used, and are also set beforethe dialogue begins.
No other variables are used.For the flight application, separate update rulesare used for the user.
There are two types of queriesAnswer Constrain QuestionPre: [lindex $lastMove 0] == ?askconstraint?Eff: set nextMove ?constraint [lindex $lastmove 1][set [lindex $lastmove 1]]?set haveTurn 0Answer Relax QuestionPre: [lindex $lastMove 0] == ?askrelax?Eff: set nextMove ?relax [lindex $lastmove 1][set [lindex $lastMove 1]R]?set haveTurn 0Figure 4: Action Rules for Userto which the user needs to react, namely, ?askcon-traint?
and ?askrelax?.
This domain is simple enoughthat we do not need separate understanding and ac-tion rules, and so we encompass all reasoning in theaction rules, shown in Fig.
4.
The first rule is foranswering system queries about the value of a pa-rameter.
The second is for answering queries aboutwhether a parameter can be relaxed.6 Combining IS and RLRL gives a way to learn the best action to perform inany given state.
However, RL lacks a formal frame-work for specifying (a) the effects of the system?sactions, (b) hand-crafted preconditions of the sys-tem?s actions, and (c) the simulated user.
Hence, wecombine RL and IS to rectify these deficits.
IS up-date rules are formulated for both the system and thesimulated user, as done in Section 5.1.
The precon-ditions on the system?s action rules, however, onlyneed to specify a subset of the preconditions, onesthat are obvious the dialogue designer.
The rest ofthe preconditions will be determined by RL, so as tominimize a cost function.
To combine these two ap-proaches, we need to (a) resolve how the IS and RLstate transitions relate to each other; (b) resolve howthe IS state relates to the RL state; and (c) specifyhow utterance costs can be specified in the generalframework of IS.Transitions: When using IS for both the systemand user simulation, the state transitions for eachare happening in lock-step (Section 5.1).
In com-bining RL and IS, the RL transitions happen at acourser granularity than the IS transitions, and grouptogether everything that happens between two suc-cessive system actions.
Thus, the RL states are thoseIS states just before a system action.273State Variables: For the system, we add all of theRL variables to the IS variables, and remove any du-plicates.
The RL variables are thus a subset of the ISvariables.
Some of the variables might be simplifica-tions of other variables.
For our flight example, wehave the exact values of the origin, destination, air-line, departure time, and number of stops, as well asa simplification of each that only indicates whetherthe parameter has been given or not.Rather than have the system?s IS rules updateall of the variables, we allow variables to be de-clared as either primitive or derived.2 Only primitivevariables are updated by the effects of the updaterules.
The derived variables are re-computed fromthe primitive ones each time an update rule is ap-plied.
For our flight example, the variables ?fromP?,?toP?, ?airlineP?, ?departureP?, ?stopsP?, ?outputP?,and ?NData?
are derived variables, and these are up-dated via a procedure.As the RL variables are a subset of the IS vari-ables, the RL states are coarser than the IS states.We do not allow hand-crafted preconditions in thesystem?s action rules to distinguish at the finer gran-ularity.
If they did, we would have an action that isonly applicable in part of an RL state, and not therest of it.
However, RL needs to find a single actionthat will work for the entire RL state, and so thataction should not be considered.
To prevent suchproblems, the hand-crafted preconditions can onlytest the values of the RL variables, and not the fullset of IS variables.
Hence, we rewrote the precon-ditions in the action rules of Fig.
3 to use the RLvariables.
This restriction does not apply to the sys-tem?s understanding rules, nor to the user rules, asthose are not subject to RL.Cost Function: RL needs to track the costs in-curred in the dialogue.
Rather than leaving this tobe specified in an ad-hoc way, we include state vari-ables to track the components of the cost.
This way,each update rule can set them to reflect the cost ofthe rule.
Just as with other interface variables (e.g.?keepTurn?
), these are write-only.
For our flight ex-ample, the output action computes the cost of dis-playing flights to the user, and the database query ac-tion computes the cost of doing the database lookup.2This same distinction is sometimes used in the planningliterature (Poole et al, 1998).7 EvaluationTo show the usefulness of starting RL with some ofthe preconditions hand-crafted, we applied RL usingfour different sets of action schemes.
The first set,?none?, includes no preconditions on any of the sys-tem?s actions.
The second through fourth sets cor-respond to the precondition distinctions in Fig.
3, of?speech act?, ?application?
and ?partial strategy?.For each set of action schemas, we trained 30 di-alogue policies using an epoch size of 100.
Each di-alogue was run with the ?-greedy method, with ?
setat 0.15.
After certain epochs, we ran the learned pol-icy 2500 times strictly according to the policy.
Wefound that policies did not always converge.
Hence,we trained the policies for each set of preconditionsfor enough epochs so that the average cost no longerimproved.
More work is needed to investigate thisissue.The results of the simulations are given in Table1.
The first row reports the average dialogue costthat the 30 learned policies achieved.
We see that allfour conditions achieved an average cost less thanthe baseline strategy of Fig.
3, which was 17.17.
Thebest result was achieved by the ?application?
precon-ditions.
This is probably because ?partial?
includedsome constraints that were not optimal, while thesearch strategy was not adequate to deal with thelarge search space in ?speech acts?
and ?none?.The more important result is in the second rowof Table 1.
The more constrained precondition setsresult in significantly fewer states being explored,ranging from 275 for the ?partial?
preconditions, upto 18,206 for no preconditions.
In terms of numberof potential policies explored (computed as the prod-uct of the number of actions explored in each state),this ranges from 1058 to 107931.
As can be seen, byplacing restrictions on the system actions, the spacethat needs to be explored is substantially reduced.The restriction in the size of the search space af-fects how quickly RL takes to find a good solution.Figure 5 shows how the average cost for each set ofNone SA App.
PartialDialogue Cost 16.65 16.95 15.24 15.68States Explored 18206 5261 4080 275Policies (log10) 7931 2008 1380 58.7Table 1: Comparison of Preconditions274152025303540455055601  10  100  1000  10000NoneSpeech ActsApplicationPartialFigure 5: Average dialogue cost versus epochspreconditions improved with the number of epochs.As can be seen, by including more preconditionsin the action definitions, RL is able to find a goodsolution more quickly.
For the ?partial?
precondi-tions, after 10 epochs, RL achieves a cost less than17.0.
For the ?application?
setting, this does not hap-pen until 40 epochs.
For ?speech act?, it takes 1000epochs, and for ?none?, it takes 3700 epochs.
So,adding hand-crafted preconditions allows RL to con-verge more quickly.8 ConclusionIn this paper, we demonstrated how RL and IS canbe combined.
From the RL standpoint, this allowsthe rich formalism of IS update rules to be used forformalizing the effects of the system?s speech ac-tions, and for formalizing the user simulation, thusenabling RL to be applied to domains that requirecomplex dialogue processing.
Second, use of IS al-lows obvious preconditions to be easily formulated,thus allowing RL to search a much smaller space ofpolicies, which enables it to converge more quicklyto the optimal policy.
This should also enable RL tobe applied to complex domains with large numbersof states and actions.From the standpoint of IS, use of RL means thatnot all preconditions need be hand-crafted.
Pre-conditions that capture how one action might bemore beneficial than another can be difficult to deter-mine for dialogue designers.
For example, knowingwhether to first ask the number of stops or the air-line, depends on the characteristics of the flights inthe database, and on users?
relative flexibility withthese two parameters.
The same problems occurfor knowing under which situations to requery thedatabase or ask for another parameter.
RL solvesthis issue as it can explore the space of different poli-cies to arrive at one that minimizes a dialogue costfunction.ReferencesJ.
Allen and C. Perrault.
1980.
Analyzing intention inutterances.
Artificial Intelligence, 15:143?178.P.
Bohlin, R. Cooper, E. Engdahl, and S. Larsson.
1999.Information states and dialogue move engines.
In Pro-ceedings of the IJCAI Workshop: Knowledge and Rea-soning in Practical Dialogue Systems, pg.
25?31.P.
Cohen and C. Perrault.
1979.
Elements of a plan-basedtheory of speech acts.
Cognitive Science, 3(3):177?212.M.
English and P. Heeman.
2005.
Learning mixed ini-tiative dialog strategies by using reinforcement learn-ing on both conversants.
In HLT and EMNLP, pages1011?1018, Vancouver Canada, October.K.
Georgila, J. Henderson, and O.
Lemon.
2005.
Learn-ing user simulations for information state update dia-logue systems.
In Eurospeech, Lisbon Portugal.S.
Larsson and D. Traum.
2000.
Information state and di-alogue management in the TRINDI dialogue move en-gine toolkit.
Natural Language Engineering, 6:323?340.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
IEEE Transactions on Speech andAudio Processing, 8(1):11?23.C.
Matheson, M. Poesio, and D. Traum.
2000.
Mod-elling grounding and discourse obligations using up-date rules.
In NAACL, Seattle, May.D.
Poole, A. Mackworth, and R. Goebel.
1998.
Com-putational Intelligence: a logical approach.
OxfordUniversity Press.K.
Scheffler and S. J.
Young.
2002.
Automatic learningof dialogue strategy using dialogue simulation and re-inforcement learning.
In HLT, pg.
12?18, San Diego.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing dialogue managment with reinforcementlearning: Experiments with the NJfun system.
Jour-nal of Artificial Intelligence Research, 16:105?133.R.
Sutton and A. Barto.
1998.
Reinforcement Learning.MIT Press, Cambridge MA.M.
Walker.
2000.
An application of reinforcement learn-ing to dialog strategy selection in a spoken dialoguesystem for email.
Journal of Artificial Intelligence Re-search, 12:387?416.275
