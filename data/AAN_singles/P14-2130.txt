Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 803?808,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsIncremental Predictive Parsing with TurboParserArne K?ohn and Wolfgang MenzelFachbereich InformatikUniversit?at Hamburg{koehn, menzel}@informatik.uni-hamburg.deAbstractMost approaches to incremental parsingeither incur a degradation of accuracy orthey have to postpone decisions, yield-ing underspecified intermediate output.
Wepresent an incremental predictive depen-dency parser that is fast, accurate, andlargely language independent.
By extend-ing a state-of-the-art dependency parser,connected analyses for sentence prefixesare obtained, which even predict propertiesand the structural embedding of upcomingwords.
In contrast to other approaches, ac-curacy for complete sentence analyses doesnot decrease.1 IntroductionWhen humans communicate by means of a natu-ral language, utterances are not produced at oncebut evolve over time.
Human interaction benefitsfrom this property by processing yet unfinishedutterances and reacting on them.
Computationalparsing on the other hand is mostly performed oncomplete sentences, a processing mode which ren-ders a responsive interaction based on incompleteutterances impossible.When spoken language is analyzed, a mismatchbetween speech recognition and parsing occurs:If parsing does not work incrementally, the over-all system loses all the desirable properties madepossible by incremental processing.
For speech di-alogue systems, this leads to increased reactiontimes and an unnatural ping-pong style of interac-tion (Schlangen and Skantze, 2011).1.1 Desirable features of incremental parsersDependency parsing assigns a head and a depen-dency label to each word form of an input sentenceand the resulting analysis of the sentence is usuallyrequired to form a tree.
An incremental dependencyparser processes a sentence word by word, buildinganalyses for sentence prefixes (partial dependencyanalyses, PDA), which are extended and modifiedin a piecemeal fashion as more words become avail-able.A PDA should come with three important (butpartly contradictory) properties: beyond being ac-curate, it should also be as stable and informative aspossible.
Stability can be measured as the amountof structure (attachments and their labels) of a PDAaiwhich is also part of the analysis anof the wholesentence.
To be maximally informative, at least allavailable word forms should be integrated into theprefix PDA.
Even such a simple requirement cannoteasily be met without predicting a structural skele-ton for the word forms in the upcoming part of thesentence(bottom-up prediction).
Other predictionsmerely serve to satisfy completeness conditions(i.e.
valency requirements) in an anticipatory way(top-down predictions).
In fact, humans are able toderive such predictions and they do so during sen-tence comprehension (Sturt and Lombardo, 2005).Without prediction, the sentence prefix ?Johndrives a?
of ?John drives a car?
can only be parsedas a disconnected structure:John drivesaSBJThe determiner remains unconnected to the rest ofthe sentence, because a possible head is not yetavailable.
However, the determiner could be inte-grated into the PDA if the connection is establishedby means of a predicted word form, which has notyet been observed.
Beuck et al (2011) propose touse virtual nodes (VNs) for this purpose.
Each VNrepresents exactly one upcoming word.
Its lexicalinstantiation and its exact position remain unspeci-fied.
Using a VN, the prefix ?John drives a?
couldthen be parsed as follows, creating a fully con-nected analysis, which also satisfies the valencyrequirements of the finite verb.803John drivesa[VirtNoun]SBJDETOBJThis analysis is clearly more informative but stillrestricted to the existence of a noun filling the ob-ject role of ?drives?
without predicting its position.Although a VN does not specify the lexical identityof the word form it represents, it can nonethelesscarry some information such as a coarse-grainedpart-of-speech category.1.2 Related workParsers that produce incremental output are rel-atively rare: PLTag (Demberg-Winterfors, 2010)aims at psycholinguistic plausibility.
It makes trade-offs in the field of accuracy and coverage (theyreport 6.2 percent of unparseable sentences on sen-tences of the Penn Treebank with less than 40words).
Due to its use of beam search, the incre-mental results are non-monotonic.
Hassan et al(2009) present a CCG-based parser that can parsein an incremental mode.
The parser guaranteesthat every parse of an increment extends the previ-ous parse monotonically.
However, using the incre-mental mode without look-ahead, parsing accuracydrops from 86.70% to 59.01%.
Obviously, insist-ing on strict monotonicity (ai?
an) is too stronga requirement, since it forces the parser to keepattachments that later turn out to be clearly wrongin light of new evidence.Being a transition-based parser, Maltparser(Nivre et al, 2007) does incremental parsing bydesign.
It is, however, not able to predict upcom-ing structure and therefore its incremental output isusually fragmented into several trees.
In addition,Maltparser needs a sufficiently large look-ahead toachieve high accuracy (Beuck et al, 2011).Beuck et al (2011) introduced incremental andpredictive parsing using Weighted Constraint De-pendency Grammar.
While their approach does notdecrease in incremental mode, it is much slowerthan most other parsers.
Another disadvantage isits hand-written grammar which prevents the parserfrom being adapted to additional languages by sim-ply training it on an annotated corpus and whichmakes it difficult to derive empirically valid con-clusions from the experimental results.2 Challenges for predictive parsingExtending a dependency parser to incremental pars-ing with VNs introduces a significant shift in theproblem to be solved: While originally the problemwas where to attach each word to (1), in the incre-mental case the additional problem arises, whichVNs to include into the analysis (2).
Problem (2),however, depends on the syntactic structure of thesentence prefix.
Therefore, it is not possible to de-termine the VNs before parsing commences, butthe decision has to be made while parsing is goingon.
We can resolve this issue by transforming prob-lem (2) into problem (1) by providing the parserwith an additional node, named unused.
It is alwaysattached to the special node 0 (the root node of ev-ery analysis) and it can only dominate VNs.
unusedand every VN it dominates are not considered partof the analysis.
Using this idea, the problem ofwhether a VN should be included into the analysisis now reduced to the problem of where to attachthat VN:John drivesa[VirtNoun] [VirtVerb] [unused]SBJDETOBJTo enable the parser to include VNs into PDAs,a set of VNs has to be provided.
While this setcould include any number of VNs, we only in-clude a set that covers most cases of predictionsince rare virtual nodes have a very low a-prioriprobability of being included and additional VNsmake the parsing problem more complex.
This setis language-dependent and has to be determined inadvance.
It can be obtained by generating PDAsfrom a treebank and counting the occurrences ofVNs in them.
Eventually, a set of VNs is used thatis a super-set of a large enough percentage (> 90%)of the observed sets.3 Gold annotations for sentence prefixesAnnotating sentence prefixes by hand is pro-hibitively costly because the number of incrementsis a multitude of the number of sentences in thecorpus.
Beuck and Menzel (2013) propose an ap-proach to automatically generate predictive depen-dency analyses from the annotation of full sen-tences.
Their method tries to generate upper boundsfor predictability which are relatively tight.
There-fore, not everything that is deemed predictable bythe algorithm is predictable in reality, but every-thing that is predictable should be deemed as pre-dictable: Let W be all tokens of the sentence and Pthe set of tokens that lie in the prefix for which anincremental analysis should be generated.
A wordw ?W \P is assumed to be predictable (w ?
Pr) ifone of the following three criteria is met:8044060801000 1 2 3 4 5 finalpercentagerelative time pointGermancorrectcorrect structural predictionwrong structural predictionwrong4060801000 1 2 3 4 5 finalpercentagerelative time pointEnglishFigure 1: Results for TurboParser for German and English with gold standard PoS (labeled)bottom-up prediction w lies on the path fromsome w??
P to 0.
E. g., given the sentence prefix?The?, an upcoming noun and a verb is predicted:The[VirtNoun] [VirtVerb]top down prediction pi(w), the head of w, is inP?Pr, and w fills a syntactic role ?
encoded by itsdependency label ?
that is structurally determined.That means w can be predicted independently ofthe lexical identity of pi(w).
An example for thisis the subject label: If pi(w) is in Pr and w is itssubject, w is assumed to be predictable.lexical top-down prediction pi(w) ?
P and wfills a syntactic role that is determined by an alreadyobserved lexical item, e.g.
the object role: If pi(w)is a known verb and w is its object, w ?
Pr becauseit is required by a valency of the verb.While this procedure is language-independent,some language-specific transformations must beapplied nonetheless.
For English, parts of gappingcoordinations can be predicted whereas others cannot.
For German, the transformations described in(Beuck and Menzel, 2013) have been used with-out further changes.
Both sets of structurally andlexically determined roles are language dependent.The label sets for German have been adopted from(Beuck and Menzel, 2013), while the sets for En-glish have been obtained by manually analyzingthe PTB (Marcus et al, 1994) for predictability.For words marked as predictable their existenceand word class, but not their lexicalization andposition can be predicted.
Therefore, we replacethe lexical item with ?[virtual]?
and generalize thepart-of-speech tag to a more coarse grained one.4 Predictive parsing with TurboParserWe adapt TurboParser (Martins et al, 2013) forincremental parsing because it does not imposestructural constraints such as single-headedness inits core algorithm.
For each parsing problem, itcreates an integer linear program ?
in the form of afactor graph ?
with the variables representing thepossible edges of the analyses.Since well-formedness is enforced by factors,additional constraints on the shape of analyses canbe imposed without changing the core algorithm ofthe parser.
We define three additional restrictionswith respect to VNs: 1) A VN that is attached tounused may not have any dependents.
2) A VNmay not be attached to 0 if it has no dependents.
3)Only VNs may be attached to the unused node.For a given sentence prefix, let A be the set ofpossible edges, V the set of all vertices, N ?
Vthe VNs and u ?V the unused node.
Moreover, letB?
A be the set of edges building a well-formedanalysis and za, I(a ?
B), where I(.)
is the indica-tor function.
The three additional conditions can beexpressed as linear constraints which ensure thatevery output is a valid PDA:z?n, j?+ z?u,n??
1, n ?
N, j ?V (1)z?0,n??
?j?Vz?n, j?, n ?
N (2)z?u,i?= 0, i ?V \N (3)The current implementation is pseudo-incremen-tal.
It reinitializes the ILP for every increment with-out passing intermediate results from one incremen-tal processing step to the next, although this mightbe an option for further optimization.High quality incremental parsing results can notbe expected from models which have only beentrained on whole-sentence annotations.
If a parseris trained on gold-standard PDAs (generated as de-scribed in section 3), it would include every VNinto every analysis because that data does not in-clude any non-attached VNs.
We therefore add non-attached VNs to the generated PDAs until theycontain at least the set of VNs that is later usedduring parsing.
For instance, each German trainingincrement contains at least one virtual verb and805two virtual nouns and each English one at least onevirtual verb and one virtual noun.
This way, the per-centage of VNs of a specific type being attached inthe training data resembles the a priori probabilitythat a VN of that type should be included by theparser while parsing.TurboParser is trained on these extended PDAsand no adaptation of the training algorithm isneeded.
The training data is heavily skewed be-cause words at the beginning of the sentences aremore prevalent than the ones at the end.
As a com-parison with a version trained on non-incrementaldata shows, this has no noticeable effect on theparsing quality.5 EvaluationThe usual methods to determine the quality of adependency parser ?
labeled and unlabeled attach-ment scores (AS) ?
are not sufficient for the evalu-ation of incremental parsers.
If the AS is computedfor whole sentences, all incremental output is dis-carded and not considered at all.
If every intermedi-ate PDA is used, words at the start of a sentence arecounted more often than the ones at the end.
No in-formation becomes available on how the accuracyof attachments evolves while parsing proceeds, andthe prediction quality (i.e.
the VNs) is completelyignored.
Therefore, we adopt the enhanced modeof evaluation proposed by Beuck et al (2013): Inaddition to the accuracy for whole sentences, theaccuracies of the n newest words of each analy-sis are computed.
This yields a curve that showshow good a word can be assumed to be attacheddepending on its distance to the most recent word.Let ?V,G?
be the gold standard analysis of anincrement and ?V?,P?
the corresponding parser out-put.
V and V?are the vertices and G and P therespective edges of the analyses.
Let V?pand V?vbethe in-prefix and virtual subset of V?, respectively.To evaluate the prediction capabilities of a parser,for each increment an optimal partial, surjectivemapping1V?
?V from the output produced by theparser to the (automatically generated) gold stan-dard is computed, where each non-virtual elementof V?has to be mapped to the corresponding ele-ment in V .
Let M be the set of all such mappings.Then the best mapping is defined as follows:?
= argmaxm?M?w?V?I(pi(m(w)) = m(pi(w)))1The mapping is partial because for some VNs in V?theremight be no corresponding VN in the gold standard.We define a word w as correctly attached (ignor-ing the label) if pi(?
(w)) = ?(pi(w)).
In an incre-mental analysis, an attachment of a word w can beclassified into four cases:correct pi(?
(w)) = ?
(pi(w)), pi(w) ?V?pcorr.
pred.
pi(?
(w)) = ?
(pi(w)), pi(w) ?V?vwrong pred.
pi(?
(w)) 6= ?
(pi(w)), pi(w) ?V?vwrong pi(?
(w)) 6= ?
(pi(w)), pi(w) ?V?pWe can count the number of VNs that have beencorrectly attached: Let T be the set of all analysesproduced by the parser and ?tthe best mapping asdefined above for each t ?
T .
Furthermore, let vn(t)be the set of VNs in t. The total number of correctpredictions of VNs is then defined as:corr =?t?T?v?vn(t)I(pi(?t(v)) = ?t(pi(v)))Precision and recall for the prediction with VNscan be computed by dividing corr by the numberof predicted VNs and the number of VNs in thegold standard, respectively.Evaluation has been carried out on the PTB con-verted to dependency structure using the LTH con-verter (Johansson and Nugues, 2007) and on theHamburg Dependency Treebank (Foth et al, 2014).From both corpora predictive PDAs padded withunused virtual nodes have been created for training.For English, the sentences of part 1-9 of the PTBwere used, for German the first 50,000 sentencesof the HDT have been selected.
Testing was doneusing one virtual noun and one virtual verb for En-glish and two virtual nouns and one virtual verb forGerman because these sets cover about 90% of theprefixes in both training sets.Figure 1 shows the evaluation results for pars-ing German and English using TurboParser.
Forboth languages the attachment accuracy rises withthe amount of context available.
The difference be-tween the attachment accuracy of the most recentword (relative time point 0, no word to the rightof it) and the second newest word (time point 1)is strongest, especially for English.
The word fiveelements left of the newest word (time point 5) getsattached with an accuracy that is nearly as high asthe accuracy for the whole sentence (final).The types of errors made for German and En-glish are similar.
For both German and English theunlabeled precision reaches more than 70% (seeTable 1).
Even the correct dependency label of up-coming words can be predicted with a fairly highprecision.
TurboParser parses an increment in about0.015 seconds, which is much faster than WCDG8064060801000 1 2 3 4 5 finalpercentagerelative time pointTurboParser4060801000 1 2 3 4 5 finalpercentagerelative time pointjwcdgFigure 2: Results for TurboParser and jwcdg for German with tagger (labeled).English German German&tagger German (jwcdg)labeled unlabeled labeled unlabeled labeled unlabeled labeled unlabeledprecision 75.47% 78.55% 67.42% 75.90% 65.21% 73.39% 32.95% 42.23%recall 57.92% 60,29% 46.77% 52.65% 45.79% 51.54% 35.90% 46.00%Table 1: Precision and recall for the prediction of virtual nodestime point 0 time point 5unlabeled labeled unlabeled labeledEn 89.28% 84.92% 97.32% 97.11%De 90.91% 88.96% 96.11% 95.65%Table 2: Stability measureswhere about eight seconds per word are needed toachieve a good accuracy (K?ohn and Menzel, 2013).The prediction recall is higher for English than forGerman which could be due to the differences ingold-standard annotation.Training TurboParser on the non-incrementaldata sets results in a labeled whole-sentence accu-racy of 93.02% for German.The whole-sentenceaccuracy for parsing with VNs is 93.33%.
Thisshows that the additional mechanism of VNs hasno negative effects on the overall parsing quality.To compare TurboParser and WCDG runningboth in the predictive incremental mode, we usejwcdg, the current implementation of this approach.jwcdg differs from most other parsers in that itdoes not act on pre-tagged data but runs an exter-nal tagger itself in a multi-tag mode.
To compareboth systems, TurboParser needs to be run in atagger-parser pipeline.
We have chosen TurboTag-ger without look-ahead for this purpose.
RunningTurboParser in this pipeline leads to only slightlyworse results compared to the use of gold-standardtags (see Figure 2).
TurboParser?s attachment ac-curacy is about ten percentage points better thanjwcdg?s across the board.
In addition, its VN pre-diction is considerably better.To measure the stability, let Pibe a prefix of thesentence Pnand aiand anbe the correspondinganalyses produced by the parser.
An attachmentof a word w ?
Piis stable if either w?s head is thesame in aiand anor w?s head is not part of Piinboth aiand an.
The second part covers the casewhere the parser predicts the head of w to lie in thefuture and it really does, according to the final parse.Table 2 shows the attachment stability of the newestword at time point 0 compared to the word fivepositions to the left of time point 0.
TurboParser?sstability turns out to be much higher than jwcdg?s:For German Beuck et al (2013) report a stabilityof only 80% at the most recent word.
Interestingly,labeling the newest attachment for English seemsto be much harder than for German.6 ConclusionUsing a parser based on ILP, we were able to an-alyze sentences incrementally and produce con-nected dependency analyses at every point in time.The intermediate structures produced by the parserare highly informative, including predictions forproperties and structural embeddings of upcom-ing words.
In contrast to previous approaches, weachieve state-of-the-art accuracy for whole sen-tences by abandoning strong monotonicity and aimat high stability instead, allowing the parser to im-prove intermediate results in light of new evidence.The parser is trained on treebank data for wholesentences from which prefix annotations are de-rived in a fully automatic manner.
To guide thisprocess, a specification of structurally and lexicallydetermined dependency relations and some addi-tional heuristics are needed.
For parsing, only a setof possible VNs has to be provided.
These are theonly language specific components required.
There-fore, the approach can be ported to other languageswith quite modest effort.807ReferencesNiels Beuck and Wolfgang Menzel.
2013.
Structuralprediction in incremental dependency parsing.
InAlexander Gelbukh, editor, Computational Linguis-tics and Intelligent Text Processing, volume 7816 ofLecture Notes in Computer Science, pages 245?257.Springer Berlin Heidelberg.Niels Beuck, Arne K?ohn, and Wolfgang Menzel.
2011.Incremental parsing and the evaluation of partial de-pendency analyses.
In Proceedings of the 1st In-ternational Conference on Dependency Linguistics.Depling 2011.Niels Beuck, Arne K?ohn, and Wolfgang Menzel.
2013.Predictive incremental parsing and its evaluation.
InKim Gerdes, Eva Haji?cov?a, and Leo Wanner, editors,Computational Dependency Theory, volume 258 ofFrontiers in Artificial Intelligence and Applications,pages 186 ?
206.
IOS press.Vera Demberg-Winterfors.
2010.
A Broad-CoverageModel of Prediction in Human Sentence Processing.Ph.D.
thesis, University of Edinburgh.Kilian A. Foth, Niels Beuck, Arne K?ohn, and WolfgangMenzel.
2014.
The Hamburg Dependency Tree-bank.
In Proceedings of the Language Resourcesand Evaluation Conference 2014.
LREC, EuropeanLanguage Resources Association (ELRA).Hany Hassan, Khalil Sima?an, and Andy Way.
2009.Lexicalized semi-incremental dependency parsing.In Proceedings of the International ConferenceRANLP-2009, pages 128?134, Borovets, Bulgaria,September.
Association for Computational Linguis-tics.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for En-glish.
In Proceedings of NODALIDA 2007, pages105?112, Tartu, Estonia, May 25-26.Arne K?ohn and Wolfgang Menzel.
2013.
Incrementaland predictive dependency parsing under real-timeconditions.
In Proceedings of the International Con-ference Recent Advances in Natural Language Pro-cessing RANLP 2013, pages 373?381, Hissar, Bul-garia, September.
INCOMA Ltd. Shoumen, BUL-GARIA.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schasberger.1994.
The Penn Treebank: Annotating predicateargument structure.
In Proceedings of the Workshopon Human Language Technology, HLT ?94, pages114?119, Stroudsburg, PA, USA.
Association forComputational Linguistics.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages617?622, Sofia, Bulgaria, August.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Davin Schlangen and Gabriel Skantze.
2011.
A gen-eral, abstract model of incremental dialogue process-ing.
Dialogue and Discourse, 2(1):83?111.Patrick Sturt and Vincenzo Lombardo.
2005.
Process-ing coordinated structures: Incrementality and con-nectedness.
Cognitive Science, 29(2):291?305.808
