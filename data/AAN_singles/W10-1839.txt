Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 243?246,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Hybrid Model for Annotating Named Entity Training CorporaRobert Voyer, Valerie Nygaard, Will Fitzgerald, and Hannah CoppermanMicrosoft475 Brannan St. Suite 330San Francisco, CA 94107, USA{Robert.Voyer, Valerie.Nygaard, Will.Fitzgerald,Hannah.Copperman}@microsoft.comAbstractIn this paper, we present a two-phase, hybridmodel for generating training data for NamedEntity Recognition systems.
In the first phase,a trained annotator labels all named entities ina text irrespective of type.
In the second phase,na?ve crowdsourcing workers complete binaryjudgment tasks to indicate the type(s) of eachentity.
Decomposing the data generation taskin this way results in a flexible, reusable cor-pus that accommodates changes to entity typetaxonomies.
In addition, it makes efficient useof precious trained annotator resources by lev-eraging highly available and cost effectivecrowdsourcing worker pools in a way thatdoes not sacrifice quality.Keywords: annotation scheme design, annota-tion tools and systems, corpus annotation, an-notation for machine learning1 BackgroundThe task of Named Entity Recognition (NER) isfundamental to many Natural Language Pro-cessing pipelines.
Named entity recognizers aremost commonly built as machine learned sys-tems that require annotated training data.
Manualannotation of named entities is an expensive pro-cess, and as a result, much recent work has beendone to acquire training corpora automaticallyfrom the web.
Automatic training corpus acquisi-tion usually requires the existence of one or morefirst-pass classifiers to identify documents thatcorrespond to a predetermined entity ontology.Using this sort of approach requires an additionalset of training data for the initial classifier.
Moreimportantly, the quality of our training corpus islimited by the accuracy of any preliminary clas-sifiers.
Each automatic step in the process corre-sponds to increased error in the resulting system.It is not unusual for NE annotation schemas tochange as the intended application of NER sys-tems evolves over time ?
an issue that is rarelymentioned in the literature.
Extending namedentity ontologies when using an automated ap-proach like the one outlined in (Nothman, 2008),for example, requires non-trivial modificationsand extensions to an existing system and mayrender obsolete any previously collected data.Our NER system serves a dual purpose; itsprimary function is to aid our deep natural lan-guage parser by identifying single and multiwordnamed entities (NE) in Wikipedia articles.
In ad-dition to rendering these phrases as opaque units,the same classifier categorizes these entities asbelonging to one of four classes: person, loca-tion, organization, and miscellaneous.
Theseclass labels serve as additional features that arepassed downstream and facilitate parsing.
Onceidentified and labeled, we then add correspond-ing entries to our semantic index for improvedranking and retrieval.We scoped each type in the repertoire men-tioned above in an attempt to most effectivelysupport our parser and the end-to-end retrievaltask.
While this taxonomy resembles the oneused in the 7th Message Understanding Confer-ence (MUC-7) NER shared task (Chinchor,1998), our specification is in fact slightly nu-anced.
For example, the organization and loca-tion classes used in our production system aremuch more limited, disallowing governmentalcommittees, subcommittees, and other organiza-tions that fall under the MUC-7 definition of or-ganization.
Indeed, the determination of types totag and the definitions of these types is verymuch dependent upon the application for which agiven NER system is being designed.
Accurate243training and evaluation of NER systems thereforerequires application-specific corpora.Previously, we collected training documentsfor our system with a more automated two-passsystem.
In the first pass, we used a set of prede-fined heuristic rules ?
based on sequences ofpart-of-speech (POS) tags and common NE pat-terns ?
to identify overlapping candidate spans inthe source data.
These candidates were then up-loaded as tasks to Amazon Mechanical Turk(AMT), in which users were asked to determineif the selected entity was one of 5 specifiedtypes.
We used majority vote to choose the bestdecision.
Candidates with no majority vote wereresubmitted for additional Turker input.There were a few drawbacks with this system.First and foremost, while the heuristics to identi-fy candidate spans were designed to deliver highrecall, it was impossible to have perfect cover-age.
This imposed an upper bound on the cover-age of the system learning from this data.
Recallwould inevitably decline if we extended our NEtaxonomy to include less formulaic types such astitles and band names, for example.
One couldimagine injecting additional layers of automaticcandidate generators into the system to improverecall, each of which would incur additionaloverhead in judgment cost or complexity.
Thenext issue was quality; many workers tried toscam the system, and others didn?t quite under-stand the task, specifically when it came to dif-ferentiating types.
The need to address these is-sues is what led us to our current annotationmodel.2 ObjectiveAs the search application supported by our NERsystem evolved, it became clear both that wewould need to be able to support additional nametypes and that there was a demand for a lighterweight system to identify (especially multiword)NE spans without the need to specify the type.The underlying technology at the core of our ex-isting NER software is well suited for such clas-sification tasks.
The central hurdle to extendingour system in this way is acquiring a suitabletraining corpus.
Consider the following list ofpotential classifiers:1.
A single type system capable of identify-ing product names2.
A targeted system for identifying onlymovie titles and person names3.
A generic NE span tagger for tagging allnamed entities4.
A generic-span tagger that tags all mul-tiword named entitiesGiven that manual annotation is an extremelycostly task, we consider optimization of our cor-pora for reuse while maintaining quality in allsupported systems to be a primary goal.
Second-ly, although throughput is important ?
it is oftensaid that quantity trumps quality in machinelearned systems ?
the quality of the data is veryhighly correlated with the accuracy of the sys-tems in question.
At the scale of our typical train-ing corpus ?
one to ten thousand documents ?
thequality of the data has a significant impact.3 MethodologyIn general, decomposing multifaceted annotationtasks into their fundamental decision points re-duces the cognitive load on annotators, increas-ing annotator throughput while ultimately im-proving the quality of the marked-up data(Medero et al, 2006).
Identifying named entitiescan be decomposed into two tasks: identifyingthe span of the entity and determining its type(s).Based on our experience, the first of these tasksrequires much more training than the second.The corner cases that arise in determining if anyarbitrary sequence of tokens is a named entitymake this first task significantly more complexthan determining if a given name is, for example,a person name.
Decomposing the task into spanidentification and type judgment has two distinctadvantages:?
The span-identification task can be givento more highly trained annotators who arefollowing a specification, while the rela-tively simpler task can be distributed tona?ve/crowdsource judges.?
The task given to the trained annotatorsgoes much more quickly, increasing theirthroughput.In a round of pilot tasks, our Corpus Devel-opment team performed dual-annotation andcomplete adjudication on a small sample of 100documents.
We used the output of these tasks tohelp identify areas of inconsistency in annotatorbehavior as well as vagueness in the specifica-tion.
This initial round provided helpful feed-back, which we used both to refine the task spec-ification and to help inform the intuitions of ourannotators.244Figure 1: A NE type task in the Crowdflower interfaceAfter these initial tasks, inter-annotator agree-ment was estimated at 91%, which can be takento be a reasonable upper bound for our automat-ed system.In our current process, the data is first markedup by a trained annotator and then checked overby a second trained annotator, and finally under-goes automatic post-processing to catch commonerrors.
Thus, our first step in addressing the issueof poor data quality is to remove the step of au-tomated NE candidate generation and to shiftpart of the cognitive load of the task from un-trained workers to expert annotators.After span-tagged data has been published byour Corpus Development team, in order to gettyped NE annotations for our existing system, wethen submit candidate spans along with a twoadditional sentences of context to workers onAMT.
Workers are presented with assignmentsthat require simple binary decisions (Figure 1).
Isthe selected entity of type X ?
yes or no?
Eachunit is presented to at least 5 different workers.We follow this procedure for all labeled spans inour tagged corpus.
This entire process can becompleted for all of the types that we?re interest-ed in ?
person, location, organization, product,title, etc.
Extending this system to cover arbitraryadditional types requires simply that we create anew task template and instructions for workers.Instead of putting these tasks directly ontoAMT, we chose to leverage Crowdflower for itsadded quality control.
Crowdflower is acrowdsourcing service built on top of AMT thatassociates a trust level with workers based ontheir performance on gold data and uses thesetrust levels to determine the correctness of work-er responses.
It provides functionality for retriev-ing aggregated reports, in which responses areaggregated not based on simple majority voting,but rather by users?
trust levels.
Our early exper-iments with this service indicate that it does infact improve the quality of the output data.
Anadded bonus of their technology is that we canassociate confidence levels with the labels pro-duced by workers in their system.This entire process yields several different an-notated versions of the same corpus: an un-typednamed entity training corpus, along with an addi-tional corpus for each named entity type.
Ideally,each NE span submitted to workers will comeback as belonging to zero or one classes.
How dowe reconcile the fact that our existing systemrequires a single label per token, when some to-kens may in fact fall under multiple categories?Merging the type labels produced by Turkers(with the help of Crowdflower) is an interestingproblem in itself.
Ultimately, we arrived at a sys-tem that allows us to remove type labels that donot meet a confidence threshold, while also bias-ing certain types over others based on their diffi-culty.
Interestingly, agreement rates amongcrowdsourcing workers can provide useful in-sight into the difficulty of labeling some typesover others, potentially indicating which typesare less precisely scoped.
We consistently sawinter-judge agreement rates in the 92%?97%range for person names and locations, whileagreement on the less well-defined category oforganizations often yielded agreement rates clos-er to 85%.4 Initial ResultsAs a first level comparison of how the new ap-proach affects the overall accuracy of our sys-tem, we trained two named entity recognizers.The first system was trained on a subset of thetraining data collected using the old approach.System 2 was trained on a subset of documentscollected using the new approach.
Both systemsare trained using only a single type ?
personnames.
For the former, we randomly selected200 docs from our previous canonical trainingset, with the guiding principle that we shouldhave roughly the same number of sentences asexist in our new training corpus (~7400 sentenc-es).
Both systems were evaluated against one ofour standard, blind measurement sets, hand-245annotated with personal names.
The results intable 1 indicate the strict phrase-level precision,recall, and F-score.It bears mentioning that many NER systemsreport token-level accuracy or F-score using aflexible phrase-level metric that gives partialcredit if either the type classification is correct orthe span boundaries are correct.
Naturally, thesemetrics result in higher accuracy numbers whencompared to the strict phrase-level metric that weuse.
Our evaluation tool gives credit to instanceswhere both boundaries and type are correct.
In-correct instances incur at least 2 penalties, count-ing as at least 1 false positive and 1 false nega-tive, depending on the nature of the error.
Weoptimize our system for high precision.System P R F-scoreOld system 89.7 70.3 78.9New system 91.6 72.1 80.7Table 1: Strict phrase-level precision, recall, andF-score.Our other target application is a generic entitytagger.
For this experiment we trained on ourcomplete set of 817 training documents (14,297sentences) where documents are tagged for allnamed entities and types are not labeled.
Weevaluated the resulting system on a blind 100-document measurement set in which generic NEspans have been manually labeled by our CorpusDevelopment team.
These results are included inTable 2.System P R F-scoreGeneric span 80.3 85.7 82.9Table 2: Strict phrase-level precision, recall and F-score for generic span tagging.5 ConclusionsThe results indicate that our new approachdoes indeed produce higher quality training data.An improvement of 1.8 F-score points is relative-ly significant, particularly given the size of thetraining set used in this experiment.
It is worthnoting that our previous canonical training setunderwent a round of manual editing after it wasdiscovered that there were significant qualityissues.
The system trained on the curated datashowed marked improvement over previous ver-sions.
Given this, we could expect to see a great-er disparity between the two systems if we usedthe output of our previous training data collec-tion system as is.The generic named entity tagger requires sig-nificantly fewer features than type-aware sys-tems, allowing us to improve F-score while alsoimproving runtime performance.
We expect to beable to improve precision to acceptable produc-tion levels (>90%) while maintaining F-scorewith a bit more feature engineering, making thissystem comparable to other state-of-the-art sys-tems.To extend and improve these initial experi-ments, we would like to use identical documentsfor both single-type systems, compare perfor-mance on additional NE types, and analyze thelearning curve of both systems as we increase thesize of the training corpus.ReferencesJoohui An, Seungwoo Lee, and Gary Geunbae Lee.2003.
Automatic acquisition of named entitytagged corpus from world wide web.
The Com-panion Volume to the Proceedings of 41st An-nual Meeting of the Association for Computa-tional Linguistics, pages 165-168.Nancy Chinchor.
1998.
Overview of MUC-7.
Pro-ceedings of the 7th Message UnderstandingConference.Julie Medero, Kazuaki Maeda, Stephanie Strassel, andChristopher Walker.
2006.
An Efficient Approachto Gold-Standard Annotation: Decision Points forComplex Tasks.
Proceedings of the Fifth Inter-national Conference on Language Resourcesand Evaluation.Joel Nothman, Tara Murphy, and James R. Curran.2009.
Analyzing Wikipedia and Gold-StandardCorpora for NER Training.
Proceedings of the12th Conference of the European Chapter ofthe Association for Computational Linguistics,pages 612-620.Joel Nothman, James R. Curran, and Tara Murphy.2008.
Transforming Wikipedia into named entitytraining data.
Proceedings of the AustralianLanguage Technology Workshop, pages 124?132.Lee Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.Proceedings of the Thirteenth Conference onComputational Natural Language Learning(CoNLL-2009), pages 147-155.246
