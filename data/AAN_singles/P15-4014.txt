Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 79?84,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPSACRY: Syntax-based Automatic Crossword puzzle Resolution sYstemGianni Barlacchi?, Massimo Nicosia?and Alessandro Moschitti?Dept.
of Computer Science and Information Engineering, University of Trento38123 Povo (TN), ItalyALT, Qatar Computing Research Institute, Hamad Bin Khalifa University5825 Doha, Qatar{gianni.barlacchi, m.nicosia, amoschitti}@gmail.comAbstractIn this paper, we present our CrosswordPuzzle Resolution System (SACRY),which exploits syntactic structures forclue reranking and answer extraction.SACRY uses a database (DB) contain-ing previously solved CPs in order togenerate the list of candidate answers.Additionally, it uses innovative features,such as the answer position in the rankand aggregated information such as themin, max and average clue rerankingscores.
Our system is based on WebCrow,one of the most advanced systems forautomatic crossword puzzle resolution.Our extensive experiments over our twomillion clue dataset show that our ap-proach highly improves the quality of theanswer list, enabling the achievement ofunprecedented results on the complete CPresolution tasks, i.e., accuracy of 99.17%.1 IntroductionCrossword Puzzles (CPs) are the most famous lan-guage games played around the world.
The auto-matic resolution of CPs is an open challenge forthe artificial intelligence (AI) community, whichmainly employs AI techniques for filling the puz-zle grid with candidate answers.
Basic approachestry to optimize the overall probability of correctlyfilling the grid by exploiting the likelihood of eachcandidate answer, while satisfying the grid con-straints.Previous work (Ernandes et al., 2005) clearlysuggests that providing the solver with an accuratelist of answer candidates is an important step forthe CP resolution task.
These can be retrieved us-ing (i) the Web, (ii) Wikipedia, (iii) dictionaries orlexical databases like WordNet or, (iv) most im-portantly, recuperated from the DBs of previouslysolved CP.
Indeed, CPs are often created reusingthe same clues of past CPs, and thus querying theDB with the target clue allows for recuperating thesame (or similar) clues of the target one.
It is in-teresting to note that, for this purpose, all previousautomatic CP solvers use standard DB techniques,e.g., SQL Full-Text query.
Existing systems forautomatic CP resolution, such as Proverb (Littmanet al., 2002) and Dr.
Fill (Ginsberg, 2011), use sev-eral different modules for generating candidate an-swer lists.
These are merged and used for defininga Constraint Satisfaction Problem, resolved by theCP solver.Our CP system, SACRY, is based on innovativeQA methods for answering CP clues.
We employ(i) state-of-the-art IR techniques to retrieve thecorrect answer by querying the DB of previouslysolved CPs, (ii) learning to rank methods basedon syntactic structure of clues and structural ker-nels to improve the ranking of clues that can po-tentially contain the answers and (iii) an aggrega-tion algorithm for generating the final list contain-ing unique candidate answers.
We implementeda specific module based on these approaches andwe plugged it into an automatic CP solver, namelyWebCrow (Ernandes et al., 2005).
The latter isone of the best systems for CP resolution and ithas been kindly made available by the authors.We tested our models on a dataset containingmore than two million clues and their associatedanswers.
This dataset is an interesting resourcethat we will make available to the research com-munity.
It can be used for tasks such as: (i) simi-lar clue retrieval/reranking, which focuses on im-proving the rank of clues ciretrieved by a searchengine, and (ii) answer reranking, which targetsthe list of aci, i.e., their aggregated clues.
Wetested SACRY on an end-to-end task by solvingten crossword puzzles provided by two of the mostfamous CP editors from The New York Times andthe Washington Post.
SACRY obtained an impres-sive CP resolution accuracy of 99.17%.79Figure 1: The architecture of WebCrowIn the reminder of this paper, Sec.
2 introducesWebCrow and its architecture.
Our models forsimilar clues retrieval and answers reranking aredescribed in Sec.
3 while Sec.
4 illustrates our ex-periments.
Finally, the conclusions and directionsfor future work are presented in Sec.
5.2 The WebCrow ArchitectureOur approaches can be used to generate accuratecandidate lists that CP solvers can exploit to im-prove their overall accuracy.
Therefore, the qual-ity of our methods can be also implicitly evalu-ated on the final resolution task.
For this purpose,we use an existing CP solver, namely WebCrow,which is rather modular and accurate and it hasbeen kindly made available by the authors.
Itsarchitecture is illustrated in Figure 1.
In the fol-lowing, we briefly introduce the database moduleof WebCrow, which is the one that we substitutedwith ours.WebCrow?s solving process can be divided intwo phases.
In the first phase, the input list ofclues activate a set of answer search modules,which produce several lists of possible solutionsfor each clue.
These lists are then merged by aspecific Merger component, which uses the con-fidence values of the lists and the probability thata candidate in a list is correct.
Eventually, a sin-gle list of answers with their associated probabil-ities is built for each input clue.
In the secondphase WebCrow fills the crossword grid by solvinga constraint-satisfaction problem.
WebCrow se-lects a single answer from each merged list of can-didates, trying to satisfy the imposed constraints.The goal of this phase is to find an admissible so-lution that maximizes the number of correctly in-serted words.
It is done using an adapted versionof the WA* algorithm (Pohl, 1970) for CP resolu-tion.2.1 CrossWord Database module (CWDB)Gathering clues contained in previously publishedCPs is essential for solving new puzzles.
A largeportion of clues in new CPs has usually already ap-peared in the past.
Clues may share similar word-ing or may be restated in a very different way.Therefore, it is important to identify the clues thathave the same answer.
WebCrow uses three differ-ent modules to retrieve clues identical or similarto a given clue from the database: the CWDB-EXACT module, which retrieves DB clues thatmatch exactly with a target clue, and weights themby the frequency they have in the clue collec-tion.
The CWDB-PARTIAL module, which usesthe MySQL?s partial matching function, query ex-pansion and positional term distances to computeclue-similarity scores, along with the Full-Textsearch functions.
The CWDB-DICTIO module,which simply returns the full list of words of cor-rect length, ranked by their number of occurrencesin the initial list.We outperform the previous approach by apply-ing learning-to-rank algorithms based on SVMsand tree kernels on clue lists generated by state-of-the-art passage retrieval systems.3 Crossword Puzzle Database (CPDB)ModuleWebCrow creates answer lists by retrieving cluesfrom the DB of previously solved crosswords.It simply uses the classical SQL operators andfull-text search.
We instead index the DB cluesand their answers with the open source searchengine Lucene (McCandless et al., 2010), usingthe state-of-the-art BM25 retrieval model.
Thisalone significantly improves the quality of the re-trieved clue list, which is further refined by apply-ing reranking.
The latter consists in promoting theclues that potentially have the same answer of thequery clue.We designed a relatively complex pipelineshown in Fig.
2.
We build a training set usingsome training clues for querying our search en-gine, which retrieves correct and incorrect can-didates from the indexed clues.
At classificationtime, the new clues are used as a search query andthe retrieved similar candidate are reranked by ourmodels.
The next sections show our approach forbuilding rerankers that can exploit structures for80solving the ineffectiveness of the simple word rep-resentation.3.1 Reranking framework based on KernelsThe basic architecture of our reranking frameworkis relatively simple: it uses a standard preferencekernel reranking approach and is similar to the oneproposed in (Severyn and Moschitti, 2012) for QAtasks.
However, we modeled different kernels suit-able for clue retrieval.The framework takes a query clue and retrievesa list of related candidate clues using a search en-gine (applied to the CPDB), according to somesimilarity criteria.
Then, the query and the can-didates are processed by an NLP pipeline.
Ourpipeline is built on top of the UIMA framework(Ferrucci and Lally, 2004) and contains many textanalysis components.
The components used forour specific tasks are: the tokenizer1, sentencedetector1, lemmatizer1, part-of-speech (POS) tag-ger1and chunker2.The annotations produced by these standardprocessors are input to our components that extractstructures as well as traditional features for rep-resenting clues.
This representation is employedto train kernel-based rerankers for reordering thecandidate lists provided by a search engine.
Sincethe syntactic parsing accuracy can impact the qual-ity of our structures and consequently the accuracyof our learning to rank algorithms, we preferred touse shallow syntactic trees over full syntactic rep-resentations.In the reranker we used the Partial Tree Kernel(PTK) (Moschitti, 2006).
Given an input tree, itgenerates all possible connected tree fragments,e.g., sibling nodes can also be separated and bepart of different tree fragments.
In other words, afragment (which is a feature) is any possible treepath, from whose nodes other tree paths can de-part.
Thus, it can generate a very rich feature spaceresulting in higher generalization ability.We combined the structural features with othertraditional ones.
We used the following groups:iKernels features (iK), which include similarityfeatures and kernels applied intra-pairs, i.e., be-tween the query and the retrieved clues:?
Syntactic similarities, i.e., cosine similarity mea-sures computed on n-grams (with n = 1, 2, 3, 4) of1http://nlp.stanford.edu/software/corenlp.shtml2http://cogcomp.cs.illinois.edu/page/software_view/13word lemmas and part-of-speech tags.?
Kernel similarities, i.e., string kernels and treekernels applied to structural representations.DKPro Similarity (DKP), which defines featuresused in the Semantic Textual Similarity (STS)challenge.
These are encoded by the UKP Lab(B?ar et al., 2013):?
Longest common substring measure and Longestcommon subsequence measure.
They determinethe length of the longest substring shared by twotext segments.?
Running-Karp-Rabin Greedy String Tiling.
Itprovides a similarity between two sentences bycounting the number of shuffles in their subparts.?
Resnik similarity.
The WordNet hierarchy isused to compute a measure of semantic related-ness between concepts expressed in the text.The aggregation algorithm in (Mihalcea et al.,2006) is applied to extend the measure from wordsto sentences.?
Explicit Semantic Analysis (ESA) similarity(Gabrilovich and Markovitch, 2007), which rep-resents documents as weighted vectors of con-cepts learned from Wikipedia, WordNet and Wik-tionary.?
Lexical Substitution (Biemann, 2013).
A super-vised word sense disambiguation system is used tosubstitute a wide selection of high-frequency En-glish nouns with generalizations.
Resnik and ESAfeatures are computed on the transformed text.WebCrow features (WC), which are the similar-ity measures computed on the clue pairs by We-bCrow (using the Levenshtein distance) and theSearch Engine score.Kernels for reranking, given a query clue qcandtwo retrieved clues c1, c2, we can rank them byusing a reranking model, namely (RR).
It usestwo pairs P = ?p1q, p2q?
and P?= ?p1q?, p2q?
?,the member of each pair are clues from thesame list generated by q and q?, respectively.In this case, we use the kernel, KRR(P, P?)
=PTK(?q, c1?, ?q?, c?1?
)+PTK(?q, c2?, ?q?, c?2?
)?PTK(?q, c1?, ?q?, c?2?)
?
PTK(?q, c2?, ?q?, c?1?
),which corresponds to the scalar product betweenthe vectors,(?
(p1q) ?
?(p2q))?(?(p1q?)
?
?(p2q?
)),in the fragment vector space generated by PTK.3.2 Learning to rank aggregated answersGroups of similar clues retrieved from the searchengine can be associated with the same answers.Since each clue receives a score from the reranker,a strategy to combine the scores is needed.
We81Figure 2: The architecture of our systemaim at aggregating clues associated with the sameanswer and building meaningful features for suchgroups.
For this purpose, we train an SVMrankwith each candidate answer acirepresented withfeatures derived from all the clues ciassociatedwith such answer, i.e., we aggregate them usingstandard operators such as average, min.
and max.We model an answer a using its set of cluesCa= {ci: aci= a} in SVMrank.
The featurevector associated with a must contains informa-tion from all c ?
Ca.
Thus, we designed novelaggregated features that we call AVG: (i) we av-erage the feature values used for each clue by thefirst reranker, i.e., those described in Sec.
3.1 aswell as the scores produced by the clue reranker.More specifically, we compute their sum, average,maximum and minimum values.
(ii) We also addthe term frequency of the answer word in CPDB.Additionally, we model the occurrences of theanswer instance in the list by means of positionalfeatures: we use n features, where n is the sizeof our candidate list (e.g., 10).
Each feature cor-responds to the positions of each candidate and itis set to the reranker score if ci?
Ca(i.e., forthe target answer candidate) and 0 otherwise.
Wecall such features (POS).
For example, if an an-swer candidate is associated with clues appearingat positions 1 and 3 of the list, Feature 1 and Fea-ture 3 will be set to the score calculated from thereranker.
We take into account the similarity be-tween the answer candidate and the input cluesusing a set of features, derived from word embed-dings (Mikolov et al., 2013).
These features con-sider (i) the similarities between the clues in a pair,(ii) the target clue and the candidate answer and(iii) the candidate clue and the candidate answer.They are computed summing the embedding vec-tors of words and computing the cosine similarity.This way we produce some evidence of semanticrelatedness.
We call such features (W2V).3.3 Generating probabilities for the solverAfter the aggregation and reranking steps we havea set of unique candidate answers ordered by theirreranking scores.
Using the latter in WebCrowgenerally produces a decrease of its accuracy sinceit expects probabilities (or values ranging from 0to 1).
The summed votes or the scores producedby the reranker can have a wider range and canalso be negative.
Therefore, we apply logistic re-gression (LGR) to learn a mapping between thereranking scores and values ranging from 0 to 1.4 ExperimentsIn our experiments we compare our approach withWebCrow both on ranking candidate answers andon the end-to-end CP resolution.4.1 Database of previously resolved CPsThe most commonly used databases of clues con-tain both single clues taken from various cross-words (Ginsberg, 2011) and entire crossword puz-zle (Ernandes et al., 2008).
They refer to relativelyclean pairs of clue/answer and other crossword re-lated information such as date of the clue, nameof the CP editor and difficulty of the clue (e.g.,clues taken from the CPs published on The Sundaynewspaper are the most difficult).
Unfortunately,they are not publicly available.Therefore, we compiled a crossword corpuscombining (i) CP downloaded from the Web3and(ii) the clue database provided by Otsys4.
We re-moved duplicates, fill-in-the-blank clues (whichare better solved by using other strategies) andclues representing anagrams or linguistic games.We collected over 6.3 millions of clues, publishedby many different American editors.
Althoughthis is a very rich database, it contains many du-plicates and non-standard clues, which introducesignificant noise in the dataset.
For this reason wecreated a compressed dataset of 2,131,034 uniqueand standard clues, with associated answers.
It ex-cludes the fill-in-the-blank clues mentioned above.4.2 Experimental SetupTo train our models, we adopted SVM-light-TK5,which enables the use of the Partial Tree Kernel(PTK) (Moschitti, 2006) in SVM-light (Joachims,2002), with default parameters.
We applied apolynomial kernel of degree 3 to the explicit fea-ture vectors (FV).
To measure the impact of thererankers as well as the CWDB module, we usedwell-known metrics for assessing the accuracy of3http://www.crosswordgiant.com4http://www.otsys.com/clue5http://disi.unitn.it/moschitti/Tree-Kernel.htm82QA and retrieval systems, i.e.
: Recall at differ-ent ranks (R@1, 5, 20, 50, 100), Mean ReciprocalRank (MRR) and Mean Average Precision (MAP).R@1 is the percentage of questions with a cor-rect answer ranked at the first position.
MRR iscomputed as follows: MRR =1|Q|?|Q|q=11rank(q),where rank(q) is the position of the first correctanswer in the candidate list.
For a set of queriesQ, MAP is the mean over the average precisionscores for each query:1Q?Qq=1AveP (q).To measure the complete CP resolution task, weuse the accuracy over the entire words filling a CPgrid (one wrong letter causes the entire definitionto be incorrect).4.3 Clue reranking experimentsGiven an input clue BM25 retrieves a list of 100clues.
On the latter, we tested our different mod-els for clue reranking.
For space constraints, weonly report a short summary of our experiments:kernel-based rerankers combined with traditionalfeatures (PTK+FV) relatively improve standard IRby 16%.
This is an interesting result as in (Barlac-chi et al., 2014), the authors showed that standardIR greatly improves on the DB methods for clueretrieval, i.e., they showed that BM25 relativelyimproves on SQL by about 40% in MRR.4.4 Answer aggregation and rerankingReranking clues is just the first step as the solvermust be fed with the list of unique answers.
Thus,we first used our best model (i.e., PTK+FV) forclue reranking to score the answers of a separateset, i.e., our answer reranking training set.
Then,we used these scores to train an additional rerankerfor aggregating identical answers.
The aggrega-tion module merges clues sharing the same answerinto a single instance.Tab.
1 shows the results for several answerreranking models tested on a development set: thefirst row shows the accuracy of the answer list pro-duces by WebCrow.
The second row reports theaccuracy of our model using a simple voting strat-egy, i.e., the score of the clue reranker is usedas a vote for the target candidate answer.
Thethird row applies Logistic Regression (LGR) totransform the SVM reranking scores in probabili-ties.
It uses Lucene score for the candidate answeras well as the max and min scores of the entirelist.
From the fourth column, the answer rerankeris trained using SVMrankusing FV, AVG, POS,W2V and some of their combinations.
We notethat: (i) voting the answers using the raw score im-Models MRR R@1 R@5 R@10 R@20 R@50 R@80WebCrow 39.12 31.51 47.37 54.38 58.60 63.34 64.06Raw voting 41.84 33.0 52.9 58.7 62.7 66.6 67.5LGR voting 43.66 35 53.7 59.3 63.4 67.4 67.7SVMrankAVG 43.5 35.3 53.5 59.4 63.9 67.4 67.7AVG+POS 44.1 36.3 53.6 58.9 63.9 67.4 67.6AVG+W2V 43.2 35 53.3 58.8 63.9 67.4 67.7AVG+POS+FV 44.4 36.7 54.2 60 64.3 67.4 67.7AVG+FV+W2V 44.1 35.8 54.4 60 64.4 67.4 67.7AVG+POS+FV+W2V44.6 36.8 54.2 59.8 64.4 67.4 67.7Table 1: Answer reranking on the dev.
set.proves WebCrow but the probabilities computedby LGR perform much better, i.e., about 2 per-cent points better than Raw voting and 4.5 pointsbetter than WebCrow; (ii) the SVMrankaggrega-tion model can provide another additional point,when positional features and standard feature vec-tors are used along with aggregated and W2C fea-tures.
(iii) The overall relative improvement of14% over WebCrow is promising for improvingthe end-to-end CP resolution task.4.5 Evaluation of the CP resolutionIn order to test the effectiveness of our method,we evaluated the resolution of full CP.
We selectedfive crosswords from The New York Times newspa-per and other five from the Washington Post.
Fig.
3shows the average resolution accuracy over theten CP of the original WebCrow compared to We-bCrow using our reranked lists.
We ran the solverby providing it with lists of different size.
Wenote that our model consistently outperforms We-bCrow.
This means that the lists of candidate an-swers generated by our models help the solver,which in turn fills the grid with higher accuracy.In particular, our CP system achieves an averageaccuracy of 99.17%, which makes it competitivewith international CP resolution challenges.Additionally, WebCrow achieves the highest ac-curacy when uses the largest candidate lists (bothoriginal or reranked) but a large list size negativelyimpacts on the speed of the solver, which in aCP competition is critical to beat the other com-petitors (if participants obtain the same score, thesolving time decides who is ranked first).
Thus,our approach also provide a speedup as the bestaccuracy is reached for just 50 candidates (in con-trast with the 100 candidates needed by the origi-nal WebCrow).5 Final RemarksIn this paper, we describe our system SACRYfor automatic CP resolution.
It is based on8396.597.097.598.098.599.025 50 75 100Number of candidatesAvg.accuracy insolving CPsWebCrowRerankerFigure 3: Average accuracy over 10 CPs.modeling rerankers for clue retrieval from DBs.SACRY achieves a higher accuracy than We-bCrow.
SACRY uses rerankers based on SVMsand structural kernels, where the latter are appliedto robust shallow syntactic structures.
Our struc-tural models applied to clue reranking enable usto learn clue paraphrasing by exploiting relationalsyntactic structures representing pairs of clues.We collected the biggest clue dataset ever,which can be also used for QA tasks since it iscomposed by pairs of clue/answer.
The datasetincludes 2,131,034 unique pairs of clue/answers,which we are going to make available to the re-search community.
The experiments show that ourmethods improve the quality of the lists generatedby WebCrow by 14% in MRR.
When used in We-bCrow solver with its best setting, its resolution er-ror relatively decreases by 50%, achieving almosta perfect resolution accuracy, i.e., 99.17%.
In thefuture, we would like to release the solver to allowresearchers to contribute to the project and makethe system even more competitive.AcknowledgmentsThis work has been supported by the EC projectCogNet, 671625 (H2020-ICT-2014-2, Researchand Innovation action).
This research is part ofthe Interactive sYstems for Answer Search (IYAS)project, conducted by the Arabic Language Tech-nologies (ALT) group at Qatar Computing Re-search Institute (QCRI) within the Hamad BinKhalifa University and Qatar Foundation.ReferencesDaniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.Dkpro similarity: An open source framework fortext similarity.
In Proceedings of ACL (SystemDemonstrations).Gianni Barlacchi, Massimo Nicosia, and AlessandroMoschitti.
2014.
Learning to rank answer candi-dates for automatic resolution of crossword puzzles.In Proceedings of CoNLL.Chris Biemann.
2013.
Creating a system for lexi-cal substitutions from scratch using crowdsourcing.Lang.
Resour.
Eval., 47(1):97?122, March.Marco Ernandes, Giovanni Angelini, and Marco Gori.2005.
Webcrow: A web-based system for crosswordsolving.
In In Proc.
of AAAI 05, pages 1412?1417.Menlo Park, Calif., AAAI Press.Marco Ernandes, Giovanni Angelini, and Marco Gori.2008.
A web-based agent challenges human expertson crosswords.
AI Magazine, 29(1).David Ferrucci and Adam Lally.
2004.
Uima: Anarchitectural approach to unstructured informationprocessing in the corporate research environment.Nat.
Lang.
Eng., 10(3-4):327?348, September.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofIJCAI.Matthew L. Ginsberg.
2011.
Dr.fill: Crosswords andan implemented solver for singly weighted csps.
J.Artif.
Int.
Res., 42(1):851?886, September.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of ACMSIGKDD, New York, NY, USA.
ACM.Michael L. Littman, Greg A. Keim, and Noam Shazeer.2002.
A probabilistic approach to solving crosswordpuzzles.
Artificial Intelligence, 134(12):23 ?
55.Michael McCandless, Erik Hatcher, and Otis Gospod-netic.
2010.
Lucene in Action, Second Edition:Covers Apache Lucene 3.0.
Manning PublicationsCo., Greenwich, CT, USA.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In ProceedingsAAAI.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML, pages 318?329.Ira Pohl.
1970.
Heuristic search viewed as path findingin a graph.
Artificial Intelligence, 1(34):193 ?
204.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In Proceedings of ACM SIGIR,New York, NY, USA.84
