A non-programming introduction to computer science viaNLP, IR, and AILillian LeeDepartment of Computer ScienceCornell UniversityIthaca, NY, USA, 14853-7501llee@cs.cornell.eduAbstractThis paper describes a new CornellUniversity course serving as a non-programming introduction to com-puter science, with natural languageprocessing and information retrievalforming a crucial part of the syllabus.Material was drawn from a wide vari-ety of topics (such as theories of dis-course structure and random graphmodels of the World Wide Web) andpresented at some technical depth, butwas massaged to make it suitable fora freshman-level course.
Student feed-back from the first running of the classwas overall quite positive, and a grantfrom the GE Fund has been awardedto further support the course?s devel-opment and goals.1 IntroductionAlgorithmic concepts and programming tech-niques from computer science are very useful toresearchers in natural language processing.
Toensure the continued strength of our field, then,it is important to encourage undergraduates in-terested in NLP to take courses conveying com-puter science content.
This is especially true forstudents not intending to become computer sci-ence majors.Usually, one points beginning students inter-ested in NLP towards the first programmingcourse (henceforth ?CS101?).
However, at manyinstitutions, CS101 is mandatory for a large por-tion of the undergraduates (e.g., all engineeringstudents) and is designed primarily to trans-mit specific programming skills.
Experiencesuggests that a significant fraction of studentsfind CS101?s emphasis on skills rather than con-cepts unstimulating, and therefore decide not totake further computer science courses.
Unfortu-nately, down the road this results in fewer en-tering NLP graduate students having been ed-ucated in important advanced computer-scienceconcepts.
Furthermore, fewer students are intro-duced to NLP at all, since the subject is oftenpresented only in upper-level computer-scienceclasses.In an attempt to combat these problems, I cre-ated a new freshman-level course, Computation,Information, and Intelligence1, designed to in-troduce entering undergraduates to some of theideas and goals of AI (and hence computer sci-ence).
The premise was that if freshmen firstlearned something of what artificial intelligenceis about, what the technical issues are, whathas been accomplished, and what remains to bedone, then they would be much more motivatedwhen taking CS101, because they would under-stand what they are learning programming for.Three major design decisions were made atthe outset:?
No programming: Teaching elementary pro-gramming would be a needless reduplication ofeffort, since programming pedagogy is alreadywell-honed in CS101 and other such classes.Moreover, it was desirable to attract studentshaving little or no programming experience: thenew course would offer them an opportunity for1http://www.cs.cornell.edu/courses/cs172/2001faJuly 2002, pp.
33-38.
Association for Computational Linguistics.Natural Language Processing and Computational Linguistics, Philadelphia,Proceedings of the Workshop on Effective Tools and Methodologies for Teachinginitial exploration at a conceptual level.
Indeed,for the first edition of the class, students withprogramming experience were actively discour-aged from enrolling, in order to ensure a morelevel playing field for those without such back-ground.2?
Emphasis on technically challenging material:Although no programming would be involved,the course would nevertheless bring studentsface-to-face with substantial technical materialrequiring mathematical and abstract reasoning(indeed, topics from graduate courses in NLPand machine learning were incorporated).
Toachieve this aim, the main coursework wouldinvolve challenging pencil-and-paper problemswith significant mathematical content.3Of course, one had to be mindful that the in-tended audience was college freshmen, and thusone could only assume basic calculus as a pre-requisite.
Even working within this constraint,though, it was possible to craft problem setsand exams in which students explored conceptsin some depth; the typical homework problemasked them not just to demonstrate comprehen-sion of lecture material but to investigate alter-native proposals.
Sample questions are includedin the appendix.?
Substantial NLP and IR content:4 Becausemany students have a lot of experience withsearch engines, and, of course, all students havea great deal of experience with language, NLPand IR are topics that freshmen can easily relateto without being introduced to a lot of back-ground first.2Students who have programmed previously are morelikely to happily enroll in further computer sciencecourses, and thus are already well-served by the standardcurriculum.3An alternative to a technically- and mathematically-oriented course would have been a ?computers and thehumanities?
class, but Cornell already offers classes onthe history of computing, the philosophy of AI, and thesocial implications of living in an information society.One of the goals for Computation, Information, and In-telligence was that students learn what ?doing AI?
is re-ally like.4In this class, I treated information retrieval as a spe-cial type of NLP for simplicity?s sake.2 Course contentThe course title, Computation, Information,and Intelligence, reflects its organization, whichwas inspired by Herb Simon?s (1977) statementthat ?Knowledge without appropriate proce-dures for its use is dumb, and procedure withoutsuitable knowledge is blind.?
More specifically,the first 15 lectures were mainly concerned withalgorithms and computation (game-tree search,perceptron learning, nearest-neighbor learning,Turing machines, and the halting problem).
Forthe purposes of this workshop, though, this pa-per focuses on the remaining 19 lectures, whichwere devoted to information, and in particular,to IR and NLP.
As mentioned above, samplehomework problems for each of the units listedbelow can be found in the appendix.We now outline the major topics of the last22 lectures.
Observe that IR was presented be-fore NLP, because the former was treated as aspecial, simpler case of the latter; that is, wefirst treated documents as bags of words beforeconsidering relations between words.Document retrieval [3 lectures].
Studentswere first introduced to the Boolean query re-trieval model, and hence to the concepts of indexdata structures (arrays and B-trees) and binarysearch.
We then moved on to the vector spacemodel5, and considered simple term-weightingschemes like tf-idf.The Web [4 lectures].
After noting how Van-nevar Bush?s (1945) famous ?Memex?
article an-ticipated the development of the Web, we stud-ied the Web?s global topology, briefly consider-ing the implications of its so-called ?bow-tie?structure (Broder et al, 2000) for web crawlers?
students were thus introduced to graph-theoretic notions of strongly-connected compo-nents and node degrees.
Then, we investigatedKleinberg?s (1998) hubs and authorities algo-rithm as an alternative to mere in-link counting:5This does require some linear algebra background inthat one needs to compute inner products, but this wascovered in the section of the course on perceptrons.
Sincetrigonometry is actually relatively fresh in the minds offirst-year students, their geometric intuitions tended toserve them fairly well.fortunately, the method is simple enough thatstudents could engage in hand simulations.
Fi-nally, we looked at the suitability of various ran-dom graph generation models (e.g., the ?rich-get-richer?
(Baraba?si et al, 1999) and ?copy-ing?
models (Kumar et al, 2000)) for capturingthe local structure of the Web, such as the phe-nomenon of in-degree distributions following apower law ?
conveniently, these concepts couldbe presented in such a way that only requiredthe students to have intuitive notions of proba-bility and the ability to take derivatives.Language structure [7 lectures].
Relying onstudents?
instincts about and experience withlanguage, we considered evidence for the exis-tence of hidden language structure; such cluesincluded possible and impossible syntactic anddiscourse ambiguities, and movement, prosodyand pause cues for constituents.
To describethis structure, we formally defined context-freegrammars.
We then showed how (a tiny frag-ment of) X-bar theory can be modeled by acontext-free grammar and, using its structuralassignments and the notion of heads of con-stituents, accounted for some of the ambiguitiesand non-ambiguities in the linguistic exampleswe previously examined.The discussion of context-free grammars nat-urally led us to pushdown automata (which pro-vided a nice contrast to the Turing machineswe studied earlier in the course).
And, hav-ing thus introduced stacks, we then investigatedthe Grosz and Sidner (1986) stack-based theoryof discourse structure, showing that languagestructures exist at granularities beyond the sen-tence level.Statistical language processing [6 lectures]We began this unit by considering word fre-quency distributions, and in particular, Zipf?slaw ?
note that our having studied power-lawdistributions in the Web unit greatly facilitatedthis discussion.
In fact, because we had pre-viously investigated generative models for theWeb, it was natural to consider Miller?s (1957)?monkeys?
model which demonstrates that verysimple generative models can account for Zipf?slaw.
Next, we looked at methods taking advan-tage of statistical regularities, including the IBMCandide statistical machine translation system,following Knight?s (1999) tutorial and treatingprobabilities as weights.
It was interesting topoint out parallels with the hubs and authoritiesalgorithm ?
both are iterative update proce-dures with auxiliary information (alignments inone case, hubs in the other).
We also discussedan intuitive algorithm for Japanese segmenta-tion drawn from one of my own recent researchcollaborations (Ando and Lee, 2000), and howword statistics were applied to determining theauthorship of the Federalist Papers (Mostellerand Wallace, 1984).
We concluded with an ex-amination of human statistical learning, focus-ing on recent evidence indicating that humaninfants can use statistics when learning to seg-ment continuous speech into words (Saffran etal., 1996).The Turing test [2 lectures] Finally, weended the course with a consideration of intel-ligence in the large.
In particular, we focusedon Turing?s (1950) proposal of the ?imitationgame?, which can be interpreted as one of thefirst appearances of the claim that natural lan-guage processing is ?AI-complete?, and Searle?s(1980) ?Chinese Room?
rebuttal that fluent lan-guage behavior is not a sufficient indication ofintelligence.
Then, we concluded with an exam-ination of the first running of the Restricted Tur-ing Test (Shieber, 1994), which served as an ob-ject lesson as to the importance of careful eval-uation in NLP, or indeed any science.3 ExperienceTwenty-three students enrolled, with only one-third initially expressing interest in majoring incomputer science.
By the end, I was approachedby four students asking if there were research op-portunities available in the topics we had cov-ered; interestingly, one of these students hadoriginally intended to major in electrical engi-neering.
Furthermore, because of the class?spromise in drawing students into further com-puter science study, the GE Fund awarded agrant for the purpose of bringing in a senior out-side speaker and supporting teaching assistantsin future years.One issue that remains to be resolved is thelack, to my knowledge, of a textbook or text-books that would both cover the syllabus topicsand employ a level of presentation suitable forfreshmen.
For first-year students to learn effec-tively, some sort of reference seems crucial, buta significant portion of the course material wasdrawn from research papers that would proba-bly be too difficult.
In the next edition of thecourse, I plan to write up and distributeformallecture notes.Overall, although Computation, Information,and Intelligence proved quite challenging for thestudents, for the most part they felt that theyhad learned a lot from the experience, and basedon this evidence and the points outlined in theprevious paragraph, I believe that the course didmake definite progress towards its goal of inter-esting students in taking further computer sci-ence courses, especially in AI, IR, and NLP.AcknowledgmentsI thank my chair Charles Van Loan for en-couraging me to develop the course describedin this paper, for discussing many aspects ofthe class with me, and for contacting the GEFund, which I thank for supplying a grant sup-porting the future development of the class.Thanks to Jon Kleinberg for many helpful dis-cussions, especially regarding curriculum con-tent, and to the anonymous reviewers for theirfeedback.
Finally, I am very grateful to myteaching assistants, Amanda Holland-Minkley,Milo Polte, and Neeta Rattan, who helped im-mensely in making the first outing of the courserun smoothly.ReferencesRie Kubota Ando and Lillian Lee.
2000.
Mostly-unsupervised statistical segmentation of Japanese.In First Conference of the North American Chap-ter of the Association for Computational Linguis-tics (NAACL), pages 241?248.Albert-La?szlo?
Baraba?si, Re?ka Albert, and HawoongJeong.
1999.
Mean-field theory for scale-free ran-dom networks.
Physica, 272:173?187.Andrei Broder, Ravi Kumar, Farzin Maghoul, Prab-hakar Raghavan, Sridhar Rajagopalan, RaymieStata, Andrew Tomkins, and Janet Wiener.
2000.Graph structure in the web.
In Proceedings of theNinth International World Wide Web Conference,pages 309?430.Vannevar Bush.
1945.
As we may think.
The At-lantic Monthly, 176(1):101?108.Ralph Grishman.
1986.
Computational Linguistics:An Introduction.
Studies in Natural LanguageProcessing.
Cambridge.Barbara J. Grosz and Candace L. Sidner.
1986.
At-tention, intentions, and the structure of discourse.Computational Linguistics, 12(3):175?204.Jon Kleinberg.
1998.
Authoritative sources in a hy-perlinked environment.
In Proceedings of the 9thACM-SIAM Symposium on Discrete Algorithms(SODA), pages 668?677.Kevin Knight.
1999.
A statistical MT tutorial work-book.
http://www.isi.edu/natural-language/-mt/wkbk.rtf, August.Ravi Kumar, Prabhakar Raghavan, Sridhar Ra-jagopolan, D. Sivakumar, Andrew Tomkins, andEli Upfal.
2000.
Stochastic models for the webgraph.
In Proceedings of the 41st IEEE Sympo-sium on the Foundations of Computer Science,pages 57?65.George A. Miller.
1957.
Some effects of intermittentsilence.
American Journal of Psychology, 70:311?313.Frederick Mosteller and David L. Wallace.
1984.
Ap-plied Bayesian and Classical Inference: The Caseof the Federalist Papers.
Springer-Verlag.Jenny R. Saffran, Richard N. Aslin, and Elissa L.Newport.
1996.
Statistical learning by 8-month-old infants.
Science, 274(5294):1926?1928, De-cember.John R. Searle.
1980.
Minds, brains, and programs.Behavioral and Brain Sciences, 3(3):417?457.Stuart M. Shieber.
1994.
Lessons from a re-stricted Turing test.
Communications of theACM, 37(6):70?78.Herb A. Simon.
1977.
Artificial intelligence systemsthat understand.
In Proceedings of the Fifth In-ternational Joint Conference on Artificial Intelli-gence, volume 2, pages 1059?1073.Alan M. Turing.
1950.
Computing machinery andintelligence.
Mind, LIX:433?60.Appendix: sample homeworkproblemsIR unitFor simplicity, in this question, let the documentvector entries be the term frequencies normal-ized by vector length.Suppose someone proposes to you to inte-grate negative information by converting a queryq = ?x1, x2, .
.
.
, xj ,?y1,?y2, .
.
.
,?yk?
to an m-dimensional query vector ?
?q as follows: the ithentry qi of ?
?q is:qi =??
?0, wi not in the query1, wi is a positive query term?1, wi is a negative query termThey claim the -1 entries in the query vector willprevent documents that contain negative queryterms from being ranked highly.Show that this claim is incorrect, as follows.Let the entire three-word vocabulary be w1 =alligator, w2 = bat, and w3 = cat, and let q =?alligator, bat, ?cat?.
Give two documents d1and d2 such that?
d1 and d2 both contain exactly 8 words (ob-viously, some will be repetitions);?
d1 does not contain the word ?cat?;?
d2 does contain the word ?cat?
; and yet,?
d2 is ranked more highly with respect to qthan d1 is.Explain your answer; remember to show thedocument vectors corresponding to d1 and d2,the query vector, and how you computed them.Make sure the documents you choose and corre-sponding document vectors satisfy all the con-straints of this problem, including how the doc-uments get transformed into document vectors.Web unitIn this question, we engage in some preliminaryexplorations as to how many ?colluding?
webpages might be needed to ?spam?
the hubs-and-authorities algorithm (henceforth HA).Let m and n be two whole numbers biggerthan 1 (m and n need not be equal, althoughthey could be).
Consider the following set ofweb pages (all presumably on the same topic):QYYYZZZ..............................12PPPm12n12nThat is, all of the m Pi pages point to Q, and allof the n Yj pages point to all of the n Zk pages.
(a) Let m = 5 and n = 3 (thus, m = 2n ?
1,and in particularm > n), and suppose HA is runfor two iterations.
What are the best hub andauthority pages?
Explain your answers, show-ing your computations of the hub and authorityscores of every web page (using the tabular for-mat from class is fine).
(b) Now, let n be some whole number greaterthan 1, and let m = n2.
Suppose HA is runfor two iterations in this situation.
What arethe best hub and authority pages?
Explain youranswers, showing your computations of the huband authority scores of every web page.
(Note:in this problem, you don?t get to choose n; we?retrying to see what happens in general if there area quadratic number of colluding web pages.
Sotreat n as an unknown but fixed constant.
)Language structure unitRecall the Grishman (1986) ?next train toBoston?
dialog:(1) A: Do you know when the nexttrain to Boston leaves?
(2) B: Yes.
(3) A: I want to know when thetrain to Boston leaves.
(4) B: I understand.
(a) Using the Grosz/Sidner model, analyzethe discourse structure of the entire conversa-tion from the point of view of speaker A. Thatis, give the discourse segments (i.e., ?DS1 con-sists of sentences 1 and 3, and DS2 consists ofsentences 2 and 4?
), the corresponding discoursesegment purposes, and the intentional structureof the conversation.
Then, show what the focusstack is after each sentence is uttered.
Explainhow you determined your answers.
(b) Repeat the above subproblem, but fromthe point of view of speaker B.
(c) Would your answers to the previous sub-problem change if sentence (4) had been ?Whyare you telling me these things??
Does theGrosz/Sidner model adequately account for thiscase?
Explain.Statistical language processing unitIn this problem, we explicitly derive a type ofpower-law behavior in the ?Miller?s monkeys?model (henceforth MMM) from class.
First, auseful fact: for any fixed integers n > 1 andk > 0,k?i=1ni = nk+1 ?
nn?
1 .In each of the following subproblems, we rankthe ?words?
(remember that ?zz?
is a ?word?in the MMM) by their probability, rather thanby corpus frequency.
Also, j refers to some fixedbut unknown integer greater than 1; hence, youranswers should generally be functions of j.
(a) Show mathematically that the number ofwords that are shorter than j letters long is2625(26j?1 ?
1).
(b) Compute the maximum possible rank for aword that is j letters long; explain your answer.
(c) Using your answers to the previous sub-problems, find the function AR(j), the aver-age rank of a word that is j letters long, show-ing your work.
(For example, you might say?AR(j) = 4?
j?.
)(d) The probability of a word of length j isP (j) = 127?
( 127)j (that we aren?t combining liketerms is meant to be helpful ...).
Show mathe-matically that the AR(j) function you computedabove and the probability function P (j) have aparticularly simple power-law relationship:AR(j) ?
??
1P (j)for some constant ?
that doesn?t depend on j.You may make some reasonable approximations,for example, saying that n+1n+2 is close enough to1 that we can replace it by 1 for argument?ssake; but please make all such approximationsexplicit.
