Similarity-based Word SenseDisambiguationYael Karov*Weizmann InstituteSh imon Ede lman tMITWe describe amethod for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD).
The method is based on word similarity and context similaritymeasures.
Words are considered similar if they appear in similar contexts; contexts are similar ifthey contain similar words.
The circularity of this definition is resolved by an iterative, convergingprocess, in which the system learns from the corpus a set of typical usages for each of the sensesof the polysemous word listed in the MRD.
A new instance of a polysemous word is assigned thesense associated with the typical usage most similar to its context.
Experiments show that thismethod can learn even from very sparse training data, achieving over 92% correct disambiguationperformance.1.
IntroductionWord sense disambiguation (WSD) is the problem of assigning a sense to an ambiguousword, using its context.
We assume that different senses of a word correspond todifferent entries in its dictionary definition.
For example, suit has two senses listed ina dictionary: 'an action in court,' and 'suit of clothes.'
Given the sentence The union'slawyers are reviewing the suit, we would like the system to decide automatically thatsuit is used there in its court-related sense (we assume that the part of speech of thepolysemous word is known).In recent years, text corpora have been the main source of information for learn-ing automatic WSD (see, for example, Gale, Church, and Yarowsky \[1992\]).
A typicalcorpus-based algorithm constructs a training set from all contexts of a polysemousword W in the corpus, and uses it to learn a classifier that maps instances of W (eachsupplied with its context) into the senses.
Because learning requires that the examplesin the training set be partitioned into the different senses, and because sense informa-tion is not available in the corpus explicitly, this approach depends critically on manualsense tagging--a laborious and time-consuming process that has to be repeated forevery word, in every language, and, more likely than not, for every topic of discourseor source of information.The need for tagged examples creates a problem referred to in previous works asthe knowledge acquisition bottleneck: training a disambiguator for W requires thatthe examples in the corpus be partitioned into senses, which, in turn, requires a fullyoperational disambiguator.
The method we propose circumvents his problem by au-tomatically tagging the training set examples for W using other examples, that do notcontain W, but do contain related words extracted from its dictionary definition.
Forinstance, in the training set for suit, we would use, in addition to the contexts of suit,* Dept.
of Applied Mathematics and Computer Science, Rehovot 76100, Israelt Center for Biological & Computational Learning, MIT E25-201, Cambridge, MA 02142.
Present address:School of Cognitive and Computing Sciences, University of Sussex, Falmer BN1 9QH, UK(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 1all the contexts of court and of clothes in the corpus, because court and clothes appear inthe machine-readable dictionary (MRD) entry of suit that defines its two senses.
Notethat, unlike the contexts of suit, which may discuss either court action or clothing, thecontexts of court are not likely to be especially related to clothing, and, similarly, thoseof clothes will normally have little to do with lawsuits.
We will use this observation totag the original contexts of suit.Another problem that affects the corpus-based WSD methods is the sparseness ofdata: these methods typically rely on the statistics of co-occurrences of words, whilemany of the possible co-occurrences are not observed even in a very large corpus(Church and Mercer 1993).
We address this problem in several ways.
First, instead oftallying word statistics from the examples for each sense (which may be unreliablewhen the examples are few), we collect sentence-level statistics, representing eachsentence by the set of features it contains (for more on features, see Section 4.2).
Second,we define a similarity measure on the feature space, which allows us to pool thestatistics of similar features.
Third, in addition to the examples of the polysemous word142 in the corpus, we learn also from the examples of all the words in the dictionarydefinition of W. In our experiments, this resulted in a training set that could be up to20 times larger than the set of original examples.The rest of this paper is organized as follows.
Section 2 describes the approachwe have developed.
In Section 3, we report the results of tests we have conducted onthe Treebank-2 corpus.
Section 4 concludes with a discussion of related methods anda summary.
Proofs and other details of our scheme can be found in the appendix.2.
Similarity-based DisambiguationOur aim is to have the system learn to disambiguate the appearances of a polysemousword W (noun, verb, or adjective) with senses Sl .
.
.
.
, sk, using as examples the appear-ances of W in an untagged corpus.
To avoid the need to tag the training examplesmanually, we augment he training set by additional sense-related examples, whichwe call a feedback set.
The feedback set for sense si of word W is the union of allcontexts that contain some noun found in the entry of si(W) in an MRD.
1 Words inthe intersection of any two sense entries, as well as examples in the intersection oftwo feedback sets, are discarded uring initialization; we also use a stop list to discardfrom the MRD definition high-frequency words, such as that, which do not contributeto the disambiguation process.
The feedback sets can be augmented, in turn, by origi-nal training-set sentences that are closely related (in a sense defined below) to one ofthe feedback-set sentences; these additional examples can then attract other originalexamples.The feedback sets constitute a rich source of data that are known to be sorted bysense.
Specifically, the feedback set of si is known to be more closely related to si thanto the other senses of the same word.
We rely on this observation to tag automaticallythe examples of W, as follows.
Each original sentence containing W is assigned thesense of its most similar sentence in the feedback sets.
Two sentences are considered tobe similar insofar as they contain similar words (they do not have to share any word);words are considered to be similar if they appear in similar sentences.
The circularityof this definition is resolved by an iterative, converging process, described below.1 By MRD we mean a machine-readable dictionary, or a thesaurus, or any combination of suchknowledge sources.42Karov and Edelman Similarity-based Word Sense Disambiguation2.1 TerminologyA context, or example, of the target word W is any sentence that contains W and(optionally) the two adjacent sentences in the corpus.
The features of a sentence 8 areits nouns, verbs, and the adjectives of W and of any noun that appears both in $ andin W's MRD definition(s), all used after stemming (it is also possible to use other typesof features, such as word n-grams or syntactic information; see Section 4.2).
As thenumber of features in the training data can be very large, we automatically assign eachrelevant feature a weight indicating the extent o which it is indicative of the sense(see Section A.3 in the appendix).
Features that appear less than two times in thetraining set, and features whose weight falls under a certain threshold, are excluded.A sentence is represented by the set of the remaining relevant features it contains.2.2 Computation of SimilarityOur method hinges on the possibility of computing similarity between the originalcontexts of W and the sentences in the feedback sets.
We concentrate on similaritiesin the way sentences use W, not on similarities in the meaning of the sentences.
Thus,similar words tend to appear in similar contexts, and their textual proximity to theambiguous word W is indicative of the sense of W. Note that contextually similarwords do not have to be synonyms, or to belong to the same lexical category.
For ex-ample, we consider the words doctor and health to be similar because they frequentlyshare contexts, although they are far removed from each other in a typical seman-tic hierarchy such as the WordNet (Miller et al 1993).
Note, further, that because welearn similarity from the training set of W, and not from the entire corpus, it tendsto capture regularities with respect o the usage of 14;, rather than abstract or generalregularities.
For example, the otherwise unrelated words war and trafficking are sim-ilar in the contexts of the polysemous word drug ('narcotic'/'medicine'), because theexpressions drug trafficking and the war on drugs appear in related contexts of drug.
As aresult, both war and trafficking are similar in being strongly indicative of the 'narcotic'sense of drug.Words and sentences play complementary roles in our approach: a sentence isrepresented by the set of words it contains, and a word by the set of sentences inwhich it appears.
Sentences are similar to the extent they contain similar words; 2wordsare similar to the extent hey appear in similar sentences.
Although this definition iscircular, it turns out to be of great use, if applied iteratively, as described below.In each iteration , we update aword similarity matrix WSMn (one matrix for eachpolysemous word), whose rows and columns are labeled by all the words encounteredin the training set of W. In that matrix, the cell (i,j) holds a value between 0 and 1,indicating the extent o which word Wi is contextually similar to word Wj.
In addition,we keep and update a separate sentence similarity matrix SSM~ for each sense Sk of W(including a matrix SSMo k that contains the similarities of the original examples tothemselves).
The rows in a sentence matrix SSM~ correspond to the original examplesof W, and the columns to the original examples of W, for n = 0, and to the feedback-setexamples for sense Sk, for n > 0.To compute the similarities, we initialize the word similarity matrix to the identitymatrix (each word is fully similar to itself and completely dissimilar to other words),and iterate (see Figure 1):1. update the sentence similarity matrices SSM~, using the word similaritymatrix WSMn;2 Ignoring word order.
This information can be put  to use by including n-grams in the feature set; seeSection 4.2.43Computational Linguistics Volume 24, Number 1WordSimilarityMatrixSentenceSimilarityMatrixFigure 1Iterative computation of word and sentence similarities.. update the word similarity matrix WSMn, using the sentence similaritymatrices SSM~.until the changes in the similarity values are small enough (see Section A.1 of theappendix for a detailed escription of the stopping conditions; a proof of convergenceappears in the appendix).2.2.1 The Affinity Formulae.
The algorithm for updating the similarity matrices in-volves an auxiliary relation between words and sentences, which we call affinity,introduced to simplify the symmetric iterative treatment of similarity between wordsand sentences.
A word W is assumed to have a certain affinity to every sentence.
Affin-ity (a real number between 0 and 1) reflects the contextual relationships between Wand the words of the sentence.
If W belongs to a sentence S, its affinity to S is 1; ifW is totally unrelated to S, the affinity is close to 0 (this is the most common case);if W is contextually similar to the words of S, its affinity to S is between 0 and 1.In a symmetric manner, a sentence S has some affinity to every word, reflecting thesimilarity of S to sentences involving that word.We say that a word belongs to a sentence, denoted as W E S, if it is textuallycontained there; in this case, sentence is said to include the word: S 9 W. Affinity isthen defined as follows:affn(W, S) = max simn(W, Wi) (1)WiCSaff,(S, W) = max simn(S, Sj) (2)8j~wwhere n denotes the iteration number, and the similarity values are defined by theword and sentence similarity matrices, WSMn and SSMn .3 The initial representationof a sentence, as the set of words that it directly contains, is now augmented by a3 At first glance it may  seem that the mean rather than the maximal  similarity of W to the words of asentence should determine the affinity between the two.
However, any definition of affinity that takesinto account more words than just the one with the maximal  similarity to W, may result in a wordbeing directly contained in the sentence, but having an affinity to it that is smaller than 1.44Karov and Edelman Similarity-based Word Sense Disambiguationsimilarity-based representation; the sentence contains more information or featuresthan the words directly contained in it.
Every word has some affinity to the sentence,and the sentence can be represented by a vector indicating the affinity of each wordto it.
Similarly, every word can be represented by the affinity of every sentence to it.Note that affinity is asymmetric: aft(S, 14;) ~ aft(W, S), because 14; may be similar toone of the words in S, which, however, is not one of the topic words of S; it is not animportant word in S. In this case, aft(W, S) is high, because 14; is similar to a wordin S, but aft(S, 14;) is low, because S is not a representative example of the usage ofthe word 14;.2.2.2 The Similarity Formulae.
We define the similarity of 14;1 to 14;2 to be the averageaffinity of sentences that include 14;1 to 14;2.
The similarity of a sentence $1 to anothersentence 82 is a weighted average of the affinity of the words in $1 to $2:simn+l(Sb $2) = ~ weight(W, $1).
affn(W, $2) (3)WE$1simn+l(14;1, 14;2) = ~ weight(S, 14;1).
affn(S, 14;2) (4)$3W1where the weights sum to 1.
4 These values are used to update the correspondingentries of the word and sentence similarity matrices, WSM and SSM.2.2.3 The Importance of Iteration.
Initially, only identical words are considered sim-ilar, so that aft(W, S) = 1 if 14; E S; the affinity is zero otherwise.
Thus, in the firstiteration, the similarity between 81 and $2 depends on the number of words from$1 that appear in $2, divided by the length of $2 (note that each word may carrydifferent weight).
In the subsequent iterations, each word 14; c $1 contributes to thesimilarity of 81 to $2 a value between 0 and 1, indicating its affinity to $2, instead ofvoting either 0 (if 14; E $2) or 1 (if 14; ~ 82).
Analogously, sentences contribute valuesto word similarity.One may view the iterations as successively capturing parameterized "genealogi-cal" relationships.
Let words that share contexts be called direct relatives; then wordsthat share neighbors (have similar co-occurrence patterns) are once-removed relatives.These two family relationships are captured by the first iteration, and also by mosttraditional similarity measures, which are based on co-occurrences.
The second itera-tion then brings together twice-removed relatives.
The third iteration captures highersimilarity relationships, and so on.
Note that the level of relationship here is a gradu-ally consolidated real-valued quantity, and is dictated by the amount and the qualityof the evidence gleaned from the corpus; it is not an all-or-none "relatedness" tag, asin genealogy.The following simple example demonstrates the difference between our similar-ity measure and pure co-occurrence-based imilarity measures, which cannot capture4 The weight of a word estimates its expected contribution to the disambiguation task and is a productof several factors: the frequency of the word in the corpus; its frequency in the training set relative tothat in the entire corpus; the textual distance from the target word; and its part of speech (more detailson word weights appear in Section A.3 of the appendix).
All the sentences that include a given wordare assigned identical weights,45Computational Linguistics Volume 24, Number 1higher-order relationships.
Consider the set of three sentence fragments:sl eat bananas2 taste bananas3 eat appleIn this "corpus," the contextual similarity of taste and apple, according to the co-occurrence-based methods, is 0, because the contexts of these two words are disjoint.In comparison, our iterative algorithm will capture some contextual similarity:?
Initialization.
Every word is similar to itself only.?
First iteration.
The sentences eat banana and eat apple have contextualsimilarity of 0.5, because of the common word eat.
Furthermore, thesentences eat banana and taste banana have contextual similarity 0.5:- -  banana is learned to be similar to apple because of their commonusage (eat banana and eat apple);- -  taste is similar to eat because of their common usage (taste bananaand eat banana);- -  taste and apple are not similar (yet).?
Second iteration.
The sentence taste banana has now some similarity to eatapple, because in the previous iteration taste was similar to eat and bananawas similar to apple.
The word taste is now similar to apple because thetaste sentence (taste banana) is similar to the apple sentence (eat apple).
Yet,banana is more similar to apple than taste, because the similarity value ofbanana and apple further increases in the second iteration.This simple example demonstrates the transitivity of our similarity measure, whichallows it to extract high-order contextual relationships.
In more complex situations, thetransitivity-dependent spread of similarity is slower, because ach word is representedby many more sentences.The most important properties of the similarity computation algorithm are con-vergence (see Section A.2 in the appendix), and utility in supporting disambiguation(described in Section 3); three other properties are as follows.
First, word similaritycomputed according to the above algorithm is asymmetric.
For example, drug is moresimilar to traffic than traffic is to drug, because traffic is mentioned more frequentlyin drug contexts than drug is mentioned in contexts of traffic (which has many otherusages).
Likewise, sentence similarity is asymmetric: if 81 is fully contained in $2, thensire(S1, $2) = 1, whereas im($2, $1) < 1.
Second, words with a small count in thetraining set will have unreliable similarity values.
These, however, are multiplied by avery low weight when used in sentence similarity evaluation, because the frequencyin the training set is taken into account in computing the word weights.
Third, inthe computation of sim(W1, W2) for a very frequent W2, the set of its sentences ivery large, potentially inflating the affinity of W1 to the sentences that contain W2.
Wecounter this tendency by multiplying sire(W1, W2) by a weight that is reciprocallyrelated to the global frequency of W2 (this weight has been left out of Equation 4, tokeep the notation there simple).46Karov and Edelman Similarity-based Word Sense Disambiguation2.3 Using Similarity to Tag the Training SetFollowing convergence, ach sentence in the training set is assigned the sense of itsmost similar sentence in one of the feedback sets of sense si, using the final sentencesimilarity matrix.
Note that some sentences in the training set belong also to one of thefeedback sets, because they contain words from the MRD definition of the target word.Those sentences are automatically assigned the sense of the feedback set to which theybelong, since they are most similar to themselves.
Note also that an original training-set sentence S can be attracted to a sentence .T from a feedback set, even if S and .Tdo not share any word, because of the transitivity of the similarity measure.2.4 Learning the typical uses of each senseWe partition the examples of each sense into typical use sets, by grouping all thesentences that were attracted to the same feedback-set sentence.
That sentence, andall the original sentences attracted to it, form a class of examples for a typical usage.Feedback-set xamples that did not attract any original sentences are discarded.
If thenumber of resulting classes is too high, further clustering can be carried out on thebasis of the distance metric defined by 1 - sim(x, y), where sire(x, y) are values takenfrom the final sentence similarity matrix.A typical usage of a sense is represented by the affinity information generalizedfrom its examples.
For each word 14;, and each cluster C of examples of the sameusage, we define:aff(W, C) = max aff(W, S) (5)SEC= max max sim(W, Wi) (6)SEC WiCSFor each cluster we construct its affinity vector, whose ith component indicates theaffinity of word i to the cluster.
It suffices to generalize the affinity information (ratherthan similarity), because new examples are judged on the basis of their similarity toeach cluster: in the computation of sire(S1, $2) (Equation 3), the only informationconcerning $2 is its affinity values.2.5 Testing New ExamplesGiven a new sentence S containing a target word W, we determine its sense by com-puting the similarity of S to each of the previously obtained clusters Ck, and returningthe sense si of the most similar cluster:sim(Snew, Ck) = ~ weight(W, Snew)" aff(W, Ck) (7)W E S.ewsim(Snew, si) = max sim(Snew, C) (8)Ccsi3.
Experimental Evaluation of the MethodWe tested the algorithm on the Treebank-2 corpus, which contains one million wordsfrom the Wall Street Journal, 1989, and is considered a small corpus for the presenttask.
During the development and the tuning of the algorithm, we used the methodof pseudowords (Gale, Church, and Yarowsky 1992; Schitze 1992), to avoid the needfor manual verification of the resulting sense tags.The method of pseudowords i based on the observation that a disambiguationprocess designed to distinguish between two meanings of the same word should also47Computational Linguistics Volume 24, Number 1Table 1The four polysemous test words, and the seed words they generated with the use of the MRD.Word Seed Wordsdrug 1. stimulant, alcoholist, alcohol, trafficker, crime2.
medicine, pharmaceutical, remedy, cure, medication, pharmacists,prescription1.
conviction, judgment, acquittal, term2.
string, word, constituent, dialogue, talk, conversation, text1.
trial, litigation, receivership, bankruptcy, appeal, action, case, lawsuit,foreclosure, proceeding2.
garment, fabric, trousers, pants, dress, frock, fur, silks, hat, boots, coat, shirt,sweater, vest, waistcoat, skirt, jacket, cloth1.
musician, instrumentalist, performer, artist, actor, twirler, comedian, dancer,impersonator imitator, bandsman, jazz, recorder, singer, vocalist, actress,barnstormer, playactor, trouper, character, actor, scene-stealer, star, baseball,ball, football, basketball2.
participant, contestant, trader, analyst, dealersentencesuitplayerbe able to separate the meanings of two different words.
Thus, a data set for testinga disambiguation algorithm can be obtained by starting with two collections of sen-tences, one containing a word X, and the other a word y,  and inserting y instead ofevery appearance of ,-Y in the first collection.
The algorithm is then tested on the unionof the two collections, in which X is now a "polysemous" word.
The performance ofthe algorithm is judged by its ability to separate the sentences that originally contained,~' from those that originally contained 32; any mistakes can be used to supervise thetuning of the algorithm.
53.1 Test dataThe final algorithm was tested on a total of 500 examples of four polysemous words:drug, sentence, suit, and player (see Table 2; although we confined the tests to nouns,the algorithm is applicable to any part of speech).
The relatively small number ofpolysemous words we studied was dictated by the size and nature of the corpus (weare currently testing additional words, using texts from the British National Corpus).As the MRD, we used a combination of the on-line versions of the Webster's andthe Oxford dictionaries, and the WordNet system (the latter used as a thesaurus only;see Section 4.3).
The resulting collection of seed words (that is, words used to generatethe feedback sets) is listed in Table 1.We found that the single best source of seed words was WordNet (used as the-saurus only).
The number of seed words per sense turned out to be of little significance.For example, whereas the MRD yielded many garment-related words, to be used asseeds for suit in the 'garment' sense, these generated a small feedback set, becauseof the low frequency of garment-related words in the training corpus.
In comparison,there was a strong correlation between the size of the feedback set and the disam-biguation performance, indicating that a larger corpus is likely to improve the results.As can be seen from the above, the original training data (before the addition of5 Note that our disambiguation algorithm works the best for polysemous words whose senses areunrelated to each other, in which case the overlap between the feedback sets is minimized; likewise,the method of training with pseudowords amounts to an assumption ofindependence of the differentsenses.48Karov and Edelman Similarity-based Word Sense DisambiguationTable 2A summary of the algorithm's performance on the four test words.Word Senses Sample Size Feedback Size % Correct % Correctper Sense Totaldrug narcotic 65 100 92.3 90.5medicine 83 65 89.1sentence judgment 23 327 100.0 92.5grammar 4 42 50.0suit court 212 1,461 98.6 94.8garment 21 81 55.0player performer 48 230 87.5 92.3participant 44 1,552 97.7the feedback sets) consisted of a few dozen examples, in comparison to thousands ofexamples needed in other corpus-based methods (Sch~itze 1992; Yarowsky 1995).
Theaverage success rate of our algorithm on the 500 appearances of the four test wordswas 92%.3.2 The Drug ExperimentWe now present in detail several of the results obtained with the word drug.
Considerfirst the effects of iteration.
A plot of the improvement in the performance vs. iterationnumber appears in Figure 2.
The success rate is plotted for each sense, and for theweighted average of both senses we considered (the weights are proportional to thenumber of examples of each sense).
Iterations 2 and 5 can be seen to yield the bestperformance; iteration 5 is to be preferred, because of the smaller difference betweenthe success rates for the two senses of the target word.Figure 3 shows how the similarity values develop with iteration umber.
For eachexample S of the 'narcotic' sense of drug, the value of simn(S, narcotic) increases with n.Figure 4 compares the similarities of a 'narcotic'-sense example to the 'narcotic' senseand to the 'medicine' sense, for each iteration.
One can see that the 'medicine' senseassignment, made in the first iteration, is gradually suppressed.
The word menace,which is a hint for the 'narcotic' sense in the sentence used in this example, didnot help in the first iteration, because it did not appear in the 'narcotic' feedbackset at all.
Thus, in iteration 1, the similarity of the sentence to the 'medicine' sensewas 0.15, vs. a similarity of 0.1 to the 'narcotic' sense.
In iteration 2, menace waslearned to be similar to other 'narcotic'-related words, yielding a small advantage forthe 'narcotic' sense.
In iteration 3, further similarity values were updated, and therewas a clear advantage to the 'narcotic' sense (0.93, vs. 0.89 for 'medicine').
Eventually,all similarity values become close to 1, and, because they are bounded by 1, they cannotchange significantly with further iterations.
The decision is, therefore, best made afterrelatively few iterations, as we just saw.Table 3 shows the most similar words found for the words with the highest weightsin the drug example (low-similarity words have been omitted).
Note that the similarityis contextual, and is affected by the polysemous target word.
For example, traffickingwas found to be similar to crime, because in drug contexts the expressions drug traffickingand crime are highly related.
In general, trafficking and crime need not be similar, ofcourse.49Computational Linguistics Volume 24, Number 1Table 3The drug experiment; the nearest neighbors of the highest-weight words.Word Most Contextually Similar WordsThe 'Medicine' Sensemedication antibiotic blood prescription medicine percentage pressureprescription analyst antibiotic blood campaign introduction law line-up medication medicinepercentage print profit publicity quarter sedative state television tranquilizer usemedicine prescription campaign competition dollar earnings law manufacturingmargin print product publicity quarter esult sale saving sedativestaff state television tranquilizer unit usedisease antibiotic blood line-up medication medicine prescriptionsymptom hypoglycemia insulin warning manufacturer p oductplant animal death diabetic evidence finding metabolism studyinsulin hypoglycemia manufacturer p oduct symptom warningdeath diabetic finding report studytranquilizer campaign law medicine prescription print publicity sedativetelevision use analyst profit statedose appeal death impact injury liability manufacturer miscarriage refusing rulingdiethylstilbestrol hormone damage ffect female prospect stateThe 'Narcotic' Senseconsumer distributor effort cessation consumption country reduction requirementvictory battle capacity cartel government mafia newspaper peoplemafia terrorism censorship dictatorship newspaper press brother nothing aspirationassassination editor leader politics rise action country doubt freedommafioso medium menace solidarity structure trade worldterrorism censorship doubt freedom mafia medium menace newspaperpress solidarity structuremurder capital-punishment symbolism trafficking furor killing substance crimerestaurant law bill case problemmenace terrorism freedom solidarity structure medium press censorship country doubtmafia newspaper way attack government magnitude people relation threatworldtrafficking crime capital-punishment furor killing murder estaurant substance symbolismdictatorship aspiration brother editor mafia nothing politics pressassassination censorship leader newspaper rise terrorismassassination brother censorship dictatorship mafia nothing press terrorismaspiration editor leader newspaper politics riselaundering army lot money arsenal baron economy explosive government handmaterial military none opinion portion talkcensorship mafia newspaper press terrorism country doubt freedommedium menace solidarity structure5098x<96949290888684828078 ~!Karov and Edelman Similarity-based Word Sense Disambiguation2 3 4 5 6 7 8 9Figure 2The drug experiment; the change in the disambiguation performance with iteration number isplotted separately for each sense (the asterisk marks the plot of the success rate for the'narcotic' sense; the other two plots are the 'medicine' sense, and the weighted average of thetwo senses).
In our experiments, the typical number of iterations was 3.Similarity of sense1 examples to sense1 feedback set0.~o.~o.O.Example #Figure 30 0Iteration #10The drug experiment; an illustration of the development of the support for a particular sensewith iteration.
The plot shows the similarity of a number of drug sentences to the 'narcotic'sense.
To facilitate visualization, the curves are sorted by the second-iteration values ofsimilarity.4.
D iscuss ionWe now discuss in some detail the choices made at the different stages of the devel-opment of the present method, and its relationship to some of the previous works onword sense disambiguation.51t _  10.90.80.70.60.50.40.30.20.1Computational Linguistics Volume 24, Number 1i i i i i i i t2 3 4 5 6 7 8 9 10Figure 4The drug experiment; the similarity of a 'narcotic'-sense example to each of the two senses.
Thesentence here was The American people and their government also woke up too late to the menace drugsposed to the moral structure of their country.
The asterisk marks the plot for the 'narcotic' sense.4.1 Flexible Sense Dist inct ionsThe possibility of strict definition of each sense of a polysemous word, and the possibil-ity of unambiguous a signment ofa given sense in a given situation are, in themselves,nontrivial issues in philosophy (Quine 1960) and linguistics (Weinreich 1980; Cruse1986).
Different dictionaries often disagree on the definitions; the split into senses mayalso depend on the task at hand.
Thus, it is important to maintain the possibility offlexible distinction of the different senses, e.g., by letting this distinction be determinedby an external knowledge source such as a thesaurus or a dictionary.
Although thisrequirement may seem trivial, most corpus-based methods do not, in fact, allow suchflexibility.
For example, defining the senses by the possible translations of the word(Dagan, Itai and Schwall 1991; Brown et al 1991; Gale, Church, and Yarowsky 1992),by the Roget's categories (Yarowsky 1992), or by clustering (Schi~tze 1992), yields agrouping that does not always conform to the desired sense distinctions.In comparison to these approaches, our reliance on the MRD for the definition ofsenses in the initialization of the learning process guarantees the required flexibilityin setting the sense distinctions.
Specifically, the user of our system may choose acertain dictionary definition, a combination of definitions from several dictionaries,or manually listed seed words for every sense that needs to be defined.
Whereaspure MRD-based methods allow the same flexibility, their potential so far has notbeen fully tapped, because definitions alone do not contain enough information fordisambiguation.4.2 Sentence FeaturesDifferent polysemous words may benefit from different types of features of the contextsentences.
Polysemous words for which distinct senses tend to appear in different top-ics can be disambiguated using single words as the context features, as we did here.Disambiguation f other polysemous words may require taking the sentence structureinto account, using n-grams or syntactic onstructs as features.
This additional infor-mation can be incorporated into our method, by (1) extracting features uch as nouns,!52Karov and Edelman Similarity-based Word Sense Disambiguationverbs, adjectives of the target word, bigrams, trigrams, and subject-verb or verb-objectpairs, (2) discarding features with a low weight (cf.
Section A.3 of the appendix), and(3) using the remaining features instead of single words (i.e., by representing a sen-tence by the set of significant features it contains, and a feature by the set of sentencesin which it appears).4.3 Using WordNetThe initialization of the word similarity matrix using WordNet, a hand-crafted se-mantic network arranged in a hierarchical structure (Miller et al 1993), may seem tobe advantageous over simply setting it to the identity matrix, as we have done.
Tocompare these two approaches, we tried to set the initial (dis)similarity between twowords to the WordNet path length between their nodes (Lee, Kim, and Lee 1993), andthen learn the similarity values iteratively.
This, however, led to worse performancethan the simple identity-matrix initialization.There are several possible reasons for the poor performance of WordNet in thiscomparison.
First, WordNet is not designed to capture contextual similarity.
For ex-ample, in WordNet, hospital and doctor have no common ancestor, and hence theirsimilarity is 0, while doctor and lawyer are quite similar, because both designate pro-fessionals, humans, and living things.
Note that, contextually, doctor should be moresimilar to hospital than to lawyer.
Second, we found that the WordNet similarity valuesdominated the contextual similarity computed in the iterative process, preventing thetransitive ffects of contextual similarity from taking over.
Third, the tree distance initself does not always correspond to the intuitive notion of similarity, because differ-ent concepts appear at different levels of abstraction and have a different number ofnested subconcepts.
For example, a certain distance between two nodes may resultfrom (1) the nodes being semantically close, but separated by a large distance, stem-ming from a high level of detail in the related synsets, or from (2) the nodes beingsemantically far from each other .
64.4 Ignoring Irrelevant ExamplesThe feedback sets we use in training the system may contain noise, in the form ofirrelevant examples that are collected along with the relevant and useful ones.
Forinstance, in one of the definitions of bank in WordNet, we find bar, which, in turn,has many other senses that are not related to bank.
Although these unrelated sensescontribute xamples to the feedback set, our system is hardly affected by this noise,because we do not collect statistics on the feedback sets (i.e., our method is not basedon mere co-occurrence frequencies, as most other corpus-based methods are).
Therelevant examples in the feedback set of the sense si will attract he examples of si;the irrelevant examples will not attract he examples of si, but neither will they dodamage, because they are not expected to attract examples of sj ~" ~ i).4.5 Related work4.5.1 The Knowledge Acquisition Bottleneck.
Brown et al (1991) and Gale, Church,and Yarowsky (1992) used the translations of ambiguous words in a bilingual corpusas sense tags.
This does not obviate the need for manual work, as producing bilingualcorpora requires manual translation work.
Dagan, Itai, and Schwall (1991) used abilingual lexicon and a monolingual corpus to save the need for translating the corpus.6 Resnik (1995) recently suggested that his particular difficulty can be overcome by a different measurethat akes into account the informativeness of the most specific ommon ancestor fthe two words.53Computational Linguistics Volume 24, Number 1The problem remains, however, that the word translations do not necessarily overlapwith the desired sense distinctions.Sch/.itze (1992) clustered the examples in the training set and manually assignedeach cluster a sense by observing 10-20 members of the cluster.
Each sense was usu-ally represented by several clusters.
Although this approach significantly decreased theneed for manual intervention, about a hundred examples had still to be tagged man-ually for each word.
Moreover, the resulting clusters did not necessarily correspondto the desired sense distinctions.Yarowsky (1992) learned discriminators for each Roget's category, saving the needto separate the training set into senses.
However, using such hand-crafted categoriesusually leads to a coverage problem for specific domains or for domains other thanthe one for which the list of categories has been prepared.Using MRD's (Amsler 1984) for word sense disambiguation was popularized byLesk (1986); several researchers subsequently continued and improved this line ofwork (Krovetz and Croft 1989; Guthrie et al 1991; V~ronis and Ide 1990).
Unlike theinformation in a corpus, the information in the MRD definitions is presorted intosenses.
However, as noted above, the MRD definitions alone do not contain enoughinformation to allow reliable disambiguation.
Recently, Yarowsky (1995) combined anMRD and a corpus in a bootstrapping process.
In that work, the definition wordswere used as initial sense indicators, automatically tagging the target word examplescontaining them.
These tagged examples were then used as seed examples in thebootstrapping process.
In comparison, we suggest o further combine the corpus andthe MRD by using all the corpus examples of the MRD definition words, instead ofthose words alone.
This yields much more sense-presorted training information.4.5.2 The Problem of Sparse Data.
Most previous works define word similarity basedon co-occurrence information, and hence face a severe problem of sparse data.
Manyof the possible co-occurrences are not observed even in a very large corpus (Churchand Mercer 1993).
Our algorithm addresses this problem in two ways.
First, we replacethe all-or-none indicator of co-occurrence by a graded measure of contextual similarity.Our measure of similarity is transitive, allowing two words to be considered similareven if they neither appear in the same sentence, nor share neighbor words.
Second,we extend the training set by adding examples of related words.
The performance ofour system compares favorably to that of systems trained on sets larger by a factorof 100 (the results described in Section 3 were obtained following learning from severaldozen examples, in comparison to thousands of examples in other automatic methods).Traditionally, the problem of sparse data is approached by estimating the prob-ability of unobserved co-occurrences u ing the actual co-occurrences in the trainingset.
This can be done by smoothing the observed frequencies 7 (Church and Mercer1993) or by class-based methods (Brown et al 1991; Pereira and Tishby 1992; Pereira,Tishby, and Lee 1993; Hirschman 1986; Resnik 1992; Brill et al 1990; Dagan, Marcus,and Markovitch 1993).
In comparison to these approaches, we use similarity informa-tion throughout training, and not merely for estimating co-occurrence statistics.
Thisallows the system to learn successfully from very sparse data.7 Smoothing is a technique widely used in applications, uch as statistical pattern recognition andprobabilistic language modeling, that require a probability density to be estimated from data.
Forsparse data, this estimation problem is severely underconstrained, and, thus, ill-posed; smoothingregularizes the problem by adopting a prior constraint that assumes that the probability density doesnot change too fast in between the examples.54Karov and Edelman Similarity-based Word Sense Disambiguation4.6 SummaryWe have described an approach to WSD that combines a corpus and an MRD to gen-erate an extensive data set for learning similarity-based disambiguation.
Our systemcombines the advantages ofcorpus-based approaches (large number of examples) withthose of the MRD-based approaches (data presorted by senses), by using the MRD def-initions to direct the extraction of training information (in the form of feedback sets)from the corpus.In our system, a word is represented by the set of sentences in which it appears.Accordingly, words are considered similar if they appear in similar sentences, and sen-tences are considered similar if they contain similar words.
Applying this definitioniteratively ields a transitive measure of similarity under which two sentences may beconsidered similar even if they do not share any word, and two words may be consid-ered similar even if they do not share neighbor words.
Our experiments show that theresulting alternative to raw co-occurrence-based imilarity leads to better performanceon very sparse data.AppendixA.1 Stopping Conditions of the Iterative AlgorithmLet fi be the increase in the similarity value in iteration i:f~(X,y) = simi(X, y ) -  simi_l(X, 32) (9)where X, y can be either words or sentences.
For each item X, the algorithm stopsupdating its similarity values to other items (that is, updating its row in the similaritymatrix) in the first iteration that satisfies maxyfi(X,  3;) _< ?, where c > 0 is a presetthreshold.According to this stopping condition, the algorithm terminates after at mostiterations (otherwise, in !, iterations with eachfi > c, we obtain sim(,V, 3;) > ?- ~ = 1,in contradiction to upper bound of 1 on the similarity values; see Section A.2 below).We found that the best results are obtained within three iterations.
After that,the disambiguation results tend not to change significantly, although the similarityvalues may continue to increase.
Intuitively, the transitive xploration of similaritiesis exhausted after three iterations.A.2 ProofsIn the following, X, 3; can be either words or sentences.Theorem 1Similarity is bounded: simn (X, 3;) _< 1ProofBy induction on the number of iteration.
At the first iteration, sim0(X, 3;) _< 1, byinitialization.
Assume that the claim holds for n, and prove for n + 1:simn+l (X, Y) = E weight(K, X)maxsimn(Xj, Yk) YkEY ~CX< ~ weight(,~, X) ?
1 (by the induction hypothesis)-- 155Computational Linguistics Volume 24, Number 1Theorem 2Similarity is reflexive: VX, sim(X, X) = 1ProofBy induction on the number of iteration, sim0(X, X) = 1, by initialization.
Assumethat the claim holds for n, and prove for n + 1:Simn+l(X, X) = E weight(~, X).
max simn(Xi, ,,~)x~cx ,~cx>- E weight(X/, X).
simn(Xi, Xi)XiE X= ~ weight(Xi, X) ?
1 (by the induction hypothesis)XiEX= 1Thus, simn+l(,V, X) _> 1.
By theorem 1, simn+l(X, X) _< 1, so simn+l(X, X) = 1.Theorem 3Similarity sim, (X, 3;) is a nondecreasing function of the number of iteration n.ProofBy induction on the number of iteration.
Consider the case of n = 1: siml(X, Y) >sim0(X, 3;) (if sim0(X, y) = 1, then X = 3;, and siml(X, 3;) = 1 as well; elsesim0(X, 3;) = 0 and siml(X, 3;) >_ 0 ~- sim0(X, 3;)).
Now, assume that the claimholds for n, and prove for n + 1:simn+l (X, Y) - siren(X, Y )== ~ weight(K, X).
maxsim,(Xj, Yk)YkEY X/~X- E weight(Xj, A').
maxsim,_l(Xj, Yt)NkEY x/ex> ~ weight(Xj, X) .
(maxsimn(Xi ,  Yk ) -  maxsim,~_l(~, Yk)),~cx \YkEY ".
YkEY> 0The last inequality holds because, by the induction hypothesis,V~,Yk, simn(~, Yk) _> simn_l(Xj, Yk)maxsimn(Xj, Yk) >_ maxsim,_l(Xj, Yk)YkEY YkEYmaxsimn(Xj, Yk)-  maxsimn_l(Xj, Yk) _> 0Yk~Y Y~EYThus, all the items under the sum are nonnegative, and so must be their weightedaverage.
As a consequence, we may conclude that the iterative stimation of similarityconverges.56Karov and Edelman Similarity-based Word Sense DisambiguationA.3 Word weightsIn our algorithm, the weight of a word estimates its expected contribution to the dis-ambiguation task, and the extent o which the word is indicative in sentence similarity.The weights do not change with iterations.
They are used to reduce the number offeatures to a manageable size, and to exclude words that are expected to be givenunreliable similarity values.
The weight of a word is a product of several factors: fre-quency in the corpus, the bias inherent in the training set, distance from the targetword, and part-of-speech label:..Global frequency.
Frequent words are less informative of sense and ofsentence similarity.
For example, the appearance of year, a frequent word,in two different sentences in the corpus we employed would notnecessarily indicate similarity between them, and would not be effectivein disambiguating the sense of most target words).
The contribution offreq(W) .
frequency is max{0, 1 max5xfreq(X) J' where max5xfreq(X) is a functionof the five highest frequencies in the global corpus, and X is any noun,or verb, or adjective there.
This factor excludes only the most frequentwords from further consideration.
As long as the frequencies are notvery high, it does not label 14;1 whose frequency is twice that of W2 asless informative.Log-likelihood factor.
Words that are indicative of the sense usually appearin the training set more than what would have been expected from theirfrequency in the general corpus.
The log-likelihood factor captures thistendency.
It is computed as..Pr (Wi 114;) (10)log Pr (Wi)where Pr(14;i) is estimated from the frequency of 14; in the entire corpus,and Pr(Wi I 14;) from the frequency of Wi in the training set, given theexamples of the current ambiguous word W (cf.
Gale, Church, andYarowsky \[1992\]).
8 To avoid poor estimation for words with a low countin the training set, We multiply the log likelihood by min{1, ~o~nt(w)10 }where count(W) is the number of occurrences of 14; in the training set.Part of speech.
Each part of speech is assigned a weight (1.0 for nouns, 0.6for verbs, and 1.0 for the adjectives of the target word).Distance from the target word.
Context words that are far from the targetword are less indicative than nearby ones.
The contribution of this factoris reciprocally related to the normalized istance: the weight of contextwords that appear in the same sentence as the target word is taken tobe 1.0; the weight of words that appear in the adjacent sentences i 0.5.The total weight of a word is the product of the above factors, each normalized byfactor(Wi, S) the sum of factors of the words in the sentence: weight(Wi, $) = z_,~'wjcs factor(W j, S) '8 Because this estimate is unreliable for words with low frequencies in each sense set, Gale, Church, andYarowsky (1992) suggested to interpolate between probabilities computed within the subcorpus andprobabilities computed over the entire corpus.
In our case, the denominator is the frequency in thegeneral corpus instead of the frequency in the sense xamples, o it is more reliable.57Computational Linguistics Volume 24, Number 1where factor(., .)
is the weight before normalization.
The use of weights contributedabout 5% to the disambiguation performance.A.4 Other uses of context similarityThe similarity measure developed in the present paper can be used for tasks other thanword sense disambiguation.
Here, we illustrate a possible application to automaticconstruction of a thesaurus.Following the training phase for a word X, we have a word similarity matrix forthe words in the contexts of :Y.
Using this matrix, we construct for each sense si of ,-Ya set of related words, R:?2..Initialize R to the set of words appearing in the MRD definition of si;Extend R recursively: for each word in R added in the previous step, addits k nearest neighbors, using the similarity matrix.Stop when no new words (or too few new words) are added.Upon termination, output for each sense si the set of its contextually similar words R.AcknowledgmentsWe thank Dan Roth for many usefuldiscussions, and the anonymous reviewersfor constructive comments on themanuscript.
This work was first presentedat the 4th International Workshop on LargeCorpora, Copenhagen, August 1996.ReferencesBrill, Eric, David Magerman, Mitchell P.Marcus, and Beatrice Santorini.
1990.Deducing linguistic structure from thestatistics of large corpora.
In DARPASpeech and Natural Language Workshop,pages 275-282, June.Brown, Peter F., Stephen Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1991.
Word sense disambiguationusing statistical methods.
In Proceedings ofthe 29th Annual Meeting, pages 264-270.Association for ComputationalLinguistics.Church, Kenneth W. and Robert L. Mercer.1993.
Introduction to the special issue oncomputational linguistics using largecorpora.
Computational Linguistics, 19:1-24.Cruse, D. Alan.
1986.
Lexical Semantics.Cambridge University Press, Cambridge,England.Dagan, Ido, Alon Itai, and Ulrike Schwall.1991.
Two languages are more informativethan one.
In Proceedings ofthe 29th AnnualMeeting, pages 130-137.
Association forComputational Linguistics.Dagan, Ido, Shaul Marcus, and ShaulMarkovitch.
1993.
Contextual wordsimilarity and estimation from sparsedata.
In Proceedings ofthe 31st AnnualMeeting, pages 164-174.
Association forComputational Linguistics.Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities,26:415-439.Guthrie, Joe A., Louise Guthrie, YorickWilks, and Homa Aidinejad.
1991.-Subject-dependent cooccurrence andword sense disambiguation.
I  Proceedingsof the 29th Annual Meeting, pages 146-152.Association for ComputationalLinguistics.Hirschman, Lynette.
1986.
Discoveringsublanguage structure.
In RalphGrishman and Richard Kittredge, editors,Analyzing Language in Restricted Domains:Sublanguage Description and Processing.Lawrence Erlbaum, Hillsdale, NJ, pages211-234.Krovetz, Robert and W. Bruce Croft.
1989.Word sense disambiguation usingmachine readable dictionaries.
InProceedings ofACM SIGIR'89, pages127-136.Lee, Joon H., Myoung H. Kim, and Yoon J.Lee.
1993.
Information retrieval based onconceptual distance in IS-A hierarchies.Journal of Documentation, 49:188-207.Lesk, Michael.
1986.
Automatic sensedisambiguation: How to tell a pine conefrom an ice cream cone.
In Proceedings ofthe 1986 ACM SIGDOC Conference, pages24-26.58Karov and Edelman Similarity-based Word Sense DisambiguationMiller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1993.
Introduction toWordNet: An on-line lexical database.CSL 43, Cognitive Science Laboratory,Princeton University, Princeton, NJ.Pereira, Fernando and Naftali Tishby.
1992.Distibutional similarity, phase transitionsand hierarchical clustering.
In WorkingNotes of the AAAI Fall Symposium onProbabilistic Approaches to Natural Language,pages 108-112.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distibutional clusteringof English words.
In Proceedings ofthe 31stAnnual Meeting, pages 183-190.Association for ComputationalLinguistics.Quine, Willard V. O.
1960.
Word and Object.MIT Press, Cambridge, MA.Resnik, Philip.
July 1992.
WordNet anddistribuitional nalysis: A class-basedapproach to lexical discovery.
In AAAIWorkshop on Statistically-based NaturalLanguage Processing Techniques, pages56-64.Resnik, Philip.
June 1995.
Disambiguatingnoun groupings with respect to WordNetsenses.
In Third Workshop on Very LargeCorpora, pages 55-68, Cambridge, MA.Schi~tze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings ofSupercomputingSymposium, pages 787-796, Minneapolis,MN.V6ronis, Jean and Nancy Ide.
1990.
Wordsense disambiguation with very largeneural networks extracted from machinereadable dictionaries.
In Proceedings ofCOLING-90, pages 389-394.Walker, Donald E. and Robert A. Amsler.1986.
The use of machine-readabledictionaries in sublanguage analysis.
InR.
Grisham, editor, Analyzing Languages inRestricted Domains: Sublanguage Descriptionand Processing.
Lawrence ErlbaumAssociates, Hillsdale, NJ.Weinreich, Uriel.
1980.
On Semantics.University of Pennsylvania Press,Philadelphia, PA.Yarowsky, David.
1992.
Word sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofCOLING-92,pages 454-460, Nantes, France.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting, pages 189-196, Cambridge, MA.Association for ComputationalLinguistics.59
