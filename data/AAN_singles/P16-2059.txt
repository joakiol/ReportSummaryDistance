Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 362?368,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Monolingual Compositional Representationsvia Bilingual SupervisionAhmed Elgohary and Marine CarpuatDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742, USAelgohary@cs.umd.edu, marine@cs.umd.eduAbstractBilingual models that capture the seman-tics of sentences are typically only eval-uated on cross-lingual transfer tasks suchas cross-lingual document categorizationor machine translation.
In this work, weevaluate the quality of the monolingualrepresentations learned with a variant ofthe bilingual compositional model of Her-mann and Blunsom (2014), when viewingtranslations in a second language as a se-mantic annotation as the original languagetext.
We show that compositional objec-tives based on phrase translation pairs out-perform compositional objectives basedon bilingual sentences and on monolingualparaphrases.1 IntroductionThe effectiveness of new representation learningmethods for distributional word representations(Baroni et al, 2014) has brought renewed interestto the question of how to compose semantic rep-resentations of words to capture the semantics ofphrases and sentences.
These representations offerthe promise of capturing phrasal or sentential se-mantics in a general fashion, and could in principlebenefit any NLP applications that analyze text be-yond the word level, and improve their ability togeneralize beyond contexts seen in training.While most prior work has focused either oncomposing words into short phrases (Mitchell andLapata, 2010; Baroni and Zamparelli, 2010; Her-mann et al, 2012; Fyshe et al, 2015), or on super-vised task-specific composition functions (Socheret al, 2013; Iyyer et al, 2015; Rockt?aschel et al,2015; Iyyer et al, 2014; Tai et al, 2015, interalia), Wieting et al (2016) recently showed thata simple composition architecture (vector averag-ing) can yield sentence models that consistentlyperform well in semantic textual similarity tasksin a wide range of domains, and outperform morecomplex sequence models (Tai et al, 2015).
Inter-estingly, these models are trained using PPDB, theparaphrase database (Ganitkevitch et al, 2013),which was learned from bilingual parallel corpora.In bilingual settings, there are also a few ex-amples of bilingual sentence models (Zou et al,2013; Hermann and Blunsom, 2014; Lauly etal., 2014; Gouws et al, 2014).
However, theyhave only been evaluated in cross-lingual trans-fer settings (e.g., cross-lingual document classifi-cation, or machine translation), which do not di-rectly evaluate the quality of the sentence-level se-mantic representations learned.In this work, we directly evaluate the usefulnessof modeling semantic equivalence using composi-tional models of translated texts for detecting se-mantic textual similarity in a single language.
Forinstance, in addition to using translated texts tomodel cross-lingual transfer from English to a for-eign language, we can view English translations asa semantic annotation of the foreign text, and eval-uate the usefulness of the resulting foreign repre-sentations.
While learning representations in lan-guages other than English is a pressing practicalproblem, this paper will focus on evaluating En-glish sentence representations learned on Englishsemantic similarity tasks to facilitate comparisonwith prior work.Our results show that sentence representationslearned using a bilingual compositional objectiveoutperform representations learned using mono-lingual evidence, whether compositional or not.
Inaddition, phrasal translations yield better represen-tations than full sentence translations, even whenapplied to sentence-level tasks.362Table 1: Positive and negative examples for each of the 3 types of supervision consideredBilingual Sentences +thus, in fact, we might say thathe hurried ahead of the decisionby our fellow member.as que podramos decir, de hecho, que se adelant ala decisin de nuestro colega.-thus, in fact, we might say thathe hurried ahead of the decisionby our fellow member.seor presidente, la votacinsobre sellafield ha sido una novedaden el parlamento europeo .English paraphrases + by our fellow member by our colleague- by our fellow member of the committee?s work+ slowly than anticipated slowly than expectedBilingual phrases + by our fellow member de nuestro colega diputado- by our fellow member miles de personas de todo+ book and buy airline tickets reserva y adquisicin de billetes+ the air fare advertised should show el precio del billete anunciado debera indicar+ a book by the american writer noam un libro del escritor norteamericano noam2 ModelsInspired by the bilingual model of (Hermann andBlunsom, 2014), and paraphrase model of (Wiet-ing et al, 2016), representations for multi-wordsegments are built with a simple bag-of-word ad-ditive combination of word representations, whichare trained to minimize the distance between se-mantically equivalent segments.2.1 Three Views of Semantic EquivalenceThe different types of semantic equivalence usedfor training are illustrated in Table 1.Parallel Sentences occur naturally, and providetraining examples that are more consistent withdownstream applications.
However, they can benoisy due to automatic sentence alignment andone-to-many mappings, and bag-of-word repre-sentations of sentence meaning are likely to be in-creasingly noisier as segments get longer.Monolingual Paraphrases are invaluable re-sources, but rarely occur naturally , and creat-ing paraphrase resources therefore requires con-siderable effort.
Ganitkevitch et al (2013)automatically-created paraphrase resources formany languages using parallel corpora.Parallel Phrases or phrasal translations mightprovide a tighter definition of semantic equiva-lence than longer sentence pairs, but phrase pairshave to be extracted automatically based on wordalignments, an automatic and noisy process.2.2 Models and Learning ObjectivesOur main model is based on the bilingual com-position model of Hermann and Blunsom (2014),which learns a word embedding matrix W from atraining set X of aligned sentence pairs ?x1, x2?.Each of x1and x2is represented as a bag-of-words, i.e.
a superset of column indices inW .
Each aligned pair ?x1, x2?
is augmentedwith k randomly selected sentences that arenot aligned to x1, and another k that are notaligned to x2.
Given this augmented example?x1, x2, x?11, ..., x?k1, x?12, ..., x?k2?, the model trainingobjective is defined as follows:Jbi(W ) =?2||W ||2F+??x1,x2,x?1,x?2?k?i=1[?
+ ||g(x1)?
g(x2)||2?
||g(x1)?
g(x?i2)||2]h[?
+ ||g(x1)?
g(x2)||2?
||g(x2)?
g(x?i1)||2]h(1)where g(x) =?i?xW:i, [.
]his the hinge func-tion (i.e.
[v]h= max(0, v)) whose margin is givenby ?
and ?
is a regularization parameter.The paraphrase-based model of Wieting et al(2016) shares the same structure as the bilingualmodel above, but differs in the nature of seg-ments used to define semantic equivalence (sen-tence pairs vs. paraphrases), the distance functionused (Euclidean distance vs. cosine similarity), aswell as the negative sampling strategies, and wordembeddings initialization and regularization.
We363Table 2: Training conditions: three types of semantic equivalence for composed representations.Condition # examples Avg.
length ProvenanceBilingual Sentences 1.9M 28 Europarl-v7 (Koehn, 2005)Bilingual phrases 3M 5 + Moses phrase extraction (Koehn et al, 2007)Monolingual phrases 3M 3 PPDB XL (Ganitkevitch et al, 2013)provide empirical comparisons with the Wietinget al (2016) embeddings, and also define a sim-plified version of that objective, Jpa, to allow forcontrolled comparisons with Jbi.Jpauses random initialization and penalizeslarge values in W with a ||W ||2Fregularizationterm1.
The choice of distance function (Euclideandistance or cosine similarity) and of the negativesampling strategy2are viewed as tunable hyperpa-rameters.3 Experiments3.1 Evaluating Sentence RepresentationsFollowing Wieting et al (2016), the models aboveare evaluated on the four Semantic Textual Simi-larity (STS) datasets (Agirre et al, 2012; Agirre etal., 2013; Agirre et al, 2014; Agirre et al, 2015),which provide pairs of English sentences from dif-ferent domains (e.g., Tweets, news, webforums,image captions), annotated with human judgmentsof similarity on a 1 to 5 scale.
Systems have to out-put a similarity score for each pair.
Systems areevaluated using the Pearson correlation betweengold and predicted rankings.The Sentences Involving CompositionalKnowledge (SICK) test set (Marelli et al, 2014)provides a complementary evaluation.
It consistsof sentence pairs annotated with semantic relat-edness scores.
While STS examples were simplydrawn from existing NLP datasets, SICK exam-ples were constructed to avoid non-compositionalphenomena such as multiword expressions,named entities and world knowledge.3.2 Experimental ConditionsAt training time we learn word embeddings foreach combination of objective (Section 2.2) and1In contrast, Wieting et al (2016) initialize W with high-quality but resource intensive embeddings ?
they are trainedusing word-level PPDB paraphrases, tuned on SimLex-999,and regularized to penalize deviations from initial GloVe em-beddings (Pennington et al, 2014).2MAX (use the unaligned phrase of minimum distance)or MIX (use MAX with probability 0.5 and sample randomlyotherwise)type of training examples (Table 2), using modi-fied implementations of open-source implementa-tions for Jbi(Hermann and Blunsom, 2014) andJpa(Wieting et al, 2016).
This results in sixmodel configurations.
Each was trained for 10epochs using tuned hyperparameters.At tuning time we use the SMT-europarl sub-set of STS-2012.
We consider mini-batch sizesof {25, 50, 100}, ?
?
{1, 10, 100} with Euclideandistance, ?
?
{0.4, 0.6, 0.8} with cosine similarly,and ?
?
{1, 10?3, 10?5, 10?7, 10?9}.
In Jbi, weconsider k ?
{1, 5, 10, 15}, and in Jpawe tunedover the sampling strategy ?
{MIX,MAX} andthe distance function used.
To speed up tuning forJpa, we follow Wieting et al (2016), by limitingtraining to 100k pairs, and tuning to 5 epochs.Tuning results confirmed the importance of neg-ative sampling and distance function in our mod-els: in Jbi, increasing k consistently helps thebilingual models, whereas the correlation score formonolingual models degrade for k > 10.
In Jpa,MAX always outperforms MIX .
Euclidean dis-tance was consistently chosen for bilingual sen-tences and monolingual phrases, while cosine sim-ilarity was chosen for bilingual phrases.At test time we construct sentence-level embed-dings by averaging the representations of words ineach sentence, and compute cosine similarity tocapture the similarity between sentences.4 FindingsTable 3 reports the Pearson correlation scoresachieved for each approach and dataset.Bilingual phrases yield the best models incontrolled settingsOverall, the best representations are obtained us-ing bilingual phrase pairs and the Jbiobjective.They outperform all other compositional modelsfor all tasks, except for one subset of STS-2015.The best objective for a given type of train-ing example varies: Jpagenerally yields better364Table 3: Pearson correlation scores obtained on the English STS sets (with per year averages) and onsemantic-relatedness task (SICK).
The left columns report results based on new representations learnedin this work, while the 2 rightmost columns report reference results from prior work (Wieting et al,2016).Monolingual Phrases Bilingual Phrases Bilingual Sentences Reference ResultsJbiJpaJbiJpaJbiJpaParagram GloVeMSRpar 0.28 0.42 0.54 0.38 0.54 0.36 0.44 0.47MSRvid 0.33 0.55 0.71 0.38 0.71 0.19 0.77 0.64SMT-eur 0.39 0.41 0.49 0.46 0.47 0.47 0.48 0.46SMT-news 0.40 0.50 0.59 0.40 0.58 0.38 0.63 0.50OnWN 0.52 0.57 0.64 0.62 0.46 0.62 0.71 0.552012 Avg 0.39 0.49 0.59 0.45 0.54 0.41 0.61 0.53headline 0.56 0.66 0.70 0.58 0.66 0.61 0.74 0.64OnWN 0.55 0.53 0.75 0.34 0.48 0.25 0.72 0.63FNWN 0.35 0.29 0.41 0.32 0.25 0.16 0.47 0.342013 Avg 0.49 0.49 0.62 0.41 0.46 0.34 0.58 0.42deft forum 0.35 0.47 0.51 0.36 0.36 0.33 0.53 0.27deft news 0.59 0.68 0.77 0.59 0.76 0.58 0.75 0.68headline 0.56 0.63 0.73 0.58 0.67 0.58 0.72 0.60images 0.58 0.73 0.73 0.59 0.66 0.49 0.80 0.61OnWN 0.65 0.62 0.80 0.55 0.55 0.47 0.81 0.58tweet news 0.59 0.66 0.73 0.64 0.56 0.69 0.77 0.512014 Avg 0.55 0.63 0.71 0.55 0.59 0.52 0.73 0.54forums 0.35 0.42 0.55 0.48 0.50 0.45 0.66 0.31students 0.66 0.66 0.73 0.73 0.65 0.69 0.77 0.63headline 0.64 0.60 0.79 0.64 0.73 0.66 0.76 0.62belief 0.46 0.71 0.68 0.67 0.48 0.61 0.77 0.41images 0.52 0.71 0.75 0.62 0.67 0.56 0.82 0.682015 Avg 0.53 0.63 0.70 0.63 0.59 0.60 0.76 0.53SICK 0.53 0.62 0.66 0.57 0.63 0.54 0.72 0.66results with monolingual phrases, while Jbiper-forms better with bilingual examples.
Bilingualphrases seem to benefit from larger number of ran-domly selected negative samples and from usingthe Euclidean distance rather than cosine similar-ity.
The best bilingual compositional representa-tions are better than non-compositional Glove em-beddings (Pennington et al, 2014), but worse thancompositional Paragram embeddings (Wieting etal., 2016).
However, Paragram initialization re-quires large amounts of text and human word simi-larity judgments for tuning, while our models wereinitialized randomly.Table 4: Undertrained word ratios (tokens seenfewer than 100 times during training) are uncor-related with performance in Table 3.DatasetMonolingualPhrasesBilingualPhrasesBilingualSentences2012 Avg 0.15 0.17 0.092013 Avg 0.16 0.17 0.112014 Avg 0.19 0.22 0.112015 Avg 0.15 0.19 0.11SICK 0.2 0.25 0.15Bilingual sentences vs. bilingual phrasesWhy do bilingual phrases outperform the bilingualsentences they are extracted from?
In this section,we verify that this is not explained by systematicbiases in the distribution of training examples.First, Table 4 shows that bilingual sentenceshave the smallest ratios of undertrained words, andare therefore not penalized by rare words morethan bilingual phrases3.Second, we see that the rankings are not bi-ased due to memorization of the phrases seen dur-ing training.
Rankings of models does not changewhen testing on unseen word sequences, as shownby SICK results with models trained using Jbiona filtered training set that contains none of the bi-grams observed at test time (Table 5).Third, the advantage of bilingual phrases overbilingual sentences is not due to the larger numberof training examples.
1.9M (and even 1M ) bilin-3Further, more than 80% of words that appear in bothbilingual sentences and bilingual phrases occur in 460 (inaverage) more bilingual sentences than in bilingual phrases.The remaining 20% were found to be the rare words (e.g.
za-zvorkova, woldesmayat, yellow-bellies) that hardly occur intest sets.365Table 5: Impact of memorization: Pearson corre-lation scores on SICK with training sets with andwithout filtering out training pairs that contain anybigrams that appear in SICK.
Number of trainingpairs (# Pairs) is shown in millions.Not Filtered Filtered# Pairs Score # Pairs ScoreMonoling.
Phrases 3M 0.52 2.3M 0.54Bilingual Phrases 3M 0.67 2.1M 0.65Bilingual Sentences 1.9M 0.66 0.47M 0.58gual phrase pairs still outperform the 1.9M bilin-gual sentence pairs on all subsets (See Table 6).Taken together, these additional results sup-port our initial intuition that the main advantageof bilingual phrases over bilingual sentences isthat phrase pairs have stronger semantic equiva-lence than sentence pairs, since phrase pairs areshorter and are constructed by identifying stronglyaligned subsets of sentence pairs.Monolingual vs. bilingual phrasesBased on the analysis thus far, we hypothesize thatparaphrase pairs with overlapping tokens makethe compositional training objective less useful.Around 40% of the paraphrase training pairs differonly by one token.
With Euclidean distance in thetraining objective, overlapping tokens cancel eachother out of the composition term.
For example,the pair ?healthy and stable, healthy and steady?yields the compositional term||(healthy + and+ stable)?
(healthy + and+ steady)||2= ||stable?
steady||2In contrast, overlap cannot occur in the bilin-gual setting, and all words within bilingual phrasescontribute to the compositional objective.
Fur-thermore, bilingual pairs provide a more explicitsemantic signal as translations can disambiguatepolysemous words (Diab, 2004; Carpuat and Wu,2007) and help discover synonyms by pivoting(Callison-Burch, 2007; Yao et al, 2012).All these factors might contribute to the abilityof training with bilingual phrases of taking advan-tage of larger number of negative samples k.5 ConclusionWe conducted the first evaluation of compositionalrepresentations learned using bilingual supervi-Table 6: Impact of training set size: Average Pear-son correlation per test set with different numbers(in millions) of bilingual phrase pairs, comparedto the full set of bilingual sentences and monolin-gually pretrained GloVe.Bilingual Phrases Sent.0.5M 1M 1.9M 3M 1.9M GloVe2012 0.55 0.58 0.59 0.59 0.54 0.532013 0.59 0.61 0.61 0.62 0.46 0.422014 0.69 0.71 0.71 0.71 0.59 0.542016 0.68 0.69 0.70 0.70 0.61 0.53SICK 0.62 0.64 0.65 0.66 0.63 0.66sion on monolingual textual similarity tasks.Phrase and sentence representations are con-structed by composing word representations usinga simple additive composition function.
We con-sidered two training objective that encourage theresulting representations to distinguish English-Spanish segment pairs that are semantically equiv-alent or not.
The resulting English sentence repre-sentations consistently outperform compositionalmodels trained to detect monolingual paraphraseson five different English semantic textual similar-ity tasks from SemEval.Bilingual phrase pairs are consistently the bestevidence of semantic equivalence in our experi-ments.
They yield better results than the sentencepairs they are extracted from, despite the noise in-troduced by the automatic extraction process.Furthermore the composed representations out-perform non-compositional word representationsderived from monolingual co-occurrence statis-tics.
While sizes of monolingual vs. bilingual cor-pora are not directly comparable, it is remarkablethat representations learned with only 500k bilin-gual phrase pairs outperform GloVe embeddingstrained on 840B tokens.Since our best models still underperform Para-gram vectors, which require a more sophisticatedinitialization process, we will turn to improvingour initialization strategies in future work.
Nev-ertheless, current results provide further evidenceof the usefulness of compositional text represen-tations, even with a simple bag-of-word additivecomposition function, and of bilingual translationpairs as a strong signal of semantic equivalence.366ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics-Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation, pages 385?393.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013. sem 2013 sharedtask: Semantic textual similarity, including a piloton typed-similarity.
In In* SEM 2013: The SecondJoint Conference on Lexical and Computational Se-mantics.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
Semeval-2014 task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th international workshop on semantic evaluation(SemEval 2014), pages 81?91.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, Wei-wei Guo, Inigo Lopez-Gazpioa, Montse Maritxalar,Rada Mihalcea, et al 2015.
Semeval-2015 task2: Semantic textual similarity, english, spanish andpilot on interpretability.
In Proceedings of the 9thinternational workshop on semantic evaluation (Se-mEval 2015), pages 252?263.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the ACL.Chris Callison-Burch.
2007.
Paraphrasing and Trans-lation.
Ph.D. thesis, University of Edinburgh.Marine Carpuat and Dekai Wu.
2007.
ImprovingStatistical Machine Translation using Word SenseDisambiguation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007), pages 61?72, Prague, June.Mona Diab.
2004.
Relieving the data acquisition bot-tleneck in word sense disambiguation.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics.Alona Fyshe, Leila Wehbe, Partha P Talukdar, BrianMurphy, and Tom M Mitchell.
2015.
A composi-tional and interpretable semantic space.
In Proc.
ofNAACL.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In HLT-NAACL, pages 758?764.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2014.
Bilbowa: Fast bilingual distributed repre-sentations without word alignments.
arXiv preprintarXiv:1410.2455.Karl Moritz Hermann and Phil Blunsom.
2014.
Multi-lingual models for compositional distributed seman-tics.
In Association for Computational Linguistics(ACL), 2014.Karl Moritz Hermann, Phil Blunsom, and Stephen Pul-man.
2012.
An unsupervised ranking model fornoun-noun compositionality.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation, pages 132?141.Mohit Iyyer, Jordan L Boyd-Graber, LeonardoMax Batista Claudino, Richard Socher, and HalDaum?e III.
2014.
A neural network for factoidquestion answering over paragraphs.
In EMNLP,pages 633?644.Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,and Hal Daum?e III.
2015.
Deep unordered compo-sition rivals syntactic methods for text classification.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing, volume 1, pages 1681?1691.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th annual meeting of the ACL oninteractive poster and demonstration sessions, pages177?180.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5, pages 79?86.Stanislas Lauly, Hugo Larochelle, Mitesh Khapra,Balaraman Ravindran, Vikas C Raykar, and AmritaSaha.
2014.
An autoencoder approach to learningbilingual word representations.
In Advances in Neu-ral Information Processing Systems, pages 1853?1861.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014.
Semeval-2014 task 1: Evaluation of367compositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
SemEval-2014.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive sci-ence, 34(8):1388?1429.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
In EMNLP, volume 14, pages 1532?1543.Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?a?s Ko?cisk`y, and Phil Blunsom.
2015.Reasoning about entailment with neural attention.arXiv preprint arXiv:1509.06664.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the conference onempirical methods in natural language processing(EMNLP), volume 1631, page 1642.Kai Sheng Tai, Richard Socher, and Christopher DManning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
arXiv preprint arXiv:1503.00075.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2016.
Towards universal paraphrastic sen-tence embeddings.
In In International Conferenceon Learning Representations (ICLR).Xuchen Yao, Benjamin Van Durme, and ChrisCallison-Burch.
2012.
Expectations of word sensein parallel corpora.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 621?625, Montr?eal,Canada, June.
Association for Computational Lin-guistics.Will Y Zou, Richard Socher, Daniel M Cer, andChristopher D Manning.
2013.
Bilingual word em-beddings for phrase-based machine translation.
InEMNLP, pages 1393?1398.368
