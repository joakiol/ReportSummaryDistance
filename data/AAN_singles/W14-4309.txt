Proceedings of the SIGDIAL 2014 Conference, pages 69?73,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsImproving Classification-Based Natural Language Understanding withNon-Expert AnnotationFabrizio Morbini and Eric Forbell and Kenji SagaeInstitute for Creative TechnologiesUniversity of Southern CaliforniaLos Angeles, CA 90094, USA{morbini,forbell,sagae}@ict.usc.eduAbstractAlthough data-driven techniques are com-monly used for Natural Language Under-standing in dialogue systems, their effi-cacy is often hampered by the lack of ap-propriate annotated training data in suffi-cient amounts.
We present an approachfor rapid and cost-effective annotation oftraining data for classification-based lan-guage understanding in conversational di-alogue systems.
Experiments using a web-accessible conversational character that in-teracts with a varied user population showthat a dramatic improvement in naturallanguage understanding and a substantialreduction in expert annotation effort canbe achieved by leveraging non-expert an-notation.1 IntroductionRobust Natural Language Understanding (NLU)remains a challenge in conversational dialoguesystems that allow arbitrary natural language inputfrom users.
Although data-driven approaches arenow commonly used to address the NLU problemas one of classification, e.g.
(Heintze et al., 2010;Leuski and Traum, 2010; Moreira et al., 2011),where input utterances are mapped automaticallyinto system-specific categories, the dependence ofsuch approaches on training data annotated withsemantic classes or dialogue acts creates a chickenand egg problem: user utterances are needed tocreate the annotated training data necessary forNLU by classification, but these cannot be col-lected without a working system that users can in-teract with.Common solutions to this problem include theuse of Wizard-of-Oz data collection, where a hu-man expert manually provides the functionality ofdata-driven modules while data is collected fromusers, or the use of scenario authors who attemptto anticipate user input to create an initial set oftraining data.
While these options offer practicalways around the training data acquisition prob-lem, they typically require substantial work fromsystem experts and provide suboptimal solutions:data-driven approaches work best when utterancesin the training data are drawn from the same distri-bution as those encountered in actual system use,but the conditions under which training data is col-lected (a human expert filling in for systems mod-ules, or a human expert generating possible userutterances) are quite different from those whereusers interact with the final system.
High qual-ity results are often obtained through an iterativeprocess where an initial training set is authoredby a scenario designer, but NLU resources aregradually updated based on real user data overtime (Gandhe et al., 2011).
Although this can ulti-mately produce training data composed primarilyof real user utterances, and therefore result in bet-ter performance from data-driven models, an ex-pert annotator is required to perform manual clas-sification of user utterances.
This is a laboriousprocess that assumes availability and willingnessof the annotator for as long as it takes to collectenough user utterances, which may range fromweeks to months or even years, depending on thesize of the domain and the number and type of ut-terance categories.The main question we address is whether an-notation by non-experts can be leveraged to speedup utterance classification and lower its cost.
Wepresent a technique that frames the annotation oftraining data as a human intelligence task suit-able for crowdsourcing.
Although there are sim-ilarities between our technique and active learning(e.g.
see (Gambck et al., 2011)), an important dif-ference is that our technique does not reduce theannotation effort by reducing the size of the datato be labeled, but by casting the annotation taskinto a simpler problem.
This allows us to take ad-vantage of the entire data generated by the users.Through an experiment with a conversational dia-69logue system deployed on the web, we show that adramatic improvement in the quality of NLU canbe achieved with non-expert data annotation, re-ducing the time required of an expert annotator by70%.2 Improving understanding with dataOur approach for creating accurate utterance clas-sifiers for NLU in conversational dialogue systemsis based on a simple strategy, which we describenext in general terms.
NLU is assumed to be per-formed through multiclass classification.The first step is to create a small initial train-ing dataset T0either through Wizard-of-Oz datacollection or by generation of utterances by a sys-tem developer or content author.
This training setis used to train a NLU model M0.
Although thismodel is likely to be inadequate, it allows usersto interact with an initial version of the system.As input utterances are collected from real users,these utterances are annotated with their desiredNLU output labels.
Periodically, at time i, we addto the initial training dataset T0the annotated userutterances accumulated up to that point.
We traina new NLU model Miusing this augmented train-ing set, Ti.1We also keep aside a small fractionof utterances to test the performance of the NLUmodels, that is, at each time i we also have an eval-uation set Eiand the union of Eiand Tiis the en-tire set of user utterances collected up to time i. Asmore utterances are added and annotated, an NLUmodel Miis expected to surpass the initial modelM0.
In general, we replace the running NLUmodel Mrwhenever we have a better perform-ing Mimodel.
This straightforward process canbe used to obtain increasingly more accurate lan-guage understanding, at the cost of data annotationin the form of labelling utterances with categoriesthat are defined according to the needs of the spe-cific system and the specific domain.
The cate-gories may be based on dialogue acts, e.g.
(Coreand Allen, 1997; Bunt et al., 2010), user informa-tion needs, e.g.
(Moreira et al., 2011), or standin for entire semantic frames, e.g.
(DeVault andTraum, 2013).
The technical nature of the task ofcategorizing utterances in schemes such as theseusually means that substantial time is required ofan expert annotator.2.1 Annotation as a human intelligence taskAlthough the task of annotating NLU training datainvolves assigning categories with technical defi-1For every time i and j with i < j it holds that Ti?
Tj.nitions to utterances, and therefore would appearto require knowledge of these technical defini-tions, in fact the task requires primarily the typeof language understanding that is common to allnative speakers of a given language.
Our main hy-pothesis is that this annotation can be structuredas a trivial task that requires no specific exper-tise, and that annotations performed this way canhave a substantial impact on the quality of utter-ance classification.
We define the NLU annotationtask as follows.Before annotation begins, each utterance cate-gory in the system is associated with one or morecanonical utterance(s) that capture the meaningand communicative intent of that category.
Thesecanonical utterances allow for trivial generationof surface text from NLU categories by simplelookup.Given a set of dialogues between the systemand individual users, each dialogue is split intosegments.
Segments are typically composed ofa user utterance and the preceding system utter-ance.
In cases where users produce multiple utter-ances without intervening system utterances, seg-ments may consist of a system utterance followedby consecutive user utterances, so that there is ex-actly one segment corresponding to each user ut-terance u in the dialogue and each segment startswith a system utterance and ends with u.Annotators work on one dialogue at a time, andwithin a dialogue they are presented with one seg-ment at a time, following the order in which theutterances appear in the dialogue.
Assignmentof a system category to the utterance targeted ina segment is a multiple choice task, involving nchoices, one of which is none of the above.
The re-maining n?1 choices are generated automaticallyby running the current NLU utterance classifier onthe target utterance, and generating the canonicalsurface realizations of each of the n ?
1 highestscoring categories.
The annotator is then asked tochoose the option containing language that meansthe same as the targeted utterance in the segment.The choice of n depends on the quality of the cur-rent NLU model, and must be made to balance thelength of the list of options with the expected num-ber of times the appropriate choice is not in the listof options.
The more none of the above annota-tions are performed, the less effective the annota-tion exercise will be in reducing the effort requiredby an annotation expert.The utterances annotated as none of the aboveare given to an expert for annotation.
The impact70System Hello and welcome....User Are you married?System Yeah, forty-four years this September.
I can?t be-lieve Linda?s stuck with me this long.System Okay, let?s get to it.
Why are you here today?User I?ve been having trouble sleeping lately.System This must be hard for you.System Are you feeling stressed at all?User Yes, quite a bitSystem So, I?ve got some questions I need to ask you, to getto know you better.System Just double checking, you haven?t been sleepinggood.
Is that right?User That?s rightSystem Have you been having any nightmares?User Every once in a while, but mostly I can?t stay asleep...Figure 1: Excerpt of a sample interaction with aSimcoach character over the web.
User input istyped into a text box on a web page.of this procedure on how quickly user utterancesare annotated as training examples for the NLUand how much expert time is saved depends onthe quality of the initial classification model andthe choice of n. These are practical factors that areexpected to vary greatly from one dialogue systemto another.
In the next section, we present one ap-plication of this procedure to an existing conversa-tional dialogue system deployed on the web, andshow examples of dialogue segments and annota-tion options.3 ExperimentTo test our hypothesis that language understand-ing can be improved with much reduced expert ef-fort, we applied the framework described above toa system that implements a conversational char-acter that talks with users about issues relatingto mental and behavioral disorders and presentshealth care options.
The system is publicly ac-cessible at http://www.simcoach.org, and receivestraffic on the order of one hundred users per week.Of these, about one quarter engage the system ina meaningful dialogue with multiple turns, withthe dialogues containing on average 16 user utter-ances.
Because our process depends crucially onuser traffic to generate data for annotation, a web-accessible system is ideally suited for it.
An ex-cerpt from a typical interaction with the system isshown in Figure 1.
The system and the NLU clas-sifier based on Maximum Entropy models (Bergeret al., 1996) are described respectively in (Rizzo etal., 2011) and (Sagae et al., 2009).3.1 Data collectionStarting with an initial system deployed with anNLU model trained with data generated by an au-thor attempting to anticipate user behavior, we ap-plied the approach described in section 2 to im-prove NLU accuracy over a period of approxi-mately five months.
The initial accuracy of theNLU classifier was 62%, measured as the numberof utterances classified correctly divided by the to-tal number of user utterances.
This accuracy fig-ure was obtained only after the five months of dataannotation, using the heldout set of manually an-notated dialogues.Although the data annotation procedure as de-scribed in section 2 could in principle be per-formed continuously as user data come in, weinstead performed all of our annotation in threerounds, the first consisting of approximately 2,000user utterances, the second one month later, con-sisting of an additional 1,000 utterances.
The lastround, collected about two months later, containedabout 2,000 utterances.
We used five annotators2working in parallel, and the average speed of eachannotator exceeded 500 utterances per hour.The total number of NLU utterance classes inthe system is 378, although only 120 classes wereused by annotators in all rounds of annotation tocover all of the utterances collected3.
In our an-notation exercise we set the number of multiplechoice items at n = 6, including 5 choices gener-ated from categories chosen by the NLU classifier,and one none of the above choice.
Figure 2 showsa sample dialogue segment with the correspondingmultiple choice items.
During annotation, clickingon a multiple choice item advances the annotationby presenting the next segment containing a userutterance to be annotated.3.2 ResultsOf the utterances in the three rounds of data col-lection, respectively 29%, 34% and 17% weremarked by annotators as none of the above.
Thesewere given to a developer of the NLU system whoassigned a category to each of them.
In this ex-pert annotation step the choice is not restricted toa small set of options, and may be any of the cat-egories in the system.
Given this rate of use of2The non-expert annotators belonged to the same teamthat developed the system but did not participate in the de-velopment of the NLU module and the NLU classes used inthe particular dialogue system used.3This difference is a further evidence of the difficulty ofcorrectly anticipating how the end users will interact with thedialogue system.71System Okay, let?s get to it.
Why are you here today?User I?ve been having trouble sleeping lately.Which of the following options correspond mostclosely to the last user utterance?
If none of them havethe same general meaning as the user utterance, select?none of the above.?
(a) I have been in a bad mood lately(b) I have nightmares often(c) I haven?t been sleeping well(d) My family is worried about me(e) I eat too much(f) None of the aboveFigure 2: Example of a dialogue segment with cor-responding multiple choice items.
The annotationtask consists of choosing the item that has approx-imately the same meaning and communicative in-tent as the targeted utterance (the user utterance).the none of the above category, the need for ex-pert annotation is not eliminated, but the amountof expert effort necessary is reduced by over 70%.The NLU classification accuracy figures ob-tained after each round of annotation are shown inTable 1.
In the table, Our Approach represents theresults obtained by the technique described here.A large improvement is observed after the firstround of annotation, with a more modest improve-ment observed after the other two rounds.
The ini-tial jump in accuracy after round 1 is explainedby the fact that the initial model based on a sys-tem author?s expectation of what users may say tothe system (approximately 3,000 utterances) is im-proved using utterances that users did in fact pro-duce in real interactions with the system.
Clearly,a more well-matched distribution of utterances inthe training data produces higher accuracy.To assess the value of our approach, we com-pare it with two other reasonable experimentalconditions: a baseline where only expert annota-tion is used (Expert Only), and a condition whereno expert annotation is used (No Expert).
The Ex-pert Only condition is meant to represent what canbe achieved with the same workload for the expertused in Our Approach.
This is achieved by randomselection of user utterances to create a set withthe same number of utterances set aside for ex-pert annotation in Our Approach.
The expert thenannotates each of these utterances to create train-ing data.
For the No Expert condition, we usedonly utterances annotated by non-experts, leavingout completely utterances labeled as none of theNLU accuracy aftereach annotation round [%]Base 1st 2nd 3rdround round roundOur Approach 62 70 73 78Expert Only 62 64 68 70No Expert 62 64 65 71Table 1: NLU accuracy obtained using the initialtraining dataset T0, after one round of annotationwith T1(2,013 utterances), after two rounds of an-notation with T2(additional 948 utterances), andafter three rounds with T3(additional 1806 utter-ances).
Accuracy is estimated on the same heldoutset of dialogues E3for all conditions, accountingfor roughly 10% of the annotated data.above.
Both Expert Only and No Expert condi-tions achieve significantly lower performance thanthe approach described here.
This indicates thatexpert annotation is important, but also that cheapand fast non-expert annotation can provide sub-stantial improvements to NLU.4 ConclusionWe described a framework for annotation of train-ing data by non-experts that can provide dramaticimprovements to natural language understandingin dialogue systems that perform NLU through ut-terance classification.
Our approach transformsthe annotation NLU training data into a task thatcan be performed by anyone with language profi-ciency.
Annotation is structured as a simple mul-tiple choice task, easily delivered over the web.Using our approach with a conversational char-acter on the web, we improved NLU accuracyfrom 62% to 78% using only less than 30% of theeffort it would be required of an expert to annotatedata without non-expert annotation.AcknowledgmentsWe thank Kelly Christoffersen, Nicolai Kalischand Tomer Mor-Barak for data annotation and up-dates to the SimCoach system, David Traum forinsightful discussions, and the anonymous review-ers.
The effort described here has been sponsoredby the U.S. Army.
Any opinions, content or infor-mation presented does not necessarily reflect theposition or the policy of the United States Govern-ment, and no official endorsement should be in-ferred.72ReferencesAdam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Comput.Linguist., 22(1):39?71, March.Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-Woong Choe, Alex Chengyu Fang, Koiti Hasida,Kiyong Lee, Volha Petukhova, Andrei Popescu-Belis, Laurent Romary, Claudia Soria, and DavidTraum.
2010.
Towards an iso standard for dia-logue act annotation.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odijk, Stelios Piperidis, MikeRosner, and Daniel Tapias, editors, Proceedingsof the Seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Val-letta, Malta, may.
European Language ResourcesAssociation (ELRA).Mark G. Core and James F. Allen.
1997.
Coding di-alogues with the DAMSL annotation scheme.
InDavid Traum, editor, Working Notes: AAAI FallSymposium on Communicative Action in Humansand Machines, pages 28?35, Menlo Park, Califor-nia.
AAAI, American Association for Artificial In-telligence.David DeVault and David Traum.
2013.
A methodfor the approximation of incremental understandingof explicit utterance meaning using predictive mod-els in nite domains.
In Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Atlanta, GA, June.Bjrn Gambck, Fredrik Olsson, and Oscar Tckstrm.2011.
Active learning for dialogue act classification.In INTERSPEECH, pages 1329?1332.
ISCA.Sudeep Gandhe, Michael Rushforth, Priti Aggar-wal, and David Traum.
2011.
Evaluation ofan integrated authoring tool for building advancedquestion-answering characters.
In 12th Annual Con-ference of the International Speech CommunicationAssociation (InterSpeech 2011), Florence, Italy, Au-gust.Silvan Heintze, Timo Baumann, and David Schlangen.2010.
Comparing local and sequential modelsfor statistical incremental natural language under-standing.
In Raquel Fern?andez, Yasuhiro Kata-giri, Kazunori Komatani, Oliver Lemon, and MikioNakano, editors, SIGDIAL Conference, pages 9?16.The Association for Computer Linguistics.Anton Leuski and David R. Traum.
2010.
Practicallanguage processing for virtual humans.
In Twenty-Second Annual Conference on Innovative Applica-tions of Artificial Intelligence (IAAI-10).Catarina Moreira, Ana Cristina Mendes, Lu?
?sa Coheur,and Bruno Martins.
2011.
Towards the rapid devel-opment of a natural language understanding mod-ule.
In Proceedings of the 10th International Con-ference on Intelligent Virtual Agents, IVA?11, pages309?315, Berlin, Heidelberg.
Springer-Verlag.Albert A. Rizzo, Belinda Lange, John G. Buckwalter,E.
Forbell, Julia Kim, Kenji Sagae, Josh Williams,Barbara O. Rothbaum, JoAnn Difede, Greg Reger,Thomas Parsons, and Patrick Kenny.
2011.
An in-telligent virtual human system for providing health-care information and support.
In Studies in HealthTechnology and Informatics.Kenji Sagae, Gwen Christian, David DeVault, andDavid R. Traum.
2009.
Towards natural languageunderstanding of partial speech recognition resultsin dialogue systems.
In Short Paper Proceedings ofthe North American Chapter of the Association forComputational Linguistics - Human Language Tech-nologies (NAACL HLT) 2009 conference.73
