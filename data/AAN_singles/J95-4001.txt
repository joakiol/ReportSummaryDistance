The Repair of Speech ActMisunderstandings by AbductiveInferenceSusan W. McRoy*University of Wisconsin-MilwaukeeGraeme Hirst*University of TorontoDuring a conversation, agents can easily come to have different beliefs about the meaning ordiscourse role of some utterance.
Participants normally rely on their expectations todeterminewhether the conversation is proceeding smoothly: if nothing unusual is detected, then under-standing is presumed to occur.
Conversely, when an agent says something that is inconsistentwith another's expectations, then the other agent may change her interpretation ofan earlier turnand direct her response to the reinterpretation, accomplishing what is known as a fourth-turnrepair.Here we describe an abductive account of the interpretation of speech acts and the repair ofspeech act misunderstandings.
Our discussion considers the kinds o fin formation that participantsuse to interpret an utterance, even if it is inconsistent with their beliefs.
It also considers theinformation used to design repairs.
We describe a mapping between the utterance-level forms(semantics) and discourse-level acts (pragmatics), and a relation between the discourse acts andthe beliefs and intentions that they express.
We specify for each discourse act, the acts that mightbe expected, if the hearer has understood the speaker correctly.
We also describe our account ofbelief and intention, distinguishing the beliefs agents actually have from the ones they act as ifthey have when they perform adiscourse act.
To support repair, we model how misunderstandingscan lead to unexpected actions and utterances and describe the processes of interpretation andrepair.
To illustrate the approach, we show how it accounts for an example repair.1.
IntroductionSpeech act misunderstandings occur when two participants differ in their understand-ing of the discourse role of some utterance.
For example, one speaker might take anutterance as an assertion while another understands it to be a request.
Although manyresearchers have considered the problem of avoiding misunderstanding (e.g., by cor-recting misconceptions), previously none has addressed the problem of identifyingand repairing misunderstandings once they occurred.
Here, we will consider a gen-eral model of dialogue that also accounts for the detection and repair of speech actmisunderstandings.1.1 The difference between misunderstanding and misconceptionThe notions of misunderstanding and misconception are easily confounded, so weshall begin by explicating the distinction.
Misconceptions are errors in the prior knowl-edge of a participant; for example, believing that Canada is one of the United States.?
Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edut Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu@ 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 4McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have stud-ied the problem of how one participant can determine the misconceptions of anotherduring a conversation (see Section 5.3 below).
Typically such errors can be recognizedimmediately when an expression is not interpretable with respect o the computer's(presumedly perfect!)
knowledge of the world.By contrast, a participant is not aware, at least initially, when misunderstanding hasoccurred.
In misunderstanding, a participant obtains an interpretation that she believesis complete and correct, but which is, however, not the one that the other participantintended her to obtain.
At the point of misunderstanding, the interpretations of thetwo participants begin to diverge.
It is possible that a misunderstanding will remainunnoticed in a conversation and the participants continue to talk at cross-purposes.Alternatively, the conversation might break down, leading one participant or the otherto decide that a misunderstanding has occurred and (possibly) attempt to resolve it.1.2 The use of repair in the negotiation of meaningAlthough they might not always recognize a misunderstanding when it occurs, dis-course participants are aware that misunderstandings canoccur.
So, participants, ratherthan just passively hoping that they have understood and have been understood, ac-tively listen for trouble and let each other know whether things seem okay.
Eachparticipant will use the subsequent discourse itself in order to judge whether previousdiscourse has been understood correctly.
When one participant produces a responsethat is consistent and coherent with what the other has just said, then the other willtake it as a display of understanding.
Otherwise, it might be taken as evidence ofmisunderstanding.
In either case, the response is used as an indication of how thesecond participant interpreted the first, as presumably his response must have somerational explanation; the indicated interpretation is called the displayed interpretation.When a participant notices a discrepancy between her own interpretation a d the onedisplayed by the other participant, she can choose to initiate a repair or to let it pass.By their choice of repairing or accepting a displayed interpretation, speakers in effectnegotiate the meaning of utterances.
1Repairs can take many forms, depending on how and when a misunderstand-ing becomes apparent.
Conversation analysts classify repairs according to how soonafter the problematic turn a participant initiates a repair (Schegloff 1992).
The mostcommon type occurs within the turn itself or immediately after it, before the otherparticipant has had a chance to reply.
These are called first-turn repairs.
The next mostcommon type, second-turn repairs, occur as the reply to the problematic turn (e.g.,as a request for clarification).
We will not consider these two types of repairs further,because they do not involve misunderstanding perse.
Rather, they are used to correctmisconceptions, misspeakings, nonhearings, etc.Third-turn and fourth-turn 2 repairs address actual misunderstandings.
If a displayof misunderstanding occurs in the turn immediately following the one that was mis-understood, and the speaker notices the problem immediately and acts to resolve it,then we say that they have made a third-turn repair (see Example 1).1 Note that this choice allows for a speaker feigning the occurrence of a misunderstanding  order toachieve some social goal.2 Schegloff (1992) distinguishes nth-turn repair from nth-position repair.
The former corresponds torepairs that begin exactly n - 1 turns after the problematic utterance, while the latter allows anarbitrary number of intervening pairs of turns.
We shall use "nth-turn" to refer to both types, allowingintervening exchanges.436McRoy and Hirst The Repair of Speech Act MisunderstandingsExample 1T1 S: Where do you do this?T2 H: To make the crops grow.T3 S: I said where do you do it.T4 H: In a tin hut in Greeba.If a display of misunderstanding occurs during a subsequent turn by the same speakerwho generated the misunderstood turn, and the hearer then reinterprets he earlierturn and produces a new response to it, then we say that they have made a fourth-turnrepair.
The fragment of conversation shown in Example 2 (Terasaki 1976) includes afourth-turn repair.
Initially, Russ interprets T1 as expressing Mother's desire to tell, thatis, as a pretelling or preannouncement, butfinds this interpretation inconsistent with hernext utterance.
In T3, instead of telling him who's going (as one would expect aftera pretelling), Mother claims that she does not know (and therefore could not tell).Russ recovers by reinterpreting T1 as an indirect request, which his T4 attempts tosatisfy.
Fox (1987) points out that such repairs involve, in effect, a reconstruction f theinitial utterance.
From an AI perspective, these reconstructions resemble the operationof a truth-maintenance system upon an abductive assumption that has proved to beincorrect.
3Example 2T1 Mother: Do you know who's going to that meeting?T2 Russ: Who?T3 Mother: I don't know.T4 Russ: Oh.
Probably Mrs. McOwen and probably Mrs. Cadry and someof the teachers.1.3 The need for both intentional and social informationThe problem of interpreting an utterance involves deciding what actions the speaker isdoing or trying to do.
This process involves not only looking at the surface form of anutterance--for example, was it stated as a declarative?--but also at the context in whichit was uttered.
This context includes the tasks that the participants are involved in,the prior beliefs that they had, and the discourse itself.
Context is important becauseit allows speakers to use the same set of words, for example, "Do you know whattime it is?
", to request he time, to express a complaint, or to ask a yes-no question.Intentional information can rule out some of these readings; for example, a belief thatthe speaker already knows the time might rule out the 'request' interpretation.The difficulty in considering misunderstandings in addition to intended interpre-tations is that it greatly increases the number of alternatives that an interpreter needsto consider, because one cannot simply ignore the interpretations that seem inconsis-tent.
However, predominant computational pproaches to dialogue, which are basedsolely on inference of intention, already have difficulty constraining the interpretationprocess.
Sociological accounts uggest a more constrained approach to interpretation3 This is distinct from the kind of plan repair described by Spencer (1990), which he models using anassumption-based truth-maintenance system.
In his work, "repair" addresses the problem ofincompleteness in a taxonomy of plans, rather than errors in interpretation.437Computational Linguistics Volume 21, Number 4and the recognition of misunderstanding, but none are computational.
Our modelextends the intentional and social accounts of discourse, combining the strengths ofboth.In the intentional accounts, speakers use their beliefs, goals, and expectations todecide what to say; when they interpret an utterance, they identify goals that mightaccount for it.
For example, a speaker who wants someone to know that she lacks apencil might say "I don't have a pencil."
A hearer might then interpret this utteranceas an attempt to convey the information.
However, for any goal that would explain anutterance, the reasons for having that goal would also be potential interpretations ofthe utterance.
Thus, for the above utterance, intentional ccounts might also considerinterpretations corresponding to an attempt to express a need for a pencil, a requestto be given the pencil, an incomplete attempt to fill out a questionnaire, and so on.
4The inherent difficulty with this approach is thus knowing when to stop searching forpotential meanings.According to the ethnomethodological account of human communication k ownas Conversation Analysis (CA), agents design their behavior with the understandingthat they will be held accountable for it.
Agents know that their utterances will be takento display their understanding of some (culturally determined) rules of conversationand the situation prior to the utterance.
Agents, aware of some rule or norm that isrelevant to their current situation, choose to follow (or not follow) the rule, dependingon how they view the consequences of their choice.
One important convention is theadjacency pair.
Adjacency pairs are sequentially constrained pairs of utterances, (suchas question-answer), in which an utterance of the first type creates an expectation forone of the second.
A hearer is not bound to produce the expected reply, but if hedoes not, he must be ready to justify his action and to accept responsibility for anyinferences that the speaker might make (Schegloff and Sacks 1973).
Where the CAapproach is weakest is in its explanation of how the recipient of an utterance is ableto understand an utterance that is the first part of an adjacency pair.
For this, an agentneeds linguistic knowledge linking the features of an utterance to a range of speechacts that can form adjacency pairs.
Agents also need to have some idea of the beliefsand intentions that particular actions express, so they can make judgments about heirappropriateness in the context.1.4 OverviewThe aim of our research is to construct a model of communicative interaction that willbe able to support he negotiation of meaning.
In particular, we want to develop ageneral model of conversation that is flexible enough to handle misunderstandings.To support his degree of flexibility, the agents that we model form expectations onthe basis of what they hear, monitor for differences in understanding, and, whenappropriate, change their own interpretations in response to new information.
Themodel specifies the relationship between this reasoning and discourse participants'beliefs, intentions, and previously expressed attitudes, as well as their knowledge ofsocial conventions.In the account, speakers elect speech acts on the basis of both their goals andtheir knowledge of which speech acts are expected to follow upon a given speechact.
They must select an utterance form that both parties would agree (in the current4 The amount  of reasoning is a function of the size of one's plan hierarchy.
So, if it is believed thatquestionnaires are used to obtain a dr iver 's license, which is needed to drive a car, which is needed toget to California, then this same utterance could even be interpreted as an incomplete attempt o get toCalifornia.
Thus, the hearer must  also assume that he and the speaker share the same plan hierarchy.438McRoy and Hirst The Repair of Speech Act Misunderstandingsdiscourse context) could accomplish the desired goal.
Interpretation and repair attemptto apply this process in reverse, working back from an observed utterance to theunderlying oal.
Such reasoning is clearly nonmonotonic; here we suggest that it can becharacterized quite naturally as abduction.
The model is expressed as a logical theoryin the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; van Arragon1990).2.
The structured intentional approachWe now introduce a model of dialogue that extends both intentional and social ac-counts of discourse.
The model unifies theories of speech act production, interpretation,and the repair of misunderstandings.
This unification is achieved by treating produc-tion as default reasoning, while using abduction to model interpretation and repair.
Inaddition, the model avoids open-ended inference about goals by using expectationsderived from social norms to guide interpretation.
As a result, the model provides aconstrained, yet principled, account of interpretation; it also links social accounts ofexpectation with other mental states.In this section, we will discuss how the model addresses the following concerns:?
The need to control the inference from observed actions to expectedreplies.
Extended inference about goals is usually unnecessary and awaste of resources.?
The need to account for nonmonotonicity in both the interpretation andproduction of utterances.
This nonmonotonicity akes two forms.
First,utterances can make only a part of the speaker's goals explicit to thehearer, so hearers must reason abductively to account for them.
Second,expectations are defeasible.
At any given moment, speakers may differ intheir beliefs about the dialogue and hence can only assume that theyunderstand each other.
Speakers manage the nonmonotonicity bnegotiating with each other to achieve understanding.?
The need to detect and correct misunderstandings.
Speakers rely on theirexpectations to decide whether they have understood each other.
Whenhearers identify an apparent inconsistency, they can reinterpret an earlierutterance and respond to it anew.
However, if they fail to identify amisunderstanding, the communication might mislead them intoprematurely believing that their goals have been achieved.?
The need for an alternative to the notion of mutual belief.
Typically,models rely on mutual beliefs without accounting for how speakersachieve them or for why speakers hould believe that they haveachieved them.2.1 Using social conventions to guide interpretation and repairOur account of interpretation avoids the extended inference required by plan-basedmodels by reversing the standard dependency between an agent's expectations andtask-related goals.
Plan-based approaches (Allen and Perrault 1979; Litman 1986; Car-berry 1990; Lambert and Carberry 1991) start by applying context-independent infer-ence rules to identify the agent's task-related plan, possibly favoring alternatives thatextend a previously recognized plan.
By contrast, our approach begins with an expec-tation, using it to premise both the analysis of utterance meaning and any inference439Computational Linguistics Volume 21, Number 4about an agent's goals.
Moreover, our approach treats apparent conflicts with expec-tations as meaningful; for example, if an utterance is inconsistent with expectations,then the reasoner will try to explain the inconsistency.The model focuses on two convention-based sources of expectation.
The firstis conventions about what attitudes (belief, desire, intention, etc) each speech actexpresses; s we call these the linguistic intentions of the speech act.
The second is con-ventions for each speech act about what act should follow; we call these linguisticexpectations.
Speakers will expect each other to display their understanding of theseconventions and how they apply to their conversation.
Thus, they can expect eachother to be consistent in the attitudes that they express and to respond to each actwith its conventional reply, unless they have (and can provide) a valid reason not to.Linguistic intentions are based on Grice's (1957) notion of reflexive intention.
Forexample, an inform(S,H,P) expresses the linguistic intentions whose content is P andintend(S,know(H,P)) (i.e., the speaker intends the hearer to believe (1) that P is true and(2) that the speaker intends that the hearer know P).
Linguistic expectations capturethe notion of adjacency pairs.
6In defining linguistic intentions, which are shown in Figure 1, we have followedexisting speech act taxonomies, especially those given by Bach and Harnish (1979),Allen (1983), and Hinkelman (1990).
7Thus, when a speaker produces an askref aboutP she expresses (and thereby intends the hearer to recognize that she expresses) thatshe does not know the referent of some description i  P, intends to find out the referentof that description, and intends the hearer to tell her that referent.
If the speaker issincere, she actually believes the content of what she expresses; if the hearer is trusting,he might come to believe that she believes it.Following Schegloff's (1988) analysis of Example 2, we provide a speech act def-inition for pretell.
8 In order to capture the linguistic intentions of pretelling, we alsoadd a new attitude, knowsBetterRef(S, H, P) that is true if the knowledge of S is strictlybetter than the knowledge of P--for example, because S is the expert or S has hadmore recent experience with P.We allow that individuals might not all share the same taxonomy of speech actsand linguistic intentions and that certain social groups or activities might have theirown specialized sets of linguistic expectations.
9 Our theory supports this flexibility byhaving each speaker evaluate the coherence of all utterances within her own view ofthe discourse.
Thus, where we refer to the "displayed interpretation" of an utterance,we mean displayed given the perspective of a particular speaker.
1?5 We assume that these attitudes are a function of discourse or illocutionary level of speech acts, ratherthan the surface or locutionary level.
This approach as worked well for us, but, as one reviewerremarked, it is an interesting issue as to whether they are also a function of the locutionary level.6 Note that although linguistic intentions often express that an action is intended (e.g., questions expressan intention that the hearer answer), the two conventions are independent.
For example, while aninvitation to visit at 6pm might create an expectation that dinner will be served, it does not express anintention to serve it.7 In the figure, we have used the symbol intend to name both the intention to achieve a situation inwhich a property holds and the intention to do action.8 Schegloff actually argues against representing such sequences as speech acts; however, as in thecomputational work cited above, we have used the notion of "discourse-level speech act" to representthe functional relationship between the surface form of an utterance, the context, and the attitudesexpressed by the speaker.9 Reithinger and Maier (1995) have used n-gram dialogue act probabilities to induce the adjacency pairsfrom a corpus of dialogues for appointment scheduling.10 Communication can occur despite such differences because speakers with similar linguistic experiencespresumably will develop similar expectations about how discourse works.
Differences in expectationsmight very well be one thing that new acquaintances must resolve in order to avoid social conflict.440McRoy and Hirst The Repair of Speech Act MisunderstandingsAct type Speech act name Linguistic intentionsinformative assert(S, H, P) know(S, P)assertref(S, H, P) knowref(S, P)assertif(S, H, P) knowif(S, P)inform(S, H, P) know(P)intend(S, know(H, P ) )informref(S, H, P) knowref(S, P)intend(S, knowref(H, P))informif(S, H, P) knowif(S, P)intend(S, knowif(H, P))inquisitive askref(S, H, P) not knowref(S, P)intend(S, knowref( S , P ) )intend(S, do(H, informref(H, S, P)))askif(S, H, P) not knowif(S, P)intend(S, knowif(S, P))intend(S, do(H, informif(H, S, P ) ) )requestive request(S, H, do(H, P) ) intend(S, do(H, P))pretell(S,H, P) knowref(S, P)knowsBetterRef(S, H, P)intend(S, do(S, informref(S, H, P)))intend(S, knowref(H, P))testref(S, H, P) knowref(S, P)intend(S, do(H, assertref(H, S, P)))testif(S, H, P) knowif(S, P)intend(S, do(H, assertif(H, S, P ) ) )Figure 1Linguistic intentions.The figure shows a list of attitudes that each act expresses; the lists are assumedto be exhaustive with respect o the theory (but not to the various connotations thatmight be associated with each act).
The set of acts itself is not necessarily exhaustive,but sufficient o handle the examples that we consider.
While our taxonomy mightseem small, most other acts appear to be specializations of those that we selected.Similarly, the model incorporates only a small number of linguistic expectations; theseare shown in Figure 2.112.2 Characterizing interpretation, production, and repairOur model unifies the fundamental tasks of interpreting speech acts, producing speechacts, and repairing speech act interpretations within a nonmonotonic framework.
Inparticular, speakers' knowledge about language is represented asa set of default rules.The rules describe conventional strategies for producing coherent utterances, therebydisplaying understanding, and strategies for identifying misunderstanding.
As a re-suit, speakers' decisions about what utterances they might coherently generate nextcorrespond to default inference over this theory, while decisions about possible in-11 Quantitative results by Jose (1988) and Nagata nd Morimoto (1993) provide vidence for theseadjacency pairs.
In addition, we have used pairs discovered byConversation A alysis from realdialogues (Schegloff 1988).441Computational Linguistics Volume 21, Number 4First turn Expected replyaskref informrefaskif informifrequest complypretell askreftestref assertreftestif assertifFigure 2Adjacency pairs (Linguistic expectations).Example Metaplan type1 A: Do you have a quarter?2 B: No.3 B: I never lend money.4 A: No, I meant o offer you one.5 B: Oh.
Thanks.6 A: Bye.Plan adoptionAcceptanceChallengeRepairRepairClosingFigure 3Examples of different types of coherence strategies.terpretations of utterances fincluding recognizing misunderstanding) correspond toabductive inference over the theory.Definition 1Given a theory T and a goal proposition G, we say that one can abduce a set ofassumptions A from ~ if T U A ~ G and T U A is consistent.Abduction has been applied to the solution of local pragmatics problems (Hobbs et al1988, 1993) and to story understanding (Charniak and Goldman 1988).The model incorporates five strategies, or metaplans, for generating coherent utter-ances: plan adoption, acceptance, challenge, repair, and closing (the model treats openingas a kind of plan adoption).
Figure 3 contains a conversation that includes an examplefor each of the five types.
In plan adoption, speakers simply choose an action that can beexpected to achieve a desired illocutionary goal, given social norms and the discoursecontext.
(The goal itself must originate within the speaker's non-linguistic planningmechanism.)
The first utterance in the figure is a plan adoption.
The second utterancein the figure, if it occurs immediately after an utterance such as the first one, wouldbe an acceptance.
With acceptance of an utterance, agents perform actions that havebeen elicited by a discourse partner.
That is, the hearer displays his understanding andacceptance of the appropriateness of a speaker's utterance (independent of whether heactually agrees with it).
Challenges display understanding of an utterance, while deny-ing its appropriateness.
For example, an agent might challenge the presuppositions ofa previous action.
The third utterance, if it occurs immediately after an utterance suchas the first one, would be a challenge.
Repairs display non-acceptance of a previouslydisplayed interpretation (see Section 1.2).
The fourth utterance, occurring after an ex-change such as/1, 3/, would be a third-turn repair by A; the fifth utterance, occurring442McRoy and Hirst The Repair of Speech Act Misunderstandingsafter (1, 3, 4), would be a fourth-turn repair by BJ 2 Closings signal that the participantsare ready to terminate the conversation (and that they accept he conversation as awhole).
The last utterance in the figure is a closing.Misunderstandings are classified according to which participant recognizes thatthe misunderstanding has occurred and whom she thinks has misunderstood.
Self-misunderstandings are those in which a hearer finds that a speaker's current utteranceis inconsistent with something that that speaker said earlier and decides that his owninterpretation of the earlier utterance must be incorrect.
Conversely, other-misunder-standings are those in which the hearer attributes a misunderstanding to the speaker.Fourth-turn repairs may occur after a self-misunderstanding is recognized; third-turnrepairs may occur after other-misunderstanding.The model addresses both classes of misunderstanding (see Section 3.3.3), but islimited to misunderstandings that appear as misrecognized speech acts) 3 Such misun-derstandings are especially important to detect, because the discourse role attributedto an utterance creates expectations that influence the interpretation of subsequentones.
These misunderstandings arealso difficult to prevent, because they can resultfrom many common sources, including intra-sentential ambiguity and mishearing.2.3 Building a model of the interpreted discourseFor a hearer to interpret an utterance as a particular metaplan or as a manifestationof misunderstanding, he needs a model of his understanding of.the prior discourse.The typical way to model interpretations has been to represent the discourse as apartially completed plan corresponding to the actual beliefs (perhaps even mutualbeliefs) of the participants (cf.
Carberry 1990).
This representation incorporates twoassumptions that must be relaxed in any model that accounts for the negotiationof meaning: first, that hearers are always credulous about what the speaker says,and second, that neither participant makes mistakes.
To relax these assumptions, thehearer's model distinguishes the beliefs that speakers claim or act as if they haveduring the dialogue from those that the hearer actually believes they have.
TM Themodel also represents he alternative interpretations that the hearer has considered asa result of repair.
15 We will now consider an axiomatization f the model.3.
The architecture of the modelOur model characterizes a participant in a dialogue, alternately acting as speaker andhearer.
In this section, we will give both the knowledge structures that enable theparticipant's behavior and the reasoning algorithms that produce it.
(Section 4 andAppendix A present machine-to-machine dialogues involving two instantiations ofthe implemented model.
)3.1 The reasoning framework: Prioritized TheoristThe model has been formulated using the Prioritized Theorist framework (Poole,Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supportsboth default and abductive reasoning.
Theorist ypifies what is known as a "proof-12 Non-understanding, which entails non-acceptance (ordeferred acceptance), is signaled by second-turnrepair.
This type of repair will not be considered here.13 Other misunderstandings arepossible; for example there can be disagreement about what object aspeaker is trying to identify with a referring expression (cf.
Heeman and Hirst 1995; Hirst et al 1994).14 This distinction is similar to the one made by Luperfoy (1992).15 For present purposes, we also assume that the complete model is accessible to the hearer; one couldbetter simulate the limitations of working memory by limiting access to only the most recent utterances.443Computational Linguistics Volume 21, Number 4based approach" to abduction because it relies on a theorem prover to collect theassumptions that would be needed to prove a given set of observations and to verifytheir consistency.
Our reasoning algorithm is based on Poole's implementat ion of The-orist, which we extended to incorporate preferences among defaults as suggested byvan Arragon (1990).
16 A Prioritized Theorist reasoner can assume any default d thatthe programmer  has designated as a potential hypothesis, unless it can prove -~d fromsome overriding fact or hypothesis.
This makes the reasoning nonmonotonic,  becausethe addition of a new fact or overriding default may make less preferable hypothesesunderivable.The syntax of Theorist is an extension of the predicate calculus.
It distinguishestwo types of formulae, facts and defaults.
In Poole's implementation, facts are givenby "FACT W.", where w is a wff.
A default can be given either by "DEFAULT (p, d)."
or"DEFAULT (p, d) : w.", where p is a priority value, d is an atomic formula with onlyfree variables as arguments, and w is a wff.
For example, we can express the defaultthat birds normal ly fly, as:DEFAULT (2, birdsFly(b) : bird(b) D fly(b).If 9 t" is the set of facts and AP is the set of defaults with priority p, then an expres-sion DEFAULT(p,d) : W asserts that d E AP and (d D w) E 5 r. The language lacksexplicit quantification; as in Prolog, variable names are understood to be universallyquantified.Facts are taken as true in the domain, whereas defaults correspond to the hy-potheses of the domain (i.e., formulae that can be assumed true when the facts aloneare insufficient o explain some observation).
A priority value is an integer associatedwith a given default (and all ground instances of it), where a default with priority i isstronger than one with priority j, if i < j.
When two defaults conflict, the stronger one(i.e., the one having the lower priority value) takes precedence.
For sets of defaults A iand AJ such that i < j, no d E AJ can be used in an explanation if --d E Ai and -~d isconsistent with defaults usable from any A h, h < i.In the Theorist framework,  explanation is a process akin to scientific theory forma-t ion - i f  a closed formula representing an observation is a logical consequence of thefacts and a consistent set of default assumptions, then it can be explained:Defini t ion 2An explanation from the set of facts 9 v and the sets of prioritized defaults A 1 .
.
.
.
.
A nof a closed formula g is a set Y U D 1 U .. .
U D n, where each D i is a set of groundinstances of elements of A i, such that:1.
)r U D 1 U .
.
.
U D n is consistent2.
,T U D 1 U .
- -U  D n ~g3.
For all D i such that 2 < i < n, there is no ,T U D t 1 U .. .
U D ~ i-1 thatsatisfies the priority constraints and is inconsistent with D i.16 Poole's Theorist implements a full first-order clausal theorem prover in Prolog.
Like Prolog, it applies aresolution-based procedure, reducing oals to their subgoals using rules of the formgoa l  *--- subgoa l l  A ?
?
?
A subgoa ln .
However, unlike Prolog, it incorporates a model-elimination strategy(Loveland 1978; Stickel 1989; Umrigar and Pitchumani 1985) to reason by cases.444McRoy and  Hirst  The Repair  of Speech Act M isunders tand ingsPriority constraints require that no ground instance of d E Ai can be in D i if its negationis explainable with defaults usable from any A J, j < i.Priorities enable one to specify that one default is stronger than another, perhapsbecause it represents an exception.
In our model, defaults will have one of threepriority values: strong, weak, or very weak.
The strongest value is reserved for attitudesabout the prior context, whereas assumptions about expectations are given as weakdefaults and assumptions about unexpected actions or interpretations are given asvery weak defaults.
This allows us to specify a preference for expected analyses whenthere is an ambiguity.3.2 The language of our modelThe model is based on a sorted first-order language, where every term is either anagent, a turn, a sequence of turns, an action, a description, or a supposition.
Thelanguage includes an infinite number of variables and function symbols of every sortand arity.
We also define several special ones to characterize suppositions, actions, andsequences of turns.3.2.1 Suppositions.
Suppositions are terms that name propositions that agents believeor express.
Suppositions can be thought of as quoted propositions, but with a limitedsyntax and semantics.
We define the following functional expressions:?
do(s,a) expresses that agent s has performed the action a;?
mistake(s, al,a2) expresses that agent s has mistaken an act al for act a2;?
and(pl,p2) expresses the conjunction of suppositions Pl and P2, where Plmust be simple (i.e., not formed from others using the function symboland);?
not p expresses the negation of a simple supposition p.17We also define several suppositions for expressions of knowledge and intention.Two suppositions are equivalent if and only if they are syntactically identical.
Tocapture the notion that speakers are normally consistent in the suppositions that theychoose to express, we need to know how different suppositions relate to each other.More to the point, we need to know when the expressing of two simple suppositionsis or is not consistent.
A complete account must take into consideration possible en-tailments among expressed propositions; however, no such account yet exists.
As aplaceholder for such a theory, there is a compatibility relation for expressed supposi-tions.
Our approach is to make compatibility a default and define axioms to excludeclearly incompatible cases, such as these:?
The suppositions Q and not Q.?
The supposition of an intention to make Q true when Q is already truein the agent's interpretation f the discourse.17 The function not is distinct from the boolean connective 7.
We use it to capture the suppositionexpressed by an agent who says something negative, e.g., "I do not want to go," which might berepresented as inform(s, h, not wantToGo}.445Computational Linguistics Volume 21, Number 4The supposition of the performance of some act that expresses, via alinguistic intention, any supposition that would be incompatible with(another supposition of) the agent's interpretation of the discourse.The supposition of an intention to perform some act expressing anysupposition that is incompatible with the agent's interpretation of thediscourse.The supposition of an intention to knowif Q if either Q or not Q isalready true in the agent's interpretation of the discourse.When suppositions are not simple, we check their compatibility by verifying that eachof the conjuncts of each supposition is compatible.
(In the system, this is implementedas a special predicate, inconsistentLI).There is a danger in treating compatibility as a default in that one might misssome intuitively incompatible cases and hence some misunderstandings might not bedetectable.
An alternative would be to base compatibility on the notion of consistencyin the underlying logic, if a complete logic has been defined.
TM3.2.2 Speech acts.
For simplicity, we represent utterances as surface-level speech actsin the manner first used by Perrault and Allen (1980).
19 Following Cohen and Levesque(1985), we limit the surface language to the acts surface-request, surface-inform,surface-informref, and surface-informif.
Example 3 shows the representation f theliteral form of Example 2, the fourth-turn repair example.
(We abbreviate "m" for"Mother", "r" for "Russ", and "whoIsGoing" for "who's going".
)Example 3T1 m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing)))T2 r: surface-request(r, m, informref(m, r, whoIsGoing))T3 m: surface-inform(m, r not knowref(m, whoIsGoing))T4 r: surface-informref(r, m, whoIsGoing)We assume that such forms can be identified by the parser, for example treating alldeclarative sentences as surface-informs.
2?18 Note that human behavior lies somewhere in between these two extremes; in particular, people do notseem to express all the entailments of what they utter (Walker 1991).19 Other representation la guages, uch as one based on case semantics, would also be compatible withthe approach and would permit greater flexibility.
The cost of the increased flexibility would beincreased ifficulty in mapping surface descriptions onto speech acts; however, because less effortwould be required in sentence processing, the total complexity of the problem need not increase.
Usinga more finely-grained representation, e could reason about sentence type, particles, and prosodyexplicitly, instead of requiring the sentence processor to interpret this information (cf.
Hinkelman 1990;Beun 1990).20 We also presume that a parser can recognize surface-informref and surface-informif syntacticallywhen the input is a sentence fragment, but it would not hurt our analysis to input them all assurface-inform.446McRoy and Hirst The Repair of Speech Act MisunderstandingsThe theory includes the discourse-level acts inform, informif, informref, assert,assertif, assertref, askref, askif, request, preteU, testref, and warn, which we representusing a similar notation.
2~,223.2.3 Turn sequences.
A turn sequence represents the interpretations of the discoursethat a participant has considered up to a particular time.
It is structured as a tree, whereeach level below the root corresponds to a single turn in the sequence, ordered as theyoccurred in time.
Each path from the root to a leaf represents a single interpretationof the dialogue.
Nodes that are siblings (i.e., that have the same parent) correspond todifferent interpretations of the same turn.
Nodes at the same level, but having differentparents, represent repairs.
The currently active interpretation is defined by its mostrecent urn, which we shall call the focus of the sequence.The purpose of this tree structure is to capture the sequential structure of thedialogue and, for each state of the dialogue, what attitudes the participants are ac-countable for having expressed.
23Branches in the sequential structure nable the par-ticipants to retract attitudes via repair and to reason about the alternatives that theyhave achieved.We will call the turn sequence whose focus is the current turn the "discoursecontext".
In order to consider previous tates of the context, such as before a possiblemisunderstanding occurred, we define a successor relation on turn sequences:Definition 3A turn sequence TS2 is a successor to turn sequence TS1 if TS2 is identical to TS1except hat TS2 has an additional turn t that is not a turn of TS1 and t is the successorto the focused turn of TS1.3.3 The characterization of a discourse participantWe will now consider the knowledge structures that enable a participant's behaviorand the reasoning algorithms that produce it.
We divide our specification of a partic-ipant into three subtheories:A set/3 of prior assumptions about the beliefs and goals expressed bythe speakers (including assumptions about misunderstanding).A set A/I of potential assumptions about misunderstandings andmetaplanning decisions.A theory T describing his or her linguistic knowledge, includingprinciples of interaction and facts relating linguistic acts.Given these three subtheories, an interpretation of an utterance is a set of groundinstances of assumptions that explain the utterance.
An utterance would be a coherent21 In the utterance language, a yes-no question is taken to be a surface-request to informif and awh-question is taken to be a surface-request to informref.
We then translate these request forms intothe discourse-level actions askif and askref.
An alternative would be to identify them as surface-askifor surface-askref during sentence processing, asHinkelman (1990) does.22 Speech act names that end with the suffix -ref take a description as an argument; speech act names thatend with -if take a supposition.
The act inform(s,p) asserts that he proposition is true.
The actinformif(s, p) asserts the truth value of the proposition named by p (i.e., informif is equivalent to"inform V inform-not").23 Tree structures are often used to represent discourse, but usually the hierarchical structure of thediscourse, rather than its temporal structure (see Lambert and Carberry 1991, 1992).447Computational Linguistics Volume 21, Number 4reply to an immediately preceding utterance if it would logically follow, given theselection of some metaplan:Defini t ion 4An interpretation of an utterance u to hearer h by speaker s in discourse context s isa set M of instances of elements of A4, such that.2.3.T U 13 U M is consistentT U 13 U M ~ utter(s, h, u, ts)T U 13 U M satisfies the priority constraints; that is, T U 13 U M is not inconflict with any stronger defaults that might apply.Defini t ion 5It would be coherent for s to utter u in discourse context s if the utterance can be de-rived from an agent's linguistic knowledge, assuming some set M meta of metaplanningdecisions, such that.2.3.
'-d-" U \]3 S M meta is cons is tentT Y 13 U M meta ~ utter(s, h, u, ts)"~ U \]3 U M meta satisfies the priority const ra in ts .That is, u is a solution to the following default reasoning problem:T U 13 U M meta ~- (3u) utter(s, h, u, ts)In the language of the model, the predicate shouldTry is used for discourse ac-tions that are coherent (M meta) and the predicate try is for actions that are explainable(M).
If shouldTry(S1,S2,A,TS) is true, it means that, given discourse context TS (whichcorresponds to a particular agent's perspective), it would be appropriate for speaker$1 to address speaker $2 with discourse-level speech act A (i.e., according to socialconventions, here represented by the linguistic expectations and the meta-plans, S1should do A next).By contrast, try(S1,S2,A,T2) would mean.
that, given a discourse context TS, $1 hasperformed the discourse-level act A. Discourse-level acts are related to surface-levelacts by the following default:DEFAULT (3, pickForm (sl, s2, asurfaceForm, a, ts)) :24decomp( asurfaceForm, a)A try(s1, s2, a, ts)D utter(s1, s2, asurfaceForm, ts).This says that the fact that the surface form asurfaceForm can be used to perform discourseact a in some context and the apparent occurrence of a would be a reason for agent slto utter asurfaceForm?24 The model does not discriminate between equally acceptable alternatives.
The default pickForm allowsus to account for the fact that the same surface form can perform several discourse acts and the samediscourse act might be accomplished by one of several different surface forms.
In our system, thisdefault is also used as an oracle, allowing us to see how different interpretations affect he participants'understanding of subsequent turns.
Because the default has a very weak priority, it can be overriddenby user input, without influencing other defaults.448McRoy and Hirst The Repair of Speech Act Misunderstandingsadopt planacceptanceintentional acts challengerepa=rclosingtry other-misunderstandingself-misunderstandingFigure 4The relationship between try and shouldTry and their possible xplanations.The predicates shouldTry and try are related because the appropriateness of a po-tential interpretation is taken as (default) evidence that it is, in fact, the correct inter-pretation:DEFAULT (1, intentionalAct(sl,s2,a, ts)):shouldTry( s l, s2, a, ts )D try(s1, $2, a, ts).The key difference is that try allows that the best interpretation might be contextuallyinappropriate (see Figure 4).Interpretation corresponds to the following problem in Theorist:EXPLAIN utter(sl, s2, u, ts).Generation corresponds to the following problem in Theorist:EXPLAIN shouldTry(sl, s2, aa, ts) A decomp(as, aa).In addition, acts of interpretation a d generation update the set of beliefs and goalsassumed to be expressed uring the discourse.
253.3.1 The discourse context.
The first component ofthe model, B, represents he beliefsand goals that the participants have expressed uring their conversation.
We assumethat an agent will maintain a record of these xpressed attitudes, represented asa turnsequence.
To keep track of the current interpretation f the dialogue, we introduce thenotion of activation of a supposition with respect o a turn sequence.
If during a turnT, a supposition is expressed by an agent hrough the utterance of some speech act orthe display of misunderstanding, then we say it becomes active in the turn sequencethat has T as its focus (see Section 3.2.3).
Moreover, once active, a supposition willremain active in all succeeding turn sequences, unless it is explicitly refuted.Individual ttrrns are represented by a set of facts of the form expressed(P,T) andexpressedNot(P,T), where P is an unnegated supposition that has not been formed fromany simpler suppositions using the function and.
2625 A related concern is how an agent's beliefs might change after an utterance has been understood as anact of a particular type.
A l though we have nothing new to add here, Perrault (1990) shows how defaultlogic might be used to address this problem.26 The intended meaning of expressedNot(P, T) is that dur ing turn T speakers have acted as if the449Computational Linguistics~ ~  discoun fcexprFigure 5How the knowledge relations fit together.~ levelfexp,,ssedefsVolume 21, Number 4ctations3.3.2 Possible hypotheses.
The second component of the model is .M, the set of poten-tial assumptions about misunderstandings and metaplanning decisions.
This is givenby the following set of Theorist defaults: 27intentionalAct, expectedReply, acceptance, adoptPlan, challenge, makeFourth TurnRepair, make-ThirdTurnRepair, econstruction, otherMisunderstanding, selfMisunderstanding, and done.The theorem prover may assume ground instances of any of these predicates if they areconsistent with all facts and with any defaults having higher priority.
As mentionedin Section 3.1, each of these defaults will have one of threepriority values: strong,weak, or very weak.
The strongest level is reserved for attitudes about beliefs and sup-positions.
Assumptions about expectations (i.e., expectedReply, acceptance, makeThird-TurnRepair, and makeFourthTurnRepair) e given as weak defaults.
Assumptions aboutunexpected actions or interpretations (i.e., adoptPlan, challenge, done, selfMisunderstand-ing, and otherMisunderstanding) are given as very weak defaults, so that axioms can bewritten to express a preference for expected analyses when there is an ambiguity.
Wewill consider each of these predicates in greater detail in the next section, when wediscuss the third component of the model.3.3.3 A speaker's theory of language.
The third component of the model is T, aspeaker's theory of communicative interaction.
This theory includes strategies for ex-pressing beliefs and intentions, for displaying understanding, and for identifying whenunderstanding has broken down.
The strategies for displaying understanding suggestperforming speech acts that have an identifiable, but defeasible, relationship to otherspeech acts in the discourse (or to the situation).
Misunderstandings are recognizedwhen an utterance is inconsistent or incoherent; strategies for repair suggest reanalyz-ing previous utterances or making the problem itself public.Relations on linguistic knowledge.
There are three important linguistic knowledge rela-tions: decomp, lintention, and lexpectation.
They are shown as circles in Figure 5; theboxes in the figure are the objects that they relate.supposition P were false.
Although expressed(not(P), T) and expressedNot(P, T) represent the same stateof affairs, the latter expression avoids infinite recursion by Theorist.27 The theory also contains defaults to capture the persistence of activation (persists), and the willingnessof participants to assume that others have a particular belief or goal (credulousB and credulousI,respectively).450McRoy and Hirst The Repair of Speech Act MisunderstandingsThe decomp relation links surface-level forms to the discourse-level forms that theymight accomplish in different contexts.
It corresponds to the body relation in STRIPS-based approaches.
2s Two speech acts are ambiguous whenever they can be performedwith the same surface-level form.
Lintentions relate discourse acts to the linguisticintentions that they conventionally express (see Section 2.1).
The lexpectation relationcaptures the notion of linguistic expectation discussed in Section 2.1, relating each actto the acts that might be expected to follow.
Where there is more than one expectedact, a condition is used to distinguish them.
For example, the axioms representing thelinguistic expectations of askref are shown below.
29FACT lexpectation(do(sl, askref(sl, $2, d)),knowref(s2, d),do(s2, informref(s2, Sl, d))).
"A speaker Sl can expect hat making an askref of d to s2will result in s2 telling Sl the referent of d, if s2 knows it.
"FACT lexpectation(dO(Sl, askref(sl, S2, d)),not knowref(s2, d),do(s2, inform(s2, Sl, not knowref(s2, d)))).
"A speaker Sl can expect hat making an askref of d to s2will result in s2 telling sl that s2 does not know the referent ofd, if s2 does not know it.
"Beliefs and goals.
In the model, participants' actual beliefs and goals are distinguishedfrom those that they express through their utterances.
For the examples consideredhere, any model of belief would suffice; for simplicity we chose to include beliefs andgoals explicitly in the initial background theory and allow agents to make assumptionsabout each other's beliefs and goals by default.
B?Expectation.
In addition to the notion of linguistic expectations, which exist in anysituation, the model incorporates a cognitive, "belief-about-the-future" notion of ex-pectation.
These expectations depend on a speaker's knowledge of social norms, herunderstanding of the discourse so far, and her beliefs about the world at a particulartime.
They are captured by the following Theorist rules:DEFAULT (2, expectedReply(pao, Pcondition, dO(Sl, areplv), ts) ) :active(pdo, ts )/~ lexpectation (P ao, Pcondition, do(s1, areply ) )/k believe( sl, Pcondition )9 expected(s1, arepl~, ts).FACT -qintentionsOk(a, ts )D -~expectedReply(pao, Pc ndition, do(s ,  a), ts).28 Pollack (1986a) calls this the "is-a-way-to" relation.29 It is actually controversial whether an askref followed by an inform-not-knowref  is a valid adjacencypair.
If such questions are taken to presuppose that the hearer knows the answer, a response to thecontrary could also be considered a challenge of this presupposit ion (Tsui 1991).30 It would have been possible to characterize actual belief us ing an appropriate set of axioms, such asthose defining a weak $4 modal  logic.
However, current formalizations do not seem to account for thecontext-sensitivity of speakers' beliefs.
See McRoy (1993b) for a discussion.451Computational Linguistics Volume 21, Number 4The second rule says that one would not expect he action areply if the linguistic in-tentions associated with it are incompatible with the context s. 31 Normally, as thediscourse progresses, expectations for action that held in previous tates of the con-text eventually cease to hold in the current context, because after the action occurs, itwould be incompatible for an agent o say that he intends to achieve something that isalready true.
The compatibility between each of the linguistic intentions of a proposedaction and each of the active suppositions in a context is captured by the predicatelintentionsOk, which is true if and only if none of the incompatibilities described inSection 3.2.1 hold.For convenience, we also define a subjunctive form of expectation toreason aboutexpectations that would arise as a result of future actions (e.g., plan adoption) orthat must be considered when evaluating a potential repair.
This type of expectationdiffers from the type defined above in that it depends on the real beliefs of the agentperforming the first (rather than the second) part of an adjacency pair and it does notdepend on the activity of any suppositions or actions.FACT lexpectation (do(s1,  al ), p, do(s2,  a2))A believe(s1, p)=- wouldExpect(st, al, a2).Metaplans and misunderstandings.
Metaplans encode strategies for selecting an appro-priate act.
The antecedents of these axioms refer to expectations.
In addition, in orderto preserve discourse coherence, they require either that the linguistic intentions ofsuggested actions be compatible with the context or that there be some overt acknowl-edgement of the discrepancy.
(The theory presented here addresses only the formercase; the latter one might be handled by adding an extra default with a strongerpriority level.)
Tables 1-6 give each of these axioms in detail.Along with these metaplans, a speaker's linguistic theory includes two diagnos-tic axioms that characterize speech act misunderstandings: self-misunderstanding a dother-misunderstanding.
The antecedents of these axioms refer to ambiguities andinconsistencies with expressed linguistic intentions, as well as expectations.
For exam-ple, Table 5 describes how an observed inconsistency of Sl performing anew might bea symptom of s2's misinterpretation of an earlier act by Sl.
Such mistakes are possiblewhen the surface form of the earlier act might be used to accomplish either aobserved ora intende d.32The defaults that characterize misunderstandings have a lower priority than themetaplans, because speakers consider misunderstandings only when no coherent inter-pretation ispossible.
The preference for coherent interpretations is especially importantwhen there is more than one discourse-level act for which the utterance is a possibledecomposition.31 Although, like expectedReply, active is a default, active will take precedence over expectedReply, because ithas been given a higher priority on the assumption that memory for suppositions i stronger thanexpectation.32 It is possible that the same surface form might accomplish several different discourse acts, in whichcase it might be desirable to evaluate the likelihood of alternative choices.
The work discussed byReithinger and Maier (1995), for example, found statistical regularities in the misinterpretations thatoccurred in their corpus of appointment-scheduling dialogues.452McRoy and Hirst The Repair of Speech Act MisunderstandingsTable 1Name Plan adoptionPurpose Introducing a new goalAxiom DEFAULT (3,adoptPlan(sl,s2,al,a2, ts) ) :hasGoal (Sl, do(s2, a2), ts )A wouldExpect(sl, do(s1, al ), do(s2, a2))D shouldTry(sl, s2, al, ts).FACT ~l intentionsOk ( al, ts )D -~adoptPlan (81, S2, al, a2, ts).Summary Speaker sl should do action al in discourse ts when:1. sl wants speaker s2 to do action a2;2. sl would expect a2 to follow an action al; and3.
sl may adopt the plan of performing al to trigger a2 (i.e., thelinguistic intentions of al are compatible with ts).Table 2Name AcceptancePurpose Producing an expected replyAxiom DEFAULT (2, acceptance(s1, areply, ts) ) :expected(s1, areply , ts)D shouldTry(sl, s2, areply, ts).FACT active(do(s1, a), ts)D -~acceptance(sl, a ts).Summary Speaker Sl should do action areply in discourse ts when:1. sl expects areply to Occur next; and2.
Sl may accept he interpretation corresponding to ts.4.
A detailed exampleTo show how our abductive account of repair works, we offer two examples that showrepair of self-misunderstanding and other-misunderstanding, respectively.
Here wewill discuss Example 2 from Russ's perspective, considering in detail Russ's reasoningabout each turn and showing an output trace from our implemented system.
FromRuss's perspective, this example demonstrates the detection of a self-misunderstandingand the production of a fourth-turn repair.
In Appendix A we show the system's outputfor a third-turn repair, interleaving the perspectives of its two participants.453Computational Linguistics Volume 21, Number 4Table 3Name Fourth-turn repairPurpose Recovering from one's own misunderstandingAxiom DEFAULT (2, makeFourth Turn Repair( sl, $2, areply, ts, ts ...... tructed ) ) :active(mistake(s1, aintended, aobserved ), ts )A reconstruction (ts, ts ...... tructed)A expected(s1, a~epl~, ts.
.
.
.
.
.
tructed)D shouldTry(sl, s2, areply, ts).FACT active(do(s1, a), ts)D ~makeFourthTurnRepair(sl, $2  a, ts, tSreconstructed).Summary Speaker Sl should do action areply in discourse ts when:1. sl has mistaken an instance of act aintended as an instance of actaobserved ;2.
A reconstruction of the discourse is possible;3. sl would expect o do ar~ply in this reconstruction; and4.
s may perform a fourth-turn repair.Table 4Name Third-turn repairPurpose Recovering from another speaker's misunderstandingAxiomSummaryDEFAULT (2, makeThirdTurnRepair(sl, 2, arepty, ts) ) :active(mistake(s2, aintended, aobserved ), ts )A a = inform(s1, s2, do(s1, aintended))A wouldExpect(sl, do(s1, aintencled), o(s2, areply))D shouldTry(sl, s2, a, ts).FACT ~lintentionsOk(arepty), tsD ~makeThirdTurnRepair(sl, s2 arept,./, ts)Speaker Sl should initiate a repair in discourse ts (that speaker s2 willlater complete) by Sl telling s2 that she had performed the action aintendedif:1.
S2 has apparently mistaken an instance of act aintended for act aobserved;2.
Sl would expect arepty to follow aintended; and3.
sl may perform a third-turn repair (i.e., it would be reasonable andcompatible for s2 to perform areply).4.1 Overv iewWe now repeat  Example  2:T1 Mother :  Do you  know who 's  go ing  to that meet ing?T2 Russ: Who?454McRoy and Hirst The Repair of Speech Act MisunderstandingsTable 5Name Self-misunderstandingPurpose Detecting one's own misunderstandingAxiom DEFAULTFACTFACT(3, selfMisunderstanding( sl, s2, pmistake, anew, ts ) ) :active(do(s1, aob .
.
.
.
d), ts )A lintention(a,~w, pt)A lintention (aobserwe, pl 2 )A inconsistentLI(pt, pt2)A ambiguous(aobs~ea, ai, te,a~d)/~ pmistake = mistake(s2, aintended, aobserved )D try(s1, s2, a .. .
.
ts).-~( selfMisunderstanding( sl, 82, pmistake, al, ts )A shouldTry(sl, s2, al, ts)).~( selfMisunderstanding( sl, $2, pmistake, al, ts )A ambiguous(a1, a2)A shouldTry(sl, s2, a2, ts)).Summary Speaker sl might be attempting action a,~w in discourse ts if:1. sl has performed action aobserve~;2.
But, the linguistic intentions of a,ew are inconsistent with thelinguistic intentions of aobservee;3. aobserved and action aintendee can be performed using a similarsurface-level speech act; and4.
s2 may have mistaken aintended for aobserved.T3 Mother:T4 Russ:I don't  know.Oh.
Probably Mrs. McOwen and probably Mrs. Cadry and someof the teachers.In the input we represent this dialogue as the following sequence:T1 m: surface-request(m, r  informif(r, m, knowref(r, whoIsGoing)))T2 r: surface-request(r, m, informref(m, r, whoIsGoing))T3 m: surface-inform(m, r, not knowref(m, whoIsGoing))T4 r: surface-informref(r, m, whoIsGoing)From Russ's perspective, these utterances had the following discourse-level interpre-tations at the time each was produced:T1 m: pretell(m, r, whoIsGoing)T2 r: askref(r, m, whoIsGoing)T3 m: inform(m, r, not knowref(m, whoIsGoing))T4 r: informref(r, m, whoIsGoing)455Computational Linguistics Volume 21, Number 4Table 6Name Other-misunderstandingPurpose Detecting another's misunderstandingAxiom DEFAULT (3, otherMisunderstanding( sl, $2, pmistake, anew, ts ) ) :act ive(do(s2,  aintended ), ts )A ambiguous(ainten&& asimilar)A wouldExpect(sl, do(s2, asimilar), dO(Sl,anew))A pmistake = mistake(s1, aintended, asimilar)D tYy(s1, $2, anew, ts).FACT otherMisunderstanding( sl, $2, pmistake, al , ts )A ambiguous(a1, a2)9 ~shouldTry(sl, s2, a2, ts).Summary Speaker Sl might be attempting action anew in discourse ts if:1.
Earlier, speaker s2 performed act aintended;2.
Actions aintended and asimilar can be performed using a similar surfaceform;3.
If s2 had performed asimitar, then anew would be expected;4.
Sl may have mistaken aintended for asirailar.After Russ hears T3, he decides that his interpretation of Mother 's  first turn as apretelling is incorrect.
This revision then leads him to reinterpret i as an askref andto provide a new response.We will now show how Russ's beliefs might progress this way.
In particular, weshall address the following questions:?
How Russ decides, after first concluding that T1 was a pretell ing, thathe will respond with an askref.?
How Russ decides, after hearing Mother 's  response T3, that his earlierdecision was incorrect.?
How Russ decides to produce an informref  in T4.Figures 6, 7, 9, and 10 will show the output of the system for each of the four turnsof this dialogue, from Russ's perspective.4.2 Initial assumptionsFor this example, we shall assume that Russ believes that he knows who is going tothe meeting (but also allows that Mother 's  knowledge about the meeting would bemore accurate than his own).
For simplicity, we represent these beliefs as facts.
33FACT believe(r, knowref(r, whoIsGoing)).FACT believe(r, knowsBetterRef(m,r, whoIsGoing)).33 We might have used priorities to express different degrees of belief.456McRoy and Hirst The Repair of Speech Act MisunderstandingsI ?- startDialogue2.>>>surface-request(m,r,informif(r,m,knowref(r,whoIsGoing)))***Interpreting Utterance***Explainingutter(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),ts(O))Is formulapickForm(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),pretell(m,r,whoIsGoing),ts(O)) ok (y/n)?y.Explanation:intentionalAct(m,r,pretell(m,r,whoIsGoing),ts(O))adoptPlan(m,r,pretell(m,r,whoIsGoing),askref(r,m,whoIsGoing),ts(O))credulousB(m,knowsBetterRef(m,r,whoIsGoing))credulousI(m,ts(O))pickForm(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),pretell(m,r,whoIsGoing),ts(O))***Updating Discourse Model***Interpretation: pretell(m, r, whoIsGoing) (turn number i)expressed(do(m, pretell(m, r, whoIsGoing)), 1)Linguistic Intentions of pretell(m,r,whoIsGoing):knowref(m,whoIsGoing)knowsBetterRef(m,r,whoIsGoing)intend(m,do(m,informref(m,r,whoIsGoing)))intend(m,knowref(r,whoIsGoing))Suppositions Added:expressed(knowref(m, wholsGoing), I)expressed(knowsBetterRef(m, r, whoIsGoing), I)expressed(intend(m, do(m, informref(m, r, wheIsGoing))), i)expressed(intend(m, knowref(r, whoIsGoing)), i)Agent m adopted plan to achieve: askref(r,m,whoIsGoing)Figure 6The output for turn 1 from Russ's perspective.We also assume that Russ believes that he knows whether (or not) he knows.FACT believe(r, knowif(r, knowref(r, whoIsGoing))).Lastly, we assume that he has linguistic expectations regarding pretell, askref, andaskif as in Section 2.1.
3434 To keep this example of manageable size, we will not assume that he has any expectations regardingtestif or testref, although in life he would.457Computational Linguistics Volume 21, Number 44.3 Turn 1: Russ decides that Mother is pretellingAccording to the model, after Russ hears Mother's surface-request, "Do you knowwho is going to that meeting?
", he interprets it by attempting toconstruct a plausibleexplanation of it.
This requires tentatively choosing adiscourse-level act on the basis ofthe decomposition relation and then attempting toabduce ither that it is an intentionaldisplay of understanding or that it is a symptom of misunderstanding.
Theorist iscalled to explain the utterance and returns with a list of assumptions that were madeto complete the explanation.
(The portion of the output from the update describesRuss's interpretation f this explanation; see Figure 6.
)In this simulation, T1 was explained as an intentional pretelling.
The explanationcontains the metaplanning assumption that Mother was pretelling as part of a plan toget Russ to ask a question.
The reasoner also attributed to her the linguistic intentionsof pretelling.
We will now consider the complete xplanation i detail.Inference begins with a call to Theorist o explain the input:utter(m, r, surface-request(m,r, informif(r,m, knowref(r, whoIsGoing))),ts(0))This utterance must be explained by finding a discourse-level speech act that it mightaccomplish and a metaplan or misunderstanding that would explain this act.
Thismakes use of the following default:DEFAULT (3, pickForm(sl, s2, asurfaceForm, a, ts) ) :decomp ( asurfaceForm, a)A try(sl,s2,a, ts)9 utter(s1, s2, asurfaceFo~m, ts).To satisfy the first premise, the reasoner would need to find a speech act that is relatedto the surface form by the decomp relation, for example, either an askif, an askref, ora pretelling:decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),pretell(m, r, whoIsGoing))decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),askref(m, r, whoIsGoing))decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),askif(m, r, knowref(r, whoIsGoing)))In this case, the possibility that Mother is attempting a pretelling was considered.
(Thesystem uses an oracle, represented by the default pickForm, to simulate this choice.
35)It is important to note that this is just one of the possible explanations available toRuss.
Nothing in his beliefs rules out abducing explanations from either the askif orthe askref interpretation.To satisfy the second premise of the rule, the reasoner must explain:try(m, r, pretell(m, r, whoIsGoing), ts(0))Two kinds of explanation are possible: a hearer might assume that the act fulfillsthe speaker's intention to coherently extend the discourse as he has understood it or35 This oracle thus allows the analyst o test different interpretations.458McRoy and Hirst The Repair of Speech Act Misunderstandingshe might assume that one of the two types of misunderstanding has occurred.
36Ifa discourse has just begun, then any utterance that starts an adjacency pair will becoherent.
In this case, Russ finds that the former type of explanation is possible usingthe metaplan (for plan adoption) to explain shouldTry(m, r, pretell(m, r whoIsGoing),ts(0)).
The relevant defaults are repeated here:DEFAULT (1, intentionalAct(sl, $2, a, ts) ) :shouldTry( sl, s2, a, ts )D try(s1, $2, a, ts).DEFAULT (3, adoptPlan (st, $2, al, a2, ts) ) :hasGoal(sl, do(s2, a2), ts)A wouldExpect(sl, do(s1, al), do(s2, a2))D shouldTry(sl, s2, al, ts).The conditions of the metaplan are satisfiable because there is a plausible goalact that a pretelling would help Mother to achieve and it is consistent for Russ toassume that achieving this act was, in fact, her goal.
37 Also, when we consider possibleevidence against Mother adopting this plan, namely whether the linguistic intentionsof pretelling were incompatible with those that have been expressed, it would beconsistent to assume that Mother is intending this plan.Russ inferswouldExpect(r, pretell(m, r, whoIsGoing), askref(r, m, whoIsGoing))because he has a linguistic expectation to that effect:FACT lexpectation(do(m, pretell(m, r, whoIsGoing)),knowsBetterRef(m, r  whoIsGoing),do(r, askref(r, m, whoIsGoing))).4.4 Turn 2: Russ decides to respond with an askrefIn turn 2, Russ produces a surface-request.
This utterance is appropriate, independentof whether or not Russ actually wants to know who is going to the meeting, becauseit displays acceptance of Mother's pretelling.
From Russ's perspective it displays ac-ceptance, because a surface-request i  one way to perform an askref, an act that isexpected according to Russ's model of the discourse after the first turn.
38As shown in Figure 7, Theorist finds that if Russ accepts Mother's pretelling, heshould perform an askref.
An askref would demonstrate acceptance because it is theexpected next act.
The derivation of this act relies on the rule for intentional actionshown earlier in Section 4.3, along with the metaplan for acceptance r peated here:36 The former possibility admits that an utterance that displays a misconception, such as a mistaken beliefabout initial knowledge, might still be coherent, unless such knowledge has been introduced into thediscourse xplicitly.
Misconceptions are addressed by second-turn epairs, which are not consideredhere.37 Because Russ's previous utterance had not been the first part of an adjacency pair, he cannot explainher utterance as acceptance or challenge.38 If, for some reason, Russ did not want to know the information, he might decide not to produce anaskref.
However, he would then be accountable for justifying his action as well as for displaying hisacceptance of Mother's displayed understanding (e.g., by including an explicit rejection of her offer);otherwise she might think that one of them has misunderstood.459Computational Linguistics Volume 21, Number 4Explaining shouldTry(r,m,A,ts(1)),decomp(A2,A)Answer: shouldTry(r,m,askref(r,m,whoIsGoing),ts(1)),decomp(surface-request(r,m,informref(m,r,whoIsGoing)),askref(r,m,whoIsGoing))Explanation:intentionalAct(r,m,askref(r,m,whoIsGoing),ts(1))acceptance(r,askref(r,m,whoIsGoing),ts(1))expectedReply(do(m,pretell(m,r,whoIsGoing)),knowsBetterKef(m,r,whoIsGoing),do(r,askref(r,m,whoIsGoing)),ts(1))***Updating Discourse Model***Interpretation: askref(r,m,whoIsGoing) (turn number 2)expressed(do(r,askref(r,m,whoIsGoing)),2)Linguistic Intentions of askref(r,m,whoIsGoing):not knowref(r,whoIsGoing)and intend(r,knowref(r,whoIsGoing))and intend(r,do(m,informref(m,r,whoIsGoing)))Suppositions Added:expressedNot(knowref(r,whoIsGoing),2)expressed(intend(r,knowref(r,whoIsGoing)),2)expressed(intend(r,do(m,informref(m,r,whoIsGoing))),2)Agent r performed expected act :  askref(r,m,whoIsGoing)***Generating Utterance***<<<surface-request(r, m, informref(m, r, whoIsGoing))Figure 7The output for turn 2 from Russ's perspective.DEFAULT (2, acceptance(s1, areply, ts) ) :expected(s1, areply, ts )D shouldTry(sl, $2, areply, ts).The askref would be expected (see Section 3.3.3) because: 39?
According to the discourse model, it is true that active(do(m pretell(m, r,whoIsGoing)), ts(1)).?
There is a linguistic expectation that askref follow pretell.?
Russ believes the conditions of this relation: knowsBetterRef(m, rwhoIsGoing).?
The linguistic intentions of askref are compatible with those alreadyexpressed.39 See Figure 8 for how Mother might interpret this turn.460McRoy and Hirst The Repair of Speech Act MisunderstandingsIf we assume that Mother produced the first turn as an askif, she might also hear T2 as anintentional askref, but for a reason different han Russ would.
Her explanation would includethe metaplanning assumption that he was doing so as part of an adopted plan to get her toproduce an informref.
Although T2 might also be explained by abducing that Russ misunder-stood T1 as an attempted pretelling, we see that she considers this explanation to be less likelybecause otherwise she would have been more inclined to make T3 a third-turn repair ("No, I 'masking you").
40Plan adoption (see Table 1) provides Mother a plausible xplanation for T2 because:1. wouldExpect(r, askref(r, m, whoIsGoing), informref(m, r, whoIsGoing)) is explainedbecause Mother has a linguistic expectation that says that an askref normally creates anexpectation for the listener to tell the speaker the answer:~ACT lexpectation(do(r, askref(r, m, whoIsGoing)),knowref(m, whoIsGoing),do(m, informref(m, r, whoIsGoing))).2.
Mother's credulousness about Russ's goals explains her belief that he wants her toperform the expected informref.3.
The linguistic intentions of askref are compatible with those that have been expressed,so it is consistent to assume that Russ is intending to use it as part of a plan.
(They areconsistent with the context because T1 expresses only that Mother does not knowwhether Russ knows and not that she does not herself know.)4.
Thus, by 1-3 and the metaplan for plan adoption, shouldTry(r, m, askref(r, m,whoIsGoing), ts(0)) is explainable.Assuming this interpretation, Mother can then demonstrate acceptance using an inform-not-knowref.Figure 8How Mother interprets T2.4.5 Turn 3: Russ decides that his interpretation of Turn 1 was  wrongMother  repl ies wi th  a sur face- in form.
This is in terpreted as a d iscourse- level  inform-not-knowref.
This act s ignals a misunders tand ing ,  because the l inguist ic  intent ionsassociated wi th  it are incompat ib le  wi th  those prev ious ly  assumed,  ru l ing  out  anexp lanat ion  that uses the defaul t  for intent ional  acts.
41F igure 9 shows  that Theor ist  abduces  that T3 is at t r ibutable  to a misunders tand ingon Russ 's  part ,  in part icular ,  to his hav ing  incorrect ly  in terpreted one of Mother ' sut terances as a prete l l ing ,  rather  than as an askref .
This exp lanat ion  succeeded becauseeach of the condi t ions  of the defaul t  for se l f -misunders tand ing  were  expla inable.
Belowwe wi l l  repeat  this rule and  then sketch the proof,  cons ider ing  each of the premisesin the default .40 In the model, it is always possible to begin an embedded sequence without addressing the question onthe floor; however, when the embedded sequence is complete, the top-level one is resumed.
It is alimitation of the model that we do not distinguish interruptions from clarifications.41 For Russ to have heard T3 as demonstrating Mother's acceptance of his T2 (i.e., as a display ofunderstanding), the linguistic intentions of inform(m, r, not knowref(m, whoIsGoing)) would need tohave been compatible with this interpretation f the discourse.
However, not knowref(m,whoIsGoing) is among these intentions, while active(knowref(m, whoIsGoing),ts(2)).
As a result, T3cannot be attributed to any expected act, and must be attributed to a misunderstanding either by Russor by Mother.461Computational Linguistics Volume 21, Number 4>>>surface-inform(m, r, not knowref(m, whoIsGoing))***Interpreting Utterance***Explaining utter(m,r,inform(m,r,not knowref(m,wholsGoing)),ts(2))Is formulapickForm(m,r,surface-inform(m,r,not knowref(m,whoIsGoing)),inform(m,r,not knowref(m,whoIsGoing)),ts(2)) ok (y/n)?y.Explanation:selfMisunderstanding(m,r,mistake(r,askref(m,r,whoIsGoing),pretell(m,r,whoIsGoing)),inform(m,r,not knowref(m,whoIsGoing)),ts(2))persists(do(m,pretell(m,r,whoIsGoing)),2)pickForm(m,r,surface-inform(m,r,not knowref(m,whoIsGoing)),inform(m,r,not knowref(m,whoIsGoing)),ts(2))***Updating Discourse Model***Interpretation:inform(m, r, not knowref(m, whoIsGoing)) (turn number 3)expressed(do(m, inform(m, r, not knowref(m, whoIsGoing))), 3)Linguistic Intentions of inform(m,r,not knowref(m,whoIsGoing)):not knowref(m,whoIsGoing)intend(m,knowif(r,not knowref(m,whoIsGoing)))Suppositions Added:expressed(mistake(r, askref(m, r, whoIsGoing),pretell(m, r, whoIsGoing)),3)expressedNot(knowref(m, whoIsGoing), 3)expressed(intend(m, knowif(r, not knowref(m, whoIsGoing))), 3)Agent r misunderstood act do(m, askref(m, r, whoIsGoing))as do(m, pretell(m, r, whoIsGoing))Figure 9The output  for turn  3 from Russ's  perspective.DEFAULTFACTFACT(3, selfMisunderstanding( sl, $2, Pmistake, anew, ts ) ) :active(do(s1, aobserved ), ts )A lintention(anew, Pt)A lintention(aobserved, P12)A inconsistentLI(pl, P12)A ambiguous(aobservm, aintended)A prnistake = mistake(s2, aintended, aobserved)D try(s1, s2, anew, is).-~(selfMisunderstanding(sl, s2, Pmistake, al, ts )A shouldTry(sl, $2, al, ts)).~( selfMisunderstanding( sl, 82, Pmistake, al ,  ts )A ambiguous(a1, a2)A shouldTry(sl, s2, a2, ts)).462McRoy and Hirst The Repair of Speech Act MisunderstandingsPremise 1: A pretelling was active in ts(2), because of Russ's interpretation ofT1.42Premises 24: A pretelling would be incompatible with an inform-not-knowrefhappening now.
The linguistic intentions of the pretelling are:and(knowref(m, whoIsGoing),and(knowsBetterRef(m, r, whoIsGoing),and(intend(m, do(m, informref(m, r  whoIsGoing))),intend(m, knowref(r, wholsGoing)))))The linguistic intentions of inform-not-knowref are:and(not knowref(m, whoIsGoing),intend(m, knowif(r, not knowref(m, whoIsGoing)))).But these intentions are inconsistent, because knowref(m, whoIsGoing)and not knowref(m, whoIsGoing) are incompatible.
As a result,inconsistentLI holds for these linguistic intentions.Premise 5: This is a plausible mistake because the acts pretell and askref bothhave the same surface form:surface-request(m, r, informif(r, m, knowref(r, whoIsGoing)))So, ambiguous(pretell(m, r, whoIsGoing), askref(m, r, whoIsGoing)).The constraints: There is no other coherent interpretation, so it is consistent toassume that a misunderstanding occurred:selfMisunderstanding(m,r,mistake(r, askref(m, r, whoIsGoing),pretell(m, r, whoIsGoing)),inform(m, r, not knowref(m, whoIsGoing)),ts(2)).Thus, try(m, r, inform(m, r, not knowref(m, whoIsGoing)), ts(2)) is explained.
Asa result of this interpretation, not knowref(m, whoIsGoing) is added to the discoursemodel as the fact expressedNot(knowref(m, whoIsGoing)).
This addition terminatesthe activation of knowref(m, whoIsGoing) from the first turn.
(At the same time, ifRuss had revised any of his real beliefs on the basis of the first turn, he might nowreconsider those revisions; however, our theory does not account for this.
)4.6 Turn 4: Russ performs a repairAfter revising his understanding of Turn 1, Russ performs a surface-informref thatdisplays his acceptance of the revised interpretation.
When Theorist is called to find acoherent discourse-level act (i.e., by using the default for intentional cts) it finds thatRuss can perform a fourth-turn repair.
The metaplan for this repair, repeated below, issimilar to that for acceptance, but involves the reconstruction f the discourse model.42 In the discourse model, this was expressed asexpressed(do(m, preteU(m, r, whoIsGoing)), 0), fromwhich one can assume persists(do(m, pretell(m, r, wholsGoing)), 2) by default.463Computational Linguistics Volume 21, Number 4DEFAULT (2, makeFourth TurnRepair( sl, $2, areply, ts ) ) :act ive(mistake ( sl, a intended, aobserved ) , ts )A reconstruction (ts, tSreconstructed)A expected(s1, areply, tSreconstructed)3 shouldTry(sl, 82, areply, ts).This metaplan applies because Russ had misunderstood a prior utterance by Mother,a reconstruction f the discourse is possible, and, within the reconstructed discourse,an informref is expected (as a reply to the misunderstood askref).
43An informref by Russ is expected (see Section 3.3.3) in the reconstructed dialoguebecause:?
There is a linguistic expectation corresponding to the adjacency pairaskref-informref .?
Russ believes its conditions.?
The linguistic intentions of informref are compatible with thereconstruction.5.
Related work5.1 Accounts based on plan recognitionPlan-based accounts interpret speech acts by chaining from subaction to action, fromactions to effects of other actions, and from preconditions to actions to identify aplan (i.e., a set of actions) that includes the observed act.
Heuristics are applied todiscriminate among alternatives.5.1.1 Allen and Perrault.
Allen and Perrault (1979), Perrault and Allen (1980) showhow plan recognition can be used to understand indirect speech acts (such as theuse of "Can you pass the salt?"
as a polite request o pass the salt).
To interpret anutterance, the approach applies a set of context-independent inference rules to identifyall plausible plans.
For example, one rule says that if a speaker wants to know the truthvalue of some proposition, then she might want the proposition to be made true.
Thefinal interpretation is then determined by a set of rating heuristics, such as "Decreasethe rating of a path if it contains an action whose effects are already true at the time theaction starts."
These rating heuristics are problematic because they conflate linguisticand pragmatic knowledge with knowledge about the search mechanism itself.
Thisapproach cannot handle more than a few relationships between utterances and plansand cannot handle any utterances that do not relate to the domain plan in a directmanner.Although we have not yet considered the problem of indirect utterances in detail,we anticipate that such explanations might include as a subtask the kind of plan-based inference that has been proposed, but this inference would be limited by thehearer's own goals and expectations.
However, many common uses of indirectnesscan be explained by the existence of a well-accepted social convention that makesthem expected.43 From Mother's perspective, if indeed she did make an askif in T1, T4 can be seen as a display ofacceptance of it, because a surface-informref is one way to do an informif.
Thus, from her perspective,she need never recognize that Russ has misunderstood.464McRoy and Hirst The Repair of Speech Act MisunderstandingsExplaining shouldTry(r,m,A,ts(3)), decomp(As,A)*?~Reconstructing Turn  Number 1?
**Suppositions Added:expressed(do(m, askref(m, r, wholsGoing)), alt(1))expressedNot(knowref(m, whoIsGoing), alt(1))expressed(intend(m, knowref(m, whoIsGoing)), alt(1))expressed(intend(m, do(r, informref(r, m, whoIsGoing))), alt(1))Answer: shouldTry(r,m,informref(r,m,whoIsGoing),ts(3)),decomp(surface-informref(r,m,whoIsGoing),informref(r,m,whoIsGoing)Explanation:intentionalAct(r,m,informref(r,m,whoIsGoing),ts(3))makeFourthTurnRepair(r,m,informref(r,m,whoIsGoing),ts(3),ts(1))reconstruction(ts(3),ts(alt(1)))?
~Updating Discourse Model~*Interpretation: informref(r,m,whoIsGoing) (turn number 4)expressed(do(r,informref(r,m,whoIsGoing)),4)Linguistic Intentions of  informref(r,m,whoIsGoing):knowref(r,whoIsGoing)and intend(r,knowref(m,whoIsGoing))Suppositions Added:expressed(knowref(r,whoIsGoing),4)expressed(intend(r,knowref(m,whoIsGoing)),4)r performed fourth turn repair?
~Generating Utterance~<<<surface-informref(r,m,whoIsGoing)Figure 10The output for turn 4 from Russ's perspective.5.1.2 Litman.
Work by Litman (1986) attempts to overcome some of the limitations ofAllen and Perrault's approach by extending the plan hierarchy to include discourse-level metaplans, in addition to domain-level p ans.
Metaplans include actions, such asintroduce, continue, or clarify and are recognized, in part, by identifying cue phrases.Although the metaplans add flexibility by increasing the number of possible paths,they also add to the problem of pruning and ordering the paths, requiring additionalheuristics.
For example, there are specific rules for choosing among alternative meta-plans on the basis of clue words, implicit expectations, or default preferences.
Litmanalso adds a new general heuristic: stop chaining if an ambiguity cannot be resolved.5.1.3 Carberry and Lambert.
Carberry (1985, 1987, 1990) uses a similar approach.Her model introduces a new set of discourse-level goals such as seek-confirmation thatare recognized on the basis of the current properties of the dialogue model and themutual beliefs of the participants.
Once a discourse-level goal is selected, a set of can-465Computational Linguistics Volume 21, Number 4didate plans is identified, and Allen-style heuristics are applied to choose one of them.Subsequent work by Lambert and Carberry (1991, 1992) introduces an intermediate,problem-solving level of plans that link the discourse-level acts to domain plans.
Theprocessing rules, by their specificity, eliminate the need for many of the heuristics.
Thesacrifice here is a loss of generality; the mechanisms for recognizing oals are specificto Carberry's implementation.5.1.4 Cawsey.
Cawsey (1991) proposes a method of extending Perrault and Allen's(1980) inference rule approach to produce repairs.
She also suggests including someof the information captured by various rating heuristics as premises in the rules, allow-ing that these new premises may be assumed by default.
For example, the followingrule is proposed for capturing pretellings:if request(Sl, S2, informif(S2, Sl, knowref(S2, D)))and know(S2, knowref(Sl, D))then know(S2, wants(Sl, knowref(S2, D)))To handle misunderstandings, she suggests that such assumptions be retractedif they become inconsistent and then any subsequent utterance whose interpretationdepends on a retracted belief be reinterpreted from scratch.
This approach is thusmuch stronger than most accounts of negotiation, such as ours, which allow that aparticipant might choose to forego a complete repair.
Allowing defeasible beliefs is astep in the right direction; however, the approach still misses the point that participantsare able to negotiate meanings.
Preconditions such as know(S2, not knowref (Sl, D))influence interpretations only to the extent that they provide support for, or evidenceagainst, a particular (abductive) explanation.
In Example 2, even if Mother knew whowas going, she could still be asking Russ a question, albeit insincerely.
Similarly, even ifRuss suspected that Mother did not know who was going, he might still have chosento treat her utterance as a pretelling, perhaps to confirm his suspicions or to delayanswering.5.1.5 Traum and Hinkelman.
Hinkelman's (1990) work incorporates some abductivereasoning in her model of utterance interpretation.
The model treats different featuresin the input, such as the mood of a sentence or the presence of a particular lexicalitem, as manifestations of different speech acts.
During interpretation, procedures thattest for particular features of the input suggest candidates.
The system then removesany candidates whose implicatures are inconsistent with prior beliefs.Traum and Hinkelman (1992) extend this work by generalizing the notion ofspeech act to conversation act.
Conversation acts include traditional speech act types aswell as what Traum and Hinkelman call grounding acts.
Conversation acts, however, arenot assumed to be understood without some positive vidence by the receiver, such asan acknowledgment.
Grounding acts include initiating, clarifying, or acknowledgingan utterance, and taking and releasing a turns.
These acts differ from our own meta-plans in that they are organized into a finite state grammar, and do not account forgrounding acts that would violate a receiver's expectations.
In conversation, ground-ing acts that violate the grammar are not recognized.
Traum and Hinkelman suggestthat such violations hould be used to trigger a repair, but admit that, except when arepair has been requested explicitly, the model itself says nothing about when a repairshould be uttered (p. 593).
4444 Interpretations that have the right pragmatic force but inconsistent implicatures are ruled out as in466McRoy and  Hirst  The Repair  of Speech Act M isunders tand ingsTraum and Allen (1994) extend the work to include a notion of social obligation,which serves much the same purpose as expectations in our model.5.2 Other expectation-driven accountsWithin the speech understanding community, the word "expectation" has been useddifferently from our use here.
Expectation i the speech context refers to what the nextword or utterance is likely to be about.
45 For example, after the computer asks the userto perform some action A, it might expect any of the following types of responses:1.
A statement about background knowledge that might be needed.2.
A statement about the underlying purpose of A.3.
A statement about related task steps (i.e., subgoals of A, tasks thatcontain A as a step, or tasks that might follow A).4.
A statement about the accomplishment of A.These expectations are independent of the belief state of an agent and are specifieddown to the semantic (and sometimes even lexical) level.
This information has longbeen used to discriminate between ambiguous interpretations and correct mistakesmade by the speech recognizer (Fink and Biermann 1986; Smith 1992).
Typically, anutterance will be interpreted according to the expectation that matches it most closely.By contrast, our approach and that of the plan-based accounts use "expectation" torefer to agents' beliefs about how future utterances might relate to prior ones.
Theseexpectations are determined both by an agent's understanding of typical behavior andby his or her mental state.
These two notions of expectation are complementary, andany dialogue model that uses speech as input must be able to represent and reasonwith both.5.3 Approaches to misconceptionMisconceptions are a deficit in an agent's knowledge of the world; they can become abarrier to understanding if they cause an agent o unintentionally evoke a concept orrelation.
To prevent misconceptions from triggering a misunderstanding, a ents cancheck for evidence of misconception a d try to resolve apparent errors.
The symptomsof misconception i clude references to entities that do not map to previously knownobjects or operations (Webber and Mays 1983) or requests for clarification (Moore1989).
Errors are corrected by replacing or deleting parts of the problematic utteranceso that it makes sense.
Several correction strategies have been suggested:?
Generalize a description by selectively ignoring some constraints ( eeGoodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh1991; Eller and Carberry 1992),?
Make a description more specific by adding extra constraints ( ee Ellerand Carberry 1992), and?
Choose a conceptual "sibling", by combining eneralization a dconstraint operations.
For example, if there is more than one strategy forHinkelman (1990); however, as with grounding acts, presumably the elimination of all possibleinterpretations could cue some type of repair mechanism, if they chose to incorporate one.45 This is the same sense of "expectation" as used by Riesbeck (1974).467Computational Linguistics Volume 21, Number 4achieving a goal, then an entity that corresponds to a step from onestrategy might be replaced by one corresponding to a step from one ofthe other strategies (see Carberry 1985, 1987; Eller and Carberry 1992;Moore 1989).Although these approaches do quite well at preventing certain classes of misun-derstandings, they cannot prevent hem all.
Moreover, these approaches may actuallytrigger misunderstandings because they always find some substitution, and yet theylack any mechanisms for detecting when one of their own previous repairs was inap-propriate.
Thus, a conversational participant will still need to be able to address actualmisunderstandings.5.4 Collaboration in the resolution of nonunderstandingIn this paper, we have concentrated on the repair of mis-understanding.
Our colleaguesHeeman and Edmonds have looked at the repair of non-understanding.
The differencebetween the two situations is that in the former, the agent derives exactly one inter-pretation of an utterance and hence is initially unaware of any problem; in the latter,the agent derives either more than one interpretation, with no way to choose betweenthem, or no interpretation atall, and so the problem is immediately apparent.
Heemanand Edmonds looked in particular at cases in which a referring expression uttered byone conversant was not understood by the other (Heeman and Hirst 1995; Edmonds1994; Hirst et al 1994).
Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark1993) have shown that in such situations, conversants will collaborate on repairingthe problem by, in effect, negotiating a reconstruction or elaboration of the referringexpression.
Heeman and Edmonds model this with a plan recognition and generationsystem that can recognize faulty plans and try to repair them.
Thus (as in our ownmodel) two copies of the system can converse with each other, negotiating referentsof referring expressions that are not understood by trying to recognize the referringplans of the other, repairing them where necessary, and presenting the new referringplan to the other for approval.6.
ConclusionsIn human dialogues, both the producer and the recipient of an utterance have a sayin determining its interpretation.
Moreover, they may both change their minds inthe face of new information.
Dialogue participants are able to negotiate the meaningof utterances because in responding to what the hearer decides are the speaker'sgoals and expectations regarding an utterance, the hearer also provides evidence ofthat decision and hence constraints on what the speaker may do next.
If the speakerdisagrees with a displayed interpretation, she can challenge it directly or decide torespond in such a way that the hearer must infer a misunderstanding.The long-term goal of our work is to construct a model of communicative interac-tion that will be able to support he negotiation of meaning.
We have considered theinformation sources and reasoning processes that agents need to determine their be-liefs about he goals and expectations a sociated with each other's utterances.
Whereasprevious models of dialogue tend to represent discourse meaning from some globalperspective, make use of either purely structural or purely intentional information,and give minimal attention to repair, in our model:?
Each agent has his or her own model of the discourse.468McRoy and Hirst The Repair of Speech Act Misunderstandings?
Agents rely on both structural and intentional information in thediscourse.?
Agents distinguish between intended actions and misunderstandings.?
Agents interpret utterances on the basis of expectations derived fromprevious utterances as well as expectations for future actions that arepredicted by the utterance under interpretation.?
Agents are able to detect and repair their own misunderstandings aswell as those of others.We see this work as providing some of the first steps toward a unified account ofinterpretation, generation, and repair.The primary contributions of this work have been to treat misunderstanding andrepair as intrinsic to conversants' core language abilities and to account for them withthe same processing mechanisms that account for normal speech.
In particular, bothinterpretation and repair are treated as explanation problems, modeled as abduction.In order to account for the repair of misunderstandings, we have proposed a repre-sentation of the discourse that captures the agent's interpretation of the conversationboth before and after a repair and that is independent of the actual beliefs of theparticipants--a dynamic mental artifact that is the object of belief and repair.
Withsuch a record of the discourse, agents can refer to alternative interpretations or to therepair process itself, potentially enabling them to recover from rejected repairs.
Byaddressing the problem of repair, this work should facilitate efforts to build naturallanguage interfaces that can better recover from their own mistakes as well as thoseof their users.AcknowledgmentsThis work was supported by the NaturalSciences and Engineering Research Councilof Canada.
We thank Ray Reiter for hissuggestion that we use abduction and JamesAllen for his advice about temporal logics.We thank Hector Levesque, MikeGruninger, Sheila McIlraith, Javier Pinto,and Stephen Shapiro, for their comments onmany of the formal aspects of this work.
Wealso thank David Chapman, Susan Haller,Diane Horton, C. Raymond Perrault, MarkSteedman, Evan Steeg, and the reviewersfor their comments on drafts of this paperor the thesis on which it is based (McRoy1993a).ReferencesAllen, James (1983).
"Recognizing intentionsfrom natural language utterances."
InMichael Brady, Robert C. Berwick, andJames Allen, editors, Computational Modelsof Discourse.
The MIT Press, 107-166.Allen, James, and Perrault, Raymond (1979).
"Plans, inference, and indirect speechacts."
In 17th Annual Meeting of theAssociation for Computational Linguistics,Proceedings ofthe Conference, 85-87.Bach, Kent, and Harnish, Robert M. (1979).Linguistic Communication a d Speech Acts.The MIT Press.Beun, Robbert-Jan (1990).
"Speech acts andmental states."
In Proceedings ofthe FifthRocky Mountain Conference on ArtificialIntelligence, Pragmatics inArtificialIntelligence, 75-80, Las Cruces, NewMexico.Brewka, Gerhard (1989).
"Preferredsubtheories: An extended logicalframework for default reasoning."
InProceedings ofthe 11th International JointConference on Artificial Intelligence,1043-1048, Detroit, MI.Calistri-Yeh, Randall J (1991).
"Utilizinguser models to handle ambiguity andmisconceptions in robust planrecognition."
User Modeling andUser-Adapted Interaction, 1(4), 289-322.Carberry, Sandra (1985).
Pragmatics Modelingin Information Systems Interfaces.
Doctoraldissertation, University of Delaware,Newark, Delaware.Carberry, Sandra (1987).
"Pragmaticmodeling: Toward a robust naturallanguage interface."
ComputationalIntelligence, 3(3), 117-136.Carberry, Sandra (1988).
"Modeling the469Computational Linguistics Volume 21, Number 4user's plans and goals.
ComputationalLinguistics, 14(3), 23-37.Carberry, Sandra (1990).
Plan Recognition iNatural Language Dialogue.
The MIT Press,Cambridge, MA.Cawsey, Alison J.
(1991).
"A belief revisionmodel of repair sequences in dialogue.
"In Ernesto Costa, editor, New Directions inIntelligent Tutoring Systems.
SpringerVerlag.Charniak, Eugene, and Goldman, Robert(1988).
"A logic for semanticinterpretation."
In 26th Annual Meeting ofthe Association for Computational Linguistics,Proceedings ofthe Conference, 87-94, Buffalo,NY.Clark, Herbert H. (1993).
Arenas of LanguageUse.
The University of Chicago Press, andStanford: Center for the Study ofLanguage and Information.Clark, Herbert H., and Wilkes-Gibbs,Deanna (1986).
"Referring as acollaborative process."
Cognition, 22:1-39.
(Reprinted in Intentions in Communication,edited by Philip R. Cohen, Jerry Morgan,and Martha Pollack.
The MIT Press,pages 463--493.
)Cohen, Philip R., and Levesque, Hector(1985).
"Speech acts and rationality."
InThe 23rd Annual Meeting of the Associationfor Computational Linguistics, Proceedings ofthe Conference, 49-60, Chicago.Edmonds, Philip G. (1994).
"Collaborationon reference to objects that are notmutually known."
In Proceedings, 15thInternational Conference on ComputationalLinguistics (COLING-94), 1118-1122, Kyoto.Eller, Rhonda, and Carberry, Sandra (1992).
"A meta-rule approach to flexible planrecognition i  dialogue."
User Modelingand User-Adapted Interaction, 2(1-2), 27-53.Fink, Pamela E., and Biermann, Alan W.(1986).
"The correction of ill-formed inputusing history-based expectation withapplications to speech understanding.
"Computational Linguistics, 12(1), 13-36.Fox, Barbara (1987).
"Interactionalreconstruction in real-time languageprocessing."
Cognitive Science, 11, 365-387.Goodman, Bradley (1985).
"Repairingreference identification failures byrelaxation."
In 23th Annual Meeting of theAssociation for Computational Linguistics,Proceedings ofthe Conference, 204-217,Chicago.Grice, H. P. (1957).
"Meaning."
ThePhilosophical Review, 66, 377-388.Heeman, Peter, and Hirst, Graeme (1995).Collaborating on referring expressions.Computational Linguistics, 21(3).Hinkelman, Elizabeth A.
(1990).
"Linguisticand Pragmatic Constraints on UtteranceInterpretation."
Doctoral dissertation,Department of Computer Science,University of Rochester, Rochester, NewYork.
Published as University ofRochester Computer Science TechnicalReport 288, May 1990.Hirst, Graeme; McRoy, Susan; Heeman,Peter; Edmonds, Philip; and Horton,Diane (1994).
"Repairing conversationalmisunderstandings andnon-understandings."
SpeechCommunication, 15, 213-229.Hobbs, Jerry R.; Stickel, Mark; Martin, Paul;and Edwards, Douglas (1988).
"Interpretation asabduction."
In 26thAnnual Meeting of the Association forComputational Linguistics, Proceedings oftheConference, 95-103.Hobbs, Jerry R.; Stickel, Mark; Appelt,Douglas E.; and Martin, Paul (1993).
"Interpretation asabduction."
ArtificialIntelligence, 63, 69-142.Jose, Paul E. (1988).
"Sequentiality of speechacts in conversational structure."
Journal ofPsycholinguistic Research, 17(1), 65-88.Lambert, Lynn, and Carberry, Sandra (1991).
"A tri-partite plan-based model ofdialogue."
In 29th Annual Meeting of theAssociation for Computational Linguistics,Proceedings ofthe Conference, 47-54,Berkeley, CA.Lambert, Lynn, and Carberry, Sandra (1992).
"Modeling negotiation dialogues."
In 30thAnnual Meeting of the Association forComputational Linguistics, Proceedings oftheConference, 193-200, Newark, Delaware.Litman, Diane J.
(1986).
"Linguisticcoherence: A plan-based alternative."
In24th Annual Meeting of the Association forComputational Linguistics, Proceedings oftheConference, 215-223, New York.Loveland, D. W. (1978).
Automated TheoremProving: A Logical Basis.
North-Holland,Amsterdam, The Netherlands.Luperfoy, Susann (1992).
"Therepresentation f multimodal userinterface dialogues using discourse pegs.
"In 30th Annual Meeting of the Association forComputational Linguistics, Proceedings oftheConference, 22-31, Newark, Delaware.McCoy, Kathleen E (1985).
"The role ofperspective in responding to propertymisconceptions."
In Proceedings oftheNinth International Joint Conference onArtificial Intelligence, volume 2, 791-793.McCoy, Kathleen E (1986).
"The ROMPERsystem: responding to object-relatedmisconceptions u ing perspective."
In24th Annual Meeting of the Association forComputational Linguistics, Proceedings ofthe470McRoy and Hirst The Repair of Speech Act MisunderstandingsConference, 97-105.McCoy, Kathleen E (1988).
"Reasoning on ahighlighted user model to respond tomisconceptions."
Computational Linguistics,14(3), 52-63.McCoy, Kathleen E (1989).
"Generatingcontext-sensitive responses to objectmisconceptions."
Artificial Intelligence,41(2), 157-195.McLaughlin, Margaret L. (1984).Conversation: How Talk is Organized.
SagePublications, Beverly Hills.McRoy, Susan W. (1993a).
AbductiveInterpretation a d Reinterpretation of NaturalLanguage Utterances.
Doctoral dissertation,Department of Computer Science,University of Toronto, Toronto, Canada.Published as CSRI Technical Report No.288, University of Toronto, Department ofComputer Science.McRoy, Susan W. (1993b).
"Belief as aneffect of an act of introspection."
In 1993AAAI Spring Symposium on Reasoning aboutMental States: Formal Theories andApplications, 86-89, Stanford University.Moore, Johanna D. (1989).
"Responding to"Huh?
": Answering vaguely articulatedfollow-up questions."
In Conference onHuman Factors in Computing Systems(CHI'89), 91-96, Austin, TX.
ACM Press /Addison-Wesley.
Also published as aspecial issue of SIGCHI Bulletin(unnumbered).Nagata, Masaaki, and Morimoto, Tsuyoshi(1993).
"An experimental statisticaldialogue model to predict he speech acttype of the next utterance."
InInternational Symposium on Spoken Dialogue,Tokyo, Japan.Perrault, C. Raymond (1990).
"Anapplication of default logic to speech acttheory."
In Intentions in Communication.Edited by Philip R. Cohen, Jerry Morgan,and Martha Pollack, The MIT Press,161-186.
An earlier version of this paperwas published as Technical ReportCSLI-87-90 by the Center for the Study ofLanguage and Information.Perrault, C. Raymond, and Allen, James(1980).
"A plan-based analysis of indirectspeech acts."
Computational Linguistics, 6,167-183.Pollack, Martha E. (1986a).
"Inferringdomain plans in question-answering.
"Technical Report TR 403, ArtificialIntelligence Center, SRI International,Menlo Park, CA.Pollack, Martha E. (1986b).
"A model ofplan inference that distinguishes betweenthe beliefs of actors and observers."
InProceedings, 24th annual meeting of theAssociation for Computational Linguistics,207-214, New York.Pollack, Martha E. (1990).
"Plans as complexmental attitudes."
In Intentions inCommunication.
Edited by Philip Cohen,Jerry Morgan, and Martha Pollack, MITPress, 77-103.Poole, David; Goebel, Randy; and Aleliunas,Romas (1987).
"Theorist: A logicalreasoning system for defaults anddiagnosis."
In The Knowledge Frontier:Essays in the Representation of Knowledge.Edited by Nick Cercone and GordonMcCalla, Springer-Verlag, New York,331-352.
Also published as ResearchReport CS-86-06, Faculty of Mathematics,University of Waterloo, February, 1986.Reithinger, Norbert, and Maier, Elisabeth(1995).
"Utilizing statistical dialogue actprocessing in verbmobil."
In Proceedings,33rd annual meeting of the Association forComputational Linguistics, 116-121,Cambridge, MA, June.Riesbeck, Christopher (1974).
"Computational understanding: Analysisof sentences and context."
TechnicalReport STAN-CS-74-437, ComputerScience Department, Stanford University.Schegloff, Emanuel A.
(1988).
"Presequencesand indirection: Applying speech acttheory to ordinary conversation."
Journalof Pragmatics, 12, 55-62.Schegloff, Emanuel A.
(1992).
"Repair afternext turn: The last structurally provideddefense of intersubjectivity nconversation."
American Journal ofSociology, 97(5), 1295-1345.Schegloff, Emanuel A., and Sacks, Harvey(1973).
"Opening up closings."
Semiotica,7, 289-327.Smith, Ronnie (1992).
"A ComputationalModel of Expectation-DrivenMixed-Initiative Dialog Processing.
"Doctoral dissertation, Department ofComputer Science, Duke University,Durham, NC.Spencer, Bruce (1990).
"Assimilation in planrecognition via truth maintenance withreduced redundancy."
Doctoraldissertation, Department of ComputerScience, University of Waterloo.Stickel, Mark E. (1989).
"A Prologtechnology theorem prover."
Journal ofAutomated Reasoning, 4 353-360.Terasaki, A.
(1976).
"Pre-announcementsequences in conversation."
Social ScienceWorking Paper 99, School of SocialScience, University of California, Irvine.Traum, David, and Allen, James (1994).471Computational Linguistics Volume 21, Number 4"Discourse obligations in dialogueprocessing."
In 32nd Annual Meeting of theAssociation for Computational Linguistics,Proceedings ofthe Conference, 1-8, LasCruces, NM.Traum, David, and Hinkelman, Elizabeth(1992).
"Conversation acts in task-orientedspoken dialogue."
ComputationalIntelligence, 8(3).
Special Issue:Computational Approaches to Non-LiteralLanguage.Tsui, Amy B. M. (1991).
"Sequencing rulesand coherence in discourse."
Journal ofPragmat'ics, 15, 111-129.Umrigar, Zerksis D., and Pitchumani, Vijay(1985).
"An experiment in programmingwith full first-order logic."
In Symposium ofLogic Programming, Boston, MA.
IEEEComputer Society Press.van Arragon, Paul (1990).
"Nested DefaultReasoning for User Modeling."
Doctoraldissertation, Department of ComputerScience, University of Waterloo, Waterloo,Ontario.
Published by the department asResearch Report CS-90-25.Walker, Marilyn A.
(1991).
"Redundancy incollaborative dialogue."
In 1991 AAAI FallSymposium on Discourse Structure in NaturalLanguage Understanding and Generation,124-129, Pacific Grove, Monterey, CA.Webber, Bonnie L., and Mays, Eric (1983).
"Varieties of user misconceptions:Detection and correction."
In Proceedingsof the Eighth International Joint Conference onArtificial Intelligence, 650-652, Karlsruhe.Appendix A: A third-turn repairThis appendix gives an annotated version of the output for the following example ofthird-turn repair (from McLaughlin (1984)).
We will consider the results of a simulationin which two copies of our system, each with its own beliefs and goals, converse witheach other.Example 4T1 A: When is the dinner for Alfred?T2 B: Is it at seven-thirty?T3 A: No, I'm asking you.T4 B: Oh, I don't know.For this example, we assume that A (a) wants B (b) to tell her the time of thedinner for Alfred, that she believes that she does not already know, that he knowswhen it is, and that he believes that she does not know, of any given time (includingseven-thirty), whether it is the time of the dinner.
Notice that the last belief of a, isher belief about what b believes about what she knows.fact hasGoal('a,fact believe(a,fact believe(a,fact believe(b,do(b, informref(b, a, whenD)),ts(O)).not knowref(a, whenD)).knowref(b, whenD)).not knowif(a, dinnerAtSevenThirty)).We also assume that speaker b, believes that he does not know the time of the dinner,but suspects it is at seven-thirty and believes that a does know when it is.fact believe(b,fact believe(b,fact believe(b,not knowref(b, whenD)).suspectRef(b, whenD, dinnerAtSevenThirty)).knowref(a, whenD)).We will show the output for both sides of the conversation, showing the perspectiveof speaker b as boxed text.472McRoy and Hirst The Repair of Speech Act MisunderstandingsFirst, speaker a generates a request o try to satisfy her goal:When is the dinner for Alfred?I ?- startDialoguel.Explaining shouldTry(a,b,A,ts(O)),decomp(U,A)Answer:shouldTry(a,b,askref(a,b,whenD),ts(O)),decomp(surface-request(a,b,informref(b,a,whenD)),askref(a,b,whenD))Explanation:intentionalAct(a,b,askref(a,b,whenD),ts(O))adoptPlan(a,b,askref(a,b,whenD),informref(b,a,whenD),ts(O))***Updating Discourse Model***Interpretation: askref(a,b,whenD) (turn number 1)Suppositions Added:expressed(do(a,askref(a,b,whenD)),l)expressedNot(knowref(a,whenD),l)expressed(intend(a,knowref(a,whenD)),l)expressed(intend(a,do(b,informref(b,a,whenD))),l)Agent a adopted plan to achieve: informref(b,a,whenD)~**Generating Utterance*s*<<<surface-request(a,b,informref(b,a,whenD))Speaker b interprets this utterance as a test:I ?- startDialogue2.>>>surface-request(a,b,informref(b,a,whenD))***Interpreting Utterance***Explaining utter(a,b,request(a,b,informref(b,a,whenD)),ts(O))Is formula pickForm(a,b,surface-request(a,b,informref(b,a,whenD)),askref(a,b,whenD),ts(O)) ok (y/n)?n.Is formula pickForm(a,b,request(a,b,surface-informref(b,a,whenD)),testref(a,b,whenD),ts(O)) ok (y/n)?y.Explanation:intentionalAct(a,b,testref(a,b,whenD),ts(O))adoptPlan(a,b,testref(a,b,whenD),assertref(b,a,wheuD,ts(O))credulousB(a,knowref(b,whenD))credulousI(a,ts(O))pickForm(a,b,surface-request(a,b,assertref(b,a,whenD)),testref(a,b,whenD),ts(O))473Computational Linguistics Volume 21, Number 4***Updating Discourse Model***Interpretation: testref(a,b,whenD) (turn number I)Suppositions Added:expressed(do(a,testref(a,b,whenD)),l)expressed(knowref(a,whenD),l)expressed(intend(a,do(b, assertref(b,a,whenD))),l)Agent a adopted plan to achieve: assertref(b,a,whenD)Speaker b now replies: Is it at seven-thirty?He produces this reply because he has a linguistic expectation that says that "if someone istesting you and you suspect hat X is the answer to their test, the appropriate thing to dois to (tentatively) propose X as the answer" .
46 (In the explanation below, this expectationis indicated by the default "expectedRep ly ' ) .Explaining shouldTry(b,a,A,ts(1)), decomp(U,A)Answer:shouldTry(b,a,askif(b,a,dinnerAtSevenThirty),ts(1)),decomp(surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),askif(b,a,dinnerAtSevenThirty))Explanation:intentionalAct(b,a,askif(b,a,dinnerAtSevenThirty),ts(1))acceptance(b,askif(b,a,dinnerAtSevenThirty),ts(1))expectedReply(do(a,testref(a,b,whenD)),suspectRef(b,whenD,dinnerAtSevenThirty),do(b,askif(b,a,dinnerAtSevenThirty)),ts(1))***Updating Discourse Model***Interpretation: askif(b,a,dinnerAtSevenThirty) (turn number 2)Suppositions Added:expressed(do(b,askif(b,a,dinnerAtSevenThirty)),2)expressedNot(knowif(b,dinnerAtSevenThirty),2)expressed(intend(b,knowif(b,dinnerAtSevenThirty)),2)expressed(intend(b,do(a,informif(a,b,dinnerAtSevenThirty))),2)Agent b performed expected act: askif(b,a,dinnerAtSevenThirty)***Generating Utterance***<<<surface-request(b,a,informif(a,b,dinnerAtSevenThirty))46 suspectRef, like knowsBetterRef, is a supposition of a less than certain belief that an agent knows theanswer.
Except for allowing an agent he (extra) possibility of responding with askif, it expresses thesame information as not knowref.
Under our initial assumptions, speaker b might also have answered"I don't know.
"474McRoy and Hirst The Repair of Speech Act MisunderstandingsSpeaker a recognizes that Speaker b has produced a yes-no question, misunderstanding herrequest as a test:>>>surface-reqnest(b,a,informif(a,b,dinnerAtSevenThirty))***Interpreting Utterance***Explaining utter(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),ts(1))Is formula pickForm(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),askif(b,a,dinnerAtSevenThirty),ts(1)) ok (y/n)Ty.Explanation:otherMisunderstanding(b,a,mistake(b,askref(a,b,whenD),testref(a,b,whenD)),askif(b,a,dinnerAtSevenThirty),ts(1))credulousB(b,suspectRef(b,whenD,dinnerAtSevenThirty))pickForm(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),askif(b,a,dinnerAtSevenThirty),ts(1))***Updating Discourse Model***Interpretation: askif(b,a,dinnerAtSevenThirty) (turn number 2)Suppositions Added:expressed(do(b,askif(b,a,dinnerAtSevenThirty)),2)expressedNot(knowif(b,dinnerAtSevenThirty),2)expressed(intend(b,knowif(b,dinnerAtSevenThirty)),2)expressed(intend(b,do(a,informif(a,b,dinnerAtSevenThirty))),2)Agent b mistook askref(a,b,whenD) for testref(a,b,whenD):expressed(mistake(b,askref(a,b,whenD),testref(a,b,whenD)),2)Speaker a then produces a third-turn repair:No, I'm asking you.Explaining shouldTry(a,b,A,ts(2)), decomp(U,A)Answer:shouldTry(a,b,inform(a,b,do(a,askref(a,b,whenD))),ts(2)),decomp(surface-inform(a,b,do(a,askref(a,b,whenD))),inform(a,b,do(a,askref(a,b,whenD))))Explanation:intentionalAct(a,b,inform(a,b,do(a,askref(a,b,whenD))),ts(2))makeThirdTurnRepair(a,b,informref(b,a,whenD),ts(2))***Updating Discourse Model***Interpretation:inform(a,b,do(a,askref(a,b,whenD))) (turn number 3)475Computational Linguistics Volume 21, Number 4Suppositions Added:expressed(do(a,inform(a,b,do(a,askref(a,b,whenD)))),3)expressed(do(a,askref(a,b,whenD)),3)expressed(intend(a,knowif(b,do(a,askref(a,b,whenD)))),3)m performed third turn repair***Generating Utterance***<<<surface-inform(a,b,do(a,askref(a,b,whenD)))Speaker b recognizes his misunderstanding:>>>surface-inform(a,b,do(a,askref(a,b,whenD)))***Interpreting Utterance***Explaining utter(a,b,inform(a,b,do(a,askref(a,b,whenD))), ts(2))Is formulapickForm(a,b,surface-inform(a,b,do(a,askref(a,b,whenD))),inform(a,b,do(a,askref(a,b,whenD))), ts(2))ok (y/n)?y.Explanation:persists(do(a,testref(a,b,whenD)),2)selfMisunderstanding(a,b,mistake(b,askref(a,b,whenD),testref(a,b,whenD)),inform(a,b,do(a,askref(a,b,whenD))), ts(2))pickForm(a,b,surface-inform(a,b,do(a,askref(a,b,whenD))),inform(a,b,do(a,askref(a,b,whenD))),ts(2))***Updating Discourse Model***Interpretation:inform(a,b,do(a,askref(a,b,whenD))) (turn number 3)Suppositions Added:expressed(do(a,inform(a,b,do(a,askref(a,b,whenD)))),3)expressed(do(a,askref(a,b,whenD)),3)expressed(intend(a, knowif(b,do(a,askref(a,b,whenD)))), 3)Agent b mistook askref(a,b,whenD) for testref(a,b,whenD):expressed(mistake(b,askref(a,b,whenD),testref(a,b,whenD)),3)Speaker b produces a fourth-turn repair: Oh, I don't know.Explaining shouldTry(b,a,A,ts(3)), decomp(U,A)***Keconstructing Turn Number i***476McRoy and Hirst The Repair of Speech Act Misunderstandingsexpressed(do(a,askref(a,b,whenD)),alt(1))expressedNot(knowref(a,whenD),alt(1))expressed(intend(a,knowref(a,whenD)),alt(1))expressed(intend(a,do(b,informref(b,a,whenD))),alt(1))Answer:shouldTry(b,a,inform(b,a,not knowref(b,whenD)),ts(3)),decomp(surface-inform(b,a,not knowref(b,whenD)),inform(b,a,not knowref(b,whenD)))Explanation:intentionalAct(b,a,inform(b,a,not knowref(b,whenD)),ts(3))makeFourthTurnRepair(b,a,inform(b,a,not knowref(b,whenD)),ts(3),ts(alt(1)))expectedReply(do(a,askref(a,b,whenD)),not knowref(b,whenD),do(b,inform(b,a,not knowref(b,whenD))),ts(alt(1)))reconstruction(ts(3),ts(alt(1)))***Updating Discourse Model***Interpretation: inform(b,a,not knowref(b,whenD)) (turn number 4)Suppositions Added:expressed(do(b,inform(b,a,not knowref(b,whenD))),4)expressedNot(knowref(b,whenD),4)expressed(intend(b,knowif(a,not knowref(b,whenD))),4)r performed fourth turn repair***Generating Utterance***<<<surface-inform(b,a,not knowref(b,whenD))Speaker a takes this utterance as an acceptance of her initial request.>>>surface-inform(b,a,not knowref(b,whenD))***Interpreting Utterance***Explaining utter(b,a,inform(b,a,not knowref(b,whenD)),ts(3))Is formula pickForm(b,a,surface-inform(b,a,not knowref(b,whenD)),inform(b,a,not knowref(b,whenD)),ts(3)) ok (y/n)?y.Explanation:persists(do(a,askref(a,b,whenD)),3)persists(do(a,askref(a,b,whenD)),2)intentionalAct(b,a,inform(b,a,not knowref(b,whenD)),ts(3))acceptance(b,inform(b,a,not knowref(b,whenD)),ts(3))expectedReply(do(a,askref(a,b,whenD)),not knowref(b,whenD),do(b,inform(b,a,not knowref(b,whenD))),ts(3))credulousB(b,not knowref(b,whenD))477Computational Linguistics Volume 21, Number 4pickForm(b,a,surface-inform(b,a,not knowref(b,whenD)),inform(b,a,not knowref(b,whenD)),ts(3))?
~Updating Discourse Mode l~Interpretation: inform(b,a,not knowref(b,whenD)) (turn number 4)Suppositions Added:expressed(do(b,inform(b,a,not knowref(b,whenD))),4)expressedNot(knowref(b,whenD),4)expressed(intend(b,knowif(a,not knowref(b,whenD))),4)Agent b performed expected act: inform(b,a,not knowref(b,whenD))478
