Assigning Polarity Scores to ReviewsUsing Machine Learning TechniquesDaisuke Okanohara1 and Jun?ichi Tsujii1,2,31 Department of Computer Science, University of Tokyo,Hongo, 7-3-1, Bunkyo-ku, Tokyo 113-00132 CREST, JST, Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-00123 School of Informatics, University of Manchester,POBox 88, Sackville St, Manchester, M60 1QD, UK{hillbig, tsujii}@is.s.u-tokyo.ac.jpAbstract.
We propose a novel type of document classification task thatquantifies how much a given document (review) appreciates the targetobject using not binary polarity (good or bad) but a continuous mea-sure called sentiment polarity score (sp-score).
An sp-score gives a veryconcise summary of a review and provides more information than binaryclassification.
The difficulty of this task lies in the quantification of po-larity.
In this paper we use support vector regression (SVR) to tacklethe problem.
Experiments on book reviews with five-point scales showthat SVR outperforms a multi-class classification method using supportvector machines and the results are close to human performance.1 IntroductionIn recent years, discussion groups, online shops, and blog systems on the Internethave gained popularity and the number of documents, such as reviews, is growingdramatically.
Sentiment classification refers to classifying reviews not by theirtopics but by the polarity of their sentiment (e.g, positive or negative).
It isuseful for recommendation systems, fine-grained information retrieval systems,and business applications that collect opinions about a commercial product.Recently, sentiment classification has been actively studied and experimentalresults have shown that machine learning approaches perform well [13,11,10,20].We argue, however, that we can estimate the polarity of a review more finely.
Forexample, both reviews A and B in Table 1 would be classified simply as positivein binary classification.
Obviously, this classification loses the information aboutthe difference in the degree of polarity apparent in the review text.We propose a novel type of document classification task where we evaluatereviews with scores like five stars.
We call this score the sentiment polarity score(sp-score).
If, for example, the range of the score is from one to five, we couldgive five to review A and four to review B.
This task, namely, ordered multi-classclassification, is considered as an extension of binary sentiment classification.In this paper, we describe a machine learning method for this task.
Oursystem uses support vector regression (SVR) [21] to determine the sp-scores ofR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
314?325, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Assigning Polarity Scores to Reviews 315Table 1.
Examples of book reviewsExample of Review binary sp-score(1,...,5)Review A I believe this is very good and a ?must read?
plus 5I can?t wait to read the next book in the series.Review B This book is not so bad.
plus 4You may find some interesting points in the book.Table 2.
Corpus A: reviews for Harry Potter series book.
Corpus B: reviews for allkinds of books.
The column of word shows the average number of words in a review,and the column of sentences shows the average number of sentences in a review.sp-score Corpus A Corpus Breview words sentences review words sentences1 330 160.0 9.1 250 91.9 5.12 330 196.0 11.0 250 105.2 5.23 330 169.1 9.2 250 118.6 6.04 330 150.2 8.6 250 123.2 6.15 330 153.8 8.9 250 124.8 6.1reviews.
This method enables us to annotate sp-scores for arbitrary reviews suchas comments in bulletin board systems or blog systems.
We explore several typesof features beyond a bag-of-words to capture key phrases to determine sp-scores:n-grams and references (the words around the reviewed object).We conducted experiments with book reviews from amazon.com each ofwhich had a five-point scale rating along with text.
We compared pairwise sup-port vector machines (pSVMs) and SVR and found that SVR outperformedbetter than pSVMs by about 30% in terms of the squared error, which is closeto human performance.2 Related WorkRecent studies on sentiment classification focused on machine learning ap-proaches.
Pang [13] represents a review as a feature vector and estimates thepolarity with SVM, which is almost the same method as those for topic classifi-cation [1].
This paper basically follows this work, but we extend this task to amulti-order classification task.There have been many attempts to analyze reviews deeply to improve ac-curacy.
Mullen [10] used features from various information sources such as ref-erences to the ?work?
or ?artist?, which were annotated by hand, and showedthat these features have the potential to improve the accuracy.
We use referencefeatures, which are the words around the fixed review target word (book), whileMullen annotated the references by hand.316 D. Okanohara and J. TsujiiTurney [20] used semantic orientation, which measures the distance fromphrases to ?excellent?
or ?poor?
by using search engine results and gives the wordpolarity.
Kudo [8] developed decision stumps, which can capture substructuresembedded in text (such as word-based dependency), and suggested that subtreefeatures are important for opinion/modality classification.Independently of and in parallel with our work, two other papers considerthe degree of polarity for sentiment classification.
Koppel [6] exploited a neu-tral class and applied a regression method as ours.
Pang [12] applied a metriclabeling method for the task.
Our work is different from their works in severalrespects.
We exploited square errors instead of precision for the evaluation andused five distinct scores in our experiments while Koppel used three and Pangused three/four distinct scores in their experiments.3 Analyzing Reviews with Polarity ScoresIn this section we present a novel task setting where we predict the degree ofsentiment polarity of a review.
We first present the definition of sp-scores andthe task of assigning them to review documents.
We then explain an evaluationdata set.
Using this data set, we examined the human performance for this taskto clarify the difficulty of quantifying polarity.3.1 Sentiment Polarity ScoresWe extend the sentiment classification task to the more challenging task of as-signing rating scores to reviews.
We call this score the sp-score.
Examples ofsp-scores include five-star and scores out of 100.
Let sp-scores take discrete val-ues1 in a closed interval [min...max].
The task is to assign correct sp-scores tounseen reviews as accurately as possible.
Let y?
be the predicted sp-score andy be the sp-score assigned by the reviewer.
We measure the performance of anestimator with the mean square error:1n?ni=1(y?i ?
yi)2, (1)where (x1, y1), ..., (xn, yn) is the test set of reviews.
This measure gives a largepenalty for large mistakes, while ordered multi-class classification gives equalpenalties to any types of mistakes.3.2 Evaluation DataWe used book reviews on amazon.com for evaluation data2 3.
Each review hasstars assigned by the reviewer.
The number of stars ranges from one to five:1 We could allow sp-scores to have continuous values.
However, in this paper we assumesp-scores take only discrete values since the evaluation data set was annotated byonly discrete values.2 http://www.amazon.com3 These data were gathered from google cache using google API.Assigning Polarity Scores to Reviews 317one indicates the worst and five indicates the best.
We converted the numberof stars into sp-scores {1, 2, 3, 4, 5} 4.
Although each review may include severalparagraphs, we did not exploit paragraph information.From these data, we made two data sets.
The first was a set of reviews forbooks in the Harry Potter series (Corpus A).
The second was a set of reviews forbooks of arbitrary kinds (Corpus B).
It was easier to predict sp-scores for CorpusA than Corpus B because Corpus A books have a smaller vocabulary and eachreview was about twice as large.
To create a data set with a uniform score distri-bution (the effect of skewed class distributions is out of the scope of this paper),we selected 330 reviews per sp-score for Corpus A and 280 reviews per sp-scorefor Corpus B 5.
Table 2 shows the number of words and sentences in the cor-pora.
There is no significant difference in the average number of words/sentencesamong different sp-scores.Table 3.
Human performance of sp-score estimation.
Test data: 100 reviews of CorpusA with 1,2,3,4,5 sp-score.Square errorHuman 1 0.77Human 2 0.79Human average 0.78cf.
Random 3.20All3 2.00Table 4.
Results of sp-score estimation: Human 1 (left) and Human 2 (right)Assigned1 2 3 4 5 TotalCorrect1 12 7 0 1 0 202 7 8 4 1 0 203 1 1 13 5 0 204 0 0 4 10 6 205 0 1 2 7 10 20Total 20 17 23 24 16 100Assigned1 2 3 4 5 totalCorrect1 16 3 0 1 0 202 11 5 3 1 0 203 2 5 7 4 2 204 0 1 2 1 16 205 0 0 0 2 18 20Total 29 14 12 9 36 1003.3 Preliminary Experiments: Human Performance for AssigningSp-scoresWe treat the sp-scores assigned by the reviewers as correct answers.
However, thecontent of a review and its sp-score may not be related.
Moreover, sp-scores mayvary depending on the reviewers.
We examined the universality of the sp-score.4 One must be aware that different scales may reflect the different reactions than justscales as Keller indicated [17].5 We actually corrected 25000 reviews.
However, we used only 2900 reviews since thenumber of reviews with 1 star is very small.
We examined the effect of the numberof training data is discussed in 5.3.318 D. Okanohara and J. TsujiiWe asked two researchers of computational linguistics independently to assignan sp-score to each review from Corpus A.
We first had them learn the relation-ship between reviews and sp-scores using 20 reviews.
We then gave them 100reviews with uniform sp-score distribution as test data.
Table 3 shows the resultsin terms of the square error.
The Random row shows the performance achievedby random assignment, and the All3 row shows the performance achieved byassigning 3 to all the reviews.
These results suggest that sp-scores would beestimated with 0.78 square error from only the contents of reviews.Table 4 shows the distribution of the estimated sp-scores and correct sp-scores.
In the table we can observe the difficulty of this task: the precise quantifi-cation of sp-scores.
For example, human B tended to overestimate the sp-scoreas 1 or 5.
We should note that if we consider this task as binary classifica-tion by treating the reviews whose sp-scores are 4 and 5 as positive examplesand those with 1 and 2 as negative examples (ignoring the reviews whose sp-scores are 3), the classification precisions by humans A and B are 95% and 96%respectively.4 Assigning Sp-scores to ReviewsThis section describes a machine learning approach to predict the sp-scores ofreview documents.
Our method consists of the following two steps: extraction offeature vectors from reviews and estimation of sp-scores by the feature vectors.The first step basically uses existing techniques for document classification.
Onthe other hand, the prediction of sp-scores is different from previous studiesbecause we consider ordered multi-class classification, that is, each sp-score hasits own class and the classes are ordered.
Unlike usual multi-class classification,large mistakes in terms of the order should have large penalties.
In this paper,we discuss two methods of estimating sp-scores: pSVMs and SVR.4.1 Review RepresentationWe represent a review as a feature vector.
Although this representation ignoresthe syntactic structure, word positions, and the order of words, it is known towork reasonably well for many tasks such as information retrieval and documentclassification.
We use binary, tf, and tf-idf as feature weighting methods [15].The feature vectors are normalized to have L2 norm 1.4.2 Support Vector RegressionSupport vector regression (SVR) is a method of regression that shares the un-derlying idea with SVM [3,16].
SVR predicts the sp-score of a review by thefollowing regression:f : Rn ?
R, y = f(x) = ?w ?
x?
+ b.
(2)Assigning Polarity Scores to Reviews 319SVR uses an -insensitive loss function.
This loss function means that all errorsinside the  cube are ignored.
This allows SVR to use few support vectors andgives generalization ability.
Given a training set, (x1, y1), ...., (xn, yn), parame-ters w and b are determined by:minimize 12 ?w ?
w?
+ C?ni=1(?i + ?
?i )subject to (?w ?
xi?
+ b) ?
yi ?
 + ?iyi ?
(?w ?
xi?
+ b) ?
 + ??i?(?
)i ?
0 i = 1, ..., n. (3)The factor C > 0 is a parameter that controls the trade-off between trainingerror minimization and margin maximization.
The loss in training data increasesas C becomes smaller, and the generalization is lost as C becomes larger.
More-over, we can apply a kernel-trick to SVR as in the case with SVMs by using akernel function.This approach captures the order of classes and does not suffer from datasparseness.
We could use conventional linear regression instead of SVR [4].
Butwe use SVR because it can exploit the kernel-trick and avoid over-training.Another good characteristic of SVR is that we can identify the features con-tributing to determining the sp-scores by examining the coefficients (w in (2)),while pSVMs does not give such information because multiple classifiers are in-volved in determining final results.
A problem in this approach is that SVRcannot learn non-linear regression.
For example, when given training data are(x = 1, y = 1), (x = 2, y = 2), (x = 3, y = 8), SVR cannot perform regressioncorrectly without adjusting the feature values.4.3 Pairwise Support Vector MachinesWe apply a multi-class classification approach to estimating sp-scores.
pSVMs[7] considers each sp-score as a unique class and ignores the order among theclasses.
Given reviews with sp-scores {1, 2, .., m}, we construct m ?
(m ?
1)/2SVM classifiers for all the pairs of the possible values of sp-scores.
The classifierfor a sp-score pair (avsb) assigns the sp-score to a review with a or b.
The classlabel of a document is determined by majority voting of the classifiers.
Ties inthe voting are broken by choosing the class that is closest to the neutral sp-score(i.e, (1 + m)/2).This approach ignores the fact that sp-scores are ordered, which causes thefollowing two problems.
First, it allows large mistakes.
Second, when the numberof possible values of the sp-score is large (e.g, n > 100), this approach suffersfrom the data sparseness problem.
Because pSVMs cannot employ examples thathave close sp-scores (e.g, sp-score = 50) for the classification of other sp-scores(e.g, the classifier for a sp-score pair (51vs100)).4.4 Features Beyond Bag-of-WordsPrevious studies [9,2] suggested that complex features do not work as expectedbecause data become sparse when such features are used and a bag-of-words320 D. Okanohara and J. TsujiiTable 5.
Feature list for experimentsFeatures Description Example in Fig.1 review 1unigram single word (I) (believe) .. (series)bigram pair of two adjacent words (I believe) ... (the series)trigram adjacent three words (I believe this) ... (in the series)inbook words in a sentence including ?book?
(I) (can?t) ... (series)aroundbook words near ?book?
within two words.
(the) (next) (in) (the)approach is enough to capture the information in most reviews.
Nevertheless,we observed that reviews include many chunks of words such as ?very good?
or?must buy?
that are useful for estimating the degree of polarity.
We confirmedthis observation by using n-grams.
Since the words around the review targetmight be expected to influence the whole sp-score more than other words, we usethese words as features.
We call these features reference.
We assume the reviewtarget is only the word ?book?, and we use ?inbook?
and ?aroundbook?
features.The ?inbook?
features are the words appear in the sentences which include theword ?book?.
The ?around book?
are the words around the word ?book?
withintwo words.
Table 5 summarizes the feature list for the experiments.5 ExperimentsWe performed two series of experiments.
First, we compared pSVMs and SVR.Second, we examined the performance of various features and weighting methods.We used Corpus A/B introduced in Sec.
3.2 for experiment data.
We removedall HTML tags and punctuation marks beforehand.
We also applied the Porterstemming method [14] to the reviews.We divided these data into ten disjoint subsets, maintaining the uniformclass distribution.
All the results reported below are the average of ten-fold cross-validation.
In SVMs and SVR, we used SVMlight6 with the quadratic polynomialkernel K(x, z) = (?x ?
z?
+ 1)2 and set the control parameter C to 100 in all theexperiments.5.1 Comparison of pSVMs and SVRWe compared pSVMs and SVR to see differences in the properties of the regres-sion approach compared with those of the classification approach.
Both pSVMsand SVR used unigram/tf-idf to represent reviews.
Table 6 shows the squareerror results for SVM, SVR and a simple regression (least square error) methodfor Corpus A/B.
These results indicate that SVR outperformed SVM in termsof the square error and suggests that regression methods avoid large mistakesby taking account of the fact that sp-scores are ordered, while pSVMs does not.We also note that the result of a simple regression method is close to the resultof SVR with a linear kernel.6 http://svmlight.joachims.org/Assigning Polarity Scores to Reviews 321Table 6.
Comparison of multi-class SVM and SVR.
Both use unigram/tf-idf.Square errorMethods Corpus A Corpus BpSVMs 1.32 2.13simple regression 1.05 1.49SVR (linear kernel) 1.01 1.46SVR (polynomial kernel (?x ?
z?
+ 1)2) 0.94 1.38Figure 1 shows the distribution of estimation results for humans (top left:human 1, top right: human 2), pSVMs (below left), and SVR (below right).
Thehorizontal axis shows the estimated sp-scores and the vertical axis shows thecorrect sp-scores.
Color density indicates the number of reviews.
These figuressuggest that pSVMs and SVR could capture the gradualness of sp-scores betterthan humans could.
They also show that pSVMs cannot predict neutral sp-scoreswell, while SVR can do so well.5.2 Comparison of Different FeaturesWe compared the different features presented in Section 4.4 and feature weight-ing methods.
First we compared different weighting methods.
We used onlyunigram features for this comparison.
We then compared different features.
Weused only tf-idf weighting methods for this comparison.Table 7 summarizes the comparison results of different feature weightingmethods.
The results show that tf-idf performed well on both test corpora.We should note that simple representation methods, such as binary or tf, givecomparable results to tf-idf, which indicates that we can add more complexfeatures without considering the scale of feature values.
For example, when weadd word-based dependency features, we have some difficulty in adjusting thesefeature values to those of unigrams.
But we could use these features together inbinary weighting methods.Table 8 summarizes the comparison results for different features.
For CorpusA, unigram + bigram and unigram + trigram achieved high performance.
The per-formance of unigram + inbook was not good, which is contrary to our intuition thatthe words that appear around the target object are more important than others.For Corpus B, the results was different, that is, n-gram features could not predictthe sp-scores well.
This is because the variety of words/phrases was much largerthan in Corpus A and n-gram features may have suffered from the data sparsenessproblem.
We should note that these feature settings are too simple, and we cannotaccept the result of reference or target object (inbook/aroundbook) directly.Note that the data used in the preliminary experiments described in Section3.3 are a part of Corpus A.
Therefore we can compare the results for humanswith those for Corpus A in this experiment.
The best result by the machinelearning approach (0.89) was close to the human results (0.78).To analyze the influence of n-gram features, we used the linear kernelk(x, z):= ?x ?
z?
in SVR training.
We used tf-idf as feature weighting.
We then322 D. Okanohara and J. Tsujii1 2 3 4 512345estimated sp-scorec orre ct sp-score10-128-106-84-62-40-21 2 3 4 512345estimated sp-scorecorrect sp-score16-1814-1612-1410-18-106-84-62-40-21 2 3 4 512345estimated sp-scorec orre ct sp-score14.0 -16.012.0 -14.010.0 -12.08.0 -10.06.0 -8.04.0 -6.02.0 -4.00.0 -2.01 2 3 4 512345estimated sp-scorecorrect sp-score14.0 -16.012.0 -14.010.0 -12.08.0 -10.06.0 -8.04.0 -6.02.0 -4.00.0 -2.0Fig.
1.
Distribution of estimation results.
Color density indicates the number of re-views.
Top left: Human A, top right: Human B, below left: pSVMs, below right: SVR.examined each coefficient of regression.
Since we used the linear kernel, the co-efficient value of SVR showed the polarity of a single feature, that is, this valueexpressed how much the occurrence of a feature affected the sp-score.
Tables 9shows the coefficients resulting from the training of SVR.
These results showthat neutral polarity words themselves, such as ?all?
and ?age?, will affect theoverall sp-scores of reviews with other neutral polarity words, such as, ?all ag(age)?, ?can?t wait?, ?on (one) star?, and ?not interest?.5.3 Learning CurveWe generated learning curves to examine the effect of the size of training data onthe performance.
Figure 2 shows the results of a classification task using unigram/tf-idf to represent reviews.
The results suggest that the performance can still beimproved by increasing the training data.Assigning Polarity Scores to Reviews 323Table 7.
Comparison results of different feature weighting methods.
We used unigramsas features of reviews.Square errorWeighting methods (unigram) Corpus A Corpus Btf 1.03 1.49tf-idf 0.94 1.38binary 1.04 1.47Table 8.
Comparison results of different features.
For comparison of different featureswe tf-idf as weighting methods.Square errorFeature (tf-idf) Corpus A Corpus Bunigram (baseline) 0.94 1.38unigram + bigram 0.89 1.41unigram + trigram 0.90 1.42unigram + inbook 0.97 1.36unigram + aroundbook 0.93 1.37Table 9.
List of bigram features that have ten best/worst polarity values estimated bySVR in Corpus A/B.
The column of pol expresses the estimated sp-score of a feature,i.e., only this feature is fired in a feature vector.
(word stemming was applied)Corpus A (best) Corpus B (best)pol bigram pol bigram1.73 best book 1.64 the best1.69 is a 1.60 read it1.49 read it 1.37 a great1.44 all ag 1.34 on of1.30 can?t wait 1.31 fast food1.20 it is 1.22 harri potter1.14 the sorcer?s 1.19 highli recommend1.14 great !
1.14 an excel1.13 sorcer?s stone 1.12 to read1.11 come out 1.01 in theCorpus A (worst) Corpus B (worst)pol bigram pol bigram-1.61 at all -1.19 veri disappoint-1.50 wast of -1.13 wast of-1.38 potter book -0.98 the worst-1.36 out of -0.97 is veri-1.28 not interest -0.96 !
!-1.18 on star -0.85 i am-1.14 the worst -0.81 the exampl-1.13 first four -0.79 bui it-1.11 a wast -0.76 veri littl-1.08 no on -0.74 onli to6 ConclusionIn this paper, we described a novel task setting in which we predicted sp-scores- degree of polarity - of reviews.
We proposed a machine learning method usingSVR to predict sp-scores.We compared two methods for estimating sp-scores: pSVMs and SVR.
Exper-imental results with book reviews showed that SVR performed better in termsof the square error than pSVMs by about 30%.
This result agrees with our324 D. Okanohara and J. Tsujii00.511.522.530 50 100 150 200 250 300 350A number of training reviews per sp-scoreS qua ree rrorCorpus ACorpus BFig.
2.
Learning curve for our task setting for Corpus A and Corpus B.
We used SVRas the classifier and unigram/tf-idf to represent of reviews.intuition that pSVMs does not consider the order of sp-scores, while SVR cap-tures the order of sp-scores and avoids high penalty mistakes.
With SVR, sp-scores can be estimated with a square error of 0.89, which is very close to thesquare error achieved by human (0.78).We examined the effectiveness of features beyond a bag-of-words and refer-ence features (the words around the reviewed objects.)
The results suggest thatn-gram features and reference features contribute to improve the accuracy.As the next step in our research, we plan to exploit parsing results such aspredicate argument structures for detecting precise reference information.
Wewill also capture other types of polarity than attitude, such as modality andwriting position [8], and we will consider estimating these types of polarity.We plan to develop a classifier specialized for ordered multi-class classifica-tion using recent studies on machine learning for structured output space [19,18]or ordinal regression [5] because our experiments suggest that both pSVMs andSVR have advantages and disadvantages.
We will develop a more efficient clas-sifier that outperforms pSVMs and SVR by combining these ideas.References1.
T .
Joachims.
Learning to Classify Text Using Support Vector Machines.
Kluwer,2002.2.
C. Apte, F. Damerau, and S. Weiss.
Automated learning of decision rules for textcategorization.
Information Systems, 12(3):233?251, 1994.3.
N. Cristianini and J. S. Taylor.
An Introduction to Support Vector Machines andother Kernel-based Learning Methods.
Cambridge University Press, 2000.4.
T. Hastie, R. Tibshirani, and J. Friedman.
The Elements of Statistical Learning.Springer, 2001.5.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
Large margin rank bound-aries for ordinal regression.
In Advances in Large Margin Classifiers, pages 115?132.MIT press, 2000.Assigning Polarity Scores to Reviews 3256.
Moshe Koppel and Jonathan Schler.
The importance of neutral examples for learn-ing sentiment.
In In Workshop on the Analysis of Informal and Formal InformationExchange during Negotiations (FINEXIN), 2005.7.
U. Kresel.
Pairwise Classification and Support Vector Machines Methods.
MITPress, 1999.8.
T. Kudo and Y. Matsumoto.
A boosting algorithm for classification of semi-structured text.
In Proceedings of the 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 301?308, 2004.9.
D. Lewis.
An evaluation of phrasal and clustered representations on a text cate-gorization task.
In Proceedings of SIGIR-92, 15th ACM International Conferenceon Research and Development in Information Retrieval, pages 37?50, 1992.10.
A. Mullen and N. Collier.
Sentiment analysis using Support Vector Machines withdiverse information sources.
In Proceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL), 2004.11.
B. Pang and L. Lee.
A sentimental education: Sentiment analysis using subjectivitysummarization based on minimum cuts.
In Proceedings of the 42nd Meeting of theAssociation for Computational Linguistics (ACL), pages 271?278, 2004.12.
B. Pang and L. Lee.
Seeing stars: Exploiting class relationships for sentimentcategorization with respect to rating scales.
In Proceedings of the 43nd Meeting ofthe Association for Computational Linguistics (ACL), 2005.13.
B. Pang, L. Lee, and S. Vaithyanathan.
Thumbs up?
sentiment classification usingmachine learning techniques.
In Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages 79?86, 2002.14.
M.F.
Porter.
An algorithm for suffix stripping, program.
Program, 14(3):130?137,1980.15.
F. Sebastiani.
Machine learning in automated text categorization.
ACM ComputingSurveys, 34(1):1?47, 2002.16.
A. Smola and B. Sch.
A tutorial on Support Vector Regression.
Technical report,NeuroCOLT2 Technical Report NC2-TR-1998-030, 1998.17.
Antonella Sorace and Frank Keller.
Gradience in linguistic data.
Lingua,115(11):1497?1524, 2005.18.
B. Taskar.
Learning Structured Prediction Models: A Large Margin Approach.
PhDthesis, Stanford University, 2004.19.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
Support vector machinelearning for interdependent and structured output spaces.
In Machine Learning,Proceedings of the Twenty-first International Conference (ICML), 2004.20.
P. D. Turney.
Thumbs up or thumbs down?
semantic orientation applied to un-supervised classification of reviews.
In Proceedings of the 40th Meeting of theAssociation for Computational Linguistics (ACL), pages 417?424, 2002.21.
V. Vapnik.
The Nature of Statistical Learning Theory.
Springer, 1995.
