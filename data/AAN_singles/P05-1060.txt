Proceedings of the 43rd Annual Meeting of the ACL, pages 483?490,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMulti-Field Information Extraction and Cross-Document FusionGideon S. Mann and David YarowskyDepartment of Computer ScienceThe Johns Hopkins UniversityBaltimore, MD 21218 USA{gsm,yarowsky}@cs.jhu.eduAbstractIn this paper, we examine the task of extracting aset of biographic facts about target individuals froma collection of Web pages.
We automatically anno-tate training text with positive and negative exam-ples of fact extractions and train Rote, Na?
?ve Bayes,and Conditional Random Field extraction modelsfor fact extraction from individual Web pages.
Wethen propose and evaluate methods for fusing theextracted information across documents to return aconsensus answer.
A novel cross-field bootstrappingmethod leverages data interdependencies to yieldimproved performance.1 IntroductionMuch recent statistical information extraction re-search has applied graphical models to extract in-formation from one particular document after train-ing on a large corpus of annotated data (Leek, 1997;Freitag and McCallum, 1999).1 Such systems arewidely applicable, yet there remain many informa-tion extraction tasks that are not readily amenable tothese methods.
Annotated data required for trainingstatistical extraction systems is sometimes unavail-able, while there are examples of the desired infor-mation.
Further, the goal may be to find a few inter-related pieces of information that are stated multipletimes in a set of documents.Here, we investigate one task that meets the abovecriteria.
Given the name of a celebrity such as1Alternatively, Riloff (1996) trains on in-domain andout-of-domain texts and then has a human filtering step.Huffman (1995) proposes a method to train a different type ofextraction system by example.
?Frank Zappa?, our goal is to extract a set of bio-graphic facts (e.g., birthdate, birth place and occupa-tion) about that person from documents on the Web.First, we describe a general method of automaticannotation for training from positive and negativeexamples and use the method to train Rote, Na?
?veBayes, and Conditional Random Field models (Sec-tion 2).
We then examine how multiple extractionscan be combined to form one consensus answer(Section 3).
We compare fusion methods and showthat frequency voting outperforms the single high-est confidence answer by an average of 11% acrossthe various extractors.
Increasing the number of re-trieved documents boosts the overall system accu-racy as additional documents which mention the in-dividual in question lead to higher recall.
This im-proved recall more than compensates for a loss inper-extraction precision from these additional doc-uments.
Next, we present a method for cross-fieldbootstrapping (Section 4) which improves per-fieldaccuracy by 7%.
We demonstrate that a small train-ing set with only the most relevant documents can beas effective as a larger training set with additional,less relevant documents (Section 5).2 Training by Automatic AnnotationTypically, statistical extraction systems (such asHMMs and CRFs) are trained using hand-annotateddata.
Annotating the necessary data by hand is time-consuming and brittle, since it may require large-scale re-annotation when the annotation schemechanges.
For the special case of Rote extrac-tors, a more attractive alternative has been proposedby Brin (1998), Agichtein and Gravano (2000), andRavichandran and Hovy (2002).483Essentially, for any text snippet of the formA1pA2qA3, these systems estimate the probabilitythat a relationship r(p, q) holds between entities pand q, given the interstitial context, as2P (r(p, q) | pA2q) = P (r(p, q) | pA2q)=?x,y?T c(xA2y)?x c(xA2)That is, the probability of a relationship r(p, q) isthe number of times that pattern xA2y predicts anyrelationship r(x, y) in the training set T .
c(.)
is thecount.
We will refer to x as the hook3 and y as thetarget.
In this paper, the hook is always an indi-vidual.
Training a Rote extractor is straightforwardgiven a set T of example relationships r(x, y).
Foreach hook, download a separate set of relevant doc-uments (a hook corpus, Dx) from the Web.4 Thenfor any particular pattern A2 and an element x, counthow often the pattern xA2 predicts y and how oftenit retrieves a spurious y?.5This annotation method extends to training otherstatistical models with positive examples, for exam-ple a Na?
?ve Bayes (NB) unigram model.
In thismodel, instead of looking for an exact A2 patternas above, each individual word in the pattern A2 isused to predict the presence of a relationship.P (r(p, q) | pA2q)?P (pA2q | r(p, q))P (r(p, q))=P (A2 | r(p, q))=?a?A2P (a | r(p, q))We perform add-lambda smoothing for out-of-vocabulary words and thus assign a positive prob-ability to any sequence.
As before, a set of relevant2The above Rote models also condition on the preceding andtrailing words, for simplicity we only model interstitial wordsA2.3Following (Ravichandran and Hovy, 2002).4In the following experiments we assume that there is onemain object of interest p, for whom we want to find certainpieces of information r(p, q), where r denotes the type of re-lationship (e.g., birthday) and q is a value (e.g., May 20th).
Werequire one hook corpus for each hook, not a separate one foreach relationship.5Having a functional constraint ?q?
6= q, r?
(p, q?)
makes thisestimate much more reliable, but it is possible to use this methodof estimation even when this constraint does not hold.documents is downloaded for each particular hook.Then every hook and target is annotated.
From thatmarkup, we can pick out the interstitial A2 patternsand calculate the necessary probabilities.Since the NB model assigns a positive probabilityto every sequence, we need to pick out likely tar-gets from those proposed by the NB extractor.
Weconstruct a background model which is a basic un-igram language model, P (A2) =?a?A2 P (a).
Wethen pick targets chosen by the confidence estimateCNB(q) = logP (A2 | r(p, q))P (A2)However, this confidence estimate does not work-well in our dataset.We propose to use negative examples to estimateP (A2 | r?
(p, q))6 as well as P (A2 | r(p, q)).
Foreach relationship, we define the target set Er to beall potential targets and model it using regular ex-pressions.7 In training, for each relationship r(p, q),we markup the hook p, the target q, and all spuri-ous targets (q?
?
{Er ?
q}) which provide negativeexamples.
Targets can then be chosen with the fol-lowing confidence estimateCNB+E(q) = logP (A2 | r(p, q))P (A2 | r?
(p, q))We call this NB+E in the following experiments.The above process describes a general method forautomatically annotating a corpus with positive andnegative examples, and this corpus can be used totrain statistical models that rely on annotated data.8In this paper, we test automatic annotation usingConditional Random Fields (CRFs) (Lafferty et al,2001) which have achieved high performance for in-formation extraction.
CRFs are undirected graphicalmodels that estimate the conditional probability of astate sequence given an output sequenceP (s | o) =1Zexp( T?t=1?k?kfk(st?1, st, o, t))6r?
stands in for all other possible relationships (including norelationship) between p and q. P (A2 | r?
(p, q)) is estimated asP (A2 | r(p, q)) is, except with spurious targets.7e.g., Ebirthyear = {\d\d\d\d}.
This is the only source ofhuman knowledge put into the system and required only around4 hours of effort, less effort than annotating an entire corpus orwriting information extraction rules.8This corpus markup gives automatic annotation that yieldsnoisier training data than manual annotation would.484p qA_2BpA_2A_2qBqFigure 1: CRF state-transition graphs for extracting a relation-ship r(p, q) from a sentence pA2q.
Left: CRF Extraction witha background model (B).
Right: CRF+E As before but withspurious target prediction (pA2q?
).We use the Mallet system (McCallum, 2002) fortraining and evaluation of the CRFs.
In order to ex-amine the improvement by using negative examples,we train CRFs with two topologies (Figure 1).
Thefirst, CRF, models the target relationship and back-ground sequences and is trained on a corpus wheretargets (positive examples) are annotated.
The sec-ond, CRF+E, models the target relationship, spu-rious targets and background sequences, and it istrained on a corpus where targets (positive exam-ples) as well as spurious targets (negative examples)are annotated.Experimental ResultsTo test the performance of the different ex-tractors, we collected a set of 152 semi-structured mini-biographies from an online site(www.infoplease.com), and used simple rules toextract a biographic fact database of birthday andmonth (henceforth birthday), birth year, occupation,birth place, and year of death (when applicable).An example of the data can be found in Table1.
In our system, we normalized birthdays, andperformed capitalization normalization for theremaining fields.
We did no further normalization,such as normalizing state names to their two letteracronyms (e.g., California ?
CA).
Fifteen nameswere set aside as training data, and the rest wereused for testing.
For each name, 150 documentswere downloaded from Google to serve as the hookcorpus for either training or testing.9In training, we automatically annotated docu-ments using people in the training set as hooks, andin testing, tried to get targets that exactly matchedwhat was present in the database.
This is a very strictmethod of evaluation for three reasons.
First, sincethe facts were automatically collected, they contain9Name polyreference, along with ranking errors, result inthe retrieval of undesired documents.Aaron Neville Frank ZappaBirthday January 24 December 21Birth year 1941 1940Occupation Singer MusicianBirthplace New Orleans Baltimore,MarylandYear of Death - 1993Table 1: Two of 152 entries in the Biographic Database.
Eachentry contains incomplete information about various celebrities.Here, Aaron Neville?s birth state is missing, and Frank Zappacould be equally well described as a guitarist or rock-star.errors and thus the system is tested against wronganswers.10 Second, the extractors might have re-trieved information that was simply not present inthe database but nevertheless correct (e.g., some-one?s occupation might be listed as writer and theretrieved occupation might be novelist).
Third, sincethe retrieved targets were not normalized, there sys-tem may have retrieved targets that were correct butwere not recognized (e.g., the database birthplace isNew York, and the system retrieves NY).In testing, we rejected candidate targets that werenot present in our target set models Er.
In somecases, this resulted in the system being unable to findthe correct target for a particular relationship, sinceit was not in the target set.Before fusion (Section 3), we gathered all thefacts extracted by the system and graded them in iso-lation.
We present the per-extraction precisionPre-Fusion Precision = # Correct Extracted Targets# Total Extracted TargetsWe also present the pseudo-recall, which is the av-erage number of times per person a correct targetwas extracted.
It is difficult to calculate true re-call without manual annotation of the entire corpus,since it cannot be known for certain how many timesthe document set contains the desired information.11Pre-Fusion Pseudo-Recall = # Correct Extracted Targets#PeopleThe precision of each of the various extractionmethods is listed in Table 2.
The data show thaton average the Rote method has the best precision,10These deficiencies in testing also have implications fortraining, since the models will be trained on annotated data thathas errors.
The phenomenon of missing and inaccurate datawas most prevalent for occupation and birthplace relationships,though it was observed for other relationships as well.11It is insufficient to count all text matches as instances thatthe system should extract.
To obtain the true recall, it is nec-essary to decide whether each sentence contains the desired re-lationship, even in cases where the information is not what thebiographies have listed.485Birthday Birth year Occupation Birthplace Year of Death Avg.Rote .789 .355 .305 .510 .527 .497NB+E .423 .361 .255 .217 .088 .269CRF .509 .342 .219 .139 .267 .295CRF+E .680 .654 .246 .357 .314 .450Table 2: Pre-Fusion Precision of extracted facts for various extraction systems, trained on 15 people each with 150 documents, andtested on 137 people each with 150 documents.Birthday Birth year Occupation Birthplace Year of Death Avg.Rote 4.8 1.9 1.5 1.0 0.1 1.9NB+E 9.6 11.5 20.3 11.3 0.7 10.9CRF 3.0 16.3 31.1 10.7 3.2 12.9CRF+E 6.8 9.9 3.2 3.6 1.4 5.0Table 3: Pre-Fusion Pseudo-Recall of extract facts with the identical training/testing set-up as above.while the NB+E extractor has the worst.
Train-ing the CRF with negative examples (CRF+E) gavebetter precision in extracted information then train-ing it without negative examples.
Table 3 lists thepseudo-recall or average number of correctly ex-tracted targets per person.
The results illustrate thatthe Rote has the worst pseudo-recall, and the plainCRF, trained without negative examples, has the bestpseudo-recall.To test how the extraction precision changes asmore documents are retrieved from the ranked re-sults from Google, we created retrieval sets of 1, 5,15, 30, 75, and 150 documents per person and re-peated the above experiments with the CRF+E ex-tractor.
The data in Figure 2 suggest that there is agradual drop in extraction precision throughout thecorpus, which may be caused by the fact that doc-uments further down the retrieved list are less rele-vant, and therefore less likely to contain the relevantbiographic data.Pre?Fusion Precision# Retrieved Documents per Person 80  160 1400.90.80.70.60.50.40.30.2160 40 20  120 0  100BirthdayBirthplaceBirthyearOccupationDeathyearFigure 2: As more documents are retrieved per person, pre-fusion precision drops.However, even though the extractor?s precisiondrops, the data in Figure 3 indicate that there con-tinue to be instances of the relevant biographic data.# Retrieved Documents Per PersonPre?Fusion Pseudo?Recall1 2 34 5 67 8 91000  20  40  60  80  100  120 140160BirthyearBirthdayBirthplaceOccupationDeathyearFigure 3: Pre-fusion pseudo-recall increases as more documentsare added.3 Cross-Document Information FusionThe per-extraction performance was presented inSection 2, but the final task is to find the single cor-rect target for each person.12 In this section, we ex-amine two basic methodologies for combining can-didate targets.
Masterson and Kushmerick (2003)propose Best which gives each candidate ascore equal to its highest confidence extraction:Best(x) = argmaxxC(x).13 We further considerVoting, which counts the number of times each can-didate x was extracted: Vote(x) = |C(x) > 0|.Each of these methods ranks the candidate targetsby score and chooses the top-ranked one.The experimental setup used in the fusion exper-iments was the same as before: training on 15 peo-ple, and testing on 137 people.
However, the post-fusion evaluation differs from the pre-fusion evalua-tion.
After fusion, the system returns one consensustarget for each person and thus the evaluation is onthe accuracy of those targets.
That is, missing tar-12This is a simplifying assumption, since there are manycases where there might exist multiple possible values, e.g., aperson may be both a writer and a musician.13C(x) is either the confidence estimate (NB+E) or the prob-ability score (Rote,CRF,CRF+E).486Best VoteRote .364 .450NB+E .385 .588CRF .513 .624CRF+E .650 .678Table 4: Average Accuracy of the Highest Confidence (Best)and Most Frequent (Vote) across five extraction fields.gets are graded as wrong.14Post-Fusion Accuracy = # People with Correct Target# PeopleAdditionally, since the targets are ranked, we alsocalculated the mean reciprocal rank (MRR).15 Thedata in Table 4 show the average system perfor-mance with the different fusion methods.
Frequencyvoting gave anywhere from a 2% to a 20% improve-ment over picking the highest confidence candidate.CRF+E (the CRF trained with negative examples)was the highest performing system overall.Birth DayFusion Accuracy Fusion MRRRote Vote .854 .877NB+E Vote .854 .889CRF Vote .650 .703CRF+E Vote .883 .911Birth yearRote Vote .387 .497NB+E Vote .778 .838CRF Vote .796 .860CRF+E Vote .869 .876OccupationRote Vote .299 .405NB+E Vote .642 .751CRF Vote .606 .740CRF+E Vote .423 .553BirthplaceRote Vote .321 .338NB+E Vote .474 .586CRF Vote .321 .476CRF+E Vote .467 .560Year of DeathRote Vote .389 .389NB+E Vote .194 .383CRF .750 .840CRF+E Vote .750 .827Table 5: Voting for information fusion, evaluated per person.CRF+E has best average performance (67.8%).Table 5 shows the results of using each of theseextractors to extract correct relationships from thetop 150 ranked documents downloaded from the14For year of death, we only graded cases where the personhad died.15The reciprocal rank = 1 / the rank of the correct target.Web.
CRF+E was a top performer in 3/5 of thecases.
In the other 2 cases, the NB+E was the mostsuccessful, perhaps because NB+E?s increased re-call was more useful than CRF+E?s improved pre-cision.Retrieval Set Size and PerformanceAs with pre-fusion, we performed a set of exper-iments with different retrieval set sizes and usedthe CRF+E extraction system trained on 150 docu-ments per person.
The data in Figure 4 show thatperformance improves as the retrieval set size in-creases.
Most of the gains come in the first 30 doc-uments, where average performance increased from14% (1 document) to 63% (30 documents).
Increas-ing the retrieval set size to 150 documents per personyielded an additional 5% absolute improvement.Post?FusionAccuracy# Retrieved Documents Per Person 00.1 0.2 0.30.4 0.5 0.60.7 0.8 0.90  2040  60  80  100 120 140 160 OccupationBirthyearBirthdayDeathyearBirthplaceFigure 4: Fusion accuracy increases with more documents perpersonPost-fusion errors come from two major sources.The first source is the misranking of correct relation-ships.
The second is the case where relevant infor-mation is not retrieved at all, which we measure asPost-Fusion Missing = # Missing Targets# PeopleThe data in Figure 5 suggest that the decrease inmissing targets is a significant contributing factorto the improvement in performance with increaseddocument size.
Missing targets were a major prob-lem for Birthplace, constituting more than half theerrors (32% at 150 documents).4 Cross-Field BootstrappingSections 2 and 3 presented methods for training sep-arate extractors for particular relationships and fordoing fusion across multiple documents.
In this sec-tion, we leverage data interdependencies to improveperformance.The method we propose is to bootstrap acrossfields and use knowledge of one relationship to im-prove performance on the extraction of another.
For487# Retrieved Documents Per PersonPost?FusionMissing Targets0 0.1 0.20.3 0.4 0.50.6 0.7 0.80.9 120 0  40  60  80  100 120 140 160BirthplaceOccupationDeathyearBirthdayBirthyearFigure 5: Additional documents decrease the number of post-fusion missing targets, targets which are never extracted in anydocument.Birth yearExtraction Precision Fusion AccuracyCRF .342 .797+ birthday .472 .861CRF+E .654 .869+ birthday .809 .891OccupationExtraction Precision Fusion AccuracyCRF .219 .606+ birthday .217 .569+ birth year(f) 21.9 .599+ all .214 .591CRF+E .246 .423+ birthday .325 .577+ birth year(f) .387 .672+ all .382 .642BirthplaceExtraction Precision Fusion AccuracyCRF .139 .321+ birthday .158 .372+ birth year(f) .156 .350CRF+E .357 .467+ birthday .350 .474+ birth year(f) .294 .350+ occupation(f) .314 .354+ all .362 .532Table 6: Performance of Cross-Field Bootstrapping Models.
(f) indicates that the best fused result was taken.
birth year(f)means birth years were annotated using the system that discov-ered the most accurate birth years.example, to extract birth year given knowledge ofthe birthday, in training we mark up each hook cor-pus Dx with the known birthday b : birthday(x, b)and the target birth year y : birthyear(x, y) andadd an additional feature to the CRF that indicateswhether the birthday has been seen in the sentence.16In testing, for each hook, we first find the birthdayusing the methods presented in the previous sec-tions, annotate the corpus with the extracted birth-day, and then apply the birth year CRF (see Figure 6next page).16The CRF state model doesn?t change.
When bootstrappingfrom multiple fields, we add the conjunctions of the fields asfeatures.Table 6 shows the effect of using this bootstrappeddata to estimate other fields.
Based on the relativeperformance of each of the individual extraction sys-tems, we chose the following schedule for perform-ing the bootstrapping: 1) Birthday, 2) Birth year, 3)Occupation, 4) Birthplace.
We tried adding in allknowledge available to the system at each point inthe schedule.17 There are gains in accuracy for birthyear, occupation and birthplace by using cross-fieldbootstrapping.
The performance of the plain CRF+Eaveraged across all five fields is 67.4%, while for thebest bootstrapped system it is 74.6%, a gain of 7%.Doing bootstrapping in this way improves forpeople whose information is already partially cor-rect.
As a result, the percentage of people whohave completely correct information improves to37% from 13.8%, a gain of 24% over the non-bootstrapped CRF+E system.
Additionally, erro-neous extractions do not hurt accuracy on extractionof other fields.
Performance in the bootstrapped sys-tem for birthyear, occupation and birth place whenthe birthday is wrong is almost the same as perfor-mance in the non-bootstrapped system.5 Training Set Size ReductionOne of the results from Section 2 is that lowerranked documents are less likely to contain the rel-evant biographic information.
While this does nothave an dramatic effect on the post-fusion accuracy(which improves with more documents), it suggeststhat training on a smaller corpus, with more relevantdocuments and more sentences with the desired in-formation, might lead to equivalent or improved per-formance.
In a final set of experiments we looked atsystem performance when the extractor is trained onfewer than 150 documents per person.The data in Figure 7 show that training on 30 doc-uments per person yields around the same perfor-mance as training on 150 documents per person.
Av-erage performance when the system was trained on30 documents per person is 70%, while average per-formance when trained on 150 documents per per-son is 68%.
Most of this loss in performance comesfrom losses in occupation, but the other relationships17This system has the extra knowledge of which fusedmethod is the best for each relationship.
This was assessed byinspection.488Frank Zappa was born on December 21.1.
BirthdayZappa : December 21, 1940.2.
Birthyear1.
Birthday2.
Birthyear 3.
BirthplaceZappa was born in 1940 in Baltimore.Figure 6: Cross-Field Bootstrapping: In step (1) The birthday,December 21, is extracted and the text marked.
In step 2, cooc-currences with the discovered birthday make 1940 a better can-didate for birthyear.
In step (3), the discovered birthyear ap-pears in contexts where the discovered birthday does not andimproves extraction of birth place.Post?FusionAccuracy# Training Documents Per Person 0.20.3 0.40.5 0.60.7 0.80.902040  60  80  100  120 140 160BirthdayBirthyearDeathyearOccupationBirthplaceFigure 7: Fusion accuracy doesn?t improve with more than 30training documents per person.have either little or no gain from training on addi-tional documents.
There are two possible reasonswhy more training data may not help, and even mayhurt performance.One possibility is that higher ranked retrieveddocuments are more likely to contain biographicalfacts, while in later documents it is more likely thatautomatically annotated training instances are in factfalse positives.
That is, higher ranked documents arecleaner training data.
Pre-Fusion precision results(Figure 8) support this hypothesis since it appearsthat later instances are often contaminating earliermodels.Pre?FusionPrecision# Training Documents Per Person 00.1 0.2 0.30.4 0.5 0.60.7 0.80  20  40  60  80  100 120 140 160BirthdayBirthyearBirthplaceOccupationDeathyearFigure 8: Pre-Fusion precision shows slight drops with in-creased training documents.The data in Figure 9 suggest an alternate possibil-ity that later documents also shift the prior towarda model where it is less likely that a relationship isobserved as fewer targets are extracted.Pre?FusionPseudo?Recall# Training Documents Per Person 01 2 34 5 67 8 910 11020  4060  80  100  120 140 160BirthdayBirthplaceDeathyearBirthyearOccupationFigure 9: Pre-Fusion Pseudo-Recall also drops with increasedtraining documents.6 Related WorkThe closest related work to the task of biographicfact extraction was done by Cowie et al (2000) andSchiffman et al (2001), who explore the problem ofbiographic summarization.There has been rather limited publishedwork in multi-document information extrac-tion.
The closest work to what we present here isMasterson and Kushmerick (2003), who performmulti-document information extraction trained onmanually annotated training data and use BestConfidence to resolve each particular template slot.In summarizarion, many systems have examinedthe multi-document case.
Notable systems areSUMMONS (Radev and McKeown, 1998) andRIPTIDE (White et al, 2001), which assume per-fect extracted information and then perform closeddomain summarization.
Barzilay et al (1999) doesnot explicitly extract facts, but instead picks outrelevant repeated elements and combines them toobtain a summary which retains the semantics ofthe original.In recent question answering research, informa-tion fusion has been used to combine multiplecandidate answers to form a consensus answer.Clarke et al (2001) use frequency of n-gram occur-rence to pick answers for particular questions.
An-other example of answer fusion comes in (Brill etal., 2001) which combines the output of multiplequestion answering systems in order to rank an-swers.
Dalmas and Webber (2004) use a WordNetcover heuristic to choose an appropriate locationfrom a large candidate set of answers.There has been a considerable amount of work intraining information extraction systems from anno-tated data since the mid-90s.
The initial work in thefield used lexico-syntactic template patterns learnedusing a variety of different empirical approaches(Riloff and Schmelzenbach, 1998; Huffman, 1995;489Soderland et al, 1995).
Seymore et al (1999) useHMMs for information extraction and explore waysto improve the learning process.Nahm and Mooney (2002) suggest a method tolearn word-to-word relationships across fields by do-ing data mining on information extraction results.Prager et al (2004) uses knowledge of birth year toweed out candidate years of death that are impos-sible.
Using the CRF extractors in our data set,this heuristic did not yield any improvement.
Moredistantly related work for multi-field extraction sug-gests methods for combining information in graphi-cal models across multiple extraction instances (Sut-ton et al, 2004; Bunescu and Mooney, 2004) .7 ConclusionThis paper has presented new experimental method-ologies and results for cross-document informationfusion, focusing on the task of biographic fact ex-traction and has proposed a new method for cross-field bootstrapping.
In particular, we have shownthat automatic annotation can be used effectivelyto train statistical information extractors such Na?
?veBayes and CRFs, and that CRF extraction accuracycan be improved by 5% with a negative examplemodel.
We looked at cross-document fusion anddemonstrated that voting outperforms choosing thehighest confidence extracted information by 2% to20%.
Finally, we introduced a cross-field bootstrap-ping method that improved average accuracy by 7%.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Extracting re-lations from large plain-text collections.
In Proceedings ofICDL, pages 85?94.R.
Barzilay, K. R. McKeown, and M. Elhadad.
1999.
Informa-tion fusion in the context of multi-document summarization.In Proceedings of ACL, pages 550?557.E.
Brill, J. Lin, M. Banko, S. Dumais, and A. Ng.
2001.
Data-intensive question answering.
In Proceedings of TREC,pages 183?189.S.
Brin.
1998.
Extracting patterns and relations from the worldwide web.
In WebDB Workshop at 6th International Confer-ence on Extending Database Technology, EDBT?98, pages172?183.R.
Bunescu and R. Mooney.
2004.
Collective information ex-traction with relational markov networks.
In Proceedings ofACL, pages 438?445.C.
L. A. Clarke, G. V. Cormack, and T. R. Lynam.
2001.
Ex-ploiting redundancy in question answering.
In Proceedingsof SIGIR, pages 358?365.J.
Cowie, S. Nirenburg, and H. Molina-Salgado.
2000.
Gener-ating personal profiles.
In The International Conference OnMT And Multilingual NLP.T.
Dalmas and B. Webber.
2004.
Information fusionfor answering factoid questions.
In Proceedings of 2ndCoLogNET-ElsNET Symposium.
Questions and Answers:Theoretical Perspectives.D.
Freitag and A. McCallum.
1999.
Information extractionwith hmms and shrinkage.
In Proceedings of the AAAI-99Workshop on Machine Learning for Information Extraction,pages 31?36.S.
B. Huffman.
1995.
Learning information extraction patternsfrom examples.
In Working Notes of the IJCAI-95 Workshopon New Approaches to Learning for Natural Language Pro-cessing, pages 127?134.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proceedings of ICML, pages 282?289.T.
R. Leek.
1997.
Information extraction using hidden markovmodels.
Master?s Thesis, UC San Diego.D.
Masterson and N. Kushmerick.
2003.
Information ex-traction from multi-document threads.
In Proceedings ofECML-2003: Workshop on Adaptive Text Extraction andMining, pages 34?41.A.
McCallum.
2002.
Mallet: A machine learning for languagetoolkit.U.
Nahm and R. Mooney.
2002.
Text mining with informationextraction.
In Proceedings of the AAAI 2220 Spring Sympo-sium on Mining Answers from Texts and Knowledge Bases,pages 60?67.J.
Prager, J. Chu-Carroll, and K. Czuba.
2004.
Question an-swering by constraint satisfaction: Qa-by-dossier with con-straints.
In Proceedings of ACL, pages 574?581.D.
R. Radev and K. R. McKeown.
1998.
Generating naturallanguage summaries from multiple on-line sources.
Compu-tational Linguistics, 24(3):469?500.D.
Ravichandran and E. Hovy.
2002.
Learning surface textpatterns for a question answering system.
In Proceedings ofACL, pages 41?47.E.
Riloff and M. Schmelzenbach.
1998.
An empirical ap-proach to conceptual case frame acquisition.
In Proceedingsof WVLC, pages 49?56.E.
Riloff.
1996.
Automatically Generating Extraction Patternsfrom Untagged Text.
In Proceedings of AAAI, pages 1044?1049.B.
Schiffman, I. Mani, and K. J. Concepcion.
2001.
Produc-ing biographical summaries: Combining linguistic knowl-edge with corpus statistics.
In Proceedings of ACL, pages450?457.K.
Seymore, A. McCallum, and R. Rosenfeld.
1999.
Learninghidden markov model structure for information extraction.In AAAI?99 Workshop on Machine Learning for InformationExtraction, pages 37?42.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995.CRYSTAL: Inducing a conceptual dictionary.
In Proceed-ings of IJCAI, pages 1314?1319.C.
Sutton, K. Rohanimanesh, and A. McCallum.
2004.
Dy-namic conditional random fields: factorize probabilisticmodels for labeling and segmenting sequence data.
In Pro-ceedings of ICML.M.
White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, andK.
Wagstaff.
2001.
Multi-document summarization via in-formation extraction.
In Proceedings of HLT.490
