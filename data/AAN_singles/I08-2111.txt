Annotation of Multiword Expressions in the Prague Dependency TreebankEduard Bejc?ek, Pavel Stran?
?k and Pavel SchlesingerInstitute of Formal and Applied LinguisticsCharles University, Prague, Czech Republic{bejcek,stranak,schlesinger}@ufal.mff.cuni.czAbstractIn this article we want to demonstrate thatannotation of multiword expressions in thePrague Dependency Treebank is a well de-fined task, that it is useful as well as feasible,and that we can achieve good consistency ofsuch annotations in terms of inter-annotatoragreement.
We show a way to measure agree-ment for this type of annotation.
We also ar-gue that some automatic pre-annotation ispossible and it does not damage the results.1 MotivationVarious projects involving lexico-semantic annota-tion have been ongoing for many years.
Among thosethere are the projects of word sense annotation, usu-ally for creating training data for word sense disam-biguation.
However majority of these projects haveonly annotated very limited number of word senses(cf.
Kilgarriff (1998)).
Even among those that aimtowards ?all words?
word-sense annotation, multi-word expressions (MWE) are not annotated adequa-tely (see (Mihalcea, 1998) or (Hajic?
et al, 2004)),because for their successful annotation a method-ology allowing identification of new MWEs duringannotation is required.
Existing dictionaries that in-clude MWEs concentrate only on the most frequentones, but we argue that there are many more MWEsthat can only be identified (and added to the dictio-nary) by annotation.There are various projects for identification of na-med entities (for an overview see (?evc??kov?
et al,2007)).
We explain below (mainly in Section 2) whywe consider named entities to be concerned with lex-ical meaning.
At this place we just wish to recall thatthese projects only select some specific parts of textand provide information only for these.
They do notaim for full lexico-semantic annotation of texts.There is also another group of projects that have totackle the problem of lexical meaning, namely tree-banking projects that aim to develop a deeper layerof annotation in adition to a surface syntactic layer.This deeper layer is generally agreed to concern lex-ical meaning.
Therefore the units of this layer cannotbe words anymore, they should be lexias.Lexia is defined by Filipec and C?erm?k (1986)as equivalent to a ?monosemic lexeme?
of (Filipec,1994) or a ?lexical unit?
of (Cruse, 1986): ?a pairof a single sense and a basic form (plus its derivedforms) with relatively stable semantic properties?.We work with the Prague Dependency Treebank(PDT, see Hajic?
(2005)), which has in addition tothe morphemic and the surface syntactic layers alsothe tectogrammatical layer.
The latter has been con-strued as the layer of the (literal) meaning of the sen-tence and thus should be composed of lexias (lexicalunits) and the relations between their occurrences.1On the tectogrammatical layer only the autose-mantic words form nodes in a tree (t-nodes).
Synse-mantic (function) words are represented by variousattributes of t-nodes.
Each t-node has a lemma: an at-tribute whose value is the node?s basic lexical form.Currently t-nodes, and consequently their t-lemmas,are still visibly derived from the morphological di-vision of text into tokens.
This preliminary handling1With a few exceptions, such as personal pronouns (that co-refer to other lexias) or coordination heads.793has always been considered unsatisfactory in FGD.2There is a clear goal to distinguish t-lemmas throughtheir senses, but this process has not been completedso far.Our project aims at improving the current stateof t-lemmas.
Our goal is to assign each t-node at-lemma that would correspond to a lexia, i.e.
thatwould really distinguish the t-node?s lexical mean-ings.
To achieve this goal, in the first phase of theproject, which we report on in this paper, we iden-tify multiword expressions and create a lexicon ofthe corresponding lexias.2 IntroductionWe annotate all occurrences of MWEs (includingnamed entities, see below) in PDT 2.0.
When wespeak of multiword expressions we mean ?idiosyn-cratic interpretations that cross word boundaries?
(Sag et al, 2002).
We understand multiword expres-sions as a type of lexias.
We distinguish also a spe-cial type of MWEs, for which we are mainly inter-ested in its type, rather than individual lexias, duringthe annotation: named entities (NE).3 Treatment ofNEs together with other MWEs is important, be-cause syntactic functions are more or less arbitraryinside a NE (consider an address with phone num-bers, etc.)
and so is the assignment of semantic roles.That is why we need each NE to be combined into asingle node, just like we do it with MWEs in general.For the purpose of annotation we have built a repos-itory of lexias corresponding to MWEs, which wecall SemLex.
We have built it using entries fromsome existing dictionaries and it is being enrichedduring the annotation in order to contain every lexiathat was annotated.
We explain this in detail in Sec-tion 4.1.3 Current state of MWEs in PDT 2.0During the annotation of valency that is a part ofthe tectogrammatical layer of PDT 2.0 the t-lemmas2Functional Generative Description (FGD, (Sgall et al,1986; Hajic?ov?
et al, 1998)) is a framework for system-atic description of a language, that the PDT project is basedupon.
In FGD units of the t-layer are construed equivalently tomonosemic lexemes (lexias) and are combined into dependencytrees, based on syntactic valency of the lexias.3NEs can in general be also single-word, but in this phase ofour project we are only interested in multiword expressions, sowhen we say NE in this paper, we always mean multiword.that correspond to lexias have been basically iden-tified for all the verbs and some nouns and adjec-tives.
The resulting valency lexicon is called PDT-VALLEX (Hajic?
et al, 2003) and we can see it asa repository of lexias based on verbs, adjectives andnouns in PDT that have valency.
4This is a starting point for having t-nodes corre-sponding to lexias.
However in the current state it isnot fully sufficient even for verbs, mainly becauseparts of MWEs are not joined into one node.
Partsof frames marked as idiomatic are still representedby separate t-nodes in a tectogrammatical tree.
Ver-bal phrasemes are also split into 2 nodes, where thenominal part is governed by the verb.
Non-verbal id-ioms have not been annotated at all.Below we give an example of the current state:an idiom meaning ?in a blink (of an eye)?
?
literally?
*what not-see?
(Figure 1).Figure 1: ?Co nevide?t?
(in a blink)4 Methodology4.1 Building SemLexEach entry we add into SemLex is considered to bea lexia.
We have also added 9 special entries to iden-tify NE types, so we do not need to add the expres-sions themselves.
These types are derived from NEclassification by (?evc??kov?
et al, 2007).
Some fre-quent names of persons, institutions or other objects(e.g.
film titles) are being added into SemLex dur-ing annotation (while keeping the information abouta NE type), because this allows for their followingoccurrences to be pre-annotated automatically (seeSection 5).
For others, like addresses or bibliographic4It is so because in PDT-VALLEX valency is not the onlycriterion for distinguishing frames (=meanings).
Two wordswith the same morphological lemma and valency frame are as-signed two different frames if their meaning differs.
Thus thePDT-VALLEX frames correspond to lexias.794entries, it makes but little sense, because they mostprobably will not reappear during the annotation.Currently (for the first stage of lexico-semanticannotation of PDT) SemLex contains only lexias cor-responding to MWEs.
Its base has been composed ofMWEs extracted from Czech WordNet (Smr?, 2003),Eurovoc (Eurovoc, 2007) and SC?FI (C?erm?k et al,1994).5 Currently there are over 30,000 multi-wordlexias in SemLex and more are being added duringannotations.The entries added by annotators must be lexias asdefined above.
Annotators define their ?sense?
infor-mally (as much as possible) and we extract an exam-ple of usage and the basic form from the annotationautomatically.
The ?sense?
information shall be re-vised by a lexicographer, based on annotated occur-rences.4.2 AnnotationPDT 2.0 uses PML (Pajas and ?te?p?nek, 2005),which is an application of XML that utilises a stand-off annotation scheme.
We have extended the PDT-PML with a new schema for so-called s-files.
Weuse these files to store all of our annotation withoutaltering the PDT itself.
These s-files are very sim-ple: basically each of them consists of a list of s-nodes.
Each s-node corresponds to an occurrence ofa MWE and it is composed of a link to the entry inSemLex and a list of identifiers of t-nodes that cor-respond to this s-node.Our annotation program reads in a tectogrammati-cal representation (t-file) and calls TrEd (Pajas, 2007)to generate plain text.
This plain text (still linked tothe tectogrammatical representation) is presented tothe annotator.
While the annotator marks MWEs al-ready present in SemLex or adds new MWEs intoSemLex, tree representations of these MWEs extrac-ted from underlying t-trees are added into their Sem-Lex entries via TrEd scripts.5 Pre-annotationBecause MWEs tend to occur repeatedly in a text,we have decided to test pre-annotation both for thespeed improvement and for improving the consis-tency of annotations.
On the assumption that all oc-5Slovn?k c?esk?
frazeologie a idiomatiky (Dictionary ofCzech Phraseology and Idiomatics)currences of a MWE share the same tree structure,while there are no restrictions on the surface wordorder other than those imposed by the tree structureitself we have decided to employ four types of pre-annotation:A) External pre-annotation provided by our col-league (see Hn?tkov?
(2002)).
With each MWE aset of rules is associated that limits possible formsand surface word order of parts of a MWE.
This ap-proach was devised for corpora that are not syntac-tically annotated.B) Our one-time pre-annotation with those lexiasfrom SemLex that were already used in annotation,and thus have a tree structure as a part of their entry.C) Dynamic pre-annotation as in B, only with theSemLex entries that have been recently added by theannotator.D) When an annotator tags an occurrence of aMWE in the text, other occurrences of this MWEin the article are identified automatically.6(A) was executed once for all of the PDT.
(B) isperformed each time we merge lexias added by an-notators into the main SemLex.
We carry out thisannotation in one batch for all PDT files remainingto annotate.
(C) should be done for each file whileit is being opened in LexemAnn GUI.
(D) happenseach time the annotator adds a new lexia into Sem-Lex and uses it to annotate an occurrence in the text.In subsequent files instances of this lexia are alreadyannotated in step (C), and later even in (B).After the pilot annotation without pre-annotation(D) we have compared instances of the same tagsand found that 10.5% of repeated lexias happenedto have two different trees.
After closer examinationthis 10.5% group is negligible because these casesare caused by ellipses, variations in lexical form suchas diminutives etc., or wrong lemmatisation, ratherthan inconsistencies in the tree structure.
These casesshow us some issues of PDT 2.0, for instance:?
ji?n?
?
Ji?n?
Korea [southern ?
South Korea] ?wrong lemmatisation6This is exactly what happens: 1) Tree structure of the se-lected MWE is identified via TrEd 2) The tree structure is addedto the lexeme?s entry in SemLex 3) All the sentences in thegiven file are searched for the same MWE using its tree structure(via TrEd) 4) Other occurrences returned by TrEd are taggedwith this MWE?s ID, but these occurrences receive an attribute?auto?, which identifies them (both in the s-files and visually inthe annotation tool) as annotated automatically.795?
obchodn?
r?editel ?
r?editelka [managing direc-tor ?
man ?
woman] ?
in future these shouldhave one t-lemma and gender should be speci-fied by an attribute of a t-node.We have not found any case that would show thatthere is such a MWE that its structure cannot be rep-resented by a single tectogrammatical tree.
1.1% ofall occurences were not connected graphs, but thishappened due to errors in data and to coordination.This corroborates our assumption that (disregardingerrors) all occurrences of a MWE share the sametree structure.
As a result, we started storing the treestructures in the SemLex entries and employ them inpre-annotation (D).
This also allows us to use pre-annotations (B) and (C), but we have decided notto use them at the moment, in order to be able toevaluate each pre-annotation step separately.
Thusthe following section reports on the experiments thatemploy pre-annotation (A) and (D).6 Analysis of AnnotationsTwo annotators already started to use (and test) thetool we have developed.
They both have got the sametexts.
The text is generated from the t-trees and pre-sented as a plain text with pre-annotated words mark-ed by colour labels.
Annotators add their tags in theform of different colour labels and they can deletethe pre-annotated tags.
In this experiment data con-sists of approx.
120,000 tokens that correspond to100,000 t-nodes.
Both annotators have marked about15,200 t-nodes (~15%) as parts of MWEs.
annotatorA has grouped them into 7,263 MWEs and annota-tor B into 6,888.
So the average length of a MWE is2.2 t-nodes.The ratio of general named entities versus Sem-Lex lexias was 52:48 for annotator A and 49:51 incase of annotator B. Annotator B used 10% morelexias than annotatorA (3,279 and 3,677), while theyboth used almost the same number of NEs.
Somecomparison is in the Table 1.type of MWE A BSemLex lexias 3,677 3,279Named Entities 3,553 3,587- person/animal 1130 1137- institution 842 772Table 1: Annotated instances of significant types ofMWEsBoth annotators also needed to add missing en-tries to the originally compiled SemLex or to editexisting entries.
annotatorA added 722 entries whilethe annotator B added 861.
They modified 796 and809 existing entries, respectively.6.1 Inter-anntator AgreementIn this section our primary goal is to assess whetherwith our current methodology we produce reliableannotation of MWEs.
To that end we measure theamount of inter-annotator agreement that is abovechance.
There are, however, a few sources of com-plications in measuring this agreement:?
Each tag of a MWE identifies a subtree of a tec-togrammatical tree (represented on the surface by aset of marked words).
This allows for partial agree-ment of tags at the beginning, at the end, but also inthe middle of a surface interval (in a sentence).?
A disagreement of the annotators on the tag isstill an agreement on the fact that this t-node is a partof a MWE and thus should be tagged.
This means wehave to allow for partial agreement on a tag.?
There is not any clear upper bound as to howmany (and how long) MWEs are there in texts.?
There is not a clear and simple way to esti-mate the amount of the agreement by chance, be-cause it must include the partial agreements men-tioned above.Since we want to keep our agreement calculationas simple as possible but we also need to take intoaccount the problems above, we have decided to startfrom pi as defined in (Artstein and Poesio, 2007) andto make a few adjustments to allow for types of par-tial agreement and estimated maximal agreement.Because we do not know how many MWEs thereare in our texts, we need to calculate the agreementover all t-nodes, rather than the t-nodes that ?shouldbe annotated?.
This also means, that the theoreticalmaximal agreement (upper bound) U , cannot be 1.If it was 1, it would be saying that all nodes are partof a MWE.Since we know that U < 1 but we do not knowit?s exact value, we use the estimated upper boundU?
(see Equation 1).
Because we calculate U?
over allt-nodes, we need to account not only for agreementon tagging a t-node, but also for agreement, that thet-node is not a part of a MWE, therefore it is not796tagged.7If N is the number of all t-nodes in our data andnA?B is the number of t-nodes annotated by at leastone annotator, then we estimate U?
as follows:U?
=nA?BN+ 0.052 ?N ?
nA?BN= 0.215 (1)The weight 0.052 used for scoring the t-nodes thatwere not annotated is explained below.
Because U?includes all the disagreements of the annotators, webelieve that the real upper bound U lies somewhatbelow it and the agreement value 0.215 is not some-thing that should (or could) be achieved.
This is how-ever based on the assumption that the data we havenot yet seen have similar ratio of MWEs as the datawe have used.To account for partial agreement we divide the t-nodes into 5 classes c and assign each class a weightw as follows:c1 If the annotators agree on the exact tag from Sem-Lex, we get maximum information: w = 1c2 If they agree, that the t-node is a part of a NE orthey agree it is a part of some lexia from Sem-Lex, but they do not agree which NE or whichlexia, we estimate we get about a half of the in-formation compared to c1: w = 0.5c3 If they agree that the t-node is a part of a MWE,but disagree whether a NE or a lexia from Sem-Lex, it is again half the information compared toc2, so w = 0.25c4 If they agree that the t-node is not a part of aMWE, w = 0.052.
This low value of w accountsfor frequency of t-nodes that are not a part of aMWE, as estimated from data: Agreement on notannotating provides the same amount of infor-mation as agreement on annotating, but we haveto take into account higher frequency of t-nodesthat are not annotated:c4 = c3 ?PannotatedPnot annotated= 0.25 ?1279761433?
0.052c5 If the annotators do not agree whether to anno-tate a t-node or not, w = 0.The number of t-nodes (n) and weights w per classc are given in Table 2.7If we did not do this, there would be no difference betweent-nodes, that were not tagged (annotators agreed they are not apart of a MWE) and the t-nodes that one annotator tagged andthe other did not (i.e.
they disagreed).Agreement DisagreementAgreement on annotation Not annotationAgreement on NE / lexiaFull agreementclass c 1 2 3 4 5t-nodes n 10,527 2,365 389 83,287 3,988weight w 1 0.5 0.25 0.052 0Table 2: The agreement per class and the associatedweightsNow that we have estimated the upper bound ofagreement U?
and the weights w for all t-nodes wecan calculate our weighted version of pi:piw =Ao ?AeU?
?AeAo is the observed agreement of annotators andAe is the agreement expected by chance (which issimilar to a baseline).
piw is thus a simple ratio of ourobserved agreement above chance and maximum a-greement above chance.Weights w come into account in calculation ofAoand Ae.We calculate Ao by multiplying the number of t-nodes in each category c by that category?s weightw, summing these 5 weighted sums and dividing thissum of all the observed agreement in the data bythe total number of t-nodes: Ao = 1N?5c=1 ncwc =0.160.Ae is the probability of agreement expected bychance over all t-nodes.
This means it is the sum ofthe weighted probabilities of all the combinations ofall the tags that can be obtained by a pair of annota-tors.
Every possible combination of tags (includingnot tagging a t-node) falls into one of the categoriesc and thus gets the appropriate weight w. Calculat-ing the value of Ae depends not only on values ofw (see Table 2), but also on the fact that SemLex iscomposed of 9 entries for NE types and over 30,000entries for individual lexias.
Based on this we haveobtained Ae = 0.047.The resulting piw is thenpiw =Ao ?AeU?
?Ae=0.160?
0.0470.215?
0.047= 0.6760When we analyse the cases of disagreement andpartial agreement we find that most of it has to dowith SemLex lexias rather than NEs.
This is mostlydue to imperfectness of the dictionary and its size(annotators could not explore each of almost 30,000797of SemLex entries).
Our current methodology, whichrelies too much on searching the SemLex, is also toblame.
This should, however, improve by employingpre-annotation (B) and (C).One more reason for disagreement consists in thefact that there are cases, for which non-trivial knowl-edge of the world is needed: ?Jang Di Pertuan AgongSultan Azlan ?
?h, the sultan of the state of Perak,[ .
.
. ]
flew back to Perak.?
Is ?Sultan Azlan ??h?
stilla part of the name or is it (or a part of it) a title?The last important reason of disagreement is sim-ple: both annotators identify the same part of textas MWE instances, but while searching the SemLexthey choose different lexias as the tags.
This can berectified by:?
Removing duplicate entries from SemLex (cur-rently there are many close identical entries orig-inating from Eurovoc and Czech WordNet).?
Imploring improved pre-annotation B and C, asmentioned above.7 ConclusionWe have annotated multi-word lexias and named en-tities in a part of PDT 2.0.
We use tectogrammati-cal tree structures of MWEs for the automatic pre-annotation.
In the analysis of inter-annotator agree-ment we show that a weighted measure that accountsfor partial agreement as well as the estimation ofmaximal agreement is needed.The resulting piw = 0.6760 is statistically sig-nificant and should gradually improve as we cleanup the annotation lexicon, more entries can be pre-annotated automatically, and further types of pre-annotation are employed.8 AcknowledgementThis work has been supported by grants 1ET2011205-05 of Grant Agency of the Academy of Science ofthe Czech Republic, projects MSM0021620838 andLC536 of the Ministry of Education and 201/05/H014of the Czech Science Foundation.ReferencesRon Artstein and Massimo Poesio.
2007.
Inter-coder agree-ment for computational linguistics.
Submitted to Computa-tional Linguistics.F.
C?erm?k, V.
C?erven?, M.
Churav?, and J. Machac?.
1994.Slovn?k c?esk?
frazeologie a idiomatiky.
Academia.D.A.
Cruse.
1986.
Lexical Semantics.
Cambridge UniversityPress.Eurovoc.
2007. http://europa.eu/eurovoc/.Josef Filipec and Franti?ek C?erm?k.
1986.
C?esk?
lexikologie.Academia.Josef Filipec.
1994.
Lexicology and lexicography: Develop-ment and state of the research.
In P. A. Luelsdorff, editor,The Prague School of Structural and Functional Linguistics,pages 163?183, Amsterdam/Philadelphia.
J. Benjamins.Jan Hajic?, Jarmila Panevov?, Zden?ka Ure?ov?, Alevtina B?-mov?, Veronika Kol?r?ov?, and Petr Pajas.
2003.
PDT-VALLEX.
In Joakim Nivre and Erhard Hinrichs, editors,Proceedings of The Second Workshop on Treebanks and Lin-guistic Theories, volume 9 of Mathematical Modeling inPhysics, Engineering and Cognitive Sciences, pages 57?68,Vaxjo, Sweden.
Vaxjo University Press.Jan Hajic?, Martin Holub, Marie Huc?
?nov?, Martin Pavl?k, PavelPecina, Pavel Stran?
?k, and Pavel Martin ?id?k.
2004.Validating and improving the Czech WordNet via lexico-semantic annotation of the Prague Dependency Treebank.
InLREC 2004, Lisbon.Jan Hajic?, 2005.
Insight into Slovak and Czech Corpus Lin-guistics, chapter Complex Corpus Annotation: The PragueDependency Treebank, pages 54?73.
Veda Bratislava, Slo-vakia.Eva Hajic?ov?, Barbara H. Partee, and Petr Sgall.
1998.
Topic-focus articulation, tripartite structures, and semantic con-tent, volume 71 of Studies in Linguistics and Philosophy.Kluwer, Dordrecht.Milena Hn?tkov?.
2002.
Znac?kov?n?
fraz?mu?
a idiomu?
vC?esk?m n?rodn?m korpusu s pomoc?
Slovn?ku c?esk?
fraze-ologie a idiomatiky.
Slovo a slovesnost.A.
Kilgarriff.
1998.
Senseval: An exercise in evaluating wordsense disambiguation programs.
In Proc.
LREC, pages 581?588, Granada.Rada Mihalcea.
1998.
Semcor semantically tagged corpus.Petr Pajas and Jan ?te?p?nek.
2005.
A Generic XML-Based For-mat for Structured Linguistic Annotation and Its Applicationto Prague DependencyTreebank 2.0.
Technical Report TR-2005-29, ?FAL MFF UK, Prague, Czech Rep.Petr Pajas.
2007.
TrEd.
http://ufal.mff.cuni.cz/?pajas/tred/index.html.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copestake,and Dan Flickinger.
2002.
Multiword expressions: A painin the neck for nlp.
In Third International Conference, CI-CLing.Magda ?evc?
?kov?, Zdene?k ?abokrtsk?, and Oldr?ich Kru?za.2007.
Zpracov?n?
pojmenovan?ch entit v c?esk?ch textech(treatment of named entities in czech texts).
Technical Re-port TR-2007-36, ?FAL MFF UK, Prague, Czech Republic.Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986.
TheMeaning of the Sentence in Its Semantic and Pragmatic As-pects.
Academia/Reidel Publ.
Comp., Praha/Dordrecht.Pavel Smr?.
2003.
Quality control for wordnet development.In Petr Sojka, Karel Pala, Pavel Smr?, Christiane Fellbaum,and Piek Vossen, editors, Proceedings of the Second Inter-national WordNet Conference?GWC 2004, pages 206?212.Masaryk University Brno, Czech Republic.798
