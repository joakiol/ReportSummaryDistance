R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
414 ?
425, 2005.?
Springer-Verlag Berlin Heidelberg 2005A Preliminary Work on Classifying Time Granularitiesof Temporal QuestionsWei Li1, Wenjie Li1, Qin Lu1, and Kam-Fai Wong21Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong{cswli, cswjli, csluqin}@comp.polyu.edu.hk2Department of Systems Engineering, the Chinese University of Hong Kong,Shatin, Hong Kongkfwong@se.cuhk.edu.hkAbstract.
Temporal question classification assigns time granularities to tempo-ral questions ac-cording to their anticipated answers.
It is very important for an-swer extraction and verification in the literature of temporal question answer-ing.
Other than simply distinguishing between "date" and "period", a more fine-grained classification hierarchy scaling down from "millions of years" to "sec-ond" is proposed in this paper.
Based on it, a SNoW-based classifier, combininguser preference, word N-grams, granularity of time expressions, special patternsas well as event types, is built to choose appropriate time granularities for theambiguous temporal questions, such as When- and How long-like questions.Evaluation on 194 such questions achieves 83.5% accuracy, almost close tomanually tagging accuracy 86.2%.
Experiments reveal that user preferencesmake significant contributions to time granularity classification.1   IntroductionTemporal questions, such as the questions with the interrogatives ?when?, ?how long?and ?which year?, seek for the occurrence time of the events or the temporal attributesof the entities.
Temporal question classification plays an important role in the litera-ture of question answering and temporal information processing.
In the evaluation ofTREC 10 Question-Answering (QA) track [1], more than 10% of questions in the testquestion corpus are temporal questions.
Different from TREC QA track, WorkshopTERQAS (http://www.timeml.org/terqas/) particularly investigated on temporal ques-tion answering instead of a general one.
It focused on temporal and event recognitionin question answering systems and paid great attention to temporal relations amongstates, events and time expressions in temporal questions.
TimeML (http://www.ti-meml.org), a temporal information (e.g.
time expression, tense & aspect) annotationstandard, has also been used for temporal question answering in this workshop [2].Correct understanding of a temporal question will greatly help extracting and verify-ing its answers and certainly improve the performance of any question answeringsystem.
Look at the following examples.[Ea].
What is the birthday of Abraham Lincoln?[Eb].
When did the Neanderthal man live?A Preliminary Work on Classifying Time Granularities of Temporal Questions 415In a general question answering system, the question classifier commonly classifiestemporal questions into two classes, i.e.
?date?
and ?period?.
With such a system, theabove two questions are both assigned a ?date?.
Whereas it is natural for the question[Ea] to be answered with a particular data (e.g.
?12/02/1809?
), it is not the case forquestion [Eb], because a proper answer could be ?35,000 years ago?.
However, if it isknown that the time granularity concerned is ?thousands of years?, answer extractionturn to be more targeted.
The need for a more fine-grained classification is obvious.Although there were different question classification hierarchies, as reported[3,4,12,13,14], few inclined to introducing the classification hierarchy (e.g.
?year?,?month?
and ?day?)
which could give a clearer direction to guide answer extractionand verification of temporal questions.
In the following, we try to find out whethertemporal questions can be further classified into finer time granularity and how toclassify them.By examining a temporal question corpus consisting of 348 questions, 293 ofwhich are gathered from UIUC question answering labelled data (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC), and the rest 55 from TREC 10 test corpus, we findtwo different cases.
On the one hand, some questions are very straightforward in ex-pressing the time granularities of the answers expected, e.g.
the questions beginningwith ?which year?
or ?for how many years?.
On the other hand, some questions arenot so obvious, e.g.
the questions headed by ?when?
or ?for how long?.
We call suchquestions ambiguous questions.
Not surprisingly, the ambiguous When- and Howlong-like questions account for a large proportion in this temporal question corpus,i.e.
197 from 348 in total.We further investigate on those 197 ambiguous questions in order to find outwhether they can be classified into finer time granularity.
Three experimenters arerequested to tag a time granularity to each question independently1.
Answers are notprovided.
The tag with two agreements is taken as the time granularity class of thecorresponding temporal question.
Otherwise the tag ?UNKNOWN?
is assigned.
Ref-erence answers for the questions are extracted from AltaVista Web Search(http://www.altavista.com).
Comparing the time granularities tagged manually withthose provided by the reference answers, we find that only 27 out of 197 questions areincorrectly tagged, in other words, the manually tagging accuracy is 86.2%.
Errorsexist though, the relatively high agreement between users?
tagging and referenceanswers lights the hope of automatically determining the time granularities of tempo-ral questions.Analysing the tagging results, it is revealed that the tagging errors arouse fromthree sources: insufficient world knowledge, different speaking habits and differentexpected information granularity among human.
See the following examples:[Ec].
When did the Neanderthal man live?User: year; Ref.
: thousands of years[Ed].
How long is human gestation?User: month; Ref.
: week[Ee].
When was the first Wall Street Journal published?User: year; Ref.
: day1The granularity hierarchy and the tagging principle will be detailed later.416 W. Li et alFor question [Ec], the time granularity should be ?thousands of years?, rather than?year?.
This error could be corrected if one knows that Neanderthal man existed35,000 years ago.
The time granularity of question [Ed] should be ?week?, but not?month?
in accordance with the habit.
For question [Ee], users?
tag is ?year?, differentfrom the reference answer?s tag ?day?.
However, both granularities are acceptable incommonsense, because the different users may want coarser or finer information.
Thisobservation suggests that incorporating question context, world knowledge, andspeaking habits would help determine the time granularities of temporal questions.In this paper, we propose a fine-grained temporal question classification scheme,i.e.
time granularity hierarchy, consisting of sixteen non-exclusive classes and scalingdown from ?millions of years?
to ?second?.
The SNoW-based classifier is then builtto combine linguistic features (including word N-grams, granularity of time expres-sions and special patterns), user preferences and event types, and assign one of thesixteen classes to each temporal question.
In our work, user preference, which charac-terizes world knowledge and speaking habits, is estimated by means of the timegranularities of the entities and/or events involved.
The SNoW-based classifierachieves 83.5% accuracy, almost close to 86.2% of manually tagging accuracy.
Ex-periments also show that user preference makes a great contribution to time granular-ity classification.The rest of this paper is organized as follows.
In the next section various relatedworks in this literature are introduced.
In Sect.
3, we demonstrate the time granularityhierarchy and principles.
User preference is fully investigated in Sect.
4.
Feature de-sign is depicted in Sect.
5.
Time granularity classifiers are introduced in Sect.
6 andthe experiment results are presented in Sect.
7.
We finally conclude this paper in thelast section.2   Related WorksIn TREC QA track, almost every QA system joining in the evaluation has a questionclassification module.
This makes question classification a hot topic.
Questions can beclassified from several aspects.
Most classification hierarchies [3,4,12,13,14] adopt theanticipated answer types as its classification criteria.
Abney et al [4] gave a coarseclassification hierarchy with seven classes (person, location, etc.).
Hovy et al [13]introduced a finer classification with forty-seven classes manually constructed from17,000 practical questions.
Li et al [3] proposed a two-level classification hierarchy, acoarser one with six classes and a finer one with fifty classes.
In all these classificationhierarchies, temporal questions are simply classified into two classes, i.e.
?date?
and?period?.
Some works classified temporal questions from other aspects.
In [2], a tem-poral question classification hierarchy is proposed according to the temporal relationamong state, event and time expression.
In [5], temporal questions are classified intothree types with regard to question structure: non-temporal, simple and complex.
DiazF.
et al [6] did an interesting work on the statistics of the number of topics along time-line.
According to whether questions or topics have a clear distribution along timeline,they can be classified into three types: atemporal, temporal clear and temporal ambigu-ous.
Focusing on ambiguous temporal questions, e.g.
when and how long-like ques-tions, we introduce a classification hierarchy in terms of the anticipated answer types.A Preliminary Work on Classifying Time Granularities of Temporal Questions 417It is an extension of two classes ?date?
and ?period?
and includes sixteen non-exclusive classes scaling down from ?millions of years?
to ?second?.Related to the work of features design, Li et al [3] built the question classifierbased on three types of features, including surface text (e.g.
N-grams), syntactic fea-tures (e.g.
part-of-speech and name entity tags), and semantic related words (wordsthat often occur with a specific question class).
Later works of Li et al [10] intro-duced semantic information and world knowledge from external resources such asWordNet.
In this paper, we introduce a new feature, user preference, which is ex-pected to imply the world knowledge in time granularity in the experiment.
Userpreference is estimated from statistics with which Diaz F. et al [6] determine whethera question is temporal ambiguous or not.
E. Saquete et al [5] suggested that questionshad different structures, i.e.
non-temporal, simple and complex, which is helpful tohandle questions more orderly.
It gives us inspiration to use question focus, i.e.whether a question is event-based or entity-based.Many machine-learning methods have been used in question classification, suchas language model [7], SNoW [3,10], maximum entropy [15] and support vector ma-chine [8,9].
In our experiments, language model is selected as the baseline model, andSNoW is selected to tackle to the large feature space and build the classifier.
In fact,SNoW has already been used in many other fields, such as text categorization, wordsense disambiguation and even facial feature detection.3   Time Granularity Hierarchy and Tagging PrinciplesIn traditional question answering systems, only two question types are time-related, i.e.?date?
and ?period?.
For the reasons explained in Sect.
1, we propose a more detailedtemporal question classification scheme, namely time granularity hierarchy scalingdown from ?millions of years?
to ?second?
in order to facilitate answer extraction andverification.
The initial time granularity hierarchy includes the following twelveclasses: ?second?, ?minute?, ?hour?, ?day?, ?week?, ?month?, ?season?, ?year?, ?dec-ade?, ?century?,  ?thousands of years?
and ?millions of years?.Granularity ?weekday?
is added to the initial hierarchy because some temporalquestions favor ?weekday?
instead of ?day?, although both of them indicate one day.Some questions favour a region of time granularity.
Look at the following examples.[Ef].
What time of year has the most air travel?[Eg].
What time of day did Emperor Hirohito die?For [Ef] question, its time granularity could be ?season?, ?month?
or even ?day?
; andfor question [Eg], the time granularity could be ?hour?
or ?minute?.
We can onlydetermine that their time granularities are less than ?year?
or ?day?
respectively, butcannot go any further.
Such situations only occur to time granularity ?year?
and?day?, so we expand the original classification hierarchy by adding another two types:?less than day?, ?less than year?.
Besides, the questions asking for festivals are classi-fied into ?special date?.Up to now, the time granularity hierarchy has sixteen classes.
The less frequenttemporal measures, such as ?microsecond?
and ?billions of years?
are ignored.
Asmentioned above, the class ?less than day?
overlaps several granularities, e.g.
?hour?and ?minute?, so the time granularity hierarchy we proposed is non-exclusive.418 W. Li et alIn reality, some temporal questions can be answered in several different timegranularities.
For example, question ?when was Abraham Lincoln born?
?,  its answerscan be a ?day?
(?12/02/1809?)
or a ?year?
(?1809?).
To resolve this confliction, weadopt two principles for time granularity annotation.[Pa].
Assign the minimum time granularity we can determine to a given temporalquestion if several time granularities are applicable.[Pb].
Select the time granularity with regard to speaking habits or user preferences.When the two principles conflict to each other, principle [Pb] takes the priority.
Withprinciple [Pa], time granularity of the above question can only be ?day?.4   User PreferenceIn general, temporal questions have two different focuses: entity-based and event-based.[a].
Entity-based question: temporal interrogative words + (be) + entity, e.g.
?When was the World War II??[b].
Event-based question: temporal interrogatives + event, e.g.
?When didMount St. Helen last have a significant eruption?
?Time granularities of entities (or events) have great significance to those of entity-based (or event-based) temporal questions.
So, in the following, we make estimationof the time granularities of entities and events from statistics, based on the intuitionthat some entities or events may favor certain types of time granularities, which iscalled user preference here.4.1   Estimation of Time Granularities of Entities and Events4.1.1   Time Granularity of EntitiesThe time granularity of the entity is derived by counting the co-occurrences of theentity and time granularities.
The statistics is gathered from AltaVista Web Search.The sentences containing both the entity and time expressions are extracted from thefirst one hundred results returned by AltaVista with the entity as the searching key-word.
The probability P of a time granularity class tgi on the occurrence of the entityis calculated as the following Equation (1).
)(#)(#)|(entityentitytgentitytgP ii?=)|(max)( entitytgPArgentityTG itgi=          (1)#( ) is the number of the sentences containing the expressions between the parenthe-sis.
TG(entity) represents the time granularity of the entity.4.1.2   Time Granularity of EventsThe time granularities of the events are not directly extracted as what is done to theentities, because they have little chance to be reused on the observation that there arerarely two identical events in a question corpus.
As an alternative, the time granularityof an event is estimated from a sequence of entity-verb-entity?
approximating theevent.
The time granularity of the verb is determined as Equation (1) by substitutingA Preliminary Work on Classifying Time Granularities of Temporal Questions 419?verb?
for ?entity?.
We choose two strategies for the estimation: maximum productand one-win-all.Maximum  product: )'|()|()|(1)|( entitytgPverbtgPentitytgPZeventtgP iiii =)|(max)( eventtgPArgeventTG itgi=                                        (2)TG(event) represents time granularity of event.
Z is used for normalization.One-win-all: )}'|(),|(),|({max)( entitytgPverbtgPentitytgPArgeventTG iiitgi=               (3)Equation (1) is smoothed in order to avoid 0 values in Equation (2).iii tgttwwtgwtgP =++?= )(#1)(#)|(                           (4)t is the number of the time granularity classes, w is either an entity or a verb.4.1.3   Experiment: Evaluating the EstimationIn the 197 ambiguous questions, 12 questions are entity-based, and the rest 185 ques-tions are event-based.
If all the 197 questions are arbitrarily assigned a tag ?year?, thetagging accuracy is 48.2%.For each entity-based or event-based question, the time granularity of the entity orevent within it are assumed as the time granularity of the question.
Compared with thetime granularity of the reference answer, for the entity-based questions, we achieve75% accuracy; for the event-based question, the accuracy of maximum product strat-egy and one-win-all strategy are 67.0% and 64.3% respectively.
It seems that maxi-mum product strategy is more effective than one-win-all strategy in this application.With maximum product strategy, the overall accuracy on all the 197 ambiguous ques-tions is 67.4%.
Notice that the accuracy of arbitrarily tagging is only 48.2%, so theestimation of the time granularities of the entities and the events is useful for deter-mining the time granularities of temporal questions.4.2   Distribution of the Time Granularity of Entities and Events4.2.1   Observation of DistributionIn the experiments of estimation, we find that some entities or events tend to favoronly one certain time granularity, some others tend to favor several time granularities,and the rest may have a uniform distribution almost on every time granularity.-1 0 1 2 3 4 5 6 7 8 9020406080100SeasonWeekProp ortio n(%)Time Granularity of "gestation"-1 0 1 2 3 4 5 6 7 8 90102030405060CenturyDecadeYearMonthDayProportion(%)Time Granularity of "Lincoln born"-1 0 1 2 3 4 5 6 7 8 901020304050YearMonthWeekdayDayHourProportion(%)Time Granularity of "take place"(a)   (b)   (c)Fig.
1.
Distribution of the time granularities of the entities and events420 W. Li et alIn Fig.
1(a), time granularity ?day?
takes a preponderant proportion, i.e.
more than80%, in the distribution of ?gestation?, which is called single-peak-distribution.
InFig.
1(b), both ?day?
and ?year?
take a large proportion, so ?Lincoln born?
is multi-peak-distributed.
In Fig.
1(c), for ?take place?, all the time granularities almost take asimilar proportion and it is a uniform distribution.4.2.2   Experiments on DistributionAssume an entity (or event) E, its possible time granularities {tgi, i=1,?t} and thecorresponding probabilities {Pi, i=1,?t} (calculated by Equation 1 and 2).
?= i iPt1?
;  ?= i iPId ),( ?
; ????>??
?=iii PPPI01),(                        (5)d is the number of time granularities tgi with higher probability Pi than average prob-ability ?
.
For simplicity, distribution DE of the time granularity of E is determined asfollows,3311>?<=????
?=dddUniformMultiSingleDE(6)Observing the experiment results in Sect.
4.1.3, 88.7%, 56.3% and 18.9% accuracyare achieved on the questions within which the time granularities of the entities orevents are estimated to be single-peak-, multi-peak-, and uniform-distributed respec-tively.
So whether the estimated time granularity of the entity or event is single-peak-,multi-peak-, or uniform-distributed highlights the confidence on the estimation, whichcan be taken as a feature associated with the estimation of the time granularities.5   Feature DesignAs described in the above section, estimation of the time granularities of the entitiesand the events is useful for determining the time granularities of temporal questions;whether a question is entity-based or not and the distribution of time granularities ofthe entities and events within the questions will also be taken as associated features.These three features are named user preference feature in total.
Besides, another fourtypes of features are considered.Word N-gramsWord N-grams feature, e.g.
unigram and bigram is the most straightforward featureand commonly used in question classification.
In general question classification, uni-gram ?when?
indicates a temporal question.
In temporal question classification, uni-gram ?birthday?
always implies a ?day?
while bigram ?when ?
born?
is a strongevidence of the time granularity ?day?.
From this aspect, word N-grams also reflectuser preference on time granularity.Granularity of Time ExpressionsTime expressions are common in temporal questions, e.g.
?July 11, 1998?
and datemodifier ?1998?
in ?1998 Superbowl?.
We take the granularities of time expressionsas features, for example,TG(?in 1998?)
= ?year?
TG(?July 11, 1998?)
= ?day?A Preliminary Work on Classifying Time Granularities of Temporal Questions 421Granularities of time expressions impose the constraints on the time granularities oftemporal questions.
If there is a time expression whose time granularity is tg in atemporal question, time granularity of this question can not be tg.
For example, ques-tion ?When is the 1998 SuperBowl?
?, its time granularity can not be ?Year?, i.e.
thetime granularity of  ?1998?.Special PatternsIn word N-gram features, words are equally processed, however, some special wordscombining with the verbs or the temporal connectives (e.g.
?when?, ?before?
and?since?)
will produce special patterns and affect the time granularities of temporalquestions.
Look at the following examples.[Eh].
Since when hasn?t John Sununu been able to fly on government planes forpersonal business?[Ei].
What time of the day does Michael Milken typically wake up?For question [Eh], the temporal preposition ?since?
combined with ?when?
highlightsthat this question is seeking for a beginning point time, which implies a finer timegranularity; for question [Ei], ?typically?
combined with verb ?wake up?
indicates agenerally occurred event, and implies that its time granularity could be ?less thanday?
or ?less than year?.Event TypesIn general, there are four event types: states, activities, accomplishments, andachievements.
States and activities favour larger time granularities, while accom-plishments and achievements favour smaller ones.
For example, the activity ?stay?will favour larger time granularity than the accomplishment event ?take place?.6   Classifier BuildingIn this work, we choose the Sparse Network of Winnow (SNoW) model as the timegranularity classifier and compare it with a commonly used Language Model (LM)classifier.6.1   Language Model (LM)As language model has already been used in question classification [7], it is taken asthe baseline model in the experiments.
Language model mainly combines two typesof features, i.e.
unigram and bigram.
Given a temporal question Q, its time granularityTG(Q) is calculated by Equation (7).??
==+==?+=njj jjimjj jitg wwtgPwtgPArgQTG i 1 11 )|()1()|(max)( ??
(7)w represents words.
m and n are the numbers of unigrams and bigrams in questionsrespectively.
?
assigns different weights to unigrams and bigrams.
In the experiment,best accuracy is achieved when 7.0=?
(see Sect.
7.3.1).6.2   Sparse Network of Winnow (SNoW)SNoW is a learning framework and applicable to the tasks with a very large numberof features.
It selects active features by updating weights of features, and learns a422 W. Li et allinear function from a corpus consisting of positive and negative examples.
LetAc={i1, ?, im} be the set of features that are active and linked to target class c. Let sibe the real valued strength associated with feature in the example.
Then the example?sclass is c if and only if,??
?Aciciic sw ?,                                                     (8)icw , is weight of feature i connected with class c, which is learned from the trainingcorpus.
SNoW has already been used in question classification [3,10] and good resultsare reported.
As mentioned in Sect.
5, five types of features are selected for our task.They are altogether counted to more than ten thousand features.
Since it is a largefeature set, SNoW is a good choice.7   Experiments7.1   SetupIn this 348-question-corpus (see Sect.
1), time granularities of 151 questions arestraightforward, while those of the rest 197 questions are ambiguous.
For the sixteentime granularity classes, we only consider ten classes including more than four ques-tions.
Questions with unconsidered time granularity classes excluded, the question cor-pus has 339 questions in total, 145 for training and 194 for testing.
As a result, the taskis to learn a model from the 145-question training corpus and classify questions in the194-question test corpus into ten classes: ?second?, ?minute?, ?hour?, ?day?, ?week-day?, ?week?, ?month?, ?season?, ?year?
and ?century?.
The SNoW classifier isdownloaded from UIUC (http://l2r.cs.uiuc.edu/~cogcomp/download.php?key=SNOW).7.2   Evaluation CriteriaThe primary evaluation standard is accuracy1, i.e.
the proportion of the correct classi-fied questions out of the test questions (see Equation 9).
However, if a question seek-ing for a finer time granularity, e.g.
?day?, has been incorrectly determined as acoarser one, e.g.
?year?, it should also be taken as partly correct, which is reflected inaccuracy2 (see Equation 10).
)(#)(#1 testcorrectAccuracy =                                                  (9)#( ) is number of questions.
)(#)(2 testQRRAccuracy i i?=????
?><=+?=)()'()()'()()'()1)()'((101)(QQQQQQQQ tgRtgRtgRtgRtgRtgRtgRtgRQRR(10)Qtg  and 'Qtg  are the reference and classification result respectively.
)( QtgR is therank of the time granularity class Qtg , scaling down from ?millions of years?
to?second?.
Rank of ?second?
is 1, while rank of ?year?
is 9.
The ranks of the last threeA Preliminary Work on Classifying Time Granularities of Temporal Questions 423time granularities, i.e.
?special date?, ?less than day?
and ?less than year?
are 14, 15and 16 respectively.
Likewise, )'( QtgR is the rank of 'Qtg .7.3   Experimental Results and AnalysisIn the experiments, language model is taken as the baseline model.
Performance ofSNoW-based classifier will be compared with that of language model.
Different com-binations of features are tested in SNoW-based classifier and their performances areinvestigated.7.3.1   LM ClassifierThe LM classifier takes two types of features: unigram and bigram.
Experiment re-sults are presented in Fig.
2.Accuracy varies with different feature weight ?
and best accuracy (accuracy168.0% and accuracy2 68.9%) achieves when ?
=0.7.
Accuracy when ?
=1.0 is higherthan that when ?
=0.
It indicates that, in the framework of language model, unigramsachieves better performance than bigrams, which accounts from the sparseness ofbigram features.-0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1455055606570Accuracy1Accuracy2Accuracy(%)?Fig.
2.
Accuracy of LM classifier.
Data in circle is the best performance achieved.7.3.2   SNOW ClassifierOur SNoW classifier requires binary features.
We then encode each feature with aninteger label.
When a feature is observed in a question, its label will appear in theextracted feature set of this question.
There are six types of features: 15 user prefer-ences (10 for the estimation of time granularities, 3 for the estimation distributions,and 2 for question focuses) (F1), 951 unigrams (F2), 9277 bigrams (F3), 10 granularityof time expressions (F4), 14 special patterns (F5), and 4 event types (F6).
Although thenumber of all features is more than ten thousand, the features in one question are nomore than twenty in general.
Accuracies of SNoW classifier on 194 test questions arepresented in Table 1.
It shows that simply using unigram features, SNoW classifierhas already achieved better accuracy than LM classifier (accuracy1: 69.5% vs. 68.0%;accuracy2: 70.3% vs. 68.9%).
From this view, SNoW classifier outperforms LM clas-sifier in handling sparse features.
When all the six types of features are used, SNoWclassifier achieves 83.5% in accuracy1 and 83.9% in accuracy2, almost close to theaccuracy of user tagging, i.e.
86.2%.424 W. Li et alTable 1.
Accuracy (%) of SNoW classifierFeature Set F2 F2, 3 F1~6Accuracy1 69.5 72.1 83.5Accuracy2 70.3 72.7 83.9Table 2.
Accuracy1 (%) on different types of time granularitiesTG second minute hour day weekdayAccuracy1 100 100 100 64.2 100TG week month season year centuryAccuracy1 100 60 100 90.5 66.7Table 3.
Accuracy (%) on combination of different types of featuresFeature Set F2,3 F1,2,3 F2,3,4 F2,3,5 F2,3,6Accuracy1 72.1 79.8 73.7 74.7 72.6Accuracy2 72.7 80.6 74.7 75.2 73.1With all the six types of features, accuracy1 on the questions with different types oftime granularity is illustrated in Table 2.
It reveals that the classification errors mainlycome from time granularity of ?month?, ?day?
and ?century?.
Low accuracy on?month?
and ?century?
accounts from absence of enough examples, i.e.
examples fortraining and testing both less than five.
Many ?day?
questions are incorrectly classi-fied into ?year?, which accounts for the low accuracy on ?day?.
The reason lies inthat there are more ?year?
questions than ?day?
questions in the training questioncorpus (116 vs. 56).In general, we can extract three F1 features, one F4 feature, less than two F5 fea-tures, and one F6 feature from one question.
It is hard for SNoW classifier to train andtest independently on each of these types of the features because of the small featurenumber in one example question.
However, the numbers of F2 and F3 features in aquestion are normally more than ten.
So we take unigrams (F2) and bigrams (F3) asthe basic feature set.
Table 3 presents the accuracy when the rest four types of fea-tures are added into the basic feature set respectively.
As expected user preferencemakes the most significant improvement, 7.82% in accuracy1 and 7.90% in accuracy2.Special patterns also play an important role, which makes 2.6% accuracy1 improve-ment.
It is strange that event type makes such a modest improvement (0.5%).
Afteranalyzing the experimental results, we find that as there are only four event types, itmakes limited contribution to 10-class time granularity classification.8   ConclusionVarious features for time granularity classification of temporal questions are investi-gated in this paper.
User preference is shown to make a significant contributionto classification performance.
SNoW classifier, combining user preference, wordA Preliminary Work on Classifying Time Granularities of Temporal Questions 425N-grams, granularity of time expressions, special patterns and event types, achieves83.5% accuracy in classification, close to manually tagging accuracy 86.2%.AcknowledgementThis project is partially supported by Hong Kong RGC CERG (Grant No:PolyU5181/03E), and partially by CUHK Direct Grant (No: 2050330).References1) TREC (ed.
): The TREC-8 Question Answering Track Evaluation.
Text Retrieval Confer-ence TREC-8, Gaithersburg, MD (1999)2) Radev D. and Sundheim B.: Using TimeML in Question Answering.http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/TimeML-use-in-qa-v1.0.pdf, (2002)3) Li, X. and Roth, D.: Learning Question Classifiers.
Proceedings of the 19th InternationalConference on Computational Linguistics (2002) 556-5624) S. Abney, M. Collins, and A. Singhal: Answer Extraction.
Proceedings of the 6th ANLPConference (2000) 296-3015) Saquete E., Mart?nez-Barco P., Mu?oz R.: Splitting Complex Temporal Questions forQuestion Answering Systems.
Proceedings of the 42nd Annual Meeting of the Associationfor Computational Linguistics (2004) 567-5746) Diaz, F. and Jones, R.: Temporal Profiles of Queries.
Yahoo!
Research Labs TechnicalReport YRL-2004-022 (2004)7) Wei Li: Question Classification Using Language Modeling.
CIIR Technical Report (2002)8) Dell Zhang and Wee Sun Lee: Question Classification Using Support Vector Machines.Proceedings of the 26th Annual International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (2003) 26-329) Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda: Question ClassificationUsing HDAG Kernel.
Proceedings of Workshop on Multilingual Summarization andQuestion Answering (2003) 61-6810) Li X., Roth D., and Small K.: The Role of Semantic Information in Learning QuestionClassifiers.
Proceedings of the International Joint Conference on Natural Language Proc-essing (2004)11) Schilder, Frank & Habel, Christopher: Temporal Information Extraction for TemporalQuestion Answering.
In New Directions in Question Answering.
Papers from the 2003AAAI Spring Symposium TR SS-03-07 (2003) 34-4412) Rohini K. Srihari, Wei Li: A Question Answering System Supported by Information Ex-traction.
Proceedings of Association for Computational Linguistics (2000) 166-17213) Eduard Hovy, Laurie Geber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran:Towards Semantics-Based Answer Pinpointing.
Proceedings of the DARPA Human Lan-guage Technology Conference (2001)14) Hermjacob U.: Parsing and Question Classification for Question Answering.
Proceedingsof the Association for Computational Linguists Workshop on Open-Domain Question An-swering (2001) 17-2215) Ittycheriah, Franz M., Zhu W., Ratnaparki A. and Mammone R.: Question Answering Us-ing Maximum Entropy Components.
Proceedings of the North American chapter of theAssociation for Computational Linguistics (2001) 33-39
