Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 89?94,Prague, June 2007. c?2007 Association for Computational LinguisticsExperiments of UNED at the ThirdRecognising Textual Entailment ChallengeA?lvaro Rodrigo, Anselmo Pen?as, Jesu?s Herrera, Felisa VerdejoDepartmento de Lenguajes y Sistemas Informa?ticosUniversidad Nacional de Educacio?n a DistanciaMadrid, Spain{alvarory, anselmo, jesus.herrera, felisa}@lsi.uned.esAbstractThis paper describes the experiments devel-oped and the results obtained in the partic-ipation of UNED in the Third RecognisingTextual Entailment (RTE) Challenge.
Theexperiments are focused on the study of theeffect of named entities in the recognitionof textual entailment.
While Named EntityRecognition (NER) provides remarkable re-sults (accuracy over 70%) for RTE on QAtask, IE task requires more sophisticatedtreatment of named entities such as the iden-tification of relations between them.1 IntroductionThe systems presented to the Third RecognizingTextual Entailment Challenge are based on the onepresented to the Second RTE Challenge (Herreraet al, 2006b) and the ones presented to the An-swer Validation Exercise (AVE) 2006 (Rodrigo etal., 2007).Since a high quantity of pairs of RTE-3 collec-tions contain named entities (82.6% of the hypothe-ses in the test collection contain at least one namedentity), the objective of this work is to study the ef-fect of named entity recognition on textual entail-ment in the framework of the Third RTE Challenge.In short, the techniques involved in the experi-ments in order to reach these objectives are:?
Lexical overlapping between ngrams of textand hypothesis.?
Entailment between named entities.?
Branch overlapping between dependency treesof text and hypothesis.In section 2, the main components of the systemsare described in detail.
Section 3 describes the infor-mation our systems use for the entailment decision.The description of the two runs submitted are givenin Section 4.
The results obtained and its analysis aredescribed in Section 5.
Section 6 shows a discussionof the results.
Finally, some conclusions and futurework are given.2 Systems DescriptionThe proposed systems are based on surface tech-niques of lexical and syntactic analysis consideringeach task (Information Extraction, Information Re-trieval, Question Answering and Text Summariza-tion) of the RTE Challenge independently.The systems accept pairs of text snippets (text andhypothesis) at the input and give a boolean value atthe output: YES if the text entails the hypothesis andNO otherwise.
This value is obtained by the appli-cation of the learned model by a SVM classifier.The main components of the systems are the fol-lowing:2.1 Linguistic processingFirstly, each text-hypothesis pair is preprocessed inorder to obtain the following information for the en-tailment decision:?
POS: a Part of Speech Tagging is performed inorder to obtain lemmas for both text and hy-pothesis using the Freeling POS tagger (Car-reras et al, 2004).89<t>...Iraq invaded Kuwait on <TIMEX>August 2 1990</TIMEX>...</t><h>Iraq invaded Kuwait in <NUMEX>1990</NUMEX></h>Figure 1: Example of an error when disambiguating the named entity type.<t>...Chernobyl accident began on<ENTITY>Saturday April 26 1986</ENTITY>...</t><h>The Chernobyl disaster was in <ENTITY>1986</ENTITY></h>Figure 2: Example of a pair that justifies the process of entailment.<pair id=??5??
entailment=??NO??
task=??IE??
length=??short?
?><t>The Communist Party USA was a small Maoist political partywhich was founded in 1965 by members of the Communist Party aroundMichael Laski who took the side of China in the Sino-Soviet split.</t><h>Michael Laski was an opponent of China.</h></pair><pair id=??7??
entailment=??NO??
task=??IE??
length=??short?
?><t>Sandra Goudie was first elected to Parliament in the 2002elections, narrowly winning the seat of Coromandel by defeatingLabour candidate Max Purnell and pushing incumbent Green MPJeanette Fitzsimons into third place.</t><h>Sandra Goudie was defeated by Max Purnell.</h></pair><pair id=??8??
entailment=??NO??
task=??IE??
length=??short??><t>Ms.
Minton left Australia in 1961 to pursue her studies inLondon.</t><h>Ms.
Minton was born in Australia.</h></pair>Figure 3: IE pairs with entailment between named entities but not between named entities relations.90?
NER: the Freeling Named Entity Recogniser isalso applied to recover the information neededby the named entity entailment module that isdescribed in the following section.
Numeric ex-pressions, proper nouns and temporal expres-sions of each text and hypothesis are tagged.?
Dependency analysis: a dependency tree ofeach text and hypothesis is obtained using Lin?sMinipar (Lin, 1998).2.2 Entailment between named entitiesOnce the named entities of the hypothesis and thetext are detected, the next step is to determine theentailment relations between the named entities inthe text and the named entities in the hypothesis.
In(Rodrigo et al, 2007) the following entailment rela-tions between named entities were defined:1.
A Proper Noun E1 entails a Proper Noun E2 ifthe text string of E1 contains the text string ofE2.2.
A Time Expression T1 entails a Time Expres-sion T2 if the time range of T1 is included inthe time range of T2.3.
A numeric expression N1 entails a numeric ex-pression N2 if the range associated to N2 en-closes the range of N1.Some characters change in different expressionsof the same named entity as, for example, in a propernoun with different wordings (e.g.
Yasser, Yaser,Yasir).
To detect the entailment in these situations,when the previous process fails, we implemented amodified entailment decision process taking into ac-count the edit distance of Levenshtein (Levensthein,1966).
Thus, if two named entities differ in less than20%, then we assume that exists an entailment rela-tion between these named entities.However, this definition of named entities entail-ment does not support errors due to wrong namedentities classification as we can see in Figure 1.
Theexpression 1990 represents a year but it is recog-nised as a numeric expression in the hypothesis.However the same expression is recognised as a tem-poral expression in the text and, therefore, the ex-pression in the hypothesis cannot be entailed by itaccording to the named entities entailment definitionabove.We quantified the effect of these errors in recog-nising textual entailment.
For this purpose, we de-veloped the following two settings:1.
A system based in dependency analysis andWordNet (Herrera et al, 2006b) that uses thecategorization given by the NER tool, wherethe entailment relations between named entitiesare the previously ones defined.2.
The same system based on dependency analysisand WordNet but not using the categorizationgiven by the NER tool.
All named entities de-tected receive the same tag and a named entityE1 entails a named entity E2 if the text stringof E1 contains the text string of E2 (see Figure2).We checked the performance of these two settingsover the test corpus set of the Second RTE Chal-lenge.
The results obtained, using the accuracy mea-sure that is the fraction of correct responses accord-ing to (Dagan et al, 2006), are shown in table 1.
Thetable shows that with an easier and a more robustprocessing (NER without classification) the perfor-mance is not only maintained, but it is even slightlyhigher.This fact led us to ignore the named entity catego-rization given by the tool and assume that text andhypothesis are related and close texts where sameexpressions must receive same categories, withoutthe need of classification.
Thus, all detected namedentities receive the same tag and we consider that anamed entity E1 entails a named entity E2 if the textstring of E1 contains the text string of E2.Table 1: Entailment between numeric expressions.AccuracySetting 1 0.610Setting 2 0.6142.3 Sentence level matchingA tree matching module, which searches for match-ing branches into the hypotheses?
dependency trees,is used.
There is a potential matching branch perleaf.
A branch from the hypothesis is considered91a ?matching branch?
only if all its nodes from theroot to the leaf are involved in a lexical entailment(Herrera et al, 2006a).
In this way, the subtree con-formed by all the matching branches from a hypoth-esis?
dependency tree is included in the respectivetext?s dependency tree, giving an idea of tree inclu-sion.We assumed that the larger is the included sub-tree of the hypothesis?
dependency tree, the moresemantically similar are the text and the hypothesis.Thus, the existence or absence of an entailment rela-tion from a text to its respective hypothesis considersthe portion of the hypothesis?
tree that is included inthe text?s tree.3 Entailment decisionA SVM classifier was applied in order to train amodel from the development corpus.
The model wastrained with a set of features obtained from the pro-cessing described above.
The features we have usedand the training strategies were the following:3.1 FeaturesWe prepared the following features to feed the SVMmodel:1.
Percentage of nodes of the hypothesis?
de-pendency tree pertaining to matching branchesaccording to section 2.3 considering, respec-tively:?
Lexical entailment between the words ofthe snippets involved.?
Lexical entailment between the lemmas ofthe snippets involved.2.
Percentage of words of the hypothesis in thetext (treated as bags of words).3.
Percentage of unigrams (lemmas) of the hy-pothesis in the text (treated as bags of lemmas).4.
Percentage of bigrams (lemmas) of the hypoth-esis in the text (treated as bags of lemmas).5.
Percentage of trigrams (lemmas) of the hypoth-esis in the text (treated as bags of lemmas).6.
A boolean value indicating if there is or not anynamed entity in the hypothesis that is not en-tailed by one or more named entities in the textaccording to the named entity entailment deci-sion described in section 2.2.Table 2: Experiments with separate training over thedevelopment corpus using cross validation.Accuracy with Accuracy withthe same model a different modelfor all tasks for each taskSetting 1 0.64 0.67Setting 2 0.62 0.66Table 3: Experiments with separate training over thetest corpus.Accuracy with Accuracy withthe same model a different modelfor all tasks for each taskSetting 1 0.59 0.62Setting 2 0.60 0.64Table 4: Results for run 1 and run 2.Accuracyrun 1 run 2IE 52.50% 53.50%IR 67% 67%QA 72% 72%SUM 58% 60%Overall 62.38% 63.12%3.2 TrainingAbout the decision of how to perform the trainingin our SVM models, we wanted to study the effectof training a unique model compared to training onedifferent model per task.For this purpose we used the following two set-tings:1.
A SVM model that uses features 2, 3, 4 and 5from section 3.1.2.
A SVM model that uses features 2, 3, 4, 5 and6 from section 3.1.Each setting was training using cross validationover the development set of the Third RTE Chal-lenge in two different ways:1.
Training a unique model for all pairs.922.
Training one model for each task.
Each modelis trained with only pairs from the same taskthat the model will predict.The results obtained in the experiments are shownin table 2.
As we can see in the table, with the train-ing of one model for each task results are slightlybetter, increasing performance of both settings.
Tak-ing into account these results, we took the decisionof using a different training for each task in the runssubmitted.Our decision was confirmed after the runs submis-sion to RTE-3 Challenge with new experiments overthe RTE-3 test corpus, using the RTE-3 developmentcorpus as training (see table 3 for results).4 Runs SubmittedTwo different runs were submitted to the Third RTEChallenge.
Each run was trained using the methoddescribed in section 3.2 with the following subset ofthe features described in section 3.1:?
Run 1 was obtained using the features 2, 3,4 and 5 from section 3.1.
These features ob-tained good results for pairs from the QA task,as we can see in (Rodrigo et al, 2007), andwe wanted to check their performance in othertasks.?
Run 2 was obtained using the following fea-tures for each task:?
IE: features 2, 3, 4, 5 and 6 from section3.1.
These ones were the features that ob-tained the best results for IE pairs in ourexperiments over the development set.?
IR: features 2, 3, 4 and 5 from section 3.1.These ones were the features that obtainedbest results for IR pairs in our experimentsover the development set.?
QA: feature 6 from section 3.1.
We chosethis feature, which had obtained an ac-curacy over 70% in previous experimentsover the development set in QA pairs, tostudy the effect of named entities in QApairs.?
SUM: features 1, 2 and 3 from section 3.1.We selected these features to show the im-portance of dependency analysis in SUMpairs as it is shown in section 6.5 ResultsAccuracy was applied as the main measure to theparticipating systems.The results obtained over the test corpus for thetwo runs submitted are shown in table 4.As we can see in both runs, different accuracy val-ues are obtained depending on the task.
The best re-sult is obtained in pairs from QA with a 72% accu-racy in the two runs, although two different systemsare applied.
This result pushes us to use this systemfor Answer Validation (Pen?as et al, 2007).
Resultsin run 2, which uses a different setting for each task,are slightly better than results in run 1, but only inIE and SUM.
However, results are too close to ac-cept a confirmation of our initial intuition that pairsfrom different tasks could need not only a differenttraining, but also the use of different approaches forthe entailment decision.6 DiscussionIn run 2 we used NER for IE and QA, the two taskswith the higher percentage of pairs with at least onenamed entity in the hypothesis (98.5% in IE and97% in QA).Our previous work about the use of named enti-ties in textual entailment (Rodrigo et al, 2007) sug-gested that NER permitted to obtain good results.However, after the RTE-3 experience, we found thatthe use of NER does not improve results in all tasks,but only in QA in a solid way with the previouswork.We performed a qualitative study over the IE pairsshowing that, as it can be expected, in pairs from IEthe relations between named entities are more im-portant that named entities themselves.Figure 3 shows some examples where all namedentities are entailed but not the relation betweenthem.
In pair 5 both Michael Laski and China areentailed but the relation between them is took theside of in the text, and was an opponent of in thehypothesis.
The same problem appears in the otherpairs with the relation left instead was born in (pair8) or passive voice instead active voice (pair 7).Comparing run 1 and run 2, dependency analysisshows its usefulness in SUM pairs, where texts andhypotheses have a higher syntactic parallelism thanin pairs from other tasks.
This statement is shown93Table 5: Percentage of hypothesis nodes in matchingbranches.PercentageSUM 75,505%IE 7,353%IR 6,422%QA 8,496%in table 5 where the percentage of hypothesis nodespertaining to matching branches in the dependencytree is much higher in SUM pairs than in the rest oftasks.This syntactic parallelism seems to be the respon-sible for the 2% increasing between the first and thesecond run in SUM pairs.7 Conclusions and future workThe experiments have been focused on the study ofthe importance of considering entailment betweennamed entities in the recognition of textual entail-ment, and the use of a separate training for each task.As we have seen, both approaches increase slightlythe accuracy of the proposed systems.
As we havealso shown, different approaches for each task couldalso increase the system performance.Future work is focused on improving the perfor-mance in IE pairs taking into account relations be-tween named entities.AcknowledgmentsThis work has been partially supported by the Span-ish Ministry of Science and Technology within theproject R2D2-SyEMBRA (TIC-2003-07158-C04-02), the Regional Government of Madrid under theResearch Network MAVIR (S-0505/TIC-0267), theEducation Council of the Regional Government ofMadrid and the European Social Fund.ReferencesX.
Carreras, I. Chao, L.
Padro?, and M. Padro?.
2004.FreeLing: An Open-Source Suite of Language An-alyzers..
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC04).
Lisbon, Portugal, 2004.I.
Dagan, O. Glickman and B. Magnini.
2006.
ThePASCAL Recognising Textual Entailment Challenge.In Quin?onero-Candela et al, editors, MLCW 2005,LNAI Volume 3944, Jan 2006, Pages 177 - 190.J.
Herrera, A.
Pen?as and F. Verdejo.
2006a.
Textual En-tailment Recognition Based on Dependency Analysisand WordNet.
In Quin?onero-Candela et al, editors,MLCW 2005, LNAI Volume 3944, Jan 2006, Pages231-239.J.
Herrera, A.
Pen?as, A?.
Rodrigo and F. Verdejo.
2006b.UNED at PASCAL RTE-2 Challenge.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.V.
I. Levensthein.
1966.
Binary Codes Capable of Cor-recting Deletions, Insertions and Reversals.
In SovietPhysics - Doklady, volume 10, pages 707710, 1966.D.
Lin.
1998.
Dependency-based Evaluation of MINI-PAR.
Workshop on the Evaluation of Parsing Systems,Granada, Spain, May, 1998.A.
Pen?as, A?.
Rodrigo, V. Sama and F. Verdejo.
2007.Overview of the Answer Validation Exercise 2006.
InLecture Notes in Computer Science.
In press.A?.
Rodrigo, A.
Pen?as, J. Herrera and F. Verdejo.
2007.The Effect of Entity Recognition on Answer Validation.In Lecture Notes in Computer Science.
In press.94
