Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?36,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsTopic Classification of Blog Posts Using Distant SupervisionStephanie D. HusbyUniversity of Albertashusby@ualberta.caDenilson BarbosaUniversity of Albertadenilson@ualberta.caAbstractClassifying blog posts by topics is usefulfor applications such as search and market-ing.
However, topic classification is timeconsuming and error prone, especially in anopen domain such as the blogosphere.
Thestate-of-the-art relies on supervised meth-ods, requiring considerable training effort,that use the whole corpus vocabulary as fea-tures, demanding considerable memory toprocess.
We show an effective alternativewhereby distant supervision is used to ob-tain training data: we use Wikipedia arti-cles labelled with Freebase domains.
Weaddress the memory requirements by usingonly named entities as features.
We test ourclassifier on a sample of blog posts, and re-port up to 0.69 accuracy for multi-class la-belling and 0.9 for binary classification.1 IntroductionWith the ever increasing popularity of blogginggrows the need of finding ways for better or-ganizing the blogosphere.
Besides identifyingSPAM from legitimate blogs, one promising ideais to classify blog posts into topics such as travel,sports, religion, and so on, which could lead tobetter ways of exploring the blogosphere.
Be-sides navigation, blog classification can be usefulas a data preprocessing step before other formsof analysis can be done: for example companiescan view the perception and reception of prod-ucts, movies, books and more based on opinionsin blogs of different segments.We approach the problem by using machinelearning.
In particular, in the development of alearning-based classifier, two crucial tasks are thechoice of the features and the building of train-ing data.
We adopt a novel approach when se-lecting features: we use an off-the-shelf NamedEntity Recognition (NER) tool to identify entitiesin the text.
Our hypothesis is that one can de-tect the topic of a post by focusing on the entitiesdiscussed in the post.
Previous text classificationtools use the entire vocabulary as potential fea-tures, which is a superset of our feature set.
Ourresults show that despite using a smaller featureset, our method can achieve very high accuracy.Obtaining training data is a challenge for mostlearning tools, as it often involves manual inspec-tion of hundreds or thousands of examples.
Weaddress this by using distant supervision, wherea separate dataset is used to obtain training datafor the classifier.
The distant dataset used hereis Freebase1, which is an open online database,along with related Wikipedia articles.
The classesin our tests are domains in Freebase, which aredefined by their curators.Summary of Results.
For our evaluation, weuse a large sample of blog posts from a pub-lic snapshot of the blogosphere, collected around2008.
These posts are manually labeled by volun-teers (undergraduate students in Computing Sci-ence), and used as the ground-truth test data.Our results indicate that training a classifierrelying on named entities using Freebase andWikipedia, can achieve high accuracy levels onmanually annotated data.
We also identify somepotential problems related to selecting the cate-gories to be used in the classification.
Overall,our results indicate that robust classifiers are pos-sible using off-the-shelf tools and freely available1http://www.freebase.com/.28training data.2 Related WorkOur work is related to topic identification tech-niques such as Latent Dirichlet Analysis (LDA),Latent Semantic Analysis (LSA) and Latent Se-mantic Indexing (LSI) (Steyvers and Griffiths,2007).
These techniques infer possible topicclasses, and use unsupervised learning (cluster-ing) approaches.
In contrast, our technique allowsthe specification of classes (topics) of interest andattempts to classify text within those classes only.Next we discuss two other lines of work moreclosely related to ours.Blog classification.
There have been few at-tempts at classifying blog posts by topic.
Mostprevious methods focus on classification of theauthors and the sentiment of the posts.Ikeda et al (2008) discussed the classificationof blog authors by gender and age.
They use asemi-supervised technique and look at the blogsin groups of two or more.
These groupings arebased on which are relatively similar and rela-tively different.
They assume that multiple en-tries from the same author are more similar toeach other than to posts from other blogs, anduse this to train the classifier.
The classifier theyuse is support vector machines, and the bag-of-words feature representation.
Thus, they considerall unique words in their classification.
They findtheir methods to be 70-95% accurate on age clas-sification, depending on the particular age class(i.e.
the 20s vs the 30s class is more difficult todistinguish than the 20s vs the 50s) and up to 91%accurate on gender identification.
This is quitedifferent than the approach presented here, as weare examining topic classification.Yang et al (2007) consider the sentiment (posi-tive or negative) analysis of blog posts.
Their clas-sifier is trained at the sentence level and appliedto the entire document.
They use emoticons tofirst create training data and then use support vec-tor machines and conditional random fields in theactual classification.
They use individual wordsas features and find that conditional random fieldsoutperform support vector machines.
This paperworks both with blog posts and distance learningbased on the emoticons, however this type of dis-tant supervision is slightly different than our ap-proach.
It may also be referred to as using weaklylabeled data.Elgersma and de Rijke (2008) classify blogsas personal vs non-personal.
The authors definepersonal blogs as diary or journal, presenting per-sonal accounts of daily life and intimate thoughtsand feelings.
They use the frequency of wordsmore often used in personal blogs versus thosemore frequently used in general blogs, pronouns,in-links, out-links and hosts as the features for theblogs.
They then perform supervised training onthe data using a set of 152 manually labeled blogsto train their classifier.
The results show that thedecision tree method produced the highest accu-racy at about 90% (Elgersma and de Rijke, 2008).A work which looks at true topic classifica-tion of blogs, as is being done here, is that ofHashimoto and Kurohashi (2008), who use a do-main dictionary to classify blog posts without ma-chine learning (i.e., using a rule-based system).They use keywords for each domain, or categoryas the basis for classification.
They then createa score of a blog post based on the number ofkeywords from each domain; the domain with thehighest count becomes the category for that post.They also expand the keywords in their domain byadding new words on the fly.
This is done by tak-ing an unknown word (one that does not currentlyexist in a domain) and attempting to categorize itusing its online search results and/or Wikipediaarticle.
They attempt to classify the results or ar-ticle and then, in turn, classify the word.
Theyfind their classification method to be up to 99%accurate.
This idea can be related to the use ofFreebase as the domain dictionary in the currentproblem, but will be expanded to include machinelearning techniques, which these authors avoid.Distant supervision.
Distant supervision is arelatively new idea in the field of machine learn-ing.
The term was first introduced by Mintz etal.
(2009) in 2009 in their paper on relation ex-traction.
The idea is to use facts in Freebase toobtain training data (i.e., provide distant supervi-sion), based on the premise that if a pair of enti-ties that have a relation in Freebase, it will likelybe expressed in some way in a new context.
Theyfound their approach to be about 66-69% accu-rate on large amounts of data.
Although the goalof their work (namely, extracting relations fromthe text) was different from ours, the use of Free-base and entities is directly related to the work29presented here.Go et al (Go et al, 2009) use distant supervi-sion to label the sentiment associated with Twitterposts.
They use tweets containing emoticons tolabel the training data, as follows.
If a tweet con-tains a :) or an : ( then it is considered to havea positive or a negative sentiment.
Those tweetswith multiple emoticons were discarded.
Thenemoticons themselves are removed from all data(to avoid them being used as features), and thelabeled data is used to train the classifier.
Theyfound their approach to be around 78-83% ac-curate using several different machine learningtechniques (Go et al, 2009).
The authors do notdiscuss their feature representations in detail, butmake use of both unigrams and bigrams.Phan et al (Phan et al, 2008) consider usinga universal data set to train a classifier for webdata similar to blogs .
This idea is very similarto the concept of distant supervision.
They con-sider Wikipedia and MEDLINE, as universal datasets, and they use the maximum entropy as theirclassifier.
They apply their methods to two prob-lems, topic clustering of web search results anddisease classification for medical abstracts; theyreport accuracy levels around 80%.3 MethodOur hypothesis is that one can predict the topic ofa blog post based on ?what?
that post is about.More precisely, we focus on the recognizablenamed entities that appear in the blog post.
Ourintuition is that if a blog post mentions ?BarackObama?
and the ?White House?
prominently, itis probably a post about politics.
On the otherhand, a post mentioning ?Edmonton Oilers?
and?Boston Bruins?
is most likely about hockey.
Nat-urally, there will be posts mentioning entitiesfrom different topics, say for example, a commentabout the president attending a hockey game.
Insuch cases, our hypothesis is that the other enti-ties in the same post would help break the tie asto which class the post belongs to.Our method consists of using a classifiertrained with all topics of interest.
We obtaintraining data using distant supervision, as fol-lows.
The topics come from Freebase, an open,online database compiled by volunteers.
At thetime of writing, it contains approximately 22 mil-lion objects which belong to one or more of a to-tal of 86 domains.
Each object in Freebase is aCategory Articles Distinct Entitiesgovernment 2,000 265,974celebrities 1,605 85,491food & drink 2,000 70,000religion 2,000 175,948sports 2,000 189,748travel 2,000 125,802other 2,000 384,139Table 1: Topic categories chosen from Freebase do-mainsunique person, place, thing or concept that existsin the world.
An example of an entity would be?Barack Obama?
or ?republican?.
A major datasource for Freebase is Wikipedia; indeed, thereis even a one-to-one mapping between articles inWikipedia and the corresponding objects in Free-base.Discussion.
Our motivation to use Freebase andWikipedia comes from their large size and freeavailability, besides the fact these are fairly highquality resources?given the dedication of theircontributors.
It should be noted that this is a per-fect example where distant supervision comes asan ideal approach, in the sense that the classifica-tion of objects into domains (i.e., topics) is donemanually, and with great care, leading to highquality training data.
Moreover, the nature of bothdatasets, which allow any web user to update andcontribute to them, leads us to believe they will re-main up-to-date, and will likely contain mentionsto recent events which the bloggers would be dis-cussing.
Thus, one should expect a high overlapbetween the named entities in these resources andthe blog posts.3.1 Classifying Blog PostsThe classification of blog posts by topic is done byusing the named entity recognition tool to extractall named entities (features) for the blog post, andfeeding those to the topic classifier.
We considertwo classification tasks:?
Multi-class: In this case, we are given a blogpost and the task is to determine which of the7 topics (as in Table 1) it belongs to.?
Binary classification: In this case, we aregiven a blog post and a specific topic (i.e.,30Blog (Test) Data Wikipedia (Training) Datawords/post entities/post words/article entities/articlecelebrities 420 49 2,411 311food & drink 256 28 1,782 144government 20,176 2,363 6,013 803other 395 50 10,930 1,245religion 516 52 3,496 402sports 498 73 4,716 741travel 359 41 2,101 239Table 2: Average word count and entity count per blog post and per Wikipedia article.class), and the task is to determine whetheror not the post belongs in that topic.The multi-class task is more relevant in an ex-ploratory scenario, where the user would browsethrough a collection of posts and use the classi-fier as a means to organize such exploration.
Thebinary classification, on the other hand, is morerelevant in a scenario where the user has a spe-cific need.
For example, a journalist interested inpolitics would rather use a classifier that filteredout posts which are not relevant.
By their nature,the binary classification task demands higher ac-curacy.Features The only features that make sense touse in our classification are those named entitiesthat appear both in the training data (Wikipedia)and the test data (the blog posts).
That is, weuse only those entities which exist in at least oneblog post and in at least one Wikipedia article.
Itis worth mentioning that this reduces drasticallythe memory needed for classification, comparedto previous methods that use the entire vocabularyas features.Each data point (blog or Wikipedia article) isrepresented by a vector, where each column of thevector is an entity.
Two feature representationswere created:?
In-out: in this representation we record thepresence (1) or absences (0) of the named en-tity in the data point; and?
Count: in this representation we record thenumber of times the named entity appears inthe data point.In-Out Count10-Fold Test 10-Fold TestNB 0.59 0.37 0.51 0.29SVM 0.26 0.18 0.49 0.22NBM 0.71 0.57 0.68 0.60Table 3: Summary of Accuracy on Multi-Class Data4 Experimental DesignWe collected the training data as follows.
First,we discarded generic Freebase domains such asCommon and Metaweb System Types, which donot correspond to meaningful topics.
We alsodiscarded other domains which were too narrow,comprising only a few objects.
We then concen-trated on domains for which we could find manyobjects and for which we could perform a reason-able evaluation.
For the purposes of this paper,the 7 domains shown in Table 1 were used as top-ics.
For each topic, we find all Freebase objectsand their corresponding Wikipedia articles, andwe collect the 2,000 longest articles (as those aremost likely to contain the most named entities).The exception was the celebrities topic, for whichonly 1,605 articles were used.
From these articles,we extract the named entities (i.e., the features),thus obtaining our training data.
In the end, weused 4,000 articles for each binary classificationexperiment and 13,605 for the multi-class one.As for test data, we used the ICWSM 2009Spinn3r Blog Dataset (Burton et al, 2009), whichwas collected during the summer of 2008, coin-ciding with the build-up for the 2008 PresidentialElections in the US.
In total, the collections hasapproximately 25M blog posts in English.
For31a b c d e f g ?
classified as0 0 0 0 0 0 50 a celebrities0 0 0 0 0 0 50 b food & drink0 0 15 27 0 0 8 c government0 0 0 0 0 0 50 d other0 0 0 0 0 0 50 e religion0 0 0 0 0 0 50 f sports0 0 0 0 0 0 50 g travelTable 4: Confusion Matrix of SVM on Test Set withIn-Out Rep.our evaluations, we relied on volunteers2 who la-beled hundreds of blogs, chosen among the mostpopular ones (this information is provided in thedataset), until we collected 50 blogs for each cat-egory.
For the binary classifications, we used 50blogs as positive examples and 200 blogs ran-domly chosen from the other topics as negativeexamples.
For the multi-class experiment, we usethe 350 blogs corresponding to the 7 categories.Both the blogs and the Wikipedia articles weretagged using the Stanford Named Entity Recog-nizer (Finkel et al, 2005), which labels the en-tities according to these types: Time, Location,Organization, Person, Money, Percent, Date, andMiscellaneous.
After several tests, we foundthat Location, Organization, Person and Miscel-laneous were the most useful for topic classifi-cation, and we thus ignored the rest for the re-sults presented here.
As mentioned above, we useonly the named entities in both the training andtest data, which, in our experiments, consisted of14,995 unique entities.Classifiers.
We performed all our tests usingthe Weka suite (Hall et al, 2009), and we testedthe following classifiers.
The first was the NaiveBayes (John and Langley, 1995) (NB for short),which has been successfully applied to text clas-sification problems (Manning et al, 2008).
Itassumes attribute independence, which makeslearning simpler when the number of attributesis large.
A variation of the NB classifier, calledNaive Bayes Multinomial (NBM) (McCallum andNigam, 1998), was also tested, as it was shownto perform better for text classification tasks inwhich the vocabulary is large (as in our case).
Fi-nally, we also used the LibSVM classifier (Chang2Undergraduate students in our lab.In-Out Count10-Fold Test 10-Fold TestNB 0.66 0.59 0.58 0.32SVM 0.33 0.22 0.53 0.22NBM 0.76 0.64 0.72 0.64Table 5: Summary of Accuracy on Multi-Class with-out Travela b c d e ?
classified as46 0 0 3 1 a celebrities3 25 21 0 1 b government40 2 0 3 5 c other5 1 1 43 0 d religion13 0 0 0 37 e sportsTable 6: Confusion Matrix of NB on Test Set with In-Out Repand Lin, 2001) (SVM), which is an implementa-tion of support vector machines, a binary linearclassifier.
The results reported in this paper wereobtained with LibSVM?s default tuning parame-ters.
SVMs are often used successfully in textclassification problems (Ikeda et al, 2008; Yanget al, 2007; Go et al, 2009).
These classifierswere chosen specifically due to their success ratewith text classification as well as with other appli-cations of distant supervision.5 Experimental ResultsWe now present our experimental results, startingwith the multi-class task, in which the goal is toclassify each post into one of 7 possible classes(as in Figure 1).Accuracy in the Multi-class Task We reportaccuracy numbers both for 10-fold cross valida-tion (on the training data) as well as on the manu-ally labelled blog posts (test data).
The summaryof results is given in Table 3.
Accuracy as high as60% was obtained using the NBM classifier.
Thestandard NB technique performed quite poorly inthis case; as expected, NBM outperformed NB bya factor of almost two, using the count represen-tation.
Overall, the count representation producedbetter results than in-out on the test data, whilelosing on the cross-validation tests.
Surprisingly,SVM performed very poorly in our tests.These results were not as high as expected, so32In-Out Count10-Fold Test 10-Fold TestNB 0.70 0.60 0.62 0.40SVM 0.47 0.38 0.67 0.40NBM 0.79 0.67 0.76 0.69Table 7: Summary of Accuracy on Multi-Class sans Travel, Food(a) (b) (c)Figure 1: Precision and Recall for Multi-Class Results Using Count Representation.
Legend: CEL (Celebrities),FOO (food & drink), GOV (government), OTH (other), REL (religion), SPO (sport), TRA (travel).we inspected why that was the case.
What wefound was that the classifiers were strongly bi-ased towards the travel topic: NB, for instance,classified 211/350=60% of the samples that way,instead of the expected 14% (50/350).
In the caseof SVM, this effect was more pronounced: 88%of the posts were classified as travel.
Table 4shows the confusion matrix for the worst resultsin our tests (SVM with in-out feature representa-tion), and fully illustrates the point.We then repeated the tests after removing thetravel topic, resulting in an increase in accuracyof about 5%, as shown in Table 5.
However, an-other inspection at the confusion matrices in thiscase revealed that the food & drink class receiveda disproportionate number of classifications.The highest accuracy numbers we obtained forthe multi-class setting were when we further re-moved the food & drink class (Table 7).
Consis-tent with previous results, our highest accuracywas achieved with NBM using the count featurerepresentation: 69%.
Table 6. gives the confusionmatrix for this task, using NB.
We can see thatthe posts are much better distributed now than inthe previous cases, approximating the ideal confu-sion matrix which would have only non-zero en-tries in the diagonal, signifying all instances werecorrectly classified.Recall in Multi-Class experiment.
Accuracy(or precision, as used in information retrieval)measures the fraction of correct answers amongthose provided by the classifier.
A complemen-tary performance metric is recall, which indicatesthe fraction of correctly classified instances out ofthe total instances of the class.
Figure 1 shows thebreakdown of precision and recall for each classusing the NBM classifier, using the Count featurerepresentation for the tests with all 7 classes (a),as well as after removing travel (b) and both traveland food&drink (c).As one can see, the overall accuracy by classdoes change (and improves) as we remove traveland then food&drink.
However, the most signif-icant change is for the class other.
On the otherhand, both the accuracy and recall for celebrities,religion and sports remain virtually unchangedwith the removal of these classes.Discussion of Multi-class results.
One clearconclusion from our tests is the superiority ofNBM using Count features for this task.
The mar-gin of this superiority comes somewhat as a sur-prise in some cases, especially when one com-pares against SVM, but does not leave much room33for argument.As expected, some classes are much easier tohandle than others.
Classes such as celebritiesare expected to be hard as documents in this topicdeal with everything about the celebrities, includ-ing their preferences in politics, sports, the foodthey like and the places they travel.
Looking atFigure 1, one possible factor for the relativelylower performance for travel and food & drinkcould be that the training data in these categorieshave the lowest average word count and entitycount (recall Table 2).
Another category with rel-atively less counts is celebrities, which can alsobe explained by the lower document count (1,605available articles relating to this topic in Free-base).Another plausible explanation is that articlesin some classes can often be classified in eithertopic.
Articles in the travel topic can include in-formation about many things that can be done andseen around the world, such as the culinary traitsof the places being discussed and the celebritiesthat visited them, or the religious figures that rep-resent them.
Thus, one would expect some over-lap among the named entities relating to these lesswell-defined classes.
These concepts tie easilyinto the various other topic categories we haveconsidered and help to explain why misclassifi-cation was higher for these cases.We also observed that with the NBM results, inall three variations of the multi-class experiments,there was a fairly consistent trade-off between re-call and precision for the celebrities class.
Theerroneous classification of posts into celebritiescould be explained in a similar way to those infood&travel.
The fact that celebrities can exist insports, politics, and religion means that many ofthe posts may fit into two or more classes and ex-plains the errors.
The best way to explore this fur-ther would be to do multiple class labels per postrather than just choosing a single label.One interesting point that Figure 1 supports isthe following.
Recall that the need for the classother is mostly to test whether the classifier canhandle ?noise?
(blogs which are too general to beclassified).
With this in mind, the trend in Figure 1(increasing classification performance as classesare removed) is encouraging, as it indicates thatmore focused classes (e.g., religion and sports)can actually be separated well by a classifier us-ing distant supervision, even in the presence ofless well-defined classes.
Indeed, taken to the ex-treme, this argument would suggest that the per-formance in the binary classification scenario forsuch classes would be the highest (which is indeedthe case as we discuss next).5.1 Binary ClassificationWe now consider a different scenario, in whichthe task is to perform a binary classification.
Thegoal is to identify posts of a specific class amongstposts of all other classes.
The percentage of cor-rectly classified posts (i.e.
test data) in this task,based on each feature representation can be seenin Table 8.Overall, all classifiers performed much betterin this setting, although NBM still produced con-sistently better results, with accuracy in the mid-90% level for the count feature representation.
Itis worth noting that SVM performed much betterfor binary classifications compared to the multi-class experiments, in some cases tying or ever soslightly surpassing other methods.Also, note that the classifiers do a much bet-ter job on the more focused classes (e.g., religion,sports), just as was the case with the multi-classscenario.
In fact, the accuracy for such classesis near-perfect (92% for religion and 93% forsports).6 ConclusionThis paper makes two observations.
First, ournovel approach of using a standard named entitytagger to extract features for classification doesnot compromise classification accuracy.
Reduc-ing the feature contributes to increasing the scala-bility of topic classification, compared to the stateof the art which is to process the entire vocabu-lary.
The second observation is that distant super-vision is effective in obtaining training data: Byusing Freebase and Wikipedia to obtain trainingdata for standard machine learning classifiers, ac-curacy as high as mid-90% were achieved on ourbinary classification task, and around 70% for themulti-class task.Our tests confirmed the superiority of NBM fortext classification tasks, which had been observedbefore.
Moreover, our test also showed that thissuperior performance is very robust across a vari-ety of settings.
Our results also show that it is im-portant to consider topics carefully, as there canbe considerable overlap in many general classes34In-Out CountClass NB NBM SVM NB NBM SVMreligion 0.63 0.90 0.80 0.43 0.92 0.81government 0.96 0.85 0.80 0.88 0.82 0.87sports 0.62 0.79 0.79 0.90 0.93 0.79celebrities 0.60 0.68 0.80 0.40 0.76 0.80average 0.71 0.81 0.79 0.65 0.86 0.82Table 8: Accuracy of Binary Classification.and this can cause misclassification.
Obviously,such overlap is inevitable?and indeed expectingthat a single topic can be found for each post canbe viewed as a restriction.
The most straight-forward way to overcome this is by allowing mul-tiple class labels per sample, rather than forcing asingle classification.Given the difficulty of the task, we believe ourresults are a clear indication that distant supervi-sion is a very promising option for topic classifi-cation of social media content.Future Work.
One immediate avenue for fu-ture work is understanding whether there are tech-niques that can separate the classes with highoverlap, such as celebrities, food&drinks andtravel.
However, it is very hard even for humansto separate these classes, so it is not clear whatlevel of accuracy can be achieved.
Another optionis to examine additional features which could im-prove the accuracy of the classifier without dras-tically increasing the costs.
Features of the blogposts such as link structure and post length, whichwe disregarded, may improve classification.Moreover, one could use unsupervised meth-ods to find relations between the named entitiesand exploit those, e.g., for bootstrapping.
A simi-lar idea would be to exploit dependencies amongrelational terms involving entities, which couldeasily be done on blogs and the Wikipedia arti-cles.
Topic selection is another area for futurework.
Our selection of topics was very generaland based on Freebase domains, but a more de-tailed study of how to select more specific top-ics would be worthwhile.
For instance, one mightwant to further classify government into politicalparties, or issues (e.g., environment, energy, im-migration, etc.
).AcknowledgementsThis was supported in part by NSERC?NaturalSciences and Engineering Research Council,and the NSERC Business Intelligence Network(project Discerning Intelligence from Text).ReferencesK.
Burton, A. Java, and I. Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings ofthe Third Annual Conference on Weblogs and SocialMedia (ICWSM 2009), San Jose, CA.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.E.
Elgersma and M. de Rijke.
2008.
Personal vs non-personal blogs.
SIGIR, July.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
Proceedings of the 43nd Annual Meet-ing of the Association for Computational Linguis-tics (ACL 2005), pages 363?370.Evgeniy Gabrilovich and Shaul Markovitch.
2006.Overcoming the brittleness bottleneck usingwikipedia: enhancing text categorization withencyclopedic knowledge.
In proceedings of the21st national conference on Artificial intelligence -Volume 2, pages 1301?1306.
AAAI Press.A.
Go, R. Bhayani, and L. Huang.
2009.
Twitter sen-timent classification using distant supervision.
Pro-cessing, pages 1?6.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reuteman, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11.C.
Hashimoto and S. Kurohashi.
2008.
Blog catego-rization exploiting domain dictionary and dynam-ically estimated domains of unknown words.
Pro-ceedings of ACL-08, HLT Short Papers (CompanionVolume), pages 69?72, June.D.
Ikeda, H. Takamura, and M. Okumura.
2008.Semi-supervised learning for blog classification.35Association for the Advancement of Artificial Intel-ligence.George H. John and Pat Langley.
1995.
Estimatingcontinuous distributions in bayesian classifiers.
InProceedings of the Eleventh Conference on Uncer-tainty in Artificial Intelligence, pages 338?345.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to Informa-tion Retrieval.
Cambridge University Press.A.
McCallum and K. Nigam.
1998.
A comparisonof event models for naive bayes text classification.In AAAI-98 Workshop on Learning for Text Catego-rization.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.Distant supervision for relation extraction withoutlabeled data.
ACL-IJCNLP ?09: Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP,2:1003?1011.X.
Phan, L. Nguyen, and S. Horiguchi.
2008.
Learn-ing to classify short and sparse text & web with hid-den topics from large-scale data collections.
Inter-national World Wide Web Conference Committee,April.M.
Steyvers and T. Griffiths, 2007.
Latent SemanticAnalysis: A Road to Meaning, chapter Probabilistictopic models.
Laurence Erlbaum.C.
Yang, K. Lin, and H. Chen.
2007.
Emotion classi-fication using web blog corpora.36
