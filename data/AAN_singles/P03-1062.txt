Learning to predict pitch accents and prosodic boundaries in DutchErwin Marsi1, Martin Reynaert1, Antal van den Bosch1,Walter Daelemans2, Ve?ronique Hoste21 Tilburg UniversityILK / Computational Linguistics and AITilburg, The Netherlands{e.c.marsi,reynaert,antal.vdnbosch}@uvt.nl2 University of Antwerp,CNTSAntwerp, Belgium{daelem,hoste}@uia.ua.ac.beAbstractWe train a decision tree inducer (CART)and a memory-based classifier (MBL)on predicting prosodic pitch accents andbreaks in Dutch text, on the basis of shal-low, easy-to-compute features.
We trainthe algorithms on both tasks individu-ally and on the two tasks simultaneously.The parameters of both algorithms andthe selection of features are optimized pertask with iterative deepening, an efficientwrapper procedure that uses progressivesampling of training data.
Results showa consistent significant advantage of MBLover CART, and also indicate that taskcombination can be done at the cost oflittle generalization score loss.
Tests oncross-validated data and on held-out datayield F-scores of MBL on accent place-ment of 84 and 87, respectively, and onbreaks of 88 and 91, respectively.
Accentplacement is shown to outperform an in-formed baseline rule; reliably predictingbreaks other than those already indicatedby intra-sentential punctuation, however,appears to be more challenging.1 IntroductionAny text-to-speech (TTS) system that aims at pro-ducing understandable and natural-sounding out-put needs to have on-board methods for predict-ing prosody.
Most systems start with generatinga prosodic representation at the linguistic or sym-bolic level, followed by the actual phonetic real-ization in terms of (primarily) pitch, pauses, andsegmental durations.
The first step involves plac-ing pitch accents and inserting prosodic boundariesat the right locations (and may involve tune choiceas well).
Pitch accents correspond roughly to pitchmovements that lend emphasis to certain words inan utterance.
Prosodic breaks are audible interrup-tions in the flow of speech, typically realized by acombination of a pause, a boundary-marking pitchmovement, and lengthening of the phrase-final seg-ments.
Errors at this level may impede the listenerin the correct understanding of the spoken utterance(Cutler et al, 1997).
Predicting prosody is known tobe a hard problem that is thought to require informa-tion on syntactic boundaries, syntactic and seman-tic relations between constituents, discourse-levelknowledge, and phonological well-formedness con-straints (Hirschberg, 1993).
However, producing allthis information ?
using full parsing, including es-tablishing semanto-syntactic relations, and full dis-course analysis ?
is currently infeasible for a real-time system.
Resolving this dilemma has been thetopic of several studies in pitch accent placement(Hirschberg, 1993; Black, 1995; Pan and McKe-own, 1999; Pan and Hirschberg, 2000; Marsi et al,2002) and in prosodic boundary placement (Wangand Hirschberg, 1997; Taylor and Black, 1998).
Thecommonly adopted solution is to use shallow infor-mation sources that approximate full syntactic, se-mantic and discourse information, such as the wordsof the text themselves, their part-of-speech tags, ortheir information content (in general, or in the textat hand), since words with a high (semantic) infor-mation content or load tend to receive pitch accents(Ladd, 1996).Within this research paradigm, we investigatepitch accent and prosodic boundary placement forDutch, using an annotated corpus of newspaper text,and machine learning algorithms to produce classi-fiers for both tasks.
We address two questions thathave been left open thus far in previous work:1.
Is there an advantage in inducing decision treesfor both tasks, or is it better to not abstract fromindividual instances and use a memory-basedk-nearest neighbour classifier?2.
Is there an advantage in inducing classifiers forboth tasks individually, or can both tasks belearned together.The first question deals with a key difference be-tween standard decision tree induction and memory-based classification: how to deal with exceptionalinstances.
Decision trees, CART (Classificationand Regression Tree) in particular (Breiman et al,1984), have been among the first successful machinelearning algorithms applied to predicting pitch ac-cents and prosodic boundaries for TTS (Hirschberg,1993; Wang and Hirschberg, 1997).
Decision treeinduction finds, through heuristics, a minimally-sized decision tree that is estimated to generalizewell to unseen data.
Its minimality strategy makesthe algorithm reluctant to remember individual out-lier instances that would take long paths in the tree:typically, these are discarded.
This may work wellwhen outliers do not reoccur, but as demonstratedby (Daelemans et al, 1999), exceptions do typicallyreoccur in language data.
Hence, machine learn-ing algorithms that retain a memory trace of indi-vidual instances, like memory-based learning algo-rithms based on the k-nearest neighbour classifier,outperform decision tree or rule inducers preciselyfor this reason.Comparing the performance of machine learningalgorithms is not straightforward, and deserves care-ful methodological consideration.
For a fair com-parison, both algorithms should be objectively andautomatically optimized for the task to be learned.This point is made by (Daelemans and Hoste, 2002),who show that, for tasks such as word-sense dis-ambiguation and part-of-speech tagging, tuning al-gorithms in terms of feature selection and classifierparameters gives rise to significant improvements inperformance.
In this paper, therefore, we optimizeboth CART and MBL individually and per task, us-ing a heuristic optimization method called iterativedeepening.The second issue, that of task combination, stemsfrom the intuition that the two tasks have a lotin common.
For instance, (Hirschberg, 1993) re-ports that knowledge of the location of breaks facil-itates accent placement.
Although pitch accents andbreaks do not consistently occur at the same posi-tions, they are to some extent analogous to phrasechunks and head words in parsing: breaks markboundaries of intonational phrases, in which typi-cally at least one accent is placed.
A learner maythus be able to learn both tasks at the same time.Apart from the two issues raised, our work is alsopractically motivated.
Our goal is a good algorithmfor real-time TTS.
This is reflected in the type offeatures that we use as input.
These can be com-puted in real-time, and are language independent.We intend to show that this approach goes a longway towards generating high-quality prosody, cast-ing doubt on the need for more expensive sentenceand discourse analysis.The remainder of this paper has the followingstructure.
In Section 2 we define the task, describethe data, and the feature generation process whichinvolves POS tagging, syntactic chunking, and com-puting several information-theoretic metrics.
Fur-thermore, a brief overview is given of the algorithmswe used (CART and MBL).
Section 3 describes theexperimental procedure (ten-fold iterative deepen-ing) and the evaluation metrics (F-scores).
Section 4reports the results for predicting accents and majorprosodic boundaries with both classifiers.
It also re-ports their performance on held-out data and on twofully independent test sets.
The final section offerssome discussion and concluding remarks.2 Task definition, data, and machinelearnersTo explore the generalization abilities of machinelearning algorithms trained on placing pitch accentsand breaks in Dutch text, we define three classifica-tion tasks:Pitch accent placement ?
given a word form in itssentential context, decide whether it should beaccented.
This is a binary classification task.Break insertion ?
given a word form in its senten-tial context, decide whether it should be fol-lowed by a boundary.
This is a binary classi-fication task.Combined accent placement and break insertion?
given a word form in its sentential context,decide whether it should be accented andwhether it should be followed by a break.
Thisis a four-class task: no accent and no break; anaccent and no break; no accent and a break;an accent and a break.Finer-grained classifications could be envisioned,e.g.
predicting the type of pitch accent, but we assertthat finer classification, apart from being arguablyharder to annotate, could be deferred to later pro-cessing given an adequate level of precision and re-call on the present task.In the next subsections we describe which data weselected for annotation and how we annotated it withrespect to pitch accents and prosodic breaks.
Wethen describe the implementation of memory-basedlearning applied to the task.2.1 Prosodic annotation of the dataThe data used in our experiments consists of 201articles from the ILK corpus (a large collection ofDutch newspaper text), totalling 4,493 sentencesand 58,097 tokens (excluding punctuation).
We setapart 10 articles, containing 2,905 tokens (excludingpunctuation) as held-out data for testing purposes.As a preprocessing step, the data was tokenised bya rule-based Dutch tokeniser, splitting punctuationfrom words, and marking sentence endings.The articles were then prosodically annotated,without overlap, by four different annotators, andwere corrected in a second stage, again without over-lap, by two corrector-annotators.
The annotators?task was to indicate the locations of accents and/orbreaks that they preferred.
They used a custom an-notation tool which provided feedback in the formof synthesized speech.
In total, 23,488 accents wereplaced, which amounts to roughly one accent in twoand a half words.
8627 breaks were marked; 4601of these were sentence-internal breaks; the remain-der consisted of breaks at the end of sentences.2.2 Generating shallow featuresThe 201 prosodically-annotated articles were subse-quently processed through the following 15 featureconstruction steps, each contributing one feature perword form token.
An excerpt of the annotated datawith all generated symbolic and numeric1 features ispresented in Table 1.Word forms (Wrd) ?
The word form tokens formthe central unit to which other features are added.Pre- and post-punctuation ?
All punctuationmarks in the data are transferred to two separate fea-tures: a pre-punctuation feature (PreP) for punctua-tion marks such as quotation marks appearing beforethe token, and a post-punctuation feature (PostP) forpunctuation marks such as periods, commas, andquestion marks following the token.Part-of-speech (POS) tagging ?
We used MBTversion 1.0 (Daelemans et al, 1996) to develop amemory-based POS tagger trained on the Eindhovencorpus of written Dutch, which does not overlapwith our base data.
We split up the full POS tags intotwo features, the first (PosC) containing the mainPOS category, the second (PosF) the POS subfea-tures.Diacritical accent ?
Some tokens bear an ortho-graphical diacritical accent put there by the author toparticularly emphasize the token in question.
Theseaccents were stripped off the accented letter, andtransferred to a binary feature (DiA).NP and VP chunking (NpC & VpC) ?
An ap-proximation of the syntactic structure is provided bysimple noun phrase and verb phrase chunkers, whichtake word and POS information as input and arebased on a small number of manually written reg-ular expressions.
Phrase boundaries are encoded perword using three tags: ?B?
for chunk-initial words,?I?
for chunk-internal words, and ?O?
for words out-side chunks.
The NPs are identified according to thebase principle of one semantic head per chunk (non-recursive, base NPs).
VPs include only verbs, notthe verbal complements.IC ?
Information content (IC) of a word w isgiven by IC(w) = ?log(P (w)), where P(w) is esti-1Numeric features were rounded off to two decimal points,where appropriate.mated by the observed frequency of w in a large dis-joint corpus of about 1.7 GB of unannotated Dutchtext garnered from various sources.
Word forms notin this corpus were given the highest IC score, i.e.the value for hapax legomenae (words that occuronce).Bigram IC ?
IC on bigrams (BIC) was calculatedfor the bigrams (pairs of words) in the data, accord-ing to the same formula and corpus material as forunigram IC.TF*IDF ?
The TF*IDF metric (Salton, 1989) es-timates the relevance of a word in a document.
Doc-ument frequency counts for all token types were ob-tained from a subset of the same corpus as usedfor IC calculations.
TF*IDF and IC (previous twofeatures) have been succesfully tested as featuresfor accent prediction by (Pan and McKeown, 1999),who assert that IC is a more powerful predictor thanTF*IDF.Phrasometer ?
The phrasometer feature (PM) isthe summed log-likelihood of all n-grams the wordform occurs in, with n ranging from 1 to 25, andcomputed in an iterative growth procedure: log-likelihoods of n + 1-grams were computed by ex-panding all stored n-grams one word to the leftand to the right; only the n + 1-grams with higherlog-likelihood than that of the original n-gram arestored.
Computations are based on the complete ILKCorpus.Distance to previous occurrence ?
The distance,counted in the number of tokens, to previous occur-rence of a token within the same article (D2P).
Un-seen words were assigned the arbitrary high defaultdistance of 9999.Distance to sentence boundaries ?
Distance ofthe current token to the start of the sentence (D2S)and to the end of the sentence (D2E), both measuredas a proportion of the total sentence length measuredin tokens.2.3 CART: Classification and regression treesCART (Breiman et al, 1984) is a statistical methodto induce a classification or regression tree from agiven set of instances.
An instance consists of afixed-length vector of n feature-value pairs, and aninformation field containing the classification of thatparticular feature-value vector.
Each node in theCART tree contains a binary test on some categor-ical or numerical feature in the input vector.
In thecase of classification, the leaves contain the mostlikely class.
The tree building algorithm starts byselecting the feature test that splits the data in such away that the mean impurity (entropy times the num-ber of instances) of the two partitions is minimal.The algorithm continues to split each partition recur-sively until some stop criterion is met (e.g.
a mini-mal number of instances in the partition).
Alterna-tively, a small stop value can be used to build a treethat is probably overfitted, but is then pruned backto where it best matches some amount of held-outdata.
In our experiments, we used the CART imple-mentation that is part of the Edinburgh Speech Tools(Taylor et al, 1999).2.4 Memory-based learningMemory-based learning (MBL), also known asinstance-based, example-based, or lazy learning(Stanfill and Waltz, 1986; Aha et al, 1991), is asupervised inductive learning algorithm for learningclassification tasks.
Memory-based learning treatsa set of training instances as points in a multi-dimensional feature space, and stores them as suchin an instance base in memory (rather than perform-ing some abstraction over them).
After the instancebase is stored, new (test) instances are classifiedby matching them to all instances in memory, andby calculating with each match the distance, givenby a distance function between the new instanceX and the memory instance Y .
Cf.
(Daelemanset al, 2002) for details.
Classification in memory-based learning is performed by the k-NN algorithm(Fix and Hodges, 1951; Cover and Hart, 1967) thatsearches for the k ?nearest neighbours?
accordingto the distance function.
The majority class of thek nearest neighbours then determines the class ofthe new case.
In our k-NN implementation2 , equi-distant neighbours are taken as belonging to thesame k, so this implementation is effectively a k-nearest distance classifier.3 Optimization by iterative deepeningIterative deepening (ID) is a heuristic search algo-rithm for the optimization of algorithmic parameter2All experiments with memory-based learning were per-formed with TiMBL, version 4.3 (Daelemans et al, 2002).Wrd PreP PostP PosC PosF DiA NpC VpC IC BIC Tf*Idf PM D2P D2S D2E A B ABDe = = Art bep,zijdofmv,neut 0 B O 2.11 5.78 0.00 4 9999 0.00 0.94 - - - -bomen = = N soort,mv,neut 0 I O 4.37 7.38 0.16 4 17 0.06 0.89 A - A-rondom = = Prep voor 0 O O 4.58 5.09 0.04 4 17 0.11 0.83 - - - -de = = Art bep,zijdofmv,neut 0 B O 1.31 5.22 0.00 5 20 0.17 0.78 - - - -molen = = N soort,ev,neut 0 I O 5.00 7.50 0.18 5 9 0.22 0.72 A - A-bij = = Prep voor 0 O O 2.50 3.04 0.00 6 9999 0.28 0.67 - - - -de = = Art bep,zijdofmv,neut 0 B O 1.31 6.04 0.00 6 3 0.33 0.61 - - - -scheepswerf = = N soort,ev,neut 0 I O 5.63 8.02 0.03 4 9999 0.39 0.56 - - - -Verolme = = N eigen,ev,neut 0 I O 6.38 7.59 0.05 0 9999 0.44 0.50 A - A-moeten = = V trans,ott,3,ev 0 B O 2.99 6.77 0.01 4 9999 0.61 0.33 - - - -verkassen = , V trans,inf 0 I O 5.75 5.99 0.02 4 9999 0.67 0.28 A B ABvindt = = V trans,ott,3,ev 0 O B 3.51 8.50 0.00 6 9999 0.72 0.22 - - - -molenaar = = N soort,ev,neut 0 B O 5.95 8.50 0.05 0 9999 0.78 0.17 - - - -Wijbrand = = N eigen,ev,neut 0 I O 7.89 8.50 0.11 0 38 0.83 0.11 A - A-Table 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen,vindt molenaar Wijbrandt.
?Miller Wijbrand thinks that the trees surrounding the mill near shipyard Verolme have to relocate.
?and feature selection, that combines classifier wrap-ping (using the training material internally to test ex-perimental variants) (Kohavi and John, 1997) withprogressive sampling of training material (Provost etal., 1999).
We start with a large pool of experiments,each with a unique combination of input featuresand algorithmic parameter settings.
In the first step,each attempted setting is applied to a small amountof training material and tested on a fixed amountof held-out data (which is a part of the full train-ing set).
Only the best settings are kept; all othersare removed from the pool of competing settings.In subsequent iterations, this step is repeated, ex-ponentially decreasing the number of settings in thepool, while at the same time exponentially increas-ing the amount of training material.
The idea is thatthe increasing amount of time required for trainingis compensated by running fewer experiments, in ef-fect keeping processing time approximately constantacross iterations.
This process terminates when onlythe single best experiment is left (or, the n best ex-periments).This ID procedure can in fact be embedded in astandard 10-fold cross-validation procedure.
In sucha 10-fold CV ID experiment, the ID procedure is car-ried out on the 90% training partition, and the result-ing optimal setting is tested on the remaining 10%test partition.
The average score of the 10 optimizedfolds can then be considered, as that of a normal 10-fold CV experiment, to be a good estimation of theperformance of a classifier optimized on the full dataset.For current purposes, our specific realization ofthis general procedure was as follows.
We used foldsof approximately equal size.
Within each ID ex-periment, the amount of held-out data was approx-imately 5%; the initial amount of training data was5% as well.
Eight iterations were performed, dur-ing which the number of experiments was decreased,and the amount of training data was increased, sothat in the end only the 3 best experiments usedall available training data (i.e.
the remaining 95%).Increasing the training data set was accomplishedby random sampling from the total of training dataavailable.
Selection of the best experiments wasbased on their F-score (van Rijsbergen, 1979) onthe target class (accent or break).
F-score, the har-monic mean of precision and recall, is chosen sinceit directly evaluates the tasks (placement of accentsor breaks), in contrast with classification accuracy(the percentage of correctly classified test instances)which is biased to the majority class (to place no ac-cent or break).
Moreover, accuracy masks relevantdifferences between certain inappropriate classifiersthat do not place accents or breaks, and better clas-sifiers that do place them, but partly erroneously.The initial pool of experiments was created bysystematically varying feature selection (the inputfeatures to the classifier) and the classifier set-tings (the parameters of the classifiers).
We re-stricted these selections and settings within reason-able bounds to keep our experiments computation-ally feasible.
In particular, feature selection was lim-ited to varying the size of the window that was usedto model the local context of an instance.
A uni-form window (i.e.
the same size for all features) wasapplied to all features except DiA, D2P, D2S, andD2E.
Its size (win) could be 1, 3, 5, 7, or 9, wherewin = 1 implies no modeling of context, whereaswin = 9 means that during classification not onlythe features of the current instance are taken into ac-count, but also those of the preceding and followingfour instances.For CART, we varied the following parameter val-ues, resulting in a first ID step with 480 experiments:?
the minimum number of examples for leafnodes (stop): 1, 10, 25, 50, and 100?
the number of partitions to split a float featurerange into (frs): 2, 5, 10, and 25?
the percentage of training material held out forpruning (held-out): 0, 5, 10, 15, 20, and 25 (0implies no pruning)For MBL, we varied the following parameter val-ues, which led to 1184 experiments in the first IDstep:?
the number of nearest neighbours (k): 1, 4, 7,10, 13, 16, 19, 22, 25, and 28?
the type of feature weighting: Gain Ratio (GR),and Shared Variance (SV)?
the feature value similarity metric: Overlap,or Modified Value Difference Metric (MVDM)with back-off to Overlap at value frequencytresholds 1 (L=1, no back-off), 2, and 10?
the type of distance weighting: None, InverseDistance, Inverse Linear Distance, and Expo-nential Decay with ?
= 1.0 (ED1) and ?
= 4.0(ED4)4 Results4.1 Tenfold iterative deepening resultsWe first determined two sharp, informed baselines;see Table 2.
The informed baseline for accent place-ment is based on the content versus function worddistinction, commonly employed in TTS systems(Taylor and Black, 1998).
We refer to this baselineas CF-rule.
It is constructed by accenting all contentwords, while leaving all function words (determin-ers, prepositions, conjunctions/complementisers andauxiliaries) unaccented.
The required word class in-formation is obtained from the POS tags.
The base-line for break placement, henceforth PUNC-rule, re-lies solely on punctuation.
A break is inserted afterany sequence of punctuation symbols containing oneTarget : Method : Prec : Rec : F :Accent CF-rule 66.7 94.9 78.3CART 78.6 ?2.8 85.7 ?1.1 82.0 ?1.7MBL 80.0 ?2.7 86.6 ?1.4 83.6 ?1.6?CARTC 78.7 ?3.0 85.6 ?0.8 82.0 ?1.6MBLC 81.0 ?2.7 86.1 ?1.1 83.4 ?1.5?Break PUNC-rule 99.2 75.7 85.9CART 93.1 ?1.5 82.2 ?3.0 87.3 ?1.5MBL 95.1 ?1.4 81.9 ?2.8 88.0 ?1.5?CARTC 94.5 ?0.8 80.2 ?3.1 86.7 ?1.6MBLC 95.7 ?1.1 80.7 ?3.1 87.6 ?1.7?Table 2: Precision, recall, and F-scores on accent, breakand combined prediction by means of CART and MBL, forbaselines and for average results over 10 folds of the IterativeDeepening experiment; a ?
indicates a significant difference(p < 0.01) between CART and MBL according to a pairedt-test.
Superscript C refers to the combined task.or more characters from the set {,!?:;()}.
It shouldbe noted that both baselines are simple rule-basedalgorithms that have been manually optimized forthe current training set.
They perform well abovechance level, and pose a serious challenge to any MLapproach.From the results displayed in Table 2, the follow-ing can be concluded.
First, MBL attains the highestF-scores on accent placement, 83.6, and break place-ment, 88.0.
It does so when trained on the ACCENTand BREAK tasks individually.
On these tasks, MBLperforms significantly better than CART (paired t-tests yield p < 0.01 for both differences).Second, the performances of MBL and CART onthe combined task, when split in F-scores on accentand break placement, are rather close to those on theaccent and break tasks.
For both MBL and CART,the scores on accent placement as part of the com-bined task versus accent placement in isolation arenot significantly different.
For break insertion, how-ever, a small but significant drop in performance canbe seen with MBL (p < 0.05) and CART (p < 0.01)when it is performed as part of the COMBINED task.As is to be expected, the optimal feature selec-tions and classifier settings obtained by iterativedeepening turned out to vary over the ten folds forboth MBL and CART.
Table 3 lists the settings pro-ducing the best F-score on accents or breaks.
A win-dow of 7 (i.e.
the features of the three preceding andfollowing word form tokens) is used by CART andMBL for accent placement, and also for break in-sertion by CART, whereas MBL uses a window ofTarget: Method: Setting:Accent CART win=7, stop=50, frs=5, held-out=5MBL win=7, MVDM with L=5, k=25, GR, ED4Break CART win=7, stop=25, frs=2, held-out=5MBL win=3, MVDM with L=2, k=28, GR, ED4Table 3: Optimal parameter settings for CART and MBL withrespect to accent and break predictionjust 3.
Both algorithms (stop in CART, and k inMBL) base classifications on minimally around 25instances.
Furthermore, MBL uses the Gain Ratiofeature weighting and Exponential Decay distanceweighting.
Although no pruning was part of the Iter-ative Deepening experiment, CART prefers to holdout 5% of its training material to prune the decisiontree resulting from the remaining 95%.4.2 External validationWe tested our optimized approach on our held-outdata of 10 articles (2,905 tokens), and on an indepen-dent test corpus (van Herwijnen and Terken, 2001).The latter contains two types of text: 2 newspapertexts (55 sentences, 786 words excluding punctua-tion), and 17 email messages (70 sentences, 1133words excluding punctuation).
This material was an-notated by 10 experts, who were asked to indicatethe preferred accents and breaks.
For the purposeof evaluation, words were assumed to be accented ifthey received an accent by at least 7 of the annota-tors.
Furthermore, of the original four break levelsannotated (i.e.
no break, light, medium, or heavy ),only medium and heavy level breaks were consid-ered to be a break in our evaluation.
Table 4 lists theprecision, recall, and F-scores obtained on the twotasks using the single-best scoring setting from the10-fold CV ID experiment per task.
It can be seenthat both CART and MBL outperformed the CF-rulebaseline on our own held-out data and on the newsand email texts, with similar margins as observed inour 10-fold CV ID experiment.
MBL attains an F-score of 86.6 on accents, and 91.0 on breaks; bothare improvements over the cross-validation estima-tions.
On breaks, however, both CART and MBLfailed to improve on the PUNC-rule baseline; on thenews and email texts they perform even worse.
In-specting MBLs output on these text, it turned outthat MBL does emulate the PUNC-rule baseline,but that it places additional breaks at positions notTarget : Test set Method : Prec : Rec : F :Accent Held-out CF-rule 73.5 94.8 82.8CART 84.3 86.1 85.2MBL 87.0 86.3 86.6News CF-rule 52.2 92.9 66.9CART 62.7 92.5 74.6MBL 66.3 89.2 76.0Email CF-rule 54.3 91.0 68.0CART 66.8 88.5 76.1MBL 71.0 88.5 78.8Break Held-out PUNC-rule 99.5 83.7 90.9CART 92.6 88.9 90.7MBL 95.5 87.0 91.0News PUNC-rule 98.8 93.1 95.9CART 80.6 95.4 87.4MBL 89.3 95.4 92.2Email PUNC-rule 93.9 87.0 90.3CART 81.6 90.2 85.7MBL 83.0 91.1 86.8Table 4: Precision, recall, and F-scores on accent and breakprediction for our held-out corpus and two external corpora ofnews and email texts, using the best settings for CART andMBL as determined by the ID experiments.marked by punctuation.
A considerable portion ofthese non-punctuation breaks is placed incorrectly ?or at least different from what the annotators pre-ferred ?
resulting in a lower precision that does notoutweigh the higher recall.5 ConclusionWith shallow features as input, we trained machinelearning algorithms on predicting the placement ofpitch accents and prosodic breaks in Dutch text,a desirable function for a TTS system to producesynthetic speech with good prosody.
Both algo-rithms, the memory-based classifier MBL and de-cision tree inducer CART, were automatically opti-mized by an Iterative Deepening procedure, a classi-fier wrapper technique with progressive sampling oftraining data.
It was shown that MBL significantlyoutperforms CART on both tasks, as well as on thecombined task (predicting accents and breaks simul-taneously).
This again provides an indication thatit is advantageous to retain individual instances inmemory (MBL) rather than to discard outlier casesas noise (CART).Training on both tasks simultaneously, in onemodel rather than divided over two, results ingeneralization accuracies similar to that of theindividually-learned models (identical on accentplacement, and slightly lower for break placement).This shows that learning one task does not seriouslyhinder learning the other.
From a practical point ofview, it means that a TTS developer can resort to onesystem for both tasks instead of two.Pitch accent placement can be learned from shal-low input features with fair accuracy.
Break in-sertion seems a harder task, certainly in view ofthe informed punctuation baseline PUNC-rule.
Es-pecially the precision of the insertion of breaks atother points than those already indicated by com-mas and other ?pseudo-prosodic?
orthographic markup is hard.
This may be due to the lack of crucialinformation in the shallow features, to inherent lim-itations of the ML algorithms, but may as well pointto a certain amount of optionality or personal pref-erence, which puts an upper bound on what can beachieved in break prediction (Koehn et al, 2000).We plan to integrate the placement of pitch ac-cents and breaks in a TTS system for Dutch, whichwill enable the closed-loop annotation of more datausing the TTS itself and on-line (active) learning.Moreover, we plan to investigate the perceptualcost of false insertions and deletions of accents andbreaks in experiments with human listeners.AcknowledgementsOur thanks go out to Olga van Herwijnen and Jacques Terkenfor the use of their TTS evaluation corpus.
All research inthis paper was funded by the Flemish-Dutch Committee (VNC)of the National Foundations for Research in the Netherlands(NWO) and Belgium (FWO).ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.
Instance-basedlearning algorithms.
Machine Learning, 6:37?66.A.W.
Black.
1995.
Comparison of algorithms for predictingpitch accent placement in English speech synthesis.
In Pro-ceedings of the Spring Meeting of the Acoustical Society ofJapan.L.
Breiman, J. Friedman, R. Ohlsen, and C. Stone.
1984.Classification and regression trees.
Wadsworth InternationalGroup, Belmont, CA.C.J.
van Rijsbergen.
1979.
Information Retrieval.
Butter-sworth, London.T.
M. Cover and P. E. Hart.
1967.
Nearest neighbor patternclassification.
Institute of Electrical and Electronics Engi-neers Transactions on Information Theory, 13:21?27.A.
Cutler, D. Dahan, and W.A.
Van Donselaar.
1997.
Prosodyin the comprehension of spoken language: A literature re-view.
Language and Speech, 40(2):141?202.W.
Daelemans and V. Hoste.
2002.
Evaluation of machinelearning methods for natural language processing tasks.
InProceedings of LREC-2002, the third International Confer-ence on Language Resources and Evaluation, pages 755?760.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996.
MBT:A memory-based part of speech tagger generator.
In E. Ejer-hed and I. Dagan, editors, Proc.
of Fourth Workshop on VeryLarge Corpora, pages 14?27.
ACL SIGDAT.W.
Daelemans, A. van den Bosch, and J. Zavrel.
1999.
For-getting exceptions is harmful in language learning.
Ma-chine Learning, Special issue on Natural Language Learn-ing, 34:11?41.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2002.
TiMBL: Tilburg MemoryBased Learner, version 4.3, reference guide.
TechnicalReport ILK-0210, ILK, Tilburg University.E.
Fix and J. L. Hodges.
1951.
Discriminatory analysis?nonparametric discrimination; consistency properties.
Tech-nical Report Project 21-49-004, Report No.
4, USAF Schoolof Aviation Medicine.J.
Hirschberg.
1993.
Pitch accent in context: Predicting intona-tional prominence from text.
Artificial Intelligence, 63:305?340.P.
Koehn, S. Abney, J. Hirschberg, and M. Collins.
2000.
Im-proving intonational phrasing with syntactic information.
InICASSP, pages 1289?1290.R.
Kohavi and G. John.
1997.
Wrappers for feature subsetselection.
Artificial Intelligence Journal, 97(1?2):273?324.D.
R. Ladd.
1996.
Intonational phonology.
Cambridge Uni-versity Press.E.
Marsi, G.J.
Busser, W. Daelemans, V. Hoste, M. Reynaert,and A. van den Bosch.
2002.
Combining informationsources for memory-based pitch accent placement.
In Pro-ceedings of the International Conference on Spoken Lan-guage Processing, ICSLP-2002, pages 1273?1276.S.
Pan and J. Hirschberg.
2000.
Modeling local context forpitch accent prediction.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguistics,Hong Kong.S.
Pan and K. McKeown.
1999.
Word informativenessand automatic pitch accent modeling.
In Proceedings ofEMNLP/VLC?99, New Brunswick, NJ, USA.
ACL.F.
Provost, D. Jensen, and T. Oates.
1999.
Efficient progressivesampling.
In Proceedings of the Fifth International Con-ference on Knowledge Discovery and Data Mining, pages23?32.G.
Salton.
1989.
Automatic text processing: The transfor-mation, analysis, and retrieval of information by computer.Addison?Wesley, Reading, MA, USA.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reason-ing.
Communications of the ACM, 29(12):1213?1228, De-cember.P.
Taylor and A.
Black.
1998.
Assigning phrase breaks frompart-of-speech sequences.
Computer Speech and Language,12:99?117.P.
Taylor, R. Caley, A. W. Black, and S. King, 1999.
Edin-burgh Speech Tools Library, System Documentation Edition1.2.
CSTR, University of Edinburgh.O.
van Herwijnen and J. Terken.
2001.
Evaluation of pros-3 forthe assignment of prosodic structure, compared to assign-ment by human experts.
In Proceedings Eurospeech 2001Scandinavia, Vol.1, pages 529?532.M.
Q. Wang and J. Hirschberg.
1997.
Automatic classificationof intonational phrasing boundaries.
Computer Speech andLanguage, 6(2):175?196.
