Proceedings of the Third Workshop on Statistical Machine Translation, pages 115?118,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMeteor, m-bleu and m-ter: Evaluation Metrics forHigh-Correlation with Human Rankings of Machine TranslationOutputAbhaya Agarwal and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{abhayaa,alavie}@cs.cmu.eduAbstractThis paper describes our submissions to themachine translation evaluation shared task inACL WMT-08.
Our primary submission is theMeteor metric tuned for optimizing correla-tion with human rankings of translation hy-potheses.
We show significant improvementin correlation as compared to the earlier ver-sion of metric which was tuned to optimizedcorrelation with traditional adequacy and flu-ency judgments.
We also describe m-bleu andm-ter, enhanced versions of two other widelyused metrics bleu and ter respectively, whichextend the exact word matching used in thesemetrics with the flexible matching based onstemming and Wordnet in Meteor .1 IntroductionAutomatic Metrics for MT evaluation have been re-ceiving significant attention in recent years.
Evalu-ating an MT system using such automatic metrics ismuch faster, easier and cheaper compared to humanevaluations, which require trained bilingual evalua-tors.
The most commonly used MT evaluation met-ric in recent years has been IBM?s Bleu metric (Pa-pineni et al, 2002).
Bleu is fast and easy to run,and it can be used as a target function in parameteroptimization training procedures that are commonlyused in state-of-the-art statistical MT systems (Och,2003).
Various researchers have noted, however, var-ious weaknesses in the metric.
Most notably, Bleudoes not produce very reliable sentence-level scores.Meteor , as well as several other proposed metricssuch as GTM (Melamed et al, 2003), TER (Snoveret al, 2006) and CDER (Leusch et al, 2006) aim toaddress some of these weaknesses.Meteor , initially proposed and released in 2004(Lavie et al, 2004) was explicitly designed to im-prove correlation with human judgments of MT qual-ity at the segment level.
Previous publications onMeteor (Lavie et al, 2004; Banerjee and Lavie,2005; Lavie and Agarwal, 2007) have described thedetails underlying the metric and have extensivelycompared its performance with Bleu and severalother MT evaluation metrics.
In (Lavie and Agar-wal, 2007), we described the process of tuning freeparameters within the metric to optimize the corre-lation with human judgments and the extension ofthe metric for evaluating translations in languagesother than English.This paper provides a brief technical description ofMeteor and describes our experiments in re-tuningthe metric for improving correlation with the humanrankings of translation hypotheses corresponding toa single source sentence.
Our experiments show sig-nificant improvement in correlation as a result of re-tuning which shows the importance of having a met-ric tunable to different testing conditions.
Also, inorder to establish the usefulness of the flexible match-ing based on stemming and Wordnet, we extend twoother widely used metrics bleu and ter which useexact word matching, with the matcher module ofMeteor .2 The Meteor MetricMeteor evaluates a translation by computing ascore based on explicit word-to-word matches be-tween the translation and a given reference trans-lation.
If more than one reference translation isavailable, the translation is scored against each refer-ence independently, and the best scoring pair is used.Given a pair of strings to be compared, Meteor cre-ates a word alignment between the two strings.
Analignment is mapping between words, such that ev-ery word in each string maps to at most one wordin the other string.
This alignment is incrementallyproduced by a sequence of word-mapping modules.The ?exact?
module maps two words if they are ex-actly the same.
The ?porter stem?
module maps twowords if they are the same after they are stemmed us-115ing the Porter stemmer.
The ?WN synonymy?
mod-ule maps two words if they are considered synonyms,based on the fact that they both belong to the same?synset?
in WordNet.The word-mapping modules initially identify allpossible word matches between the pair of strings.We then identify the largest subset of these wordmappings such that the resulting set constitutes analignment as defined above.
If more than one maxi-mal cardinality alignment is found, Meteor selectsthe alignment for which the word order in the twostrings is most similar (the mapping that has theleast number of ?crossing?
unigram mappings).
Theorder in which the modules are run reflects word-matching preferences.
The default ordering is tofirst apply the ?exact?
mapping module, followed by?porter stemming?
and then ?WN synonymy?.Once a final alignment has been produced betweenthe system translation and the reference translation,the Meteor score for this pairing is computed asfollows.
Based on the number of mapped unigramsfound between the two strings (m), the total num-ber of unigrams in the translation (t) and the totalnumber of unigrams in the reference (r), we calcu-late unigram precision P = m/t and unigram recallR = m/r.
We then compute a parametrized har-monic mean of P and R (van Rijsbergen, 1979):Fmean =P ?R?
?
P + (1?
?)
?RPrecision, recall and Fmean are based on single-word matches.
To take into account the extent towhich the matched unigrams in the two strings arein the same word order, Meteor computes a penaltyfor a given alignment as follows.
First, the sequenceof matched unigrams between the two strings is di-vided into the fewest possible number of ?chunks?such that the matched unigrams in each chunk areadjacent (in both strings) and in identical word or-der.
The number of chunks (ch) and the number ofmatches (m) is then used to calculate a fragmenta-tion fraction: frag = ch/m.
The penalty is thencomputed as:Pen = ?
?
frag?The value of ?
determines the maximum penalty(0 ?
?
?
1).
The value of ?
determines thefunctional relation between fragmentation and thepenalty.
Finally, the Meteor score for the align-ment between the two strings is calculated as:score = (1 ?
Pen) ?
FmeanThe free parameters in the metric, ?, ?
and ?
aretuned to achieve maximum correlation with the hu-man judgments as described in (Lavie and Agarwal,2007).3 Extending Bleu and Ter withFlexible MatchingMany widely used metrics like Bleu (Papineni et al,2002) and Ter (Snover et al, 2006) are based onmeasuring string level similarity between the refer-ence translation and translation hypothesis, just likeMeteor .
Most of them, however, depend on find-ing exact matches between the words in two strings.Many researchers (Banerjee and Lavie, 2005; Liu andGildea, 2006), have observed consistent gains by us-ing more flexible matching criteria.
In the followingexperiments, we extend the Bleu and Ter metricsto use the stemming and Wordnet based word map-ping modules from Meteor .Given a translation hypothesis and reference pair,we first align them using the word mapping modulesfrom Meteor .
We then rewrite the reference trans-lation by replacing the matched words with the cor-responding words in the translation hypothesis.
Wenow compute Bleu and Ter with these new refer-ences without changing anything inside the metrics.To get meaningful Bleu scores at segment level,we compute smoothed Bleu as described in (Lin andOch, 2004).4 Re-tuning Meteor for Rankings(Callison-Burch et al, 2007) reported that the inter-coder agreement on the task of assigning ranks toa given set of candidate hypotheses is much betterthan the intercoder agreement on the task of assign-ing a score to a hypothesis in isolation.
Based onthat finding, in WMT-08, only ranking judgmentsare being collected from the human judges.The current version of Meteor uses parametersoptimized towards maximizing the Pearson?s corre-lation with human judgments of adequacy scores.
Itis not clear that the same parameters would be op-timal for correlation with human rankings.
So wewould like to re-tune the parameters in the metricfor maximizing the correlation with ranking judg-ments instead.
This requires computing full rankingsaccording to the metric and the humans and thencomputing a suitable correlation measure on thoserankings.4.1 Computing Full RankingsMeteor assigns a score between 0 and 1 to everytranslation hypothesis.
This score can be converted116Language JudgmentsBinary SentencesEnglish 3978 365German 2971 334French 1903 208Spanish 2588 284Table 1: Corpus Statistics for Various Languagesto rankings trivially by assuming that a higher scoreindicates a better hypothesis.In development data, human rankings are avail-able as binary judgments indicating the preferred hy-pothesis between a given pair.
There are also caseswhere both the hypotheses in the pair are judged tobe equal.
In order to convert these binary judgmentsinto full rankings, we do the following:1.
Throw out all the equal judgments.2.
Construct a directed graph where nodes corre-spond to the translation hypotheses and everybinary judgment is represented by a directededge between the corresponding nodes.3.
Do a topological sort on the resulting graph andassign ranks in the sort order.
The cycles in thegraph are broken by assigning same rank to allthe nodes in the cycle.4.2 Measuring CorrelationFollowing (Ye et al, 2007), we first compute theSpearman correlation between the human rankingsand Meteor rankings of the translation hypothesescorresponding to a single source sentence.
Let N bethe number of translation hypotheses and D be thedifference in ranks assigned to a hypothesis by tworankings, then Spearman correlation is given by:r = 1?6?D2N(N2 ?
1)The final score for the metric is the average of theSpearman correlations for individual sentences.5 Experiments5.1 DataWe use the human judgment data from WMT-07which was released as development data for the eval-uation shared task.
Amount of data available forvarious languages is shown in Table 1.
Developmentdata contains the majority judgments (not every hy-potheses pair was judged by same number of judges)which means that in the cases where multiple judgesjudged the same pair of hypotheses, the judgmentgiven by majority of the judges was considered.English German French Spanish?
0.95 0.9 0.9 0.9?
0.5 3 0.5 0.5?
0.45 0.15 0.55 0.55Table 2: Optimal Values of Tuned Parameters for VariousLanguagesOriginal Re-tunedEnglish 0.3813 0.4020German 0.2166 0.2838French 0.2992 0.3640Spanish 0.2021 0.2186Table 3: Average Spearman Correlation with HumanRankings for Meteor on Development Data5.2 MethodologyWe do an exhaustive grid search in the feasible rangesof parameter values, looking for parameters thatmaximize the average Spearman correlation over thetraining data.
To get a fair estimate of performance,we use 3-fold cross validation on the developmentdata.
Final parameter values are chosen as the bestperforming set on the data pooled from all the folds.5.3 Results5.3.1 Re-tuning Meteor for RankingsThe re-tuned parameter values are shown in Ta-ble 2 while the average Spearman correlations forvarious languages with original and re-tuned param-eters are shown in Table 3.
We get significant im-provements for all the languages.
Gains are speciallypronounced for German and French.Interestingly, weight for recall becomes even higherthan earlier parameters where it was already high.So it seems that ranking judgments are almost en-tirely driven by the recall in all the languages.
Alsothe re-tuned parameters for all the languages exceptGerman are quite similar.5.3.2 m-bleu and m-terTable 4 shows the average Spearman correlationsof m-bleu and m-ter with human rankings.
ForEnglish, both m-bleu and m-ter show considerableimprovements.
For other languages, improvementsin m-ter are smaller but consistent.
m-bleu , how-ever, doesn?t shows any improvements in this case.A possible reason for this behavior can be the lack ofa ?WN synonymy?
module for languages other thanEnglish which results in fewer extra matches over theexact matching baseline.
Additionally, French, Ger-man and Spanish have a richer morphology as com-pared to English.
The morphemes in these languages117Exact Match Flexible MatchEnglish: Bleu 0.2486 0.2747Ter 0.1598 0.2033French: Bleu 0.2906 0.2889Ter 0.2472 0.2604German: Bleu 0.1829 0.1806Ter 0.1509 0.1668Spanish: Bleu 0.1804 0.1847Ter 0.1787 0.1839Table 4: Average Spearman Correlation with HumanRankings for m-bleu and m-tercarry much more information and different forms ofthe same word may not be as freely replaceable as inEnglish.
A more fine grained strategy for matchingwords in these languages remains an area of furtherinvestigation.6 ConclusionsIn this paper, we described the re-tuning of Me-teor parameters to better correlate with humanrankings of translation hypotheses.
Results on thedevelopment data indicate that the re-tuned ver-sion is significantly better at predicting ranking thanthe earlier version.
We also presented enhancedBleu and Ter that use the flexible word match-ing module from Meteor and show that this re-sults in better correlations as compared to the de-fault exact matching versions.
The new version ofMeteor will be soon available on our website at:http://www.cs.cmu.edu/~alavie/METEOR/ .
Thisrelease will also include the flexible word matchermodule which can be used to extend any metric withthe flexible matching.AcknowledgmentsThe work reported in this paper was supported byNSF Grant IIS-0534932.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Translationand/or Summarization, pages 65?72, Ann Arbor,Michigan, June.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 136?158, Prague, Czech Republic, June.Association for Computational Linguistics.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: AnAutomatic Metric for MT Evaluation with High Levelsof Correlation with Human Judgments.
In Proceedingsof the Second ACL Workshop on Statistical MachineTranslation, pages 228?231, Prague, Czech Republic,June.Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.2004.
The Significance of Recall in Automatic Metricsfor MT Evaluation.
In Proceedings of the 6th Confer-ence of the Association for Machine Translation in theAmericas (AMTA-2004), pages 134?143, Washington,DC, September.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006.CDER: Efficient MT Evaluation Using Block Move-ments.
In Proceedings of the Thirteenth Conference ofthe European Chapter of the Association for Compu-tational Linguistics.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metricsfor machine translation.
In COLING ?04: Proceedingsof the 20th international conference on ComputationalLinguistics, page 501, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Ding Liu and Daniel Gildea.
2006.
Stochastic itera-tive alignment for machine translation evaluation.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 539?546, Morristown, NJ, USA.Association for Computational Linguistics.I.
Dan Melamed, Ryan Green, and Joseph Turian.
2003.Precision and Recall of Machine Translation.
In Pro-ceedings of the HLT-NAACL 2003 Conference: ShortPapers, pages 61?63, Edmonton, Alberta.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA,July.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas(AMTA-2006), pages 223?231, Cambridge, MA, Au-gust.C.
van Rijsbergen, 1979.
Information Retrieval.
Butter-worths, London, UK, 2nd edition.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence level machine translation evaluation as a rank-ing.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 240?247, Prague,Czech Republic, June.
Association for ComputationalLinguistics.118
