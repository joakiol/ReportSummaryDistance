Hardware for Hidden Markov-Model-Based, Large-VocabularyReal-Time Speech RecognitionM.
Weintraub, G. Chen, J. Mankoski, H. MurveitSRI InternationalA.
StSlzle, S. Narayanaswamy, P. Schrupp, B. Richards, J. Rabaey, R. BrodersenUniversity of California, Berkeley, CAAbst ractSRI and U.C.
Berkeley have begun a cooper-ative effort to develop a new architecture forreal-time implementation f spoken languagesystems (SLS).
Our goal is to develop fastspeech recognition algorithms, and support-ing hardware capable of recognizing continu-ous speech from a bigram- or trigram-based20,000-word vocabulary or a 1,000- to 5,000-word SLS.1 In t roduct ionIn order to implement a hid-den Markov-model-based, real-time, large-vocabulary speech recognition system it isnecessary to support the most time-criticalparts of the algorithm in custom VLSI hard-ware.
For systems with 20,000 words andmore it is also necessary to have algorithmicsupport to reduce the amount of computa-tions that have to be done.Our first version of real-time speech recog-nition hardware \[1\] was targeted for sys-tems with up to 3,000 words.
It continu-ously processed all the states of the Markovmodel that describe the vocabulary words,even states that are not likely to fit to thespeech that has to be recognized.
However,for larger systems (20,000 words and more) itis necessary to perform a pruning algorithmsuch that only states with a high probabil-ity are processed.
Nevertheless, it is desir-able to process as many states as possibleto reduce the risk that meaningful states arepruned.
Our first hardware version used dy-namic memories with an access cycle time of200 ns, and since accessing memory is thecrucial bottleneck of the system, not morethan 5,000 states per frame could be pro-cessed.
Other shortcomings were that theboard design turned out to be very compli-cated: at the end of each frame most of thememories had to be flipped between proces-sors.
Since these memories also had to beaccessed by the host processor, about 50% ofthe board area was used just for multiplexingthe buses.
In addition, the grammar pro-cessing system was implemented with cus-tom hardware such that only systems with astatistical bigram grammar \[1\] could be sup-ported.This paper describes a new system thatimplements pruning and is capable of pro-cessing up to 20 million active states persecond.
Assuming that a frame has a du-ration of 10 ms and a phoneme is modeledwith three states, a vocabulary word is mod-eled with an average of six phonemes, 50%of the vocabulary words are active at anygiven frame and that one third of the phones82in these words are active, the new systemcan perform in real time for vocabulary sizesof up to 60,000 words.
It also contains cus-tom hardware to support grammar process-ing routines that are common to a whole vari-ety of grammar models.
Thus, grammar pro-cessing or natural anguage processing canbe implemented on a general-purpose pro-cessor that uses the custom hardware for thetime-critical part.
To ease the system designand to reduce the memory requirements, weuse a switching processor architecture and acaching scheme to preload only a small sub-set of the model parameters ata given frame.The new hardware contains 14 custom VLSIprocessors that access fast static memorieswith cycle times of 20 MtIz.2 Arch i tec tureFig.
1 shows the overall architecture of therecognition hardware.
The phone process-ing system updates the state probabilities us-ing the Viterbi algorithm, while the grammarprocessing system takes care of the transitionbetween phones.
The communication be-tween these subsystems is done using "gram-mar nodes".
Associated with a grammarnode is a probability that gives the proba-bility function that a phone starts (sourcegrammar node) or that a phone ends (desti-nation grammar node).
These nodes are notstates in the hidden Markov model, whichmeans, a transition into a grammar nodedoes not consume a frame delay, and theydo not output a speech segment.
Their pur-pose is solely to formalize the communicationbetween the subsystems.The grammar subsystem multiplies thedestination grammar node probabilities(DGNP) with transition probabilities tosource grammar nodes (see Fig.
1).
Thesource grammar node probability (SGNP) ofa certain phone is the maximum probabilityof all the incoming transitions.The recognition hardware is partitionedaccording to Fig.
2: The phone processingsystem and the part of the grammar sys-tem that computes the best SGNP is imple-mented on a custom board using application-specific integrated circuits.
The compu-tation of the product of the DGNP withthe transition probability is performed ongeneral-purpose hardware.
Thus, differentalgorithms to dynamically derive the tran-sition probabilities between phones can beimplemented on the general-purpose hard-ware while the computationally most inten-sive part of the grammar system, finding thebest SGNP, can be done with custom VLSIhardware.Fig.
3 shows the overall architecture ofthecustom board.
At any given frame two pro-cesses, each implemented with three customVLSI processors, are operating in parallel.One process computes the state probabilitiesof active phones that are listed in the Ac-tiveword Memory (Viterbi process) while theother process generates a list of active phonesfor the next frame (ToActiveWord process).2.1 Vi terb i  processThe Viterbi process equentially reads activephones from the ActiveWord Memory andcomputes their state probabilities.
Based ona pruning threshold derived from the beststate probability of that current frame, theViterbi process decides whether the phonemeshould stay active in the next frame and/orwhether it has a high probability to end sothat succeeding phonemes can be activated.Based on this decision, information associ-ated with this phone is sent to the ToAc-tiveWord processor and/or to the general-purpose grammar processor.
To preventarithmetic overflow, the Viterbi process alsonormalizes probabilities based on the beststate probability of the previous frame.83Phone SystemDestinationGrammar NodeProbabilityJSourceGrammar NodeProbabilityGrammar System. '
' " "  ~ ~ ~ .
.
.
.  "
- /  Source Destination/" x ~ ~"  ~ ~ ": Gramm~Node Grammar Node/',,X,J' " ~ '<, ,~.~.~-~ " ~,- J  ,' Probability ProbabilityFigure 1: Basic Architecture of the Speech Recognition Hardware?/?//\.,f m.: / \Phone  System Gra  .. ar Sys temI ?
t. I  .
j lFullCustomBoardtI!#i /i /j "General-PurposeBoardsFigure 2: Hardware Partition84The model parameters that describe thetopology of phonemes are partitioned intotwo memories.
One memory is located on theprob and the back processor (see Fig.
3) anddescribes the graph of the hidden Markovchain for certain prototype phonemes.
Thisdescription can span up to 128 states, par-titioned into up to 32 prototype phonemes.The other memory is an off-chip static mem-ory that contains the transition probabilitiesof up to 64,000 unique phonemes.
Thus, thetopology of a phoneme is defined with a 5-bit value to indicate the graph and a 16-bitaddress that specifies the transition proba-bilities.To reduce the memory bandwidth the pro-cessors contain a dual-ported register file tocache the state probabilities of the previousframe (see \[1\]).2.2 ToAct iveWord  processThe ToActiveWord process has two inputs:it gets information from the Viterbi processassociated with phones that were active inthe current frame and should stay active inthe next frame.
The other input is from thegrammar processor that gives informationabout phonemes that are newly activated be-cause their predecessor phonemes had a highprobability to end.
Given these inputs, theToActiveWord process generates a list of ac-tive phonemes for the next frame.
A certainphoneme can be activated several times be-cause it might be activated by the grammarprocessor as well as by the Viterbi process.Also, the grammar processor could activatethe phoneme several times, especially if itis the first phoneme of a word with severalpredecessor words that have a high probabil-ity to end.
To avoid replication in the Ac-tiveWord Memory, the ToActiveWord pro-cess merges all these different instances of anactive phoneme into one, based on the bestprobability that this phone starts.2.3 Cach ing  mode l  parametersTo decrease the amount of memory on thesystem board, we use a caching scheme forthe output probabilities, the parameters withthe biggest storage requirements: only asmall subset of these parameters-the sub-set that corresponds to the output probabil-ities for a given speech segment-are loadedonto the board.
This loading operation isoverlapped with the processing of the framewhose output probabilities had been down-loaded in the previous frame.
With this ap-proach it is possible to use different mod-eling techniques for computing the outputprobability distributions.
The current ap-proach is to use as many as four indepen-dent discrete probability distributions thatare stored and combined on the "OutputDistribution Board."
Other modeling ap-proaches such as continuous distributionsand tied mixtures are also possible, as longas the probabilities can be computed andloaded in real time.2.4 Swi tch ing  processorsA frame is processed if the Viterbi processfinished the computation of the state proba-bilities of the active phones in the Active-Phone Memory and if the ToActiveWordprocess finished the generation of the listof active phones for the next frame.
Con-ceptually, the ActiveList Memories as wellas the memories containing the state proba-bilities have to be swapped before the nextframe can be processed.
However, instead ofswapping the memories, we activate a sec-ond set of processing elements that are con-nected to the memories in the right way.
Fig.4 sketches this principle.
During frame Athe ToActiveWord process A is active andbuilds up the ActiveWordMemoryA.
Simul-taneously, ViterbiB is active and processesthe active phonemes listed in the Active-WordMemoryB.
In the next frame, ViterbiB85AcUveWordMemorysequentiallist ofactivephonemesAcUveWordMemoryEl El El IViM'hi , rocessTo / From GrammarJ E!
El LJ L \] .oAc..wo.,,__ I"ActlveWordMemory AacCuSallist ofac(ivephonemesActlveWordMemory Bl~obabflityMe~iorLe s TableAH E-t,g!.g' FH .E!,,,g!.g, i --.I E l  ILl E l  L\] r .
, _ .
,  .....
II%??
"11 - I "Figure 3: Basic Architecture of the PhoneProcessing SystemFigure 4: Switching Processorsand ToActiveWordA are inactive and Viter-biA and ToActiveWordB are active.
Thisway, no multiplexors are needed to swapmemories.
All that is required is to activatethe right set of processors.
This approachalso has the advantage that the complete sys-tem is symmetric: the subsystem that hasthe elements A is identical to the subsystemwith elements B.3 ImplementationAll the memories on the system are accessibleby the host CPU via VME bus.
To reducethe number of discrete components on thesystem, the host CPU communicates only tothe custom VLSI processors.
These proces-sors have a small instruction set to read andwrite memories and internal status registers.Using this approach, no address or data bushas to be multiplexed.The testing strategy for the custom proces-sors is scanpath testing.
Individual chips canbe tested by using a generic scantest setup,or they can be tested on the board by usingthe existing VME interface.
A dedicated on-chip test controller supervises this VME testmode so that even the VME interface con-troller can be tested.
This way, every stateon the complete board (except he test con-troller itself) is observuble and controllablewithout a change of hardware.The board has two copies of six genericVLSI processors that implement the ToAc-tiveWord and Viterbi processes.
The chipswere designed with the Berkeley LagerIV sili-con assembly system \[2\] and are currently un-der fabrication using a 2-#m CMOS technol-ogy.
The table below summarizes the statis-tics for the processors.86VLSI  Custom processorsname transist.
I size \[mm2\] I PadsViterbi chipset:Prob i 57,000 12.1x12.2 204Back 38,000 11.2x11.8 204Add 12,000 10.5x10.5 204ToActiveList chipset:Request 27,000 10.5x11.2 204GndProb 5,530 4.9 x 5.6 83Data 15,700 11.2x10.5 204HS_Int 19,800 6.7 x 5.8 10831,900 6.9x7.4 130 SHAnt4 Status  and future workAll the chips listed have been designed andverified.
They are currently being fabricatedthrough MOSIS using a 2-#m CMOS pro-cess.
We have received some of the chipsback, and are currently testing them andbuilding the two custom boards.
After com-pleting the construction of the current hard-ware design, we will be developing softwaretools to support his architecture, and to runrecognition experiments and real-time sys-tems.Once we have completed the constructionof the first system, we will evaluate the cur-rent architecture to determine the computa-tional and algorithmic bottlenecks.
To fullyuse the capabilities of this design we will bedeveloping a large vocabulary recognizer torun on this board.
A major area of researchwill be the design and implementation f al-gorithms for real-time grammar processingcomputation, since these parts of the systemwill be running on general-purpose CPUs(TMS320C30s communicating with SUNs).recognition for large vocabularies in realtime.
The system will be at least by a factorof 50 more powerful than existing solutions.References\[1\]\[2\]J. Rabaey, R. Brodersen, A. StSlzle,S.
Narayanaswamy, D. Chen, R. Yu,P.
Schrupp, H. Murveit, and A. Santos,VLSI Signal Processing III, chapter ALarge Vocabulary Real Time ContinuousSpeech Recognition System, pages 61-74,IEEE Press, 1988.C.
S. Shung, R. Jain, K. Rimey, E. Wang,M.
B. Srivastava, E. Lettang, S. K. Azim,P.
N. Hilfinger, J. Rabaey, and R. W.Brodersen, An Integrated CAD Sys-tem for Algorithm-Specific IC Design.
In22nd Hawaii Int.
Conf.
System Science(HICSS-22), January 1989.5 Conc lus ionWe have presented a novel architecture thatuses full custom integrated circuits to per-form hidden Markov-moddel-based speech87
