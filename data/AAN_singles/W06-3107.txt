Proceedings of the Workshop on Statistical Machine Translation, pages 47?54,New York City, June 2006. c?2006 Association for Computational LinguisticsSearching for alignments in SMT.
A novel approach based on an Estimationof Distribution Algorithm ?Luis Rodr?
?guez, Ismael Garc?
?a-Varea, Jose?
A. Ga?mezDepartamento de Sistemas Informa?ticosUniversidad de Castilla-La Manchaluisr@dsi.uclm.es, ivarea@dsi.uclm.es, jgamez@dsi.uclm.esAbstractIn statistical machine translation, an align-ment defines a mapping between thewords in the source and in the target sen-tence.
Alignments are used, on the onehand, to train the statistical models and, onthe other, during the decoding process tolink the words in the source sentence to thewords in the partial hypotheses generated.In both cases, the quality of the alignmentsis crucial for the success of the translationprocess.
In this paper, we propose an al-gorithm based on an Estimation of Dis-tribution Algorithm for computing align-ments between two sentences in a paral-lel corpus.
This algorithm has been testedon different tasks involving different pairof languages.
In the different experimentspresented here for the two word-alignmentshared tasks proposed in the HLT-NAACL2003 and in the ACL 2005, the EDA-based algorithm outperforms the best par-ticipant systems.1 IntroductionNowadays, statistical approach to machine trans-lation constitutes one of the most promising ap-proaches in this field.
The rationale behind this ap-proximation is to learn a statistical model from a par-allel corpus.
A parallel corpus can be defined as a set?This work has been supported by the Spanish ProjectsJCCM (PBI-05-022) and HERMES 05/06 (Vic.
Inv.
UCLM)of sentence pairs, each pair containing a sentence ina source language and a translation of this sentencein a target language.
Word alignments are neces-sary to link the words in the source and in the tar-get sentence.
Statistical models for machine trans-lation heavily depend on the concept of alignment,specifically, the well known IBM word based mod-els (Brown et al, 1993).
As a result of this, differ-ent task on aligments in statistical machine transla-tion have been proposed in the last few years (HLT-NAACL 2003 (Mihalcea and Pedersen, 2003) andACL 2005 (Joel Martin, 2005)).In this paper, we propose a novel approach to dealwith alignments.
Specifically, we address the prob-lem of searching for the best word alignment be-tween a source and a target sentence.
As there isno efficient exact method to compute the optimalalignment (known as Viterbi alignment) in most ofthe cases (specifically in the IBM models 3,4 and 5),in this work we propose the use of a recently ap-peared meta-heuristic family of algorithms, Estima-tion of Distribution Algorithms (EDAs).
Clearly, byusing a heuristic-based method we cannot guaranteethe achievement of the optimal alignment.
Nonethe-less, we expect that the global search carried outby our algorithm will produce high quality resultsin most cases, since previous experiments with thistechnique (Larran?aga and Lozano, 2001) in differentoptimization task have demonstrated.
In addition tothis, the results presented in section 5 support theapproximation presented here.This paper is structured as follows.
Firstly, Sta-tistical word alignments are described in section 2.Estimation of Distribution Algorithms (EDAs) are47introduced in section 3.
An implementation of thesearch for alignments using an EDA is described insection 4.
In section 5, we discuss the experimentalissues and show the different results obtained.
Fi-nally, some conclussions and future work are dis-cussed in section 6.2 Word Alignments In Statistical MachinetranslationIn statistical machine translation, a word alignmentbetween two sentences (a source sentence f and atarget sentence e) defines a mapping between thewords f1...fJ in the source sentence and the wordse1..eI in the target sentence.
The search for the op-timal alignment between the source sentence f andthe target sentence e can be stated as:a?
= argmaxa?APr(a|f , e) = argmaxa?APr(f ,a|e) (1)being A the set of all the possible alignments be-tween f and e.The transformation made in Eq.
(1) allows us toaddress the alignment problem by using the statitisti-cal approach to machine translation described as fol-lows.
This approach can be stated as: a source lan-guage string f = fJ1 = f1 .
.
.
fJ is to be translatedinto a target language string e = eI1 = e1 .
.
.
eI .Every target string is regarded as a possible transla-tion for the source language string with maximum a-posteriori probability Pr(e|f).
According to Bayes?decision rule, we have to choose the target stringthat maximizes the product of both the target lan-guage model Pr(e) and the string translation modelPr(f |e).
Alignment models to structure the trans-lation model are introduced in (Brown et al, 1993).These alignment models are similar to the conceptof Hidden Markov models (HMM) in speech recog-nition.
The alignment mapping is j ?
i = aj fromsource position j to target position i = aj .
In sta-tistical alignment models, Pr(f ,a|e), the alignmenta is usually introduced as a hidden variable.
Never-theless, in the problem described in this article, thesource and the target sentences are given, and we arefocusing on the optimization of the aligment a.The translation probability Pr(f ,a|e) can berewritten as follows:Pr(f ,a|e) =J?j=1Pr(fj , aj |f j?11 , aj?11 , eI1)=J?j=1Pr(aj |f j?11 , aj?11 , eI1)?Pr(fj |f j?11 , aj1, eI1) (2)The probability Pr(f ,a|e) can be estimated byusing the word-based IBM statistical alignmentmodels (Brown et al, 1993).
These models, how-ever, constrain the set of possible alignments so thateach word in the source sentence can be aligned atmost to one word in the target sentence.
Of course,?real?
alignments, in most of the cases, do not fol-low this limitation.
Hence, the alignments obtainedfrom the IBM models have to be extended in someway to achieve more realistic alignments.
This isusually performed by computing the alignments inboth directions (i.e, first from f to e and then frome to f ) and then combining them in a suitable way(this process is known as symmetrization).3 Estimation of Distribution AlgorithmsEstimation of Distribution Algorithms (EDAs)(Larran?aga and Lozano, 2001) are metaheuristicswhich has gained interest during the last five yearsdue to their high performance when solving com-binatorial optimization problems.
EDAs, as wellas genetics algorithms (Michalewicz, 1996), arepopulation-based evolutionary algorithms but, in-stead of using genetic operators are based on the es-timation/learning and posterior sampling of a prob-ability distribution, which relates the variables orgenes forming and individual or chromosome.
Inthis way the dependence/independence relations be-tween these variables can be explicitly modelled inthe EDAs framework.
The operation mode of acanonical EDA is shown in Figure 1.As we can see, the algorithm maintains a popu-lation of m individuals during the search.
An in-dividual is a candidate or potential solution to theproblem being optimized, e.g., in the problem con-sidered here an individual would be a possible align-ment.
Usually, in combinatorial optimization prob-lems an individual is represented as a vector of inte-gers a = ?a1, .
.
.
, aJ?, where each position aj can481.
D0 ?
Generate the initial population (m individuals)2.
Evaluate the population D03.
k = 14.
Repeat(a) Dtra ?
Select s ?
m individuals from Dk?1(b) Estimate/learn a new modelM from Dtra(c) Daux ?
Sample m individuals fromM(d) Evaluate Daux(e) Dk ?
Select m individuals from Dk?1 ?Daux(f) k = k + 1Until stop conditionFigure 1: A canonical EDAtake a set of finite values ?aj = {0, .
.
.
, I}.
The firststep in an evolutionary algorithm is to generate theinitial population D0.
Although D0 is usually gener-ated randomly (to ensure diversity), prior knowledgecan be of utility in this step.Once we have a population our next step is toevaluate it, that is, we have to measure the goodnessor fitness of each individual with respect to the prob-lem we are solving.
Thus, we use a fitness functionf(a) = Pr(f ,a|e) (see Eq.
(3)) to score individu-als.
Evolutionary algorithms in general and EDAs inparticular seek to improve the quality of the individ-uals in the population during the search.
In geneticalgorithms the main idea is to build a new popula-tion from the current one by copying some individu-als and constructing new ones from those containedin the current population.
Of course, as we aim toimprove the quality of the population with respect tofitness, the best/fittest individuals have more chanceto be copied or selected for recombination.In EDAs, the transition between populations isquite different.
The basic idea is to summarizethe properties of the individuals in the populationby learning a probability distribution that describesthem as much as possible.
Since the quality of thepopulation should be improved in each step, onlythe s fittest individuals are selected to be included inthe dataset used to learn the probability distributionPr(a1, .
.
.
,aJ), in this way we try to discover thecommon regularities among good individuals.
Thenext step is to obtain a set of new individuals bysampling the learnt distribution.
These individualsare scored by using the fitness function and added tothe ones forming the current population.
Finally, thenew population is formed by selecting n individualsfrom the 2n contained in the current one.
A commonpractice is to use some kind of fitness-based elitismduring this selection, in order to guarantee that thebest(s) individual(s) is/are retained.The main problem in the previous description isrelated to the estimation/learning of the probabilitydistribution, since estimating the joint distribution isintractable in most cases.
In the practice, what islearnt is a probabilistic model that consists in a fac-torization of the joint distribution.
Different levelsof complexity can be considered in that factoriza-tion, from univariate distributions to n-variate onesor Bayesian networks (see (Larran?aga and Lozano,2001, Chapter 3) for a review).
In this paper, asthis is the first approximation to the alignment prob-lem with EDAs and, because of some questions thatwill be discussed later, we use the simplest EDAmodel: the Univariate Marginal Distribution Algo-rithm or UMDA (Muhlenbein, 1997).
In UMDAit is assumed that all the variables are marginallyindependent, thus, the n-dimensional probabilitydistribution, Pr(a1, .
.
.
, aJ), is factorized as theproduct of J marginal/unidimensional distributions:?Jj=1 Pr(aj).
Among the advantages of UMDAwe can cite the following: no structural learning isneeded; parameter learning is fast; small dataset canbe used because only marginal probabilities have tobe estimated; and, the sampling process is easy be-cause each variable is independently sampled.4 Design of an EDA to search foralignmentsIn this section, an EDA algorithm to align a sourceand a target sentences is described.4.1 RepresentationOne of the most important issues in the definitionof a search algorithm is to properly represent thespace of solutions to the problem.
In the problemconsidered here, we are searching for an ?optimal?alignment between a source sentence f and a targetsentence e. Therefore, the space of solutions can bestated as the set of possible alignments between bothsentences.
Owing to the constraints imposed by theIBM models (a word in f can be aligned at most toone word in e), the most natural way to represent a49solution to this problem consists in storing each pos-sible alignment in a vector a = a1...aJ , being J thelength of f. Each position of this vector can take thevalue of ?0?
to represent a NULL alignment (that is,a word in the source sentence that is aligned to nowords in the target sentence) or an index represent-ing any position in the target sentence.
An exampleof alignment is shown in Figure 4.1.Please , I would like to book a roomnulldesearia reservar una habitacion ..e :Por favorf : ,( 0     1     2     4     6     7     8     9 )Figure 2: Example of alignment and its representa-tion as a vector4.2 Evaluation functionDuring the search process, each individual (searchhypothesis) is scored using the fitness function de-scribed as follows.
Let a = a1 ?
?
?
aJ be the align-ment represented by an individual.
This alignment ais evaluated by computing the probability p(f ,a|e).This probability is computed by using the IBMmodel 4 as:p(f ,a|e) =?(?,pi)?
?f ,a?p(?, pi|e)I?i=1n(?i|ei)?I?i=1?i?k=1t(?ik|ei)?I?i=1,?i>0d=1(pii1 ?
c?i |Ec(e?i),Fc(?i1))?I?i=1?i?k=2d>1(piik ?
pii(k?1)|Fc(?ik))?
(J ?
?0?0)pJ?2?00 p?01 ?
?0?k=1t(?0k|e0) (3)where the factors separated by ?
symbols denotefertility, translation, head permutation, non-headpermutation, null-fertility, and null-translation prob-abilities1.This model was trained using the GIZA++ toolkit(Och and Ney, 2003) on the material available for thedifferent alignment tasks described in section 5.14.3 SearchIn this section, some specific details about the searchare given.
As was mentioned in section 3, the algo-rithm starts by generating an initial set of hypothe-ses (initial population).
In this case, a set of ran-domly generated alignments between the source andthe target sentences are generated.
Afterwards, allthe individuals in this population (a fragment of areal population is shown in figure 3) are scored usingthe function defined in Eq.(4.2).
At this point, theactual search starts by applying the scheme shownin section 3, thereby leading to a gradual improve-ment in the hypotheses handled by the algorithm ineach step of the search.This process finishes when some finalization cri-terium (or criteria) is reached.
In our implementa-tion, the algorithm finishes when it passes a certainnumber of generations without improving the qual-ity of the hypotheses (individuals).
Afterwards, thebest individual in the current population is returnedas the final solution.Regarding the EDA model, as commented before,our approach rely on the UMDA model due mainlyto the size of the search space defined by the task.The algorithm has to deal with individuals of lengthJ , where each position can take (I + 1) possiblevalues.
Thus, in the case of UMDA, the number offree parameters to be learnt for each position is I(e.g., in the English-French task avg(J) = 15 andavg(I) = 17.3).
If more complex models were con-sidered, the size of the probability tables would havegrown exponentially.
As an example, in a bivariatemodel, each variable (position) is conditioned on an-other variable and thus the probability tables P (.|.
)to be learnt have I(I + 1) free parameters.
In or-der to properly estimate the probabilty distributions,the size of the populations has to be increased con-siderably.
As a result, the computational resources1The symbols in this formula are: J (the length of e), I (thelength of f ), ei (the i-th word in eI1), e0 (the NULL word), ?i(the fertility of ei), ?ik (the k-th word produced by ei in a), piik(the position of ?ik in f ), ?i (the position of the first fertile wordto the left of ei in a), c?i (the ceiling of the average of all pi?ikfor ?i, or 0 if ?i is undefined).501 1 5 3 2 0 6 0 (-60.7500)1 6 5 2 3 0 0 5 (-89.7449)1 2 2 6 4 0 5 0 (-90.2221)1 2 3 5 0 3 6 2 (-99.2313)0 6 0 2 4 6 3 5 (-99.7786)2 0 0 2 2 0 3 4 (-100.587)1 0 1 6 3 6 0 5 (-101.335)Figure 3: Part of one population generated duringthe search for the alignments between the Englishsentence and then he tells us the correct result !and the Romanian sentence si ne spune noua rezul-tatul corect !.
These sentences are part of the HLT-NAACL 2005 shared task.
Some individuals andtheir scores (fitness) are shown.required by the algorithm rise dramatically.Finally, as was described in section 3, some pa-rameters have to be fixed in the design of an EDA.On the one hand, the size of each population mustbe defined.
In this case, this size is proportional tothe length of the sentences to be aligned.
Specifi-cally, the size of the population adopted is equal tothe length of source sentence f multiplied by a factorof ten.On the other hand, as we mentioned in section 3the probability distribution over the individuals isnot estimated from the whole population.
In thepresent task about 20% of the best individuals ineach population are used for this purpose.As mentioned above, the fitness function used inthe algorithm just allows for unidirectional align-ments.
Therefore, the search was conducted inboth directions (i.e, from f to e and from e tof ) combining the final results to achieve bidirec-tional alignments.
To this end, diffferent approaches(symmetrization methods) were tested.
The resultsshown in section 5.2 were obtained by applying therefined method proposed in (Och and Ney, 2000).5 Experimental ResultsDifferent experiments have been carried out in or-der to assess the correctness of the search algorithm.Next, the experimental metodology employed andthe results obtained are described.5.1 Corpora and evaluationThree different corpora and four different test setshave been used.
All of them are taken from thetwo shared tasks in word alignments developed inHLT/NAACL 2003 (Mihalcea and Pedersen, 2003)and ACL 2005 (Joel Martin, 2005).
These two tasksinvolved four different pair of languages, English-French, Romanian-English, English-Inuktitut andEnglish-Hindi.
English-French and Romanian-English pairs have been considered in these exper-iments (owing to the lack of timeto properly pre-process the Hindi and the Inuktitut).
Next, a briefdescription of the corpora used is given.Regarding the Romanian-English task, the testdata used to evaluate the alignments consisted in248 sentences for the 2003 evaluation task and 200for the 2005 evaluation task.
In addition to this, atraining corpus, consisting of about 1 million Ro-manian words and about the same number of En-glish word has been used.
The IBM word-basedalignment models were training on the whole cor-pus (training + test).
On the other hand, a subsetof the Canadian Hansards corpus has been used inthe English-French task.
The test corpus consists of447 English-French sentences.
The training corpuscontains about 20 million English words, and aboutthe same number of French words.
In Table 1, thefeatures of the different corpora used are shown.To evaluate the quality of the final alignments ob-tained, different measures have been taken into ac-count: Precision, Recall, F-measure, and AlignmentError Rate.
Given an alignment A and a referencealignment G (both A and G can be split into twosubsets AS ,AP and GS , GP , respectively represent-ing Sure and Probable alignments) Precision (PT ),Recall (RT ), F-measure (FT ) and Alignment ErrorRate (AER) are computed as (where T is the align-ment type, and can be set to either S or P ):PT = |AT?GT ||AT |RT = |AT?GT ||GT |FT = |2PTRT ||PT +RT |AER = 1?
|AS?GS |+ |AP?GP ||AP |+ |GS |51Table 1: Features of the corpora used in the different alignment taskEn-Fr Ro-En 03 Ro-En 05Training size 1M 97K 97KVocabulary 68K / 86K 48K / 27K 48K / 27KRunning words 20M / 23M 1.9M / 2M 1.9M / 2MTest size 447 248 200It is important to emphasize that EDAs are non-deterministics algorithms.
Because of this, the re-sults presented in section 5.2 are actually the meanof the results obtained in ten different executions ofthe search algorithm.5.2 ResultsIn Tables 2, 3 and 4 the results obtained from thedifferent tasks are presented.
The results achievedby the technique proposed in this paper are com-pared with the best results presented in the sharedtasks described in (Mihalcea and Pedersen, 2003)(Joel Martin, 2005).
The results obtained by theGIZA++ hill-climbing algorithm are also presented.In these tables, the mean and the variance of the re-sults obtained in ten executions of the search algo-rithm are shown.
According to the small variancesobserved in the results we can conclude that the non-deterministic nature of this approach it is not statis-tically significant.According to these results, the proposed EDA-based search is very competitive with respect to thebest result presented in the two shared task.In addition to these results, additional experi-ments were carried out in to evaluate the actual be-havior of the search algorithm.
These experimentswere focused on measuring the quality of the algo-rithm, distinguishing between the errors producedby the search process itself and the errors producedby the model that leads the search (i.e, the errors in-troduced by the fitness function).
To this end, thenext approach was adopted.
Firstly, the (bidirec-tional) reference alignments used in the computationof the Alignment Error Rate were split into two setsof unidirectional alignments.
Owing to the fact thatthere is no exact method to perform this decomposi-tion, we employed the method described in the fol-lowing way.
For each reference alignment, all thepossible decompositions into unidirectional align-ments were perfomed, scoring each of them withthe evaluation function F (a) = p(f ,a|e) defined insection (3), and being selected the best one, aref .Afterwards, this alignment was compared with thesolution provided by the EDA, aeda .
This com-parison was made for each sentence in the test set,being measuried the AER for both alignments aswell as the value of the fitness function.
At thispoint, we can say that a model-error is produced ifF (aeda) > F (aref ).
In addition, we can say that asearch-error is produced if F (aeda) < F (aref ).
Intable 5, a summary for both kinds of errors for theEnglish-Romanian 2005 task is shown.
In this tablewe can also see that these results correlate with theAER figures.These experiments show that most of the errorswere not due to the search process itself but to an-other different factors.
From this, we can concludethat, on the one hand, the model used to lead thesearch should be improved and, on the other, dif-ferent techniques for symmetrization should be ex-plored.6 Conclusions and Future WorkIn this paper, a new approach, based on the use of anEstimation of Distribution Algorithm has been pre-sented.
The results obtained with this technique arevery promising even with the simple scheme hereconsidered.According to the results presented in the previ-ous section, the non-deterministic nature of the algo-rithm has not a real influence in the performance ofthis approach.
Therefore, the main theoretical draw-back of evolutionary algorithms have been provennot to be an important issue for the task we have ad-dressed here.Finally, we are now focusing on the influence ofthese improved alignments in the statistical modelsfor machine translation and on the degree of accu-52Table 2: Alignment quality (%) for the English-French task with NULL alignmentsSystem Ps Rs Fs Pp Rp Fp AEREDA 73.82 82.76 78.04 83.91 29.50 43.36 13.61 ?0.03GIZA++ 73.61 82.56 77.92 79.94 32.96 46.67 15.89Ralign.EF1 72.54 80.61 76.36 77.56 36.79 49.91 18.50XRCE.Nolem.EF.3 55.43 93.81 69.68 72.01 36.00 48.00 21.27Table 3: Alignment quality (%) for the Romanian-English 2003 task with NULL aligmentsSystem Ps Rs Fs Pp Rp Fp AEREDA 94.22 49.67 65.05 76.66 60.97 67.92 32.08 ?0.05GIZA++ 95.20 48.54 64.30 79.89 57.82 67.09 32.91XRCE.Trilex.RE.3 80.97 53.64 64.53 63.64 61.58 62.59 37.41XRCE.Nolem-56k.RE.2 82.65 54.12 65.41 61.59 61.50 61.54 38.46Table 4: Alignment quality (%) for the Romanian-English 2005 taskSystem Ps Rs Fs Pp Rp Fp AEREDA 95.37 54.90 69.68 80.61 67.83 73.67 26.33 ?0.044GIZA++ 95.68 53.29 68.45 81.46 65.83 72.81 27.19ISI.Run5.vocab.grow 87.90 63.08 73.45 87.90 63.08 73.45 26.55ISI.Run4.simple.intersect 94.29 57.42 71.38 94.29 57.42 71,38 28.62ISI.Run2.simple.union 70.46 71.31 70.88 70.46 71.31 70.88 29.12Table 5: Comparison between reference aligments (decomposed into two unidirectional alignments) andthe alignments provided by the EDA.
Search errors and model errors for EDA and GIZA++ algorithms arepresented.
In addition, the AER for the unidirectional EDA and reference alignments is also shown.
Theseresult are obtained on the Romanian-English 05 taskRomanian-English English-RomanianEDA search errors (%) 35 (17.5 %) 18 (9 %)EDA model errors (%) 165 (82.5 %) 182 (91 %)GIZA++ search errors (%) 87 (43 %) 81 (40 %)GIZA++ model errors (%) 113 (57 %) 119 (60 %)AER-EDA 29.67 % 30.66 %AER-reference 12.77 % 11.03 %53racy that could be achieved by means of these alig-ments.
In addition to this, the integration of thealigment algorithm into the training process of thestatistical translation models is currently being per-formed.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisti-cal machine translation: Parameter estimation.
Comp.Linguistics, 19(2):263?311.Ted Pedersen Joel Martin, Rada Mihalcea.
2005.
Wordalignment for languages with scarce resources.
InRada Mihalcea and Ted Pedersen, editors, Proceed-ings of the ACL Workshop on Building and ExploitingParallel Texts: Data Driven Machine Translation andBeyond, pages 1?10, Michigan, USA, June 31.
Asso-ciation for Computational Linguistics.P.
Larran?aga and J.A.
Lozano.
2001.
Estimation ofDistribution Algorithms.
A New Tool for EvolutionaryComputation.
Kluwer Academic Publishers.Z.
Michalewicz.
1996.
Genetic Algorithms + DataStructures = Evolution Programs.
Springer-Verlag.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Rada Mihalcea andTed Pedersen, editors, HLT-NAACL 2003 Workshop:Building and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 1?10, Edmonton,Alberta, Canada, May 31.
Association for Computa-tional Linguistics.Heinz Muhlenbein.
1997.
The equation for responseto selection and its use for prediction.
EvolutionaryComputation, 5(3):303?346.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In ACL00, pages 440?447,Hongkong, China, October.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.54
