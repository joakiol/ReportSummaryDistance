Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 28?32,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsRecipes for building voice search UIs for automotiveMartin Labsky, Ladislav Kunc, Tomas Macek, Jan Kleindienst, Jan VystrcilIBM Prague Research and Development LabV Parku 2294/4, 148 00 Prague 4Czech Republic{martin.labsky, ladislav kunc1, tomas macek,jankle, jan vystrcil}@cz.ibm.comAbstractIn this paper we describe a set of tech-niques we found suitable for buildingmulti-modal search applications for au-tomotive environments.
As these ap-plications often search across differenttopical domains, such as maps, weatheror Wikipedia, we discuss the problemof switching focus between different do-mains.
Also, we propose techniques use-ful for minimizing the response time of thesearch system in mobile environment.
Weevaluate some of the proposed techniquesby means of usability tests with 10 novicetest subjects who drove a simulated lanechange test on a driving simulator.
We re-port results describing the induced drivingdistraction and user acceptance.1 IntroductionThe task of designing mobile search user inter-faces (UIs) that combine multiple application do-mains (such as navigation, POI and web search)is significantly harder than just placing all sin-gle domain solutions adjacent to one another.
Wepropose and evaluate a set of UI techniques use-ful for implementing such systems.
The tech-niques are exemplified using a prototype multi-modal search assistant tailored for in-car use.
Theprototype supports several application domains in-cluding navigation and POI search, Wikipedia,weather forecasts and car owner?s manual.
Fi-nally, we report usability evaluation results usingthis prototype.2 Related WorkTwo examples of multi-modal search UIs for au-tomotive are the Toyota Entune1and the Honda1http://www.toyota.com/entune/Link2.
Both infotainment systems integrate aset of dedicated mobile applications includinga browser, navigation, music services, stocks,weather or traffic information.
Both use a tablet ora smartphone to run the mobile applications whichbrings the advantage of faster upgrades of the in-car infotainment suite.
Home screens of these sys-tems consist of a matrix of square tiles that corre-spond to individual applications.The answers presented to the user should onlycontain highly relevant information, e.g.
present-ing only points of interest that are near the cur-rent location.
This is called conversational maximof relevance (Paul, 1975).
Many other lessonslearned by evaluating in-car infotainment systemsare discussed in (Green, 2013).In recent years, personal assistant systems likeSiri (Aron, 2011), Google Now!
(Google, 2013)and the Dragon Mobile Assistant (Nuance, 2013)started to penetrate the automotive environment.Most of these applications are being enhancedwith driving modes to enable safer usage whiledriving.
Dragon Mobile Assistant can detectwhether the user is in a moving car and auto-matically switches to ?Driver Mode?
that relieson speech recognition and text-to-speech feed-back.
Siri recently added spoken presentationof incoming text messages and voice mail, andit also allows to dictate responses.
Besides thespeech-activated assistant functionality, GoogleNow!
tries to exploit various context variables(e.g.
location history, user?s calendar, search his-tory).
Context is used for pro-active reminders thatpop-up in the right time and place.
Speech recog-nition of Google Now!
has an interesting featurethat tries to act upon incomplete/interim recogni-tion results; sometimes the first answer is howevernot the right one which is later detected and theanswer is replaced when results are refined.2http://owners.honda.com/hondalink/nextgeneration283 UI techniques to support search whiledrivingBelow we present selected techniques we founduseful while designing and testing prototypesearch UIs for automotive.3.1 Nearly stateless VUIWhile driving and interacting with an applicationUI, it often happens that the driver must interruptinteraction with the system due to a sudden in-crease of cognitive load associated with the pri-mary task of driving.
The interaction is eitherpostponed or even abandoned.
The UI activitymay later be resumed but often the driver willnot remember the context where s/he left off.
Inheavily state-based systems such as those basedon hierarchical menus, reconstruction of applica-tion context in the driver?s mind may be costly andassociated with multiple glances at the display.In order to minimize the need for memorizingor reconstructing the application context, we ad-vocate UIs that are as stateless as possible fromthe user?s point of view.
In the context of spokeninput, this means the UI should be able to processall voice input regardless of its state.This is important so that the driver does not needto recall the application state before s/he utters arequest.
For instance, being able to ask ?Wherecan we get a pizza?
only after changing screen to?POI search?
can be problematic as the driver (1)needs to change screens, (2) needs to rememberwhat the current screen is, and (3) may need tolook at the display to check the screen state.
Allof these issues may increase driver distraction (itshaptic, visual and mental components).3.2 Self-sufficient auditory channelAccording to the subjective results of usabilitytests described in Section 6 and according to ear-lier work on automotive dictation (Macek et al.,2013), many drivers were observed to rely primar-ily on the audio-out channel to convey informationfrom the UI while driving and they also preferredit to looking at a display.
A similar observationwas made also for test drivers who listened to andnavigated news articles and short stories (Kunc etal., 2014).Two recommendations could be abstracted fromthe above user tests.
First, the UI should produceverbose audio output that fully describes whathappens with the system (in cases when the drivercontrols the UI while driving).
This includes spo-ken output as well as earcons indicating importantmicro-states of the system such as ?listening?
or?processing?.
Second, the UI should enable theuser to easily replay what has been said by thesystem, e.g.
by pressing a button, to offset the se-rial character of spoken output.
These steps shouldmake it possible for selected applications to run ina display-less mode while driving or at least mini-mize the number of gazes at the display.3.3 Distinguish domain transition typesBy observing users accessing functions of mul-tiple applications through a common UI, we ob-served several characteristic transition types.Hierarchical.
The user navigates a menu tree,often guided by GUI hints.Within domain.
Users often perform multipleinteractions within one application, such as per-forming several Wikipedia queries, refining themand browsing the retrieved results.Application switching.
Aware of the namingsof the applications supported by the system, usersoften switch explicitly to a chosen domain beforeuttering a domain-specific command.Direct task invocation.
Especially in case of UIshaving a unifying persona like Siri (Aron, 2011),users do not view the system as a set of appli-cations and instead directly request app-specificfunctions, regardless of their past interaction.Subdialog.
The user requests functionality outof the current application domain.
The corre-sponding application is invoked to handle the re-quest and then the focus returns automatically tothe original domain.
Examples include taking anote or checking the weather forecast while in themiddle of another task.Undo.
A combined ?undo?
or ?go back?
fea-ture accessible globally at a key press proved use-ful during our usability testing to negate any un-wanted actions accidentally triggered.Figure 1 shows samples for the above transi-tion types using an example multi-domain searchassistant further described in Section 4.
Similarlists of transition types ware described previously,e.g.
(Milward et al., 2006).
Based on observinghuman interactions with our prototype system, webuilt a simple probabilistic model to control thelikelihood of the system taking each of the abovetransition types, and used it to rescore the resultsof the ASR and NLU systems.29Figure 1: Transitions in a multi-domain system.3.4 Early and incremental feedback aboutthe application stateMobile search UIs often depend both on local andremote resources such as ASR and NLU servicesand various data providers.
In mobile environ-ments, availability and response times of remoteservices may vary significantly.
Most mobile UIsaddress this problem by responding with a beepand displaying a ?processing?
sign until the fi-nal answer is rendered.
We describe a UI tech-nique that combines redundant local and remoteresources (ASR and NLU) to quickly come upwith a partial meaningful response that addressesthe user?s request.
Chances are that the first re-sponse based on partial understanding is wrongand the following prompt must correct it.Figure 2 shows a template definition for a sys-tem prompt that starts playing once the system isconfident enough about the user?s intent being aweather forecast question.
The system providesforecasts for the current location by default butcan switch to other locations if specified by theuser.
Supposing the system is equipped with real-time ASR and NLU that quickly determine thehigh-level intent of the user, such as ?weather fore-cast?, the initial part of the prompt can start play-ing almost immediately after the user has stoppedspeaking.
While a prefix of this prompt is play-ing, more advanced ASR and NLU models de-liver a finer-grained and more precise interpreta-tion of the input, including any slot-value pairslike ?location=London?.
Once this final interpre-tation is known, the playback can be directed viathe shortest path to the identified variable promptsegments like <location>.
Further, the selec-tion of prompt prefix to be played can be guidedby a current estimate of service delays to mini-mize chances of potential pauses before speakingprompt segments whose values are not yet known.Figure 2: Sample incremental prompt graph.
Seg-ments are annotated with durations in round brack-ets and min/max times before an unknown slotvalue has to be spoken (ms).4 Voice search assistant prototypeIn this section we briefly present a voice search in-terface that was developed by incrementaly imple-menting the four UI techniques presented above.While interim versions of this system were onlyevaluated subjectively, formal evaluation resultsare presented for the final version in Section 6.The voice search assistant covers six applica-tion domains shown in Figure 3.
Navigation ser-vices include spoken route guidance together withunified destination entry by voice (addresses andPOIs).
Some POIs are accompanied by user re-views that can be read out as part of POI details.Figure 3: Prototype home screen (apps as tiles).Further, the user can search various knowledgesources like Wikipedia, Wolfram Alpha and theweb.
The retrieved results are pre-processed andthe first one is played back to the user with thepossibility of navigating the result list.To simulate asynchronous events, the systemreads out Skype text messages.
The driver can alsocreate location and time based reminders that popup during the journey.Finally, the system supports full-text searchover the car owner?s manual.
Relevant text pas-sages are read out and displayed based on a prob-lem description or question uttered by the driver.305 Usability testing setup and procedureA low-fidelity driving simulator setup similar tothe one described in (Curin et al., 2011) wasused to conduct lane change tests using (Mattes,2003).
Tests were conducted with 10 novice sub-jects and took approximately 1 hour and 20 min-utes per participant.
At the beginning and at theend of the test, subjects filled in pre-test and post-test questionnaires.
Before the actual test, eachparticipant practised both driving and using theprototype for up to 20 minutes.
The evaluatedtest consisted of four tasks: an initial undistracteddrive (used to adapt a custom LCT ideal path foreach participant), two distracted driving trips incounter-balanced order, and a final undistracteddrive (used for evaluation).
Each of the four driveswas performed at constant speed of 60km/h andtook about 3.5 minutes.
During the distracteddriving tasks, the users were instructed verballyto perform several search tasks using the proto-type.
During task 1, subjects had to set destina-tion to ?office?, then find a pharmacy along theroute, check the weather forecast and take a noteabout the forecast conditions.
Task 2 only dif-fered slightly by having a different destination andPOI, and by the user searching Wikipedia insteadof asking about weather.6 Usability testing resultsObjective distraction was measured using meandeviation (MDev) and standard deviation(SDLP ) of the vehicle?s lateral position (Mattes,2003).
Two versions of both statistics wereobtained: overall (computed over the whole trip)and using lane-keeping segments only.
The graphin Figure 4 shows averaged results for the finalundistracted drive and for the first and seconddistracted driving tasks (reflecting the order of thetasks, not their types).
We observe that using thesearch UI led to significant distraction during lanechange segments but not during lane keeping.Also, the distraction results for the first trip showhigher variance which we attribute to the usersstill adapting to the driving simulator and tousing the UI.
The observed distraction levels arecomparable to our earlier results obtained for atext dictation UI when used with a GUI display(Curin et al., 2011).Several observations came out of the subjec-tive feedback collected using forms.
The users re-ported extensive use of the auditory channel (both00,10,20,30,40,50,60,70,80,91UndistractedFirst taskSecondtask[m]OverallMDevOverallSDLPLane keeping MDevLane keeping SDLPFigure 4: Driving distraction while using a multi-modal search UI.in and out) only with occasional glimpses at thescre n (w however observed that objectively theylooked at the display more often than they reportedsubjectively).
Users also missed some informa-tion in the voice output channel such as audio indi-cation of route calculation progress (which couldtake several seconds).
Reading any text from thescreen was found difficult, and users requested thatplayback be improved; see related follow-up study(Kunc et al., 2014).
Interestingly, multiple partic-ipants requested voice commands that would du-plicate buttons like ?next?
and ?previous?, even incases where speech would be less efficient.
Thismay show a tendency to stick with a single modal-ity as described by (Suhm et al., 2001).
Addi-tionally, the users requested better synchronizationof navigation announcements like ?take exit 4 in200 metres?
with the output of other applications.The baseline behaviour utilized in the test wasthat high-priority navigation prompts interruptedthe output of other applications.
Navigation, POIsearch, simple note-taking and constrained searchdomains like weather and Wikipedia were foundmost useful (in this order).
Open web searchand browsing an original car owner?s manual wereconsidered too distracting to use while driving.7 ConclusionWe described several recipes for building spokensearch applications for automotive and exempli-fied them on a prototype search UI.
Early us-ability testing results for the prototype were pre-sented.
Our future work focuses on improving theintroduced techniques and exploring alternative UIparadigms (Macek et al., 2013).AcknowledgementThe presented work is part of an IBM and Nuancejoint research project.31ReferencesJacob Aron.
2011.
How innovative is apple?s newvoice assistant, siri?
New Scientist, 212(2836):24.J.
Curin, M. Labsky, T. Macek, J. Kleindienst,H.
Young, A. Thyme-Gobbel, H. Quast, andL.
Koenig.
2011.
Dictating and editing short textswhile driving: distraction and task completion.
InProceedings of the 3rd International Conference onAutomotive User Interfaces and Interactive Vehicu-lar Applications.Google.
2013.
Google now assistant.
Available athttp://www.google.com/landing/now/.Paul A Green.
2013.
Development and evaluationof automotive speech interfaces: useful informationfrom the human factors and the related literature.
In-ternational Journal of Vehicular Technology, 2013.L.
Kunc, M. Labsky, T. Macek, J. Vystrcil, J. Klein-dienst, T. Kasparova, D. Luksch, and Z. Medenica.2014.
Long text reading in a car.
In Proceedingsof the 16th International Conference on Human-Computer Interaction Conference (HCII).Tom?a?s Macek, Tereza Ka?sparov?a, Jan Kleindienst,Ladislav Kunc, Martin Labsk?y, and Jan Vystr?cil.2013.
Mostly passive information delivery in acar.
In Proceedings of the 5th International Confer-ence on Automotive User Interfaces and InteractiveVehicular Applications, AutomotiveUI ?13, pages250?253, New York, NY, USA.
ACM.Stefan Mattes.
2003.
The lane-change-task as a toolfor driver distraction evaluation.
In Proceedings ofthe Annual Spring Conference of the GFA/ISOES,volume 2003.David Milward, Gabriel Amores, Nate Blaylock,Staffan Larsson, Peter Ljunglof, Pilar Manchon, andGuillermo Perez.
2006.
D2.2: Dynamic multimodalinterface reconfiguration.
In Talk and Look: Toolsfor Ambient Linguistic Knowledge IST-507802 De-liverable D2.2.Nuance.
2013.
Dragon mobile assistant.
Available athttp://www.dragonmobileapps.com.Grice H Paul.
1975.
Logic and conversation.
Syntaxand semantics, 3:41?58.Bernhard Suhm, Brad Myers, and Alex Waibel.
2001.Multimodal error correction for speech user inter-faces.
ACM Transactions on Computer-Human In-teraction (TOCHI), 8(1):60?98.32
