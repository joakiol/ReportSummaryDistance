Design and evaluation of grammar checkers in multiple languagesAntje HELFRICHNLG, Microsoft Corp.I Microsoft WayRedmond, WA 98052antjehOmicrosoft.comBradley MUSICNLG, Microsoft Corp.1 Microsoft WayRedmond, WA 98052brmusic@microsofl.comAbstractThis paper describes i sues hn~olved hi the development of a grammar checker ht multiple languages at MiclvsoftColporation.
Focus is on design (selecting and priorit&ing error i(lent~ication rules) and evaluation (determinhtgproduct quality).IntroductionThe goal of the project discussed here is to developFrench, German and Spanish grammar checkersfor a broad user base consisting of millions ofMicrosoft Word customers - users who createdocuments of all types, styles and content, usingvarious terminology and dialects, and who wantproofing tools that help eliminate mistakes in anefficient and non-intruding lhshion.The fact that the user base is so broadposes many challenges, among them the questionsof which errors are most common among such adiverse set of users, and what types of input thegrammar checkers need to be tested and evaluated011.This paper will describe the connnonmethods and processes that we use across thelanguage teams for design and evaluation, whilefocusing on language-specific characteristics forthe actual product design.
The central role of largetext corpora in the three languages (includingregional variations) for both design and evaluationwill be discussed.Design: How do we know what the rightfeatures are?In the design phase of the software developmentprocess, we ask the question: What should theproduct do for the user?
For a grammar checker,the main features are the error detection andcorrection rules, or "critiques".
The goal of thedesign phase is to determine which level ofproofing and which error types typical MicrosoftWord users care about most.
It is important toremember that our grammar checker is not astandalone product, but a component withinMicrosoft Word, and that the main goal of the useris to create documents as efficiently as possible.We don't want to distract, delay or bother peoplewith a picky proofing component that points outlinguistic issues most users don't care about (evenif we could critique those with high precision j) oreagerly highlights any "suspicious" sentence with apotential problem.
Instead, we focus on critiquesthat are actually helpful to the majority of usersfrom their point of view and support hem in theirultimate goal of creating rammatically cleandocuments efficiently.Researching the customerThe first step towards determining the feature set isto describe the target user of the grammar checker.One early decision was to focus on native users,since we are developing a grammar checker andnot a language-learning tool.
However, many ofthe grammar mistakes native users make are also -or even more - common among non-native users,so we know that the grammar checker will behelpful to this population as well.The target user base for our grammarcheckers are current and future Microsoft Wordusers, and we benefit fi'om information that hasalready been gathered about the Microsoft Worduser profile.
We know that Microsoft Word is usedmostly at the workplace, and we know what typesof documents various professionals create in therespective countries.In addition to relying on general MicrosoftWord user information, we learn about people'sproofing behavior in interviews, focus groups andEven actual errors can belong in this category: the Frenchcapitalization rules lbr language names vs. people (e.g..franfais vs. Francais), for instance, are clear, but customerresearch shows that many users don't want such errors to bepointed out.1036surveys, conducted in the target markets Germany(where we include Swiss and Austrian speakers),France, Canada, Spain, and Latin America.
Wedevelop discussion guides and questionnaires togather detailed inforrnation about how peopleeusure that their doculnents are "gralnnlar-clean",starting with questions about he types ofdocuments they write, whether they care about hecorrectness of their writing equally for alldocuments (and have found, not surprisingly, thatthe level of desired proofing depends on theintended formality o1' the document, which in turndepends on the target audience for the text) andproceed with questions about how they prool' theirtexts and what types of issties they feel they needhelp with.Focus groups and survey participantsprovide a lot of input on tim question of whaterrors people care about most.
We know fromthese studies to focus on actual grammar errorsinstead of on stylistic issues, since there is nocommon agreement ahout he latter and people aregenerally less interested in seeing them pointedout.
We also receive detailed feedback onlanguage-slgecific priorities for error detection: wehave learned for illstance that tVrench speakers careabout agreement and getting tense and mood right,German speakers care about selection of case,capitalization and spelling together vs. apart rules,and Spanish speakers care about agreement,correct use o\[" clitics, and confusabie words, anlongother error types.Selecting and prioritizing the featuresAfter delermining tim target user for the granunarchecker, we systematically compile the set ofcritiques that will be helpful to this user base.
Forfeatures like the user interface we use data gainedfrom user feedback concerning the existing Englishgrammar checker and confirm the findings in thetarget countries; the actual error recognition rules,however, are selected solely on a hmguage-specificbasis.The methods we apply in order todetermine the critique sets are systematic and areshared among the tealrls.
First, error types andpotential critiques are compiled based Oll thesources listed below; in a second step we prioritizeand trim down the list o1' potential critiquesaccording to criteria of frequency, taelpl'tlllless, andreliability.Language/linguistic knowledge: Eachlanguage team consists of linguists andcomputational linguists who grew tip and wereeducated in the native language community.
Wepainfully remember grammar rules that weredrilled into us back in elementary school and havetheoretical nd practical experiences that rangefl'ona language teaching to transhttion/localizationbackgrounds to PhDs in linguistics.
While weknow that disagreement errors are conamon in allour target languages due to forced agreement(between subject and verb or between anoun andits articles/modifiers), we pay special attention tolanguage-specific phenomena and error types.
Forinstance, analysis of French errors reveals a highdegree of confusion between infinitive and pastparticipial verb forms, presumably due to theirphonetic equality; we therefore developed specialconfusable word detection algorithms for theFrench grammar checker.Another aspect el' language knowledge isto observe trends and changes in language use,whether the changes are speaker-induced (e.g.gradually changing case requirements after specificprepositions in German) or externally motivatedlike the spelling reform in Germany, which hashuge consequences for the grammar checker 2.Reference bool<s: Books about ypical (andfrequent) grammar errors can he hard to come by,depending on the language being analyzed, thoughwe did find sources for typical "grammalicalstumbling blocks" for all langt, ages.
Excellentiidormalion came from books about writing goodbusiness letters, since their target readers overhtpwith our target users, and they contain good lists ofgrammar issues that people often grapple with (e.g.capitalization i multil)le word expressions,inchiding standard business letter phrases, inGerman).
Unfortunately most of these give no (orvery little) indication of the frequency of the error.Customer research: As described above,we spend considerable time and effort toinvestigate what errors native language usersstruggle with and would like help with.2 The spelling reforna affects the grannnar checker since manychanges in capitalization rules and spelling together vs. ap:ulrules require syntactic parsing in order to identify and correctmistakes.
An example is "zur Zeit" which is still spelled apartwhen governing a genitive object, but is, according to the ImWspelling rules, spelled together and with lower case ("zurzeit")when used adverbially.1037Market analysis: We study the market brgrammar checkers and proofing tools in general inthe French/German/Spanish-speakiug co ntries, toreview what products and features users arefamiliar with and might expect in a grammarchecker.Text corpus: We process and reviewmillions o1' sentences for each language to find outwhich errors actually occur and at what frequency.All of the sources listed above contributeto the design process.
The most decisive factorsstem fi'om our customer research, which informs usabout what users view as their biggest grammarchallenges, and the corpus analysis, which inlkmnsus about what errors users actually make.
Corpusanalysis plays such a central role in out" featuredesign that it is discussed separately in the nextsection.Analyzing text and error dataOur text corpora are central for product design andevaluation, and we are investing heavily increating, acquMng, categorizing, storing, tracking,and maintaining data for the grammar checker andfurore product development projects.
While wehave to compile three separate corpora for French,German and Spanish, the methods and principleswe apply to building and maintaining the corporaare shared.The corpus used in the grammar checkerprotect is representative of the documents thattarget users create, and therefore the input that thegrammar checker will have to deal with.
It includesa mix ot' documents from various media (e.g.newspaper vs. web site), styles (e.g.
formal vs.casual) and content (e.g.
finance vs. science).
Theproportion of each category is predeterminedaccording to the Microsoft Word user profiledescribed above.The research community benefits fromaccess to published corpora not available forcommercial use.
In contrast, acorporation thatneeds data for development and testing purposes ismuch more restricted.
The following list gives anoverview of some of the challenges we are facedwith:Copyright issues: While we are surroundedby a lot of text, especially on the internet, many ofthese documents are copyrighted and cannot beused without permission; we need to followdetailed legal guidelines and procedures, whichcan cause substantial lag time between identifyinguseful corpus sources and actually acquiring andusing thena.Size: We need huge amounts of corpus, inall languages, in order to represent the variousmedia, styles, and contents.
To render test resultsmeaningful, we need to ensure that all of the errortypes we develop critiques for have sufficientrepresentation in the corpus.Edited vs. unedited ata: For our purposes,we are especially interested in text that has notundergone proofing and revision, in order to finderrors people actually make while entering text, aswell as to later test the quality of the grammarchecker.
Such documents are extremely hard tocome by, so we found ways to have such uneditedtext data specifically created for our project.Edited data is used to verify that the grammarchecker does not falsely identify errors in correctinput.Blind vs. non-blind dala: We divide ourcorpus into two parts of equal size andcorresponding content as far as document types,subject malter and writing styles are concerned.Halt' of the corpus is awfilable to the whole teamand is used for design and development as well astesting: The program manager uses this corpus toidentify and analyze rror types and frequency, andto support developers by providing corpus samplesfor specific grammatical constructions or erroroccurrences; the test team uses it to provide openfeedback to developers about he precision of theparser and llle grannnar checker.
Tim other half o1'the corpus is "blind" and only awtilable to the testteam; it is used to lneasure the accuracy of bothparser and grammar checker.
When the test teamfinds "bugs" (e.g.
missed error identification, orfaulty analysis of a correct sentence asgrammatically wrong) in the blind corpus, theunderlying pattern o1' the problem is reported, butthe specific sentence is not revealed in order toprevent uning the product o individual sentencesand biasing the accuracy numbers.
Doubling thecorpus in this way means that we need more datain terms of sheet quantity; it also poses additionalchallenges for categorizing, tracking, and securingthe data.Cleaning: While we don't want the corpusclean in terms of gramlnar errors, we do need toprocess it to standardize the format, remove1038elements like HTMI, formatting codes, hardro tu l l lS ,  e tc .
so  we c{111 use  it i l l  aUtO lnated  too ls .The extensive ffort put into design helpsto ensure that the product focuses on errors peopleactually make and care about.
The next sectiondescribes the lesting done to determine if we'veachieved acceptable quality for identification andcorrection of these error types.Evaluation: l tow do we know when we'redone?l )uring tim development process, testers givefeedback and quality assessment, based on both theblind and non-blind corpora, and usil'lg a variety o1'tools to provide quick turn-around after a change tothe system.
Development feedback shows lheeffects of each change to the lexicon, nlorphology,grammar  or c r i t iqu ing  system, where the testerssystematically apply language-iildependenln-iethods of analysis and reporlirig, l)ovelopersneed io know the impact of any chal-iges they makeas soon as possible, so that l'urlher developnlonican proceed with confidence or, in the case of allunexpected negative impact, problenls can becorrected before furiher developnaent.
Qualityassessment is partially reflected in terms of agreed-upon nlelrics, such as recall, precision, and falseflags per page.As we approach the end of thedevolopnlent process, we continue to nmiliior themetrics against pro-defined goals, but also shiftfocus io other kinds of testing with orientationtowards the user's experience wilh the glallllnal"checker.This section will briefly outline keymetrics used for quality assessment aswell assome of the user-focused testing we do beforeshipping the final version.PrecisionPrecision = good flags/total flags, e.g.
if thegranlnlar checker correctly identifies 160 errors ona given corpus, and iricorreclly flags 15words/phrases a erfors ill lhat corpus, theprecision will be 160/I 75=91%.
Determiningprecision has less meaning the more the test corpushas undefgone diting.
Ill the cxl.fcllle case o1" ahighly edited text (e.g.
a published book) where inprinciple there should be no grammar errorspresent at all, the only flags a grammar checkercould possibly get would be l'alse flags, thusprecision would be 0%, which would give aninacetlrate illlprossion of prodtlct quality.
3Precision is reported on a wu-iety of corpora withinthe hmguage teams, these having the samerepresentativity across the teams.Reca l lRecall = good flags/expected flags, i.e.
whatpercentage o1' the errors is actually spoUed.Research on users' impressions of granlmarchecker quality consistently shows that i_lSOl'S \[fieless concerned about recall than about he numberof false flags.
Tiffs has entailed a cross-linguisticprioritization of improving quality by the reductionof I'alse flags.
In tin'ms of metrics, this nleans thatincreasing precision and decreasing the false flagper page rate have had a higher priority than recallfor those glamlllar checkers.
()rio challenge hero istile fact lhat methods for rodt l c i l lg  false fhtgs C\[lllrisk loss of good fhtgs that would be helpful to ourusers, so a light hand is required to balancereduction of the absohito number of false flags vs.still spotting and correeling the errors people reallymake.False flags per pageAlthough highly edited texts are less interesting fordcternaining precision, they are iml)Ollant as a basisfor measuring how 'noisy' the gl'allllltar checker ison a finished docunlent.
4 This call be n-ieasured intornis of false flags per page, with the ideal beingzero - however language being as coinplox as it is,it is in fact extremely difficult to achieve no falseflags in a system that attempts to pal'SO and correctthe frequent errors i l l agroonloi l t ,  n-iood, etc.
Molerealistically, a trade-off has to be accepted thatgives the critiques l'OOnl lO work, while still stayingunder what's considered an annoying level of i'alsef lags per page.
\] i \]  the lVrench, Gel'Ilia.Ill ~.llld Spanishgralllinar checker development of fort, we sot a goal3 This was a f law in a recent evaluation of a French gl'anlmarchecker done by tile French Academy, where agrammar-checking product was run against French lilcralure from thelast fotlr centu,ics, wilh the none too surprising result hai tsuggested changes to the great authors' prose.
\[AFP99\],I Note that noisiness i affected by faclors other than grammarchecker quality; lor instance the UI can hell) to reduceannoying flags by rcnlolnboring editing of each sentence so asllOl lo bother tlscrs with SalllO crfors ()liCe they've boonexplicitly ignored, as is done hi Microsoft Word+1039el' having less than one false flag per page.
Oncewe were well below that for each language (whilestill achieving precision and coverage goals), wesubjected the grammar checkers to beta testing (seebelow) to confirm whether the users' impressionsof the helpfulness of the grammar checker conformto the metrics.Market analysisAlthough the Natural Language Group doesn't selllhe grammar checkers as standalone products, it isstill interesting for us to determine how we fareagainst grammar checkers already on the market.Since we can't be sure that other grammarcheckers have been evaluated in exactly the sameway, we can't rely on the competitors' reportedmetrics, sttch as false flag per page rates.
Wetheret'ore do our own objective quality comparisonbased on the same bl ind corpus as we evaluateourselves against.
Here is where the strict divisionbetween blind and non-blind is absolutely essentialto avoid skewing the results - if the non-blindcorpus were used, we would show ourselves to anadvantage, since the developers also have access tothat corpus and naturally train against it.Final testing: 'real world', bug bashes, betatestingEven given a low false flag per page rate andacceptable precision and recall measures, when allis said and done the user's impression of qualityand usel'ulness can still come down to highlyspecific contexts.
Regardless of how we score onour own metrics, users will often turn a grammarchecker otT due to a 'spectacular' false flag and/orannoyance.
An example of what is meant by aspectacular false flag is Nous  sommes "~ *Noussomme,  where sommes is the correct first personphu'al present form of the French verb ~itre 'to be',while s'omme without he -s has both masculineand feminine noun readings, entailing thepossibility o1' a misparse of the verb as a noun, andtherefore a potential false flag.
A Canadian userwho encountered this false flag when using aproduct hat is now off the market immediatelyturned off that grammar checker for good.
Anerror on a common word like this can give users avery low opinion of tim grammar checker qualityand cause them turn it off for this flag alone.Regardless of what the metrics tell us as to overallquality, it still comes down to a subjective userexperience.Therefore, when the product is gettingclose to its final shippable state, we break awayfi'om the metrics to gain insigbt into how usersexperience the grammar checker quality andusefulness.
'Real world testing' refers to testpasses where the test teams use the grammarchecker to edit documents like actual users will.Rather than focusing on detailed analysis ofspecific errors, they gain a general impression ofthe product quality.Bug bashes are another type of testing,where native speaker users fiom outside our groupare asked to set aside a dedicated time to t'indingbugs in the grammar checker.
These normally takeplace over several hours.
Users may be asked toexplore the limits o1' the grammar checker, forinstance by executing certain tasks, such asproofing an existing document, changing theirsettings, etc.
The purpose is to find functional andlinguistic bugs that may have been missed by ourown extensive testing.
We also ask theparticipants to answer a few questions on theiroverall experience with the grammar checker.Finally, beta testing is simply where thegrammar checker is used in native speakers' dailydocument production environment - they are askedto use it in their daily work, submitting bugs viaemail.
Eventually they are also asked to recordtheir impressions on the usefulness of the grammarchecker, with as many specifics as possible.ConclusionDeveloping agrammar checker for a broad userbase presents many challenges, and this paperfocused oll two areas: design and evaluation.
Themultilingual project environment allows forsubstantial leveraging of knowledge, methods andprocesses; in the end, though, a grammar checker'svalue is determined by the support it provides to aspecific language community.
To this end, nativelanguage data guides the development, includingthe analysis of large corpora and intense study ofthe target market's customer and their proofingneeds.References\[AFP99\] Agence France-Presse press release, May21, 1999, "L'Acaddmie fi'an~aise met ongarde centre des logiciels de correction".1040
