Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115?2126,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPrune-and-Score: Learning for Greedy Coreference ResolutionChao Ma, Janardhan Rao Doppa?, J. Walker Orr, Prashanth MannemXiaoli Fern, Tom Dietterich and Prasad TadepalliSchool of Electrical Engineering and Computer Science, Oregon State University{machao,orr,mannemp,xfern,tgd,tadepall}@eecs.oregonstate.edu?
School of Electrical Engineering and Computer Science, Washington State Universityjana@eecs.wsu.eduAbstractWe propose a novel search-based approachfor greedy coreference resolution, wherethe mentions are processed in order andadded to previous coreference clusters.Our method is distinguished by the useof two functions to make each corefer-ence decision: a pruning function thatprunes bad coreference decisions from fur-ther consideration, and a scoring functionthat then selects the best among the re-maining decisions.
Our framework re-duces learning of these functions to ranklearning, which helps leverage powerfuloff-the-shelf rank-learners.
We show thatour Prune-and-Score approach is superiorto using a single scoring function to makeboth decisions and outperforms sever-al state-of-the-art approaches on multiplebenchmark corpora including OntoNotes.1 IntroductionCoreference resolution is the task of clustering aset of mentions in the text such that all mentions inthe same cluster refer to the same entity.
It is oneof the first stages in deep language understandingand has a big potential impact on the rest of thestages.
Several of the state-of-the-art approacheslearn a scoring function defined over mention pair,cluster-mention or cluster-cluster pair to guide thecoreference decision-making process (Daum?e II-I, 2006; Bengtson and Roth, 2008; Rahman andNg, 2011b; Stoyanov and Eisner, 2012; Chang etal., 2013; Durrett et al., 2013; Durrett and Klein,2013).
One common and persistent problem withthese approaches is that the scoring function has tomake all the coreference decisions, which leads toa highly non-realizable learning problem.Inspired by the recent success of theHC-SearchFramework (Doppa et al., 2014a) for studying avariety of structured prediction problems (Lam etal., 2013; Doppa et al., 2014c), we study a novelapproach for search-based coreference resolutioncalled Prune-and-Score.
HC-Search is a divide-and-conquer solution that learns multiple compo-nents with pre-defined roles, and each of themcontribute towards the overall goal by making therole of the other components easier.
The HC-Search framework operates in the space of com-plete outputs, and relies on the loss function whichis only defined on the complete outputs to drive it-s learning.
Unfortunately, this method does notwork for incremental coreference resolution sincethe search space for coreference resolution con-sists of partial outputs, i.e., a set of mentions onlysome of which have been clustered so far.We develop an alternative framework to HC-Search that allows us to effectively learn from par-tial output spaces and apply it to greedy corefer-ence resolution.
The key idea of our work is toaddress the problem of non-realizability of the s-coring function by learning two different function-s: 1) a pruning function to prune most of the baddecisions, and 2) a scoring function to pick thebest decision among those that are remaining.
OurPrune-and-Score approach is a particular instanti-ation of the general idea of learning nearly-soundconstraints for pruning, and leveraging the learnedconstraints to learn improved heuristic function-s for guiding the search.
The pruning constraintscan take different forms (e.g., classifiers, decision-list, or ranking functions) depending on the searcharchitecture.
Therefore, other coreference resolu-tion systems (Chang et al., 2013; Durrett and K-lein, 2013; Bj?orkelund and Kuhn, 2014) can alsobenefit from this idea.
While our basic idea of two-level selection might appear similar to the coarse-to-fine inference architectures (Felzenszwalb andMcAllester, 2007; Weiss and Taskar, 2010), thedetails differ significantly.
Importantly, our prun-ing and scoring functions operate sequentially at2115each greedy search step, whereas in the cascadesapproach, the second level function makes its pre-diction only when the first level decision-makingis done.Summary of Contributions.
The main contribu-tions of our work are as follows.
First, we moti-vate and introduce the Prune-and-Score approachto search-based coreference resolution.
Second,we identify a decomposition of the overall lossof the Prune-and-Score approach into the pruningloss and the scoring loss, and reduce the problemof learning these two functions to rank learning,which allows us to leverage powerful and efficien-t off-the-shelf rank learners.
Third, we evaluateour approach on OntoNotes, ACE, and MUC da-ta, and show that it compares favorably to sever-al state-of-the-art approaches as well as a greedysearch-based approach that uses a single scoringfunction.The remainder of the paper proceeds as follows.In Section 2, we dicuss the related work.
We intro-duce our problem setup in Section 3 and then de-scribe our Prune-and-Score approach in Section 4.We explain our approaches for learning the prun-ing and scoring functions in Section 5.
Section 6presents our experimental results followed by theconclusions in Section 7.2 Related WorkThe work on learning-based coreference resolu-tion can be broadly classified into three types.First, the pair-wise classifier approaches learn aclassifier on mention pairs (edges) (Soon et al.,2001; Ng and Cardie, 2002; Bengtson and Roth,2008), and perform some form of approximate de-coding or post-processing using the pair-wise s-cores to make predictions.
However, the pair-wiseclassifier approach suffers from several drawback-s including class imbalance (fewer positive edgescompared to negative edges) and not being able toleverage the global structure (instead making in-dependent local decisions).Second, the global approaches such as Struc-tured SVMs and Conditional Random Fields(CRFs) learn a cost function to score a potentialclustering output for a given input set of men-tions (Mccallum and Wellner, 2003; Finley andJoachims, 2005; Culotta et al., 2007; Yu andJoachims, 2009; Haghighi and Klein, 2010; Wicket al., 2011; Wick et al., 2012; Fernandes et al.,2012).
These methods address some of the prob-lems with pair-wise classifiers, however, they suf-fer from the intractability of ?Argmin?
inference(finding the least cost clustering output among ex-ponential possibilities) that is encountered duringboth training and testing.
As a result, they resort toapproximate inference algorithms (e.g., MCMC,loopy belief propagation), which can suffer fromlocal optima.Third, the incremental approaches construct theclustering output incrementally by processing thementions in some order (Daum?e III, 2006; De-nis and Baldridge, 2008; Rahman and Ng, 2011b;Stoyanov and Eisner, 2012; Chang et al., 2013;Durrett et al., 2013; Durrett and Klein, 2013).These methods learn a scoring function to guidethe decision-making process and differ in the formof the scoring function (e.g., mention pair, cluster-mention or cluster-cluster pair) and how it is beinglearned.
They have shown great success and arevery efficient.
Indeed, several of the approach-es that have achieved state-of-the-art results onOntoNotes fall under this category (Chang et al.,2013; Durrett et al., 2013; Durrett and Klein,2013; Bj?orkelund and Kuhn, 2014).
However,their efficiency requirement leads to a highly non-realizable learning problem.
Our Prune-and-Scoreapproach is complementary to these methods, aswe show that having a pruning function (or a setof learned pruning rules) makes the learning prob-lem easier and can improve over the performanceof scoring-only approaches.
Also, the models in(Chang et al., 2013; Durrett et al., 2013) try toleverage cluster-level information implicitly (vi-a latent antecedents) from mention-pair features,whereas our model explicitly leverages the clusterlevel information.Coreference resolution systems can benefitby incorporating the world knowledge includingrules, constraints, and additional information fromexternal knowledge bases (Lee et al., 2013; Rah-man and Ng, 2011a; Ratinov and Roth, 2012;Chang et al., 2013; Zheng et al., 2013; Hajishirziet al., 2013).
Our work is orthogonal to this lineof work, but domain constraints and rules can beincorporated into our model as done in (Chang etal., 2013).3 Problem SetupCoreference resolution is a structured pre-diction problem where the set of mentionsm1,m2, ?
?
?
,mDextracted from a document cor-2116reponds to a structured input x and the structuredoutput y corresponds to a partition of the men-tions into a set of clusters C1, C2, ?
?
?
, Ck.
Eachmention mibelongs to exactly one of the clustersCj.
We are provided with a training set of input-output pairs drawn from an unknown distributionD, and the goal is to return a function/predictorfrom inputs to outputs.
The learned predictoris evaluated against a non-negative loss functionL : X ?Y?Y 7?
<+, L(x, y?, y) is the loss asso-ciated with predicting incorrect output y?for inputx when the true output is y (e.g., B-Cubed Score).In this work, we formulate the coreferenceresolution problem in a search-based framework.There are three key elements in this framework:1) the Search space Spwhose states correspondto partial clustering outputs; 2) the Action prun-ing function Fprunethat is used to prune irrelevantactions at each state; and 3) the Action scoringfunction Fscorethat is used to construct a com-plete clustering output by selecting actions fromthose that are left after pruning.
Spis a 3-tuple?I, A, T ?, where I is the initial state function, Agives the set of possible actions in a given state,and T is a predicate which is true for terminal s-tates.
In our case, s0= I(x) corresponds to a s-tate where every mention is unresolved, and A(si)consists of actions to place the next mention mi+1in each cluster in sior a NEW action which createsa new cluster for it.
Terminal nodes correspond tostates with all mentions resolved.We focus on greedy search.
The decision pro-cess for constructing an output corresponds to s-electing a sequence of actions leading from theinitial state to a terminal state using both Fpruneand Fscore, which are parameterized functionsover state-action pairs (Fprune(?1(s, a)) ?
< andFscore(?2(s, a)) ?
<), where ?1and ?2stand forfeature functions.
We want to learn the parametersof both Fpruneand Fscoresuch that the predictedoutputs on unseen inputs have low expected loss.4 Greedy Prune-and-Score ApproachOur greedy Prune-and-Score approach for coref-erence resolution is parameterized by a pruningfunction Fprune: S ?
A 7?
<, a scoring func-tion Fscore: S ?
A 7?
<, and a pruning param-eter b ?
[1, Amax], where Amaxis the maximumnumber of actions at any state s ?
S .
Given aset of input mentions m1,m2, ?
?
?
,mDextractedfrom a document (input x), and a pruning param-Algorithm 1 Greedy Prune-and-Score ResolverInput: x = set of mentions m1,m2, ?
?
?
,mDfroma document D, ?I, A, T ?
= Search space defini-tion, Fprune= learned pruning function, b = prun-ing parameter, Fscore= learned scoring function1: s?
I(x) // initial state2: while not T (s) do3: A??
Top b actions from A(s) according toFprune// prune4: ap?
argmaxa?A?Fscore(s, a) // score5: s?
Apply apon s6: end while7: return coreference output corresponding to seter b, our Prune-and-Score approach makes pre-dictions as follows.
The search starts at the ini-tial state s0= I(x) (see Algorithm 1).
At eachnon-terminal state s, the pruning function Fpruneretains only the top b actions (A?)
from A(s) (Step3), and the scoring function Fscorepicks the bestscoring action ap?
A?
(Step 4) to reach the nextstate.
When a terminal state is reached its con-tents are returned as the prediction.
Figure 1 illus-trates the decision-making process of our Prune-and-Score approach for an example state.We now formalize the learning objective of ourPrune-and-Score approach.
Let y?
be the predictedcoreference output for a coreference input-outputpair (x, y?).
The expected loss of the greedyPrune-and-Score approach E(Fprune,Fscore) for agiven pruning function Fpruneand scoring func-tion Fscorecan be defined as follows.E(Fprune,Fscore) = E(x,y?
)?DL (x, y?, y?
)Our goal is to learn an optimal pair of pruningand scoring functions(Foprune,Foscore)that min-imizes the expected loss of the Prune-and-Scoreapproach.
The behavior of our Prune-and-Scoreapproach depends on the pruning parameter b,which dictates the workload of pruning and scor-ing functions.
For small values of b (aggressivepruning), pruning function learning may be harder,but scoring function learning will be easier.
Simi-larly, for large values of b (conservative pruning),scoring function learning becomes hard, but prun-ing function learning is easy.
Therefore, we wouldexpect beneficial behavior if pruning function canaggressively prune (small values of b) with littleloss in accuracy.
It is interesting to note that ourPrune-and-Score approach degenerates to existingincremental approaches that use only the scoringfunction for search (Daum?e III, 2006; Rahman and2117(a) Text with input set of mentionsRamallah ( West Bank2)110-15 ( AFP3) - Eyewitnesses4reported that Palestinians5demonstrated today Sunday in the West Bank6against the Sharm el-Sheikh7summit to beheld in Egypt8tomorrow Monday.
In Ramallah9, around 500 people10took to the town11?sstreets chanting slogans denouncing the summit ...(b) Illustration of Prune-and-Score approach1m9m 3m 4m6m2m1C1a 2a 3a 4a 5a 6a 7a5m10m 7m 11m2C 3C 4C 5C 6CState: s = {C1, C2, C3, C4, C5, C6} Actions: A(s) = {a1, a2, a3, a4, a5, a6, a7}Pruning step:Scoring step:2.5             2.2               1.9                1.5              1.4              0.7              0.44.5             3.1              2.62a 1a 7a 5a 6a 3a 4a1a 2a 7aA?
(s) = {a2, a1, a7}b = 3Decision: a1is the best action for state sFprunevaluesFscorevaluesFigure 1: Illustration of Prune-and-Score approach.
(a) Text with input set of mentions.
Mentions are highlightedand numbered.
(b) Illustration of decision-making process for mention m11.
The partial clustering output corre-sponding to the current state s consists of six clusters denoted by C1, C2, ?
?
?
, C6.
Highlighted circles correspondto the clusters.
Edges from mention m11to each of the six clusters and to itself stand for the set of possible actionsA(s) in state s, and are denoted by a1, a2, ?
?
?
, a7.
The pruning function Fprunescores all the actions in A(s) andonly keeps the top 3 actions A?= {a2, a1, a7} as specified by the pruning parameter b.
The scoring function picksthe best scoring action a1?
A?as the final decision, and mention m11is merged with cluster C1.Ng, 2011b) when b =?.
Additionally, for b = 1,our pruning function coincides with the scoringfunction.Analysis of Representational Power.
The fol-lowing proposition formalizes the intuition that t-wo functions are strictly better than one in expres-sive power.
See Appendix for the proof.Proposition 1.
Let Fpruneand Fscorebe func-tions from the same function space.
Then for alllearning problems, minFscoreE(Fscore,Fscore) ?min(Fprune,Fscore)E(Fprune,Fscore).
More-over there exist learning problems for whichminFscoreE(Fscore,Fscore) can be arbitrarilyworse than min(Fprune,Fscore)E(Fprune,Fscore).5 Learning AlgorithmsIn general, learning the optimal(Foprune,Foscore)pair can be intractable due to their potential inter-dependence.
Specifically, when learning Fprunein the worst case there can be ambiguity aboutwhich of the non-optimal actions to retain, andfor only some of those an effective Fscorecan befound.
However, we observe a loss decomposi-tion in terms of the individual losses due to Fpruneand Fscore, and develop a stage-wise learning ap-proach that first learns Fpruneand then learns acorresponding Fscore.5.1 Loss DecompositionThe overall loss of the Prune-and-Score approachE (Fprune,Fscore) can be decomposed into prun-ing loss prune, the loss due to Fprunenot be-ing able to retain the optimal terminal state inthe search space; and scoring loss score|Fprune,the additional loss due to Fscorenot guiding thegreedy search to the best terminal state after prun-ing using Fprune.
Below, we will define theselosses more formally.Pruning Loss is defined as the expected loss ofthe Prune-and-Score approach when we performgreedy search with Fpruneand F?score, the opti-mal scoring function.
A scoring function is said tobe optimal if at every state s in the search space2118Sp, and for any set of remaining actions A(s), itcan score each action a ?
A(s) such that greedysearch can reach the best terminal state (as eval-uated by task loss function L) that is reachablefrom s through A(s).
Unfortunately, computingthe optimal scoring function is highly intractablefor the non-decomposable loss functions that areemployed in coreference resolution (e.g., B-CubedF1).
The main difficulty is that the decision at anyone state has interdependencies with future deci-sions (see Section 5.5 in (Daum?e III, 2006) formore details).
So we need to resort to some formof approximate optimal scoring function that ex-hibits the intended behavior.
This is very similarto the dynamic oracle concept developed for de-pendency parsing (Goldberg and Nivre, 2013).Let y?prunebe the coreference output corre-sponding to the terminal state reached from inputx by Prune-and-Score approach when performingsearch using Fpruneand F?score.
Then the pruningloss can be expressed as follows.prune= E(x,y?
)?DL(x, y?prune, y?
)Scoring Loss is defined as the additional loss dueto Fscorenot guiding the greedy search to the bestterminal state reachable via the pruning functionFscore(i.e., y?prune).
Let y?
be the coreference out-put corresponding to the terminal state reached byPrune-and-Score approach by performing searchwith Fpruneand Fscorefor an input x.
Then thescoring loss can be expressed as follows:score|Fprune= E(x,y?
)?DL (x, y?, y?)?
L(x, y?prune, y?
)The overall loss decomposition of our Prune-and-Score approach can be expressed as follows.E (Fprune,Fscore)= E(x,y?
)?DL(x, y?prune, y?)?
??
?prune+E(x,y?
)?DL (x, y?, y?)?
L(x, y?prune, y?)?
??
?score|Fprune5.2 Stage-wise LearningThe loss decomposition motivates a learning ap-proach that targets minimizing the errors of prun-ing and scoring functions independently.
In par-ticular, we optimize the overall loss of the Prune-and-Score approach in a stage-wise manner.
Wefirst train a pruning function?Fpruneto optimizethe pruning loss component pruneand then traina scoring function?Fscoreto optimize the scoringloss score|?Fpruneconditioned on?Fprune.?Fprune?
argminFprune?Fpprune?Fscore?
argminFscore?Fsscore|?FpruneNote that this approach is myopic in the sense that?Fpruneis learned without considering the impli-cations for learning?Fscore.
Below, we first de-scribe our approach for pruning function learning,and then explain our scoring function learning al-gorithm.5.3 Pruning Function LearningIn our greedy Prune-and-Score approach, the roleof the pruning function Fpruneis to prune awayirrelevant actions (as specified by the pruning pa-rameter b) at each search step.
More specifically,we want Fpruneto score actions A(s) at each s-tate s such that the optimal action a??
A(s) isranked within the top b actions to minimize prune.For this, we assume that for any training input-output pair (x, y?)
there exists a unique action se-quence, or solution path (initial state to terminalstate), for producing y?from x.
More formally, let(s?0, a?0), (s?1, a?1), ?
?
?
, (s?D,?)
correspond to thesequence of state-action pairs along this solutionpath, where s?0is the initial state and s?Dis the ter-minal state.
The goal is to learn the parameters ofFprunesuch that at each state s?i, a?i?
A(s?i) isranked among the top b actions.While we can employ an online-LaSO style ap-proach (III and Marcu, 2005; Xu et al., 2009) tolearn the parameters of the pruning function, it isquite inefficient, as it must regenerate the samesearch trajectory again and again until it learn-s to make the right decision.
Additionally, thisapproach limits applicability of the off-the-shelflearners to learn the parameters of Fprune.
Toovercome these drawbacks, we apply offline train-ing.Reduction to Rank Learning.
We reduce thepruning function learning to a rank learning prob-lem.
This allows us to leverage powerful and effi-cient off-the-shelf rank-learners (Liu, 2009).
Thereduction is as follows.
At each state s?ion the so-lution path of a training example (x, y?
), we createan example by labeling optimal action a?i?
A(s?i)as the only relevant action, and then try to learn2119a ranking function that can rank actions such thatthe relevant action a?iis in the top b actions, whereb is the input pruning paramter.
In other word-s, we have a rank learning problem, where thelearner?s goal is to optimize the Precision at Top-b.
The training approach creates such an exam-ple for each state s in the solution path.
The setof aggregate imitation examples collected over al-l the training data is then given to a rank learner(e.g., LambdaMART (Burges, 2010)) to learn theparameters of Fpruneby optimizing the Precisionat Top-b loss.
See appendix for the pseudocode.If we can learn a function Fprunethat is con-sistent with these imitation examples, then thelearned pruning function is guaranteed to keepthe solution path within the pruned space for al-l the training examples.
We can also employmore advanced imitation learning algorithms in-cluding DAgger (Ross et al., 2011) and SEARN(Hal Daum?e III et al., 2009) if we are provid-ed with an (approximate) optimal scoring functionF?scorethat can pick optimal actions at states thatare not in the solution path (i.e., off-trajectory s-tates).5.4 Scoring Function LearningGiven a learned pruning function Fprune, we wantto learn a scoring function that can pick the bestaction from the b actions that remain after prun-ing at each state.
We formulate this problem in theframework of imitation learning (Khardon, 1999).More formally, let (s?0, a?0), (s?1, a?1), ?
?
?
, (s??D,?
)correspond to the sequence of state-action pairsalong the greedy trajectory obtained by runningthe Prune-and-Score approach with FpruneandF?score, the optimal scoring function, on a train-ing example (x, y?
), where s?
?Dis the best terminalstate in the pruned space.
The goal of our imita-tion training approach is to learn the parametersof Fscoresuch that at each state s?i, a?i?
A?isranked higher than all other actions in A?, whereA??
A(s?i) is the set of b actions that remain afterpruning.It is important to note that the distribution ofstates in the pruned space due to Fpruneon thetesting data may be somewhat different from thoseon training data.
Therefore, we train our scoringfunction via cross-validation by training the scor-ing function on heldout data that was not used totrain the pruning function.
This methodology iscommonly employed in Re-Ranking and Stackingapproaches (Collins, 2000; Cohen and de Carval-ho, 2005).Our scoring function learning procedure usescross validation and consists of the following foursteps.
First, we divide the training data D in-to k folds.
Second, we learn k different pruners,where each pruning function Fipruneis learned us-ing the data from all the folds excluding the ithfold.
Third, we generate ranking examples forscoring function learning as described above us-ing each pruning function Fipruneon the data itwas not trained on.
Finally, we give the aggregateset of ranking examples R to a rank learner (e.g.,SVM-Rank or LambdaMART) to learn the scoringfunction Fscore.
See appendix for the pseudocode.Approximate Optimal Scoring Function.
If thelearned pruning function is not consistent with thetraining data, we will encounter states s?ithat arenot on the target path, and we will need some su-pervision for learning in those cases.
As discussedbefore in Section 5.1, computing an optimal scor-ing functionF?scoreis intractable for combinatorialloss functions that are used for coreference resolu-tion.
So we employ an approximate function fromexisting work that is amenable to evaluate partialoutputs (Daum?e III, 2006).
It is a variant of theACE scoring function that removes the bipartitematching step from the ACE metric.
Moreoverthis score is computed only on the partial coref-erence output corresponding to the ?after state?s?resulting from taking action a in state s, i.e.,F?score(s, a) = F?score(s?).
To further simplify thecomputation, we give uniform weight to the threetypes of costs: 1) Credit for correct linking, 2)Penalty for incorrect linking, and 3) Penalty formissing links.
Intuitively, this is similar to thecorrect-link count computed only on a subgraph.We direct the reader to (Daum?e III, 2006) for moredetails (see Section 5.5).6 Experiments and ResultsIn this section, we evaluate our greedy Prune-and-Score approach on three benchmark corpora?
OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004(NIST, 2004), and MUC6 (MUC6, 1995) ?
andcompare it against the state-of-the-art approachesfor coreference resolution.
For OntoNotes data,we report the results on both gold mentions andpredicted mentions.
We also report the results ongold mentions for ACE 2004 and MUC6 data.21206.1 Experimental SetupDatasets.
For OntoNotes corpus, we employ theofficial split for training, validation, and testing.There are 2802 documents in the training set; 343documents in the validation set; and 345 docu-ments in the testing set.
The ACE 2004 corpuscontains 443 documents.
We follow the (Culot-ta et al., 2007; Bengtson and Roth, 2008) splitin our experiments by employing 268 documentsfor training, 68 documents for validation, and 107documents (ACE2004-CULOTTA-TEST) for test-ing.
We also evaluate our system on the 128newswire documents in ACE 2004 corpus for afair comparison with the state-of-the-art.
TheMUC6 corpus containts 255 documents.
We em-ploy the official test set of 30 documents (MUC6-TEST) for testing purposes.
From the remaining225 documents, which includes 195 official train-ing documents and 30 dry-run test documents, werandomly pick 30 documents for validation, anduse the remaining ones for training.Evaluation Metrics.
We compute three most pop-ular performance metrics for coreference resolu-tion: MUC (Vilain et al., 1995), B-Cubed (Bag-ga and Baldwin, 1998), and Entity-based CEAF(CEAF?4) (Luo, 2005).
As it is commonly donein CoNLL shared tasks (Pradhan et al., 2012), weemploy the average F1 score (CoNLL F1) of thesethree metrics for comparison purposes.
We evalu-ate all the results using the updated version1(7.0)of the coreference scorer.Features.
We built2our coreference resolverbased on the Easy-first coreference system (Stoy-anov and Eisner, 2012), which is derived from theReconcile system (Stoyanov et al., 2010).
We es-sentially employ the same features as in the Easy-first system.
However, we provide some high-level details that are necessary for subsequent dis-cussion.
Recall that our features ?
(s, a) for bothpruning and scoring functions are defined overstate-action pairs, where each state s consists ofa set of clusters and an action a corresponds tomerging an unprocessed mention m with a clus-ter C in state s or create one for itself.
Therefore,?
(s, a) defines features over cluster-mention pairs(C,m).
Our feature vector consists of three part-s: a) mention pair features; b) entity pair features;and c) a single indicator feature to represent NEW1http://code.google.com/p/reference-coreference-scorers/2See http://research.engr.oregonstate.edu/dral/ for oursoftware.action (i.e., mention m starts its own cluster).
Formention pair features, we average the pair-wisefeatures over all links between m and every men-tion mcin cluster C (often referred to as average-link).
Note that, we cannot employ the best-linkfeature representation because we perform offlinetraining and do not have weights for scoring thelinks.
For entity pair features, we treat mentionm as a singleton entity and compute features bypairing it with the entity represented by cluster C(exactly as in the Easy-first system).
The indica-tor feature will be 1 for the NEW action and 0 forall other actions.We have a total of 140 features:90 mention pair features; 49 entity pair features;and one NEW indicator feature.
We believe thatour approach can benefit from employing featuresof the mention for the NEW action (Rahman andNg, 2011b; Durrett and Klein, 2013).
However,we were constrained by the Reconcile system andcould not leverage these features for the NEW ac-tion.Base Rank-Learner.
Our pruning and scoringfunction learning algorithms need a base rank-learner.
We employ LambdaMART (Burges,2010), a state-of-the art rank learner from theRankLib3library.
LambdaMART is a variant ofboosted regression trees.
We use a learning rateof 0.1, specify the maximum number of boost-ing iterations (or trees) as 1000 noting that its ac-tual value is automatically decided based on thevalidation set, and tune the number of leaves pertree based on the validation data.
Once we fixthe hyper-parameters of LambdaMART, we trainthe final model on all of the training data.
Lamb-daMART uses an internal train/validation split ofthe input ranking examples to decide when to stopthe boosting iterations.
We fixed this ratio to 0.8noting that the performance is not sensitive to thisparameter.
For scoring function learning, we used5 folds for the cross-validation training.Pruning Parameter b.
The hyper-parameter bcontrols the amount of pruning in our Prune-and-Score approach.
We perform experiments with d-ifferent values of b and pick the best value basedon the performance on the validation set.Singleton Mention Filter for OntoNotes Cor-pus.
We employ the Illinois-Coref system (Changet al., 2012) to extract system mentions for ourOntoNotes experiments, and observe that the num-3http://sourceforge.net/p/lemur/wiki/RankLib/2121ber of predicted mentions is thrice the number ofgold mentions.
Since the training data provides theclustering supervision for only gold mentions, it isnot clear how to train with the system mention-s that are not part of gold mentions.
A commonway of dealing with this problem is to treat all theextra system mentions as singleton clusters (Dur-rett and Klein, 2013; Chang et al., 2013).
Howev-er, this solution most likely will not work with ourcurrent feature representation (i.e., NEW action isrepresented as a single indicator feature).
Recallthat to predict these extra system mentions as s-ingleton clusters with our incremental clusteringapproach, the learned model should first predic-t a NEW action while processing these mention-s to form a temporary singleton cluster, and thenrefrain from merging any of the subsequent men-tions with that cluster so that it becomes a single-ton cluster in the final clustering output.
Howev-er, in OntoNotes corpus, the training data does notinclude singleton clusters for the gold mentions.Therefore, only the large number (57%) of systemmentions that are not part of gold mentions willconstitute the set of singleton clusters.
This leadsto a highly imbalanced learning problem becauseour model needs to learn (the weight of the sin-gle indicator feature) to predict NEW as the bestaction for a large set of mentions, which will biasour model to predict large number of NEW actionsduring testing.
As a result, we will generate manysingleton clusters, which will hurt the recall of themention detection after post-processing.
There-fore, we aim to learn a singleton mention filterthat will be used as a pre-processor before trainingand testing to overcome this problem.
We wouldlike to point out that our filter is complementary toother solutions (e.g., employing features that candiscriminate a given mention to be anaphoric ornot in place of our single indicator feature, or us-ing a customized loss to weight our ranking exam-ples for cost-sensitive training)(Durrett and Klein,2013).Filter Learning.
The singleton mention filter isa classifier that will label a given mention as ?s-ingleton?
or not.
We represent each mention min a document by averaging the mention-pair fea-tures ?(m,m?)
of the k-most similar mentions(obtained by ranking all other mentions m?in thedocument with a learned ranking functionR givenm) and then learn a decision-tree classifier by opti-mizing the F1 loss.
We learn the mention-rankingfunction R by optimizing the recall of positivepairs for a given k, and employ LambdaMART asour base ranker.
The hyper-parameters are tunedbased on the performance on the validation set.6.2 ResultsWe first describe the results of the learned single-ton mention filter, and then the performance ofour Prune-and-Score approach with and withoutthe filter.
Next, we compare the results of our ap-proach with several state-of-the-art approaches forcoreference resolution.Singleton Mention Filter Results.
Table 1 showsthe performance of the learned singleton mentionfilter with k = 2 noting that the results are ro-bust for all values of k ?
2.
As we can see, thelearned filter improves the precision of the men-tion detection with only small loss in the recall ofgold mentions.Mention Detection AccuracyP R F1Before- 43.18% 86.99% 57.71%filtering (16664/38596) (16664/19156)After- 79.02% 80.98% 79.97%filtering (15516/19640) (15516/19156)Table 1: Performance of the singleton mention filter onthe OntoNotes 5.0 development set.
The numerators ofthe fractions in the brackets show the exact numbers ofmentions that are matched with the gold mentions.Prune-and-Score Results.
Table 2 shows the per-formance of Prune-and-Score approach with andwithout the singleton mention filter.
We can seethat the results with filter are much better than thecorresponding results without the filter.
These re-sults show that our approach can benefit from hav-ing a good singleton mention filter.Filter settings MUC B3CEAF?4CoNLLOntoNotes 5.0 Dev Set w. Predict Ment.O.S.
(w.o.
Filter) 66.73 53.40 44.23 54.79P&S (w.o.
Filter) 65.93 52.96 50.24 56.38P&S (w. Filter) 71.18 58.87 57.88 62.64Table 2: Performance of Prune-and-Score approachwith and without the singleton mention filter, and Only-Score approach without the filter.Table 3 shows the performance of different con-figurations of our Prune-and-Score approach.
Aswe can see, Prune-and-Score gives better resultsthan the configuration where we employ only thescoring function (b = ?)
for small values of b.2122MUC B3CEAF?4CoNLLP R F1 P R F1 P R F1 Avg-F1a.
Results on OntoNotes 5.0 Test Set with Predicted MentionsPrune-and-Score 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56Only-Scoring 75.95 61.53 67.98 63.94 47.37 54.42 58.54 49.76 53.79 58.73HOTCoref 67.46 74.3 70.72 54.96 62.71 58.58 52.27 59.4 55.61 61.63CPL3M - - 69.48 - - 57.44 - - 53.07 60.00Berkeley 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41Fernandes et al., 2012 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65Stanford 65.31 64.11 64.71 56.54 48.58 52.26 46.67 52.29 49.32 55.43b.
Results on OntoNotes 5.0 Test Set with Gold MentionsPrune-and-Score 88.10 85.85 86.96 76.82 76.16 76.49 80.90 74.06 77.33 80.26Only-Scoring 86.96 84.52 85.73 74.51 74.25 74.38 79.04 70.67 74.62 78.24CPL3M - - 84.80 - - 78.74 - - 68.75 77.43Berkeley 85.73 89.26 87.46 78.23 75.11 76.63 82.89 70.86 76.40 80.16Stanford 89.94 78.17 83.64 81.75 68.95 74.81 73.97 61.20 66.98 75.14c.
Results on ACE2004 Culotta Test Set with Gold MentionsPrune-and-Score 85.57 72.68 78.60 90.09 77.02 83.04 74.64 86.02 79.42 80.35Only-Scoring 82.75 69.25 75.40 88.54 74.22 80.75 73.69 85.22 78.58 78.24CPL3M - - 78.29 - - 82.20 - - 79.26 79.91Stanford 82.91 69.90 75.85 89.14 74.05 80.90 75.67 77.45 76.55 77.77d.
Results on ACE2004 Newswire with Gold MentionsPrune-and-Score 89.72 75.72 82.13 90.89 76.15 82.87 72.43 86.83 78.69 81.23Only-Scoring 86.92 76.49 81.37 88.10 75.83 81.51 73.15 84.31 78.05 80.31Easy-first - - 80.1 - - 81.8 - - - -Stanford 84.75 75.34 79.77 87.50 74.59 80.53 73.32 81.49 77.19 79.16e.
Results on MUC6 Test Set with Gold MentionsPrune-and-Score 89.53 82.75 86.01 86.48 76.18 81.00 60.74 80.33 68.68 78.56Only-Scoring 86.77 80.96 83.76 81.72 72.99 77.11 57.56 75.38 64.91 75.26Easy-first - - 88.2 - - 77.5 - - - -Stanford 91.19 69.54 78.91 91.07 63.39 74.75 62.43 69.62 65.83 73.16Table 4: Comparison of Prune-and-Score with state-of-the-art approaches.
Metric values reflect version 7 ofCoNLL scorer.The performance is clearly better than the degen-erate case (b = ?)
over a wide range of b values,suggesting that it is not necessary to carefully tunethe parameter b.Pruning param.
b MUC B3CEAF?4CoNLLOntoNotes 5.0 Dev Set w. Predict Ment.2 69.12 56.80 56.30 60.743 70.50 57.89 57.24 61.884 71.00 58.65 57.41 62.355 71.18 58.87 57.88 62.646 70.93 58.66 57.85 62.488 70.12 58.13 57.37 61.8710 70.24 58.34 56.27 61.6120 67.97 57.73 56.63 60.78?
67.03 56.31 55.56 59.63Table 3: Performance of Prune-and-Score approachwith different values of the pruning parameter b. Forb =?, Prune-and-Score becomes an Only-Scoring al-gorithm.Comparison to State-of-the-Art.
Table 4shows the results of our Prune-and-Score ap-proach compared with the following state-of-the-art coreference resolution approaches: HOTCorefsystem (Bj?orkelund and Kuhn, 2014); Berkeleysystem with the FINAL feature set (Durrett and K-lein, 2013); CPL3M system (Chang et al., 2013);Stanford system (Lee et al., 2013); Easy-first sys-tem (Stoyanov and Eisner, 2012); and Fernan-des et al., 2012 (Fernandes et al., 2012).
On-ly Scoring is the special case of our Prune-and-Score approach where we employ only the scoringfunction.
This corresponds to existing incremen-tal approaches (Daum?e III, 2006; Rahman and Ng,2011b).
We report the best published results forCPL3M system, Easy-first, and Fernandes et al.,2012.
We ran the publicly available software togenerate the results for Berkeley and Stanford sys-tems with the updated CoNLL scorer.
We includethe results of Prune-and-Score for best b on the de-velopment set with singleton mention filter for thecomparison.
In Table 4, ?-?
indicates that we couldnot find published results for those cases.
We see2123that results of the Prune-and-Score approach arecomparable to or better than the state-of-the-art in-cluding Only-Scoring.7 Conclusions and Future WorkWe introduced the Prune-and-Score approach forgreedy coreference resolution whose main ideais to learn a pruning function along with a scor-ing function to effectively guide the search.
Weshowed that our approach improves over the meth-ods that only learn a scoring function, and givescomparable or better results than several state-of-the-art coreference resolution systems.Our Prune-and-Score approach is a particularinstantiation of the general idea of learning nearly-sound constraints for pruning, and leveraging thelearned constraints to learn improved heuristicfunctions for guiding the search (See (Chen etal., 2014) for another instantiation of this idea formulti-object tracking in videos).
Therefore, oth-er coreference resolution systems (Chang et al.,2013; Durrett and Klein, 2013; Bj?orkelund andKuhn, 2014) can also benefit from this idea.
Oneway to further improve the peformance of ourapproach is to perform a search in the LimitedDiscrepancy Search (LDS) space (Doppa et al.,2014b) using the learned functions.Future work should apply this general idea toother natural language processing tasks includingdependency parsing (Nivre et al., 2007) and in-formation extraction (Li et al., 2013).
We wouldexpect more beneficial behavior with the prun-ing constraints for problems with large action sets(e.g., labeled dependency parsing).
It would be in-teresting and useful to generalize this approach tosearch spaces where there are multiple target pathsfrom the initial state to the terminal state, e.g., asin the Easy-first framework.AcknowledgmentsAuthors would like to thank Veselin Stoyanov(JHU) for answering several questions related tothe Easy-first and Reconcile systems; Van Dang(UMass, Amherst) for technical discussions relat-ed to the RankLib library; Kai-Wei Chang (UIUC)for the help related to the Illinois-Coref mentionextractor; and Greg Durrett (UC Berkeley) for hishelp with the Berkeley system.
This work wassupported in part by NSF grants IIS 1219258, I-IS 1018490 and in part by the Defense AdvancedResearch Projects Agency (DARPA) and the AirForce Research Laboratory (AFRL) under Con-tract No.
FA8750-13-2-0033.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the author(s)and do not necessarily reflect the views of the NS-F, the DARPA, the Air Force Research Laboratory(AFRL), or the US government.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing (EMNLP), pages 294?303.Anders Bj?orkelund and Jonas Kuhn.
2014.
Learn-ing structured perceptrons for coreference resolutionwith latent antecedents and non-local features.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 47?57, Baltimore, Maryland,June.
Association for Computational Linguistics.Christopher Burges.
2010.
From RankNet to Lamb-daRank to LambdaMART: An overview.
MicrosoftTechnical Report, (MSR-TR-2010).Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,Mark Sammons, and Dan Roth.
2012.
Illinois-Coref: The UI system in the CoNLL-2012 sharedtask.
In Joint Conference on EMNLP and CoNLL- Shared Task, pages 113?117, Jeju Island, Korea,July.
Association for Computational Linguistics.Kai-Wei Chang, Rajhans Samdani, and Dan Roth.2013.
A constrained latent variable model forcoreference resolution.
In Proceedings of Em-pirical Methods in Natural Language Processing(EMNLP), pages 601?612.Sheng Chen, Alan Fern, and Sinisa Todorovic.
2014.Multi-object tracking via constrained sequential la-beling.
In To appear in Proceedings of IEEE Con-ference on Computer Vision and Pattern Recognition(CVPR).William W. Cohen and Vitor Rocha de Carvalho.
2005.Stacked sequential learning.
In Proceedings of In-ternational Joint Conference on Artificial Intelli-gence (IJCAI), pages 671?676.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of Inter-national Conference on Machine Learning (ICML),pages 175?182.2124Aron Culotta, Michael L. Wick, and Andrew Mc-Callum.
2007.
First-order probabilistic modelsfor coreference resolution.
In Proceedings of Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics (HLT-NAACL), pages 81?88.Hal Daum?e III.
2006.
Practical Structured LearningTechniques for Natural Language Processing.
Ph.D.thesis, University of Southern California, Los Ange-les, CA.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
InProceedings of Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 660?669.Janardhan Rao Doppa, Alan Fern, and Prasad Tadepal-li.
2014a.
HC-Search: A learning framework forsearch-based structured prediction.
Journal of Arti-ficial Intelligence Research (JAIR), 50:369?407.Janardhan Rao Doppa, Alan Fern, and Prasad Tade-palli.
2014b.
Structured prediction via output s-pace search.
Journal of Machine Learning Research(JMLR), 15:1317?1350.Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern,and Prasad Tadepalli.
2014c.
HC-Search for multi-label prediction: An empirical study.
In Proceed-ings of AAAI Conference on Artificial Intelligence(AAAI).Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1971?1982.Greg Durrett, David Leo Wright Hall, and Dan Klein.2013.
Decentralized entity-level modeling for coref-erence resolution.
In Proceedings of Association ofComputational Linguistics (ACL) Conference, pages114?124.Pedro F. Felzenszwalb and David A. McAllester.
2007.The generalized A* architecture.
Journal of Artifi-cial Intelligence Research (JAIR), 29:153?190.Eraldo Rezende Fernandes, C?
?cero Nogueira dos San-tos, and Ruy Luiz Milidi?u.
2012.
Latent structureperceptron with feature induction for unrestrictedcoreference resolution.
International Conference onComputational Natural Language Learning (CoNL-L), pages 41?48.Thomas Finley and Thorsten Joachims.
2005.
Su-pervised clustering with support vector machines.In Proceedings of International Conference on Ma-chine Learning (ICML), pages 217?224.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the Association for ComputationalLinguistics, 1:403?414.Aria Haghighi and Dan Klein.
2010.
Coreference res-olution in a modular, entity-centered model.
In Pro-ceedings of Human Language Technology Confer-ence of the North American Chapter of the Associa-tion of Computational Linguistics (HLT-NAACL).Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld,and Luke S. Zettlemoyer.
2013.
Joint corefer-ence resolution and named-entity linking with multi-pass sieves.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 289?299.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning Journal (MLJ), 75(3):297?325.Hal Daum?e III and Daniel Marcu.
2005.
Learningas search optimization: Approximate large marginmethods for structured prediction.
In ICML.Roni Khardon.
1999.
Learning to take actions.
Ma-chine Learning Journal (MLJ), 35(1):57?90.Michael Lam, Janardhan Rao Doppa, Xu Hu, SinisaTodorovic, Thomas Dietterich, Abigail Reft, andMarymegan Daly.
2013.
Learning to detect basaltubules of nematocysts in sem images.
In ICCVWorkshop on Computer Vision for Accelerated Bio-sciences (CVAB).
IEEE.Heeyoung Lee, Angel X. Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4):885?916.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (A-CL), pages 73?82.Tie-Yan Liu.
2009.
Learning to rank for informationretrieval.
Foundations and Trends in InformationRetrieval, 3(3):225?331.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of the Confer-ence on Human Language Technology and Empir-ical Methods in Natural Language Processing, HLT?05, pages 25?32, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Andrew Mccallum and Ben Wellner.
2003.
To-ward conditional models of identity uncertainty withapplication to proper noun coreference.
In Pro-ceedings of Neural Information Processing Systems(NIPS), pages 905?912.
MIT Press.MUC6.
1995.
Coreference task definition.
In Pro-ceedings of the Sixth Message Understanding Con-ference (MUC-6), pages 335?344.2125Vincent Ng and Claire Cardie.
2002.
Improvingmachine learning approaches to coreference resolu-tion.
In Proceedings of Association of Computation-al Linguistics (ACL) Conference, pages 104?111.NIST.
2004.
The ACE evaluation plan.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 shared task: Modeling multilingual unrestrict-ed coreference in ontonotes.
In Proceedings of theJoint Conference on EMNLP and CoNLL: SharedTask, pages 1?40.Altaf Rahman and Vincent Ng.
2011a.
Coreferenceresolution with world knowledge.
In Proceedingsof Association of Computational Linguistics (ACL)Conference, pages 814?824.Altaf Rahman and Vincent Ng.
2011b.
Narrowing themodeling gap: A cluster-ranking approach to coref-erence resolution.
Journal of Artificial IntelligenceResearch (JAIR), 40:469?521.Lev-Arie Ratinov and Dan Roth.
2012.
Learning-based multi-sieve co-reference resolution withknowledge.
In Proceedings of Empirical Methodsin Natural Language Processing (EMNLP) Confer-ence, pages 1234?1244.St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.2011.
A reduction of imitation learning and struc-tured prediction to no-regret online learning.
Jour-nal of Machine Learning Research - ProceedingsTrack, 15:627?635.Wee Meng Soon, Daniel Chung, Daniel Chung YongLim, Yong Lim, and Hwee Tou Ng.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.Veselin Stoyanov and Jason Eisner.
2012.
Easy-firstcoreference resolution.
In Proceedings of Inter-national Conference on Computational Linguistics(COLING), pages 2519?2534.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.Coreference resolution with reconcile.
In Proceed-ings of Association of Computational Linguistics (A-CL) Conference, pages 156?161.Marc B. Vilain, John D. Burger, John S. Aberdeen,Dennis Connolly, and Lynette Hirschman.
1995.A model-theoretic coreference scoring scheme.
InMUC, pages 45?52.David Weiss and Benjamin Taskar.
2010.
Structuredprediction cascades.
Journal of Machine LearningResearch - Proceedings Track, 9:916?923.Michael L. Wick, Khashayar Rohanimanesh, KedarBellare, Aron Culotta, and Andrew McCallum.2011.
SampleRank: Training factor graphs withatomic gradients.
In Proceedings of InternationalConference on Machine Learning (ICML).Michael L. Wick, Sameer Singh, and Andrew McCal-lum.
2012.
A discriminative hierarchical model forfast coreference at large scale.
In Proceedings of As-sociation of Computational Linguistics (ACL) Con-ference, pages 379?388.Yuehua Xu, Alan Fern, and Sung Wook Yoon.
2009.Learning linear ranking functions for beam searchwith application to planning.
Journal of MachineLearning Research (JMLR), 10:1571?1610.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.
InProceedings of International Conference on Ma-chine Learning (ICML).Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.Choi, and Andrew McCallum.
2013.
Dynamicknowledge-base alignment for coreference resolu-tion.
In Conference on Computational Natural Lan-guage Learning (CoNLL).2126
