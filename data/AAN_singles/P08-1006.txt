Proceedings of ACL-08: HLT, pages 46?54,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsTask-oriented Evaluation of Syntactic Parsers and Their RepresentationsYusuke Miyao?
Rune S?tre?
Kenji Sagae?
Takuya Matsuzaki?
Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan?School of Computer Science, University of Manchester, UK?National Center for Text Mining, UK{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper presents a comparative evalua-tion of several state-of-the-art English parsersbased on different frameworks.
Our approachis to measure the impact of each parser when itis used as a component of an information ex-traction system that performs protein-proteininteraction (PPI) identification in biomedicalpapers.
We evaluate eight parsers (based ondependency parsing, phrase structure parsing,or deep parsing) using five different parse rep-resentations.
We run a PPI system with severalcombinations of parser and parse representa-tion, and examine their impact on PPI identi-fication accuracy.
Our experiments show thatthe levels of accuracy obtained with these dif-ferent parsers are similar, but that accuracyimprovements vary when the parsers are re-trained with domain-specific data.1 IntroductionParsing technologies have improved considerably inthe past few years, and high-performance syntacticparsers are no longer limited to PCFG-based frame-works (Charniak, 2000; Klein and Manning, 2003;Charniak and Johnson, 2005; Petrov and Klein,2007), but also include dependency parsers (Mc-Donald and Pereira, 2006; Nivre and Nilsson, 2005;Sagae and Tsujii, 2007) and deep parsers (Kaplanet al, 2004; Clark and Curran, 2004; Miyao andTsujii, 2008).
However, efforts to perform extensivecomparisons of syntactic parsers based on differentframeworks have been limited.
The most popularmethod for parser comparison involves the directmeasurement of the parser output accuracy in termsof metrics such as bracketing precision and recall, ordependency accuracy.
This assumes the existence ofa gold-standard test corpus, such as the Penn Tree-bank (Marcus et al, 1994).
It is difficult to applythis method to compare parsers based on differentframeworks, because parse representations are oftenframework-specific and differ from parser to parser(Ringger et al, 2004).
The lack of such comparisonsis a serious obstacle for NLP researchers in choosingan appropriate parser for their purposes.In this paper, we present a comparative evalua-tion of syntactic parsers and their output represen-tations based on different frameworks: dependencyparsing, phrase structure parsing, and deep pars-ing.
Our approach to parser evaluation is to mea-sure accuracy improvement in the task of identify-ing protein-protein interaction (PPI) information inbiomedical papers, by incorporating the output ofdifferent parsers as statistical features in a machinelearning classifier (Yakushiji et al, 2005; Katrenkoand Adriaans, 2006; Erkan et al, 2007; S?tre et al,2007).
PPI identification is a reasonable task forparser evaluation, because it is a typical informationextraction (IE) application, and because recent stud-ies have shown the effectiveness of syntactic parsingin this task.
Since our evaluation method is applica-ble to any parser output, and is grounded in a realapplication, it allows for a fair comparison of syn-tactic parsers based on different frameworks.Parser evaluation in PPI extraction also illu-minates domain portability.
Most state-of-the-artparsers for English were trained with the Wall StreetJournal (WSJ) portion of the Penn Treebank, andhigh accuracy has been reported for WSJ text; how-ever, these parsers rely on lexical information to at-tain high accuracy, and it has been criticized thatthese parsers may overfit to WSJ text (Gildea, 2001;46Klein and Manning, 2003).
Another issue for dis-cussion is the portability of training methods.
Whentraining data in the target domain is available, asis the case with the GENIA Treebank (Kim et al,2003) for biomedical papers, a parser can be re-trained to adapt to the target domain, and larger ac-curacy improvements are expected, if the trainingmethod is sufficiently general.
We will examinethese two aspects of domain portability by compar-ing the original parsers with the retrained parsers.2 Syntactic Parsers and TheirRepresentationsThis paper focuses on eight representative parsersthat are classified into three parsing frameworks:dependency parsing, phrase structure parsing, anddeep parsing.
In general, our evaluation methodol-ogy can be applied to English parsers based on anyframework; however, in this paper, we chose parsersthat were originally developed and trained with thePenn Treebank or its variants, since such parsers canbe re-trained with GENIA, thus allowing for us toinvestigate the effect of domain adaptation.2.1 Dependency parsingBecause the shared tasks of CoNLL-2006 andCoNLL-2007 focused on data-driven dependencyparsing, it has recently been extensively studied inparsing research.
The aim of dependency pars-ing is to compute a tree structure of a sentencewhere nodes are words, and edges represent the re-lations among words.
Figure 1 shows a dependencytree for the sentence ?IL-8 recognizes and activatesCXCR1.?
An advantage of dependency parsing isthat dependency trees are a reasonable approxima-tion of the semantics of sentences, and are readilyusable in NLP applications.
Furthermore, the effi-ciency of popular approaches to dependency pars-ing compare favorable with those of phrase struc-ture parsing or deep parsing.
While a number of ap-proaches have been proposed for dependency pars-ing, this paper focuses on two typical methods.MST McDonald and Pereira (2006)?s dependencyparser,1 based on the Eisner algorithm for projectivedependency parsing (Eisner, 1996) with the second-order factorization.1http://sourceforge.net/projects/mstparserFigure 1: CoNLL-X dependency treeFigure 2: Penn Treebank-style phrase structure treeKSDEP Sagae and Tsujii (2007)?s dependencyparser,2 based on a probabilistic shift-reduce al-gorithm extended by the pseudo-projective parsingtechnique (Nivre and Nilsson, 2005).2.2 Phrase structure parsingOwing largely to the Penn Treebank, the mainstreamof data-driven parsing research has been dedicatedto the phrase structure parsing.
These parsers outputPenn Treebank-style phrase structure trees, althoughfunction tags and empty categories are stripped off(Figure 2).
While most of the state-of-the-art parsersare based on probabilistic CFGs, the parameteriza-tion of the probabilistic model of each parser varies.In this work, we chose the following four parsers.NO-RERANK Charniak (2000)?s parser, based on alexicalized PCFG model of phrase structure trees.3The probabilities of CFG rules are parameterized oncarefully hand-tuned extensive information such aslexical heads and symbols of ancestor/sibling nodes.RERANK Charniak and Johnson (2005)?s rerank-ing parser.
The reranker of this parser receives n-best4 parse results from NO-RERANK, and selectsthe most likely result by using a maximum entropymodel with manually engineered features.BERKELEY Berkeley?s parser (Petrov and Klein,2007).5 The parameterization of this parser is op-2http://www.cs.cmu.edu/?sagae/parser/3http://bllip.cs.brown.edu/resources.shtml4We set n = 50 in this paper.5http://nlp.cs.berkeley.edu/Main.html#Parsing47Figure 3: Predicate argument structuretimized automatically by assigning latent variablesto each nonterminal node and estimating the param-eters of the latent variables by the EM algorithm(Matsuzaki et al, 2005).STANFORD Stanford?s unlexicalized parser (Kleinand Manning, 2003).6 Unlike NO-RERANK, proba-bilities are not parameterized on lexical heads.2.3 Deep parsingRecent research developments have allowed for ef-ficient and robust deep parsing of real-world texts(Kaplan et al, 2004; Clark and Curran, 2004; Miyaoand Tsujii, 2008).
While deep parsers computetheory-specific syntactic/semantic structures, pred-icate argument structures (PAS) are often used inparser evaluation and applications.
PAS is a graphstructure that represents syntactic/semantic relationsamong words (Figure 3).
The concept is thereforesimilar to CoNLL dependencies, though PAS ex-presses deeper relations, and may include reentrantstructures.
In this work, we chose the two versionsof the Enju parser (Miyao and Tsujii, 2008).ENJU The HPSG parser that consists of an HPSGgrammar extracted from the Penn Treebank, anda maximum entropy model trained with an HPSGtreebank derived from the Penn Treebank.7ENJU-GENIA The HPSG parser adapted tobiomedical texts, by the method of Hara et al(2007).
Because this parser is trained with bothWSJ and GENIA, we compare it parsers that areretrained with GENIA (see section 3.3).3 Evaluation MethodologyIn our approach to parser evaluation, we measurethe accuracy of a PPI extraction system, in which6http://nlp.stanford.edu/software/lex-parser.shtml7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/This study demonstrates that IL-8 recognizes andactivates CXCR1, CXCR2, and the Duffy antigenby distinct mechanisms.The molar ratio of serum retinol-binding protein(RBP) to transthyretin (TTR) is not useful to as-sess vitamin A status during infection in hospi-talised children.Figure 4: Sentences including protein namesENTITY1(IL-8) SBJ??
recognizes OBJ??
ENTITY2(CXCR1)Figure 5: Dependency paththe parser output is embedded as statistical featuresof a machine learning classifier.
We run a classi-fier with features of every possible combination of aparser and a parse representation, by applying con-versions between representations when necessary.We also measure the accuracy improvements ob-tained by parser retraining with GENIA, to examinethe domain portability, and to evaluate the effective-ness of domain adaptation.3.1 PPI extractionPPI extraction is an NLP task to identify proteinpairs that are mentioned as interacting in biomedicalpapers.
Because the number of biomedical papers isgrowing rapidly, it is impossible for biomedical re-searchers to read all papers relevant to their research;thus, there is an emerging need for reliable IE tech-nologies, such as PPI identification.Figure 4 shows two sentences that include pro-tein names: the former sentence mentions a proteininteraction, while the latter does not.
Given a pro-tein pair, PPI extraction is a task of binary classi-fication; for example, ?IL-8, CXCR1?
is a positiveexample, and ?RBP, TTR?
is a negative example.Recent studies on PPI extraction demonstrated thatdependency relations between target proteins are ef-fective features for machine learning classifiers (Ka-trenko and Adriaans, 2006; Erkan et al, 2007; S?treet al, 2007).
For the protein pair IL-8 and CXCR1in Figure 4, a dependency parser outputs a depen-dency tree shown in Figure 1.
From this dependencytree, we can extract a dependency path shown in Fig-ure 5, which appears to be a strong clue in knowingthat these proteins are mentioned as interacting.48(dep_path (SBJ (ENTITY1 recognizes))(rOBJ (recognizes ENTITY2)))Figure 6: Tree representation of a dependency pathWe follow the PPI extraction method of S?tre etal.
(2007), which is based on SVMs with SubSetTree Kernels (Collins and Duffy, 2002; Moschitti,2006), while using different parsers and parse rep-resentations.
Two types of features are incorporatedin the classifier.
The first is bag-of-words features,which are regarded as a strong baseline for IE sys-tems.
Lemmas of words before, between and afterthe pair of target proteins are included, and the linearkernel is used for these features.
These features arecommonly included in all of the models.
Filteringby a stop-word list is not applied because this settingmade the scores higher than S?tre et al (2007)?s set-ting.
The other type of feature is syntactic features.For dependency-based parse representations, a de-pendency path is encoded as a flat tree as depicted inFigure 6 (prefix ?r?
denotes reverse relations).
Be-cause a tree kernel measures the similarity of treesby counting common subtrees, it is expected that thesystem finds effective subsequences of dependencypaths.
For the PTB representation, we directly en-code phrase structure trees.3.2 Conversion of parse representationsIt is widely believed that the choice of representa-tion format for parser output may greatly affect theperformance of applications, although this has notbeen extensively investigated.
We should thereforeevaluate the parser performance in multiple parserepresentations.
In this paper, we create multipleparse representations by converting each parser?s de-fault output into other representations when possi-ble.
This experiment can also be considered to bea comparative evaluation of parse representations,thus providing an indication for selecting an appro-priate parse representation for similar IE tasks.Figure 7 shows our scheme for representationconversion.
This paper focuses on five representa-tions as described below.CoNLL The dependency tree format used in the2006 and 2007 CoNLL shared tasks on dependencyparsing.
This is a representation format supported byseveral data-driven dependency parsers.
This repre-Figure 7: Conversion of parse representationsFigure 8: Head dependenciessentation is also obtained from Penn Treebank-styletrees by applying constituent-to-dependency conver-sion8 (Johansson and Nugues, 2007).
It should benoted, however, that this conversion cannot workperfectly with automatic parsing, because the con-version program relies on function tags and emptycategories of the original Penn Treebank.PTB Penn Treebank-style phrase structure treeswithout function tags and empty nodes.
This is thedefault output format for phrase structure parsers.We also create this representation by convertingENJU?s output by tree structure matching, althoughthis conversion is not perfect because forms of PTBand ENJU?s output are not necessarily compatible.HD Dependency trees of syntactic heads (Fig-ure 8).
This representation is obtained by convert-ing PTB trees.
We first determine lexical heads ofnonterminal nodes by using Bikel?s implementationof Collins?
head detection algorithm9 (Bikel, 2004;Collins, 1997).
We then convert lexicalized treesinto dependencies between lexical heads.SD The Stanford dependency format (Figure 9).This format was originally proposed for extractingdependency relations useful for practical applica-tions (de Marneffe et al, 2006).
A program to con-vert PTB is attached to the Stanford parser.
Althoughthe concept looks similar to CoNLL, this representa-8http://nlp.cs.lth.se/pennconverter/9http://www.cis.upenn.edu/?dbikel/software.html49Figure 9: Stanford dependenciestion does not necessarily form a tree structure, and isdesigned to express more fine-grained relations suchas apposition.
Research groups for biomedical NLPrecently adopted this representation for corpus anno-tation (Pyysalo et al, 2007a) and parser evaluation(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).PAS Predicate-argument structures.
This is the de-fault output format for ENJU and ENJU-GENIA.Although only CoNLL is available for depen-dency parsers, we can create four representations forthe phrase structure parsers, and five for the deepparsers.
Dotted arrows in Figure 7 indicate imper-fect conversion, in which the conversion inherentlyintroduces errors, and may decrease the accuracy.We should therefore take caution when comparingthe results obtained by imperfect conversion.
Wealso measure the accuracy obtained by the ensem-ble of two parsers/representations.
This experimentindicates the differences and overlaps of informationconveyed by a parser or a parse representation.3.3 Domain portability and parser retrainingSince the domain of our target text is different fromWSJ, our experiments also highlight the domainportability of parsers.
We run two versions of eachparser in order to investigate the two types of domainportability.
First, we run the original parsers trainedwith WSJ10 (39832 sentences).
The results in thissetting indicate the domain portability of the originalparsers.
Next, we run parsers re-trained with GE-NIA11 (8127 sentences), which is a Penn Treebank-style treebank of biomedical paper abstracts.
Accu-racy improvements in this setting indicate the pos-sibility of domain adaptation, and the portability ofthe training methods of the parsers.
Since the parserslisted in Section 2 have programs for the training10Some of the parser packages include parsing modelstrained with extended data, but we used the models trained withWSJ section 2-21 of the Penn Treebank.11The domains of GENIA and AImed are not exactly thesame, because they are collected independently.with a Penn Treebank-style treebank, we use thoseprograms as-is.
Default parameter settings are usedfor this parser re-training.In preliminary experiments, we found that de-pendency parsers attain higher dependency accuracywhen trained only with GENIA.
We therefore onlyinput GENIA as the training data for the retrainingof dependency parsers.
For the other parsers, we in-put the concatenation of WSJ and GENIA for theretraining, while the reranker of RERANK was not re-trained due to its cost.
Since the parsers other thanNO-RERANK and RERANK require an external POStagger, a WSJ-trained POS tagger is used with WSJ-trained parsers, and geniatagger (Tsuruoka et al,2005) is used with GENIA-retrained parsers.4 Experiments4.1 Experiment settingsIn the following experiments, we used AImed(Bunescu and Mooney, 2004), which is a popularcorpus for the evaluation of PPI extraction systems.The corpus consists of 225 biomedical paper ab-stracts (1970 sentences), which are sentence-split,tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the cor-pus.
Multi-word protein names are concatenatedand treated as single words.
The accuracy is mea-sured by abstract-wise 10-fold cross validation andthe one-answer-per-occurrence criterion (Giulianoet al, 2006).
A threshold for SVMs is moved toadjust the balance of precision and recall, and themaximum f-scores are reported for each setting.4.2 Comparison of accuracy improvementsTables 1 and 2 show the accuracy obtained by usingthe output of each parser in each parse representa-tion.
The row ?baseline?
indicates the accuracy ob-tained with bag-of-words features.
Table 3 showsthe time for parsing the entire AImed corpus, andTable 4 shows the time required for 10-fold crossvalidation with GENIA-retrained parsers.When using the original WSJ-trained parsers (Ta-ble 1), all parsers achieved almost the same levelof accuracy ?
a significantly better result than thebaseline.
To the extent of our knowledge, this isthe first result that proves that dependency parsing,phrase structure parsing, and deep parsing perform50CoNLL PTB HD SD PASbaseline 48.2/54.9/51.1MST 53.2/56.5/54.6 N/A N/A N/A N/AKSDEP 49.3/63.0/55.2 N/A N/A N/A N/ANO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/ARERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/ABERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/ASTANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/AENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)CoNLL PTB HD SD PASbaseline 48.2/54.9/51.1MST 49.1/65.6/55.9 N/A N/A N/A N/AKSDEP 51.6/67.5/58.3 N/A N/A N/A N/ANO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/ARERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/ABERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/ASTANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/AENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)WSJ-trained GENIA-retrainedMST 613 425KSDEP 136 111NO-RERANK 2049 1372RERANK 2806 2125BERKELEY 1118 1198STANFORD 1411 1645ENJU 1447 727ENJU-GENIA 821Table 3: Parsing time (sec.
)equally well in a real application.
Among theseparsers, RERANK performed slightly better than theother parsers, although the difference in the f-scoreis small, while it requires much higher parsing cost.When the parsers are retrained with GENIA (Ta-ble 2), the accuracy increases significantly, demon-strating that the WSJ-trained parsers are not suffi-ciently domain-independent, and that domain adap-tation is effective.
It is an important observation thatthe improvements by domain adaptation are largerthan the differences among the parsers in the pre-vious experiment.
Nevertheless, not all parsers hadtheir performance improved upon retraining.
ParserCoNLL PTB HD SD PASbaseline 424MST 809 N/A N/A N/A N/AKSDEP 864 N/A N/A N/A N/ANO-RERANK 851 4772 882 795 N/ARERANK 849 4676 881 778 N/ABERKELEY 869 4665 895 804 N/ASTANFORD 847 4614 886 799 N/AENJU 832 4611 884 789 1005ENJU-GENIA 874 4624 895 783 1020Table 4: Evaluation time (sec.
)retraining yielded only slight improvements forRERANK, BERKELEY, and STANFORD, while largerimprovements were observed for MST, KSDEP, NO-RERANK, and ENJU.
Such results indicate the dif-ferences in the portability of training methods.
Alarge improvement from ENJU to ENJU-GENIA showsthe effectiveness of the specifically designed do-main adaptation method, suggesting that the otherparsers might also benefit from more sophisticatedapproaches for domain adaptation.While the accuracy level of PPI extraction isthe similar for the different parsers, parsing speed51RERANK ENJUCoNLL HD SD CoNLL HD SD PASKSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)HD 57.1 (+0.5) 58.1 (+0.9)SD 58.3 (+1.1)Table 5: Results of parser/representation ensemble (f-score)differs significantly.
The dependency parsers aremuch faster than the other parsers, while the phrasestructure parsers are relatively slower, and the deepparsers are in between.
It is noteworthy that thedependency parsers achieved comparable accuracywith the other parsers, while they are more efficient.The experimental results also demonstrate thatPTB is significantly worse than the other represen-tations with respect to cost for training/testing andcontributions to accuracy improvements.
The con-version from PTB to dependency-based representa-tions is therefore desirable for this task, although itis possible that better results might be obtained withPTB if a different feature extraction mechanism isused.
Dependency-based representations are com-petitive, while CoNLL seems superior to HD and SDin spite of the imperfect conversion from PTB toCoNLL.
This might be a reason for the high per-formances of the dependency parsers that directlycompute CoNLL dependencies.
The results for ENJU-CoNLL and ENJU-PAS show that PAS contributes to alarger accuracy improvement, although this does notnecessarily mean the superiority of PAS, because twoimperfect conversions, i.e., PAS-to-PTB and PTB-to-CoNLL, are applied for creating CoNLL.4.3 Parser ensemble resultsTable 5 shows the accuracy obtained with ensemblesof two parsers/representations (except the PTB for-mat).
Bracketed figures denote improvements fromthe accuracy with a single parser/representation.The results show that the task accuracy significantlyimproves by parser/representation ensemble.
Inter-estingly, the accuracy improvements are observedeven for ensembles of different representations fromthe same parser.
This indicates that a single parserepresentation is insufficient for expressing the trueBag-of-words features 48.2/54.9/51.1Yakushiji et al (2005) 33.7/33.1/33.4Mitsumori et al (2006) 54.2/42.6/47.7Giuliano et al (2006) 60.9/57.2/59.0S?tre et al (2007) 64.3/44.1/52.0This paper 54.9/65.5/59.5Table 6: Comparison with previous results on PPI extrac-tion (precision/recall/f-score)potential of a parser.
Effectiveness of the parser en-semble is also attested by the fact that it resulted inlarger improvements.
Further investigation of thesources of these improvements will illustrate the ad-vantages and disadvantages of these parsers and rep-resentations, leading us to better parsing models anda better design for parse representations.4.4 Comparison with previous results on PPIextractionPPI extraction experiments on AImed have been re-ported repeatedly, although the figures cannot becompared directly because of the differences in datapreprocessing and the number of target protein pairs(S?tre et al, 2007).
Table 6 compares our best re-sult with previously reported accuracy figures.
Giu-liano et al (2006) and Mitsumori et al (2006) donot rely on syntactic parsing, while the former ap-plied SVMs with kernels on surface strings and thelatter is similar to our baseline method.
Bunescu andMooney (2005) applied SVMs with subsequencekernels to the same task, although they providedonly a precision-recall graph, and its f-score isaround 50.
Since we did not run experiments onprotein-pair-wise cross validation, our system can-not be compared directly to the results reportedby Erkan et al (2007) and Katrenko and Adriaans52(2006), while S?tre et al (2007) presented better re-sults than theirs in the same evaluation criterion.5 Related WorkThough the evaluation of syntactic parsers has beena major concern in the parsing community, and acouple of works have recently presented the com-parison of parsers based on different frameworks,their methods were based on the comparison of theparsing accuracy in terms of a certain intermediateparse representation (Ringger et al, 2004; Kaplanet al, 2004; Briscoe and Carroll, 2006; Clark andCurran, 2007; Miyao et al, 2007; Clegg and Shep-herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,2007a; Sagae et al, 2008).
Such evaluation requiresgold standard data in an intermediate representation.However, it has been argued that the conversion ofparsing results into an intermediate representation isdifficult and far from perfect.The relationship between parsing accuracy andtask accuracy has been obscure for many years.Quirk and Corston-Oliver (2006) investigated theimpact of parsing accuracy on statistical MT.
How-ever, this work was only concerned with a single de-pendency parser, and did not focus on parsers basedon different frameworks.6 Conclusion and Future WorkWe have presented our attempts to evaluate syntac-tic parsers and their representations that are based ondifferent frameworks; dependency parsing, phrasestructure parsing, or deep parsing.
The basic ideais to measure the accuracy improvements of thePPI extraction task by incorporating the parser out-put as statistical features of a machine learningclassifier.
Experiments showed that state-of-the-art parsers attain accuracy levels that are on parwith each other, while parsing speed differs sig-nificantly.
We also found that accuracy improve-ments vary when parsers are retrained with domain-specific data, indicating the importance of domainadaptation and the differences in the portability ofparser training methods.Although we restricted ourselves to parserstrainable with Penn Treebank-style treebanks, ourmethodology can be applied to any English parsers.Candidates include RASP (Briscoe and Carroll,2006), the C&C parser (Clark and Curran, 2004),the XLE parser (Kaplan et al, 2004), MINIPAR(Lin, 1998), and Link Parser (Sleator and Temperley,1993; Pyysalo et al, 2006), but the domain adapta-tion of these parsers is not straightforward.
It is alsopossible to evaluate unsupervised parsers, which isattractive since evaluation of such parsers with gold-standard data is extremely problematic.A major drawback of our methodology is thatthe evaluation is indirect and the results dependon a selected task and its settings.
This indicatesthat different results might be obtained with othertasks.
Hence, we cannot conclude the superiority ofparsers/representations only with our results.
In or-der to obtain general ideas on parser performance,experiments on other tasks are indispensable.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan),Genome Network Project (MEXT, Japan), andGrant-in-Aid for Young Scientists (MEXT, Japan).ReferencesD.
M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.T.
Briscoe and J. Carroll.
2006.
Evaluating the accu-racy of an unlexicalized statistical parser on the PARCDepBank.
In COLING/ACL 2006 Poster Session.R.
Bunescu and R. J. Mooney.
2004.
Collective infor-mation extraction with relational markov networks.
InACL 2004, pages 439?446.R.
C. Bunescu and R. J. Mooney.
2005.
Subsequencekernels for relation extraction.
In NIPS 2005.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
InACL 2005.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL-2000, pages 132?139.S.
Clark and J. R. Curran.
2004.
Parsing the WSJ usingCCG and log-linear models.
In 42nd ACL.S.
Clark and J. R. Curran.
2007.
Formalism-independentparser evaluation with CCG and DepBank.
In ACL2007.A.
B. Clegg and A. J. Shepherd.
2007.
Benchmark-ing natural-language parsers for biological applica-tions using dependency graphs.
BMC Bioinformatics,8:24.53M.
Collins and N. Duffy.
2002.
New ranking algorithmsfor parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL 2002.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In 35th ACL.M.-C. de Marneffe, B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.J.
M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In COLING1996.G.
Erkan, A. Ozgur, and D. R. Radev.
2007.
Semi-supervised classification for extracting protein interac-tion sentences using dependency parsing.
In EMNLP2007.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In EMNLP 2001, pages 167?202.C.
Giuliano, A. Lavelli, and L. Romano.
2006.
Exploit-ing shallow linguistic information for relation extrac-tion from biomedical literature.
In EACL 2006.T.
Hara, Y. Miyao, and J. Tsujii.
2007.
Evaluating im-pact of re-training a lexical disambiguation model ondomain adaptation of an HPSG parser.
In IWPT 2007.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InNODALIDA 2007.R.
M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, andA.
Vasserman.
2004.
Speed and accuracy in shallowand deep stochastic parsing.
In HLT/NAACL?04.S.
Katrenko and P. Adriaans.
2006.
Learning relationsfrom biomedical corpora using dependency trees.
InKDECB, pages 61?80.J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii.
2003.
GE-NIA corpus ?
a semantically annotated corpus forbio-textmining.
Bioinformatics, 19:i180?182.D.
Klein and C. D. Manning.
2003.
Accurate unlexical-ized parsing.
In ACL 2003.D.
Lin.
1998.
Dependency-based evaluation of MINI-PAR.
In LREC Workshop on the Evaluation of ParsingSystems.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL 2005.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In EACL2006.T.
Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.2006.
Extracting protein-protein interaction informa-tion from biomedical text with SVM.
IEICE - Trans.Inf.
Syst., E89-D(8):2464?2466.Y.
Miyao and J. Tsujii.
2008.
Feature forest models forprobabilistic HPSG parsing.
Computational Linguis-tics, 34(1):35?80.Y.
Miyao, K. Sagae, and J. Tsujii.
2007.
Towardsframework-independent evaluation of deep linguisticparsers.
In Grammar Engineering across Frameworks2007, pages 238?258.A.
Moschitti.
2006.
Making tree kernels practical fornatural language processing.
In EACL 2006.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective depen-dency parsing.
In ACL 2005.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL 2007.S.
Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.2006.
Lexical adaptation of link grammar to thebiomedical sublanguage: a comparative evaluation ofthree approaches.
BMC Bioinformatics, 7(Suppl.
3).S.
Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,J.
Ja?rvinen, and T. Salakoski.
2007a.
BioInfer: a cor-pus for information extraction in the biomedical do-main.
BMC Bioinformatics, 8(50).S.
Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-monen, and T. Salakoski.
2007b.
On the unification ofsyntactic annotations under the Stanford dependencyscheme: A case study on BioInfer and GENIA.
InBioNLP 2007, pages 25?32.C.
Quirk and S. Corston-Oliver.
2006.
The impact ofparse quality on syntactically-informed statistical ma-chine translation.
In EMNLP 2006.E.
K. Ringger, R. C. Moore, E. Charniak, L. Vander-wende, and H. Suzuki.
2004.
Using the Penn Tree-bank to evaluate non-treebank parsers.
In LREC 2004.R.
S?tre, K. Sagae, and J. Tsujii.
2007.
Syntacticfeatures for protein-protein interaction extraction.
InLBM 2007 short papers.K.
Sagae and J. Tsujii.
2007.
Dependency parsing anddomain adaptation with LR models and parser ensem-bles.
In EMNLP-CoNLL 2007.K.
Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii.
2008.Challenges in mapping of syntactic representationsfor framework-independent parser evaluation.
In theWorkshop on Automated Syntatic Annotations for In-teroperable Language Resources.D.
D. Sleator and D. Temperley.
1993.
Parsing Englishwith a Link Grammar.
In 3rd IWPT.Y.
Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-Naught, S. Ananiadou, and J. Tsujii.
2005.
Develop-ing a robust part-of-speech tagger for biomedical text.In 10th Panhellenic Conference on Informatics.A.
Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii.
2005.Biomedical information extraction with predicate-argument structure patterns.
In First InternationalSymposium on Semantic Mining in Biomedicine.54
