Proceedings of the First Workshop on Metaphor in NLP, pages 45?51,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsCross-Lingual Metaphor Detection Using Common Semantic FeaturesYulia Tsvetkov Elena Mukomel Anatole GershmanLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{ytsvetko,helenm,anatoleg}@cs.cmu.eduAbstractWe present the CSF - Common Semantic Fea-tures method for metaphor detection.
Thismethod has two distinguishing characteristics:it is cross-lingual and it does not rely on theavailability of extensive manually-compiledlexical resources in target languages other thanEnglish.
A metaphor detecting classifier istrained on English samples and then applied tothe target language.
The method includes pro-cedures for obtaining semantic features fromsentences in the target language.
Our exper-iments with Russian and English sentencesshow comparable results, supporting our hy-pothesis that a CSF-based classifier can be ap-plied across languages.
We obtain state-of-the-art performance in both languages.1 IntroductionMetaphors are very powerful pervasive communica-tion tools that help deliver complex concepts andideas simply and effectively (Lakoff and Johnson,1980).
Automatic detection and interpretation ofmetaphors is critical for many practical languageprocessing tasks such as information extraction,summarization, opinion mining, and translation.
Inthis paper, we focus on the automatic metaphor de-tection task.
This problem gained much attentionin natural language processing research mostly us-ing the detection principles articulated by the Prag-glejaz Group (2007).
According to these princi-ples, a lexical unit (a word or expression) is usedmetaphorically if its contextual meaning is differentfrom its ?basic contemporary?
meaning.
To applythis method, we need to be able to determine the ba-sic meaning of a lexical unit and then test if this in-terpretation makes sense in the current context.Several approaches to automatic detection ofmetaphors have been proposed (Gedigian et al2006; Krishnakumaran and Zhu, 2007; Shutova etal., 2010), all of which rely on the availability ofextensive manually crafted lexical resources suchas WordNet, VerbNet, FrameNet, TreeBank, etc.Unfortunately, such resources exist only for a fewresource-rich languages such as English.
For mostother languages, such resources either do not existor are of a low quality.To our knowledge this work is the first empiri-cal study of cross-lingual metaphor detection.
Wepresent the Common Semantic Features (CSF) ap-proach to metaphor detection in languages withoutextensive lexical resources.
In a target languageit requires only a dependency parser and a target-English dictionary.
We classify sentences into lit-eral and metaphoric using automatically extractedcoarse-grained semantic properties of words such astheir propensity to refer to abstract versus concreteconcepts, animate entities, artifacts, body parts, etc.These properties serve as features for the key re-lations in a sentence, which include Subject-Verb-Object (SVO) and Adjective-Noun (AN).
A clas-sifier trained on English sentences obtains a 0.78F -score.
The same classifier, trained solely onEnglish sentences, achieves a similar level of per-formance on sentences from other languages suchas Russian; this is the central contribution of thiswork.
An additional important contribution is that inRussian we obtain the necessary semantic features45without recourse to sophisticated non-English lexi-cal resources.
In this paper, we focus on the sen-tences where verbs are used metaphorically, leavingAdjective-Noun relations for future work.
Based onour examination of over 500 metaphorical sentencesin English and Russian collected from general newsarticles, we estimate that verb-based metaphors con-stitute about 40-50% of all metaphors.We present and discuss our experiments withthree sets of features: (1) features corresponding tothe lexicographer file names defined in WordNet3.0 (Fellbaum, 1998), (2) features based on abstract-ness vs. concreteness computed using Vector SpaceModels (VSM), and (3) features based on the typesof named entities, if present.
Our main target lan-guage in these experiments has been Russian, but wealso present preliminary experiments with Spanish.The paper is organized as follows: Section 2 con-tains an overview of the resources we use; Sec-tion 3 discusses the methodology; Section 4 presentsthe experiments; in Section 5, we discuss relatedwork, and we conclude with suggestions for futureresearch in Section 6.2 DatasetsWe use the following English lexical resources totrain our model:TroFi Example Base1 (Birke and Sarkar, 2007) of3,737 English sentences from the Wall Street Jour-nal.
Each sentence contains one of the seed verbsand is marked L by human annotators if the verbis used in a literal sense.
Otherwise, the sentenceis marked N (non-literal).
The model was evalu-ated on 25 target verbs with manually annotated 1to 115 sentences per verb.
TroFi does not define thebasic meanings of these verbs, but provides exam-ples of literal and metaphoric sentences which weuse to train and evaluate our metaphor identificationmethod.WordNet (Fellbaum, 1998) is an English lexicaldatabase where each entry contains a set of syn-onyms (a synset) all representing the same con-cept.
This database is compiled from a set of1http://www.cs.sfu.ca/ anoop/students/jbirke/45 lexicographer files2 such as ?noun.body?
or?verb.cognition?
identified by a number from 0 to44, called lexicographer file number (henceforthlexFN ).
The lexFN of each synset is contained inthe database.
We use lexFNs as coarse-grain se-mantic features of nouns and verbs.MRC Psycholinguistic Database3 (Wilson, 1988)is a dictionary containing 150,837 words with up to26 linguistic and psycholinguistic attributes rated byhuman subjects in psycholinguistic experiments.
Itincludes 4,295 words rated with degrees of abstract-ness; the ratings range from 158 (highly abstract)to 670 (highly concrete).
We use these words as aseed when we calculate the values of abstractnessand concreteness features for nouns and verbs in ourtraining and test sets.Word Representations via Global Context is acollection of 100,232 words and their vector rep-resentations.4 These representations were extractedfrom a statistical model embedding both local andglobal contexts of words (Huang et al 2012), in-tended to capture better the semantics of words.
Weuse these vectors to calculate the values of abstract-ness and concreteness features of a word.3 MethodologyWe treat the metaphor detection problem as a taskof binary classification of sentences.
A sentenceis represented by one or more key relations suchas Subject-Verb-Object triples and Adjective-Nounpairs.
In this paper, we focus only on the SVO rela-tions and we allow either the S part or the O part tobe empty.
If all relations representing a sentence areclassified literal by our model then the whole sen-tence is tagged literal.
Otherwise, the sentence istagged metaphoric.2See http://wordnet.princeton.edu/man/lexnames.5WN.htmlfor a full list of lexicographer file names.3http://ota.oucs.ox.ac.uk/headers/1054.xml4http://www.socher.org/index.php/Main/Improving-WordRepresentationsViaGlobalContextAndMultipleWordPrototypes463.1 ModelWe classify an SVO relation x as literal vs.metaphorical using a logistic regression classifier:p(y | x) ?
exp?j?jhj(y, x),where hj(?)
are feature values computed for eachword in x, ?j are the corresponding weights, andy ?
{L,M} refer to our classes: L for literal andM for metaphoric.
The parameters ?j are learnedduring training.3.2 FeaturesAn SVO relation is a concatenation of features forthe S, V, and O parts.
The S and O parts containthree types of features: (1) semantic categories of aword, (2) degree of abstractness of a word, and (3)types of named entities.
The V part contains onlythe first two types of features.Semantic categories are features correspondingto the WordNet lexFNs, introduced in Section 2.Since S and O are assumed to be nouns,5 each has26 semantic category features corresponding to thelexFNs for nouns (3 through 28).
These categoriesinclude noun.animal, noun.artefact, noun.body,noun.cognition, noun.food, noun.location, etc.
TheV part has 15 semantic category features corre-sponding to lexical ids for verbs (29 through 43),for example, verb.motion and verb.cognition.
A lex-ical item can belong to several synsets with differentlexFNs.
For example, the word ?head?
when usedas a noun participates in 33 synsets, 3 of which havelexFN 08 (noun.body).
The value of the featurecorresponding to this lexFN is 3/33 = 0.09.For a non-English word, we first obtain its mostcommon translations to English and then select allcorresponding English WordNet synsets.
For exam-ple, when Russian word `??????'
is translated as?head?
and ?brain?, we select all the synsets for thenouns head and brain.
There are 38 such synsets (33for head and 5 for brain).
Four of these synsets havelexFN 08 (noun.body).
Therefore, the value ofthe feature corresponding to this lexFN is 4/38 =0.10.
This dictionary-based mapping of non-English5We currently exclude pronouns from the relations that welearn.words into WN synsets is rather coarse.
A more dis-criminating approach may improve the overall per-formance.
In addition, WN synsets may not alwayscapture all the meanings of non-English words.
Forexample, Russian word `????'
refers to both the?foot?
and the ?leg?.
WN has synsets for foot, legand extremity, but not for lower extremity.Degree of abstractness According to Turney et al(2011), ?Abstract words refer to ideas and conceptsthat are distant from immediate perception, such aseconomics, calculating and disputable.?
Concretewords refer to physical objects and actions.
Wordswith multiple senses can refer to both concrete andabstract concepts.
Evidence from several languagessuggests that concrete verbs tend to have concretesubjects and objects.
If either the subject or an objectof a concrete verb is abstract, then the verb is typi-cally used in a figurative sense, indicating the pres-ence of a metaphor.
For example, when we hear that?an idea was born?, we know that the word ?born?is used figuratively.
This observation motivates ourdecision to include the degree of abstractness in ourfeature set.To calculate the degree of abstractness of Englishlexical items we use the vector space representationsof words computed by Huang et al(2012) and a sep-arate supervised logistic regression classifier trainedon a set of abstract and concrete words from theMRC dataset.
Each value in a word?s vector is a fea-ture, thus, semantically similar words have similarfeature values.
Degrees of abstractness are posteriorprobabilities of the classifier predictions.For non-English words, we use the following pro-cedure.
Suppose word w has n English transla-tions whose degrees of abstractness are a1, a2, .
.
.
anin decreasing order.
If the majority is deemedabstract then ABSTRACT (w) = a1, otherwiseABSTRACT (w) = an.
This heuristic prefers theextreme interpretations, and is based on an observa-tion that translations tend to be skewed to one side orthe other of ?abstractness?.
Our results may improveif we map non-English words more precisely into themost contextually-appropriate English senses.Named entities (NE) is an additional categoryof features instrumental in metaphor identification.Specifically, we would like to distinguish whetheran action (a verb in SVO) is performed by a human,47an organization or a geographical entity.
These dis-tinctions are often needed to detect metonymy, as in?the White House said?.
Often, these entities arementioned by their names which are not found incommon dictionaries.
Fortunately, there are manynamed entity recognizers (NER) for all major lan-guages.
In addition, Shah et al(2010) showedthat named entities tend to survive popular machinetranslation engines and can be relatively reliably de-tected even without a native NER.
Based on theseobservations, we decided to include three booleanfeatures corresponding to these NE categories: per-son, organization, and location.4 ExperimentsWe train two classifiers: the first to calculate the de-gree of abstractness of a given word and the secondto classify an SVO relation as metaphoric or literal.Both are logistic regression classifiers trained withthe creg regression modeling framework.6 To min-imize the number of free parameters in our model weuse `1 regularization.4.1 Measuring abstractnessTo train the abstractness classifier, we normalize ab-stractness scores of nouns from the MRC datasetto probabilities, and select 1,225 most abstract and1,225 most concrete words.
From these words, weset aside 25 randomly selected samples from eachcategory for testing.
We obtain the vector space rep-resentations of the remaining 1,400 samples and usethe dimensions of these representations as features.We train the abstractness classifier on the 1,400 la-beled samples and test it on the 50 samples that wereset aside, obtaining 76% accuracy.
The degree of ab-stractness of a word is the posterior probability pro-duced by the abstractness classifier.4.2 Metaphor detectionWe train the metaphor classifier using labeled En-glish SVO relations.
To obtain these relations,we use the Turbo parser (Martins et al 2010) toparse 1,592 literal and 1,609 metaphorical man-ually annotated sentences from the TroFi Exam-ple Base and extract 1,660 sentences that haveSVO relations that contain annotated verbs: 6966https://github.com/redpony/cregliteral and 964 metaphorical training instances.For example, the verb flourish is used literally in?Methane-making bacteria flourish in the stom-ach?
and metaphorically in ?Economies flourish infree markets?.
From the first sentence we extractSVO relation <bacteria, flourish, NIL>,and <economies, flourish, NIL> from thesecond.
We then build feature vectors, using featurecategories described in Section 3.We train several versions of the metaphor classi-fier for each feature category and for their combina-tions.
The feature categories are designated as fol-lows:?
WN - Semantic categories based on WordNet lexFNs?
VSM - Degree of abstractness based on word vectors?
NE - Named Entity categoriesWe evaluate the metaphor classifiers using 10-foldcross validation.
The results are listed in Table 1.Feature categories AccuracyWN 63.7%VSM 64.1%WN+VSM 67.7%WN+NE 64.5%WN+VSM+NE 69.0%Table 1: 10-fold cross validation results of themetaphor classifier.Our results are comparable to the accuracy of64.9% reported by Birke and Sarkar (2007) on theTroFi dataset.
The combination of all feature cate-gories significantly improves over this baseline.4.2.1 English metaphor detectionWe compute precision, recall and F -score on atest set of 98 English sentences.
This test set consistsof 50 literal and 48 metaphorical sentences, whereeach metaphoric sentence contains a verb used in afigurative sense.
The test sentences were selectedfrom general news articles by independent collec-tors.
Table 2 shows the results.In this experiment, the WN group of features con-tributes the most.
The addition of NE, while not im-proving the overall F -score, helps to reduce falsepositives and better balance precision and recall.The VSM features are considerably weaker perhaps48Feature categories Precision Recall F -scoreWN 0.75 0.81 0.78VSM 0.57 0.71 0.63WN+VSM 0.66 0.90 0.76WN+NE 0.78 0.79 0.78WN+VSM+NE 0.68 0.71 0.69Table 2: Evaluation of the metaphor classifier onthe test set of 50 literal and 48 metaphoric Englishsentences from news articles.because we used single model vector space repre-sentations where each word uses only one vector thatcombines all its senses.4.2.2 Russian metaphor detectionIn a cross-lingual experiment, we evaluate our al-gorithm on a set of 140 Russian sentences: 62 literaland 78 metaphoric, selected from general news arti-cles by two independent collectors.
As in English,each metaphoric sentence contains a verb used in afigurative sense.
We used the AOT parser7 to ob-tain the SVO relations and the Babylon dictionary8to obtain English translations of individual words.The example sentence in Figure 1 contains one SVOrelation with missing O part.
We show the set of fea-tures and their values that were extracted from wordsin this relation.The results of the Russian test set, listed in Ta-ble 3, are similar to the English results, supportingour hypothesis that a semantic classifier can workacross languages.
As in the previous experiment, theWN features are the most effective and the NE fea-tures contribute to improved precision.Feature categories Precision Recall F -scoreWN 0.74 0.76 0.75VSM 0.66 0.73 0.69WN+VSM 0.70 0.73 0.71WN+NE 0.82 0.71 0.76WN+VSM+NE 0.74 0.72 0.73Table 3: Evaluation of the metaphor classifier onthe test set of 62 literal and 78 metaphoric Russiansentences from news articles.While we did not conduct a full-scale experiment7www.aot.ru8www.babylon.comwith Spanish, we ran a pilot using 51 sentences: 24literal and 27 metaphoric.
We obtained the F -scoreof 0.66 for the WN+VSM combination.
We take it asa positive sign and will conduct more experiments.5 Related workOur work builds on the research of Birke and Sarkar(2007) who used an active learning approach to cre-ate an annotated corpus of sentences with literaland figurative senses of 50 common English verbs.The result was the TroFi Example Base set of 3,737labeled sentences, which was used by the authorsto train several classifiers.
These algorithms weretested on sentences containing 25 English verbs notincluded in the original set.
The authors report F -scores around 64.9%.
We used this dataset for train-ing and evaluation, and Birke and Sarkar?s (2007)results as a baseline.In a more recent work, Turney et al(2011) sug-gested that the degree of abstractness of a word?scontext is correlated with the likelihood that theword is used metaphorically.
To compute the ab-stractness of a word, the authors use a variationof Turney and Littman?s (2003) algorithm compar-ing the word to twenty typically abstract words andtwenty typically concrete words.
Latent SemanticAnalysis (Deerwester et al 1990) is used to mea-sure semantic similarity between each pair of words.A feature vector is generated for each word and alogistic regression classifier is used.
The result isan average F -score of 63.9% on the TroFi dataset,9compared to Birke and Sarkar?s (2007) 64.9%.
Inanother experiment on 100 adjective-noun phraseslabeled as literal or non-literal, according to thesense of the adjective, this algorithm obtains an av-erage accuracy of 79%.
While we obtain compara-ble results, our work extends this method in severalimportant directions.
First, we show how to applya metaphor classifier across languages.
Second, weextend our feature set beyond abstractness criteria.Finally, we propose an alternative technique to mea-sure degrees of abstractness.9Turney et al(2011) report on two experimental setups withTroFi, our setup is closer to their first experiment.49????????
?????
?????????????
.
?Society ripens over decades?SVO = <???????
?, ????
?, NIL>Subject VerbWNnoun.group 0.54noun.state 0.23noun.possession 0.15noun.location 0.08verb.change 0.75verb.body 0.125verb.communication 0.125VSM Abstractness 0.87 Abstractness 0.93Figure 1: Features extracted for a Russian test sentence classified as metaphoric by our model.6 Conclusions and future workWe presented CSF ?
an approach to metaphor de-tection based on semantic rather than lexical fea-tures.
We described our experiments with an ini-tial set of fairly coarse-grained features and showedhow these features can be obtained in languages thatlack extensive lexical resources.
Semantic, as op-posed to lexical features, are common to all lan-guages which allows a classifier trained to detectmetaphors in one language to be successfully ap-plied to sentences in another language.
Our resultssuggest that metaphors can be detected on a con-ceptual level, independently of whether they are ex-pressed in Russian or English, supporting Lakoffand Johnson?s (1980) claim that metaphors are partsof a pervasive conceptual system.Our current work has been limited to the detectionof figurative SVO relations, which account for abouthalf of all metaphors in English and Russian.
Otherlanguages such as Farsi have a greater proportion ofmetaphors based on figurative use of adjectives andnouns.
We plan to include more relations and ex-pand our set of semantic features as part of the futureresearch.AcknowledgmentsWe are grateful to Chris Dyer for his invaluable advice.We are also grateful to the three anonymous reviewers fortheir constructive suggestions.
Supported by the Intelli-gence Advanced Research Projects Activity (IARPA) viaDepartment of Defense US Army Research Laboratorycontract number W911NF-12-C-0020.
The U.S. Govern-ment is authorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copyrightannotation thereon.
Disclaimer: The views and conclu-sions contained herein are those of the authors and shouldnot be interpreted as necessarily representing the officialpolicies or endorsements, either expressed or implied, ofIARPA, DoD/ARL, or the U.S. Government.ReferencesJulia Birke and Anoop Sarkar.
2007.
Active learning forthe identification of nonliteral language.
In Proceed-ings of the Workshop on Computational Approaches toFigurative Language, FigLanguages ?07, pages 21?28.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Language, Speech and Com-munication.
MIT Press.Matt Gedigian, John Bryant, Srini Narayanan, and Bran-imir Ciric.
2006.
Catching metaphors.
In Proceedingsof the 3rd Workshop on Scalable Natural LanguageUnderstanding, pages 41?48.Pragglejaz Group.
2007.
MIP: A method for identify-ing metaphorically used words in discourse.
Metaphorand Symbol, 22(1):1?39.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Annual Meeting of the Association for Computa-tional Linguistics, ACL 2012.Saisuresh Krishnakumaran and Xiaojin Zhu.
2007.Hunting elusive metaphors using lexical resources.
InProceedings of the Workshop on Computational ap-proaches to Figurative Language, pages 13?20.George Lakoff and Mark Johnson.
1980.
Conceptualmetaphor in everyday language.
The Journal of Phi-losophy, pages 453?486.50Andre?
F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo.
2010.Turbo parsers: dependency parsing by approximatevariational inference.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 34?44.Rushin Shah, Bo Lin, Anatole Gershman, and RobertFrederking.
2010.
SYNERGY: a named entity recog-nition system for resource-scarce languages such asSwahili using online machine translation.
In Proceed-ings of the Second Workshop on African LanguageTechnology, AfLaT 2010, pages 21?26.Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010.Metaphor identification using verb and noun cluster-ing.
In Proceedings of the 23rd International Confer-ence on Computational Linguistics, pages 1002?1010.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orien-tation from association.
ACM Transactions on Infor-mation and System Security, 21(4):315?346.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?11, pages680?690.Michael Wilson.
1988.
MRC Psycholinguistic Database:Machine-usable dictionary, version 2.00.
Behav-ior Research Methods, Instruments, & Computers,20(1):6?10.51
