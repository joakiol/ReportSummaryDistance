Robust Reading: Identification and Tracing of Ambiguous NamesXin Li Paul Morie Dan RothDepartment of Computer ScienceUniversity of Illinois, Urbana, IL 61801{xli1,morie,danr}@uiuc.eduAbstractA given entity, representing a person, a locationor an organization, may be mentioned in textin multiple, ambiguous ways.
Understandingnatural language requires identifying whetherdifferent mentions of a name, within and acrossdocuments, represent the same entity.We develop an unsupervised learning approachthat is shown to resolve accurately the nameidentification and tracing problem.
At the heartof our approach is a generative model of howdocuments are generated and how names are?sprinkled?
into them.
In its most general form,our model assumes: (1) a joint distribution overentities, (2) an ?author?
model, that assumesthat at least one mention of an entity in a docu-ment is easily identifiable, and then generatesother mentions via (3) an appearance model,governing how mentions are transformed fromthe ?representative?
mention.
We show how toestimate the model and do inference with it andhow this resolves several aspects of the prob-lem from the perspective of applications suchas questions answering.1 IntroductionReading and understanding text is a task that requires theability to disambiguate at several levels, abstracting awaydetails and using background knowledge in a variety ofways.
One of the difficulties that humans resolve instan-taneously and unconsciously is that of reading names.Most names of people, locations, organizations and oth-ers, have multiple writings that are used freely within andacross documents.The variability in writing a given concept, along withthe fact that different concepts may have very similarwritings, poses a significant challenge to progress in nat-ural language processing.
Consider, for example, an opendomain question answering system (Voorhees, 2002) thatattempts, given a question like: ?When was PresidentKennedy born??
to search a large collection of articles inorder to pinpoint the concise answer: ?on May 29, 1917.?The sentence, and even the document that contains theanswer, may not contain the name ?President Kennedy?
;it may refer to this entity as ?Kennedy?, ?JFK?
or ?JohnFitzgerald Kennedy?.
Other documents may state that?John F. Kennedy, Jr. was born on November 25, 1960?,but this fact refers to our target entity?s son.
Other men-tions, such as ?Senator Kennedy?
or ?Mrs.
Kennedy?are even ?closer?
to the writing of the target entity, butclearly refer to different entities.
Even the statement?John Kennedy, born 5-29-1941?
turns out to refer to adifferent entity, as one can tell observing that the doc-ument discusses Kennedy?s batting statistics.
A similarproblem exists for other entity types, such as locations,organizations etc.
Ad hoc solutions to this problem, aswe show, fail to provide a reliable and accurate solution.This paper presents the first attempt to apply a unifiedapproach to all major aspects of this problem, presentedhere from the perspective of the question answering task:(1) Entity Identity - do mentions A and B (typically,occurring in different documents, or in a question and adocument, etc.)
refer to the same entity?
This problemrequires both identifying when different writings refer tothe same entity, and when similar or identical writingsrefer to different entities.
(2) Name Expansion - given awriting of a name (say, in a question), find other likelywritings of the same name.
(3) Prominence - givenquestion ?What is Bush?s foreign policy?
?, and given thatany large collection of documents may contain severalBush?s, there is a need to identify the most prominent, orrelevant ?Bush?, perhaps taking into account also somecontextual information.At the heart of our approach is a global probabilisticview on how documents are generated and how names(of different entity types) are ?sprinkled?
into them.
Inits most general form, our model assumes: (1) a joint dis-tribution over entities, so that a document that mentions?President Kennedy?
is more likely to mention ?Oswald?or ?
White House?
than ?Roger Clemens?
; (2) an ?au-thor?
model, that makes sure that at least one mentionof a name in a document is easily identifiable, and thengenerates other mentions via (3) an appearance model,governing how mentions are transformed from the ?rep-resentative?
mention.
Our goal is to learn the model froma large corpus and use it to support robust reading - en-abling ?on the fly?
identification and tracing of entities.This work presents the first study of our proposedmodel and several relaxations of it.
Given a collection ofdocuments we learn the models in an unsupervised way;that is, the system is not told during training whether twomentions represent the same entity.
We only assume theability to recognize names, using a named entity recog-nizer run as a preprocessor.
We define several inferencesthat correspond to the solutions we seek, and evaluate themodels by performing these inferences against a largecorpus we annotated.
Our experimental results suggestthat the entity identity problem can be solved accurately,giving accuracies (F1) close to 90%, depending on thespecific task, as opposed to 80% given by state of the artad-hoc approaches.Previous work in the context of question answeringhas not addressed this problem.
Several works in NLPand Databases, though, have addressed some aspects ofit.
From the natural language perspective, there hasbeen a lot of work on the related problem of corefer-ence resolution (Soon et al, 2001; Ng and Cardie, 2003;Kehler, 2002) - which aims at linking occurrences ofnoun phrases and pronouns within a document based ontheir appearance and local context.
(Charniak, 2001)presents a solution to the problem of name structurerecognition by incorporating coreference information.
Inthe context of databases, several works have looked at theproblem of record linkage - recognizing duplicate recordsin a database (Cohen and Richman, 2002; Hernandez andStolfo, 1995; Bilenko and Mooney, 2003).
Specifically,(Pasula et al, 2002) considers the problem of identity un-certainty in the context of citation matching and suggestsa probabilistic model for that.
Some of very few workswe are aware of that works directly with text data andacross documents, are (Bagga and Baldwin, 1998; Mannand Yarowsky, 2003), which consider one aspect of theproblem ?
that of distinguishing occurrences of identicalnames in different documents, and only of people.The rest of this paper is organized as follows: We for-malize the ?robust reading?
problem in Sec.
2.
Sec.
3describes a generative view of documents?
creation andthree practical probabilistic models designed based on it,and discusses inference in these models.
Sec.
4 illustrateshow to learn these models in an unsupervised setting, andSec.
5 describes the experimental study.
Sec.
6 concludes.2 Robust ReadingWe consider reading a collection of documents D ={d1, d2, .
.
.
, dm}, each of which may contain men-tions (i.e.
real occurrences) of |T | types of enti-ties.
In the current evaluation we consider T ={Person, Location,Organization}.An entity refers to the ?real?
concept behind a mentionand can be viewed as a unique identifier to a real-worldobject.
Examples might be the person ?John F. Kennedy?who became a president, ?White House?
?
the residenceof the US presidents, etc.
E denotes the collection of allpossible entities in the world and Ed = {edi }ld1 is the setof entities mentioned in document d. M denotes the col-lection of all possible mentions and Md = {mdi }nd1 isthe set of mentions in document d. Mdi (1 ?
i ?
ld) isthe set of mentions that refer to entity edi ?
Ed.
For en-tity ?John F.
Kennedy?, the corresponding set of mentionsin a document may contain ?Kennedy?, ?J.
F. Kennedy?and ?President Kennedy?.
Among all mentions of an en-tity edi in document d we distinguish the one occurringfirst, rdi ?
Mdi , as the representative of edi .
In practice,rdi is usually the longest mention of edi in the documentas well, and other mentions are variations of it.
Repre-sentatives are viewed as a typical representation of anentity mentioned in a specific time and place.
For ex-ample, ?President J.F.Kennedy?
and ?Congressman JohnKennedy?
may be representatives of ?John F. Kennedy?in different documents.
R denotes the collection of allpossible representatives and Rd = {rdi }ld1 ?
Md is theset of representatives in document d. This way, each doc-ument is represented as the collection of its entities, rep-resentatives and mentions d = {Ed, Rd,Md}.Elements in the name space W = E?R?M each havean identifying writing (denoted as wrt(n) for n ?
W )1and an ordered list of attributes, A = {a1, .
.
.
, ap},which depends on the entity type.
Attributes used in thecurrent evaluation include both internal attributes, suchas, for People, {title, firstname, middlename, lastname,gender} as well as contextual attributes such as {time, lo-cation, proper-names}.
Proper-names refer to a list ofproper names that occur around the mention in the doc-ument.
All attributes are of string value and the valuescould be missing or unknown2.The fundamental problem we address in robust read-ing is to decide what entities are mentioned in a givendocument (given the observed set Md) and what the mostlikely assignment of entity to each mention is.3 A Model of Document GenerationWe define a probability distribution over documents d ={Ed, Rd,Md}, by describing how documents are beinggenerated.
In its most general form the model has thefollowing three components:(1) A joint probability distribution P (Ed) that governs1The observed writing of a mention is its identifying writing.For entities, it is a standard representation of them, i.e.
the fullname of a person.2Contextual attributes are not part of the current evaluation,and will be evaluated in the next step of this work.EEdRdMdeeidMidriddJohn Fitzgerald KennedyJohn Fitzgerald KennedyPresident John F. Kennedy{President Kennedy, Kennedy, JFK}House of RepresentativesHouse of RepresentativesHouse of Representatives{House of Representatives, The House}Figure 1: Generating a documenthow entities (of different types) are distributed into a doc-ument and reflects their co-occurrence dependencies.
(2) The number of entities in a document, size(Ed),and the number of mentions of each entity in Ed,size(Mdi ), need to be decided.
The current evaluationmakes the simplifying assumption that these numbers aredetermined uniformly over a small plausible range.
(3) The appearance probability of a name generated(transformed) from its representative is modelled as aproduct distribution over relational transformations of at-tribute values.
This model captures the similarity be-tween appearances of two names.
In the current eval-uation the same appearance model is used to calculateboth the probability P (r|e) that generates a representa-tive r given an entity e and the probability P (m|r) thatgenerates a mention m given a representative r. Attributetransformations are relational, in the sense that the dis-tribution is over transformation types and independent ofthe specific names.Given these, a document d is assumed to be gener-ated as follows (see Fig.
1): A set of size(Ed) entitiesEd ?
E is selected to appear in a document d, accord-ing to P (Ed).
For each entity edi ?
Ed, a representativerdi ?
R is chosen according to P (rdi |edi ), generating Rd.Then mentions Mdi of an entity are generated from eachrepresentative rdi ?
Rd ?
each mention mdj ?
Mdi isindependently transformed from rdi according to the ap-pearance probability P (mdj |rdi ).
Assuming conditionalindependency between Md and Ed given Rd, the proba-bility distribution over documents is thereforeP (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd),and the probability of the document collection D is:P (D) =?d?DP (d).Given a mention m in a document d (Md is the set ofobserved mentions in d), the key inference problem is todetermine the most likely entity e?m that corresponds toit.
This is done by computing:Ed = argmaxE?
?EP (Ed, Rd|Md, ?)
(1)= argmaxE?
?EP (Ed, Rd,Md|?
), (2)where ?
is the learned model?s parameters.
This gives theassignment of the most likely entity e?m for m.3.1 Relaxations of the ModelIn order to simplify model estimation and to evaluatesome assumptions, several relaxations are made to formthree simpler probabilistic models.Model I: (the simplest model) The key relaxation hereis in losing the notion of an ?author?
?
rather than firstchoosing a representative for each document, mentionsare generated independently and directly given an entity.That is, an entity ei is selected from E according to theprior probability P (ei); then its actual mention mi is se-lected according to P (mi|ei).
Also, an entity is selectedinto a document independently of other entities.
In thisway, the probability of the whole document set can becomputed simply as follows:P (D) = P ({(ei,mi)}ni=1) =n?i=1P (ei)P (mi|ei),and the inference problem for the most likely entity givenm is:e?m = argmaxe?EP (e|m, ?)
= argmaxe?EP (e)P (m|e).
(3)Model II: (more expressive) The major relaxationmade here is in assuming a simple model of choos-ing entities to appear in documents.
Thus, in order togenerate a document d, after we decide size(Ed) and{size(Md1 , size(Md2 ), .
.
. }
according to uniform distri-butions, each entity edi is selected into d independentlyof others according to P (edi ).
Next, the representative rdifor each entity edi is selected according to P (rdi |edi ) andfor each representative the actual mentions are selectedindependently according to P (mdj |rdj ).
Here, we have in-dividual documents along with representatives, and thedistribution over documents is:P (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd)?|Ed|?i=1[P (edi )P (rdi |edi )]?
(rdj ,mdj )P (mdj |rdj )after we ignore the size components (they do not influ-ence inferences).
The inference problem here is the sameas in Equ.
(2).Model III: This model performs the least relaxation.After deciding size(Ed) according to a uniform distri-bution, instead of assuming independency among enti-ties which does not hold in reality (For example, ?Gore?and ?George.
W. Bush?
occur together frequently, but?Gore?
and ?Steve.
Bush?
do not), we select entities us-ing a graph based algorithm: entities in E are viewedas nodes in a weighted directed graph with edges (i, j)labelled P (ej |ei) representing the probability that entityej is chosen into a document that contains entity ei.
Wedistribute entities to Ed via a random walk on this graphstarting from ed1 with a prior probability P (edi ).
Repre-sentatives and mentions are generated in the same wayas in Model II.
Therefore, a more general model for thedistribution over documents is:P (d) ?
P (ed1)P (rd1 |ed1)|Ed|?i=2[P (edi |edi?1)P (rdi |edi )]??
(rdj ,mdj )P (mdj |rdj ).The inference problem is the same as in Equ.
(2).3.2 Inference AlgorithmsThe fundamental problem in robust reading can be solvedas inference with the models: given a mention m, seek themost likely entity e ?
E for m according to Equ.
(3) forModel I or Equ.
(2) for Model II and III.
Instead of allentities in the real world, E can be viewed without lossas the set of entities in a closed document collection thatwe use to train the model parameters and it is known aftertraining.
The inference algorithm for Model I (with timecomplexity O(|E|)) is simple and direct: just computeP (e,m) for each candidate entity e ?
E and then choosethe one with the highest value.
Due to exponential num-ber of possible assignments of Ed, Rd to Md in ModelII and III, precise inference is infeasible and approximatealgorithms are therefore designed:In Model II, we adopt a two-step algorithm: First, weseek the representatives Rd for the mentions Md in docu-ment d by sequentially clustering the mentions accordingto the appearance model.
The first mention in each groupis chosen as the representative.
Specifically, when con-sidering a mention m ?
Md, P (m|r) is computed foreach representative r that have already been created anda fixed threshold is then used to decide whether to create anew group for m or to add it to one of the existing groupswith the largest P (m|r).
In the second step, each rep-resentative rdi ?
Rd is assigned to its most likely entityaccording to e?
= argmaxe?EP (e) ?P (r|e).
This algo-rithm has a time complexity of O((|Md|+ |E|) ?
|Md|).Model III has a similar algorithm as Model II.
Theonly difference is that we need to consider the globaldependency between entities.
Thus in the second step,instead of seeking an entity e for each representative rseparately, we determine a set of entities Ed for Rd ina Hidden Markov Model with entities in E as hiddenstates and Rd as observations.
The prior probabilities,the transitive probabilities and the observation probabil-ities are given by P (e), P (ej |ei) and P (r|e) respec-tively.
Here we seek the most likely sequence of enti-ties given those representatives in their appearing orderusing the Viterbi algorithm.
The total time complexity ise1= George Bush e2= George W. Bush e3= Steve Bushm1,r1=President Bushm2=Bushm4,r2=Steve Bushm5=Bushm3=J.
QuayleEntities Ed1 d2Figure 2: An conceptual example.
The arrows representthe correct assignment of entities to mentions.
r1, r2 arerepresentatives.O(|Md|2 + |E|2 ?
|Md|).
The |E|2 component can besimplified by filtering out unlikely entities for a represen-tative according to their appearance similarity.3.3 DiscussionBesides different assumptions, some fundamental differ-ences exist in inference with the models as well.
In ModelI, the entity of a mention is determined completely inde-pendently of other mentions, while in Model II, it relieson other mentions in the same document for clustering.In Model III, it is not only related to other mentions butto a global dependency over entities.
The following con-ceptual example illustrates those differences as in Fig.
2.Example 3.1 Given E = {George Bush, George W. Bush,Steve Bush}, documents d1, d2 and 5 mentions in them, andsuppose the prior probability of entity ?George W. Bush?
ishigher than those of the other two entities, the entity assign-ments to the five mentions in the models could be as follows:For Model I, mentions(e1) = ?, mentions(e2) ={m1,m2,m5} and mentions(e3) = {m4}.
The result iscaused by the fact that a mention tends to be assigned to theentity with higher prior probability when the appearance simi-larity is not distinctive.For Model II, mentions(e1) = ?, mentions(e2) ={m1,m2} and mentions(e3) = {m4,m5}.
Local depen-dency (appearance similarity) between mentions inside eachdocument enforces the constraint that they should refer to thesame entity, like ?Steve Bush?
and ?Bush?
in d2.For Model III, mentions(e1) = {m1,m2}, mentions(e2)= ?, mentions(e3) = {m4,m5}.
With the help of globaldependency between entities, for example, ?George Bush?
and?J.
Quayle?, an entity can be distinguished from another onewith a similar writing.3.4 Other TasksOther aspects of ?Robust Reading?
can be solved basedon the above inference problem.Entity Identity: Given two mentions m1 ?
d1,m2 ?
d2,determine whether they correspond to the same entity by:m1 ?
m2 ??
argmaxe?EP (e,m1) = argmaxe?EP (e,m2)for Model I andm1 ?
m2 ??
argmaxe?EP (Ed1 , Rd1 ,Md1) =argmaxe?EP (Ed2 , Rd2 ,Md2).for Model II and III.Name Expansion: Given a mention mq in a query q,decide whether mention m in the document collection Dis a ?legal?
expansion of mq:mq ?
m ??
e?mq = argmaxe?EP (Eq, Rq,Mq)& m ?
mentions(e?
).Here it?s assumed that we already know the possiblementions of e?
after training the models with D.Prominence: Given a name n ?
W , the most promi-nent entity for n is given by (P (e) is given by the priordistribution PE and P (n|e) is given by the appearancemodel.):e?
= argmaxe?EP (e)P (n|e).4 Learning the ModelsConfined by the labor of annotating data, we learn theprobabilistic models in an unsupervised way given a col-lection of documents; that is, the system is not told dur-ing training whether two mentions represent the same en-tity.
A greedy search algorithm modified after the stan-dard EM algorithm (We call it Truncated EM algorithm)is adopted here to avoid complex computation.Given a set of documents D to be studied and the ob-served mentions Md in each document, this algorithmiteratively updates the model parameter ?
(several under-lying probabilistic distributions described before) and thestructure (that is, Ed and Rd) of each document d. Dif-ferent from the standard EM algorithm, in the E-step, itseeks the most likely Ed and Rd for each document ratherthan the expected assignment.4.1 Truncated EM AlgorithmThe basic framework of the Truncated EM algorithm tolearn Model II and III is as follows:1.
In the initial (I-) step, an initial (Ed0 , Rd0) is assignedto each document d by an initialization algorithm.After this step, we can assume that the documentsare annotated with D0 = {(Ed0 , Rd0,Md)}.2.
In the M-step, we seek the model parameter ?t+1that maximizes P (Dt|?).
Given the ?labels?
sup-plied in the previous I- or E-step, this amounts to themaximum likelihood estimation.
(to be described inSec.
4.3).3.
In the E-step, we seek (Edt+1, Rdt+1) for eachdocument d that maximizes P (Dt+1|?t+1) whereDt+1 = {(Edt+1, Rdt+1,Md)}.
It?s the same infer-ence problem as in Sec.
3.2.4.
Stopping Criterion: If no increase is achieved overP (Dt|?t), the algorithm exits.
Otherwise the algo-rithm will iterate over the M-step and E-step.The algorithm for Model I is similar to the above one,but much simpler in the sense that it does not have the no-tions of documents and representatives.
So in the E-stepwe only seek the most likely entity e for each mentionm ?
D, and this simplifies the parameter estimation inthe M-step accordingly.
It usually takes 3?
10 iterationsbefore the algorithms stop in our experiments.4.2 InitializationThe purpose of the initial step is to acquire an initial guessof document structures and the set of entities E in a closedcollection of documents D. The hope is to find all entitieswithout loss so duplicate entities are allowed.
For all themodels, we use the same algorithm:A local clustering is performed to group mentions in-side each document: simple heuristics are applied to cal-culating the similarity between mentions; and pairs ofmentions with similarity above a threshold are then clus-tered together.
The first mention in each group is chosenas the representative (only in Model II and III) and anentity having the same writing with the representative iscreated for each cluster3.
For all the models, the set ofentities created in different documents become the globalentity set E in the following M- and E-steps.4.3 Estimating the Model ParametersIn the learning process, assuming documents have al-ready been annotated D = {(e, r,m)}n1 from previous I-or E-step, several underlying probability distributions ofthe relaxed models are estimated by maximum likelihoodestimation in each M-step.
The model parameters includea set of prior probabilities for entities PE , a set of tran-sitive probabilities for entity pairs PE|E (only in ModelIII) and the appearance probabilities PW |W of each namein the name space W being transformed from another.?
The prior distribution PE is modelled as a multi-nomial distribution.
Given a set of labelled entity-mention pairs {(ei,mi)}n1 ,P (e) = freq(e)nwhere freq(e) denotes the number of pairs containingentity e.?
Given all the entities appearing in D, the transitiveprobability P (e|e) is estimated byP (e2|e1) ?
P (wrt(e2)|wrt(e1)) = doc#(wrt(e2), wrt(e1))doc#(wrt(e1)) .Here, the conditional probability between two real-world entities P (e2|e1) is backed off to the one be-tween the identifying writings of the two entitiesP (wrt(e2)|wrt(e1)) in the document set D to avoid3Note that the performance of the initialization algorithm is97.3% precision and 10.1% recall (measures are defined later.
)sparsity problem.
doc#(w1, w2, ...) denotes the num-ber of documents having the co-occurrence of writingsw1, w2, ....?
Appearance probability, the probability of onename being transformed from another, denoted asP (n2|n1) (n1, n2 ?
W ), is modelled as a productof the transformation probabilities over attribute val-ues 4.
The transformation probability for each attributeis further modelled as a multi-nomial distribution overa set of predetermined transformation types: TT ={copy,missing, typical, non?
typical}5.Suppose n1 = (a1 = v1, a2 = v2, ..., ap = vp) andn2 = (a1 = v?1, a2 = v?2, ..., ap = v?p) are two names be-longing to the same entity type, the transformation prob-abilities PM |R, PR|E and PM |E , are all modelled as aproduct distribution (naive Bayes) over attributes:P (n2|n1) = ?pk=1P (v?k|vk).We manually collected typical and non-typical trans-formations for attributes such as titles, first names,last names, organizations and locations from multiplesources such as U.S. government census and online dic-tionaries.
For other attributes like gender, only copytransformation is allowed.
The maximum likelihood es-timation of the transformation probability P (t, k) (t ?TT, ak ?
A) from annotated representative-mentionpairs {(r,m)}n1 is:P (t, k) = freq(r,m) : vrk ?t vmkn (4)vrk ?t vmk denotes the transformation from attributeak of r to that of m is of type t. Simple smoothing isperformed here for unseen transformations.5 Experimental StudyOur experimental study focuses on (1) evaluating thethree models on identifying three entity types (Peo-ple, Locations, Organization); (2) comparing our in-duced similarity measure between names (the appearancemodel) with other similarity measures; (3) evaluating thecontribution of the global nature of our model, and fi-nally, (4) evaluating our models on name expansion andprominence ranking.5.1 MethodologyWe randomly selected 300 documents from 1998-2000New York Times articles in the TREC corpus (Voorhees,4The appearance probability can be modelled differently byusing other string similarity between names.
We will comparethe model described here with some other non-learning similar-ity metrics later.5copy denotes v?k is exactly the same as vk; missing denotes?missing value?
for v?k; typical denotes v?k is a typical variationof vk, for example, ?Prof.?
for ?Professor?, ?Andy?
for ?An-drew?
; non-typical denotes a non-typical transformation.2002).
The documents were annotated by a named entitytagger for People, Locations and Organizations.
The an-notation was then corrected and each name mention waslabelled with its corresponding entity by two annotators.In total, about 8, 000 mentions of named entities whichcorrespond to about 2, 000 entities were labelled.
Thetraining process gets to see only the 300 documents andextracts attribute values for each mention.
No supervisionis supplied.
These records are used to learn the proba-bilistic models.In the 64 million possible mention pairs, most are triv-ial non-matching one ?
the appearances of the two men-tions are very different.
Therefore, direct evaluation overall those pairs always get alost 100% accuracy in ourexperiments.
To avoid this, only the 130, 000 pairs ofmatching mentions that correspond to the same entity areused to evaluate the performance of the models.
Sincethe probabilistic models are learned in an unsupervisedsetting, testing can be viewed simply as the evaluation ofthe learned model, and is thus done on the same data.
Thesame setting was used for all models and all comparisonperformed (see below).To evaluate the performance, we pair two mentionsiff the learned model determined that they correspondto the same entity.
The list of predicted pairs is thencompared with the annotated pairs.
We measure Preci-sion (P ) ?
Percentage of correctly predicted pairs, Recall(R) ?
Percentage of correct pairs that were predicted, andF1 = 2PRP+R .Comparisons: The appearance model induces a ?simi-larity?
measure between names, which is estimated dur-ing the training process.
In order to understand whetherthe behavior of the generative model is dominated bythe quality of the induced pairwise similarity or by theglobal aspects (for example, inference with the aid ofthe document structure), we (1) replace this measure bytwo other ?local?
similarity measures, and (2) comparethree possible decision mechanisms ?
pairwise classifica-tion, straightforward clustering over local similarity, andour global model.
To obtain the similarity required bypairwise classification and clustering, we use this for-mula sima(n1, n2) = P (n1|n2) to convert the appear-ance probability described in Sec.
4.3 to it.The first similarity measure we use is a sim-ple baseline approach: two names are similar iffthey have identical writings (that is, simb(n1, n2) =1 if n1, n2 are identical or 0 otherwise).
The secondone is a state-of-art similarity measure sims(n1, n2) ?
[0, 1] for entity names (SoftTFIDF with Jaro-Winkler dis-tance and ?
= 0.9); it was ranked the best measure in arecent study (Cohen et al, 2003).Pairwise classification is done by pairing two men-tions iff the similarity between them is above a fixedthreshold.
For Clustering, a graph-based clustering al-All(P/L/O) Identity SoftTFIDF AppearancePairwise 70.7 (64.7/64.1/83.7) 82.1 (79.9/77.3/89.5) 81.5 (83.6/70.9/90.7)Clustering 70.7 (64.7/64.1/83.7) 79.8 (70.6/76.7/91.0) 79.6 (70.9/76.1/91.0)Model II 70.7 (64.7/64.1/83.7) 82.5 (79.8/77.4/90.2) 89.0 (92.7/81.9/92.9)Table 1: Comparison of different decision levels and sim-ilarity measures.
Three similarity measures are evaluated(rows) across three decision levels (columns).
Performance isevaluated by the F1 values over the whole test set.
The firstnumber averages all entity types; numbers in parentheses repre-sent People, Location and Organization respectively.gorithm is used.
Two nodes in the graph are connectedif the similarity between the corresponding mentions isabove a threshold.
In evaluation, any two mentions be-longing to the same connected component are paired thesame way as we did in Sec.
5.1 and all those pairs are thencompared with the annotated pairs to calculate Precision,Recall and F1.Finally, we evaluate the baseline and the SoftTFIDFmeasure in the context of Model II, where the appear-ance model is replaced.
We found that the probabil-ities directly converted from the SoftTFIDF similaritybehave badly so we adopt this formula P (n1|n2) =e10?sims(n1,n2)?1e10?1 instead to acquire P (n1|n2) needed byModel II.
Those probabilities are fixed as we estimateother model parameters in training.5.2 ResultsThe bottom line result is given in Tab.
1.
All the similaritymeasures are compared in the context of the three levelsof decisions ?
local decision (pairwise), clustering andour probabilistic model II.
Only the best results in theexperiments, achieved by trying different thresholds inpairwise classification and clustering, are shown.The behavior across rows indicates that, locally, ourunsupervised learning based appearance model is aboutthe same as the state-of-the-art SoftTFIDF similarity.
Thebehavior across columns, though, shows the contribu-tion of the global model, and that the local appearancemodel behaves better with it than a fixed similarity mea-sure does.
A second observation is that the Location ap-pearance model is not as good as the one for People andOrganization, probably due to the attribute transforma-tion types chosen.Tab.
2 presents a more detailed evaluation of the differ-ent approaches on the entity identity task.
All the threeprobabilistic models outperform the discriminatory ap-proaches in this experiment, an indication of the effec-tiveness of the generative model.We note that although Model III is more expressiveand reasonable than model II, it does not always performbetter.
Indeed, the global dependency among entities inModel III achieves two-folded outcomes: it achieves bet-ter precision, but may degrade the recall.
The followingexample, taken from the corpus, illustrates the advantageof this model.Entity Type Mod InDoc InterDoc AllF1(%) F1(%) R(%) P(%) F1(%)All Entities B 86.0 68.8 58.5 85.5 70.7D 86.5 78.9 66.4 95.8 79.8I 96.3 85.0 79.0 94.1 86.2II 96.5 88.1 85.9 92.2 89.0III 96.5 87.9 84.4 93.6 88.9People B 82.4 59.0 48.5 86.3 64.7D 82.4 67.1 54.5 91.5 70.6I 96.2 84.8 80.6 94.8 87.4II 96.4 91.7 94.0 91.5 92.7III 96.4 88.9 89.8 91.3 90.5Location B 88.8 63.0 54.8 75.0 64.1D 91.4 76.0 61.3 95.9 76.7I 92.9 78.9 70.9 89.1 79.5II 93.8 81.4 76.2 88.1 81.9III 93.8 82.8 76.0 91.2 83.3Organization B 95.3 82.8 72.6 96.4 83.7D 95.8 90.7 83.9 98.9 91.1I 98.8 91.8 86.5 98.5 92.3II 98.5 92.5 88.6 97.5 92.9III 98.8 93.0 88.5 98.6 93.4Table 2: Performance of different approaches over all testexamples.
B, D, I, II and III denote the baseline model, theSoftTFIDF similarity model with clustering, and the three prob-abilistic models.
We distinguish between pairs of mentions thatare inside the same document (InDoc, 15% of the pairs) or not(InterDoc).Example 5.1 ?Sherman Williams?
is mentioned along withthe baseball team ?Dallas Cowboys?
in 8 out of 300 documents,while ?Jeff Williams?
is mentioned along with ?LA Dodgers?in two documents.In all models but Model III, ?Jeff Williams?
is judged to cor-respond to the same entity as ?Sherman Williams?
since theirappearances are similar and the prior probability of the latter ishigher than the former.
Only Model III, due to the co-occurringdependency between ?Jeff Williams?
and ?Dodgers?, identi-fies it as corresponding to an entity different from ?ShermanWilliams?.While this shows that Model III achieves better preci-sion, the recall may go down.
The reason is that globaldependencies among entities enforces restrictions overpossible grouping of similar mentions; in addition, witha limited document set, estimating this global depen-dency is inaccurate, especially when the entities them-selves need to be found when training the model.Hard Cases: To analyze the experimental results further,we evaluated separately two types of harder cases of theentity identity task: (1) mentions with different writingsthat refer to the same entity; and (2) mentions with sim-ilar writings that refer to different entities.
Model II andIII outperform other models in those two cases as well.Tab.
3 presents F1 performance of different approachesin the first case.
The best F1 value is only 73.1%, indicat-ing that appearance similarity and global dependency arenot sufficient to solve this problem when the writings arevery different.
Tab.
4 shows the performance of differ-ent approaches for disambiguating similar writings thatcorrespond to different entities.Both these cases exhibit the difficulty of the problem,and that our approach provides a significant improvementover the state of the art similarity measure ?
column Dvs.
column II in Tab.
4.
It also shows that it is necessaryto use contextual attributes of the names, which are notyet included in this evaluation.Model B D I II IIIPeop 0 77.9 79.2 86.0 82.6Loc 0 30.4 55.1 58.5 61.5Org 0 77.7 69.5 71.7 71.2All 0 63.3 68.4 73.1 72.5Table 3: Identifying different writings of the same entity(F1).
We filter out identical writings and report only on casesof different writings of the same entity.
The test set contains46, 376 matching pairs (but in different writings) in the wholedata set.Model B D I II IIIPeop 75.2 83.0 60.8 89.7 88.0Loc 86.5 80.7 80.0 90.3 90.3Org 80.0 89.4 71.0 93.1 92.6All 78.7 78.9 68.1 90.7 89.7Table 4: Identifying similar writings of differententities(F1).
The test set contains 39, 837 pairs of mentionsthat associated with different entities in the 300 documents andhave at least one token in common.5.3 Other TasksIn the following experiments, we evaluate the genera-tive model on other tasks related to robust reading.
Wepresent results only for Model II, the best one in previousexperiments.Name Expansion: Given a mention m in a query, we findthe most likely entity e ?
E for m using the inference al-gorithm as described in Sec.
3.2.
All unique mentions ofthe entity in the documents are output as the expansionsof m. The accuracy for a given mention is defined as thepercentage of correct expansions output by the system.The average accuracy of name expansion of Model II isshown in Tab.
5.
Here is an example:Query: Who is Gore ?Expansions: Vice President Al Gore, Al Gore, Gore.Prominence Ranking: We refer to Example 3.1 and useit to exemplify quantitatively how our system supportsprominence ranking.
Given a query name n, the rankingof the entities with regard to the value of P (e) ?
P (n|e)(shown in brackets) by Model II is as follows.Input: George Bush1.
George Bush (0.0448) 2.
George W. Bush (0.0058)Input: Bush1.
George W. Bush (0.0047) 2.
George Bush (0.0015)3.
Steve Bush (0.0002)6 Conclusion and Future WorkThis paper presents an unsupervised learning approach toseveral aspects of the ?robust reading?
problem ?
cross-document identification and tracing of ambiguous names.We developed a model that describes the natural gen-eration process of a document and the process of howEntity Type People Location OrganizationAccuracy(%) 90.6 100 100Table 5: Accuracy of name expansion.
Accuracy is averagedover 30 randomly chosen queries for each entity type.names are ?sprinkled?
into them, taking into account de-pendencies between entities across types and an ?author?model.
Several relaxations of this model were developedand studied experimentally, and compared with a state-of-the-art discriminative model that does not take a globalview.
The experiments exhibit encouraging results andthe advantages of our model.This work is a preliminary exploration of the robustreading problem.
There are several critical issues that ourmodel can support, but were not included in this prelimi-nary evaluation.
Some of the issues that will be includedin future steps are: (1) integration with more contextualinformation (like time and place) related to the target enti-ties, both to support a better model and to allow temporaltracing of entities; (2) studying an incremental approachof training the model; that is, when a new document isobserved, coming, how to update existing model param-eters ?
(3) integration of this work with other aspects ofgeneral coreference resolution (e.g., other terms like pro-nouns that refer to an entity) and named entity recognition(which we now take as given); and (4) scalability issuesin applying the system to large corpora.AcknowledgmentsThis research is supported by NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-9984168 and anONR MURI Award.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross-documentcoreferencing using the vector space model.
In ACL.M.
Bilenko and R. Mooney.
2003.
Adaptive duplicate detectionusing learnable string similarity measures.
In KDD.E.
Charniak.
2001.
Unsupervised learning of name structurefrom coreference datal.
In NAACL.W.
Cohen and J. Richman.
2002.
Learning to match and clus-ter large high-dimensional data sets for data integration.
InKDD.W.
Cohen, P. Ravikumar, and S. Fienberg.
2003.
A comparisonof string metrics for name-matching tasks.
In IIWeb Work-shop 2003.M.
Hernandez and S. Stolfo.
1995.
The merge/purge problemfor large databases.
In SIGMOD.A.
Kehler.
2002.
Coherence, Reference, and the Theory ofGrammar.
CSLI Publications.G.
Mann and D. Yarowsky.
2003.
Unsupervised personal namedisambiguation.
In CoNLL.V.
Ng and C. Cardie.
2003.
Improving machine learning ap-proaches to coreference resolution.
In ACL.H.
Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser.2002.
Identity uncertainty and citation matching.
In NIPS.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.
Computa-tional Linguistics (Special Issue on Computational AnaphoraResolution), 27:521?544.E.
Voorhees.
2002.
Overview of the TREC-2002 question an-swering track.
In Proceedings of TREC, pages 115?123.
