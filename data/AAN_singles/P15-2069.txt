Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 419?424,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsTR9856: A Multi-word Term Relatedness BenchmarkRan Levy and Liat Ein-Dor and Shay Hummel and Ruty Rinott and Noam SlonimIBM Haifa Research Lab, Mount Carmel, Haifa, 31905, Israel{ranl,liate,shayh,rutyr,noams}@il.ibm.comAbstractMeasuring word relatedness is an impor-tant ingredient of many NLP applications.Several datasets have been developed inorder to evaluate such measures.
The maindrawback of existing datasets is the fo-cus on single words, although natural lan-guage contains a large proportion of multi-word terms.
We propose the new TR9856dataset which focuses on multi-word termsand is significantly larger than existingdatasets.
The new dataset includes manyreal world terms such as acronyms andnamed entities, and further handles termambiguity by providing topical contextfor all term pairs.
We report baselineresults for common relatedness methodsover the new data, and exploit its magni-tude to demonstrate that a combination ofthese methods outperforms each individ-ual method.1 IntroductionMany NLP applications share the need to deter-mine whether two terms are semantically related,or to quantify their degree of ?relatedness?.
De-veloping methods to automatically quantify termrelatedness naturally requires benchmark data ofterm pairs with corresponding human relatednessscores.
Here, we propose a novel benchmark datafor term relatedness, that addresses several chal-lenges which have not been addressed by previ-ously available data.
The new benchmark datais the first to consider relatedness between multi?word terms, allowing to gain better insights re-garding the performance of relatedness assessmentmethods when considering such terms.
Second, incontrast to most previous data, the new data pro-vides a context for each pair of terms, allowing todisambiguate terms as needed.
Third, we use asimple systematic process to ensure that the con-structed data is enriched with ?related?
pairs, be-yond what one would expect to obtain by randomsampling.
In contrast to previous work, our en-richment process does not rely on a particular re-latedness algorithm or resource such as Wordnet(Fellbaum, 1998), hence the constructed data isless biased in favor of a specific method.
Finally,the new data triples the size of the largest previ-ously available data, consisting of 9, 856 pairs ofterms.
Correspondingly, it is denoted henceforthas TR9856.
Each term pair was annotated by 10human annotators, answering a binary question ?related/unrelated.
The relatedness score is given asthe mean answer of annotators where related = 1and unrelated = 0.We report various consistency measures thatindicate the validity of TR9856.
In addition,we report baseline results over TR9856 for sev-eral methods, commonly used to assess term?relatedness.
Furthermore, we demonstrate how thenew data can be exploited to train an ensemble?based method, that relies on these methods as un-derlying features.
We believe that the new TR9856benchmark, which is freely available for researchpurposes,1along with the reported results, willcontribute to the development of novel term relat-edness methods.2 Related workAssessing the relatedness between single wordsis a well known task which received substantialattention from the scientific community.
Corre-spondingly, several benchmark datasets exist.
Pre-sumably the most popular among these is theWordSimilarity-353 collection (Finkelstein et al,2002), covering 353 word pairs, each labeled by13?16 human annotators, that selected a continu-ous relatedness score in the range 0-10.
These hu-1https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml419man results were averaged, to obtain a relatednessscore for each pair.
Other relatively small datasetsinclude (Radinsky et al, 2011; Halawi et al, 2012;Hill et al, 2014).A larger dataset is Stanford?s Contextual WordSimilarities dataset, denoted SCWS (Huang et al,2012) with 2,003 word pairs, where each word ap-pears in the context of a specific sentence.
The au-thors rely on Wordnet (Fellbaum, 1998) for choos-ing a diverse set of words as well as to enrichthe dataset with related pairs.
A more recentdataset, denoted MEN (Bruni et al, 2014) con-sists of 3,000 word pairs, where a specific relat-edness measure was used to enrich the data withrelated pairs.
Thus, these two larger datasets arepotentially biased in favor of the relatedness al-gorithm or lexical resource used in their devel-opment.
TR9856 is much larger and potentiallyless biased than all these previously available data.Hence, it allows to draw more reliable conclu-sions regarding the quality and characteristics ofexamined methods.
Moreover, it opens the doorfor developing term relatedness methods withinthe supervised machine learning paradigm as wedemonstrate in Section 5.2.It is also worth mentioning the existence of re-lated datasets, constructed with more specific NLPtasks in mind.
For examples, datasets constructedto assess lexical entailment (Mirkin et al, 2009)and lexical substitution (McCarthy and Navigli,2009; Kremer et al, 2014; Biemann, 2013) meth-ods.
However, the focus of the current work ison the more general notion of term?relatedness,which seems to go beyond these more concrete re-lations.
For example, the words whale and oceanare related, but are not similar, do not entail oneanother, and can not properly substitute one an-other in a given text.3 Dataset generation methodologyIn constructing the TR9856 data we aimed to ad-dress the following issues: (i) include terms thatinvolve more than a single word; (ii) disambiguateterms, as needed; (iii) have a relatively high frac-tion of ?related?
term pairs; (iv) focus on termsthat are relatively common as opposed to eso-teric terms; (v) generate a relatively large bench-mark data.
To achieve these goals we defined andfollowed a systematic and reproducible protocol,which is described next.
The complete details areincluded in the data release notes.3.1 Defining topics and articles of interestWe start by observing that framing the related-ness question within a pre-specified context maysimplify the task for humans and machines alike,in particular since the correct sense of ambigu-ous terms can be identified.
Correspondingly,we focus on 47 topics selected from Debatabase2.
For each topic, 5 human annotators searchedWikipedia for relevant articles as done in (Aharoniet al, 2014).
All articles returned by the annota-tors ?
an average of 21 articles per topic ?
wereconsidered in the following steps.
The expectationwas that articles associated with a particular topicwill be enriched with terms related to that topic,hence with terms related to one another.3.2 Identifying dominant terms per topicIn order to create a set of terms related to a topic ofinterest, we used the Hyper-geometric (HG) test.Specifically, given the number of sentences in theunion of articles identified for all topics; the num-ber of sentences in the articles identified for a spe-cific topic, i.e., in the topic articles; the total num-ber of sentences that include a particular term, t;and the number of sentences within the topic ar-ticles, that include t, denoted x; we use the HGtest to assess the probability p, to observe ?
xoccurrences of t within sentences selected at ran-dom out of the total population of sentences.
Thesmaller p is, the higher our confidence that t is re-lated to the examined topic.
Using this approach,for each topic we identify all n?gram terms, withn = 1, 2, 3 , with a p-value ?
0.05, after applyingBonfferroni correction.
We refer to this collectionof n?gram terms as the topic lexicon and refer ton?gram terms as n?terms.3.3 Selecting pairs for annotationFor each topic, we define Sdefas the set of manu-ally identified terms mentioned in the topic def-inition.
E.g., for the topic ?The use of per-formance enhancing drugs in professional sportsshould be permitted?, Sdef= {?performance en-hancing drugs?,?professional sports?}.
Given thetopic lexicon, we anticipate that terms with a smallp?value will be highly related to terms in Sdef.Hence, we define Stop,nto include the top 10 n?terms in the topic lexicon, and add to the datasetall pairs in Sdef?Stop,nfor n = 1, 2, 3.
Similarly,we define Smisc,nto include an additional set of 102http://idebate.org/debatabase420n?terms, selected at random from the remainingterms in the topic lexicon, and add to the datasetall pairs in Sdef?
Smisc,n.
We expect that the av-erage relatedness observed for these pairs will besomewhat lower.
Finally, we add to the dataset60 ?
|Sdef| pairs ?
i.e., the same number of pairsselected in the two previous steps ?
selected at ran-dom from ?n,mStop,n?
Smisc,m.
We expect thatthe average relatedness observed for this last set ofpairs will be even lower.3.4 Relatedness labeling guidelinesEach annotator was asked to mark a pair of termsas ?related?, if she/he believes there is an imme-diate associative connection between them, and as?unrelated?
otherwise.
Although ?relatedness?
isclearly a vague notion, in accord with previouswork ?
e.g., (Finkelstein et al, 2002), we assumedthat human judgments relying on simple intuitionwill nevertheless provide reliable and reproducibleestimates.
As discussed in section 4, our resultsconfirm this assumption.The annotators were further instructed to con-sider antonyms as related, and to use resourcessuch as Wikipedia to confirm their understandingregarding terms they are less familiar with.
Fi-nally, the annotators were asked to disambiguateterms as needed, based on the pair?s associatedtopic.
The complete labeling guidelines are avail-able as part of the data release.We note that in previous work, given a pair ofwords, the annotators were typically asked to de-termine a relatedness score within the range of 0to 10.
Here, we took a simpler approach, askingthe annotators to answer a binary related/unrelatedquestion.
To confirm that this approach yields sim-ilar results to previous work we asked 10 annota-tors to re-label the WS353 data using our guide-lines ?
except for the context part.
Comparing themean binary score obtained via this re-labeling tothe original scores provided for these data we ob-serve a Spearman correlation of 0.87, suggestingthat both approaches yield fairly similar results.4 The TR9856 data ?
details andvalidationThe procedure described above led to a collec-tion of 9, 856 pairs of terms, each associated withone out of the 47 examined topics.
Out of thesepairs, 1, 489 were comprised of single word terms(SWT) and 8, 367 were comprised of at least onemulti-word term (MWT).
Each pair was labeledby 10 annotators that worked independently.
Thebinary answers of the annotators were averaged,yielding a relatedness score between 0 to 1 ?
de-noted henceforth as the data score.Using the notations above, pairs from Sdef?Stop,nhad an average data score of 0.66; pairsfrom Sdef?
Smisc,nhad an average data scoreof 0.51; and pairs from Stop,n?
Smisc,mhad anaverage relatedness score of 0.41.
These resultssuggest that the intuition behind the pair selectionprocedure described in Section 3.3 is correct.
Wefurther notice that 31% of the labeled pairs hada relatedness score ?
0.8, and 33% of the pairshad a relatedness score ?
0.2, suggesting the con-structed data indeed includes a relatively high frac-tion of pairs with related terms, as planned.To evaluate annotator agreement we followed(Halawi et al, 2012; Snow et al, 2008) and di-vided the annotators into two equally sized groupsand measured the correlation between the resultsof each group.
The largest subset of pairs forwhich the same 10 annotators labeled all pairscontained roughly 2,900 pairs.
On this subset, weconsidered all possible splits of the annotators togroups of size 5, and for each split measured thecorrelation of the relatedness scores obtained bythe two groups.
The average Pearson correlationwas 0.80.
These results indicate that in spite of theadmitted vagueness of the task, the average anno-tation score obtained by different sets of annota-tors is relatively stable and consistent.Several examples of term pairs and their corre-sponding dataset scores are given in Table 1.
Notethat the first pair includes an acronym ?
wipo ?which the annotators are expected to resolve toWorld Intellectual Property Organization.4.1 Transitivity analysisAnother way to evaluate the quality and consis-tency of a term relatedness dataset is by measur-ing the transitivity of its relatedness scores.
Givena triplet of term pairs (a, b) , (b, c) and (a, c), thetransitivity rule implies that if a is related to b,and b is related to c then a is related to c. Usingthis rule, transitivity can be measured by comput-ing the relative fraction of pair triplets fulfilling it.Note that this analysis can be applied only if allthe three pairs exist in the data.
Here, we used thefollowing intuitive transitivity measure: let (a, b),(b, c), and (a, c), be a triplet of term pairs in the421Term 1 Term 2 Scorecopyright wipo 1.0grand theftautoviolent videogames1.0video gamessalesviolent videogames0.7civil rights affirmativeaction0.6rights public prop-erty0.5nation of is-lamaffirmativeaction0.1racial sex discrimi-nation0.1Table 1: Examples of pairs of terms and their as-sociated dataset scores.dataset, and let R1, R2, and R3be their related-ness scores, respectively.
Then, for high valuesof R2, R1is expected to be close to R3.
Morespecifically, on average, |R3?
R1| is expected todecrease with R2.
Figure 1 shows that this behav-ior indeed takes place in our dataset.
The p-valueof the correlation between mean(|R3?
R1|) andR2is ?
1e ?
10.
Nevertheless, the curves of theWS353 data (both with the original labeling andwith our labeling) do not show this behavior, prob-ably due to the very few triplet term pairs existingin these data, resulting with a very poor statistics.Besides validating the transitivity behavior, theseresults emphasize the advantage of the relativelydense TR9856 data, in providing sufficient statis-tics for performing this type of analysis.Figure 1: mean(|R3?R1|) vs. R2.5 Results for existing techniquesTo demonstrate the usability of the new TR9856data, we present baseline results of commonlyused methods that can be exploited to predictterm relatedness, including ESA (Gabrilovich andMarkovitch, 2007), Word2Vec (W2V) (Mikolovet al, 2013) and first?order positive PMI (PMI)(Church and Hanks, 1990).
To handle MWTs, weused summation on the vector representations ofW2V and ESA.
For PMI, we tokenized each MWTand averaged the PMI of all possible single?wordpairs.
For all these methods we used the March2015 Wikipedia dump and a relatively standardconfiguration of the relevant parameters.
In ad-dition, we report results for an ensemble of thesemethods using 10-fold cross validation.5.1 Evaluation measuresPrevious experiments on WS353 and otherdatasets reported Spearman Correlation (?)
be-tween the algorithm predicted scores and theground?truth relatedness scores.
Here, we alsoreport Pearson Correlation (r) results and demon-strate that the top performing algorithm becomesthe worst performing algorithm when switchingbetween these two correlation measures.
In ad-dition, we note that a correlation measure givesequal weight to all pairs in the dataset.
How-ever, in some NLP applications it is more impor-tant to properly distinguish related pairs from un-related ones.
Correspondingly, we also report re-sults when considering the problem as a binaryclassification problem, aiming to distinguish pairswith a relatedness score ?
0.8 from pairs with arelatedness score ?
0.2.5.2 Correlation resultsThe results of the examined methods are summa-rized in Table 2.
Note that these methods are notdesigned for multi-word terms, and further do notexploit the topic associated with each pair for dis-ambiguation.
The results show that all methodsare comparable except for ESA in terms of Pear-son correlation, which is much lower.
This suggestthat ESA scores are not well scaled, a property thatmight affect applications using ESA as a feature.Next, we exploit the relatively large size ofTR9856 to demonstrate the potential for using su-pervised machine learning methods.
Specifically,we trained a simple linear regression using thebaseline methods as features, along with a token422Method r ?ESA 0.43 0.59W2V 0.57 0.56PMI 0.55 0.58Table 2: Baseline results for common methods.length feature, that counts the combined numberof tokens per pair, in a 10-fold cross validationsetup.
The resulting model outperforms all indi-vidual methods, as depicted in Table 3.Method r ?ESA 0.43 0.59W2V 0.57 0.56PMI 0.55 0.58Lin.
Reg.
0.62 0.63Table 3: Mean results over 10-fold cross valida-tion.5.3 Single words vs. multi-wordsTo better understand the impact of MWTs, we di-vided the data into two subsets.
If both termsare SWTs the pair was assigned to the SWP sub-set; otherwise it was assigned to the MWP sub-set.
The SWP subset included 1, 489 pairs and theMWP subset comprised of 8, 367 pairs.
The ex-periment in subsection 5.2 was repeated for eachsubset.
The results are summarized in Table 4.
Ex-cept for the Pearson correlation results of ESA, forall methods we observe lower performance overthe MWP subset, suggesting that assessing term?relatedness is indeed more difficult when MWTsare involved.Method r ?SWP MWP SWP MWPESA 0.41 0.43 0.63 0.58W2V 0.62 0.55 0.58 0.55PMI 0.63 0.55 0.63 0.59Table 4: Baseline results for SWP vs. MWP.5.4 Binary classification resultsWe turn the task into binary classification taskby considering the 3, 090 pairs with a data score?
0.8 as positive examples, and the 3, 245 pairswith a data score ?
0.2 as negative examples.
Weuse a 10-fold cross validation to choose an opti-mal threshold for the baseline methods as well asto learn a Logistic Regression (LR) classifier, thatfurther used the token length feature.
Again, theresulting model outperforms all individual meth-ods, as indicated in Table 5.Method Mean ErrorESA 0.19W2V 0.22PMI 0.21Log.
Reg.
0.18Table 5: Binary classification results.6 DiscussionThe new TR9856 dataset has several important ad-vantages compared to previous datasets.
Most im-portantly ?
it is the first dataset to consider the re-latedness between multi?word terms; ambiguousterms can be resolved using a pre?specified con-text; and the data itself is much larger than previ-ously available data, enabling to draw more reli-able conclusions, and to develop supervised ma-chine learning methods that exploit parts of thedata for training and tuning.The baseline results reported here for com-monly used techniques provide initial intrigu-ing insights.
Table 4 suggests that the perfor-mance of specific methods may change substan-tially when considering pairs composed of uni-grams vs. pairs in which at least one term is aMWT.
Finally, our results demonstrate the poten-tial of supervised?learning techniques to outper-form individual methods, by using these methodsas underlying features.In future work we intend to further investigatethe notion of term relatedness by manually label-ing the type of the relation identified for highly re-lated pairs.
In addition, we intend to develop tech-niques that aim to exploit the context provided foreach pair, and to consider the potential of more ad-vanced ?
and in particular non?linear ?
supervisedlearning methods.AcknowledgmentsThe authors thank Ido Dagan and Mitesh Khaprafor many helpful discussions.ReferencesEhud Aharoni, Anatoly Polnarov, Tamar Lavee, DanielHershcovich, Ran Levy, Ruty Rinott, Dan Gutfre-423und, and Noam Slonim.
2014.
A benchmark datasetfor automatic detection of claims and evidence in thecontext of controversial topics.
ACL 2014, page 64.Chris Biemann.
2013.
Creating a system for lexi-cal substitutions from scratch using crowdsourcing.Language Resources and Evaluation, 47(1):97?122.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
J. Ar-tif.
Intell.
Res.
(JAIR), 49:1?47.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Christiane Fellbaum.
1998.
WordNet.
Wiley OnlineLibrary.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In IJCAI, vol-ume 7, pages 1606?1611.Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, andYehuda Koren.
2012.
Large-scale learning ofword relatedness with constraints.
In Proceedings ofthe 18th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1406?1414.
ACM.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Annual Meeting of the Associationfor Computational Linguistics (ACL).Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and Ste-fan Thater.
2014.
What substitutes tell us - anal-ysis of an ?all-words?
lexical substitution corpus.In Proceedings of the 14th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 540?549, Gothenburg, Sweden,April.
Association for Computational Linguistics.Diana McCarthy and Roberto Navigli.
2009.
The en-glish lexical substitution task.
Language resourcesand evaluation, 43(2):139?159.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Shachar Mirkin, Ido Dagan, and Eyal Shnarch.
2009.Evaluating the inferential utility of lexical-semanticresources.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 558?566.
Associationfor Computational Linguistics.Kira Radinsky, Eugene Agichtein, EvgeniyGabrilovich, and Shaul Markovitch.
2011.
Aword at a time: computing word relatedness usingtemporal semantic analysis.
In Proceedings of the20th international conference on World wide web,pages 337?346.
ACM.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the conferenceon empirical methods in natural language process-ing, pages 254?263.
Association for ComputationalLinguistics.424
