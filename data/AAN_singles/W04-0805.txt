The Italian Lexical Sample Task at SENSEVAL-3Bernardo Magnini, Danilo Giampiccolo and Alessandro VallinITC-Irst, Istituto per la Ricerca Scientifica e TecnologicaVia Sommarive, 18 ?
38050 Trento, Italy{magnini, giampiccolo, vallin}@itc.itAbstractThe Italian lexical sample task atSENSEVAL-3 provided a framework toevaluate supervised and semi-supervisedWSD systems.
This paper reports on thetask preparation ?
which offered the op-portunity to review and refine the ItalianMultiWordNet ?
and on the results of thesix participants, focussing on both themanual and automatic tagging procedures.1 IntroductionThe task consisted in automatically determiningthe correct meaning of a word within a given con-text (i.e.
a short text snippet).
Systems?
resultswere compared on the one hand to those achievedby human annotators (upper bound), and on theother hand to those returned by a basic algorithm(baseline).In the second section of this paper an overviewof the task preparation is given and in the follow-ing one the main features of the participating sys-tems are briefly outlined and the results of theevaluation exercise are presented.In the conclusions we give an overall judgementof the outcome of the task, suggesting possible im-provements for the next campaign.2 Manual AnnotationA collection of manually labeled instances wasbuilt for three main reasons:1. automatic evaluation (using the Scorer2 pro-gram) required a Gold Standard list of sensesprovided by human annotators;2. supervised WSD systems need a labeled set oftraining data, that in our case was twice largerthan the test set;3. manual semantic annotation is a time-consuming activity, but SENSEVAL repre-sents the framework to build reusable bench-mark resources.
Besides, manual sense taggingentails the revision of the sense inventory,whose granularity does not always satisfy an-notators.2.1 Corpus and Words ChoiceThe document collection from which the anno-tators selected the text snippets containing thelemmata to disambiguate was the macro-balancedsection of the Meaning Italian Corpus (Bentivogliet al, 2003).
This corpus is an open domain col-lection of newspaper articles that contains about 90million tokens covering a time-spam of 4 years(1998-2001).
The corpus was indexed in order tobrowse it with the Toolbox for Lexicographers(Giuliano, 2002), a concordancer that enables tag-gers to highlight the occurrences of a token withina context.Two taggers chose 45 lexical entries (25 nouns,10 adjectives and 10 verbs) according to theirpolysemy in the sense inventory, their polysemy inthe corpus and their frequency (Edmonds, 2000).The words that had already been used atSENSEVAL-2 were avoided.
Ten words wereshared with the Spanish, Catalan and Basque lexi-cal sample tasks.Annotators were provided with a formula thatindicated the number of labeled instances for eachlemma1, so they checked that the words were con-1 No.
of labeled instances for each lemma = 75 + (15*no.
of attested senses) +(7* no.
of attested multiwords), where 75 is a fixed number of examples distrib-uted over all the attested senses.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemssiderably frequent and polysemous before startingto tag and save the instances.As a result, average polysemy attested in the la-beled data turned out to be quite high: six sensesfor the nouns, six for the adjectives and seven forthe verbs.2.2 Sense Inventory and Manual TaggingDifferently from the Italian lexical sample taskat SENSEVAL-2, where the instances were taggedaccording to ItalWordNet (Calzolari et al, 2002),this year annotators used the Italian MultiWord-Net, (hereafter MWN) developed at ITC-Irst (Pi-anta, 2002).
This lexical-semantic databaseincludes about 42,000 lemmata and 60,000 wordsenses, corresponding to 34,000 synsets.
Instead ofdistributing to participants the senses of eachlemma and a limited hierarchical data structure ofthe semantic relations of the senses (as happened atSENSEVAL-2), the entire resource was madeavailable.
Nevertheless, none of the six participat-ing systems, being supervised, actually neededMWN.The annotators?
task was to tag one occurrenceof each selected word in all the saved instances,assigning only one sense drawn from the ItalianMWN.
The Toolbox for Lexicographers enabledannotators to browse the document collection andto save the relevant text snippets, while a graphicalinterface2 was used to annotate the occurrences,storing them in a database.
Generally, instancesconsisted of the sentence containing the ambiguouslemma, with a preceding and a following sentence.Nevertheless, annotators tended to save the mini-mal piece of information that a human would needto disambiguate the lemma, which was oftenshorter than three sentences.The two annotators were involved simultane-ously: firstly, each of them saved a part of the in-stances and tagged the occurrences, secondly theytagged the examples that had been chosen by theother one.More importantly, they interacted with a lexi-cographer, who reviewed the sense inventorywhenever they encountered difficulties.
Sometimesthere was an overlap between two or more wordsenses, while in other cases MWN needed to beenriched, adding new synsets, relations or defini-2 This tool was designed and developed by Christian Girardi at ITC-Irst, Trento,Italy.tions.
All the 45 lexical entries we considered werethoroughly reviewed, so that word senses were asclear as possible to the annotators.
On the onehand, the revision of MWN made manual taggingeasier, while on the other hand it led to a high InterTagger Agreement (that ranged between 73 and 99per cent), consequently reflected in the K statistics(that ranged between 0.68 and 0.99).Table 1 below summarizes the results of themanual tagging.Table 1.
Manual Annotation ResultsOnce the instances had been collected andtagged by both the annotators, we asked them todiscuss the examples about which they disagreedand to find a definitive meaning for them.Since the annotators built the corpus while tag-ging, they tended to choose occurrences whosemeaning was immediately straightforward, avoid-ing problematic cases.
As a consequence, the ITAturned out to be so high and the distribution of thesenses in the labeled data set did not reflect theactual frequency in the Italian language, whichmay have affected the systems?
performance.Annotators assigned different senses to 674 in-stances over a total of 7584 labeled examples.Generally, disagreement depended on trivial mis-takes, and in most cases one of the two assignedmeanings was chosen as the final one.
Neverthe-less, in 46 cases the third and last annotation wasdifferent from the previous two, which could dem-onstrate that a few word senses were not com-pletely straightforward even after the revision ofthe sense inventory.For example, the following instance for thelemma ?vertice?
(vertex, acme, peak) was anno-tated in three different ways:La struttura lavorativa ?
spiega Grandi ?
ha un carattere paramilita-re.
Al vertice della piramide c??
il direttore, poi i manager, quelli conla cravatta e la camicia a mezze maniche.Annotator 1 tagged with sense 2 (Factotum,?the highest point of something?
), while annotator2 decided for sense 4 (Geometry, ?the point of in-Averagepolysemyin MWNAveragepolysemy inthe labeled setI.T.A.AverageK# trainingexamples# testexamples25 nouns 10 6 0.9 2835 134310 adjectives 8 6 0.89 1111 52410 verbs 9 7 0.89 1199 572tersection of lines or the point opposite the base ofa figure?)
because the text refers to the vertex of apyramid.
Actually, the snippet reported this ab-stract image to describe the structure of an enter-prise, so in the end the two taggers opted for sense5 (Administration, ?the group of the executives ofa corporation?).
Therefore, subjectivity in manualtagging was considerably reduced by adjusting thesense repository and selecting manually each sin-gle instance, but it could not be eliminated.3 Automatic AnnotationWe provided participants with three data sets:labeled training data (twice larger than the test set),unlabeled training data (about 10 times the labeledinstances) and test data.
In order to facilitate par-ticipation, we PoS-tagged the labeled data sets us-ing an Italian version of the TnT PoS-tagger(Brants, 2000), trained on the Elsnet corpus.3.1 Participants?
resultsThree groups participated in the Italian lexicalsample task, testing six systems: two developed byITC-Irst - Italy - (IRST-Kernels and IRST-Ties),three by Swarthmore College - U.S.A. - (swat-hk-italian, Italian-swat_hk-bo and swat-italian) andone by UNED - Spain.Table 2 below reports the participants?
results,sorted by F-measure.system precision recall attempted F-measureIRST-Kernels 0.531 0.531 100% 0.531swat-hk-italian 0.515 0.515 100% 0.515UNED 0.498 0.498 100% 0.498italian-swat_hk-bo 0.483 0.483 100% 0.483swat-italian 0.465 0.465 100% 0.465IRST-Ties 0.552 0.309 55.92% 0.396baseline 0.183 0.183 100% 0.183Table 2.
Automatic Annotation Results (fine-grained score)The baseline results were obtained running a sim-ple algorithm that assigned to the instances of thetest set the most frequent sense of each lemma inthe training set.
All the systems outperformed thebaseline and obtained similar results.
Compared tothe baseline of the other Lexical Sample tasks, oursis much lower because we interpreted the formuladescribed above (see footnote 1), and tagged thesame number of instances for all the senses of eachlemma disregarding their frequency in the docu-ment collection.
As a result, the distribution of theexamples over the attested senses did not reflectthe one in natural language, which may have af-fected the systems?
performance.While at SENSEVAL-2 test set senses wereclustered in order to compute mixed- and coarse-grained scores, this year we decided to return justthe fine-grained measure, where an automaticallytagged instance is correct only if the sense corre-sponds to the one assigned by humans, and wrongotherwise (i.e.
one-to-one mapping).There are different sense clustering methods,but grouping meanings according to some sort ofsimilarity is always an arbitrary decision.
We in-tended to calculate a domain-based coarse-grainedscore, where word senses were clustered accordingto the domain information provided in WordNetDomains (Magnini and Cavagli?, 2000).
Unfortu-nately, this approach would have been significantwith nouns, but not with adjectives and verbs, thatbelong mostly to the generic Factotum domain, sowe discarded the idea.All the six participating systems were super-vised, which means they all used the training dataset and no one utilized either unlabelled instancesor the lexical database.
UNED used also SemCoras an additional source of training examples.IRST-Kernels system exploited Kernel methodsfor pattern abstraction and combination of differentknowledge sources, in particular paradigmatic andsyntagmatic information, and achieved the best F-measure score.IRST-Ties, a generalized pattern abstractionsystem originally developed for Information Ex-traction tasks and mainly based on the boostedwrapper induction algorithm, used only lemma andPOS as features.
Proposed as a ?baseline?
systemto discover syntagmatic patterns, it obtained a quitelow recall (about 55 per cent), which affected theF-measure, but proved to be the most precise sys-tem.Swarthmore College wrote three supervisedclassifiers: a clustering system based on cosinesimilarity, a decision list system and a naive bayesclassifier.
Besides, Swarthmore group took advan-tage of two systems developed at the Hong KongPolytechnic University: a maximum entropy classi-fier and system which used boosting (Italian-swat_hk-bo).
The run swat-hk-italian joined all thefive classifiers according to a simple majority-votescheme, while swat-hk-italian did the same usingonly the three classifiers developed in Swarthmore.The system presented by the UNED group em-ployed similarity as a learning paradigm, consid-ering the co-occurrence of different nouns andadjectives.3.2 General Remarks on Task ComplexityAs we mentioned above, the 45 words for theItalian lexical sample task were chosen accordingto their polysemy and frequency.
We addresseddifficult words, that had at least 5 senses in MWN.Actually, polysemy does not seem to be directlyrelated to systems?
results (Calzolari, 2002), in factthe average F-measure of our six runs for thenouns (0.512) was higher than for adjectives(0.472) and verbs (0.448), although the former hadmore attested senses in the labeled data.Complexity in returning the correct sense seemsto depend on the blurred distinction between simi-lar meanings rather than on the number of sensesthemselves.
If we consider the nouns ?attacco?
(attack) and ?esecuzione?
(performance, execu-tion), for which the systems obtained the worst andone of the best average results respectively, wenotice that the 4 attested senses of ?esecuzione?were clearly distinguished and referred to differentdomains (Factotum, Art, Law and Politics), whilethe 6 attested senses of ?attacco?
were more subtlydefined.
Senses 2, 7 and 11 were very difficult todiscriminate and often appeared in metaphoricalcontexts.
Senses 5 and 6, for their part, belong tothe Sport domain and are not always easy to dis-tinguish.4 ConclusionsThe results of the six systems participating inthe evaluation exercise showed some improve-ments compared to the average performance atSENSEVAL-2, though data sets and sense reposi-tories were considerably different.We are pleased with the successful outcome ofthe experiments in terms of participation, althoughregrettably no system exploited the unlabeledtraining set, which was intended to offer a lesstime-consuming resource.
On the other hand, thelabeled instances that have been collected representa useful and reusable benchmark.As a final remark we think it could be interest-ing to consider the actual distribution of wordsenses in Italian corpora in collecting the examplesfor the next campaign.AcknowledgementsWe would like to thank Christian Girardi andOleksandr Vagin for their technical support;Claudio Giuliano and the Ladin Cultural Centrefor the use of their Toolbox for Lexicographers;Pamela Forner, Daniela Andreatta and ElisabettaFauri for the revision of the Italian MWN and onthe semantic annotation of the examples; and LuisaBentivogli and Emanuele Pianta for their precioussuggestions during the manual annotation.ReferencesLuisa Bentivogli, Christian Girardi and EmanuelePianta.
2003.
The MEANING Italian Corpus.
In Pro-ceedings of the Corpus Linguistics 2003 conference,Lancaster, UK: 103-112.Francesca Bertagna, Claudia Soria and Nicoletta Cal-zolari.
2001.
The Italian Lexical Sample Task.
InProceedings of SENSEVAL-2: Second InternationalWorkshop on Evaluating Word Sense Disambigua-tion Systems, Toulouse, France: 29-32.Thorsten Brants.
2000.
TnT - a Statistical Part-of-Speech Tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference ANLP-2000, Seattle, WA: 224-231.Nicoletta Calzolari, Claudia Soria, Francesca Bertagnaand Francesco Barsotti.
2002.
Evaluating lexical re-sources using SENSEVAL.
Natural Language Engi-neering, 8(4): 375-390.Philip Edmonds.
2000.
Designing a task forSENSEVAL-2.
(http://www.sle.sharp.co.uk/SENSEVAL2/archive/index.htm)Claudio Giuliano.
2002.
A Toolbox for LexicographersIn Proceedings of the tenth EURALEX InternationalCongress, Copenhagen, Denmark: 113-118.Bernardo Magnini and Gabriela Cavagli?.
2000.
Inte-grating Subject Field Codes into WordNet.
In Pro-ceedings of LREC-2000, Athens, Greece: 1413-1418.Emanuele Pianta, Luisa Bentivogli and ChristianGirardi.
2002.
MultiWordNet: developing an alignedmultilingual database.
In Proceedings of the FirstInternational Conference on Global WordNet, My-sore, India: 293-302.
