A Selectionist Theory of Language AcquisitionChar les  D .
Yang*Art i f ic ia l  Inte l l igence LaboratoryMassachuset ts  Ins t i tu te  of Techno logyCambr idge ,  MA 02139charles@ai, mit.
eduAbst ractThis paper argues that developmental patterns inchild language be taken seriously in computationalmodels of language acquisition, and proposes a for-mal theory that meets this criterion.
We first presentdevelopmental facts that are problematic for sta-tistical learning approaches which assume no priorknowledge of grammar, and for traditional learnabil-ity models which assume the learner moves from oneUG-defined grammar to another.
In contrast, weview language acquisition as a population of gram-mars associated with "weights", that compete in aDarwinian selectionist process.
Selection is madepossible by the variational properties of individualgrammars; specifically, their differential compatibil-ity with the primary linguistic data in the environ-ment.
In addition to a convergence proof, we presentempirical evidence in child language development,that a learner is best modeled as multiple grammarsin co-existence and competition.1 Learnab i l i ty  and  Deve lopmentA central issue in linguistics and cognitive scienceis the problem of language acquisition: How doesa human child come to acquire her language withsuch ease, yet without high computational power orfavorable learning conditions?
It is evident hat anyadequate model of language acquisition must meetthe following empirical conditions:?
Learnability: such a model must converge to thetarget grammar used in the learner's environ-ment, under plausible assumptions about thelearner's computational machinery, the natureof the input data, sample size, and so on.?
Developmental compatibility: the learner mod-eled in such a theory must exhibit behaviorsthat are analogous to the actual course of lan-guage development (Pinker, 1979).
* I would like to thank Julie Legate, Sam Gutmann,  BobBerwick, Noam Chomsky, John Frampton,  and  John Gold-smith  for comments  and discussion.
This  work is supportedby an NSF graduate fellowship.It is worth noting that the developmental compati-bility condition has been largely ignored in the for-mal studies of language acquisition.
In the rest ofthis section, I show that if this condition is taken se-riously, previous models of language acquisition havedifficulties explaining certain developmental facts inchild language.1.1 Against  Stat is t ica l  Learn ingAn empiricist approach to language acquisition has(re)gained popularity in computational linguisticsand cognitive science; see Stolcke (1994), Charniak(1995), Klavans and Resnik (1996), de Marcken(1996), Bates and Elman (1996), Seidenberg (1997),among numerous others.
The child is viewed as aninductive and "generalized" data processor such asa neural network, designed to derive structural reg-ularities from the statistical distribution of patternsin the input data without prior (innate) specificknowledge of natural anguage.
Most concrete pro-posals of statistical learning employ expensive andspecific computational procedures such as compres-sion, Bayesian inferences, propagation of learningerrors, and usually require a large corpus of (some-times pre-processed) ata.
These properties imme-diately challenge the psychological p ausibility of thestatistical learning approach.
In the present discus-sion, however, we are not concerned with this butsimply grant that someday, someone might devisea statistical learning scheme that is psychologicallyplausible and also succeeds in converging to the tar-get language.
We show that even if such a schemewere possible, it would still face serious challengesfrom the important but often ignored requirementof developmental compatibility.One of the most significant findings in child lan-guage research of the past decade is that differentaspects of syntactic knowledge are learned at differ-ent rates.
For example, consider the placement offinite verb in French, where inflected verbs precedenegation and adverbs:Jean voit souvent/pas Marie.Jean sees often/not Marie.This property of French is mastered as early as429the 20th month, as evidenced by the extreme rarityof incorrect verb placement in child speech (Pierce,1992).
In contrast, some aspects of language are ac-quired relatively late.
For example, the requirementof using a sentential subject is not mastered by En-glish children until as late as the 36th month (Valian,1991), when English children stop producing a sig-nificant number of subjectless sentences.When we examine the adult speech to children(transcribed in the CHILDES corpus; MacWhinneyand Snow, 1985), we find that more than 90% ofEnglish input sentences contain an overt subject,whereas only 7-8% of all French input sentences con-tain an inflected verb followed by negation/adverb.A statistical earner, one which builds knowledgepurely on the basis of the distribution of the inputdata, predicts that English obligatory subject useshould be learned (much) earlier than French verbplacement - exactly the opposite of the actual find-ings in child language.Further evidence against statistical learning comesfrom the Root Infinitive (RI) stage (Wexler, 1994;inter alia) in children acquiring certain languages.Children in the RI stage produce a large number ofsentences where matrix verbs are not finite - un-grammatical in adult language and thus appearinginfrequently in the primary linguistic data if at all.It is not clear how a statistical learner will inducenon-existent patterns from the training corpus.
Inaddition, in the acquisition of verb-second (V2) inGermanic grammars, it is known (e.g.
Haegeman,1994) that at an early stage, children use a largeproportion (50%) of verb-initial (V1) sentences, amarked pattern that appears only sparsely in adultspeech.
Again, an inductive learner purely driven bycorpus data has no explanation for these disparitiesbetween child and adult languages.Empirical evidence as such poses a serious prob-lem for the statistical learning approach.
It seemsa mistake to view language acquisition as an induc-tive procedure that constructs linguistic knowledge,directly and exclusively, from the distributions of in-put data.1.2 The  Trans format iona l  ApproachAnother leading approach to language acquisition,largely in the tradition of generative linguistics, ismotivated by the fact that although child language isdifferent from adult language, it is different in highlyrestrictive ways.
Given the input to the child, thereare logically possible and computationally simple in-ductive rules to describe the data that are neverattested in child language.
Consider the followingwell-known example.
Forming a question in Englishinvolves inversion of the auxiliary verb and the sub-ject:Is the man t tall?where "is" has been fronted from the position t, theposition it assumes in a declarative sentence.
A pos-sible inductive rule to describe the above sentence isthis: front the first auxiliary verb in the sentence.This rule, though logically possible and computa-tionally simple, is never attested in child language(Chomsky, 1975; Crain and Nakayama, 1987; Crain,1991): that is, children are never seen to producesentences like:, Is the cat that the dog t chasing is scared?where the first auxiliary is fronted (the first "is"),instead of the auxiliary following the subject of thesentence (here, the second "is" in the sentence).Acquisition findings like these lead linguists topostulate that the human language capacity is con-strained in a finite prior space, the Universal Gram-mar (UG).
Previous models of language acquisi-tion in the UG framework (Wexter and Culicover,1980; Berwick, 1985; Gibson and Wexler, 1994) aretransformational, borrowing a term from evolution(Lewontin, 1983), in the sense that the learner movesfrom one hypothesis/grammar to another as inputsentences are processed.
1 Learnability results canbe obtained for some psychologically plausible algo-rithms (Niyogi and Berwick, 1996).
However, thedevelopmental compatibility condition still poses se-rious problems.Since at any time the state of the learner is identi-fied with a particular grammar defined by UG, it ishard to explain (a) the inconsistent patterns in childlanguage, which cannot be described by ally singleadult grammar (e.g.
Brown, 1973); and (b) thesmoothness of language development (e.g.
Pinker,1984; Valiant, 1991; inter alia), whereby the childgradually converges to the target grammar, ratherthan the abrupt jumps that would be expected frombinary changes in hypotheses/grammars.Having noted the inadequacies of the previousapproaches to language acquisition, we will pro-pose a theory that aims to meet language learn-ability and language development conditions imul-taneously.
Our theory draws inspirations from Dar-winian evolutionary biology.2 A Se lec t ion is t  Mode l  o f  LanguageAcqu is i t ion2.1 The  Dynamics  of  Darwin ian  Evo lu t ionEssential to Darwinian evolution is the concept ofvariational thinking (Lewontin, 1983).
First, differ-1 Note that  the t ransformat ional  pproach is not restr ictedto UG-based models; for example, Bril l 's influential work(1993) is a corpus-based model which successively revises aset of syntactic_rules upon presentat ion of part ial ly bracketedsentences.
Note that  however, the state of the learning sys-tem at any t ime is still a single set of rules, that  is, a single"grammar" .430ences among individuals are viewed as "real", as op-posed to deviant from some idealized archetypes, asin pre-Darwinian thinking.
Second, such differencesresult in variance in operative functions among indi-viduals in a population, thus allowing forces of evo-lution such as natural selection to operate.
Evolu-tionary changes are therefore changes in the distri-bution of variant individuals in the population.
Thiscontrasts with Lamarckian transformational think-ing, in which individuals themselves undergo directchanges (transformations) (Lewontin, 1983).2.2 A popu la t ion  of g rammarsLearning, including language acquisition, can becharacterized as a sequence of states in which thelearner moves from one state to another.
Transfor-mational models of language acquisition identify thestate of the learner as a single grammar/hypothesis.As noted in section 1, this makes difficult to explainthe inconsistency in child language and the smooth-ness of language development.We propose that the learner be modeled as a pop-ulation of "grammars", the set of all principled lan-guage variations made available by the biological en-dowment of the human language faculty.
Each gram-mar Gi is associated with a weight Pi, 0 <_ Pi <_ 1,and ~p i  -~ 1.
In a linguistic environment E, theweight pi(E, t) is a function of E and the time vari-able t, the time since the onset of language acquisi-tion.
We say thatDef in it ion:  Learning converges ifVe,0 < e < 1,VGi, \[ pi (E , t+ 1) -p i (E , t )  \[< eThat is, learning converges when the compositionand distribution of the grammar population are sta-bilized.
Particularly, in a monolingual environmentET in which a target grammar T is used, we say thatlearning converges to T if limt-.cv pT(ET, t) : 1.2.3 A Learn ing  A lgor i thmWrite E -~ s to indicate that a sentence s is an ut-terance in the linguistic environment E. Write s E Gif a grammar G can analyze s, which, in a narrowsense, is parsability (Wexler and Culicover, 1980;Berwick, 1985).
Suppose that there are altogetherN grammars in the population.
For simplicity, writePi for pi(E, t) at time t, and p~ for pi(E, t+ 1) at timet + 1.
Learning takes place as follows:The  A lgor i thm:Given an input sentence s, the childwith the probability Pi, selects a grammar Gi{, ?
i f sEG i  P}=P i+V(1-P i )pj (1 - V)Pj if j ~ ip; = (1 - V)pi?
i fsf \[G~ p,j N--~_l+(1--V)pj if j~ iComment :  The algorithm is the Linear reward-pena l ty  (LR-p) scheme (Bush and Mostellar, 1958),one of the earliest and most extensively studiedstochastic algorithms in the psychology of learning.It is real-time and on-line, and thus reflects therather limited computational capacity of the childlanguage learner, by avoiding sophisticated data pro-cessing and the need for a large memory to storepreviously seen examples.
Many variants and gener-alizations of this scheme are studied in Atkinson etal.
(1965), and their thorough mathematical treat-ments can be found in Narendra and Thathac!lar(1989).The algorithm operates in a selectionist man-ner: grammars that succeed in analyzing input sen-tences are rewarded, and those that fail are pun-ished.
In addition to the psychological evidence forsuch a scheme in animal and human learning, thereis neurological evidence (Hubel and Wiesel, 1962;Changeux, 1983; Edelman, 1987; inter alia) that thedevelopment of neural substrate is guided by the ex-posure to specific stimulus in the environment in aDarwinian selectionist fashion.2.4 A Convergence  Proo fFor simplicity but without loss of generality, assumethat there are two grammars (N -- 2), the targetgrammar T1 and a pretender T2.
The results pre-sented here generalize to the N-grammar case; seeNarendra and Thathachar (1989).Def in i t ion:  The penalty probability of grammar Tiin a linguistic environment E isca = Pr(s ?
T~ I E -~ s)In other words, ca represents the probability thatthe grammar T~ fails to analyze an incoming sen-tence s and gets punished as a result.
Notice thatthe penalty probability, essentially a fitness measureof individual grammars, is an intrinsic property of aUG-defined grammar relative to a particular linguis-tic environment E, determined by the distributionalpatterns of linguistic expressions in E. It is not ex-plicitly computed, as in (Clark, 1992) which uses theGenetic Algorithm (GA).
2The main result is as follows:Theorem:e2 if I 1 -V(c l+c2)  l< 1 (1) t_~ooPl_tlim ( )  - C1 "\[- C2Proof  sketch: Computing E\[pl(t + 1) \[ pl(t)\] asa function of Pl (t) and taking expectations on both2Claxk's model and the present one share an importantfeature: the outcome of acquisition is determined by the dif-ferential compatibil it ies of individual grammars.
The choiceof the GA introduces various psychological and linguistic as-sumptions that can not be justified; see Dresher (1999) andYang (1999).
Furthermore, no formal proof of convergence isgiven.431sides giveE\[pl(t + 1) = \[1 - ~'(el -I- c2)\]E~Ol(t)\] + 3'c2 (2)Solving \[2\] yields \[11.Comment  1: It is easy to see that Pl ~ 1 (andp2 ~ 0) when cl = 0 and c2 > 0; that is, the learnerconverges to the target grammar T1, which has apenalty probability of 0, by definition, in a mono-lingual environment.
Learning is robust.
Supposethat there is a small amount of noise in the input,i.e.
sentences uch as speaker errors which are notcompatible with the target grammar.
Then cl > 0.If el << c2, convergence to T1 is still ensured by \[1\].Consider a non-uniform linguistic environment inwhich the linguistic evidence does not unambigu-ously identify any single grammar; an example ofthis is a population in contact with two languages(grammars), say, T1 and T2.
Since Cl > 0 and c2 > 0,\[1\] entails that pl and P2 reach a stable equilibriumat the end of language acquisition; that is, languagelearners are essentially bi-lingual speakers as a resultof language contact.
Kroch (1989) and his colleagueshave argued convincingly that this is what happenedin many cases of diachronic hange.
In Yang (1999),we have been able to extend the acquisition modelto a population of learners, and formalize Kroch'sidea of grammar competition over time.Comment  2: In the present model, one can di-rectly measure the rate of change in the weight of thetarget grammar, and compare with developmentalfindings.
Suppose T1 is the target grammar, hencecl = 0.
The expected increase of Pl, APl is com-puted as follows:E\[Apl\] = c2PlP2 (3)Since P2 = 1 - pl, APl \[3\] is obviously a quadraticfunction of pl(t).
Hence, the growth of Pl will pro-duce the familiar S-shape curve familiar in the psy-chology of learning.
There is evidence for an S-shapepattern in child language development (Clahsen,1986; Wijnen, 1999; inter alia), which, if true, sug-gests that a selectionist learning algorithm adoptedhere might indeed be what the child learner employs.2.5 Unambiguous  Ev idence  is UnnecessaryOne way to ensure convergence is to assume the ex-istence of unambiguous evidence (cf.
Fodor, 1998):sentences that are only compatible with the targetgrammar but not with any other grammar.
Unam-biguous evidence is, however, not necessary for theproposed model to converge.
It follows from the the-orem \[1\] that even if no evidence can unambiguouslyidentify the target grammar from its competitors, itis still possible to ensure convergence as long as allcompeting rammars fail on some proportion of in-put sentences; i.e.
they all have positive penaltyprobabilities.
Consider the acquisition of the target,a German V2 grammar, in a population of grammarsbelow:1.
German: SVO, OVS, XVSO2.
English: SVO, XSVO3.
Irish: VSO, XVSO4.
Hixkaryana: OVS, XOVSWe have used X to denote non-argument categoriessuch as adverbs, adjuncts, etc., which can quitefreely appear in sentence-initial positions.
Note thatnone of the patterns in (1) could conclusively distin-guish German from the other three grammars.
Thus,no unambiguous evidence appears to exist.
How-ever, if SVO, OVS, and XVSO patterns appear inthe input data at positive frequencies, the Germangrammar has a higher overall "fitness value" thanother grammars by the virtue of being compatiblewith all input sentences.
As a result, German willeventually eliminate competing rammars.2.6 Learn ing  in a Parametr i c  SpaceSuppose that natural language grammars vary ina parametric space, as cross-linguistic studies sug-gest.
3 We can then study the dynamical behaviorsof grammar classes that are defined in these para-metric dimensions.
Following (Clark, 1992), we saythat a sentence s expresses a parameter c~ if a gram-mar must have set c~ to some definite value in orderto assign a well-formed representation to s. Con-vergence to the target value of c~ can be ensured bythe existence of evidence (s) defined in the sense ofparameter expression.
The convergence to a singlegrammar can then be viewed as the intersection ofparametric grammar classes, converging in parallelto the target values of their respective parameters.3 Some Deve lopmenta l  P red ic t ionsThe present model makes two predictions that can-not be made in the standard transformational theo-ries of acquisition:1.
As the target gradually rises to dominance, thechild entertains a number of co-existing ram-mars.
This will be reflected in distributionalpatterns of child language, under the null hy-pothesis that the grammatical knowledge (inour model, the population of grammars andtheir respective weights) used in production isthat used in analyzing linguistic evidence.
Forgrammatical phenomena that are acquired rela-tively late, child language consists of the outputof more than one grammar.3Although different heories of grammar, e.g.
GB, HPSG,LFG, TAG, have different ways of instantiating this idea.4322.
Other things being equal, the rate of develop-ment is determined by the penalty probabili-ties of competing rammars relative to the in-put data in the linguistic environment \[3\].In this paper, we present longitudinal evidenceconcerning the prediction in (2).
4 To evaluate de-velopmental predictions, we must estimate the thepenalty probabilities of the competing rammars ina particular linguistic environment.
Here we exam-ine the developmental rate of French verb placement,an early acquisition (Pierce, 1992), that of Englishsubject use, a late acquisition (Valian, 1991), that ofDutch V2 parameter, also a late acquisition (Haege-man, 1994).Using the idea of parameter expression (section2.6), we estimate the frequency of sentences thatunambiguously identify the target value of a pa-rameter.
For example, sentences that contain finiteverbs preceding adverb or negation ("Jean voit sou-vent/pas Marie" ) are unambiguous indication for the\[+\] value of the verb raising parameter.
A grammarwith the \[-\] value for this parameter is incompatiblewith such sentences and if probabilistically selectedfor the learner for grammatical nalysis, will be pun-ished as a result.
Based on the CHILDES corpus,we estimate that such sentences constitute 8% of allFrench adult utterances to children.
This suggeststhat unambiguous evidence as 8% of all input datais sufficient for a very early acquisition: in this case,the target value of the verb-raising parameter is cor-rectly set.
We therefore have a direct explanationof Brown's (1973) observation that in the acquisi-tion of fixed word order languages uch as English,word order errors are "trifingly few".
For example,English children are never to seen to produce wordorder variations other than SVO, the target gram-mar, nor do they fail to front Wh-words in questionformation.
Virtually all English sentences displayrigid word order, e.g.
verb almost always (immedi-ately) precedes object, which give a very high (per-haps close to 100%, far greater than 8%, which issufficient for a very early acquisition as in the case ofFrench verb raising) rate of unambiguous evidence,sufficient o drive out other word order grammarsvery early on.Consider then the acquisition of the subject pa-rameter in English, which requires a sentential sub-ject.
Languages like Italian, Spanish, and Chinese,on the other hand, have the option of dropping thesubject.
Therefore, sentences with an overt subjectare not necessarily useful in distinguishing English4In Yang (1999), we show that a child learner, en route toher target grammar, entertains multiple grammars.
For ex-ample, a significant portion of English child language showscharacteristics of a topic-drop optional subject grammar likeChinese, before they learn that subject use in English is oblig-atory at around the 3rd birthday.from optional subject languages.
5 However, thereexists a certain type of English sentence that is in-dicative (Hyams, 1986):There is a man in the room.Are there toys on the floor?The subject of these sentences is "there", a non-referential lexical item that is present for purelystructural reasons - to satisfy the requirement inEnglish that the pre-verbal subject position mustbe filled.
Optional subject languages do not havethis requirement, and do not have expletive-subjectsentences.
Expletive sentences therefore xpress the\[+\] value of the subject parameter.
Based on theCHILDES corpus, we estimate that expletive sen-tences constitute 1% of all English adult utterancesto children.Note that before the learner eliminates optionalsubject grammars on the cumulative basis of exple-tive sentences, he has probabilistic access to multi-ple grammars.
This is fundamentally different fromstochastic grammar models, in which the learner hasprobabilistic access to generative ~ules.
A stochasticgrammar is not a developmentally adequate modelof language acquisition.
As discussed in section 1.1,more than 90% of English sentences contain a sub-ject: a stochastic grammar model will overwhehn-ingly bias toward the rule that generates a subject.English children, however, go through long periodof subject drop.
In the present model, child sub-ject drop is interpreted as the presence of the trueoptional subject grammar, in co-existence with theobligatory subject grammar.Lastly, we consider the setting of the Dutch V2parameter.
As noted in section 2.5, there appears tono unambiguous evidence for the \[+\] value of the V2parameter: SVO,  VSO,  and OVS grammars ,  mem-bers of the \[-V2\] class, are each compatible with cer-tain proportions of expressions produced.by the tar-get V2  grammar .
However,  observe that despite ofits compatibility with with some input patterns, anOVS grammar  can not survive long in the populationof compet ing grammars .
This is because an OVSgrammar  has an extremely high penalty probability.Examinat ion  of CHILDES shows that OVS patternsconsist of only 1.3% of all input sentences to chil-dren, whereas SVO patterns constitute about 65%of all utterances, and XVSO, about 34%.
There-fore, only SVO and VSO grammar, members of the\[-V2\] class, are "contenders" alongside the (target)V2 grammar, by the virtue of being compatible withsignificant portions of input data.
But notice thatOVS patterns do penalize both SVO and VSO gram-mars, and are only compatible with the \[+V2\] gram-5Notice that this presupposes the child's prior knowledgeof and access to both obligatory and optional subject gram-mars.433mars.
Therefore, OVS patterns are effectively un-ambiguous evidence (among the contenders) for theV2 parameter, which eventually drive SVO and VSOgrammars out of the population.In the selectioni-st model, the rarity of OVS sen-tences predicts that the acquisition of the V2 pa-rameter in Dutch is a relatively late phenomenon.Furthermore, because the frequency (1.3%) of DutchOVS sentences i  comparable to the frequency (1%)of English expletive sentences, we expect hat DutchV2 grammar is successfully acquired roughly at thesame time when English children have adult-levelsubject use (around age 3; Valian, 1991).
AlthoughI am not aware of any report on the timing of thecorrect setting of the Dutch V2 parameter, there isevidence in the acquisition of German, asimilar lan-guage, that children are considered to have success-fully acquired V2 by the 36-39th month (Clahsen,1986).
Under the model developed here, this is notan coincidence.4 Conc lus ionTo capitulate, this paper first argues that consider-ations of language development must be taken seri-ously to evaluate computational models of languageacquisition.
Once we do so, both statistical learn-ing approaches and traditional UG-based learnabil-ity studies are empirically inadequate.
We proposedan alternative model which views language acqui-sition as a selectionist process in which grammarsform a population and compete to match linguis-tic* expressions present in the environment.
Thecourse and outcome of acquisition are determined bythe relative compatibilities of the grammars with in-put data; such compatibilities, expressed in penaltyprobabilities and unambiguous evidence, are quan-tifiable and empirically testable, allowing us to makedirect predictions about language development.The biologically endowed linguistic knowledge en-ables the learner to go beyond unanalyzed distribu-tional properties of the input data.
We argued insection 1.1 that it is a mistake to model languageacquisition as directly learning the probabilistic dis-tribution of the linguistic data.
Rather, language ac-quisition is guided by particular input evidence thatserves to disambiguate the target g rammar  from thecompet ing grammars .
The  ability to use such evi-dence for g rammar  selection is based on the learner'slinguistic knowledge.
Once  such knowledge is as-sumed,  the actual process of language acquisition isno more  remarkable than generic psychological mod-els of learning.
The  selectionist theory, if correct,show an example  of the interaction between domain-specific knowledge and domain-neutral  mechanisms,which combine  to explain properties of language andcognition.ReferencesAtkinson, R., G. Bower, and E. Crothers.
(1965).An Introduction to Mathematical Learning Theory.New York: Wiley.Bates, E. and J. Elman.
(1996).
Learning rediscov-ered: A perspective on Saffran, Aslin, and Newport.Science 274: 5294.Berwick, R. (1985).
The acquisition of syntacticknowledge.
Cambridge, MA: MIT Press.Brill, E. (1993).
Automatic grammar induction andparsing free text: a transformation-based approach.ACL Annual Meeting.Brown, R. (1973).
A first language.
Cambridge,MA: Harvard University Press.Bush, R. and F. Mostellar.
Stochastic models \]'orlearning.
New York: Wiley.Charniak, E. (1995).
Statistical anguage learning.Cambridge, MA: MIT Press.Chomsky, N. (1975).
Reflections on language.
NewYork: Pantheon.Changeux, J.-P. (1983).
L'Homme Neuronal.
Paris:Fayard.Clahsen, H. (1986).
Verbal inflections in Germanchild language: Acquisition of agreement markingsand the functions they encode.
Linguistics 24: 79-121.Clark, R. (1992).
The selection of syntactic knowl-edge.
Language Acquisition 2: 83-149.Crain, S. and M.  Nakayama (1987).
Structure de-pendency  in grammar  formation.
Language 63: 522-543.Dresher, E. (1999).
Chart ing the learning path: cuesto parameter setting.
Linguistic Inquiry 30: 27-67.Edelman, G. (1987).
Neural Darwinism.
: The the-ory of neuronal group selection.
New York: BasicBooks.Fodor, J. D. (1998).
Unambiguous triggers.
Lin-guistic Inquiry 29: 1-36.Gibson, E. and K. Wexler (1994).
Triggers.
Linguis-tic Inquiry 25: 355-407.Haegeman, L. (1994).
Root infinitives, clitics, andtruncated structures.
Language Acquisition.Hubel, D. and T. Wiesel (1962).
Receptive fields,binocular interaction and functional architecture inthe cat's visual cortex.
Journal of Physiology 160:106-54.Hyams, N. (1986) Language acquisition and the the-ory of parameters.
Reidel: Dordrecht.Klavins, J. and P. Resnik (eds.)
(1996).
The balanc-ing act.
Cambridge, MA: MIT Press.Kroch, A.
(1989).
Reflexes of grammar in patternsof language change.
Language variation and change1: 199-244.Lewontin, R. (1983).
The organism as the subjectand object of evolution.
Scientia 118: 65-82.de Marcken, C. (1996).
Unsupervised language ac-quisition.
Ph.D. dissertation, MIT.434MacWhinney, B. and C. Snow (1985).
The ChildLanguage Date Exchange System.
Journal of ChildLanguage 12, 271-296.Narendra, K. and M. Thathachar (1989).
Learningautomata.
Englewood Cliffs, N J: Prentice Hall.Niyogi, P. and R. Berwick (1996).
A language learn-ing model for finite parameter space.
Cognition 61:162-193.Pierce, A.
(1992).
Language acquisition and andsyntactic theory: a comparative analysis of Frenchand English child grammar.
Boston: Kluwer.Pinker, S. (1979).
Formal models of language learn-ing.
Cognition 7: 217-283.Pinker, S. (1984).
Language learnability and lan-guage development.
Cambridge, MA: Harvard Uni-versity Press.Seidenberg, M. (1997).
Language acquisition anduse: Learning and applying probabilistic con-straints.
Science 275: 1599-1604.Stolcke, A.
(1994) Bayesian Learning of Probabilis-tic Language Models.
Ph.D. thesis, University ofCalifornia at Berkeley, Berkeley, CA.Valian, V. (1991).
Syntactic subjects in the earlyspeech of American and Italian children.
Cognition40: 21-82.Wexler, K. (1994).
Optional infinitives, head move-ment, and the economy of derivation in child lan-guage.
In Lightfoot, D. and N. Hornstein (eds.
)Verb movement.
Cambridge: Cambridge UniversityPress.Wexler, K. and P. Culicover (1980).
Formal princi-ples of language acquisition.
Cambridge, MA: MITPress.Wijnen, F. (1999).
Verb placement in Dutch childlanguage: A longitudinal analysis.
Ms. Universityof Utrecht.Yang, C. (1999).
The variational dynamics of natu-ral language: Acquisition and use.
Technical report,MIT AI Lab.435
