Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 412?419,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsStatistical Machine Translation of Texts with Misspelled WordsNicola Bertoldi Mauro Cettolo Marcello FedericoFBK - Fondazione Bruno Kesslervia Sommarive 18 - 38123 Povo, Trento, Italy{bertoldi,cettolo,federico}@fbk.euAbstractThis paper investigates the impact of mis-spelled words in statistical machine transla-tion and proposes an extension of the transla-tion engine for handling misspellings.
The en-hanced system decodes a word-based confu-sion network representing spelling variationsof the input text.We present extensive experimental results ontwo translation tasks of increasing complex-ity which show how misspellings of differenttypes do affect performance of a statistical ma-chine translation decoder and to what extentour enhanced system is able to recover fromsuch errors.1 IntroductionWith the widespread adoption of the Internet, ofmodern communication, multimedia and mobile de-vice technologies, the amount of multilingual in-formation distributed and available to anyone, any-where, has exploded.
So called social media haverapidly reshaped information exchange among Inter-net users, providing new means of communication(blogs, tweets, etc.
), collaboration (e.g.
wikis), andsharing of multimedia content, and entertainment.In particular, social media have today become alsoan important market for advertisement as well as aglobal forum for consumer opinions (Kushal et al,2003).The growing spread of user-generated content isscaling-up the potential demand for on-line machinetranslation (MT) but also setting new challenges tothe field of natural language processing (NLP) ingeneral.
The language written and spoken in thesocial media presents an impressive variety of con-tent and styles (Schler et al, 2006), and writing con-ventions that rapidly evolve over time.
Moreover,much of the content is expressed in informal style,that more or less violates the standard grammar, con-tains many abbreviations and acronyms, and finallymany misspelled words.
From the point of view ofMT, language of social media is hence very differentfrom the one represented in the text corpora nowa-days available to train statistical MT systems.Facing all these challenges, we pragmaticallyscaled down our ambition and decided to investigatea basic, somehow preliminary, well defined prob-lem: the impact of misspelled words in statisticalMT.
Unintentional typing errors are indeed remark-ably frequent in online chats, blogs, wikis, reviews,and hence constitute a major source of noise (Subra-maniam et al, 2009).In this paper we aim at studying performancedegradation of statistical MT under different levelsand kinds of noise, and at analyzing to what extentstatistical MT is able to recover from errors by en-riching its input with spelling variations.After a brief overview of NLP literature relatedto noisy texts, in Section 3 we consider differenttypes of misspellings and derive simple but realisticmodels that are able to reproduce them.
Such mod-els are then used to generate errors in texts passedto a phrase-based statistical MT system.
Next, inSection 4 we introduce an extension of a statisticalMT system able to handle misspellings by exploitingconfusion network decoding (Bertoldi et al, 2008).Experiments are reported in Section 5 that in-412vestigate the trade-off between complexity of theextended MT decoder versus translation accuracy.Moreover, as the proposed model for handling mis-spellings embeds specific assumptions on how er-rors are generated, we also measure the robustnessof the enhanced MT decoder with respect to differ-ent noise sources.
Experiments are reported on twotasks of different complexity, the translation of Eu-roparl texts and weather bulletins, involving Englishand Italian languages.2 Previous WorkMost contributions addressing NLP of noisy user-generated content are from the text mining commu-nity.
A survey about the different types of noise thatmight affect text mining is in (Subramaniam et al,2009), while an analysis of how noise phenomena,commonly occurring in blogs, affect an opinion min-ing application is in (Dey and Haque, 2009).Concerning spelling correction literature, manyworks apply the noisy channel model which con-sists of two components: a source model (priorof word probabilities) and a channel (error) model,that accounts for spelling transformations on let-ter sequences.
Several approaches have beenproposed under this framework, that mainly dif-fer in the employed error model; see for exam-ple: (Church and Gale, 1991), (Brill and Moore,2000) and (Toutanova and Moore, 2002).Comprehensive surveys on methods to model andrecover spelling errors can be found in (Kukich,1992) and (Pedler, 2007); in particular, the latterwork is specifically centered on methods for cor-recting so-called real-word errors (cf.
Section 3).The detection of errors and the suggestion of cor-rections typically rely on the availability of text cor-pora or human-made lexical resources.
Search forcorrect alternatives can be based on word similaritymeasures, such as the edit distance (Mitton, 1995),anagram hashing (Reynaert, 2006), and semanticdistance based on WordNet (Hirst and Budanitsky,2005).
More sophisticated approaches have beenproposed by (Fossati and Di Eugenio, 2008), thatmixes surface and Part-Of-Speech Information, and(Schaback and Li, 2007), which combines similaritymeasures at the character, phonetic, word, syntax,and semantic levels into one global feature-basedframework.a) *W *w had just come in from Australia [Australia]b) good service we *staid one week.
[Tahiti]c) The room was *exellent but the hallway was *filty .
[NJ]d) is a good place to stay, if you are looking for a hotel*arround LAX airport.
[Tahiti]e) The staff was *freindly ...
I was *conerned aboutthe noise [CT]Table 1: Examples of misspellings found in on-line re-views of an hotel close to Los Angeles Int?l Airport.
Cor-responding corrections are: a) We, , b) stayed, c) excel-lent, filthy, d) around, e) friendly, concerned.Concerning the literature of statistical MT, inter-est in noisy data has been so far considering is-sues different from misspelled words.
For instance,(Davis et al, 1995) and (Vogel, 2003) address train-ing methods coping with noisy parallel data, in thesense that translations do not perfectly match.
Workon speech translation (Casacuberta et al, 2008) fo-cused instead on efficient methods to couple speechrecognition and MT in order to avoid error propaga-tion.
Very recently, (Carrera et al, 2009) conducteda qualitative study on the impact of noisy social me-dia content on statistical and rule-based MT.
Unfor-tunately, this work does not report any quantitativeresult, it is only based on a small selection of exam-ples that are manually evaluated, and finally it doesnot address the problem of integrating error correc-tion with MT.3 Types of MisspellingsIn general, a misspelled word is a sequence of let-ters that corresponds to no correctly spelled word ofthe same language (non-word error), or to a correctspelling of another word (real-word error).
In theexamples shown in Table 1, all marked errors arenon-word errors, but the one in sentence b), whichindeed is likely a misspelling of the word stayed.Causes of a misspelling may be an unintentionaltyping error (e.g.
*freindly for friendly), or lack ofknowledge about the proper spelling.
Typing errorscan originate from six different typing operations(Kukich, 1992): substitution, insertion, deletion,transposition, run-on, and split.1 Lack of knowledgecould be the cause of the misspelled *exellent in sen-tence c).1 Run-on and split are the special cases of deleting and in-serting blank spaces, respectively.4131.
your - you?re2.
then - than3.
its - it?s4.
to - too - two5.
were - where - we?re6.
there - their - they?re7.
a - an - and8.
off - of9.
here - hear10.
lose - looseTable 2: List of frequent real-word errors found in blogs.Source: http://www.theprobabilist.com.An interesting combination of cause and effect iswhen lack of linguistic competence results in con-fusing the spelling of a word with the spelling ofanother word that sounds similarly (Hirst and Bu-danitsky, 2005).
This could be likely the case of thePolynesian tourist that authored sentence b).A short list of words frequently confused in blogsis reported in Table 2 while a longer list can be foundin the Wikipedia.2 Real-word errors typically foolspell checkers because their identification requiresanalyzing the context in which they occur.In this paper, we automatically corrupt clean textwith three types of noise described below.
This pro-cedure permits us to analyze the MT performanceagainst different sources and levels of noise and tosystematically evaluate our error-recovering strat-egy.Non-word Noise We randomly replace words inthe text according to a list of 4,100 frequently non-word errors provided in the Wikipedia.
A qualitativeanalysis of these errors reveals that all of them origi-nate by one or two keyboard typing errors of the kinddescribed beforehand.
Practically, non-word noise isintroduced by defining a desired level of corruptionof the source text.Real-word Noise Similarly to the previous case,real-word errors are automatically introduced byanother list of frequently misused words in theWikipedia.
This list contains about 300 pairs of con-fusable words to which we also added the 10 fre-quent real-word errors occurring in blogs reportedin Table 2.2See Wikipedia?s ?list of frequently misused Englishwords?.Random Noise Finally, we may corrupt the origi-nal text by randomly replacing, inserting, and delet-ing characters in it up to a desired percentage.4 Error-recovering Statistical MTAn enhancement of a statistical MT system is pro-posed with the goal of improving robustness to mis-spellings in the input.
Rrror recovery is realizedby performing a sequence of actions before the ac-tual translation, which create reliable spelling alter-natives of the input and store them into a compactword-based Confusion Network (CN).Starting from the possibly noisy input text,spelling variations are generated by assuming thateach character is a potential typing error, indepen-dent from other characters.The variants are represented as a character-basedCN that models possible substitutions, insertion,deletions of each character, with an empirically de-termined weight assigned to each alternative.
Thenetwork is then searched by a non-monotonic searchprocess that scores possible character sequencesthrough a character n-gram language model, andoutputs a set of multiple spelling variants that is fi-nally converted into a word-based CN.
The result-ing word-based network is finally passed to the MTengine.
In the following, more details are providedon the augmented MT system with the help of Fig-ure 1, which shows how the system acts on the cor-rupted example ?all off ame?, supposed to be ?hallof fame?.Step 1 The input text (a) is split into a sequenceof characters (b) including punctuation marks andblank spaces ( ), which are here considered as stan-dard characters.
Moreover, single characters inter-leaved with the conventional empty character .Step 2 A CN (c) is built by adding all alternativecharacters of the keyboard to each input character,including the space character and the empty char-acter.
When the string character is , the only ad-mitted alternative is .
Possible alternative spellingsof the original string correspond to paths in the CN.Notice that each CN column beginning with a stan-dard character permits to manage insertion, substi-tution and split errors, while each column beginningwith the empty character permits to handle deletionand run-on errors.414...de?acbj?eg...dc?b...aeym?...rbcd?ea......sz?waf......b?ae?_eda...c?bfd...c?b?dcb...ae?...rcfdca...?ebdkpi?o...ecd...ab?_?ac...b?edkpl...
?o?...bedac?op...lk...ih ...z...saw?deb?ca...agc?b........ah emaf_fo_llla em_fo_la lh emaf_oluh em_ffo_llela maf_fo_lema_ffo_llaarca della gloria...hull......?halloofhall mefameoffall(a)(b) ?e?m?a?_?f?f?o?_?l?la??
(c)(d)(e)(f)p(w|a) ?
0.9112354Figure 1: The whole process to translate the mistakeninput ?all off ame [hall of fame]?
into ?arca della gloria?.A probability distribution of confusablekeystrokes is generated based on the distancebetween the keys on a standard QWERTY key-board.
This distribution is intended to model how aspelling error is actually produced.
Hence, characteralternatives in the CN are associated to a probabilitygiven by:p (x|y) ?
?1k ?
d(x, y) + 1(1)where d(x, y) is the physical distance between thekey of x and the key of y on the keyboard layout;for example, the character a has a distance of 3 fromthe character c on the considered keyboard layout.The free parameter k tunes the discriminative powerof the model between correct and wrong typing.
Inthis paper, k was empirically set to 0.1.
The  andcharacters are assigned a default distance of 9 and999 from any other character, respectively.For the sake of clarity, the probability p(w|a) ofjust one entry is reported in Figure 1.Step 3 The generation of spelling variations (d) isoperated by means of the same decoder employedfor translation (see below), but in a much simplifiedconfiguration which does not exploit any translationmodel.
It is designed to search the input character-based CN for the n-best character sequences whichbetter ?correct?
the mistaken input.
In Figure 1 thebest sequence is marked by bold boxes (c), and theempty character  is removed for the sake of clarity(d).
This process relies only on the character-based6-gram language model trained on monolingual datain the source language.
It is worth noticing that thegenerated spelling alternatives may in principle stillcontain non-words, just because they are selected bya character-based language model, which does notexplicitly embed the notion of word.Transposition errors are modeled both (i) indi-rectly through consecutive substitutions with appro-priate characters and (ii) directly by permitting somere-orderings of adjacent characters.
Moreover, pre-liminary experiments revealed that the explicit han-dling of deletion and run-on errors by interleavinginput characters with the empty character  (Step 1)is crucial to achieve good performance.
Althoughthe size of the character-based CN doubles, its de-coding time increases only by a small factor.Step 4 The n-best character sequences (d) aretransformed into a word-based CN (e) (Mangu etal., 2000).
First, each character-based sequence istransformed into a unifilar word-based lattice, whoseedges correspond to words and timestamps to thecharacter positions.
Then, the unifilar lattices are putin parallel to create one lattice with all spelling vari-ations of the input text (a).
Finally, a word-based CNis generated by means of the lattice-tool available inthe SRILM Toolkit (Stolcke, 2002).Step 5 Translation of the CN (e) is performedwith the Moses decoder (Koehn et al, 2007), thathas been successfully applied mainly to text trans-lation, but also to process multiple input hypothe-ses (Bertoldi et al, 2008), representing, for exam-ple, speech transcriptions, word segmentations, textswith possible punctuation marks, etc.
In general,415set #sent.
English Italian#wrd dict.
#wrd dict.EP train 1.2M 36M 106K 35M 146Ktest 2K 60K 6.5K 60K 8.3KWF train 42K 996K 2641 994K 2843test 328 8789 606 8704 697Table 3: Statistics of train/test data of the Europarl (EP)and the Weather Forecast (WF) tasks.Moses looks for the best translation exploring thesearch space defined by a set of feature functions(models), which are log-linearly interpolated withweights estimated during a tuning stage.The rationale of storing the spelling alternativesinto a word-based CN instead of n-best list is two-fold: (i) the CN contains a significantly larger num-ber of variations, and (ii) the translation system ismuch more efficient to translate CNs instead of n-best lists.5 ExperimentsExtensive experiments have been conducted on theEuroparl shared task, from English to Italian, asspecified by the Workshop on Statistical MachineTranslation of the ACL 2008.3 Additional experi-ments were conducted on a smaller task, namely thetranslation of weather forecast bulletins between thesame language pair.
Statistics on texts employed inexperiments are reported in Table 3.For both tasks, we created evaluation data by ar-tificially corrupting input text with the noise sourcesdescribed in Section 3.
The module for generatingspelling variations (Step 3) was trained on additional4M and 16M running words in English and Italian,respectively.We empirically investigated the following issues:(a) performance of the standard MT engine versusnature and level of the input noise; (b) performanceof the error-recovering MT engine versus number ofprovided spelling variations; (c) portability of theapproach to another task and translation direction;(d) computational requirements of the approach.5.1 Impact of NoiseThe first set of experiments involved the translationof corrupted versions of the Europarl test set.
Fig-3http://www.statmt.org/wmt08/1015202520105210.5010152025BLEUNoise Level (%)baselinerandom, no-recoverynon-word, no-recoveryreal-word, no-recoveryFigure 2: Translation performance as function of thenoise level (in log-scale) for different types of noise.ure 2 plots three curves of BLEU(%) scores, corre-sponding to different noise sources and noise ratios,given in terms of percentage of word error rate.
Italso shows the BLEU score on the original cleantext.
Notice that this baseline performance (25.16)represents the state-of-the-art4 for this task.The major outcome of these experiments is thatthe different types of errors seem to affect MT per-formance in a very similar manner.
Quantitatively,performance degradation begins even for low noiselevels ?
about 0.5 absolute BLEU loss at 1% ofnoise level ?
and reaches 50% when text corruptionreaches the level of 30%.
The similar impact of non-word and random errors is somehow expected.
Theplain reason is that both types of errors very likely5generate Out-Of-Vocabulary (OOV) words.We find instead less predictable that the impact ofreal-word errors is indistinguishable from that of theother two noise sources.
Notice also that most of thereal-word errors produce indeed words known to theMT system.
Hence, the question regards the behav-ior of the MT system when the sentence includes onOOV word or an out-of-context known word.
Em-pirically it seems that in both cases the decoder pro-duces translations with the same amount of errors.In some sense, the good news is that real-word er-rors do not induce more translation errors than OOVwords do.4http://matrix.statmt.org/matrix5Modulo noise in the parallel data and the chance that a ran-dom error generates a true word.4161520255020105210.50152025BLEUNoise Level (%)baselineno-recoverysinglemultiple, 200  2025105210.502025BLEUNoise Level (%)baselineno-recoverysinglemultiple, 200Figure 3: Performance of error-recovering method with random (left) and real-word (right) noise.5.2 Impact of Multiple CorrectionsExperiments presented here address evaluation ofour enhanced MT system.
In addition to nature andlevel of noise, translation performance is also an-alyzed with respect to the number (1 and 200) ofspelling alternatives generated at Step 3.
Figure 3plots BLEU scores for random (left plot) and real-word (right plot) noises.
For comparison purposes,the curves with no error recovery are also shown.Results with non-word noise are not provided sincethey are pretty similar to those with random noise.It is worth noticing that real-word errors are re-covered in a different way than random errors; infact, for the latter a single spelling alternative seemssufficient to guarantee a substantial error recovery,whereas for real-word errors this is not the case.Concerning the use of spelling variations, it isworth remarking that our system is able to fully re-cover from both random and non-word errors up tonoise levels of 10%, which remains high even fornoise levels up to 20%, where the BLEU degrada-tion is limited to around 5% relative.Real-word errors are optimally recovered in thecase of multiple spelling variations until they do notexceed 2% of the words in the input text; after that,the decrement of the MT quality becomes signif-icant but still limited to about 5% BLEU relativefor a noise level of 10%.
So the question arisesabout what could be a realistic real-word noise level.Clearly this question is not easy to address.
How-ever, to get a rough idea we can look at the exam-ples reported in Table 1.
These five sentences wereextracted from a text of about 100 words (of whichTable 1 only shows the sentences containing errors)that contain in total 8 errors: 7 of which are non-words and 1 is a real-word.
Although from thesefigures reliable statistics cannot be estimated, a rea-sonable assumption could be that a global noise levelof 10%6 might contain a 1/10 ratio for real-word vs.non-word errors.
Thus, looking at the real-word er-ror curve of Figure 3, the inability to recover errorsfor noise levels greater than 2-5% should actually beacceptable given this empirical observation.Another relevant remark from Figure 3 is thatfor low noise levels (less than 1%) the use of theerror-recovering module is counterproductive, sinceit introduces more errors than those actually affect-ing the original input text, causing a slight degra-dation of the translation performance.
If the com-putational cost to generate variants, which will beanalyzed in the next paragraph, is also taken into ac-count, it results evident the importance of design-ing a good strategy for enabling or disabling on de-mand the error-recovering stage.
A starting point fordefining an effective activation strategy is the esti-mation of the noise rate.
For doing this, non-wordscan be counted by exploiting proper dictionaries orspell checkers; concerning real-word noise, its ratecan be inferred either from the non-word rate, or bymeans of the perplexity, which is expected to be-come higher as the real-word error rate increases(Subramaniam et al, 2009).
Once the noise levelof the input text is known, the decision of activat-ing the correction module can be easily taken on a6By the way, at this noise rate, an error-recovering strategywould be highly recommended.4170 10 2030 40 5060501010.10  0 1020 30 4050 60BLEUNoise Level (%)English-Italianbaselineno-recoverymultiple, 2000 10 2030 40 50501010.10  0 1020 30 4050Noise Level (%)Italian-Englishbaselineno-recoverymultiple, 200Figure 4: Effects of random noise and noise correctionon translation performance for the WF task.threshold basis.
Alternatively, the proper workingpoint, in terms of precision and recall, of the correc-tion model could be dynamically chosen as a func-tion of the actual noise level.5.3 Computational CostsAlthough our investigation does not address explic-itly computational aspects of translating noisy in-put, nevertheless some general considerations can bedrawn.The effectiveness of our recovering approach re-lies on the compact representation of many spellingalternatives in a word-based CN.
The CN decod-ing has been shown to be efficient, just minimallylarger than the single string decoding (Bertoldi etal., 2008).
On the contrary, in the current enhancedMT setting, the sequence of Steps 1 to 4 for build-ing the CN from the noisy input text is quite costly.Rather than to an intrinsic complexity, this is due toour choice of creating a rich character-based CN inStep 3 for the sake of flexibility and to a naive im-plementation of Step 4.5.4 PortabilitySo far we have analyzed in detail our approachon the medium-large sized Europarl task, for theEnglish-to-Italian translation direction.
For assess-ing portability, we also considered a simpler task?the translation of weather forecast bulletins?
wherethe translation quality is definitely higher, for thesame language pair but in both translation directions.The choice of the weather forecast task is not bychance.
In fact, as the automatically translated bul-letins are published on the Web, a very high transla-tion quality is required, and then the presence of anytyping error in the original text could be a concern.
(By the way, for this task the presence of real-worderrors is very marginal.
)Figure 4 plots curves of MT performance underrandom noise conditions against multiple spellingvariations, for two translation directions.
It canbe noticed that the error-recovering system behavesqualitatively as for the Europarl task but even betterfrom a quantitative viewpoint.
Again, the recoveringmodel introduces spurious errors which affect trans-lation quality for low levels of noisy input, but inthis case the break-even point is less than 0.1% noiselevel.
On the other side, errors corrupting the inputtext are fully recovered up to 30-40% of noise lev-els, for which the BLEU score would be more thanhalved for non-corrected texts.6 Future WorkThere are a number of important issues that thiswork has still left open.
First of all, we focusedon a specific way of generating spelling varia-tions, based on single characters, but other possiblechoices should be investigated and compared to ourapproach, like the use of n-grams of words.An important open question regards efficiency ofthe proposed recovering strategy, since the problemhas been only sketched in Section 5.3.
It is our in-tention to analyze the intrinsic complexity of ourmodel, possibly discover its bottlenecks and imple-ment a more efficient solution.Another topic, mentioned in Section 5.2, is the ac-tivation strategy of the misspelling recovery.
Somefurther investigation is required on how its workingpoint can be effectively selected; in fact, since theenhanced system necessarily introduces spurious er-rors, it would be desirable to increase its precisionfor low-corrupted input texts.7 ConclusionsThis paper addressed the issue of automaticallytranslating written texts that are corrupted by mis-spelling errors.
An enhancement of a state-of-the-artstatistical MT system is proposed which efficientlyperforms the translation of multiple spelling variantsof noisy input.
These alternatives are generated by acharacter-based error recovery system under the as-sumption that misspellings are due to typing errors.The enhanced MT system has been tested on textscorrupted with increasing noise levels of three dif-ferent sources: random, non-word, and real-word er-rors.418Analysis of experimental results has led us todraw the following conclusions:?
The impact of misspelling errors on MT perfor-mance depends on the noise rate, but not on thenoise source.?
The capability of the enhanced MT system torecover from errors differs according to thenoise source: real-word noise is significantlyharder to remove than random and non-wordnoise, which behave substantially the same.?
The exploitation of several spelling alternativespermits to almost fully recover from errors ifthe noise rate does not exceed 10% for non-word noise and 2% for real-word noise, whichare likely above the corruption level observedin many social media.?
Finally, performance slightly decreases wheninput text is correct or just mistaken at a negli-gible level, because the error recovery modulerewards recall rather than precision and hencetends to overgenerate correction alternatives,even if not needed.AcknowledgmentsThis work was supported by the EuroMatrixPlusproject (IST-231720), which is funded by the EC un-der the 7th Framework Programme for Research andTechnological Development.ReferencesN.
Bertoldi, et al 2008.
Efficient speech translationthrough confusion network decoding.
IEEE Trans-actions on Audio, Speech, and Language Processing,16(8):1696?1705.E.
Brill and R. C. Moore.
2000.
An improved errormodel for noisy channel spelling correction.
In Pro-ceedings of ACL.
Hong Kong.J.
Carrera, et al 2009.
Machine trans-lation for cross-language social media.http://www.promt.com/company/technology/pdf/machine translation for cross language social media.pdf.F.
Casacuberta, et al 2008.
Recent efforts in spoken lan-guage processing.
IEEE Signal Processing Magazine,25(3):80?88.K.
W. Church and W. A. Gale.
1991.
Probability scor-ing for spelling correction.
Statistics and Computing,1(2):93?103.M.
W. Davis, et al 1995.
Text alignment in the realworld: Improving alignments of noisy translations us-ing common lexical features, string matching strate-gies and n-gram comparisons.
In Proceedings ofEACL, Dublin, Ireland.L.
Dey and S. M. Haque.
2009.
Studying the effects ofnoisy text on text mining applications.
In Proceedingsof AND, pages 107?114, Barcelona, Spain.D.
Fossati and B.
Di Eugenio.
2008.
I saw tree trees inthe park: How to correct real-word spelling mistakes.In Proceedings of LREC, Marrakech, Morocco.G.
Hirst and A. Budanitsky.
2005.
Correcting real-wordspelling errors by restoring lexical cohesion.
NaturalLanguage Engineering, 11(01):87?111.P.
Koehn, et al 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL- Demo and Poster Sessions, pages 177?180, Prague,Czech Republic.K.
Kukich.
1992.
Spelling correction for the telecom-munications network for the deaf.
Communications ofthe ACM, 35(5):80?90.D.
Kushal, et al 2003.
Mining the peanut gallery:opinion extraction and semantic classification of prod-uct reviews.
In Proceedings of the WWW conference,pages 519?528, Budapest, Hungary.L.
Mangu, et al 2000.
Finding consensus in speechrecognition: Word error minimization and other appli-cations of confusion networks.
Computer, Speech andLanguage, 14(4):373?400.R.
Mitton.
1995.
English Spelling and the Computer(Studies in Language and Linguistics).
Addison Wes-ley Publishing Company.J.
Pedler.
2007.
Computer correction of real-wordspelling errors in dyslexic text.
Ph.D. thesis, Univer-sity of London.M.
Reynaert.
2006.
Corpus-induced corpus cleanup.
InProceedings of LREC, Genoa, Italy.J.
Schaback and F. Li.
2007.
Multi-level feature extrac-tion for spelling correction.
In IJCAI - Workshop onAnalytics for Noisy Unstructured Text Data, pages 79?86, Hyderabad, India.J.
Schler, et al 2006.
Effects of age and gender on blog-ging.
In Proceedings of AAAI-CAAW, Palo Alto, CA.A.
Stolcke.
2002.
Srilm - an extensible language model-ing toolkit.
In Proceedings of ICSLP, Denver, CO.L.
V. Subramaniam, et al 2009.
A survey of types of textnoise and techniques to handle noisy text.
In Proceed-ings of AND, pages 115?122, Barcelona, Spain.K.
Toutanova and R. C. Moore.
2002.
Pronunciationmodeling for improved spelling correction.
In Pro-ceedings of ACL, pages 144?151, Philadelphia, PAS.
Vogel.
2003.
Using noisy biligual data for statisti-cal machine translation.
In Proceedings of EACL, Bu-dapest, Hungary.419
