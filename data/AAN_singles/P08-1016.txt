Proceedings of ACL-08: HLT, pages 130?138,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLexicalized phonotactic word segmentationMargaret M. FleckDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61801, USAmfleck@cs.uiuc.eduAbstractThis paper presents a new unsupervised algo-rithm (WordEnds) for inferring word bound-aries from transcribed adult conversations.Phone ngrams before and after observedpauses are used to bootstrap a simple dis-criminative model of boundary marking.
Thisfast algorithm delivers high performance evenon morphologically complex words in Englishand Arabic, and promising results on accuratephonetic transcriptions with extensive pronun-ciation variation.
Expanding training data be-yond the traditional miniature datasets pushesperformance numbers well above those previ-ously reported.
This suggests that WordEndsis a viable model of child language acquisitionand might be useful in speech understanding.1 IntroductionWords are essential to most models of language andspeech understanding.
Word boundaries define theplaces at which speakers can fluently pause, andlimit the application of most phonological rules.Words are a key constituent in structural analy-ses: the output of morphological rules and the con-stituents in syntactic parsing.
Most speech recog-nizers are word-based.
And, words are entrenchedin the writing systems of many languages.Therefore, it is generally accepted that childrenlearning their first language must learn how to seg-ment speech into a sequence of words.
Similar,but more limited, learning occurs when adults hearspeech containing unfamiliar words.
These wordsmust be accurately delimited, so that they can beadded to the lexicon and nearby familiar words rec-ognized correctly.
Current speech recognizers typi-cally misinterpret such speech.This paper will consider algorithms which seg-ment phonetically transcribed speech into words.For example, Figure 1 shows a transcribed phrasefrom the Buckeye corpus (Pitt et al, 2005; Pitt etal., 2007) and the automatically segmented output.Like almost all previous researchers, I use human-transcribed input to work around the limitations ofcurrent speech recognizers.In most available datasets, words are transcribedusing standard dictionary pronunciations (hence-forth ?dictionary transcriptions?).
These transcrip-tions are approximately phonemic and, more impor-tantly, assign a constant form to each word.
I willalso use one dataset with accurate phonetic tran-scriptions, including natural variation in the pronun-ciation of words.
Handling this variation is an im-portant step towards eventually using phone latticesor features produced by real speech recognizers.This paper will focus on segmentation of speechbetween adults.
This is the primary input for speechrecognizers.
Moreover, understanding such speechis the end goal of child language acquisition.
Modelstested only on simplified child-directed speech areincomplete without an algorithm for upgrading theunderstander to handle normal adult speech.2 The task in more detailThis paper uses a simple model of the segmentationtask, which matches prior work and the availabledatasets.
Possible enhancements to the model arediscussed at the end.130"all the kids in there # are people that have kids # or that are having kids"IN REAL: ohlThikidsinner # ahrpiyp@lThA?HAvkids # ohrThADurHAviynqkidsDICT: ahlThiykidzinTher # ahrpiyp@lThAtHAvkidz # owrThAtahrHAvinqkidzOUT REAL: ohl Thi kids inner # ahr piyp@l ThA?
HAv kids # ohr ThADur HAviynq kidsDICT: ahl Thiy kidz in Ther # ahr piyp@l ThAt HAv kidz # owr ThAt ahr HAvinq kidzFigure 1: Part of Buckeye corpus dialog 2101a, in accurate phonetic transcription (REAL) and dictionary pronuncia-tions (DICT).
Both use modified arpabet, with # marking pauses.
Notice the two distinct pronunciations of ?that?
inthe accurate transcription.
Automatically inserted word boundaries are shown at bottom.2.1 The input dataThis paper considers only languages with an estab-lished tradition of words, e.g.
not Chinese.
I assumethat the authors of each corpus have given us reason-able phonetic transcriptions and word boundaries.The datasets are informal conversations in which de-batable word segmentations are rare.The transcribed data is represented as a sequenceof phones, with neither prosodic/stress informationnor feature representations for the phones.
Thesephone sequences are presented to segmentation al-gorithms as strings of ASCII characters.
Largephonesets may be represented using capital lettersand punctuation or, more readably, using multi-character phone symbols.
Well-designed (e.g.
easilydecodable) multi-character codes do not affect thealgorithms or evaluation metrics in this paper.
Test-ing often also uses orthographic datasets.Finally, the transcriptions are divided into?phrases?
at pauses in the speech signal (silences,breaths, etc).
These pause phrases are not neces-sarily syntactic or prosodic constituents.
Disfluen-cies in conversational speech create pauses whereyou might not expect them, e.g.
immediately fol-lowing the definite article (Clark and Wasow, 1998;Fox Tree and Clark, 1997).
Therefore, I have chosencorpora in which pauses have been marked carefully.2.2 Affixes and syllablesA theory of word segmentation must explain how af-fixes differ from free-standing function words.
Forexample, we must explain why English speakersconsider ?the?
to be a word, but ?-ing?
to be an affix,although neither occurs by itself in fluent preparedEnglish.
We must also explain why the Arabic de-terminer ?Al-?
is not a word, though its syntactic andsemantic role seems similar to English ?the?.Viewed another way, we must show how to esti-mate the average word length.
Conversational En-glish has short words (about 3 phones), becausemost grammatical morphemes are free-standing.Languages with many affixes have longer words,e.g.
my Arabic data averages 5.6 phones per word.Pauses are vital for deciding what is an af-fix.
Attempts to segment transcriptions withoutpauses, e.g.
(Christiansen et al, 1998), have workedpoorly.
Claims that humans can extract words with-out pauses seem to be based on psychological exper-iments such as (Saffran, 2001; Jusczyk and Aslin,1995) which conflate words and morphemes.
Eventhen, explicit boundaries seem to improve perfor-mance (Seidl and Johnson, 2006).Another significant part of this task is finding syl-lable boundaries.
For English, many phone stringshave multiple possible syllabifications.
Becausewords average only 1.26 syllables, segmenting pre-syllabified input has a very high baseline: 100% pre-cision and 80% recall of boundary positions.2.3 Algorithm testingUnsupervised algorithms are presented with thetranscription, divided only at phrase boundaries.Their task is to infer the phrase-internal word bound-aries.
The primary worry in testing is that develop-ment may have biased the algorithm towards a par-ticular language, speaking style, and/or corpus size.Addressing this requires showing that different cor-pora can be handled with a common set of parame-ter settings.
Therefore a test/training split within onecorpus serves little purpose and is not standard.Supervised algorithms are given training datawith all word boundaries marked, and must inferword boundaries in a separate test set.
Simple su-pervised algorithms perform extremely well (Cairnset al, 1997; Teahan et al, 2000), but don?t addressour main goal: learning how to segment.Notice that phrase boundaries are not randomly131selected word boundaries.
Syntactic and commu-nicative constraints make pauses more likely at cer-tain positions than others.
Therefore, the ?super-vised?
algorithms for this task train on a representa-tive set of word boundaries whereas ?unsupervised?algorithms train on a biased set of word boundaries.Moreover, supplying all the word boundaries foreven a small amount of data effectively tells the su-pervised algorithms the average word length, a pa-rameter which is otherwise not easy to estimate.Standard evaluation metrics include the precision,recall and F-score 1 of the phrase-internal bound-aries (BP, BR, BF), of the extracted word tokens(WP, WR, WF), and of the resulting lexicon of wordtypes (LP, LR, LF).
Outputs don?t look good untilBF is at least 90%.3 Previous workLearning to segment words is an old problem, withextensive prior work surveyed in (Batchelder, 2002;Brent and Cartwright, 1996; Cairns et al, 1997;Goldwater, 2006; Hockema, 2006; Rytting, 2007).There are two major approaches.
Phonotactic meth-ods model which phone sequences are likely withinwords and which occur primarily across or adjacentto word boundaries.
Language modelling methodsbuild word ngram models, like those used in speechrecognition.
Statistical criteria define the ?best?model fitting the input data.
In both cases, detailsare complex and variable.3.1 Phonotactic MethodsSupervised phonotactic methods date back at leastto (Lamel and Zue, 1984), see also (Harringtonet al, 1989).
Statistics of phone trigrams providesufficient information to segment adult conversa-tional speech (dictionary transcriptions with sim-ulated phonology) with about 90% precision and93% recall (Cairns et al, 1997), see also (Hockema,2006).
Teahan et al?s compression-based model(2000) achieves BF over 99% on orthographic En-glish.
Segmentation by adults is sensitive to phono-tactic constraints (McQueen, 1998; Weber, 2000).To build unsupervised algorithms, Brent andCartwright suggested (1996) inferring phonotac-tic constraints from phone sequences observed at1F = 2PRP+R where P is the precision and R is the recall.phrase boundaries.
However, experimental resultsare poor.
Early results using neural nets by Cairnset al (1997) and Christiansen et al(1998) are dis-couraging.
Rytting (2007) seems to have the bestresult: 61.0% boundary recall with 60.3% preci-sion 2 on 26K words of modern Greek data, aver-age word length 4.4 phones.
This algorithm usedmutual information plus phrase-final 2-phone se-quences.
He obtained similar results (Rytting, 2004)using phrase-final 3-phone sequences.Word segmentation experiments by Christiansenand Allen (1997) and Harrington et al (1989).
sim-ulated the effects of pronunciation variation and/orrecognizer error.
Rytting (2007) uses actual speechrecognizer output.
These experiments broke usefulnew ground, but poor algorithm performance (BF?
50% even on dictionary transcriptions) makes ithard to draw conclusions from their results.3.2 Language modelling methodsSo far, language modelling methods have been moreeffective.
Brent (1999) and Venkataraman (2001)present incremental splitting algorithms with BFabout 82% 3 on the Bernstein-Ratner (BR87) corpusof infant-directed English with disfluencies and in-terjections removed (Bernstein Ratner, 1987; Brent,1999).
Batchelder (2002) achieved almost identicalresults using a clustering algorithm.
The most re-cent algorithm (Goldwater, 2006) achieves a BF of85.8% using a Dirichlet Process bigram model, esti-mated using a Gibbs sampling algorithm.4Language modelling methods incorporate a biastowards re-using hypothesized words.
This suggeststhey should systematically segment morphologicallycomplex words, so as to exploit the structure theyshare with other words.
Goldwater, the only authorto address this issue explicitly, reports that her algo-rithm breaks off common affixes (e.g.
?ing?, ?s?
).Batchelder reports a noticable drop in performanceon Japanese data, which might relate to its morecomplex words (average 4.1 phones).2These numbers have been adjusted so as not to includeboundaries between phrases.3Numbers are from Goldwater?s (2006) replication.4Goldwater numbers are from the December 2007 versionof her code, with its suggested parameter values: ?0 = 3000,?1 = 300, p# = 0.2.1324 The new approachPrevious algorithms have modelled either wholewords or very short (e.g.
2-3) phone sequences.The new approach proposed in this paper, ?lexical-ized phonotactics,?
models extended sequences ofphones at the starts and ends of word sequences.This allows a new algorithm, called WordEnds, tosuccessfully mark word boundaries with a simple lo-cal classifier.4.1 The ideaThis method models sequences of phones that startor end at a word boundary.
When words are long,such a sequence may cover only part of the worde.g.
a group of suffixes or a suffix plus the end of thestem.
A sequence may also include parts of multipleshort words, capturing some simple bits of syntax.These longer sequences capture not only purelyphonotactic constraints, but also information aboutthe inventory of lexical items.
This improves han-dling of complex, messy inputs.
(Cf.
Ando andLee?s (2000) kanji segmenter.
)On the other hand, modelling only partial wordshelps the segmenter handle long, infrequent words.Long words are typically created by productive mor-phology and, thus, often start and end just like otherwords.
Only 32% of words in Switchboard occurboth before and after pauses, but many of the other68% have similar-looking beginnings or endings.Given an inter-character position in a phrase, itsright and left contexts are the character sequencesto its right and left.
By convention, phrases inputto WordEnds are padded with a single blank at eachend.
So the middle position of the phrase ?afunjoke?has right context ?jokeunionsq?
and left context ?unionsqafun.
?Since this is a word boundary, the right context lookslike the start of a real word sequence, and the leftcontext looks like the end of one.
This is not true forthe immediately previous position, which has rightcontext ?njokeunionsq?
and left context ?unionsqafu.
?Boundaries will be marked where the right andleft contexts look like what we have observed at thestarts and ends of phrases.4.2 Statistical modelTo formalize this, consider a fixed inter-characterposition in a phrase.
It may be a word boundary (b)or not (?b).
Let r and l be its right and left contexts.The input data will (see Section 4.3) give us P (b|r)and P (b|l).
Deciding whether to mark a boundary atthis position requires estimating P (b|r, l).To express P (b|r, l) in terms of P (b|l) andP (b|r), I will assume that r and l are conditionallyindependent given b.
This corresponds roughly to aunigram language model.
Let P (b) be the probabil-ity of a boundary at a random inter-character posi-tion.
I will assume that the average word length, andtherefore P (b), is not absurdly small or large.P (b|r, l) is P (r,l|b)P (b)P (r,l) .
Conditional indepen-dence implies that this is P (r|b)P (l|b)P (b)P (r,l) , which isP (r)P (b|r)P (l)P (b|l)P (b)P (r,l) .
This isP (b|r)P (b|l)QP (b) where Q =P (r,l)P (r)P (l) .
Q is typically not 1, because a right andleft context often co-occur simply because they bothtend to occur at boundaries.To estimate Q, write P (r, l) as P (r, l, b) +P (r, l,?b).
Then P (r, l, b) is P (r)P (b|r)P (l)P (b|l)P (b) .
Ifwe assume that r and l are also conditionally inde-pendent given ?b, then a similar equation holds forP (r, l,?b).
So Q = P (b|r)P (b|l)P (b) +P (?b|r)P (?b|l)P (?b)Contexts that occur primarily inside words (e.g.not at a syllable boundary) often restrict the adjacentcontext, violating conditional independence given?b.
However, in these cases, P (b|r) and/or P (b|l)will be very low, so P (b|r, l) will be very low.
So(correctly) no boundary will be marked.Thus, we can compute P (b|r, l) from P (b|r),P (b|l), and P (b).
A boundary is marked ifP (b|r, l) ?
0.5.4.3 Estimating context probabilitiesEstimation of P (b|r) and P (b|l) uses a simplengram backoff algorithm.
The details will be shownfor P (b|l).
P (b|r) is similar.Suppose for the moment that word boundaries aremarked.
The left context l might be very long andunusual.
So we will estimate its statistics using ashorter lefthand neighborhood l?.
P (b|l) is then es-timated as the number of times l?
occurs before aboundary, divided by the total number of times l?occurs in the corpus.The suffix l?
is chosen to be the longest suffix ofl which occurs at least 10 times in the corpus, i.e.often enough for a reliable estimate in the presence133corpus language transcription sm size med size lg size pho/wd wd/phr hapaxBR87 English dictionary 33K ?
?
2.9 3.4 31.7Switchboard English dictionary 34K 409K 3086K 3.1 5.9 33.8Switchboard English orthographic 34K 409K 3086K [3.8] 5.9 34.2Buckeye English dictionary 32K 290K ?
3.1 5.9 41.9Buckeye English phonetic 32K 290K ?
2.9 5.9 66.0Arabic Arabic dictionary 30K 405K ?
5.6 5.9 60.3Spanish Spanish dictionary 37K 200K ?
3.7 8.4 49.1Table 1: Key parameters for each test dataset include the language, transcription method, number of words (small,medium, large subsets), average phones per word, average words per phrase, and percent of word types that occur onlyonce (hapax).
Phones/word is replaced by characters/word for the orthographic corpus.of noise.5 l?
may cross word boundaries and, if ourposition is near a pause, may contain the blank at thelefthand end of the phrase.
The length of l?
is limitedto Nmax characters to reduce overfitting.Unfortunately, our input data has boundaries onlyat pauses (#).
So applying this method to the raw in-put data produces estimates of P (#|r) and P (#|l).Because phrase boundaries are not a representativeselection of word boundaries, P (#|r) and P (#|l)are not good estimates of P (b|r) and P (b|l).
More-over, initially, we don?t know P (b).Therefore, WordEnds bootstraps the estimationusing a binary model of the relationship betweenword and phrase boundaries.
To a first approxima-tion, an ngram occurs at the end of a phrase if andonly if it can occur at the end of a word.
Since themagnitude of P (#, l) isn?t helpful, we simply checkwhether it is zero and, accordingly, set P (b|l) to ei-ther zero or a constant, very high value.In fact, real data contains phrase endings cor-rupted by disfluencies, foreign words, etc.
So Word-Ends actually sets P (b|l) high only if P (#|l) isabove a threshold (currently 0.003) chosen to reflectthe expected amount of corruption.In the equations from Section 4.2, if either P (b|r)or P (b|l) is zero, then P (b|r, l) is zero.
If both val-ues are very high, then Q is P (b|r)P (b|l)P (b) + ?, with ?very small.
So P (b|r, l) is close to 1.
So, in the boot-strapping phase, the test for marking a boundary isindependent of P (b) and reduces to testing whetherP (#|r) and P (#|l) are both over threshold.So, WordEnds estimates P (#|r) and P (#|l)from the input data, then uses this bootstrapping5A single character is used if no suffix occurs 10 times.method (Nmax = 5) 6 to infer preliminary wordboundaries.
The preliminary boundaries are used toestimate P (b) and to re-estimate P (b|r) and P (b|l),using Nmax = 4.
Final boundaries are then marked.5 Mini-morphIn a full understanding system, output of the wordsegmenter would be passed to morphological and lo-cal syntactic processing.
Because the segmenter ismyopic, certain errors in its output would be eas-ier to fix with the wider perspective available tothis later processing.
Because standard models ofmorphological learning don?t address the interactionwith word segmentation, WordEnds does a simpleversion of this repair process using a placeholder al-gorithm called Mini-morph.Mini-morph fixes two types of defects in the seg-mentation.
Short fragments are created when twonearby boundaries represent alternative reasonablesegmentations rather than parts of a common seg-mentation.
For example, ?treestake?
has potentialboundaries both before and after the s. This issuewas noted by Harrington et al (1988) who used a listof known very short words to detect these cases.
Seealso (Cairns et al, 1997).
Also, surrounding wordssometimes mislead WordEnds into undersegmentinga phone sequence which has an ?obvious?
analysisusing well-established component words.Mini-morph classifies each word in the segmenta-tion as a fragment, a word that is reliable enough touse in subdividing other words, or unknown status.6Values for Nmax were chosen empirically.
They could beadjusted for differences in entropy rate, but this is very similaracross the datasets in this paper.134Because it has only a feeble model of morphology,Mini-morph has been designed to be cautious: mostwords are classified as unknown.To classify a word, we compare its frequency w asa word in the segmentation to the frequencies p and swith which it occurs as a prefix and suffix of wordsin the segmentation (including itself).
The word?sfragment ratio f is 2wp+s .Values of f are typically over 0.8 for freely occur-ring words, under 0.1 for fragments and strongly-attached affixes, and intermediate for clitics, someaffixes, and words with restricted usage.
However,most words haven?t been seen enough times for fto be reliable.
So a word is classified as a fragmentif p + s ?
1000 and f ?
0.2.
It is classified as areliable word if p + s ?
50 and f ?
0.5.To revise the input segmentation of the corpus,Mini-morph merges each fragment with an adjacentword if the newly-created merged word occurredat least 10 times in the input segmentation.
Whenmergers with both adjacent words are possible, thealgorithm alternates which to prefer.
Each word isthen sudivided into a sequence of reliable words,when possible.
Because words are typically shortand reliable words rare, a simple recursive algorithmis used, biased towards using shorter words.
7WordEnds calls Mini-morph twice, once to revisethe preliminary segmentation produced by the boot-strapping phase and a second time to revise the finalsegmentation.6 Test corporaWordEnds was tested on a diverse set of seven cor-pora, summarized in Table 1.
Notice that the Arabicdataset has much longer words than those used byprevious authors.
Subsets were extracted from thelarger corpora, to control for training set size.
Gold-water?s algorithm, the best performing of previousmethods, was also tested on the small versions.
8The first three corpora all use dictionary tran-scriptions with 1-character phone symbols.
TheBernstein-Ratner (BR87) corpus was describedabove (Section 3.2).
The Arabic corpus was createdby removing punctuation and word boundaries fromthe Buckwalter version of the LDC?s transcripts of7Subdivision is done only once for each word type.8It is too slow to run on the larger ones.Gulf Arabic Conversational Telephone Speech (Ap-pen, 2006).
Filled pauses and foreign words werekept as is.
Word fragments were kept, but the telltalehyphens were removed.
The Spanish corpus wasproduced in a similar way from the Callhome Span-ish dataset (Wheatley, 1996), removing all accents.Orthographic forms were used for words withoutpronunciations (e.g.
foreign, fragments)The other two English dictionary transcriptionswere produced in a similar way from the Buckeyecorpus (Pitt et al, 2005; Pitt et al, 2007) and Missis-sippi State?s corrected version of the LDC?s Switch-board transcripts (Godfrey and Holliman, 1994;Deshmukh et al, 1998).
These use a ?readablephonetic?
version of arpabet.
Each phone is rep-resented with a 1?2 character code, chosen to looklike English orthography and to ensure that charactersequences decode uniquely into phone sequences.Buckeye does not provide dictionary pronunciationsfor word fragments, so these were transcribed as?X?.
Switchboard was also transcribed using stan-dard English orthography.The Buckeye corpus also provides an accuratephonetic transcription of its data, showing allo-phonic variation (e.g.
glottal stop, dental/nasalflaps), segment deletions, quality shifts/uncertainty,and nasalization.
Some words are ?massively?
re-duced (Johnson, 2003), going well beyond standardphonological rules.
We represented its 64 phonesusing codes with 1?3 characters.7 Test resultsTable 2 presents test results for the small corpora.The numbers for the four English dictionary and or-thographic transcriptions are very similar.
This con-firms the finding of Batchelder (2002) that variationsin transcription method have only minor impacts onsegmenter performance.
Performance seems to belargely determined by structural and lexical proper-ties (e.g.
word length, pause frequency).For the English dictionary datasets, the primaryoverall evaluation numbers (BF and WF) for thetwo algorithms differ less than the variation createdby tweaking parameters or re-running Goldwater?s(randomized) algorithm.
Both degrade similarly onthe phonetic version of Buckeye.
The most visi-ble overall difference is speed.
WordEnds processes135WordEnds Goldwatercorpus transcription BP BR BF WF LF BP BR BF WF LFBR87 dictionary 94.6 73.7 82.9 70.7 36.6 89.2 82.7 85.8 72.5 56.2Switchboard dictionary 91.3 80.5 85.5 72.0 37.4 73.9 93.5 82.6 65.8 27.8Switchboard orthographic 90.0 75.5 82.1 66.3 33.7 73.1 92.4 81.6 63.6 28.4Buckeye dictionary 89.7 82.2 85.8 72.3 37.4 74.6 94.8 83.5 68.1 26.7Buckeye phonetic 71.0 64.1 67.4 44.1 28.6 49.6 95.0 65.1 35.4 12.8Arab dictionary 88.1 68.5 77.1 56.6 40.4 47.5 97.4 63.8 32.6 9.5Spanish dictionary 89.3 48.5 62.9 38.7 16.6 69.2 92.8 79.3 57.9 17.0Table 2: Results for WordEnds and Goldwater on the small test corpora.
See Section 2.3 for definitions of metrics.medium w/out morph medium largecorpus transcription BF WF LF BF WF LF BF WF LFSwitchboard dictionary 90.4 78.8 39.4 93.0 84.8 44.2 94.7 88.1 44.3Switchboard orthographic 89.6 77.4 37.3 91.6 81.8 41.1 94.1 87.0 41.1Buckeye dictionary 91.2 80.3 41.5 93.7 86.1 47.8 ?
?
?Buckeye phonetic 72.1 48.4 27.1 75.0 54.2 28.2 ?
?
?Arab dictionary 85.7 69.1 49.5 86.4 70.6 50.0 ?
?
?Spanish dictionary 75.1 52.2 19.7 76.3 55.0 20.2 ?
?
?Table 3: Results for WordEnds on the medium and large datasets, also on the medium dataset without Mini-morph.See Table 1 for dataset sizes.each small dataset in around 30-40 seconds.
Gold-water requires around 2000 times as long: 14.5-32hours, depending on the dataset.However, WordEnds keeps affixes on wordswhereas Goldwater?s algorithm removes them.
Thiscreates a systematic difference in the balance be-tween boundary recall and precision.
It also causesGoldwater?s LF values to drop dramatically be-tween the child-directed BR87 corpus and the adult-directed speech.
For the same reason, WordEndsmaintains good performance on the Arabic dataset,but Goldwater?s performance (especially LF) ismuch worse.
It is quite likely that Goldwater?s al-gorithm is finding morphemes rather than words.Datasets around 30K words are traditional for thistask.
However, a child learner has access to muchmore data, e.g.
Weijer (1999) measured 1890 wordsper hour spoken near an infant.
WordEnds per-forms much better when more data is available (Ta-ble 3).
Numbers for even the harder datasets (Buck-eye phonetic, Spanish) are starting to look promis-ing.
The Spanish results show that data with infre-quent pauses can be handled in two very differentways: aggressive model-based segmentation (Gold-water) or feeding more data to a more cautious seg-menter (WordEnds).The two calls to Mini-morph sometimes make al-most no difference, e.g.
on the Arabic data.
Butit can make large improvements, e.g.
BF +6.9%,WF +10.5%, LF +5.8% on the BR corpus.
Table 3shows details for the medium datasets.
Its contribu-tion seems to diminish as the datasets get bigger, e.g.improvements of BF +4.7%, WF +9.3%, LF +3.7%on the small dictionary Switchboard corpus but onlyBF +1.3%, WF +3.3%, LF +3.4% on the large one.8 Some specifics of performanceExamining specific mistakes confirms that Word-Ends does not systematically remove affixes on En-glish dictionary data.
On the large Switchboard cor-pus, ?-ed?
is never removed from its stem and ?-ing?is removed only 16 times.
The Mini-morph post-processor misclassifies, and thus segments off, someaffixes that are homophonous with free-standingwords, such as ?-en?/?in?
and ?-es?/?is?.
A smartermodel of morphology and local syntax could proba-bly avoid this.136There is a visible difference between English?the?
and the Arabic determiner ?Al-?.
The En-glish determiner is almost always segmented off.From the medium-sized Switchboard corpus, only434 lexical items are posited with ?the?
attached to afollowing word.
Arabic ?Al?
is sometimes attachedand sometimes segmented off.
In the medium Ara-bic dataset, the correct and computed lexicons con-tain similar numbers of words starting with Al (4873and 4608), but there is only partial overlap (2797words).
Some of this disagreement involves foreignlanguage nouns, which the markup in the originalcorpus separates from the determiner.9Mistakes on twenty specific items account for24% of the errors on the large Switchboard corpus.The first two items, accounting for over 11% of themistakes, involve splitting ?uhhuh?
and ?umhum?.Most of the rest involve merging common colloca-tions (e.g.
?a lot?)
or splitting common compoundsthat have a transparent analysis (e.g.
?something?
).9 Discussion and conclusionsPerformance of WordEnds is much stronger thanprevious reported results, including good results onArabic and promising results on accurate phonetictranscriptions.
This is partly due to good algorithmdesign and partly due to using more training data.This sets a much higher standard for models of childlanguage acquisition and also suggests that it is notcrazy to speculate about inserting such an algorithminto the speech recognition pipeline.Performance would probably be improved by bet-ter models of morphology and/or phonology.
Anngram model of morpheme sequences (e.g.
likeGoldwater uses) might avoid some of the mistakesmentioned in Section 8.
Feature-based or gesturalphonology (Browman and Goldstein, 1992) mighthelp model segmental variation.
Finite-state mod-els (Belz, 2000) might be more compact.
Prosody,stress, and other sub-phonemic cues might disam-biguate some problem situations (Hockema, 2006;Rytting, 2007; Salverda et al, 2003).However, it is not obvious which of these ap-proaches will actually improve performance.
Ad-ditional phonetic features may not be easy to detect9The author does not read Arabic and, thus, is not in a posi-tion to explain why the annotaters did this.reliably, e.g.
marking lexical stress in the presenceof contrastive stress and utterance-final lengthening.The actual phonology of fast speech may not bequite what we expect, e.g.
performance on the pho-netic version of Buckeye was slightly improved bymerging nasal flap with n, and dental flap with d andglottal stop.
The sets of word initial and final seg-ments may not form natural phonological classes,because they are partly determined by morpholog-ical and lexical constraints (Rytting, 2007).Moreover, the strong performance from the basicsegmental model makes it hard to rule out the possi-bility that high performance could be achieved, evenon data with phonetic variation, by throwing enoughtraining data at a simple segmental algorithm.Finally, the role of child-directed speech needs tobe examined more carefully.
Child-directed speechdisplays helpful features such as shorter phrases andfewer reductions (Bernstein Ratner, 1996; van deWeijer, 1999).
These features may make segmenta-tion easier to learn, but the strong results presentedhere for adult-directed speech make it trickier to ar-gue that this help is necessary for learning.Moreover, it is not clear how learning to seg-ment child-directed speech might make it easier tolearn to segment speech directed at adults or olderchildren.
It?s possible that learning child-directedspeech makes it easier to learn the basic principlesof phonology, semantics, or higher-level linguisticstructure.
This might somehow feed back into learn-ing segmentation.
However, it?s also possible that itsonly raison d?e?tre is social: enabling earlier commu-nication between children and adults.AcknowledgmentsMany thanks to the UIUC prosody group, MitchMarcus, Cindy Fisher, and Sharon Goldwater.ReferencesRie Kubota Ando and Lillian Lee.
2000.
Mostly-Unsupervised Statistical Segmentation of Japanese.Proc ANLP-NAACL 2000:241?248.Appen Pty Ltd. 2006.
Gulf Arabic Conversational Tele-phone Speech, Transcripts Linguistic Data Consor-tium, PhiladelphiaEleanor Olds Batchelder 2002.
Bootstrapping the lexi-con: A computational model of infant speech segmen-tation.
Cognition 83, pp.
167?206.137Anja Belz 2000.
Multi-Syllable Phonotactic Modelling.5th ACL SIGPHON, pp.
46?56.Nan Bernstein Ratner.
1987.
The phonology of parentchild speech.
In K. Nelson and A.
Van Kleeck (Eds.
),Children?s Language: Vol 6, Lawrence Erlbaum.Nan Bernstein Ratner 1996.
From ?Signal to Syntax?
:But what is the Nature of the Signal?
In James Mor-gan and Katherine Demuth (eds) Signal to Syntax,Lawrence Erlbaum, Mahwah, NJ.Michael R. Brent.
1999.
An Efficient, ProbabalisticallySound Algorithm for Segmentation and Word Discov-ery.
Machine Learning 1999:71?105.Michael R. Brent and Timothy A. Cartwright.
1996.
Dis-tributional Regularity and Phonotactic Constraints areUseful for Segmentation Cognition 1996:93?125.C.
P. Browman and L. Goldstein.
1992.
Articulatoryphonology: An overview.
Phonetica 49:155?180.Paul Cairns, Richard Shillcock, Nick Chater, and JoeLevy.
1997.
Bootstrapping Word Boundaries: ABottom-up Corpus-based Approach to Speech Seg-mentation.
Cognitive Psychology, 33:111?153.Morten Christiansen and Joseph Allen 1997.
Copingwith Variation in Speech Segmentation GALA 1997.Morten Christiansen, Joseph Allen, Mark Seidenberg.1998.
Learning to Segment Speech Using MultipleCues: A Connectionist Model.
Language and Cogni-tive Processes 12/2?3, pp.
221-268.Herbert H. Clark and Thomas Wasow.
1998.
RepeatingWords in Spontaneous Speech.
Cognitive Psychology37:201?242.N.
Deshmukh, A. Ganapathiraju, A. Gleeson, J. Hamakerand J. Picone.
1998.
Resegmentation of Switch-board.
Proc.
Intern.
Conf.
on Spoken LanguageProcessing:1543-1546.Jean E. Fox Tree and Herbert H. Clark.
1997.
Pronounc-ing ?the?
as ?thee?
to signal problems in speaking.Cognition 62(2):151?167.John J. Godfrey and Ed Holliman.
1993.
Switchboard-1 Transcripts.
Linguistic Data Consortium, Philadel-phia, PA.Sharon Goldwater.
2006.
Nonparametric Bayesian Mod-els of Lexical Acquisition.
Ph.D. thesis, Brown Univ.Jonathan Harrington, Gordon Watson, and MaggieCooper.
1989.
Word boundary detection in broadclass and phoneme strings.
Computer Speech andLanguage 3:367?382.Jonathan Harrington, Gordon Watson, and MaggieCooper.
1988.
Word Boundary Identification fromPhoneme Sequence Constraints in Automatic Contin-uous Speech Recognition.
Coling 1988, pp.
225?230.Stephen A. Hockema.
2006.
Finding Words in Speech:An Investigation of American English.
LanguageLearning and Development, 2(2):119-146.Keith Johnson 2003.
Massive reduction in conversa-tional American English.
Proc.
of the Workshop onSpontaneous Speech: Data and Analysis.Peter W. Jusczyk and Richard N. Aslin.
1995.
Infants?Detection of the Sound Patterns of Words in FluentSpeech.
Cognitive Psychology 29(1)1?23.Lori F. Lamel and Victor W. Zue.
1984.
Properties ofConsonant Sequences within Words and Across WordBoundaries.
Proc.
ICASSP 1984:42.3.1?42.3.4.James M. McQueen.
1998.
Segmentation of ContinuousSpeech Using Phonotactics.
Journal of Memory andLanguage 39:21?46.Mark Pitt, Keith Johnson, Elizabeth Hume, Scott Kies-ling, and William Raymond.
2005.
The Buckeye Cor-pus of Conversational Speech: Labeling Conventionsand a Test of Transcriber Reliability.
Speech Commu-nication, 45, 90-95.M.
A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W.
Ray-mond., E. Hume, and E. Fosler-Lussier.
2007.
Buck-eye Corpus of Conversational Speech (2nd release)Department of Psychology, Ohio State University,Columbus, OHC.
Anton Rytting 2004.
Greek Word Segmentation usingMinimal Information.
HLT-NAACL 2004, pp.
78?85.C.
Anton Rytting 2007.
Preserving Subsegmental Vari-ation in Modelling Word Segmentation.
Ph.D. thesis,Ohio State, Columbus OH.J.
R. Saffran.
2001 Words in a sea of sounds: The outputof statistical learning.
Cognition 81:149-169.Anne Pier Salverda, Delphine Dahan, and James M. Mc-Queen.
2003.
The role of prosodic boundaries in theresolution of lexical embedding in speech comprehen-sion.
Cognition 90:51?89.Amanda Seidl and Elizabeth K. Johnson.
2006.
In-fant Word Segmentation Revisited: Edge AlignmentFacilitates Target Extraction.
Developmental Science9(6):565?573.W.
J. Teahan, Y. Wen, R. McNab, I. H. Witten 2000A compression-based algorithm for Chinese word seg-mentation.
Computational Linguistics 26/3, pp.
375?393.Anand Venkataraman.
2001.
A Statistical Model forWord Discovery in Transcribed Speech.
Computa-tional Linguistics, 27(3):351?372.A.
Weber.
2000 Phonotactic and acoustic cues for wordsegmentation.
Proc.
6th Intern.
Conf.
on Spoken Lan-guage Processing, Vol.
3: 782-785. ppJoost van de Weijer 1999.
Language Input for WordDiscovery.
Ph.D. thesis, Katholieke Universiteit Ni-jmegen.Barbara Wheatley.
1996.
CALLHOME Spanish Tran-scripts.
Linguistic Data Consortium, Philadelphia.138
