Text Categorization using Feature ProjectionsYoungjoong KoDepartment of Computer Science,Sogang University1 Sinsu-dong, Mapo-guSeoul, 121-742, Koreakyj@nlpzodiac.sogang.ac.kr,Jungyun SeoDepartment of Computer Science,Sogang University1 Sinsu-dong, Mapo-guSeoul, 121-742, Koreaseojy@ccs.sogang.ac.krAbstractThis paper proposes a new approach for textcategorization, based on a feature projectiontechnique.
In our approach, training data arerepresented as the projections of trainingdocuments on each feature.
The voting for aclassification is processed on the basis ofindividual feature projections.
The finalclassification of test documents isdetermined by a majority voting from theindividual classifications of each feature.Our empirical results show that the proposedapproach, Text Categorization using FeatureProjections (TCFP), outperforms k-NN,Rocchio, and Na?ve Bayes.
Most of all,TCFP is about one hundred times faster thank-NN.
Since TCFP algorithm is very simple,its implementation and training process canbe done very easily.
For these reasons,TCFP can be a useful classifier in the areas,which need a fast and high-performance textcategorization task.IntroductionAn issue of text categorization is to classifydocuments into a certain number of pre-definedcategories.
Text categorization is an activeresearch area in information retrieval andmachine learning.
A wide range of supervisedlearning algorithms has been applied to thisissue, using a training data set of categorizeddocuments.
The Na?ve Bayes (McCalum et al,1998; Ko et al, 2000), Nearest Neighbor (Yanget al, 2002), and Rocchio (Lewis et al, 1996)are well-known algorithms.Among these learning algorithms, we focuson the Nearest Neighbor algorithm.
In particular,the k-Nearest Neighbor (k-NN) classifier in textcategorization is one of the state-of-the-artmethods including Support Vector Machine(SVM) and Boosting algorithms.
Since theNearest Neighbor algorithm is much simplerthan the other algorithms, the k-NN classifier isintuitive and easy to understand, and it learnsquickly.
But the weak point of k-NN is too slowat running time.
The main computation is theon-line scoring of all training documents, inorder to find the k nearest neighbors of a testdocument.
In order to reduce the scalingproblem in on-line ranking, a number oftechniques have been studied in the literature.Techniques such as instance pruning technique(Wilson et al, 2000) and projection (Akkus et al,1996) are well known.The instance pruning technique is one of themost straightforward ways to speedclassification in a nearest neighbor system.
Itreduces time necessary and storage requirementsby removing instances from the training set.
Alarge number of such reduction techniques havebeen proposed, including the Condensed NearestNeighbor Rule (Hart, 1968), IB2 and IB3 (Aha etal., 1991), and the Typical Instance BasedLearning (Zhang, 1992).
These and otherreduction techniques were surveyed in depth in(Wilson et al, 1999), along with several newreduction techniques called DROP1-DROP5.
Ofthese, DROP4 had the best performance.Another trial to overcome this problem existson feature projections.
Akkus and Guvenirpresented a new approach to classification basedon feature projections (Akkus et al, 1996).
Theycalled their resulting algorithm k-NearestNeighbor on Feature Projections (k-NNFP).
Inthis approach, the classification knowledge isrepresented as the sets of projections of trainingdata on each feature dimension.
Theclassification of an instance is based on a votingby the k nearest neighbors of each feature in atest instance.
The resulting system allowed theclassification to be much faster than that ofk-NN and its performance were comparable withk-NN.In this paper, we present a particularimplementation of text categorization usingfeature projections.
When we applied the featureprojection technique to text categorization, wefound several problems caused by the specialproperties of text categorization problem.
Wedescribe these problems in detail and propose anew approach to solve them.
The proposedsystem shows the better performance than k-NNand it is much faster than k-NN.The rest of this paper is organized as follows.Section 1 simply presents k-NN and k-NNFPalgorithm.
Section 2 explains a new approachusing feature projections.
In section 3, wediscuss empirical results in our experiments.Section 4 is devoted to an analysis of timecomplexity and strong points of the newproposed classifier.
The final section presentsconclusions.1.
k-NN and k-NNFP AlgorithmIn this section, we simply describe k-NN andk-NNFP algorithm.1.1 k-NN AlgorithmAs an instance-based classification method,k-NN has been known as an effective approachto a broad range of pattern recognition and textclassification problems (Duda et al, 2001; Yang,1994).
In k-NN algorithm, a new input instanceshould belong to the same class as their k nearestneighbors in the training data set.
After all thetraining data is stored in memory, a new inputinstance is classified with the class of k nearestneighbors among all stored training instances.For the distance measure and the documentrepresentation, we use the conventional vectorspace model in text categorization; eachdocument is represented as a vector of termweights, and similarity between two documentsis measured by the cosine value of the anglebetween the corresponding vectors (Yang et al,2002).Let a document d with n terms (t) berepresented as the feature vector:>=< ),(),...,,(),,( 21 dtwdtwdtwd nrrrr (1)We compute the weight vectors for eachdocument using one of the conventional TF-IDFschemes (Salton et al, 1988).
The weight ofterm t in document d is calculated as follows:dnNdttfdtw trrr )/log()),(log1(),( ?+= (2)wherei) ),( dtwris the weight of term t in document drii) ),( dttfris the within-document Term Frequency (TF)iii) )/log(tnN is the Inverted Document Frequency(IDF)iv) N is the number of documents in the training setv) nt is the number of training documents in which toccursvi) ?
?= dt dtwd r rr 2),( is the 2-norm of vector drGiven an arbitrary test document d, the k-NNclassifier assigns a relevance score to eachcandidate category cj using the followingformula:???
?=jk DdRdj dddcsIrrrrr)(),cos(),( (3)where )(dRkrdenotes a set of the k nearestneighbors of document d and Dj is a set oftraining documents in class cj.1.2 k-Nearest Neighbor on Feature Projection(k-NNFP) AlgorithmThe k-NNFP is a variant of k-NN method.
Themain difference is that instances are projected ontheir features in the n-dimensional space (seefigure 1) and distance between two instances iscalculated according to a single feature.
Thedistance between two instances di and dj withregard to m-th feature tm is distm(tm(i), tm(j)) asfollows:),(),())(),(( jmimmm dtwdtwjtitdistmrr?= (4)where )(itm denotes m-th feature t in a instanceidr.The classification on a feature is doneaccording to votes of the k-nearest neighbors ofthat feature in a test instance.
The finalclassification of the test instance is determinedby a majority voting from individualclassification of each feature.
If there are nfeatures, this method returns n?
k votes whereask-NN method returns k votes.2.
A New Approach of TextCategorization on Feature ProjectionsFirst of all, we show an example of featureprojections in text categorization for more easyunderstanding.
We then enumerate the problemsto be duly considered when the featureprojection technique is applied to textcategorization.
Finally, we propose a newapproach using feature projections to overcomethese problems.2.1 An Example of Feature Projections inText CategorizationWe give a simple example of the featureprojections in text categorization.
To simplifyour description, we suppose that all documentshave just two features (f1 and f2) and twocategories (c1 and c2).
The TF-IDF value byformula (2) is used as the weight of a feautre.Each document is normalized as a unit vectorand each category has three instances:{ }3211 ,, dddc = and { }6542 ,, dddc = .
Figure 1shows how document vectors in conventionalvector space are transformed into featureprojections and stored on each feature dimension.The result of feature projections on a term (orfeature) can be seen as a set of weights ofdocuments for the term.
Since a term with 0.0weight is useless, the size of the set equals to theDF value of the term.							     		Figure 1.
Feature representation on featureprojections2.2 Problems in Applying Feature Projectionsto Text CategorizationThere are three problems: (1) the diversity of theDocument Frequency (DF) values of terms, (2)the property of using TF-IDF value of a term asthe weight of the feature, and (3) the lack ofcontextual information.2.2.1 The diversity of the Document Frequencyvalues of termsTable 1 shows a distribution of the DF values ofthe terms in Newsgroup data set.
The numericalvalues of Table 1 are calculated from trainingdata set with 16,000 documents and 10,000features chosen by feature selection.
The k infourth column means the number of nearestneighbors selected in k-NNFP; the k in k-NNFPwas set to 20 in our experiments.Table 1.
A distribution of the DF values of the termsin Newsgroup data setAverageDFmaximumDFMinimumDFThe # offeaturesDF < k (20)54.59 8,407 4 6,489According to Table 1, more than a half of thefeatures have the DF values less than k (20).This result is also explained by Zipf?s law.
Theproblem is that some features have the DFvalues less than k while other features have theDF values much greater than k. For a feature thathas a DF value less than k, all the elements ofthe feature projections on the feature could andshould participate for voting.
In this case, thenumber of elements chosen for voting is lessthan k. For other features, only maximum kelements among the elements of the featureprojections should be chosen for voting.Therefore, we need to normalize the voting ratiofor each feature.
As shown in formula (5), weuse a proportional voting method to normalizethe voting ratio.2.2.2 The property of using TF-IDF value of aterm as weight of a featureThe TF-IDF value of a term is their presumedvalue for identifying the content of a document(Salton et al, 1983).
On feature projections,elements with a high TF-IDF value for a featurebecome more useful classification criterions forthe feature than any elements with low TF-IDFvalues.
Thus we use only elements with TF-IDFvalues above the average TF-IDF value forvoting.
The selected elements also participate forproportional voting with the same importance asTF-IDF value of each element.
The voting ratioof each category cj in a feature tm(i) of a testdocument idris calculated by the followingformula:????
?=mmmmjjIltlmIltmlmm dtwltcydtwitcr)()(),())(,(),())(,(rr (5)In above formula, Im denotes a set of elementsselected for voting and { }1.0))(,( ?ltcy mj is afunction; if the category for a element )(ltm isequal to jc , the output value is 1.
Otherwise, theoutput value is 0.2.2.3 The lack of contextual informationSince each feature votes separately on featureprojections, contextual information is missed.We use the idea of co-occurrence frequency forapplying contextual information to ouralgorithm.To calculate a co-occurrence frequency valuebetween two terms ti and tl, we count the numberof documents that include both terms.
It isseparately calculated in each category of trainingdata.
Finally, the co-occurrence frequency valueof two terms is obtained by a maximum valueamong co-occurrence frequency values in eachcategory as follows:{ }),,(max),( jlicli cttcottcoj= (6)where ),( li ttco denotes a co-occurrencefrequency value of ti and tl, and),,( jli cttco denotes a co-occurrence frequencyvalue of ti and tl in a category cj.TF-IDF values of two terms ti and tj, whichoccur in a test document d, are modified byreflecting the co-occurrence frequency value.That is, the terms with a high co-occurrencefrequency value and a low category frequencyvalue could have higher term weights as follows:where i) tw(ti,d) denotes a modified term weightassigned to term ti, ii) cf denotes the categoryfrequency, the number of categories in which tiand tj co-occur, and iii) ),(max jttco i is themaximum value among all co-occurrencefrequency values.Finally, in order to apply these improvements(formulae (5) and (7)) to our algorithm, wecalculate the voting score of each category jcin mt of a test document idras the followingformula:))(,(),())(,( itcrdttwitcs mimm jj ?=r(8)Here, since the modified TF-IDF value of afeature in a test document has to be alsoconsidered as an important factor, it is used forvoting score instead of the simple voting value(1).
(7))),(log(max1)),(log(1)log(111),(),( ????????????????++????????
?++?=jijiii ttcottcocfdtwdttwrr2.3 A New Text Categorization Algorithmusing Feature ProjectionsA new text categorization algorithm usingfeature projections, named TCFP, is describedin the following:In training phase, our algorithm needs only avery simple process; the training documents areprojected on their each feature and numericalvalues for the proportional voting (formula (5))are calculated.3.
Empirical Evaluation3.1 Data Sets and Experimental SettingsTo test our proposed approach, we used twodifferent data sets.
For fair evaluation, we usedthe five-fold cross-validation method.
Therefore,all results of our experiments are averages offive runs.The Newsgroups data set, collected by KenLang, contains about 20,000 articles evenlydivided among 20 UseNet discussion groups(McCalum et al, 1998).
After removing wordsthat occur only once or on a stop word list, theaverage vocabulary from five training data has51,325 words (with no stemming).
The seconddata set comes from the WebKB project atCMU (Yang et al, 2002).
We use the four mostpopulous entity-representing categories: course,faculty, project, and student.
The resultingdata set consists of 4,198 pages with avocabulary of 18,742 words.
It is an uneven dataset; the largest category has 1,641 pages and thesmallest one has 503 pages.We applied statistical feature selection at apreprocessing stage for each classifier, using a2?
statistics (Yang et al, 1997).To compare TCFP to other algorithms forspeeding classification, we implementedk-NNFP and k-NN with reduction.
We usedDROP4 as reduction technique (Wilson et al,1999).
By DROP4, only 26% of the originaltraining documents in both data sets wasretained.
The k in k-NNFP was set to 20 andthe k in k-NN with reduction was set to 30.
Inaddition, we implement other classifiers: NaiveBayes, k-NN, and Rocchio classifier.
The k ink-NN was set to 30 and and ?=16 and ?=4 wereused in Rocchio classifier.As performance measures, we followed thestandard definition of recall, precision, and F1measure.
For evaluating performance averageacross categories, we used the micro-averagingmethod.3.2 Experimental Results3.2.1 Comparison of TCFP and k-NN (andother algorithms for speeding classification )Figure 2 and Table 2 show results from TCFP,k-NN, k-NN with reduction, and k-NNFP.
Inaddition, we added other type of TCFP to ourexperiment.
It was TCFP without contextualinformation (not using formula (7)).    	      	Figure 2.
Comparison of TCFP , k-NN, k-NNFP, andk-NN with reductiontest document: dr=<t1,t2,?,tn>, category set:C={c1,c2,?,cm}beginfor each category cjvote[cj] =0for each feature titw(ti,d) is calculated by formula (7)/* majority voting*/for each feature tifor each category cjvote[cj]=vote[cj]+tw(ti,d)?r(cj,ti)by formula (8)for each category cjprediction = ][maxarg jccvotejreturn predictionendTable 2.
The top micro-average F1 of each classifierTCFPTCFPwithoutcontextk-NN k-NNFPk-NNwithreduction85.41 85.14 85.15 81.93 81.34As a result, TCFP achieved the highestmicro-average F1 score.
Also, TCFP withoutcontextual information presented the nearlysame performance as k-NN.
Although, over allvocabulary sizes, TCFP without contextualinformation achieved little lower performancethan TCFP, it also can be useful classifier for itssimplicity and the fast running time(see Table 5).3.2.2 Comparison with other classifiersThe comparisons with other classifiers areshown in Figure 3 and Table 3.
In thisexperiment, we used Na?ve Bayes, and Rocchioclassifier.    	      Figure 3.
Comparison with other classifiersTable 3.
The top micro-average F1 of each classifierTCFP k-NN NB Rocchio85.41 85.15 82.51 81.68The result shows that TCFP produced the higherperformance than the other classifiers.3.2.3 Comparison of performances in anuneven data set, WebKB.In the above experiments, the Newsgroup dataset, which is an evenly divided data set, wasused.
If we use an uneven data set, we can face aproblem.
The cause of the problem is that acategory of the larger size has more votingcandidates than a category of the smaller size.We simply modified the majority voting scorecalculated in TCFP algorithm by the followingformula:{ } ??????
?= ),(/),(max][][ jicjj cdnumcdnumcvotecvotei(9)where num(d,cj) denotes the number of trainingdocument in category cj.The results of the modified algorithm areshown in Table 4.
As we can see in this table, themodified TCFP algorithm performed similarlyon the uneven data set, WebKB; the modifiedTCFP algorithm achieved the highest score.Table 4.
The top micro-average F1 of each classifierTCFP k-NN NB Rocchio k-NNFPk-NNwithreduction86.6 84.83 85.22 85.98 82.78 81.343.2.4 Run-time observationTable 5 shows the average running times in CPUseconds for each classifier on the Newsgroupdata.
Note that we included only testing phasewith 4,000 documents.Table 5.
Average running time of each classifierTCFPwithoutcontextRocchio NB TCFPk-NNwithreductionk-NN0.69 0.8 1.22 1.38 37.97 142.5Since the computations depend on thevocabulary sizes, we calculated the abovenumerical value by averaging running timesfrom 1,000 to 10,000 terms.
In Table 5, therunning time of TCFP is similar to other fasterclassifiers: Rocchio and Na?ve Bayes.
Also it isabout one hundred times faster than that of k-NN.Note that TCFP without contextual informationis the fastest classifier.4.
DiscussionsFirst of all, time complexities between k-NN andTCFP are compared.
Using the inverted-fileindexing of training documents, the timecomplexity of k-NN is O(m2l/n) (Yang, 1994),where m is the number of unique words in thedocument, l is the number of training documents,and n is the number of unique terms in thetraining collection.
TCFP has the timecomplexity of O(m2).
Even more, the timecomplexity of TCFP without contextualinformation is O(mc), where c is the number ofcategories.
That is, the classification of TCFPrequires a simple calculation in proportion to thenumber of unique terms in the test document.On the other hand, in k-NN, a search in thewhole training space must be done for each testdocument.The other strong points of TCFP are thesimplicity of algorithm and high-performance.Since the algorithm of TCFP is very simple likek-NN, TCFP can be implemented quite easilyand its training phase can also be a simpleprocess.
In our experiments, we achieved thebetter performance than k-NN.
We analyze thatour algorithm is more robust from irrelevantfeatures than k-NN.
When a document containsirrelevant features, the angle of the documentvector is changed in k-NN.
In TCFP, however,the irrelevant features contribute to only votingof the features.
Hence TCFP decreases the badeffect of the irrelevant features.ConclusionsIn this paper, a new type of text categorization,TCFP, has been presented.
This algorithm hasbeen compared with k-NN and other classifiers.Since each feature in TCFP individuallycontributes to the classification process, TCFP isrobust from irrelevant features.
By the simplicityof TCFP algorithm, its implementation andtraining process can be done very easily.
Theexperimental results show that, on theperformance, TCFP is superior to Rocchio,Na?ve Bayes, and k-NN.
Moreover, itoutperforms other classifiers for speedingclassification such as k-NNFP and k-NN withreduction.
In running time observation, TCFP isabout one hundred times faster than k-NN.Therefore, we can use TCFP in the areas, whichrequire a fast and high-performance textclassifier.ReferencesAha, D. W., Dennis K., and Marc K. A.
(1991)Instance-Based Learning Algorithms.
MachineLearning, vol.
6, pp.
37-66.Akkus A. and Guvenir H.A.
(1996) K NearestNeighbor Classification on Feature Projections.
InProceedings of ICML?
96, Itally, pp.
12-19.Duda R.O., Hart P.E., and Stork D.G.
(2001) PatternClassification.
John Wiley & Sons, Second Edition.Hart, P. E. (1968) The Condensed Nearest NeighborRule.
Institute of Electrical and ElectronicsEngineers Transactions on Information Theory.
Vol.14, pp.
515-516.Ko Y. and Seo J.
(2000) Automatic TextCategorization by Unsupervised Learning.
InProceedings of the 18th International Conference onComputational Linguistics (COLING), pp.
453-459.Lewis D.D., Schapire R.E., Callan J.P., and Papka R.(1996) Training Algorithms for Linear TextClassifiers.
In Proceedings of the 19th InternationalConference on Research and Development inInformation Retrieval (SIGIR?96), pp.289-297.McCallum A. and Nigam K. (1998) A Comparison ofEvent Models for Na?ve Bayes Text Classification.AAAI ?98 workshop on Learning for TextCategorization.
pp.
41-48.Salton G. and McGill M.J. (1983) Introduction toModern Information Retrieval.
McGraw-Hill, Inc.Salton G. and Buckley C. (1988) Term weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24:513-523.Wilson D. R. and Martinez T. R. (2000) AnIntegrated Instance-based Learning Algorithm,Computational Intelligence, Volume 16, Number 1,pp.
1-28.Wilson, D. R. and Martinez T. R. (2000) ReductionTechniques for Exemplar-Based LearningAlgorithms.
Machine Learning, vol.
38, no.
3, pp.257-286.Yang Y.
(1994) Expert network: Effective andefficient learning from human decisions in textcategorization and retrieval.
In Proceedings of 17thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval(SIGIR?94), pp 13-22.Yang Y. and Pedersen J.P. (1997) Feature selectionin statistical learning of text categorization.
In TheFourteenth International Conference on MachineLearning, pages 412-420.Yang Y., Slattery S., and Ghani R. (2002) A study ofapproaches to hypertext categorization, Journal ofIntelligent Information Systems, Volume 18,Number 2.Zhang, J.
(1992) Selecting Typical Instances inInstance-Based Learning.
Proceedings of the NinthInternational Conference on Machine Learning.
