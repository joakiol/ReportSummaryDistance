Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 540?545,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsFully Unsupervised Word Segmentation with BVE and MDLDaniel Hewlett and Paul CohenDepartment of Computer ScienceUniversity of ArizonaTucson, AZ 85721{dhewlett,cohen}@cs.arizona.eduAbstractSeveral results in the word segmentation liter-ature suggest that description length providesa useful estimate of segmentation quality infully unsupervised settings.
However, sincethe space of potential segmentations grows ex-ponentially with the length of the corpus, notractable algorithm follows directly from theMinimum Description Length (MDL) princi-ple.
Therefore, it is necessary to generatea set of candidate segmentations and selectbetween them according to the MDL princi-ple.
We evaluate several algorithms for gen-erating these candidate segmentations on arange of natural language corpora, and showthat the Bootstrapped Voting Experts algo-rithm consistently outperforms other methodswhen paired with MDL.1 IntroductionThe goal of unsupervised word segmentation is todiscover correct word boundaries in natural lan-guage corpora where explicit boundaries are absent.Often, unsupervised word segmentation algorithmsrely heavily on parameterization to produce the cor-rect segmentation for a given language.
The goalof fully unsupervised word segmentation, then, is torecover the correct boundaries for arbitrary naturallanguage corpora without explicit human parameter-ization.
This means that a fully unsupervised algo-rithm would have to set its own parameters basedonly on the corpus provided to it.In principle, this goal can be achieved by creat-ing a function that measures the quality of a seg-mentation in a language-independent way, and ap-plying this function to all possible segmentations ofthe corpora to select the best one.
Evidence from theword segmentation literature suggests that descrip-tion length provides a good approximation to thissegmentation quality function.
We discuss the Min-imum Description Length (MDL) principle in moredetail in the next section.
Unfortunately, evaluatingall possible segmentations is intractable, since a cor-pus of length n has 2n?1 possible segmentations.
Asa result, MDL methods have to rely on an efficientalgorithm to generate a relatively small number ofcandidate segmentations to choose between.
It isan empirical question which algorithm will generatethe most effective set of candidate segmentations.In this work, we compare a variety of unsupervisedword segmentation algorithms operating in conjunc-tion with MDL for fully unsupervised segmentation,and find that the Bootstrapped Voting Experts (BVE)algorithm generally achieves the best performance.2 Minimum Description LengthAt a formal level, a segmentation algorithm is afunction SEGMENT(c, ?)
that maps a corpus c anda vector of parameters ?
?
?
to one of thepossible segmentations of that corpus.
The goalof fully unsupervised segmentation is to reduceSEGMENT(c, ?)
to SEGMENT(c) by removing theneed for a human to specify a particular ?.
One wayto achieve this goal is to generate a set of candidatesegmentations by evaluating the algorithm for mul-tiple values of ?, and then choose the segmentationthat minimizes some cost function.
Thus, we candefine SEGMENT(c) in terms of SEGMENT(c, ?
):SEGMENT(c) = argmin??
?COST(SEGMENT(c, ?
))(1)540Now, selecting the best segmentation is treated as amodel selection problem, where each segmentationprovides a different model of the corpus.
Intuitively,a general approach is to choose the simplest modelthat explains the data, a principle known as Occam?sRazor.
In information theory, this intuitive princi-ple of simplicity or parsimony has been formalizedas the Minimum Description Length (MDL) princi-ple, which states that the most likely model of thedata is the one that requires the fewest bits to en-code (Rissanen, 1983).
The number of bits requiredto represent a model is called its description length.Previous work applying the MDL principle to seg-mentation (Yu, 2000; Argamon et al, 2004; Zhikovet al, 2010) is motivated by the observation that ev-ery segmentation of a corpus implicitly defines a lex-icon, or set of words.More formally, the segmented corpus S is a listof words s1s2 .
.
.
sN .
L(S), the lexicon implicitlydefined by S, is simply the set of unique words in S.The description length of S can then be broken intotwo components, the description length of the lex-icon and the description length of the corpus giventhe lexicon.
If we consider S as being generatedby sampling words from a probability distributionover words in the lexicon, the number of bits re-quired to represent each word si in S is simply itssurprisal, ?
logP (si).
The information cost of thecorpus given the lexicon is then computed by sum-ming the surprisal of each word si in the corpus:CODE(S|L(S)) = ?
?Ni=1logP (si) (2)To properly compute the description length of thesegmentation, we must also include the cost of thelexicon.
Adding in the description length of the lex-icon forces a trade-off between the lexicon size andthe size of the compressed corpus.
For purposes ofthe description length calculation, the lexicon is sim-ply treated as a separate corpus consisting of char-acters rather than words.
The description length canthen be computed in the usual manner, by summingthe surprisal of each character in each word in thelexicon:CODE(L(S)) = ?
?w?L(S)?k?wlogP (k) (3)where k ?
w refers to the characters in word win the lexicon.
As noted by Zhikov et al (Zhikovet al, 2010), an additional term is needed for theinformation required to encode the parameters of thelexicon model.
This quantity is normally estimatedby (k/2) log n, where k is the degrees of freedom inthe model and n is the length of the data (Rissanen,1983).
Substituting the appropriate values for thelexicon model yields:|L(S)| ?
12?
logN (4)The full description length calculation is simply thesum of three terms shown in 2, 3, and 4.
From thisdefinition, it follows that a low description lengthwill be achieved by a segmentation that defines asmall lexicon, which nonetheless reduces the corpusto a short series of mostly high-frequency words.3 Generating Candidate SegmentationsRecent unsupervised MDL algorithms rely onheuristic methods to generate candidate segmenta-tions.
Yu (2000) makes simplifying assumptionsabout the nature of the lexicon, and then performs anExpectation-Maximization (EM) search over this re-duced hypothesis space.
Zhikov et al (2010) presentan algorithm called EntropyMDL that generates acandidate segmentation based on branching entropy,and then iteratively refines the segmentation in anattempt to greedily minimize description length.We selected three entropy-based algorithms forgenerating candidate segmentations, because suchalgorithms do not depend on the details of any par-ticular language.
By ?unsupervised,?
we mean op-erating on a single unbroken sequence of characterswithout any boundary information; Excluded fromconsideration are a class of algorithms that are semi-supervised because they require sentence boundariesto be provided.
Such algorithms include MBDP-1(Brent, 1999), HDP (Goldwater et al, 2009), andWordEnds (Fleck, 2008), each of which is discussedin Section 5.3.1 Phoneme to MorphemeTanaka-Ishii and Jin (2006) developed Phoneme toMorpheme (PtM) to implement ideas originally de-veloped by Harris (1955).
Harris noticed that ifone proceeds incrementally through a sequence ofphonemes and asks speakers of the language tocount the letters that could appear next in the se-quence (today called the successor count), the pointswhere the number increases often correspond tomorpheme boundaries.
Tanaka-Ishii and Jin cor-541rectly recognized that this idea was an early ver-sion of branching entropy, given by HB(seq) =?
?c?S P (c|seq) logP (c|seq), where S is the setof successors to seq.
They designed their PtM algo-rithm based on branching entropy in both directions,and it was able to achieve scores near the state of theart on word segmentation in phonetically-encodedEnglish and Chinese.
PtM posits a boundary when-ever the increase in the branching entropy exceedsa threshold.
This threshold provides an adjustableparameter for PtM, which we exploit to generate 41candidate segmentations by trying every threshold inthe range [0.0, 2.0], in steps of 0.05.3.2 Voting ExpertsThe Voting Experts (VE) algorithm (Cohen andAdams, 2001) is based on the premise that wordsmay be identified by an information theoretic signa-ture: Entropy within a word is relatively low, en-tropy at word boundaries is relatively high.
Thename Voting Experts refers to the ?experts?
that voteon possible boundary locations.
VE has two ex-perts: One votes to place boundaries after sequencesthat have low internal entropy (surprisal), given byHI(seq) = ?
logP (seq), the other votes after se-quences that have high branching entropy.
All se-quences are evaluated locally, within a sliding win-dow, so the algorithm is very efficient.
A boundaryis generated whenever the vote total at a given loca-tion exceeds a threshold, and in some cases only ifthe vote total is a local maximum.
VE thus has threeparameters that can be manipulated to generate po-tential segmentations: Window size, threshold, andlocal maximum.
Pairing VE with MDL was first ex-amined by Hewlett and Cohen (2009).
We generateda set of 104 segmentations by trying every viablethreshold and local max setting for each window sizebetween 2 and 9.3.3 Bootstrapped Voting ExpertsThe Bootstrapped Voting Experts (BVE) algorithm(Hewlett and Cohen, 2009) is an extension to VE.BVE works by segmenting the corpus repeatedly,with each new segmentation incorporating knowl-edge gained from previous segmentations.
As withmany bootstrapping methods, three essential com-ponents are required: some initial seed knowledge,a way to represent knowledge, and a way to lever-age that knowledge to improve future performance.For BVE, the seed knowledge consists of a high-precision segmentation generated by VE.
After thisseed segmentation, BVE segments the corpus re-peatedly, lowering the vote threshold with each iter-ation.
Knowledge gained from prior segmentationsis represented in a data structure called the knowl-edge trie.
During voting, this knowledge trie pro-vides statistics for a third expert that places votes incontexts where boundaries were most frequently ob-served during the previous iteration.
Each iterationof BVE provides a candidate segmentation, and ex-ecuting BVE for window sizes 2-8 and both localmax settings generated a total of 126 segmentations.4 ExperimentsThere are two ways to evaluate the quality of a seg-mentation algorithm in the MDL framework.
Thefirst is to directly measure the quantity of the seg-mentation chosen by MDL.
For word segmentation,this is typically done by computing the F-score,where F = (2 ?
Precision ?
Recall)/(Precision +Recall), for both boundaries (BF) and words (WF)found by the algorithm.
The second is to com-pare the minimal description length among the can-didates to the true description length of the corpus.4.1 ResultsWe chose a diverse set of natural language cor-pora, including some widely-used corpora to facil-itate comparison.
For each corpus, we generated aset of candidate segmentations with PtM, VE, andBVE, as described in the previous section.
Fromeach set of candidates, results for the segmentationwith minimal description length are presented in thetables below.
Where possible, results for other algo-rithms are presented in italics, with semi-supervisedalgorithms set apart.
Source code for all algorithmsevaluated here, as well as data files for all corpora,are available online1.One of the most commonly-used benchmark cor-pora for unsupervised word segmentation is theBR87 corpus.
This corpus is a phonemic encod-ing of the Bernstein Ratner corpus (Bernstein Rat-ner, 1987) from the CHILDES database of child-directed speech (MacWhinney, 2000).
The perfor-1http://code.google.com/p/voting-experts542mance of the algorithms on BR87 is shown in Ta-ble 1 below.
As with all experiments in this work,the input was presented as one continuous sequenceof characters with no word or sentence boundaries.Published results for two unsupervised algorithms,the MDL-based algorithm of Yu (2000) and theEntropyMDL (EMDL) algorithm of Zhikov et al(2010), on this widely-used benchmark corpus areshown in italics.
Set apart in the table are pub-lished results for three semi-supervised algorithms,MBDP-1 (Brent, 1999), HDP (Goldwater, 2007),and WordEnds (Fleck, 2008), described in Section5.
These algorithms operate on a version of the cor-pus that includes sentence boundaries.Algorithm BP BR BF WP WR WFPtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762Yu 0.722 0.724 0.723 NR NR NREMDL NR NR 0.907 NR NR 0.750MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682HDP 0.903 0.808 0.852 0.752 0.696 0.723WordEnds 0.946 0.737 0.829 NR NR 0.707Table 1: Results for the BR87 corpus.Results for one corpus, the first 50,000 charac-ters of George Orwell?s 1984, have been reportedin nearly every VE-related paper.
It thus providesa good opportunity to compare to the other VE-derived algorithms: Hierarchical Voting Experts ?3 Experts (Miller and Stoytchev, 2008) and MarkovExperts (Cheng and Mitzenmacher, 2005).
Table 2shows the results for candidate algorithms as well asthe two other VE-derived algorithms, HVE-3E andME.Algorithm BP BR BF WP WR WFPtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504ME 0.809 0.787 0.798 NR 0.542 NRTable 2: Results for the first 50,000 characters of 1984.Chinese and Thai are both commonly writtenwithout spaces between words, though some punc-tuation is often included.
Because of this, theselanguages provide an excellent real-world challengefor unsupervised segmentation.
The results shownin Table 3 were obtained using the first 100,000words of the Chinese Gigaword corpus (Huang,2007), written in Chinese characters.
The wordboundaries specified in the Chinese Gigaword Cor-pus were used as a gold standard.
Table 4 showsresults for a roughly 100,000 word subset of a cor-pus of Thai novels written in the Thai script, takenfrom a recent Thai word segmentation competition,InterBEST 2009.
Working with a similar but muchlarger corpus of Thai text, Zhikov et al were ableto achieve slightly better performance (BF=0.934,WF=0.822).Algorithm BP BR BF WP WR WFPtM+MDL 0.894 0.610 0.725 0.571 0.390 0.463VE+MDL 0.871 0.847 0.859 0.657 0.639 0.648BVE+MDL 0.834 0.914 0.872 0.654 0.717 0.684Table 3: Results for a corpus of orthographic Chinese.Algorithm BP BR BF WP WR WFPtM+MDL 0.863 0.934 0.897 0.702 0.760 0.730VE+MDL 0.916 0.837 0.874 0.702 0.642 0.671BVE+MDL 0.889 0.969 0.927 0.767 0.836 0.800Table 4: Results for a corpus of orthographic Thai.The Switchboard corpus (Godfrey and Holli-man, 1993) was created by transcribing sponta-neous speech, namely telephone conversations be-tween English speakers.
Results in Table 5 are fora roughly 64,000 word section of the corpus, tran-scribed orthographically.Algorithm BP BR BF WP WR WFPtM+MDL 0.761 0.837 0.797 0.499 0.549 0.523VE+MDL 0.779 0.855 0.815 0.530 0.582 0.555BVE+MDL 0.890 0.818 0.853 0.644 0.592 0.617Yu 0.674 0.665 0.669 NR NR NRWordEnds 0.900 0.755 0.821 NR NR 0.663HDP 0.731 0.924 0.816 NR NR 0.636Table 5: Results for a subset of the Switchboard corpus.4.2 Description LengthTable 6 shows the best description length achievedby each algorithm for each of the test corpora.
Inmost cases, BVE compressed the corpus more thanVE, which in turn achieved better compression thanPtM.
In Chinese, the two VE-algorithms were ableto compress the corpus beyond the gold standard543size, which may mean that these algorithms aresometimes finding repeated units larger than words,such as phrases.Algorithm BR87 Orwell SWB CGW ThaiPtM+MDL 3.43e5 6.10e5 8.79e5 1.80e6 1,23e6VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6Table 6: Best description length achieved by each algo-rithm compared to the actual description length of thecorpus.5 Related WorkThe algorithms described in Section 3 are all rela-tively recent algorithms based on entropy.
Many al-gorithms for computational morphology make useof concepts similar to branching entropy, such assuccessor count.
The HubMorph algorithm (John-son and Martin, 2003) adds all known words to atrie and then performs DFA minimization (Hopcroftand Ullman, 1979) to convert the trie to a finite statemachine.
In this DFA, it searches for sequences ofstates (stretched hubs) with low branching factor in-ternally and high branching factor at the boundaries,which is analogous to the chunk signature that drivesVE and BVE, as well as the role of branching en-tropy in PtM.MDL is analogous to Bayesian inference, wherethe information cost of the model CODE(M) actsas the prior distribution over models P (M), andCODE(D|M), the information cost of the data giventhe model, acts as the likelihood function P (D|M).Thus, Bayesian word segmentation methods maybe considered related as well.
Indeed, one of theearly Bayesian methods, MBDP-1 (Brent, 1999)was adapted from an earlier MDL-based method.Venkataraman (2001) simplified MBDP-1, relaxedsome of its assumptions while preserving the samelevel of performance.
Recently, Bayesian methodswith more sophisticated language models have beendeveloped, including one that models language gen-eration as a hierarchical Dirichlet process (HDP),in order to incorporate the effects of syntax intoword segmentation (Goldwater et al, 2009).
An-other recent algorithm, WordEnds, generalizes in-formation about the distribution of characters nearword boundaries to improve segmentation (Fleck,2008), which is analogous to the role of the knowl-edge trie in BVE.6 DiscussionFor the five corpora tested above, BVE achievedthe best performance in conjunction with MDL, andalso achieved the lowest description length.
We haveshown that the combination of BVE and MDL pro-vides an effective approach to unsupervised wordsegmentation, and that it can equal or surpass semi-supervised algorithms such as MBDP-1, HDP, andWordEnds in some cases.All of the languages tested here have relativelyfew morphemes per word.
One area for future workis a full investigation of the performance of these al-gorithms in polysynthetic languages such as Inukti-tut, where each word contains many morphemes.
Itis likely that in such languages, the algorithms willfind morphs rather than words.AcknowledgementsThis work was supported by the Office of Naval Re-search under contract ONR N00141010117.
Anyopinions, findings, and conclusions or recommen-dations expressed in this publication are those of theauthors and do not necessarily reflect the views ofthe ONR.ReferencesShlomo Argamon, Navot Akiva, Amihood Amir, andOren Kapah.
2004.
Efficient Unsupervised Recur-sive Word Segmentation Using Minimum DescriptionLength.
In Proceedings of the 20th International Con-ference on Computational Linguistics, Morristown,NJ, USA.
Association for Computational Linguistics.Nan Bernstein Ratner, 1987.
The phonology of parent-child speech, pages 159?174.
Erlbaum, Hillsdale, NJ.Michael R. Brent.
1999.
An Efficient, ProbabilisticallySound Algorithm for Segmentation and Word Discov-ery.
Machine Learning, (34):71?105.Jimming Cheng and Michael Mitzenmacher.
2005.
TheMarkov Expert for Finding Episodes in Time Series.In Proceedings of the Data Compression Conference,pages 454?454.
IEEE.Paul Cohen and Niall Adams.
2001.
An algorithmfor segmenting categorical time series into meaning-ful episodes.
In Proceedings of the Fourth Symposiumon Intelligent Data Analysis.544Margaret M. Fleck.
2008.
Lexicalized phonotactic wordsegmentation.
In Proceedings of The 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 130?138,Columbus, Ohio, USA.
Association for ComputationalLinguistics.John J. Godfrey and Ed Holliman.
1993.
Switchboard- 1Transcripts.Sharon Goldwater, Thomas L Griffiths, and Mark John-son.
2009.
A Bayesian Framework for Word Segmen-tation: Exploring the Effects of Context.
Cognition,112(1):21?54.Sharon Goldwater.
2007.
Nonparametric Bayesian mod-els of lexical acquisition.
Ph.D. dissertation, BrownUniversity.Zellig S. Harris.
1955.
From Phoneme to Morpheme.Language, 31(2):190?222.Daniel Hewlett and Paul Cohen.
2009.
Bootstrap VotingExperts.
In Proceedings of the Twenty-first Interna-tional Joint Conference on Artificial Intelligence.J.
E. Hopcroft and J. D. Ullman.
1979.
Introductionto Automata Theory, Languages, and Computation.Addison-Wesley.Chu-Ren Huang.
2007.
Tagged Chinese Gigaword(Catalog LDC2007T03).
Linguistic Data Consortium,Philadephia.Howard Johnson and Joel Martin.
2003.
Unsupervisedlearning of morphology for English and Inuktitut.
Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL 2003), pages 43?45.Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Lawrence Erlbaum Associates,Mahwah, NJ, 3rd editio edition.Matthew Miller and Alexander Stoytchev.
2008.
Hierar-chical Voting Experts: An Unsupervised Algorithm forHierarchical Sequence Segmentation.
In Proceedingsof the 7th IEEE International Conference on Develop-ment and Learning, pages 186?191.Jorma Rissanen.
1983.
A Universal Prior for Integersand Estimation by Minimum Description Length.
TheAnnals of Statistics, 11(2):416?431.Kumiko Tanaka-Ishii and Zhihui Jin.
2006.
FromPhoneme to Morpheme: Another Verification Usinga Corpus.
In Proceedings of the 21st InternationalConference on Computer Processing of Oriental Lan-guages, pages 234?244.Anand Venkataraman.
2001.
A procedure for unsuper-vised lexicon learning.
In Proceedings of the Eigh-teenth International Conference on Machine Learning.Hua Yu.
2000.
Unsupervised Word Induction usingMDL Criterion.
In Proceedings of the InternationalSymposium of Chinese Spoken Language Processing,Beijing, China.Valentin Zhikov, Hiroya Takamura, and Manabu Oku-mura.
2010.
An Efficient Algorithm for Unsuper-vised Word Segmentation with Branching Entropy andMDL.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 832?842, Cambridge, MA.
MIT Press.545
