vIicrophone Arrays and Neural Networks for RobustSpeech RecognitionC.
Che +, Q. Lin +, J. Pearson*, B. de Vries*, and J. Flanagan ++CAIP Center, Rutgers University, Piscataway, NJ 08855-1390and*David Sarnoff Research Center, Princeton, NJ 08543-5300ABSTRACTThis paper explores use of synergistically-integrated systemsof microphone arrays and neural networks for robust speechrecognition in variable acoustic environments, where the usermust not be encumbered by microphone quipment.
Existingspeech recognizers work best for "high-quality close-talkingspeech."
Performance of these recognizers is typically de-graded by environmental interference and mismatch in train-ing conditions and testing conditions.
It is found that use ofmicrophone arrays and neural network processors can elevatethe recognition performance of existing speech recognizers inan adverse acoustic environment, hus avoiding the need toretrain the recognizer, a complex and tedious task.
We alsopresent results showing that a system of microphone arraysand neural networks can achieve a higher word recognitionaccuracy in an unmatched training/testing condition thanthat obtained with a retrained speech recognizer using arrayspeech for both training and testing, i.e., a matched train-ing/testing condition.1.
INTRODUCTIONHidden Markov Models (HMM's) have to date been acceptedas an effective classification method for large vocabulary con-tinuous speech recognition.
Existing HMM-based recognitionsystems, e.g., SPHINX and DECIPHER, work best for "high-quality close-talking speech."
They require consistency insound capturing equipment and in acoustic environments be-tween training and testing sessions.
When testing conditionsdiffer from training conditions, performance of these recog-nizers is typically degraded if they are not retrained to copewith new environmental effects.Retraining of HMM-based recognizers i complex and time-consuming.
It requires recollection of a large amount ofspeech data under corresponding conditions and reestimationof HMM's parameters.
Particularly great time and effort areneeded to retrain a recognizer which operates in a speaker-independent mode, which is the mode of greatest general in-terest.Room reverberation and ambient noise also degrade per-formance of speech recognizers.
The degradation becomesmore prominent as the microphone is positioned more dis-rant from the speaker, for instance, in a teleconferenc-ing application.
Previous work has demonstrated thatbeamforming/matched-filter microphone arrays can providehigher signal-to-noise ratios than can conventional micro-phones used at distances (see, e.g., \[1, 2\]).
Consequently,there is increasing interest in microphone arrays for hands-free operation of speech processing systems \[3\]-\[7\].In this report, a system of microphone arrays and neural net-works is described which expands the power and advantagesof existing ARPA speech recognizers to practical acoustic en-vironments where users need not be encumbered by hand-held or body-worn microphone systems.
(Examples includeCombat Information Centers, large group conferences, andmobile hands-busy eyes~busy maintenance tasks.)
Anotheradvantage o f  the system is that the speech recognizer neednot be retrained for each particular application environment.Through neural network computing, the system learns andcompensates for environmental interference.
The neural net-work transforms peech-feature data (such as cepstrum co-efflcients) obtained from a distant-talking microphone arrayto those corresponding to a high-quality, close-talking micro-phone.
The performance ofthe speech recognizer can therebybe elevated in the hostile acoustic environment without re-training of the recognizer.The remainder of the paper is organized as follows.
First,a new speech corpus with simultaneous recording from dif-ferent microphones is described in Section 2.
Next, thesystem of microphone arrays and neural networks is dis-cussed in Section 3.
The system is evaluated using both theSPHINX speech recognizer and a Dynamic-Time-Warping(DTW) based recognizer.
The results are presented in Sec-tions 4 and 5, respectively.
In Section 6, performance compar-isons are made of different network architectures to identifyan optimal design for room-acoustic equalization.
Finally, wesummarize the study and discuss our future work in Section7.2.
SPEECH CORPUSA speech database has been recently created at the CAIPCenter for evaluation of the integrated system of microphonearrays, neural networks, and ARPA speech recognizers.
Thedatabase consists of 50 male and 30 female speakers.
Eachspeaker speaks 20 isolated command-words, 10digits, and 10continuous entences of the Resource Management task.
Ofthe continuous entences, two are the same for all speakersand the remaining 8 sentences are chosen at random.
Tworecording sessions are made for each speaker.
One sessionis for simultaneous recording from a head-mounted close-talking microphone (HMD 224) and from a 1-D beamformingline array microphone (see Section 3.1).
The other is for si-multaneous recording of the head-mounted close-talking mi-crophone and a desk-mounted microphone (PCC 160).
Therecording is done with an Ariel ProPort with a sampling fre-342(A)2 (~ .
l + I I-10000 PIt ' ~ + qlpl+- ........ ; .............. ?
I I0.5 1 1.5 2(a) x10 *0.5 1 1,5 2(C) xlO+'O~I't?00 ' , l  ~ '~"1"~'~'"i,,~  " t-1 .
.
.
.
, , ,~  t '~mm'; ''+'" .............. ;0.5 1 1.5 2(D) x 1040,5 1 1.5x10 ~Figure 1: Speech wavefozms fzom the head-mounted micro-phone ( A and O), ~om the 1-D line array microphone (B),and from the desk-mounted microphone (D).
CA) and (B)are simultaneously zecozded in a session and (C) and (D) ina following session.
The utterance is: "Miczophone arzay,"spoken by a male speaker !ABF).100 oo ?/I.olI+wlI FEM~JRS EXIRACT~IAMPALarlle.focabulaflrma,am., q~,,,,d~m.
,~ .
,~b~ /(~Sl~LIII ?~RqC'IIBI/SOF CLOSGoTAL~,INQ 91~I~ECHm)0 ?
),, |?Figuze 2: Block diagram of the robust speech zecognitionsystem.
The neural network processor is trained using si-multaneously recorded speech.
The trained neural networkprocessor is then used to transform spectral features of arrayinput to those appropriate to close-talking.
The transformedspectral featuzes are inputs to the speech recognition system.No retraining or modification of the speech recognizer is nee-essary.
The training of the neural net typically zequires about10 seconds of signal.corporating microphone arrays, neural networks, and ARPAspeech recognizers.quency of 16 kHz and 16-bit linear quantization.
The record-ing environment is a hard-walled laboratory room of 6 ?
6 ?
2.7meters, having a reverberation time of approximately 0.5 sec-ond.
Both the desk-mounted microphone and the line axraymicrophone are placed 3 meters from the subjects.
Ambientnoise in the laboratory room is from several workstations,fans, and a large-size video display equipment for telecon-fereneing.
The measured 'A'  scale sound pressure level is50 dB.
Indicative of the quality differences in outputs f~omvarious sound pickup systems, signal waveforms are givenin Figure 1.
Because of wave propagation from the speakerto distant microphones, a delay of approximately 9 msec isnoticed in outputs of the line array and the desk-mountedmicrophone.
Wave propagation between the subject's lipsto the head-mounted close-talking microphone is negligible.The reader is referred to \[8\] for more details.3.
SYSTEM OF MICROPHONEARRAYS AND NEURALNETWORKSFigure 2 schematically shows the overall system design for ro-bust speech recognition in variable acoustic environments, in-3.1.
Beamforming Microphone ArraysAs the distance between microphones and talker increases,the effects of room reverberation and ambient noise be-come more prominent.
Previous studies have shown thatbeamforming/matched-fl lter array microphones are effectivein counteracting environmental interference.
Microphone ar-rays can improve sound quality of the captured signal, andavoid hand-held, body-worn, or tethered equipment hatmight encumber the talker and restrict movement.The microphone array we use here is a one-dimensional beam-forming line array.
It uses direct-path arrivals to produce aslngle-beam delay-and-sum beamformer \[1, 2\].
(The talkertypically faces the center of the llne array.)
The array con-sists of 33 omni-direetlonal sensors, which are nonuniformlypositioned (nested over three octaves).
From Figure 1 it isseen that the wavefozm of the array resembles that of theclose-talking microphone more than the desk-mounted mi-crophone.3.2 .
Neural Network ProcessorsOne of the neural network processors we have explored, isbased on multi-layer perceptrons (MLP).
The MLP has 3 lay-343TRAIN ING US ING THE STANDARD BACKPROPAGATIONONE H IDDEN LAYER WITH 4 S IGMOID NEURONSI - l  ?
~r r~.AcTo~ (:zrI.$.~II -  iFigure 3: A feedforward elay network for mapping the cep-stral coefficients of array speech to those of close-talkingspeech.ers.
The input layer has 9 nodes, covering the current speechframe and four preceding and four following frames, as indi-cated in Figure 3.
There are 4 sigmoid nodes in the hiddenlayer and 1 linear node in the output layer.
13 such MLP'sare included, with one for each of the 13 cepstrum coefficientsused in the SPHINX speech recognizer \[14\].
(Refer also toFigure 2.)
The neural network is trained using a modifiedbackpropagation method when microphone-array speech andclose-talking speech are both available (see Figure 3).It is found that 10-seconds of continuous peech material aresufficient to train the neural networks and allow them to"learn" the acoustic environment.
In the present study, theneural nets are trained in a speaker-dependent mode; Thatis, 13 different neural networks (one for each cepstrum coeffi-cient) are dedicated to each subject 1.
The trained networksare then utilized to transform cepstrum coefficients of arrayspeech to those of close-talking speech, which are then usedas inputs to the SPHINX speech recognizer.4.
EVALUATION RESULTS WITHSPHINX RECOGNIZERAs a baseline evaluation, recognition performance is mea-sured on the command-word subset of the CAIP database.Performance is assessed for matched and unmatched test-ing/training conditions and include both the pretrained andretrained SPHINX system.The results for the pretrained SPHINX are given in Table 1.It includes four processing conditions: (i) close-talking; (ii)line array; (ili) line array with mean subtraction (MNSUB)\[15\]; and, (iv) line array with the neural network processor(NN).Table 2 gives the results for the retrained SPHINX underfive processing conditions: (i) close-talking; (fi) line array;1 The learning rate is 0.01 and the momentum term is 0.5.
Thetr,~i~i~.g terminates at I000 epocch~s.Testing MicrophoneLine-ArrayLine-Array +MNSUB~ 3Word Accuracy88%16%24%82%Table 1: Baseline evaluation of recognition performance (%correct), using the pretrained SPHINX speech recognizer.
(iii) desk-mounted microphone; (iv) line array with meansubtraction (MNSUB); and, (v) line array with the neuralnetwork processor (NN).
The SPHINX speech recognizer isretrained using the CAIP speech corpus to eliminate systemconditions in coUection of the Resource Management task (onwhich the original SPHINX system has been trained) and theCAIP speech database.As shown in Tables 1 and 2, the array-neural net system iscapable of elevating word accuracy of the speech recognizer.For the retrained SPHINX, the microphone array and neuralnetwork system improves word accuracy from 21% to 85% fordistant talking under reverberant conditions.
On the otherhand, the mean subtraction method under these conditionsimproves the performance only marginally.It is also seen from Table 2 that if the SPHINX system hasbeen retrained with array speech at a distance of 3 meters, theperformance is as high as 82%.
The figure, obtained under amatched training/testing condition, is, however, lower thanthat obtained under an unmatched training/testing conditionwith microphone array and neural network.
Similar resultshave been achieved for speaker identification \[9, 10\].5.
EVALUATION RESULTS WITHDTW RECOGNIZERTo more effectively and efficiently assess the capability ofmicrophone arrays and neural network equalizers, a DTW-based speech recognizer is implemented \[12\].
The back endof DTW classification is simple, and hence, the results do nottend to be influenced by the complex back end of an HMM-based recognizer, including language models and word-pairgrammars.Testing Training TrainingClose-Talking Line-ArrayClose-TalkingLine-ArrayDesk-mountedLine-Array + MNSUBLine-Array + NN95%21%13%27%85%82%Table 2: Baseline evaluation of recognition performance (%correct), using a retrained SPHINX recognizer based on theCAIP speech database.344111118C<6C L25C3C ,1000 2000 3000 4000 5000Number of Iterations (Epochs)Figure 4: Word recognition accuracy on the testing set as afunction of the number of iterations when training the neuralnetwork processor.The DTW recognizer is applied to recognition of the com-mand words.
End-points of close-talking speech are automat-ically determined by the two-level approach \[11\] 2.
Attemptshave also been made to automatically detect end-points of ar-ray speech \[13\], but in the present paper, the starting/endingpoints are inferred from the simultaneously recorded close-talking speech, with an additional delay resulting from wavepropagation.
The DTW recognizer is speaker dependent,and is trained using close-talking speech.
The measured fea-tures are 12th-order LPC-derived cepstral coefficients over aframe of 16 msec.
The frame is Hamming-windowed and theconsecutive windows overlap by 8 msec.
The DTW recog-nizer is tested on array speech (with the originally computedand neural-network corrected cepstral coefficients) and on theother set of the close-talking recording.
The Euclidean dis-tance is utilized as the distortion measure.The recognition results, pooled over 10 male speakers, arepresented in Table 3.
The configuration of MLP used in thisDTW based evaluation differs from that in Section 4.
A singleMLP with no window-sliding is now used to collectively trans-form all of 12 cepstral coefficients from array speech to cclose-talking.
The MLP has 40 hidden nodes and 12 output nodes.The network is again speaker-dependently trained with stan-dard backpropagation algorithms.
The learning rate is set to0.1 and the momentum term to 0.5.
The backpropagationprocedure terminates after 5000 iterations (epochs).It can be seen that the results in Table 3 are similar to thosein Tables 2 and 1.
The use of microphone arrays and neuralnetworks elevates the DTW word accuracy from 34% to 94%under reverberant conditions.
The elevated accuracy is closeto that obtained for close-talking speech (98~0).2The automatic results conform well with manual editing.Close-TalkingLine-ArrayLine-Array + NNWord Accuracy98%34%94%Table 3: Baseline valuation of recognition performance usingDTW classification algorithms.Figure 4 il lustrates the relationship between the number oftraining iterations of the neural networks and the word recog-nition accuracies.
It is seen that as the iteration number in-creases from 100 to 1000, the recognition accuracy qnickiyrises from 32% to 87%.
It can also be seen that after 5000iterations the network is not overtralned, since recognitionaccuracy on the testing set is still improving.6.
PERFORMANCE COMPARISONOF D IFFERENT NETWORKARCHITECTURESWe also perform comparative xperiments with respect todifferent network architectures.
It has been suggested in thecommunications l iterature that recurrent non-linear neuralnetworks may outperform feedforward networks as equaliz-ers.
Since our problem can be interpreted as a room acous-tics equalization task, we decide to evaluate the performanceof recurrent nets.
For the experiments reported here, weonly train on data from the 3rd eepstral coefficient (out of13 bands).
The input to the neural net is the cepstral datafrom the microphone array; the target cepstral coefficient istaken from the close-talking microphone.
The squared errorbetween the target data and the neural net output is used asthe cost function.
The neural nets are trained by gradient de-scent.
The following three different architectures have beenevaluated: (i) a linear feedforward net (adaline) \[16\], (i.i) anon-linear feedforward net, (iii) and a non-linear ecurrentnetwork.
The input layer of all nets consisted of a tapped de-lay line.
The network configurations are depicted in Figures5 and 6.Experimental results are summarized in Table 4, where theentry "nflops/epoch" stands for the number of (floatingpoint) operations required during training per epoch.
Theentry "#parameters" holds the number of adaptive weightsin the network.It is clear that, for this dataset, the non-linear networks per-form better than the linear nets, but at the expense of consid-erably more computations during adaptation.
This is not aproblem if we assume that the transfer function from speakerto microphone is constant, but in a changing environment(moving speaker, doors opening, changing background noise)this is a problem, as the neural net needs to track the changein real-time.
It should be noted that the used cost function,the squared error, is in all likelihood not a monotonic func-tion of the recognizer performance.
Currently experimentsare underway that evaluate the performance of various net-work architectures in terms of word recognition accuracy.345Figure 5: The feedforward net.
The hidden units are non-linear (tanh).recto'rentI ~ ~ I hidden layer0Figure 6: The recurrent network has a similar structure asthe 2-layer feedforward.architecture flnalsqe nflops/epoch #parameters adaptation ruleno processing .12adaliine (I tap) .0952 ', 14,000 1 delta ruleadaline (5) .0844 - 40,000 (1) 5 deltaadaline (11) .0825 ', 80,000 (2) 11 deltai~dnet(5~,l) .0707 "1924,000(48) I 15 backproprecnet (5,2r,1) .0782 - 2478500 (62) 19 bptt.0775 '.
3772?000 f94) i ffwdnet ($,4,1) -3 n,ooo(94) 29 backpropFigure 7: Experimental results of different neural networkconfigurations.
The various runs are ordered by increasingperformance.
Final sqe (squared error) is the mean sqe pertime step on the test database.
The ops/epoch denotes thenumber of floating point operations per epoch during train-ing.
The number in brackets i  the number of flops per epochdivided by flops/epoch for adaline (5 taps).
~ parametersdenotes the number of adaptive parameters in the network.7.
CONCLUSION ANDDISCUSSIONThe above evaluation results suggest hat the system of mi-crophone array and neural network processors can?
effectively mitigate nvironmental coustic interference?
without retraining the recognizer, elevate word recog-nition accuracies of HMM-based and/or DTW-basedspeech recognizers in variable acoustic environments olevels comparable to those obtained for close-talking,high-quality speech?
achieve word recognition accuracies, under unmatchedtraining and testing conditions, that exceed those ob-tained with a retrained speech recognizer using arrayspeech for both retraining and testing, i.e., under emmatched training and testing conditionsSimilar results have also been achieved for studies on speakerrecognition \[9, 10\].In future work, we expect to extend the comparative eval-uations of different neural network architectures, o thatthe performance of neural network equalization can be ad-dressed in terms of word recognition accuracy.
We also wantto extend the evaluation experiments o continuous peech.For comparison, the DECIPHER system will be included,and possibly other advanced ARPA speech recognizers.
TheCAIP Center has concomitant NSF projects on developing2-D and 3-D microphone arrays.
These new array micro-phones have better spatial volume selectivity and can pro-vide a high signal-to-noise ratio.
They will be incorporatedinto this study.
Further work will compare the system of mi-crophone array and neural network with other existing noisecompensation algorithms, uch as Codebook Dependent Cep-strum Normalization (CDCN) \[17\] and Parallel Model Com-bination (PMC) \[18\].8.
ACKNOWLEDGMENTThis work is supported by ARPA Contract No: DABT63-93-C-0037.
The work is also in part supported by NSF GrantNo:  MIP-9121541.References1.
Flanagan, J., Berkley~ D,  Elko, G., West, J ,  andSondhi, M., "Autodirective microphone systems," Acus-tica 73, 1991, pp.
58-71.2.
Flanagan, J., Surendran, A., and Jan, E., "Spatially se-lective sound capture for speech and audio processing,"Speech Communication, 13, Nos.
1-2, 1993. pp.
207-222.3.
Silverman, H. F., "Some analysis of microphone arraysfor speech data acquisition," IEEE Trans.
Acous.
SpeechSignal Processing 35, 1987, pp.
1699-1712.4.
Berldey, D. A. and Flanagan, J. L., "HuMaNet: Anexperimental human/machine communication networkbased on ISDN," AT ~4 T Tech J., 1990, pp.
87-98.5.
Che, C., Rahim, M,  and Flanagan J.
"Robust speechrecognition in a multimedia teleconferencing environ-meat," \].
Acous.
Soc.
Am.
9Z (4), pt 2, p. 2476(A),1992.3466.
Sullivan, T. and Stern, R. M., "Multi-microphonecorrelation-based processing for robust speech recogni-tion," Proc.
ICASSP-93, April, 1993.7.
Lin, Q., Jan, E., and Flanagan, J., "Microphone-arraysand speaker identification," Accepted for publication inthe special issue on Robust Speech Processing of theIEEE Trans.
on Speech and Audio Processing, 1994.8.
Lin, Q., C. Che, and It.
Van Dyek: "Description of CAIPspeech corpus," CAIP Technical Report, Rutgers Uni-versity, in preparation, 1994.9.
Lin, Q., Jan, E., Che, C., and Flanagan, J., "Speakeridentification i teleconferencing environments u ing mi-crophone arrays and neural networks," Proc.
of ESCAWorkshop on Speaker, Recognition, Identification, andVerification, Switzerland, April, 1994.10.
Lin, Q., Che, C., Jan, E., and Flanagan, J.,"Speaker/speech recognition using microphone arraysand neural networks," paper accepted for the SPIE Con-ference, San Diego, July, 1994.11.
Rabiner, L. and Sambur, M. "An algorithm for deter-mining the endpoints of isolated utterances," Bell Syst.Tech.
J.
54, No.
2, 1975, pp.
297-315.12.
Sakoe, H. and Chiba, S. "Dynamic programming opti-mization for spoken word recognition."
IEEE Trans.
onAcous.
Speech Signal Processing P6, 1978, pp.
43-49.13.
Srivastava, S., Che, C., and Lin, Q., "End-point de-tection of microphone-array speech signals," Paper ac-cepted for 127th meeting of Acous.
Soc.
of Amer.,Boston, June, 1994.14.
Lee, K.-F., Automatic Speech Recognition: The Develop-ment Of The SPHINX System, Kluwer Academic Pub-fishers, Boston, 1989.15.
Furui, S. "Cepstral analysis technique for automaticspeaker verification," IEEE Trans.
Acoustics, Speechand Signal Processing, Vol 29, 1979, pp 254--272.16. de Vries, B., "Short term memory structures for dy-namic neural networks," to appear in Artificial NeuralNetworks with Applications in Speech and Vision (Ed.R.
Mammone).17.
Liu, F.-H., Acero, A., and Stern, R. M., "Efficient jointcompensation of speech for the effect of additive noiseand linear filtering," ICASSP-92, April, 1992, pp 25%260.18.
Gales, M. J. F. and Young, S. J., "Cepstral parametercompensation for HMM recognition," Speech Communi-cation 12, July, 1993, pp.
231-239.347
