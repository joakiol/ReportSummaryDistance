Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsOverview for the First Shared Task onLanguage Identification in Code-Switched DataThamar SolorioDept.
of Computer ScienceUniversity of HoustonHouston, TX, 77004solorio@cs.uh.eduElizabeth Blair, Suraj Maharjan, Steven BethardDept.
of Computer and Information SciencesUniversity of Alabama at BirminghamBirmingham, AL, 35294{eablair,suraj,bethard}@uab.eduMona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdiDept.
of Computer ScienceGeorge Washington UniversityWashington, DC 20052{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.eduJulia Hirschberg and Alison ChangDept.
of Computer ScienceColumbia UniversityNew York, NY 10027julia@cs.columbia.eduayc2135@columbia.eduPascale FungDept.
of Electronic & Computer EngineeringHong Kong University of Science and TechnologyClear Water Bay, Kowloon, Hong Kongpascale@ece.ust.hkAbstractWe present an overview of the first sharedtask on language identification on code-switched data.
The shared task in-cluded code-switched data from four lan-guage pairs: Modern Standard Arabic-Dialectal Arabic (MSA-DA), Mandarin-English (MAN-EN), Nepali-English (NEP-EN), and Spanish-English (SPA-EN).
A to-tal of seven teams participated in the taskand submitted 42 system runs.
The evalua-tion showed that language identification atthe token level is more difficult when thelanguages present are closely related, as inthe case of MSA-DA, where the predictionperformance was the lowest among all lan-guage pairs.
In contrast, the language pairswith the higest F-measure where SPA-ENand NEP-EN.
The task made evident thatlanguage identification in code-switcheddata is still far from solved and warrantsfurther research.1 IntroductionThe main goal of this language identification sharedtask is to increase awareness of the outstandingchallenges in the automated processing of Code-Switched (CS) data and motivate more research inthis direction.
We define CS broadly as a commu-nication act, whether spoken or written, where twoor more languages are being used interchangeably.In its spoken form, CS has probably been aroundever since different languages first came in contact.Linguists have studied this phenomenon since themid 1900s.
In contrast, the Natural Language Pro-cessing (NLP) community has only recently startedto pay attention to CS, with the earliest work inthis area dating back to Joshi?s theoretical workproposing an approach to parsing CS data (Joshi,1982) based on the Matrix and Embedded languageframework.
With the wide-spread use of social me-dia, CS is now being used more and more in writtenlanguage and thus we are seeing an increase in pub-lished papers dealing with CS.
We are specificallyinterested in intrasentential code switched phenom-ena.
As a result of this task, we have successfullycreated the first set of annotated data for severallanguage pairs with a coherent set of labels acrossthe languages.
As the shared task results show,CS poses new research questions that warrant newNLP approaches, and thus we expect to see a sig-nificant increase in NLP work in the coming yearsaddressing CS phenomena in data.The shared task covers four language pairs andis focused on social media data.
We provided par-ticipants with annotated data from Twitter for the62language pairs: Modern Standard Arabic-Arabicdialects (MSA-DA), Mandarin-English (MAN-EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).These language pairs represent a good variety interms of language typology and relatedness amongpairs.
They also cover languages with different rep-resentation in terms of number of speakers worldwide.
Participants were asked to make predictionson unseen Twitter data for each language pair.
Wealso provided participants with test data from a?surprise genre?
with the objective of assessing therobustness of language identification systems togenre variation.2 Task DescriptionThe task consists of labeling each token/word inthe input file with one of six labels: lang1, lang2,other, ambiguous, mixed, and named entities NE.The lang1, lang2 labels refer to the two languagesaddressed in the subtask, for example for the lan-guage pair MSA-DA, lang1 would be an MSA andlang2 is DA.
The other category is a label used totag all punctuation marks, emoticons, numbers, andsimilar tokens that do not represent actual words inany of the given languages.
The ambiguous labelis for instances where it is not possible to assigna language with certainty, for example, a lexicalform that belongs to both languages, appearing in acontext that does not indicate one language over theother.
The mixed category is for words composedof CS morphemes, such as the word snapchateando?to chat?
from SPA-EN, the word overai from NEP-EN, or the word hayqwlwn1?they will say?, fromMSA-DA, where the ?ha?
is a DA future morphemeand the stem ?yqwlwn?
is MSA.The NE label isincluded in this task in an effort to allow for a morefocused analysis of CS data with the exclusion ofproper nouns.
NEs have a very different behaviorthan most other words in a language vocabularyand thus from our perspective they need to be iden-tified to be handled properly.Table 1 shows Twitter examples taken from thetraining data.
The annotation guidelines are postedon the workshop website2.
We post the ones usedfor SPA-EN as for the other language pairs the onlydifferences are the examples provided.1We use Buckwalter transliteration scheme http://www.qamus.org/transliteration.htm2http://emnlp2014.org/workshops/CodeSwitch/call.htmlLanguage Pair ExampleMSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.HAfZ AlmyrAzy ElY qnAp drym llHdyvEn >wlwyAt Alvwrp fy AlmrHlp Al-HAlyp wqDyp tSHyH msAr AlvwrpAl<ElAmy(Today O?Clock 11 I will be[a ]guest[ of] Mr. HafezAlMirazi on Channel Dreamto talk about [the ]priorities[ of]the revolution in the stage the currentand [the ]issue[ of] correcting[the ]path[ of] the revolution Media)NEP-EN My car at the workshop for a muchneeded repairs... ABA pocket khalihune bho(My car at the workshop for a muchneeded repairs.
.
.
now my pocket willbe empty)SPA-EN Por primera vez veo a @username ac-tually being hateful!
it was beautiful:)(For the first time I get to see @user-name actually being hateful!
it wasbeautiful:)Table 1: Examples of Twitter data used in theshared task.3 Related WorkIn the past, most language identification researchhas been done at the document level.
Some re-searchers, however, have developed methods toidentify languages within multilingual documents(Singh and Gorla, 2007; Nguyen and Do?gru?oz,2013; King and Abney, 2013).
Their test datacomes from a variety of sources, including webpages, bilingual forum posts, and jumbled datafrom monolingual sources, but none of them aretrained on code-switched data, opting instead for amonolingual training set per language.
This couldprove to be a problem when working on code-switched data, particularly in shorter samples suchas social media data, as the code-switching contextis not present in training material.One system tackled both the problems of code-switching and social media in language and code-switched status identification (Lignos and Marcus,2013).
Lignos and Marcus gathered millions ofmonolingual tweets in both English and Spanish inorder to model the two languages, and used crowd-sourcing to annotate tens of thousands of Span-ish tweets, approximately 11% of which containedcode-switched content.
This system was able toachieve 96.9% word-level accuracy and a 0.936F-measure in identifying code-switched tweets.The issue still stands that relatively little code-switching data, such as that used in Lignos and63Marcus?
research, is readily available.
Even intheir data, the percentage of code-switched tweetswas barely over a tenth of the total test data.
Therehave been other corpora built, particularly for otherlanguage pairs such as Mandarin-English (Li etal., 2012; Lyu et al., 2010), but the amount of dataavailable and the percentage of code-switching datawithin that data are not up to the standards of otherareas of the natural language processing field.
Withthis in mind, we sought to provide corpora for mul-tiple language pairs, each with a better distributionof code-switching phenomena.4 Data SetsMost of the data for the shared task comes formTwitter.
However, we also collected and annotateddata from other social media sources, includingFacebook, web forums, and blogs.
These additionalsources of data were used as the surprise data.
Inthis section we describe briefly the corpora curatedfor the shared task.Language-pair Training Test SurpriseMAN-EN 1000 313 n/aMSA-DA 5,838 2332, 1,777 12,017NEP-EN 9,993 3,018 (2,874) 1,087SPA-EN 11,400 3,060 (1,626) 1,102Table 2: Statistics of the shared task data setsper language pairs.
The numbers are according towhat was actually annotated, numbers in parenthe-sis show what the participating systems were ableto crawl from Twitter.
The Surprise genre comesfrom various sources, other than Twitter.Table 2 shows some statistics about the differ-ent datasets used in this task.
We strive to providedataset sizes that would allow a robust analysis ofresults.
However, an unexpected challenge wasthe rate at which tweets became unavailable.
Dif-ferent language pairs had different attrition rateswith SPA-EN being the most affected language andMSA-DA and NEP-EN the least affected.
Notethat we provided two test datasets for MSA-DA.Since we separated the data on a per user basis, thefirst test set had a highly skewed distribution.
Thesecond test set was distributed to participants toallow a comparison with a data set having a classdistribution more similar to the training set.4.1 SPA-EN dataDeveloping the corpus involved two primary steps:locating code-switching tweets and using crowd-sourcing to annotate their tokens with languagetags.
A small portion of the tweets were annotatedin-lab and this was used as the gold data for qualitycontrol in the crowdsourcing annotation.To avoid biasing the data used in this task, weused a two step process to select the tweets: first weidentified CS tweets by doing a keyword search onTwitter?s API.
We selected a few frequently usedEnglish words and restricted the search to tweetsidentified by Twitter as Spanish from users in Cali-fornia and Texas.
An additional set of tweets wasthen collected by using frequent Spanish words inan all English tweet, from users in the same loca-tions.
We filtered these tweets to remove tweetscontaining URLs, duplicates, spam tweets andretweets.In-lab annotators labeled the filtered tweets usingthe guidelines referenced above.
From this set oflabeled data we then ranked the users in this set bythe percentage of CS tweets.
We selected the 12most prolific CS users and then pulled all of theiravailable tweets.
These 12 users contributed thetweets used in the shared task.
The tweets werelabeled using CrowdFlower3.
After analyzing thenumber and content distribution of the tweets, theSPA-EN data was split into a 11,400 tweet trainingset and a 3,014 tweet test set.The SPA-EN Surprise Genre (SPA-EN-SG) in-cluded Facebook comments from the Veteranascommunity4and the Chicanas community5andblog data from the Albino Bean6.
Data was col-lected using Python scripts that implemented theBeautiful Soup library and the third-party PythonFacebook SDK (for Blogger and Facebook respec-tively).
Post and comment IDs were used to iden-tify Facebook posts, and URLs were used to iden-tify Blogger posts.
The collected posts were format-ted to match those collected from Twitter.
In-labannotators were used to annotate approximately 1Ktokens.
All the data we collected in this mannerwas released as surprise data to all participants.4.2 NEP-EN dataThe collection of NEP-EN data followed a simi-lar approach to that of SPA-EN.
We first focusedon finding users that switched frequently between3http://www.crowdflower.com/4https://www.facebook.com/VeteranaPinup5https://www.facebook.com/pages/Chicanas/4444837722938936http://thealbinobean.blogspot.com/64Nepali and English.
In addition, the users mustnot be using Devnagari script as done by Nepaleseto write Nepali, but must have used its Roman-ized form.
We started by manually reading tweetsfrom some of our Nepali friends.
We then crawledtheir followers who corresponded with them usingcode-switched tweets or replies.
We found thata lot of these users were regular code-switchersthemselves.
We repeated the same process with thefollowers and collected nearly 30 such users.
Wethen collected about 2,000 tweets each from theseusers using the Twitter API.
We filtered out all theretweets and the tweets with URLs, following thesame process that was used for SPA-EN.For the surprise test data, we crawled code-switched data from Facebook comments and posts.We found that most Nepalese comments had a richamount of code-switched data.
However, we couldnot crawl their data because of privacy issues.
Nev-ertheless, we could crawl data from public Face-book pages.
We identified some public Nepali Face-book pages where anyone could comment.
Thesepages include FM, news and public figures?
publicFacebook pages.
We crawled the latest 10 feedsfrom these public pages using the Facebook APIand gathered about 12,000 comments and posts forthe shared task.Initially, we sought out help from Nepali gradu-ate students at the University of Alabama at Birm-ingham to annotate 100 tweets (1739 tokens).
Wegave the same annotation file to two annotators todo the annotation.
We found that they agreed withan accuracy of 95.34%.
These tweets were then re-viewed and used as initial gold data in Crowdflowerto annotate the first 1000 tweets.
The annotationjob was enabled only in Nepal and Bhutan.
Wedisabled India, even though people living in someregions of India (Darjeeling, Sikkim) also speakand write in Nepali, as most spammers were com-ing from India.
We then ran two batches of 5000tweets and one batch of 3000 tweets along with theinitial 1,000 tweets as the gold data.
This NEP-ENdata was then split into a 9,993 tweet training setand a 2,874 tweet test set.
No Twitter user appearedin both sets.4.3 MAN-EN dataThe MAN-EN tweets were collected from Twitterwith the Twitter API.
Users were selected fromlists of most followed Twitter accounts in Taiwan(where Mandarin Chinese is the official language).These users?
tweets were checked for Mandarin En-glish bilingualism and added to our data collectionif they contained both languages.The next round of usernames came from thelists of users that our original top accounts werefollowing.
The tweets written by this new set ofusers were then examined for Mandarin Englishcode switching and stored as data if they matchedthe criteria.The jieba tokenizer7was used to segment theMandarin sections of the tweets and compute off-sets of each segment.
We format the code switch-ing tweets into columns including language type,labels, and offsets.
Named entities were labeledmanually by a single annotator.The data was split by user into 1000 tweets fortraining and 313 for testing.
No MAN-EN surprisedata for the current shared task.4.4 MSA-DA dataFor the MSA-DA language pair, we selected Egyp-tian Arabic (EGY) as the Arabic dialect.
We har-vested data from two social media sources: Twitter[TWT] and Blog commentaries [COM].
The TWTdata served as the main gold standard data for thetask where we provided fully annotated data forTraining/Tuning and Test.
We provided two TWTdata sets for the test data that exemplified differenttag distributions.
The COM data set comprisedonly test data and it served as the Arabic surprisedata set.To reduce the potential of TWT data attritionfrom users deleting their accounts or tweets, weselected tweets that are less prone to deletion and/orchange.
Thereby we harvested tweets by a selectset of Egyptian Public Figures.
The percentageof deleted tweets and deactivated accounts amongthose users is significantly lower if we compare itto the tweets crawled from random Egyptian users.We used the ?Tweepy?
library to crawl the time-lines of 12 Public Figures.
Similar to other lan-guage pairs, we excluded all re-tweets, tweets withURLs, tweets mentioning other users, and tweetscontaining Latin characters.
We accepted 9,947tweets, for each we extracted the tweet-id and user-id.
Using these IDs, we retrieved the tweets text,tokenized it and assigned character offsets.
To guar-antee consistency and avoid any misalignment is-sues, we compiled the full pipeline into the ?ArabicTweets Token Assigner?
package which is made7https://github.com/fxsjy/jieba65available through the workshop website8.For COM, we selected 6723 commentaries (halfMSA and half DA) from ?youm7?9commen-taries provided by the Arabic Online CommentaryDataset (Zaidan and Callison-Burch, 2011).
TheCOM data set was processed (12017 total tokens)using the same pipeline created for the task.
Wealso provided the participants with the data format-ted with character offsets to maintain consistencyacross data sets in the Arabic subtask.The annotation of MSA-DA language pair datais based on two sets of guidelines.
The first setis a generic set of guidelines for code switchingin general across different language pairs.
Theseguidelines provide the overarching framework forannotating code switched data on the morpholog-ical, lexical, syntactic, and pragmatic levels.
Thesecond set of guidelines is language pair specific.We created the guidelines for the Arabic languagespecifically.
We enlisted the help of 3 annotatorsin addition to a super annotator, hence resultingin 4 annotators overall for the whole collection ofthe data.
All the annotators are native speakersof Egyptian Arabic with excellent proficiency inMSA.
The super annotator only annotated 10% ofthe overall data and served as the adjudicator.
Theannotation process was iterative with several repe-titions of the cycle of training, annotation, revision,adjudication until we approached a stable Inter An-notator Agreement (IAA) of over 90% pairwiseagreement.5 Survey of Shared Task SystemsWe received submissions from seven differentteams.
Each participating system had the freedomto submit responses to any of the language pairscovered in the shared task.
All seven participantssubmitted system responses for SPA-EN, makingthis language pair the most popular in this sharedtask and MAN-EN the least popular.All but one participating system used a machinelearning algorithm or language models, or even acombination of both, as part of their configuration.A couple of the participating systems used hand-crafted rules of some sort, either at the intermediatesteps or as the final post-processing step.
We alsoobserved a good number of systems using exter-nal resources, in the form of labeled monolingual8http://emnlp2014.org/workshops/CodeSwitch/call.html9An Egyptian newspaper, www.youm7.comcorpora, language specific gazetteers, off the shelftools (NE recognizers, language id systems, or mor-phological analyzers) and even unsupervised datacrawled from the same users present in the datasets provided.
Affixes were also used in some formby different systems.The architecture of the different systems rangedfrom a simple approach based on frequencies ofcharacter n-grams combined in a rule-based system,to more complex approaches using word embed-dings, extended Markov Models, and CRF autoen-coders.
The majority of the systems that partici-pated in more than one language pair did little to nocustomization to account for the morphological dif-ferences of the specific language pairs beyond lan-guage specific parameter-tuning, which probablyreflects participants?
goal to develop a multilingualid system.Due to the presence of the NE label, severalsystems included a component for NE recognitionwhere there was one available for the specific lan-guage.
In addition, many systems also includedcase information.
One unexpected finding fromthe shared task was that no participating systemtried to embed in their models some form of lin-guistic theory or framework about CS.
Only onesystem made an explicit reference to CS theories(Chittaranjan et al., 2014) in their motivation to usecontextual information, which can be consideredas a loose embedding of CS theory.
While systemperformance was competitive (see next section),there is still room for improvement and perhapssome of that improvement can come out of addingthis kind of knowledge into the models.
Lastly, wewere surprised to see that not all systems made useof character encoding information, even though forMandarin-English that would have been a strongindicator.
In Table 3 we present a summary high-lighting some of the design choices of participatingsystems.6 ResultsWe used the following evaluation metrics: Accu-racy, Precision, Recall, and F-measure.
We useF-measure to provide a ranking of systems.
In theevaluation at the tweet level we use the standardf-measure.
For the evaluation at the token levelwe use instead the average weighted f-measure toaccount for the highly imbalanced distribution ofclasses.To provide a fair evaluation, we only scored pre-66SystemMachineLearningRules CaseCharacterEncodingExternal Resources LM Affixes Context(Chittaranjan et al., 2014)CRF44 dbpedia dumps, online sources?
3(Shrestha, 2014) 44 spell checker(Jain and Bhat, 2014)CRF44 English dictionary4 4 ?
2(King et al., 2014)eMMANERgazet, TwitterNLP, Stan-ford NER4 4 4(Bar and Dershowitz, 2014)SVM4Illocution Twitter Lexicon,monolingual corpora (NE lists)4 4 ?
2(Lin et al., 2014)CRF44Hindi-Nepali Wikipedia, JRC,CoNLL 2003 shared task, langid predictors: cld2 and ldig4 4(Barman et al., 2014)kNN, SVM4 4BNC, LexNorm4 ?
1Table 3: Comparison of shared task participating system algorithm choices.
CRF stands for ConditionalRandom Fields, SVM for Support Vector Machines and LM for Language Models.dictions on tweets submitted by all teams.
Allsystems were compared to a simple lexicon-basedbaseline.
The lexicon was gathered from the train-ing data for classes lang1 and lang2 only.
Emoti-cons, punctuation marks, usernames and URLs areby default tagged as other.
In the case of a tie or anew token, the baseline system assigns the majorityclass for that language pair.Figure 1 shows prediction performance on theTwitter test data for each language pair at the tweetlevel.
The system predictions for this task are takendirectly from the individual token predictions inthe following manner: if the system predictions forthe same tweet contain at least one tag from eachlanguage (lang1 and lang2), the tweet is labeledas code-switched, otherwise it is labeled as mono-lingual.
As illustrated, each language pair showsdifferent patterns.
Comparing the systems that par-ticipated in all language pairs, there is no clearwinner across the board.
However, (Chittaranjan etal., 2014) was in the top three places in at least onetest file for each language pair.
Table 4 shows theresults at the token level by label.
Here again thefigures show F-measure per class label and the lastcolumn is the weighted average f-measure (Avg-F).One of the few general trends on these results isthat most participating systems were not able tocorrectly identify the minority classes ?ambiguous?and ?other?.
There are only few instances of theselabels in the training set and some test sets did nothave one of these classes present.
The impact onfinal system performance from these classes is notsignificant.
However, to study CS patterns we willneed to have these labels identified properly.The MAN-EN pair received four system re-sponses and all four of them reached an F-measure>80% and outperformed the simple baseline by aconsiderable margin.
We expected this languagepair to be the easiest one for the shared task sinceeach language uses a different encoding script.
Avery rough but accurate distinction between Man-darin and English could be achieved by lookingat the character encoding.
However, according tothe system descriptions provided, not all systemsused encoding information.
The best performingsystems for MAN-EN are (King et al., 2014) and(Chittaranjan et al., 2014).
The former slightlyoutperformed the latter at the Tweet level (see Fig-ure 1a) task while the opposite was true at the tokenlevel (see Table 4 rows 4 and 5).In the case of SPA-EN, all seven systems out-performed the simple baseline.
The best perform-ing system in all SPA-EN tasks was (Bar andDershowitz, 2014).
This system achieved an F-measure of 82.2%, 2.9 percentage points above thesecond best system (Lin et al., 2014) on the tweetlevel task (see Figure 1(d)).
In the token levelevaluation, (Bar and Dershowitz, 2014) reachedan Avg.
F-measure of 94%.
This top performingsystem uses a sequential classification approachwhere the labels from the preceding words are usedas features in the model.
Another design choicethat might have given the edge to this system is thefact that their model combines character- and word-based language models in what the authors call?intra- and inter-word level?
features.
Both typesof language models are trained on large amountsof monolingual data and NE lists, which again pro-vides additional knowledge that other systems arenot exploiting.
For instance, the NE lexicons mightaccount for the best results in the NE class in boththe Twitter data and the Surprise genre (see Table 4last row for SPA-EN and second to last for SPA-EN Surprise).
Most systems showed considerable67(Jain andBhat, 2014)(Lin etal., 2014)(Chittaranjanet al., 2014)(King etal., 2014)0.60.70.80.910.8380.8880.8920.894F-measureBaseline(a) MAN-EN(Chittaranjanet al., 2014)(King etal., 2014)(Jain andBhat, 2014)(Elfardy etal., 2014)(Lin etal., 2014)0.10.20.30.40.1520.1180.0480.0440.0950.1960.2600.3380.3600.417F-measureBaseline Test1Baseline Test2(b) MSA-DA.
Dark gray bars show performance on Test1 and light gray bars show performance for Test2(King etal., 2014)(Lin etal., 2014)(Jain andBhat, 2014)(Shrestha,2014)(Chittaranjanet al., 2014)(Barman etal., 2014)0.80.910.9520.9620.9720.9740.9750.977F-measureBaseline(c) NEP-EN(Shrestha,2014)(King etal., 2014)(Chittaranjanet al., 2014)(Barman etal., 2014)(Jain andBhat, 2014)(Lin etal., 2014)(Bar andDershowitz,2014)0.60.70.80.910.6340.7030.7530.7540.7830.7930.822F-measureBaseline(d) SPA-ENFigure 1: Prediction results on language identification at the tweet level.
This is a binary task to distinguishbetween a monolingual and a CS tweet.
We show performance of participating systems using F-measureas the evaluation metric.
The solid line shows the lexicon baseline performance.differences in prediction performance in both gen-res.
In all cases the Avg.
F-measure was higheron the Twitter test data than on the surprise genre.Although the surprise genre is too small to drawstrong conclusions, all language pairs with surprisegenre test data showed a decrease in performanceof around 10%.We analyzed system outputs and found someconsistent sources of error.
Lexical forms that existin both languages were frequently mislabeled by68most systems.
For example the word for ?he?
wasfrequently mislabeled by at least one system.
Inmost of the cases systems were predicting EN aslabel when the target language was SPA.
Cases likethis were even more prone to errors when thesewords fell in the CS point, as in this tweet: ni elheader he hecho (I haven?t even done the header).Tweets like this one, with just one token from theother language, were difficult for most systems.Named entities were also frequent sources of error,especially when they were spelled with lower casesletters.By far the hardest language pair in this sharedtask was MSA-DA, as anticipated.
Especially whenconsidering the typological similarities betweenMSA and DA.
This is mainly due to the fact thatDA and MSA are close variants of one another andhence they share considerable amount of lexicalitems.
The shared lexical items could be simplecognates of one another, or faux amis where theyare homographs or homophones, but have com-pletely different meaning.
Both categories con-stitute a significant challenge.
Accordingly, thebaseline system had the lowest performance fromall language pairs in both test sets.
We note chal-lenges in this language pair on each linguistic levelwhere CS occurs especially for the shared lexicalitems.On the phonological level, DA writers tend tomimic the MSA script for DA words even if theyare pronounced differently.
For example: ?heart?
ispronounced in DA Alob and in MSA as qalob butcommonly written in MSA as ?qalob?
in DA data.Also many phonological differences are in shortvowels that are underspecified in written Arabic,adding another layer of ambiguity.On the morphological level, there is no avail-able morphological analyzer able to recognize suchshared words and hence they are mostly misclassi-fied.
Language identification for MSA-DA CS texthighly depends on the context.
Typically some Ara-bic variety word serves as a marker for a contextswitch such as mElh$ for DA or mn?
for MSA.
Butif shared lexical items are used, it is challengingto identify the Arabic variant.
An example fromthe training data is qlb meaning either heart as anoun or change as a verb in the phrase lw qlb mjrm,corresponding to ?If the heart of a criminal?
or ?ifhe changes into a criminal?.
These challenges ren-der language identification for CS MSA-DA datafar from solved as evident by the fact that the high-est scoring system reached an F-measure of only41.7% in Test2 for CS identification.
Moreover,this is the only language pair where at least onesystem was not able to outperform the baseline andin the case of Test2 only one system (Lin et al.,2014) outperformed the baseline.Most teams did well for the NEP-EN shared task,and all teams outperformed the baseline.
The rea-son for the high performance might be the highnumber of codeswitched tweets in the training andtest data for NEP-EN (much higher than other lan-guage pairs).
This allowed systems to have moresamples of CS instances.
The other reason for goodperformance by most participants in both evalua-tions might be that Nepali and English are two verydifferent languages.
The structure of the words andsyntax of word formation are very different.
Wesuspect, for instance, that there is a much loweroverlap of character n-grams in this language pairthan in SPA-EN, which makes for an easier task.
Atthe Tweet level, system performance ranged overa small set of values, the lowest F-measure was95.2% while the highest was 97.7%.
Looking atthe numbers in Table 4, we can see that even NErecognition seemed to be a much easier task for thislanguage pair than for SPA-EN (compare resultsfor the NE category in both SPA-EN sets to thoseof both NEP-EN data sets).
The best performingsystem for the Twitter test data is (Barman et al.,2014) with an F-measure of 97.7%.
The resultstrend in the surprise genre is not consistent withwhat we observed for the Twitter test data.
Thetop ranked system for Twitter sunk to the 4th placewith an F-measure or 59.6%, a considerable dropof almost 40 percentage points.
In this case, theoverall numbers indicate a much wider differencein the genres than what we observed for other lan-guages, such as SPA-EN, for example.
We shouldnote that the class distribution in the surprise data isconsiderably different from what the models usedfor training, and from that of the test data as well.In the Twitter data there was a larger number of CStweets than monolingual ones, while in the surprisegenre the majority class was monolingual.
Thiswill account for a good portion of the differencesin performance.
But here as well, the small numberof labeled instances makes it hard to draw strongconclusions.69Test Set System lang1 lang2 NE other ambiguous mixed Avg-FMAN-ENBaseline 0.9 0.47 0 0.29 - 0 0.761(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892MSA-DA Test 1(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720Baseline 0.92 0.06 0 0.89 0 - 0.819(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936MSA-DA Test 2Baseline 0.54 0.27 0 0.94 0 0 0.385(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799MSA-DA Surprise(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801NEP-ENBaseline 0.67 0.76 0 0.61 - 0 0.678(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959NEP-EN Surprise(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855SPA-ENBaseline 0.72 0.56 0 0.75 0 0 0.704(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940SPA-EN Surprise(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839Table 4: Performance results on language identification at the token level.
A ?-?
indicates there were notokens of this class in the test set.
We ranked systems using weighted averaged f-measure (Avg-F).
The ?
*?marks the system by (Elfardy et al., 2014).
This system was not considered in the ranking for the sharedtask as it was developed by co-organizers of the task.7 Lessons LearnedAmong the things we want to improve for futureshared tasks is the issue of data loss due to re-moval of tweets or users deleting their accounts.We decided to use Twitter data to have a relevantcorpus.
However, the trade-off is the lack of rightsto distribute the data ourselves.
This is not just aburden for the participants.
It is an awful waste ofresources as the data that was expensive to gatherand label is not being used beyond the small groupof researchers involved in the creation of the cor-pus.
This will deter us from using Twitter data forfuture shared tasks, at least until a better solutionis identified.70(Chittaranjanet al., 2014)(King etal., 2014)(Jain andBhat, 2014)(Lin etal., 2014)(Elfardy etal., 2014)00.10.20.30.1700.1940.2220.2760.277F-measure(a) MSA-DA Surprise Genre Results(Chittaranjanet al., 2014)(Jain andBhat, 2014)(Barman etal., 2014)(King etal., 2014)(Shrestha,2014)(Lin etal., 2014)0.40.50.60.70.5540.5710.5960.6040.6320.702F-measure(b) NEP-EN Surprise Genre Results(Shrestha,2014)(King etal., 2014)(Chittaranjanet al., 2014)(Barman etal., 2014)(Jain andBhat, 2014)(Lin etal., 2014)(Bar andDershowitz,2014)0.40.50.60.70.80.6330.6400.7040.7100.7250.7270.753F-measure(c) SPA-EN Surprise Genre ResultsFigure 2: Prediction results on language identification at the document level for the surprise genre.
Thisis a binary task to distinguish between a monolingual and a code-switched text.
We show performance ofparticipating systems using F-measure as the evaluation metric.Using crowdsourcing for annotating the data is acheap and easy way for generating resources.
Butwe found out that even when following best prac-tices for quality control, there was a substantialamount of noise in the gold data.
We plan to con-tinue working on refining the annotation guidelinesand quality control processes to reduce the amountof noise in gold annotations.8 ConclusionThis is the first shared task on language identifica-tion in CS data.
Yet, the response was quite positiveas we received 42 system runs from seven differentteams, plus submissions for MSA-AD from a subgroup of the task organizers (Elfardy et al., 2014).The systems presented are overall robust and withinteresting differences from one another.
Althoughwe did not see a single system ranking in the topplaces across all language pairs and tasks, we didsee systems showing robust performance indicat-ing some level of language independence.
But theresults are not consistent at the tweet/documentlevel.
The language pair that proved to be the mostdifficult for the task was MSA-DA, where the lexi-con baseline system was hard to beat even with anF-measure of 47.1%.This shared task showed that language identifica-tion in code-switched data is still an open problemthat warrants further investigation.
Perhaps in thenear future we will see systems that embed someform of linguistic theory about CS and maybe thatwould result in more accurate predictions.Our goal is to support new research addressingCS data.
Discussions about the challenge for thenext shared task are already underway.
One pos-sibility might be parsing.
We plan to investigatethe challenges in parsing CS data and we will startby exploring the hardships in manually annotatingCS with syntactic information.
We would also liketo explore the possibility of classifying CS pointsaccording to their socio-pragmatic role.71AcknowledgmentsWe would like to thank all shared task partici-pants.
We also thank Brian Hester and MohamedElbadrashiny for their invaluable support in thedevelopment of the gold standard data and analy-sis of results.
We also thank the in-lab annotatorsand the CrowdFlower contributors.
This work waspartly funded by NSF under awards 1205475 and1205556.ReferencesKfir Bar and Nachum Dershowitz.
2014.
Tel Aviv Uni-versity system description for the code-switchingworkshop shared task.
In Proceedings of the FirstWorkshop on Computational Approaches to Code-Switching, Doha, Qatar, October.
ACL.Utsab Barman, Joachim Wagner, Grzegorz Chrupala,and Jennifer Foster.
2014.
DCU-UVT: Word-level language classification with code-mixed data.In Proceedings of the First Workshop on Computa-tional Approaches to Code-Switching, Doha, Qatar,October.
ACL.Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, andMonojit Choudhury.
2014.
A framework to labelcode-mixed sentences in social media.
In Proceed-ings of the First Workshop on Computational Ap-proaches to Code-Switching, Doha, Qatar, October.ACL.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2014.
AIDA: Identifying code switching ininformal Arabic text.
In Proceedings of the FirstWorkshop on Computational Approaches to Code-Switching, Doha, Qatar, October.
ACL.Naman Jain and Riyaz Ahmad Bhat.
2014.
Languageidentification in codeswitching scenario.
In Pro-ceedings of the First Workshop on ComputationalApproaches to Code-Switching, Doha, Qatar, Octo-ber.
ACL.A.
Joshi.
1982.
Processing of sentences with in-trasentential code-switching.
In J?an Horeck?y, editor,COLING-82, pages 145?150, Prague, July.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 1110?1119, Atlanta, Georgia, June.
Association for Com-putational Linguistics.Levi King, Eric Baucom, Tim Gilmanov, SandraK?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-drigues.
2014.
The IUCL+ system: Word-levellanguage identification via extended Markov models.In Proceedings of the First Workshop on Computa-tional Approaches to Code-Switching, Doha, Qatar,October.
ACL.Ying Li, Yue Yu, and Pascale Fung.
2012.
AMandarin-English code-switching corpus.
In Nico-letta Calzolari, Khalid Choukri, Thierry Declerck,Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eighth International Conference onLanguage Resources and Evaluation (LREC-2012),pages 2515?2519, Istanbul, Turkey, May.
EuropeanLanguage Resources Association (ELRA).
ACL An-thology Identifier: L12-1573.Constantine Lignos and Mitch Marcus.
2013.
Towardweb-scale analysis of codeswitching.
In 87th An-nual Meeting of the Linguistic Society of America.Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and LoriLevin.
2014.
The CMU submission for the sharedtask on language identification in code-switcheddata.
In Proceedings of the First Workshop on Com-putational Approaches to Code-Switching, Doha,Qatar, October.
ACL.D.C.
Lyu, T.P.
Tan, E. Chng, and H. Li.
2010.
SEAME:a Mandarin-English code-switching speech corpusin South-East Asia.
In INTERSPEECH, volume 10,pages 1986?1989.Dong Nguyen and A. Seza Do?gru?oz.
2013.
Word levellanguage identification in online multilingual com-munication.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Process-ing, pages 857?862, Seattle, Washington, USA, Oc-tober.
Association for Computational Linguistics.Prajwol Shrestha.
2014.
An incremental approach forlanguage identification in codeswitched text.
In Pro-ceedings of the First Workshop on ComputationalApproaches to Code-Switching, Doha, Qatar, Octo-ber.
ACL.Anil Kumar Singh and Jagadeesh Gorla.
2007.
Identifi-cation of languages and encodings in a multilingualdocument.
In Proceedings of ACL-SIGWAC?s WebAs Corpus3, Belgium.Omar F. Zaidan and Chris Callison-Burch.
2011.
TheArabic online commentary dataset: An annotateddataset of informal Arabic with high dialectal con-tent.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: Short Papers - Volume2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.Association for Computational Linguistics.72
