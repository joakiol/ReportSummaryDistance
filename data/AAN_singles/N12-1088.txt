2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 699?709,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsLow-Dimensional Discriminative RerankingJagadeesh JagarlamudiDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742, USAjags@umiacs.umd.eduHal Daume?
IIIDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742, USAhal@umiacs.umd.eduAbstractThe accuracy of many natural language pro-cessing tasks can be improved by a rerankingstep, which involves selecting a single outputfrom a list of candidate outputs generated bya baseline system.
We propose a novel fam-ily of reranking algorithms based on learningseparate low-dimensional embeddings of thetask?s input and output spaces.
This embed-ding is learned in such a way that predictionbecomes a low-dimensional nearest-neighborsearch, which can be done computationally ef-ficiently.
A key quality of our approach is thatfeature engineering can be done separately onthe input and output spaces; the relationshipbetween inputs and outputs is learned auto-matically.
Experiments on part-of-speech tag-ging task in four languages show significantimprovements over a baseline decoder and ex-isting reranking approaches.1 IntroductionMapping inputs to outputs lies at the heart of manyNatural Language Processing applications.
For ex-ample, given a sentence as input: part-of-speech(POS) tagging involves finding the appropriate POStag sequence (Thede and Harper, 1999); pars-ing involves finding the appropriate tree structure(Kubler et al, 2009) and statistical machine trans-lation (SMT) involves finding correct target lan-guage translation (Brown et al, 1993).
The accuracyachieved on such tasks can often be improved signif-icantly with the help of a discriminative rerankingstep (Collins and Koo, 2005; Charniak and John-son, 2005; Shen et al, 2004; Watanabe et al, 2007).For the POS tagging, reranking is relative less ex-plored due to the already higher accuracies in En-glish (Collins, 2002), but it is shown to improve ac-curacies in other languages such as Chinese (Huanget al, 2007).
In this paper, we propose a novel ap-proach to discriminative reranking and show its ef-fectiveness in POS tagging.
Reranking allows us touse arbitrary features defined jointly on input andoutput spaces that are often difficult to incorporateinto the baseline decoder due to the computationaltractability issues.
The effectiveness of rerankingdepends on the joint features defined over both inputand output spaces.
This has led the community tospend substantial efforts in defining joint features forreranking (Fraser et al, 2009; Chiang et al, 2009).Unfortunately, developing joint features over theinput and output space can be challenging, espe-cially in problems for which the exact mapping be-tween the input and the output is unclear (for in-stance, in automatic caption generation for images,semantic parsing or non-literal translation).
In con-trast to prior work, our approach uses features de-fined separately within the input and output spaces,and learns a mapping function that can map an ob-ject from one space into the other.
Since our ap-proach requires within-space features, it makes thefeature engineering relatively easy.For clarity, we will discuss our approach in thecontext of POS tagging, though of course it gener-alizes to any reranking problem.
At test time, inPOS tagging, we receive a sentence and a list ofcandidate output POS sequences as input.
We runa feature extractor on the input sentence to obtaina representation x ?
Rd1 ; we run an independent699feature extractor on each of the m-many outputsto obtain representations y?1, .
.
.
, y?m ?
Rd2 .
Wewill project all of these points down to a low k-dimensional space by means of matrices A ?
Rd1?k(for x as ATx) and B ?
Rd2?k (for y?
as BT y?
).We then select as the output the y?j that maximizescosine similar to x in the lower-dimensional space:maxj cos(ATx, BT y?j).
The goal is to learn the pro-jection matrices A and B so that the result of thisoperation is a low-loss output.Given training data of sentences and their refer-ence tag sequences, our approach implicitly uses allpossible pairwise feature combinations across theviews and learns the matrices A and B that can mapa given sentence (as its feature vector) to its cor-responding tag sequence.
Considering all possiblepairwise combinations enables our model to auto-matically handle long range dependencies such asa word at a position effecting the tag choice at anyother position.Experiments performed on four languages (En-glish, Chinese, French and Swedish) show the ef-fectiveness of our approach in comparison to thebaseline decoder and to the existing reranking ap-proaches (Sec.
4).
Using only the within-space fea-tures, our models are able to beat reranking ap-proaches that use more informative joint features.While it is possible to include joint features into ourmodels, we leave this for future work.2 Models for Low-Dimensional RerankingIn this section, we describe our approach to learninglow-dimensional representations for reranking.
Wefirst fix some notation, then discuss the intuition be-hind the problem we wish to solve.
We propose bothgenerative-style and discriminative-style approachesto formalizing this intuition, as well as a softenedvariant of the discriminative model.
In the subse-quent section, we discuss computational issues re-lated to these models.2.1 NotationLet xi ?
Rd1 and yi ?
Rd2 be the feature vectorsrepresenting the ith(1 ?
?
?n) sentence and its refer-ence tag sequence from the training data.
Each sen-tence is also associated with mi number of candi-date tag sequences, output by the baseline decoder,and are represented as y?ij ?
Rd2 j = 1 ?
?
?mi.
Eachcandidate tag sequence (y?ij) is also associated witha non-negative loss Lij .
Note that we place abso-lutely no constraints on the loss function.
Moreover,letX (d1?n) and Y (d2?n) denote the data matri-ces with xi and yi as columns respectively.
Finally,let ?u,v?
denote the dot product of the two vectorsu and v.2.2 IntuitionAs stated in the introduction, our goal is to learnprojections A ?
Rd1?k and B ?
Rd2?k in such away that test-time predictions are made with highaccuracy (or low loss).
At test time, the output willbe chosen by maximizing cosine similarity betweenthe input and the output, after projecting these vec-tors into a low-dimensional space using A and B,respectively.
The cosine similarity in our context is:xTABT y?j?xTAATx?y?Tj BBT y?j(1)Our goal is to learn A and B in such a way that they?j with maximum cosine similarity to an x is ac-tually the correct output.
In what follows, we willdescribe our models to find one-dimensional projec-tion vectors a ?
Rd1 and b ?
Rd2 , but the general-ization to matrices A and B is very trivial.2.3 A Generative-Style ModelThe first model we propose is akin to a gener-ative probabilistic model, in the sense that it at-tempts to model the relationship between an inputand its desired output, without taking alternate pos-sible outputs into account.
In the context of the in-tuition sketched in the previous section, the idea isto choose A and B so as to maximize the cosinesimilarities on the training data between each inputand it?s correct (or minimal-loss) output.
This modelintentionally ignores the information present in thealternative, incorrect outputs.
The hope is that bymaking the cosine similarities with the best outputas high as possible, all the alternate outputs will lookbad in comparison.Given a training data of sentences and theirreference tag sequences represented as X and Y(Sec.
2.1), our generative model finds projection di-rections, in word and tag spaces, along which the700aligned sentence and tag sequence pairs have maxi-mum cosine similarity.
In the one-dimensional set-ting, it finds directions a ?
Rd1 and b ?
Rd2 suchthat the correlation as defined in Eq.
2 is maximized.aTXY Tb?aTXXTa?bTY Y Tb(2)Since the objective is invariant to the scaling of vec-tors a and b, it can be rewritten as:argmaxa,baTXY Tb (3)s.t.
aTXXTa = 1 and bTY Y Tb = 1(4)We refer to the constraints in Eq.
4 as length con-straints in the rest of this paper.To understand why maximizing this objectivefunction learns a good mapping function betweenthe sentence and the tag sequence, consider decom-posing the objective function as follows:aTXY Tb =n?i=1?xi,a?
?yi,b?=n?i=1(d1?l=1xlial ?d2?m=1ymi bm)=n?i=1(d1?l=1d2?m=1xlial ymi bm)=n?i=1(d1,d2?l,m=1wlm?lmi)(5)where we replaced the scalars xliymi and albm with?lmi and wlm respectively.
So finally, the objectivecan be expressed as aTXY Tb =?i?w, ?
(xi,yi)?where w is the weight vector and ?
(xi,yi) is a vec-tor of size (d1 ?
d2) and is given by the Kroneckerproduct of the two feature vectors xi and yi.In this form, the generative objective functionbears similarity to the linear boundary surfacewidely used in machine learning, except that theweights are restricted to be the outer product of twovectors.
From the reduced expressions, it is clearthat our generative model considers all possible pair-wise combinations of the input features (d1?d2) andlearns which of them are more important than others.Intuitively, it puts higher weight on a word and tagpair that co-occur frequently in the training data, atthe same time each of these are infrequent in theirown views.2.4 A Discriminative-Style ModelThe primary disadvantage of our generative model isthat it only uses input sentences and their referencetag sequences and does not use the incorrect candi-date tag sequences of a given sentence at all.
In whatfollows, we describe a model that utilize the incor-rect candidate tag sequences as negative examplesto improve the projection directions (a and b).
Ourgoal is to address this by adding constraints to ourmodel that explicitly penalize ranking high-loss out-puts higher than low-loss outputs, as is often done inthe context of maximum-margin structure predictiontechniques (Taskar et al, 2004).In this section, we describe a discriminativemodel that keeps track of the margin deviations andfinds the projection directions iteratively.
Intuitively,after the projection into the lower dimensional sub-space, the cosine similarity of a sentence to its refer-ence tag sequence must be greater than that of itsincorrect candidate tag sequences.
Moreover, themargin between these similarities should be propor-tional to the loss of the candidate translation, i.e.
themore dissimilar a candidate tag sequence to its ref-erence is, the farther it should be from the referencein the projected space.From the decomposition shown in Eq.
5, for agiven pair of source sentence xi and a tag sequenceyj , the generative model assigns a score of :?a,xi??b,yj?
= aTxiyTj bEach input sentence is also associated with a listof candidate tag sequences and since each of thesecandidate sequences are incorrect they should be as-signed a score less than that of the reference tag se-quence.
Drawing ideas from structure prediction lit-erature (Bakir et al, 2007), we modify the objec-tive function in order to include these terms.
Thisidea can be captured using a loss augmented mar-gin constraint for each sentence, tag sequence pair(Tsochantaridis et al, 2004).
Let ?i denote a non-negative slack variable, then we define our new op-timization problem as:arg maxa,b,??01?
??
aTXY Tb?
?i?i (6)s.t.
aTXXTa = 1 and bTY Y Tb = 1?i ?j aTxiyTi b?
aTxiy?Tijb ?
1?
?iLij701where 0 ?
?
?
1 is a weight parameter.
This ob-jective function is ensuring that the margin betweenthe reference and the candidate tag sequences in theprojected space (as given by aTxiyTi b?aTxiy?Tijb)is proportional to its loss (Lij).
Notice that the slackis defined for each sentence and it remains the samefor all of its candidate tag sequences.2.5 A Softened Discriminative ModelOne disadvantage of the discriminative model de-scribed in the previous section is that it cannot beoptimized in closed form (as discussed in the nextsection).
In this section, we consider a model thatlies between the generative model and the (fully)discriminative model.
This softened model has at-tractive computational properties (it is easy to com-pute) and will also form a building block for the op-timization of the full discriminative model.For each sentence xi, its reference tag sequenceyi should be assigned a higher score than any of itscandidate tag sequences y?ij i.e.
we want to maxi-mize aTxiyTi b?aTxiy?Tijb.
In the fully discrimina-tive model, we enforce that this is at least one (mod-ulo slack).
In the relaxed version, we instead requirethat this hold on average.
In order to achieve thiswe add the following terms to the objective function:?j = 1 ?
?
?miaTxiyTi b?
aTxiy?Tijb = aTxirTijb (7)where rij = yi ?
y?ij is the residual vector betweenthe reference and the candidate sequences.
Now,we simply sum all these terms for a given sentenceweighted by their loss and encourage it to be as highas possible, i.e.
we maximize1mimi?j=1Lij(aTxirTijb)= aTxi( 1mimi?j=1LijrTij)b (8)The normalization bymi takes care of unequal num-bers of candidate tag sequences that often arises be-cause of the difference in the lengths of the inputsentences.
Now let R denote a matrix of the samesize as that of Y (i.e.
d2 ?
n) with its ith column asgiven by 1mi?mij=1 Lijrij , then we add the followingterm to the generative objective function:n?i=1aTxi( 1mimi?j=1LijrTij)b = aTXRTb (9)Finally, the projection directions are obtained bysolving the following optimization problem :argmaxa,b(1?
?
)aTXY Tb+ ?
aTXRTb (10)s.t.
aTXXTa = 1 and bTY Y Tb = 1where 0 ?
?
?
1 is the weight parameter to betuned on the development set.3 OptimizationIn this section, we describe how we solve the opti-mization problems associated with our models.
Firstwe discuss the solution of the generative model.Next, we discuss the softened discriminative model,since its solution will be used as a subroutine in ourfinal discussion of the fully discriminative model.3.1 Optimizing the Generative ModelThe optimization problem corresponding to the gen-erative model turns out to be identical to that ofcanonical correlation analysis (CCA) (Hotelling,1936; Hardoon et al, 2004), which immediatelysuggests a solution by solving an eigensystem.
Inparticular, the projection directions are obtained bysolving the following generalized eigensystem:(0 CxyCyx 0)(ab)=(Cxx 00 Cyy)(ab)(11)where Cxx = (1 ?
?
)XXT + ?I , Cyy = (1 ??
)Y Y T + ?I are autocovariance matrices, Cxy =XY T is the cross-covariance matrix, Cyx = CTxy,?
is a regularization parameter and I is the identitymatrix of appropriate size.
Using these eigenvectorsas columns, we form projection matrices A and B.These projection matrices are used to project sen-tences and tag sequences into a common lower di-mensional subspace.
In general, using all the eigen-vectors is sub-optimal from the generalization per-spective so we retain only top k eigenvectors.3.2 Optimizing the Softened ModelIn the softened discriminative version, the summa-tion of all the difference terms over all candidate tagsequences and sentences (Eq.
9), enables a simplerobjective function whose optimum can be derivedby following a procedure very similar to that of the702generative model.
In particular, the projection direc-tions are obtained by solving Eq.
11 except that Cxyis replaced with X((1?
?
)Y T + ?RT ).3.3 Optimizing the Discriminative ModelTo solve the discriminative model, we begin by con-structing the Lagrange dual.
Let ?1, ?2 and ?ijbe the Lagrangian multipliers corresponding to thelength and the margin constraints respectively, thenthe Lagrangian of Eq.
6 is given by:L = 1?
??
aTXY Tb?n?i=1?i?
?1(aTXXTa?
1)?
?2(bTY Y Tb?
1)+n,mi?i=1,j=1?ij(aTxirTijb?
1 +?iLij)Differentiating the Lagrangian with respect to theparameters a,b and setting them to zero yieldsthe solution for the parameters in terms of the La-grangian multipliers ?ij as follows:(0 C?xyC?yx 0)(ab)=(Cxx 00 Cyy)(ab)(12)where C?xy = X(1???
Y T + RT)and R is a ma-trix of size d2 ?
n with ith column as given by1mi?mij=1 ?ijrij .
We use superscript ?
on the cross-covariance matrix to indicate that it is dependent onthe Lagrangian multipliers ?ij .
In other words, thesolution is similar to that of the previous formulationexcept that the residual vectors are weighted by theLagrangian multipliers instead of the loss function.Unlike the max margin formulations of SVM, it isnot easy to rewrite the parameters a,b in terms ofthe Lagrangian multipliers ?ij as C?xy itself dependson ?ij?s.
Hence, rewriting the parameters in termsof the Lagrangian multipliers and then solving thedual is not amenable in this case.In order to solve this optimization problem, weresort to an alternate optimization technique in theprimal space.
It proceeds in two stages.
In the firststage, we keep the Lagrangian multipliers ?ij fixedand then solve for the parameters a,b, ?1, ?2 and?i.
Projection directions a,b and their Lagrangianmultipliers ?1, ?2 are obtained by solving the gen-eralized eigenvalue problem given in Eq.
12.
UsingAlgorithm 1 Alternate optimization algorithm forsolving the parameters of Discriminative Model.Input: X,Y, Y?
, L, ?, ?Output: A,B1: ?i, j ?ij = Lij ;2: rij = yi ?
y?ij ; Cxx = (1 ?
?
)XXT + ?I;Cyy = (1?
?
)Y Y T + ?I3: repeat4: Form R with ith column as 1mi?mij=1 ?ijrij5: C?xy = X(1???
Y T +RT)6: Solve for the eigenvectors of Eq.
12.
.7: Form matrices A,B with top k eigenvectorsas columns; k is determined using dev.
set.8: Let An & Bn be normalized versions of Aand B s.t.
they follow the length constraints.9: for each sentence i = 1 ?
?
?n do10: j = 1?
?
?mi, ?ij =(1?
xTi AnBTn rij)Lij11: ?i = min{0 , ?ij | s.t.
?ij > 0}12: if ?i > 0 then13: dij = xTi AnBTn rij ?(1?
?iLij)14: ?ij = ?ij ?
?
dij15: end if16: end for17: until slack values doesn?t change18: return A,Bthese projection directions, we determine the slackvariable ?i for each sentence.
In the second stageof the alternate optimization, we fix a,b and ?i andtake a gradient descent step along ?ij?s to minimizethe function.
We repeat this process until conver-gence.
In our experiments, we noticed that this al-gorithm converges within five iterations, so we onlyrun it for five iterations.The pseudocode of our approach is shown inAlg.
1.
First we initialize the Lagrangian multipli-ers proportional to the loss of the candidate tag se-quences (step 1).
This ensures that the eigenvectorssolved in step 6 are same as the output given by thesoftened model (Sec.
2.5).
In general, in our experi-ments, we observed that this is a good starting point.After solving the generalized eigenvalue problem instep.
6, we consider the top k eigenvectors, as de-termined by the error on the development set andnormalize them so that they follow the length con-straints (steps 7 and 8).
In the rest of the algorithm,703we use these normalized projection directions to findthe slack values which are in turn used to find the up-date direction for the Lagrangian variables.In step 10, we compute the potential slack value(?ij) for each constraint so that it is satisfied andthen choose the minimum of the positive ?ij val-ues as the slack for this sentence (step 11).
If thechosen slack value is equal to zero, it implies that?ij ?
0 ?j = 1 ?
?
?mi which in turn implies thatall the constraints of a given input sentence are sat-isfied by the current projection directions and hencethere is no need to update the Lagrangian multipli-ers.
Otherwise, some of the constraints are still notsatisfied and hence we will update their correspond-ing Lagrangian multipliers in steps 13 and 14.
Inspecific, step 13 computes the deviation of the mar-gin constraints with the new slack value and step 14updates the Lagrangian multipliers along the gradi-ent direction.In principle, our approach is similar to the cuttingplane algorithm used to optimize slack re-scalingversion of Structured SVM (Tsochantaridis et al,2004), but it differs in selecting the slack variable(step 11).
The cutting plane method chooses ?i asthe maximum of {0, ?ij} where as we choose theminimum of the positive ?ij values as the slack.
In-tuitively, this means that the cutting plane algorithmchooses a constraint that is most violated which re-sults in fewer constraints.
This is crucial in struc-tured SVM, because solving the dual problem is cu-bic in terms of the number of examples and con-straints.
In contrast, our approach selects the slacksuch that at least one of the constraints is satisfiedand adds all the remaining constraints to the activeset.
Since step 6 considers a weighted average of allthese constraints the complexity depends only on thenumber of training examples and not the constraints.3.4 Combining with Viterbi Decoding ScoreAll the three formulations discussed until now do notconsider the Viterbi decoding score assigned to eachcandidate tag sequence.
As explained in Collins andKoo (2005), the decoding score plays an importantrole in reranking the candidate sentences.
Here, wedescribe a simple linear combination of the Viterbidecoding score and the score obtained by projectinginto the low-dimensional subspace, using projectiondirections obtained by any of the above models.For a given sentence xi and candidate tag se-quence pair y?ij , let sij and pij (Eq.
1) be the scoresassigned by Viterbi decoding and the lower dimen-sional projections respectively.
Then we define thefinal score for this pair as a simple linear combina-tion of these two scores as:Score(xi, y?ij) = sij + w pij (13)The weight w is optimized using a grid search onthe development data set, we search for w from 0 to100 with an increment of 1 and choose the value forwhich the error is minimum on the development set.3.5 Reranking for POS TaggingTo summarize our approach, we convert the train-ing data into feature vectors and use any of thethree methods discussed above to find the lower di-mensional projection directions (a and b).
Each ofthose approaches involve solving a similar general-ized eigenvalue problem (Eq.
11) with the cross co-variance matrix Cxy defined differently in the threeapproaches.
This problem can be solved in differ-ent ways, but we use the following approach since itreduces the size of the eigenvalue problem.C?1yy CTxyC?1xx Cxy b = ?
b (14)a =1??
C?1xx Cxy b (15)where ?
is the eigenvalue.
Assuming that d2 ?
d1,which is usually true in POS tagging because ofthe smaller tag vocabulary, these equations solvea smaller eigenvalue problem.
After solving theeigenvalue problem, we form matricesA andB withcolumns as the top k eigenvectors a and b respec-tively.
Given a new sentence and candidate tag se-quence pair (xi, y?ij), their similarity is obtained us-ing Eq.
1.
Now, based on the development data setwe find the weight (w) for the linear combination ofthe projection and Viterbi decoding scores (Eq.
13).During the reranking stage, we first use Eq.
1 tocompute the projection score for all the candidatetag sequences and then use Eq.
13 to combine thisscores with the decoding score.
The candidate tagsequences are reranked based on this final score.4 ExperimentsIn this section, we report POS tagging experimentson four languages: English, Chinese, French and704Train.
Dev.
TestEnglish (En.)
# sent.
15K 2K 1791# words 362K 47K 43KChinese (Zh.)
# sent.
50K 4K 3647# words 292K 26K 25KFrench (Fr.)
# sent.
9K 2K 1351# words 254K 57K 40KSwedish (Sv.)
# sent.
8K 2K 1431# words 137K 31K 28KTable 1: Training and test data statistics.Swedish.
The data in all these languages is obtainedfrom the CoNLL 2006 shared task on multilingualdependency parsing (Buchholz and Marsi, 2006).We only consider the word and its fine grained POStag (columns 2 and 5 respectively) and ignore thedependency links in the data.
Table 1 shows the datastatistics in each of these languages.We use a second order Hidden Markov Model(Thede and Harper, 1999) based tagger as a baselinetagger in our experiments.
This model uses trigramtransition and emission probabilities and is shownto achieve good accuracies in English and other lan-guages (Huang et al, 2007).
We refer to this as thebaseline tagger in the rest of this paper and is used toproduce n-best list for each candidate sentence.
Then-best list for training data is produced using multi-fold cross-validation like Collins and Koo (2005)and Charniak and Johnson (2005).
The first block ofTable 2 shows the accuracies of the top-ranked tagsequence (according to the Viterbi decoding score)and the oracle accuracies on the 10-best list.
Asexpected the accuracies on English and French arehigh and are on par with the state-of-the-art systems.From the oracle scores, it is clear that though there isa chance for improvement using reranking, the scopefor improvement in English is less compared to the5 point improvement reported for parsing (Charniakand Johnson, 2005).
This indicates the difficultyof the reranking problem for POS tagging in well-resourced languages.4.1 Reranking Features and BaselinesIn this paper, except for Chinese, we use suffixes oflength two to four as features in the word view andunigram and bigram tag sequences as features in thetag view.
That is, we convert each word of the sen-tence into suffixes of length two to four and thentreat each sentence as a bag of suffixes.
Similarly,we treat a candidate POS tag sequence as a bag ofunigram and bigram tag features.
For Chinese, weuse character sequences of length one and two asfeatures for the sentences and use unigram and bi-gram POS tag sequences on the tag view.
We didnot include any alignment based features, i.e.
fea-tures that depend on the position.We compare our models with a boosting-baseddiscriminative approach (Collins and Koo, 2005)and its regularized version (Huang et al, 2007).
Inorder to enable a fair comparison, we use suffix andtag pairs as features for both these models.
For ex-ample, we would generate the following features forthe word ?selling?
in the phrase ?the/DT selling/NNpressure/NN?
: (ng, NN), (ng, DT NN), (ing,NN),(ing,DT NN), (ling,NN), (ling,DT NN).
For com-parison purposes, we also show results by runningthe baseline rerankers with n-gram features.4.2 ResultsThere are following hyper parameters in each of ourmodels, regularization parameter ?
, weight parame-ter ?
in the discriminative and softened discrimina-tive models, the linear combination weight w withthe Viterbi decoding score, and finally, the size ofthe lower dimensional subspace (k).
We use gridsearch to tune these parameters based on the devel-opment data set.
The optimal hyperparameter valuesdiffer based on the model and the language, but thetagging accuracy is relatively robust with respect tothese parameter values.
For English, the best valuesfor the discriminative model are ?
= 0.95, ?
= 0.3and k = 75.
For the same language, Fig.
1 showsthe performance with respect to ?
and ?
parameters,respectively, with other parameters fixed to their op-timal values.
Notice that, although the performancevaries it is always more than the accuracy of thebaseline tagger (96.74%).Table 2 shows the results of different models onthe development and test data sets.
On the test dataset, the baseline reranking approaches perform bet-ter than the HMM decoder in Chinese and Swedishlanguages, but they underperform in English andFrench languages.
This is justifiable because the in-dividual characters are good indicators of POS tag70596.7496.7696.7896.896.8296.8496.860.78  0.8  0.82  0.84  0.86  0.88  0.9  0.92  0.94  0.96  0.98  1DiscriminativeSoftened-DiscGenerative?96.7496.7696.7896.896.8296.8496.860  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8DiscriminativeSoftened-Disc?Figure 1: Tagging accuracy with hyperparameters ?
and ?
on English development data set.Development Set Test setEnglish Chinese French Swedish English Chinese French SwedishBaseline 96.74 92.55 96.94 93.22 96.15 92.31 97.41 93.23Oracle 98.85 98.41 98.61 96.96 98.39 98.19 99.00 96.48Collins (Sufx) 96.66 93.00 96.87 93.50 96.06 92.81 97.35 93.44Regularized (Sufx) 96.60 93.12 96.90 93.36 96.00 92.88 97.38 93.35Generative 96.82 93.14 96.97 93.46 96.24 92.95 97.43 93.26Softened-Disc 96.85 93.14 97.04 93.49 96.32 92.87 97.53 93.24Discriminative 96.85 93.17 97.03 93.50 96.3 92.91 97.53 93.36Collins (n-gm) 96.74 93.14 97.06 93.44 96.13 92.74 97.54 93.45Regularized (n-gm) 96.78 93.14 97.01 93.45 96.14 92.80 97.52 93.40Table 2: Accuracy of the baseline HMM tagger and different reranking approaches.
For comparison purposes, we alsoshowed the results of Collins and Koo (2005) its regularized versions with n-gram features.
The improvements of ourdiscriminative models are statistically significant at p = 0.01 and p = 0.05 levels on Chinese and English respectively.information for Chinese and this additional informa-tion is being exploited by the reranking approaches.Swedish, on the other hand, is a Germanic languagewith compound word phenomenon which makes thebaseline HMM decoder weaker compared to Englishand French.The fourth block shows the performance of ourmodels.
Except in Swedish, one of our models out-perform the baseline decoder and the other rerank-ing approaches.
The fact that our models outperformthe baseline system and other reranking approachesindicate that, by considering all the pairwise com-binations of the input features our models capturedependencies that are left by other models.
Amongthe different formulations of our approach, maxi-mizing the margin between the correct and incorrectcandidates performed better than generative, and en-suring that the margin is proportional to the loss ofthe candidate sequence (discriminative) led to evenmore improved results.
Except in Chinese, our dis-criminative version performed at least as well as theother variants.
Compared to the baseline decoder,the discriminative version achieves a maximum im-provement of 0.6 points in Chinese while achieving0.15, 0.12 and 0.13 points of improvement in En-glish, French and Swedish languages respectively.We also reported the results of the baselinererankers with n-gram features in the fifth block ofTable 2.
We remind the reader that our models useonly suffix features, so for a fair comparison the706En.
Zh.
Fr.
Sv.Generative 94.83 89.89 96.1 91.89Softened-Disc 95.04 89.61 95.97 91.95Discriminative 94.95 89.76 95.82 92.11Table 3: Accuracies without combining with Viterbi de-coding score.reader should compare our results with the baselinererankers run with the suffix features.
The perfor-mance of these baseline rankers improved when weinclude the n-gram features but it is still less thanthe discriminative model in most cases.Finally, Table 3 shows the performance of ourmodels without combining with the Viterbi decod-ing score.
As shown, the performance drops signif-icantly and is in accordance with the behavior ob-served elsewhere (Collins and Koo, 2005).5 Related WorkIn this section, we discuss approaches that are mostrelevant to our problem and the approach.In NLP literature, discriminative reranking hasbeen well explored for parsing (Collins and Koo,2005; Charniak and Johnson, 2005; Shen and Joshi,2003; McDonald et al, 2005; Johnson and Ural,2010) and statistical machine translation (Shen etal., 2004; Watanabe et al, 2007; Liang et al, 2006).Collins (2002) proposed two reranking approaches,namely boosting algorithm and a voted perceptron,for the POS tagging task.
Later Huang et al (2007)propose a regularized version of the objective usedby Collins (2002) and show an improved perfor-mance for Chinese.
In all of the above rerankingapproaches, the feature functions are defined jointlyon the input and output, whereas in our approach,the features are defined separately within each viewand the algorithm learns the relationship betweenthem automatically.
This is the primary differencebetween our approach and the existing rerankers.In principle, our margin formulations are similarto the max margin formulations of CCA (Szedmaket al, 2007) and maximum margin regression (Szed-mak et al, 2006; Wang et al, 2007).
These ap-proaches solve the following optimization problem:min ?W?2 + C1T ?
(16)s.t.
?yi,W?(x)i?
?
1?
?i ?i = 1 ?
?
?nOur approach differs from these formulations in twomain ways: the score assigned by our generativemodel (equivalent to CCA) for an input-output pair(xTi abTyi) can be converted into this format bysubstituting W ?
baT but in doing so we areignoring the rank constraint.
It is often observedthat, dimensionality reduction leads to an improvedperformance and thus the rank constraint becomescrucial.
Another major difference is that, the con-straints in Eq.
16 represent that any input and out-put pair should have at least a margin of 1 (moduloslack), whereas in our approach, the constraints in-clude incorrect outputs along with their loss value.In other words, our formulation is more suitable forthe reranking problem while Eq.
16 is more suitablefor regression or classification tasks.
Our genera-tive model is very similar to the supervised semantichashing work (Bai et al, 2010) but the way we opti-mize is completely different from theirs.6 DiscussionIn this paper, we proposed a novel family of mod-els for discriminative reranking problem and showedimprovements for the POS tagging task in four dif-ferent languages.
Here, we restricted our scope toshowing the utility of our technique and, hence, didnot experiment with different features, though it isan important direction.
By using only within spacefeatures, our models are able to beat the rerank-ing approaches that use potentially more informa-tive alignment-based features.
It is also possible toinclude alignment-based features into our models byposing the problem as a feature selection problem onthe covariance matrices (Jagarlamudi et al, 2011).Our approach involves an inverse computation andan eigenvalue problem.
Although our models scaleto medium size data sets (our Chinese data set has50K examples and 33K features), these operationscan be expensive.
But there are alternative approx-imation techniques that scale well to large data sets(Halko et al, 2009).
We leave this for future work.AcknowledgmentsWe thank Zhongqiang Huang for providing the codefor the baseline systems, Raghavendra Udupa andthe anonymous reviewers for their insightful com-ments.
This work is partially funded by NSF grantsIIS-1153487 and IIS-1139909.707ReferencesBing Bai, Jason Weston, David Grangier, RonanCollobert, Kunihiko Sadamasa, Yanjun Qi, OlivierChapelle, and Kilian Weinberger.
2010.
Learningto rank with (a lot of) word features.
Inf.
Retr.,13(3):291?314, June.Gu?khan H. Bakir, Thomas Hofmann, BernhardScho?lkopf, Alexander J. Smola, Ben Taskar, andS.
V. N. Vishwanathan.
2007.
Predicting StructuredData (Neural Information Processing).
The MITPress.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19:263?311, June.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, CoNLL-X ?06, pages 149?164, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 173?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, NAACL ?09, pages 218?226, Stroudsburg,PA, USA.
Association for Computational Linguistics.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31:25?70, March.Michael Collins.
2002.
Ranking algorithms for named-entity extraction: boosting and the voted perceptron.In Proceedings of the 40th Annual Meeting on As-sociation for Computational Linguistics, ACL ?02,pages 489?496, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Alexander Fraser, Renjing Wang, and Hinrich Schu?tze.2009.
Rich bitext projection features for parse rerank-ing.
In Proceedings of the 12th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, EACL ?09, pages 282?290, Stroudsburg,PA, USA.
Association for Computational Linguistics.Nathan Halko, Per-Gunnar.
Martinsson, and A. JoelTropp.
2009.
Finding structure with randomness:Stochastic algorithms for constructing approximatematrix decompositions.
Technical report, CaliforniaInstitute of Technology.David R. Hardoon, Sandor R. Szedmak, and John R.Shawe-taylor.
2004.
Canonical correlation analy-sis: An overview with application to learning methods.Neural Comput., 16:2639?2664, December.Harold Hotelling.
1936.
Relation between two sets ofvariables.
Biometrica, 28:322?377.Zhongqiang Huang, Mary Harper, and Wen Wang.2007.
Mandarin part-of-speech tagging and discrim-inative reranking.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1093?1102,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Jagadeesh Jagarlamudi, Raghavendra Udupa, HalDaume?
III, and Abhijit Bhole.
2011.
Improvingbilingual projections via sparse covariance matrices.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages930?940, Edinburgh, Scotland, UK., July.
Associationfor Computational Linguistics.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown parsers.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 665?668,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sandra Kubler, Ryan McDonald, Joakim Nivre, andGraeme Hirst.
2009.
Dependency Parsing.
Morganand Claypool Publishers.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 761?768, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 91?98, Stroudsburg, PA, USA.
Association forComputational Linguistics.Libin Shen and Aravind K. Joshi.
2003.
An SVM basedvoting algorithm with application to parse reranking.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003 - Volume 4,CONLL ?03, pages 9?16, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.708Libin Shen, Anoop Sarkar, and Franz Och.
2004.
Dis-criminative reranking for machine translation.
In Hu-man Language Technology Conference and the 5thMeeting of the North American Association for Com-putational Linguistics: HLT-NAACL 2004, Boston,USA, May.S.
Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.2006.
Learning via linear operators: Maximum mar-gin regression; multiclass and multiview learning atone-class complexity.
Technical report, University ofSouthampton.Sandor Szedmak, Tijl De Bie, and David R. Hardoon.2007.
A metamorphosis of canonical correlation anal-ysis into multivariate maximum margin learning.
InProceedings of the fifteenth European Symposium onArtificial Neural Networks.Ben Taskar, Carlos.
Guestrin, and Daphne Koller.
2004.Max margin markov networks.
In Proceedings ofNIPS 16.Scott M. Thede and Mary P. Harper.
1999.
A second-order Hidden Markov Model for part-of-speech tag-ging.
In Proceedings of the Annual Meeting on Asso-ciation for Computational Linguistics, pages 175?182.Association for Computational Linguistics.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the twenty-first inter-national conference on Machine learning, ICML ?04,pages 104?, New York, NY, USA.
ACM.Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-mak.
2007.
Kernel regression based machine trans-lation.
In Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics; CompanionVolume, Short Papers, NAACL-Short ?07, pages 185?188, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online Large-Margin Training for Sta-tistical Machine Translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic, June.
Association forComputational Linguistics.709
