Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 214?223,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDualSum: a Topic-Model based approach for update summarizationJean-Yves DelortGoogle ResearchBrandschenkestrasse 1108002 Zurich, Switzerlandjydelort@google.comEnrique AlfonsecaGoogle ResearchBrandschenkestrasse 1108002 Zurich, Switzerlandealfonseca@google.comAbstractUpdate summarization is a new challengein multi-document summarization focusingon summarizing a set of recent documentsrelatively to another set of earlier docu-ments.
We present an unsupervised proba-bilistic approach to model novelty in a doc-ument collection and apply it to the genera-tion of update summaries.
The new model,called DUALSUM, results in the second orthird position in terms of the ROUGE met-rics when tuned for previous TAC competi-tions and tested on TAC-2011, being statis-tically indistinguishable from the winningsystem.
A manual evaluation of the gen-erated summaries shows state-of-the art re-sults for DUALSUM with respect to focus,coherence and overall responsiveness.1 IntroductionUpdate summarization is the problem of extract-ing and synthesizing novel information in a col-lection of documents with respect to a set of doc-uments assumed to be known by the reader.
Thisproblem has received much attention in recentyears, as can be observed in the number of partic-ipants to the special track on update summariza-tion organized by DUC and TAC since 2007.
Theproblem is usually formalized as follows: Giventwo collections A and B, where the documents inA chronologically precede the documents in B,generate a summary of B under the assumptionthat the user of the summary has already read thedocuments in A.Extractive techniques are the most commonapproaches in multi-document summarization.Summaries generated by such techniques consistof sentences extracted from the document collec-tion.
Extracts can have coherence and cohesionproblems, but they generally offer a good trade-off between linguistic quality and informative-ness.While numerous extractive summarizationtechniques have been proposed for multi-document summarization (Erkan and Radev,2004; Radev et al 2004; Shen and Li, 2010; Li etal., 2011), few techniques have been specificallydesigned for update summarization.
Most exist-ing approaches handle it as a redundancy removalproblem, with the goal of producing a summary ofcollection B that is as dissimilar as possible fromeither collection A or from a summary of collec-tionA.
A problem with this approach is that it caneasily classify as redundant sentences in whichnovel information is mixed with existing informa-tion (from collection A).
Furthermore, while thisapproach can identify sentences that contain novelinformation, it cannot model explicitly what thenovel information is.Recently, Bayesian models have successfullybeen applied to multi-document summarizationshowing state-of-the-art results in summarizationcompetitions (Haghighi and Vanderwende, 2009;Jin et al 2010).
These approaches offer clear andrigorous probabilistic interpretations that manyother techniques lack.
Furthermore, they have theadvantage of operating in unsupervised settings,which can be used in real-world scenarios, acrossdomains and languages.
To our best knowledge,previous work has not used this approach for up-date summarization.In this article, we propose a novel nonpara-metric Bayesian approach for update summariza-tion.
Our approach, which is a variation of Latent214Dirichlet Allocation (LDA) (Blei et al 2003),aims to learn to distinguish between common in-formation and novel information.
We have eval-uated this approach on the ROUGE scores anddemonstrate that it produces comparable resultsto the top system in TAC-2011.
Furthermore, ourapproach improves over that system when evalu-ated manually in terms of linguistic quality andoverall responsiveness.2 Related work2.1 Bayesian approaches in SummarizationMost Bayesian approaches to summarization arebased on topic models.
These generative mod-els represent documents as mixtures of latent top-ics, where a topic is a probability distribution overwords.
In TOPICSUM (Haghighi and Vander-wende, 2009), each word is generated by a sin-gle topic which can be a corpus-wide backgrounddistribution over common words, a distributionof document-specific words or a distribution ofthe core content of a given cluster.
BAYESSUM(Daume?
and Marcu, 2006) and the Special Wordsand Background model (Chemudugunta et al2006) are very similar to TOPICSUM.A commonality of all these models is the use ofcollection and document-specific distributions inorder to distinguish between the general and spe-cific topics in documents.
In the context of sum-marization, this distinction helps to identify theimportant pieces of information in a collection.Models that use more structure in the repre-sentation of documents have also been proposedfor generating more coherent and less redun-dant summaries, such as HIERSUM (Haghighiand Vanderwende, 2009) and TTM (Celikyilmazand Hakkani-Tur, 2011).
For instance, HIERSUMmodels the intuitions that first sentences in docu-ments should contain more general information,and that adjacent sentences are likely to sharespecic content vocabulary.
However, HIERSUM,which builds upon TOPICSUM, does not showa statistically signicant improvement in ROUGEover TOPICSUM.A number of techniques have been proposed torank sentences of a collection given a word distri-bution (Carbonell and Goldstein, 1998; Goldsteinet al 1999).
The Kullback-Leibler divergence(KL) is a widely used measure in summarization.Given a target distribution T that we want a sum-mary S to approximate, KL is commonly used asthe scoring function to select the subset of sen-tences S?
that minimizes the KL divergence withT :S?
= argminSKL(T, S) =?w?VpT (w) logpT (w)pS(w)where w is a word from the vocabulary V. Thisstrategy is called KLSum.
Usually, a smoothingfactor ?
is applied on the candidate distribution Sin order to avoid the divergence to be undefined1.This objective function selects the most repre-sentative sentences of the collection, and at thesame time it also diversifies the generated sum-mary by penalizing redundancy.
Since the prob-lem of finding the subset of sentences from acollection that minimizes the KL divergence isNP-complete, a greedy algorithm is often used inpractice2.
Some variations of this objective func-tion can be considered, such as penalizing sen-tences that contain document-specific topics (Ma-son and Charniak, 2011) or rewarding sentencesappearing closer to the beginning of the docu-ment.Wang et al(2009) propose a Bayesian ap-proach for summarization that does not use KLfor reranking.
In their model, Bayesian Sentence-based Topic Models, every sentence in a docu-ment is assumed to be associated to a unique la-tent topic.
Once the model parameters have beencalculated, a summary is generated by choosingthe sentence with the highest probability for eachtopic.While hierarchical topic modeling approacheshave shown remarkable effectiveness in learningthe latent topics of document collections, they arenot designed to capture the novel information ina collection with respect to another one, which isthe primary focus of update summarization.2.2 Update SummarizationThe goal of update summarization is to generatean update summary of a collection B of recentdocuments assuming that the users already readearlier documents from a collection A.
We refer1In our experiments we set ?
= 0.01.2In our experiments, we follow the same approach as in(Haghighi and Vanderwende, 2009) by greedily adding sen-tences to a summary so long as they decrease KL divergence.215to collection A as the base collection and to col-lection B as the update collection.Update summarization is related to novelty de-tection which can be defined as the problem ofdetermining whether a document contains new in-formation given an existing collection (Soboroffand Harman, 2005).
Thus, while the goal of nov-elty detection is to determine whether some infor-mation is new, the goal of update summarizationis to extract and synthesize the novel information.Update summarization is also related to con-trastive summarization, i.e.
the problem of jointlygenerating summaries for two entities in order tohighlight their differences (Lerman and McDon-ald, 2009).
The primary difference here is thatupdate summarization aims to extract novel or up-dated information in the update collection with re-spect to the base collection.The most common approach for update sum-marization is to apply a normal multi-documentsummarizer, with some added functionality to re-move sentences that are redundant with respectto collection A.
This can be achieved using sim-ple filtering rules (Fisher and Roark, 2008), Max-imal Marginal Relevance (Boudin et al 2008), ormore complex graph-based algorithms (Shen andLi, 2010; Wenjie et al 2008).
The goal here isto boost sentences in B that bring out completelynovel information.
One problem with this ap-proach is that it is likely to discard as redundantsentences in B containing novel information if itis mixed with known information from collectionA.Another approach is to introduce specific fea-tures intended to capture the novelty in collectionB.
For example, comparing collections A and B,FastSum derives features for the collection B suchas number of named entities in the sentence thatalready occurred in the old cluster or the numberof new content words in the sentence not alreadymentioned in the old cluster that are subsequentlyused to train a Support Vector Machine classifier(Schilder et al 2008).
A limitation with this ap-proach is there are no large training sets availableand, the more features it has, the more it is af-fected by the sparsity of the training data.3 DualSum3.1 Model FormulationThe input for DUALSUM is a set of pairs of collec-tions of documents C = {(Ai,Bi)}i=1...m, whereAi is a base document collection and Bi is an up-date document collection.
We use c to refer to acollection pair (Ac,Bc).In DUALSUM, documents are modeled as a bagof words that are assumed to be sampled from amixture of latent topics.
Each word is associatedwith a latent variable that specifies which topicdistribution is used to generate it.
Words in a doc-ument are assumed to be conditionally indepen-dent given the hidden topic.As in previous Bayesian works for summariza-tion (Daume?
and Marcu, 2006; Chemuduguntaet al 2006; Haghighi and Vanderwende, 2009),DUALSUM not only learns collection-specific dis-tributions, but also a general background distri-bution over common words, ?G and a document-specific distribution ?cd for each document d incollection pair c, which is useful to separate thespecific aspects from the general aspects of c. Themain novelty is that DUALSUM introduces spe-cific machinery for identifying novelty.To capture the differences between the base andthe update collection for each pair c, DUALSUMlearns two topics for every collection pair.
Thejoint topic, ?Ac captures the common informationbetween the two collections in the pair, i.e.
themain event that both collections are discussing.The update topic, ?Bc focuses on the specific as-pects that are specific of the documents inside theupdate collection.In the generative model,?
For a document d in a collection Ac, wordscan be originated from one of three differ-ent topics: ?G, ?cd and ?Ac , the last one ofwhich captures the main topic described inthe collection pair.?
For a document d in a collection Bc, wordscan be originated from one of four differenttopics: ?G, ?cd, ?Ac and ?Bc .
The last onewill capture the most important updates tothe main topic.To make this representation easier, we can alsostate that both collections are generated from thefour topics, but we constrain the topic probability2161.
Sample ?G ?
Dir(?G)2.
For each collection pair c = (Ac,Bc):?
Sample ?Ac ?
Dir(?A)?
Sample ?Bc ?
Dir(?B)?
For each document d of type ucd ?
{A,B}:- Sample ?cd ?
Dir(?D)- If (ucd = A) sample ?cd ?
Dir(?A)- If (ucd = B) sample ?cd ?
Dir(?B)- For each word w in document d:(a) Sample a topic z ?
Mult(?cd), z ?
{G, cd,Ac,Bc}(b) Sample a word w ?Mult(?z)Figure 1: Generative model in DUALSUM.wz?D?
?B?B?D ?G?G?Bu?A?A?AFigure 2: Graphical model representation of DUAL-SUM.for ?Bc to be always zero when generating a basedocument.We denote ucd ?
{A,B} the type of a docu-ment d in pair c. This is an observed, Booleanvariable stating whether the document d belongsto the base or the update collection inside the pairc.The generation process of documents in DU-ALSUM is described in Figure 1, and the platediagram corresponding to this generative storyis shown in Figure 2.
DUALSUM is an LDA-like model, where topic distributions are multi-nomial distributions over words and topics thatare sampled from Dirichlet distributions.
We use?
= (?G, ?D, ?A, ?B) as symmetric priors for theDirichlet distributions generating the word distri-butions.
In our experiments, we set ?G = 0.1 and?D = ?A = ?B = 0.001.
A greater value is as-signed to ?G in order to reflect the intuition thatthere should be more words in the backgroundthan in the other distributions, so the mass is ex-pected to be shared on a larger number of words.Unlike for the word distributions, mixing prob-abilities are drawn from a Dirichlet distributionwith asymmetric priors.
The prior knowledgeabout the origin of words in the base and up-date collections is again encoded at the level thehyper-parameters.
For example, if we set ?A =(5, 3, 2, 0), this would reflect the intuition that,on average, in the base collections, 50% of thewords originate from the background distribution,30% from the document-specific distribution, and20% from the joint topic.
Similarly, if we set?B = (5, 2, 2, 1), the prior reflects the assumptionthat, on average, in the update collections, 50% ofthe words originate from the background distri-bution, 20% from the document-specific distribu-tion, 20% from the joint topic, and 10% from thenovel, update topic3.
The priors we have actuallyused are reported in Section 4.3.2 Learning and inferenceIn order to find the optimal model parameters, thefollowing equation needs to be computed:p(z, ?, ?|w,u) =p(z, ?, ?,w,u)p(w,u)Omitting hyper-parameters for notational sim-plicity, the joint distribution over the observedvariables is:p(w,u) = p(?G)??cp(?Ac)p(?Bc)??dp(ucd)p(?cd)?
?p(?cd|ucd)d?cd ?
?n?cdnp(wcdn|zcdn)p(zcdn|?cd)where ?
denotes the 4-dimensional simplex4.Since this equation is intractable, we need to per-form approximate inference in order to estimatethe model parameters.
A number of Bayesian sta-tistical inference techniques can be used to ad-dress this problem.3To highlight the difference between asymmetric andsymmetric priors we put the indices in superscript and sub-script respectively.4Remember that, for base documents, words cannotbe generated by the update topic, so ?
denotes the 3-dimensional simplex for base documents.217Variational approaches (Blei et al 2003) andcollapsed Gibbs sampling (Griffiths and Steyvers,2004) are common techniques for approximate in-ference in Bayesian models.
They offer differentadvantages: the variational approach is arguablyfaster computationally, but the Gibbs samplingapproach is in principal more accurate since itasymptotically approaches the correct distribution(Porteous et al 2008).
In this section, we pro-vide details on a collapsed Gibbs sampling strat-egy to infer the model parameters of DUALSUMfor a given dataset.Collapsed Gibbs sampling is a particular caseof Markov Chain Monte Carlo (MCMC) that in-volves repeatedly sampling a topic assignment foreach word in the corpus.
A single iteration of theGibbs sampler is completed after sampling a newtopic for each word based on the previous assign-ment.
In a collapsed Gibbs sampler, the modelparameters are integrated out (or collapsed), al-lowing to only sample z.
Let us call wcdn the n-thword in document d in collection c, and zcdn itstopic assignment.
For Gibbs sampling, we needto calculate p(zcdn|w,u, z?cdn) where z?cdn de-notes the random vector of topic assignments ex-cept the assignment zcdn.p(zcdn = j|w,u, z?cdn, ?A, ?B, ?)
?n(wcdn)?cdn,j + ?j?Vv=1 n(v)?cdn,j + V ?jn(cd)?cdn,j + ?ucdj?k?K(n(cd)?cdn,k + ?ucdk )where K = {G, cd,Ac,Bc}, n(v)?cdn,j denotes thenumber of times word v is assigned to topic jexcluding current assignment of word wcdn andn(cd)?cdn,k denotes the number of words in documentd of collection c that are assigned to topic j ex-cluding current assignment of word wcdn.After each sampling iteration, the model pa-rameters can be estimated using the following for-mulas5.
?kw =n(w)k + ?k?Vv=1 n(v)k + V ?k?cdk =n(cd)k + ?k?n(cd).
+ V ?k5The interested reader is invited to consult (Wang, 2011)for more details on using Gibbs sampling for LDA-like mod-elswhere k ?
K, n(v)k denotes the number of timesword v is assigned to topic k, and n(cd)k denotesthe number of words in document d of collectionc that are assigned to topic k.By the strong law of large numbers, the averageof sample parameters should converge towardsthe true expected value of the model parameter.Therefore, good estimates of the model parame-ters can be obtained averaging over the sampledvalues.
As suggested by Gamerman and Lopes(2006), we have set a lag (20 iterations) betweensamples in order to reduce auto-correlation be-tween samples.
Our sampler also discards the first100 iterations as burn-in period in order to avoidaveraging from samples that are still strongly in-fluenced by the initial assignment.4 Experiments in UpdateSummarizationThe Bayesian graphical model described in theprevious section can be run over a set of newscollections to learn the background distribution,a joint distribution for each collection, an updatedistribution for each collection and the document-specific distributions.
Once this is done, one ofthe learned collections can be used to generate thesummary that best approximates this collection,using the greedy algorithm described by Haghighiand Vanderwende (2009).
Still, there are some pa-rameters that can be defined and which affects theresults obtained:?
DUALSUM?s choice of hyper-parameters af-fects how the topics are learned.?
The documents can be represented with n-grams of different lengths.?
It is possible to generate a summary that ap-proximates the joint distribution, the update-only distribution, or a combination of both.This section describes how these parametershave been tuned.4.1 Parameter tuningWe use the TAC 2008 and 2009 update taskdatasets as training set for tuning the hyper-parameters for the model, namely the pseudo-counts for the two Dirichlet priors that affects thetopic mix assignment for each document.
By per-forming a grid search over a large set of pos-sible hyper-parameters, these have been fixed to218?A = (90, 190, 50, 0) and ?B = (90, 170, 45, 25)as the values that produced the best ROUGE-2score on those two datasets.Regarding the base collection, this can be inter-preted as setting as prior knowledge that roughly27% of the words in the original dataset originatefrom the background distribution, 58% from thedocument-specific distributions, and 15% fromthe topic of the original collection.
We remindthe reader that the last value in ?A is set to zerobecause, due to the problem definition, the origi-nal collection must have no words generated fromthe update topic, which reflects the most recentdevelopments that are still not present in the basecollections A.Regarding the update set, 27% of the words areassumed to originate again from the backgrounddistribution, 51% from the document-specific dis-tributions, 14% from an topic in common withthe original collection, and 8% from the update-specific topic.
One interesting fact to note fromthese settings is that most of the words belong totopics that are specific to single documents (58%and 51% respectively for both sets A and B) andto the background distribution, whereas the jointand update topics generate a much smaller, lim-ited set of words.
This helps these two distribu-tions to be more focused.The other settings mentioned at the beginningof this section have been tuned using the TAC-2010 dataset, which we reserved as our develop-ment set.
Once the different document-specificand collection-specific distributions have been ob-tained, we have to choose the target distribu-tion T to with which the possible summaries willbe compared using the KL metric.
Usually, thehuman-generated update summaries not only in-clude the terms that are very specific about the lastdevelopments, but they also include a little back-ground regarding the developing event.
There-fore, we try, for KLSum, a simple mixture be-tween the joint topic (?A) and the update topic(?B).Figure 3 shows the ROUGE-2 results obtainedas we vary the mixture weight between the joint?A distribution and the update-specific ?B distri-bution.
As can be seen at the left of the curve, us-ing only the update-specific model, which disre-gards the generic words about the topic described,produces much lower results.
The results improveas the relative weight of the joined topic modelFigure 3: Variation in ROUGE-2 score in the TAC-2010 dataset as we change the mixture weight for thejoined topic model between 0 and 1.Figure 4: Effect of the mixture weight in ROUGE-2scores (TAC-2010 dataset).
Results are reported us-ing bigrams (above, blue), unigrams (middle, red) andtrigrams (below, yellow).increases until it plateaus at a maximum aroundroughly the interval [0.6, 0.8], and from that pointperformance slowly degrades as at the right partof the curve the update model is given very littleimportance in generating the summary.
Based onthese results, from this point onwards, the mixtureweight has been set to 0.7.
Note that using onlythe joint distribution (setting the mixture weightto 1.0) also produces reasonable results, hintingthat it successfully incorporates the most impor-tant n-grams from across the base and the updatecollections at the same time.A second parameter is the size of the n-gramsfor representing the documents.
The originalimplementations of SUMBASIC (Nenkova andVanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over sin-219gle words (unigrams).
Still, Haghighi and Van-derwende (2009) report some improvements inthe ROUGE-2 score when representing words asa bag of bigrams, and Darling (2010) mentionsimilar improvements when running SUMBASICwith bigrams.
Figure 4 shows the effect on theROUGE-2 curve when we switch to using uni-grams and trigrams.
As stated in previous work,using bigrams has better results than using uni-grams.
Using trigrams was worse than either ofthem.
This is probably because trigrams are toospecific and the document collections are small,so the models are more likely to suffer from datasparseness.4.2 BaselinesDUALSUM is a modification of TOPICSUM de-signed specifically for the case of update sum-marization, by modifying TOPICSUM?s graphicalmodel in a way that captures the dependency be-tween the joint and the update collections.
Still, itis important to discover whether the new graphi-cal model actually improves over simpler applica-tions of TOPICSUM to this task.
The three base-lines that we have considered are:?
Running TOPICSUM on the set of collectionscontaining only the update documents.
Wecall this run TOPICSUMB.?
Running TOPICSUM on the set of collectionscontaining both the base and the update doc-uments.
Contrary to the previous run, thetopic model for each collection in this runwill contain information relevant to the baseevents.
We call this run TOPICSUMA?B.?
Running TOPICSUM twice, once on the setof collections containing the update docu-ments, and the second time on the set ofcollections containing the base documents.Then, for each collection, the obtained baseand update models are combined in a mix-ture model using a mixture weight betweenzero and one.
The weight has been tuned us-ing TAC-2010 as development set.
We callthis run TOPICSUMA+TOPICSUMB.4.3 Automatic evaluationDUALSUM and the three baselines6 have been6Using the settings obtained in the previous section, hav-ing been optimized on the datasets from previous TAC com-petitions.automatically evaluated using the TAC-2011dataset.
Table 1 shows the ROUGE results ob-tained.
Because of the non-deterministic natureof Gibbs sampling, the results reported here arethe average of five runs for all the baselines andfor DUALSUM.
DUALSUM outperforms two ofthe baselines in all three ROUGE metrics, and italso outperforms TOPICSUMB on two of the threemetrics.The top three systems in TAC-2011 have beenincluded for comparison.
The results betweenthese three systems, and between them and DU-ALSUM, are all indistinguishable at 95% confi-dence.
Note that the best baseline, TOPICSUMB,is quite competitive, with results that are indis-tinguishable to the top participants in this year?sevaluation.
Note as well that, because we havefive different runs for our algorithms, whereaswe just have one output for the TAC participants,the confidence intervals in the second case wereslightly bigger when checking for statistical sig-nificance, so it is slightly harder for these systemsto assert that they outperform the baselines with95% confidence.
These results would have madeDUALSUM the second best system for ROUGE-1 and ROUGE-SU4, and the third best system interms of ROUGE-2.The supplementary materials contain a detailedexample of the the topic model obtained for thebackground in the TAC-2011 dataset, and the baseand update models for collection D1110.
Asexpected, the top unigrams and bigrams are allclosed-class words and auxiliary verbs.
Becausetrigrams are longer, background trigrams actu-ally include some content words (e.g.
universityor director).
Regarding the models for ?A and?B, the base distribution contains words relatedto the original event of an earthquake in Sichuanprovince (China), and the update distribution fo-cuses more on the official (updated) death tollnumbers.
It can be noted here that the tokenizerwe used is very simple (splitting tokens separatedwith white-spaces or punctuation) so that num-bers such as 7.9 (the magnitude of the earthquake)and 12,000 or 14,000 are divided into two tokens.We thought this might be a for the bigram-basedsystem to produce better results, but we ran thesummarizers with a numbers-aware tokenizer andthe statistical differences between versions stillhold.220Method R-1 R-2 R-SU4TOPICSUMB 0.3442 0.0868 0.1194TOPICSUMA?B 0.3385 0.0809 0.1159TOPICSUMA+TOPICSUMB 0.3328 0.0770 0.1125DUALSUM 0.3575???
0.0924??
0.1285??
?TAC-2011 best system (Peer 43) 0.3559??
0.0958??
0.1308??
?TAC-2011 2nd system (Peer 25) 0.3582??
0.0926?
0.1276?
?TAC-2011 3rd system (Peer 17) 0.3558??
0.0886 0.1279?
?Table 1: Results on the TAC-2011 dataset.
?, ?
and ?
indicate that a result is significantly better than TOPICSUMB,TOPICSUMA?B and TOPICSUMA+TOPICSUMB, respectively (p < 0.05).4.4 Manual evaluationWhile the ROUGE metrics provides an arguableestimate of the informativeness of a generatedsummary, it does not account for other importantaspects such as the readability or the overall re-sponsiveness.
To evaluate such aspects, a manualevaluation is required.
A fairly standard approachfor manual evaluation is through pairwise com-parison (Haghighi and Vanderwende, 2009; Ce-likyilmaz and Hakkani-Tur, 2011).In this approach, raters are presented with pairsof summaries generated by two systems and theyare asked to say which one is best with respectto some aspects.
We followed a similar approachto compare DualSum with Peer 43 - the best sys-tem with respect to ROUGE-2, on the TAC 2011dataset.
For each collection, raters were presentedwith three summaries: a reference summary ran-domly chosen from the model summaries, and thesummaries generated by Peer 43 and DualSum.They were asked to read the summaries and saywhich one of the two generated summaries is bestwith respect to: 1) Overall responsiveness: whichsummary is best overall (both in terms of contentand fluency), 2) Focus: which summary containsless irrelevant details, 3) Coherence: which sum-mary is more coherent and 4) Non-redundancy:which summary repeats less the same informa-tion.
For each aspect, the rater could also replythat both summary were of the same quality.For each of the 44 collections in TAC-2011, 3ratings were collected from raters7.
Results arereported in Table 2.
DualSum outperforms Peer43 in three aspects, including Overall Responsive-ness, which aggregates all the other scores andcan be considered the most important one.
Re-7In total 132 raters participated to the task via our owncrowdsourcing platform, not mentioned yet for blind review.Best systemAspect Peer 43 Same DualSumOverall Responsiveness 39 25 68Focus 41 22 69Coherence 39 30 63Non-redundancy 40 53 39Table 2: Results of the side-by-side manual evaluation.garding Non-redundancy, DualSum and Peer 43obtain similar results but the majority of ratersfound no difference between the two systems.Fleiss ?
has been used to measure the inter-rateragreement.
For each aspect, we observe ?
?
0.2which corresponds to a slight agreement; but if wefocus on tasks where the 3 ratings reflect a prefer-ence for either of the two systems, then ?
?
0.5,which indicates moderate agreement.4.5 Efficiency and applicabilityThe running time for summarizing the TAC col-lections with DualSum, averaged over a hundredruns, is 4.97 minutes, using one core (2.3 GHz).Memory consumption was 143 MB.It is important to note as well that, while TOP-ICSUM incorporates an additional layer to modeltopic distributions at the sentence level, we notedearly in our experiments that this did not improvethe performance (as evaluated with ROUGE) andconsequently relaxed that assumption in Dual-Sum.
This resulted in a simplification of themodel and a reduction of the sampling time.While five minutes is fast enough to be ableto experiment and tune parameters with the TACcollections, it would be quite slow for a real-time summarization system able to generate sum-maries on request.
As can be seen from the platediagram in Figure 2, all the collections are gen-erated independently from each other.
The onlyexception, for which it is necessary to have all221the collections available at the same time dur-ing Gibbs sampling, is the background distribu-tion, which is estimated from all the collectionssimultaneously, roughly representing 27% of thewords, that should appear distributed across alldocuments.The good news is that this background distri-bution will contain closed-class words in the lan-guage, which are domain-independent (see sup-plementary material for examples).
Therefore,we can generate this distribution from one ofthe TAC datasets only once, and then it can bereused.
Fixing the background distribution to apre-computed value requires a very simple mod-ification of the Gibbs sampling implementation,which just needs to adjust at each iteration thecollection and document-specific models, and thetopic assignment for the words.Using this modified implementation, it is nowpossible to summarize a single collection inde-pendently.
The summarization of a single col-lection of the size of the TAC collections is re-duced on average to only three seconds on thesame hardware settings, allowing the use of thissummarizer in an on-line application.5 ConclusionsThe main contribution of this paper is DUALSUM,a new topic model that is specifically designed toidentify and extract novelty from pairs of collec-tions.It is inspired by TOPICSUM (Haghighi andVanderwende, 2009), with two main changes:Firstly, while TOPICSUM can only learn the maintopic of a collection, DUALSUM focuses on thedifferences between two collections.
Secondly,while TOPICSUM incorporates an additional layerto model topic distributions at the sentence level,we have found that relaxing this assumption andmodeling the topic distribution at document leveldoes not decrease the ROUGE scores and reducesthe sampling time.The generated summaries, tested on the TAC-2011 collection, would have resulted on the sec-ond and third position in the last summarizationcompetition according to the different ROUGEscores.
This would make DUALSUM statisticallyindistinguishable from the top system with 0.95confidence.We also propose and evaluate the applicabilityof an alternative implementation of Gibbs sam-pling to on-line settings.
By fixing the back-ground distribution we are able to summarize adistribution in only three seconds, which seemsreasonable for some on-line applications.As future work, we plan to explore the use ofDUALSUM to generate more general contrastivesummaries, by identifying differences betweencollections whose differences are not of temporalnature.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement number 257790.
We would alsolike to thank Yasemin Altun and the anonymousreviewers for their useful comments on the draftof this paper.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.Florian Boudin, Marc El-Be`ze, and Juan-ManuelTorres-Moreno.
2008.
A scalable MMR approachto sentence scoring for multi-document update sum-marization.
In Coling 2008: Companion volume:Posters, pages 23?26, Manchester, UK, August.Coling 2008 Organizing Committee.J.
Carbonell and J. Goldstein.
1998.
The use of mmr,diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of the21st annual international ACM SIGIR conferenceon Research and development in information re-trieval, pages 335?336.
ACM.Asli Celikyilmaz and Dilek Hakkani-Tur.
2011.
Dis-covery of topically coherent sentences for extrac-tive summarization.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages491?499, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specific as-pects of documents with a probabilistic topic model.In NIPS, pages 241?248.W.M.
Darling.
2010.
Multi-document summarizationfrom first principles.
In Proceedings of the thirdText Analysis Conference, TAC-2010.
NIST.Hal Daume?, III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings ofthe 21st International Conference on Computa-tional Linguistics and the 44th annual meeting222of the Association for Computational Linguistics,ACL-2006, pages 305?312, Stroudsburg, PA, USA.Association for Computational Linguistics.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res., 22:457?479, De-cember.S.
Fisher and B. Roark.
2008.
Query-focused super-vised sentence ranking for update summaries.
InProceedings of the first Text Analysis Conference,TAC-2008.Dani Gamerman and Hedibert F. Lopes.
2006.Markov Chain Monte Carlo: Stochastic Simulationfor Bayesian Inference.
Chapman and Hall/CRC.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell.
1999.
Summarizing text docu-ments: sentence selection and evaluation metrics.In Proceedings of the 22nd annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, SIGIR ?99, pages121?128, New York, NY, USA.
ACM.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academyof Sciences, 101(Suppl.
1):5228?5235, April.A.
Haghighi and L. Vanderwende.
2009.
Exploringcontent models for multi-document summarization.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 362?370.
Association for Com-putational Linguistics.Feng Jin, Minlie Huang, and Xiaoyan Zhu.
2010.
Thethu summarization systems at tac 2010.
In Proceed-ings of the third Text Analysis Conference, TAC-2010.Kevin Lerman and Ryan McDonald.
2009.
Con-trastive summarization: an experiment with con-sumer reviews.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Short Papers, NAACL-Short ?09, pages 113?116,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Xuan Li, Liang Du, and Yi-Dong Shen.
2011.
Graph-based marginal ranking for update summarization.In Proceedings of the Eleventh SIAM InternationalConference on Data Mining.
SIAM / Omnipress.Rebecca Mason and Eugene Charniak.
2011.
Ex-tractive multi-document summaries should explic-itly not contain document-specific content.
In Pro-ceedings of the Workshop on Automatic Summariza-tion for Different Genres, Media, and Languages,WASDGML ?11, pages 49?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.A.
Nenkova and L. Vanderwende.
2005.
The im-pact of frequency on summarization.
Microsoft Re-search, Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.Ian Porteous, David Newman, Alexander Ihler, ArthurAsuncion, Padhraic Smyth, and Max Welling.2008.
Fast collapsed Gibbs sampling for latentDirichlet alcation.
In KDD ?08: Proceeding ofthe 14th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 569?577, New York, NY, USA, August.
ACM.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys?,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Inf.
Process.
Manage.,40:919?938, November.Frank Schilder, Ravikumar Kondadadi, Jochen L. Lei-dner, and Jack G. Conrad.
2008.
Thomson reutersat tac 2008: Aggressive filtering with fastsum forupdate and opinion summarization.
In Proceedingsof the first Text Analysis Conference, TAC-2008.Chao Shen and Tao Li.
2010.
Multi-document sum-marization via the minimum dominating set.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages984?992, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ian Soboroff and Donna Harman.
2005.
Novelty de-tection: the trec experience.
In Proceedings of theconference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, HLT ?05, pages 105?112, Stroudsburg, PA,USA.
Association for Computational Linguistics.Dingding Wang, Shenghuo Zhu, Tao Li, and YihongGong.
2009.
Multi-document summarization us-ing sentence-based topic models.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,ACLShort ?09, pages 297?300, Stroudsburg, PA,USA.
Association for Computational Linguistics.Yi Wang.
2011.
Distributed gibbs sampling of latentdirichlet alcation: The gritty details.Li Wenjie, Wei Furu, Lu Qin, and He Yanxiang.
2008.Pnr2: ranking sentences with positive and nega-tive reinforcement for query-oriented update sum-marization.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics -Volume 1, COLING ?08, pages 489?496, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.223
