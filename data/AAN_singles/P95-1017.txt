Evaluating Automated and Manual Acquisition ofAnaphora Resolution StrategiesChinatsu  Aone  and Scot t  Wi l l i am Bennet tSystems Research and Appl icat ions Corporat ion  (SRA)2000 15th Street Nor thArl ington, VA 22201aonec~sra.corn,  bennet t~sra .comAbst rac tWe describe one approach to build an au-tomatically trainable anaphora resolutionsystem.
In this approach, we use Japanesenewspaper articles tagged with discourseinformation as training examples for a ma-chine learning algorithm which employsthe C4.5 decision tree algorithm by Quin-lan (Quinlan, 1993).
Then, we evaluateand compare the results of several variantsof the machine learning-based approachwith those of our existing anaphora resolu-tion system which uses manually-designedknowledge sources.
Finally, we compareour algorithms with existing theories ofanaphora, in particular, Japanese zero pro-nouns.1 In t roduct ionAnaphora resolution is an important but still diffi-cult problem for various large-scale natural anguageprocessing (NLP) applications, uch as informationextraction and machine tr~slation.
Thus far, notheories of anaphora have been tested on an empir-ical basis, and therefore there is no answer to the"best" anaphora resolution algorithm.
I Moreover,an anaphora resolution system within an NLP sys-tem for real applications must handle:?
degraded or missing input (no NLP systemhas complete lexicons, grammars, or semanticknowledge and outputs perfect results), and?
different anaphoric phenomena in different do-mains, languages, and applications.Thus, even if there exists a perfect heory, it mightnot work well with noisy input, or it would not coverall the anaphoric phenomena.1Walker (Walker, 1989) compares Brennan, Friedmana~ad Pollard's centering approach (Brennan et al, 1987)with Hobbs' algorithm (Hohbs, 1976) on a theoreticalbasis.These requirements have motivated us to de-velop robust, extensible, and trainable anaphoraresolution systems.
Previously (Aone and Mc-Kee, 1993), we reported our data-driven multilin-gual anaphora resolution system, which is robust,exteusible, and manually trainable.
It uses dis-course knowledge sources (KS's) which are manu-ally selected and ordered.
(Henceforth, we call thesystem the Manually-Designed Resolver, or MDR.
)We wanted to develop, however, truly automaticallytrainable systems, hoping to improve resolution per-formance and reduce the overhead of manually con-structing and arranging such discourse data.In this paper, we first describe one approachwe are taking to build an automatically trainableanaphora resolution system.
In this approach, wetag corpora with discourse information, and usethem as training examples for a machine learningalgorithm.
(Henceforth, we call the system the Ma-chine Learning-based Resolver, or MLR.)
Specifi-cally, we have tagged Japanese newspaper articlesabout joint ventures and used the C4.5 decision treealgorithm by Quinlan (Quinlan, 1993).
Then, weevaluate and compare the results of the MLR withthose produced by the MDR.
Finally, we compareour algorithms with existing theories of anaphora,in particular, Japanese zero pronouns.2 App ly ing  a Mach ine  Learn ingTechn ique  to  Anaphora  Reso lu t ionIn this section, we first discuss corpora which wecreated for training and testing.
Then, we describethe learning approach chosen, and discuss trainingfeatures and training methods that we employed forour current experiments.2.1 Training and Test CorporaIn order to both train and evaluate an anaphoraresolution system, we have been developing cor-pora which are tagged with discourse information.The tagging has been done using a GUI-based toolcalled the Discourse Tagging Tool (DTTool) ac-cording to "The Discourse Tagging Guidelines" we122have developed.
2 The tool allows a user to link ananaphor with its antecedent and specify the typeof the anaphor (e.g.
pronouns, definite NP's, etc.
).The tagged result can be written out to an SGML-marked file, as shown in Figure 1.For our experiments, we have used a discourse-tagged corpus which consists of Japanese newspaperarticles about joint ventures.
The tool lets a user de-fine types of anaphora s necessary.
The anaphorictypes used to tag this corpus are shown in Table 1.NAME anaphora re tagged when proper namesare used anaphorically.
For example, in Figure 1,"Yamaichi (ID=3)" and "Sony-Prudential (ID=5)"referring back to "Yamaichi Shouken (ID=4)" (Ya-maichi Securities) and "Sony-Prudential Seimeiho-ken (ID=6)" (Sony-Prudential Life Insurance) re-spectively are NAME anaphora.
NAME anaphorain Japanese are different from those in English inthat any combination of characters in an antecedentcan be NAME anaphora s long as the character or-der is preserved (e.g.
"abe" can be an anaphor of"abcde").Japanese definite NPs (i.e.
DNP anaphora) arethose prefixed by "dou" (literally meaning "thesame"), "ryou" (literally meaning "the two"), anddeictic determiners like "kono"(this) and "sono"(that).
For example, "dou-sha" is equivalent to "thecompany", and "ryou-koku" to "the two countries".The DNP anaphora with "dou" and "ryou" pre-fixes are characteristic of written, but not spoken,Japanese texts.Unlike English, Japanese has so-called zero pro-nouns, which are not explicit in the text.
In thesecases, the DTTool lets the user insert a "Z" markerjust before the main predicate of the zero pronoun toindicate the existence of the anaphor.
We made dis-tinction between QZPRO and ZPRO when taggingzero pronouns.
QZPRO ("quasi-zero pronoun") ischosen when a sentence has multiple clauses (sub-ordinate or coordinate), and the zero pronouns inthese clauses refer back to the subject of the initialclause in the same sentence, as shown in Figure 2.The anaphoric types are sub-divided according tomore semantic riteria such as organizations, people,locations, etc.
This is because the current appli-cation of our multilingual NLP system is informa-tion extraction (Aone et al, 1993), i.e.
extractingfrom texts information about which organizationsare forming joint ventures with whom.
Thus, resolv-ing certain anaphora (e.g.
various ways to refer backto organizations) affects the task performance morethan others, as we previously reported (Aone, 1994).Our goal is to customize and evaluate anaphora res-olution systems according to the types of anaphorawhen necessary.2Our work on the DTTool and tagged corpora wasreported in a recent paper (Aone and Bennett, 1994).2.2 Learning MethodWhile several inductive learning approaches couldhave been taken for construction of the trainableanaphoric resolution system, we found it useful tobe able to observe the resulting classifier in the formof a decision tree.
The tree and the features usedcould most easily be compared to existing theories.Therefore, our initial approach as been to employQuinlan's C4.5 algorithm at the heart of our clas-sification approach.
We discuss the features usedfor learning below and go on to discuss the trainingmethods and how the resulting tree is used in ouranaphora resolution algorithm.2.3 Training FeaturesIn our current machine learning experiments, wehave taken an approach where we train a decisiontree by feeding feature vectors for pairs of an anaphorand its possible antecedent.
Currently we use 66features, and they include lezical (e.g.
category),syntactic (e.g.
grammatical role), semantic (e.g.
se-mantic class), and positional (e.g.
distance betweenanaphor and antecedent) features.
Those featurescan be either unary features (i.e.
features of either ananaphor or an antecedent such as syntactic numbervalues) or binary features (i.e.
features concerningrelations between the pairs such as the positional re-lation between an anaphor and an antecedent.)
Westarted with the features used by the MDR, gener-alized them, and added new features.
The featuresthat we employed are common across domains andlanguages though the feature values may change indifferent domains or languages.
Example of trainingfeatures are shown in Table 2.The feature values are obtained automatically bprocessing a set of texts with our NLP system, whichperforms lexical, syntactic and semantic analysis andthen creates discourse markers (Kamp, 1981) foreach NP and S. 3 Since discourse markers tore theoutput of lexical, syntactic and semantic process-ing, the feature vectors are automatically calculatedfrom them.
Because the system output is not alwaysperfect (especially given the complex newspaper ar-ticles), however, there is some noise in feature values.2.4 Training MethodsWe have employed ifferent raining methods usingthree parameters: anaphoric hains, anaphoric typeidentification, and confidence factors.The anaphoric chain parameter is used in selectingtraining examples.
When this parameter is on, weselect a set of positive training examples and a setof negative training examples for each anaphor in atext in the following way:3 Existence of zero pronouns in sentences i detectedby the syntax module, and discourse maxkers are createdfor them.123<CORe: m='I"><COREF n~'4">ttl--lEff-</mR~:<u.~J- m='s'>y-'-- ?
~')l,~Y:,,~,)t,?.
@~l~ (~P,-'ll~l~:.~t, :?4t.
lr)~)<CORE\]: m='O" rcPE='~ RB:='i"></COR~>III@b~.
~)q~'~6<COR~ ZD='2e rVPE='ZPm-t~-" REFf'I"></COREF>~Ii3"~.
<CORe: ZD='~' WRf"NANE--OR6" RB:f'4">ttI--</COE~<COREF ~"8">q~, l ,~ l tC)~e ' t - "~.
'3 t t~t t l l~ :~ '~ '&</COR~<COR~ m='s" WR='tt~E-O~ REFf"#'>y-'---.
~')t,-~>-b,,v)l,</mR~{:~-, <COmF n)="?'
WPE='Dm" REF='8">C r~ 5, ~-7" I, <,'CUT~  <CORBF m='9" WR='ZT4~O-O~ 8EEf'5"> </OR~ f f  -~ T <CO~ m=" ~o" TYR='~O-U~ RE~='5">Figure 1: Text Tagged with Discourse Information using SGMLTagsDNPDNP-FDNP-LDNP-ORGDNP-PDNP-TDNP-BOTHDNP-BOTH-ORGDNP-BOTH-LDNP-BOTH-PREFLEXIVENAMENAME-FNAME-LNAME-ORGNAME-PDPROLOCITIMEIQZPROQZPRO-ORGQZPRO-PZPROZPRO-IMPZPRO-ORGZPRO-PTable 1: Summary of Anaphoric TypesMeaningDefinite NPDefinite NPDefinite NPDefinite NPDefinite NPDefinite NPwhose referent is a facilitywhose referent is a locationwhose referent is an organizationwhose referent is a personwhose referent is timeDefinite NP whose referent is two entitiesDefinite NP whose referent is two organization entitiesDefinite NP whose referent is two location entitiesDefinite NP whose referent is two person entitiesReflexive expressions (e.$.
"jisha ~)Proper nameProper name for facilityProper name for locationProper name for organizationProper name for personDeictic pronoun (this, these)Locational indexical (here, there)Time indexical (now, then, later)Quasi-zero pronounQuasi-zero pronoun whose referent is an organizationQuasi-zero pronoun whose referent is a personZero pronounZero pronoun in an impersonal constructionZero pronoun whose referent is an organizationZero pronoun whose referent is a personJDEL Dou-ellipsisSONY-wa RCA-to teikeishi, VCR-wo QZPROSony-subj RCA-with joint venture VCR-obj (it)kaihatsusuru to QZPRO happyoushitadevelop that (it) announced"(SONY) announced that SONY will form a joint venture with RCAand (it) will develop VCR's.
"Figure 2: QZPRO ExampleTable 2: Examples of Training FeaturesUnary feature Binaxy featureLexical category matching-categorySyntactic topicalized matching-topicalizedSemantic semantic-class subsuming-semantic-classPositional antecedent-precedes-anaphor124Positive training examples are those anaphor-antecedent pairs whose anaphor is directly linked toits antecedent in the tagged corpus and also whoseanaphor is paired with one of the antecedents ontheanaphoric chain, i.e.
the transitive closure betweenthe anaphor and the first mention of the antecedent.For example, if B refers to A and C refers to B, C-A is a positive training example as well as B-A andC-B.Negative training examples are chosen by pairingan anaphor with all the possible antecedents in a textexcept for those on the transitive closure describedabove.
Thus, if there are possible antecedents in thetext which are not in the C-B-A transitive closure,say D, C-D and B-D are negative training examples.When the anaphoric hain parameter is off, onlythose anaphor-antecedent pairs whose anaphora redirectly linked to their antecedents in the corpus areconsidered as positive xamples.
Because of the wayin which the corpus was tagged (according to ourtagging uidelines), an anaphor is linked to the mostrecent antecedent, except for a zero pronoun, whichis linked to its most recent overt antecedent.
In otherwords, a zero pronoun is never linked to another zeropronoun.The anaphoric type identification parameter isutilized in training decision trees.
With this param-eter on, a decision tree is trained to answer "no"when a pair of an anaphor and a possible antecedentare not co-referential, or answer the anaphoric typewhen they are co-referential.
If the parameter is off,a binary decision tree is trained to answer just "yes"or "no" and does not have to answer the types ofanaphora.The confidence factor parameter (0-100) is usedin pruning decision trees.
With a higher confidencefactor, less pruning of the tree is performed, and thusit tends to overfit the training examples.
With alower confidence factor, more pruning is performed,resulting in a smaller, more generalized tree.
Weused confidence factors of 25, 50, 75 and 100%.The anaphoric hain parameter described abovewas employed because an anaphor may have morethan one "correct" antecedent, in which case thereis no absolute answer as to whether one antecedentis better than the others.
The decision tree approachwe have taken may thus predict more than one an-tecedent to pair with a given anaphor.
Currently,confidence values returned from the decision tree areemployed when it is desired that a single antecedentbe selected for a given anaphor.
We are experiment-ing with techniques to break ties in confidence valuesfrom the tree.
One approach is to use a particularbias, say, in preferring the antecedent closest o theanaphor among those with the highest confidence (asin the results reported here).
Although use of theconfidence values from the tree works well in prac-tice, these values were only intended as a heuristicfor pruning in Quinlan's C4.5.
We have plans to usecross-validation across the training set as a methodof determining error-rates by which to prefer onepredicted antecedent over another.Another approach is to use a hybrid method wherea preference-trained decision tree is brought in tosupplement the decision process.
Preference-trainedtrees, like that discussed in Connolly et al (Connollyet al, 1994), are trained by presenting the learn-ing algorithm with examples of when one anaphor-antecedent pair should be preferred over another.Despite the fact that such trees are learning prefer-ences, they may not produce sufficient preferences topermit selection of a single best anaphor-antecedentcombination (see the "Related Work" section be-low).3 TestingIn this section, we first discuss how we configuredand developed the MLRs and the MDR for testing.Next, we describe the scoring methods used, andthen the testing results of the MLRs and the MDR.In this paper, we report the results of the four typesof anaphora, namely NAME-ORG, QZPRO-ORG,DNP-ORG, and ZPRO-ORG, since they are the ma-jority of the anaphora ppearing in the texts andmost important for the current domain (i.e.
jointventures) and application (i.e.
information extrac-tion).3.1 Testing the MLRaTo build MLRs, we first trained decision trees with1971 anaphora 4 (of which 929 were NAME-ORG;546 QZPRO-ORG; 87 DNP-ORG; 282 ZPRO-ORG)in 295 training texts.
The six MLRs using decisiontrees with different parameter combinations are de-scribed in Table 3.Then, we trained decision trees in the MLR-2configuration with varied numbers of training texts,namely 50, 100, 150,200 and 250 texts.
This is doneto find out the minimum number of training texts toachieve the optimal performance.3.2 Testing the MDRThe same training texts used by the MLRs servedas development data for the MDR.
Because the NLPsystem is used for extracting information about jointventures, the MDR was configured to handle onlythe crucial subset of anaphoric types for this ex-periment, namely all the name anaphora nd zeropronouns and the definite NPs referring to organi-zations (i.e.
DNP-ORG).
The MDR applies differentsets of generators, filters and orderers to resolve dif-ferent anaphoric types (Aone and McKee, 1993).
Agenerator generates a set of possible antecedent hy-potheses for each anaphor, while a filter eliminates*In both training and testing, we did not in-clude anaphora which refer to multiple discontinuousantecedents.125MLR-1MLR-2MLR-3MLR-4MLR-5MLR-6Table 3: Six Configurations ofMLRsyes noyes noyes noyes noyes yesno noconfidence factorlOO%75% '50% "25%75%75%unlikely hypotheses from the set.
An orderer ankshypotheses in a preference order if there is more thanone hypothesis left in the set after applying all theapplicable filters.
Table 4 shows KS's employed forthe four anaphoric types.3.3 Scoring MethodWe used recall and precision metrics, as shown inTable 5, to evaluate the performance of anaphoraresolution.
It is important o use both measuresbecause one can build a high recall-low precisionsystem or a low recall-high precision system, neitherof which may be appropriate in certain situations.The NLP system sometimes fails to create discoursemarkers exactly corresponding to anaphora in textsdue to failures of hxical or syntactic processing.
Inorder to evaluate the performance of the anaphoraresolution systems themselves, we only consideredanaphora whose discourse markers were identified bythe NLP system in our evaluation.
Thus, the systemperformance evaluated against all the anaphora intexts could be different.Table 5: Recall and Precision Metrics for EvaluationRecall = Nc/I, Precision = Nc/NnI Number of system-identified anaphora in inputN~ Number of correct resolutionsNh Number of resolutions attempted3.4 Testing ResultsThe testing was done using 1359 anaphora (of which1271 were one of the four anaphoric types) in 200blind test texts for both the MLRs and the MDR.
Itshould be noted that both the training and testingtexts are newspaper articles about joint ventures,and that each article always talks about more thanone organization.
Thus, finding antecedents of orga-nizational anaphora is not straightforward.
Table 6shows the results of six different MLRs and the MDRfor the four types of anaphora, while Table 7 showsthe results of the MLR-2 with different sizes of train-ing examples,4 Evaluat ion4.1 The MLRs  vs. the MDRUsing F-measures 5 as an indicator for overall perfor-mance, the MLRs with the chain parameters turnedon and type identification turned off (i.e.
MLR-1, 2,3, and 4) performed the best.
MLR-1, 2, 3, 4, and 5all exceeded the MDR in overall performance basedon F-measure.Both the MLRs and the MDR used the char-acter subsequence, the proper noun category, andthe semantic lass feature values for NAME-ORGanaphora (in MLR-5, using anaphoric type identifi-cation).
It is interesting to see that the MLR addi-tionally uses the topicalization feature before testingthe semantic class feature.
This indicates that, infor-mation theoretically, if the topicalization feature ispresent, the semantic lass feature is not needed forthe classification.
The performance of NAME-ORGis better than other anaphoric phenomena becausethe character subsequence f ature has very high an-tecedent predictive power.4.1.1 Evaluation of the MLItsChanging the three parameters in the MLRscaused changes in anaphora resolution performance.As Table 6 shows, using anaphoric hains withoutanaphoric type identification helped improve theMLRs.
Our experiments with the confidence fac-tor parameter indicates the trade off between recalland precision.
With 100% confidence factor, whichmeans no pruning of the tree, the tree overfits theexamples, and leads to spurious uses of features uchas the number of sentences between an anaphor andan antecedent ear the leaves of the generated tree.This causes the system to attempt more anaphorresolutions albeit with lower precision.
Conversely,too much pruning can also yield poorer results.MLR-5 illustrates that when anaphoric type iden-tification is turned on the MLR's performance dropsSF-measure is calculated by:F= (~2+1.0) ?P x R#2 x P+Rwhere P is precision, R is recall, and /3 is the relativeimportance given to recall over precision.
In this case,= 1.0.126NAME-ORGDNP-ORGQZPRO-ORGZPRO-ORGGeneratorsTable 4: KS's  used by the MDRFilterscurrent-textcurrent-textcurrent-paragraphcurrent-paragraphsyntactic-category-propnnam~chax-subsequencesemantic-class-orgsemantic-dass-orgsemantic-amount-singularnot-in-the-same-dcsemantic-dass-from-prednot-in-the-same-dcsere antic-dass-from-predOrderersreverse-recencytopica\]izationsubject-nprecencytopica\]izationsubject-npcategory-nprecencytopicalizationsubject-npcategory-nprecency# exmplsMLR-1MLR-2MLR-3MLR-4MLR-5MLR-6MDRTable 6: Recal l  and Precision of the MLRs and the MDRNAME-ORG631R P84.79 92.2484.79 93.0483.20 94.0983.84 94.3085.74 92.8068.30 91.7076.39 90.09DNP-ORG54R P44.44 50.0044.44 52.1737.04 58.8238.89 60.0044.44 55.8129.63 64.0035.19 50.00383R P65.62 80.2564.84 84.6963.02 84.9164.06 85.1256.51 89.6754.17 90.8367.19 67.19ZPRO-ORG203R P4O.78 64.6239.32 73.6435.92 73.2737.86 76.4715.53 78.0513.11 75.0043.20 43.20Average1271R P70.20 83.4969.73 86.7367.53 88.0468.55 88.5563.84 89.5553.49 89.7466.51 72.91F-measure1271F76.2777.3076.4377.2874.5467.0369.57texts50I001502OO25O295MDRTable 7 :MLR-2  Conf igurat ion with Varied Train ing Data  SizesNAME-ORG DNP-ORG QZPRO-ORG ZPRO-ORGR P R P R81.30 91.94 35.19 48.72 59.3882.09 92.01 38.89 53.85 63.0282.57 91.89 48.15 60.47 55.7383.99 91.70 46.30 60.98 63.0284.79 93.21 44.44 53.33 65.1084.79 93.04 44.44 52.17 64.8476.39 90.09 35.19 50.00 67.19Average F-measureP R P R P F76.77 29.13 56.07 64.31 81.92 72.0685.82 28.64 62.77 65.88 85.89 74.5785.60 20.39 70.00 62.98 87.28 73.1782.88 36.41 65.22 68.39 84.99 75.7983.89 40.78 73.04 70.04 86.53 77.4284.69 39.32 73.64 69.73 86.73 77.3067.19 43.20 43.20 66.51 72.91 69.57127but still exceeds that of the MDR.
MLR-6 shows theeffect of not training on anaphoric hains.
It resultsin poorer performance than the MLR-1, 2, 3, 4, and5 configurations and the MDR.One of the advantages of the MLRs is that dueto the number of different anaphoric types presentin the training data, they also learned classifiersfor several additional anaphoric types beyond whatthe MDR could handle.
While additional codingwould have been required for each of these typesin the MDR, the MLRs picked them up without ad-ditional work.
The additional anaphoric types in-cluded DPRO, REFLEXIVE, and TIMEI (cf.
Ta-ble 1).
Another advantage is that, unlike the MDR,whose features are hand picked, the MLRs automat-ically select and use necessary features.We suspect that the poorer performance of ZPRO-OR(; and DNP-ORG may be due to the followingdeficiency of the current MLR algorithms: Becauseanaphora resolution is performed in a "batch mode"for the MLRs, there is currently no way to perco-late the information on an anaphor-antecedent linkfound by a system after each resolution.
For exam-ple, if a zero pronoun (Z-2) refers to another zeropronoun (Z-l), which in turn refers to an overt NP,knowing which is the antecedent of Z-1 may be im-portant for Z-2 to resolve its antecedent correctly.However, such information is not available to theMLRs when resolving Z-2.4.1.2 Evaluation of  the MDROne advantage ofthe MDR is that a tagged train-ing corpus is not required for hand-coding the reso-lution algorithms.
Of course, such a tagged corpusis necessary to evaluate system performance quan-titatively and is also useful to consult with duringalgorithm construction.However, the MLR results seem to indicate thelimitation of the MDR in the way it uses ordererKS's.
Currently, the MDR uses an ordered list ofmultiple orderer KS's for each anaphoric type (cf.Table 4), where the first applicable orderer KS in thelist is used to pick the best antecedent when there ismore than one possibility.
Such selection ignores thefact that even anaphora of the same type may usedifferent orderers (i.e.
have different preferences), de-pending on the types of possible antecedents and onthe context in which the particular anaphor was usedin the text.4.2 Training Data  Size vs.
Per formanceTable 7 indicates that with even 50 training texts,the MLR achieves better performance than theMDR.
Performance seems to reach a plateau atabout 250 training examples with a F-measure ofaround 77.4.5 Re la ted  WorkAnaphora resolution systems for English texts basedon various machine learning algorithms, including adecision tree algorithm, are reported in Connolly etal.
(Connolly et al, 1994).
Our approach is differentfrom theirs in that their decision tree identifies whichof the two possible antecedents for a given anaphoris "better".
The assumption seems to be that theclosest antecedent is the "correct" antecedent.
How-ever, they note a problem with their decision tree inthat it is not guaranteed to return consistent clas-sifications given that the "preference" relationshipbetween two possible antecedents is not transitive.Soderland and Lehnert's machine learning-basedinformation extraction system (Soderland and Lehn-ert, 1994) is used specifically for filling particulartemplates from text input.
Although a part of itstask is to merge multiple referents when they corefer(i.e.
anaphora resolution), it is hard to evaluate howtheir anaphora resolution capability compares withours, since it is not a separate module.
The onlyevaluation result provided is their extraction result.Our anaphora resolution system is modular, and canbe used for other NLP-based applications such asmachine translation.
Soderland and Lehnert's ap-proach relies on a large set of filled templates used fortraining.
Domain-specific features from those tem-plates are employed for the learning.
Consequently,the learned classifiers are very domain-specific, andthus the approach relies on the availability of newfilled template sets for porting to other domains.While some such template sets exist, such as thoseassembled for the Message Understanding Confer-ences, collecting such large amounts of training datafor each new domain may be impractical.Zero pronoun resolution for machine translationreported by Nakaiwa nd Ikehara (Nakaiwa nd Ike-hara, 1992) used only semantic attributes of verbsin a restricted omain.
The small test results (102sentences from 29 articles) had high success rateof 93%.
However, the input was only the firstparagraphs of newspaper articles which containedrelatively short sentences.
Our anaphora resolu-tion systems reported here have the advantages ofdomain-independence and full-text handling withoutthe need for creating an extensive domain knowledgebase.Various theories of Japanese zero pronouns havebeen proposed by computational linguists, for ex-ample, Kameyama (Kameyama, 1988) and Walkeret aL (Walker et al, 1994).
Although these the-ories are based on dialogue examples rather thantexts, "features" used by these theories and thoseby the decision trees overlap interestingly.
For ex-ample, Walker et ai.
proposes the following rankingscheme to select antecedents of zero pronouns.
(GRAMMATICAL or ZERO) TOPIC > EMPATHY >SUBJECT > OBJECT2 > OBJECT > OTHERS128In examining decision trees produced with anaphorictype identification turned on, the following featureswere used for QZPRO-ORG in this order: topical-ization, distance between an anaphor and an an-tecedent, semantic lass of an anaphor and an an-tecedent, and subject NP.
We plan to analyze furtherthe features which the decision tree has used for zeropronouns and compare them with these theories.6 Summary  and  Future  WorkThis paper compared our automated and manual ac-quisition of anaphora resolution strategies, and re-ported optimistic results for the former.
We planto continue to improve machine learning-based sys-tem performance by introducing other relevant fea-tures.
For example, discourse structure informa-tion (Passonneau and Litman, 1993; Hearst, 1994),if obtained reliably and automatically, will be an-other useful domain-independent feature.
In addi-tion, we will explore the possibility of combiningmachine learning results with manual encoding ofdiscourse knowledge.
This can be accomplished byallowing the user to interact with the produced clas-sifters, tracing decisions back to particular examplesand allowing users to edit features and to evaluatethe efficacy of changes.Re ferencesChinatsu Aone and Scott W. Bennett.
1994.
Dis-course Tagging Tool and Discourse-tagged Mul-tilingual Corpora.
In Proceedings of Interna-tional Workshop on Sharable Natural LanguageResources (SNLR).Chinatsu Aone and Douglas McKee.
1993.Language-Independent A aphora Resolution Sys-tem for Understanding Multilingual Texts.
InProceedings of 31st Annual Meeting of the ACL.Chinatsu Aone, Sharon Flank, Paul Krause, andDoug McKee.
1993.
SRA: Description of theSOLOMON System as Used for MUC-5.
In Pro-ceedings of Fourth Message Understanding Con-ference (MUC-5).Chinatsu Aone.
1994.
Customizing and Evaluatinga Multilingual Discourse Module.
In Proceedingsof the 15th International Conference on Compu-tational Linguistics (COLING).Susan Brennan, Marilyn Friedman, and Carl Pol-lard.
1987.
A Centering Approach to Pronouns.In Proceedings of 25th Annual Meeting of theACL.Dennis Connolly, John D. Burger, and David S.Day.
1994.
A Machine Learning Approach toAnaphoric Reference.
In Proceedings of Interna-tional Conference on New Methods in LanguageProcessing (NEMLAP).Marti A. Hearst.
1994.
Multi-Paragraph Segmenta-tion of Expository Text.
In Proceedings of 32ndAnnual Meeting of the ACL.Jerry R. Hobbs.
1976.
Pronoun Resolution.
Tech-nical Report 76-1, Department of Computer Sci-ence, City College, City University of New York.Megumi Kameyama.
1988.
Japanese Zero Pronom-inal Binding, where Syntax and Discourse Meet.In Papers from the Second International Workshoon Japanese Syntax.Hans Kamp.
1981.
A Theory of Truth and SemanticRepresentation.
In J. Groenendijk et al, editors,Formal Methods in the Study of Language.
Math-ematical Centre, Amsterdam.Hiromi Nakaiwa and Satoru Ikehara.
1992.
ZeroPronoun Resolution in a Japanese to English Ma-chine Translation Systemby using Verbal Seman-tic Attribute.
In Proceedings of the Fourth Con-ference on Applied Natural Language Processing.Rebecca J. Passonneau and Diane J. Litman.
1993.Intention-Based Segmentation: Human Reliabil-ity and Correlation with Linguistic Cues.
In Pro-ceedings of 31st Annual Meeting of the ACL.J.
Ross quinlan.
1993.
C~.5: Programs forMachineLearning.
Morgan Kaufmann Publishers.Stephen Soderland and Wendy Lehnert.
1994.Corpus-driven Knowledge Acquisition for Dis-course Analysis.
In Proceedings of AAAI.Marilyn Walker, Masayo Iida, and Sharon Cote.1994.
Japanese Discourse and the Process of Cen-tering.
Computational Linguistics, 20(2).Marilyn A. Walker.
1989.
Evaluating Discourse Pro-cessing Algorithms.
In Proceedings of 27th AnnualMeeting of the ACL.129
