Proceedings of the SIGDIAL 2013 Conference, pages 142?144,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsInteractive Error Resolution Strategies forSpeech-to-Speech Translation SystemsRohit Kumar, Matthew Roy, Sankaranarayanan Ananthakrishnan,Sanjika Hewavitharana, Frederick ChoiSpeech, Language and Multimedia Business UnitRaytheon BBN TechnologiesCambridge, MA, USA{rkumar, mroy, sanantha, shewavit, fchoi}@bbn.comAbstract1In this demonstration, we will showcaseBBN?s Speech-to-Speech (S2S) transla-tion system that employs novel interac-tion strategies to resolve errors throughuser-friendly dialog with the speaker.The system performs a series of analysison input utterances to detect out-of-vocabulary (OOV) named-entities andterms, sense ambiguities, homophones,idioms and ill-formed inputs.
This analy-sis is used to identify potential errors andselect an appropriate resolution strategy.Our evaluation shows a 34% (absolute)improvement in cross-lingual transfer oferroneous concepts in our English to Ira-qi-Arabic S2S system.1 IntroductionGreat strides have been made in Speech-to-Speech (S2S) translation systems that facilitatecross-lingual spoken communication (Stallard et.al., 2011).
However, in order to achieve broaddomain coverage and unrestricted dialog capabil-ity, S2S systems need to be transformed frompassive conduits of information to active partici-pants in cross-lingual dialogs.
These active par-ticipants must detect key causes of communica-tion failures and recover from them in an effi-cient, user-friendly manner.Disclaimer: This paper is based upon work supported by theDARPA BOLT Program.
The views expressed are those ofthe authors and do not reflect the official policy or positionof the Department of Defense or the U.S. Government.Distribution Statement A (Approved for Public Release,Distribution Unlimited)Our ongoing work on eyes-free S2S systems isfocused on detecting three types of errors thataffect S2S systems.
First, out-of-vocabulary(OOV) words are misrecognized as phoneticallysimilar words that do not convey the intendedconcept.
Second, ambiguous words such as hom-ophones and homographs often lead to recogni-tion and translation errors.
Also, unseen idiomsproduce erroneous literal translations.
Third, usererrors such as mispronunciations and incompleteutterances lead to ASR errors.
We will demon-strate our interactive error resolution strategies torecover from each of these error types.Section 2 presents our system architecture.Section 3 describes nine interactive error resolu-tion strategies that are the focus of this demon-stration.
An evaluation of our English to Iraqi-Arabic S2S system is summarized in Section 4.2 System ArchitectureFigure 1 shows the architecture of our two-wayFigure 1: BBN S2S System with Error Recoveryin English to Iraqi-Arabic direction142English to Iraqi-Arabic S2S translation system.In the English to Iraqi direction, the initial Eng-lish ASR hypothesis and its corresponding trans-lation are processed through a series of analysis(e.g.
parsing, sense disambiguation) and errordetection (e.g.
ASR/MT confidence, Homo-phone/Idiom/Named-Entity detection) modules.A detailed discussion on the various error detec-tion modules can be found in Prasad et.
al.(2012).
A novel Inference Bridge data structuresupports storage of these analyses in an intercon-nected and retraceable manner.
The potentialerroneous spans are identified and ranked in anorder of severity using this data structure.Based on the top ranked error, one of nine er-ror resolution strategies (discussed in Section 3),is selected and executed.
Each strategy is com-posed of a sequence of steps which include ac-tions such as TTS output, user input processing,translation (unconstrained or constrained) andother error type specific operations.
This se-quence is hand-crafted to efficiently recoverfrom an error.
Following a multi-expert design(Turunen and Hakulinen, 2003), each strategyrepresents an error-specific expert.3 Error Resolution StrategiesFigure 2 illustrates the sequence of steps for thenine interaction strategies used by our system.The OOV Name and ASR Error strategies aredesigned to interactively resolve errors caused byOOV words (names and non-names) as well asother generic ASR and MT errors.
When a spanof words is identified as an OOV named-entity,the user is asked to confirm whether the audiosegment corresponding to those words is a name.Upon user confirmation, the audio segment isspliced into the output target language utterance.This is based on the principle that audio seg-ments containing names are understandableacross languages.In the case where a generic erroneous span isdetected, the user is asked to rephrase the utter-ance.
This strategy is suitable for handling multi-ple error types including OOVs, mispronuncia-tions, and generic ASR/MT errors.
Additionally,the ASR Errors strategy has been designed toFigure 2.
Interaction Strategies for Error Resolution143capture a large fraction of the OOV name falsenegatives (i.e.
missed detections) by allowing theuser to indicate if the identified erroneous span isa name.
Because of the confusability between theerrors handled by these two strategies, we havefound it beneficial to maintain reciprocity be-tween them to recover from all the errors handledby each of these strategies.The four Word Sense (WS) disambiguationstrategies resolve sense ambiguity errors.
Theunderlying principle behind these strategies isthat the sense of an ambiguous word must beconfirmed by at least two of four possible inde-pendent sources of evidence.
These four sourcesinclude (a) the translation system (sense lookupcorresponding to phrase pair associated with theambiguous word), (b) a list of source-languagecontextual keywords that disambiguate a word,(c) the sense predicted by a sense-disambiguationmodel and (d) sense specified by the user.
Be-sides the objective to minimize user effort, thesemultiple sources are necessary because not all ofthem may be available for every ambiguousword.
Case 1: No Mismatch strategy correspondsto the case where sources (a) and (c) agree.
Case2: Filtered strategy corresponds to the casewhere (a) and (b) agree.
In both of these cases,the system proceeds to present the translation tothe Arabic speaker without performing any errorresolution.
If these three sources are unable toresolve the sense of a word, the user is asked toconfirm the sense identified by source (a) as il-lustrated in Case 3: Mismatch strategy.
If theuser rejects that sense, a list of senses is present-ed to the user (Case 4: Backoff strategy).
Theuser-specified sense then drives constrained de-coding to obtain an accurate translation.Albeit simpler, the two homophone resolutionstrategies mimic the word sense disambiguationstrategies in principle and design.
The observedhomophone variant produced by the ASR mustbe confirmed either by a homophone disambigu-ation model (Case 1: No Mismatch) or by theuser (Case 2: Mismatch).
The input utterance ismodified (if needed) by substituting the resolvedhomophone variant in the ASR output which isthen translated and presented to the Arabicspeaker.Strategies for resolving errors associated withidioms and incomplete utterances primarily relyon informing the user about these errors and elic-iting a rephrasal.
For idioms, the user is also giv-en the choice to force a literal translation whenappropriate.Following a mixed-initiative design, at alltimes, the user has the ability to rephrase theirutterance as well as to force the system to pro-ceed with the current translation.
This allows theuser to override system false alarms wheneversuitable.
The interface also allows the user torepeat the last system message which is helpfulfor comprehension of some of the synthesizedsystem prompts for unfamiliar users.4 Summary of EvaluationOur S2S system equipped with the error resolu-tion strategies discussed in the previous sectionwas evaluated on 103 English utterances (25unique utterances repeated by multiple speakers).Each utterance was designed to elicit one of theerror types listed in Section 1.The ASR word error rate for these utteranceswas 23%.
The error detection components wereable to identify 59% of these errors and the cor-responding error resolution strategies were cor-rectly triggered.The erroneous concepts in 13 of the 103 utter-ances (12.6%) were translated without any error.Using the error resolution strategies, an addition-al 34% of the erroneous concepts were accurate-ly translated.
This increased precision isachieved at the cost of user effort.
On average,the strategies needed 1.4 clarifications turns perutterance.Besides focusing on improving the error de-tection and resolution capabilities, we are cur-rently working on extending these capabilities totwo-way S2S systems.
Specifically, we are de-signing interactive strategies that engage bothusers in eyes-free cross-lingual communication.ReferencesDavid Stallard, Rohit Prasad, Prem Natarajan, FredChoi, Shirin Saleem, Ralf Meermeier, KristeKrstovski, Shankar Ananthakrishnan, and JacobDevlin.
2011.
The BBN TransTalk Speech-to-Speech Translation System.
Speech and LanguageTechnologies, InTech, 31-52Rohit Prasad, Rohit Kumar, Sankaranarayanan Anan-thakrishnan, Wei Chen, Sanjika Hewavitharana,Matthew Roy, Frederick Choi, Aaron Challenner,Enoch Kan, Arvind Neelakantan, and PremkumarNatarajan.
2012.
Active Error Detection and Reso-lution for Speech-to-Speech Translation.
Intl.Workshop on Spoken Language Translation(IWSLT), Hong KongMarkku Turunen, and Jaakko Hakulinen, 2003.
Jaspis- An Architecture for Supporting Distributed Spo-ken Dialogues.
Proc.
of Eurospeech, Geneva, Swit-zerland144
