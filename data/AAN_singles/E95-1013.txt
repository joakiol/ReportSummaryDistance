Literal Movement GrammarsAnnius V. Groenink*CWIKruislaan 4131098 SJ AmsterdamThe Netherlandsavg@cwi,nlAbstractLiteral movement grammars (LMGs) pro-vide a general account of extraposition phe-nomena through an attribute mechanism al-lowing top-down displacement of syntacti-cal information.
LMGs provide a simpleand efficient reatment of complex linguisticphenomena such as cross-serial dependen-cies in German and Dutch--separating thetreatment of natural anguage into a parsingphase closely resembling traditional context-free treatment, and a disambiguation phasewhich can be carried out using matching, asopposed to full unification employed in mostcurrent grammar formalisms of linguisticalrelevance.1 IntroductionThe motivation for the introduction of the literal move-ment grammars presented in this paper is twofold.
Thefirst motivation is to examine whether, and in whichways, the use of unification is essential to automatedtreatment of natural anguage.
Unification is an ex-pensive operation, and pinpointing its precise role inNLP may give access to more efficient treatment oflanguage than in most (Prolog-based) scientific appli-cations known today.
The second motivation is thedesire to apply popular computer-science paradigms,such as the theory of attribute grammars and modu-lar equational specification, to problems in linguistics.These formal specification techniques, far exceedingthe popular Prolog in declarativity, may give new in-sight into the formal properties of natural anguage,and facilitate prototyping for large language applica-tions in the same way as they are currently being used tofacilitate prototyping of programming language tools.For an extensive illustration of how formal specifi-cation techniques can be made useful in the treatmentof natural language, see (Newton, 1993) which de-scribes the abstract specification of several accounts ofphrase structure, features, movement, modularity and*This work is supported by SION grant 612-317-420of the Netherlands Organization for Scientific Research~wo).90parametrization so as to abstract away from the exactlanguage being modelled.
The specification language(ASL) used by Newton is a very powerful formalism.The class of specification formalisms we have in mindincludes less complex, equational techniques such asASF+SDF (Bergstra et al, 1989) (van Deursen, 1992)which can be applied in practice by very efficient exe-cution as a term rewriting system.Literal movement grammars are a straightforwardextension of context-free grammars.
The derivationtrees of an LMG analysis can be easily transformedinto trees belonging to a context-free backbone whichgives way to treatment by formal specification systems.In order to obtain an efficient implementation, somerestrictions on the general form of the formalism arenecessary.1.1 Structural Context Sensitivity in NaturalLanguageEquational specification systems such as theASF+SDF system operate through sets of equationsover signatures that correspond to arbitrary formsof context-free grammar.
An attempt at an equa-tional specification of a grammar based on context-free phrase structure rules augmented with feature con-straints may be to use the context-free backbone as asignature, and then implement further analysis throughequations over this signature.
This seems entirely ana-loguous to the static semantics of a programming lan-guage: the language itself is context-free, and the staticsemantics are defined in terms of functions over theconstructs of the language.In computer-science applications it is irrelevantwhether the evaluation of these functions is carriedout during the parsing phase (I-pass treatment), orafterwards (2-pass treatment).
This is not a trivialproperty of computer languages: acomputer languagewith static semantics restrictions i a context-sensitivesublanguage of a context-free language that is eitherunambiguous or has the finite ambiguity property: forany input sentence, there is only a finite number ofpossible context-free analyses.In section 1.3 we will show that due to phenom-ena of extraposition or discontinuous constituency ex-hibited by natural anguages, a context-free backbonefor a sufficiently rich fragment of natural anguage nolonger has the property of finite ambiguity.
Hence aninitial stage of sentence processing cannot be based ona purely context-free analysis.The LMG formalism presented in this paper at-tempts to eliminate infinite ambiguity by providingan elementary, but adequate treatment of movement.Experience in practice suggests that after relocatingdisplaced constituents, a further analysis based on fea-ture unification no longer exploits unbounded struc-tural embedding.
Therefore it seems that after LMG-analysis, there is no need for unification, and furtheranalysis can be carried out through functional match-ing techniques.1.2 AimsWe aim to present a grammar formalism thatt~ is sufficiently powerful to model relevant frag-ments of natural anguage, at least large enoughfor simple applications such as an interface to adatabase system over a limited domain.t, is sufficiently elementary to act as a front-end tocomputer-scientific tools that operate on context-free languages.t~ has a (sufficiently large) subclass that allows ef-ficient implementation through standard (Earley-based) left-to-right parsing techniques.1.3 RequirementsThree forms of movement in Dutch will be a leadingthread throughout this paper.
We will measure theadequacy of a grammar formalism in terms of its abilityto give a unified account of these three phenomena.Topical izat ion The (leftward) movement of the ob-jects of the verb phrase, as in(1) \[Which book\]/ did John forget oreturn el to the library?Dutch sentence  s t ructure  The surface order ofsentences in Dutch takes three different forms: thefinite verb appears inside the verb phrase in relativeclauses; before the verb phrase in declarative clauses,and before the subject in questions:(2) ... dat Jan \[vP Marie kuste \](3) Jan kustei \[vP Marie el \](4) kustei Jan \[,ca Marie ei \] ?We think of these three (surface) forms a s merely beingdifferent representations of the same (deep) structure,and will take this deep structure to be the form (2) thatdoes not show movement.Cross-serial dependencies In Dutch and German,it is possible to construct sentences containing arbitrarynumbers of crossed ependencies, such as in... dat Marie Jani Fredj Annekthat(5) hoordei helpenj overtuigen kheard help convince(that Mary heard John help Fred convince Anne).
Herethe i, j , k denote which noun is the first object of whichverb.
The analysis we have in mind for this isdat Marie Jani Fredj Annek\[ve hoorde el helpen ej overtuigen e~.\]Note that this analysis (after relocation of the extra-posed objects) is structurally equal to the correspond-ing English VP.
The accounts of Dutch in this paperwill consistently assign "deep structures" to sentencesof Dutch which correspond to the underlying structureas it appears in English.
Similar accounts can be givenfor other languages--so as to get a uniform treatmentof a group of similar (European) languages uch asGerman, French and Italian.If we combine the above three analyses, the final anal-ysis of (3) will becomeJan kustei Mariej \[w el ej \]Although this may look like an overcomplication, thisabundant use of movement is essential in any uniformtreatment of Dutch verb constructions.
Hence it turnsout to occur in practice that a verb phrase has no lexicalexpansion at all, when a sentence shows both objectand verb extraposition.
Therefore, as conjectured inthe introduction, a 2-pass treatment ofnatural languagebased on a context-free backbone will in general fai l--as there are infinitely many ways of building an emptyverb phrase from a number of empty constituents.2 Definition and ExamplesThere is evidence that suggests that the typical humanprocessing of movement is to first locate displaced in-formation (the filler), and then find the logical location(the trace), to substitute that information.
It also seemsthat by and large, displaced information appears earlierthan (or left of) its logical position, as in all examplesgiven in the previous ection.
The typical unification-based approach to such movement is to structurallyanalyse the displaced constituent, and use this anal-ysed information in the treatment of the rest of thesentence.
This method is called gap-threading; see(Alshawi, 1992).If we bear in mind that a filler is usually found tothe left of the corresponding trace, it is worth takinginto consideration todevelop a way of deferring treat-ment of syntactical data.
E.g.
for example sentence 1this means that upon finding the displaced constituentwhich book, we will not evaluate that constituent, butrather emember during the treatment of the remainingpart of the sentence, that this data is still to be fittedinto a logical place.This is not a new idea.
A number of non-concatenative grammar formalisms has been put for-ward, such as head-wrapping rammars (HG) (Pol-lard, 1984), extraposition grammars (XG) (Pereira,1981).
and tree adjoining grammars (TAG) (Krochand Joshi, 1986).
A discussion of these formalismsas alternatives to the LMG formalism is given in sec-tion 4.91Lessons in parsing by hand in high school (e.g.
inEnglish or Latin classes) informally illustrate the pur-pose of literal movement grammars: as opposed to thetraditional inguistic point of view that there is onlyone head which dominates a phrase, constituents of asentence have several key components.
A verb phrasefor example not only has its finite verb, but also one ormore objects.
It is precisely these key components hatcan be subject to movement.
Now when such a keycomponent is found outside the consitituent i  belongsto, the LMG formalism implements a simple mecha-nism to pass the component down the derivation tree,where it is picked up by the constituent that containsits trace.It is best to think of LMGs versus context-freegrammars as a predicate version of the (propositional)paradigm of context-free grammars, in that nonter-minals can have arguments.
If we call the generalclass of such grammars predicate grammars, the dis-tinguishing feature of LMG with respect to other pred-icate grammar formalisms uch as indexed grammars I(Weir, 1988) (Aho, 1968) is the ability of binding orquantification i the right hand side of a phrase struc-ture rule.1 2.1 Definit ion We fix disjoint sets N, T, V of non-terminal symbols, terminal symbols and variables.
Wewill write A, B, C .
.
.
to denote nonterminal symbols,a, b, c .
.
.
to denote terminal symbols, and x, y, z forvariables.
A sequence ala2.
?
?
a,~ or a E T* is calleda (terminal) word or string.
We will use the symbolsa, b, e for terminal words.
(Note the use of bold facefor sequences.
)1 2.2 Definit ion (term) A sequence t l t2 .
.
.
t~ ort E (V U T)* is called a term.
If a term consists ofvariables only, we call it a vector and usually write x.1 2.3 Definition (similarity type)  A (partial) func-tion # mapping N to the natural numbers is called asimilarity type.1 2.4 Definition (predicate) Let # be a similaritytype, A E N and n = /~(A), and for 1 <_ i <_ n,let ti be a term.
Then a predicate qa of type # isa terminal a (a terminal predicate) or a syntacticalunit of the form A ( t l , t 2, ?
?., t,~ ), called a nonterminalpredicate.
If all t~ = xl are vectors, we say that= A(a~l, ~e2, .
.
.
, a~n) is apattern.Informally, we think of the arguments of a nonterminalas terminal words.
A predicate A(x) then stands fora constituent A where certain information with termi-nal yield x has been extraposed (i.e.
found outsidethe constituent), and must hence be left out of the Aconstituent i self.1 2.5 Definition (item) Let/z be a similarity type,~p a predicate of type #, and t a term.
Then an itemof type # is a syntactical unit of one of the followingforms:1 Indexed grammars are a weak form of monadic predicategrammar, as a nonterminal can have at most one argument.1.
qo (a nonterminal or terminal predicate)2. x:~ (a quantifier item)3.
~/ t  (a slash item)We will use ?,  qJ to denote items, and a,/3, 3' to denotesequences of items.1 2.6 Definit ion Let /z be a similarity type.
Arewrite rule R of type/2 is a syntactical unit qo ---,qbl (I)2 ? '
?
qb,~ where qo is a pattern of type #, and forI < i < n, ~i  is an item of type #.A literal movement grammar is a triple (#, S, P)where # is a similarity type, S E N, #(S)  = 0 and Pis a set of rewrite rules of type #.Items on the right hand side of a rule can either referto variables, as in the following rule:A(x, yz) -~ BO/x  a/y C(z)or bind new variables, as the first two items inA 0 ---, x :B 0 y:C(x) D(y).A slash item such as B() /x  means that x should beused instead of the actual "input" to recognize the non-terminal predicate B().
I.e.
the terminal word x shouldbe recognized as B0 ,  and the item BO/x  itself willrecognize the empty string.
A quantifier item x:B()means that a constituent B() is recognized from theinput, and the variable x, when used elsewhere in therule, will stand for the part of the input recognized.1 2.7 Definition (rewrite semantics) Let R =A(Xh.
.
.
,  x,~) ~ ~1(I)2 , .
.
~rn  be a rewrite rule, thenan instantiation of R is the syntactical entity obtainedby substituting for each i and for each variable x E xla terminal word a~.A grammar derives the string a iff S 0 =~ a whereG ===~ is a relation between predicates and sequences ofitems defined inductively by the following axioms andinference rules: 2G a ~ aG qo ==* a when qo --* a is an instantiationof a rule in Gqo ~ /3 A(t l , .
.
.
, t ,~)  7 A(t l , .
.
.
, t ,~)  ~ aMPG,-t3 a 7-Lc,/3 ?
/a  7/E:E(/3 a 3')\[a/x\]2Note that \[a/x\] in the :E rule is not an item, but standsfor the substitution of a for z.92B(aa) ~ a/a b B(a) c a~ aB(aa) ~ b B(a) c/EB(a) =~a a/a b B(e) c a~aB(a)~a b B(?)
c/EB(?)
=~B(a) =~G bcMPMPsO ~ x:  A 0 B(x) A 0 ==~ aaS 0 =~ aa B(aa)B(aa) ~ bbcc:EB(aa) =~G bbccMPS 0 ~ aabbccFigure 1.
Derivation of aabbcc .1 2.8 Example (a'~b'~c '*) The following, very ele-mentary LMG recognizes the trans-context free lan-guage anbnc  n :s0 ~ ~:AO B(~)A 0 ---* a A 0A 0 --~B(xy) ~ a/x b B(y) c8(6) ~Figure 1 shows how aabbcc  is derived according tothe grammar.
The informal tree analysis in figure 2s0A0a A 06B(y) = B(aa)a/a b B(a) ca/a b B(e) cE 6Figure 2.
Informal tree analysis.illustrates more intuitively how displaced information(the two a symbols in this case) is 'moved back down'into the tree, until it gets 'consumed' by a slash item.It also shows how we can extract a context-free 'deepstructure' for further analysis by, for example, formalspecification tools: if we transform the tree, as shownin figure 3, by removing quantified (extraposed) ata,and abstracting away from the parameters, we see thatthe grammar, in a sense, works by transforming the lan-guage anbnc n to the context-free language (ab)ncn.Figure 4 shows how we can derive a context free 'back-bone grammar' from the original grammar.12.9 Example (cross-serial dependencies inDutch) The following LMG captures precisely thethree basic types of extraposition defined in section1.3: the three Dutch verb orders, topicalization andcross-serial verb-object dependencies.s ~ s'(~)S'(e) -~ dat  NP VP(e,e)S'(e) * n:NP S'(n)S'(n) -~ v:V NP VP(v,n)S'(e) ~ NP v:V VP(v,e)re(v, n) -~ m:NP W(v,,~m)VP(v,,~) --, V'(v, n)V(c, ~) --, v7V(v, ~) --, Vt/vV(?,n) - ,  VT NP/nV'(v, n) ~ VT/v NP/n12(?,nm) ---, VR NP/n 12(e,m)V(v, nm) ---* VR/v gP/n V(e,m)V ~ VIV --+ VTV ~ VRA sentence S' has one argument which is used, ifnonempty, to fill a noun phrase trace.
A VP has twoSXP Ba b B ca b B cIFigure 3.
Context free backbone.93S --, XPBB -~ a b B cB -~ eXP -* eFigure 4.
Backbone grammar.arguments: the first is used to fill verb traces, the sec-ond is treated as a list of noun phrases to which morenoun phrases can be appended.
A V' is similar to a VPexcept hat it uses the list of noun phrases in its secondargument to fill noun phrase traces rather than addingto it.Figure 5 shows how this grammar accepts the sen-tenceMarie zag Fred Anne kussen.We see that it is analyzed asMarie zag i Fredj AnnekIV' ei ej \[V, kussen e~ \]\]which as anticipated in section 1.3 has precisely thebasic, context-free underlying structure of the corre-sponding English sentence Mary saw Fred kiss Anneindicated in figure 5 by terminal words in bold face.Note that arbitrary verbs are recognized by a quanti-s()s'(e)!
\[ VP(v ~- zag, e')Marie z ag ~ n ~  .
.
.
.~  nNP , = Fred)n = Fred Anne) Fred p IAnne Vt(zag, Fred Anne)V R / ~ E ,  Anne)VT NP/AnneI :kussen eFigure 5.
Derivation of a Dutch sentencefier item v:V, and only when, further down the tree, atrace is filled with such a verb in items such as VR/v,its subcategorization types VI, VT and VR start playinga role.3 Formal PropertiesThe LMG formalism in its unrestricted form is shownto be Turing complete in (Groenink, 1995a).
But thegrammars presented in this paper satisfy a number ofvital properties that allow for efficient parsing tech-niques.Before building up material for a complexity result,notice the following proposition, which shows, usingonly part of the strength of the formalism, that theliteral movement grammars are closed under intersec-tion.1 3.1 Proposition (intersection) Given two lit-eral movement grammars G1 --- (#1,$1, P1) andGz = (tzz, $2, Pz) such that dom(#l)  n dom(#2) = O,we can construct the grammar G I  = (#1 U #z U{(S, 0)}, S, P1 U P2 U {R}) where we add the ruleR:so  -~ =S,O Sz()/xClearly, GI recognizes precisely those sentences whichare recognized by both G1 and Gz.We can use this knowledge in example 2.9 to restrictmovement of verbs to verbs of finite morphology, byadding a nonterminal VFIN, replacing the quantifieritems v:V that locate verb fillers with v:VFIN, whereVFIN generates all finite verbs.
Any extraposed verbwill then be required to be in the intersection of VFINand one of the verb types VI, VT or VR, reducingpossible ambiguity and improving the efficiency ofleft-to-right recognition.The following properties allow us to define restrictionsof the LMG formalism whose recognition problem hasa polynomial time complexity.1 3.2 Definition (non-combinatorial) An LMG isnon-combinatorial f every argument of a nonterminalon the RHS of a rule is a single variable (i.e.
we donot allow composite terms within predicates).
If Gis a non-combinatorial LMG,  then any terminal stringoccurring (either as a sequence of items or inside apredicate) in a full G-derivation is a substring of thederived string.
The grammar of example 2.8 is non-combinatorial; the grammar of example 2.9 is not (theoffending rule is the first VP production).1 3.3 Definition ( left-binding) An LMG G is left-binding when1.
W.r.t.
argument positions, an item in the RHS ofa rule only depends on variables bound in itemsto its left.2.
For any vector x ~ ?
?
?
x,~ of n > 1 variables on theLHS, each of xl upto xn-~ occurs in exactly oneitem, which is of the form qo/xl.
Furthermore,for each 1 < I < k < n the item referring to xzappears left of any item referring to x~.For example, the following rule is left binding:A(xyz, v) ~ u:B(v) C(v)/x DO/y E(u,z)but these ones are not:(a) g(y) ---* C(x) x:D(y)(b) A(xy) ---* A(x) B(y)(c) A(xyz)~ A(z) BO/x  CO/y94because in (a), x is bound right of its use; in (b),the item A(x) is not of the form qo/x and in (e), thevariables in the vector zyz occur in the wrong order(zzy).Ifa grammar satisfies condition 1, then for any deriv-able string, there is a derivation such that the modusponens and elimination rules are always applied to theleftmost item that is not a terminal.
Furthermore, the:E rule can be simplified to:EGThe proof tree in example 2.8 (figure 1) is an exampleof such a derivation.Condition 2 eliminates the nondeterminism in find-ing the right instantiation for rules with multiple vari-able patterns in their LHS.Both grammars from section 2 are left-binding.1 3.4 Definition (left-recursive) An LMG G isleft-recursive if there exists an instantiated nonterminalG predicate qa such that there is a derivation of ~o ~ ~pc~for any sequence of items c~.The following two rules show that left-recursion iLMG is not always immediately apparent:A(y) ~ BO/Y A(e)B 0 ~for we haveA(?)
~ B()/?
a(e) B 0 ~A(~) =:~ A(e)/EWe now show that the recognition problem for an arbi-trary left-binding, non-combinatorial LMG has a poly-nomial worst-case time complexity.1 3.5 Theorem (polynomial complexity) LetG be a LMG of similarity type # that is non-combinatorial, left binding and not left-recursive.
Letm be the maximum number of items on the right handside of rules in G, and let p be the greatest arity ofpredicates occurring in G. Then the worst case timecomplexity of the recognition problem for G does notexceed O(IGIm(1 + p)nl+'~+2P), where n is the sizeof the input string ala2" ?
.a,~.Proof  (sketch) We adopt he memoizing recursive de-scent algorithm presented in (Leermakers, 1993).
AsG is not left-binding, the terminal words associatedwith variables occurring in the grammar rules can befully determined while proceeding through sentenceand rules from left to right.
Because the grammar isnon-combinatorial, the terminal words substituted inthe argument positions of a nonterminal are alwayssubstrings of the input sentence, and can hence be rep-resented as a pair of integers.The recursive descent algorithm recursively com-putes set-valued recognition functions of the form:\[~o\](i) = {jl~o ~ a i+ l " "  .a j}where instead of a nonterminal as  in the context-free case, qo is any instantiated nonterminal predicateA(bl , .
.
.
,  b,~).
As bl , .
.
.
,b,~ are continuous ub-strings of the input sentence ala2 ?
?
?
an, we can re-formulate this as\[A\](i, (tl, r,) , .
.
.
,  r,,))= { j lA (ah+ 1.. .a, , , , .
.
.
,at.+l.
.
.ar~)ai+ 1 ?
?.
a j  }Where # = #(A) < p. The arguments i, l l , .
.
.
, l~,and r l , .
?., r t, are integer numbers ranging from 0 ton - 1 and 1 to n respectively.
Once a result of sucha recognition function has been computed, it is storedin a place where it can be retrieved in one atomicoperation.
The number of such results to be stored isO(n) for each possible nonterminal nd each possiblecombination of, at most 1 + 2p, arguments; o the totalspace complexity is O(IGIn2+2p).Much of the extra complication w.r.t, the context-free case is coped with at compile time; for example,if there is one rule for nonterminal A:A(x,,x2) ~ x3:Ba(xj) B2() B3(x3)/x2then the code for \[g\](i, (ll, r,), (12, r2)) will beresult := emptyfor kl E \[B1\](i, (/1, rl))do 13 := ir 3 := k 1rot efor k 3 e \[B3\](/2, (/3, r3))if (k 3 =:  r2)add k2 to resultreturn resultThe extra effort remaining at parse time is in copy-ing arguments and an occasional extra comparison(the if statement in the example), taking rn(1 + p)steps everytime the innermost for statement is reached,and the fact that not O(n), but O(n  l+2p) argument-value pairs need to be memoized.
Merging the re-sults in a RHS sequence of rn items can be done inO(m(1 + p)n ~-1) time.
The result is a set of O(n)size.
As there are at most O(IGln 1+2p) results to becomputed, the overall time complexity of the algorithmis O(IGIm(1 + p)nl+m+2P).
\[\]| 3.6 Remark  If all nonterminals in the grammar arenullary (p = 0), then the complexity result coincideswith the values found for the context-free recursivedescent algorithm (Leermakers, 1993).
Nullary LMGincludes the context-free case, but still allows move-ment local to a rule; the closure result 3.1 still holds forthis class of grammars.
As all we can do with bindingand slashing local to a rule is intersection, the nullaryLMGs must be precisely the closure of the context-freegrammars under finite intersection.These results can be extended to more efficient al-gorithms which can cope with left-recursive gram-mars such as memoizing recursive ascent (Leermak-ers, 1993).
A very simple improvement is obtainedby bilinearizing the grammar (which is possible if it95is left binding), giving a worst case complexity ofo(Ic\[(1 + p)n3+2,).4 Other Approaches to Separation ofMovementA natural question to ask is whether the LMG for-malism (for the purpose of embedding in equationalspecification systems, or eliminating unification as astage of sentence processing) really has an advantageover existing mildly context-sensitive approaches tomovement.
Other non-concatenative formalisms arehead-wrapping grammars (HG) (Pollard, 1984), extra-position grammars (XG) (Pereira, 1981) and variousexotic forms of tree adjoining grammar (Kroch andJoshi, 1986).
For overviews ee (Weir, 1988), (Vijay-Shanker et al, 1986) and (van Noord, 1993).
The mostapplicable of these formalisms for our purposes eemto be HG and XG, as both of these show good re-sults in modeling movement phenomena, nd both aresimilar in appearance to context-free grammars; as inLMG, a context-free grammar has literally the samerepresentation when expressed in HG or XG.
Hence itis to be expected that incorporating these approachesinto a system based on a context-free front-end will notrequire a radical change of perspective.4.1 Head GrammarsA notion that plays an important role in various formsof Linguistic theory is that of a head.
Although thereis a great variation in the form and function of headsin different heories, in general we might say that thehead of a constituent is the key component of that con-stituent.
The head grammar formalism, introduced byPollard in (Pollard, 1984) divides a constituent intothree components: a left context, a terminal head anda right context.
In a HG rewrite rule these parts of aconstituent can be addressed separately when buildinga constituent from a number of subconstituents.An accurate and elegant account of Dutch cross-serial dependencies using HG is sketched in (Pollard,1984).
However, we have not been able to constructhead grammars that are able to model verb move-ment, cross-serial dependencies and topicalization atthe same time.
For every type of constituent, thereis only one head, and hence only one element of theconstituent that can be the subject o movement.
34.2 Extraposition GrammarsWhereas head grammars provide for an account ofverb fronting and cross-serial dependencies, Pereira,3However, astraightforward extension of head grammarsdefined in (Groenink, 1995a) which makes use of arbitrary tu-pies, rather than dividing constituents into three components,is (1) capable of representing the three target phenomena ofDutch all at once and (2) weakly equivalent to a (stronglylimiting) restriction of literal movement grammars.
Headgrammars and their generalizations, being linear context-free rewriting systems (Weir, 1988), have been shown tohave polynomial complexity.introducing extraposition grammars in (Pereira, 1981),is focused on displacement ofnoun phrases in English.Extraposition grammars are in appearance very similarto context-free grammars, but allow for larger patternson the left hand side of PS rules.
This makes it possibleto allow a topicalized NP only if somewhere to its rightthere is an unfilled trace:S --~ Topic STopic .
.
.
XP  --* NPWhile XG allows for elegant accounts of cross-serialdependencies and topicalization, it seems again hardto simultaneously account for verb and noun move-ment, especially if the bracketing constraint introducedin (Pereira, 1981), which requires that XG derivationgraphs have a planar representation, is not relaxed.
4Furthermore, the practical application of XG seemsto be a problem.
First, it is not obvious how we shouldinterpret XG derivation graphs for further analysis.Second, as Pereira points out, it is nontrivial to makethe connection between the XG formalism and stan-dard (e.g.
Earley-based) parsing strategies o as toobtain truly efficient implementations.5 ConclusionsWe have presented the LMG formalism, examplesof its application, and a complexity result for a con-strained subclass of the formalism.
Example 2.9 showsthat an LMG can give an elegant account of movementphenomena.
The complexity result 3.5 is primarily in-tended to give an indication of how the recognitionproblem for LMG relates to that for arbitrary contextfree grammars.
It should be noted that the result inthis paper only applies to non-combinatorial LMGs,excluding for instance the grammar of example 2.9 aspresented here.There are other formalisms (HG and XG) whichprovide sensible accounts of the three movement phe-nomena sketched in section 1.3, but altogether do notseem to be able to model all phenomena t once.
In(Groenink, 1995b) we give a more detailed analysis ofwhat is and is not possible in these formalisms.Future Work1.
The present proof of polynomial complexity doesnot cover a very large class of literal movement gram-mars.
It is to be expected that larger, Turing complete,classes will be formally intractable but behave reason-ably in practice.
It is worthwile to look at possible prac-tical implementations for larger classes of LMGs, andinvestigate the (theoretical and practical) performanceof these systems on various representative grammars.2.
Efficient treatment of LMG strongly dependson the left-binding property of the grammars, which4Theoretically simultaneous treatment ofthe three move-ment phenomena is not impossible in XG (a technique similartopit-stopping inGB allows one to wrap extrapositions overnatural bracketing islands), but grammars and derivationsbecome very hard to understand.96seems to restrict grammars to treatment of leftwardextraposition.
In reality, a smaller class of rightwardmovement phenomena will also need to be treated.
Itis shown in (Groenink, 1995b) that these can easilybe circumvented in left-binding LMG, by introducingartificial, "parasitic" extraposition.AcknowledgementsI would like to thank Jasper Kamperman, Ren6 Leer-makers, Jan van Eijck and Eelco Visser for their en-thousiasm, for carefully reading this paper, and formany general and technical comments that have con-tributed a great deal to its consistency and readability.David J. Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Uni-versity of Pennsylvania.ReferencesA.V.
Aho.
1968.
Indexed Grammars -an  Extensionto Context-free grammars.
JACM, 15:647-671.Hiyan Alshawi, editor.
1992.
The Core LanguageEngine.
MIT Press.J.A.
Bergstra, J. Heering, and P. Klint, editors.
1989.Algebraic Specification.
ACM Press Frontier Se-ries.
The ACM Press in co-operation with Addison-Wesley.Annius V. Groenink.
1995a.
Accounts ofMovement--a Formal Comparison.
Unpublishedmanuscript.Annius V. Groenink.
1995b.
Mechanisms for Move-ment.
Paper presented at the 5th CLIN (Compu-tational Linguistics In the Netherlands) meeting,November 1994.A.S.
Kroch and A.K.
Joshi.
1986.
Analyzing Extra-position in a TAG.
In Ojeda Huck, editor, Syntaxand Semantics: Discontinuous Constituents.
Acad.Press, New York.Ren6 Leermakers.
1993.
The Functional Treatment ofParsing.
Kluwer, The Netherlands.Michael Newton.
1993.
Formal Specification ofGrammar.
Ph.D. thesis, University of Edinburgh.Fernando Pereira.
1981.
Extraposition Grammars.Computational Linguistics, 7(4):243-256.Carl J. Pollard.
1984.
Generalized Phrase Struc-ture Grammars, Head Grammars, and Natural Lan-guage.
Ph.D. thesis, Standford University.Arie van Deursen.
1992.
Specification and Genera-tion of a A-calculus environment.
Technical report,CWI, Amsterdam.
Published in revised form in VanDeursen, Executable Language Definitions--CaseStudies and Origin Tracking, PhD Thesis, Univer-sity of Amsterdam, 1994.Gertjan van Noord.
1993.
Reversibility in NaturalLanguage.
Ph.D. thesis, Rijksuniversiteit Gronin-gen.K.
Vijay-Shanker, David J. Weir, and A.K.
Joshi.
1986.Tree Adjoining and Head Wrapping.
In 11th int.conference on Computational Linguistics.97
