Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 43?51,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsA modular open-source focused crawler for mining monolingual andbilingual corpora from the webVassilis PapavassiliouInstitute for Language and Speech ProcessingAthena Research and Innovation CenterAthens, Greece{vpapa, prokopis}@ilsp.grProkopis Prokopidis Gregor ThurmairLinguatecGottfried-Keller-Str.
12, 81245Munich, Germanyg.thurmair@linguatec.deAbstractThis paper discusses a modular and open-source focused crawler (ILSP-FC) for theautomatic acquisition of domain-specificmonolingual and bilingual corpora fromthe Web.
Besides describing the mainmodules integrated in the crawler (dealingwith page fetching, normalization, clean-ing, text classification, de-duplication anddocument pair detection), we evaluate sev-eral of the system functionalities in an ex-periment for the acquisition of pairs of par-allel documents in German and Italian forthe "Health & Safety at work" domain.1 Introduction and motivationThere is a growing literature on using the Web forconstructing various types of text collections, in-cluding monolingual, comparable, parallel and/ordomain-specific corpora.
Such resources canbe used by linguists studying language use andchange (Kilgarriff and Grefenstette, 2003), and atthe same time they can be exploited in applied re-search fields like machine translation and multi-lingual information extraction.
Moreover, thesecollections of raw data can be automatically an-notated and used to produce, by means of induc-tion tools, a second order or synthesized deriva-tives: rich lexica (with morphological, syntacticand lexico-semantic information), large bilingualdictionaries (word andmultiword based) and trans-fer grammars.To this end, several tools (i.e.
web crawlers,HTMLparsers, language identifiers, HTML clean-ers, etc.)
have been developed and combined inorder to produce corpora useful for specific tasks.However, to the best of our knowledge, most ofthe available systems either omit some processingtasks or require access to the results of a search en-gine.
For instance, the BootCaT toolkit (Baroni etal., 2006), a well-known suite of Perl scripts forbootstrapping specialized language corpora fromthe web, uses the Bing search engine and allowsup to 5,000 queries per month.In this paper, we present ILSP-FC, a modularsystem that includes components and methods forall the tasks required to acquire domain-specificcorpora from the Web.
The system is available asan open-source Java project1 and due to its modu-lar architecture, each of its components can be eas-ily substituted by alternatives with the same func-tionalities.
Depending on user-defined configura-tion, the crawler employs processing workflowsfor the creation of either monolingual or bilingualcollections.
For users wishing to try the system be-fore downloading it, two web services2 allow themto experiment with different configuration settingsfor the construction of monolingual and bilingualdomain-specific corpora.The organization of the rest of the paper is as fol-lows.
In Section 2, we refer to recent related work.In Section 3, we describe in detail the workflowof the proposed system.
A solution for bootstrap-ping the focused crawler input is presented in Sec-tion 4.
Then, an experiment on acquiring paralleldocuments in German and Italian for the "Health& Safety at work" domain (H&S) is described inSection 5, which also includes evaluation resultson a set of criteria including parallelness and do-main specificity.
We conclude and mention futurework in Section 6.2 Related workWeb crawling for building domain-specific mono-lingual and/or parallel data involves several tasks(e.g.
link ranking, cleaning, text classification,near-duplicates removal) that remain open issues.Even though there are several proposed methods1http://nlp.ilsp.gr/redmine/projects/ilsp-fc2http://nlp.ilsp.gr/ws/43for each of these tasks, in this section we refer onlyto a few indicative approaches.Olston and Najork (2010) outline the funda-mental challenges and describe the state-of-the-artmodels and solutions for web crawling.
A gen-eral framework to fairly evaluate focused crawlingalgorithms under a number of performance met-rics is proposed by Srinivasan et al(2005).
Ashort overview of cleaning methods is presented inSpousta et al(2008) and the comparison of suchmethods is discussed in Baroni et al(2008).
Sev-eral algorithms (Qi and Davison, 2009) exploit themain content and the HTML tags of a web pagein order to classify a page as relevant to a targeteddomain or not.
Methods for the detection and re-moval of near-duplicates (i.e.
acquired web pagesthat have almost the same content) are reviewedand compared in Theobald et al(2008).Efficient focused web crawlers can be builtby adapting existing open-source frameworks likeHeritrix3, Nutch4 and Bixo5.
For instance, Com-bine6 is an open-source focused crawler that isbased on a combination of a general web crawlerand a text classifier.
Other approaches make useof search engines APIs to identify in-domain webpages (Hong et al 2010) or multilingual web sites(Resnik and Smith, 2003).
Starting from thesepages, Almeida and Sim?es (2010) try to detectwhich links point to translations, while Shi et al(2006) harvest multilingual web sites and extractparallel content from them.
Bitextor (Espl?-Gomisand Forcada, 2010) combines language identifica-tion with shallow features that represent HTMLstructures to mine parallel pages.Besides structure similarity, systems like PT-Miner (Nie et al 1999) and WeBiText (D?siletset al 2008) filtered fetched web pages by keep-ing only those containing languagemarkers in theirURLs.
Chen et al(2004) proposed the ParallelText Identification System, which incorporated acontent analysis module using a predefined bilin-gual wordlist.
Similarly, Zhang et al(2006) andUtiyama et al(2009) adopted the use of alignersin order to estimate the content similarity of candi-date parallel web pages or mixed languages pages.Barbosa et al(2012) proposed the use of bilin-gual dictionaries and generated translations (e.g.byGoogle Translate andMicrosoft Bing) to extract3http://crawler.archive.org/4http://nutch.apache.org5http://openbixo.org/6http://combine.it.lth.se/parallel content from multilingual sites.3 System architectureIn this section, we describe the main modules inte-grated in ILSP-FC.
In general, the crawler initial-izes its frontier (i.e.
the list of pages to be visited)from a seed URL list provided by the user (or con-structed semi-automatically, see Section 4), clas-sifies fetched pages as relevant to the targeted do-main, extracts links from fetched web pages andadds them to the list of pages to be visited.Focused crawlerpage fetchingnormalizationcleaninglanguageidentificationtext classificationlink extractionexportingdeduplicationseedURL listdomaindefinitionin-domainpagesdetection ofparallel documentsdocument pairsFigure 1: System architectureIn order to ensure modularity and scalability, thecrawler is built using Bixo, an open source webmining toolkit that allows easy configuration ofworkflows and runs on top of the Hadoop7 frame-work for distributed data processing.3.1 Page FetcherThe first module concerns page fetching.
Amultithreaded crawling implementation has beenadopted in order to ensure concurrent visiting ofmultiple hosts.
Users can configure several set-tings that determine the fetching process, includ-ing number of concurrent harvesters and filteringout specific document types.
The crawler alwaysrespects standard robots.txt files, while politenesscan also be affected with the use of settings re-garding time intervals for revisiting URLs from thesame website, maximum number of URLs from aspecific host per iteration, maximum number of at-tempts to fetch a web page etc.7http://hadoop.apache.org443.2 NormalizerThe normalizer module uses the Apache Tikatoolkit 8 to parse the structure of each fetched webpage and extract its metadata.
Extracted metadataare exported at a later stage (see Subsection 3.7)if the web document is considered relevant to thedomain.
The text encoding of the web page is alsodetected based on the HTTP Content-Encodingheader and the charset part of the Content-Typeheader, and if needed, the content is converted intoUTF-8.
Besides default conversion, special care istaken for normalization of specific characters likeno break space, narrow no-break space, three-per-em space, etc.3.3 CleanerApart from its textual content, a typical web pagealso contains boilerplate, i.e.
"noisy" elements likenavigation headers, advertisements, disclaimers,etc., which are of only limited or no use for the pro-duction of good-quality language resources.
Forremoving boileplate, we use a modified version ofBoilerpipe 9 (Kohlsch?tter et al2010) that alsoextracts structural information like title, headingand list item.
At this stage, text is also segmentedin paragraphs on the basis of specific HTML tagslike<p>, <br> and<li>.
Paragraphs judged to beboilerplate and/or detected as titles, etc.
are prop-erly annotated (see Subsection 3.7).3.4 Language IdentifierThe next processing module deals with languageidentification.
We use the Cybozu10 languageidentification library that considers n-grams as fea-tures and exploits a Naive Bayes classifier for lan-guage identification.
If a web page is not in thetargeted language, its only further use is in extrac-tion of new links.
Even though the main content ofa web page is in the targeted language, it is likelythat the web page includes a few paragraphs thatare not in this language.Thus, the language iden-tifier is also applied on each paragraph and marksthem properly (see Subsection 3.7).3.5 Text ClassifierThe aim of this module is to identify if a pagethat is normalized and in the targeted languagecontains data relevant to the targeted domain.
To8http://tika.apache.org9http://code.google.com/p/boilerpipe/10http://code.google.com/p/language-detection/this end, the content of the page is comparedto a user-provided domain definition.
Followingthe string-matching method adopted by the Com-bine web crawler, the definition consists of termtriplets (<relevance weight, (multi-word) term,subdomain>) that describe a domain and, option-ally, subcategories of this domain.
Language-dependent stemmers from the Lucene11 projectare used to stem user-provided terms and docu-ment content.
Based on the number of terms?
oc-currences, their location in the web page and theweights of found terms, a page relevance score pis calculated as follows:p =N?i=14?j=1nij ?
wti ?
wlj ,whereN is the amount of terms in the domain def-inition, wti is the weight of term i, wlj is the weightof location j and nij denotes the number of occur-rences of term i in location j.
The four discretelocations in a web page are title, metadata, key-words, and plain text, with respective weights of10, 4, 2, and 1.Moreover, the amount of unique domain termsfound in the main content of the page,m, is calcu-lated.
Then, the values p andm are compared withtwo predefined thresholds (t1 and t2) and if bothvalues are higher than the thresholds, the web pageis categorized as relevant to the domain and stored.It is worth mentioning that the user can affect thestrictness of the classifier by setting the values ofboth thresholds in the crawler's configuration file.3.6 Link ExtractorEven when a web page is not stored (because itwas deemed irrelevant to the domain, or not inthe targeted language), its links are extracted andadded to the list of links scheduled to be visited.Since the crawling strategy is a critical issue fora focused crawler, the links should be ranked andthe most promising links (i.e.
links that point to"in-domain" web pages or candidate translations)should be followed first.
To this end, a score sl iscalculated for each link l as follows:sl = c+ p/L+N?i=1ni ?
wiwhere L is the amount of links originating fromthe source page, N is the amount of terms in thedomain definition, ni denotes the number of occur-rences of the i-th term in the link's surrounding textand wi is the weight of the i-th term.
By using this11http://lucene.apache.org/45formulation, the score link is mainly influenced bythe "domainess" of its surrounding text.The parameter c is only added in case thecrawler is used for building bilingual collections.It gets a high positive value if the link under con-sideration originates from a web page in L1 and"points" to a web page that is probably in L2.This is the case when, for example, L2 is Ger-man and the anchor text contains strings like "de","Deutsch", etc.
The insertion of this parameterforces the crawler to visit candidate translationsbefore following other links.3.7 ExporterThe Exporter module generates an XML file foreach stored web document.
Each file containsmetadata (e.g.
language, domain, URL, etc.
)about the corresponding document inside a headerelement.
Moreover, a <body> element containsthe content of the document segmented in para-graphs.
Apart from normalized text, each para-graph element <p> is enriched with attributes pro-viding more information about the process out-come.
Specifically, <p> elements in the XMLfiles may contain the following attributes: i)crawlinfo with possible values boilerplate, mean-ing that the paragraph has been considered boil-erplate (see Subsection 3.3), or ooi-lang, meaningthat the paragraph is not in the targeted language;ii) typewith possible values: title, heading and lis-titem; and iii) topicwith a string value including allterms from the domain definition detected in thisparagraph.3.8 De-duplicatorIgnoring the fact12 that the web contains manynear-duplicate documents could have a negativeeffect in creating a representative corpus.
Thus,the crawler includes a de-duplicator module thatrepresents each document as a list containing theMD5 hashes of the main content's paragraphs, i.e.paragraphs without the crawlinfo attribute.
Eachdocument list is checked against all other docu-ment lists, and for each candidate pair, the inter-section of the lists is calculated.
If the ratio ofthe intersection cardinality to the cardinality of theshortest list is more than 0.8, the documents areconsidered near-duplicates and the shortest is dis-carded.12Baroni et al(2009) reported that during building of theWacky corpora, the amount of collected documents was re-duced by more than 50% after de-duplication.3.9 Pair DetectorAfter in-domain pages are downloaded, the PairDetector module uses two complementary meth-ods to identify pairs of pages that could be con-sidered parallel.
The first method is based on co-occurrences, in two documents, of images with thesame filename, while the second takes into accountstructural similarity.In order to explain the workflow of the pair de-tection module, we will use the multilingual web-site http://www.suva.ch as a running exam-ple.
Crawling this website using the processes de-scribed in previous subsections provides a pool of707 HTML files (and their exported XML counter-parts) that are found relevant to the H&S domainand in the targeted DE and IT languages (376 and331 files, respectively).Each XML file is parsed and the followingfeatures are extracted: i) the document lan-guage; ii) the depth of the original source page,(e.g.
for http://domain.org/d1/d2/d3/page.html, depth is 4); iii) the amount of para-graphs; iv) the length (in terms of tokens) of theclean text; and v) the fingerprint of the main con-tent, which is a sequence of integers that representthe structural information of the page, in a waysimilar to the approach described by Espl?-Gomisand Forcada (2010).
For instance, the fingerprintof the extract in Figure 2 is [-2, 28, 145, -4, 9, -3,48, -5, 740] with boilerplate paragraphs ignored; -2, -3 and -4 denote that the type attributes of corre-sponding<p> elements have title, heading and lis-titem values, respectively; -5 denotes the existenceof the topic attribute in the last <p>; and positiveintegers are paragraph lengths in characters.The language feature is used to filter out pairs offiles that are in the same language.
Pages that havea depth difference above 1 are also filtered out, onthe assumption that it is very likely that translationsare found at the same or neighbouring depths of theweb site tree.Next, we extract the filenames of the imagesfrom HTML source and each document is repre-sented as a list of image filenames.
Since it is verylikely that some images appear inmanyweb pages,we count the occurrence frequency of each imageand discard relatively frequent images (i.e.
Face-book and Twitter icons, logos etc.)
from the lists.In order to classify images into "critical" or"common" (see Figure 3) we need to calcu-late a threshold.
In principle, one should ex-46<p type="title">Strategia degli investimenti</p> <!-- -2, 28--><p >I ricavi degli investimenti sono un elemento essenziale per finanziare lerendite e mantenere il potere d'acquisto dei beneficiari delle rendite.</p><!--145--><p type="listitem">Document:</p> <!-- -4, 9 --><p crawlinfo="boilerplate" type="listitem">Factsheet "La strategia d'investimentodella Suva in sintesi" (Il link viene aperto in una nuova finestra) </p> <!--ignored --><p type="heading">Perch?
la Suva effettua investimenti finanziari?</p> <!-- -3,48--><p topic="prevenzione degli infortuni;infortunio sul lavoro">Nonostante i moltisforzi compiuti nella prevenzione degli infortuni sul lavoro e nel tempo liberoogni anno accadono oltre 2500 infortuni con conseguenze invalidanti o mortali.In questi casi si versa una rendita per invalidit?
agli infortunati oppure unarendita per orfani o vedovile ai superstiti.
Nello stesso anno in cuiattribuisce una rendita, la Suva provvede ad accantonare i mezzi necessari apagare le rendite future.
La maggior parte del patrimonio investito dalla Suva ?rappresentato proprio da questi mezzi, ossia dal capitale di copertura dellerendite.
La restante parte del patrimonio ?
costituta da accantonamenti perprestazioni assicurative a breve termine come le spese di cura, le indennit?giornaliere e le riserve.</p> <!-- -5, 740-->Figure 2: An extract of an XML file for an Italian web page relevant to the H&S domain.Figure 3: Critical (dashed) and common (dotted) images in a multilingual (EN/DE) site.pect that low/high frequencies correspond to "crit-ical"/"common" images.
We employ a non-parametric approach for estimating the probabil-ity density function (Alpaydin, 2010) of the imagefrequencies using the following formula:p?
(x) = 1MhM?t=1K(x?xth )where the random variable x defines the positions(i.e.
images frequencies) at which the p?
(x) will beestimated, M is the amount of images, xt denotesthe values of data samples in the region of widthh around the variable x, and K(?)
is the normalkernel that defines the influence of values xt in theestimation of p?(x).
The optimal value for h, theoptimal bandwidth of the kernel smoothing win-dow, was calculated as described in Bowman andAzzalini (1997).Figure 4 illustrates the normalized histogram ofimage frequencies in the example collection andthe estimated probability density function.
Onecan identify a main lobe in the low values, whichcorresponds to "critical" images.
Thus, the thresh-old is chosen to be equal to the minimum just af-ter this lobe.
The underlining assumption is that ifa web page in L1 contains image(s) then the webpage with its translation in L2 will contain more orless the same images.
In case this assumption is notvalid for a multilingual site (i.e.
there are only im-ages that appear in all pages, e.g.
template icons),probably all images will be included.
To eliminatethis, we discard images that exist in more than 10%of the total HTML files.Following this step, each document is exam-ined against all others and two documents are con-sidered parallel if a) the ratio of their paragraphamounts (the ratio of their lengths in terms of para-470 2 4 6 8 10 12 14 16 1800.10.20.30.40.5log2(frequency)densityFigure 4: The normalized histogram and the esti-mated pdf of the image frequencies.graphs), b) the ratio of their clean text lengths (interms of tokens), and c) the Jaccard similarity co-efficient of their image lists, are higher than em-pirically predefined thresholds.More pairs are detected by examining structuresimilarity.
Since the XML files contain informa-tion about structure, content (i.e.
titles, headings,list items) and domain specificity (i.e.
paragraphswith the topic attribute), we use these files insteadof examining the similarity of the HTML source.A 3-dimensional feature vector is constructed foreach candidate pair of parallel documents.
Thefirst element in this vector is the ratio of their fin-gerprint lengths, the second is the ratio of theirsizes in paragraphs, and the third is the ratio of theedit distance of the fingerprints of the two docu-ments to the maximum fingerprint length.
Clas-sification of a pair as parallel is achieved using asoft-margin polynomial Support Vector Machinetrained with the positive and negative examplescollected during our previous work (Pecina et al2012).
Note that the dataset included only candi-date pairs that met the criteria regarding the ratioof paragraphs amounts and the ratio of text lengths,mentioned above.
As a result, negative instances(i.e.
pairs of documents that have similar structurebut are not real pairs) did not heavily outnumberpositive ones and thus the training was not imbal-anced (Akbani et al 2004).4 Bootstrapping the input of the focusedcrawlerIn the work presented in previous sections, we as-sumed that users had access to already existinglists of seed terms and URLs for the initializa-tion of the frontier and the classifier.
But what ifmanually compiled resources for a particular do-main/language(s) combination (e.g.
ES/FR termi-nology for endocrinology or lists of EN/DE websites related to floriculture) are impossible or diffi-cult to find?
Can we bootstrap such resources andprovide them to users for post-editing?
In this sec-tion, we present ongoing work towards this goalusing the category graph and the external links ofmultilingual editions of Wikipedia.We initialize the bootstrapping process bysearching for a term defining the domain of in-terest (e.g.
"ballet", "automotive accessories") inthe category graph of the EN wikipedia.
If a cat-egory is found, we recursively collect all pages inthis category and its subcategories for a predefineddepth.
For each page we extract its title and weconsider it a term that can participate in a list ofdomain-related seed terms.
We use a set of patternmatching rules that exclude certain titles like thoseof disambiguation and redirect pages.
Other rulesexclude titles that refer to lists of related pages ortitles that use upper case or title case and are proba-bly abbreviations and named entities, respectively.Obviously, in a different setting where, for exam-ple, a user is interested in discovering named enti-ties related to a domain, these titles should be han-dled differently.The next step involves utilizing the links fromeach EN page to articles in wikipedias written inother languages.
Based on which languages we areinterested in, we again consider each title a seedterm in language LANG, this time also storing theinformation that the term is also a LANG transla-tion of the EN term.During traversing the EN category graph andvisiting corresponding articles in other languages,we also populate a list of seed URLs for the fo-cused crawler, by keeping record of all links toURLs outside wikipedia.org.
At this stage,we have all necessary resources to initiate mono-lingual focused crawls in each language we are in-terested in.An optional last stage targets the automatic dis-covery of sites with multilingual content whereparallel documents can be extracted from.
Duringthis stage, we visit each of the external links wecollected and detect the language of the web pagethis link points to.
From this web page, we extractits links and examine whether the anchor text ofeach link matches a set of patterns indicating that48this link points to a translation (in a way similar tothe process described in Subsection 3.6).
If trans-lation links are found, we store the site as a candi-date for bilingual focused crawling.
Also, since itis common that links to multilingual editions of aweb site are not present in all of its pages, we re-peat the same process for the home page of the site.Notice that it is a task for the FC to detect whetherthese sites (or one of their sections) also containparallel documents in the targeted domain.In a first set of experiments following this ap-proach, we used September 2012 snapshots13 forEnglish, French, German, Greek, Portuguese andSpanish wikipedias (EN, FR, DE, EL, PT and ES,respectively).
Although we leave detailed eval-uation of created resources for future work, wepresent as example output a list of terms related to"Flowers" in Table 1.
Notice that, since the num-ber of articles of multilingual wikipedias variesconsiderably, the term list extracted for languageslike EL is, as expected, smaller compared, for ex-ample, to the 547 and 293 terms collected for ENand ES, respectively.
Finally, using the URLs ex-tracted from the articles on the "Flowers" domain,Table 2 contains a sample of web sites detected forcontaining relevant multilingual content.5 Evaluation ResultsIn order to assess the quality of the resources thatILSP-FC can produce, we evaluated it in a task ofacquiring pairs of parallel documents in Germanand Italian for the "Health & Safety at work" (Ar-beitsschutz/Sicurezza sul lavoro) domain.
We as-sume that this task is relatively difficult, i.e.
thatthe number of documents in this domain and pair oflanguages is relatively small in the web.
Overall,our system delivered 807 document pairs for H&S,containing 1.40 and 1.21 million tokens for IT andDE, respectively.
Numbers refer to tokens in themain content of the acquired web pages, i.e.
to to-kens in paragraphs without the attribute crawlinfo(see Subsection 3.7).A sample of the acquired corpora were evalu-ated against a set of criteria discussed in the fol-lowing subsections.
We randomly selected 103document pairs for manual inspection.
The samplesize was calculated according to a 95% confidencelevel and an at most 10% confidence interval.13We use the JavaWikipedia Library (Zesch et al 2008) toconvert each snapshot into a database that allows structuredaccess to several aspects of categories, articles, sections etc.5.1 ParallelnessThe number of the correctly identified parallel doc-ument pairs was obviously critical in this particularevaluation setting.
We focused on the precision ofthe pair detector module, since it is not feasible tocount how many pairs were missed.
In the subsetexamined, 94 and 4 document pairs were judged asparallel and not parallel, respectively.
The other5 pairs were considered borderline cases, wheremore than 20% of the sentences in one documentwere translated in the other.
Since about 95% ofthe crawled data are of good or sufficiently goodquality, this shows that they are usable for furtherprocessing, e.g.
for sentence alignment.5.2 Domain specificityWe next evaluated how many documents in the se-lected data fit the targeted domain in both the ITand the DE partitions.
The overall precision wasabout 77%, with 79 IT documents and 80DE docu-ments found relevant to the narrow domain chosenfor evaluation.Reported results on text-to-topic classificationsometimes score higher; however they neglect acritical factor of influence, namely the distance be-tween training and prediction datasets.
In the "realworld", scores between 75% and 85% are realisticto assume.
It should be mentioned that the preci-sion of the topic classifier strongly depends on thequality of the seed terms: by inspecting results,modifying the seed term list and re-crawling, re-sults could easily be improved further.5.3 Language identificationSince the language identifier is applied on everyparagraph of the main content of each web page,we examined how many of the paragraphs havebeen marked correctly.
Overall, 5223 and 4814paragraphs of IT and DE documents were checkedand only 13 and 65wrong assignments were found,respectively.Most errors (about 80%) were found in a sin-gle document with a lot of tokens denoting chem-ical substances that seem to confuse the languageidentifier.
When excluding this document, figuresrise to 99,67% and 99,95% for the DE and IT par-titions, respectively.
The rest of the errors mainlyoccurred in paragraphs containing sentences in dif-ferent languages.49EN: 547 DE: 255 EL: 22 ES: 293 FR: 286 IT: 143 PT: 164Gardenia Gardenien ????????
Gardenia Gard?nia Gardenia GardeniaCalendula Ringelblumen ??????????
Calendula Calendula Calendula CalendulaLilium Lilien ?????
Lilium Lys Lilium L?rioPeony Pfingstrosen ???????
Paeoniaceae Pivoine Paeonia PaeoniaceaeTulip Tulpen ???????
Tulipa Tulipe Tulipa TulipaFlower Bl?te ?????
Flor Fleur Fiore FlorCrocus Krokusse ??????
Crocus Crocus Crocus CrocusAnemone Windr?schen ???????
Anemone An?mone Anemone AnemoneTable 1: Sample seed terms for the "Flowers" domain in 7 languages, collected automatically frommulti-lingual editions ofWikipedia.
The header of the table refers to the total terms collected for each language.Wikipedia article Seed URL WebSite LangsEN: Omphalodes_verna http://goo.gl/msyIc http://www.luontoportti.com de,en,es,frES: Tropaeolum http://goo.gl/Ec5uK http://www.chileflora.com de,en,esEN: Erythronium americanum http://goo.gl/nEP2L http://wildaboutgardening.org en,frDE: Nickendes_Leimkraut http://goo.gl/nuHNe http://www.wildblumen.at de,en,ptDE: Titanenwurz http://goo.gl/rLl9W http://www.wilhelma.de de,enTable 2: Automatically detected web sites with multilingual content related to the "Flowers" domain.Column 1 presents the original LANG.wikipedia.org article from which the (shortened for readabilitypurposes) seed URLs in column 2 were extracted.
The seed URLs led to the 3rd column web sites, inwhich content in the languages of the 4th column was found.5.4 Boilerplate removalFor this evaluation aspect, we evaluated howmany"good" paragraphs were judged to be boilerplate,and how many "bad" paragraphs were missed.
Weexamined 23178 and 23176 paragraphs of IT andDE documents and found 2326 and 2591 errorswith an overall error rate around 10%.
It shouldbe noted that different strategies for boilerplate re-moval can be followed.
One "classical" option is toremove everything that does not belong to the text,i.e.
headers, advertisements etc.
that "frame" realcontent.
Another option is to attempt to removeeverything which is irrelevant for MT sentencealignment; this goes beyond the first approach as italso removes short textual chunks, copyright dis-claimers, etc.
Most of the errors reported hereweremainly due to this difference; i.e.
they were para-graphs that were deemed not usable for MT align-ment.6 Conclusions and future workIn this paper we described and evaluated ILSP-FC,a system for mining domain-specific monolingualand bilingual corpora from the web.
The systemis available as open-source and is modular in thesense that each of its components can be easily sub-stituted with similar software performing the samefunctionalities.
The crawler can also be tested viaweb services that allow the user to perform exper-iments without the need to install it.We have already used the crawler in producingmonolingual and parallel corpora and other deriva-tive resources.
Evaluation has shown that the sys-tem can be used effectively in collecting resourcesof high quality, provided that the user can initial-ize it with lists of seed terms and URLs that can beeasily found on the web.
For domains for whichno similar lists are available, we presented ongo-ing work for bootstrapping them frommultilingualeditions of Wikipedia.
Future work includes eval-uation and improvement of the bootstrapping com-ponent, more sophisticated methods for text clas-sification, and grouping of collected data based ongenre.AcknowledgmentsWork by the first two authors was partially fundedby the European Union QTLaunchPad (FP7,Grant 296347) and Abu-MaTran (FP7-People-IAPP, Grant 324414) projects.
An initial versionof this work was produced during the EU Panaceaproject (FP7-ICT, Grant 248064).50ReferencesRehan Akbani, Stephen Kwek, and Nathalie Japkow-icz.
2004.
Applying support vector machines toimbalanced datasets.
In In Proceedings of the 15thEuropean Conference onMachine Learning (ECML,pages 39--50.Jos?
Jo?o Almeida and Alberto Sim?es.
2010.
Auto-matic parallel corpora and bilingual terminology ex-traction from parallel websites.
In 3rd Workshop onBuilding and Using Comparable Corpora .Ethem Alpaydin.
2010.
Introduction to MachineLearning.
The MIT Press, 2nd edition.Luciano Barbosa, Vivek Kumar Rangarajan Sridhar,Mahsa Yarmohammadi, and Srinivas Bangalore.2012.
Harvesting parallel text in multiple languageswith limited supervision.
In COLING, pages 201--214.Marco Baroni, Adam Kilgarriff, Jan Pomik?lek, andPavel Rychl?.
2006.
WebBootCaT: Instant Domain-Specific Corpora to Support Human Translators.In Proceedings of the 11th Annual Conference ofEAMT, pages 47--252, Norway.Marco Baroni, Francis Chantree, Adam Kilgarriff, andSerge Sharoff.
2008.
Cleaneval: a competition forcleaning web pages.
In LREC'08.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
TheWaCkyWideWeb: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209--226.Adrian W. Bowman and Adelchi Azzalini.
1997.Applied smoothing techniques for data analysis:the kernel approach with S-Plus illustrations, vol-ume 18.
Oxford University Press.Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.2004.
Discovering parallel text from theWorldWideWeb.
In Proceedings of ACSW Frontiers '04, vol-ume 32, pages 157--161, Darlinghurst, Australia.Alain D?silets, Benoit Farley, Marta Stojanovic, andGenevi?ve Patenaude.
2008.
WeBiText: BuildingLarge Heterogeneous Translation Memories fromParallel Web Content.
In Proceedings of Translat-ing and the Computer (30), London, UK.Miquel Espl?-Gomis and Mikel L. Forcada.
2010.Combining Content-Based and URL-Based Heuris-tics to Harvest Aligned Bitexts from MultilingualSites with Bitextor.
The Prague Bulletin of Math-emathical Lingustics, 93:77--86.Gumwon Hong, Chi-Ho Li, Ming Zhou, and Hae-Chang Rim.
2010.
An empirical study on web min-ing of parallel data.
In Proceedings of the 23rd COL-ING, pages 474--482.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the web as corpus.Computational Linguistics, 29(3):333--348.Jian-Yun Nie, Michel Simard, Pierre Isabelle, andRichard Durand.
1999.
Cross-language informationretrieval based on parallel texts and automatic min-ing of parallel texts from the Web.
In Proceedingsof the 22nd annual international ACM SIGIR con-ference on research and development in informationretrieval, pages 74--81, New York.Christopher Olston and Marc Najork.
2010.
Webcrawling.
Found.
Trends Inf.
Retr., 4(3):175--246.Pavel Pecina, Antonio Toral, Vassilis Papavassiliou,Prokopis Prokopidis, and Josef van Genabith.
2012.Domain adaptation of statistical machine translationusing web-crawled resources: a case study.
In Pro-ceedings of the 16th Annual Conference of EAMT,pages 145--152, Trento, Italy.Xiaoguang Qi and Brian D. Davison.
2009.
Web pageclassification: Features and algorithms.
ACM Com-puting Surveys, 41:11--31.Philip Resnik and Noah A. Smith.
2003.
The Web as aparallel corpus.
Computational Linguistics, 29:349--380.Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A dom tree alignment model for mining paral-lel data from the web.
In COLING/ACL-2006, pages489--496.Miroslav Spousta, Michal Marek, and Pavel Pecina.2008.
Victor: the Web-Page Cleaning Tool.
In Pro-ceedings of the 4th Web as Corpus Workshop - Canwe beat Google?, pages 12--17, Marrakech.Padmini Srinivasan, Filippo Menczer, and GautamPant.
2005.
A General Evaluation Framework forTopical Crawlers.
Information Retrieval, 8:417--447.Martin Theobald, Jonathan Siddharth, and AndreasPaepcke.
2008.
Spotsigs: robust and efficient nearduplicate detection in large web collections.
In Pro-ceedings of the 31st annual international ACM SI-GIR conference on research and development in in-formation retrieval, pages 563--570.Masao Utiyama, Daisuke Kawahara, Keiji Yasuda, andEiichiro Sumita.
2009.
Mining parallel texts frommixed-language web pages.
InMT Summit.Torsten Zesch, Christof M?ller, and Iryna Gurevych.2008.
Extracting lexical semantic knowledge fromwikipedia and wiktionary.
In Proceedings of the 6thInternational Conference on Language Resourcesand Evaluation, Marrakech.Ying Zhang, Ke Wu, Jianfeng Gao, and Phil Vines.2006.
Automatic Acquisition of Chinese-EnglishParallel Corpus from the Web.
In Proceedings ofthe 28th European Conference on Information Re-trieval, pages 420--431.51
