Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUser Input and Interactions on Microsoft Research ESL AssistantClaudia Leacock Michael Gamon Chris BrockettButler Hill Group Microsoft Research Microsoft ResearchP.O.
Box 935 One Microsoft Way One Microsoft WayRidgefield, CT, 06877, USA Redmond, WA, 98052, USA Redmond, WA, 98052, USAclaudia.leacock@gmail.com mgamon@microsoft.com chrisbkt@microsoft.comAbstractESL Assistant is a prototype web-based writ-ing-assistance tool that is being developed forEnglish Language Learners.
The system fo-cuses on types of errors that are typicallymade by non-native writers of American Eng-lish.
A freely-available prototype was dep-loyed in June 2008.
User data from thissystem are manually evaluated to identifywriting domain and measure system accuracy.Combining the user log data with the eva-luated rewrite suggestions enables us to de-termine how effectively English languagelearners are using the system, across ruletypes and across writing domains.
We findthat repeat users typically make informedchoices and can distinguish correct sugges-tions from incorrect.1 IntroductionMuch current research in grammatical error detec-tion and correction is focused on writing by Eng-lish Language Learners (ELL).
The MicrosoftResearch ESL Assistant is a web-based proofread-ing tool designed primarily for ELLs who are na-tive speakers of East-Asian languages.
Initialsystem development was informed by pre-existingELL error corpora, which were used both to identi-fy common ELL mistakes and to evaluate systemperformance.
These corpora, however, werecreated from data collected under arguably artifi-cial classroom or examination conditions, leavingunresolved the more practical question as towhether the ESL Assistant can actually help a per-son who produced the text to improve their Englishlanguage writing skills in course of more realisticeveryday writing tasks.In June of 2008, a prototype version of this sys-tem was made freely available as a web service12 Related Work.Both the writing suggestions that visitors see andthe actions that they then take are recorded.
Asthese more realistic data begin to accumulate, wecan now begin to answer the above question.Language learner error correction techniques  typi-cally fall into either of two categories: rule-basedor data-driven.
Eeg-Olofsson and Knutsson (2003)report on a rule-based system that detects and cor-rects preposition errors in non-native Swedish text.Rule-based approaches have also been used to pre-dict definiteness and indefiniteness of Japanesenoun phrases as a preprocessing step for Japaneseto English machine translation (Murata and Nagao1993; Bond et al 1994; Heine, 1998), a task that issimilar to the prediction of English articles.
Morerecently, data-driven approaches have gainedpopularity and been applied to article prediction inEnglish (Knight and Chander 1994; Minnen et al2000; Turner and Charniak 2007), to an array ofJapanese learners?
errors in English (Izumi et al2003), to verb errors (Lee and Seneff, 2008), andto article and preposition correction in texts writtenby non-native ELLs (Han et al 2004, 2006; Nagataet al 2005; Nagata et al 2006; De Felice and Pul-man, 2007; Chodorow et al 2007; Gamon et al2008, 2009; Tetreault and Chodorow, 2008a).1  http://www.eslassistant.com733 ESL AssistantESL Assistant takes a hybrid approach that com-bines statistical and rule-based techniques.
Ma-chine learning is used for those error types that aredifficult to identify and resolve without taking intoaccount complex contextual interactions, like ar-ticle and preposition errors.
Rule-based approacheshandle those error types that are amenable to simp-ler solutions.
For example, a regular expression issufficient for identifying when a modal is (incor-rectly) followed by a tensed verb.The output of all modules, both machine-learnedand rule-based, is filtered through a very large lan-guage model.
Only when the language model findsthat the likelihood of the suggested rewrite is suffi-ciently larger than the original text is a suggestionshown to the user.
For a detailed description ofESL Assistant?s architecture, see Gamon et al(2008, 2009).Although this and the systems cited in section 2are designed to be used by non-native writers, sys-tem performance is typically reported in relation tonative text ?
the prediction of a preposition, forexample, will ideally be consistent with usage innative, edited text.
An error is counted each timethe system predicts a token that differs from theobserved usage and a correct prediction is countedeach time the system predicts the usage that occursin the text.
Although somewhat artificial, this ap-proach to evaluation offers the advantages of beingfully automatable and having abundant quantitiesNounRelated(61%)Articles (ML)We have just checked *the our stock.life is *journey/a journey, travel it well!I think it 's *a/the best way to resolve issues like this.Noun NumberLondon is one of the most attractive *city/cities in the world.You have to write down all the details of each *things/thing to do.Conversion always takes a lot of *efforts/effort.Noun Of NounPlease send the *feedback of customer/customer feedback to me bymail.PrepositionRelated(27%) Preposition (ML)I'm *on home today, call me if you have a problem.It seems ok and I did not pay much attention *on/to it.Below is my contact, looking forward *your/to your response, thanks!Verb and PrepositionBen is involved *this/in this transaction.I should *to ask/ask a rhetorical question ?But I?ll think *it/about it a second time.VerbRelated(10%)Gerund / Infinitive(ML)He got me *roll/to roll up my sleeve and make a fist.On Saturday, I with my classmate went *eating/to eat.After *get/getting a visa, I want to study in New York.Auxiliary Verb (ML)To learn English we should *be speak/speak  it as much as possible .Hope you will *happy/be happy in Taiwan .what *is/do you want to say?Verb formationIf yes, I will *attached/attach and resend to Geoff .The time and setting are *display/displayed at the same time.You had *order/ordered 3 items ?
this time.I am really *hope/hoping to visit UCLA.Cognate/Verb Con-fusionWe cannot *image/imagine what the environment really is at the siteof end user .Irregular Verbs I *teached/taught him all the things that I know ?AdjRelated(2%) Adjective Confu-sionsShe is very *interesting/interested in the problem.So *Korea/Korean Government is intensively fostering trade .?
and it is *much/much more reliable than your Courier Service.Adjective order Employing the *Chinese ancient/ancient Chinese proverb, that is  ?Table 1: ESL Assistant grammatical error modules.
ML modules are machine learned.74of edited data readily available.
With respect toprepositions and articles, the ESL Assistant's clas-sifiers achieve state-of-the-art performance whencompared to results reported in the literature (Ga-mon et al 2008), inasmuch as comparison is possi-ble when the systems are evaluated on differentsamples of native text.
For articles, the system had86.76% accuracy as compared to 86.74% reportedby Turner and Charniak (2007), who have the mostrecently reported results.
For the harder problem ofprepositions, ESL Assistant?s accuracy is compara-ble to those reported by Tetreault and Chodorow(2008a) and De Felice and Pulman (2007).3.1 Error TypesThe ELL grammatical errors that  ESL Assistanttries to correct were distilled from analysis of themost frequent errors made in Chinese and JapaneseEnglish language learner corpora (Gui and Yang,2001; Izumi et al 2004).
The error types are shownin Table 1: modules identified with ML are ma-chine-learned, while the remaining modules arerule-based.
ESL Assistant does not attempt to iden-tify those errors currently found by MicrosoftWord?, such as subject/verb agreement.ESL Assistant further contains a component tohelp address lexical selection issues.
Since thismodule is currently undergoing major revision, wewill not report on the results here.3.2 System DevelopmentWhereas evaluation on native writing is essentialfor system development and enables us to compareESL Assistant performance with that of other re-ported results, it tells us little about how the systemwould perform when being used by its true targetaudience ?
non-native speakers of English engagedin real-life writing tasks.
In this context, perfor-mance measurement inevitably entails manualevaluation, a process that is notoriously time con-suming, costly and potentially error-prone.
Humaninter-rater agreement is known to be problematicFigure 1: Screen shot of ESL Assistant75on this task: it is likely to be high in the case ofcertain user error types, such as over-regularizedverb inflection (where the system suggests replac-ing ?writed?
with ?wrote?
), but other error typesare difficult to evaluate, and much may hinge uponwho is performing the evaluation: Tetreault andChodorow (2008b) report that for the annotation ofpreposition errors ?using a single rater as a goldstandard, there is the potential to over- or under-estimate precision by as much as 10%.
?With these caveats in mind, we employed a sin-gle annotator to evaluate system performance onnative data from the 1-million-word ChineseLearner?s of English corpus (Gui and Yang, 2001;2003).
Half of the corpus was utilized to informsystem development, while the remaining half washeld back for "unseen" evaluation.
While the abso-lute numbers for some modules are more reliablethan for others, the relative change in numbersacross evaluations has proven a beneficialyardstick of improved or degraded performance inthe course of development.3.3 The User Interface and Data CollectionFigure 1 shows the ESL Assistant user interface.When a visitor to the site types or pastes text intothe box provided and clicks the "Check" button,the text is sent to a server for analysis.
Any loca-tions in the text that trigger an error flag are thendisplayed as underscored with a wavy line (knownas a "squiggle").
If the user hovers the mouse overa squiggle, one or more suggested rewrites are dis-played in a dropdown list.
Then, if the user hoversover one of these suggestions, the system launchesparallel web searches for both original and rewritephrases in order to allow the user to compare real-word examples found on the World Wide Web.
Toaccept a suggestion, the user clicks on the sug-gested rewrite, and the text is emended.
Each ofthese actions, by both system and user, are loggedon the server.Since being launched in June, 2008, ESL Assis-tant has been visited over 100,000 times.
Current-ly, the web page is being viewed between one totwo thousand times every day.
From these numbersalone it seems safe to conclude that there is muchpublic interest in an ESL proofreading tool.Fifty-three percent of visitors to the ESL Assis-tant web site are from countries in East Asia ?
itsprimary target audience ?
and an additional 15%are from the United States.
Brazil, Canada, Ger-many, and the United Kingdom each account forabout 2% of the site?s visitors.
Other countriesrepresented in the database each account for 1% orless of all those who visit the site.3.4 Database of User InputUser data are collected so that system performancecan be evaluated on actual user input ?
as opposedto running pre-existing learner corpora through thesystem.
User data provide invaluable insight intowhich rewrite suggestions users spend time view-ing, and what action they subsequently take on thebasis of those suggestions.These data must be screened, since not all of thetextual material entered by users in the web site isvalid learner English language data.
As with anypublicly deployed web service, we find that nu-merous users will play with the system, enteringnonsense strings or copying text from elsewhere onthe website and pasting it into the text box.To filter out the more obvious non-English data,we eliminate input that contains, for example, noalphabetic characters, no vowels/consonants in asentence, or no white space.
?Sentences?
consist-ing of email subject lines are also removed, as areall the data entered by the ESL Assistant develop-ers themselves.
Since people often enter the samesentence many times within a session, we also re-move repetitions of identical sentences within asingle session.Approximately 90% of the people who have vi-sited the web site visit it once and never return.This behavior is far from unusual on the web,where site visits may have no clear purpose beyondidle curiosity.
In addition, some proportion of visi-tors may in reality be automated "bots" that can benearly indistinguishable from human visitors.Nevertheless, we observe a significant numberof repeat visitors who return several times to usethe system to proofread email or other writing, andthese are the users that we are intrinsically interest-ed in.
To measure performance, we therefore de-cided to evaluate on data collected from users whologged on and entered plausibly English-like texton at least four occasions.
As of 2/10/2009, thefrequent user database contained 39,944 session-unique sentences from 578 frequent users in 5,305sessions.76Data from these users were manually annotatedto identify writing domains as shown in Table 2.Fifty-three percent of the data consists of peopleproofreading email.2The dominance of email datais presumably due to an Outlook plug-in that isavailable on the web site, and automates copyingemail content into the tool.
The non-technical do-main consists of student essays, material posted ona personal web site, or employees writing abouttheir company ?
for example, its history orprocesses.
The technical writing is largely confe-rence papers or dissertations in the fields of, forexample, medicine and computer science.
The?other?
category includes lists and resumes (a writ-ing style that deliberately omits articles and gram-matical subjects), as well as text copied fromonline newspapers or other media and pasted in.Writing Domain PercentEmail 53%Non-technical / essays 24%Technical / scientific 14%Other (lists, resumes, etc) 4%Unrelated sentences 5%Table 2: Writing domains of frequent usersSessions categorized as ?unrelated sentences?
typi-cally consist of a series of short, unrelated sen-tences that each contain one or more errors.
Theseusers are testing the system to see what it does.While this is a legitimate use of any grammarchecker, the user is unlikely to be proofreading hisor her writing, so these data are excluded fromevaluation.4 System Evaluation & User InteractionsWe are manually evaluating the rewrite sugges-tions that ESL Assistant generated in order to de-termine both system accuracy and whether useracceptances led to an improvement in their writing.These categories are shown in Table 3.
Note thatresults reported for non-native text look very dif-ferent from those reported for native text (dis-cussed in Section 3) because of the neutralcategories which do not appear in the evaluation ofnative text.
Systems reporting 87% accuracy onnative text cannot achieve anything near that on2 These are anonymized to protect user privacy.non-native ELL text because almost one third ofthe flags fall into a neutral category.In 51% of the 39,944 frequent user sentences,the system generated at least one grammatical errorflag, for a total of 17,832 flags.
Thirteen percent ofthe time, the user ignored the flags.
The remaining87% of the flags were inspected by the user, and ofthose, the user looked at the suggested rewriteswithout taking further action 31% of the time.
For28% of the flags, the user hovered over a sugges-tion to trigger a parallel web search but did notaccept the proposed rewrite.
Nevertheless, 41% ofinspected rewrites were accepted, causing the orig-inal string in the text to be revised.
Overall, theusers inspected about 15.5K suggested rewrites toaccept about 6.4K.
A significant number of usersappear to be inspecting the suggested revisions andmaking deliberate choices to accept or not accept.The next question is: Are users making the rightchoices?
To help answer this question, 34% of theuser sessions have been manually evaluated forsystem accuracy ?
a total of approximately 5.1Kgrammatical error flags.
For each error categoryand for the three major writing domains, we:Evaluation Subcategory: DescriptionGoodCorrect flag: The correction fixes aproblem in the user input.NeutralBoth Good: The suggestion is a legiti-mate alternative to well-formed originalinput: I like working/to work.Misdiagnosis: the original input con-tained an error but the suggested rewriteneither improves nor further degradesthe input: If you have fail machine onhand.Both Wrong: An error type is correctlydiagnosed but the suggested rewritedoes not correct the problem: can yougive me suggestionNon-ascii: A non-ascii or text markupcharacter is in the immediate context.. (suggests the in-stead of a)BadFalse Flag: The suggestion resulted inan error or would otherwise lead to adegradation over the original user input.Table 3: Evaluation categories771.
Calculated system accuracy for all flags,regardless of user actions.2.
Calculated system accuracy for only thoserewrites that the user accepted3.
Compared the ratio of good to bad flags.Results for the individual error categories areshown in Figure 2.
Users consistently accept agreater proportion of good suggestions than theydo bad ones across all error categories.
This ismost pronounced for the adjective-related modules,where the overall rate of good suggestions im-proved 17.6% after the user made the decision toaccept a  suggestion, while the system?s false posi-tive rate dropped 14.1% after the decision.
For thenoun-related modules, the system?s most produc-Rewrite Suggestion Evaluation Accepted SuggestionNounRelatedModules3,017 suggestions972 acceptancesPrepositionRelatedModules1,465 suggestions479 acceptancesVerbRelatedModules469 suggestions157 acceptancesAdjectiveRelatedModules125 suggestions40 acceptancesFigure 2: User interactions by module categorygood56%neut28%bad16%good63%neut26%bad11%good37%neut39%bad24% good45%neut42%bad13%good62%neut32%bad6%good72%neut25%bad3%good45%neut32%bad23%good63%neut28%bad9%78tive modules, the overall good flag rate increasedby 7% while the false positive rate dropped 5%.All differences in false positive rates are statistical-ly significant in Wilcoxon?s signed-ranks test.When all of the modules are evaluated acrossthe three major writing domains, shown in figure 3,the same pattern of user discrimination betweengood and bad flags holds.
This is most evident inthe technical writing domain, where the overallrate of good suggestions improved 13.2% afteraccepting the suggestion and the false positive ratedropped 15.1% after the decision.
It is least markedfor the essay/nontechnical writing domain.
Herethe overall good flag rate increased by only .3%while the false positive rate dropped 1.6%.
Again,all of the differences in false positive rates are sta-tistically significant in Wilcoxon?s signed-rankstest.
These findings are consistent with those forthe machine learned articles and prepositions mod-ules in the email domain (Chodorow et al underreview).A probable explanation for the differences seenacross the domains is that those users who areproofreading non-technical writing are, as a whole,less proficient in English than the users who arewriting in the other domains.
Users who are proof-reading technical writing are typically writing adissertation or paper in English and therefore tendto relatively fluent in the language.
The email do-main comprises people who are confident enoughin their English language skills to communicatewith colleagues and friends by email in English.With the essay/non-technical writers, it often is notclear who the intended audience is.
If there is anyindication of audience, it is often an instructor.
Us-ers in this domain appear to be the least English-Rewrite Suggestion Evaluation Accepted SuggestionEmailDomain2,614 suggestions772 acceptancesNon-TechnicalWritingDomain1,437 suggestions684 acceptancesTechnicalWritingDomain1,069 suggestions205 acceptancesFigure 3: User interactions by writing domaingood53%neutral32%bad15%good63%neutral28%bad9%good56%neutral32%bad12%good56%neutral34%bad10%good38%neutral28%bad34%good52%neutral29%bad19%79language proficient of the ESL Assistant users, so itis unsurprising that they are less effective in dis-criminating between good and bad flags than theirmore proficient counterparts.
Thus it appears thatthose users who are most in need of the system arebeing helped by it least ?
an important direction forfuture work.Finally, we look at whether the neutral flags,which account for 29% of the total flags, have anyeffect.
The two neutral categories highlighted inTable 3, flags that either misdiagnose the error orthat diagnose it but do not correct it, account for74% of ESL Assistant?s neutral flags.
Althoughthese suggested rewrites do not improve the sen-tence, they do highlight an environment that con-tains an error.
The question is: What is the effect ofidentifying an error when the rewrite doesn?t im-prove the sentence?To estimate this, we searched for cases whereESL Assistant produced a neutral flag and, thoughthe user did not accept the suggestion, a revisedsentence that generated no flag was subsequentlysubmitted for analysis.
For example, one user en-tered: ?This early morning  i got a from head office??.
ESL Assistant suggested deleting from, whichdoes not improve the sentence.
Subsequently, inthe same session, the user submitted, ?This earlymorning I heard from the head office ??
In thisinstance, the system correctly identified the loca-tion of an error.
Moreover, even though the sug-gested rewrite was not a good solution, theinformation was sufficient to enable the user to fixthe error on his or her own.Out of 1,349 sentences with neutral suggestionsthat were not accepted, we identified (using afuzzy match) 215 cases where the user voluntarilymodified the sentence so that it contained no flag,without accepting the suggestion.
In 44% of thesecases, the user had simply typed in the suggestedcorrection instead of accepting it ?
indicating thattrue acceptance rates might be higher than we orig-inally estimated.
Sixteen percent of the time, thesentence was revised but there remained an errorthat the system failed to detect.
In the other 40% ofcases, the voluntary revision improved the sen-tence.
It appears that merely pointing out the poss-ible location of an error to the user is oftensufficient to be helpful.5 ConclusionIn conclusion, judging from the number of peoplewho have visited the ESL Assistant web site, thereis considerable interest in ESL proofreading toolsand services.When using the tool to proofread text, users donot accept the proposed corrections blindly ?
theyare selective in their behavior.
More importantly,they are making informed choices ?
they can dis-tinguish correct suggestions from incorrect ones.Sometimes identifying the location of an error,even when the solution offered is wrong, itself ap-pears sufficient to cause the user to repair a prob-lem on his or her own.
Finally, the userinteractions that we have recorded indicate thatcurrent state-of-the-art grammatical error correc-tion technology has reached a point where it can behelpful to English language learners in real-worldcontexts.AcknowledgmentsWe thank Bill Dolan, Lucy Vanderwende, JianfengGao, Alexandre Klementiev and Dmitriy Belenkofor their contributions to the ESL Assistant system.We are also grateful to the two reviewers of thispaper who provided valuable feedback.ReferencesFrancis Bond, Kentaro Ogura, and Satoru Ikehara.
1994.Countability and number in Japanese to English ma-chine translation.
In Proceedings of the 15th Confe-rence on Computational Linguistics (pp.
32-38).Kyoto, Japan.Martin Chodorow, Michael Gamon, and Joel Tetreault.Under review.
The utility of grammatical error detec-tion systems for English language learners: Feedbackand Assessment.Martin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involving pre-positions.
In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions (pp.
25-30).
Pra-gue, Czech Republic.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the Fourth ACL-SIGSEM Workshopon Prepositions (pp.
45-50).
Prague, Czech Republic.Jens Eeg-Olofsson and Ola Knutsson.
2003.
Automaticgrammar checking for second language learners ?
theuse of prepositions.
Proceedings of NoDaLiDa 2003.Reykjavik, Iceland.80Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-dre Klementiev, William B. Dolan, Dmitriy Belenko,and Lucy Vanderwende.
2008.
Using contextualspeller techniques and language modeling for ESLerror correction.
In Proceedings of the Third Interna-tional Joint Conference on Natural LanguageProcessing (pp.
449-455).
Hyderabad, India.Michael Gamon, Claudia Leacock, Chris Brockett, Wil-liam B. Dolan,  Jianfeng Gao, Dmitriy Belenko, andAlexandre Klementiev.
2009.
Using statistical tech-niques and web search to correct ESL errors.
To ap-pear in CALICO Journal, Special Issue on AutomaticAnalysis of Learner Language.Shicun Gui and Huizhong Yan.
2001.
Computer analy-sis of Chinese learner English.
Presentation at HongKong University of Science and Technolo-gy.http://lc.ust.hk/~centre/conf2001/keynote/subsect4/yang.pdf.Shicun Gui and Huizhong Yang.
(Eds.).
2003.
Zhong-guo Xuexizhe Yingyu Yuliaohu.
(Chinese LearnerEnglish Corpus).
Shanghai Waiyu Jiaoyu Chubanshe.
(In Chinese).Na-Rae Han, Martin Chodorow, and Claudia Leacock2004).
Detecting errors in English article usage witha maximum entropy classifier trained on a large, di-verse corpus.
In Proceedings of the 4th Interna-tional Conference on Language Resources andEvaluation.
Lisbon, Portugal.Na-Rae Han, Martin Chodorow, and Claudia Leacock2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2), 115-129.Julia E. Heine.
1998.
Definiteness predictions for Japa-nese noun phrases.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics (pp.
519-525).
Montreal,Canada.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi, and Hitoshi Isahara.
2003.
Automaticerror detection in the Japanese learners' English spo-ken data.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics (pp.145-148).
Sapporo, Japan.Kevin Knight and Ishwar Chander,.
1994.
Automaticpostediting of documents.
In Proceedings of the 12thNational Conference on Artificial Intelligence (pp.779-784).
Seattle: WA.John Lee.
2004.
Automatic article restoration.
In Pro-ceedings of the Human Language Technology Confe-rence of the North American Chapter of theAssociation for Computational Linguistics (pp.
31-36).
Boston, MA.John Lee and Stephanie Seneff.
2008.
Correcting mi-suse of verb forms.
In Proceedings of ACl-08/HLT(pp.
174-182).
Columbus, OH.Guido Minnen, Francis Bond, and Anne Copestake.2000.
Memory-based learning for article generation.In Proceedings of the Fourth Conference on Compu-tational Natural Language Learning and of theSecond Learning Language in Logic Workshop (pp.43-48).
Lisbon, Portugal.Masaki Murata and Makoto Nagao.
1993.
Determina-tion of referential property and number of nouns inJapanese sentences for machine translation into Eng-lish.
In Proceedings of the Fifth International Confe-rence on Theoretical and Methodological Issues inMachine Translation (pp.
218-225).
Kyoto, Japan.Ryo Nagata, Takahiro Wakana, Fumito Masui, AtsuiKawai, and Naoki Isu.
2005.
Detecting article errorsbased on the mass count distinction.
In R. Dale, W.Kam-Fie, J. Su and O.Y.
Kwong (Eds.)
Natural Lan-guage Processing - IJCNLP 2005, Second Interna-tional Joint Conference Proceedings (pp.
815-826).New York: Springer.Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, andNaoki Isu.
2006.
A feedback-augmented method fordetecting errors in the writing of learners of English.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(pp.
241-248).
Sydney, Australia.Joel Tetreault and Martin Chodorow.
2008a.
The upsand downs of preposition error detection in ESL.COLING.
Manchester, UK.Joel Tetreault and Martin Chodorow.
2008b.
Nativejudgments of non-native usage: Experiments in pre-position error detection.
In Proceedings of the Work-shop on Human Judgments in ComputationalLinguistics, 22nd International Conference on Com-putational Linguistics (pp 43-48).
Manchester, UK.Jenine Turner and Eugene Charniak.
2007.
Languagemodeling for determiner selection.
In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Companion Volume, ShortPapers (pp.
177-180).
Rochester, NY.81
