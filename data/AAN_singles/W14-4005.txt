Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 43?50,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsApplying HMEANT to English-Russian TranslationsAlexander Chuchunkov Alexander Tarelkin{madfriend,newtover,galinskaya}@yandex-team.ruYandex LLCLeo Tolstoy st. 16, Moscow, RussiaIrina GalinskayaAbstractIn this paper we report the results offirst experiments with HMEANT (a semi-automatic evaluation metric that assessestranslation utility by matching semanticrole fillers) on the Russian language.
Wedeveloped a web-based annotation inter-face and with its help evaluated practica-bility of this metric in the MT researchand development process.
We studied reli-ability, language independence, labor costand discriminatory power of HMEANTby evaluating English-Russian translationof several MT systems.
Role labelingand alignment were done by two groupsof annotators - with linguistic backgroundand without it.
Experimental results werenot univocal and changed from very highinter-annotator agreement in role labelingto much lower values at role alignmentstage, good correlation of HMEANT withhuman ranking at the system level sig-nificantly decreased at the sentence level.Analysis of experimental results and anno-tators?
feedback suggests that HMEANTannotation guidelines need some adapta-tion for Russian.1 IntroductionMeasuring translation quality is one of the mostimportant tasks in MT, its history began long agobut most of the currently used approaches andmetrics have been developed during the last twodecades.
BLEU (Papineni et al., 2002), NIST(Doddington, 2002) and METEOR (Banerjee andLavie, 2005)metric require reference translationto compare it with MT output in fully automaticmode, which resulted in a dramatical speed-up forMT research and development.
These metrics cor-relate with manual MT evaluation and provide re-liable evaluation for many languages and for dif-ferent types of MT systems.However, the major problem of popular MTevaluation metrics is that they aim to capture lexi-cal similarity of MT output and reference transla-tion (fluency), but fail to evaluate the semantics oftranslation according to the semantics of reference(adequacy) (Lo and Wu, 2011a).
An alternativeapproach that is worth mentioning is the one pro-posed by Snover et al.
(2006), known as HTER,which measures the quality of machine translationin terms of post-editing.
This method was provedto correlate well with human adequacy judgments,though it was not designed for a task of gisting.Moreover, HTER is not widely used in machinetranslation evaluation because of its high labor in-tensity.A family of metrics called MEANT was pro-posed in 2011 (Lo and Wu, 2011a), which ap-proaches MT evaluation differently: it measureshow much of an event structure of reference doesmachine translation preserve, utilizing shallow se-mantic parsing (MEANT metric) or human anno-tation (HMEANT) as a gold standard.We applied HMEANT to a new language ?Russian ?
and evaluated the usefulness of met-ric.
The practicability for the Russian languagewas studied with respect to the following criteriaprovided by Birch et al.
(2013):Reliability ?
measured as inter-annotator agree-ment for individual stages of evaluation task.Discriminatory Power ?
the correlation ofrankings of four MT systems (by manual evalu-ation, BLEU and HMEANT) measured on a sen-tence and test set levels.Language Independence ?
we collected theproblems with the original method and guidelinesand compared these problems to those reported byBojar and Wu (2012) and Birch et al.
(2013).Efficiency ?
we studied the labor cost of anno-tation task, i. e. average time required to evaluate43translations with HMEANT.
Besides, we testedthe statement that semantic role labeling (SRL)does not require experienced annotators (in ourcase, with linguistic background).Although the problems of HMEANT were out-lined before (by Bojar and Wu (2012) and Birchet al.
(2013)) and several improvements were pro-posed, we decided to step back and conduct ex-periments with HMEANT in its original form.
Nochanges to the metric, except for the annotationinterface enhancements, were made.This paper has the following structure.
Sec-tion 2 reports the previous experiments withHMEANT; section 3 summarizes the methods be-hind HMEANT; section 4 ?
the settings for ourown experiments; sections 5 and 6 are dedicatedto results and discussion.2 Related WorkSince the beginning of the machine translation erathe idea of semantics-driven approach for transla-tion wandered around in the MT researchers com-munity (Weaver, 1955).
Recent works by Lo andWu (2011a) claim that this approach is still per-spective.
These works state that in order for ma-chine translation to be useful, it should convey theshallow semantic structure of the reference trans-lation.2.1 MEANT for Chinese-EnglishTranslationsThe original paper on MEANT (Lo and Wu,2011a) proposes the semi-automatic metric, whichevaluates machine translations utilizing annotatedevent structure of a sentence both in reference andmachine translation.
The basic assumption be-hind the metric can be stated as follows: trans-lation shall be considered "good" if it preservesshallow semantic (predicate-argument) structureof reference.
This structure is described in the pa-per on shallow semantic parsing (Pradhan et al.,2004): basically, we approach the evaluation byasking simple questions about events in the sen-tence: "Who did what to whom, when, where, whyand how?".
These structures are annotated andaligned between two translations.
The authors ofMEANT reported results of several experiments,which utilized both human annotation and seman-tic role labeling (as a gold standard) and automaticshallow semantic parsing.
Experiments show thatHMEANT correlates with human adequacy judg-ments (for three MT systems) at the value of 0.43(Kendall tau, sentence level), which is very closeto the correlation of HTER (BLEU has only 0.20).Also inter-annotator agreement was reported fortwo stages of annotation: role identification (se-lecting the word span) and role classification (la-beling the word span with role).
For the former,IAA ranged from 0.72 to 0.93 (which can be in-terpreted as a good agreement) and for the latter,from 0.69 to 0.88 (still quite good, but should beput in doubt).
IAA for the alignment stage was notreported.2.2 HMEANT for Czech-EnglishTranslationsMEANT and HMEANT metrics were adoptedfor an experiment on evaluation of Czech-Englishand English-Czech translations by Bojar andWu (2012).
These experiments were based ona human-evaluated set of 40 translations fromWMT121, which were submitted by 13 systems;each system was evaluated by exactly one anno-tator, plus an extra annotator for reference trans-lations.
This setting implied that inter-annotatoragreement could not be examined.
HMEANT cor-relation with human assessments was reported as0.28, which is significantly lower than the valueobtained by Lo and Wu (2011a).2.3 HMEANT for German-EnglishTranslationsBirch et al.
(2013) examined HMEANT thor-oughly with respect to four criteria, which addressthe usefulness of a task-based metric: reliability,efficiency, discriminatory power and language in-dependence.
The authors conducted an experi-ment to evaluate three MT systems: rule-based,phrase-based and syntax-based on a set of 214 sen-tences (142 German and 72 English).
IAA wasbroken down into the different stages of annotationand alignment.
The experimental results showedthat whilst the IAA for HMEANT is satisfying atthe first stages of the annotation, the compound-ing effect of disagreement at each stage (up tothe alignment stage) greatly reduced the effectiveoverall IAA ?
to 0.44 on role alignment for Ger-man, and, only slightly better, 0.59 for English.HMEANT successfully distinguished three typesof systems, however, this result could not be con-sidered reliable as IAA is not very high (and rank1http://statmt.org/wmt1244correlation was not reported).
The efficiency ofHMEANT was stated as reasonably good; how-ever, it was not compared to the labor cost of (forexample) HTER.
Finally, the language indepen-dence of the metric was implied by the fact thatoriginal guidelines can be applied both to Englishand German translations.3 Methods3.1 Evaluation with HMEANTThe underlying annotation cycle of HMEANTconsists of two stages: semantic role labeling(SRL) and alignment.
During the SRL stage, eachannotator is asked to mark all the frames (a pred-icate and associated roles) in reference translationand hypothesis translation.
To annotate a frame,one has to mark the frame head ?
predicate (whichis a verb, but not a modal verb) and its argu-ments, role fillers, which are linked to that pred-icate.
These role fillers are given a role from theinventory of 11 roles (Lo and Wu, 2011a).
Therole inventory is presented in Table 1, where eachrole corresponds to a specific question about thewhole frame.Who?
What?
Whom?Agent Patient BenefactiveWhen?
Where?
Why?Temporal Locative PurposeHow?Manner, Degree, Negation, Modal, OtherTable 1.
The role inventory.On the second stage, the annotators are askedto align the elements of frames from referenceand hypothesis translations.
The annotators linkboth actions and roles, and these alignments canbe matched as ?Correct?
or ?Partially Correct?
de-pending on how well the meaning was preserved.We have used the original minimalistic guidelinesfor the SRL and alignment provided by Lo and Wu(2011a) in English with a small set of Russian ex-amples.3.2 Calculating HMEANTAfter the annotation, HMEANT score of thehypothesis translation can be calculated as theF-score from the counts of matches of predicatesand their role fillers (Lo and Wu, 2011a).
Pred-icates (and roles) without matches are not ac-counted, but they result in the lower value overall.We have used the uniform model of HMEANT,which is defined as follows.#Fi?
number of correct role fillers for predicatei in machine translation;#Fi(partial) ?
number of partially correct rolefillers for predicate i in MT;#MTi, #REFi?
total number of role fillers inMT or reference for predicate i;Nmt, Nref?
total number of predicates in MT orreference;w ?
weight of the partial match (0.5 in the uniformmodel).P =?matched i#Fi#MTiR =?matched i#Fi#REFiPpart=?matched i#Fi(partial)#MTiRpart=?matched i#Fi(partial)#REFiPtotal=P + w ?
PpartNmtRtotal=R+ w ?RpartNrefHMEANT =2 ?
Ptotal?RtotalPtotal+Rtotal3.3 Inter-Annotator AgreementLike Lo and Wu (2011a) and Birch et al.
(2013)we studied inter-annotator agreement (IAA).
It isdefined as an F1-measure, for which we considerone of the annotators as a gold standard:IAA =2 ?
P ?RP +RWhere precision (P ) is the number of labels (roles,predicates or alignments) that match between an-notators divided by the total number of labels byannotator 1; recall (R) is the number of matchinglabels divided by the total number of labels by an-notator 2.
Following Birch et al.
(2013), we con-sider only exact word span matches.
Also we haveadopted the individual stages of the annotationprocedure that are described in (Birch et al.
2013):role identification (selecting the word span), roleclassification (marking the word span with a role),action identification (marking the word span as apredicate), role alignment (linking roles betweentranslations) and action alignment (linking frameheads).
Calculating IAA for each stage separately45helped to isolate the disagreements and to see,which stages resulted in a low agreement valueoverall.
To look at the most common role dis-agreements we also created the pairwise agree-ment matrix, every cell (i, j) of which is the num-ber of times the role i was confused with the rolej by any pair of annotators.3.4 Kendall?s Tau Rank Correlation WithHuman JudgmentsFor the set of translations used in our experiments,we had a number of relative human judgments (theset was taken from WMT132).
We used the rankaggregation method described in (Callison-Burchet al., 2012) to build up one ranking from thesejudgments.
This method is called Expected WinScore (EWS) and for MT system Sifrom the set{Sj} it is defined the following way:score(Si) =1|{Sj}|?j,j 6=iwin(Si, Sj)win(Si, Sj) + win(Sj, Si)Where win(Si, Sj) is the number of times systemi was given a rank higher than system j. Thismethod of aggregation was used to obtain the com-parisons of systems, which outputs were neverpresented together to assessors during the evalu-ation procedure at WMT13.After we had obtained the ranking of systemsby human judgments, we compared this rankingto the ranking by HMEANT values of machinetranslations.
To do that, we used Kendall?s tau(Kendall, 1938) rank correlation coefficient andreported the results as Lo and Wu (2011a) and Bo-jar (Bojar and Wu, 2012).4 Experimental Setup4.1 Test SetFor our experiments we used the set of translationsfrom WMT13.
We tested HMEANT on a set offour best MT systems (Bojar et al., 2013) for theEnglish-Russian language pair (Table 2).From the set of direct English-Russian transla-tions (500 sentences) we picked those which al-lowed to build a ranking for the four systems (94sentences); then out of these we randomly picked50 and split them into 6 tasks of 25 so that eachof the 50 sentences was present in exactly threetasks.
Each task consisted of 25 reference transla-tions and 100 hypothesis translations.2http://statmt.org/wmt13System EWS (WMT)PROMT 0.4949Online-G 0.475Online-B 0.3898CMU-Primary 0.3612Table 2.
The top four MT systems for the en-rutranslation task at WMT13.
The scores werecalculated for the subset of translations which weused in experiments.4.2 Annotation InterfaceAs far as we know there is no publically availableinterface for HMEANT annotation.
Thus, firstof all, having the prototype (Lo and Wu, 2011b)and taking into account comments and sugges-tions of Bojar and Wu (2012) (e.g., ability to goback within the phases of annotation), we createda web-based interface for role labeling and align-ment.
This interface allows to annotate a set ofreferences with one machine translation at a time(Figure 1) and to align actions and roles.
We alsoprovided a timer which allowed to measure thetime required to label the predicates and roles.4.3 AnnotatorsWe asked to participate two groups of annota-tors: 6 researchers with linguistic background (lin-guists) and 4 developers without it.
Every annota-tor did exactly one task; each of the 50 sentenceswas annotated by three linguists and at least twodevelopers.5 ResultsAs a result of the experiment, 638 frames wereannotated in reference translations (overall) and2 016 frames in machine translations.
More de-tailed annotation statistics are presented in Table3.
A closer look indicates that the ratio of alignedframes and roles in references was larger than inany of machine translations.5.1 Manual RankingAfter the test set was annotated, we comparedmanual ranking and ranking by HMEANT; on thesystem level, these rankings were similar; how-ever, on the sentence level, there was no correla-tion between rankings at all.
Thus we decided totake a closer look at the manual assessments.
Forthe selected 4 systems most of the pairwise com-46Figure 1.
The screenshot of SRL interface.
The tables under the sentences contain the informationabout frames (the active frame has a red border and is highlighted in the sentence, inactive frames (notshown) are semi-transparent).Source # Frames # Roles Aligned frames, % Aligned roles, %Reference 638 1 671 86.21 % 74.15 %PROMT 609 1 511 79.97 % 67.57 %Online-G 499 1 318 77.96 % 66.46 %Online-B 469 1 257 78.04 % 68.42 %CMU-Primary 439 1 169 75.17 % 66.30 %Table 3.
Annotation statistics.parisons were obtained in a transitive way, i. e.using comparisons with other systems.
Further-more, we encountered a number of useless rank-ings, where all the outputs were given the samerank.
After all, for many sentences the rankingof systems was based on a few pairwise compar-isons provided by one or two annotators.
Theserankings seemed to be not very reliable, thus wedecided to rank four machine translations for eachof the 50 sentences manually to make sure that theranking has a strong ground.
We asked 6 linguiststo do that task.
The average pairwise rank correla-tion (between assessors) reached 0.77, making theoverall ranking reliable; we aggregated 6 rankingsfor each sentence using EWS.5.2 Correlation with Manual AssessmentsTo look at HMEANT on a system level, we com-pared rankings produced during manual assess-ment and HMEANT annotation tasks.
Those rank-ings were then aggregated with EWS (Table 4).It should be noticed that HMEANT allowed torank systems correctly.
This fact indicates thatHMEANT has a good discriminatory power on thelevel of systems, which is a decent argument forSystem Manual HMEANT BLEUPROMT 0.532 0.443 0.126Online-G 0.395 0.390 0.146Online-B 0.306 0.374 0.147CMU-Primary 0.267 0.292 0.136Table 4.
EWS over manual assessments, EWSover HMEANT and BLEU scores for MTsystems.the usage of this metric.
Also it is worth to notethat ranking by HMEANT matched the ranking bythe number of frames and roles (Table 3).On a sentence level, we studied the rank corre-lation of ranking by manual assessments and byHMEANT values for each of the annotators.
Themanual ranking was aggregated by EWS from themanual evaluation task (see Section 5.1).
Resultsare reported in Table 5.We see that resulting correlation values are sig-nificantly lower than those reported by Lo and Wu(2011a) ?
our rank correlation values did not reach0.43 on average across all the annotators (and even0.28 as reported by Bojar and Wu (2012)).47Annotator ?Linguist 1 0.0973Linguist 2 0.3845Linguist 3 0.1157Linguist 4 -0.0302Linguist 5 0.1547Linguist 6 0.1468Developer 1 0.1794Developer 2 0.2411Developer 3 0.1279Developer 4 0.1726Table 5.
The rank correlation coefficients forHMEANT and human judgments.
Reliableresults (with p-value >0.05) are in bold.5.3 Inter-Annotator AgreementFollowing Lo and Wu (2011a) and Birch et al.
(2013) we report the IAA for the individual stagesof annotation and alignment.
These results areshown in Table 6.StageLinguists DevelopersMax Avg Max AvgREF, id 0.959 0.803 0.778 0.582MT, id 0.956 0.795 0.667 0.501REF, class 0.862 0.715 0.574 0.466MT, class 0.881 0.721 0.525 0.434REF, actions 0.979 0.821 0.917 0.650MT, actions 0.971 0.839 0.700 0.577Actions ?
align 0.908 0.737 0.429 0.332Roles ?
align 0.709 0.523 0.378 0.266Table 6.
The inter-annotator agreement for theindividual stages of annotation and alignmentprocedures.
Id, class, align stand foridentification, classification and alignmentrespectively.The results are not very different from those re-ported in the papers mentioned above, except foreven lower agreement for developers.
The factthat the results could be reproduced on a new lan-guage seems very promising, however, the lack oftraining for the annotators without linguistic back-ground resulted in lower inter-annotator agree-ment.Also we studied the most common role dis-agreements for each pair of annotators (either lin-guists or developers).
As it can be deduced fromthe IAA values, the agreement on all roles is lowerfor linguists, however, both groups of annotatorsshare the roles on which the agreement is best ofall: Predicate, Agent, Locative, Negation, Tempo-ral.
Most common disagreements are presented inTable 7.Role A Role B %, L %, DWhom What 18.0 15.2Whom Who 13.7 23.1Why None 17.0 22.3How (manner) What 10.5 -How (manner) How (degree) - 19.0How (modal) Action 18.1 16.3Table 7.
Most common role disagreements.
Lastcolumns (L for linguists, D for developers) standfor the ratio of times Role A was confused withRole B across all the label types (roles, predicate,none).These disagreements can be explained by thefact that some annotators looked ?deeper?
in thesentence semantics, whereas other annotators onlytried to capture the shallow structure as fast as pos-sible.
This fact explains, for example, disagree-ment on the Whom role ?
for some sentences, e. g.?mogli by ubedit~ politiqeskih liderov?
(?could persuade the political leaders?)
it requiressome time to correctly mark politiqeskih lid-erov (political leaders) as an answer to Whom,not What.
The disagreement on the Purpose (a lotof times it was annotated only by one expert) is ex-plained by the fact that there were no clear instruc-tions on how to mark clauses.
As for the Actionand Modal, this disagreement is based on the re-quirement that Action should consist of one wordonly; this requirement raised questions about com-plex verbs, e.g.
?zakonqil delat~?
(?stoppeddoing?).
It is ambiguous how to annotate theseverbs: some annotators decided to mark it asModal+Action, some ?
as Action+What.
Proba-bly, the correct way to mark it should be just asAction.5.4 EfficiencyAdditionnaly, we conducted an efficiency experi-ment in the group of linguists.
We measured theaverage time required to annotate a predicate (inreference or machine translation) and a role.
Re-sults are presented in Table 8.48AnnotatorREF MTRole Action Role ActionLinguist 1 14 26 11 36Linguist 2 10 12 8 12Linguist 3 13 14 8 23Linguist 4 16 15 9 15Linguist 5 13 20 11 24Linguist 6 17 35 9 32Table 8.
Average times (in seconds) required toannotate actions and roles.These results look very promising; using thenumbers in Table 3, we get the average time re-quired to annotate a sentence: 1.5 ?
2 minutes for areference (and even up to 4 minutes for slower lin-guists) and 1.5 ?
2.5 minutes for a machine trans-lation.
Also for a group of ?slower?
linguists (1, 5,6) inter-annotator agreement was lower (-0.05 onaverage) than between ?faster?
linguists (2, 3, 4)for all stages of annotation and alignment.
Aver-age time to annotate an action is similar for the ref-erence and MT outputs, but it takes more time toannotate roles in references than in machine trans-lations.6 Discussion6.1 Problems with HMEANTAs we can see, HMEANT is an acceptably reliableand efficient metric.
However, we have met someobstacles and problems with original instructionsduring the experiments with Russian translations.We believe that these obstacles are the main causesof low inter-annotator agreement at the last stagesof annotation procedure and low correlation ofrankings.Frame head (predicate) is required.
This re-quirement does not allow frames without predicateat all, e.g.
?On mo drug?
(?He is my friend?)
?the Russian translation of ?is?
(present tense) is anull verb.One-word predicates.
There are cases wherecomplex verbs (e.g., which consist of two verbs)can be correctly translated as a one-word verb.For example, ?ostanovils?
(?stopped?)
iscorrectly rephrased as ?perestal delat~?
(?ceased doing?
).Roles only of one type can be aligned.
Some-times one role can be correctly rephrased as an-other role, but roles of different type can not bealigned.
For example, ?On uehal iz goroda?
(?He went away from the town?)
means the sameas ?On pokinul gorod?
(?He left the town?
).The former has a structure of Who + Action +Where, the latter ?
Who + Action + What.Should we annotate as much as possible?
Itis not clear from the guideline whether we shouldannotate almost everything that looks like a frameor can be interpreted as a role.
There are someprepositional phrases which can not be easily clas-sified as one role or another.
Example: ?Nam nestoit ob tom volnovat~s?
(?We shouldnot worry about this?)
?
it is not clarified how todeal with ?ob tom?
(?about this?)
prepositionalphrase.7 ConclusionIn this paper we describe a preliminary series ofexperiments with HMEANT, a new metric for se-mantic role labeling.
In order to conduct these ex-periments we developed a special web-based an-notation interface with a timing feature.
A teamof 6 linguists and 4 developers annotated RussianMT output of 4 systems.
The test set of 50 En-glish sentences along with reference translationswas taken from the WMT13 data.
We measuredIAA for each stage of annotation process, com-pared HMEANT ranking with manual assessmentand calculated the correlation between HMEANTand manual evaluation.
We also measured anno-tation time and collected a feedback from anno-tators, which helped us to locate the problems andbetter understand the SRL process.
Analysis of thepreliminary experimental results of Russian MToutput annotation led us to the following conclu-sions about HMEANT as a metric.Language Independence.
For a relativelysmall set of Russian sentences, we encounteredproblems with the guidelines, but they were notspecific to the Russian language.
This can benaively interpreted as language independence ofthe metric.Reliability.
Inter-annotator agreement is highfor the first stages of SRL, but we noted that it de-creases on the last stages because of the compoundeffect of disagreements on previous stages.Efficiency.
HMEANT proved to be really ef-fective in terms of time required to annotate ref-erences and MT outputs and can be used in pro-duction environment, though the statement thatHMEANT annotation task does not require quali-49fied annotators was not confirmed.Discriminatory Power.
On the system level,HMEANT allowed to correctly rank MT systems(according to the results of manual assessmenttask).
On the sentence level, correlation with hu-man rankings is low.To sum up, first experience with HMEANTwas considered to be successful and allowed usto make a positive decision about applicabilityof the new metric to the evaluation of English-Russian machine translations.
We have to say thatHMEANT guidelines, annotation procedures andthe inventory of roles work in general, however,low inter-annotator agreement at the last stagesof annotation task and low correlation with hu-man judgments on the sentence level suggest usto make respective adaptations and conduct newseries of experiments.AcknowledgementsWe would like to thank our annotators for their ef-forts and constructive feedback.
We also wish toexpress our great appreciation to Alexey Baytinand Maria Shmatova for valuable ideas and ad-vice.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
In Proceed-ings of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization, pages 65?72.Alexandra Birch, Barry Haddow, Ulrich Germann,Maria Nadejde, Christian Buck, and Philipp Koehn.2013.
The Feasibility of HMEANT as a HumanMT Evaluation Metric.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, page52?61, Sofia, Bulgaria, August.
Association forComputational Linguistics.Ondrej Bojar and Dekai Wu.
2012.
Towards aPredicate-Argument Evaluation for MT.
In Pro-ceedings of the Sixth Workshop on Syntax, Semanticsand Structure in Statistical Translation, page 30?38,Jeju, Republic of Korea, July.
Association for Com-putational Linguistics.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 workshopon statistical machine translation.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, page 1?44, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, page10?51, Montr?al, Canada, June.
Association forComputational Linguistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Maurice G Kendall.
1938.
A new measure of rankcorrelation.
Biometrika, pages 81?93.Chi-kiu Lo and Dekai Wu.
2011a.
MEANT: Aninexpensive, high-accuracy, semi-automatic metricfor evaluating translation utility based on semanticroles.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, page 220?229, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Chi-kiu Lo and Dekai Wu.
2011b.
A radically sim-ple, effective annotation and alignment methodol-ogy for semantic frame based smt and mt evaluation.LIHMT 2011, page 58.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Sameer S Pradhan, Wayne Ward, Kadri Hacioglu,James H Martin, and Daniel Jurafsky.
2004.
Shal-low semantic parsing using support vector machines.In HLT-NAACL, pages 233?240.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine transla-tion in the Americas, pages 223?231.Warren Weaver.
1955.
Translation.
Machine transla-tion of languages, 14:15?23.50
