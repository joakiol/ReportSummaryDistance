A Model for Fine-Grained Alignment of Multilingual TextsLea CYRUS and Hendrik FEDDES?Arbeitsbereich LinguistikUniversity of Mu?nsterHu?fferstra?e 27, 48149 Mu?nster, Germany{lea,feddes}@marley.uni-muenster.deAbstractWhile alignment of texts on the sentential levelis often seen as being too coarse, and word align-ment as being too fine-grained, bi- or multi-lingual texts which are aligned on a level in-between are a useful resource for many pur-poses.
Starting from a number of examples ofnon-literal translations, which tend to makealignment difficult, we describe an alignmentmodel which copes with these cases by explicitlycoding them.
The model is based on predicate-argument structures and thus covers the middleground between sentence and word alignment.The model is currently used in a recently initi-ated project of a parallel English-German tree-bank (FuSe), which can in principle be extendedwith additional languages.1 IntroductionWhen building parallel linguistic resources, oneof the most obvious problems that need besolved is that of alignment.
Usually, in sentence-or word-aligned corpora, alignments are un-marked relations between corresponding ele-ments.
They are unmarked because the kindof correspondence between two elements is ei-ther obvious or beyond classification.
E. g., ina sentence-aligned corpus, the n : m relationsthat hold between sentences express the factthat the propositions contained in n sentencesin L1 are basically the same as the propositionsin m sentences in L2 (lowest common denomi-nator).
No further information about the kindof correspondence could possibly be added onthis degree of granularity.
On the other hand, inword-aligned corpora, words are usually alignedas being ?lexically equivalent?
or are not alignedat all.1 Although there are many shades of ?lexi-cal equivalence?, these are usually not explicitly?
We would like to thank our colleague Frank Schu-macher for many valuable comments on this paper.1Cf.
the approach described in (Melamed, 1998).categorised.
As (Hansen-Schirra and Neumann,2003) point out, for many research questionsneither type of alignment is sufficient, since themost interesting phenomena can be found on alevel between these two extremes.We propose a more finely grained modelof alignment which is based on monolingualpredicate-argument structures, since we assumethat, while translations can be non-literal in avariety of ways, they must be based on simi-lar predicates and arguments for some kind oftranslational equivalence to be achieved.
Fur-thermore, our model explicitly encodes the waysin which the two versions of a text deviate fromeach other.
(Salkie, 2002) points out that thepossibility to investigate what types of non-literal translations occur on a regular basis isone of the major profits that linguists and trans-lation theorists can draw from parallel corpora.In Section 2, we begin by describing someways in which translations can deviate fromone another.
We then describe in detail thealignment model, which is based on a monolin-gual predicate-argument structure (Section 3).In Section 4 we conclude by introducing theparallel treebank project FuSe which uses themodel described in this paper to align Germanand English texts from the Europarl parallelcorpus (Koehn, 2002).2 Differences in TranslationsIn most cases, translations are not absolutelyliteral counterparts of their source texts.
In or-der to avoid translationese, i. e. deviations fromthe norms of the target language, a skilledtranslator will apply certain mechanisms, which(Salkie, 2002) calls ?inventive translations?
andwhich need to be captured and systematised.The following section will give some examples22As we work with English and German, all exam-ples are taken from these two languages.
They are takenfrom the Europarl corpus (see Section 4) and are ab-breviated where necessary.
Unfortunately, it is not eas-of common discrepancies encountered betweena source text and its translation.2.1 NominalisationsQuite frequently, verbal expressions in L1 areexpressed by corresponding nominalisations inL2.
This departure from the source text resultsin a completely different structure of the tar-get sentence, as can be seen in (1) and (2),where the English verb harmonise is expressedas Harmonisierung in German.
The argumentof the English verb functioning as the grammat-ical subject is realised as a postnominal modifierin the German sentence.
(1) The laws against racism must be har-monised.3(2) DieTheHarmonisierungharmonisationderof theRechtsvorschriftenlawsgegenagainstdentheRassismusracismistisdringendurgentlyerforderlich.necessary.This case is particularly interesting, because itinvolves a case of modality.
In the English sen-tence, the verb is modified by the modal aux-iliary must.
In order to express the modalityin the German version, a different strategy isapplied, namely the use of an adjective withmodal meaning (erforderlich, ?necessary?).
Con-sequently, there are two predications in the Ger-man sentence as opposed to only one predica-tion in the English sentence.2.2 VoiceA further way in which translations can dif-fer from their source is the choice of active orpassive voice.
This is exemplified by (3) and(4).
Here, the direct object of the English sen-tence corresponds to the grammatical subject ofthe German sentence, while the subject of theEnglish sentence is realised as a prepositionalphrase with durch in the German version.
(3) The conclusions of the Theato reportsafeguard them perfectly.4ily discernible from the corpus data which language isthe source language.
Consequently, our use of the terms?source?, ?target?, ?L1?, and ?L2?
does not admit of anyconclusions as to whether one of the languages is thesource language, and if so, which one.3Europarl:de-en/ep-00-01-19.al, 489.4Europarl:de-en/ep-00-01-18.al, 749.
(4) DurchBydietheSchlu?folgerungenconclusionsdesof theBerichtsreportTheatoTheatowerdenaresietheyuneingeschra?nktunlimitedlybewahrt.safeguarded2.3 NegationSometimes, a positive predicate expression istranslated by negating its antonym.
This is thecase in (5) and (6): both sentences contain anegative statement, but while the negation is in-corporated into the English adjective by meansof the negative prefix in-, it is achieved syntac-tically in the German sentence.
(5) the Directive is inapplicable in Den-mark5(6) dietheRichtlinieDirectiveistisininDa?nemarkDenmarknichtnotanwendbarapplicable2.4 Information StructureSentences and their translations can be organ-ised differently with regard to their informationstructure.
Sentences (7) and (8) are a good ex-ample for this type of non-literal translation.
(7) Our motion will give you a great deal offood for thought, Commissioner6(8) EineAReiherowvonofAnregungensuggestionswerdenwillwirweIhnen,you,HerrMr.Kommissar,Commissioner,mitwithunsererourEntschlie?ungresolutionmitgebengiveThe German sentence is rather inconspicuous,with the grammatical subject being a prototyp-ical agent (wir, ?we?).
In the English version,however, it is the means that is realised in sub-ject position and thus perspectivised.
The cor-responding constituent in German (mit unsererEntschlie?ung, ?with our motion?)
is but an ad-verbial.
In English, the actual agent is not re-alised as such and can only be identified by aprocess of inference based on the presence of thepossessive pronoun our.
Thus, while being moreor less equivalent in meaning, this sentence pairdiffers significantly in its overall organisation.5Europarl:de-en/ep-00-01-18.al, 2522.6Europarl:de-en/ep-00-01-18.al, 53.3 Alignment ModelThe alignment model we propose is based onthe assumption that a representation of transla-tional equivalence can best be approximated byaligning the elements of monolingual predicate-argument structures.
Section 3.1 describes thislayer of the model in detail and shows how someof the differences in translations described inSection 2 can be accomodated on such a level.We assume that the annotation model describedhere is an extension to linguistic data which arealready annotated with phrase-structure trees,i.
e. treebanks.
Section 3.2 shows how the bind-ing of predicates and arguments to syntacticnodes is modelled.
Section 3.3 describes the de-tails of the alignment layer and the tags usedto mark particular kinds of alignments, thus ac-counting for some more of the differences shownin Section 2.3.1 Predicates and ArgumentsThe predicate-argument structures used in ourmodel consist solely of predicates and their ar-guments.
Although there is usually more thanone predicate in a sentence, no attempt is madeto nest structures or to join the predicationslogically in any way.
The idea is to make thepredicate-argument structure as rich as is ne-cessary to be able to align a sentence pair whilekeeping it as simple as possible so as not tomake it too difficult to annotate.
In the samevein, quantification, negation, and other opera-tors are not annotated.
In short, the predicate-argument structures are not supposed to cap-ture the semantics of a sentence exhaustively inan interlingua-like fashion.To have clear-cut criteria for annotators todetermine what a predicate is, we rely on theheuristic assumption that predicates are morelikely to be expressed by tokens belonging tosome word classes than by tokens belonging toothers.
Potential predicate expressions in thismodel are verbs, deverbal adjectives and nouns7or other adjectives and nouns which show a syn-tactic subcategorisation pattern.
The predicatesare represented by the capitalised citation formof the lexical item (e. g. harmonise).
They areassigned a class based on their syntactic form(v, n, a for ?verbal?, ?nominal?, and ?adjectival?,respectively), and derivationally related predi-7For all non-verbal predicate expressions for which aderivationally related verbal expression exists it is as-sumed that they are deverbal derivations, etymologicalcounter-evidence notwithstanding.cates form a predicate group.Arguments are given short intuitive rolenames (e. g. ent harmonised, i. e. the entitybeing harmonised) in order to facilitate theannotation process.
These role names have tobe used consistently only within a predicategroup.
If, for example, an argument of the pred-icate harmonise has been assigned the roleent harmonised and the annotator encoun-ters a comparable role as argument to the pred-icate harmonisation, the same role name forthis argument has to be used.8The usefulness of such a structure can beshown by analysing the sentence pair (1) and(2) in Section 2.1.
While the syntactic con-structions differ considerably, the predicate-argument structure shows the correspondencequite clearly (see the annotated sentences inFigure 19): in the English sentence, we findthe predicate harmonise with its argumentent harmonised, which corresponds to thepredicate harmonisierung and its argumentharmonisiertes in the German sentence.
Theinformation that a predicate of the class v isaligned with a predicate of the class n can beused to query the corpus for this type of non-literal translations.The active vs. passive translation in sentences(3) and (4) is another phenomenon which is ac-comodated by a predicate-argument structure(Figure 2): the subject np502 in the Englishsentence corresponds to the passivised subjectnp502 (embedded in pp503) in the German sen-tence on the basis of having the same argumentrole (safeguarder vs. bewahrer) in a com-parable predication.It is sometimes assumed that predicate-argument structure can be derived or recov-ered from constituent structure or functionaltags such as subject and object.10 It is truethat these annotation layers provide importantheuristic clues for the identification of predi-8Keeping the argument names consistent for all pred-icates within a group while differentiating the predicateson the basis of syntactic form are complementary prin-ciples, both of which are supposed to facilitate queryingthe corpus.
The consistency of argument names withina group, for example, enables the researcher to anal-yse paradigmatically all realisations of an argument ir-respective of the syntactic form of the predicate.
At thesame time, the differentiation of predicates makes possi-ble a syntagmatic analysis of the differences of argumentstructures depending on the syntactic form of the pred-icate.9All figures are at the end of the paper.10See e. g. (Marcus et al, 1994).cates and arguments and may eventually speedup the annotation process in a semi-automaticway.
But, as the examples above have shown,predicate-argument structure goes beyond theassignment of phrasal categories and grammati-cal functions, because the grammatical categoryof predicate expressions and consequently thegrammatical functions of their arguments canvary considerably.
Also, the predicate-argumentstructure licenses the alignment relation byshowing explicitly what it is based on.3.2 Binding LayerAs mentioned above, we assume that the an-notation model described here is used on topof syntactically annotated data.
Consequently,all elements of the predicate-argument structuremust be bound to elements of the phrasal struc-ture (terminal or non-terminal nodes).
Thesebindings are stored in a dedicated binding layerbetween the constituent layer and the predicate-argument layer.A problem arises when there is no direct cor-respondence between argument roles and con-stituents.
For instance, this is the case whenevera noun is postmodified by a participle clause: inFigure 3, the argument role ent raised of thepredicate raise is realised by np525, but theparticiple clause (ipa517) containing the pred-icate (raised6) needs to be excluded, becausenot excluding it would lead to recursion.
Con-sequently, there is no simple way to link theargument role to its realisation in the tree.In these cases, the argument role is linked tothe appropriate phrase (here: np525) and theconstituent that contains the predicate (ipa517)is pruned out, which results in a discontinu-ous argument realisation.
Thus, in general, thebinding layer allows for complex bindings, withmore than one node of the constituent structureto be included in and sub-nodes to be explicitlyexcluded from a binding to a predicate or argu-ment.11When an expected argument is absent on thephrasal level due to specific syntactic construc-tions, the binding of the predicate is tagged ac-cordingly, thus accounting for the missing argu-ment.
For example, in passive constructions likein Table 1, the predicate binding is tagged as pv.Other common examples are imperative con-structions.
Although information of this kindmay possibly be derived from the constituent11See the database documentation (Feddes, 2004) fora more detailed description of this mechanism.structure, it is explicitly recorded in the bindinglayer as it has a direct impact on the predicate-argument structure and thus might prove use-ful for the automatic extraction of valency pat-terns.Sentence wenn korrekt gedolmetscht wurdeGloss if correctly interpreted was?Binding pv|Pred/Arg dolmetschenTable 1: Example of a tagged predicate binding(Europarl:de-en/ep-00-01-18.al, 2532)Note that the passive tag can also be ex-ploited in order to query for sentence pairs like(3) and (4) (in Section 2.2), where an active sen-tence is translated with a passive: it is straight-forward to find those instances of aligned predi-cates where only one binding carries the passivetag.3.3 Alignment LayerOn the alignment layer, the elements of a pair ofpredicate-argument structures are aligned witheach other.
Arguments are aligned on the basisof corresponding roles within the predications.Comparable to the tags used in the bindinglayer that account for specific constructions (seeSection 3.2), the alignments may also be taggedwith further information.
These tags are usedto classify types of non-literalness like those dis-cussed in Sections 2.3 and 2.4.12Sentences (5) and (6) are an example for atagged alignment.
As Section 2.3 has shown,negation may be incorporated in a predicate inL1, but not in L2.
Since our predicate-argumentstructure does not include syntactic negation,this results in the alignment of a predicate inL1 with its logical opposite in L2.
To accountfor this fact, predicate alignments of this kindare tagged as absolute opposites (abs-opp).Similarly, alignment tagging is applied whenpredications are in some way incompatible, asis the case with sentences (7) and (8) in Sec-tion 2.4.
As can be seen in the aligned annota-tion (Figure 4), the different information struc-ture of these sentences has caused the two cor-responding argument roles of giver and mit-geber to be realised by two incompatible ex-pressions representing different referents (np50012The deviant translations described in Sections 2.1and 2.2 are already represented via predicate class (seeSection 3.1) and on the binding layer (see Section 3.2),respectively.vs.
wir 5).
In this case, the alignment betweenthe incompatible arguments is tagged incomp.If there is no corresponding predicate-argument structure in the other language (ase.
g. the adjectival predicate in sentence (2)) orif an argument within a structure does not havea counterpart in the other language, there willbe no alignment.Table 2 gives an overview of the annotationlayers as described in this section.Layer FunctionPhrasal constituent structure of language ABinding binding ?
predicates/arguments to ?
nodespa predicate-argument structuresAlignment aligning l predicates and argumentspa predicate-argument structuresBinding binding ?
predicates/arguments to ?
nodesPhrasal constituent structure of language BTable 2: The layers of the predicate-argumentannotationAll elements of the alignment structure aresupposed to mark explicitly the way they con-tribute to or distort the resulting translationalequivalence of a sentence pair.13 First and fore-most, if two elements are aligned to each other,this alignment is licensed by their having com-parable roles in the predicate-argument struc-tures.
This is the default case.
If, however, aparticular alignment relation, either of predi-cates or of arguments, is deviant in some way,this deviance is explicitly marked and classifiedon the alignment layer.4 Application and OutlookThe alignment model we have described is cur-rently being used in a project to build a tree-bank of aligned parallel texts in English andGerman with the following linguistic levels: postags, constituent structure and functional re-lations, plus the predicate-argument structureand the alignment layer to ?fuse?
the two?
hence our working title for the treebank,FuSe, which additionally stands for functionalsemantic annotation (Cyrus et al, 2003; Cyruset al, 2004).Our data source, the Europarl corpus (Koehn,2002), contains sentence-aligned proceedings ofthe European parliament in eleven languages13Cf.
the ?translation network?
described in (Santos,2000) for a much more complex approach to describingtranslation in a formal way; this model, however, goeswell beyond what we think is feasible when annotatinglarge amounts of data.and thus offers ample opportunity for extend-ing the treebank at a later stage.14 For syntac-tic and functional annotation we basically adaptthe tiger annotation scheme (Albert and oth-ers, 2003), making adjustments where we deemappropriate and changes which become neces-sary when adapting to English an annotationscheme which was originally developed for Ger-man.We use Annotate for the semi-automaticassignment of pos tags, hierarchical struc-ture, phrasal and functional tags (Brants, 1999;Plaehn, 1998a).
Annotate stores all annota-tions in a relational database.15 To stay consis-tent with this approach we have developed anextension to the Annotate database structureto model the predicate-argument layer and thebinding layer.Due to the monolingual nature of the Anno-tate database structure, the alignment layer(Section 3.3) cannot be incorporated into it.Hence, additional types of databases are needed.For each language pair (currently English andGerman), an alignment database is definedwhich represents the alignment layer, thus fus-ing two extended Annotate databases.
Addi-tionally, an administrative database is neededto define sets of two Annotate databases andone alignment database.
The final parallel tree-bank will be represented by the union of thesesets (Feddes, 2004).While annotators use Annotate to enterphrasal and functional structures comfortably,the predicate-argument structures and align-ments are currently entered into a structuredtext file which is then imported into thedatabase.
A graphical annotation tool for theselayers is under development.
It will make bind-ing the predicate-argument structure to the con-stituent structure easier for the annotators andsuggest argument roles based on previous deci-sions.Possiblities of semi-automatic methods tospeed up the annotation and thus reduce thecosts of building the treebank are currently be-ing investigated.16 Still, quite a bit of manual14There are a few drawbacks to Europarl, such as itslimited register and the fact that it is not easily dis-cernible which language is the source language.
How-ever, we believe that at this stage the easy accessibility,the amount of preprocessing and particularly the lack ofcopyright restrictions make up for these disadvantages.15For details about the Annotate database structuresee (Plaehn, 1998b).16One track we follow is to investigate if it is feasible towork will remain.
We believe, however, that theeffort that goes into such a gold-standard paral-lel treebank is very much worthwhile since thetreebank will eventually prove useful for a num-ber of fields and can be exploited for numer-ous applications.
To name but a few, translationstudies and contrastive analyses will profit par-ticularly from the explicit annotation of transla-tional differences.
nlp applications such as Ma-chine Translation could, e. g., exploit the con-stituent structures of two languages which aremapped via the predicate-argument-structure.Also, from the disambiguated predicates andtheir argument structures, a multilingual va-lency dictionary could be derived.ReferencesStefanie Albert et al 2003. tiger Annota-tionsschema.
Technical report, Universita?tdes Saarlandes, Universita?t Stuttgart, Uni-versita?t Potsdam.
Unpublished Draft ?
24July 2003.Thorsten Brants.
1999.
Tagging and Parsingwith Cascaded Markov Models: Automation ofCorpus Annotation, volume 6 of Saarbru?ckenDissertations in Computational Linguisticsand Language Technology.
Saarland Univer-sity, Saarbru?cken.Lea Cyrus, Hendrik Feddes, and Frank Schu-macher.
2003.
FuSe ?
a multi-layered paral-lel treebank.
Poster presented at the SecondWorkshop on Treebanks and Linguistic The-ories, 14?15 November 2003, Va?xjo?, Sweden(TLT 2003).
http://fuse.uni-muenster.de/Publications/0311_tltPoster.pdf.Lea Cyrus, Hendrik Feddes, and FrankSchumacher.
2004.
Annotating predicate-argument structure for a parallel treebank.In Charles J. Fillmore, Manfred Pinkal,Collin F. Baker, and Katrin Erk, editors,Proc.
LREC 2004 Workshop on BuildingLexical Resources from Semantically An-notated Corpora, Lisbon, May 30, 2004,pages 39?46.
http://fuse.uni-muenster.de/Publications/0405_lrec.pdf.Hendrik Feddes.
2004.
FuSe databasestructure.
Technical report, Ar-beitsbereich Linguistik, University ofhave the annotators mark predicate-argument structureson raw texts and have the phrasal and functional layersadded in a later stage, possibly supported by methodswhich derive these layers partially from the predicate-argument structures.
This is, however, still very tenta-tive.Mu?nster.
http://fuse.uni-muenster.de/Publications/dbStruktur.pdf.Silvia Hansen-Schirra and Stella Neumann.2003.
The challenge of working with multilin-gual corpora.
In Stella Neumann and SilviaHansen-Schirra, editors, Proceedings of theworkshop on Multilingual Corpora: Linguis-tic Requirements and Technical Perspectives.Corpus Linguistics 2003, Lancaster, pages 1?6.Philipp Koehn.
2002.
Europarl: A multilin-gual corpus for evaluation of machine trans-lation.
Unpublished draft, http://www.isi.edu/~koehn/publications/europarl/.Mitch Marcus, G. Kim, M. Marcinkiewicz,R.
MacIntyre, A. Bies, M. Ferguson, K. Katz,and B. Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument struc-ture.
In Proc.
ARPA Human Language Tech-nology Workshop.I.
Dan Melamed.
1998.
Manual annota-tion of translational equivalence: The blinkerproject.
Technical Report 98-07, IRCS, Uni-versity of Pennsylvania.
http://citeseer.ist.psu.edu/melamed98manual.html.Oliver Plaehn.
1998a.
Annotate Bedi-enungsanleitung.
Technical report, Univer-sita?t des Saarlandes, FR 8.7, Saarbru?cken.http://www.coli.uni-sb.de/sfb378/negra-corpus/annotate-manual.ps.gz.Oliver Plaehn.
1998b.
Annotate Datenbank-Dokumentation.
Technical report, Univer-sita?t des Saarlandes, FR 8.7, Saarbru?cken.http://www.coli.uni-sb.de/sfb378/negra-corpus/datenbank.ps.gz.Raphael Salkie.
2002.
How can linguists profitfrom parallel corpora?
In Lars Borin, editor,Parallel Corpora, Parallel Worlds, pages 93?109.
Rodopi, Amsterdam.Diana Santos.
2000.
The translation network:A model for a fine-grained description oftranslations.
In Jean Ve?ronis, editor, ParallelText Processing: Alignment and Use of Trans-lation Corpora, volume 13 of Text, Speechand Language Technology, chapter 8.
Kluwer,Dordrecht.TheDielawsHarmonisierungagainstder Rechtsvorschriftenracismgegenmust bedenharmonisedRassismus ist dringendACerforderlichPC HDNKVCNKNKMONKHDPPMNRACNPNPSBPCHDNKIBNKVC0 1 2 3PP4 5 6500MNR501502503NKSNKNPAGNPSB HDAPPD0 1 2 3 4 5 6 7 8 9500 501502503504505SHARMONISIERTES [503]ENT_HARMONISED [502]Binding layerPredicate?argument layerPredicate?argument layerBinding layerAlignment layerERFORDERLICHES [504]HARMONISE?v [6]HARMONISIERUNG?n [1] ERFORDERLICH?a [9]Figure 1: Alignment of a verb/direct-object construction with a noun/modifier constructionTheDurchconclusionsdieofSchlu?folgerungenthedesTheatoBerichtsreportTheatosafeguardwerdenthemsieperfectlyuneingeschr?nkt bewahrtNK NKNKNKNK MNRACNPOAPCMONKHDNKNKPPNKPGNPNPAGSBACHDNPODPCMO0 1 2PP3 4 5 6 7SBP8500501502503HDSIPAVC0 1 2 3 4 5 6 7 8 9500 501502503504SBinding layer (tagged)Predicate?argument layerAlignment layerPredicate?argument layerSAFEGUARD?v [6] SAFEGUARDER [502] ENT_SAFEGUARDED [7]BEWAHREN?v [9] BEWAHRER [502] BEWAHRTES [7]Binding layerpvFigure 2: Active vs. passive voice in translations: an example of a tagged binding (pv)the issue raised by the President of the Socialist Group yesterday about the reinstatement of the debateNK NK NKACNPPCNK NKPPPGACNPPCHDPPSBP MO NK NK MNR MNRACNP[...]RAISER [510] ENT_RAISED [525?517]RAISE?v [6]Predicate?argument layer[...]Binding layerPCNK NKPP518PGACNPPCNK NKIPAMNRPPMNRNP4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20500505510514517520522524525Figure 3: Complex binding of an argument: an example of a pruned constituent (dash-dotted line)OurEinemotionReihewillvongiveAnregungenyouwerdenawirgreatIhnendeal,ofHerrfoodKommissarfor,thoughtmit,unsererCommissionerEntschlie?ung mitgebenAC PCNKNKNKNKACNKPCNKNKPPMNRNK NKPPPG NKNPAPP ACNPPCACNPPCNPNKOANKNPNKDAPPPPPGMO HDHD OINPODHD SBIBVCNPSB HDIBVC VOS0 1 2 3 4 5 6 7 8 9S10 11 12 13 14500 501 5020 1 25063 450755086 7 85119 10 1151312 13500 501504507509511513EMPFANGENDES [6]RECIPIENT [4]MITGEBENDES [5]GIVER [500]MITGEGEBENES [506]ENT?GIVEN [509]GIVE?v [3]MITGEBEN?v [14]Binding layerPredicate?argument layerAlignment layerPredicate?argument layerBinding layer(tagged) incompFigure 4: Different information structure: an example of a tagged alignment (incomp)
