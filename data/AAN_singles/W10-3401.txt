Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), page 1,Beijing, August 2010INVITED KEYNOTE PRESENTATIONDistributional Semantics and the LexiconEduard Hovy Information Sciences Institute  University of Southern Californiahovy@isi.eduThe lexicons used in computational linguistics systems contain morphological, syntactic, and occasionally also some semantic information (such as definitions, pointers to an ontology, verb frame filler preferences, etc.).
But the human cognitive lexicon contains a great deal more, crucially, expectations about how a word tends to combine with others: not just general information-extraction-like patterns, but specific instantial expectations.
Such in-formation is very useful when it comes to lis-tening in bad aural conditions and reading texts in which background information is taken for granted; without such specific ex-pectation, one would be hard-pressed (and computers are completely unable) to form co-herent and richly connected multi-sentence interpretations.
Over the past few years, NLP work has in-creasingly treated topic signature word distri-butions (also called ?context vectors?, ?topic models?, etc.)
as a de facto replacement for semantics.
Whether the task is wordsense dis-ambiguation, certain forms of textual entail-ment, information extraction, paraphrase learning, and so on, it turns out to be very use-ful to consider a word(sense) as being defined by the distribution of word(senses) that regu-larly accompany it (in the classic words of Firth, ?you shall know a word by the company it keeps?).
And this is true not only for indi-vidual wordsenses, but also for larger units such as topics: the product of LDA and similar topic characterization engines is similar.
In this talk I argue for a new kind of seman-tics, which is being called Distributional Se-mantics.
It combines traditional symbolic logic-based semantics with (computation-based) statistical word distribution information.
The core resource is a single lexico-semantic lexicon that can be used for a variety of tasks, provided that it is reformulated accordingly.
I show how to define such a semantics, how to build the appropriate lexicon, how to format it, and how to use it for various tasks.
The talk pulls together a wide range of related topics, including Pantel-style resources like DIRT, inferences / expectations such as those used in Schank-style expectation-based parsing and expectation-driven NLU, PropBank-style word valence lexical items, and the treatment of negation and modalities.
I conclude by arguing that the human cognitive lexicon has to have the same kinds of properties as the Distributional Semantics lexicon, given the ways people do things with words.1
