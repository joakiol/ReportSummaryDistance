Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 771?781,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsCollaborative Ranking: A Case Study on Entity LinkingZheng ChenComputer Science DepartmentGraduate CenterCity University of New Yorkzchen1@gc.cuny.eduHeng JiComputer Science DepartmentQueens College and Graduate CenterCity University of New Yorkhengji@cs.qc.cuny.eduAbstractIn this paper, we present a new rankingscheme, collaborative ranking (CR).
In con-trast to traditional non-collaborative rankingscheme which solely relies on the strengthsof isolated queries and one stand-alone rank-ing algorithm, the new scheme integrates thestrengths from multiple collaborators of aquery and the strengths from multiple rankingalgorithms.
We elaborate three specific formsof collaborative ranking, namely, micro col-laborative ranking (MiCR), macro collabora-tive ranking (MaCR) and micro-macro collab-orative ranking (MiMaCR).
Experiments onentity linking task show that our proposedscheme is indeed effective and promising.1 IntroductionMany natural language processing tasks can be for-malized as a ranking problem, namely to rank acollection of candidate ?objects?
with respect to a?query?.
For example, intensive studies were de-voted to parsing in which multiple possible pars-ing trees or forests are ranked with respect to a sen-tence (Collins, 2000; Charniak and Johnson, 2005;Huang, 2008), machine translation in which multi-ple translation hypotheses are ranked with respect toa source sentence (Och, 2002; Shen et al, 2005),anaphora resolution in which multiple antecedentsare ranked with respect to an anaphora (Yang etal., 2008), and question answering in which mul-tiple possible answers are ranked with respect to aquestion (Ravichandran et al, 2003).
Previous stud-ies mainly focused on improving the ranking perfor-mance using one stand-alone learning algorithm onisolated queries.Although a wide range of learning algorithms (un-supervised, supervised or semi-supervised) is avail-able, each with its strengths and weaknesses, thereis not a learning algorithm that can work best onall types of data.
In such a situation, it wouldbe desirable to build a ?collaborative?
model byintegrating multiple models.
Such an idea formsthe basis of ensemble methodology and it is well-known that ensemble methods (e.g., bagging, boost-ing) can improve the performance of many prob-lems, in which classification is the most intensivelystudied (Rokach, 2009).
The other situation is re-lated with isolated queries handled by learning al-gorithms.
The single query may not be formulatedwith the best terms or the query itself may not con-tain comprehensive information required for a high-performance ranking algorithm.
Therefore, tech-niques of query expansion or query reformulationcan be introduced and previous research has shownthe effectiveness of those techniques in such applica-tions as information retrieval and question answer-ing (Manning et al, 2008; Riezler et al, 2007).Nevertheless, previous research normally considersquery reformulation as a new query for the rankingsystem, it would be more desirable to form a larger-scale ?collaborative?
group for the query and makea unified decision based on the group.Inspired from human collaborative learning inwhich two or more people form a group and ac-complish work together, we propose a new rankingscheme, collaborative ranking, which aims to imi-tate human collaborative learning and enhance sys-tem ranking performance.
The main idea is to seekcollaborations for each query from two levels:(1) query-level: search a group of query collabo-rators, and make the joint decision from the grouptogether with the query using a stand-alone rankingalgorithm.
(2) ranker-level: design a group of multiplerankers, and make the joint decision from the entiregroup on a single query.771query query collaborator candidate object query collaboration group ranker ranking output(a) non-collaborative ranking (b) micro collaborative ranking (c) macro collaborative ranking (d) micro-macro collaborative rankingFigure 1: Non-collaborative ranking and three collaborative ranking approaches.Figure 1 presents an intuitive illustration of fourranking approaches, including the traditional non-collaborative ranking and three collaborative rank-ing forms: micro collaborative ranking (MiCR),macro collaborative ranking (MaCR), and micro-macro collaborative ranking (MiMaCR).Compared with the traditional non-collaborativeranking that only leverages the information con-tained in a single query and only applies one rankingfunction (Figure 1 (a)), the three collaborative rank-ing approaches have the following advantages:(1)MiCR (corresponding to query-level collabo-ration1) leverages the information contained in thecollaborators of a query.
Figure 1 (b) demonstratesthat 6 query collaborators together with the queryform a query collaboration group.
(2)MaCR (corresponding to ranker-level collabo-ration2) integrates the strengths from two or morerankers.
Figure 1 (c) demonstrates an example of 3rankers.
(3)MiMaCR combines the advantages fromMiCR and MaCR as shown in Figure 1 (d).In this paper, we will show the efficacy of collab-orative ranking on the entity linking task defined inthe Knowledge Base Population (KBP) track (Ji etal., 2010) at Text Analysis Conference (TAC).
Eachquery in the task is associated with a name string andits context document.
Traditional approaches for en-tity linking only made use of the lexical or docu-ment level information contained in the query, how-ever, it may not be sufficient for the task.
The intu-ition why query-level collaboration may work is thatit leverages more comprehensive information aboutthe entity mention frommultiple ?collaborators?
(re-1Query is normally expressed by small-scale data structure,so called micro.2Ranker is normally implemented by large-scale algorithm,so called macro.lated documents containing the name string).
Fur-thermore, previous work on this task mainly focusedon comparing one ranking algorithm with the oth-ers, however, each ranking algorithm has its ownstrengths, and therefore, ranker-level collaborationcan potentially improve the performance.
Last, thecombination of query-level and ranker-level collab-oration can lead to further performance gains.2 Non-collaborative RankingLet q denote a query.
Let o(q) ={o(q)1 , .
.
.
, o(q)n(q)}denote the object set associated with q, where n(q)denotes the size of the o(q).
The goal of non-collaborative ranking is to seek a ranking functionf such that it computes ranking scores for the can-didates in the object set, i.e., y(q) = f(o(q)) ={y(q)1 , .
.
.
, y(q)n(q)}.Earlier studies on non-collaborative rankingmainly explored unsupervised approaches, e.g., vec-tor space model, link based algorithm such asPageRank (Page et al, 1998).
Unsupervised ap-proaches are based on well-established statisticaland probability theory, nevertheless, they sufferfrom some drawbacks, for example, it is hard totune parameters.
Recently, supervised approaches(named ?learning to rank?)
that automatically learnranking functions from training data become the fo-cus of ranking research.
In the literature, super-vised approaches are categorized into three classes,namely, pointwise, pairwise, and listwise.
We sum-marize a comparison of the three approaches in Ta-ble 1.
We use the following notations in the table.LetQ = {q1, .
.
.
, qN} denote the set ofN queriesin the training data, each query qi is associatedwith a set of objects o(qi) ={o(qi)1 , .
.
.
, o(qi)n(qi)}and a set of ground-truth ranking scores y(qi) =772pointwise pairwise listwiseapproachoverviewcommon: 1) use training samples; 2) learn the best ranking function by minimizing a given loss function;3) apply the ranking function at ranking steptransform ranking to regression orclassification on single objectstransform ranking to classification onobject pairsranking by learning fromlists of objectstraining set{ !
"(#$), %"(#$)&}"'*,?,+-.$/;  !
"#,?,${%&'()*), &-()*), ./}'"#,?,012*3;-"#,?,012*3;-4'; !"#,?,$.
= 5+1  !"
#$(%&) > #'(%&)?1 !"
#$(%&) ?
#'(%&) {(#(%&), .
(%&))}/02,?,3loss functionpointwise loss, e.g., squareloss(Chen et al, 2009)pairwise loss, e.g., hinge loss(Zhang,2004), exponential loss(Bartlett et al,2003), logistic loss(Lin, 2002)listwise loss, e.g., crossentropy loss(Cao et al,2007),cosine loss(Qin et al,2007)pros andconspros: classification is well studiedcons:1) only consider one objectat a time ignoring relationshipamong objectspros: classification is well studiedcons:1) only consider pairwiseorders; 2) biased towards lists withmore objectspros: fully considerrelationship among objectscons: 1) less well studied intheoryselectedalgorithmsDiscriminative model for IR(Nallapati, 2004);McRank (Li et al, 2007)SVM Ranking(Joachims, 2002);RankBoost(Freund et al, 2003);RankNet(Burges et al, 2005)ListNet (Cao et al, 2007);RankCosine(Qin et al, 2007);ListMLE (Xia et al, 2008)Table 1: Comparison of pointwise, pairwise and listwise ranking approaches.
{y(qi)1 , .
.
.
, y(qi)n(qi)}.
Let x(qi)j = ?
(qi, o(qi)j ) denotea feature vector associated with each query-objectpair (qi, o(qi)j ).3 Collaborative Ranking3.1 Micro Collaborative Ranking(MiCR)Micro collaborative ranking is characterized by inte-grating joint strengths from multiple query collabo-rators and the query itself.
It is based on the follow-ing assumptions:?
Expandability: Query is expandable, that is, itis able to find potential collaborators.?
Redundancy: Collaborators and query mayshare redundant information.?
Diversity: Collaborators exhibit multifacetedinformation that may complement the informationcontained in the query.?
Robustness: Noisy collaborators are allowable,and they could be put under control.Let cq(q) = {cq1, .
.
.
, cqk} be the k collabo-rators of a query q.
For each object o(q)j associ-ated with q, we form k + 1 feature vectors x(q)j =?
(q, o(q)j ), x(cq1)j = ?
(cq1, o(cq1)j ), .
.
.
, x(cqk)j =?
(cqk, o(cqk)j ) .
Let f be a ranking function whichis obtained by either an unsupervised or supervisedapproach.
There are two important steps that dis-tinguish MiCR from traditional non-collaborativeranking approaches:?
Step (1): searching the best k collaborators of q.?
Step (2): simulating the interaction of k collab-orators at the ranking step.Solutions for step (1) can vary from case to case.In our case study presented later, we transformthe collaborator searching problem into a clusteringproblem.
Collaborators of a query are then formedby members (excluding the query) in a cluster whichcontains the query and k is the size of the cluster mi-nus one.We transform the problem of step (2) into solv-ing a function g1 such that a ranking score y(q)j canbe computed for each object o(q)j .
One approachto computing g1 is to firstly compute the rankingscores of collaborators and query using the rankingfunction f and then combine those ranking scoresin some way (Formula 1).
The other approach is tolearn a supervised ranking function f ?
which takescollaborators and query as input (Formula 2).y(q)j = g1(f(x(q)j), f(x(cq1)j), .
.
.
, f(x(cqk)j)) (1)773y(q)j = g1(?)
= f?
(x(q)j , x(cq1)j , .
.
.
, x(cqk)j)(2)We present three specific forms of g1 in Formula1, namely, max, min, and weighted.
We can alsodefine a special case of weighted, called ?average?in which w0 = w1 .
.
.
= wk = 1/(k + 1).?
max: y(q)j = max(f(x(q)j), .
.
.
, f(x(cqk)j))?
min: y(q)j = min(f(x(q)j), .
.
.
, f(x(cqk)j))?
weighted: y(q)j = w0f(x(q)j)+k?i=1wif(x(cqi)j)We will discuss three supervised versions of g1(Formula 2) in section 4.4.
A general algorithm forMiCR is presented in Algorithm 1.Algorithm 1 MiCR Algorithm.Input:a query q; a set of objects o(q); a function g1Output:a set of ranking scores y(q)1: Search k collaborators of q:cq(q) = {cq1, .
.
.
, cqk}.2: for j = 1; j <= n(q); j + + do3: Form k + 1 feature vectors: x(q)j , x(cq1)j , .
.
.
, x(cqk)j .4: Compute function y(q)j = g1(?
).5: end for6: return y(q)3.2 Macro Collaborative Ranking(MaCR)Macro collaborative ranking is characterized by in-tegrating joint strengths from multiple rankers.
It isbased on the following assumptions:?
Independence: Each ranker can make its ownranking decisions.?
Diversity: Each ranker has its own strengths inmaking ranking decisions.?
Collaboration: Rankers in the group could col-laborate to make a consensus decision under somemechanism.Let x(q)j = ?
(q, o(q)j ) be the feature vector formedfrom the pair consisting of query q and an associatedobject o(q)j .
Let F?
= {f1, .
.
.
, fm} be m existingranking functions.
We transform the computation ofcollaboration among rankers into solving the follow-ing composite function g2:y(q)j = g2(f1(x(q)j), .
.
.
, fm(x(q)j)) (3)Similar with MiCR, g2 can be expressed by max,min, weighted (average) respectively:?
max: y(q)j = max{fi(x(q)j)}mi=1?
min: y(q)j = min{fi(x(q)j)}mi=1?
weighted: y(q)j =m?i=1wifi(x(q)j)It is worth noting that max and min can be use-ful only if the ranking scores produced by variousrankers can be compared to each other directly, how-ever, in practice, this can hardly be true.A special form of ranking problem is that onlythe best object is required as output.
In this case, wehave another version of g2 which is called voting:?
voting: y(q)j =m?i=1sign(fi(x(q)j))in which sign(?)
is an indicator functionsign(?)
={1 if fi outputs o(q)j as the best object0 otherwiseA general algorithm for MaCR is presented in Al-gorithm 2.Algorithm 2 MaCR Algorithm.Input:a query q; a set of objects o(q); a set of m rank-ing functions F?
; a composite function g2Output:a set of ranking scores y(q)1: for j = 1; j <= n(q); j + + do2: Form a feature vector x(q)j .3: Compute ranking scores:f1(x(q)j ), .
.
.
, fm(x(q)j ).4: Compute composite function: y(q)j = g2(?
).5: end for6: return y(q)3.3 Micro-Macro Collaborative Ranking(MiMaCR)The above two ranking approaches can be furtherintegrated into a joint model which is named Micro-Macro Collaborative Ranking (MiMaCR).
In orderto compute query-level and ranker-level collabora-tion jointly, we solve the following complex com-posite function g3:y(q)j = g2(g1(?))
(4)in which, for each object o(q)j , firstly we compute mmicro-ranking scores using m ranking functions onquery-level collaborators:774m????????
?g1(f1(x(q)j), f1(x(cq1)j), .
.
.
, f1(x(cqk)j)).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.g1(fm(x(q)j), fm(x(cq1)j), .
.
.
, fm(x(cqk)j))and secondly, we compute a macro-ranking scoreusing g2.We can similarly define g1 and g2 as those inMiCR andMaCR.
A general algorithm for MiMaCRis presented in Algorithm 3.Algorithm 3 MiMaCR Algorithm.Input:a query q; a set of objects o(q); a set of rankingfunctions F?
; functions g1, g2Output:a set of ranking scores y(q)1: Search k collaborators of q:cq(q) = {cq1, .
.
.
, cqk}.2: for j = 1; j <= n(q); j + + do3: Form k + 1 feature vectors: x(q)j , x(cq1)j , .
.
.
, x(cqk)j .4: Compute m micro-ranking scores using F?
and g1.5: Compute the macro-ranking score using g2.6: end for7: return y(q)4 A Case Study on Entity LinkingTo demonstrate the efficacy of our collaborativeranking scheme, we apply it to the entity linkingtask defined in the TAC-KBP2010 program (Ji etal., 2010) because there is a large amount of train-ing and evaluation data available and various non-collaborative ranking approaches have been pro-posed, as summarized in (McNamee and Dang,2009; Ji et al, 2010).4.1 Task DefinitionThe entity linking task aims to align a textual men-tion of a named entity (person,organization or geo-political) to an appropriate entry in a knowledgebase (KB), which may or may not contain the en-tity.
More formally, given a large corpus C, let q =(q.id, q.string, q.text) denote a query in the taskwhich is a triple consisting of query id (q.id), namestring (q.string) and context document (q.text ?C).
Let o(q) ={o(q)1 , .
.
.
, o(q)n(q)}denote the candi-date KB entries associated with the query.
Each KBentry is a tuple consisting of KB id, KB title, KB in-fobox (a set of attribute-value pairs that summarizeor highlight the key features of the concept or sub-ject of this entry) and KB text.
The goal is to rankthe KB entries and determine whether the top en-try id should be considered as the answer, otherwiseNIL should be returned.A specific example of the task is as follows,given a name string ?Michael Jordan?
and its con-text document ?...England Youth International goal-keeper Michael Jordan...?.
From the name string,we retrieve a set of candidate KB entries includ-ing ?Michael Jordan (mycologist)?, ?Michael Jor-dan (footballer)?, etc.
The entity linking systemshould return the id of ?Michael Jordan (footballer)?as the answer, rather than the id of ?Michael Jordan?who is most well known as a basketball player.4.2 General FrameworkA general framework of entity linking consists oftwo crucial components, one for candidate gener-ation, the other for candidate ranking, as shownin Figure 2.
In this paper, we developed the firstcomponent by following the procedures describedin (Chen et al, 2010) which extensively leveragedresources mined from Wikipedia.
The performanceof the first component is 96.8% measured by recall(the percentage of queries in which the candidatescover the true answer).
We then focus on the secondcomponent.KnowledgeBaseQuery Expansion &Candidate GenerationCandidate RankingAnswerQueryFigure 2: A general framework of entity linking sys-tem.4.3 Baseline RankersWe developed 8 baseline rankers, including 4 un-supervised rankers (f1, f2, f3, f4) and 4 supervisedrankers(f5, f6, f7, f8).775?Naive (f1): since the answer for each query caneither be a KB id or NIL, the naive ranker simplyoutputs NIL for all queries.
?Entity (f2): f2 is defined as weighted combina-tion of entity similarities in three types (person, or-ganization and geo-political).
Name entities are ex-tracted from q.text and KB text respectively usingStanford NER toolkit3.
The formulas to compute en-tity similarities are defined in (Yoshida et al, 2010).
?Tfidf (f3): f3 is defined as cosine similarity be-tween q.text and KB text using tfidf weights.
?Profile (f4): f4 is defined as profile similaritybetween q.text and KB text (Chen et al, 2010).We used a slot filling toolkit (Chen et al, 2011) togenerate the profile (attribute-value pairs) for eachquery.
?Maxent (f5): a pointwise ranker implementedusing OpenNLP Maxent toolkit4 which is based onmaximum entropy model.
?SVM (f6): a pointwise ranker implemented us-ing SV M light (Joachims, 1999).
?SVM ranking (f7): a pairwise ranker imple-mented using SV M rank (Joachims, 2006).
?ListNet (f8): a listwise ranker presented in (Caoet al, 2007).The four supervised rankers apply exactly thesame set of features except that SVM ranking (f7)needs to double expand the feature vector.
The fea-tures are categorized into three levels, surface fea-tures (Dredze et al, 2010; Zheng et al, 2010), doc-ument features (Dredze et al, 2010; Zheng et al,2010), and profiling features (entity slots that are ex-tracted by the slot filling toolkit (Chen et al, 2011)).4.4 MiCR for Entity LinkingWe convert the collaborator searching problem intoa clustering problem, i.e., for a given query q in thetask, we retrieve at most K = 300 documents fromthe large corpus C, each of which contains q.string;we then apply a clustering algorithm to generateclusters over the documents, and form query collab-orators (excluding q.text) from the cluster that con-tains q.text.We experimented the following two clustering ap-proaches:3http://nlp.stanford.edu/software/CRF-NER.shtml4http://maxent.sourceforge.net/about.html(1)agglomerative clustering: it iteratively mergesclusters from singleton documents until a stopthreshold is reached.
Document similarity is de-fined as cosine similarity using tfidf weights.
We ap-plied group-average linking strategy to merge clus-ters (Manning et al, 2008).
(2)graph-based clustering: it iteratively partitionsclusters from one single cluster until a stop thresholdis reached.
Document similarity is similarly definedas agglomerative clustering.
We selected normalizedspectral clustering as our clustering algorithm (Shiand Malik, 2000).We first selected f3 as our basic ranking func-tion, and investigated whether the ranker can ben-efit from query collaborators formed by either ag-glomerative clustering or graph clustering.
We im-plemented three versions of composite function g1(max, min and average), and experimented their per-formance using three unsupervised rankers f2, f3, f4respectively.Last, we implemented three supervised versionsof g1 (Maxent, SVM and ListNet respectively) byadding cluster-level features and retraining the mod-els in three supervised rankers f5, f6, f8 respec-tively.
Cluster-level features include maximum,minimum, average tfidf/entity similarities betweenthe candidate and the query collaboration group.4.5 MaCR for Entity LinkingWe implemented two versions of composite func-tion g2, average and voting.
Furthermore, we in-vestigated how the performance can be affected byincrementally adding more rankers into the rankerset F?.
To do so, we first sorted the 8 rankers ac-cording to their performance on the development setfrom the highest to the lowest, and starting with thehighest performance ranker, we added one ranker ata time, until we have all the 8 rankers.
It is worthnoting that, when there are even number of rankersin the set F?, ?ties?
could take place using votingfunction.
In order to break the ties, we rank thecandidate higher if it is output as the answer froma higher performance ranker.4.6 MiMaCR for Entity LinkingWe investigated how the final performance can beboosted by jointly computing micro-ranking scoresand macro-ranking score.7765 Experiments5.1 Data and Evaluation MetricWe used TAC-KBP2009 evaluation data as our train-ing (75%) and development set (25%), and usedTAC-KBP2010 evaluation data as our blind testingset (shown in Table 2).Corpus QueriesPER ORG GPE TotalTraining&Dev 627 2710 567 3904Testing 750 750 750 2250Table 2: Training, development and testing corpus.The reference KB consists of 818,741 entrieswhich are extracted from an October 2008 dump ofEnglish Wikipedia.
The source text corpus (denotedas C in section 4.1) consists of 1,777,888 documentsin 5 genres (mostly Newswire and Web Text).We used the official evaluation metric for TAC-KBP2010 entity linking task, that is, micro-averagedaccuracy.
It is computed bymicro-averaged accuracy = #correct answers#queriesAn answer is considered as correct if the systemoutput (either a KB entry id or NIL) exactly matchesthe key.5.2 Performance of 8 Baseline RankersTable 3 shows the performance of the 8 baselinerankers in 4 columns: Overall for all queries, PERfor person queries, ORG for organization queries,and GPE for geo-political queries.
Each columnis further split into All, KB (for Non-NIL queries)and NIL (for NIL queries).
It shows that all the foursupervised rankers perform better than the four un-supervised rankers.
Naive ranker obtains the low-est overall micro-average accuracy (54.5%) but thehighest NIL accuracy (100%).
Among the four un-supervised rankers, profile ranker performs the best,which clearly shows that the extracted attributes ofentities are effective for disambiguating confusablenames.
For example, our data analysis shows thatthe attribute value of ?per:alternative-name?
fromthe context document is particularly useful if a per-son query is only mentioned by its last name.
Theattribute ?per:title?
is another important indicator todiscriminate one person from the other.
For geo-political queries, if the query is a city name, at-tribute ?gpe:state?
is useful to distinguish cities withthe same name but in different states or provinces.Among the four supervised rankers, ListNet outper-forms SVM ranking and then SVM ranking outper-forms the two pointwise rankers.
It may confirmprevious research findings that listwise ranking issuperior to pairwise ranking and pairwise rankingis superior to pointwise ranking (Cao et al, 2007;Zheng et al, 2010).
The best baseline ranker (List-Net) obtains an absolute overall accuracy gain of26.6% over the naive ranker.5.3 Impact of MiCRTo study the impact of MiCR, we first select f3(tfidf ranker) as our ranking function.
Figure 3shows the performance of applying different querycollaborator searching strategies (graph or agglom-erative clustering) and different versions of g1 (av-erage, max and min respectively).
We intention-ally adjust the meaning of threshold (x-axis) for bothgraph clustering and agglomerative clustering, suchthat at threshold 0, both clustering algorithms gen-erate the largest number of clusters (i.e., each doc-ument is a cluster), and at threshold 1, they gen-erate only one cluster.
We now take the averagefunction (Figure 3 (a)) into considerations, as graphclustering algorithm gradually partitions from onecluster (corresponding to threshold 1) to more clus-ters, the number of query collaborators gradually re-duces, meanwhile, the accuracy gradually increasesand reaches the highest (73.6%) at threshold of 0.45,which clearly shows that removing noisy collabora-tors in the query collaboration group can improvethe performance.
As the threshold continues drop-ping below 0.45, the number of query collaboratorsreduces and the performance significantly drops un-til it reaches the baseline performance of tfidf ranker(68.3%).
It clearly shows that maintaining a control-lable number of query collaborators can improve theperformance.
For the agglomerative clustering, it isthe other story.
As it continues merging from sin-gleton clusters (corresponding to threshold 0) to onesingle cluster, the performance continues increasinguntil in the end it reaches the highest accuracy of72.6%.
However, unlike graph clustering, a peaknever appears in the middle which implies that ag-glomerative clustering is inferior to graph clustering.777Overall (%) PER (%) ORG (%) GPE (%)All KB NIL All KB NIL All KB NIL All KB NILNaive 54.5 0.0 100 70.8 0.0 100 59.7 0 100 33.0 0 100Entity 65.6 48.6 79.7 82.1 52.1 94.5 68.4 46.2 83.3 46.1 48.5 41.3Tfidf 68.3 45.0 87.7 83.6 54.3 95.7 66.2 45.9 80.0 54.9 40.3 84.6Profile 75.0 58.7 88.6 90.8 82.2 94.4 73.3 62.7 80.4 61.0 46.1 91.1Maxent 77.4 72.3 81.6 86.5 82.6 94.4 73.3 62.7 80.4 61.0 71.5 72.1SVM 78.1 73.0 82.3 91.1 81.7 94.9 78.7 70.0 84.6 64.4 71.1 51.0SVM Rank 80.3 66.7 91.7 91.3 76.3 97.6 77.3 59.7 89.1 72.3 66.7 83.8ListNet 81.1 69.7 90.6 90.8 77.6 96.2 79.0 64.0 89.1 73.5 69.7 81.4Table 3: Comparison of 8 baseline rankers.0.0 0.2 0.4 0.6 0.8 1.0686970717273740.0 0.2 0.4 0.6 0.8 1.067.067.568.068.569.069.570.070.50.0 0.2 0.4 0.6 0.8 1.05658606264666870(c) Min Function(b) Max Function(a) Average FunctionMicro-AverageAccuracy(%)ThresholdGraph-AveAggr-Ave73.672.668.367.470.268.3ThresholdMicro-AverageAccuracy(%)Graph-MaxAggr-Max69.056.768.3ThresholdMicro-AverageAccuracy(%)Graph-MinAggr-MinFigure 3: MiCR: comparison of average, max, and min functions combined with Graph and Agglomerative(Aggr)-based query collaborator searching strategies (tfidf ranker).The max function (Figure 3 (b)) leverages thestrengths from the strongest collaborator in thegroup, which can potentially improve KB accuracy,but meanwhile hurt NIL accuracy.
As shown in thefigure, as more collaborators join in the group, theperformance increases first for both graph and ag-glomerative clustering, however, it starts to deterio-rate when arriving at a threshold, and in the end, theperformance drops even lower than the baseline oftfidf ranker.The min function (Figure 3 (c)) leverages thestrengths from the weakest collaborator in the group,which can potentially improve NIL accuracy, butmeanwhile hurt KB accuracy.
Our data analysisshows that the gain in NIL accuracy can not affordthe larger loss in non-NIL accuracy, therefore, theperformance continues dropping as the threshold in-creases.
Min function is a counter example showingthat searching query collaborators can not alwayslead to benefits.To summarize so far, the best strategy for tfidfranker in MiCR approach is graph-ave (applyinggraph clustering and using average function) whichobtains overall accuracy gain of 5.3% over the base-line (68.3%).
We further validate the performanceof graph-ave using f2, f4 ranking functions, for en-tity ranker, we obtain accuracy gain of 6.3%, and forprofile ranker, we obtain accuracy gain of 3.0%.We then experiment the three supervised g1 func-tions (ListNet, Maxent, and SVM respectively)using graph clustering as the query collaboratorsearching strategy.
Figure 6 shows that ListNet,Maxent, SVM rankers obtain accuracy gain of 1.4%,4.6%, 4.2% respectively over the baselines (corre-sponding to those points at threshold 0).5.4 Impact of MaCRFigure 4 shows that the MaCR approach obtainsabsolute accuracy gain of 1.3% (voting function)and 0.5% (average function) over the best baselineranker (81.1%) when we add the 7th ranker (entityranker).
The improvement of voting function is sta-tistically significant at a 99.6% confidence level byconducting Wilcoxon Matched-Pairs Signed-RanksTest on the 10 folds of the testing set.
However, theimprovement of average function is not significant atthe 0.05 level which implies that average is inferiorto voting.
We observe that the performance drops778when there are even number of rankers in the rankerset using voting function, which implies that our tiebreaking strategy is not very effective.We also experimented the voting function on thetop 10 KBP2009 entity linking systems (each sys-tem performance is shown in the table embedded inFigure 5, and experiment is similarly done as de-scribed in section 4.5).
Figure 5 shows that it canobtain absolute accuracy gain of 4.7% over the topentity linking system (82.2%).
The reasons why weachieve relative smaller gains using our own rankerset are as follows: (1) we use the same candidateobject set for all rankers, while different KBP2009systems may use their own set of objects.
(2) ourtop 4 supervised rankers apply almost the same setof features, while different KBP2009 systems mayapply more diversified features.
Therefore, diversityis a highly important factor that makes MaCR ap-proach effective.0 1 2 3 4 5 6 7 8 980.881.081.281.481.681.882.082.282.482.681.181.681.681.481.281.080.881.782.481.882.281.1 81.381.7Micro-Average Accuracy(%)#rankersvoting average81.2Figure 4: MaCR: comparison of voting and average.0 1 2 3 4 5 6 7 8 9 10 1182838485868783.784.68585.986.485.885.182.2Micro-Average Accuracy(%)#systems (rankers)System ID Performance1 82.22 80.33 79.84 78.85 76.76 73.57 71.18 68.29 65.910 59.682.286.9Figure 5: MaCR: applying voting function to the top10 KBP2009 entity linking systems.5.5 Impact of MiMaCRWe applied the following settings in our Mi-MaCR approach: selecting graph clustering as thequery collaborator searching strategy, including fiverankers (tfidf, entity, Maxent, SVM and ListNet) inthe ranker set, using average function to computemicro-ranking scores for the tfidf and entity ranker,using the three corresponding supervised versionsof g1 to compute micro-ranking scores for Maxent,SVM and ListNet respectively, and finally apply-ing voting function to compute the macro-rankingscore.
In Figure 6, the curve of ?MiMaCR?
showshow the performance of MiMaCR is affected bythe threshold in graph clustering.
We obtain thebest micro-average accuracy of 83.7% at threshold0.3, which is 2.6 % higher than the best baselineranker (81.1%).
The improvement is statisticallysignificant at a 98.6% confidence level by conduct-ing Wilcoxon Matched-Pairs Signed-Ranks Test onthe 10 folds of the testing set.
The score reportedhere is on par with the second best in the KBP2010evaluation.0.0 0.2 0.4 0.6 0.8 1.077.077.578.078.579.079.580.080.581.081.582.082.583.083.584.082.082.3 82.577.481.178.1Micro-AverageAccuracy (%)ThresholdMiMaCR ListNet Maxent SVM83.782.3Figure 6: MiMaCR: Comparison of MiMaCR andthree supervised versions of g1 (ListNet, Maxent,and SVM respectively).6 Related WorkIn the literature of information retrieval, query ex-pansion is a useful technique that involves the pro-cess of reformulating a query, and as a consequence,is capable to extend the ability of a query and im-prove the retrieval performance.
Various approachesfor query expansion have been proposed, as summa-rized in (Manning et al, 2008).
TheMiCR presentedin this paper is superior to query expansion in twoaspects, firstly, we leverage more information con-tained in multiple query collaborators; secondly, weplace great emphasis on interactions among mem-bers in the query collaboration group.In the literature of machine learning, therehas been a considerable amount of research onensemble-based classification, which is to build apredictive classification model by integrating multi-ple classifiers.
A comprehensive survey is presentedin (Rokach, 2009).
In contrast, ensemble-basedranking has only recently attracted research interests(Hoi and Jin, 2008; Wei et al, 2010).
Although theMaCR presented here is in essence ensemble-based779ranking, we extend it to MiMaCR which integratesthe strengths from both MiCR and MaCR.It is worth noting that ?collaborative ranking?
pre-sented here should be distinguished from ?collabo-rative filtering?
in that ?collaborative filtering?
usesthe known preferences of a group of users to gen-erate personalized recommendations while ?collab-orative ranking?
leverages query collaborators andranker collaborators to enhance the overall rankingperformance.There has been an increasing amount of researchon entity linking, especially through KBP2009 andKBP2010.
Various unsupervised or supervised ap-proaches have been proposed, as summarized in(McNamee and Dang, 2009; Ji et al, 2010).
How-ever, most of the previous research mainly fo-cused on one or two ranking algorithms on isolatedqueries.
In this paper, we have extended the workby systematically studying the possibility of perfor-mance enhancement through query-level collabora-tion and ranker-level collaboration.7 ConclusionsWe presented a new ranking scheme called collab-orative ranking with three specific forms, MiCR,MaCR and MiMaCR and demonstrated its effective-ness on entity linking task.
However, our scheme isnot restricted to this specific task and it is generallyapplicable to many other other applications such asquestion answering.
In MiCR, effective searchingof query collaborators and active interplay amongmembers in the query collaboration group are twokey factors that make MiCR successful.
In MaCR,diversity is a highly important factor to make it suc-cessful.
Overall, MiMaCR can bootstrap the per-formance to its maximum if integrating MiCR andMaCR properly.
However, the better performance isat the expense of much more computations.AcknowledgmentsThis work was supported by the U.S. Army Re-search Laboratory under Cooperative AgreementNumber W911NF-09-2-0053, the U.S. NSF CA-REER Award under Grant IIS-0953149 and PSC-CUNY Research Program.
The views and con-clusions contained in this document are those ofthe authors and should not be interpreted as repre-senting the official policies, either expressed or im-plied, of the Army Research Laboratory or the U.S.Government.
The U.S. Government is authorizedto reproduce and distribute reprints for Govern-ment purposes notwithstanding any copyright nota-tion hereon.ReferencesP.
L. Bartlett, M. I. Jordan and J. D. McAuliffe.
2003.Convexity, classification, and risk bounds.
TechnicalReport 638, Statistics Department, University of Cali-fornia, Berkeley.C.
Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,N.
Hamilton, and G. Hullender.
2005.
Learning toRank Using Gradient Descent.
In Proceedings of the22th International Conference on Machine Learning(ICML 2005).Z.
Cao, T. Qin, T.-Y.
Liu, M.-F. Tsai and H. Li.
2007.Learning to rank: from pairwise approach to listwiseapproach In Proceedings of the 24th InternationalConference on Machine Learning (ICML 2007), pages129-136.E.
Charniak and M. Johnson.
2005.
Coarseto-fine-grained n-best parsing and discriminative reranking.In ACL-05, pages 173-180.W.
Chen, T.-Y.
Liu, Y. Lan, Z. Ma, and H. Li.
2009.Ranking measures and loss functions in learning torank.
In Advances in Neural Information ProcessingSystems 22 (NIPS 2009), pages 315-323.Z.
Chen, S. Tamang, A. Lee, X. Li, W.-P. Lin, M. Snover,J.
Artiles, M. Passantino and H. Ji.
2010.
CUN-YBLENDER TAC-KBP2010 Entity Linking and SlotFilling SystemDescription.
In Proceedings of Text An-alytics Conference (TAC2010).Z.
Chen, S. Tamang, A. Lee and H. Ji.
2011.
A Toolkitfor Knowledge Base Population.
In SIGIR.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of the 17th Interna-tional Conference on Machine Learning (ICML 2000),pages 175-182.M.
Dredze, P. McNamee, D. Rao, A. Gerber and T. Finin.2010.
Entity Disambiguation for Knowledge BasePopulation.
In Proc.
COLING 2010.Y.
Freund, R. Iyer, R. Schapire, and Y.
Singer.
2003.An efficient boosting algorithm for combining pref-erences.
In Journal of Machine Learning Research,4:933-969.S.
Hoi and R. Jin.
2008.
Semi-supervised ensembleranking.
In Proc.
of the 23rd AAAI Conf.
on ArtificialIntelligence.L.
Huang.
2008.
Forest Reranking: Discriminative Pars-ing with Non-Local Features.
In ACL-HLT-08, pages586-594.780H.
Ji, R. Grishman, H. T. Dang and K. Griffit.
2010.
AnOverview of the TAC2010 Knowledge Base Popula-tion Track.
In Proceedings of Text Analytics Confer-ence (TAC2010).T.
Joachims.
1999.
Making large-Scale SVM Learn-ing Practical.
Advances in Kernel Methods - SupportVector Learning, B. Scho?lkopf and C. Burges and A.Smola (ed.
), MIT-Press, 1999.T.
Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the 8thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining(KDD 2002).T.
Joachims.
2006.
Training Linear SVMs in LinearTime.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Y.
Lan, T.-Y.
Liu, T. Qin, Z. Ma, and H. Li.
2008.
Query-level stability and generalization in learning to rank.In Proceedings of the 25th International Conferenceon Machine Learning (ICML 2008), pages 512-519.P.
Li, C. Burges, and Q. Wu.
2007.
Mcrank: Learn-ing to rank using multiple classification and gradientboosting In Advances in Neural Information Process-ing Systems 20 (NIPS2007).Y.
Lin.
2002.
Support vector machines and the bayesrule in classification.
In Data Mining and KnowledgeDiscovery, pages 259-275.C.
D. Manning, P. Raghavan and H. Schu?tze.
2008 .
In-troduction to Information Retrieval.
Cambridge Uni-versity Press.P.
McNamee and H. Dang.
2009.
Overview of the TAC2009 Knowledge Base Population Track.
In Proceed-ings of TAC.R.
Nallapati.
2004.
Discriminative models for informa-tion retrieval.
In SIGIR.F.
J. Och.
2002.
Statistical Machine Translation:From Single-Word Models to Alignment Templates.Ph.D.
thesis, Computer Science Department, RWTHAachen, Germany, October.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The PageRank Citation Ranking: Bringing Order tothe Web.
Technical report, Stanford Digital LibraryTechnologies Project.T.
Qin, X.-D. Zhang, M.-F. Tsai, D.-S. Wang, T.-Y.
Liuand H. Li.
2007.
Query-level loss functions for infor-mation retrieval.
In Information Processing and Man-agement.T.
Qin, T.-Y.
Liu, X.-D. Zhang,D.-S. Wang, and H. Li.2008.
Global Ranking Using Continuous ConditionalRandom Fields.
In Advances in Neural InformationProcessing Systems 21 (NIPS 2008).D.
Ravichandran, E. Hovy and F. J. Och.
2003.
Statis-tical QA - Classifier vs. Re-ranker: What?s the differ-ence?
In Proceedings of the ACL Workshop on Multi-lingual Summarization and Question Answering.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittaland Y. Liu.
2007.
Statistical Machine Translation forQuery Expansion in Answer Retrieval.
In Proceedingsof ACL.L.
Rokach.
2009.
Ensemble-based classifiers.
Artif In-tell Rev DOI 10.1007/s10462-009-9124-7.L.
Shen, A. Sarkar, and F. J. Och.
2005.
Discriminativereranking for machine translation.
In Proceedings ofHLT-NAACL.J.
Shi and J. Malik.
2000.
Normalized Cuts and ImageSegmentation.
In Machine Intelligence, vol.
22, no.
8,pages 888-905.F.
Wei, W. Li and S. Liu.
2010. iRANK: A rank-learn-combine framework for unsupervised ensemble rank-ing.
In Journal of the American Society for Infor-mation Science and Technology,61: 1232C1243.
doi:10.1002/asi.21296.X.
Yang and J. Su and C.L.
Tan 2008.
A Twin-CandidateModel for Learning-based Anaphora Resolution.
InComputational Linguistics, vol.
34, no.
3, pages 327-356.M.
Yoshida, M. Ikeda, S. Ono, I. Sato, and H. Nakagawa.2010.
Person name disambiguation by boostrapping.In SIGIR.T.
Zhang.
2004.
Statistical analysis of some multicate-gory large margin classification methods.
In Journalof Machine Learning Research, 5, 1225-1251.W.
Zhang, J. Su, C. L. Tan and W.T.
Wang.
2010.
EntityLinking Leveraging Automatically Generated Annota-tion.
In Proc.
COLING 2010.Z.
Zheng, F. Li, M. Huang, X. Zhu.
2010.
Learning toLink Entities with Knowledge Base.
In Proc.
HLT-NAACL2010.781
