Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2243?2248,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Generate Compositional Color DescriptionsWill Monroe,1 Noah D. Goodman,2 and Christopher Potts3Departments of 1Computer Science, 2Psychology, and 3LinguisticsStanford University, Stanford, CA 94305wmonroe4@cs.stanford.edu, {ngoodman, cgpotts}@stanford.eduAbstractThe production of color language is essentialfor grounded language generation.
Color de-scriptions have many challenging properties:they can be vague, compositionally complex,and denotationally rich.
We present an effec-tive approach to generating color descriptionsusing recurrent neural networks and a Fourier-transformed color representation.
Our modeloutperforms previous work on a conditionallanguage modeling task over a large corpusof naturalistic color descriptions.
In addition,probing the model?s output reveals that it canaccurately produce not only basic color termsbut also descriptors with non-convex denota-tions (?greenish?
), bare modifiers (?bright?,?dull?
), and compositional phrases (?fadedteal?)
not seen in training.1 IntroductionColor descriptions represent a microcosm ofgrounded language semantics.
Basic color termslike ?red?
and ?blue?
provide a rich set of seman-tic building blocks in a continuous meaning space;in addition, people employ compositional color de-scriptions to express meanings not covered by ba-sic terms, such as ?greenish blue?
or ?the color ofthe rust on my aunt?s old Chevrolet?
(Berlin andKay, 1991).
The production of color language isessential for referring expression generation (Krah-mer and Van Deemter, 2012) and image captioning(Kulkarni et al, 2011; Mitchell et al, 2012), amongother grounded language generation problems.We consider color description generation as agrounded language modeling problem.
We presentColor Top-1 Sample(83, 80, 28) ?green?
?very green?
(232, 43, 37) ?blue?
?royal indigo?
(63, 44, 60) ?olive?
?pale army green?
(39, 83, 52) ?orange?
?macaroni?Table 1: A selection of color descriptions sampled from ourmodel that were not seen in training.
Color triples are in HSL.Top-1 shows the model?s highest-probability prediction.an effective new model for this task that uses a longshort-term memory (LSTM) recurrent neural net-work (Hochreiter and Schmidhuber, 1997; Graves,2013) and a Fourier-basis color representation in-spired by feature representations in computer vision.We compare our model with LUX (McMahan andStone, 2015), a Bayesian generative model of colorsemantics.
Our model improves on their approachin several respects, which we demonstrate by exam-ining the meanings it assigns to various unusual de-scriptions: (1) it can generate compositional colordescriptions not observed in training (Fig.
3); (2) itlearns correct denotations for underspecified modi-fiers, which name a variety of colors (?dark?, ?dull?;Fig.
2); and (3) it can model non-convex denota-tions, such as that of ?greenish?, which includesboth greenish yellows and blues (Fig.
4).
As a result,our model also produces significant improvementson several grounded language modeling metrics.2 Model formulationFormally, a model of color description generation isa probability distribution S(d | c) over sequences of2243c<s> light blue</s>light blueLSTMFCsoftmaxclight blueFCFCsoftmaxf f f fd0 d1 d2Figure 1: Left: sequence model architecture; right: atomic-description baseline.
FC denotes fully connected layers.tokens d conditioned on a color c, where c is repre-sented as a 3-dimensional real vector in HSV space.1Architecture Our main model is a recurrent neu-ral network sequence decoder (Fig.
1, left panel).An input color c = (h, s, v) is mapped to a rep-resentation f (see Color features, below).
At eachtime step, the model takes in a concatenation of fand an embedding for the previous output token di,starting with the start token d0 = <s>.
This con-catenated vector is passed through an LSTM layer,using the formulation of Graves (2013).
The out-put of the LSTM at each step is passed through afully-connected layer, and a softmax nonlinearity isapplied to produce a probability distribution for thefollowing token.2 The probability of a sequence isthe product of probabilities of the output tokens upto and including the end token </s>.We also implemented a simple feed-forward neu-ral network, to demonstrate the value gained bymodeling descriptions as sequences.
This architec-ture (atomic; Fig.
1, right panel) consists of twofully-connected hidden layers, with a ReLU nonlin-earity after the first and a softmax output over allfull color descriptions seen in training.
This modeltherefore treats the descriptions as atomic symbolsrather than sequences.Color features We compare three representations:?
Raw: The original 3-dimensional color vectors,in HSV space.1HSV: hue-saturation-value.
The visualizations and tablesin this paper instead use HSL (hue-saturation-lightness), whichyields somewhat more intuitive diagrams and differs from HSVby a trivial reparameterization.2Our implementation uses Lasagne (Dieleman et al, 2015),a neural network library based on Theano (Al-Rfou et al, 2016).?
Buckets: A discretized representation, dividingHSV space into rectangular regions at three res-olutions (90?10?10, 45?5?5, 1?1?1) andassigning a separate embedding to each region.?
Fourier: Transformation of HSV vectors intoa Fourier basis representation.
Specifically, therepresentation f of a color (h, s, v) is given byf?jk` = exp [?2pii (jh?
+ ks?
+ `v?
)]f =[Re{f?}
Im{f?}]
j, k, ` = 0..2where (h?, s?, v?)
= (h/360, s/200, v/200).The Fourier representation is inspired by the use ofFourier feature descriptors in computer vision appli-cations (Zhang and Lu, 2002).
It is a nonlinear trans-formation that maps the 3-dimensional HSV spaceto a 54-dimensional vector space.
This representa-tion has the property that most regions of color spacedenoted by some description are extreme along asingle direction in Fourier space, thus largely avoid-ing the need for the model to learn non-monotonicfunctions of the color representation.Training We train using Adagrad (Duchi et al,2011) with initial learning rate ?
= 0.1, hidden layersize and cell size 20, and dropout (Hinton et al,2012) with a rate of 0.2 on the output of the LSTMand each fully-connected layer.
We identified thesehyperparameters with random search, evaluating ona held-out subset of the training data.We use random normally-distributed initializationfor embeddings (?
= 0.01) and LSTM weights (?
=0.1), except for forget gates, which are initialized toa constant value of 5.
Dense weights use normalizeduniform initialization (Glorot and Bengio, 2010).3 ExperimentsWe demonstrate the effectiveness of our model us-ing the same data and statistical modeling metrics asMcMahan and Stone (2015).Data The dataset used to train and evaluate ourmodel consists of pairs of colors and descriptionscollected in an open online survey (Munroe, 2010).Participants were shown a square of color and askedto write a free-form description of the color ina text box.
McMahan and Stone filtered the re-sponses to normalize spelling differences and ex-clude spam responses and descriptions that occurred2244Model Feats.
Perp.
AIC Acc.atomic raw 28.31 1.08?106 28.75%atomic buckets 16.01 1.31?106 38.59%atomic Fourier 15.05 8.86?105 38.97%RNN raw 13.27 8.40?105 40.11%RNN buckets 13.03 1.26?106 39.94%RNN Fourier 12.35 8.33?105 40.40%HM buckets 14.41 4.82?106 39.40%LUX raw 13.61 4.13?106 39.55%RNN Fourier 12.58 4.03?106 40.22%Table 2: Experimental results.
Top: development set; bottom:test set.
AIC is not comparable between the two splits.
HM andLUX are from McMahan and Stone (2015).
We reimplementedHM and re-ran LUX from publicly available code, confirmingall results to the reported precision except perplexity of LUX,for which we obtained a figure of 13.72.very rarely.
The resulting dataset contains 2,176,417pairs divided into training (1,523,108), development(108,545), and test (544,764) sets.Metrics We quantify model effectiveness with thefollowing evaluation metrics:?
Perplexity: The geometric mean of the recip-rocal probability assigned by the model to thedescriptions in the dataset, conditioned on therespective colors.
This expresses the same ob-jective as log conditional likelihood.
We followMcMahan and Stone (2015) in reporting per-plexity per-description, not per-token as in thelanguage modeling literature.?
AIC: The Akaike information criterion(Akaike, 1974) is given by AIC = 2` + 2k,where ` is log likelihood and k is the totalnumber of real-valued parameters of the model(e.g., weights and biases, or bucket proba-bilities).
This quantifies a tradeoff betweenaccurate modeling and model complexity.?
Accuracy: The percentage of most-likely de-scriptions predicted by the model that exactlymatch the description in the dataset (recall@1).Results The top section of Table 2 shows devel-opment set results comparing modeling effective-ness for atomic and sequence model architecturesand different features.
The Fourier feature transfor-mation generally improves on raw HSV vectors anddiscretized embeddings.
The value of modeling de-scriptions as sequences can also be observed in theseresults; the LSTM models consistently outperformtheir atomic counterparts.Additional development set experiments (notshown in Table 2) confirmed smaller design choicesfor the recurrent architecture.
We evaluated a modelwith two LSTM layers, but we found that the modelwith only one layer yielded better perplexity.
Wealso compared the LSTM with GRU and vanilla re-current cells; we saw no significant difference be-tween LSTM and GRU, while using a vanilla recur-rent unit resulted in unstable training.
Also note thatthe color representation f is input to the model at ev-ery time step in decoding.
In our experiments, thisyielded a small but significant improvement in per-plexity versus using the color representation as theinitial state.Test set results appear in the bottom section.
Ourbest model outperforms both the histogram baseline(HM) and the improved LUX model of McMahanand Stone (2015), obtaining state-of-the-art resultson this task.
Improvements are highly significanton all metrics (p < 0.001, approximate permutationtest, R = 10,000 samples; Pad?, 2006).4 AnalysisGiven the general success of LSTM-based mod-els at generation tasks, it is perhaps not surprisingthat they yield good raw performance when appliedto color description.
The color domain, however,has the advantage of admitting faithful visualiza-tion of descriptions?
semantics: colors exist in a 3-dimensional space, so a two-dimensional visualiza-tion can show an acceptably complete picture of anentire distribution over the space.
We exploit thisto highlight three specific improvements our modelrealizes over previous ones.We construct visualizations by querying themodel for the probability S(d | c) of the same de-scription for each color in a uniform grid, summingthe probabilities over the hue dimension (left cross-section) and the saturation dimension (right cross-section), normalizing them to sum to 1, and plottingthe log of the resulting values as a grayscale image.2245020406080100Lightness0 20 40 60 80 100Saturation"light"020406080100Lightness0 20 40 60 80 100Saturation"bright"020406080100Lightness0 20 40 60 80 100Saturation"dark"020406080100Lightness0 20 40 60 80 100Saturation"dull"Figure 2: Conditional likelihood of bare modifiers according toour generation model as a function of color.
White representsregions of high likelihood.
We omit the hue dimension, as thesemodifiers do not express hue constraints.Formally, each visualization is a pair of functions(L,R), whereL(s, `) = log[?
dh S(d | c = (h, s, `))?dc?
S(d | c?
)]R(h, `) = log[?
ds S(d | c = (h, s, `))?dc?
S(d | c?
)]The maximum value of each function is plotted aswhite, the minimum value is black, and intermediatevalues linearly interpolated.Learning modifiers Our model learns accuratemeanings of adjectival modifiers apart from the fulldescriptions that contain them.
We examine this inFig.
2, by plotting the probabilities assigned to thebare modifiers ?light?, ?bright?, ?dark?, and ?dull?.?Light?
and ?dark?
unsurprisingly denote high andlow lightness, respectively.
Less obviously, theyalso exclude high-saturation colors.
?Bright?, on theother hand, features both high-lightness colors andsaturated colors?
?bright yellow?
can refer to theprototypical yellow, whereas ?light yellow?
cannot.Finally, ?dull?
denotes unsaturated colors in a vari-ety of lightnesses.Compositionality Our model generalizes to com-positional descriptions not found in the training set.Fig.
3 visualizes the probability assigned to the020406080100Lightness0 20 40 60 80 100Saturation0 60 120 180 240 300Hue"faded"020406080100Lightness0 20 40 60 80 100Saturation0 60 120 180 240 300Hue"teal"020406080100Lightness0 20 40 60 80 100Saturation0 60 120 180 240 300Hue"faded teal"Figure 3: Conditional likelihood of ?faded?, ?teal?, and ?fadedteal?.
The two meaning components can be seen in the twocross-sections: ?faded?
denotes a low saturation value, and?teal?
denotes hues near the center of the spectrum.020406080100Lightness0 20 40 60 80 100Saturation0 60 120 180 240 300Hue"greenish"020406080100Lightness0 20 40 60 80 100Saturation0 60 120 180 240 300Hue"greenish"Figure 4: Conditional likelihood of ?greenish?
as a function ofcolor.
The distribution is bimodal, including greenish yellowsand blues but not true greens.
Top: LUX; bottom: our model.novel utterance ?faded teal?, along with ?faded?
and?teal?
individually.
The meaning of ?faded teal?
isintersective: ?faded?
colors are lower in saturation,excluding the colors of the rainbow (the V on theright side of the left panel); and ?teal?
denotes col-ors with a hue near 180?
(center of the right panel).Non-convex denotations The Fourier featuretransformation and the nonlinearities in the modelallow it to capture a rich set of denotations.
In partic-ular, our model addresses the shortcoming identifiedby McMahan and Stone (2015) that their model can-not capture non-convex denotations.
The description2246Color Top-1 Sample(36, 86, 63) ?orange?
?ugly?
(177, 85, 26) ?teal?
?robin?s?
(29, 45, 71) ?tan?
?reddish green?
(196, 27, 71) ?grey?
?baby royal?Table 3: Error analysis: some color descriptions sampled fromour model that are incorrect or incomplete.?greenish?
(Fig.
4) has such a denotation: ?green-ish?
specifies a region of color space surrounding,but not including, true greens.Error analysis Table 3 shows some examples oferrors found in samples taken from the model.
Themain type of error the system makes is ungrammati-cal descriptions, particularly fragments lacking a ba-sic color term (e.g., ?robin?s?).
Rarer are grammati-cal but meaningless compositions (?reddish green?
)and false descriptions.
When queried for its singlemost likely prediction, argmaxd S(d | c), the resultis nearly always an acceptable, ?safe?
description?manual inspection of 200 such top-1 predictions didnot identify any errors.5 Conclusion and future workWe presented a model for generating composi-tional color descriptions that is capable of produc-ing novel descriptions not seen in training and sig-nificantly outperforms prior work at conditional lan-guage modeling.3 One natural extension is theuse of character-level sequence modeling to capturecomplex morphology (e.g., ?-ish?
in ?greenish?
).Kawakami et al (2016) build character-level mod-els for predicting colors given descriptions in addi-tion to describing colors.
Their model uses a Lab-space color representation and uses the color to ini-tialize the LSTM instead of feeding it in at each timestep; they also focus on visualizing point predictionsof their description-to-color model, whereas we ex-amine the full distributions implied by our color-to-description model.Another extension we plan to investigate is mod-eling of context, to capture how people describe col-ors differently to contrast them with other colors via3We release our code at https://github.com/stanfordnlp/color-describer.pragmatic reasoning (DeVault and Stone, 2007; Gol-land et al, 2010; Monroe and Potts, 2015).AcknowledgmentsWe thank Jiwei Li, Jian Zhang, Anusha Balakrish-nan, and Daniel Ritchie for valuable advice anddiscussions.
This research was supported in partby the Stanford Data Science Initiative, NSF BCS1456077, and NSF IIS 1159679.ReferencesHirotugu Akaike.
1974.
A new look at the statisticalmodel identification.
IEEE Transactions on AutomaticControl, 19(6):716?723.Rami Al-Rfou, Guillaume Alain, Amjad Almahairi,Christof Angermueller, Dzmitry Bahdanau, NicolasBallas, et al 2016.
Theano: A Python framework forfast computation of mathematical expressions.
arXivpreprint arXiv:1605.02688.Brent Berlin and Paul Kay.
1991.
Basic color terms:Their universality and evolution.
University of Cali-fornia Press.David DeVault and Matthew Stone.
2007.
Managingambiguities across utterances in dialogue.
In Ron Art-stein and Laure Vieu, editors, Proceedings of DECA-LOG 2007: Workshop on the Semantics and Pragmat-ics of Dialogue.Sander Dieleman, Jan Schl?ter, Colin Raffel, Eben Ol-son, S?ren Kaae S?nderby, Daniel Nouri, et al 2015.Lasagne: First release.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In AISTATS.Dave Golland, Percy Liang, and Dan Klein.
2010.
Agame-theoretic approach to generating spatial descrip-tions.
In EMNLP.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R. Salakhutdinov.2012.
Improving neural networks by preventingco-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Sepp Hochreiter and J?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.2247Kazuya Kawakami, Chris Dyer, Bryan Routledge, andNoah A. Smith.
2016.
Character sequence modelsfor colorful words.
In EMNLP.Emiel Krahmer and Kees Van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.Computational Linguistics, 38(1):173?218.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, et al 2011.
Babytalk: Understanding and generating image descrip-tions.
In CVPR.Brian McMahan and Matthew Stone.
2015.
A Bayesianmodel of grounded color semantics.
Transactions ofthe Association for Computational Linguistics, 3:103?115.Margaret Mitchell, Xufeng Han, Jesse Dodge, AlyssaMensch, Amit Goyal, Alex Berg, et al 2012.
Midge:Generating image descriptions from computer visiondetections.
In EACL.Will Monroe and Christopher Potts.
2015.
Learning inthe Rational Speech Acts model.
In Proceedings of the20th Amsterdam Colloquium.Randall Munroe.
2010.
Color survey results.
Online athttp://blog.xkcd.com/2010/05/03/color-surveyresults.Sebastian Pad?, 2006.
User?s guide to sigf:Significance testing by approximate randomisa-tion.
http://www.nlpado.de/~sebastian/software/sigf.shtml.Dengsheng Zhang and Guojun Lu.
2002.
Shape-basedimage retrieval using generic Fourier descriptor.
Sig-nal Processing: Image Communication, 17(10):825?848.2248
