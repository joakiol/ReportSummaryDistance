Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375?384,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsBetter Evaluation Metrics Lead to Better Machine TranslationChang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,21Department of Computer Science, National University of Singapore2NUS Graduate School for Integrative Sciences and Engineering{liuchan1,danielhe,nght}@comp.nus.edu.sgAbstractMany machine translation evaluation met-rics have been proposed after the seminalBLEU metric, and many among them havebeen found to consistently outperform BLEU,demonstrated by their better correlations withhuman judgment.
It has long been the hopethat by tuning machine translation systemsagainst these new generation metrics, ad-vances in automatic machine translation eval-uation can lead directly to advances in auto-matic machine translation.
However, to datethere has been no unambiguous report thatthese new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline.In this paper, we demonstrate that tuningJoshua, a hierarchical phrase-based statisticalmachine translation system, with the TESLAmetrics results in significantly better human-judged translation quality than the BLEU-tuned baseline.
TESLA-M in particular issimple and performs well in practice on largedatasets.
We release all our implementationunder an open source license.
It is our hopethat this work will encourage the machinetranslation community to finally move awayfrom BLEU as the unquestioned default andto consider the new generation metrics whentuning their systems.1 IntroductionThe dominant framework of machine translation(MT) today is statistical machine translation (SMT)(Hutchins, 2007).
At the core of the system is thedecoder, which performs the actual translation.
Thedecoder is parameterized, and estimating the optimalset of parameter values is of paramount importancein getting good translations.
In SMT, the parame-ter space is explored by a tuning algorithm, typicallyMERT (Minimum Error Rate Training) (Och, 2003),though the exact method is not important for ourpurpose.
The tuning algorithm carries out repeatedexperiments with different decoder parameter val-ues over a development data set, for which referencetranslations are given.
An automatic MT evaluationmetric compares the output of the decoder againstthe reference(s), and guides the tuning algorithm to-wards iteratively better decoder parameters and out-put translations.
The quality of the automatic MTevaluation metric therefore has an immediate effecton the whole system.The first automatic MT evaluation metric to showa high correlation with human judgment is BLEU(Papineni et al, 2002).
Together with its close vari-ant the NIST metric, they have quickly become thestandard way of tuning statistical machine transla-tion systems.
While BLEU is an impressively sim-ple and effective metric, recent evaluations haveshown that many new generation metrics can out-perform BLEU in terms of correlation with humanjudgment (Callison-Burch et al, 2009; Callison-Burch et al, 2010).
Some of these new metrics in-clude METEOR (Banerjee and Lavie, 2005; Lavieand Agarwal, 2007), TER (Snover et al, 2006),MAXSIM (Chan and Ng, 2008; Chan and Ng,2009), and TESLA (Liu et al, 2010).Given the close relationship between automaticMT and automatic MT evaluation, the logical expec-tation is that a better MT evaluation metric would375lead to better MT systems.
However, this linkagehas not yet been realized.
In the SMT community,MT tuning still uses BLEU almost exclusively.Some researchers have investigated the use of bet-ter metrics for MT tuning, with mixed results.
Mostnotably, Pado?
et al (2009) reported improved humanjudgment using their entailment-based metric.
How-ever, the metric is heavy weight and slow in practice,with an estimated runtime of 40 days on the NISTMT 2002/2006/2008 dataset, and the authors had toresort to a two-phase MERT process with a reducedn-best list.
As we shall see, our experiments use thesimilarly sized WMT 2010 dataset, and most of ourruns take less than one day.Cer et al (2010) compared tuning a phrase-basedSMT system with BLEU, NIST, METEOR, andTER, and concluded that BLEU and NIST are stillthe best choices for MT tuning, despite the provenhigher correlation of METEOR and TER with hu-man judgment.In this work, we investigate the effect of MERTusing BLEU, TER, and two variants of TESLA,TESLA-M and TESLA-F, on Joshua (Li et al,2009), a state-of-the-art hierarchical phrase-basedSMT system (Chiang, 2005; Chiang, 2007).
Ourempirical study is carried out in the context of WMT2010, for the French-English, Spanish-English, andGerman-English machine translation tasks.
Weshow that Joshua responds well to the change ofevaluation metric, in that a system trained on met-ric M typically does well when judged by the samemetric M. We further evaluate the different systemswith manual judgments and show that the TESLAfamily of metrics (both TESLA-M and TESLA-F)significantly outperforms BLEU when used to guidethe MERT search.The rest of this paper is organized as follows.
InSection 2, we describe the four evaluation metricsused.
Section 3 outlines our experimental set up us-ing the WMT 2010 machine translation tasks.
Sec-tion 4 presents the evaluation results, both automaticand manual.
Finally, we discuss our findings in Sec-tion 5, future work in Section 6, and conclude inSection 7.2 Evaluation metricsThis section describes the metrics used in our exper-iments.
We do not seek to explain all their variantsand intricate details, but rather to outline their corecharacteristics and to highlight their similarities anddifferences.
In particular, since all our experimentsare based on single references, we omit the com-plications due to multiple references and refer ourreaders instead to the respective original papers forthe details.2.1 BLEUBLEU is fundamentally based on n-gram match pre-cisions.
Given a reference text R and a translationcandidate T , we generate the bag of all n-grams con-tained inR and T for n = 1, 2, 3, 4, and denote themas BNGnR and BNGnT respectively.
The n-gram pre-cision is thus defined asPn =|BNGnR ?
BNGnT||BNGnT|To compensate for the lack of the recall measure,and hence the tendency to produce short translations,BLEU introduces a brevity penalty, defined asBP ={1 if|T| > |R|e1?|R|/|T | if|T| ?
|R|where the | ?
| operator denotes the size of a bag orthe number of words in a text.
The metric is finallydefined asBLEU(R,T) = BP?
4?P1P2P3P4BLEU is a very simple metric requiring neithertraining nor language-specific resources.
Its use ofthe brevity penalty is however questionable, as sub-sequent research on n-gram-based metrics has con-sistently found that recall is in fact a more potentindicator than precision (Banerjee and Lavie, 2005;Zhou et al, 2006; Chan and Ng, 2009).
As weshall see, despite the BP term, BLEU still exhibitsa strong tendency to produce short translations.2.2 TERTER is based on counting transformations ratherthan n-gram matches.
The metric is defined as the376minimum number of edits needed to change a can-didate translation T to the reference R, normalizedby the length of the reference, i.e.,TER(R,T) = number of edits|R|One edit is defined as one insertion, deletion, orsubstitution of a single word, or the shift of a con-tiguous sequence of words, regardless of size anddistance.
Minimizing the edit distance so definedhas been shown to be NP-complete, so the evalua-tion is carried out in practice by a heuristic greedysearch algorithm.TER is a strong contender as the leading new gen-eration automatic metric and has been used in majorevaluation campaigns such as GALE.
Like BLEU,it is simple and requires no language specific re-sources.
TER also corresponds well to the humanintuition of an evaluation metric.2.3 TESLA-MTESLA1 is a family of linear programming-basedmetrics proposed by Liu et al (2010) that incor-porates many newer ideas.
The simplest varia-tion is TESLA-M2, based on matching bags of n-grams (BNG) like BLEU.
However, unlike BLEU,TESLA-M formulates the matching process as areal-valued linear programming problem, therebyallowing the use of weights.
An example weightedBNG matching problem is shown in Figure 1.Two kinds of weights are used in TESLA-M.First, the metric emphasizes the content words bydiscounting the weight of an n-gram by 0.1 for ev-ery function word it contains.
Second, the similaritybetween two n-grams is a function dependent on thelemmas, the WordNet synsets (Fellbaum, 1998), andthe POS tag of every word in the n-grams.Each node in Figure 1 represents one weighted n-gram.
The four in the top row represent one BNG,and the three at the bottom represent the other BNG.The goal of the linear programming problem is toassign weights to the links between the two BNGs,so as to maximize the sum of the products of the linkweights and their corresponding similarity scores.1The source code of TESLA is available atnlp.comp.nus.edu.sg/software/2M stands for minimal.w=1.0 w=0.1 w=0.1 w=0.1w=0.01 w=0.1 w=0.1s=0.1 s=0.8s=0.5 s=0.8Good morning morning , , sir sir .Hello , , Querrien Querrien .s=0.4(a) The matching problemw=1.0 w=0.1 w=0.1 w=0.1w=0.01 w=0.1 w=0.1w=0.1w=0.01 w=0.1s885Go8d m r o8d m r Gn nGimd imdG.g,HH8Gn nGel,ddm, el,ddm, G.(b) The solutionFigure 1: Matching two weighted bags of n-grams.w denotes the weight and s denotes the similarity.The constraints of the linear programming prob-lem are: (1) all assigned weights must be non-negative, and (2) the sum of weights assigned to alllinks connecting a node cannot exceed the node?sweight.
Intuitively, we allow splitting n-grams intofractional counts, and match them giving priority tothe pairs with the highest similarities.The linear programming formulation ensures thatthe matching can be solved uniquely and efficiently.Once the solution is found and let the maximizedobjective function value be S, the precision is com-puted as S over the sum of weights of the translationcandidate n-grams.
Similarly, the recall is S over thesum of weights of the reference n-grams.
The pre-cision and the recall are then combined to form theF-0.8 measure:Fn =Precision?
Recall0.8?
Precision + 0.2?
RecallThis F-measure gives more importance to the re-call, reflecting its closer correlation with humanjudgment.
Fn for n = 1, 2, 3 are calculated and av-eraged to produce the final score.TESLA-M gains an edge over the previous twometrics by the use of lightweight linguistic featuressuch as lemmas, synonym dictionaries, and POS377Metric Spearman?s rhoTESLA-F .94TESLA-M .93meteor-next-* .921-TERp .90BLEU-4-v13a-c .89Table 1: Selected system-level Spearman?s rho cor-relation with the human judgment for the into-English task, as reported in WMT 2010.Metric Spearman?s rhoTESLA-M .93meteor-next-rank .821-TERp .81BLEU-4-v13a-c .80TESLA-F .76Table 2: Selected system-level Spearman?s rho cor-relation with the human judgment for the out-of-English task, as reported in WMT 2010.tags.
While such tools are usually available even forlanguages other than English, it does make TESLA-M more troublesome to port to non-English lan-guages.TESLA-M did well in the WMT 2010 evaluationcampaign.
According to the system-level correla-tion with human judgments (Tables 1 and 2), it rankstop for the out-of-English task and very close to thetop for the into-English task (Callison-Burch et al,2010).2.4 TESLA-F3TESLA-F builds on top of TESLA-M.
While word-level synonyms are handled in TESLA-M by exam-ining WordNet synsets, no modeling of phrase-levelsynonyms is possible.
TESLA-F attempts to rem-edy this shortcoming by exploiting a phrase tablebetween the target language and another language,known as the pivot language.Assume the target language is English and thepivot language is French, i.e., we are provided withan English-French phrase table.
Let R and T be the3TESLA-F refers to the metric called TESLA in (Liu et al,2010).
To minimize confusion, in this work we call the metricTESLA-F and refer to the whole family of metrics as TESLA.F stands for full.w=1.=0s858G=1od 0s8m8r8nmiFigure 2: A degenerate confusion network inFrench.
The phrase table maps Good morning , sir .to Bonjour , monsieur .w=1.=0s858G8odmrn0i858G8odg,0HsseH18G8gdo d8G8gdoFigure 3: A confusion network in French.
Thephrase table maps Hello , to Bonjour , with P = 0.9and to Salut , with P = 0.1.reference and the translation candidate respectively,both in English.
As an example,R: Good morning , sir .T: Hello , Querrien .TESLA-F first segments both R and T intophrases to maximize the probability of the sen-tences.
For example, suppose both Good morning ,sir .
and Hello , can be found in the English-Frenchphrase table, and proper name Querrien is out-of-vocabulary, then a likely segmentation is:R: ||| Good morning , sir .
|||T: ||| Hello , ||| Querrien ||| .
|||Each English phrase is then mapped to a bagof weighted French phrases using the phrase table,transforming the English sentences into confusionnetworks resembling Figures 2 and 3.
French n-grams are extracted from these confusion networkrepresentations, known as pivot language n-grams.The bag of pivot language n-grams generated by Ris then matched against that generated by T withthe same linear programming formulation used inTESLA-M.TESLA-F incorporates all the F-measures used inTESLA-M, with the addition of (1) the F-measuresgenerated over the pivot language n-grams describedabove, and (2) the normalized language model score,defined as 1n logP , where n is the length of thetranslation, and P the language model probability.Unlike BLEU and TESLA-M which rely on simpleaverages (geometric and arithmetic average respec-tively) to combine the component scores, TESLA-378F trains the weights over a set of human judg-ments using a linear ranking support vector machine(RSVM).
This allows TESLA-F to exploit its com-ponents more effectively, but also makes it more te-dious to work with and introduces potential domainmismatch problems.TESLA-F makes use of even more linguistic in-formation than TESLA-M, and has the capabilityof recognizing some forms of phrase synonyms.TESLA-F ranked top for the into-English evalua-tion task in WMT 2010 (Table 1).
However, theadded complexity, in particular the use of the lan-guage model score and the tuning of the componentweights appear to make it less stable than TESLA-Min practice.
For example, it did not perform as wellin the out-of-English task.3 Experimental setupWe run our experiments in the setting of the WMT2010 news commentary machine translation cam-paign, for three language pairs:1.
French-English (fr-en): the training text con-sists of 84624 sentences of French-English bi-text.
The average French sentence length is 25words.2.
Spanish-English (es-en): the training text con-sists of 98598 sentences of Spanish-English bi-text.
The average Spanish sentence length is 25words.3.
German-English (de-en): the training text con-sists of 100269 sentences of German-Englishbitext.
The average German sentence length is22 words.The average English sentence length is 21 wordsfor all three language pairs.
The text domain isnewswire report, and the English sides of the train-ing texts for the three language pairs overlap sub-stantially.
The development data are 2525 four-waytranslated sentences, in English, French, Spanish,and German respectively.
Similarly, the test dataare 2489 four-way translated sentences.
As a conse-quence, all MT evaluations involve only single ref-erences.We follow the standard approach for training hi-erarchical phrase-based SMT systems.
First, we to-kenize and lowercase the training texts and createfr-en es-en de-enBLEU 3:49 (4) 5:09 (6) 2:41 (4)TER 4:03 (4) 3:59 (4) 3:59 (5)TESLA-M 13:00 (3) 17:34 (5) 13:40 (4)TESLA-F 35:07 (4) 40:54 (4) 40:28 (5)Table 3: Z-MERT training times in hours:minutesand number of iterations in parenthesisword alignments using the Berkeley aligner (Lianget al, 2006; Haghighi et al, 2009) with five iter-ations of training.
Then, we create suffix arraysand extract translation grammars for the develop-ment and test set with Joshua in its default setting.The maximum phrase length is 10.
For the languagemodel, we use SRILM (Stolcke, 2002) to build atrigram model with modified Kneser-Ney smooth-ing from the monolingual training data supplied inWMT 2010.Parameter tuning is carried out using Z-MERT (Zaidan, 2009).
TER and BLEU are al-ready implemented in the publicly released versionof Z-MERT, and Z-MERT?s modular design makesit easy to integrate TESLA-M and TESLA-F into thepackage.
The maximum number of MERT iterationsis set to 100, although we observe that in practice,the algorithm converges after 3 to 6 iterations.
Thenumber of intermediate initial points per iteration isset to 20 and the n-best list is capped to 300 trans-lations.
Table 3 shows the training times and thenumber of MERT iterations for each of the languagepairs and evaluation metrics.We use the publicly available version of TESLA-F, which comes with phrase tables and a rankingSVM model trained on the WMT 2010 developmentdata.4 Automatic and manual evaluationsThe results of the automatic evaluations are pre-sented in Table 4.
The best score according to eachmetric is shown in bold.
Note that smaller TERscores are better, as are larger BLEU, TESLA-M,and TESLA-F scores.4We note that Joshua generally responds well tothe change of tuning metric.
A system tuned on met-4The TESLA-F scores shown here have been monotonicallyscaled.379tune\test BLEU TER TESLA-M TESLA-FBLEU 0.5237 0.6029 0.3922 0.4114TER 0.5239 0.6028 0.3880 0.4095TESLA-M 0.5005 0.6359 0.4170 0.4223TESLA-F 0.4992 0.6377 0.4164 0.4224(a) The French-English tasktune\test BLEU TER TESLA-M TESLA-FBLEU 0.5641 0.5764 0.4315 0.4328TER 0.5667 0.5725 0.4204 0.4282TESLA-M 0.5253 0.6246 0.4511 0.4398TESLA-F 0.5331 0.6111 0.4498 0.4409(b) The Spanish-English tasktune\test BLEU TER TESLA-M TESLA-FBLEU 0.4963 0.6329 0.3369 0.3927TER 0.4963 0.6355 0.3191 0.3851TESLA-M 0.4557 0.7055 0.3784 0.4070TESLA-F 0.4642 0.6888 0.3753 0.4068(c) The German-English taskTable 4: Automatic evaluation scoresP(A) KappaFrench-English 0.6846 0.5269Spanish-English 0.6124 0.4185German-English 0.3973 0.0960Table 5: Inter-annotator agreementric M usually does the best or very close to the bestwhen evaluated by M. On the other hand, the dif-ferences between different systems can be substan-tial, especially between BLEU/TER and TESLA-M/TESLA-F.In addition to the automatic evaluation, we en-listed twelve judges to manually evaluate the first200 test sentences.
Four judges are assigned toeach of the three language pairs.
For each test sen-tence, the judges are presented with the source sen-tence, the reference English translation, and the out-put from the four competing Joshua systems.
Theorder of the translation candidates is randomized sothat the judges will not see any patterns.
The judgesare instructed to rank the four candidates, and tiesare allowed.The inter-annotator agreement is reported in Ta-ble 5.
We consider the judgment for a pair of systemoutputs as one data point.
Let P (A) be the propor-tion of times that the annotators agree, and P (E)fr-en es-en de-enBLEU 44.1% 33.8% 49.6%TER 41.4% 34.4% 47.8%TESLA-M 65.8% 49.5% 57.8%TESLA-F 66.4% 53.8% 55.1%Table 6: Percentage of times each system producesthe best translationbe the proportion of times that they would agree bychance.
The Kappa coefficient is defined asKappa = P(A)?
P(E)1?
P(E)In our experiments, each data point has three pos-sible values: A is preferred, B is preferred, and nopreference, hence P (E) = 1/3.
Our Kappa is cal-culated in the same way as the WMT workshops(Callison-Burch et al, 2009; Callison-Burch et al,2010).Kappa coefficients between 0.4 and 0.6 are con-sidered moderate, and our values are in line withthose reported in the WMT 2010 translation cam-paign.
The exception is the German-English pair,where the annotators only reach slight agreement.This might be caused by the lower quality of Ger-man to English translations compared to the othertwo language pairs.Table 6 shows the proportion of times each sys-tem produces the best translation among the four.We observe that the rankings are largely consis-tent across different language pairs: Both TESLA-F and TESLA-M strongly outperform BLEU andTER.
Note that the values in each column do notadd up to 100%, since the candidate translations areoften identical, and even a different translation canreceive the same human judgment.Table 7 shows our main result, the pairwise com-parison between the four systems for each of the lan-guage pairs.
Again the rankings consistently showthat both TESLA-F and TESLA-M strongly out-perform BLEU and TER.
All differences are sta-tistically significant under the Sign Test at p =0.01, with the exception of TESLA-M vs TESLA-F in the French-English task, BLEU vs TER in theSpanish-English task, and TESLA-M vs TESLA-Fand BLEU vs TER in the German-English task.
Theresults provide strong evidence that tuning machine380A\B BLEU TER TESLA-M TESLA-FBLEU - 11.4% / 6.5% 29.1% / 52.1% 28.0% / 52.3%TER 6.5% / 11.4% - 28.6% / 54.5% 27.5% / 55.0%TESLA-M 52.1% / 29.1% 54.5% / 28.6% - 7.6% / 8.8%TESLA-F 52.3% / 28.0% 55.0% / 27.5% 8.8% / 7.6% -(a) The French-English task.
All differences are significant under the Sign Test at p = 0.01, except thestrikeout TESLA-M vs TESLA-F.A\B BLEU TER TESLA-M TESLA-FBLEU - 25.8% / 22.3% 31.0% / 50.6% 24.4% / 50.8%TER 22.3% / 25.8% - 31.9% / 51.0% 26.4% / 52.4%TESLA-M 50.6% / 31.0% 51.0% / 31.9% - 25.9% / 33.4%TESLA-F 50.8% / 24.4% 52.4% / 26.4% 33.4% / 25.9% -(b) The Spanish-English task.
All differences are significant under the Sign Test at p = 0.01, exceptthe strikeout BLEU vs TER.A\B BLEU TER TESLA-M TESLA-FBLEU - 21.8% / 18.4% 28.1% / 36.9% 27.3% / 35.3%TER 18.4% / 21.8% - 26.9% / 39.5% 27.3% / 37.5%TESLA-M 36.9% / 28.1% 39.5% / 26.9% - 24.3% / 21.3%TESLA-F 35.3% / 27.3% 37.5% / 27.3% 21.3% / 24.3% -(c) The German-English task.
All differences are significant under the Sign Test at p = 0.01, exceptthe strikeout BLEU vs TER, and TESLA-M vs TESLA-F.Table 7: Pairwise system comparisons.
Each cell shows the proportion of time the system tuned on A ispreferred over the system tuned on B, and the proportion of time the opposite happens.
Notice that the upperright half of each table is the mirror image of the lower left half.381translation systems using the TESLA metrics leadsto significantly better translation output.5 DiscussionWe examined the results manually, and found thatthe relationship between the types of mistakes eachsystem makes and the characteristics of the corre-sponding metric to be intricate.
We discuss our find-ings in this section.First we observe that BLEU and TER tend to pro-duce very similar translations, and so do TESLA-F and TESLA-M. Of the 2489 test sentences in theFrench-English task, BLEU and TER produced dif-ferent translations for only 760 sentences, or 31%.Similarly, TESLA-F and TESLA-M gave differentoutputs for only 857 sentences, or 34%.
In contrast,BLEU and TESLA-M gave different translations for2248 sentences, or 90%.
It is interesting to find thatBLEU and TER should be so similar, consideringthat they are based on very different principles.
As ametric, TESLA-M is certainly much more similar toBLEU than TER is, yet they behave very differentlywhen used as a tuning metric.We also observe that TESLA-F and TESLA-Mtend to produce much longer sentences than doBLEU and TER.
The average sentence lengths of theTESLA-F- and TESLA-M-tuned systems across allthree language pairs are 26.5 and 26.6 words respec-tively, whereas those for BLEU and TER are only22.4 and 21.7 words.
Comparing the translationsfrom the two groups, the tendency of BLEU andTER to pick shorter paraphrases and to drop func-tion words is unmistakable, often to the detriment ofthe translation quality.
Some typical examples fromthe French-English task are shown in Figure 4.Interestingly, the human translations average only22 words, so BLEU and TER translations are in factmuch closer on average to the reference lengths, yettheir translations often feel too short.
In contrast,manual inspections reveal no tendency for TESLA-Fand TESLA-M to produce overly long translations.These observations suggest that the brevitypenalty of BLEU is not aggressive enough.
Nei-ther is TER, which penalizes insertions and dele-tions equally.
Interestingly, by placing much moreemphasis on the recall, TESLA-M and TESLA-Fproduce translations that are statistically too long,but feel much more ?correct?
lengthwise.Another major difference between TESLA-M/TESLA-F and BLEU/TER is that the TESLAsheavily discount n-grams with function words.
Onemight thus expect the TESLA-tuned systems to beless adept at function words; yet they translate themsurprisingly well, as shown in Figure 4.
One ex-planation is of course the sentence length effect wehave discussed.
Another reason may be that sincethe metric does not care much about function words,the language model is given more freedom to pickfunction words as it sees fit, without the fear of largepenalties.
Paradoxically, by reducing the weightsof function words, we end up making better trans-lations for them.TER is the only metric that allows cheap blockmovements, regardless of size or distance.
Onemight reasonably speculate that a TER-tuned systemshould be more prone to reordering phrases.
How-ever, we find no evidence that this is so.The relative performance of TESLA-M vsTESLA-F is unsurprising.
TESLA-F, being heav-ier and slower, produces somewhat better resultsthan its minimalist counterpart, though the marginis far less pronounced than that between TESLA-M and the conventional BLEU and TER.
Since ex-tra resources including bitexts are needed in usingTESLA-F, TESLA-M emerges as the MT evaluationmetric of choice for tuning SMT systems.6 Future workWe have presented empirical evidence that theTESLA metrics outperform BLEU for MT tuningin a hierarchical phrase-based SMT system.
Atthe same time, some open questions remain unan-swered.
We intend to investigate them in our futurework.The work of (Cer et al, 2010) investigated the ef-fect of tuning a phrase-based SMT system and foundthat of the MT evaluation metrics that they tried,none of them could outperform BLEU.
We wouldlike to verify whether TESLA tuning is still pre-ferred over BLEU tuning in a phrase-based SMTsystem.Based on our observations, it may be possible toimprove the performance of BLEU-based tuning by(1) increasing the brevity penalty; (2) introducing382BLEU in the future , americans want a phone that allow the user to .
.
.TER in the future , americans want a phone that allow the user to .
.
.TESLA-M in the future , the americans want a cell phone , which allow the user to .
.
.TESLA-F in the future , the americans want a phone that allow the user to .
.
.BLEU .
.
.
also for interest on debt of the state .
.
.TER .
.
.
also for interest on debt of the state .
.
.TESLA-M .
.
.
also for the interest on debt of the state .
.
.TESLA-F .
.
.
also for the interest on debt of the state .
.
.BLEU and it is hardly the end of carnival-like transfers .TER and it is hardly the end of carnival-like transfers .TESLA-M and it is far from being the end of the carnival-like transfers .TESLA-F and it is far from being the end of the carnival-like transfers .BLEU it is not certain that the state can act without money .TER it is not certain that the state can act without money .TESLA-M it is not certain that the state can act without this money .TESLA-F it is not certain that the state can act without this money .BLEU but the expense of a debt of the state .
.
.TER but the expense of a debt of the state .
.
.TESLA-M but at the expense of a greater debt of the state .
.
.TESLA-F but at the expense of a great debt of the state .
.
.Figure 4: Comparison of selected translations from the French-English taska recall measure and emphasizing it over precision;and/or (3) introducing function word discounting.
Inthe ideal case, such a modified BLEU metric woulddeliver results similar to that of TESLA-M, yet witha runtime cost closer to BLEU.
It would also makeporting existing tuning code easier.7 ConclusionWe demonstrate for the first time that a practicalnew generation MT evaluation metric can signifi-cantly improve the quality of automatic MT com-pared to BLEU, as measured by human judgment.We hope this work will encourage the MT researchcommunity to finally move away from BLEU and toconsider tuning their systems with a new generationmetric.All the data, source code, and results reported inthis work can be downloaded from our website athttp://nlp.comp.nus.edu.sg/software.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
Proceedings of theACL 2005 Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on statisti-cal machine translation and metrics for machine trans-lation.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR.Daniel Cer, Christopher D. Manning, and Daniel Juraf-sky.
2010.
The best lexical metric for phrase-basedstatistical MT system optimization.
In Human Lan-guage Technologies: The 2010 Annual Conference of383the North American Chapter of the Association forComputational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2008.
MaxSim:A maximum similarity metric for machine translationevaluation.
In Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2009.
MaxSim: per-formance and effects of translation fluency.
MachineTranslation, 23(2):157?168, September.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT press.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of 47th Annual Meetingof the Association for Computational Linguistics andthe 4th IJCNLP of the AFNLP.John W. Hutchins.
2007.
Machine translation: A con-cise history.
Computer Aided Translation: Theory andPractice.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: Anautomatic metric for MT evaluation with high levels ofcorrelation with human judgments.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Joshua: An open source toolkit forparsing-based machine translation.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010.Tesla: Translation evaluation of sentences with linear-programming-based analysis.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,and Christopher D. Manning.
2009.
Measuring ma-chine translation quality as semantic equivalence: Ametric based on entailment features.
Machine Trans-lation, 23(2):181?193, August.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the Seventh Conference of the Asso-ciation for Machine Translation in the Americas.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing.Omar Zaidan.
2009.
Z-MERT: A fully configurable opensource tool for minimum error rate training of machinetranslation systems.
The Prague Bulletin of Mathe-matical Linguistics, 91:79?88.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.
Re-evaluating machine translation results with paraphrasesupport.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing.384
