Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39?41,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsShared Task on Prediction of Dropout Over Time in Massively OpenOnline CoursesCarolyn P. Ros?Language Technologies Instituteand Human-Computer Interaction InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213cprose@cs.cmu.eduGeorge SiemensCenter for Distributed EducationUniversity of Texas at Arlington701 South Nedderman Drive, Arlington, TX76019gsiemens@uta.eduAbstractThe shared task on Prediction of DropoutOver Time in MOOCs involves analysisof data from 6 MOOCs offered throughCoursera.
Data from one MOOC with ap-proximately 30K students was distributed astraining data and consisted of discussion fo-rum data (in SQL) and clickstream data (inJSON format).
The prediction task was Pre-dicting Attrition Over Time.
Based on behav-ioral data from a week?s worth of activity in aMOOC for a student, predict whether the stu-dent will cease to actively participate afterthat week.
This paper describes the task.A full write up of the results is publishedseparately (Ros?
& Siemens, 2014).1 OverviewResearch on Massively Open Online Courses(MOOCs)1 is an emerging area for real world impactof technology for analysis of social media at a largescale (Breslow et al., 2013).
Modeling user experi-ence in MOOCs supports research towards under-standing user needs better so that experiences that aremore conducive to learning can be offered.
Beyondthat, automated analyses enable adaptive technologyto tailor the experience of users in real time (Ros?
etal., 2014a).
This paper describes a shared task de-signed to enlist the involvement of the language tech-nologies community in this endeavor and to identifywhat value expertise within the field might bring.1 http://www.moocresearch.com/reportsOne area for impact of natural language processing inthe MOOC space is in modeling behavior within thethreaded discussion forums.
In a typical MOOC,between 5% and 10% of students actively participatein the threaded discussion forums.
Previously pub-lished research demonstrates that characteristics ofposting behavior are predictive of dropout along theway (Ros?
et al., 2014b; Wen et al., 2014a; Wen etal., 2014b; Yang et al., 2013; Yang et al., 2014).However, ideally, we would like to make predictionsfor the other 90% to 95% of students who do not post.Thus, in this shared task, we challenge participants touse models of social interaction as displayed throughthe text-based interaction between students in thethreaded discussions (from the minority of studentswho participate in them) to make meaning from theclickstream data we have from all students.
If thediscussion data can be thus leveraged to make moreeffective models of the clickstream data, then a mean-ingful prediction about drop out along the way canalso be made about the students who do not post tothe discussion forums.One of the biggest challenges in the shared task is thatthe participants were only given data from oneCoursera MOOC as training and development data.Their task was to produce a predictive model thatcould be applied to data from other MOOCs they didnot have access to.
A separate report describes a de-tailed analysis of the results applying submitted mod-els to each of 5 test MOOCs (Ros?
& Siemens, 2014).12 research teams signed up for the shared task, in-cluding an international assortment of academic andindustrial teams.
Out of these 12 teams, only 4 sub-mitted final models (Sinha et al., 2014; Sharkey &Sanders, 2014; Amnueypornsakul et al., 2014; Kloftet al., 2014 ).In the remainder of this paper we describe the sharedtask in greater detail and discuss plans for future re-lated research.392 Shared TaskParticipants in the shared task were given a completeSQL dump and clickstream dump from one CourseraMOOC as training data.
The student-week was theunit of analysis.
In other words, a prediction wasmade for each student for each week of their activeparticipation to predict whether that week was the lastweek of their active participation.
Scripts were pro-vided to parse the data into a form that could be usedfor the task, e.g., aggregating entries per user perweek.
Scripts were also provided for running a test ofthe trained model on test data.
The purpose of thescripts was to standardize the way in which eachteam?s work would later be evaluated on the testMOOCs that participants did not have access to.A major part of the work in doing the task is in de-termining what an effective representation would beof the behavior trace associated with each student-week that would enable making an accurate predic-tion.
In other words, the question is what are the dan-ger signs that a student is especially vulnerable todrop out?
The rules of the task were such that the in-formation the model was allowed to use for makingthe prediction could be extracted from the whole par-ticipation history of all training students (includingboth the SQL data and the clickstream data) up to andincluding the week a prediction was being made for.Each of the four finalist teams submitted a final modeltrained on the training MOOC and a write up include-ing result trained on a designated subset of studentsfrom the training MOOC and tested on the remainingstudents.
Results were presented in terms of preci-sion, recall, and fmeasure for the held out users.We recommend that participants make use of the textdata to bootstrap effective models that use only click-stream data.
However, participants were welcome toleverage either type of data in the models they submit-ted.
In our evaluation presented separately (Ros?
&Siemens, 2014), we evaluated the models on the testMOOCs in three different ways: First, an evaluationwas conducted on data from students who activelyparticipated in the discussion forums.
Second, anevaluation was conducted on data from students whonever participated in the discussion forums.
And fi-nally, and evaluation was conducted on the set of stu-dents that includes both types of students.Each submission consisted of a write up describingthe technical approach and a link to a downloadablezip file containing the trained model and code and/ora script for using the trained model to make predic-tions about the test sets.
The code was required to berunnable by launching a single script in Ubuntu 12.04.A code stub for streamlining the preparation of thesubmission was distributed with the data.
The follow-ing programming languages were acceptable: R 3.1,C++ 4.7, Java 1.6, or Python 2.7.
The script was re-quired to be able to run within 24 hours on a 2400MHz machine with 6 cores.3 Looking ForwardComputational modeling of massive scale social in-teraction (as in MOOCs and other environments forlearning at scale) has the potential to yield newknowledge about the inner-workings of interaction insuch environments so that support for healthy com-munity formation can be designed and built.
Howev-er, the state-of-the-art in graphical models applied tolarge scale social data provides representations of thedata that are challenging to interpret in light of specif-ic questions that may be asked from a learning scienc-es or social psychological perspective.
What is need-ed are new methodologies for development and inter-pretation of models that bridge expertise from ma-chine learning and language technologies on one sideand  learning sciences, sociolinguistics, and socialpsychology on the other side.
The field of languagetechnologies has the human capital to take leadershipin making these breakthroughs.The shared task described in this paper is the first onelike it where a data set from a Coursera MOOC hasbeen made publically available so that a wide range ofcomputational modeling techniques can be evaluatedside by side (Ros?
& Siemens, 2014).
However, thereis recognition that such shared tasks may play an im-portant role in shaping the future of the field of Learn-ing Analytics going forward (Pea, 2014).One of the major challenges in running a shared tasklike this is ensuring the protection of privacy of theMOOC participants.
Such concerns have been thefocus of much discussion in the area of learning atscale (Asilomar Convention, 2014).Data sharing ethics were carefully considered in thedesign of this shared task.
In particular, all of thestudents who participated in the MOOC that producedthe training data were told that their data would beused for research purposes.
The data was carefullypreprocessed to remove personal identifiers about thestudents and the university that hosted the course.
Allof the workshop participants who got access to thedata were required to participate in human subjectstraining and to agree to use the data only for thisworkshop, and not to share it beyond their team.
Datawas shared through a secure web connection.
Ap-proval for use of the data in this fashion was approvedby the Institutional Review Board of the hosting uni-versity as well as the university that ran the MOOC.It was a goal in development of this shared task toserve as a forerunner in what we hope will become amore general practice of community wide collabora-tion on large scale learning analytics (Suthers et al.,2013).40AcknowledgementsThe authors would like to thank Norman Bier for as-sistance in working through the data sharing logistics.This work was funded in part by NSF Grant OMA-0836012.ReferencesAmnueypornsakul, B., Bhat, S., & Chinprutthiwong,P.
(2014).
Predicting Attrition Along the Way:The UIUC Model, in Proceedings of the 2014 Em-pirical Methods in Natural Language ProcessingWorkshop on Modeling Large Scale Social Interac-tion in Massively Open Online Courses, Qatar, Oc-tober 2014.Asilomar Convention (2014).
The Asilomar Conven-tion for Learning Research in Higher Education,June 13, 2014.Breslow, L., Pritchard, D., De Boer, J., Stump, G.,Ho, A., & Seaton, D. (2013).
Studying Learning inthe Worldwide Classroom : Research into edX?sFirst MOOC, Research & Practice in Assessment(8).Kloft, M., Stiehler, F., Zheng, Z., & Pinkward, N.(2014).
Predicting MOOC Dropout over WeeksUsing Machine Learning Methods, in Proceedingsof the 2014 Empirical Methods in Natural Lan-guage Processing Workshop on Modeling LargeScale Social Interaction in Massively Open OnlineCourses, Qatar, October 2014.Pea, R. (2014).
The Learning Analytics Workgroup:A Report on Building the Field of Learning Analyt-ics for Personalized Learning at Scale, StanfordUniversity.Ros?, C. P. & Siemens, G. (2014).
Shared Task Re-port : Results of the EMNLP 2014 Shared Task onPredictions of Dropout Over Time in MOOCs,Langauge Technologies Institute Technical Report.Ros?, C. P., Goldman, P., Sherer, J.
Z., Resnick, L.(2014a).
Supportive Technologies for Group Dis-cussion in MOOCs, Current Issues in EmergingeLearning, Special issue on MOOCs, December2014.Ros?, C. P., Carlson, R., Yang, D., Wen, M., Resnick,L., Goldman, P. & Sherer, J.
(2014b).Social Fac-tors that Contribute to Attrition in MOOCs, in Pro-ceedings of the First ACM Conference on Learning@ Scale.Sinha, T., Li, N., Jermann, P., & Dillenbourg, P.(2014).
Capturing ?attrition intensifying?
structuraltraits from didactic interaction sequences ofMOOC learners, in Proceedings of the 2014 Em-pirical Methods in Natural Language ProcessingWorkshop on Modeling Large Scale Social Interac-tion in Massively Open Online Courses, Qatar, Oc-tober 2014.Sharkey, M. & Sanders, R. (2014).
A Process forPredicting MOOC Attrition, in Proceedings of the2014 Empirical Methods in Natural LanguageProcessing Workshop on Modeling Large ScaleSocial Interaction in Massively Open OnlineCourses, Qatar, October 2014.Suthers, D., Lund, K., Ros?, C. P., Teplovs, C., Law,N.
(2013).
Productive Multivocality in the Analy-sis of Group Interactions, edited volume, Springer.Wen, M., Yang, D., & Ros?, C. P. (2014b).
LinguisticReflections of Student Engagement in MassiveOpen Online Courses, in Proceedings of the Inter-national Conference on Weblogs and Social MediaWen, M., Yang, D., & Ros?, C. P. (2014a).
SentimentAnalysis in MOOC Discussion Forums: What doesit tell us?
in Proceedings of Educational Data Min-ing.Yang, D., Sinha, T., Adamson, D., & Ros?, C. P.(2013).
Turn on, Tune in, Drop out: Anticipatingstudent dropouts in Massive Open Online Courses,in NIPS Data-Driven Education Workshop.Yang, D., Wen, M., & Ros?, C. P. (2014).
Peer Influ-ence on Attrition in Massively Open Online Cours-es, in Proceedings of Educational Data Mining.41
