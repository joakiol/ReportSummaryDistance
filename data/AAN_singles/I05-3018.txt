Combination of Machine Learning Methodsfor Optimum Chinese Word SegmentationMasayuki AsaharaChooi-Ling GohKenta FukuokaYotaro WatanabeNara Institute of Science and Technology, JapanE-mail: cje@is.naist.jpAi AzumaYuji MatsumotoTakashi TsuzukiMatsushita ElectricIndustrial Co., Ltd.AbstractThis article presents our recent work for par-ticipation in the Second International Chi-nese Word Segmentation Bakeoff.
Oursystem performs two procedures: Out-of-vocabulary extraction and word segmenta-tion.
We compose three out-of-vocabularyextraction modules: Character-based tag-ging with different classifiers ?
maximumentropy, support vector machines, and con-ditional random fields.
We also com-pose three word segmentation modules ?character-based tagging by maximum en-tropy classifier, maximum entropy markovmodel, and conditional random fields.
Allmodules are based on previously proposedmethods.
We submitted three systems whichare different combination of the modules.1 OverviewWe compose three systems: Models a, b and c for theclosed test tracks on all four data sets.For Models a and c, three out-of-vocabulary (OOV)word extraction modules are composed: 1.
MaximumEntropy (MaxEnt) classifier-based tagging; 2.
Max-imum Entropy Markov Model (MEMM)-based wordsegmenter with Conditional Random Fields (CRF)-based chunking; 3.
MEMM-based word segmenterwith Support Vector Machines (SVM)-based chunk-ing.
Two lists of OOV word candidates are constructedeither by voting or merging the three OOV word ex-traction modules.
Finally, a CRFs-based word seg-menter produces the final results using either of thevoted list (Model a) or the merged list (Model c).Most of the classifiers use surrounding words andcharacters as the contextual features.
Since word andcharacter features may cause data sparse problem, weutilize a hard clustering algorithm (K-means) to defineword classes and character classes in order to over-come the data sparse problem.
The word classes areused as the hidden states in MEMM and CRF-basedword segmenters.
The character classes are used as thefeatures in character-based tagging, character-basedchunking and word segmentation.Model b is our previous method proposed in (Gohet al, 2004b): First, a MaxEnt classifier is used to per-form character-based tagging to identify OOV wordsin the test data.
In-vocabulary (IV) word list togetherwith the extracted OOV word candidates is used inMaximum Matching algorithm.
Overlapping ambi-guity is denoted by the different outputs from For-ward and Backward Maximum Matching algorithm.Finally, character-based tagging by MaxEnt classifierresolves the ambiguity.Section 2 describes Models a and c. Section 3 de-scribes Model b.
Section 4 discusses the differencesamong the three models.2 Models a and cModels a and c use several modules.
First, a hardclustering algorithm is used to define word classes andcharacter classes.
Second, three OOV extraction mod-ules are trained with the training data.
These modules,then, extract the OOV words in the test data.
Third,the OOV word candidates produced by the three OOVextraction modules are refined by voting (Model a) ormerging (Model c) them.
The final word list is com-posed by appending the OOV word candidates to theIV word list.
Finally, a CRF-based word segmenteranalyzes the sentence based on the new word list.2.1 Clustering for word/character classesWe perform hard clustering for all wordsand characters in the training data.
K-means algorithm is utilized.
We use R 2.2.1(http://www.r-project.org/) to performk-means clustering.134Since the word types are too large, we cannot run k-means clustering on the whole data.
Therefore, we di-vide the word types into 4 groups randomly.
K-meansclustering is performed for each group.
Words in eachgroup are divided into 5 disjoint classes, producing 20classes in total.
Preceding and succeeding words in thetop 2000 rank are used as the features for the cluster-ing.
We define the set of the OOV words as the 21stclass.
We also define two other classes for the begin-of-sentence (BOS) and end-of-sentence (EOS).
So, wedefine 23 classes in total.20 classes are defined for characters.
K-means clus-tering is performed for all characters in the trainingdata.
Preceding and succeeding characters and BIESposition tags are used as features for the clustering:?B?
stands for ?the first character of a word?
; ?I?
standsfor ?an intermediate character of a word?
; ?E?
standsfor ?the last character of a word?
; ?S?
stands for ?thesingle character word?.
Characters only in the test dataare not assigned with any character class.2.2 Three OOV extraction modulesIn Models a and c, we use three OOV extraction mod-ules.First and second OOV extraction modules usethe output of a Maximam Entropy Markov Model(MEMM)-based word segmenter (McCallum et al,2000) (Uchimoto et al, 2001).
Word list is composedby the words appeared in 80% of the training data.The words occured only in the remaining 20% of thetraining data are regarded as OOV words.
All wordcandidates in a sentence are extracted to form a trel-lis.
Each word is assigned with a word class.
Theword classes are used as the hidden states in the trellis.In encoding, MaxEnt estimates state transition proba-bilities based on the preceding word class (state) andobserved features such as the first character, last char-acter, first character class, last character class of thecurrent word.
In decoding, a simple Viterbi algorithmis used.The output of the MEMM-based word segmenter issplitted character by character.
Next, character-basedchunking is performed to extract OOV words.
We usetwo chunkers: based on SVM (Kudo and Matsumoto,2001) and CRF (Lafferty et al, 2001).
The chunkerannotates BIO position tags: ?B?
stands for ?the firstcharacter of an OOV word?
; ?I?
stands for ?other char-acters in an OOV word?
; ?O?
stands for ?a characteroutside an OOV word?.The features used in the two chunkers are the char-acters, the character classes and the information ofother characters in five-character window size.
Theword sequence output by the MEMM-based word seg-menter is converted into character sequence with BIESposition tags and the word classes.
The position tagswith the word classes are also introduced as the fea-tures.The third one is a variation of the OOV module insection 3 which is character-based tagging by MaxEntclassifier.
The difference is that we newly introducecharacter classes in section 2.1 as the features.In summary, we introduce three OOV word extrac-tion modules: ?MEMM+SVM?, ?MEMM+CRF?
and?MaxEnt classifier?.2.3 Voting/Merging the OOV wordsThe word list for the final word segmenter are com-posed by voting or merging.
Voting means the OOVwords which are extracted by two or more OOV wordextraction modules.
Merging means the OOV wordswhich are extracted by any of the OOV word extrac-tion modules.
The model with the former (voting)OOV word list is used in Model a, and the model withthe latter (merging) OOV word list is used in Model c.2.4 CRF-based word segmenterFinal word segmentation is carried out by a CRF-basedword segmenter (Kudo and Matsumoto, 2004) (Pengand McCallum, 2004).
The word trellis is composedby the similar method with MEMM-based word seg-menter.
Though state transition probabilities are esti-mated in the case of MaxEnt framework, the proba-bilities are normalized in the whole sentence in CRF-based method.
CRF-based word segmenter is robust tolength-bias problem (Kudo and Matsumoto, 2004) bythe global normalization.
We will discuss the length-bias problem in section 4.2.5 Note on MSR dataUnfortunately, we could not complete Models a andc for the MSR data due to time constraints.
There-fore, we submitted the following 2 fragmented mod-els: Model a for MSR data is MEMM-based wordsegmenter with OOV word list by voting; Model c forMSR data is CRF-based word segmenter with no OOVword candidate.3 Model bModel b uses a different approach.
First, we extract theOOV words using a MaxEnt classifier with only thecharacter as the features.
We did not use the characterclasses as the features.
Each character is assigned withBIES position tags.
Word segmentation by character-based tagging is firstly introduced by (Xue and Con-verse, 2002).
In encoding, we extract characters withinfive-character window size for each character positionin the training data as the features for the classifier.In decoding, the BIES position tag is deterministicallyannotated character by character in the test data.
The135words that appear only in the test data are treated asOOV word candidates.We can obtain quite high unknown word recall withthis model but the precision is a bit low.
However,the following segmentation model will try to elimi-nate some false unknown words.
In the next step, weappend OOV word candidates into the IV word listextracted from the training data.
The segmentationmodel is similar to the OOV extraction method, exceptthat the features include the output from the MaximumMatching (MaxMatch) algorithm.
The algorithm runsin both forward (FMaxMatch) and backward (BMax-Match) directions using the final word list as the ref-erences.
The outputs of FMaxMatch and BMaxMatchare also assigned with BIES tags.
The differences be-tween the FMaxMatch and BMaxMatch outputs indi-cate the positions where the overlapping ambiguitiesoccur.
The final word segmentation is carried out byMaxEnt classifier again.Note, both procedures in Model b use whole train-ing data in the training phase.
The dictionary used inthe MaxMatch algorithm is extracted from the trainingdata only during the training phase.
So, the training ofsegmentation model does not explicitly consider OOVwords.
We did not use the word and character classesas features in Model b unlike in the case of Models aand c. The details of the model can be found in (Gohet al, 2004b).
The difference is that we do not pro-vide character types here because it is forbidden inthis round.
Besides, we also did not prune the OOVwords because this step involve the intervention of hu-man knowledge.4 Discussions and ConclusionsTable 1 summarizes the results of the three models.The proposed systems employ purely corpus-basedstatistical/machine learning method.
Now, we discusswhat we observe in the three models.
We remark twoproblems in word segmentation: OOV word problemand length-bias problem.OOV word problem is that simple word-basedMarkov Model family cannot analyze the words notincluded in the word list.
One of the solutions ischaracter-based tagging (Xue and Converse, 2002)(Goh et al, 2004a).
The simple character-based tag-ging (Model b) achieved high ROOV but the precisionis low.
We tried to refine OOV extraction by votingand merging (Model a and c).
However, the ROOVof Models a and c are not as good as that of Modelb.
Figure 1 shows type-precision and type-recall ofeach OOV extraction modules.
While voting helps tomake the precision higher, voting deteriorates the re-call.
Defining some hand written rules to prune falseOOV words will help to improve the IV word segmen-tation (Goh et al, 2004b), because the precision ofOOV word extraction becomes higher.
Other types ofOOV word extraction methods should be introduced.For example, (Uchimoto et al, 2001) embeded OOVmodels in MEMM-based word segmenter (with POStagging).
Less than six-character substrings are ex-tracted as the OOV word candidates in the word trel-lis.
(Peng and McCallum, 2004) proposed OOV wordextraction methods based on CRF-based word seg-menter.
Their CRF-based word segmenter can com-pute a confidence in each segment.
The high confi-dent segments that are not in the IV word list are re-garded as OOV word candidates.
(Nakagawa, 2004)proposed integration of word and OOV word positiontag in a trellis.
These three OOV extraction method aredifferent from our methods ?
character-based tagging.Future work will include implementation of these dif-ferent sorts of OOV word extraction modules.Length bias problem means the tendency that the lo-cally normalized Markov Model family prefers longerwords.
Since choosing the longer words reduces thenumber of words in a sentence, the state-transitions arereduced.
The less the state-transitions, the larger thelikelihood of the whole sentence.
Actually, the length-bias reflects the real distribution in the corpus.
Still,the length-bias problem is nonnegligible to achievehigh accuracy due to small exceptional cases.
We usedCRF-based word segmenter which relaxes the prob-lem (Kudo and Matsumoto, 2004).
Actually, the CRF-based word segmenter achieved high RIV .We could not complete Model a and c for MSR.After the deadline, we managed to complete Modela (CRF + Voted Unk.)
and c (CRF + Merged Unk.
)The result of Model a was precesion 0.976, recall0.966, F-measure 0.971, OOV recall 0.570 and IV re-call 0.988.
The result of Model c was precesion 0.969,recall 0.963, F-measure 0.966, OOV recall 0.571 andIV recall 0.974.
While the results are quite good, un-fortunately, we could not submit the outputs in time.While our results for the three data sets (AS,CITYU, MSR) are fairly good, the result for the PKUdata is not as good.
There is no correlation betweenscores and OOV word rates.
We investigate unseencharacter distributions in the data set.
There is no cor-relation between scores and unseen character distribu-tions.We expected Model c (merging) to achieve higherrecall for OOV words than Model a (voting).
How-ever, the result was opposite.
The noises in OOVword candidates should have deteriorated the F-valueof overall word segmentation.
One reason might bethat our CRF-based segmenter could not encode theoccurence of OOV words.
We defined the 21st wordclass for OOV words.
However, the training data forCRF-based segmenter did not contain the 21st class.We should include the 21st class in the training data136Table 1: Our Three Models and Results: F-value/ROOV /RIV (Rank of F-value)AS CITYU MSR PKUModel a CRF + Voted Unk.
CRF + Voted Unk.
MEMM + Voted Unk.
CRF + Voted Unk.0.947/0.606/0.971 0.942/0.629/0.967 0.949/0.378/0.971 0.934/0.521/0.955(2/11) (2/15) (16/29) (10/23)Model b Char.-based tagging Char.-based tagging Char.-based tagging Char.-based tagging0.952/0.696/0.963 0.941/0.736/0.953 0.958/0.718/0.958 0.941/0.760/0.941(1/11) (3/15) (6/29) (7/23)Model c CRF + Merged Unk.
CRF + Merged Unk.
CRF + No Unk.
CRF + Merged Unk.0.939/0.445/0.967 0.928/0.598/0.940 0.943/0.025/0.990 0.917/0.325/0.940(7/11) (8/15) (21/29) (14/23)150/764MEMM+SVMMEMM+CRF MaxEntVoted Precision = 1727/2504=0.689Voted Recall = 1727/3226=0.535Merged Precision = 2532/6003=0.421Merged Recall = 2532/3226=0.78469/599 586/2136165/480 420/579184/304958/1141AS51/406MEMM+SVMMEMM+CRF MaxEntVoted Precision = 1068/1714=0.623Voted Recall = 1068/1670=0.639Merged Precision = 1367/3531=0.387Merged Recall = 1367/1670=0.81842/352 206/105987/439 114/188109/196758/891CITYUcorrectly extracted types(left side)extracted types(right side)57/555MEMM+SVMMEMM+CRF MaxEntVoted Precision = 1196/1659=0.720Voted Recall = 1196/1991=0.600Merged Precision = 1628/4454=0.365Merged Recall = 1628/1991=0.81740/330 335/191093/293 149/243245/333709/790MSR67/882MEMM+SVMMEMM+CRF MaxEntVoted Precision = 1528/2827=0.540Voted Recall = 1528/2863=0.533Merged Precision = 2184/7064=0.309Merged Recall = 2184/2863=0.76287/720 502/2635181/727 217/424201/407929/1269PKUFigure 1: OOV Extraction Precision and Recall by Typeby regarding some words as pseudo OOV words.We also found a bug in the CRF-based OOV wordextration module.
The accuracy of the module mightbe slightly better than the reported results.
However,the effect of the bug on overall F-value might be lim-ited, since the module was only part of the OOV ex-traction module combination ?
voting and merging.AcknowledgementWe would like to express our appreciation to Dr. TakuKudo who developed SVM-based chunker and gave usseveral fruitful comments.ReferencesChooi-Ling Goh, Masayuki Asahara, and Yuji Mat-sumoto.
2004a.
Chinese Word Segmentation byClassification of Characters.
In Proc.
of ThirdSIGHAN Workshop, pages 57?64.Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-sumoto.
2004b.
Pruning False Unknown Words toImprove Chinese Word Segmentation.
In Proc.
ofPACLIC-18, pages 139?149.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith Support Vector Machines.
In Proc.
of NAACL-2001, pages 192?199.Taku Kudo and Yuji Matsumoto.
2004.
ApplyingConditional Random Fields to Japanese Morpho-logical Analysis.
In Proc.
of EMNLP-2004, pages230?237.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proc.
of ICML-2001, pages 282?289.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum Entropy Markov Mod-els for Information Extraction and Segmentation.
InProc.
of ICML-2000, pages 591?598.Tetsuji Nakagawa.
2004.
Chinese and Japanese WordSegmentation Using Word-Level and Character-Level Information.
In Proc.
of COLING-2004,pages 466?472.Fuchun Peng and Andrew McCallum.
2004.
ChineseSegmentation and New Word Detection using Con-ditional Random Fields.
In Proc.
of COLING-2004,pages 562?568.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-hara.
2001.
The Unknown Word Problem: aMorphological Analysis of Japanese Using Maxi-mum Entropy Aided by a Dictionary.
In Proc.
ofEMNLP-2001, pages 91?99.Nianwen Xue and Susan P. Converse.
2002.
Combin-ing Classifiers for Chinese Word Segmentation.
InProc.
of First SIGHAN Workshop, pages 63?70.137
