Trainable Methods for Surface Natural Language GenerationAdwai t  RatnaparkhiIBM T J  Watson Research CenterP.O.
Box 218Yorktown Heights, NY 10598aratnapa@us,  ibm.
comAbstractWe present hree systems for surface natural an-guage generation that are trainable from annotatedcorpora.
The first two systems, called NLG1 andNLG2, require a corpus marked only with domain-specific semantic attributes, while the last system,called NLG3, requires a corpus marked with bothsemantic attributes and syntactic dependency infor-mation.
All systems attempt to produce agrammat-ical natural language phrase from a domain-specificsemantic representation.
NLG1 serves a baselinesystem and uses phrase frequencies to generate awhole phrase in one step, while NLG2 and NLG3use maximum entropy probability models to indi-vidually generate ach word in the phrase.
The sys-tems NLG2 and NLG3 learn to determine both theword choice and the word order of the phrase.
Wepresent experiments in which we generate phrases todescribe flights in the air travel domain.1 In t roduct ionThis paper presents three trainable systems for sur-face natural language generation (NLG).
SurfaceNLG, for our purposes, consists of generating agrammatical natural language phrase that expressesthe meaning of an input semantic representation.The systems take a "corpus-based" or "machine-learning" approach to surface NLG, and learn togenerate phrases from semantic input by statisti-cally analyzing examples of phrases and their cor-responding semantic representations.
The determi-nation of the content in the semantic representation,or "deep" generation, is not discussed here.
Instead,the systems assume that the input semantic repre-sentation is fixed and only deal with how to expressit in natural language.This paper discusses previous approaches to sur-face NLG, and introduces three trainable systemsfor surface NLG, called NLG1, NLG2, and NLG3.Quantitative valuation of experiments in the airtravel domain will also be discussed.2 Previous ApproachesTemplates are the easiest way to implement surfaceNLG.
A template for describing a flight nounphrase in the air travel domain might be f l ightdeparting from $city-fr at $time-dep andarriving in $city-to at $time-arr where thewords starting with "$" are actually variables --representing the departure city, and departure time,the arrival city, and the arrival time, respectively--whose values will be extracted from the environmentin which the template is used.
The approach ofwriting individual templates is convenient, but maynot scale to complex domains in which hundredsor thousands of templates would be necessary, andmay have shortcomings in maintainability and textquality (e.g., see (Reiter, 1995) for a discussion).There are more sophisticated surface genera-tion packages, uch as FUF/SURGE (Elhadad andRobin, 1996), KPML (Bateman, 1996), MUMBLE(Meteer et al, 1987), and RealPro (Lavoie and Ram-bow, 1997), which produce natural anguage textfrom an abstract semantic representation.
Thesepackages require linguistic sophistication i  order towrite the abstract semantic representation, but theyare flexible because minor changes to the input canaccomplish major changes to the generated text.The only trainable approaches (known to the au-thor) to surface generation are the purely statisticalmachine translation (MT) systems uch as (Bergeret al, 1996) and the corpus-based generation sys-tem described in (Langkilde and Knight, 1998).
TheMT systems of (Berger et al, 1996) learn to gen-erate text in the target language straight from thesource language, without the aid of an explicit se-mantic representation.
I  contrast, (Langkilde andKnight, 1998) uses corpus-derived statistical knowl-edge to rank plausible hypotheses from a grammar-based surface generation component.3 T ra inab le  Sur face  NLGIn trainable surface NLG, the goal is to learn themapping from semantics to words that would other-wise need to be specified in a grammar or knowledgebase.
All systems in this paper use attribute-value194pairs as a semantic representation, which suffice asa representation for a limited domain like air travel.For example, the set of attribute-value pairs { $city-fr = New York City, $city-to = Seattle , $time-dep= 6 a.m., $date-dep = Wednesday } represent themeaning of the noun phrase % flight to Seattle thatdeparts from New York City at 6 a.m. on Wednes-day".
The goal, more specifically, is then to learnthe optimal attribute ordering and lexical choice forthe text to be generated from the attribute-valuepairs.
For example, the NLG system should auto-matically decide if the attribute ordering in "flightsto New York in the evening" is better or worse thanthe ordering in "flights in the evening to New York".Furthermore, it should automatically decide if thelexical choice in "flights departing to New York" isbetter or worse than the choice in "flights leaving toNew York".
The motivation for a trainable surfacegenerator is to solve the above two problems in away that reflects the observed usage of language ina corpus, but without the manual effort needed toconstruct a grammar or knowledge base.All the trainable NLG systems in this paper as-sume the existence of a large corpus of phrases inwhich the values of interest have been replaced withtheir corresponding attributes, or in other words, acorpus of generation templates.
Figure 1 shows asample of training data, where only words markedwith a "$" are attributes.
All of the NLG systemsin this paper work in two steps as shown in Table 2.The systems NLG1, NLG2 and NLG3 all implementstep 1; they produce a sequence of words intermixedwith attributes, i.e., a template, from the the at-tributes alone.
The values are ignored until step 2,when they replace their corresponding attributes inthe phrase produced by step 1.3.1 NLGI :  the basel ineThe surface generation model NLG1 simply choosesthe most frequent template in the training data thatcorresponds to a given set of attributes.
Its perfor-mance is intended to serve as a baseline result to themore sophisticated models discussed later.
Specifi-cally, nlgl(A) returns the phrase that correspondsto the attribute set A:nlgl(A) = { argInaXphraseeTA\[empty string\] C(phrase, A) TATA =where TA are the phrases that have occurred withA in the training data, and where C(phrase, A) isthe training data frequency of the natural anguagephrase phrase and the set of attributes A. NLG1will fail to generate anything if A is a novel combi-nation of attributes.3.2 NLG2:  n -gram mode lThe surface generation system NLG2 assumes thatthe best choice to express any given attribute-valueset is the word sequence with the highest probabil-ity that mentions all of the input attributes exactlyonce.
When generating a word, it uses local infor-mation, captured by word n-grams, together withcertain non-local information, namely, the subset ofthe original attributes that remain to be generated.The local and non-local information is integratedwith use of features in a maximum entropy prob-ability model, and a highly pruned search procedureattempts to find the best scoring word sequence ac-cording to the model.3.2.1 P robab i l i ty  Mode lThe probability model in NLG2 is a conditional dis-tribution over V U * s top, ,  where V is the genera-tion vocabulary and where .
s top .
is a special "stop"symbol.
The generation vocabulary V consists of allthe words seen in the training data.
The form of themaximum entropy probability model is identical tothe one used in (Berger et al, 1996; Ratnaparkhi,1998):k f$(wi ,wi-1 ,wi-2,at~ri)YI j=I  Otj p(wilwi-l, wi-2,attri) =Z(Wi - l ,  w i -2 ,  attr i )kto t j= lwhere wi ranges over V t3 .
s top .
and{wi-l,wi-2,attri} is the history, where wi de-notes the ith word in the phrase, and attri denotesthe attributes that remain to be generated at posi-tion i in the phrase.
The fj ,  where fj(a, b) E {0, 1},are called features and capture any informationin the history that might be useful for estimatingp(wi\[wi-1, wi-2, attri).
The features used in NLG2are described in the next section, and the featureweights aj ,  obtained from the Improved IterativeScaling algorithm (Berger et al, 1996), are set tomaximize the likelihood of the training data.
Theprobability of the sequence W = wl ... wn, giventhe attribute set A, (and also given that its lengthis n) is:Pr(W = wa...wnllen(W) = n,A) =nH p(wilwi_1, wi_2, attri )i= l3.2.2 Feature  Se lec t ionThe feature patterns, used in NLG2 are shown inTable 3.
The actual features are created by match-ing the patterns over the training data, e.g., an ac-tual feature derived from the word bi-gram templatemight be:1 if wi = fromf (w i ,  Wi--1, Wi--2, attr~) = and wi-t = f l ightand $city -- fz E attri0 otherwise195flights on $air from $city-fr to $city-to the $time-depint of $date-depStrip flights on $air from $city-fr to $city-to leaving after $time-depaft on $date-depflights leaving from $city-fr going to $city-to after Stime-depaft on $date-depflights leaving from $city-fr to $city-to the $time-depint of Sdate-dep$air flight $fltnum from $city-fr to $city-to on $date-dep$city-fr to $city-to $air flight Sfltnum on the $date-depStrip flights from $city-fr to $city-toInput to Step 1:Output of Step 1:Table 1: Sample training data{ $city-fr, $city-to, $time-dep, $date-dep }'% flight to $city-to that departs from $city-fr atStime-dep on $date-dep"Input to Step 2:Output of Step 2:"a flight to $city-to that departs from $city-fr at$time-dep on $date-dep", { $city-fr = New YorkCity, $city-to = Seattle , $time-dep = 6 a.m.,$date-dep = Wednesday }'% flight to Seattle that departs from New YorkCity at 6 a.m. on Wednesday"Table 2: Two steps of NLG processLow frequency features involving word n-gramstend to be unreliable; the NLG2 system thereforeonly uses features which occur K times or more inthe training data.3.2.3 Search ProcedureThe search procedure attempts to find a word se-quence wl ... wn of any length n ~ M for the inputattribute set A such that1.
wn is the stop symbol , s top ,2.
All of the attributes in A are mentioned at leastonce3.
All of the attributes in A are mentioned at mostonceand where M is an heuristically set maximum phraselength.The search is similar to a left-to-right breadth-first-search, except hat only a fraction of the wordsequences are considered.
More specifically, thesearch procedure implements he recurrence:WN,1 = top(N, (wlw e V})Wg,i+l = top(N, next(WN,i))The set WN# is the top N scoring sequences oflength i, and the expression ext(WN,i) returnsall sequences wl...Wi+l such that wl .
.
.wi  EWN,i, and wi+l E V U .stop.
.
The expressiontop(N, next(WN#)) finds the top N sequences innext(Wg,i).
During the search, any sequence thatends with , s top .
is removed and placed in the setof completed sequences.
If N completed hypothesesare discovered, or if WN,M is computed, the searchterminates.
Any incomplete sequence which doesnot satisfy condition (3) is discarded and any com-plete sequence that does not satisfy condition (2) isalso discarded.When the search terminates, there will be at mostN completed sequences, ofpossibly differing lengths.Currently, there is no normalization for differentlengths, i.e., all sequences of length n < M areequiprobable:Pr(len(W) = n) = -~ n < M=0 n>MNLG2 chooses the best answer to express the at-tribute set A as follows:nlg2(A) = argmaXwew,,g 2 Pr( len(W) = n) .Pr(Wl len(W ) = n, A)where Wnt~2 are the completed word sequences thatsatisfy the conditions of the NLG2 search describedabove.3.3 NLG3: dependency informationNLG3 addresses a shortcoming of NLG2, namelythat the previous two words are not necessarily thebest informants when predicting the next word.
In-stead, NLG3 assumes that conditioning on syntacti-cally related words in the history will result on moreaccurate surface generation.
The search procedurein NLG3 generates a syntactic dependency tree from196DescriptionNo Attributes remainingWord bi-gram with attributeWord tri-gram with attributeFeature f(wi,  Wi-1, Wi-2, attri) .
.
.
.1 if wi =?
and attri = {}, 0 otherwise1 if wi =?
and wi-1 =?
and ?
E attri, 0 otherwise1 if wi =?
and wi-lwi-~ =??
and ?
E attri, 0 otherwiseTable 3: Features patterns for NLG2.
Any occurrence of "?"
will be instantiated with an actual value fromtraining data.top-to-bottom instead of a word sequence from left-to-right, where each word is predicted in the contextof its syntactically related parent, grandparent, andsiblings.
NLG3 requires a corpus that has been an-notated with tree structure like the sample depen-dency tree shown in Figure 1.3.3.1 P robab i l i ty  Mode lThe probability model for NLG3, shown in Figure 2,conditions on the parent, the two closest siblings, thedirection of the child relative to the parent, and theattributes that remain to be generated.Just as in NLG2, p is a distribution over V t2.
s top , ,  and the Improved Iterative Scaling algo-rithm is used to find the feature weights aj.
Theexpression chi(w) denotes the ith closest child tothe headword w, par(w) denotes the parent of theheadword w, dir E { le f t ,  r ight}  denotes the direc-tion of the child relative to the parent, and attrw,idenotes the attributes that remain to be generatedin the tree when headword w is predicting its ithchild.
For example, in Figure 1, if w ="flights",then Chl(W) ="evening" when generating the leftchildren, and chl(w) ="from" when generating theright children.
As shown in Figure 3, the proba-bility of a dependency tree that expresses an at-tribute set A can be found by computing, for eachword in the tree, the probability of generating itsleft children and then its right children.
1 In thisformulation, the left children are generated inde-pendently from the right children.
As in NLG2,NLG3 assumes the uniform distribution for thelength probabilities P r (# of left children = n) andPr (# of right children = n) up to a certain maxi-mum length M'  = 10.3.3.2 Feature  Select ionThe feature patterns for NLG3 are shown in Ta-ble 4.
As before, the actual features are created bymatching the patterns over the training data.
Thefeatures in NLG3 have access to syntactic informa-tion whereas the features in NLG2 do not.
Low fre-quency features involving word n-grams tend to beunreliable; the NLG3 system therefore only uses fea-tures which occur K times or more in the trainingdata.
Furthermore, if a feature derived from Table 4looks at a particular word chi(w) and attribute a,we only allow it if a has occurred as a descendent of1We use a dummy ROOT node to generate the top mosthead word of the phrasechi(w) in some dependency tree in the training set.As an example, this condition allows features thatlook at chi(w) ="to" and $city-toE attrw,i but dis-allows features that look at ch~(w) ="to" and $city-frE attrw,i.3.4 Search ProcedureThe idea behind the search procedure for NLG3 issimilar to the search procedure for NLG2, namely, toexplore only a fraction of the possible trees by con-tinually sorting and advancing only the top N treesat any given point.
However, the dependency treesare not built left-to-right like the word sequences inNLG2; instead they are built from the current head(which is initially the root node) in the followingorder:1.
Predict the next left child (call it xt)2.
If it is *stop, ,  jump to (4)3.
Recursively predict children of xt.
Resume from(1)4.
Predict the next right child (call it Xr)5.
If it is *stop*, we are done predicting childrenfor the current head6.
Recursively predict children ofxr.
Resume from(4)As before, any incomplete trees that have generateda particular attribute twice, as well as completedtrees that have not generated a necessary attributeare discarded by the search.
The search terminateswhen either N complete trees or N trees of the max-imum length M are discovered.
NLG3 chooses thebest answer to express the attribute set A as follows:nlga(A) = argmax Pr(TIA )TET.Igawhere Tntga are the completed ependency trees thatsatisfy the conditions of the NLG3 search describedabove.4 Exper imentsThe training and test sets used to evaluate NLG1,NLG2 and NLG3 were derived semi-automaticallyfrom a pre-existing annotated corpus of user queriesin the air travel domain.
The annotation schemeused a total of 26 attributes to represent flights.197flightse v e n ~ + )I IChicago(+) afternoon(+)Ithe(-)Figure 1': Sample dependency tree for the phrase evening flights from Chicago in the afternoon.
- and +signs indicate left or right child, respectively.I-Ik YJ (chi \ [ t?)
' to 'chi - - l (~) 'chi - -2(~)'Pa~(~)'dtr 'att~o,  i)p(chiCw)\[w, chi- 1 (w), chi-2 (w), par (w), dir, attr~,i ) - ~"Jffi' ~ J- -  Z (w,ch i_  1 (w) ,ch i -  2 (w) ,par(w),d i~,attr tu, i )Z(w, ehi_l(w),chi_2(w),par(w),dir, attrw,4) = ~v,, l-\[j=lk OL~/J(w"w'chi-l(*Z)'chl-2(w)'par(tv)'dir'att*'~'i)Figure 2: NLG3: Equations for the probability of the ith child of head word w, or chi(w)Pr(TIA)Prl?lt(wlA)Prri~ht(w\[A)= YI~eTPrl~ft(wlA)Prr~ght(wl A)---- Pr(# of left children = n) YL=ln p(chi(w)lw,chi-l(w),chi-2(w),par(w),dir = left,attr~,i)= Pr(~ of right children = n) rI~=l p(chi(w)lw, chi-1 (w), chi-2 (w),par(w), dir = right, attrw,~)Figure 3: NLG3: Equations for the probability of a dependency tree TDescriptionSiblingsParent + siblingParent + grandparentFeature f (chi(w), w, ch~_ 1 (w), chi_2 (w), par (w), dir, attrw,i) = .
.
.1 if chi(w) =?
and chi- l (w) =?
and chi-2(w) =?
and dir =?
and?
E attrw,i, 0 otherwise1 if chi(w) =?
and chi_t(w) =?
and w =?
and dir =?
and ?
Eattrw,i, 0 otherwise1 ifchi(w) =?
and w =?
and par(w) =?
and dir =?
and ?
?
attrw,i,0 otherwiseTable 4: Features patterns for NLG3.
Any occurrence of "?"
will be instantiated with an actual value fromtraining data.System ParametersNLG1NLG2 N=IO,M=30,K=3NLG3 N=5,M=30,K=IO% Correct % OK % Bad % No output % error reductionfrom NLGI84.9 4.9 7.2 3.0 -88.2 4.7 6.4 0.7 2289.9 4.4 5.5 0.2 33Table 5: Weighted evaluation of trainable surface generation systems by judge ASystem Parameters % Correct % OK % Bad % No output % error reductionfrom NLG1NLG1 81.6 8.4 7.0 3.0 -NLG2 N=IO,M=30,K=3 86.3 5.8 7.2 0.7 26NLG3 N=5,M=30,K=10 88.4 4.0 7.4 0.2 37Table 6: Weighted evaluation of trainable surface generation systems by judge B198SystemNLG1NLG2NLG3Parameters % Correct % OK % Bad % No output % error reductionfrom NLG148.4 6.8 24.2 20.5N=IO,M=30,K=3 64.7 12.1 22.6 0.5 32N=5,M=30,K=IO 63.1 11.6 23.7 1.6 29Table 7: Unweighted evaluation of trainable surface generation systems by judge ASystem ParametersNLG1NLG2 N=IO,M=30,K=3NLG3 N=5,M=30,K=IO% Correct % OK % Bad % No output % error reductionfrom NLG141.1 8.9 29.5 20.562.1 13.7 23.7 0.5 3665.3 11.1 22.1 1.6 41Table 8: Unweighted evaluation of trainable surface generation systems by judge BThe training set consisted of 6000 templates describ-ing flights while the test set consisted of 1946 tem-plates describing flights.
All systems used the sametraining set, and were tested on the attribute setsextracted from the phrases in the test set.
For ex-ample, if the test set contains the template "flightsto $city-to leaving at Stime-dep", the surface gener-ation systems will be told to generate a phrase forthe attribute set { $city-to, Stime-dep }.
The out-put of NLG3 on the attribute set { $city-to, $city-fr,$time-dep } is shown in Table 9.There does not appear to be an objective auto-matic evaluation method 2 for generated text thatcorrelates with how an actual person might judgethe output.
Therefore, two judges - -  the authorand a colleague - -  manually evaluated the output ofall three systems.
Each judge assigned each phrasefrom each of the three systems one of the followingrankings:Correct: Perfectly acceptableOK: Tense or agreement is wrong, but word choiceis correct.
(These errors could be corrected bypost-processing with a morphological nalyzer.
)Bad: Words are missing or extraneous words arepresentNo Output:  The system failed to produce any out-putWhile there were a total 1946 attribute sets fromthe test examples, the judges only needed to evalu-ate the 190 unique attribute sets, e.g., the attributeset { $city-fr $city-to } occurs 741 times in the testdata.
Subjective valuation of generation output is2Measur ing word overlap or edit distance between the sys-tem's  output  and a "reference" set would be an automat icscoring method.
We believe that  such a method does notaccurately measure the correctness or grammat ica l i ty  of thetext.not ideal, but is arguably superior than an auto-matic evaluation that fails to correlate with humanlinguistic judgement.The results of the manual evaluation, as well asthe values of the search and feature selection param-eters for all systems, are shown in Tables 5, 6, 7, and8.
(The values for N, M, and K were determined bymanually evaluating the output of the 4 or 5 mostcommon attribute sets in the training data).
Theweighted results in Tables 5 and 6 account for mul-tiple occurrences of attribute sets, whereas the un-weighted results in Tables 7 and 8 count each uniqueattribute set once, i.e., { $city-fr $city-to } is counted741 times in the weighted results but once in the un-weighted results.
Using the weighted results, whichrepresent testing conditions more realistically thanthe unweighted results, both judges found an im-provement from NLG1 to NLG2, and from NLG2to NLG3.
NLG3 cuts the error rate from NLG1 byat least 33% (counting anything without a rank ofCorrect as wrong).
NLG2 cuts the error rate byat least 22% and underperforms NLG3, but requiresfar less annotation i its training data.
NLG1 has nochance of generating anything for 3% of the data - -it fails completely on novel attribute sets.
Using theunweighted results, both judges found an improve-ment from NLG1 to NLG2, but, surprisingly, judgeA found a slight decrease while judge B found anincrease in accuracy from NLG2 to NLG3.
The un-weighted results how that the baseline NLG1 doeswell on the common attribute sets, since it correctlygenerates only less than 50% of the unweighted casesbut over 80% of the weighted cases.5 D iscuss ionThe NLG2 and NLG3 systems automatically at-tempt o generalize from the knowledge inherent inthe training corpus of templates, o that they cangenerate templates for novel attribute sets.
There199Probability Generated Text0.1075820.008224410.005647120.003433720.0012465$time-dep flights from $city-fr to $city-to$time-dep flights between $city-fr and $city-toStime-dep flights $city-fr to $city-toflights from $city-fr to $city-to at Stime-depStime-dep flights from $city-fr to to $city-toTable 9: Sample output from NLG3.
(Dependency tree structures are not shown.)
Typical values forattributes: $time-dep -- "10 a.m.", $city-fr = "New York", $city-to = "Miami"is some additional cost associated with producingthe syntactic dependency annotation ecessary forNLG3, but virtually no additional cost is associatedwith NLG2, beyond collecting the data itself andidentifying the attributes.The trainable surface NLG systems in this pa-per differ from grammar-based systems in how theydetermine the attribute ordering and lexical choice.NLG2 and NLG3 automatically determine attributeordering by simultaneously searching multiple or-derings.
In grammar-based approaches, uch pref-erences need to be manually encoded.
NLG2 andNLG3 solve the lexical choice problem by learningthe words (via features in the maximum entropyprobability model) that correlate with a given at-tribute and local context, whereas (Elhadad et al,1997) uses a rule-based approach to decide the wordchoice.While trainable approaches avoid the expense ofcrafting a grammar to determine attribute order-ing and lexicai choice, they are less accurate thangrammar-based approaches.
For short phrases, ac-curacy is typically 100% with grammar-based ap-proaches ince the grammar writer can either cor-rect or add a rule to generate the phrase of interestonce an error is detected.
Whereas with NLG2 andNLG3, one can tune the feature patterns, search pa-rameters, and training data itself, but there is noguarantee that the tuning will result in 100% gener-ation accuracy.Our approach differs from the corpus-basedsurface generation approaches of (Langkilde andKnight, 1998) and (Berger et al, 1996).
(Langkildeand Knight, 1998) maps from semantics to wordswith a concept ontology, grammar, and lexicon, andranks the resulting word lattice with corpus-basedstatistics, whereas NLG2 and NLG3 automaticallylearn the mapping from semantics to words from acorpus.
(Berger et ai., 1996) describes a statisticalmachine translation approach that generates text inthe target language directly from the source text.NLG2 and NLG3 are also statistical learning ap-proaches but generate from an actual semantic rep-resentation.
This comparison suggests that statis-tical MT systems could also generate text from an"interlingua", in a way similar to that of knowledge-based translation systems.We suspect that our statistical generation ap-proach should perform accurately in domains of sim-ilar complexity to air travel.
In the air travel do-main, the length of a phrase fragment to describean attribute is usually only a few words.
Domainswhich require complex and lengthy phrase fragmentsto describe a single attribute will be more challeng-ing to model with features that only look at wordn-grams for n E {2, 3).
Domains in which thereis greater ambiguity in word choice will require amore thorough search, i.e., a larger value of N, atthe expense of CPU time and memory.
Most im-portantly, the semantic annotation scheme for airtravel has the property that it is both rich enoughto accurately represent meaning in the domain, butsimple enough to yield useful corpus statistics.
Ourapproach may not scale to domains, such as freelyoccurring newspaper text, in which the semantic an-notation schemes do not have this property.Our current approach as the limitation that itignores the values of attributes, even though theymight strongly influence the word order and wordchoice.
This limitation can be overcome by usingfeatures on values, so that NLG2 and NLG3 mightdiscover - -  to use a hypothetical example - -  that"flights leaving $city-fr" is preferred over "flightsfrom $city-fr" when $city-fr is a particular value,such as "Miami".6 Conc lus ionsThis paper presents the first systems (known to theauthor) that use a statistical learning approach toproduce natural language text directly from a se-mantic representation.
Information to solve theattribute ordering and lexical choice problems--which would normally be specified in a large hand-written graxnmar-- is automatically collected fromdata with a few feature patterns, and is combinedvia the max imum entropy framework.
NLG2 showsthat using just local n-gram information can out-perform the baseline, and NLG3 shows that usingsyntactic information can further improve genera-tion accuracy.
We conjecture that NLG2 and NLG3should work in other domains which have a com-plexity similar to air travel, as well as available an-200notated ata.7 AcknowledgementsThe author thanks Scott McCarley for serving as thesecond judge, and Scott Axelrod, Kishore Papineni,and Todd Ward for their helpful comments on thiswork.
This work was supported in part by DARPAContract # MDA972-97-C-0012.ReferencesJohn Bateman.
1996.
Kpml development envi-ronment - multilingual linguistic resource devel-opment and sentence generation.
Technical re-port, German Centre for Information Technol-ogy (GMD), Institute for Integrated Informationand Publication Systems (IPSI), Darmstadt, Ger-many.Adam Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum EntropyApproach to Natural Language Processing.
Com-putational Linguistics, 22(1):39-71.Michael Elhadad and Jacques Robin.
1996.
Anoverview of surge: a reusable comprehensive syn-tactic realization component.
Technical Report96-03, Ben Gurion University, Beer Sheva, Israel.Michael Elhadad, Kathleen McKeown, and JacquesRobin.
1997.
Floating constraints in lexicalchoice.
Computational Linguistics, pages 195-239.Irene Langkilde and Kevin Knight.
1998.
Genera-tion that exploits corpus-based statistical knowl-edge.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguisticsand 17th International Conference on Computa-tional Linguistics, University of Montreal, Mon-treal, Quebec, Canada.Benoit Lavoie and Owen Rambow.
1997.
A fastand portable realizer for text generation systems.In Proceedings of the Fifth Conference on Ap-plied Natural Language Processing, pages 265-268,Washington D.C., March 31-April 3.M.
W. Meteer, D. D. McDonald, S.D.
Anderson,D.
Forster, L.S.
Gay, A.K.
Huettner, and P. Si-bun.
1987.
Mumble-86: Design and implementa-tion.
Technical Report Technical Report COINS87-87, University of Massachusetts at Amherst.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
Ph.D. thesis, University of Pennsylvania.Ehud Reiter.
1995.
Nlg vs. Templates.
In Proceed-ings of the 5th European Workshop on NaturalLanguage Generation, Leiden, The Netherlands.201
