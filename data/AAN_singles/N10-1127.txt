Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 849?857,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Direct Syntax-Driven Reordering Model for Phrase-Based MachineTranslationNiyu GeIBM T.J.Watson ResearchYorktown Heights, NY 10598niyuge@us.ibm.comAbstractThis paper presents a direct word reorderingmodel with novel syntax-based features for sta-tistical machine translation.
Reordering modelsaddress the problem of reordering source lan-guage into the word order of the target language.IBM Models 3 through 5 have reordering com-ponents that use surface word information butvery  little context information to determine thetraversal order of the source sentence.
Since thelate 1990s, phrase-based machine translationsolves much of the local reorderings by usingphrasal translations.
The problem of long-distance reordering has become a central re-search topic in modeling distortions.
We presenta syntax driven maximum entropy reorderingmodel that directly predicts the source traversalorder and is able to model arbitrarily long dis-tance word movement.
We show that this modelsignificantly improves machine translation qual-ity.1    IntroductionMachine translation reordering models model theproblem of the word order when translating asource language into a target language.
For exam-ple in Spanish and Arabic, adjectives often comeafter the nouns they modify whereas in Englishmodifying adjectives usually precede the nouns.When translating Spanish or Arabic into English,the position of the adjectives need to be properlyreordered to be placed before the nouns to makefluent English.In this paper, we present a word reordering modelthat models the word reordering process in transla-tion.
The paper is organized as follows.
?2 out-lines previous approaches to reordering.
?3 detailsour model and its training and decoding process.
?4 discusses experiments to evaluate the modeland ?5 presents machine translation results.
?6 isdiscussion and conclusion.2    Previous WorkThe word reordering problem has been one of themajor problems in statistical machine translation(SMT).
Since exploring all possible reorderingsof a source sentence is an NP-complete problem(Knight 1999), SMT systems limit words to be re-ordered within a window of length k.  IBM Models3 through 5 (Brown et.al.
1993) model reorderingsbased on surface word information.
For example,Model 4 attempts to assign target-language posi-tions to source-language words by modeling d(j | i,l, m) where j is the target-language position, i is thesource-language position, l and m are respectivelysource and target sentence lengths.
These modelsare not effective in modeling reorderings becausethey don?t have enough context and lack structuralinformation.Phrase-based SMT systems such as (Koehn et.al.2003) move from using words as translation unitsto using phrases.
One of the advantages of phrase-based SMT systems is that local reorderings areinherent in the phrase translations.
However,phrase-based SMT systems capture reordering in-stances and not reordering phenomena.
For exam-ple, if the Arabic phrase ?the car red?
and itsEnglish translation ?the red car?
is seen in thetraining data, phrase-based SMT is able to producethe correct English for the Arabic ?the car red?.However it will not be able to produce ?the bluecar?
for the Arabic ?the car blue?
if the trainingdata does not contain this phrase pair.
Phrases donot capture the phenomenon that Arabic adjectivesand nouns need to be reordered.
Another problemwith phrase-based SMT is the problem of long-range reorderings.
Recent work on reordering hasbeen focusing on capturing general reordering849phenomena (as opposed to instances) and on solv-ing long-range reordering problems.
(Al-onaizan et.al.
2006) proposes 3 distor-tion models, the inbound, outbound, and pair mod-els.
They together model the likelihood oftranslating a source word at position i given thatthe source word at position j has just been trans-lated.
These models perform better than n-grambased language models but are limited in their useof only  the surface strings.Instead of directly modeling the distanceof word movement, phrasal level reordering mod-els model how to move phrases,  also called orien-tations.
Orientations typically apply to adjacentphrases.
Two adjacent phrases can be eitherplaced monotonically (sometimes called straight)or swapped (non-monotonically or inverted).Early orientation models do not use lexical con-tents such as (Zens et.
al., 2004).
More recently,(Xiong et.al.
2006;  Zens 2006; Och et.
al, 2004;Tillmann, 2004;  Kumar et al, 2005, Ni et al,2009) all presented models that use lexical featuresfrom the phrases to predict their orientations.These models are very powerful in predicting localphrase placements.
More recently (Galley et.al.2008) introduced a hierarchical orientation modelthat captures some non-local phrase reorderings bya shift reduce algorithm.
Because of the heavy useof lexical features, these models tend to sufferfrom data sparseness problems.
Another limitationis that these models are restricted to reorderingswith no gaps and phrases that are adjacent.We present a probabilistic reordering modelthat models directly the source translation se-quence and explicitly assigns probabilities to thereorderings of the source input with no restrictionson gap, length or adjacency.
This is different fromthe approaches of pre-order such as (Xia andMcCord 2004; Collins et.al.
2005; Kanthak et.
al.2005; Li et.
al., 2007).
Although our model canbe used to produce top N pre-ordered source, theexperiments reported here do not use the model inthe pre-order mode.
Instead, the reordering modelis used to generate a reorder lattice which encodesmany reorderings and their costs (negative logprobability).
This reorder lattice is independent ofthe translation decoder.
In principle, any decodercan use this lattice for its reordering needs.
Wehave integrated the reorder lattice into a phrase-based.
The experiments reported here are from thephrase-based decoder.We present the reordering model based onmaximum entropy models.
We then describe thesyntactic features in the context of Chinese to Eng-lish translation.3    Maximum Entropy Reordering ModelThe model takes a source sequence of length n:],...,[ 21 nsssS =and models its translation or visit order accordingto the target language:],...,[ 21 nvvvV =where vj is the source position for target position j.For example, if the 2nd source word is to be trans-lated first, then v1 = 2.
We find V such that)2(),|(max)1()|(maxarg11...1}{?=?
?=njjjVvvSvpSVp?In equation (1) {? }
is the set of possible visit or-ders.
We want to find a visit order V such that theprobability p(V|S) is maximized.
Equation (2) is acomponent-wise decomposition of (1).Let)...,( 11 ?== jj vvShandvfWe use the maximum entropy model to estimateequation (2):?=kkk hfhZhfp )3()),(exp()(1)|( ?
?where Z(h) is the normalization constant)4(),(exp)( ?
?=f kkk hfhZ ?
?In equation (3), ?k(f, h) are binary-valued features.During training, instead of exploring all possiblepermutations,  samples are drawn given the correctpath only.3.1   Feature OverviewMost of our features ?k(f, h) are syntax-based.They examine how each parse node is reorderedduring translation.
We also have a few non-syntaxfeatures that inspect the surface words and part-of-speech tags.
They complement syntax features bycapturing lexical dependencies and guardingagainst parsing errors.
Instead of directly model-850Step:                  1  2  3   4  5  6  7  8  9  10  11Visit Sequence:   1  9  10 2  8  7  6  3  4   5   11Figure 1.
A Chinese-English Parallel Sentence with Chinese Parseing the absolute source position vj, we model thejump from the last source position vj-1.
All featuresshare two common components: j (for jump), andcov (for coverage).
Jumps are bucketed andcapped at 4 to prevent data sparsity.
Coverage isan integer indicating the visiting status of thewords between the jump.
Coverage is 0 if none ofthe words was visited prior to this step, 1 if allwere visited, and 2 if some but not all were visited.
(j, cov) are present in all features and are removedfrom the descriptions below.
A couple of featuresuse a variation of Jump and Coverage.
These willbe described in the feature description.3.2    Parse-based Syntax FeaturesWe use the sentence pair in Figure 1. as a work-ing example when describing the features.
Shownin the figure are a Chinese-English parallel sen-tence pair, the word alignments between them, andthe Chinese parse tree.
The parse tree is simpli-fied.
Some details such as part-of-speech tags areomitted and denoted by triangles.
The first step isto determine the source visit sequence from theword alignment, also shown at the bottom of Fig-ure 1.
If a target is aligned to more than onesource, we assume the visit order is left to right.In Figure 1, source words 2 and 8 are aligned to theEnglish ?at?
and we define the visit sequence to be8 following 2.Chinese and  English differ in the positioning ofthe modifiers.
In English, non-adjectival modifiersfollow the object they modify.
This is mostprominent in the use of relative clauses and prepo-sitional phrases.
Chinese in contrast is a pre-modification language where modifiers whetheradjectival, clausal or prepositional typically pre-cede the object they modify.
In Figure 1.,  theChinese prepositional phrase PP (in lightly shadedbox in the parse tree) spanning range  [2,8] pre-cedes the verb phrase VP2 at positions [9,10].These two phrases are swapped in English asshown by the two lightly shaded boxes in thealignment grid.
The relative clause CP (in darkshaded box in the parse tree) in Chinese spanningrange [3,6] precedes the noun phrase NP3 at posi-tion 7 whereas these two phrases are againswapped in English.851The phenomenon for the reordering model tocapture is that node VP1?s two children PP andVP2 (lightly shaded) need to be swapped regard-less of how long the PP phrase is.
This is also truefor node NP2 whose two children CP and NP3(dark shaded) need to be reversed.Parse-based features model how to reorder theconstituents in the parse by learning how to walkthe parse nodes.
For every non-unary node in theparse we learn such features as which of its child isvisited first and for subsequent visits how to jumpfrom one child to another.
For the treelet VP1 PP VP2 in Figure 1, we learn to visit the child VP2first, then PP.We now define the notion of ?node visit?.
Whena source word si is visited at step j, we find its pathto root from the  leaf node denoted as PathToRooti.We say all the nodes contained in PathToRooti arebeing visited at that step.
Parse-based features areapplied to every qualifying node in PathToRooti.Unary extensions do not qualify and are ignored.Since part-of-speech tags are unary branches,parse-based features apply from the lowest-levellabels.
Another condition depends on the jumpand is discussed in section ?3.4.
All our featuresare encoded by a vector of integers and are denotedas ?
(?)
in this paper.
We now describe the fea-tures.3.2.1   First Child FeaturesThe first-child feature applies when a node is vis-ited for the first time.
The feature learns which ofthe node?s child to visit first.
This feature learnssuch phenomena as translating the main verb firstunder a VP or translating the main NP first underan NP.
The feature is defined as ?
(currentLabel,parentLabel, nthNode, j, cov) wherecurrentLabel = label of the current parse nodeparentLabel = label of the parent nodenthNode = an integer indicating the nth occurrenceof the current nodeIn Figure 1, when source word 9 is visited at step2, its PathToRootis computed which is [VP2, VP1,IP1].
The first-child feature applied to VP2 is?
(VP2, VP1, 1, 4, 1) sincecurrentLabel = VP2;  parentLabel = VP1;nthChild = 1: VP2 is the 1st VP among its parent?schildrenj = 4: actual jump from 1 is 8 and is capped.cov = 0: words in between the jump [1,9] are notyet visited at this step.The semantics of this feature is that when a VPnode is visited, the first VP child under it is visitedfirst.
This feature learns to visit the first VP firstwhich is usually the head VP no matter where it ispositioned or how many modifiers precede it.3.2.2   Node Jump FeaturesThis feature applies on all subsequent visits to theparse node.
This feature models how to jump fromone sibling to another sibling.
This feature hasthese components: ?
(currentLabel, parentLabel,fromLable,  nodeJump,cov) wherefromLabel = the node label where the jump is fromnodeJump = node distance from that nodeThis feature effectively captures syntactic reorder-ings by looking at the node jump instead of surfacedistance jump.
In our example, a node-jump fea-ture for jumping from source 10 to 2 at step 4 atVP1 level is ?
(PP, VP1, VP2,  -1, 2) wherecurrentLabel = PP where source word 2 is underparentLabel = VP1fromLabel = VP2 where source word 10 is undernodeJump = -1 since the jump is from VP2 to PPcov = 2 because in between [2,10] word 9 has beenvisited and other words have not.This feature captures the necessary informationfor the ?PP VP?
reorderings regardless of how longthe PP or VP phrase is.3.2.3   Jump Over Sibling FeaturesTo make a correct jump from one sibling to theother, siblings that are jumped over should also beconsidered.
For example in Chinese, while jump-ing over a PP to cover a VP is a good jump, jump-ing over an ADVP to cover a VP may not bebecause adverbs in both Chinese and English oftenprecede the verb they modify.
The jump-over-sibling features help distinguish these cases.
Thisfeature?s components are ?
(currentLabel, parent-Label, jumpOverSibling, siblingCov, j) where jum-pOverSibling is the label of the sibling that isjumped over and siblingCov is the coverage statusof that sibling.This feature applies to every sibling that isjumped over.
At step 2 where the jump is fromsource 1 to 9, this feature at VP1 level is ?
(VP2,VP1, PP, 0, 4) because PP is a sibling of VP2 and852is jumped over, PP is not covered at this step, andthe jump is capped to be 4.3.2.4   Back Jump Sibling FeaturesFor every forward jump of length greater than 1,there is a backward jump to cover those words thatwere skipped.
In these situations we want to knowhow far we can move forward before we mustjump backward.
The back-jump-sibling featureapplies when the jump is backward (distance isnegative) and inspects the sibling to the right.
Itgenerates ?
(currentLabel,  rightSiblingCov, j).When jumping from 10 to 2 at step 4, this featureis ?
(PP, 1, -4) where -4 is the jump andcurrentLabel = PP where source word 2 is underrightSiblingCoverage = 1 since VP2 has beencompleted visited at this time.
This feature learnsto go back to PP when its right sibling (VP2) iscompleted.3.2.5    Broken FeaturesTranslations do not always respect the constituentboundaries defined by the source parse tree.
Con-sider the fragment in Figure 2.Figure 2.
A ?Broken?
TreeAfter the VV under VP2 is translated (?accountfor?
),  a transition is made to translate the ADVP(?approximately?)
leaving VP2 partially translated.We say that the node VP2 is  broken at this step.This type of feature has been shown to be usefulfor machine translation (Marton & Resnik 2008).Here, broken features model the context underwhich a node is broken by observing the feature?
(curTag, prevTag, parentLabel, j, cov).
For thetransition of source word 2 to source word 1 inFigure 2, a broken feature applies at VP2: ?
(AD,VV, VP2, -1 ,1).
This feature learns that a VP canbe broken when making a jump from a verb (VV)to an adverb (AD).3.3    Non-Parse FeaturesNon-parse features do not use or use less fine-grained information from the parse tree.3.3.1   Barrier FeaturesBarrier features model the intuition that certainwords such as punctuation should not move freely.This phenomenon has been observed and shown tobe helpful in (Xiong et.
al., 2008).
We call thesewords barrier words.
Barrier features are ?
(barri-erWord, cov, j).
All punctuations are barrierwords.3.3.2    Number of Zero Islands FeaturesAlthough word reorderings can involve wordsfar apart, certain jump patterns are highly unlikely.For example, the coverage pattern ?1010101010?where every other source word is translated wouldbe very improbable.
Let the right most coveredsource word be the frontier.
For every jump, thenumber-of-zero-islands feature computes the num-ber of uncovered source islands to the left of thefrontier.
Additionally it takes into account thenumber of parse nodes in between.
This feature isdefined as ?
(numZeroIslands, j, num-ParseNodesInBetween).
The number of parsenodes is the number of maximum spanning nodesin between the jump.
The jump at step 2 fromsource 1 to 9 triggers this number-zero-island fea-ture ?
(1, 4, 1).
The source coverage status at step 2is 10000000100 because the first source word hasbeen visited and the current visit is source 9.
Allwords in between have not been visited.
There is 1contiguous sequence of 0?s between the first ?1?and the last ?1?, hence the numZeroIslands = 1.There is one parse node PP that spans all thesource words from 2 to 8, therefore the last argu-ment to the feature is 1.
If instead, the transitionwas from source 1 to 8, then there would be 2maximum spanning parse nodes for source [2,7]which are nodes P and NP2.
The feature would be?
(1, 4, 2).
This feature discourages scatteredjumps that leave lots of zero islands and jump overlots of parse nodes.3.4    TrainingTraining the maximum entropy reordering modelneeds word alignments and source-side parses.
Weuse hand alignments from LDC.
The training data853statistics are shown in Table 1.
We use the (Levyand Manning 2003) parser on Chinese.Data #Sentences #WordsLDC2006E93 10,408 230,764LDC2008E57 11,463 194,024Table 1.
Training DataFrom the word alignments we first determine thesource visit sequence.
Table 2 details how the visitsequence is determined in various cases.Alignment Type S-T Visit Sequence1-1 Left to right from targetm-1 Left to right from source1-m Left most target link?
Attaches leftTable 2.
Determining visit sequenceThe first column shows alignment type fromsource (S) to target (T).
1-1 means one sourceword aligns to one target word.
m-1 means manysource words align to one target and vice versa.
?means unaligned source words.After the source visit sequence is decided, fea-tures are generated.
Note that the height of the treeis not uniform for all the words.
To preserve thestructure and also alleviate the depth problem, weuse the lowest-level-common-ancestor approach.For every jump, we generate features bottom upuntil we reach the node that is the common ances-tor of the origin and the destination of the jump.
InFigure 1 there is a jump from source 7 to 6 at step7.
The lowest-level-common-ancestor for source 6and 7 is the node NP2 and features are generatedup to the level of NP2.
Features on this trainingdata are shown in the second column in Table 5.The MaxEnt model on this data is efficientlytrained at 15 minutes per iteration (24 sen-tences/sec or 471 words/sec).4   Experiments4.1   Reorder EvaluationTo evaluate how accurate the reordering model is,we first compute its prediction accuracy.
Wechoose the first 100 sentences from NIST MT03 asour test set for this evaluation.
We manually wordalign them to the first set of reference using LDCannotation guidelines version 1.0 of April 2006.An average of 73% of the training sentences con-tain unaligned source words and over 87% of thetest sentences contain unaligned source words.The unaligned source words are mostly functionwords.
Because the visit sequence of unalignedsource words are determined not by truth but byheuristics (Table 2), they pose a problem in evalua-tion.We thus evaluate the model by measuring the ac-curacy of its decision conditioned on true history.We measure performance on the model?s top-Nchoices for N = 1,2, and 3.
Results are in Table 3.The table also shows the accuracy of  no reorder-ing in the Monotone column.Top-N Accuracy Monotone1 80.56% 65.39%2 90.66% -3 93.05% -Table 3.
Reordering model performanceFigure 3 plots accuracy vs. MaxEnt training itera-tion.
Accuracy starts low at 74.7% and reaches ishighest at iteration 8 and fluctuates around 80.5%thereafter.71727374757677787980811 2 3 4 5 6 7 8 9 10 11 12 13 14 15Figure 3.
Accuracy vs.  MaxEnt Training IterationWe analyze 50 errors from the top-1 run.
The er-rors are categorized and shown in Table 4.Error Category PercentageLexical 34%Parse 30%Model 20%Reference 16%Table 4.
Error Analysis?Lexical?
errors are those that rise from lexicalchoice of source words.
For example, an ?ADVPVP?
structure would normally be visited mono-tonically.
However, in case of  Chinese  phrase ?sodo?, they should be swapped.
More than a third ofthe errors are of this nature.
Errors in the Refer-854ence category are those that are marked wrong be-cause of the particular English reference.
The pro-posed reorderings are correct but they don?t matchthe reference reorderings.
Another 30% of the er-rors are due to parsing errors.
The Model errorsare due to two sources.
One is the depth problemmentioned above.
Local statistics for some verydeep treelets overwhelm the global statistics andlocal jumps win over the long jumps in these cases.Another problem is the data sparseness.
For ex-ample, the model has learned to reorder the ?PPVP?
structure but there is not much data for ?PPADVP VP?.
The model fails to jump over PP intoADVP.4.2    Feature UtilityWe conduct ablation studies to see the utilities ofeach feature.
We take the best feature set whichgives the performance in Table 3 and takes awayone feature type at a time.
The results are in Table5.
The first row keeps all the features.
The Sub-tract column shows performance after subtractingeach feature while keeping all the other features.The Add column shows performance of adding thefeature.
Using just first-child features gets75.97%.
Adding node-jump features moves theaccuracy to 78.40% and so on.Features #Features Sub-tractAdd-  80.56% -First Child  7,559 79.87% 75.97%Node Jump  6,334 79.52% 78.40%JumpOver Sib.
2,403 80.52% 79.00%BackJump  602 80.48% 79.05%Broken  15,183 80.30% 79.13%Barrier  158 80.26% 79.22%NumZ Islands 200 79.52% 80.56%Table 5.
Ablation study on features5    Translation Experiments5.1    Reorder Lattice GenerationThe reordering model is used to generate reorderlattices which are used by machine translation de-coders.
Reorder lattices have been frequently usedin decoding in works such as (Zhang et.
al 2007,Kumar et.al.
2005, Hildebrand et.al.
2008), toname just a few.
The main difference here is thatour lattices encode probabilities from the reorder-ing model and are not used to preorder the source.The lattice contains reorderings and their cost(negative log probability).
Figure 4 shows a reor-der lattice example.
Nodes are lattice states.
Arcsstore source word positions to be visited (trans-lated) and their cost and they are delimited bycomma in the figure.
Lower cost indicates betterchoice.
Figure 4 is much simplified for readability.It shows only the best path (highlighted) and a fewneighboring arcs.
For example, it shows sourcewords 1, 2, and 8 are the top 3 choices at step 1.Position 1 is the best choice with the lowest cost of0.302 and so on.Figure 4.
A lattice exampleThe sentence is shown at the bottom of the figure.The first part of the reference (true) path is indi-cated by the alignment which is source sequence 1,8, 9, and 2.
We see that this matches the lattice?stop-1 choice.Lattice generation takes source sentence andsource parse as input.
The lattice generation proc-ess makes use of a beam search algorithm.
Everynode in the lattice generates top-N next possiblepositions and the rest is pruned away.
A coveragevector is maintained on each path to ensure eachsource word is visited exactly once.
A widebeam width explores many source positions at anystep and results in a bushy lattice.
This is neededfor machine translation because the parses are er-rorful.
The structures that are hard for MT to reor-der are also hard for parsers to parse.
Labels criti-cal to reordering such as CP are among the leastaccurate labels.
Overall parsing accuracy is83.63% but CP accuracy is 73.11%.
We need awide beam to include more long jumps to compen-sate the parsing errors.5.2    Machine TranslationWe run MT experiments on NIST Chinese-Englishtest sets MT03-05.
We compare the performance855of using distance-based reordering and using maxi-mum entropy reordering lattices.
The decoder is alog-linear phrase based decoder.
Translation mod-els are trained from HMM alignments.
Asmoothed 5-gram English LM is built on the Eng-lish Gigaword corpus and English side of the Chi-nese-English parallel corpora.
In the experiments,lexicalized distance-based reordering allows up to9 words to be jumped over.
MT performance ismeasured by BLEUr4n4 (Papineni et.al.
2001).The test set statistics and experiment results areshow in Table 6.
Decoding with MaxEnt reorderlattices shows significant improvement for all con-ditions.Data #Segs LexSkip-9Reord   Lattice GainMT03 919 0.3005 0.3315 +3.1MT04 1788 0.3250 0.3388 +1.38MT05 1082 0.2957 0.3236 +2.79Table 6.
MT resultsFigures 5 shows an example from MT outputwith word alignments to the Chinese input.
TheMaxEnt reordering model correctly reorders twosource modifiers at source positions 8 and 22.
TheSkip9 output reorders locally whereas the MaxEntlattice output shows much more complex reorder-ings.6    ConclusionsWe present a direct syntax-based reordering modelthat captures source structural information.
Themodel is capable of handling reorderings of arbi-trary length.
Long-range reorderings are essentialin translation between languages with great wordorder differences such as Chinese-English andArabic-English.
We have shown that phrase basedSMT can benefit significantly from such a reorder-ing model.The current model is not regularized and featureselection by thresholding the feature counts is quiteprimitive.
Regularizing the model will preventoverfitting, especially given the small training dataset.
Regularization will also make the ablationstudy more meaningful.The reordering model presented here aims atcapturing structural differences between sourceand target languages.
It does not have enoughlexical features to deal with lexical idiosyncrasies.ME Lattice MT               Skip9 MTFigure 5.
MT comparisonOur initial attempt at adding lexical pair jump fea-tures ?
(fromWord, toWord, j) has not proved use-ful.
It hurt accuracy by 3% (from 80% to 77%).We see from Table 4 that 34% of the errors are dueto source lexical choices which indicates the weak-ness of the current lexical features.
Regularizationof the model might also make a difference with thelexical features.Reordering and word choice in translation are notindependent of each other.
We have shown someinitial success with a separate reordering model.
Inthe future, we will build joint models on reorderingand translation.
This approach will also addresssome of the reordering problems due to sourcelexical idiosyncrasies.7   AcknowledgementWe would like to acknowledge the support ofDARPA under Grant HR0011-08-C-0110 for fund-ing part of this work.
The views, opinions, and/orfindings contained in this article are those of theauthor and should not be interpreted as represent-ing the official views or policies, either expressedor implied, of the Defense Advanced ResearchProjects Agency or the Department of Defense.References856A.S.Hildebrand, K.Rottmann, Mohamed Noamany, QinGao, S. Hewavitharana, N. Bach and Stephan Voga.2008.
Recent Improvements in the CMU Large ScaleChinese-English SMT System.
In Proceedings ofACL 2008 (Short Papers)C. Wang, M. Collins, and Philipp Koehn.
2007.
Chi-nese Syntactic Reordering for Statistical MachineTranslation.
In Proceedings of EMNLP 2007Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li, and Yi Guan.
2007.
A ProbabilisticApproach to syntax-based Reordering for StatisticalMachine Translation.
In Proceedings of ACL 2007.Christoph Tillmannn.
2004.
A Block OrientationModel for Statistical Machine Translation.
In Pro-ceedings of HLT-NAACL 2004.David Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In Pro-ceedings of ACL 2005.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Compuntational Linguistics, Vol.
23, pp 377-404Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofACL 2006.Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liuand Shouxun Lin.
2008.
Refinements in FTG-basedStatistical Machine Translation.
In Proceedings ofICJNLP 2008Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou.2007.
Phrase Reordering Model Integrating Syntac-tic Knowledge for SMT.
In Proceedings of EMNLP2007Fei Xia and Michael McCord.
2004.
Improving a Sta-tistical MT System with Automatically Learned Re-write Patterns.
In Proceedings of COLING 2004.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical MachineTranslation.
Computational Linguistics, Vol.
30(4).pp.
417-449Kenji Yamada and Kevin Knight 2001.
A Syntax-basedStatistical Translation Model.
In Proceedings ofACL 2001Kevin Knight.
1999.
Decoding Complexity in WordReplacement Translation Models.
ComputationalLinguistics, 25(4):607-615Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2001.
A Method for Automatic Evaluationfor MT.
In Proceedings of ACL 2001Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of ACL 2005.Michell Galley, Christoph D. Manning.
2008.
A Simpleand Effective Hierarchical Phrase ReorderingModel.
Proceedings of the EMNLP 2008Perter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation.Computation Linguistics, 19(2).Philip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of NLT/NAACL 2003.Richard Zens, Hermann Ney, Taro Watanabe, and Eii-chiro Sumita.
2004.
Reordering Constraints forPhrase-based Statistical Machine Translation.
InProceedings of COLING 2004.Richard Zens and Hermann Ney.
2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion.
In Proceedings of the Workshop on StatisticalMachine Translation, 2006.Roger Levy and Christoph Manning.
2003.
Is it harderto parse Chinese, or the Chinese Treebank?
In Pro-ceedings of ACL 2003Shankar Kumar and William Byrne.
2005.
LocalPhrase Roerdering Models for Statistical MachineTranslation.
In Proceedings of  HLT/EMNLP 2005Stephan Kanthak, David Vilar, Evgeny Matusov, Rich-ard Zens, and Hermann Ney.
2005.
Novel Reorder-ing Approaches in Phrase-based Statistical MachineTranslation.
In Proceedings of the Workshop onBuilding and Using Parallel Texts 2005.Y.
Al-Onaizan .
K. 2006  Distortion Models for Statisti-cal Machine Translation.
In Proceedings of ACL2006.Yizhao Ni, C.J.Saunders, S. Szedmak and M.Niranjan2009 Handling phrase reorderings for machinetranslation.
In Proceedings of ACL2009Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Improved Chunk-level Reordering for Statistical Ma-chine Translation.
In Proceedings of HLT/NAACL2007.Yuval Marton and Philip Resnik.
2008.
Soft SyntacticConstraints for Hierarchical Phrased-based Transla-tion.
In Proceedings of ACL 2008.857
