Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,Social Sciences, Humanities, and Education ?LaTeCH ?
SHELT&R 2009, pages 26?34,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsEvaluating the pairwise string alignment of pronunciationsMartijn WielingUniversity of GroningenThe Netherlandsm.b.wieling@rug.nlJelena Prokic?University of GroningenThe Netherlandsj.prokic@rug.nlJohn NerbonneUniversity of GroningenThe Netherlandsj.nerbonne@rug.nlAbstractPairwise string alignment (PSA) is an im-portant general technique for obtaining ameasure of similarity between two strings,used e.g., in dialectology, historical lin-guistics, transliteration, and in evaluatingname distinctiveness.
The current studyfocuses on evaluating different PSA meth-ods at the alignment level instead of viathe distances it induces.
About 3.5 millionpairwise alignments of Bulgarian phoneticdialect data are used to compare four al-gorithms with a manually corrected goldstandard.
The algorithms evaluated in-clude three variants of the Levenshtein al-gorithm as well as the Pair Hidden MarkovModel.
Our results show that while allalgorithms perform very well and alignaround 95% of all alignments correctly,there are specific qualitative differences inthe (mis)alignments of the different algo-rithms.1 IntroductionOur cultural heritage is not only accessiblethrough museums, libraries, archives and theirdigital portals, it is alive and well in the variedcultural habits practiced today by the various peo-ples of the world.
To research and understand thiscultural heritage we require instruments which aresensitive to its signals, and, in particular sensitiveto signals of common provenance.
The presentpaper focuses on speech habits which even todaybear signals of common provenance in the vari-ous dialects of the world?s languages, and whichhave also been recorded and preserved in majorarchives of folk culture internationally.
We presentwork in a research line which seeks to developdigital instruments capable of detecting commonprovenance among pronunciation habits, focusingin this paper on the issue of evaluating the qualityof these instruments.Pairwise string alignment (PSA) methods, likethe popular Levenshtein algorithm (Levenshtein,1965) which uses insertions (alignments of a seg-ment against a gap), deletions (alignments of a gapagainst a segment) and substitutions (alignmentsof two segments) often form the basis of deter-mining the distance between two strings.
Sincethere are many alignment algorithms and specificsettings for each algorithm influencing the dis-tance between two strings (Nerbonne and Klei-weg, 2007), evaluation is very important in deter-mining the effectiveness of the distance methods.Determining the distance (or similarity) be-tween two phonetic strings is an important aspectof dialectometry, and alignment quality is impor-tant in applications in which string alignment isa goal in itself, for example, determining if twowords are likely to be cognate (Kondrak, 2003),detecting confusable drug names (Kondrak andDorr, 2003), or determining whether a string isthe transliteration of the same name from anotherwriting system (Pouliquen, 2008).In this paper we evaluate string distance mea-sures on the basis of data from dialectology.
Wetherefore explain a bit more of the intended use ofthe pronunciation distance measure.Dialect atlases normally contain a large num-ber of pronunciations of the same word in variousplaces throughout a language area.
All pairs ofpronunciations of corresponding words are com-pared in order to obtain a measure of the aggre-gate linguistic distance between dialectal varieties(Heeringa, 2004).
It is clear that the quality of themeasurement is of crucial importance.Almost all evaluation methods in dialectometryfocus on the aggregate results and ignore the in-dividual word-pair distances and individual align-ments on which the distances are based.
The fo-cus on the aggregate distance of 100 or so word26pairs effectively hides many differences betweenmethods.
For example, Heeringa et al (2006) findno significant differences in the degrees to whichseveral pairwise string distance measures correlatewith perceptual distances when examined at an ag-gregate level.
Wieling et al (2007) and Wielingand Nerbonne (2007) also report almost no differ-ence between different PSA algorithms at the ag-gregate level.
It is important to be able to evaluatethe different techniques more sensitively, which iswhy this paper examines alignment quality at thesegment level.Kondrak (2003) applies a PSA algorithm toalign words in different languages in order to de-tect cognates automatically.
Exceptionally, hedoes provide an evaluation of the string alignmentsgenerated by different algorithms.
But he restrictshis examination to a set of only 82 gold standardpairwise alignments and he only distinguishes cor-rect and incorrect alignments and does not look atmisaligned phones.In the current study we introduce and evaluateseveral alignment algorithms more extensively atthe alignment level.
The algorithms we evaluateinclude the Levenshtein algorithm (with syllabic-ity constraint), which is one of the most popularalignment methods and has successfully been usedin determining pronunciation differences in pho-netic strings (Kessler, 1995; Heeringa, 2004).
Inaddition we look at two adaptations of the Lev-enshtein algorithm.
The first adaptation includesthe swap-operation (Wagner and Lowrance, 1975),while the second adaptation includes phonetic seg-ment distances, which are generated by applyingan iterative pointwise mutual information (PMI)procedure (Church and Hanks, 1990).
Finally weinclude alignments generated with the Pair Hid-den Markov Model (PHMM) as introduced to lan-guage studies by Mackay and Kondrak (2005).They reported that the Pair Hidden Markov Modeloutperformed ALINE, the best performing algo-rithm at the alignment level in the aforementionedstudy of Kondrak (2003).
The PHMM has alsosuccessfully been used in dialectology by Wielinget al (2007).2 DatasetThe dataset used in this study consists of 152words collected from 197 sites equally distributedover Bulgaria.
The transcribed word pronuncia-tions include diacritics and suprasegmentals (e.g.,intonation).
The total number of different phonetictypes (or segments) is 98.1The gold standard pairwise alignment was au-tomatically generated from a manually correctedgold standard set of N multiple alignments (seeProkic?
et al, 2009 ) in the following way:?
Every individual string (including gaps) inthe multiple alignment is aligned with ev-ery other string of the same word.
With 152words and 197 sites and in some cases morethan one pronunciations per site for a cer-tain word, the total number of pairwise align-ments is about 3.5 million.?
If a resulting pairwise alignment contains agap in both strings at the same position (agap-gap alignment), these gaps are removedfrom the pairwise alignment.
We justify this,reasoning that no alignment algorithm maybe expected to detect parallel deletions in asingle pair of words.
There is no evidence forthis in the single pair.To make this clear, consider the multiple align-ment of three Bulgarian dialectal variants of theword ?I?
(as in ?I am?
):j "A s"A z ij "AUsing the procedure above, the three generatedpairwise alignments are:j "A s j "A s "A z i"A z i j "A j "A3 AlgorithmsFour algorithms are evaluated with respect to thequality of their alignments, including three vari-ants of the Levenshtein algorithm and the PairHidden Markov Model.3.1 The VC-sensitive Levenshtein algorithmThe Levenshtein algorithm is a very efficient dy-namic programming algorithm, which was first in-troduced by Kessler (1995) as a tool for computa-tionally comparing dialects.
The Levenshtein dis-tance between two strings is determined by count-ing the minimum number of edit operations (i.e.insertions, deletions and substitutions) needed totransform one string into the other.1The dataset is available online at the websitehttp://www.bultreebank.org/BulDialects/27For example, the Levenshtein distance between[j"As] and ["Azi], two Bulgarian dialectal variantsof the word ?I?
(as in ?I am?
), is 3:j"As delete j 1"As subst.
s/z 1"Az insert i 1"Azi3The corresponding alignment is:j "A s"A z i1 1 1The Levenshtein distance has been used fre-quently and successfully in measuring linguis-tic distances in several languages, including Irish(Kessler, 1995), Dutch (Heeringa, 2004) and Nor-wegian (Heeringa, 2004).
Additionally, the Lev-enshtein distance has been shown to yield aggre-gate results that are consistent (Cronbach?s ?
=0.99) and valid when compared to dialect speak-ers judgements of similarity (r ?
0.7; Heeringa etal., 2006).Following Heeringa (2004), we have adaptedthe Levenshtein algorithm slightly, so that it doesnot allow alignments of vowels with consonants.We refer to this adapted algorithm as the VC-sensitive Levenshtein algorithm.3.2 The Levenshtein algorithm with the swapoperationBecause metathesis (i.e.
transposition of sounds)occurs relatively frequently in the Bulgarian di-alect data (in 21 of 152 words), we extend theVC-sensitive Levenshtein algorithm as describedin section 3.1 to include the swap-operation (Wag-ner and Lowrance, 1975), which allows two ad-jacent characters to be interchanged.
The swap-operation is also known as a transposition, whichwas introduced with respect to detecting spellingerrors by Damerau (1964).
As a consequence theDamerau distance refers to the minimum numberof insertions, deletions, substitutions and transpo-sitions required to transform one string into theother.
In contrast to Wagner and Lowrance (1975)and in line with Damerau (1964) we restrict theswap operation to be only allowed for string Xand Y when xi = yi+1 and yi = xi+1 (with xibeing the token at position i in string X):xi xi+1yi yi+1>< 1Note that a swap-operation in the alignment is in-dicated by the symbol ?><?.
The first number fol-lowing this symbol indicates the cost of the swap-operation.Consider the alignment of [vr"7] and [v"7r],2two Bulgarian dialectal variants of the word ?peak?(mountain).
The alignment involves a swap andresults in a total Levenshtein distance of 1:v r "7v "7 r>< 1However, the alignment of the transcription [vr"7]with another dialectal transcription [v"ar] does notallow a swap and yields a total Levenshtein dis-tance of 2:v r "7v "a r1 1Including just the option of swapping identicalsegments in the implementation of the Leven-shtein algorithm is relatively easy.
We set thecost of the swap operation to one3 plus twice thecost of substituting xi with yi+1 plus twice thecost of substituting yi with xi+1.
In this way theswap operation will be preferred when xi = yi+1and yi = xi+1, but not when xi 6= yi+1 and/oryi 6= xi+1.
In the first case the cost of the swapoperation is 1, which is less than the cost of thealternative of two substitutions.
In the second casethe cost is either 3 (if xi 6= yi+1 or yi 6= xi+1) or5 (if xi 6= yi+1 and yi 6= xi+1), which is higherthan the cost of using insertions, deletions and/orsubstitutions.Just as in the previous section, we do not allowvowels to align with consonants (except in the caseof a swap).3.3 The Levenshtein algorithm withgenerated segment distancesThe VC-sensitive Levenshtein algorithm as de-scribed in section 3.1 only distinguishes betweenvowels and consonants.
However, more sensi-tive segment distances are also possible.
Heeringa(2004) experimented with specifying phoneticsegment distances based on phonetic features and2We use transcriptions in which stress is marked onstressed vowels instead of before stressed syllables.
We fol-low in this the Bulgarian convention instead of the IPA con-vention.3Actually the cost is set to 0.999 to prefer an alignmentinvolving a swap over an alternative alignment involving onlyregular edit operations.28also based on acoustic differences derived fromspectrograms, but he did not obtain improved re-sults at the aggregate level.Instead of using segment distances as these are(incompletely) suggested by phonetic or phono-logical theory, we tried to determine the sounddistances automatically based on the availabledata.
We used pointwise mutual information(PMI; Church and Hanks, 1990) to obtain thesedistances.
It generates segment distances by as-sessing the degree of statistical dependence be-tween the segments x and y:PMI(x, y) = log2(p(x, y)p(x) p(y))(1)Where:?
p(x, y): the number of times x and y occurat the same position in two aligned stringsX and Y , divided by the total number ofaligned segments (i.e.
the relative occurrenceof the aligned segments x and y in the wholedataset).
Note that either x or y can be a gapin the case of insertion or deletion.?
p(x) and p(y): the number of times x (or y)occurs, divided by the total number of seg-ment occurrences (i.e.
the relative occurrenceof x or y in the whole dataset).
Dividing bythis term normalizes the empirical frequencywith respect to the frequency expected if xand y are statistically independent.The greater the PMI value, the more segments tendto cooccur in correspondences.
Negative PMI val-ues indicate that segments do not tend to cooccurin correspondences, while positive PMI values in-dicate that segments tend to cooccur in correspon-dences.
The segment distances can therefore begenerated by subtracting the PMI value from 0 andadding the maximum PMI value (i.e.
lowest dis-tance is 0).
In that way corresponding segmentsobtain the lowest distance.Based on the PMI value and its conversion tosegment distances, we developed an iterative pro-cedure to automatically obtain the segment dis-tances:1.
The string alignments are generated using theVC-sensitive Levenshtein algorithm (see sec-tion 3.1).44We also used the Levenshtein algorithm without thevowel-consonant restriction to generate the PMI values, butthis had a negative effect on the performance.2.
The PMI value for every segment pair is cal-culated according to (1) and subsequentlytransformed to a segment distance by sub-tracting it from zero and adding the maxi-mum PMI value.3.
The Levenshtein algorithm using these seg-ment distances is applied to generate a newset of alignments.4.
Step 2 and 3 are repeated until the alignmentsof two consecutive iterations do not differ(i.e.
convergence is reached).The potential merit of using PMI-generated seg-ment distances can be made clear by the followingexample.
Consider the strings [v"7n] and [v"7?k@],Bulgarian dialectal variants of the word ?outside?.The VC-sensitive Levenshtein algorithm yieldsthe following (correct) alignment:v "7 nv "7 ?
k @1 1 1But also the alternative (incorrect) alignment:v "7 nv "7 ?
k @1 1 1The VC-sensitive Levenshtein algorithm gener-ates the erroneous alignment because it has no wayto identify that the consonant [n] is nearer to theconsonant [?]
than to the consonant [k].
In con-trast, the Levenshtein algorithm which uses thePMI-generated segment distances only generatesthe correct first alignment, because the [n] occursrelatively more often aligned with [?]
than with[k] so that the distance between [n] and [?]
willbe lower than the distance between [n] and [k].The idea behind this procedure is similar to Ris-tad?s suggestion to learn segment distances for editdistance using an expectation maximization algo-rithm (Ristad and Yianilos, 1998).
Our approachdiffers from their approach in that we only learnsegment distances based on the alignments gener-ated by the VC-sensitive Levenshtein algorithm,while Ristad and Yianilos (1998) learn segmentdistances by considering all possible alignments oftwo strings.3.4 The Pair Hidden Markov ModelThe Pair Hidden Markov Model (PHMM) alsogenerates alignments based on automatically gen-erated segment distances and has been used suc-29Figure 1: Pair Hidden Markov Model.
Imagecourtesy of Mackay and Kondrak (2005).cessfully in language studies (Mackay and Kon-drak, 2005; Wieling et al, 2007).A Hidden Markov Model (HMM) is a proba-bilistic finite-state transducer that generates an ob-servation sequence by starting in an initial state,going from state to state based on transition prob-abilities and emitting an output symbol in eachstate based on the emission probabilities in thatstate for that output symbol (Rabiner, 1989).
ThePHMM was originally proposed by Durbin et al(1998) for aligning biological sequences and wasfirst used in linguistics by Mackay and Kondrak(2005) to identify cognates.
The PHMM differsfrom the regular HMM in that it outputs two ob-servation streams (i.e.
a series of alignments ofpairs of individual segments) instead of only a se-ries of single symbols.
The PHMM displayed inFigure 1 has three emitting states: the substitution(?match?)
state (M) which emits two aligned sym-bols, the insertion state (Y) which emits a symboland a gap, and the deletion state (X) which emitsa gap and a symbol.The following example shows the state se-quence for the pronunciations [j"As] and ["Azi] (En-glish ?I?
):j "A s"A z iX M M YBefore generating the alignments, all probabil-ities of the PHMM have to be estimated.
Theseprobabilities consist of the 5 transition probabili-ties shown in Figure 1: , ?, ?, ?XY and ?M .
Inaddition there are 98 emission probabilities for theinsertion state and the deletion state (one for ev-ery segment) and 9604 emission probabilities forthe substitution state.
The probability of starting inone of the three states is set equal to the probabilityof going from the substitution state to that particu-lar state.
The Baum-Welch expectation maximiza-tion algorithm (Baum et al, 1970) can be used toiteratively reestimate these probabilities until a lo-cal optimum is found.To prevent order effects in training, every wordpair is considered twice (e.g., wa ?
wb and wb ?wa).
The resulting insertion and deletion probabil-ities are therefore the same (for each segment), andthe probability of substituting x for y is equal tothe probability of substituting y for x, effectivelyyielding 4802 distinct substitution probabilities.Wieling et al (2007) showed that using Dutchdialect data for training, sensible segment dis-tances were obtained; acoustic vowel distanceson the basis of spectrograms correlated signifi-cantly (r = ?0.72) with the vowel substitutionprobabilities of the PHMM.
Additionally, proba-bilities of substituting a symbol with itself weremuch higher than the probabilities of substitut-ing an arbitrary vowel with another non-identicalvowel (mutatis mutandis for consonants), whichwere in turn much higher than the probabilities ofsubstituting a vowel for a consonant.After training, the well known Viterbi algorithmcan be used to obtain the best alignments (Rabiner,1989).4 EvaluationAs described in section 2, we use the generatedpairwise alignments from a gold standard of multi-ple alignments for evaluation.
In addition, we lookat the performance of a baseline of pairwise align-ments, which is constructed by aligning the stringsaccording to the Hamming distance (i.e.
only al-lowing substitutions and no insertions or deletions;Hamming, 1950).The evaluation procedure consists of comparingthe alignments of the previously discussed algo-rithms including the baseline with the alignmentsof the gold standard.
For the comparison we usethe standard Levenshtein algorithm without anyrestrictions.
The evaluation proceeds as follows:1.
The pairwise alignments of the four algo-rithms, the baseline and the gold standard aregenerated and standardized (see section 4.1).When multiple equal-scoring alignments are30generated by an algorithm, only one (i.e.
thefinal) alignment is selected.2.
In each alignment, we convert each pair ofaligned segments to a single token, so that ev-ery alignment of two strings is converted to asingle string of segment pairs.3.
For every algorithm these transformed stringsare aligned with the transformed strings ofthe gold standard using the standard Leven-shtein algorithm.4.
The Levenshtein distances for all thesestrings are summed up resulting in the totaldistance between every alignment algorithmand the gold standard.
Only if individualsegments match completely the segment dis-tance is 0, otherwise it is 1.To illustrate this procedure, consider the followinggold standard alignment of [vl"7k] and [v"7lk], twoBulgarian dialectal variants of the word ?wolf?
:v l "7 kv "7 l kEvery aligned segment pair is converted to a singletoken by adding the symbol ?/?
between the seg-ments and using the symbol ?-?
to indicate a gap.This yields the following transformed string:v/v l/"7 "7/l k/kSuppose another algorithm generates the follow-ing alignment (not detecting the swap):v l "7 kv "7 l kThe transformed string for this alignment is:v/v l/- "7/"7 -/l k/kTo evaluate this alignment, we align this string tothe transformed string of the gold standard and ob-tain a Levenshtein distance of 3:v/v l/"7 "7/l k/kv/v l/- "7/"7 -/l k/k1 1 1By repeating this procedure for all alignments andsumming up all distances, we obtain total dis-tances between the gold standard and every align-ment algorithm.
Algorithms which generate high-quality alignments will have a low distance fromthe gold standard, while the distance will be higherfor algorithms which generate low-quality align-ments.4.1 StandardizationThe gold standard contains a number of align-ments which have alternative equivalent align-ments, most notably an alignment containing aninsertion followed by a deletion (which is equalto the deletion followed by the insertion), or analignment containing a syllabic consonant such as["?
"], which in fact matches both a vowel and aneighboring r-like consonant and can therefore bealigned with either the vowel or the consonant.
Inorder to prevent punishing the algorithms whichdo not match the exact gold standard in thesecases, the alignments of the gold standard and allalignment algorithms are transformed to one stan-dard form in all relevant cases.For example, consider the correct alignment of[v"iA] and [v"ij], two Bulgarian dialectal variationsof the English plural pronoun ?you?
:v "i Av "i jOf course, this alignment is as reasonable as:v "i Av "i jTo avoid punishing the first, we transform all in-sertions followed by deletions to deletions fol-lowed by insertions, effectively scoring the twoalignments the same.For the syllabic consonants we transform allalignments to a form in which the syllabic con-sonant is followed by a gap and not vice versa.For instance, aligning [v"?
"x] with [v"Arx] (English:?peak?)
yields:v "?
"xv "A r xWhich is transformed to the equivalent alignment:v "?
"xv "A r x5 ResultsWe will report both quantitative results using theevaluation method discussed in the previous sec-tion, as well as the qualitative results, where wefocus on characteristic errors of the different align-ment algorithms.5.1 Quantitative resultsBecause there are two algorithms which use gen-erated segment distances (or probabilities) in theiralignments, we first check if these values are sen-sible and comparable to each other.315.1.1 Comparison of segment distancesWith respect to the PMI results (convergencewas reached after 7 iterations, taking less than5 CPU minutes), we indeed found sensible re-sults: the average distance between identical sym-bols was significantly lower than the distance be-tween pairs of different vowels and consonants(t < ?13, p < .001).
Because we did not allowvowel-consonants alignments in the Levenshteinalgorithm, no PMI values were generated for thosesegment pairs.Just as Wieling et al (2007), we found sen-sible PHMM substitution probabilities (conver-gence was reached after 1675 iterations, takingabout 7 CPU hours): the probability of matchinga symbol with itself was significantly higher thanthe probability of substituting one vowel for an-other (similarly for consonants), which in turn washigher than the probability of substituting a vowelwith a consonant (all t?s > 9, p < .001).To allow a fair comparison between the PHMMprobabilities and the PMI distances, we trans-formed the PHMM probabilities to log-oddsscores (i.e.
dividing the probability by the rela-tive frequency of the segments and subsequentlytaking the log).
Because the residues after thelinear regression between the PHMM similaritiesand PMI distances were not normally distributed,we used Spearman?s rank correlation coefficientto assess the relationship between the two vari-ables.
We found a highly significant Spearman?s?
= ?.965 (p < .001), which means that the re-lationship between the PHMM similarities and thePMI distances is very strong.
When looking at theinsertions and deletions we also found a significantrelationship: Spearman?s ?
= ?.736 (p < .001).5.1.2 Evaluation against the gold standardUsing the procedure described in section 4, we cal-culated the distances between the gold standardand the alignment algorithms.
Besides reportingthe total number of misaligned tokens, we also di-vided this number by the total number of alignedsegments in the gold standard (about 16 million)to get an idea of the error rate.
Note that the errorrate is 0 in the perfect case, but might rise to nearly2 in the worst case, which is an alignment consist-ing of only insertions and deletions and thereforeup to twice as long as the alignments in the goldstandard.
Finally, we also report the total numberof alignments (word pairs) which are not exactlyequal to the alignments of the gold standard.The results are shown in Table 1.
We canclearly see that all algorithms beat the baselineand align about 95% of all string pairs correctly.While the Levenshtein PMI algorithm aligns moststrings perfectly, it misaligns slightly more indi-vidual segments than the PHMM and the Leven-shtein algorithm with the swap operation (i.e.
itmakes more segment alignment errors per wordpair).
The VC-sensitive Levenshtein algorithmin general performs slightly worse than the otherthree algorithms.5.2 Qualitative resultsLet us first note that it is almost impossible forany algorithm to achieve a perfect overlap with thegold standard, because the gold standard was gen-erated from multiple alignments and therefore in-corporates other constraints.
For example, while acertain pairwise alignment could appear correct inaligning two consonants, the multiple alignmentcould show contextual support (from pronuncia-tions in other varieties) for separating the conso-nants.
Consequently, all algorithms discussed be-low make errors of this kind.In general, the specific errors of the VC-sensitive Levenshtein algorithm can be separatedinto three cases.
First, as we illustrated in section3.3, the VC-sensitive Levenshtein algorithm hasno way to distinguish between aligning a conso-nant with one of two neighboring consonants andsometimes chooses the wrong one (this also holdsfor vowels).
Second, it does not allow alignmentsof vowels with consonants and therefore cannotdetect correct vowel-consonant alignments such ascorrespondences of [u] with [v] initially.
Third,for the same reason the VC-sensitive Levenshteinalgorithm is also not able to detect metathesis ofvowels with consonants.The misalignments of the Levenshtein algo-rithm with the swap-operation can also be split inthree cases.
It suffers from the same two prob-lems as the VC-sensitive Levenshtein algorithm inchoosing to align a consonant incorrectly with oneof two neighboring consonants and not being ableto align a vowel with a consonant.
Third, eventhough it aligns some of the metathesis cases cor-rectly, it also makes some errors by incorrectly ap-plying the swap-operation.
For example, considerthe alignment of [s"irjIni] and [s"irjnI], two Bul-garian dialectal variations of the word ?cheese?, inwhich the swap-operation is applied:32Algorithm Misaligned segments (error rate) Incorrect alignments (%)Baseline (Hamming algorithm) 2510094 (0.1579) 726844 (20.92%)VC-sens.
Levenshtein algorithm 490703 (0.0309) 191674 (5.52%)Levenshtein PMI algorithm 399216 (0.0251) 156440 (4.50%)Levenshtein swap algorithm 392345 (0.0247) 161834 (4.66%)Pair Hidden Markov Model 362423 (0.0228) 160896 (4.63%)Table 1: Comparison to gold standard alignments.
All differences are significant (p < 0.01).s "i rj I n is "i rj n I0 0 0 >< 1 1However, the two I?s are not related and should notbe swapped, which is reflected in the gold standardalignment:s "i rj I n is "i rj n I0 0 0 1 0 1The incorrect alignments of the Levenshteinalgorithm with the PMI-generated segment dis-tances are mainly caused by its inability to alignvowels with consonants and therefore, just as theVC-sensitive Levenshtein algorithm, it fails to de-tect metathesis.
On the other hand, using seg-ment distances often solves the problem of select-ing which of two plausible neighbors a consonantshould be aligned with.Because the PHMM employs segment substi-tution probabilities, it also often solves the prob-lem of aligning a consonant to one of two neigh-bors.
In addition, the PHMM often correctlyaligns metathesis involving equal as well as sim-ilar symbols, even realizing an improvement overthe Levenshtein swap algorithm.
Unfortunately,many wrong alignments of the PHMM are alsocaused by allowing vowel-consonant alignments.Since the PHMM does not take context into ac-count, it also aligns vowels and consonants whichoften play a role in metathesis when no metathesisis involved.6 DiscussionThis study provides an alternative evaluation ofstring distance algorithms by focusing on their ef-fectiveness in aligning segments.
We proposed,implemented, and tested the new procedure on asubstantial body of data.
This provides a new per-spective on the quality of distance and alignmentalgorithms as they have been used in dialectology,where aggregate comparisons had been at timesfrustratingly inconclusive.In addition, we introduced the PMI weight-ing within the Levenshtein algorithm as a sim-ple means of obtaining segment distances, andshowed that it improves on the popular Leven-shtein algorithm with respect to alignment accu-racy.While the results indicated that the PHMM mis-aligned the fewest segments, training the PHMMis a lengthy process lasting several hours.
Con-sidering that the Levenshtein algorithm with theswap operation and the Levenshtein algorithmwith the PMI-generated segment distances aremuch quicker to (train and) apply, and that theyhave only slightly lower performance with respectto the segment alignments, we actually prefer us-ing those methods.
Another argument in favor ofusing one of these Levenshtein algorithms is thatit is a priori clearer what type of alignment errorsto expect from them, while the PHMM algorithmis less predictable and harder to comprehend.While our results are an indication of the goodquality of the evaluated algorithms, we only evalu-ated the algorithms on a single dataset for which agold standard was available.
Ideally we would liketo verify these results on other datasets, for whichgold standards consisting of multiple or pairwisealignments are available.AcknowledgementsWe are grateful to Peter Kleiweg for extending theLevenshtein algorithm in the L04 package with theswap-operation.
We also thank Greg Kondrak forproviding the original source code of the Pair Hid-den Markov Models.
Finally, we thank ThereseLeinonen and Sebastian Ku?rschner of the Univer-sity of Groningen and Esteve Valls i Alecha of theUniversity of Barcelona for their useful feedbackon our ideas.33ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique occur-ring in the statistical analysis of probabilistic func-tions of Markov Chains.
The Annals of Mathemati-cal Statistics, 41(1):164?171.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.Fred J. Damerau.
1964.
A technique for computer de-tection and correction of spelling errors.
Communi-cations of the ACM, 7:171?176.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis: Probabilistic Models of Proteins and Nu-cleic Acids.
Cambridge University Press, UnitedKingdom, July.Richard Hamming.
1950.
Error detecting and errorcorrecting codes.
Bell System Technical Journal,29:147?160.Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,and John Nerbonne.
2006.
Evaluation of string dis-tance algorithms for dialectology.
In John Nerbonneand Erhard Hinrichs, editors, Linguistic Distances,pages 51?62, Shroudsburg, PA. ACL.Wilbert Heeringa.
2004.
Measuring Dialect Pronunci-ation Differences using Levenshtein Distance.
Ph.D.thesis, Rijksuniversiteit Groningen.Brett Kessler.
1995.
Computational dialectology inIrish Gaelic.
In Proceedings of the seventh con-ference on European chapter of the Association forComputational Linguistics, pages 60?66, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Grzegorz Kondrak and Bonnie Dorr.
2003.
Identifica-tion of Confusable Drug Names: A New Approachand Evaluation Methodology.
Artificial Intelligencein Medicine, 36:273?291.Grzegorz Kondrak.
2003.
Phonetic Alignment andSimilarity.
Computers and the Humanities, 37:273?291.Vladimir Levenshtein.
1965.
Binary codes capable ofcorrecting deletions, insertions and reversals.
Dok-lady Akademii Nauk SSSR, 163:845?848.Wesley Mackay and Grzegorz Kondrak.
2005.
Com-puting word similarity and identifying cognates withPair Hidden Markov Models.
In Proceedings ofthe 9th Conference on Computational Natural Lan-guage Learning (CoNLL), pages 40?47, Morris-town, NJ, USA.
Association for Computational Lin-guistics.John Nerbonne and Peter Kleiweg.
2007.
Toward a di-alectological yardstick.
Journal of Quantitative Lin-guistics, 14:148?167.Bruno Pouliquen.
2008.
Similarity of names acrossscripts: Edit distance using learned costs of N-Grams.
In Bent Nordstro?m and Aarne Ranta, ed-itors, Proceedings of the 6th international Con-ference on Natural Language Processing (Go-Tal?2008), volume 5221, pages 405?416.Jelena Prokic?, Martijn Wieling, and John Nerbonne.2009.
Multiple sequence alignments in linguistics.In Piroska Lendvai and Lars Borin, editors, Proceed-ings of the EACL 2009 Workshop on Language Tech-nology and Resources for Cultural Heritage, SocialSciences, Humanities, and Education.Lawrence R. Rabiner.
1989.
A tutorial on HiddenMarkov Models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 20:522?532.Robert Wagner and Roy Lowrance.
1975.
An exten-sion of the string-to-string correction problem.
Jour-nal of the ACM, 22(2):177?183.Martijn Wieling and John Nerbonne.
2007.
Dialectpronunciation comparison and spoken word recog-nition.
In Petya Osenova, editor, Proceedings ofthe RANLPWorkshop on Computational Phonology,pages 71?78.Martijn Wieling, Therese Leinonen, and John Ner-bonne.
2007.
Inducing sound segment differencesusing Pair Hidden Markov Models.
In Mark EllisonJohn Nerbonne and Greg Kondrak, editors, Comput-ing and Historical Phonology: 9th Meeting of theACL Special Interest Group for Computational Mor-phology and Phonology, pages 48?56.34
