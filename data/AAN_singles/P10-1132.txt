Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImproved Unsupervised POS Induction through Prototype DiscoveryOmri Abend1?
Roi Reichart2 Ari Rappoport11Institute of Computer Science, 2ICNCHebrew University of Jerusalem{omria01|roiri|arir}@cs.huji.ac.ilAbstractWe present a novel fully unsupervised al-gorithm for POS induction from plain text,motivated by the cognitive notion of proto-types.
The algorithm first identifies land-mark clusters of words, serving as thecores of the induced POS categories.
Therest of the words are subsequently mappedto these clusters.
We utilize morpho-logical and distributional representationscomputed in a fully unsupervised manner.We evaluate our algorithm on English andGerman, achieving the best reported re-sults for this task.1 IntroductionPart-of-speech (POS) tagging is a fundamentalNLP task, used by a wide variety of applications.However, there is no single standard POS tag-ging scheme, even for English.
Schemes varysignificantly across corpora and even more soacross languages, creating difficulties in usingPOS tags across domains and for multi-lingualsystems (Jiang et al, 2009).
Automatic inductionof POS tags from plain text can greatly alleviatethis problem, as well as eliminate the efforts in-curred by manual annotations.
It is also a problemof great theoretical interest.
Consequently, POSinduction is a vibrant research area (see Section 2).In this paper we present an algorithm basedon the theory of prototypes (Taylor, 2003), whichposits that some members in cognitive categoriesare more central than others.
These practically de-fine the category, while the membership of otherelements is based on their association with the?
Omri Abend is grateful to the Azrieli Foundation forthe award of an Azrieli Fellowship.central members.
Our algorithm first clusterswords based on a fine morphological representa-tion.
It then clusters the most frequent words,defining landmark clusters which constitute thecores of the categories.
Finally, it maps the restof the words to these categories.
The last twostages utilize a distributional representation thathas been shown to be effective for unsupervisedparsing (Seginer, 2007).We evaluated the algorithm in both English andGerman, using four different mapping-based andinformation theoretic clustering evaluation mea-sures.
The results obtained are generally betterthan all existing POS induction algorithms.Section 2 reviews related work.
Sections 3 and4 detail the algorithm.
Sections 5, 6 and 7 describethe evaluation, experimental setup and results.2 Related WorkUnsupervised and semi-supervised POS tagginghave been tackled using a variety of methods.Schu?tze (1995) applied latent semantic analysis.The best reported results (when taking into ac-count all evaluation measures, see Section 5) aregiven by (Clark, 2003), which combines dis-tributional and morphological information withthe likelihood function of the Brown algorithm(Brown et al, 1992).
Clark?s tagger is very sen-sitive to its initialization.
Reichart et al (2010b)propose a method to identify the high quality runsof this algorithm.
In this paper, we show thatour algorithm outperforms not only Clark?s meanperformance, but often its best among 100 runs.Most research views the task as a sequential la-beling problem, using HMMs (Merialdo, 1994;Banko and Moore, 2004; Wang and Schuurmans,2005) and discriminative models (Smith and Eis-ner, 2005; Haghighi and Klein, 2006).
Several1298techniques were proposed to improve the HMMmodel.
A Bayesian approach was employed by(Goldwater and Griffiths, 2007; Johnson, 2007;Gao and Johnson, 2008).
Van Gael et al (2009)used the infinite HMM with non-parametric pri-ors.
Grac?a et al (2009) biased the model to inducea small number of possible tags for each word.The idea of utilizing seeds and expanding themto less reliable data has been used in several pa-pers.
Haghighi and Klein (2006) use POS ?pro-totypes?
that are manually provided and tailoredto a particular POS tag set of a corpus.
Fre-itag (2004) and Biemann (2006) induce an ini-tial clustering and use it to train an HMM model.Dasgupta and Ng (2007) generate morphologicalclusters and use them to bootstrap a distributionalmodel.
Goldberg et al (2008) use linguistic con-siderations for choosing a good starting point forthe EM algorithm.
Zhao and Marcus (2009) ex-pand a partial dictionary and use it to learn dis-ambiguation rules.
Their evaluation is only at thetype level and only for half of the words.
Raviand Knight (2009) use a dictionary and an MDL-inspired modification to the EM algorithm.Many of these works use a dictionary provid-ing allowable tags for each or some of the words.While this scenario might reduce human annota-tion efforts, it does not induce a tagging schemebut remains tied to an existing one.
It is furthercriticized in (Goldwater and Griffiths, 2007).Morphological representation.
Many POS in-duction models utilize morphology to some ex-tent.
Some use simplistic representations of termi-nal letter sequences (e.g., (Smith and Eisner, 2005;Haghighi and Klein, 2006)).
Clark (2003) modelsthe entire letter sequence as an HMM and uses itto define a morphological prior.
Dasgupta and Ng(2007) use the output of the Morfessor segmenta-tion algorithm for their morphological representa-tion.
Morfessor (Creutz and Lagus, 2005), whichwe use here as well, is an unsupervised algorithmthat segments words and classifies each segmentas being a stem or an affix.
It has been tested onseveral languages with strong results.Our work has several unique aspects.
First,our clustering method discovers prototypes in afully unsupervised manner, mapping the rest ofthe words according to their association with theprototypes.
Second, we use a distributional repre-sentation which has been shown to be effective forunsupervised parsing (Seginer, 2007).
Third, weuse a morphological representation based on sig-natures, which are sets of affixes that represent afamily of words sharing an inflectional or deriva-tional morphology (Goldsmith, 2001).3 Distributional AlgorithmOur algorithm is given a plain text corpus and op-tionally a desired number of clusters k. Its outputis a partitioning of words into clusters.
The al-gorithm utilizes two representations, distributionaland morphological.
Although eventually the latteris used before the former, for clarity of presenta-tion we begin by detailing the base distributionalalgorithm.
In the next section we describe the mor-phological representation and its integration intothe base algorithm.Overview.
The algorithm consists of two mainstages: landmark clusters discovery, and wordmapping.
For the former, we first compute a dis-tributional representation for each word.
We thencluster the coordinates corresponding to high fre-quency words.
Finally, we define landmark clus-ters.
In the word mapping stage we map each wordto the most similar landmark cluster.The rationale behind using only the high fre-quency words in the first stage is twofold.
First,prototypical members of a category are frequent(Taylor, 2003), and therefore we can expect thesalient POS tags to be represented in this smallsubset.
Second, higher frequency implies more re-liable statistics.
Since this stage determines thecores of all resulting clusters, it should be as accu-rate as possible.Distributional representation.
We use a sim-plified form of the elegant representation of lexi-cal entries used by the Seginer unsupervised parser(Seginer, 2007).
Since a POS tag reflects thegrammatical role of the word and since this rep-resentation is effective to parsing, we were moti-vated to apply it to the present task.Let W be the set of word types in the corpus.The right context entry of a word x ?
W is a pairof mappings r intx : W ?
[0, 1] and r adjx :W ?
[0, 1].
For each w ?
W , r adjx(w) is anadjacency score of w to x, reflecting w?s tendencyto appear on the right hand side of x.For each w ?
W , r intx(w) is an interchange-ability score of x with w, reflecting the tendencyof w to appear to the left of words that tend to ap-pear to the right of x.
This can be viewed as a1299similarity measure between words with respect totheir right context.
The higher the scores the morethe words tend to be adjacent/interchangeable.Left context parameters l intx and l adjx aredefined analogously.There are important subtleties in these defini-tions.
First, for two words x,w ?
W , r adjx(w)is generally different from l adjw(x).
For exam-ple, if w is a high frequency word and x is a lowfrequency word, it is likely that w appears manytimes to the right of x, yielding a high r adjx(w),but that x appears only a few times to the left of wyielding a low l adjw(x).
Second, from the defi-nition of r intx(w) and r intw(x), it is clear thatthey need not be equal.These functions are computed incrementally bya bootstrapping process.
We initialize all map-pings to be identically 0.
We iterate over the wordsin the training corpus.
For every word instance x,we take the word immediately to its right y andupdate x?s right context using y?s left context:?w ?
W : r intx(w) +=l adjy(w)N(y)?w ?
W : r adjx(w) +={1 w = yl inty(w)N(y) w 6= yThe division by N(y) (the number of times yappears in the corpus before the update) is done inorder not to give a disproportional weight to highfrequency words.
Also, r intx(w) and r adjx(w)might become larger than 1.
We therefore nor-malize them after all updates are performed by thenumber of occurrences of x in the corpus.We update l intx and l adjx analogously usingthe word z immediately to the left of x.
The up-dates of the left and right functions are done inparallel.We define the distributional representation of aword type x to be a 4|W | + 2 dimensional vectorvx.
Each word w yields four coordinates, one foreach direction (left/right) and one for each map-ping type (int/adj).
Two additional coordinatesrepresent the frequency in which the word appearsto the left and to the right of a stopping punc-tuation.
Of the 4|W | coordinates correspondingto words, we allow only 2n to be non-zero: then top scoring among the right side coordinates(those of r intx and r adjx), and the n top scoringamong the left side coordinates (those of l intxand l adjx).
We used n = 50.The distance between two words is defined tobe one minus the cosine of the angle between theirrepresentation vectors.Coordinate clustering.
Each of our landmarkclusters will correspond to a set of high frequencywords (HFWs).
The number of HFWs is muchlarger than the number of expected POS tags.Hence we should cluster HFWs.
Our algorithmdoes that by unifying some of the non-zero coordi-nates corresponding to HFWs in the distributionalrepresentation defined above.We extract the words that appear more than Ntimes per million1 and apply the following proce-dure I times (5 in our experiments).We run average link clustering with a threshold?
(AVGLINK?, (Jain et al, 1999)) on these words,in each iteration initializing every HFW to haveits own cluster.
AVGLINK?
means running the av-erage link algorithm until the two closest clustershave a distance larger than ?.
We then use the in-duced clustering to update the distributional rep-resentation, by collapsing all coordinates corre-sponding to words appearing in the same clusterinto a single coordinate whose value is the sumof the collapsed coordinates?
values.
In order toproduce a conservative (fine) clustering, we used arelatively low ?
value of 0.25.Note that the AVGLINK?
initialization in eachof the I iterations assigns each HFW to a sepa-rate cluster.
The iterations differ in the distribu-tional representation of the HFWs, resulting fromthe previous iterations.In our English experiments, this process re-duced the dimension of the HFWs set (the num-ber of coordinates that are non-zero in at least oneof the HFWs) from 14365 to 10722.
The aver-age number of non-zero coordinates per word de-creased from 102 to 55.Since all eventual POS categories correspond toclusters produced at this stage, to reduce noise wedelete clusters of less than five elements.Landmark detection.
We define landmark clus-ters using the clustering obtained in the final iter-ation of the coordinate clustering stage.
However,the number of clusters might be greater than thedesired number k, which is an optional parame-ter of the algorithm.
In this case we select a sub-set of k clusters that best covers the HFW space.We use the following heuristic.
We start from themost frequent cluster, and greedily select the clus-1We used N = 100, yielding 1242 words for English and613 words for German.1300ter farthest from the clusters already selected.
Thedistance between two clusters is defined to be theaverage distance between their members.
A clus-ter?s distance from a set of clusters is defined tobe its minimal distance from the clusters in theset.
The final set of clusters {L1, ..., Lk} and theirmembers are referred to as landmark clusters andprototypes, respectively.Mapping all words.
Each word w ?
W is as-signed the cluster Li that contains its nearest pro-totype:d(w,Li) = minx?Li{1 ?
cos(vw, vx)}Map(w) = argminLi{d(w,Li)}Words that appear less than 5 times are consid-ered as unknown words.
We consider two schemesfor handling unknown words.
One randomly mapseach such word to a cluster, using a probabil-ity proportional to the number of unique knownwords already assigned to that cluster.
However,when the number k of landmark clusters is rela-tively large, it is beneficial to assign all unknownwords to a separate new cluster (after running thealgorithm with k?
1).
In our experiments, we usethe first option when k is below some threshold(we used 15), otherwise we use the second.4 Morphological ModelThe morphological model generates another wordclustering, based on the notion of a signature.This clustering is integrated with the distributionalmodel as described below.4.1 Morphological RepresentationWe use the Morfessor (Creutz and Lagus, 2005)word segmentation algorithm.
First, all words inthe corpus are segmented.
Then, for each stem,the set of all affixes with which it appears (its sig-nature, (Goldsmith, 2001)) is collected.
The mor-phological representation of a word type is thendefined to be its stem?s signature in conjunctionwith its specific affixes2 (See Figure 1).We now collect all words having the same rep-resentation.
For instance, if the words joined andpainted are found to have the same signature, theywould share the same cluster since both have theaffix ?
ed?.
The word joins does not share the samecluster with them since it has a different affix, ?
s?.This results in coarse-grained clusters exclusivelydefined according to morphology.2A word may contain more than a single affix.Types join joins joined joiningStem join join join joinAffixes ?
s ed ingSignature {?, ed, s, ing}Figure 1: An example for a morphological representation,defined to be the conjunction of its affix(es) with the stem?ssignature.In addition, we incorporate capitalization infor-mation into the model, by constraining all wordsthat appear capitalized in more than half of theirinstances to belong to a separate cluster, regard-less of their morphological representation.
Themotivation for doing so is practical: capitalizationis used in many languages to mark grammaticalcategories.
For instance, in English capitalizationmarks the category of proper names and in Ger-man it marks the noun category .
We report En-glish results both with and without this modifica-tion.Words that contain non-alphanumeric charac-ters are represented as the sequence of the non-alphanumeric characters they include, e.g., ?vis-a`-vis?
is represented as (?-?, ?-?).
We do not as-sign a morphological representation to words in-cluding more than one stem (like weatherman), towords that have a null affix (i.e., where the wordis identical to its stem) and to words whose stemis not shared by any other word (signature of size1).
Words that were not assigned a morphologi-cal representation are included as singletons in themorphological clustering.4.2 Distributional-Morphological AlgorithmWe detail the modifications made to our basedistributional algorithm given the morphologicalclustering defined above.Coordinate clustering and landmarks.
Weconstrain AVGLINK?
to begin by forming links be-tween words appearing in the same morphologi-cal cluster.
Only when the distance between thetwo closest clusters gets above ?
we remove thisconstraint and proceed as before.
This is equiv-alent to performing AVGLINK?
separately withineach morphological cluster and then using the re-sult as an initial condition for an AVGLINK?
coor-dinate clustering.
The modified algorithm in thisstage is otherwise identical to the distributional al-gorithm.Word mapping.
In this stage words that are notprototypes are mapped to one of the landmark1301clusters.
A reasonable strategy would be to mapall words sharing a morphological cluster as a sin-gle unit.
However, these clusters are too coarse-grained.
We therefore begin by partitioning themorphological clusters into sub-clusters accordingto their distributional behavior.
We do so by apply-ing AVGLINK?
(the same as AVGLINK?
but with adifferent parameter) to each morphological clus-ter.
Since our goal is cluster refinement, we use a?
that is considerably higher than ?
(0.9).We then find the closest prototype to each suchsub-cluster (averaging the distance across all ofthe latter?s members) and map it as a single unitto the cluster containing that prototype.5 Clustering EvaluationWe evaluate the clustering produced by our algo-rithm using an external quality measure: we takea corpus tagged by gold standard tags, tag it usingthe induced tags, and compare the two taggings.There is no single accepted measure quantifyingthe similarity between two taggings.
In order tobe as thorough as possible, we report results usingfour known measures, two mapping-based mea-sures and two information theoretic ones.Mapping-based measures.
The induced clus-ters have arbitrary names.
We define two map-ping schemes between them and the gold clus-ters.
After the induced clusters are mapped, wecan compute a derived accuracy.
The Many-to-1measure finds the mapping between the gold stan-dard clusters and the induced clusters which max-imizes accuracy, allowing several induced clustersto be mapped to the same gold standard cluster.The 1-to-1 measure finds the mapping betweenthe induced and gold standard clusters which max-imizes accuracy such that no two induced clus-ters are mapped to the same gold cluster.
Com-puting this mapping is equivalent to finding themaximal weighted matching in a bipartite graph,whose weights are given by the intersection sizesbetween matched classes/clusters.
As in (Reichartand Rappoport, 2008), we use the Kuhn-Munkresalgorithm (Kuhn, 1955; Munkres, 1957) to solvethis problem.Information theoretic measures.
These arebased on the observation that a good clustering re-duces the uncertainty of the gold tag given the in-duced cluster, and vice-versa.
Several such mea-sures exist; we use V (Rosenberg and Hirschberg,2007) and NVI (Reichart and Rappoport, 2009),VI?s (Meila, 2007) normalized version.6 Experimental SetupSince a goal of unsupervised POS tagging is in-ducing an annotation scheme, comparison to anexisting scheme is problematic.
To address thisproblem we compare to three different schemesin two languages.
In addition, the two Englishschemes we compare with were designed to tagcorpora contained in our training set, and havebeen widely and successfully used with these cor-pora by a large number of applications.Our algorithm was run with the exact same pa-rameters on both languages: N = 100 (high fre-quency threshold), n = 50 (the parameter thatdetermines the effective number of coordinates),?
= 0.25 (cluster separation during landmarkcluster generation), ?
= 0.9 (cluster separationduring refinement of morphological clusters).The algorithm we compare with in most detailis (Clark, 2003), which reports the best currentresults for this problem (see Section 7).
SinceClark?s algorithm is sensitive to its initialization,we ran it a 100 times and report its average andstandard deviation in each of the four measures.In addition, we report the percentile in which ourresult falls with respect to these 100 runs.Punctuation marks are very frequent in corporaand are easy to cluster.
As a result, including themin the evaluation greatly inflates the scores.
Forthis reason we do not assign a cluster to punctua-tion marks and we report results using this policy,which we recommend for future work.
However,to be able to directly compare with previous work,we also report results for the full POS tag set.We do so by assigning a singleton cluster to eachpunctuation mark (in addition to the k requiredclusters).
This simple heuristic yields very highperformance on punctuation, scoring (when allother words are assumed perfect tagging) 99.6%(99.1%) 1-to-1 accuracy when evaluated againstthe English fine (coarse) POS tag sets, and 97.2%when evaluated against the German POS tag set.For English, we trained our model on the39832 sentences which constitute sections 2-21 ofthe PTB-WSJ and on the 500K sentences fromthe NYT section of the NANC newswire corpus(Graff, 1995).
We report results on the WSJ partof our data, which includes 950028 words tokensin 44389 types.
Of the tokens, 832629 (87.6%)1302English Fine k=13 Coarse k=13 Fine k=34Prototype Clark Prototype Clark Prototype ClarkTagger ?
?
% Tagger ?
?
% Tagger ?
?
%Many?to?1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 9055.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 901?to?1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 10054.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 1000.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 1000.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98German k=17 k=26Prototype Clark Prototype ClarkTagger ?
?
% Tagger ?
?
%Many?to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 6058.9 59.1 1.4 40 63.2 62.8 1.2 601?to?1 53.7 52.0 1.8 77 56.0 52.0 2.1 9948.0 46.0 2.3 78 50.7 45.9 2.6 99NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 1000.819 0.829 0.025 66 0.785 0.839 0.025 100V 0.646 0.645 0.010 50 0.675 0.657 0.008 1000.552 0.553 0.013 48 0.596 0.574 0.010 100Table 1: Top: English.
Bottom: German.
Results are reported for our model (Prototype Tagger), Clark?s average score (?
),Clark?s standard deviation (?)
and the fraction of Clark?s results that scored worse than our model (%).
For the mapping basedmeasures, results are accuracy percentage.
For V ?
[0, 1], higher is better.
For high quality output, NV I ?
[0, 1] as well, andlower is better.
In each entry, the top number indicates the score when including punctuation and the bottom number the scorewhen excluding it.
In English, our results are always better than Clark?s.
In German, they are almost always better.are not punctuation.
The percentage of unknownwords (those appearing less than five times) is1.6%.
There are 45 clusters in this annotationscheme, 34 of which are not punctuation.We ran each algorithm both with k=13 andk=34 (the number of desired clusters).
We com-pare the output to two annotation schemes: the finegrained PTB WSJ scheme, and the coarse grainedtags defined in (Smith and Eisner, 2005).
Theoutput of the k=13 run is evaluated both againstthe coarse POS tag annotation (the ?Coarse k=13?scenario) and against the full PTB-WSJ annotationscheme (the ?Fine k=13?
scenario).
The k=34 runis evaluated against the full PTB-WSJ annotationscheme (the ?Fine k=34?
scenario).The POS cluster frequency distribution tends tobe skewed: each of the 13 most frequent clustersin the PTB-WSJ cover more than 2.5% of the to-kens (excluding punctuation) and together 86.3%of them.
We therefore chose k=13, since it is boththe number of coarse POS tags (excluding punctu-ation) as well as the number of frequent POS tagsin the PTB-WSJ annotation scheme.
We chosek=34 in order to evaluate against the full 34 tagsPTB-WSJ annotation scheme (excluding punctua-tion) using the same number of clusters.For German, we trained our model on the 20296sentences of the NEGRA corpus (Brants, 1997)and on the first 450K sentences of the DeWACcorpus (Baroni et al, 2009).
DeWAC is a cor-pus extracted by web crawling and is thereforeout of domain.
We report results on the NEGRApart, which includes 346320 word tokens of 49402types.
Of the tokens, 289268 (83.5%) are notpunctuation.
The percentage of unknown words(those appearing less than five times) is 8.1%.There are 62 clusters in this annotation scheme,51 of which are not punctuation.We ran the algorithms with k=17 and k=26.k=26 was chosen since it is the number of clus-ters that cover each more than 0.5% of the NE-GRA tokens, and in total cover 96% of the (non-punctuation) tokens.
In order to test our algo-rithm in another scenario, we conducted experi-ments with k=17 as well, which covers 89.9% ofthe tokens.
All outputs are compared against NE-GRA?s gold standard scheme.We do not report results for k=51 (where thenumber of gold clusters is the same as the numberof induced clusters), since our algorithm producedonly 42 clusters in the landmark detection stage.We could of course have modified the parame-ters to allow our algorithm to produce 51 clusters.However, we wanted to use the exact same param-eters as those used for the English experiments tominimize the issue of parameter tuning.In addition to the comparisons described above,we present results of experiments (in the ?Fine1303B B+M B+C F(I=1) FM-to-1 53.3 54.8 58.2 57.3 61.01-to-1 50.2 51.7 55.1 54.8 60.0NVI 0.782 0.720 0.710 0.742 0.652V 0.569 0.598 0.615 0.597 0.636Table 2: A comparison of partial versions of the model inthe ?Fine k=13?
WSJ scenario.
M-to-1 and 1-to-1 results arereported in accuracy percentage.
Lower NVI is better.
B is thestrictly distributional algorithm, B+M adds the morphologi-cal model, B+C adds capitalization to B, F(I=1) consists ofall components, where only one iteration of coordinate clus-tering is performed, and F is the full model.M-to-1 1-to-1 V VIPrototype 71.6 63.5 0.677 2.00Clark 69.8 54.5 0.659 2.18HK ?
41.3 ?
?J 43?62 37?47 ?
4.23?5.74GG ?
?
?
2.8GJ ?
40?49.9 ?
4.03?4.47VG ?
?
0.54-0.59 2.5?2.9GGTP-45 65.4 44.5 ?
?GGTP-17 70.2 49.5 ?
?Table 4: Comparison of our algorithms with the recent fullyunsupervised POS taggers for which results are reported.
Themodels differ in the annotation scheme, the corpus size andthe number of induced clusters (k) that they used.
HK:(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.GG: (Goldwater and Griffiths, 2007), 24K tokens, coarsetags, k=17.
J : (Johnson, 2007), 1.17M tokens, fine tags,k=25?50.
GJ: (Gao and Johnson, 2008), 1.17M tokens, finetags, k=50.
VG: (Van Gael et al, 2009), 1.17M tokens, finetags, k=47?192.
GGTP-45: (Grac?a et al, 2009), 1.17M to-kens, fine tags, k=45.
GGTP-17: (Grac?a et al, 2009), 1.17Mtokens, coarse tags, k=17.
Lower VI values indicate betterclustering.
VI is computed using e as the base of the loga-rithm.
Our algorithm gives the best results.k=13?
scenario) that quantify the contribution ofeach component of the algorithm.
We ran the basedistributional algorithm, a variant which uses onlycapitalization information (i.e., has only one non-singleton morphological class, that of words ap-pearing capitalized in most of their instances) anda variant which uses no capitalization information,defining the morphological clusters according tothe morphological representation alone.7 ResultsTable 1 presents results for the English and Ger-man experiments.
For English, our algorithm ob-tains better results than Clark?s in all measures andscenarios.
It is without exception better than theaverage score of Clark?s and in most cases betterthan the maximal Clark score obtained in 100 runs.A significant difference between our algorithmand Clark?s is that the latter, like most algorithmswhich addressed the task, induces the clustering0 5 10 15 20 25 30 35 40 4500.20.40.60.81Gold StandardInducedFigure 2: POS class frequency distribution for our modeland the gold standard, in the ?Fine k=34?
scenario.
The dis-tributions are similar.by maximizing a non-convex function.
Thesefunctions have many local maxima and the specificsolution to which algorithms that maximize themconverge strongly depends on their (random) ini-tialization.
Therefore, their output?s quality oftensignificantly diverges from the average.
This issueis discussed in depth in (Reichart et al, 2010b).Our algorithm is deterministic3.For German, in the k=26 scenario our algorithmoutperforms Clark?s, often outperforming even itsmaximum in 100 runs.
In the k=17 scenario, ouralgorithm obtains a higher score than Clark withprobability 0.4 to 0.78, depending on the measureand scenario.
Clark?s average score is slightly bet-ter in the Many-to-1 measure, while our algorithmperforms somewhat better than Clark?s average inthe 1-to-1 and NVI measures.The DeWAC corpus from which we extractedstatistics for the German experiments is out of do-main with respect to NEGRA.
The correspond-ing corpus in English, NANC, is a newswire cor-pus and therefore clearly in-domain with respectto WSJ.
This is reflected by the percentage of un-known words, which was much higher in Germanthan in English (8.1% and 1.6%), lowering results.Table 2 shows the effect of each of our algo-rithm?s components.
Each component providesan improvement over the base distributional algo-rithm.
The full coordinate clustering stage (sev-eral iterations, F) considerably improves the scoreover a single iteration (F(I=1)).
Capitalization in-formation increases the score more than the mor-phological information, which might stem fromthe granularity of the POS tag set with respect tonames.
This analysis is supported by similar ex-periments we made in the ?Coarse k=13?
scenario(not shown in tables here).
There, the decrease inperformance was only of 1%?2% in the mapping3The fluctuations inflicted on our algorithm by the randommapping of unknown words are of less than 0.1% .1304Excluding Punctuation Including Punctuation Perfect PunctuationM-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI VVan Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al, 2009) and ours with various punctuation assign-ment schemes.
Left section: punctuation tokens are excluded.
Middle section: punctuation tokens are included.
Right section:perfect assignment of punctuation is assumed.based measures and 3.5% in the V measure.Finally, Table 4 presents reported results for allrecent algorithms we are aware of that tackled thetask of unsupervised POS induction from plaintext.
Results for our algorithm?s and Clark?s arereported for the ?Fine, k=34?
scenario.
The set-tings of the various experiments vary in terms ofthe exact annotation scheme used (coarse or finegrained) and the size of the test set.
However, thescore differences are sufficiently large to justifythe claim that our algorithm is currently the bestperforming algorithm on the PTB-WSJ corpus forPOS induction from plain text4.Since previous works provided results only forthe scenario in which punctuation is included, thereported results are not directly comparable.
Inorder to quantify the effect various punctuationschemes have on the results, we evaluated the?iHMM: PY-fixed?
model (Van Gael et al, 2009)and ours when punctuation is excluded, includedor perfectly tagged5.
The results (Table 3) indi-cate that most probably even after an appropriatecorrection for punctuation, our model remains thebest performing one.8 DiscussionIn this work we presented a novel unsupervised al-gorithm for POS induction from plain text.
The al-gorithm first generates relatively accurate clustersof high frequency words, which are subsequentlyused to bootstrap the entire clustering.
The dis-tributional and morphological representations thatwe use are novel for this task.We experimented on two languages with map-ping and information theoretic clustering evalua-tion measures.
Our algorithm obtains the best re-ported results on the English PTB-WSJ corpus.
Inaddition, our results are almost always better thanClark?s on the German NEGRA corpus.4Grac?a et al (2009) report very good results for 17 tags inthe M-1 measure.
However, their 1-1 results are quite poor,and results for the common IT measures were not reported.Their results for 45 tags are considerably lower.5We thank the authors for sending us their data.We have also performed a manual error anal-ysis, which showed that our algorithm performsmuch better on closed classes than on openclasses.
In order to asses this quantitatively, letus define a random variable for each of the goldclusters, which receives a value corresponding toeach induced cluster with probability proportionalto their intersection size.
For each gold cluster,we compute the entropy of this variable.
In ad-dition, we greedily map each induced cluster to agold cluster and compute the ratio between theirintersection size and the size of the gold cluster(mapping accuracy).We experimented in the ?Fine k=34?
scenario.The clusters that obtained the best scores were(brackets indicate mapping accuracy and entropyfor each of these clusters) coordinating conjunc-tions (95%, 0.32), prepositions (94%, 0.32), de-terminers (94%, 0.44) and modals (93%, 0.45).These are all closed classes.The classes on which our algorithm performedworst consist of open classes, mostly verb types:past tense verbs (47%, 2.2), past participle verbs(44%, 2.32) and the morphologically unmarkednon-3rd person singular present verbs (32%, 2.86).Another class with low performance is the propernouns (37%, 2.9).
The errors there are mostlyof three types: confusions between common andproper nouns (sometimes due to ambiguity), un-known words which were put in the unknownwords cluster, and abbreviations which were givena separate class by our algorithm.
Finally, the al-gorithm?s performance on the heterogeneous ad-verbs class (19%, 3.73) is the lowest.Clark?s algorithm exhibits6 a similar patternwith respect to open and closed classes.
Whilehis algorithm performs considerably better on ad-verbs (15% mapping accuracy difference and 0.71entropy difference), our algorithm scores consid-erably better on prepositions (17%, 0.77), su-perlative adjectives (38%, 1.37) and plural propernames (45%, 1.26).6Using average mapping accuracy and entropy over the100 runs.1305Naturally, this analysis might reflect the arbi-trary nature of a manually design POS tag setrather than deficiencies in automatic POS induc-tion algorithms.
In future work we intend to ana-lyze the output of such algorithms in order to im-prove POS tag sets.Our algorithm and Clark?s are monosemous(i.e., they assign each word exactly one tag), whilemost other algorithms are polysemous.
In order toassess the performance loss caused by the monose-mous nature of our algorithm, we took the M-1greedy mapping computed for the entire datasetand used it to compute accuracy over the monose-mous and polysemous words separately.
Resultsare reported for the English ?Fine k=34?
scenario(without punctuation).
We define a word to bemonosemous if more than 95% of its tokens areassigned the same gold standard tag.
For English,there are approximately 255K polysemous tokensand 578K monosemous ones.
As expected, ouralgorithm is much more accurate on the monose-mous tokens, achieving 76.6% accuracy, com-pared to 47.1% on the polysemous tokens.The evaluation in this paper is done at the tokenlevel.
Type level evaluation, reflecting the algo-rithm?s ability to detect the set of possible POStags for each word type, is important as well.
Itcould be expected that a monosemous algorithmsuch as ours would perform poorly in a type levelevaluation.
In (Reichart et al, 2010a) we discusstype level evaluation at depth and propose typelevel evaluation measures applicable to the POSinduction problem.
In that paper we compare theperformance of our Prototype Tagger with lead-ing unsupervised POS tagging algorithms (Clark,2003; Goldwater and Griffiths, 2007; Gao andJohnson, 2008; Van Gael et al, 2009).
Our al-gorithm obtained the best results in 4 of the 6measures in a margin of 4?6%, and was secondbest in the other two measures.
Our results werebetter than Clark?s (the only other monosemousalgorithm evaluated there) on all measures in amargin of 5?21%.
The fact that our monose-mous algorithm was better than good polysemousalgorithms in a type level evaluation can be ex-plained by the prototypical nature of the POS phe-nomenon (a longer discussion is given in (Reichartet al, 2010a)).
However, the quality upper boundfor monosemous algorithms is obviously muchlower than that for polysemous algorithms, andwe expect polysemous algorithms to outperformmonosemous algorithms in the future in both typelevel and token level evaluations.The skewed (Zipfian) distribution of POS classfrequencies in corpora is a problem for many POSinduction algorithms, which by default tend to in-duce a clustering having a balanced distribution.Explicit modifications to these algorithms were in-troduced in order to bias their model to producesuch a distribution (see (Clark, 2003; Johnson,2007; Reichart et al, 2010b)).
An appealing prop-erty of our model is its ability to induce a skeweddistribution without being explicitly tuned to doso, as seen in Figure 2.Acknowledgements.
We would like to thankYoav Seginer for his help with his parser.ReferencesMichele Banko and Robert C. Moore, 2004.
Part ofSpeech Tagging in Context.
COLING ?04.Marco Baroni, Silvia Bernardini, Adriano Ferraresi andEros Zanchetta, 2009.
The WaCky Wide Web: ACollection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation.Chris Biemann, 2006.
Unsupervised Part-of-Speech Tagging Employing Efficient Graph Cluster-ing.
COLING-ACL ?06 Student Research Work-shop.Thorsten Brants, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouze, Jenifer C. Lai and Robert Mercer, 1992.Class-Based N-Gram Models of Natural Language.Computational Linguistics, 18(4):467?479.Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech In-duction.
EACL ?03.Mathias Creutz and Krista Lagus, 2005.
Inducing theMorphological Lexicon of a Natural Language fromUnannotated Text.
AKRR ?05.Sajib Dasgupta and Vincent Ng, 2007.
Unsu-pervised Part-of-Speech Acquisition for Resource-Scarce Languages.
EMNLP-CoNLL ?07.Dayne Freitag, 2004.
Toward Unsupervised Whole-Corpus Tagging.
COLING ?04.Jianfeng Gao and Mark Johnson, 2008.
A Compar-ison of Bayesian Estimators for Unsupervised Hid-den Markov Model POS Taggers.
EMNLP ?08.Yoav Goldberg, Meni Adler and Michael Elhadad,2008.
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start).
ACL ?08.1306John Goldsmith, 2001.
Unsupervised Learning of theMorphology of a Natural Language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater and Tom Griffiths, 2007.
FullyBayesian Approach to Unsupervised Part-of-SpeechTagging.
ACL ?07.Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-nando Pereira, 2009.
Posterior vs. Parameter Spar-sity in Latent Variable Models.
NIPS ?09.David Graff, 1995.
North American News Text Cor-pus.
Linguistic Data Consortium.
LDC95T21.Aria Haghighi and Dan Klein, 2006.
Prototype-drivenLearning for Sequence Labeling.
HLT?NAACL ?06.Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,1999.
Data Clustering: A Review.
ACM ComputingSurveys 31(3):264?323.Wenbin Jiang, Liang Huang and Qun Liu, 2009.
Au-tomatic Adaptation of Annotation Standards: Chi-nese Word Segmentation and POS Tagging ?
A CaseStudy.
ACL ?09.Mark Johnson, 2007.
Why Doesnt EM Find GoodHMM POS-Taggers?
EMNLP-CoNLL ?07.Harold W. Kuhn, 1955.
The Hungarian method forthe Assignment Problem.
Naval Research LogisticsQuarterly, 2:83-97.Marina Meila, 2007.
Comparing Clustering ?
an In-formation Based Distance.
Journal of MultivariateAnalysis, 98:873?895.Bernard Merialdo, 1994.
Tagging English Text witha Probabilistic Model.
Computational Linguistics,20(2):155?172.James Munkres, 1957.
Algorithms for the Assignmentand Transportation Problems.
Journal of the SIAM,5(1):32?38.Sujith Ravi and Kevin Knight, 2009.
Minimized Mod-els for Unsupervised Part-of-Speech Tagging.
ACL?09.Roi Reichart and Ari Rappoport, 2008.
UnsupervisedInduction of Labeled Parse Trees by Clustering withSyntactic Features.
COLING ?08.Roi Reichart and Ari Rappoport, 2009.
The NVI Clus-tering Evaluation Measure.
CoNLL ?09.Roi Reichart, Omri Abend and Ari Rappoport, 2010a.Type Level Clustering Evaluation: New Measuresand a POS Induction Case Study.
CoNLL ?10.Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.Improved Unsupervised POS Induction Using In-trinsic Clustering Quality and a Zipfian Constraint.CoNLL ?10.Andrew Rosenberg and Julia Hirschberg, 2007.
V-Measure: A Conditional Entropy-Based ExternalCluster Evaluation Measure.
EMNLP ?07.Hinrich Schu?tze, 1995.
Distributional part-of-speechtagging.
EACL ?95.Yoav Seginer, 2007.
Fast Unsupervised IncrementalParsing.
ACL ?07.Noah A. Smith and Jason Eisner, 2005.
ContrastiveEstimation: Training Log-Linear Models on Unla-beled Data.
ACL ?05.John R. Taylor, 2003.
Linguistic Categorization: Pro-totypes in Linguistic Theory, Third Edition.
OxfordUniversity Press.Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-mani, 2009.
The Infinite HMM for UnsupervisedPOS Tagging.
EMNLP ?09.Qin Iris Wang and Dale Schuurmans, 2005.
Im-proved Estimation for Unsupervised Part-of-SpeechTagging.
IEEE NLP?KE ?05.Qiuye Zhao and Mitch Marcus, 2009.
A Simple Un-supervised Learner for POS Disambiguation RulesGiven Only a Minimal Lexicon.
EMNLP ?09.1307
