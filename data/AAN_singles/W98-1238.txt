/ll///lll/II//I//IILearning Feature-Value Grammars from Plain TextTony C. SmithDepartment of Computer Science, University of WaikatoHamilton, New Zealandt cs?cs, waikato, ac.
nzAbstractThis paper outlines preliminary work aimed atlearning Feature-Value Grammars from plaintext.
Common suffixes are gleaned from aword suffix tree and used to form a firstapproximation of how regular inflection ismarked.
Words are generalised according tothese suffixes and then subjected to trigramanalysis in an attempt o identify agreementdependencies.
They are subsequently abeledwith a feature whose value is given by the com-mon suffix.
A means for converting the featuredependencies into a unification grammar isde-scribed wherein feature structures are projec-ted on to unlabeled words.
Irregularly inflec-ted words are subsumed into common categor-ies through the process of unification.1 MotivationUnification grammars (UGs) have become the estab-lished formalism for natural language understand-ing systems, primarily because of their clean denota-tional semantics and their ability to capture complexgrammatical constraints through feature dependen-cies (Uszkoreit & Zaenen, 1996).
But engineeringeven modestly sized UGs can take a very long time,making the idea of constructing a comprehensive, ro-bust, competent UG by hand virtually intractable.Recent advances in stochastic language modeling,however, have made it possible to incorporate stat-istical information i to UGs (Abney, 1996 and Smith& Cleary, 1997), thus giving access to the complexityestimates now widely regarded as essential for auto-matically learning adequate grammars from positivedata alone.
But this still leaves open the question ofexactly how such learning can be achieved for UGs.A probabilistic unification grammar (PUG) hasthree principal components: 1) a context-free ac-count of linear precedence relations, 2) a set of fea-tures for expressing rammatical dependencies, and3) probability distributions for the rules and features.Methods for unsupervised learning of the first andlast of these components have already been suitablyworked out.
For example, the context-free descrip-tion can be addressed with solutions borrowed fromwork in learning PCFGs  (Jelinek et al 1992), and thedistribution can be estimated by training on sampledata (Eisele, 1994 and Brew, 1995).
The outstand-ing problem then is how to derive a satisfactory setof features in the absence of overt semantic inform-ation.This paper describes preliminary work aimed atlearning a Feature-Value Grammar from plain text.It is based on the generally held notion that syn-tactic agreement and morphological inflection areclosely related (Abney, 1987 and Fukui & Speas,1986).
Morphological clues about inflectional affixesare gleaned from the vocabulary of a language usinga word suffix tree.
Common suffixes are assumedto identify related semantic elements undergoing thesame inflectional process, allowing the contexts inwhich they occur to be generalised through the cre-ation of feature structures.
Feature values are setaccording to the common suffix and projected on tounlabeled words.
The contexts and the agreementconstraints are thereafter expressed using a unifica-tion formalism.
Irregularly inflected words are sub-sumed into existing categories by unifying the con-texts in which they occur with those established forregularly inflected words.Smith 291 Learning Feature-Value GrammarsTony C. Smith (1998) Learning Feature-Value Grammars from Plain Text.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98Workshop on Paradigms and Grounding in Language Learning, ACL, pp 291-294.2 Feature  ident i f i ca t ionAUG encodes lexical properties as feature struc-tures (specifying such things as part-of-speech, num-ber, tense, person, thematic role, etc.)
whose val-ues percolate up through a subsumption hierarchyby the process of unification (Sanfilippo, 1993).
Syn-tactic constraints are imposed by forcing agreementbetween features of grammatically related struc-tures.Kazman (1994) argues that features correspondto semantic properties associated with thematic at-egories (e.g.
nouns, verbs and adjectives) and thatlearning syntax is equivalent o figuring out howthese properties impose constraints on the functionalcategories (e.g.
determiners, auxiliaries, and com-plementizers) of a particular language.
This studytakes the slightly stronger position that the processby which thematic and functional categories are com-bined is mediated by morphological inflection.
LikeKazman's ystem, Babel, the focus is on the role ofinflectional affixes in the acquisition of agreement.But unlike Babel, which makes inferences over se-mantically related words identified through set oper-ations on input already tagged with attributes, thiswork addresses feature identification as a bootstrap-ping problem, where inflectional affixes and the con-straints they impose are inferred from plain text.A first approximationThe first objective is to detect when and how inflec-tion is manifest.
This is addressed through gener-alisation on a word suffix tree (WST) constructedfor the vocabulary of the language.
A WST is aderivative of a letter-based multiway trie built froman ordered set of words.
Each .distinct sequence ofcharacters along a path in the trie is collapsed intoa single node, resulting in a WST for which all leafnodes are common suffixes to the prefix terminatedby their parent node (Andersson, 1996).
A sampleportion of a WST is shown in Figure 1.
Note thatthe symbol $ is a kind of NULL suffix, which showsthat the parent node is itself a suffix and thus corres-ponds to the end of an actual word.
It follows thatits leaf nodes correspond to genuine morphologicalsuffixes.Given that regular inflection is largely realisedthrough suffixion on root categories, a first approx-imation of these categories may be given by assign-ing a common lexical identity to words that sharethe same set of suffixes.
That is, it is assumed thatwords which inflect in the same ways likely belong tothe same syntactic ategory.
Clearly not all suffixesare inflectional.
Therefore, some general restrictions?
-?
~.?
?Figure 1: Portion of a word suffix tree.are applied in an analysis of the WST in an effort togarner a set of possible inflectional suffixes.
First,any suffix which has a suffix itself cannot be inflec-tional, based on the assumption that inflectional suf-fixes always occur at the end of a word.
Second, rootcategories must have at least two inflected forms,thus a prefix may only be a possible root category ifit appears to have at least two inflectional suffixes.The corollary is that a suffix is not inflectional if it isthe only inflectional suffix.
Under these restrictions,the suffix set for "aim" in Figure 1 would be {$, ed,ing, s}.Inf lection in contextTo identify which suffixes are grammatically signific-ant, a contextual nalysis of how they are used mustbe carried out.
This can be done by generalisingover the trigrams of a large sample of text in whicheach word has been replaced by its correspondingsuffix as given by the WST analysis.
Almost all func-tional categories and irregularly inflected words haveno inflectional suffixes associated with them and aretherefore left unchanged.The trigrams are sorted and processed in decreas-ing order according to their frequency.
Feature struc-tures are hypothesised toreconcile trigrams that dif-fer in only one term.
For example, some of the mostfrequent trigrams might be as follows:Smith 292 Learning Feature- Value GrammarsBII!1!1IIInIIliIIIIIiIIBIIII//m/I//IIIl///1) the -s were2) the -s -83) the -$ was4) the -8 -s5) the -$ -ed6) the -s -edAssume in this instance that -s has replaced ogs inphrase 1, 2 and 6, and $ has replaced og in phrases3, 4, and 5.
In addition, assume that the prefix iswalk for the suffixes given in the third position forphrases 2, 4, 5 and 6.These six related trigrams imply an agreementconstraint that can be captured with a feature struc-ture.
For example, were and -$ appear after the con-text the -s, but not after the -$, and was and -s occurafter the $ but not after -s, indicating a possible de-pendency between the last two terms.
Phrases 5 and6 imply a common syntactic role for $ and -s, thuswe might infer that the dependency is one of featureagreement.
As the second term is uniformly a suf-fix, we might assume that it projects the agreementand is therefore inflectional.
To characterise this, weassociate a feature with the words appearing in thesecond position, and assign it the value of the suffixin each instance, giving the following lexical entries:dog(E1 =$ )dogs(F1 =-s)To characterise the dependency in the first fourphrases, we project the feature structure on to thewords in the third position, assigning the correspond-ing feature value needed to preserve the dependency,as follows:dogs(F1 =-s)dog(El =$ )were(F1 =-s)walk(El =-s)was(E1 =$ )walks(El =$ )Phrases 5 and 6 must be made to have the samefeature structure, but this appears to entail assign-ing two different values to the feature structure forwalked.
However, given that walked is not con-strained by this particular feature, its value can beleft ungrounded, giving:dogs(El =-s)dog(F1 =$ )were(El -'--s)walk(El =-s)was(E1 =$ )walks(F1 =$ )walked(F1 =X)From this limited set of phrases, it appears unne-cessary to extend the inflectional constraint to theword the.
However, given a trigram of the form"a $ was" without the complementary t igram "a $were", agreement would force projection of the fea-ture structure on to the determiner.Once a word has been identified as an inflectedform, this provides additional information for thegeneralisation f subsequent trigrams.
If a term isknown to project an agreement constraint in one in-stance, this curtails the number of hypotheses thatmust be tested to determine the source of any newconstraints.
That is, if were and walk come up inanother set of related trigrams, the existing featureF1 can be trialed first as a possible xplanation.Capturing the syntactic constraintsDeriving features in the manner described in theprevious ection provides an account of inflectionalagreement.
To translate this into syntactic on-straints requires the addition of corresponding uni-fication rules.
Thus, as each trigram is processed,any changes to the feature structure must generatea rule that captures the linear precedence r lation.This can be done efficiently with logic programs, uchas Prolog DCGs.
Initially, the grammar is formedby generating clauses to cover dependencies betweenpairs of terms, annotated with the appropriate f a-ture structures and values.
The grammar is builtup by combining adjacent clauses and unifying theirvariables (i.e.
features).
The unification also allowsrules for irregular inflections to be transformed intoa more general form.Irregular inflectionIrregular words may follow some of the same inflec-tional patterns as regular words, such as the presentand gerundive forms in English verbs, and thus canbe generalised with the same mechanism.
In other in-stances, they may force the creation of a new featurestructure which captures the same agreement con-straint.
To avoid this, every new rule is comparedagainst existing rules to see if they have a commonstructure which can be generalised.
Only rules whichdiffer by a single term need to be examined, and onlyif the features of that term are grounded in the es-tablished case.
If the new rule can be unified withan old rule by a consistent change in its correspond-ing feature values, then the lexicon is adjusted andthe new rule is discarded.
Since irregular forms donot differ in their usage, sufficiently large samples oftext (enough to cause a match between rules) will al-low the same agreement constraint to be captured byone rule.
This solution also applies to words whoseSmith 293 Learning Feature- Value Grammarssuffixing is irregular because of orthographic conven-tions, as when abated and abates are categorised bythe suffix set {d, s} instead of the more common {ed,s}.3 RemarksTo the extent that inflectional agreement morpho-logy and syntactic agreement s ructures are linked,generalisation over inflectional suffixes is likely theonly means by which a unification grammar can belearned from plain text.
This work represents aninitial attempt at doing just that.The WST is a suitable data structure for uncov-ering suffixes, but is insufficient for identifying thosewhich mark inflection.
This requires a characterisa-tion of how individual suffixes are used contextually,and identification of instances where they appear toimpose agreement constraints.Limiting context analysis to trigrams has the ob-vious disadvantage that long distance dependenciescannot be reliably inferred unless they happen topercolate up through a series of unification opera-tions between smaller phrases.
It is possible thatsome statistical techniques for finding lexical depend-encies, such as those used in constructing link gram-mars, would be a more effective way to build featurestructures and the grammar.Perhaps the most appealing aspect to this ap-proach is that it attempts to combine morphologicalconstraints and syntactic onstraints within a singlemodel for grammar induction.
In so doing it has un-covered a number of interesting problems and ideaswhich should generate interesting discussions in alanguage learning workshop.Andreas Eisele.
Towards probabilistic extensions ofconstraint-based grammars.
Deliverable rl.2.b,DYANA-2, September 1994.Naoki Fukui and Peggy Speas.
Specifiers and projec-tion.
MIT Working Papers in Linguistics, 8:128-172, 1986.F.
Jelinek, J. D. Lafferty, and R. L. Mercer.
Basicmethods of probabilistic ontext-free grammars.In Speech Recognition and Understanding: Re-cent Advances, Trends and Applications.
Proceed-ings of the NA TO Advanced Study Institute, pages345-360, 1992.Rick Kazman.
Simulating the child's acquisition ofthe lexicon and syntax---experiences with babel.Machine Learning, 16:87-120, 1994.A.
Sanfilippo.
Lkb encoding of lexical knowledge.
InT.
Briscoe, A. Copestake, and V. de Paiva, edit-ors, Default Inheritance within Unification-BasedApproaches to the Lexicon.
Cambridge UniversityPress, 1993.Tony C. Smith and John G.Cleary.
Probabilisticunification grammars.
In Workshop Notes: A CSC'97 Australasian Natural Language ProcessingSummer Workshop, pages 25-32, Macquarie Uni-versity, February 1997.Hans Uszkoreit and Annie Zaenen.
Grammar form-alisms.
In Ron Cole, editor, A Survey of theState of the Art in Human Language Technology,chapter 3.3.
Center for Spoken Language Under-standing, University of Pisa, Italy, 1996.Re ferencesSteven Abney.
The Noun Phrase in its SententialAspect.
PhD thesis, MIT, 1987. unpublished.Steven Abney.
Stochastic attribute-value grammars.The Computation and Language E-Print Archive,page 21, October 1996.
9610003.A.
Andersson, N. Jesper Larsson, and Kurt Swan-son.
Suffix trees on words.
In D. Hirschbergand G. Myers, editors, Lecture Notes in ComputerScience 1075, Combinatorial Pattern Matching,pages 102-115.
Springer-Verlag, 1996.Chris Brew.
Stochastic hpsg.
In Proceedings ofEACL-95, 1995.S.
F. Chen.
Building probabilistic models for NaturalLanguage.
PhD thesis, Harvard University, Cam-bridge, Massachusetts, Cambridge, Mass., 1996.Smith 294 Learning Feature-Value GrammarsIIIIIIIIIIIIII
