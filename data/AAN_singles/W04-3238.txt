Spelling correction as an iterative processthat exploits the collective knowledge of web usersSilviu Cucerzan and Eric BrillMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{silviu,brill}@microsoft.comAbstractLogs of user queries to an internet search engine pro-vide a large amount of implicit and explicit informa-tion about language.
In this paper, we investigatetheir use in spelling correction of search queries, atask which poses many additional challenges beyondthe traditional spelling correction problem.
We pre-sent an approach that uses an iterative transformationof the input query strings into other strings that corre-spond to more and more likely queries according tostatistics extracted from internet search query logs.1 IntroductionThe task of general purpose spelling correction hasa long history (e.g.
Damerau, 1964; Rieseman andHanson, 1974; McIlroy, 1982), traditionally focus-ing on resolving typographical errors such as in-sertions, deletions, substitutions, andtranspositions of letters that result in unknownwords (i.e.
words not found in a trusted lexicon ofthe language).
Typical word processing spellcheckers compute for each unknown word a smallset of in-lexicon alternatives to be proposed aspossible corrections, relying on information aboutin-lexicon-word frequencies and about the mostcommon keyboard mistakes (such as typing m in-stead of n) and phonetic/cognitive mistakes, bothat word level (e.g.
the use of acceptible instead ofacceptable) and at character level (e.g.
the misuseof f instead of ph).
Very few spell checkers attemptto detect and correct word substitution errors,which refer to the use of in-lexicon words in inap-propriate contexts and can also be the result ofboth typographical mistakes (such as typing coedinstead of cord) and cognitive mistakes (e.g.
prin-cipal and principle).
Some research efforts totackle this problem have been made; for exampleHeidorn et al (1982) and Garside et al (1987) de-veloped systems that rely on syntactic patterns todetect substitution errors, while Mays et al (1991)employed word co-occurrence evidence from alarge corpus to detect and correct such errors.The former approaches were based on the imprac-tical assumption that all possible syntactic usesof all words (i.e.
part-of-speech) are known, andpresented both recall and precision problems be-cause many of the substitution errors are not syn-tactically anomalous and many unusual syntacticconstructions do not contain errors.
The latter ap-proach had very limited success under the assump-tions that each sentence contains at most onemisspelled word, each misspelling is the result of asingle point change (insertion, deletion, substitu-tion, or transposition), and the defect rate (the rela-tive number of errors in the text) is known.
Adifferent body of work (e.g.
Golding, 1995; Gold-ing and Roth, 1996; Mangu and Brill, 1997) fo-cused on resolving a limited number of cognitivesubstitution errors, in the framework of contextsensitive spelling correction (CSSC).
Althoughpromising results were obtained (92-95% accu-racy), the scope of this work was very limited as itonly addressed known sets of commonly confusedwords, such as {peace, piece}).1.1 Spell Checking of Search Engine QueriesThe task of web-query spelling correction ad-dressed in this work has many similarities to tradi-tional spelling correction but also poses additionalchallenges.
Both the frequency and severity ofspelling errors for search queries are significantlygreater than in word processing.
Roughly 10-15%of the queries sent to search engines contain errors.Typically, the validity of a query cannot be de-cided by lexicon look-up or by checking its gram-maticality.
Because web queries are very short (onaverage, less than 3 words), techniques that use amultitude of features based on relatively wide con-text windows, such as those investigated in CSSC,are difficult to apply.
Rather than being well-formed sentences, most queries consist of one con-cept or an enumeration of concepts, many timescontaining legitimate words that are not found inany traditional lexicon.Just defining what a valid web query is representsa difficult enterprise.
We clearly cannot use only astatic trusted lexicon, as many new names andconcepts (such as aznar, blog, naboo, nimh, nsync,and shrek) become popular every day and it wouldbe extremely difficult if not impossible to maintaina high-coverage lexicon.
In addition, employingvery large lexicons can result in more errors sur-facing as word substitutions, which are very diffi-cult to detect, rather than as unknown words.One alternative investigated in this work is to ex-ploit the continuously evolving expertise of mil-lions of people that use web search engines, ascollected in search query logs (seen as histogramsover the queries received by a search engine).
Insome sense, we could say that the validity of aword can be inferred from its frequency in whatpeople are querying for, similarly to Wittgen-stein?s (1968) observation that ?the meaning of aword is its use in the language?.
Such an approachhas its own caveats.
For example, it would be er-roneous to simply extract from web-query logs allthe queries whose frequencies are above a certainvalue and consider them valid.
Misspelled queriessuch as britny spears are much more popular thancorrectly spelled queries such as bayesian nets andamd processors.
Our challenge is to try to utilizequery logs to learn what queries are valid, and tobuild a model for valid query probabilities, despitethe fact that a large percentage of the logged que-ries are misspelled and there is no trivial way todetermine the valid from invalid queries.2 Problem Formulation.
Prior WorkComprehensive reviews of the spelling correctionliterature were provided by Peterson (1980),Kukich (1992), and Jurafsky and Martin (2000).
Inthis section, we survey a few lexicon-based spell-ing correction approaches by using a series of for-mal definitions of the task and presenting concreteexamples showing the strengths and the limits cor-responding to each situation.
We iteratively rede-fine the problem, starting from an approach purelybased on a trusted lexicon and ending up with anapproach in which the role of the trusted lexicon isgreatly diminished.
While doing so, we also makeconcrete forward steps in our attempt to provide adefinition of valid web queries.Let ?
be the alphabet of a language and *?
?L abroad-coverage lexicon of the language.
The sim-plest and historically the first definition of lexicon-based spelling correction (Damerau, 1964) is:Given an unknown word Lw \*??
, find Lw ?
'such that ),(min)',( vwdistwwdistLv?= .i.e.
for any out-of-lexicon word in a text, find theclosest word form(s) in the available lexicon andhypothesize it as the correct spelling alternative.dist  can be any string-based function; for exam-ple, it can be the ratio between the number of let-ters two words do not have in common and thenumber of letters they share.1 The two most usedclasses of distances in spelling correction are editdistances, as proposed by Damerau (1964) andLevenshtein (1965), and correlation matrix dis-tances (Cherkassky et al, 1974).
In our study, weuse a modified version of the Damerau-Lev-enshtein edit distance, as presented in Section 3.One flaw of the preceding formulation is that itdoes not take into account the frequency of wordsin a language.
A simple solution to this problem isto compute the probability of words in the targetlanguage as maximum likelihood estimates (MLE)over a large corpus and reformulate the generalspelling-correction problem as follows:Given Lw \*??
, find Lw ?'
such that??
)',( wwdist and )(max)'(),(:vPwPvwdistLv ??
?= .In this formulation, all in-lexicon words that arewithin some ?reasonable?
distance ?
of the un-known word are considered as good candidates,the correction being chosen based on its priorprobability in the language.
While there is an im-plicit conditioning on the original spelling becauseof the domain on which the best correction issearched, this objective function only uses theprior probability of words in the language and notthe actual distances between each candidate andthe input wordOne solution that allows using a probabilistic editdistance is to condition the probability of a correc-tion on the original spelling )|( wvP :Given Lw \*??
, find Lw ?'
such that??
)',( wwdist and )|(max)|'(),(:wvPwwPvwdistLv ??
?= .In a noisy channel model framework, as em-ployed for spelling correction by Kernigham et al(1990), the objective function can be written byusing Bayesian inversion as the product betweenthe prior probability of words in a language )(vP(the language model), and the likelihood of mis-spelling a word v as w, )|( vwP  (which models thenoisy channel and will be called the error model).In the above formulations, unknown words arecorrected in isolation.
This is a rather major flawbecause context is extremely important for spellingcorrection, as illustrated in the following example:power crd  power cordvideo crd  video card1Note that the function does not have to be symmetric; thus,the notation dist(w,w?)
is used with a loose sense.The misspelled word crd should be corrected totwo different words depending on its contexts.2A formulation of the spelling correction problemthat takes into account context is the following:Given a string *?
?s , rl wccs = , with Lw \*?
?and *, Lcc rl ?
, find Lw ?'
such that ??
)',( wwdistand )|(max)|'(),(: rlvwdistLvrlwccvPwccwP??
?= .Spaces and other word delimiters are ignored inthis formulation and the subsequent formulationsfor simplicity, although text tokenization repre-sents an important part of the spelling-correctionprocess, as discussed in Sections 5 and 6.The task definitions enumerated up to this point(on which most traditional spelling correction sys-tems are based) ignore word substitution errors.
Inthe case of web searches, it is extremely importantto provide correction suggestions for valid wordswhen they are more meaningful as a search querythan the original query, for example:golf war  gulf warsap opera  soap operaThis problem is partially addressed by the task ofCSSC, which can be formalized as follows:Given a set of confusable valid word formsin a language },...,,{ 21 nwwwW =  and a stringril cwcs = , choose Ww j ?
such that)|(max)|(..1 rilknkriljcwcwPcwcwP== .In the CSSC literature, the sets of confusables arepresumed known, but they could also be built foreach in-lexicon word w as all words 'w  with??
)',( wwdist , similarly to the approach investi-gated by Mays et al (1991), in which they chose a1=?
and employed an edit distance with all pointchanges having the same cost 1.The generalized problem of phrasal spelling cor-rection can then be formulated as follows:Given *?
?s , find *' Ls ?
such that ??
)',( ssdistand )|(max)|'(),(:*stPssPtsdistLt ??
?= .Typically, a correction is desirable when *Ls ?(i.e.
at least one of the component words is un-known) but, as shown above, there are frequentcases (e.g.
golf war) when sequences of validwords should be changed to other word sequences.Note that word boundaries are hidden in this latter2To simplify the exposition, we only consider two highlyprobable corrections, but other valid alternatives exist, e.g.video cd.formulation, making it more general and allowingit to cover two other important spelling errorclasses, concatenation and splitting, e.g.
:power point slides  powerpoint slideschat inspanich   chat in spanishYet, it still does not account for another importantclass of cases in web query correction which isrepresented by out-of-lexicon words that are validin certain contexts (therefore, *' Ls ?
), for example:amd processors  amd processors (no change)The above phrase represents a legitimate query,despite the fact that it may contain unknown wordswhen employing a traditional English lexicon.Some even more interesting cases not handled bytraditional spellers and also not covered by thelatter formulation are those in which in-lexiconwords should be changed to out-of-lexicon words,as in the following examples, where two validwords must be concatenated into an out of lexiconword:gun dam planet  gundam planetlimp biz kit  limp bizkitThese observations lead to an even more generalformulation of the spelling-correction problem:Given *?
?s , find *' ?
?s  such that ??
)',( ssdistand )|(max)|'(),(:*stPssPtsdistt ???
?= .For the first time, the formulation no longermakes explicit use of a lexicon of the language.3 Insome sense, the actual language in which the webqueries are expressed becomes less important thanthe query-log data from which the string probabili-ties are estimated.
This probability model can beseen as a substitute for a measure of the meaning-fulness of strings as web-queries.
For example, animplausible random noun phrase in any of the tra-ditional corpora such as sad tomatoes is meaning-ful in the context of web search (being the name ofa somewhat popular music band).3 The Error Model.
String Edit FunctionsAll formulations of the spelling correction taskgiven in the previous section used a string distancefunction and a threshold to restrict the space inwhich alternative spellings are searched.
Variousprevious work has addressed the problem ofchoosing appropriate functions (e.g.
Kernigham etal.
1990, Brill and Moore, 2002; Toutanova andMoore, 2003).3A trusted lexicon may still be used in the estimation of thelanguage model probability for the computation of )|( stP .The choice of distance function d and threshold ?could be extremely important for the accuracy of aspeller.
At one extreme, the use of a too restrictivefunction/threshold combination can result in notfinding the best correction for a given query.
Forexample, using the vanilla Damerau-Levenshteinedit distance (defined as the minimum number ofpoint changes required to transform a string intoanother, where a point change is one of the follow-ing operations: insertion of a letter, deletion of aletter, and substitution of one letter with anotherletter) and a threshold 1=?
, the correction donadlduck  donald duck would not be possible.
At theother extreme, the use of a less limiting functionmight have as consequence suggesting veryunlikely corrections.
For example, using the sameclassical Levenshtein distance and 2=?
wouldallow the correction of the string donadl duck, butwill also lead to bad corrections such as log wood dog food (based on the frequency of the queries,as incorporated in )(sP ).
Nonetheless, large dis-tance corrections are still desirable in a diversity ofsituations, for example:platnuin rings   platinum ringsditroitigers   detroit tigersThe system described in this paper makes use of amodified context-dependent weighted Damerau-Levenshtein edit function which allows insertion,deletion, substitution, immediate transposition, andlong-distance movement of letters as pointchanges, for which the weights were interactivelyrefined using statistics from query logs.4 The Language Model.
Exploiting LargeWeb Query LogsA misspelling such as ditroitigers is far from thecorrect alternative and thus, it might be extremelydifficult to find its correct spelling based solely onedit distance.
Nonetheless, the correct alternativecould be reached by allowing intermediate validcorrection steps, such as ditroitigers  detroitti-gers  detroit tigers.
But what makes detroittigersa valid correction step?
Recall that the last formu-lation of spelling correction in Section 3 did notexplicitly use a lexicon of the language.
Rather,any string that appears in the query log used fortraining can be considered a valid correction andcan be suggested as an alternative to the currentweb query based on the relative frequency of thequery and the alternative spelling.
Thus, a spellchecker built according to this formulation couldsuggest the correction detroittigers because thisalternative occurs frequently enough in the em-ployed query log.
However, detroittigers itselfcould be corrected to detroit tigers if presented asa stand-alone query to this spell checker, based onsimilar query-log frequency facts, which naturallyleads to the idea of an iterative correction ap-proach.albert einstein 4834albert einstien 525albert einstine 149albert einsten 27albert einsteins 25albert einstain 11albert einstin 10albert eintein 9albeart einstein 6aolbert einstein 6alber einstein 4albert einseint 3albert einsteirn 3albert einsterin 3albert eintien 3alberto einstein 3albrecht einstein 3alvert einstein 3Table 1.
Counts of different (mis)spellings of AlbertEinstein?s name in a web query log.Essential to such an approach are three typicalproperties of the query logs (e.g.
see Table 1):?
words in the query logs are misspelled in vari-ous ways, from relatively easy-to-correct mis-spellings to very-difficult-to-correct ones, thatmake the user?s intent almost impossible torecognize;?
the less malign (difficult to correct) a misspell-ing is the more frequent it is;?
the correct spellings tend to be more frequentthan misspellings.In this context, the spelling correction problemcan be given the following iterative formulation:Given a string *0 ?
?s , find a sequence*21 ,..., ?
?nsss   such that  ?
?+ ),( 1ii ssdist ,)|(max)|(),(:1 * itsdisttiistPssPi ???
?+ = , 1..0 ???
ni ,and )|(max)|(),(:* ntsdisttnnstPssPn ???
?= .An example of correction that can be made byiteratively applying the base spell checker is:anol scwartegger   arnold schwarzeneggerMisspelled query: anol scwarteggerFirst iteration: arnold schwartneggerSecond iteration: arnold schwarzneggerThird iteration: arnold schwarzeneggerFourth iteration: no further correctionUp to this point, we underspecified the notion ofstring in the task formulations given.
One possibil-ity is to consider whole queries as the strings to becorrected and iteratively search for better loggedqueries according to the agreement between theirrelative frequencies and the character error model.This is equivalent to identifying all queries in thequery log that are misspellings of other queries andfor any new query, find a correction sequence oflogged queries.
While such an approach exploitsthe vast information available in web-query logs, itonly covers exact matches of the queries that ap-pear in these logs and provides a low coverage ofinfrequent queries.
For example, a query such asbritnet spear inconcert could not be corrected ifthe correction britney spears in concert does notappear in the employed query log, although thesubstring britnet spear could be corrected to brit-ney spears.To address the shortcomings of such an approach,we propose a system based on the following for-mulation, which uses query substrings:Given *0 ?
?s , find a sequence*21 ,..., ?
?nsss ,such that for each 1..0 ??
ni  there exist the de-compositions ii liiliii wwwws 1,111,11i0,10, ...s ,... +++ == ,where k hjw ,  are words or groups of words such that?
?+ ),( 1,10, kiki wwdist , ilkni ..1  ,1..0 ?????
and)|(max)|(** ),(:1 itsdisttiistPssPi ???
?+ = , 1..0 ???
ni ,and )|(max)|(** ),(: ntsdisttnnstPssPn ???
?= .Note that the length of the string decompositionmay vary from one iteration to the next one, forexample:In the implementation evaluated in this paper, weallowed decompositions of query strings intowords and word bigrams.
The tokenization processuses space and punctuation delimiters in additionto the information provided about multi-wordcompounds (e.g.
add-on and back-up) by a trustedEnglish lexicon with approximately 200k entries.By using the tokenization process described above,we extracted word unigram and bigram statisticsfrom query logs to be used as the system?s lan-guage model.5 Query CorrectionAn input query is tokenized using the same spaceand word-delimiter information in addition to theavailable lexical information as used for process-ing the query log.
For each token, a set of alterna-tives is computed using the weighted Levenshteindistance function described in Section 3 and twodifferent thresholds for in-lexicon and out-of-lexicon tokensMatches are searched in the space of word uni-grams and bigrams extracted from query logs inaddition to the trusted lexicon.
Unigrams and bi-grams are stored in the same data structure onwhich the search for correction alternatives isdone.
Because of this, the proposed system han-dles concatenation and splitting of words in ex-actly the same manner as it handlestransformations of words to other words.Once the sets of all possible alternatives are com-puted for each word form in the query, a modifiedViterbi search (in which the transition probabilitiesare computed using bigram and unigram query-logstatistics and output probabilities are replaced withinverse distances between words) is employed tofind the best possible alternative string to the inputquery under the following constraint: no two adja-cent in-vocabulary words are allowed to changesimultaneously.
This constraint prevents changessuch as log wood  dog food.
An algorithmic con-sequence of this constraint is that there is no needto search all the possible paths in the trellis, whichmakes the modified search procedure much faster,as described further.
We assume that the list ofalternatives for each word is randomly ordered butthe input word is on the first position of the listwhen the word is in the trusted lexicon.
In thiscase, the searched paths form what we call fringes.Figure 1 presents an example of a trellis in whichw1, w2 and w3 are in-lexicon word forms.
Observethat instead of computing the cost of k1k2 possiblepaths between the alternatives corresponding to w1and w2, we only need to compute the cost of k1+k2paths.31 =l42 =l20 =l  0s  britenetspear   inconcert1s  britneyspears  in concert2s  britney spears in concert3s  britney spears in concert1121111kaaaw2222122kaaaw3323133kaaaw4424144kaaaw5525155kaaaw6626166kaaaw7727177kaaawstop wordunknownwordFigure 1.
Example of trellis of the modified Viterbi searchBecause we use word-bigram statistics, stopwords such as prepositions and conjunctions mayinterfere negatively with the best path search.
Forexample, in correcting a query such as platunumand rigs, the language model based on word bi-grams would not provide a good context for theword form rigs.To avoid this type of problems, stop words andtheir most likely misspelling are given a specialtreatment.
The search is done by first ignoringthem, as in Figure 1, where w4 is presumed to besuch a word.
Once a best path is found by ignoringstop words, the best alternatives for the skippedstop words (or their misspellings) are computed ina second Viterbi search with fringes in which theextremities are fixed, as presented in Figure 2.1121111kaaaw2222122kaaaw3323133kaaaw4424144kaaaw5525155kaaaw6626166kaaaw7727177kaaawstop wordFigure 2.
Modified Viterbi search ?
stop-word treatmentThe approach of search with fringes coupled withan iterative correction process is both very effi-cient and very effective.
In each iteration, thesearch space is much reduced.
Changes such as logwood  dog food are avoided because they can notbe made in one iteration and there are no interme-diate corrections conditionally more probable thanthe left-hand-side query (log wood) and less prob-able than the right-hand-side query (dog food).An iterative process is prone to other types ofproblems.
Short queries can be iteratively trans-formed into other un-related queries; therefore,changing such queries is restricted additionally inour system.
Another restriction we imposed is tonot allow changes of in-lexicon words in the firstiteration, so that easy-to-fix unknown-word errorsare handled before any word substitution error.6 EvaluationFor this work, we are concerned primarily withrecall because providing good suggestions for mis-spelled queries can be viewed as more importantthan abstaining to provide alternative query sug-gestions for valid queries as long as these sugges-tions are reasonable (for example, suggestingcowboy ropes for cowboy robes may not have ma-jor cost to a user).
A real system would have acomponent that decides whether to surface a spell-ing suggestion based on where we want to be onthe ROC curve, thus negotiating between precisionand recall.One problem with evaluating a spell checker de-signed to correct search queries is that evaluationdata is hard to get.
Even if the system were usedby a search engine and click-through informationwere available, such information would provideonly a crude measure of precision and would notallow us to measure recall, by capturing only casesin which the corrections proposed by that particu-lar speller are clicked on by the users.We performed two different evaluations of theproposed system.4 The first evaluation was doneon a test set comprising 1044 unique randomlysampled queries from a daily query log, whichwere annotated by two annotators.
Their inter-agreement rate was 91.3%.
864 of these querieswere considered valid by both annotators; for theother 180, the annotators provided spelling correc-tions.
The overall agreement of our system withthe annotators was 81.8%.
The system suggested131 alternative queries for the valid set, counted asfalse positives, and 156 alternative queries for themisspelled set.
Table 2 shows the accuracy ob-tained by the proposed system and results from anablation study where we disabled various compo-nents of the system, to measure their influence onperformance.4The test data sets can be downloaded fromhttp://research.microsoft.com/~silviu/WorkAll queries Valid MisspelledNr.
queries 1044 864 180Full system 81.8 84.8 67.2No lexicon 70.3 72.2 61.1No query log 77.0 82.1 52.8All edits equal 80.4 83.3 66.1Unigrams only 54.7 57.4 41.71 iteration only 80.9 88.0 47.22 iterations only 81.3 84.4 66.7No fringes 80.6 83.3 67.2Table 2.
Accuracy of various instantiations of the systemBy completely removing the trusted lexicon, theaccuracy of the system on misspelled queries(61.1%) was higher than in the case of only usinga trusted lexicon and no query log data (52.8%).
Itcan also be observed that the language model builtusing query logs is by far more important than thechannel model employed: using a poorer charactererror model by setting all edit weights equal didnot have a major impact on performance (66.1%recall), while using a poorer language model thatonly employs unigram statistics from the querylogs crippled the system (41.7% recall).
Anotherinteresting aspect is related to the number of itera-tions.
Because the first iteration is more conserva-tive than the following iterations, using only oneiteration led to fewer false positives but also to amuch lower recall (47.2%).
Two iterations weresufficient to correct most of the misspelled queriesthat the full system could correct.
While fringesdid not have a major impact on recall, they helpedavoid false positives (and had a major impact onspeed).81.281.681.880.769.468.967.266.165707580851 month 2 months 3 months 4 monthsAll queriesMispelled queriesFigure 3.
Accuracy and recall as functions of the number ofmonthly query logs used to train the language modelFigure 3 shows the performance of the full systemas a function of the number of monthly query logsemployed.
While both the total accuracy and therecall increased when using 2 months of data in-stead of 1 month, by using more query log data (3and 4 month), the recall (or accuracy on mis-spelled queries) still improves but at the expenseof having more false positives for valid queries,which leads to an overall slightly smaller accuracy.A post-analysis of the results showed that the sys-tem suggested in many cases reasonable correc-tions but different from the gold standard ones.Many false positives could be considered reason-able suggestions, although it is not clear whetherthey would have been helpful to the users (e.g.2002 kawasaki ninja zx6e  2002 kawasaki ninjazx6r was counted as an error, although the sugges-tion represents a more popular motorcycle model).In the case of misspelled queries in which theuser?s intent was not clear, the suggestion made bythe system could be considered valid despite thefact that it disagreed with the annotators?
choice(e.g.
gogle  google instead of the gold standardcorrection goggle).To address the problems generated by the fact thatthe annotators could only guess the user intent, weperformed a second evaluation, on a set of queriesrandomly extracted from query log data, by sam-pling pairs of successive queries ),( 21 qq  sent bythe same users in which the queries differ fromone another by an un-weighted edit distance of atmost 1+(len( 1q )+len( 2q ))/10 (i.e.
allow a pointchange for every 5 letters).
We then presented thelist to human annotators who had the option to re-ject a pair, choose one of the queries as a valid cor-rection of the other, or propose a correction forboth when none of them were valid but the in-tended valid query was easy to guess from the se-quence, as in example 3 below:(audio flie, audio file)  audio file(bueavista, buena vista)  buena vista(carrabean nooms, carrabean rooms)  caribbean roomsTable 3 shows the performance obtained by dif-ferent instantiations of the system on this set.Full system 73.1No lexicon 59.2No query log 44.9All edits equal 69.9Unigrams only 43.01 iteration only 45.52 iterations only 68.2No fringes 71.0Table 3.
Accuracy of the proposed system on a set whichcontains misspelled queries that the users had reformulatedThe main system disagreed 99 times with the goldstandard, in 80 of these cases suggesting a differ-ent correction.
40 of the corrections were not ap-propriate (e.g.
porat was corrected by our systemto pirate instead of port in chinese porat alsocalled xiamen), 15 were functionally equivalentcorrections given our target search engine (e.g.audio flie  audio files instead of audio file), 17were different valid suggestions (e.g.
bellsouthlphone isting  bellsouth phone listings instead ofbellsouth telephone listing), while 8 representedgold standard errors (e.g.
the speller correctly sug-gested brandy sniffters  brandy snifters insteadof brandy sniffers).
Out of 19 cases in which thesystem did not make a suggestion, 13 were genu-ine errors (e.g.
paul waskiewiscz with the correctspelling paul waskiewicz), 4 were cases in whichthe original input was correct, although differentfrom the user?s intent (e.g.
cooed instead of coed)and 2 were gold standard errors (e.g.
commandos 3walkthrough had the wrong correction commando3 walkthrough, as this query refers to a popularvideogame called ?commandos 3?
).Differences Gold std errors Format  Diff.
valid Real Errors80+19 8+2 15+0 17+4 40+13The above table shows a synthesis of this erroranalysis on the second evaluation set.
The firstnumber in each column refers to a precision error(i.e.
the speller suggested something different thanthe gold standard), while the second refers to arecall error (i.e.
no suggestion).As a result of this error analysis, we could argua-bly consider that while the agreement with thegold standard experiments are useful for measur-ing the relative importance of components, they donot give us an absolute measure of  system useful-ness/accuracy.Agreement Correctness Precision Recall73.1 85.5 88.4 85.4In the above table, we consider correctness as therelative number of times the suggestion made bythe speller was correct or reasonable; precisionmeasures the number of correct suggestions in thetotal number of spelling suggestions made by thesystem; recall is computed as the relative numberof correct/reasonable suggestions made when suchsuggestions were needed.As an additional verification and to confirm thedifficulty of the test queries, we sent a set of themto Google and observed that Google speller?sagreement with the gold standard was slightlylower than our system?s agreement.7 ConclusionTo our knowledge, this paper is the first to show asuccessful attempt of using the collective knowl-edge stored in search query logs for the spellingcorrection task.
We presented a technique to minethis extremely informative but very noisy resourcethat actually exploits the errors made by people asa way to do effective query spelling correction.
Adirection that we plan to investigate is the adapta-tion of such a technique to the general purposespelling correction, by using statistics from bothquery-logs and large office document collections.AcknowledgementsWe wish to thank Robert Ragno and Robert Roun-thwaite for helpful comments and discussions.ReferencesBrill, E. and R. Moore.
2000.
An improved error model fornoisy channel spelling correction.
In Proceedings of the ACL2000, pages 286-293.Cherkassky, V., N. Vassilas, G.L.
Brodt, R.A. Wagner, andM.J.
Fisher.
1974.
The string to string correction problem.
InJournal of ACM, 21(1):168-178.Damerau, F.J. 1964.
A technique for computer detection andcorrection of spelling errors.
In Communications of ACM,7(3):171-176.Garside, R., G. Leech and G. Sampson.
1987.
Computationalanalysis of English: a corpus-based approach, Longman.Golding, A.R.
1995.
A Bayesian hybrid method for context-sensitive spelling correction.
In Proceedings of the Work-shop on Very Large Corpora, pages 39-53.Golding, A.R.
and D. Roth.
1996.
Applying winnow to con-text-sensitive spelling correction.
In Proceedings of ICML1996, pages 182-190.Heidorn, G.E., K. Jensen, L.A. Miller, R.J. Byrd and M.S.Chodorow.
1982.
The EPISTLE text-critiquing system.
InIBM Systems Journal, 21(3):305-326.Jurafsky, D. and J.H.
Martin.
2000.
Speech and languageprocessing.
Prentice-Hall.Kernighan, M., K. Church, and W. Gale.
1990.
A spellingcorrection program based on a noisy channel model.
In Pro-ceedings of COLING 1990.Kukich, K. 1992.
Techniques for automatically correctingwords in a text.
In Computing Surveys, 24(4):377-439.Mays, E., F.J. Damerau and R.L.
Mercer.
1991.
Context-based spelling correction.
In Information Processing andManagement, 27(5):517-522.Mangu, L. and E. Brill.
1997.
Automatic rule acquisition forspelling correction.
In Proceedings of the ICML 1997, pages734-741.McIlroy, M.D.
1982.
Development of a spelling list.
In J-IEEE-TRANS-COMM, 30(1);91-99.Peterson, J.L.
1980.
Computer programs for spelling correc-tion: an experiment in program design.
Springer-Verlag.Rieseman, E.M. and A.R.
Hanson.
1974.
A contextual post-processing system for error correction using binary n-grams.In IEEE Transactions on Computers, 23(5):480-493.Toutanova, K. and R. C. Moore.
2002.
Pronunciation Model-ing for Improved Spelling Correction.
In Proceedings of theACL 2002.pages 141-151.Wittgenstein, L. 1968.
Philosophical Investigations.
BasilBlackwell, Oxford.
