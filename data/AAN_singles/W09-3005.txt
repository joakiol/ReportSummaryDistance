Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35?43,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPBy all these lovely tokens...?Merging Conflicting TokenizationsChristian Chiarcos, Julia Ritz and Manfred StedeSonderforschungsbereich 632 ?Information Structure?University of PotsdamKarl-Liebknecht-Str.
24-25, 14476 Golm, Germany{chiarcos|julia|stede}@ling.uni-potsdam.deAbstractGiven the contemporary trend to modularNLP architectures and multiple annotationframeworks, the existence of concurrenttokenizations of the same text representsa pervasive problem in everyday?s NLPpractice and poses a non-trivial theoreticalproblem to the integration of linguistic an-notations and their interpretability in gen-eral.
This paper describes a solution forintegrating different tokenizations using astandoff XML format, and discusses theconsequences for the handling of querieson annotated corpora.1 Motivation1.1 Tokens: Functions and goalsFor most NLP tasks and linguistic annotations,especially those concerned with syntax (part-of-speech tagging, chunking, parsing) and the inter-pretation of syntactic structures (esp., the extrac-tion of semantic information), tokens representthe minimal unit of analysis: words (lexemes,semantic units, partly morphemes) on the onehand and certain punctuation symbols on the otherhand.
From a corpus-linguistic perspective, tokensalso represent the minimal unit of investigation,the minimal character sequence that can be ad-dressed in a corpus query (e.g.
using search toolslike TIGERSearch (Ko?nig and Lezius, 2000) orCWB (Christ, 1994)).
Tokens also constitute thebasis for ?word?
distance measurements.
In manyannotation tools and their corresponding formats,the order of tokens provides a timeline for thesequential order of structural elements (MMAX(Mu?ller and Strube, 2006), GENAU (Rehm et al,2009), GrAF (Ide and Suderman, 2007), TIGERXML (Ko?nig and Lezius, 2000)).
In several multi-?Taken from the poem September by Helen Hunt Jackson.layer formats, tokens also define the absolute po-sition of annotation elements, and only by refer-ence to a common token layer, annotations fromdifferent layers can be related with each other(NITE (Carletta et al, 2003), GENAU).Thus, by their function, tokens have the fol-lowing characteristics: (i) tokens are totally or-dered, (ii) tokens cover the full (annotated portionof the) primary data, (iii) tokens are the smallestunit of annotation, and (iv) there is only one sin-gle privileged token layer.
The last aspect is es-pecially relevant for the study of richly annotateddata, as an integration and serialization of anno-tations produced by different tools can be estab-lished only by reference to the token layer.
Froma corpus-linguistic perspective, i.e., when focus-ing on querying of annotated corpora, tokens needto be well-defined and all information annotatedto a particular text is to be preserved without anycorruption.
We argue that for this purpose, char-acteristic (iii) is to be abandoned, and we will de-scribe the data format and an algorithm for merg-ing different tokenizations and their respective an-notations.Our goal is a fully automated merging of anno-tations that refer to different tokenizations (hence-forth T ?
and T ?)
of the same text.
We regard thefollowing criteria as crucial for this task:Information preservation.
All annotations ap-plied to the original tokenizations should be pre-served.Theoretically well-defined notion of token.
Itshould be possible to give a plausible list of posi-tive criteria that define character sequences as to-kens.
Knowledge about the token definition is es-sential for formulating queries for words, e.g.
in acorpus search interface.Integrative representation.
All annotations thatare consistent with the merged tokenization shouldrefer to the merged tokenization.
This is necessaryin order to query across multiple annotations orig-35inating from different annotation layers or tools.Unsupervised merging.
The integration of con-flicting tokenizations should not require manualinterference.1.2 TokenizationTokenization is the process of mapping sequencesof characters to sequences of words (cf.
Guo1997).
However, different research questions orapplications induce different conceptions of theterm ?word?.
For a shallow morphosyntactic anal-ysis (part of speech tagging), a ?simple?
tokeniza-tion using whitespaces and punctation symbols asdelimiters seems acceptable for the examples in(1).
A full syntactic analysis (parsing), however,could profit from the aggregation of complex nom-inals into one token each.
(1) a. department storeb.
Herzog-von der Heide1c.
Red Cross/Red Crescent movementSimilarly, examples (2a) and (2b) can be ar-gued to be treated as one token for (mor-pho)syntactic analyses, respectively.
Despite in-tervening whitespaces and punctuation symbols,they are complex instances of the ?classical?
part-of-speech adjective.
For certain semantic analysessuch as in information extraction, however, it maybe useful to split these compounds in order to ac-cess the inherent complements (E 605, No.
22).
(2) a. E 605-intoxicatedb.
No.
22-ratedFinally, (3) illustrates a morphology-based tok-enization strategy: the principle of splitting atmorpheme boundaries (Marcus et al, 1993, PTB)(token boundaries represented by square brack-ets).
Morphological tokenization may help distri-butional (co-occurrence-based) semantics and/orparsing; however, the resulting tokens might beargued as being less intuitive to users of a corpussearch tool.
(3) a.
[Mitchell][?s], [they][?ve], [do][n?t]b.
[wo][n?t], [ca][n?t], [ai][n?t]These examples show that different applications(tagging, parsing, information extraction) and thefocus on different levels of description (morphol-ogy, syntax, semantics) require specialized tok-enization strategies.
When working with multiple1Double surname consisting of Herzog and von der Heide.tools for standard NLP tasks, thus, it is the normrather than the exception that they disagree in theirtokenization, as shown in ex.
(4).
(4) doesn?ta.
[does][n?t] (Marcus et al, 1993, PTB)b.
[doesn][?
][t] (Brants, 2000, TnT)When creating a corpus that is annotated at multi-ple levels and/or using several tools, different tok-enizations are not always avoidable, as some tools(automatic NLP tools, but also tools for manualannotation) have integrated tokenizers.
Anotherchallenge is the representation of token bound-aries.
Commonly, token boundaries are repre-sented by a line break (?\n?)
or the whitespace?character?
(?
?)
?
in which case token-internalwhitespaces are replaced, usually by an under-score (?
?)
?, thereby corrupting the original data.This practice makes reconciling/merging the dataa difficult enterprise.Given this background, we suggest an XML-based annotation of token boundaries, such thattoken boundaries are marked without affecting theoriginal primary data.
In a straightforward XMLmodel, tokens are represented by XML elementsenclosing primary text slices (c.f.
the BNC encod-ing scheme (Burnard, 2007)).
However, treatingtokens as spans of text by means of the XML hier-archy is impossible for tokenization conflicts as in(4.a) and (4.b).2 Conflicting tokenizations:Straightforward strategiesBy ?straightforward strategies?, we mean ap-proaches that aim to preserve the definition of to-kens as atomic, minimal, unambiguous units ofannotation when unifying different tokenizations(henceforth T ?
and T ?)
of the same text.
By ?un-supervised straightforward strategies?, we meantokenization strategies that operate on the primarydata only, without consulting external resourcessuch as dictionaries or human expertise.Unsupervised straightforward strategies to thetask include:1. no merging In a conservative approach, wecould create independent annotation projects forevery tokenization produced, and thus representall tokenizations independently.
This, however,rules out any integration or combined evaluationof annotations to T ?
and annotations to T ?.362.
normalization Adopt one of the source tok-enizations, say T ?, as the ?standard?
tokenization.Preserve only the information annotated to T ?
thatis consistent with T ?.
Where tokenization T ?
de-viates from T ?, all annotations to T ?
are lost.23.
maximal tokens For every token boundaryin T ?
that is also found in T ?, establish a tokenboundary in the merged tokenization (cf.
Guo?s1997 ?critical tokenization?).
However, with to-kens assumed to be the minimal elements of anno-tation, we lose linguistic analyses of fine-grainedtokens.
With respect to (4.a) and (4.b), the max-imal token would be the whole phrase doesn?t.Again, this results in a loss of information, as allannotations applied to does, doesn, n?t, ?
and t re-fer to units that are smaller than the resulting to-ken.4.
maximal common substrings For everytoken boundary in T ?
or T ?, establish a tokenboundary, thereby producing minimal tokens:one token for every maximal substring sharedbetween T ?
and T ?
(cf.
Guo?s 1997 ?shortesttokenization?).
By defining the original tokens(?supertokens?)
as annotations spanning overtokens, all annotations are preserved.
However,the concept of ?token?
loses its theoretical motiva-tion; there is no guarantee that maximal commonsubstrings are meaningful elements in any sense:The maximum common substring tokenizationof 4.a and 4.b is [does][n][?
][t], but [n] is nota well-defined token.
It is neither defined withrespect to morphology (like PTB tokens) nor isit motivated from orthography (like TnT tokens),but it is just the remainder of their intersection.As shown in Table 1, none of the strategiessketched above fulfills all criteria identified in Sec-tion 1.1: Avoiding a merging process counteractsdata integration; token normalization and maximaltokens violate information preservation, and maxi-mal common substrings violate the requirement tospecify a theoretically well-defined notion of to-ken.As an alternative, we propose a formalism forthe lossless integration and representation of con-2Alternatively, transformation rules to map annotationsfrom T ?
to T ?
would have to be developed.
This does, how-ever, not guarantee information preservation, and, addition-ally, it requires manual work, as such transformations areannotation-specific.
Thus, it is not an option for the fullyautomated merging of tokenizations.Table 1: Deficits of ?straightforward?
merging ap-proachesno normalize max.
max.
commonmerge tokens substringsinformation preservation+ ?
?
+well-defined tokens+ + (?)
?integrative?
+ + +unsupervised(+) + + +flicting tokenizations by abandoning the assump-tion that tokens are an atomic, primitive con-cept that represents the minimal unit of annota-tion.
Rather, we introduce annotation elementssmaller than the actual token ?
so-called termi-nals or terms for short ?
that are defined accord-ing to the maximum common substrings strategydescribed above.Then, tokens are defined as nodes that spanover a certain range of terms similar to phrasenodes that dominate other nodes in syntax annota-tions.
The representation of conflicting tokeniza-tions, then, requires a format that is capable toexpress conflicting hierarchies.
For this purpose,we describe an extension of the PAULA format, ageneric format for text-oriented linguistic annota-tions based on standoff XML.3 Conflicting tokenizations in thePAULA format3.1 Annotation structures in PAULA 1.0The PAULA format (Dipper, 2005; Dipper andGo?tze, 2005) is a generic XML format, used as apivot format in NLP pipelines (Stede et al, 2006)and in the web-based corpus interface ANNIS(Chiarcos et al, 2008).
It uses standoff XML rep-resentations, and is conceptually closely related tothe formats NITE XML (Carletta et al, 2003) andGraF (Ide and Suderman, 2007).PAULA was specifically designed to support thelossless representation of different types of text-oriented annotations (layer-based/timeline anno-tations, hierarchical annotations, pointing rela-tions), optimized for the annotation of multiplelayers, including conflicting hierarchies and sim-ple addition/deletion routines for annotation lay-ers.
Therefore, primary data is stored in a separate37Table 2: PAULA 1.0 data typesnodes (structural units of annotation)token character spans in the primary data that form the basisfor higher-level annotationmarkable (spans of) token(s) that can be annotated with lin-guistic information.
Markables represent flat, layer-basedannotations defined with respect to the sequence of tokensas a general timeline.struct hierarchical structures (DAGs or trees) are formed byestablishing a dominance relation between a struct (e.g.,a phrase) node as parent, and tokens, markables, or otherstruct nodes as children.edges (relational units of annotation, connecting tokens,markables, structs)dominance relation directed edge between a structand its childrenpointing relations directed edge between nodes ingeneral (tokens, markables, structs)labels (annotations: node or edge labels)features represent annotations attached to a particular(structural or relational) unit of annotationfile.
Multiple annotations are also stored in sepa-rate files to avoid interference between concurrentannotations.
Annotations refer to the primary dataor to other annotations by means of XLinks andXPointers.As types of linguistic annotation, we distinguishnodes (token, markable, struct), edges (dominanceand pointing relations) and labels (annotations), assummarized in Table 2.
Each type of annotationis stored in a separate file, so that competing orambiguous annotations can be represented in anencapsulated way.PAULA 1.0 is already sufficiently expressive forcapturing the data-heterogeneity sketched above,including the representation of overlapping seg-ments, intersecting hierarchies, and alternative an-notations (e.g., for ambiguous annotations), butonly for annotations above the token level.
Fur-ther, PAULA 1.0 relies on the existence of aunique layer of non-overlapping, atomic tokens asminimal units of annotation: For all nodes, theirposition and sequential order is defined with re-spect to the absolute position of tokens that theycover; and for the special case of markables, theseare defined solely in terms of their token range.Finally, PAULA 1.0 tokens are totally ordered,they cover the (annotated) primary data com-pletely, and they are non-overlapping.
Only onthis basis, the extension and (token-)distance ofannotated elements can be addressed; and onlyby means of unambiguous reference, informationfrom different layers of annotation can be com-bined and evaluated.3.2 Introducing terminal nodesIn our extension of the PAULA format, we in-troduce the new concept of term nodes: atomicterminals that directly point to spans of primarydata.
Terms are subject to the same constraints astokens in PAULA 1.0 (total order, full coverage,non-overlapping).
So, terms can be used in placeof PAULA 1.0 tokens to define the extension andposition of super-token level and sub-token levelannotation elements.Markables are then defined with respect to(spans of) terminal nodes rather than tokens, suchthat alternative tokenizations can be expressed asmarkables in different layers that differ in their ex-tensions.Although terms adopt several functions for-merly associated with tokens, a privileged tokenlayer is still required: In many query languages,including ANNIS-QL (Chiarcos et al, 2008), to-kens define the application domain of regular ex-pressions on the primary data.
More impor-tantly, tokens constitute the basis for conventional(?word?)
distance measurements and (?word?
)coverage queries.
Consequently, the constraintson tokens (total order, full coverage and absenceof overlap) remain.The resulting specifications for structural unitsof annotation are summarized in Table 3.
Distin-guishing terminal elements and re-defining the to-ken layer as a privileged layer of markables al-lows us to disentangle the technical concept of?atomic element?
and ?token?
as the convention-ally assumed minimal unit of linguistic analysis.3.3 A merging algorithmIn order to integrate annotations on tokens, it isnot enough to represent two tokenizations side byside with reference to the same layer of terminalnodes.
Instead, a privileged token layer is to be es-tablished and it has to be ensured that annotationscan be queried with reference to the token layer.38Table 3: PAULA extensions: revised node typesterms specify character spans in the primary datathat form the basis for higher-level annota-tionmarkable defined as above, with terms taking theplace of tokensstructs defined as above, with terms taking theplace of tokenstokens sub-class of structs that are non-overlapping, arranged in a total order,and cover the full primary dataThen, all annotations whose segmentation is con-sistent with the privileged token layer are directlylinked with tokens.Alg.
3.1 describes our merging algorithm, andits application to the four main cases of conflict-ing tokenization is illustrated in Figure 1.3 Thefollowing section describes its main characteris-tics and the consequences for querying.4 DiscussionAlg.
3.1 produces a PAULA project with one sin-gle tokenization.
So, it is possible to define queriesspanning across annotations with originally differ-ent tokenization:Extension and precedence queries aretokenization-independent: Markables refer tothe term layer, not the tok layer, structs also(indirectly) dominate term nodes.Dominance queries for struct nodes and tokensyield results whenever the struct node dominatesonly nodes with tok-compatible source tokeniza-tion: Structs dominate tok nodes wherever theoriginal tokenization was consistent with theprivileged tokenization tok (case A and C in Fig.1).Distance queries are defined with respect to thetok layer, and are applicable to all elements thatare are defined with reference to the tok layer (infigure 1: tok?a, tok?a, tok?b, tok?b in case A; tokabin case B; toka, tokb, tokab in case C; tokab, tokcin case D).
They are not applicable to elementsthat do not refer to the tok layer (B: toka, tokb; D:toka, tokbc).3Notation: prim ?
primary data / tok, term ?
annota-tion layers / t ?
L ?
t is a node on a layer L / a..b ?
con-tinuous span from tok/term a to tok/term b / a, b ?
list oftok/term/markable nodes a, b / t = [a] ?
t is a node (struct,markable, tok) that points to a node, span or list aThe algorithm is unsupervised, and the tokenconcept of the output tokenization is well-definedand consistent (if one of the input tokenizationsis adopted as target tokenization).
Also, as shownbelow, it is integrative (enabling queries across dif-ferent tokenizations) and information-preserving(reversible).4.1 Time complexityAfter a PAULA project has been created, the timecomplexity of the algorithm is quadratic with re-spect to the number of characters in the primarydata n. This is due to the total order of tokens:Step 2 and 3.a are applied once to all original to-kens from left to right.
Step 5 can be reformulatedsuch that for every terminal node, the relationshipbetween the directly dominating tok?
and tok?
ischecked.
Then, Step 5 is also in O(n).
In terms ofthe number of markables m, the time complexityin Step 3.b is in O(n m): for every markable, thecorresponding term element is to be found, tak-ing at most n repositioning operations on the termlayer.
Assuming that markables within one layerare non-overlapping4 and that the number of lay-ers is bound by some constant c5, then m ?
n c,so that 3.b is in O(n?
c).For realistic scenarios, the algorithm is thusquadratic.4.2 ReversibilityThe merging algorithm is reversible ?
and, thus,lossless ?
as shown by the splitting algorithm inAlg.
3.2.
For reasons of space, the correctnessof this algorithm cannot be demonstrated here, butbroadly speaking, it just removes every node thatcorresponds to an original token of the ?other?
tok-enization, plus every node that points to it, so thatonly annotations remain that are directly appliedto the target tokenization.4.3 Querying merged tokenizationsWe focus in this paper on the merging of analy-ses with different tokenizations for the purpose ofusers querying a corpus across multiple annota-4Although PAULA supports overlapping markableswithin one single layer, even with identical extension, this isa reasonable assumption: In practice, overlapping markableswithin one single layer are rare.
More often, there is even alonger sequence of primary data between one markable of aparticular layer and the next.
In our experience, such ?gaps?occur much more often than overlapping markables.5Again, this is a practical simplication.
Theoretically, thenumber of layers is infinite.39Alg.
3.1 Merging different tokenizations0.
assume that we have two annotations analysis?
and analysis?
for the same primary data, but with different tokenizations1.
create PAULA 1.0 annotation projects for analysis?
and analysis?
with primary data files prim?
and prim?
and tokenlayers tok?
and tok?
respectively.2.
harmonize primary dataif prim?
equals prim?, then(i) rename prim?
to prim(ii) set al references in analysis?
from prim?
to prim(iii) create a new annotation project analysis by copying prim and all annotation layers from analysis?
and analysis?otherwise terminate with error msg3.
harmonize terminal nodescreate a new annotation layer term, then(a) for all overlapping tokens t?
?
tok?
and t?
?
tok?
: identify the maximal common substrings of t?
and t?for every substring s, create a new element terms pointing to the corresponding character span in the primary datafor every substring s, redefine t?
and t?
as markables referring to terms(b) redefine markable spans as spans of terminal nodesfor every token t = [terms?
..terms? ]
?
tok?
?
tok?
and every markable m = [w..xty..z]: set m =[w..xterms?
..terms?y..z]4. select token layerrename tok?
to tok, or rename tok?
to tok, (cf.
the normalization strategy in Sect.
2) orrename term to tok (cf.
the minimal tokens strategy in Sect.
2)5. token integrationfor every original token ot = [a..b] ?
(tok?
?
tok?)
\ tok:if there is a token t ?
tok such that t = [a..b], then define ot as a struct with ot = [t], elseif there are tokens t?, .., tn ?
tok such that t?..tn form a continuous sequence of tokens and t?
= [a..x] and tn = [y..b],then define ot as a struct such that ot = [t?, .., tn],otherwise: change nothingFigure 1: Merging divergent tokenizations40Alg.
3.2 Splitting a PAULA annotation projectwith two different tokenizations0.
given a PAULA annotation project analysis with tokenlayer tok, terminal layer term, and two layers l?
and l?
(that may be identical to term or tok) that convey theinformation of the original token layers tok?
and tok?1.
create analysis?
and analysis?
as copies of analysis2.
if l?
represents a totally ordered, non-overlapping list ofnodes that cover the primary data completely, then modifyanalysis?:a.
for every node in l?
: substitute references to tok?
byreferences to term?b.
remove l?
from analysis?c.
if l?
6= tok?, remove tok?
from analysis?d.
for every annotation element (node/relation) e inanalysis?
that directly or indirectly points to anothernode in analysis?
that is no longer present, remove efrom analysis?e.
remove every annotation layer from analysis?
thatdoes not contain an annotation elementf.
for every markable in l?
: remove references to term?,define the extension of l?
nodes directly in terms ofspans of text in prim?g.
if l?
6= term?, remove term?3.
perform step 2. for l?
and analysis?tion layers.
Although the merging algorithm pro-duces annotation projects that allow for queries in-tegrating annotations from analyses with differenttokenization, the structure of the annotations is al-tered, such that the behaviour of merged and un-merged PAULA projects may be different.
Obvi-ously, token-level queries must refer to the priv-ileged tokenization T ?.
Operators querying forthe relative precedence or extension of markablesare not affected: in the merged annotation project,markables are defined with reference to the layerterm: originally co-extensional elements E?
andE?
(i.e.
elements covering the same tokens in thesource tokenization) will also cover the same ter-minals in the merged project.
Distance operators(e.g.
querying for two tokens with distance 2, i.e.with two tokens in between), however, will oper-ate on the new privileged tokenization, such thatresults from queries on analysis may differ fromthose on analysis?.
Dominance operators arealso affected, as nodes that directly dominated atoken in analysis?
or analysis?
now indirectlydominate it in analysis, with a supertoken as anintermediate node.Alg.
3.3 Iterative merging: modifications of Alg.3.1, step.3if analysis?
has a layer of terminal nodes term?, then letT ?
= term?, otherwise T ?
= tok?if analysis?
has a layer of terminal nodes term?, then letT ?
= term?, otherwise T ?
= tok?create a new annotation layer term, then1.
for all overlapping terminals/tokens t?
?
T ?
and t?
?T ?
: identify the maximal common substrings of t?
andt?for every substring s, create a new element termspointing to the corresponding character span in the pri-mary datafor every substring s, redefine t?
and t?
as markablesreferring to terms2.
redefine markable spans as spans of terminal nodesfor every node t = [terms?
..terms? ]
?
T ?
?
T ?and every markable m = [w..xty..z]: setm = [w..xterms?
..terms?y..z]3. for all original terminals t ?
T ?
?T ?
: if t is not directlypointed at, remove t from analysisAccordingly, queries applicable to PAULAprojects before the merging are not directly appli-cable to merged PAULA projects.
Users are to beinstructed to keep this in mind and to be aware ofthe specifications for the merged tokenization andits derivation.65 Extensions5.1 Merging more than two tokenizationsIn the current formulation, Alg.
3.1 is applied totwo PAULA 1.0 projects and generates extendedPAULA annotation projects with a term layer.The algorithm, however, may be applied itera-tively, if step 3 is slightly revised, such that ex-tended PAULA annotation projects can also bemerged, see Alg.
3.3.5.2 Annotation integrationThe merging algorithm creates a struct node forevery original token.
Although this guarantees re-versibility, one may consider to remove such re-dundant structs.
Alg.
3.4 proposes an optionalpostprocessing step for the merging algorithm.This step is optional because these operations are6The information, however, is preserved in the format andmay be addressed by means of queries that, for example, op-erate on the extension of terminals.41Alg.
3.4 Annotation integration: Optional post-processing for merging algorithm6.a.
remove single-token supertokenfor every original token ot = [t] ?
tok?
?
tok?
witht ?
tok: replace all references in analysis to ot byreferences to t, remove ot6.b.
merging original token layers tok?
and tok?
(iftok?
6= tok and tok?
6= tok)define new ?super token?
layer stok.for every ot ?
tok?
?
tok?
:if ot = [t] for some t ?
tok, then see 6.aif ot = [t?, .., tn] for some t?, .., tn ?
tok, andthere is ot?
= [t?, .., tn] ?
tok?
?
tok?
?
stok,then replace all references in analysis to ot?
byreferences to ot, move ot to layer stok, removeot?
from analysismove all remaining ot ?
tok?
?
tok?
to stok, removelayers tok?
and tok?6.c.
unify higher-level annotationsfor every markable mark?
= [term?..termn] andterm?, .., termn ?
term:if there is a markable mark?
in analysis suchthat mark?
= [term?..termn], then replace allreferences in analysis to mark?
by references tomark?, remove mark?for every struct struct?
= [c?, .., cn] that covers ex-actly the same children as another struct struct?
=[c?, .., cn], replace all references to struct?
by refer-ences to struct?, remove struct?destructive: We lose the information about the ori-gin (analysis?
vs.
analysis?)
of stok elementsand their annotations.6 Summary and Related ReasearchIn this paper, we describe a novel approach for theintegration of conflicting tokenizations, based onthe differentiation between a privileged layer oftokens and a layer of atomic terminals in a stand-off XML format: Tokens are defined as structuredunits that dominate one or more terminal nodes.Terminals are atomic units only within the re-spective annotation project (there is no unit ad-dressed that is smaller than a terminal).
By iter-ative applications of the merging algorithm, how-ever, complex terms may be split up in smallerunits, so that they are not atomic in an absolutesense.Alternatively, terms could be identified a prioriwith the minimal addressable unit available, i.e.,characters (as in the formalization of tokens ascharspans and charseqs in the ACE informationextraction annotations, Henderson 2000).
It is notclear, however, how a character-based term defini-tion would deal with sub-character and zero exten-sion terms: A character-based definition of termsthat represent traces is possible only by corrupt-ing the primary data.7 Consequently, a character-based term definition is insufficient unless we re-strict ourselves to a particular class of languages,texts and phenomena.The role of terminals can thus be compared totimestamps: With reference to a numerical time-line, it is always possible to define a new eventbetween two existing timestamps.
Formats specif-ically designed for time-aligned annotations, e.g.,EXMARaLDA (Schmidt, 2004), however, typi-cally lack a privileged token layer and a formalconcept of tokens.
Instead, tokens, as well aslonger or shorter sequences, are represented asmarkables, defined by their extension on the time-line.Similarly, GrAF (Ide and Suderman, 2007), al-though being historically related to PAULA, doesnot have a formal concept of a privileged tokenlayer in the sense of PAULA.8 We do, however,assume that terminal nodes in GrAF can be com-pared to PAULA 1.0 tokens.For conflicting tokenizations, Ide and Suderman(2007) suggest that ?dummy?
elements are definedcovering all necessary tokenizations for controver-sially tokenized stretches of primary data.
Suchdummy elements combine the possible tokeniza-tions for strategies 1 (no merging) and 3 (maxi-mal tokens), so that the information preservationdeficit of strategy 3 is compensated by strategy 1,and the integrativity deficit of strategy 1 is com-pensated by strategy 3 (cf.
Table 1).
However, to-kens, if defined in this way, are overlapping andthus only partially ordered, so that distance opera-tors are no longer applicable.97Similarly, phonological units that are not expressed inthe primary data can be subject to annotations, e.g., short eand o in various Arabic-based orthographies, e.g., the Ajamiorthography of Hausa.
A term with zero extension at the po-sition of a short vowel can be annotated as having the phono-logical value e or o without having character status.8https://www.americannationalcorpus.org/graf-wiki/wiki/WikiStart#GraphModel,2009/05/089This can be compensated by marking the base segmen-tation differently from alternative segmentations.
In the ab-stract GrAF model, however, this can be represented only bymeans of labels, i.e., annotations.
A more consistent con-42Another problem that arises from the introduc-tion of dummy nodes is their theoretical status, asit is not clear how dummy nodes can be distin-guished from annotation structured on a concep-tual level.
In the PAULA formalization, dummynodes are not necessary, so that this ambiguity isalready resolved in the representation.ReferencesThorsten Brants.
2000.
TnT A Statistical Part-of-Speech Tagger.
In Proceedings of the Sixth Con-ference on Applied Natural Language ProcessingANLP-2000.
Seattle, WA.Lou Burnard (ed.).
2007.
Reference Guidefor the British National Corpus (XML Edi-tion).
http://www.natcorp.ox.ac.uk/XMLedition/URG/bnctags.html.Jean Carletta, Stefan Evert, Ulrich Heid, JonathanKilgour, Judy Robertson, and Holger Voormann.2003.
The NITE XML Toolkit: Flexible Annotationfor Multi-modal Language Data.
Behavior ResearchMethods, Instruments, and Computers 35(3), 353-363.Christian Chiarcos, Stefanie Dipper, Michael Go?tze,Ulf Leser, Anke Lu?deling, Julia Ritz, and ManfredStede.
2009.
A Flexible Framework for IntegratingAnnotations from Different Tools and Tagsets TAL(Traitement automatique des langues) 49(2).Oli Christ.
1994.
A modular and flexible architec-ture for an integrated corpus query system.
COM-PLEX?94, Budapest, Hungary.Stefanie Dipper.
2005.
XML-based Stand-off Repre-sentation and Exploitation of Multi-Level LinguisticAnnotation.
In Rainer Eckstein and Robert Tolks-dorf (eds:): Proceedings of Berliner XML Tage,pages 39-50.Stefanie Dipper and Michael Go?tze.
2005.
AccessingHeterogeneous Linguistic Data ?
Generic XML-based Representation and Flexible Visualization.
InProceedings of the 2nd Language & TechnologyConference 2005, Poznan, Poland, pages 23?30.Stefanie Dipper, Michael Go?tze.
2006.
ANNIS:Complex Multilevel Annotations in a LinguisticDatabase.
Proceedings of the 5th Workshop on NLPand XML (NLPXML-2006): Multi-DimensionalMarkup in Natural Language Processing.
Trento,Italy.Jin Guo.
1997.
Critical Tokenization and its Proper-ties, Computational Linguistic, 23(4), pp.569-596.ception would encode structural information on the structurallevel, and only linguistic annotation and metadata on the con-tents level.John C. Henderson.
2000.
A DTD for Reference KeyAnnotation of EDT Entities and RDC Relationsin the ACE Evaluations (v. 5.2.0, 2000/01/05),http://projects.ldc.upenn.edu/ace/annotation/apf.v5.2.0.dtd (2009/06/04)Nancy Ide and Keith Suderman.
2007.
GrAF: AGraph-based Format for Linguistic Annotations.
InProceedings of the Linguistic Annotation Work-shop,held in conjunction with ACL 2007, Prague,June 28-29, 1-8.Esther Ko?nig and Wolfgang Lezius.
2000.
A descrip-tion language for syntactically annotated corpora.In: Proceedings of the COLING Conference, pp.1056-1060, Saarbru?cken, Germany.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn treebank.
Computa-tional Linguistics 19, pp.313-330.Christoph Mu?ller and Michael Strube.
2006.
Multi-Level Annotation of Linguistic Data with MMAX2.In: S. Braun et al (eds.
), Corpus Technology andLanguage Pedagogy.
New Resources, New Tools,New Methods.
Frankfurt: Peter Lang, 197?214.Georg Rehm, Oliver Schonefeld, Andreas Witt, Chris-tian Chiarcos, and Timm Lehmberg.
2009.SPLICR: A Sustainability Platform for LinguisticCorpora and Resources.
In: Text Resources andLexical Knowledge.
Selected Papers the 9th Confer-ence on Natural Language Processing (KONVENS2008), Berlin, Sept. 30 ?
Oct. 2, 2008.
Mouton deGruyter.Helmut Schmid.
2002.
Tokenizing & Tagging.
InLu?deling, Anke and Kyto?, Merja (Hrsg.)
CorpusLinguistics.
An International Handbook.
(HSK Se-ries).
Mouton de Gryuter, BerlinThomas Schmidt.
2004.
Transcribing and Annotat-ing Spoken Language with Exmaralda.
Proceedingsof the LREC-workshop on XML Based Richly Anno-tated Corpora.
Lisbon, Portugal.
Paris: ELRA.Manfred Stede, Heike Bieler, Stefanie Dipper, andArthit Suriyawongkul.
2006.
SUMMaR: Combin-ing Linguistics and Statistics for Text Summariza-tion.
Proceedings of the 17th European Conferenceon Artificial Intelligence (ECAI-06).
pp 827-828.Riva del Garda, Italy.Ralph Weischedel, Sameer Pradhan, Lance Ramshawand Linnea Micciulla.
2006.
OntoNotes Release1.0.
Linguistic Data Consortium, Philadelphia.43
