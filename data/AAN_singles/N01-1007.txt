Unsupervised Learning of Name StructureFrom Coreference Data?Eugene CharniakBrown Laboratory for Linguistic Information ProcessingDepartment of Computer ScienceBrown University, Box 1910, Providence, RIec@cs.brown.eduAbstractWe present two methods for learning the struc-ture of personal names from unlabeled data.The rst simply uses a few implicit constraintsgoverning this structure to gain a toehold on theproblem | e.g., descriptors come before rstnames, which come before middle names, etc.The second model also uses possible coreferenceinformation.
We found that coreference con-straints on names improve the performance ofthe model from 92.6% to 97.0%.
We are in-terested in this problem in its own right, butalso as a possible way to improve named entityrecognition (by recognizing the structure of dif-ferent kinds of names) and as a way to improvenoun-phrase coreference determination.1 IntroductionWe present two methods for the unsupervisedlearning of the structure of personal names asfound in Wall Street Journal text.
More specif-ically, we consider a \name" to be a sequence ofproper nouns from a single noun-phrase (as in-dicated by Penn treebank-style parse trees).
Forexample, \Defense Secretary George W. Smith"would be a name and we would analyze it intothe components \Defense Secretary" (a descrip-tor), \George" (a rst name), \W."
(a middlename, we do not distinguish between initialsand \true" names), and \Smith" (a last name).We consider two unsupervised models forlearning this information.
The rst simply usesa few implicit constraints governing this struc-ture to gain a toehold on the problem | e.g.,descriptors come before rst names, which come?
This research was supported in part by NSF grantLIS SBR 9720368.
The author would like to thank MarkJohnson and the rest of the Brown Laboratory for Lin-guistic Information Processing (BLLIP) for general ad-vice and encouragement.before middle names, etc.
We henceforth callthis the \name" model.
The second model alsouses possible coreference information.
Typicallythe same individual is mentioned several timesin the same article (e.g., we might later en-counter \Mr.
Smith"), and the pattern of suchreferences, and the mutual constraints amongthem, could very well help our unsupervisedmethods determine the correct structure.
Wecall this the \coreference" model.
We were at-tracted to this second model as it might oer asmall example of how semantic information likecoreference could help in learning structural in-formation.To the best of our knowledge there has notbeen any previous work on learning personalstructure.
We are aware of one previous caseof unsupervised learning of lexical informationfrom possible coreference, namely that of Ge et.al.
[5] where possible pronoun coreference wasused to learn the gender of nouns.
In this case aprogram with an approximately 65% accuracyin determining the correct antecedent was usedto collect information on pronouns and theirpossible antecedents.
The gender of the pro-noun was then used to suggest the gender ofthe noun-phrase that was proposed as the an-tecedent.
The current work is quite dierentin both goal and methods, but similar in spirit.More generally this work is part of a growingbody of work on learning language-related in-formation from unlabeled corpora [1,2,3,8,9,10,11].2 Problem Definition and DataPreparationWe assume that people?s names have six (op-tional) components as exemplied in the follow-ing somewhat contrived example:Word Label Label NumberDefense descriptor 0Secretary descriptor 0Mr.
honorific 1John first-name 2W.
middle-name 3Smith last-name 4Jr.
close 5Our models make the following assumptionsabout personal names:?
all words of label l (the label number) mustoccur before all words of label l + 1?
with the exception of descriptors, a maxi-mum of one word may appear for each label?
every name must include either a rst nameor a last name?
in a loose sense, honorifics and closes are\closed classes", even if we do not knowwhich words are in the classes.
We im-plement this by requiring that words giventhese labels must appear in our dictionaryonly as proper names, and that they musthave appeared at least three times in thetraining set used for the lexicon (sections2-21 of the Penn Wall Street Journal tree-bank)Section 5 discusses these constraints and as-sumptions.The input to the name model is a noisy listof personal names.
This list is approximately85% correct; that is, about 15% of the wordsequences are not personal names, but rathernon-names, or the names of other types of en-tities.
We obtained these names by running aprogram inspired by that of Collins and Singer[4] for unsupervised learning of named entityrecognition.
This program takes as input pos-sible names plus contextual information abouttheir occurrences.
It then categorizes each nameas one of person, place, or organization.
A possi-ble name is considered to be a sequence of oneor more proper nouns immediately dominatedby a noun-phrase where the last of the propernouns is the head (rightmost noun) of the nounphrase.
We used as input to this program theparsed text found in the BLLIP WSJ 1987-89WSJ Corpus | Release 1 [6].
Because of a mi-nor error, the parser used in producing this cor-pus had a unwarranted propensity to label un-capitalized words as proper nouns.
To correctfor this we only allowed capitalized words to beconsidered proper nouns.
In section 5 we notean unintended consequence of this decision.The coreference model for our tasks is alsogiven a list of all personal names (as de-ned above) in each Wall Street Journal arti-cle.
Although the BLLIP corpus has machine-generated coreference markers, these are ig-nored.The output of both programs is an assign-ment from each name to a sequence of labels,one for each word in the name.
Performance ismeasured by the percent of words labeled cor-rectly and percent of names for which all of thelabels are correct.3 The Probability ModelsWe now consider the probability models thatunderlie our learning mechanisms.
Both modelsare generative in that they assign probabilitiesto all possible labelings of the names.
(For thecoreference model the model generates all pos-sible labelings given the proposed antecedent.
)Let ~l be a sequence of label assignments to thename ~n (a sequence of words).
For the namemodel we estimatearg max~lp(~l | ~n) = arg max~lp(~l, ~n) (1)We estimate this latter probability by assum-ing that the number of words assigned label l,n(l), is independent of which other labels haveappeared.
Our assumptions imply that with theexception of descriptor, all labels may occur zeroor one times.
We arbitrarily assume that theremay be zero to fourteen descriptors.
We thenassume that the words in the name are inde-pendent of one another given their label.
Thuswe get the following equation:p(~l, ~n) =?l=0,5p(N (l) = n(l))?i=0,n(l)p(w(i) | l)(2)Here w(i) is the ith word from ~n assigned thelabel l in ~l and N (l) is a random variable whosevalue is the number of words in the name withlabel l. To put this slightly dierently, we rstguess the number of words with each label laccording to the distribution p(N (l) = n(l)).Given the ordering constraints, this completelydetermines which words in ~n get which label.We then guess each of the words according tothe distribution p(w(i) | l).
The name modeldoes not use information concerning how ofteneach name occurs.
(That is, it implicitly as-sumes that all names occur equally often.
)We have also considered somewhat more com-plex approximations to p(~l, ~n).
See section 5.The coreference model is more complicated.Here we estimatearg max~lp(~l | ~n,~c) = arg max~lp(~l, ~n | ~c) (3)That is, for each name the program identieszero or one possible antecedent name.
It doesthis using a very crude lter.
The last wordof the proposed antecedent (unless that word is\Jr.
", in which case it looks at the second to lastword) must also appear in ~n as well.
If no suchname exists, then ~c = 4 and we estimate thedistribution according to equation 2.
If morethan one such name exists, we choose the rstone appearing in the article.Even if there is such a name, the programdoes not assume that the two names are, in fact,coreferent.
Rather, a hidden random variable Rdetermines how the two names relate.
There arethree possibilities:?
~c is not coreferent (R = 4), in which casethe probability is estimated according tothe ~c = 4 case.?
~c is not coreferent but is a member ofthe same family as ~n (e.g., \John Sebas-tian Bach" and \Carl Philipp EmmanuelBach").
This case (R = f) is computedas the non-coreference case, but the sec-ond occurrence is given \credit" for the lastname.?
~c is coreferent to ~n, in which case we com-pute the probability as described below.
Inthis case we assume that any words sharedby the two names must appear with thesame label, and except for descriptions, la-bels may not change between them (e.g.,if ~c has a rst name, then ~n can be givena rst name only if it is the same word asthat in ~c).
This does not allow for nick-names and other such cases, but they arerare in the Wall Street Journal.More formally, we havep(~n,~l | ~c) =?Rp(R)p(~n,~l | R,~c) (4)We then estimate p(~n,~l | R,~c) as follows:?
if R = 4, compute p(~n,~l) as estimatedfrom equation 2.?
if R = f , then p(~n,~l)/p(s | ~l(s)) where s isthe word in common that caused the previ-ous name to be selected as a possible coref-erent and ~l(s) is the label assigned to s ac-cording to ~l.?
if R = ~c, use equation 8 below.In equation 4 the R = 4 case is reasonablystraight-forward: we simply use equation 2 asthe non-coreferent distribution.
For R = f , aswe noted earlier, we want to claim that the newname is a member of the same family as that ofthe earlier name.
Thus, as we said earlier, weget \credit" for the repeated family name.
Thisis why we take the non-coreferent probabilityand divide by the probability of what we taketo be the family name.This leaves the coreferent case.
The basicidea is that we view the labeling of new name(~l) as a transformation of the labeling of theold one (~l0).
However, we do not know ~l0 so wehave to sum over all possible former labelingsL?.
This is expressed asp(~n,~l | ~c) =?~l?2L?p(~l0 | ~c)p(~n,~l | ~l0,~c) (5)The rst term, p(~l0 | ~c), is easy to computefrom equation 2 using Bayes law.
We now turnour attention to the second term.To establish a more detailed relationship be-tween the old and new names we compute pos-sible correspondences between the two names,where a correspondence species for each wordin the old name if it is retained in the new name,and if so, the word in the new name to which itcorresponds.
Two words may correspond onlyif they are the same lexical item.
(The conversedoes not hold.)
Since in principle there can bemultiple correspondences, we introduce the cor-respondences ?
by summing the probability overall of them:p(~n,~l | ~l0,~c) =?
?p(~n,~l, ?
| ~l0,~c) (6)?
max?p(~n,~l, ?
| ~l0,~c) (7)In the second equation we simplify by makingthe assumption that the sum will be dominatedby one of the correspondences, a very good as-sumption.
Furthermore, as is intuitively plau-sible, one can identify the maximum ?
with-out actually computing the probabilities: it isthe ?
with the maximum number of words re-tained from ~c.
Henceforth we use ?
to denotethis maximum-probability correspondence.By specifying ?
we divide the words of theold name into two groups, those (R) that areretained in the new name and those (S) that aresubtracted when going to the new name.
Simi-larly we have divided the words of the new nameinto two classes, those retained and those added(A).
We then assume that the probability of aword being subtracted or retained is indepen-dent of the word and depends only on its label(e.g., the probability of a subtraction given thelabel l is p(s | l)).
Furthermore, we assume thatthe labels of words in R do not change between~l0 and ~l.
Once we have pinned down R and S,any words left in ~l must be added.
However,we do not yet \know" the labels of those, sowe need a probability term p(l | a).
Lastly, forwords that are added in the new name, we needto guess the particular word corresponding tothe label type.
This gives us the following dis-tribution:p(~n,~l, ?
| ~c,~l0) =?w2Sp(s | ~l0(w))?w2Rp(r | ~l0(w))?w2Ap(l | a)p(w | l) (8)Taken together, equations 2 | 8 dene ourprobability model.4 ExperimentsFrom the work on named entity recognition weobtained a list of 145,670 names, of which 87,809were marked as personal names.
A second pro-gram creates an ordered list of names that ap-pear in each article in the corpus.
The two les,names and article-name occurrences, are the in-put to our procedures.With one exception, all the probabilities re-quired by the two models are initialized withflat distributions | i.e., if a random variablecan take n possible values, each value is 1/n.The probabilites so set are:1. p(N (l) = n(l)) from equation 2 (the prob-ability that label l appears n(l) times),2. p(w(i) | l) from equation 2 (the probabilityof generating w(i) given it has label l),3. p(s | ~l0(w)), p(r | ~l0(w)), and p(a | ~l(w))from equation 8, the probabilities that athe label ~l(w) will be subtracted, retained,or added when going from the old name tothe new name.We then used the expectation-maximization(EM) algorithm to re-estimate the values.
Weinitially decided to run EM for 100 iterations asour benchmark.
In practice no change in perfor-mance was observed after about 15 iterations.The one exception to the flat probability dis-tribution rule is the probability distributionp(R), the probability of an antecedent beingcoreferent, a family relation, or non-coreferent.This distribution was set at .993, .002, and .005respectively for the three alternatives and thevalues were not re-estimated by EM.1 Figure1 show some of the probabilities for individualwords given the possible labels.The result shown in Figure 1 are basicallycorrect, with \Director" having a high proba-bility as a descriptor, (0.0059), \Ms."
having ahigh probability as honorific (0.058), etc.
Someof the small non-zero probabilities are due togenuine ambiguity (e.g., Fisher does occur as arst name as well as a last name) but more ofit is due to small confusions in particular cases(e.g., \Director" as a last-name, or \John" asdescriptor).After EM training we evaluated the programon 309 personal names from our names list thatwe had annotated by hand.
These names wereobtained by random selection of names labeled1These were the first values we tried and, as theyworked satisfactorily, we simply left them alone.Word p(w | 0) p(w | 1) p(w | 2) p(w | 3) p(w | 4) p(w | 5)Director 0.0059 0 2.0 10?5 0 0.00016 0Ms.
1.2 10?7 0.058 0.0041 2.9 10?14 0 0John 0.0037 2.9 10?6 0.038 9.7 10?6 0 0T.
0.0018 1.2 10?12 .00032 0.02 0 0Fisher 0 0 4.5 10?5 7.4 10?5 0.00073 0III 0 0 0 0 6.8 10?5 0.16Figure 1: Some example probabilities of words given labels after 15 EM iterationsas personal names by the name-entity recog-nizer.
If the named entity recognizer had mis-takenly classied something as a personal nameit was not used in our test data.For the name model we straightforwardlyused equation 2 to determine the most probablelabel sequence ~l for each name.
Note, however,that the testing data does not itself include anyinformation on whether or not the test namewas a rst or subsequent occurrence of an indi-vidual in the text.
To evaluate the coreferencemodel we looked at the possible coreference datato nd if the test-data name was most commonas a rst occurrence, or if not, which possibleantecedent was the most common.
If rst occur-rence prevailed, ~l was determined from equation2, and otherwise it was determined using equa-tion 3 with ~c set to the most common possiblecoreferent for this name.We compare the most probable labels ~l for atest example with the hand-labeled test data.We report percentage of words that are giventhe correct label and percentage of names thatare completely correct.
The results of our ex-periments are as follows:Model Label% Name%Name 92.6 85.1Coreference 97.0 94.5As can be seen, information about possiblecoreference was a decided help in this task, lead-ing to an error reduction of 59% for the numberof labels correct and 63% for names correct.5 Error AnalysisThe errors tend to arise from three situations:the name disobeys the name structure assump-tions upon which the program is based, thename is anomalous in some way, or sparse data.We consider each of these in turn.Many of the names we encounter do not obeyour assumptions.
Probably the most commonsituation is last names that, contrary to our as-sumption, are composed of more than word e.g.,\Van Dam".
Actually, a detail of our process-ing has caused this case to be under-representedin our data and testing examples.
As noted inSection 2, uncapitalized proper nouns were notallowed.
The most common extra last name isprobably \van," but all of these names were ei-ther truncated or ignored because of our pro-cessing step.In principle, it should be possible to allow formultiple last names, or alternatively have a newlabel for \rst of two last names".
In practice,it is of course the case that the more parameterswe give EM to ddle with, the more mischief itcan get into.
However, for a practical programthis is probably the most important extensionwe envision.Names may be anomalous while obeying ourrestrictions at least in the letter if not the spirit.Chinese names have something very much likethe rst-middle-last name structure we assume,but the family name comes rst.
This is partic-ularly relevant for the coreferent model, since itwill be the family name that is repeated.
Thereis nothing in our model that prevents this, butit is suciently rare that the program gets \con-fused".
In a similar way, we marked both \Dr.
"and \Sir" as honorics in our test data.
How-ever, the Wall Street Journal treats them verydierently from \Mr."
in that the former tendto be included even in the rst mention of aname, while the latter is not.
Thus in somecases our program labeled \Dr."
and \Sir" asdescriptors.Lastly, there are situations where we imag-ine that if the program had more data (or if thelearning mechanisms were somehow \better") itwould get the example right.
For example, thename \Mikio Suzuki" appears only once in ourcorpus, as does the word \Mikio".
\Suzuki"appears two times, the rst being in \YotaroSuzuki" who is mentioned earlier in the samearticle.
Unfortunately, because \Mikio" doesnot appear elsewhere, the program is at a lossto decide which label to give it.
However, be-cause Yotaro is assumed to be a rst name, theprogram makes \Mikio Suzuki" coreferent with\Yotaro Suzuki" by labeling \Mikio" descriptor.As noted briefly in section 3, we have consid-ered more complicated probabilities models toreplace equation 2.
The most obvious of these isto allow the distribution over numbers of wordsfor each label to be conditioned on the previ-ous label | e.g., a bi-label model.
This modelgenerally performed poorly, although the coref-erence versions often performed as well as thecoreference model reported here.
Our hypoth-esis is that we are seeing problems similar tothose that have bedeviled applying EM to taskslike part-of-speech tagging [7].
In such cases EMtypically tries to lower probabilities of the cor-pus by using the tags to encode common word-word combinations.
As the models correspond-ing to equations 2 and 8 do not include anylabel-label probabilities, this problem does notappear in these models.6 Other ApplicationsIt is probably clear to most readers that thestructure and probabilities learned by thesemodels, particularly the coreferent model, couldbe used for tasks other than assigning structureto names.
For starters, we would imagine thata named entity recognition program that usedinformation about name structure could do abetter job.
The named entity recognition pro-gram used to create the input looks at only afew features of the context in which the nameappears, the complete name, and the individ-ual words that appear in the name irrespectiveof the other words.
Since the dierent kindsof names (person, company and location) dierin structure from one another, a program thatsimultaneously establishes both structure andtype would have an extra source of information,thus enabling it to do a better job.Our name-structure coreferent model is alsolearning a lot of information that would be use-ful for a program whose primary purpose is todetect coreference.
One way to see this is tolook at some of the probabilities that the pro-gram learned.
Consider the probability that wewill have an honorific in a rst occurrence of aname:p(n(1) = 1) = .000044 (9)This is very low.
Contrast this with the prob-ability that we add an honorific in the secondoccurrence:p(a | honorific) = 1 (10)These dramatic probabilities are not, in fact,accurate, as EM tends to exaggerate the eectsby moving words that do not obey the trend outof the honorific category.
They are, however, in-dicative of the fact that in the Wall Street Jour-nal names are introduced without honorics,but subsequent occurrences tend to have them(a fact we were not aware of at the start of thisresearch).Another way to suggest the usefulness ofthis research for coreference and named-entityrecognition is to consider the cases where ourprogram?s crude lter suggests a possible an-tecedent, but the probabilistic model of equa-tion 4 rejects this analysis.
The rst 15 casesare given in gure 2.
As can be seen, except for\Mr.
President" and \President Reagan", allof the examples are either not coreferent or arenot people at all.7 ConclusionWe have presented two methods for the un-supervised discovery of personal-name struc-ture.
The two methods dier in that the seconduses multiple, possibly coreferent, occurrencesof names to constrain the problem.
The meth-ods perform at a level of 92.6 and 97.0 percentaccuracy respectively.The methods are of potential interest in theirown right, e.g., to improve the level of detailprovided by Penn Treebank style parses.
Aswe have also noted, we should also be able touse this research in the quest for better unsu-pervised learning of named-entity recognition,and the model that attends to coreference in-formation can potentially be useful for programsaimed directly at this latter problem.Finally, many of us believe that the powerof unsupervised learning methods for linguisticVice President President ReaganMr.
President President ReaganDean P. Guerin Guerin & TurnerRonald Reagan White House SpeakerHouse James WrightRev.
Leon H. Sullivan Sullivan PrinciplesMr.
Sullivan Sullivan PrinciplesRev.
Leon Sullivan Sullivan PrinciplesSouth Dakota Republican PartyRepublicanKansas Republican Republican PartyCyril Wagner Jr. Wagner & BrownGeneral Preston R. Laurence A. TischTischLt.
Col. Oliver Whitney NorthNorth Seymour Jr.Republican White House RepublicansHouseJ.
Seward Johnson Johnson & JohnsonVice President President NixonFigure 2: The rst 15 cases in which the coref-erent model rejected the coreferent hypothesisinformation will be proportional to the depth ofsemantic or pragmatic analysis that goes intothe features they consider.
The vastly superiorperformance of the coreference model over thebasic name model moves this believe some smalldistance from hope to hypothesis.References1.
Berland, M. and Charniak, E. Findingparts in very large corpora.
In Proceedingsof the ACL 1999.
ACL, New Brunswick NJ,1999, 57{64.2.
Brill, E. Unsupervised learning of disam-biguation rules for part of speech tagging.
InProceedings of the Third Workshop on VeryLarge Corpora.
1995, 1{13.3.
Caraballo, S. A.
Automatic constructionof a hypernym-labeled noun hierarchy fromtext.
In Proceedings of the ACL 1999.
ACL,New Brunswick NJ, 1999.4.
Collins, M. and Singer, Y. Unsuper-vised models for named entity classification.In Proceedings of the 1999 Joint Sigdat Con-ference on Empirical Methods in NaturalLanguage Processing and Very Large Cor-pora.
Association for Computational Linguis-tics, 1999.5.
Ge, N., Hale, J. and Charniak, E. Astatistical approach to anaphora resolution.In Proceedings of the Sixth Workshop onVery Large Corpora.
1998, 161{171.6.
Linguistic Data Consortium.
BLLIP1987-1989 WSJ Corpus Release 1.
2000.7.
Merialdo, B. Tagging English text with aprobabilistic model.
Computational Linguis-tics 20 (1994), 155{172.8.
Riloff, E. Automatically generating ex-traction patterns from untagged text.
InProceedings of the Thirteenth NationalConference on Artificial Intelligence.
AAAIPress/MIT Press, Menlo Park, 1996, 1044{1049.9.
Riloff, E. and Shepherd, J.
A corpus-based approach for building semantic lexi-cons.
In Proceedings of the Second Confer-ence on Empirical Methods in Natural Lan-guage Processing.
1997, 117{124.10.
Roark, B. and Charniak, E. Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction.
In36th Annual Meeting of the Association forComputational Linguistics and 17th Interna-tional Conference on Computational Linguis-tics.
1998, 1110{1116.11.
Yarowsky, D. Unsupervised word sensedisambiguation rivaling supervised methods.In Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguis-tics.
1995, 189{196.
