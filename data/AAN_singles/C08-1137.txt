Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1089?1096Manchester, August 2008Sentence Type Based Reordering Model for Statistical MachineTranslationJiajun Zhang, Chengqing Zong, Shoushan LiNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing 100190, China{jjzhang, cqzong, sshanli}@nlpr.ia.ac.cnAbstractMany reordering approaches have beenproposed for the statistical machinetranslation (SMT) system.
However, theinformation about the type of sourcesentence is ignored in the previousworks.
In this paper, we propose a groupof novel reordering models based on thesource sentence type for Chinese-to-English translation.
In our approach, anSVM-based classifier is employed toclassify the given Chinese sentences intothree types: special interrogative sen-tences, other interrogative sentences, andnon-question sentences.
The differentreordering models are developed ori-ented to the different sentence types.Our experiments show that the novel re-ordering models have obtained an im-provement of more than 2.65% in BLEUfor a phrase-based spoken languagetranslation system.1 IntroductionThe phrase-based translation approach has beenthe popular and widely used strategy to the sta-tistical machine translation (SMT) since Och, etal.
(2002) proposed the log-linear model.
How-ever, reordering is always a key issue in the de-coding process.
A number of models have beendeveloped to deal with the problem of reorder-ing.
The existing reordering approaches couldbe divided into two categories: one is integratedinto the decoder and the other is employed as apreprocessing module.?
2008.
Licensed under the Creative Commons Attribu-tion-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.Many reordering methods belong to the for-mer category.
Distortion model was first em-ployed by Koehn et al (2003); a lexicalized re-ordering model was proposed by Och et al(2004) and Koehn et al (2005); and the formalsyntax-based reordering models were proposedby Chiang (2005) and Xiong et al (2006).
It isworthy to note that little syntactic knowledge isused in the models mentioned above.Compared to the reordering models that areintegrated into the decoder, the reordering at thesource side can utilize more syntactic knowl-edge, with the goal of  adjusting the source lan-guage sentence to make its word order closer tothat of the target language.
The most notablemodels are given by Xia and McCord (2004),Collins et al (2005), Li et al (2007) and Wanget al (2007).
Xia and McCord (2004) parsed thesource and target sides of the training data andthen automatically extracted the rewriting pat-terns.
The rewriting patterns are employed onthe input source sentence to make the word or-der more accordant to target language.
Collins etal.
(2005) described an approach to reorder Ger-man in German-to-English translation.
Themethod concentrates on the German clauses andsix types of transforming rules are applied to theparsed source sentence.
However, all the rulesare manually built.
Li et al (2007) used a parserto get the syntactic tree of the source languagesentence.
In this method, a maximum entropymodel is developed to determine how probablethe children of a node are to be reordered.
Obvi-ously, there is also disadvantage in this methodbecause the parsing tree is obtained by a fullparser and contains too many nodes that are notinvolved in desired reorderings.
Wang et al(2007) discussed three categories which are con-sidered to be the most prominent candidates forreordering in Chinese-to-English translation,including verb phrases (VPs), noun phrases(NPs), and localizer phrases (LCPs).
The1089method deals with some special modifiers ofVPs and NPs because they have the propertythat some specific modifiers appear before VPsor NPs in Chinese but occur after VPs or NPs inits English translation.
We observe that all thetransformation rules in this method are hardcrafted.
Furthermore, there are some other re-lated works, such as Costa-jussa and Fonollosa?swork (2006) and Zhang et al?s work (2007).Costa-jussa and Fonollosa (2006) considered thesource reordering as a translation task whichtranslates the source sentence into reorderedsource sentence.
A chunk-level reorderingmodel was first proposed by Zhang et al (2007).However, all the existing models make nodistinction between the different types of thesource sentence.
Intuitively, we have differentreordering information in different sentence type.Taking Chinese special interrogative sentence asan example, there is a fixed phrase that usuallyoccurs at the end of Chinese sentence but ap-pears at the beginning part of its English transla-tion.
See the following Chinese to English trans-lation:Chinese: ?
?
?
???
?
??
?English: What kind of seats do you like ?Obviously, the Chinese question phrase ????
?
??
(What kind of seats)?
should beput at the beginning of its English translation.However, many phrase-based systems fail to dothis.In this paper, we are interested in investigat-ing the value of Chinese sentence types in reor-dering for Chinese-to-English spoken languagetranslation.
Due to the syntactic difference be-tween Chinese and English, different sentencetype provides different reordering information.A phrase-ahead model is developed to exploitand utilize the reordering information of specialinterrogative sentences.
A phrase-back model isemployed to catch and make use of the reorder-ing information of other sentence types.
How-ever, the sentence type should be first identifiedby an SVM-based classifier before reorderingthe source sentence.
The method overall is usedas a preprocessing module for translation.
Wewill introduce our method in detail later.The remainder of this paper is organized asfollows: Section 2 introduces our motivations;Section 3 gives the details on the implementa-tion of our approach; the experiments are shownin Section 4; and the final concluding remarksare given in Section 5.2 Our MotivationsIn this section, before we analyze the Chinese-to-English spoken language translation corpus,some definitions are given first.2.1 Definitionsz Special interrogative sentence / other inter-rogative sentence / non-question sentenceChinese sentence can be divided into questionsentence and non-question sentence.
If a Chi-nese question sentence is translated into theEnglish sentence of wh-questions, the sentenceis named as a Chinese special interrogative sen-tence; otherwise, it is called the Chinese otherinterrogative sentence.
Figure 1-3 show someexamples for the three sentence types respec-tively.z SQP / TP / SPIn Chinese special interrogative sentence, thequestion phrase is always moved ahead while itis translated into English.
Correspondingly, thequestion phrase is named as the special questionphrase (SQP).
For example, the question  phrase????
?
??
(What kind of seats)?
in theexample mentioned above is an SQP.A few quantifier phrases (QPs) like ??
?
(many times)?, ???
?
(many years)?
in Chi-nese and some LCPs like ???
??
?
(afterthe accident happened)?, ???
??
?
(beforethe meeting ends)?
together with some NPs liketemporal phrases are named temporal phrase(TP) in our model.
Some LCPs like ???
?
(atthe front of the hotel)?, ???
?
(near the ta-ble)?
and a few NPs like spatial phrases arecalled spatial phrase (SP) in our model.
As PPs1,TPs and SPs are the most prominent candidatesfor reordering in Chinese other interrogativesentences and non-question sentences, they willbe handled in the phrase-back reordering model.Figure 1.
An example of Chinese special inter-rogative sentence with its English translation.Figure 2.
An example of Chinese other inter-rogative sentence with its English translation.1 PPs here mean prepositional phrases?
?
?
??
?
?Can you speak Japanese ??
?
?
???
?
??
?What kind of seats do you like ?1090My wallet was stolen in the subway .Figure 3.
An example of Chinese non-questionsentence with its English translation.2.2 Analysis of Corpus and  Our MotivationsIn order to have an overview of the distributionof the Chinese sentence types, we have made asurvey based on our training set for translation,which contains about 277k Chinese and Englishsentence pairs.
We found that about 17.2% ofthe sentences are special interrogative sentences,about 25.5% of sentences are other interrogativesentences and the remainders are all non-question sentences.Each sentence type has its own reorderingstrategy, as demonstrated in Figures 1-3.
Thereis a settled phrase (SQP) in Chinese special in-terrogative sentence which usually appears atthe end but will be translated first in English,just as Figure 1 illustrates.
For other interroga-tive sentences, some specific Chinese words like????????
will just be translated into?Can?
or ?Do?
and come first in English.
Atpresent, this information is not used in our ap-proach.
Figure 2 gives an example.
For non-questions, the reordering candidates usuallyneed to be moved back during translation.
Anexample is shown in Figure 3.According to the analysis above, it is mean-ingful to develop reordering models based onthe source sentence types.2.3 FrameworkAs we mentioned above, our framework is illus-trated as follows:Figure 4.
Architecture of the framework, whereC1 means the special interrogative sentence, C2is other interrogative sentence and C3 is non-question sentence.Conventional preprocessing approaches di-vide the translation into two phases:(1) 'S S T?
?
'''cS S S T?
?
?ccS'SReordering is first done in the source sidewhich changes the source sentence S into reor-dered one S , and then a standard phrase-basedtranslation engine is used to translate the reor-dered source sentence S  into target languagesentence T.?
??
?
??
?
??
?
?In our method, to utilize the information ofsentence types, a new approach is proposed toimprove the translation performance by devel-oping a hybrid model as follows:(2)Before the source sentence is reordered, anSVM-based classifier is first employed to de-termine its sentence type S , then, different re-ordering model is used to reorder the sourcesentence with the specific sentence type .
Af-ter getting the reordered source sentence , weuse our phrase-based SMT to obtain the optimaltarget language sentence.The contribution of this paper is embodied inthe first two steps of our method.In the first step, an SVM classifier is used toidentify the type of source sentence2.In the second step, two reordering models arebuilt according to the different sentence types.
Aphrase-ahead reordering model is developed forthe special interrogative sentences which usesshallow parsing technology to recognize themost prominent candidates for reordering (spe-cial question phrase) and extracts reorderingtemplates from bilingual corpus.
For other sen-tence types, we build a phrase-back reorderingmodel which uses shallow parsing technology toidentify the phrases that are almost alwaysmoved back during translation and appliesmaximum entropy algorithm to determinewhether we should reorder them.Source textsentence 3 Models and AlgorithmsIn this section, we first introduce the sentencetype classifier model, and then we describe indetail the two reordering models, phrase-aheadreordering model and phrase-back reorderingmodel.3.1 Sentence Type IdentificationMany models are used for classification such asNa?ve Bayes, decision tree and maximum en-tropy.
In our approach, we use an SVM-basedclassifier to classify the sentence types.
SVM2 There are three sentence types: special interrogative sen-tence, other interrogative sentence and non-question sen-tence, which are defined in sub-section 2.1.TargetsentenceC1C3C2Phrase-aheadmodelPhrase-backmodelPhrase-baseddecoderSVMclassifierPhrase-backmodel1091has been shown to be highly effective at tradi-tional text categorization.
For our problem, weregard a sentence as a text.
The decision bound-ary in SVM is a hyperplane, represented by vec-tor , which separates the two classes, leavingthe largest margin between the vectors of thetwo classes (Vapnik, 1998).
The search of mar-gin corresponds to a constrained optimizationproblem.
SupposewG{1, 1}jc ?
?
(positive andnegative) be the correct class of sentence js , thesolution can be formalized as:: j j jjw c?=?G Gs 0j   ?
?
(3)Where the jsGis feature vector of our sen-tence js .
We get j?
s through solving a dualoptimization problem.
Identifying the type of asentence is just to determine which side of wG?shyperplane it will fall in.Feature selection is an important issue.
Wedirectly use all the words occurring in the sen-tence as features.Some readers may argue that the features todistinguish the sentence types are very obviousin Chinese.
For example, ???
can easily sepa-rate the interrogative sentences from non-question sentences.
In this case, a simple classi-fier like decision tree will work.
It is true whenthe punctuation always appears in the sentence.However, sometimes there is no punctuation inthe spoken language text.
Under this situation,the decision tree will lose the most powerfulfeatures, but the performance of SVM is not af-fected by the punctuations.
The experimentalresults verifying this will be given in Section4.3.2 Phrase-ahead Reordering ModelAs we mentioned above, about 17.2% of thespoken language sentences are special interroga-tive sentences.
Furthermore, we note that eachChinese special interrogative sentence has oneor more special question phrases (SQP) that wedefined in section 2.1.
Due to the difference be-tween Chinese and English word order, the SQPneeds to be moved ahead3 when it is translatedinto English.Let S be a Chinese special interrogative sen-tence, our first problem is to recognize the SQPsin S. If we have known the SQP, namely S be-comes  (  is the left part of  the 0    S SQP S1 0S1S0S3 There is a specific situation that the SQP don?t have to bemoved.
In this case, we suppose it needs to be moved, butthe distance is 0.sentence before SQP, and  is the right part ofthe sentence after SQP), our second problem isto find the correct position in where SQP willbe moved to.For the first problem, because each syntacticcomponent is possible a SQP, for example, ????
?
???
in Figure 1 is NP, ??
??(Where)?
in Chinese sentence ??
?
??
??
?
?
?
(Where can I buy the ticket?)?
isPP (also a VP modifier), ???
?
(How togo)?
in ??
??
??
?
?
(How to go to thebeach?)?
is VP, it is very difficult to find theSQP by syntax.
In our model, we first find outall the key words, which we list below, in thespecial interrogative sentences through mutualinformation.
Then, we define the syntactic com-ponent containing the key word as an SQP.
In-stead of full syntactic parser, we utilize a CRFtoolkit named FlexCrfs4 to train, test and predictthe SQPs chunking.??
What?
(??
/ ???)
Where?
(??
/ ???)
How much/many/old??
(??/???
?)
What about/How?
(??
/ ???)
Who/whose/whom?
(??
/ ???)
How many/old When????
Why?(??
/ ???)
When/whereTable 1.
The special key words setFor the second problem, we note that thereare only three positions where the SQP will bemoved to:  (1) the beginning of the sentence; (2)just after the rightmost punctuation (?,?, ?;?
or?:?)
before the SQP; (3) or after a regular phrasesuch as ???
(May I ask)?
and ???
?
(Please tell me)?.
Therefore, we can learn thereordering templates from bilingual corpus 5 .The simple algorithm is illustrated in Figure 5,and some reordering templates are shown in Ta-ble 2.On the whole, When we reorder the specialinterrogative sentence, we first identify the SQP,then we find out whether there are punctuations(?,?
, ?;?
or ?:?)
before SQP; if any, we keep therightmost punctuation index, otherwise we keepthe index 0 (beginning of sentence).
In the third4 See http://flexCRF.sourceforge.net5 The bilingual corpus is the corpus combined by the train-ing corpus for chunking SQPs and its corresponding Eng-lish translation.1092step, if we find that a reordering template likesome one given in Table 2 can match the sen-tence, we just apply the template, otherwise wejust move the SQP after the index that we keptefore (0 or punctuation index).empirical value N is 10 in our ex-eriment.bFigure 5.
Reordering template extraction algo-rithm.
ThepX1??
X2 SQP X1 ??
SQP X2X1 ??
?
X2 SQP X1 ??
?
SQP X2X1 P X1 ?
?
??
X2 SQ ?
?
??
SQP X2X1 ?
SQP X1 ?
P X2?
X2 ?
SQ??
?
?Table 2.
Some reordering templates3.3 Phrase-back Reordering ModelIn this paper, we employ the phrase-back reor-dering model for Chinese other interrogativeposi-tiomakes our model suitable form e???
(sign your name)?identified as a NP.z The form of phrase-back reordering rules:sentences and non-question sentences.Inspired by the work of Wang et al (2007),we only consider the most prominent candidatesfor reordering.
The VP modifiers like PP, TP,and SP which we defined in sub-section 2.1 aretypically in pre-verb position in Chinese but al-most always appear after the verb in its corre-sponding English translation.
Wang et al (2007)concentrate on VP, NP, then determine whethertheir modifiers should be moved back.
Instead,our interests are focused on the modifiers: PP,TP and SP; namely, we consider the modifiersPP, TP and SP as triggers, and the first VP oc-curring after triggers will be the candidaten where the triggers may be moved to.Changing the focus gives us the ability tohandle a specific situation that there is no VPafter the triggers for recognition error or otherreasons.
As the example in Figure 6, there is noVP after PP (??
???)
because the phrase ????
next to PP is wrongly recognized to be a NP.To deal with the case, we will further define afake verb phrase (FVP): the phrase after PP (TPor SP) until the punctuation (?,?, ?;?
or ?.?).
Thephrase ???
(sign your name)?
in Figure 6 isan FVP.
Here, FVP is given the same functionwith VP, thus itor  situations.Figure 6.
An example of FVP.
In our model thewhole sentence is recognized as a VP, ??
??(here)?
is a PP, andisUnlike hard reordering rules of Wang et al(2007), we develop a probabilistic reorderingmodel to alleviate the impact of the errorscaused by the parser when recognizing PPs, TPs,SPs and VPs.
We believe that no reordering isbetter than bad reordering.
The rule forms and1:  Input: special interrogative sentence pair (s, t) in whichse which aligns tondex-1]NONE then;_Phrase if Count(C_Phrase)<NSQP is labeled and their alignment M is given2:  R={}3: Find the rightmost punctuation index c_punc_index beforeSQP and English index e_punc_index aligned toc_punc_index4: Find the smallest index e_smallest_index of English whichalign to the SQPC_Phra5: Get the Chinese phrase[e_punc_index+1, e_smallest_i6:  if C_Phrase is7:       Continue ;8:  end ifPhrase in R then 8:  if C_9:       Count(C_Phrase)++;10: else11:     Insert C_Phrase into R12:     Count(C_Phrase)=1;13: end if14: remove C?
?
??
??
?the probabilistic model will be given as follows:A : 1 222 1A XA straightA XA1 XA A inver??
?
?Where, 1 { , , }A PP TP SPted?
{ , }VP FVP?
2A1 2{ }X phrases between A  and A?z We use the Maximum Entropy Modelwhich is implemented by Zhang6.
The model istrained from bilingual spoken language corpusdetermine whether 1A  should be moved after2A .
The features that we investigated includethe leftmost, rightmost, and their POSstoof 1Aand 2A .
It leads to the following formula:exp( ( , ))( | )exp( ( , ))i iii iO ih O AP O Ah O A??=??
?
(4)sWhere, { , }O traignt inverted?
, ( , )ih O A  is afeature, and i?
is the weight of the feature.When app  the rules, we first identifypairs like ( 1 2A XA ) in the sentence, and thenm beginning t1A  behind 2A  if ( | ) ( | )P inverted A P straight A> .After all the pairs are prlyingfro o end of the sentence, we moveocessed, we will get thereordered source result.6http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html15: output R10934 ExperimentsWe have conducted several experiments toevaluate the models.
In this section, we firstintroduce the corpora, and then we discuss theperformance of the SVM-based classifier,chunking and reordering models respectively.4.1 CorporaWe perform our experiments on Chinese-to-English speech translation task.
The statistics ofthe corpus is given in Table 3 where CE_trainmeans the Chinese-to-English training data re-leased by IWSLT 2007; CE_sent_filtered meansthe bilingual sentence pairs filtered from theopen resources of the bilingual sentences on thewebsite; CE_dict_filtered means the bilingualdictionary filtered from the open resources ofthe bilingual dictionaries on the website;CE_dev123 denotes the bilingual sentence pairsobtained by the combination of the developmentdata IWSLT07_CE_devset1, IWSLT07_CE_devset2and IWSLT07_CE_devset3 which are releasedby the IWSLT 2007; CE_dev4 and CE_dev5 arethe remainder of development data released byIWSLT 2007; CE_test means the final test setreleased by IWSLT 2007.We combine the data from the top four rowsas our training set.
We use CE_dev4 as our de-velopment set.
CE_dev5 and CE_test are ourtwo test data.
The test data released by IWSLT2007 is based on the clean text with punctuationinformation, so we add the punctuation informa-tion on the Chinese sentences of CE_dev4 andCE_dev5 by our SVM sentence type classifier toform the final development set.
The detailedstatistics are given in Table 4.4.2 Classification ResultTo evaluate the performance of SVM-basedclassifier on classifying the sentence types, wefirst use a simple decision tree to divide theChinese sentences of our training data for trans-lation into three sentence types.
Then we cleanthem by hand in order to remove the errors.
Atlast, 10k sentences for each sentence type arerandomly selected as the experiment data.
Foreach sentence type, 80% of the data are used astraining data, 20% as test data.
Table 5 gives theclassification results.Punctuation in Table 5 means the punctuationwhich occurs at the end of the sentence such as???
and ???.
We can see from the table thatSVM classifier performs very well even if weremove the punctuations at the end of every sen-tence.
Therefore, almost no errors will be passedto the reordering stage.Data Chinese EnglishCE_train 39,953 39,953CE_sent_filtered 188,282 188,282CE_dict_filtered 31,132 31,132CE_dev123 24,192 24,192CE_dev4 489 3,423CE_dev5 500 3,500CE_test 489 2,934Table 3.
Statistics of training data, developmentdata and test dataChinese Englishsentences 276,633Train setwords 1,665,073 1,198,984sentences 489 489*7 Dev setCE_dev4 words 6241 47609sentences 500 500*7 Test setCE_dev5 words 6596 52567sentences 489 489*6 Test setCE_test words 3166 22574Table 4.
Detailed statistics of training data ondevelopment setAccuracy (%)With punctuation 99.80Without punctuation 98.00Table 5.
The accuracy of SVM classifier4.3 Chunking ResultsIn our experiment, except that VPs are obtainedby a syntactic parser (Klein and Manning, 2003),SQPs, PPs, TPs, SPs are all chunked by theFlexCrfs.The chunking data used for training and testin Table 6 are annotated by ourselves.
Everychunk is  annotated according to the definitionthat we define in sub-section 2.1.
The raw train-ing and test data are all extracted from our train-ing set for translation.
TPs, SPs are annotatedtogether; SQPs, PPs are annotated respectively.The statistics of the training and test data areshown in Table 6.
Table 7 gives the chunkingresults.The precision, recall and F-Measure are met-rics for the chunking results.
F-Measure followsthe criteria of CoNLL-20007.2*( * )precision recallF Measureprecision recall?
= +7 See  http://www.cnts.ua.ac.be/conll2000/chunking/1094Because the SQPs have the regularity thateach one contains a key word listed in Table 1,the result of SQPs chunking is quite good.Moreover, the chunking of PPs, TPs and SPsalso performs well.Train Testsentences 10,000 500 SQPchunks 10030 501sentences 10,000 500 PPchunks 10106 512sentences 11,000 500 SP and TPchunks 10342 523Table 6.
Statistics of train and test dataPrecision (%)Recall(%)F-Measure(%)SQP 95.52 95.52 95.52PP 94.65 93.31 93.98SP and TP 93.92 92.68 93.25Table 7.
Chunking results on test set4.4 Translation ResultsFor the translation experiments, BLEU-4 andNIST are used as the evaluation metric.
Thebaseline SMT uses the standard phrase-baseddecoder that applies the log-linear model (Ochand Ney, 2002).In the preprocessing module, all the Chinesewords are segmented by the free software toolkitICTCLAS3.08, and the POS tags are obtainedby using the Stanford parser with its POS pars-ing function.
For the decoder, the phrase table isobtained as described in (Koehn et al, 2005),and our 4-gram language model is trained by theopen SRILM9 toolkit.
It should be noted that weuse monotone decoding in translation.We have done three groups of experimentsfor translation.
The first one is to test the effectof phrase-ahead reordering model, the result ofwhich is shown in Table 8.
Compared to thebaseline system, phrase-ahead reordering modelimproves the results of the two test sets by0.41% and 1.87% in BLEU respectively.
Thedifference in the performance gains can be at-tributed to the fact that there are 100 Chinesespecial interrogative sentences in Test 2, whileonly 30 are found in Test 1.
Accordingly, thereordering candidates of Test 1 are much fewerthan that of Test 2.
Thus, we can conclude thatthe more special interrogative sentences the bet-ter performance of the translation.
Furthermore,8 See http://www.nlp.org.cn9 See http://www.speech.sri.com/projects/srilmthe results show that the reordering on specialinterrogative sentences is a good try.The second experiment is conducted to testthe effect of phrase-back reordering model.
Ta-ble 8 gives the results.
For the two test sets, themodel brings an improvement to the baseline by2.24% and 0.93% in BLEU respectively.
How-ever, the difference between them is still verybig.
We think there are two reasons: firstly,there are much more special interrogative sen-tences in Test 2 than in Test 1, so the sentencesof other sentence types in Test 2 are much fewerthan that in Test 1.
Thus, fewer candidates arefound in Test 2 than in Test 1.
Secondly, theaverage sentence length of Test 2 (6.5 words) ismuch shorter than that of Test 1 (13.2 words).We know that if the sentence is very short, thePP, TP, and SP will seldom occur.
Naturally,only 89 candidates are found in Test 2 but 366in Test 1.
Regardless of the difference, thephrase-back reordering model indeed improvesthe translation quality significantly.The last experiment merges the two reorder-ing model together.
The results in Table 8 showthat the overall reordering model has done verywell in both test sets: it improves the two testsets by 2.65% and 2.78% in BLEU score respec-tively.
It demonstrates that every reorderingmodel has a positive effect on translation.Therefore, our reordering model based on thesentence type is quite successful.5 Conclusions and Future WorkIn this paper, we have investigated the effect ofthe Chinese sentence types on reordering prob-lem for Chinese-to-English statistical machinetranslation.
We have succeeded in applying aphrase-ahead reordering model to process thespecial interrogative sentences and a phrase-back reordering model to deal with other sen-tence types.
Experiments show that our reorder-ing model obtains a significant improvement inBLEU score on the IWSLT-07 task.With the encouraging experimental results,we believe that we can mine more reorderinginformation from the Chinese sentence types.
Inthis paper, we only apply a phrase-back modelto reorder Chinese other interrogative sentences.In the next step, we will try to develop a specialreordering model for this sentence type.
Fur-thermore, we plan to integrate the phrase-backmodel into phrase-ahead model for special inter-rogative sentences and investigate the value ofthis integration.1095Table 8.
Statistics of translation resultsNotes: candidates here mean how many candidate reordering phrases are recognized for each model.
Sentencesmean the number of sentences belonging to the specific sentence type, i.e.
for phrase-ahead reordering in Test 1,31 special question phrases (SQP) are recognized in 30 Chinese special interrogative sentences.AcknowledgmentsThe research work described in this paper hasbeen partially supported by the Natural ScienceFoundation of China under Grant No.
60575043and 60736014, the National High-Tech Researchand Development Program (863 Program) ofChina under Grant No.
2006AA01Z194 and2006AA010108, the National Key TechnologiesR&D Program of China under Grant No.2006BAH03B02, and Nokia (China) Co. Ltd aswell.ReferencesCao Wang, Michael Collins and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing, 2007.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming ZhouMinghui Li and Yi Guan.
2007.
A probabilistic ap-proach to syntax-based reordering for statisticalmachine translation.
In Proceedings of 45th Meet-ing of the Association for Computational Linguis-tics .Dan Klein and Christopher D. manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of  41stMeeting of the Association for Computational Lin-guistics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of 43rd Meeting of Association for Com-putational Linguistics.Deyi Xiong, Qun Liu and Shouxun Lin.
2006. maxi-mum entropy based phrase reordering model forstatistical machine translation.
In Proceedings ofthe joint conference of the International Committeeon Computational Linguistics and the Associationfor Computational Linguistics 2006.Fei Xia and Michael McCord.
2004.
Improving a Sta-tistical MT system with automatically learned re-write patterns.
In Proceedings of 20th InternationalConference on Computational Linguistics.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models forstatistical machine translation.
In Proceedings of40th Meeting of Association for ComputationalLinguistics.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machinetranslation.
Computational Linguistics, 30:417-449Marta R. Costa-jussa and Jose A.R.
Fonollosa.
2006.Statistical machine reordering.
In proceedings ofConference on Empirical Methods in Natural Lan-guage Processing 2006.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In proceedings of 43rd Meeting of theAssociation for Computational Linguistics.Philipp Koehn, Franz J. Och.
and Daniel Marcu.
2003.Statistical Phrase-based Translation.
In proceed-ings of HLT-NAACL 2003.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinburgn System Descriptionfor the 2005 IWSLT Speech Translation Evalua-tion.
In International Workshp on Spoken Lan-guage Translation.Yuqi Zhang, Richard Zens and Hermann Ney.
2007.Chunk-level reordering of source language sen-tence with automatically learned rules for statisticalmachine translation.
In Proceedings of SSST,NAACL-HLT 2007/AMTA Workshop on Syntax andStructure in Statistical Translation.Vladimir Naumovich Vapnik.
1998.
Statistical Learn-ing Theory.
John Wiley and Sons, Inc.BLEU (%) NIST Sentences CandidatesBaseline 32.16 6.4844 500Phrase-ahead reordering 32.57 6.5579 30 31Phrase-back reordering 34.40 6.6857 470 366Test 1CE_dev5Phrase-ahead+phrase-back 34.81 6.7584 500 397Baseline 34.04 5.8340 489Phrase-ahead reordering 35.91 6.0693 100 97Phrase-back reordering 34.97 5.9172 389 89Test 2CE_testPhrase-ahead+phrase-back 36.82 6.1535 489 1861096
