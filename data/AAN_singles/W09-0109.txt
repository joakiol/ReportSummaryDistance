Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 41?42,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsWhat Do Computational Linguists Need to Know about Linguistics?Robert C. MooreMicrosoft ResearchRedmond, Washington, USAbobmoore@microsoft.comAbstractIn this position paper, we argue that al-though the data-driven, empirical para-digm for computational linguistics seemsto be the best way forward at the moment,a thorough grounding in descriptive lin-guistics is still needed to do competentwork in the field.
Examples are given ofhow knowledge of linguistic phenomenaleads to understanding the limitations ofparticular statistical models and to betterfeature selection for such models.Over the last twenty years, the field of com-putational linguistics has undergone a dramaticshift in focus from hand encoding linguistic factsin computer-oriented formalisms to applying sta-tistical analysis and machine learning techniquesto large linguistic corpora.
Speaking as someonewho has worked with both approaches, I believethat this change has been largely for the good, butI do not intend to argue that point here.
Instead, Iwish to consider what computational linguists (if itis still appropriate to call them that) need to knowabout linguistics, in order to work most produc-tively within the current data-driven paradigm.My view is that, while computational linguistsmay not need to know the details of particular lin-guistic theories (e.g., minimalism, LFG, HPSG),they do need to have an extensive understanding ofthe phenomena of language at a descriptive level.I can think of at least two somewhat distinct ap-plications of this sort of knowledge in empiricalcomputational linguistics.One application is to understand the structurallimitations of particular types of statistical models.For example, a descriptive generalization aboutlanguage is that coordinated structures tend to beinterpreted in such a way as to maximize structuralparallelism.
Thus, in the phrase ?young men andwomen?, ?young?
would normally be interpretedas applying to both ?men?
and ?women?, but inthe phrase ?young men and intelligent women?,?young?
would normally be interpreted as apply-ing only to ?men?.
Although both interpreta-tions are structurally possible for both phrases, thepreferred interpretations are the ones that maxi-mize structural parallelism.
This is a phenom-enon that is not describable in a general way ina simple statistical model in the form of a proba-bilistic context-free grammar (PCFG).
We couldenumerate many specific cases by making fine-grained distinctions in the nonterminals of thegrammar, but the tendency to favor parallel coordi-nated structures in general would not be expressed.This is not necessarily fatal to successful engi-neering applications of PCFGs, but a competentcomputational linguist should understand what thelimitations of the formalism are.Let me give another example from the notori-ously empirical field of statistical machine transla-tion (SMT).
At least some linguistic structure hasbeen creeping back into SMT recently in the formof hierarchical translation models, many of whichcan be viewed as instances of synchronous proba-bilistic (or more generally, weighted) context-freegrammars (SPCFGs).
This approach seems quitepromising, but since it is based on a bilingual ver-sion of PCFGs, not only does it share the limi-tations of monolingual PCFGs alluded to above,but it also has additional structural limitations inthe kind of generalizations over types of bilingualmappings it can model.My favorite example of such a limitation isthe translation of constituent (i.e., ?WH?)
ques-tions between languages that move questionedconstituents to the front of the question (?WH-movement?)
and those that leave the questionedconstituents in situ.
English is an example of theformer type of language, and Chinese (so I amtold) is an example of the latter.
If we wanted tomake a model of question translation from Chi-41nese to English, we would like it to represent ina unitary (or at least finitary) way the generaliza-tion, ?Translate the questioned constituent fromChinese to English and move it to the front of theEnglish sentence being constructed.?
This gener-alization cannot be expressed in an SPCFG, be-cause this type of model allows reordering to takeplace only among siblings of the same parent inthe constituent structure.
Fronting a questionedconstituent, however, typically requires movingan embedded constituent up several levels in theconstituent structure.
While we can express spe-cific instances of this type of movement using anSPCFG by flattening the intervening structure, wecannot hope to capture the generalization in fullbecause WH-movement in English is famouslyunbounded, as in ?What translation formalism didMoore claim to show that WH-movement couldnot be modeled in?
?In addition to providing a basis for understand-ing the limitations of what phenomena variousstatistical models can capture, a good knowledgeof descriptive linguistics is also very useful asa source of features in statistical models.
Agood example of this comes from acoustic mod-eling in speech recognition.
Acoustic models inspeech recognition are typically composed of se-quences of ?phone?
models, where a phone cor-responds approximately to the linguistic unit ofa phoneme.
For good recognition performance,however, phone models need to be contextualizedaccording to the other phones around them.
Com-monly, ?triphone?
models are used, in which aseparate model is used for each combination ofthe phone preceding and following the phone be-ing modeled.
This can require over 100000 dis-tinct models, depending on how many triphonesare possible in a given language, which createsa sparse data problem for statistical estimation,since many of the possible combinations are onlyrarely observed.One response to this sparse data problem is tocluster the states of the triphone models to reducethe number of separate models that need to be es-timated, and an effective way to do this is to usedecision trees.
Using a decision tree clusteringprocedure, the set of all possible triphones is re-cursively split on relevant features of the triphone.At each decision point, the feature chosen for split-ting is the one that produces the greatest improve-ment in the resulting model.
But what featuresshould be used in such a decision tree?
I onceheard a leading speech recognition engineer saythat he chose his feature set by including all thefeatures he could find in the linguistic phoneticsliterature.
Given that feature set, the decision treelearning procedure decided which ones to actuallyuse, and in what order.The examples presented above illustrate someof the kinds of linguistic knowledge that a compe-tent computational linguist needs to know in orderperform research at the highest level.
I am con-cerned that many of the students currently grad-uating in the field do not seem to have receivedsufficient exposure to the structure of language atthis level of detail.
For instance, a few years agoI pointed out the problem of modeling questiontranslation between Chinese and English to oneof the brightest young researchers working withSPCFGs, and the problem had never occurred tohim, even though he was a fluent speaker of bothlanguages.
I am sure this would be one of thefirst things that would occur to anyone broughtup on the debates of the 1980s about the limi-tations of context-free grammar, upon first expo-sure to the SPCFG formalism.
So, although I ama firm believer that the data-driven empirical ap-proach computational linguistics will remain themost fruitful research paradigm for the foresee-able future, I also think that researchers need afirm grounding in descriptive linguistics.42
